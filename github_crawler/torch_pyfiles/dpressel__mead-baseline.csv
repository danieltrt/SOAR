file_path,api_count,code
setup.py,0,"b'import os\nimport re\nimport ast\nimport shutil\nfrom itertools import product\nfrom setuptools import setup, find_packages\n\n\ndef get_version(file_name, version_name=""__version__""):\n    with open(file_name) as f:\n        tree = ast.parse(f.read())\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Assign):\n                if node.targets[0].id == version_name:\n                    return node.value.s\n    raise ValueError(f""Unable to find an assignment to the variable {version_name} in file {file_name}"")\n\n\nclass About(object):\n    NAME = \'baseline\'\n    AUTHOR = \'dpressel\'\n    VERSION = get_version(\'baseline/version.py\')\n    EMAIL = ""{}@gmail.com"".format(AUTHOR)\n    URL = ""https://www.github.com/{}/{}"".format(AUTHOR, NAME)\n    DOWNLOAD_URL = ""{}/archive/{}.tar.gz"".format(URL, VERSION)\n    DOC_URL = ""{}/tree/master/"".format(URL)\n    DOC_NAME = \'docs/{}.md\'.format(NAME)\n\ndef get_configs(config_loc):\n    """"""include everything in config_loc as package data.""""""\n    configs = []\n    for f in os.listdir(config_loc):\n        configs.append(os.path.join(config_loc, f))\n    write_manifest(configs)\n    return configs\n\ndef write_manifest(lines):\n    with open(""MANIFEST.in"", ""w"") as f:\n        f.write(""\\n"".join(map(lambda x: \'include {}\'.format(x), lines)))\n\ndef fix_links(text):\n    """"""Pypi doesn\'t seem to host multiple docs so replace local links with ones to github.""""""\n    regex = re.compile(r""\\[(.*?)\\]\\(((?:docs|docker)/.*?\\.md)\\)"")\n    text = regex.sub(r""[\\1]({}\\2)"".format(About.DOC_URL), text)\n    return text\n\ndef read_doc(f_name, new_name=None, fix_fn=fix_links):\n    """"""\n    Because our readme is outside of this dir we need to copy it in so\n    that it is picked up by the install.\n    """"""\n    if new_name is None:\n        new_name = f_name\n    path = os.path.dirname(os.path.realpath(__file__))\n    doc_loc = os.path.normpath(os.path.join(path, \'..\', f_name))\n    new_loc = os.path.join(path, new_name)\n    if os.path.isfile(doc_loc):\n        shutil.copyfile(doc_loc, new_loc)\n    descript = open(new_loc, \'r\').read()\n    return fix_fn(descript)\n\ndef main():\n    setup(\n        name=\'mead-{}\'.format(About.NAME),\n        version=About.VERSION,\n        description=\'Strong Deep-Learning Baseline algorithms for NLP\',\n        long_description=read_doc(About.DOC_NAME, new_name=\'README.md\'),\n        long_description_content_type=""text/markdown"",\n        author=About.AUTHOR,\n        author_email=About.EMAIL,\n        license=\'Apache 2.0\',\n        url=About.URL,\n        download_url=About.DOWNLOAD_URL,\n        packages=find_packages(exclude=[\'tests\', \'layers*\', \'api-examples\']),\n        package_data={\n            \'mead\': get_configs(\'mead/config\'),\n        },\n        include_package_data=True,\n        install_requires=[\n            \'numpy\',\n            \'six\',\n            \'mead-layers=={}\'.format(About.VERSION),\n        ],\n        extras_require={\n            \'test\': [\'pytest\', \'mock\', \'contextdecorator\', \'pytest-forked\'],\n            \'report\': [\'visdom\', \'tensorboardX\'],\n            \'yaml\': [\'pyyaml\'],\n            \'tf2\': [\'tensorflow_addons\'],\n            \'grpc\': [\'grpc\'],\n            \'onnx\': [\'onnxruntime\']\n        },\n        entry_points={\n            \'console_scripts\': [\n                \'mead-train = mead.trainer:main\',\n                \'mead-export = mead.export:main\',\n                \'mead-clean = mead.clean:main\',\n                \'mead-eval = mead.eval:main\',\n                \'bleu = baseline.bleu:main\',\n                \'conlleval = baseline.conlleval:main\',\n            ]\n        },\n        classifiers={\n            \'Development Status :: 3 - Alpha\',\n            \'Environment :: Console\',\n            \'Intended Audience :: Developers\',\n            \'Intended Audience :: Science/Research\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Natural Language :: English\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n            \'Programming Language :: Python :: 3.7\',\n            \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        },\n        keywords=[\'deep-learning\', \'nlp\', \'pytorch\', \'tensorflow\'],\n    )\n\nif __name__ == ""__main__"":\n    main()\n'"
addons/__init__.py,0,b''
addons/paired.py,5,"b'from torch.utils.data.dataset import IterableDataset, TensorDataset\nfrom baseline.vectorizers import Token1DVectorizer, BPEVectorizer1D\nfrom baseline.progress import create_progress_bar\nfrom baseline.reader import register_reader\nfrom eight_mile.utils import str2bool, write_yaml, read_yaml, Offsets\nimport torch\nimport glob\nimport numpy as np\nimport os\n\n\nclass MultiFileLoader(IterableDataset):\n\n    def __init__(self, directory, pattern, vocabs, vectorizers, nctx):\n        super().__init__()\n        self.src_vectorizer = vectorizers[\'src\']\n        self.tgt_vectorizer = vectorizers[\'tgt\']\n        self.pattern = pattern\n        self.nctx = nctx\n        self.directory = directory\n        self.vocab = vocabs\n        self.samples = 0\n        self.rank = 0\n        self.world_size = 1\n        if torch.distributed.is_initialized():\n            self.rank = torch.distributed.get_rank()\n            self.world_size = torch.distributed.get_world_size()\n\n        if os.path.exists(f""{directory}/md.yml""):\n            f = read_yaml(f""{directory}/md.yml"")\n            self.samples = f[\'num_samples\']\n        else:\n            files = list(glob.glob(f""{directory}/{self.pattern}""))\n            pg = create_progress_bar(len(files))\n            for file in pg(files):\n                with open(file) as rf:\n                    for _ in rf:\n                        self.samples += 1\n            write_yaml({\'num_samples\': self.samples}, f""{directory}/md.yml"")\n\n    def __len__(self):\n        return self.samples\n\n    def __iter__(self):\n        # Each node has the same worker_info, so the unique offsets for each is\n        # rank * num_workers + worker_id\n        # and the total available workers is world_size * num_workers\n        worker_info = torch.utils.data.get_worker_info()\n        files = sorted(list(glob.glob(f""{self.directory}/{self.pattern}"")))\n\n        if worker_info is None:\n            num_workers_per_node = 1\n            node_worker_id = 0\n        else:\n            num_workers_per_node = worker_info.num_workers\n            node_worker_id = worker_info.id\n        all_workers = (self.world_size * num_workers_per_node)\n        files_per_worker = len(files) // all_workers\n        offset = self.rank * num_workers_per_node + node_worker_id\n        start_idx = offset * files_per_worker\n        end_idx = start_idx + files_per_worker if offset < all_workers - 1 else len(files)\n\n        for file in files[start_idx:end_idx]:\n            with open(file) as rf:\n                for line in rf:\n                    response = self.process_line(line)\n                    if response:\n                        yield response\n\n    def process_line(self, line):\n        """"""Read in a line and turn it into an entry\n\n        The entries will get collated by the data loader\n\n        :param line:\n        :return:\n        """"""\n\nclass Batcher(IterableDataset):\n\n    def __init__(self, dataset, batchsz):\n        self.batchsz = batchsz\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)//self.batchsz\n\n    def _batch(self, batch_list):\n        x = np.stack([item[0] for item in batch_list])\n        y = np.stack([item[1] for item in batch_list])\n        x_lens = np.stack([item[2] for item in batch_list])\n        y_lens = np.stack([item[3] for item in batch_list])\n        return {\'src\': x, \'tgt\': y, \'src_lengths\': x_lens, \'tgt_lengths\': y_lens }\n\n    def __iter__(self):\n\n        dataset_iter = iter(self.dataset)\n        steps_per_epoch = len(self.dataset)//self.batchsz\n        for indices in range(steps_per_epoch):\n            step = [next(dataset_iter) for _ in range(self.batchsz)]\n            yield self._batch(step)\n\n\nclass NextTurnPredictionFileLoader(MultiFileLoader):\n\n    def process_line(self, line):\n        pair = line.strip().split(\'\\t\')\n        # Unfortunately, this occassionally happens, a bunch of blank turns etc.\n        if len(pair) != 2:\n            return None\n        q, r = pair\n        q = q.strip()\n        r = r.strip()\n        if q == \'\' or r == \'\':\n            return None\n        q_vec, q_valid_lengths = self.src_vectorizer.run(reversed(q.split()), self.vocab)\n        q_vec = np.roll(q_vec[::-1], -(self.src_vectorizer.mxlen - q_valid_lengths))\n        r_vec, r_valid_lengths = self.tgt_vectorizer.run([\'<GO>\'] + r.split(), self.vocab)\n        assert q_valid_lengths > 0 and q_valid_lengths <= self.src_vectorizer.mxlen\n        assert r_valid_lengths > 0 and r_valid_lengths <= self.src_vectorizer.mxlen\n        return q_vec, r_vec, q_valid_lengths, r_valid_lengths\n\n\n@register_reader(task=\'seq2seq\', name=\'paired-dir\')\nclass MultiFileDatasetReader:\n    """"""Provide a base-class to do operations that are independent of token representation\n    """"""\n\n    def __init__(self, vectorizers, trim, mxlen=64, pattern=\'*.txt\', reader_type=""ntp"", **kwargs):\n        self.nctx = mxlen\n        self.pattern = pattern\n        self.reader_type = reader_type\n        self.vectorizers = vectorizers\n\n    def build_vocabs(self, _=None, **kwargs):\n        return {\'src\': self.vectorizers[\'src\'].vocab}, self.vectorizers[\'tgt\'].vocab\n\n    def load(self, directory, src_vocabs, tgt_vocab, batch_size, **kwargs):\n        # The src and target vectorizers are coupled\n        loader = Batcher(NextTurnPredictionFileLoader(directory, self.pattern, tgt_vocab, self.vectorizers, self.nctx), batch_size)\n        return loader\n\n'"
addons/reader_conllcased.py,0,"b'from baseline.reader import CONLLSeqReader\nimport numpy as np\nfrom collections import Counter\nimport codecs\nimport io\nimport os\nimport re\nimport baseline.data\n\n\nclass CONLLSeqMixedCaseReader(CONLLSeqReader):\n\n    def __init__(self, max_sentence_length=-1, max_word_length=-1, word_trans_fn=None,\n                 vec_alloc=np.zeros, vec_shape=np.shape, trim=False, extended_features=dict()):\n        super(CONLLSeqMixedCaseReader, self).__init__(max_sentence_length, max_word_length, word_trans_fn, vec_alloc, vec_shape, trim, extended_features)\n        self.idx = 2 # GO=0, START=1, EOS=2\n\n\n    def load(self, filename, vocabs, batchsz, shuffle=False, do_sort=True):\n        ts = []\n        words_vocab = vocabs[\'word\']\n        chars_vocab = vocabs[\'char\']\n        mxlen = self.max_sentence_length\n        maxw = self.max_word_length\n        extracted = self.read_examples(filename)\n        texts = extracted[\'texts\']\n        labels = extracted[\'labels\']\n\n        for i in range(len(texts)):\n\n            xs_ch = self.vec_alloc((mxlen, maxw), dtype=np.int)\n            xs = self.vec_alloc((mxlen), dtype=np.int)\n            xs_lc = self.vec_alloc((mxlen), dtype=np.int)\n            ys = self.vec_alloc((mxlen), dtype=np.int)\n            keys = self.extended_features.keys()\n\n            item = {}\n            for key in keys:\n                item[key] = self.vec_alloc((mxlen), dtype=np.int)\n\n            text = texts[i]\n            lv = labels[i]\n\n            length = mxlen\n            for j in range(mxlen):\n\n                if j == len(text):\n                    length = j\n                    break\n\n                w = text[j]\n                nch = min(len(w), maxw)\n                label = lv[j]\n\n                if label not in self.label2index:\n                    self.idx += 1\n                    self.label2index[label] = self.idx\n\n                ys[j] = self.label2index[label]\n                xs_lc[j] = words_vocab.get(w.lower(), 0)\n                xs[j] = words_vocab.get(w, 0)\n                # Extended features\n                for key in keys:\n                    item[key][j] = vocabs[key].get(extracted[key][i][j])\n                for k in range(nch):\n                    xs_ch[j, k] = chars_vocab.get(w[k], 0)\n            item.update({\'x\': xs, \'x_lc\': xs_lc, \'xch\': xs_ch, \'y\': ys, \'lengths\': length, \'ids\': i})\n            ts.append(item)\n        examples = baseline.data.SeqWordCharTagExamples(ts, do_shuffle=shuffle, do_sort=do_sort)\n        return baseline.data.SeqWordCharLabelDataFeed(examples, batchsz=batchsz, shuffle=shuffle,\n                                                      vec_alloc=self.vec_alloc, vec_shape=self.vec_shape), texts\n\n    def build_vocab(self, files):\n        vocab_word = Counter()\n        vocab_ch = Counter()\n        vocab_word[\'<UNK>\'] = 1\n        vocabs = {}\n        keys = self.extended_features.keys()\n        for key in keys:\n            vocabs[key] = Counter()\n\n        maxw = 0\n        maxs = 0\n        for file in files:\n            if file is None:\n                continue\n\n            sl = 0\n            with codecs.open(file, encoding=\'utf-8\', mode=\'r\') as f:\n                for line in f:\n\n                    line = line.strip()\n                    if line == \'\':\n                        maxs = max(maxs, sl)\n                        sl = 0\n\n                    else:\n                        states = re.split(""\\s"", line)\n                        sl += 1\n                        w = states[0]\n                        vocab_word[w] += 1\n                        vocab_word[w.lower()] += 1\n                        maxw = max(maxw, len(w))\n                        for k in w:\n                            vocab_ch[k] += 1\n                        for key, index in self.extended_features.items():\n                            vocabs[key][states[index]] += 1\n\n        self.max_word_length = min(maxw, self.max_word_length) if self.max_word_length > 0 else maxw\n        self.max_sentence_length = min(maxs, self.max_sentence_length) if self.max_sentence_length > 0 else maxs\n        print(\'Max sentence length %d\' % self.max_sentence_length)\n        print(\'Max word length %d\' % self.max_word_length)\n\n        vocabs.update({\'char\': vocab_ch, \'word\': vocab_word})\n        return vocabs\n\n\ndef create_seq_pred_reader(mxlen, mxwlen, word_trans_fn, vec_alloc, vec_shape, trim, **kwargs):\n    return CONLLSeqMixedCaseReader(mxlen, mxwlen, word_trans_fn, vec_alloc, vec_shape, trim)\n'"
addons/reader_pandas.py,0,"b""import os\nimport re\nfrom collections import Counter\nimport pandas as pd\nimport baseline\nfrom baseline.reader import TSVSeqLabelReader, register_reader\n\n\n@register_reader(task='classify', name='pandas')\nclass PandasReader(TSVSeqLabelReader):\n    def __init__(self, vectorizers, trim=False, **kwargs):\n        super(PandasReader, self).__init__(vectorizers, trim, **kwargs)\n        self.label = kwargs.get('label', 'label')\n        self.text = kwargs.get('text', 'text')\n        self.sep = kwargs.get('sep', ',')\n        self.header = kwargs.get('header', 'infer')\n\n    def build_vocab(self, files, **kwargs):\n        label_idx = len(self.label2index)\n        if isinstance(files, str):\n            if os.path.isdir(files):\n                base = files\n                files = filter(os.path.isfile, [os.path.join(base, x) for x in os.listdir(base)])\n            else:\n                files = [files]\n\n        vocab = dict()\n        for k in self.vectorizers.keys():\n            vocab[k] = Counter()\n            vocab[k]['<UNK>'] = 1000000\n\n        for f in files:\n            if f is None:\n                continue\n            df = pd.read_csv(f, sep=self.sep, header=self.header)\n            for label, text in zip(df[self.label], df[self.text]):\n                text = ' '.join(list(filter(lambda s: len(s) != 0, [self.clean_fn(w) for w in text.split()])))\n                text = list(filter(lambda s: len(s) != 0, re.split('\\s+', text)))\n                if not text:\n                    continue\n\n                for k, vectorizer in self.vectorizers.items():\n                    vocab_file = vectorizer.count(text)\n                    vocab[k].update(vocab_file)\n\n                if label not in self.label2index:\n                    self.label2index[label] = label_idx\n                    label_idx += 1\n\n        return vocab, self.get_labels()\n\n    def load(self, filename, vocabs, batchsz, **kwargs):\n        shuffle = kwargs.get('shuffle', False)\n        sort_key = kwargs.get('sort_key', None)\n        if sort_key is not None and not sort_key.endswith('_lengths'):\n            sort_key += '_lengths'\n\n        examples = []\n        df = pd.read_csv(filename, sep=self.sep, header=self.header)\n        for label, text in zip(df[self.label], df[self.text]):\n            text = ' '.join(list(filter(lambda s: len(s) != 0, [self.clean_fn(w) for w in text.split()])))\n            text = list(filter(lambda s: len(s) != 0, re.split('\\s+', text)))\n            if not text:\n                continue\n            y = self.label2index[label]\n            example_dict = dict()\n            for k, vectorizer in self.vectorizers.items():\n                example_dict[k], lengths = vectorizer.run(text, vocabs[k])\n                if lengths is not None:\n                    example_dict['{}_lengths'.format(k)] = lengths\n\n            example_dict['y'] = y\n            examples.append(example_dict)\n        return baseline.data.ExampleDataFeed(baseline.data.DictExamples(examples,\n                                                                        do_shuffle=shuffle,\n                                                                        sort_key=sort_key),\n                                             batchsz=batchsz, shuffle=shuffle, trim=self.trim)\n\n\n"""
addons/reader_parallel_classify.py,0,"b'import re\nimport codecs\nfrom collections import Counter\nimport baseline\nfrom baseline.utils import listify\nfrom baseline.reader import register_reader, SeqLabelReader, _filter_vocab, _norm_ext\n\n\n@register_reader(task=\'classify\', name=\'parallel\')\nclass ParallelSeqLabelReader(SeqLabelReader):\n    REPLACE = { ""\'s"": "" \'s "",\n                ""\'ve"": "" \'ve "",\n                ""n\'t"": "" n\'t "",\n                ""\'re"": "" \'re "",\n                ""\'d"": "" \'d "",\n                ""\'ll"": "" \'ll "",\n                "","": "" , "",\n                ""!"": "" ! "",\n                }\n\n    def __init__(self, vectorizers, trim=False, truncate=False, **kwargs):\n        super(ParallelSeqLabelReader, self).__init__()\n\n        self.label2index = {}\n        self.vectorizers = vectorizers\n        self.clean_fn = kwargs.get(\'clean_fn\')\n        if self.clean_fn is None:\n            self.clean_fn = lambda x: x\n        self.trim = trim\n        self.truncate = truncate\n        self.data = _norm_ext(kwargs.get(\'data_suffix\', kwargs.get(\'data\', \'in\')))\n        self.labels = _norm_ext(kwargs.get(\'label_suffix\', kwargs.get(\'labels\', \'labels\')))\n\n\n    SPLIT_ON = \'[\\t\\s]+\'\n\n    @staticmethod\n    def splits(text):\n        return list(filter(lambda s: len(s) != 0, re.split(\'\\s+\', text)))\n\n    def get_labels(self):\n        labels = [\'\'] * len(self.label2index)\n        for label, index in self.label2index.items():\n            labels[index] = label\n        return labels\n\n    @staticmethod\n    def do_clean(l):\n        l = l.lower()\n        l = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", l)\n        for k, v in ParallelSeqLabelReader.REPLACE.items():\n            l = l.replace(k, v)\n        return l.strip()\n\n    @staticmethod\n    def get_sentence(line, clean_fn):\n        text = re.split(ParallelSeqLabelReader.SPLIT_ON, line)\n        text = \' \'.join(list(filter(lambda s: len(s) != 0, [clean_fn(w) for w in text])))\n        text = list(filter(lambda s: len(s) != 0, re.split(\'\\s+\', text)))\n        return text\n\n    def build_vocab(self, files, **kwargs):\n        label_idx = len(self.label2index)\n        files = listify(files)\n        vocab = {k: Counter() for k in self.vectorizers.keys()}\n        for file_name in files:\n            if file_name is None: continue\n            with codecs.open(file_name + self.data, encoding=\'utf-8\', mode=\'r\') as data_file:\n                with codecs.open(file_name + self.labels, encoding=\'utf-8\', mode=\'r\') as label_file:\n                    for d, l in zip(data_file, label_file):\n                        if d.strip() == """": continue\n                        label = l.rstrip()\n                        text = ParallelSeqLabelReader.get_sentence(d, self.clean_fn)\n                        if len(text) == 0: continue\n\n                        for k, vectorizer in self.vectorizers.items():\n                            vocab_file = vectorizer.count(text)\n                            vocab[k].update(vocab_file)\n\n                        if label not in self.label2index:\n                            self.label2index[label] = label_idx\n                            label_idx += 1\n\n        vocab = _filter_vocab(vocab, kwargs.get(\'min_f\', {}))\n\n        return vocab, self.get_labels()\n\n    def load(self, filename, vocabs, batchsz, **kwargs):\n        shuffle = kwargs.get(\'shuffle\', False)\n        sort_key = kwargs.get(\'sort_key\', None)\n        if sort_key is not None and not sort_key.endswith(\'_lengths\'):\n            sort_key += \'_lengths\'\n\n        examples = []\n\n        with codecs.open(filename + self.data, encoding=\'utf-8\', mode=\'r\') as data_f:\n            with codecs.open(filename + self.labels, encoding=\'utf-8\', mode=\'r\') as label_f:\n                for d, l in zip(data_f, label_f):\n                    if d.strip() == """": continue\n                    label = l.rstrip()\n                    text = ParallelSeqLabelReader.get_sentence(d, self.clean_fn)\n                    if len(text) == 0:\n                        continue\n                    y = self.label2index[label]\n                    example_dict = dict()\n                    for k, vectorizer in self.vectorizers.items():\n                        example_dict[k], lengths = vectorizer.run(text, vocabs[k])\n                        if lengths is not None:\n                            example_dict[\'{}_lengths\'.format(k)] = lengths\n\n                    example_dict[\'y\'] = y\n                    examples.append(example_dict)\n        return baseline.data.ExampleDataFeed(baseline.data.DictExamples(examples,\n                                                                        do_shuffle=shuffle,\n                                                                        sort_key=sort_key),\n                                             batchsz=batchsz, shuffle=shuffle, trim=self.trim, truncate=self.truncate)\n'"
addons/reporting_xpctl.py,0,"b'from __future__ import print_function\n\nimport os\nimport getpass\nimport socket\nfrom baseline.reporting import EpochReportingHook\nfrom mead.utils import read_config_file_or_json\nfrom xpctl.core import ExperimentRepo\nfrom baseline.reporting import register_reporting\n\n@register_reporting(name=\'xpctl\')\nclass XPCtlReporting(EpochReportingHook):\n    def __init__(self, **kwargs):\n        super(XPCtlReporting, self).__init__(**kwargs)\n        # throw exception if the next three can\'t be read from kwargs\n        self.cred = read_config_file_or_json(kwargs[\'cred\'])\n        self.label = kwargs.get(\'label\', None)\n        self.exp_config = read_config_file_or_json(kwargs[\'config_file\'])\n        self.task = kwargs[\'task\']\n        self.print_fn = print\n        self.username = kwargs.get(\'user\', getpass.getuser())\n        self.hostname = kwargs.get(\'host\', socket.gethostname())\n        self.checkpoint_base = None\n        self.checkpoint_store = kwargs.get(\'checkpoint_store\', \'/data/model-checkpoints\')\n        self.save_model = kwargs.get(\'save_model\', False) # optionally save the model\n\n        self.repo = ExperimentRepo().create_repo(**self.cred)\n        self.log = []\n\n    def _step(self, metrics, tick, phase, tick_type, **kwargs):\n        """"""Write intermediate results to a logging memory object that ll be pushed to the xpctl repo\n\n        :param metrics: A map of metrics to scores\n        :param tick: The time (resolution defined by `tick_type`)\n        :param phase: The phase of training (`Train`, `Valid`, `Test`)\n        :param tick_type: The resolution of tick (`STEP`, `EPOCH`)\n        :return:\n        """"""\n        msg = {\'tick_type\': tick_type, \'tick\': tick, \'phase\': phase}\n        for k, v in metrics.items():\n            msg[k] = v\n        self.log.append(msg)\n\n    def done(self):\n        """"""Write the log to the xpctl database""""""\n        if self.save_model:\n           self.backend = self.exp_config.get(\'backend\', \'default\')\n           backends = {\'default\': \'tf\', \'tensorflow\': \'tf\', \'pytorch\': \'pyt\'}\n           self.checkpoint_base = self._search_checkpoint_base(self.task, backends[self.backend])\n\n        self.repo.put_result(self.task, self.exp_config, self.log,\n                            checkpoint_base=self.checkpoint_base,\n                            checkpoint_store=self.checkpoint_store,\n                            print_fn=self.print_fn,\n                            hostname=self.hostname,\n                            username=self.username,\n                            label=self.label)\n\n    @staticmethod\n    def _search_checkpoint_base(task, backend):\n        """"""Finds if the checkpoint exists as a zip file or a bunch of files.""""""\n        zip = ""{}-model-{}-{}.zip"".format(task, backend, os.getpid())\n        non_zip = ""{}-model-{}-{}"".format(task, backend, os.getpid())\n        print(zip)\n        if os.path.exists(zip):\n            return zip\n        elif os.path.exists("".graph"".format(non_zip)):\n            return non_zip\n        return None\n\n\ndef create_reporting_hook(**kwargs):\n    return XPCtlReporting(**kwargs)\n'"
addons/vec_text.py,0,"b""import numpy as np\nfrom baseline.utils import listify\nfrom baseline.vectorizers import Token1DVectorizer, register_vectorizer, _token_iterator\n\n\n@register_vectorizer(name='text')\nclass Text1DVectorizer(Token1DVectorizer):\n    def _next_element(self, tokens, vocab):\n        for atom in self.iterable(tokens):\n            yield atom\n\n    def run(self, tokens, vocab):\n        if self.mxlen < 0:\n            self.mxlen = self.max_seen\n        vec1d = np.full(self.mxlen, '', dtype=np.object)\n        for i, atom in enumerate(self._next_element(tokens, vocab)):\n            if i == self.mxlen:\n                i -= 1\n                break\n            vec1d[i] = atom\n        valid_length = i + 1\n\n        if self.time_reverse:\n            vec1d = vec1d[::-1]\n            return vec1d, None\n        return vec1d, valid_length\n\n\n@register_vectorizer(name='dict_text')\nclass DictText1DVectorizer(Text1DVectorizer):\n    def __init__(self, **kwargs):\n        super(DictText1DVectorizer, self).__init__(**kwargs)\n        self.fields = listify(kwargs.get('fields', 'text'))\n        self.delim = kwargs.get('token_delim', '@@')\n\n    def iterable(self, tokens):\n        return _token_iterator(self, tokens)\n"""
api-examples/analyze_calibration.py,0,"b'""""""Calculate and plot metrics on calibration like in https://arxiv.org/abs/1706.04599\n\nWhile not needed having matplotlib installed gives nicer results.\n""""""\n\nimport os\nimport re\nimport pickle\nimport argparse\nimport numpy as np\nimport baseline as bl\nfrom eight_mile.calibration import (\n    expected_calibration_error,\n    maximum_calibration_error,\n    multiclass_calibration_bins,\n    binary_calibration_bins,\n    average_confidence,\n)\n\n\nparser = argparse.ArgumentParser(description=""Analyze the calibration of a classifier"")\nparser.add_argument(\'--model\', help=\'The path to either the .zip file created by training or to the client bundle created by exporting\', required=True, type=str)\nparser.add_argument(\'--backend\', help=\'The deep learning backend your model was trained with\', choices={\'tf\', \'pytorch\'}, default=\'tf\')\nparser.add_argument(\'--device\', help=\'The device to run your model on\')\nparser.add_argument(\'--batchsz\', help=\'The number of examples to run at once\', default=100, type=int)\nparser.add_argument(\'--data\', help=""The data to test calibration on in the label first format"", required=True)\nparser.add_argument(\'--bins\', help=""The number of bins to use in calibration"", default=10, type=int)\nparser.add_argument(\'--hist-bins\', \'--hist_bins\', help=""The number of bins to use when creating the confidence histogram"", default=100, type=int)\nparser.add_argument(\'--output\', help=""The name of the output pickle that holds calibration stats"")\nparser.add_argument(\'--prefer_eager\', help=""If running in TensorFlow, should we prefer eager model"", type=bl.str2bool, default=False)\nargs = parser.parse_args()\n\nif args.backend == \'tf\':\n    from eight_mile.tf.layers import set_tf_eager_mode\n    set_tf_eager_mode(args.prefer_eager)\n\n# Read in the dataset\nlabels, texts = bl.read_label_first_data(args.data)\n\n# Batch the dataset and the labels\nbatched = [texts[i : i + args.batchsz] for i in range(0, len(texts), args.batchsz)]\nbatched_labels = [labels[i : i + args.batchsz] for i in range(0, len(labels), args.batchsz)]\n\n# Load the model\nm = bl.ClassifierService.load(args.model, backend=args.backend, device=args.device)\n# Extract the label vocab from the model: Note this is p messy and we should have a better story about handling label vocabs\nlabel_vocab = {l: i for i, l in enumerate(m.get_labels())}\n\n# Process batches to collect the probabilities and gold labels for each example\nprobs = []\nlabels = []\nfor texts, labs in zip(batched, batched_labels):\n    prob = m.predict(texts, dense=True)\n    probs.append(bl.to_numpy(prob))\n    labels.extend(label_vocab[l] for l in labs)\n\nprobs = np.concatenate(probs, axis=0)\nlabels = np.array(labels)\n\n# Binning\nbins = multiclass_calibration_bins(labels, probs, bins=args.bins)\nhist_bins = multiclass_calibration_bins(labels, probs, bins=args.hist_bins)\nbinary_bins = binary_calibration_bins(labels, probs[:, 1], bins=args.bins) if len(label_vocab) == 2 else None\nacc = np.mean(labels == np.argmax(probs, axis=1))\nconf = average_confidence(probs)\n\n# Metrics\nprint(f""ECE: {expected_calibration_error(bins.accs, bins.confs, bins.counts)}"")\nprint(f""MCE: {maximum_calibration_error(bins.accs, bins.confs, bins.counts)}"")\n\n# Save the needed data to a pickle so we can recreate the graphs if we need to\nargs.output = f""{re.sub(r\'.pkl$\', \'\', args.output)}.pkl"" if args.output is not None else f""{args.model}-calibration-stats.pkl""\nwith open(args.output, ""wb"") as wf:\n    pickle.dump({\n        ""multiclass"": bins,\n        ""histogram"": hist_bins,\n        ""binary"": binary_bins,\n        ""acc"": acc,\n        ""conf"": conf,\n        ""num_classes"": len(label_vocab)\n    }, wf)\n\ntry:\n    # If they have matplotlib installed plot the reliability graphs and the confidence histograms\n    import matplotlib.pyplot as plt\n    from eight_mile.calibration import reliability_diagram, reliability_curve, confidence_histogram\n    reliability_diagram(\n        bins.accs,\n        bins.confs,\n        bins.edges,\n        num_classes=len(label_vocab),\n        title=f""Reliability Diagram\\n{args.model}""\n    )\n    confidence_histogram(\n        hist_bins.edges,\n        hist_bins.counts,\n        acc=acc,\n        avg_conf=conf,\n        x_ticks=np.arange(0, 1.01, 0.2),\n        title=f""Confidence Histogram\\n{args.model}""\n    )\n    # If we using a binary classifier look at the reliability curve a la sklearn\n    if binary_bins:\n        reliability_curve(binary_bins.accs, binary_bins.confs, title=f""Reliability Curve\\n{args.model}"")\n    plt.show()\nexcept ImportError:\n    pass\n'"
api-examples/bio_to_iobes.py,0,"b'import os\nimport argparse\nfrom baseline.utils import convert_bio_conll_to_iobes\n\n\nparser = argparse.ArgumentParser(description=\'Convert a CONLL file with BIO tagging to IOBES.\')\nparser.add_argument(\'--io_dir\', help=\'Input/Output dir\', default=\'../data\')\nparser.add_argument(\'--train_file\', help=\'Training file relative name\', default=\'eng.train.bio\')\nparser.add_argument(\'--valid_file\', help=\'Validation file relative name\', default=\'eng.testa.bio\')\nparser.add_argument(\'--test_file\', help=\'Test file relative name\', default=\'eng.testb.bio\')\nparser.add_argument(\'--suffix\', help=\'Suffix to append\', default=\'.iobes\')\nparser.add_argument(""--fields"", help=""The fields to convert"", default=[-1], type=int, nargs=""+"")\nparser.add_argument(""--delim"", help=""delimiter for the fields"")\n\nargs = parser.parse_args()\n\ntrain_output = args.train_file[:-4] if args.train_file.endswith("".bio"") else args.train_file\nvalid_output = args.valid_file[:-4] if args.valid_file.endswith("".bio"") else args.valid_file\ntest_output = args.test_file[:-4] if args.test_file.endswith("".bio"") else args.test_file\n\nconvert_bio_conll_to_iobes(\n    os.path.join(args.io_dir, args.train_file), os.path.join(args.io_dir, train_output + args.suffix),\n    fields=args.fields,\n    delim=args.delim\n)\n\nconvert_bio_conll_to_iobes(\n    os.path.join(args.io_dir, args.valid_file), os.path.join(args.io_dir, valid_output + args.suffix),\n    fields=args.fields,\n    delim=args.delim\n)\n\nconvert_bio_conll_to_iobes(\n    os.path.join(args.io_dir, args.test_file), os.path.join(args.io_dir, test_output + args.suffix),\n    fields=args.fields,\n    delim=args.delim\n)\n'"
api-examples/classify-text.py,0,"b'import baseline as bl\nimport argparse\nimport os\nfrom baseline.utils import str2bool\n\nparser = argparse.ArgumentParser(description=\'Classify text with a model\')\nparser.add_argument(\'--model\', help=\'The path to either the .zip file created by training or to the client bundle \'\n                                    \'created by exporting\', required=True, type=str)\nparser.add_argument(\'--text\', help=\'The text to classify as a string, or a path to a file with each line as an example\',\n                    type=str)\nparser.add_argument(\'--backend\', help=\'backend\', choices={\'tf\', \'pytorch\', \'onnx\'}, default=\'tf\')\nparser.add_argument(\'--remote\', help=\'(optional) remote endpoint, normally localhost:8500\', type=str) # localhost:8500\nparser.add_argument(\'--name\', help=\'(optional) service name as the server may serves multiple models\', type=str)\nparser.add_argument(\'--device\', help=\'device\')\nparser.add_argument(\'--preproc\', help=\'(optional) where to perform preprocessing\', choices={\'client\', \'server\'},\n                    default=\'client\')\nparser.add_argument(\'--batchsz\', help=\'batch size when --text is a file\', default=100, type=int)\nparser.add_argument(\'--model_type\', type=str, default=\'default\')\nparser.add_argument(\'--modules\', default=[])\nparser.add_argument(\'--prefer_eager\', help=""If running in TensorFlow, should we prefer eager model"", type=str2bool, default=False)\n\nargs = parser.parse_args()\n\nif args.backend == \'tf\':\n    from eight_mile.tf.layers import set_tf_eager_mode\n    set_tf_eager_mode(args.prefer_eager)\n\nfor mod_name in args.modules:\n    bl.import_user_module(mod_name)\n\nif os.path.exists(args.text) and os.path.isfile(args.text):\n    texts = []\n    with open(args.text, \'r\') as f:\n        for line in f:\n            text = line.strip().split()\n            texts += [text]\n\nelse:\n    texts = [args.text.split()]\nbatched = [texts[i:i + args.batchsz] for i in range(0, len(texts), args.batchsz)]\n\nm = bl.ClassifierService.load(args.model, backend=args.backend, remote=args.remote,\n                              name=args.name, preproc=args.preproc,\n                              device=args.device, model_type=args.model_type)\nfor texts in batched:\n    for text, output in zip(texts, m.predict(texts)):\n        print(""{}, {}"".format("" "".join(text), output[0][0]))\n\n'"
api-examples/convert_huggingface_checkpoints.py,5,"b'import os\nimport argparse\nimport torch\nfrom typing import Tuple, Dict\nfrom eight_mile.pytorch.layers import EmbeddingsStack\nfrom eight_mile.pytorch.serialize import save_tlm_npz, convert_transformers_keys\nfrom baseline.pytorch.lm import TransformerMaskedLanguageModel\nfrom eight_mile.utils import read_config_stream\n\nfrom eight_mile.pytorch.embeddings import LookupTableEmbeddings, LearnedPositionalLookupTableEmbeddings\nfrom baseline.utils import web_downloader\n# From https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py\n\n\nBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    ""bert-base-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json"",\n    ""bert-large-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json"",\n    ""bert-base-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json"",\n    ""bert-large-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json"",\n    ""bert-base-multilingual-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json"",\n    ""bert-base-multilingual-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json"",\n    ""bert-base-chinese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json"",\n    ""bert-base-german-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json"",\n    ""bert-large-uncased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json"",\n    ""bert-large-cased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json"",\n    ""bert-large-uncased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json"",\n    ""bert-large-cased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json"",\n    ""bert-base-cased-finetuned-mrpc"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json"",\n    ""bert-base-german-dbmdz-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json"",\n    ""bert-base-german-dbmdz-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json"",\n    ""bert-base-japanese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json"",\n    ""bert-base-japanese-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json"",\n    ""bert-base-japanese-char"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json"",\n    ""bert-base-japanese-char-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json"",\n    ""bert-base-finnish-cased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json"",\n    ""bert-base-finnish-uncased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json"",\n    ""bert-base-dutch-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/config.json"",\n}\n\nBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    ""bert-base-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin"",\n    ""bert-large-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin"",\n    ""bert-base-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin"",\n    ""bert-large-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin"",\n    ""bert-base-multilingual-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin"",\n    ""bert-base-multilingual-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin"",\n    ""bert-base-chinese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin"",\n    ""bert-base-german-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin"",\n    ""bert-large-uncased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin"",\n    ""bert-large-cased-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin"",\n    ""bert-large-uncased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin"",\n    ""bert-large-cased-whole-word-masking-finetuned-squad"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin"",\n    ""bert-base-cased-finetuned-mrpc"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin"",\n    ""bert-base-german-dbmdz-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-pytorch_model.bin"",\n    ""bert-base-german-dbmdz-uncased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-pytorch_model.bin"",\n    ""bert-base-japanese"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-pytorch_model.bin"",\n    ""bert-base-japanese-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-pytorch_model.bin"",\n    ""bert-base-japanese-char"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-pytorch_model.bin"",\n    ""bert-base-japanese-char-whole-word-masking"": ""https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-pytorch_model.bin"",\n    ""bert-base-finnish-cased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/pytorch_model.bin"",\n    ""bert-base-finnish-uncased-v1"": ""https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/pytorch_model.bin"",\n    ""bert-base-dutch-cased"": ""https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/pytorch_model.bin"",\n}\n\n\ndef create_transformer_lm(config_url: str) -> Tuple[TransformerMaskedLanguageModel, int]:\n    config = read_config_stream(config_url)\n    pdrop = config[\'attention_probs_dropout_prob\']\n    activation = config[\'hidden_act\']\n    d_model = config[\'hidden_size\']\n    d_ff = config[\'intermediate_size\']\n    layer_norm_eps = float(config[\'layer_norm_eps\'])\n    mxlen = config[\'max_position_embeddings\']\n    num_heads = config[\'num_attention_heads\']\n    num_layers = config[\'num_hidden_layers\']\n    pad = config[\'pad_token_id\']\n    if pad != 0:\n        raise Exception(f""Unexpected pad value {pad}"")\n    if layer_norm_eps != 1e-12:\n        raise Exception(f""Expected layer norm to be 1e-12, received {layer_norm_eps}"")\n\n    tt_vsz = config[\'type_vocab_size\']\n    vsz = config[\'vocab_size\']\n    embeddings = {\'x\': LearnedPositionalLookupTableEmbeddings(vsz=vsz, dsz=d_model, mxlen=mxlen),\n                  \'tt\': LookupTableEmbeddings(vsz=tt_vsz, dsz=d_model)}\n    model = TransformerMaskedLanguageModel.create(embeddings,\n                                                  d_model=d_model, d_ff=d_ff, num_heads=num_heads,\n                                                  tgt_key=\'x\',\n                                                  num_layers=num_layers,\n                                                  embeddings_dropout=pdrop,\n                                                  dropout=pdrop,\n                                                  activation=activation,\n                                                  layer_norms_after=True,\n                                                  embeddings_reduction=\'sum-layer-norm\')\n    return model, num_layers\n\n\ndef download_and_convert_checkpoint(bert_checkpoint: str, num_layers: int, target_dir: str) -> Dict:\n\n    checkpoint_basename = os.path.basename(bert_checkpoint)\n    checkpoint_disk_loc = os.path.join(target_dir, checkpoint_basename)\n    if not os.path.exists(checkpoint_disk_loc):\n        web_downloader(bert_checkpoint, checkpoint_disk_loc)\n    state_dict = torch.load(checkpoint_disk_loc)\n\n    mapped_keys = convert_transformers_keys(num_layers, state_dict)\n    return mapped_keys\n\n\ndef write_npz(output_file: str, model: TransformerMaskedLanguageModel):\n    save_tlm_npz(model, output_file)\n\n\nparser = argparse.ArgumentParser(description=\'Grab a HuggingFace BERT checkpoint down and convert it to a TLM NPZ file\')\nparser.add_argument(\'--model\', help=\'This is the key of a HuggingFace input model\', default=\'bert-base-uncased\')\nparser.add_argument(\'--target_dir\', help=\'This is the target directory where we will put the checkpoints\', default=\'.\')\nargs = parser.parse_args()\n\nconfig_url = BERT_PRETRAINED_CONFIG_ARCHIVE_MAP[args.model]\nbert_checkpoint = BERT_PRETRAINED_MODEL_ARCHIVE_MAP[args.model]\nmodel, num_layers = create_transformer_lm(config_url)\nmapped_keys = download_and_convert_checkpoint(bert_checkpoint, num_layers, args.target_dir)\nunknown_keys = model.load_state_dict(mapped_keys, strict=False)\nfor k in unknown_keys.missing_keys:\n    if k not in [\'output_layer.weight\', \'output_layer.bias\']:\n        print(f\'Warning: missing key: {k}\')\nfor k in unknown_keys.unexpected_keys:\n    print(f\'Warning: unexpected key {k}\')\noutput_file = os.path.join(args.target_dir, args.model + \'.npz\')\nwrite_npz(output_file, model)'"
api-examples/create-ngram-embeddings.py,0,"b'""""""Program to generate preprocessed N-gram vocabulary for contextual embeddings\n\nTo do this, we need to collect all of the N-gram vocab from a corpus, and then\nrun each N-gram chunk through the embedder.  The vocabulary for the embedder is\ngoing to be 1-grams and it will process an array.  For example, if we have a sample\nsentence: ""The dog crossed the road"", and we are collecting trigrams, our vocab will\ninclude entries like: `[""<PAD> The dog"", ""The dog crossed"", ""dog crossed the"",...]` etc.\n\nTo prepare the output file (which will be in `word2vec` binary format, we evaluate N-gram\nand we populate a single word2vec entry with delimiters:\n\n`""The@@dog@@crossed <vector>`\n\nThe `<vector>` is found by running, e.g. ELMo with the entry ""The dog crossed"", which yields\na `Time x Word` output, and we select a pooled representation and send it back\n\n""""""\nimport argparse\nimport baseline\nimport sys\nsys.path.append(\'../python/addons\')\nfrom baseline.tf.embeddings import *\nfrom baseline.embeddings import *\nfrom baseline.vectorizers import create_vectorizer, TextNGramVectorizer\nfrom baseline.reader import CONLLSeqReader, TSVSeqLabelReader\nfrom baseline.embeddings import write_word2vec_file\nfrom baseline.utils import ngrams\nimport tensorflow as tf\nimport numpy as np\nimport codecs\nimport re\nfrom collections import Counter\nBATCHSZ = 32\n\n\ndef pool_op(embedding, name):\n    if len(embedding.shape) == 3:\n        assert embedding.shape[0] == 1\n        embedding = embedding.squeeze()\n    T = embedding.shape[0]\n    if T == 1:\n        return embedding.squeeze()\n    if name == \'mean\':\n        return np.mean(embedding, axis=0)\n    if name == \'sum\':\n        return np.sum(embedding, axis=0)\n    if name == \'max\':\n        return np.max(embedding, axis=0)\n    if name == \'last\':\n        return embedding[-1]\n    elif name == \'first\':\n        embedding[0]\n    center = T//2 + 1\n    return embedding[center]\n\n\ndef get_unigram_vectorizer(s, vf, mxlen, lower=True):\n    """"""Get a vectorizer object by name from `BASELINE_VECTORIZERS` registry\n\n    :param s: The name of the vectorizer\n    :param vf: A vocabulary file (which might be ``None``)\n    :param mxlen: The vector length to use\n    :param lower: (``bool``) should we lower case?  Defaults to ``True``\n    :return: A ``baseline.Vectorizer`` subclass\n    """"""\n    vec_type = \'token1d\'\n    transform_fn = baseline.lowercase if lower else None\n    if s == \'bert\':\n        vec_type = \'wordpiece1d\'\n    if s == \'elmo\':\n        vec_type = \'elmo\'\n    return create_vectorizer(type=vec_type, transform_fn=transform_fn, vocab_file=vf, mxlen=mxlen)\n\n\ndef get_embedder(embed_type, embed_file):\n    """"""Get an embedding object by type so we can evaluate one hot vectors\n\n    :param embed_type: (``str``) The name of the embedding in the `BASELINE_EMBEDDINGS`\n    :param embed_file: (``str``) Either the file or a URL to a hub location for the model\n    :return: An embeddings dict containing vocab and graph\n    """"""\n    if embed_type == \'bert\' or embed_type == \'elmo\':\n        embed_type += \'-embed\'\n    embed = baseline.load_embeddings(\'word\', embed_type=embed_type,\n                                     embed_file=embed_file, keep_unused=True, trainable=False, known_vocab={})\n    return embed\n\nparser = argparse.ArgumentParser(description=\'Encode a sentence as an embedding\')\nparser.add_argument(\'--input_embed\', help=\'Input embedding model. This will typically be a serialized contextual model\')\nparser.add_argument(\'--type\', default=\'default\', choices=[\'elmo\', \'default\'])\nparser.add_argument(\'--files\', required=True, nargs=\'+\')\nparser.add_argument(\'--output_embed\', default=\'elmo.bin\', help=\'Output embedding model in word2vec binary format\')\nparser.add_argument(\'--lower\', type=baseline.str2bool, default=False)\nparser.add_argument(\'--vocab_file\', required=False, help=\'Vocab file (required only for BERT)\')\nparser.add_argument(\'--max_length\', type=int, default=100)\nparser.add_argument(\'--ngrams\', type=int, default=3)\nparser.add_argument(\'--reader\', default=\'conll\', help=\'Supports CONLL or TSV\')\nparser.add_argument(\'--column\', default=\'0\', help=\'Default column to read features from\')\nparser.add_argument(\'--op\', type=str, default=\'mean\')\nargs = parser.parse_args()\n\n\n# Create our vectorizer according to CL\nuni_vec = get_unigram_vectorizer(args.type, args.vocab_file, args.ngrams)\n\ndef read_tsv_features(files, column, filtsz, lower):\n    """"""Read features from CONLL file, yield a Counter of words\n\n    :param files: Which files to read to form the vocab\n    :param column: What column to read from (defaults to \'0\')\n    :param filtsz: An integer value for the ngram length, e.g. 3 for trigram\n    :param lower: Use lower case\n    :return: A Counter of words\n    """"""\n\n    words = Counter()\n    text_column = int(column)\n\n    transform_fn = lambda z: z.lower() if lower else z\n\n    for file_name in files:\n        if file_name is None:\n            continue\n        with codecs.open(file_name, encoding=\'utf-8\', mode=\'r\') as f:\n            for il, line in enumerate(f):\n                columns = line.split(\'\\t\')\n                text = columns[text_column]\n                sentence = text.split()\n                if len(text) == 0:\n                    print(\'Warning, invalid text at {}\'.format(il))\n                    continue\n                pad = [\'<UNK>\'] * (filtsz//2)\n                words.update(ngrams(pad + [transform_fn(x) for x in sentence] + pad, filtsz=filtsz))\n    return words\n\n\ndef read_conll_features(files, column, filtsz, lower):\n    """"""Read features from CONLL file, yield a Counter of words\n\n    :param files: Which files to read to form the vocab\n    :param column: What column to read from (defaults to \'0\')\n    :param filtsz: An integer value for the ngram length, e.g. 3 for trigram\n    :param lower: Use lower-case\n    :return: A Counter of words\n    """"""\n    words = Counter()\n    conll = CONLLSeqReader(None)\n\n    text_column = str(column)\n    # This tabulates all of the ngrams\n    for file in files:\n        print(\'Adding vocab from {}\'.format(file))\n\n        examples = conll.read_examples(file)\n\n        transform_fn = lambda z: z.lower() if lower else z\n        for sentence in examples:\n            pad = [\'<UNK>\'] * (filtsz//2)\n            words.update(ngrams(pad + [transform_fn(x[text_column]) for x in sentence] + pad, filtsz=filtsz))\n    return words\n\nreader_fn = read_conll_features if args.reader == \'conll\' else read_tsv_features\nwords = reader_fn(args.files, args.column, args.ngrams, args.lower)\n# print them too\nprint(words.most_common(25))\n\n# build a vocab for the output file comprised of the ngram words\noutput_vocab = list(words)\n\n\n# Make a session\nwith tf.compat.v1.Session() as sess:\n    # Get embeddings\n    embed = get_embedder(args.type, args.input_embed)\n\n    # This is our decoder graph object\n    embedder = embed[\'embeddings\']\n\n    # This is the vocab\n    vocab = embed[\'vocab\']\n\n    # Declare a tf graph operation\n    y = embedder.encode()\n\n    init_op = tf.compat.v1.global_variables_initializer()\n    sess.run(init_op)\n\n    vecs = []\n    one_hot_batch = []\n    for i, token in enumerate(output_vocab):\n        tokens = token.split(\'@@\')\n        # Run vectorizer to get ints and length of vector\n        if i % BATCHSZ == 0 and i > 0:\n            # This resets the session, which is needed for ELMo to get same results when batching\n            sess.run(init_op)\n            vecs += [pool_op(emb, args.op) for emb in sess.run(y, feed_dict={embedder.x: one_hot_batch})]\n            one_hot_batch = []\n        one_hot, sentence_len = uni_vec.run(tokens, vocab)\n        one_hot_batch.append(one_hot)\n    if one_hot_batch:\n        sess.run(init_op)\n        vecs += [pool_op(emb, args.op) for emb in sess.run(y, feed_dict={embedder.x: one_hot_batch})]\n\n    write_word2vec_file(args.output_embed, output_vocab, vecs)\n'"
api-examples/eager-lm-pytorch.py,15,"b'import baseline\nimport argparse\nimport baseline.pytorch.embeddings\nimport eight_mile.pytorch.layers as L\nfrom eight_mile.utils import revlut, str2bool\nfrom eight_mile.pytorch.layers import SequenceLoss\nfrom eight_mile.pytorch.optz import EagerOptimizer\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport logging\nimport numpy as np\nimport time\n\n\ndef to_device(m):\n    return m.cuda()\n\n\ndef to_host(o):\n    return o.cpu().float().numpy()\n\n\nparser = argparse.ArgumentParser(description=\'Train a Layers model with TensorFlow API\')\nparser.add_argument(\'--model_type\', help=\'What type of model to build\', type=str, default=\'default\')\nparser.add_argument(\'--hsz\', help=\'How many hidden units for pooling\', type=int, default=200)\nparser.add_argument(\'--layers\', help=\'How many layers\', type=int, default=2)\nparser.add_argument(\'--stacksz\', help=\'How many hidden units for stacking\', type=int, nargs=\'+\')\nparser.add_argument(\'--name\', help=\'(optional) signature name\', type=str)\nparser.add_argument(\'--epochs\', help=\'Number of epochs to train\', type=int, default=2)\nparser.add_argument(\'--batchsz\', help=\'Batch size\', type=int, default=20)\nparser.add_argument(\'--nctx\', help=\'Context steps\', type=int, default=35)\nparser.add_argument(\'--train\', help=\'Training file\', default=\'../data/ptb/train.txt\')\nparser.add_argument(\'--valid\', help=\'Validation file\', default=\'../data/ptb/valid.txt\')\nparser.add_argument(\'--test\', help=\'Testing file\', default=\'../data/ptb/test.txt\')\nparser.add_argument(\'--embeddings\', help=\'Pretrained embeddings file\', default=\'/data/embeddings/GoogleNews-vectors-negative300.bin\')\nparser.add_argument(\'--ll\', help=\'Log level\', type=str, default=\'info\')\nparser.add_argument(\'--lr\', help=\'Learning rate\', type=float, default=0.02)\nparser.add_argument(\'--temperature\', help=\'Sample temperature during generation\', default=1.0)\nparser.add_argument(\'--start_word\', help=\'Sample start word\', default=\'the\')\nparser.add_argument(\'--dropout\', help=\'Dropout\', type=float, default=0.1)\nparser.add_argument(\'--num_heads\', help=\'Number of heads (only for Transformer)\', type=int, default=4)\nparser.add_argument(\'--transformer\', help=\'Are we using a Transformer (default is LSTM) LM\', type=str2bool, default=False)\nparser.add_argument(""--device"", type=str,\n                    default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                    help=""Device (cuda or cpu)"")\nargs = parser.parse_known_args()[0]\n\nembed_type = \'learned-positional\' if args.transformer else \'default\'\n\nfeature_desc = {\n    \'word\': {\n        \'vectorizer\': baseline.Token1DVectorizer(mxlen=-1, transform_fn=baseline.lowercase),\n        \'embed\': {\'embed_file\': args.embeddings, \'embed_type\': embed_type, \'unif\': 0.05}\n    }\n}\n\n\nclass DictionaryDatasetWrapper(Dataset):\n    def __init__(self, x, y):\n        self.tensor_dataset = TensorDataset(x, y)\n\n    def __getitem__(self, index):\n        # stuff\n        x, y = self.tensor_dataset[index]\n        return {\'word\': x.to(args.device)}, y.to(args.device)\n\n    def __len__(self):\n        return len(self.tensor_dataset)\n\n\nclass Data:\n\n    def __init__(self, ts, batchsz):\n        self.ds = self._to_tensors(ts)\n        self.batchsz = batchsz\n\n    def _to_tensors(self, ts):\n        x = []\n        y = []\n        for sample in ts:\n            x.append(sample[\'word\'].squeeze())\n            y.append(sample[\'y\'].squeeze())\n        return DictionaryDatasetWrapper(torch.tensor(np.stack(x), dtype=torch.long), torch.tensor(np.stack(y), dtype=torch.long))\n\n    def get_input(self, training=False):\n        return DataLoader(self.ds, batch_size=self.batchsz, shuffle=training)\n\n\nvectorizers = {k: v[\'vectorizer\'] for k, v in feature_desc.items()}\nreader = baseline.LineSeqReader(vectorizers, nctx=args.nctx)\n\ntrain_file = args.train\nvalid_file = args.valid\ntest_file = args.test\n\n\n# This builds a set of counters\nvocabs = reader.build_vocab([train_file, valid_file, test_file])\n\n\n# This builds a set of embeddings objects, these are typically not DL-specific\n# but if they happen to be addons, they can be\nembeddings = dict()\nfor k, v in feature_desc.items():\n    embed_config = v[\'embed\']\n    embeddings_for_k = baseline.embeddings.load_embeddings(k, known_vocab=vocabs[k], **embed_config)\n    embeddings[k] = embeddings_for_k[\'embeddings\']\n    # Reset the vocab to the embeddings one\n    vocabs[k] = embeddings_for_k[\'vocab\']\n\n\ntrain_set = Data(reader.load(train_file, vocabs=vocabs, batchsz=1, tgt_key=""word""), args.batchsz)\nvalid_set = Data(reader.load(valid_file, vocabs=vocabs, batchsz=1, tgt_key=""word""), args.batchsz)\ntest_set = Data(reader.load(test_file, vocabs=vocabs, batchsz=1, tgt_key=""word""), args.batchsz)\n\n\nif args.transformer:\n    transducer = L.TransformerEncoderStackWithTimeMask(args.num_heads, d_model=args.hsz, layers=args.layers,\n                                                       pdrop=args.dropout, input_sz=embeddings[\'word\'].get_dsz())\nelse:\n    transducer = L.LSTMEncoderWithState(embeddings[\'word\'].get_dsz(), args.hsz, args.layers, pdrop=args.dropout)\nmodel = to_device(L.LangSequenceModel(embeddings[""word""].get_vsz(), L.EmbeddingsStack(embeddings), transducer))\n\n\ndef generate_text(model, start_string, temperature=1.0, num_generate=20):\n    input_eval = torch.tensor([vocabs[""word""].get(s) for s in start_string.split()]).long().view(1, -1).to(args.device)\n    rlut = revlut(vocabs[""word""])\n    # Empty string to store our results\n    text_generated = [start_string]\n\n    h = None\n    for i in range(num_generate):\n        predictions, h = model({""word"": input_eval, ""h"": h})\n        # remove the batch dimension\n        predictions = torch.softmax(predictions / temperature, dim=-1)\n        predictions = predictions.squeeze(0)\n\n        # using a multinomial distribution to predict the word returned by the model\n        predicted_id = torch.multinomial(predictions, num_samples=1)[-1, 0]\n        # We pass the predicted word as the next input to the model\n        # along with the previous hidden state\n        input_eval = predicted_id.unsqueeze(0).unsqueeze(0)\n\n        text_generated.append(rlut[predicted_id.cpu().numpy().item()])\n\n    return text_generated\n\n\ncrit = SequenceLoss(LossFn=torch.nn.CrossEntropyLoss)\n\n\ndef loss(model, h, x, y):\n    x[""h""] = h\n    logits, h = model(x)\n    vsz = embeddings[""word""].vsz\n    l = crit(logits, y)\n\n    return l, h\n\n\ndef repackage_hidden(h):\n    """"""Wraps hidden states in new Variables, to detach them from their history.""""""\n    if h is None:\n        return None\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\n\noptimizer = EagerOptimizer(loss, optim=""sgd"", lr=args.lr)\n\n\nfor epoch in range(args.epochs):\n    loss_accum = 0.\n    step = 0\n    start = time.time()\n    h = None\n\n    for x, y in train_set.get_input(training=True):\n        # Optimize the model\n        if h is not None:\n            h = repackage_hidden(h)\n        loss_value, h = optimizer.update_with_hidden(model, h, x, y)\n        loss_accum += loss_value\n        step += 1\n    print(f\'training time {time.time() - start}\')\n\n    mean_loss = loss_accum / step\n    print(f\'Training Loss {mean_loss}, Perplexity {np.exp(mean_loss)}\')\n\n\n    step = 0\n    loss_accum = 0\n\n    for x, y in valid_set.get_input(training=False):\n        with torch.no_grad():\n            # Track progress\n            # compare predicted label to actual label\n            loss_value, h = loss(model, h, x, y)\n            h = repackage_hidden(h)\n\n            loss_accum += to_host(loss_value)\n            step += 1\n\n    mean_loss = loss_accum / step\n    print(f\'Valid Loss {mean_loss}, Perplexity {np.exp(mean_loss)}\')\n\n    text = generate_text(model, args.start_word, args.temperature)\n    print(\' \'.join(text))\n\nfor x, y in test_set.get_input(training=False):\n    with torch.no_grad():\n        loss_value, h = loss(model, h, x, y)\n        h = repackage_hidden(h)\n        loss_accum += to_host(loss_value)\n        step += 1\n\nmean_loss = loss_accum / step\nprint(f\'Test Loss {mean_loss}, Perplexity {np.exp(mean_loss)}\')\n'"
api-examples/eager-lm.py,0,"b'import baseline\nimport argparse\nimport baseline.embeddings\nimport baseline.tf.embeddings\nimport eight_mile.tf.layers as L\nfrom eight_mile.utils import get_version, revlut, str2bool\nfrom eight_mile.tf.layers import SET_TRAIN_FLAG, set_tf_log_level\nfrom eight_mile.tf.optz import EagerOptimizer\nimport tensorflow as tf\nimport logging\nimport numpy as np\nimport time\n\nNUM_PREFETCH = 2\nSHUF_BUF_SZ = 5000\n\n\nclass Data:\n\n    def __init__(self, ts, batchsz):\n        self.x, self.y = Data._to_tensors(ts)\n\n\ndef to_tensors(ts):\n    X = []\n    y = []\n    for sample in ts:\n        X.append(sample[\'word\'].squeeze())\n        y.append(sample[\'y\'].squeeze())\n    return np.stack(X), np.stack(y)\n\n\nparser = argparse.ArgumentParser(description=\'Train a Layers model with TensorFlow API\')\nparser.add_argument(\'--model_type\', help=\'What type of model to build\', type=str, default=\'default\')\nparser.add_argument(\'--hsz\', help=\'How many hidden units for pooling\', type=int, default=200)\nparser.add_argument(\'--layers\', help=\'How many layers\', type=int, default=2)\nparser.add_argument(\'--stacksz\', help=\'How many hidden units for stacking\', type=int, nargs=\'+\')\nparser.add_argument(\'--name\', help=\'(optional) signature name\', type=str)\nparser.add_argument(\'--epochs\', help=\'Number of epochs to train\', type=int, default=2)\nparser.add_argument(\'--batchsz\', help=\'Batch size\', type=int, default=20)\nparser.add_argument(\'--nctx\', help=\'Context steps\', type=int, default=35)\nparser.add_argument(\'--train\', help=\'Training file\', default=\'../data/ptb/train.txt\')\nparser.add_argument(\'--valid\', help=\'Validation file\', default=\'../data/ptb/valid.txt\')\nparser.add_argument(\'--test\', help=\'Testing file\', default=\'../data/ptb/test.txt\')\nparser.add_argument(\'--embeddings\', help=\'Pretrained embeddings file\', default=\'/data/embeddings/GoogleNews-vectors-negative300.bin\')\nparser.add_argument(\'--ll\', help=\'Log level\', type=str, default=\'info\')\nparser.add_argument(\'--lr\', help=\'Learning rate\', type=float, default=0.02)\nparser.add_argument(\'--temperature\', help=\'Sample temperature during generation\', default=1.0)\nparser.add_argument(\'--start_word\', help=\'Sample start word\', default=\'the\')\nparser.add_argument(\'--dropout\', help=\'Dropout\', type=float, default=0.1)\nparser.add_argument(\'--num_heads\', help=\'Number of heads (only for Transformer)\', type=int, default=4)\nparser.add_argument(\'--transformer\', help=\'Are we using a Transformer (default is LSTM) LM\', type=str2bool, default=False)\nargs = parser.parse_known_args()[0]\n\nembed_type = \'learned-positional\' if args.transformer else \'default\'\nfeature_desc = {\n    \'word\': {\n        \'vectorizer\': baseline.Token1DVectorizer(mxlen=-1, transform_fn=baseline.lowercase),\n        \'embed\': {\'embed_file\': args.embeddings, \'embed_type\': embed_type, \'unif\': 0.05}\n    }\n}\n\nset_tf_log_level(\'ERROR\')\nvectorizers = {k: v[\'vectorizer\'] for k, v in feature_desc.items()}\nreader = baseline.LineSeqReader(vectorizers, nctx=args.nctx)\n\ntrain_file = args.train\nvalid_file = args.valid\ntest_file = args.test\n\n\n# This builds a set of counters\nvocabs = reader.build_vocab([train_file,\n                             valid_file,\n                             test_file])\n\n\n\n# This builds a set of embeddings objects, these are typically not DL-specific\n# but if they happen to be addons, they can be\nembeddings = dict()\nfor k, v in feature_desc.items():\n    embed_config = v[\'embed\']\n    embeddings_for_k = baseline.embeddings.load_embeddings(k, known_vocab=vocabs[k], **embed_config)\n    embeddings[k] = embeddings_for_k[\'embeddings\']\n    # Reset the vocab to the embeddings one\n    vocabs[k] = embeddings_for_k[\'vocab\']\n\n\nX_train, y_train = to_tensors(reader.load(train_file, vocabs=vocabs, batchsz=1, tgt_key=""word""))\nX_valid, y_valid = to_tensors(reader.load(valid_file, vocabs=vocabs, batchsz=1, tgt_key=""word""))\nX_test, y_test = to_tensors(reader.load(test_file, vocabs=vocabs, batchsz=1, tgt_key=""word""))\n\n\ndef train_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n    dataset = dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    dataset = dataset.batch(args.batchsz, True)\n    dataset = dataset.map(lambda x, y: ({\'word\': x}, y))\n    dataset = dataset.prefetch(NUM_PREFETCH)\n    return dataset\n\n\ndef eval_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n    dataset = dataset.batch(args.batchsz, True)\n    dataset = dataset.map(lambda x, y: ({\'word\': x}, y))\n    return dataset\n\n\ndef predict_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n    dataset = dataset.batch(1)\n    dataset = dataset.map(lambda x, y: ({\'word\': x}, y))\n    return dataset\n\n\nif args.transformer:\n    transducer = L.TransformerEncoderStackWithTimeMask(args.num_heads, d_model=args.hsz, layers=args.layers, pdrop=args.dropout)\nelse:\n    transducer = L.LSTMEncoderWithState(None, args.hsz, args.layers, pdrop=args.dropout)\nmodel = L.LangSequenceModel(embeddings[""word""].get_vsz(), L.EmbeddingsStack(embeddings), transducer)\n\n\ndef generate_text(model, start_string, temperature=1.0, num_generate=20):\n    input_eval = np.array([vocabs[""word""].get(s) for s in start_string.split()], dtype=np.int32).reshape(1, -1)\n    rlut = revlut(vocabs[""word""])\n    # Empty string to store our results\n    text_generated = [start_string]\n\n    h = None\n    for i in range(num_generate):\n        predictions, h = model({""word"": input_eval, ""h"": h})\n        # remove the batch dimension\n        predictions = tf.nn.softmax(predictions / temperature, axis=-1)\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a multinomial distribution to predict the word returned by the model\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n        # We pass the predicted word as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(rlut[predicted_id])\n\n    return text_generated\n\n\ndef loss(model, h, x, y):\n    x[""h""] = h\n    logits, h = model(x)\n    vsz = embeddings[""word""].get_vsz()\n    targets = tf.reshape(y, [-1])\n    bt_x_v = tf.nn.log_softmax(tf.reshape(logits, [-1, vsz]), axis=-1)\n    one_hots = tf.one_hot(targets, vsz)\n    example_loss = -tf.reduce_sum(one_hots * bt_x_v, axis=-1)\n    loss = tf.reduce_mean(example_loss)\n    return loss, h\n\noptimizer = EagerOptimizer(loss, optim=""adam"", lr=args.lr)\nfor epoch in range(args.epochs):\n\n\n    loss_accum = 0.\n    step = 0\n    start = time.time()\n    h = None\n\n\n    SET_TRAIN_FLAG(True)\n\n    for x, y in train_input_fn():\n        # Optimize the model\n        loss_value, h = optimizer.update_with_hidden(model, h, x, y)\n        loss_accum += loss_value\n        step += 1\n    print(\'training time {}\'.format(time.time() - start))\n\n    mean_loss = loss_accum / step\n    print(\'Training Loss {}, Perplexity {}\'.format(mean_loss, np.exp(mean_loss)))\n\n\n    step = 0\n    loss_accum = 0\n    SET_TRAIN_FLAG(False)\n\n    for x, y in eval_input_fn():\n        # Track progress\n        # compare predicted label to actual label\n        loss_value, h = loss(model, h, x, y)\n        loss_accum += loss_value\n        step += 1\n\n    mean_loss = loss_accum / step\n    print(\'Valid Loss {}, Perplexity {}\'.format(mean_loss, np.exp(mean_loss)))\n\n    text = generate_text(model, args.start_word, args.temperature)\n    print(\' \'.join(text))\n'"
api-examples/eager-tagger.py,0,"b'\nimport os\nimport argparse\nfrom eight_mile.utils import listify\nimport baseline\nimport eight_mile.tf.layers as L\nfrom eight_mile.utils import get_version\nimport baseline.embeddings\nimport baseline.tf.embeddings\nfrom eight_mile.tf.optz import EagerOptimizer\nfrom baseline.tf.tfy import SET_TRAIN_FLAG\nimport tensorflow as tf\nimport logging\nimport numpy as np\n\n\nNUM_PREFETCH = 2\nSHUF_BUF_SZ = 5000\ndef get_logging_level(ll):\n    ll = ll.lower()\n    if ll == \'debug\':\n        return logging.DEBUG\n    if ll == \'info\':\n        return logging.INFO\n    return logging.WARNING\n\n\ndef to_tensors(ts):\n    X = []\n    Xch = []\n    y = []\n    for sample in ts:\n        X.append(sample[\'word\'].squeeze())\n        Xch.append(sample[\'char\'].squeeze())\n        y.append(sample[\'y\'].squeeze())\n    return np.stack(X), np.stack(Xch), np.stack(y)\n\n\n\n\nparser = argparse.ArgumentParser(description=\'Train a Layers model with TensorFlow API\')\nparser.add_argument(\'--model_type\', help=\'What type of model to build\', type=str, default=\'default\')\nparser.add_argument(\'--hsz\', help=\'How many hidden units for pooling\', type=int, default=200)\nparser.add_argument(\'--layers\', help=\'How many hidden units for stacking\', type=int, default=2)\nparser.add_argument(\'--epochs\', help=\'Number of epochs to train\', type=int, default=20)\nparser.add_argument(\'--batchsz\', help=\'Batch size\', type=int, default=20)\nparser.add_argument(\'--mxlen\', help=\'Maximum post length (number of words) during training\', type=int, default=100)\nparser.add_argument(\'--train\', help=\'Training file\', default=\'../data/oct27.train\')\nparser.add_argument(\'--valid\', help=\'Validation file\', default=\'../data/oct27.dev\')\nparser.add_argument(\'--test\', help=\'Testing file\', default=\'../data/oct27.test\')\nparser.add_argument(\'--embeddings\', help=\'Pretrained embeddings file\', default=\'/data/embeddings/glove.twitter.27B.200d.txt\')\nparser.add_argument(\'--ll\', help=\'Log level\', type=str, default=\'info\')\nparser.add_argument(\'--lr\', help=\'Learning rate\', type=float, default=0.001)\nparser.add_argument(\'--tf_ll\', help=\'TensorFlow Log level\', type=str, default=\'warn\')\n\nargs = parser.parse_known_args()[0]\n\nL.set_tf_log_level(args.tf_ll)\nfeature_desc = {\n    \'word\': {\n        \'vectorizer\': baseline.Dict1DVectorizer(mxlen=-1, transform_fn=baseline.lowercase),\n        \'embed\': {\'embed_file\': args.embeddings, \'embed_type\': \'default\', \'unif\': 0.25}\n    },\n    \'char\': {\n        \'vectorizer\': baseline.Dict2DVectorizer(mxlen=-1, mxwlen=40),\n        \'embed\': {\'dsz\': 30, \'embed_type\': \'char-conv\', \'wsz\': 30}\n    }\n}\nvectorizers = {k: v[\'vectorizer\'] for k, v in feature_desc.items()}\nreader = baseline.CONLLSeqReader(vectorizers, named_fields={""0"": ""text"", ""-1"": ""y""})\n\ntrain_file = args.train\nvalid_file = args.valid\ntest_file = args.test\n\n# This builds a set of counters\nvocabs = reader.build_vocab([train_file,\n                             valid_file,\n                             test_file])\n\nlabels = reader.label2index\n\n# This builds a set of embeddings objects, these are typically not DL-specific\n# but if they happen to be addons, they can be\nembeddings = dict()\nfor k, v in feature_desc.items():\n    embed_config = v[\'embed\']\n    embeddings_for_k = baseline.embeddings.load_embeddings(k, known_vocab=vocabs[k], **embed_config)\n    embeddings[k] = embeddings_for_k[\'embeddings\']\n    # Reset the vocab to the embeddings one\n    vocabs[k] = embeddings_for_k[\'vocab\']\n\n\nX_train, Xch_train, y_train = to_tensors(reader.load(train_file, vocabs=vocabs, batchsz=1)[0])\nX_valid, Xch_valid, y_valid = to_tensors(reader.load(valid_file, vocabs=vocabs, batchsz=1)[0])\nX_test, Xch_test, y_test = to_tensors(reader.load(test_file, vocabs=vocabs, batchsz=1)[0])\n\n\ndef train_input_fn():\n    SET_TRAIN_FLAG(True)\n    dataset = tf.data.Dataset.from_tensor_slices((X_train, Xch_train, y_train))\n    dataset = dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    dataset = dataset.batch(args.batchsz)\n    dataset = dataset.map(lambda x, xch, y: ({\'word\': x, \'char\': xch, \'lengths\': tf.compat.v1.count_nonzero(x, axis=1)}, y))\n    dataset = dataset.prefetch(NUM_PREFETCH)\n    return dataset\n\n\ndef eval_input_fn():\n    SET_TRAIN_FLAG(False)\n    dataset = tf.data.Dataset.from_tensor_slices((X_valid, Xch_valid, y_valid))\n    dataset = dataset.batch(args.batchsz)\n    dataset = dataset.map(lambda x, xch, y: ({\'word\': x, \'char\': xch, \'lengths\': tf.compat.v1.count_nonzero(x, axis=1)}, y))\n    return dataset\n\n\ndef predict_input_fn():\n    SET_TRAIN_FLAG(False)\n    dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n    dataset = dataset.batch(1)\n    dataset = dataset.map(lambda x, xch, y: ({\'word\': x, \'char\': xch, \'lengths\': tf.compat.v1.count_nonzero(x, axis=1)}, y))\n    return dataset\n\n\ntransducer = L.BiLSTMEncoderSequence(None, args.hsz, args.layers, 0.5)\nmodel = L.TagSequenceModel(len(labels), embeddings, transducer)\n\ntrain_loss_results = []\ntrain_accuracy_results = []\n\n\ndef loss(model, x, y):\n  unary = model.transduce(x)\n  return model.decoder_model.neg_log_loss(unary, y, x[\'lengths\'])\n\n\noptim = EagerOptimizer(loss, optim=""adam"", lr=args.lr)\n\n\nimport time\nnum_epochs = args.epochs\nfor epoch in range(num_epochs):\n\n\n    # Training loop - using batches of 32\n    loss_acc = 0.\n    step = 0\n    start = time.time()\n    for x, y in train_input_fn():\n        # Optimize the model\n        loss_value = optim.update(model, x, y)\n\n        loss_acc += loss_value\n        step += 1\n\n    print(\'training time {}\'.format(time.time() - start))\n    mean_loss = loss_acc / step\n    print(\'Training Loss {}\'.format(mean_loss))\n\n    acc = 0\n    total = 0\n    for x, y in eval_input_fn():\n        # Optimize the model\n\n        # Track progress\n        # compare predicted label to actual label\n        y_ = model(x).numpy()\n        y = y.numpy()\n        #y_ = tf.argmax(model(x), axis=-1, output_type=tf.int32)\n        lengths = x[\'lengths\'].numpy()\n\n        B = y.shape[0]\n        bacc = 0\n        for b in range(B):\n            bacc += np.sum(y_[b][:lengths[b]] == y[b][:lengths[b]])\n        acc += bacc\n        btotal = np.sum(lengths)\n        total += btotal\n        print(\'batch acc {}/{}\'.format(bacc, btotal))\n\n\n    print(\'ACC\', acc/float(total))\n\n'"
api-examples/ed-text.py,0,"b'from __future__ import print_function\nimport baseline as bl\nimport argparse\nimport os\nfrom baseline.utils import str2bool\n\nparser = argparse.ArgumentParser(description=\'Encoder-Decoder execution\')\nparser.add_argument(\'--model\', help=\'An encoder-decoder model\', required=True, type=str)\nparser.add_argument(\'--text\', help=\'raw value or a file\', type=str)\nparser.add_argument(\'--backend\', help=\'backend\', default=\'tf\')\nparser.add_argument(\'--remote\', help=\'(optional) remote endpoint\', type=str) # localhost:8500\nparser.add_argument(\'--name\', help=\'(optional) signature name\', type=str)\nparser.add_argument(\'--target\', help=\'A file to write decoded output (or print to screen)\')\nparser.add_argument(\'--tsv\', help=\'print tab separated\', type=bl.str2bool, default=False)\nparser.add_argument(\'--batchsz\', help=\'Size of a batch to pass at once\', default=32, type=int)\nparser.add_argument(\'--device\', help=\'device\')\nparser.add_argument(\'--alpha\', type=float, help=\'If set use in the gnmt length penalty.\')\nparser.add_argument(\'--beam\', type=int, default=30, help=\'The size of beam to use.\')\nparser.add_argument(\'--prefer_eager\', help=""If running in TensorFlow, should we prefer eager model"", type=str2bool)\n\nargs = parser.parse_known_args()[0]\n\nif args.backend == \'tf\':\n    from eight_mile.tf.layers import set_tf_eager_mode\n    set_tf_eager_mode(args.prefer_eager)\n\nbatches = []\nif os.path.exists(args.text) and os.path.isfile(args.text):\n    with open(args.text, \'r\') as f:\n        batch = []\n        for line in f:\n            text = line.strip().split()\n            if len(batch) == args.batchsz:\n                batches.append(batch)\n                batch = []\n            batch.append(text)\n\n        if len(batch) > 0:\n            batches.append(batch)\n\nelse:\n    batch = [args.text.split()]\n    batches.append(batch)\n\nm = bl.EncoderDecoderService.load(args.model, backend=args.backend, beam=args.beam,\n                                  remote=args.remote, name=args.name, device=args.device)\n\nf = open(args.target, \'w\') if args.target is not None else None\n\nfor texts in batches:\n    decoded = m.predict(texts, alpha=args.alpha, beam=args.beam)\n    for src, dst in zip(texts, decoded):\n        src_str = \' \'.join(src)\n        dst_str = \' \'.join(dst)\n        if args.tsv:\n            line = src_str + \'\\t\' + dst_str\n        else:\n            line = dst_str\n\n        print(line, file=f, flush=True)\n\nif f is not None:\n    f.close()\n\n'"
api-examples/embed-text.py,0,"b""import os\nimport argparse\nimport numpy as np\nimport baseline as bl\n\nparser = argparse.ArgumentParser(description='Embed text and save to a .npy')\nparser.add_argument('--model', help='An embedding model', required=True, type=str)\nparser.add_argument('--text', help='raw value', type=str)\nparser.add_argument('--backend', help='backend', default='tf')\nparser.add_argument('--remote', help='(optional) remote endpoint', type=str) # localhost:8500\nparser.add_argument('--name', help='(optional) service name', type=str)\nparser.add_argument('--device', help='device')\nparser.add_argument('--preproc', help='(optional) where to perform preprocessing', choices={'client', 'server'}, default='client')\nargs = parser.parse_args()\n\nif os.path.exists(args.text) and os.path.isfile(args.text):\n    texts = []\n    with open(args.text, 'r') as f:\n        for line in f:\n            text = line.strip().split()\n            texts += [text]\n    out = os.path.splitext(args.text)[0]\nelse:\n    texts = [args.text.split()]\n    out = 'cli_text'\n\nm = bl.EmbeddingsService.load(\n    args.model, backend=args.backend,\n    remote=args.remote, name=args.name,\n    preproc=args.preproc, device=args.device,\n)\n\nembedded = m.predict(texts)\n\nnp.save(out, embedded)\n"""
api-examples/generate-mlm.py,14,"b'import numpy as np\nimport torch\nimport logging\nimport os\nimport glob\nfrom argparse import ArgumentParser\nimport baseline\n#from eight_mile.pytorch.layers import EmbeddingsStack\nfrom eight_mile.pytorch.serialize import tlm_load_state_dict\nfrom baseline.pytorch.lm import TransformerMaskedLanguageModel\nfrom baseline.utils import str2bool, read_json, Offsets, revlut\nfrom baseline.vectorizers import Token1DVectorizer, BPEVectorizer1D\nfrom baseline.pytorch.embeddings import *\nfrom transformer_utils import find_latest_checkpoint\nlogger = logging.getLogger(__file__)\n\n\ndef decode_sentence(model, vectorizer, query, word2index, index2word, device, sample=True):\n    vec, length = vectorizer.run(query, word2index)\n    UNK = word2index.get(\'<UNK>\')\n    MASK = word2index.get(\'[MASK]\')\n    for i in range(length):\n        if vec[i] == UNK:\n            vec[i] = MASK\n\n    detok = [index2word[v] for v in vec if v != 0]\n    print(\'[De-tok] \' + \' \'.join(detok))\n    toks = torch.from_numpy(vec).unsqueeze(0).to(device=device)\n\n    length = torch.from_numpy(np.array(length)).unsqueeze(0).to(device=device)\n\n    with torch.no_grad():\n        predictions, _ = model({\'x\': toks}, None)\n        predictions = predictions.squeeze(0)\n        #predictions = model({\'x\': toks, \'src_len\': length, \'h\': None}).squeeze(0)\n        words = []\n        for i in range(length):\n\n            if vec[i] == MASK:\n                if not sample:\n                    output = torch.argmax(predictions[i], -1).item()\n                    word = index2word[output]\n                else:\n                    sample_dist = predictions[i].exp()\n                    output = torch.multinomial(sample_dist, num_samples=1)\n                    output = output.squeeze(0).item()\n\n                    word = index2word[output]\n                words.append(word)\n            else:\n                words.append(index2word[vec[i]])\n\n        return words\n\n\ndef create_model(embeddings, d_model, d_ff, num_heads, num_layers, rpr_k, d_k, checkpoint_name, activation):\n    if len(rpr_k) == 0 or rpr_k[0] < 1:\n        rpr_k = None\n    logger.info(""Creating tied encoder decoder model"")\n    model = TransformerMaskedLanguageModel.create({\'x\': embeddings},\n                                                  hsz=d_model,\n                                                  d_ff=d_ff,\n                                                  tie_weights=True,\n                                                  dropout=0,\n                                                  gpu=False,\n                                                  num_heads=num_heads,\n                                                  layers=num_layers,\n                                                  rpr_k=rpr_k,\n                                                  d_k=d_k,\n                                                  activation=activation,\n                                                  src_keys=[\'x\'], tgt_key=\'x\')\n    tlm_load_state_dict(model, checkpoint_name)\n    #model.load_state_dict(torch.load(checkpoint_name))\n    model.eval()\n    print(model)\n    return model\n\n\ndef run():\n    parser = ArgumentParser()\n    parser.add_argument(""--basedir"", type=str)\n    parser.add_argument(""--checkpoint"", type=str, help=\'Checkpoint name or directory to load\')\n    parser.add_argument(""--sample"", type=str2bool, help=\'Sample from the decoder?  Defaults to `false`\', default=0)\n    parser.add_argument(""--vocab"", type=str, help=\'Vocab file to load\', required=False)\n    parser.add_argument(""--query"", type=str, default=\'hello , <unk> are you today ?\')\n    parser.add_argument(""--dataset_cache"", type=str, default=os.path.expanduser(\'~/.bl-data\'),\n                        help=""Path or url of the dataset cache"")\n    parser.add_argument(""--d_model"", type=int, default=512, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--d_ff"", type=int, default=2048, help=""FFN dimension"")\n    parser.add_argument(""--d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--num_heads"", type=int, default=8, help=""Number of heads"")\n    parser.add_argument(""--num_layers"", type=int, default=8, help=""Number of layers"")\n    parser.add_argument(""--nctx"", type=int, default=128, help=""Max context length (for both encoder and decoder)"")\n    parser.add_argument(""--embed_type"", type=str, default=\'positional\',\n                        help=""register label of the embeddings, so far support positional or learned-positional"")\n    parser.add_argument(""--subword_model_file"", type=str, required=True)\n    parser.add_argument(""--subword_vocab_file"", type=str, required=True)\n    parser.add_argument(""--activation"", type=str, default=\'relu\')\n    parser.add_argument(\'--rpr_k\', help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[3, 5, 48, 48, 48, 48], nargs=\'+\')\n\n    parser.add_argument(""--device"", type=str,\n                        default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                        help=""Device (cuda or cpu)"")\n    args = parser.parse_args()\n\n    if torch.cuda.device_count() == 1:\n        torch.cuda.set_device(0)\n        args.device = torch.device(""cuda"", 0)\n\n\n    vocab_file = args.vocab\n\n    if os.path.isdir(args.checkpoint):\n        vocab_file = os.path.join(args.checkpoint, \'vocabs.json\')\n        checkpoint = find_latest_checkpoint(args.checkpoint)\n        logger.warning(""Found latest checkpoint %s"", checkpoint)\n    else:\n        checkpoint = args.checkpoint\n\n    vocab = read_json(vocab_file)\n    # If we are not using chars, then use \'x\' for both input and output\n    preproc_data = baseline.embeddings.load_embeddings(\'x\', dsz=args.d_model, counts=False, known_vocab=vocab, embed_type=args.embed_type)\n    embeddings = preproc_data[\'embeddings\']\n    vocab = preproc_data[\'vocab\']\n    model = create_model(embeddings, d_model=args.d_model, d_ff=args.d_ff, num_heads=args.num_heads, num_layers=args.num_layers,\n                         rpr_k=args.rpr_k, d_k=args.d_k, checkpoint_name=checkpoint, activation=args.activation)\n    model.to(args.device)\n\n    vectorizer = BPEVectorizer1D(model_file=args.subword_model_file, vocab_file=args.subword_vocab_file, mxlen=args.nctx)\n    index2word = revlut(vocab)\n    print(\'[Query]\', args.query)\n    print(\'[Response]\', \' \'.join(decode_sentence(model, vectorizer, args.query.split(), vocab, index2word, args.device, sample=args.sample)))\n\nrun()\n'"
api-examples/iob_to_bio.py,0,"b'import os\nimport argparse\nfrom baseline.utils import convert_iob_conll_to_bio\n\n\nparser = argparse.ArgumentParser(description=\'Convert a CONLL file tagged with IOB to BIO.\')\nparser.add_argument(\'--io_dir\', help=\'Input/Output dir\', default=\'../data\')\nparser.add_argument(\'--train_file\', help=\'Training file relative name\', default=\'eng.train\')\nparser.add_argument(\'--valid_file\', help=\'Validation file relative name\', default=\'eng.testa\')\nparser.add_argument(\'--test_file\', help=\'Test file relative name\', default=\'eng.testb\')\nparser.add_argument(\'--fields\', help=""The fields to convert."", default=[-1], type=int, nargs=""+"")\nparser.add_argument(\'--suffix\', help=\'Suffix to append\', default=\'.bio\')\nargs = parser.parse_args()\n\n\nconvert_iob_conll_to_bio(os.path.join(args.io_dir, args.train_file), os.path.join(args.io_dir, args.train_file + args.suffix), fields=args.fields)\nconvert_iob_conll_to_bio(os.path.join(args.io_dir, args.valid_file), os.path.join(args.io_dir, args.valid_file + args.suffix), fields=args.fields)\nconvert_iob_conll_to_bio(os.path.join(args.io_dir, args.test_file), os.path.join(args.io_dir, args.test_file + args.suffix), fields=args.fields)\n'"
api-examples/iob_to_iobes.py,0,"b'import os\nimport argparse\nfrom baseline.utils import convert_iob_conll_to_iobes\n\n\nparser = argparse.ArgumentParser(description=\'Convert a CONLL file tagged with IOB to IOBES.\')\nparser.add_argument(\'--io_dir\', help=\'Input/Output dir\', default=\'../data\')\nparser.add_argument(\'--train_file\', help=\'Training file relative name\', default=\'eng.train\')\nparser.add_argument(\'--valid_file\', help=\'Validation file relative name\', default=\'eng.testa\')\nparser.add_argument(\'--test_file\', help=\'Test file relative name\', default=\'eng.testb\')\nparser.add_argument(\'--suffix\', help=\'Suffix to append\', default=\'.iobes\')\nparser.add_argument(""--fields"", help=""The fields to convert"", default=[-1], type=int, nargs=""+"")\nargs = parser.parse_args()\n\n\nconvert_iob_conll_to_iobes(os.path.join(args.io_dir, args.train_file), os.path.join(args.io_dir, args.train_file + args.suffix), fields=args.fields)\nconvert_iob_conll_to_iobes(os.path.join(args.io_dir, args.valid_file), os.path.join(args.io_dir, args.valid_file + args.suffix), fields=args.fields)\nconvert_iob_conll_to_iobes(os.path.join(args.io_dir, args.test_file), os.path.join(args.io_dir, args.test_file + args.suffix), fields=args.fields)\n'"
api-examples/iobes_result_to_bio.py,0,"b'import argparse\nfrom baseline.reader import _norm_ext\nfrom baseline.utils import convert_iobes_conll_to_bio\n\nparser = argparse.ArgumentParser(description=""Convert a CONLL file from IOBES to BIO. This defaults to changing the right two most columns and is used to convert the output of IOBES taggers so that conlleval.pl will work on them."")\nparser.add_argument(""--file"", help=""The file to convert"", default=""conllresults.conll"")\nparser.add_argument(""--fields"", help=""The fields to convert"", default=[-2, -1], type=int, nargs=""+"")\nparser.add_argument(""--delim"", ""-d"", help=""The delimiter between items"", default="" "")\nparser.add_argument(""--suffix"", help=""Suffix to append"", default="".bio"", type=_norm_ext)\nargs = parser.parse_args()\n\nconvert_iobes_conll_to_bio(args.file, args.file + args.suffix, args.fields, args.delim)\n'"
api-examples/layers-classify-hf-datasets.py,10,"b'import argparse\nimport baseline\nimport baseline.embeddings\nimport baseline.pytorch.embeddings\nfrom collections import Counter\nfrom eight_mile.confusion import ConfusionMatrix\nfrom eight_mile.pytorch.optz import EagerOptimizer\nimport eight_mile.pytorch.layers as L\nimport nlp as nlp_datasets\nimport numpy as np\nimport os\nimport time\nimport torch\nimport torch.nn.functional as F\n\n\ndef to_device(d):\n    if isinstance(d, dict):\n        return {k: v.cuda() for k, v in d.items() if k != \'id\'}\n    return d.cuda()\n\n\ndef to_host(o):\n    return o.cpu().float().numpy()\n\n\ndef create_vocabs(datasets, vectorizers):\n    vocabs = {k: Counter() for k in vectorizers.keys()}\n    for dataset in datasets:\n        for k, v in vectorizers.items():\n            vocabs[k] = Counter()\n            for example in dataset:\n                vocabs[k] += v.count(example[\'sentence\'].split())\n    return vocabs\n\n\ndef create_featurizer(vectorizers, vocabs, primary_key=\'word\'):\n    def convert_to_features(batch):\n\n        features = {k: [] for k in vectorizers.keys()}\n\n        features[\'lengths\'] = []\n        features[\'id\'] = batch[\'idx\']\n        features[\'y\'] = batch[\'label\']\n\n        for i, text in enumerate(batch[\'sentence\']):\n            for k, v in vectorizers.items():\n                vec, lengths = v.run(text.split(), vocabs[k])\n                if k == primary_key:\n                    features[\'lengths\'].append(lengths)\n                features[k].append(vec.tolist())\n\n        return features\n    return convert_to_features\n\n\nparser = argparse.ArgumentParser(description=\'Train a Layers model with PyTorch API\')\nparser.add_argument(\'--model_type\', help=\'What type of model to build\', type=str, default=\'default\')\nparser.add_argument(\'--poolsz\', help=\'How many hidden units for pooling\', type=int, default=100)\nparser.add_argument(\'--dsz\', help=\'Embeddings dimension size\', type=int, default=300)\nparser.add_argument(\'--stacksz\', help=\'How many hidden units for stacking\', type=int, nargs=\'+\')\nparser.add_argument(\'--name\', help=\'(optional) signature name\', type=str)\nparser.add_argument(\'--epochs\', help=\'Number of epochs to train\', type=int, default=2)\nparser.add_argument(\'--batchsz\', help=\'Batch size\', type=int, default=50)\nparser.add_argument(\'--filts\', help=\'Parallel convolution filter widths (if default model)\', type=int, default=[3, 4, 5], nargs=\'+\')\nparser.add_argument(\'--mxlen\', help=\'Maximum post length (number of words) during training\', type=int, default=100)\nparser.add_argument(\'--dataset\', help=\'HuggingFace Datasets id\', default=[\'glue\', \'sst2\'], nargs=\'+\')\nparser.add_argument(\'--embeddings\', help=\'Pretrained embeddings file\', default=\'https://www.dropbox.com/s/699kgut7hdb5tg9/GoogleNews-vectors-negative300.bin.gz?dl=1\')\nparser.add_argument(\'--ll\', help=\'Log level\', type=str, default=\'info\')\nparser.add_argument(\'--lr\', help=\'Learning rate\', type=float, default=0.001)\nparser.add_argument(\'--blcache\', help=\'Cache for embeddings\', default=os.path.expanduser(\'~/.bl-data\'))\nparser.add_argument(\'--output\', type=str, help=\'Write a glue-style file, e.g. [SST-2.tsv]\')\nparser.add_argument(\'--device\', type=str,\n                    default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                    help=""Device (cuda or cpu)"")\nargs = parser.parse_known_args()[0]\n\nembeddings_file = baseline.EmbeddingDownloader(args.embeddings, embedding_dsz=args.dsz, embedding_sha1=None, data_download_cache=args.blcache).download()\n\nfeature_desc = {\n    \'word\': {\n        \'vectorizer\': baseline.Token1DVectorizer(mxlen=100, transform_fn=baseline.lowercase),\n        \'embed\': {\'file\': embeddings_file, \'type\': \'default\', \'unif\': 0.25, \'dsz\': args.dsz}\n    }\n}\n\nvectorizers = {k: v[\'vectorizer\'] for k, v in feature_desc.items()}\n\ndataset = nlp_datasets.load_dataset(*args.dataset)\nvocabs = create_vocabs(dataset.values(), vectorizers)\n\n# This builds a set of embeddings objects, these are typically not DL-specific\n# but if they happen to be addons, they can be\nembeddings = dict()\nfor k, v in feature_desc.items():\n    embed_config = v[\'embed\']\n    embeddings_for_k = baseline.embeddings.load_embeddings(\'word\', embed_file=embed_config[\'file\'], known_vocab=vocabs[k],\n                                                           embed_type=embed_config.get(\'type\', \'default\'),\n                                                           unif=embed_config.get(\'unif\', 0.), use_mmap=True)\n\n    embeddings[k] = embeddings_for_k[\'embeddings\']\n    # Reset the vocab to the embeddings one\n    vocabs[k] = embeddings_for_k[\'vocab\']\n\n\ntrain_set = dataset[\'train\']\nvalid_set = dataset[\'validation\']\ntest_set = dataset[\'test\']\n\nconvert_to_features = create_featurizer(vectorizers, vocabs)\ntrain_set = train_set.map(convert_to_features, batched=True)\ntrain_set.set_format(type=\'torch\', columns=list(vectorizers.keys()) + [\'y\', \'lengths\'])\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batchsz)\n\nvalid_set = valid_set.map(convert_to_features, batched=True)\nvalid_set.set_format(type=\'torch\', columns=list(vectorizers.keys()) + [\'y\', \'lengths\'])\nvalid_loader = torch.utils.data.DataLoader(valid_set, batch_size=args.batchsz)\n\ntest_set = test_set.map(convert_to_features, batched=True)\ntest_set.set_format(type=\'torch\', columns=list(vectorizers.keys()) + [\'y\', \'id\', \'lengths\'])\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=args.batchsz)\n\n\nstacksz = len(args.filts) * args.poolsz\n\n\nmodel = to_device(\n    L.EmbedPoolStackModel(2, L.EmbeddingsStack(embeddings), L.WithoutLength(L.ParallelConv(args.dsz, args.poolsz, args.filts)), L.Highway(stacksz))\n)\n\n\ndef loss(model, x, y):\n    y_ = model(x)\n    l = F.nll_loss(y_, y)\n    return l\n\n\noptimizer = EagerOptimizer(loss, optim=""adam"", lr=0.001)\n\nfor epoch in range(args.epochs):\n    loss_acc = 0.\n    step = 0\n    start = time.time()\n    model.train()\n    for x in train_loader:\n        x = to_device(x)\n        y = x.pop(\'y\')\n        loss_value = optimizer.update(model, x, y)\n        loss_acc += loss_value\n        step += 1\n    print(\'training time {}\'.format(time.time() - start))\n    mean_loss = loss_acc / step\n    print(\'Training Loss {}\'.format(mean_loss))\n    cm = ConfusionMatrix([\'0\', \'1\'])\n    model.eval()\n    for x in valid_loader:\n        x = to_device(x)\n        with torch.no_grad():\n            y = x.pop(\'y\')\n            y_ = np.argmax(to_host(model(x)), axis=1)\n            cm.add_batch(y, y_)\n    print(cm)\n    print(cm.get_all_metrics())\n\n\nif args.output:\n    model.eval()\n    print(f\'Writing GLUE-style output file {args.output}\')\n    with open(args.output, \'w\') as wf:\n        wf.write(\'id\\tlabel\\n\')\n        with torch.no_grad():\n            for x in test_loader:\n                ids = x[\'id\']\n                x = to_device(x)\n                y = x.pop(\'y\')\n                ys = np.argmax(to_host(model(x)), axis=1)\n                for id, y in zip(ids, ys):\n                    wf.write(f\'{id.item()}\\t{y.item()}\\n\')\n\n\n'"
api-examples/layers-classify-pytorch.py,9,"b'import argparse\nimport baseline.embeddings\nfrom eight_mile.confusion import ConfusionMatrix\nimport baseline\nfrom eight_mile.pytorch.optz import OptimizerManager, EagerOptimizer\nimport baseline.pytorch.embeddings\nimport eight_mile.pytorch.layers as L\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport logging\nimport numpy as np\nimport time\nimport torch\n\n\ndef to_device(m):\n    return m.cuda()\n\n\ndef to_host(o):\n    return o.cpu().float().numpy()\n\n\nparser = argparse.ArgumentParser(description=\'Train a Layers model with PyTorch API\')\nparser.add_argument(\'--model_type\', help=\'What type of model to build\', type=str, default=\'default\')\nparser.add_argument(\'--poolsz\', help=\'How many hidden units for pooling\', type=int, default=100)\nparser.add_argument(\'--stacksz\', help=\'How many hidden units for stacking\', type=int, nargs=\'+\')\nparser.add_argument(\'--name\', help=\'(optional) signature name\', type=str)\nparser.add_argument(\'--epochs\', help=\'Number of epochs to train\', type=int, default=2)\nparser.add_argument(\'--batchsz\', help=\'Batch size\', type=int, default=50)\nparser.add_argument(\'--filts\', help=\'Parallel convolution filter widths (if default model)\', type=int, default=[3, 4, 5], nargs=\'+\')\nparser.add_argument(\'--mxlen\', help=\'Maximum post length (number of words) during training\', type=int, default=100)\nparser.add_argument(\'--train\', help=\'Training file\', default=\'../data/stsa.binary.phrases.train\')\nparser.add_argument(\'--valid\', help=\'Validation file\', default=\'../data/stsa.binary.dev\')\nparser.add_argument(\'--test\', help=\'Testing file\', default=\'../data/stsa.binary.test\')\nparser.add_argument(\'--embeddings\', help=\'Pretrained embeddings file\', default=\'/data/embeddings/GoogleNews-vectors-negative300.bin\')\nparser.add_argument(\'--ll\', help=\'Log level\', type=str, default=\'info\')\nparser.add_argument(\'--lr\', help=\'Learning rate\', type=float, default=0.001)\nparser.add_argument(""--device"", type=str,\n                    default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                    help=""Device (cuda or cpu)"")\nargs = parser.parse_known_args()[0]\n\n\nfeature_desc = {\n    \'word\': {\n        \'vectorizer\': baseline.Token1DVectorizer(mxlen=100, transform_fn=baseline.lowercase),\n        \'embed\': {\'file\': args.embeddings, \'type\': \'default\', \'unif\': 0.25}\n    }\n}\n# Create a reader that is using our vectorizers to parse a TSV file\n# with rows like:\n# <label>\\t<sentence>\\n\n\nclass DictionaryDatasetWrapper(Dataset):\n    def __init__(self, x, x_lengths, y):\n        self.tensor_dataset = TensorDataset(x, x_lengths, y)\n\n    def __getitem__(self, index):\n        # stuff\n        x, x_length, y = self.tensor_dataset[index]\n        return {\'word\': x.to(args.device), ""lengths"": x_length.to(args.device)}, y.to(args.device)\n\n    def __len__(self):\n        return len(self.tensor_dataset)\n\n\nclass Data:\n\n    def __init__(self, ts, batchsz):\n        self.ds = self._to_tensors(ts)\n        self.batchsz = batchsz\n\n    def _to_tensors(self, ts):\n        x = []\n        x_lengths = []\n        y = []\n        for sample in ts:\n            x.append(sample[\'word\'].squeeze())\n            x_lengths.append(sample[\'word_lengths\'].squeeze())\n            y.append(sample[\'y\'].squeeze())\n        return DictionaryDatasetWrapper(torch.tensor(np.stack(x), dtype=torch.long), torch.tensor(np.stack(x_lengths), dtype=torch.long), torch.tensor(np.stack(y), dtype=torch.long))\n\n    def get_input(self, training=False):\n        return DataLoader(self.ds, batch_size=self.batchsz, shuffle=training)\n\n\nvectorizers = {k: v[\'vectorizer\'] for k, v in feature_desc.items()}\nreader = baseline.TSVSeqLabelReader(vectorizers, clean_fn=baseline.TSVSeqLabelReader.do_clean)\n\ntrain_file = args.train\nvalid_file = args.valid\ntest_file = args.test\n\n\n# This builds a set of counters\nvocabs, labels = reader.build_vocab([train_file,\n                                     valid_file,\n                                     test_file])\n\n# This builds a set of embeddings objects, these are typically not DL-specific\n# but if they happen to be addons, they can be\nembeddings = dict()\nfor k, v in feature_desc.items():\n    embed_config = v[\'embed\']\n    embeddings_for_k = baseline.embeddings.load_embeddings(\'word\', embed_file=embed_config[\'file\'], known_vocab=vocabs[k],\n                                                           embed_type=embed_config.get(\'type\', \'default\'),\n                                                           unif=embed_config.get(\'unif\', 0.), use_mmap=True)\n\n    embeddings[k] = embeddings_for_k[\'embeddings\']\n    # Reset the vocab to the embeddings one\n    vocabs[k] = embeddings_for_k[\'vocab\']\n\n\ntrain_set = Data(reader.load(train_file, vocabs=vocabs, batchsz=1), args.batchsz)\nvalid_set = Data(reader.load(valid_file, vocabs=vocabs, batchsz=1), args.batchsz)\ntest_set = Data(reader.load(test_file, vocabs=vocabs, batchsz=1), args.batchsz)\n\nstacksz = len(args.filts) * args.poolsz\nnum_epochs = 2\n\nmodel = to_device(\n    L.EmbedPoolStackModel(2, L.EmbeddingsStack(embeddings), L.WithoutLength(L.ParallelConv(300, args.poolsz, args.filts)), L.Highway(stacksz))\n)\n\n\ndef loss(model, x, y):\n    y_ = model(x)\n    l = F.nll_loss(y_, y)\n    return l\n\n\noptimizer = EagerOptimizer(loss, optim=""adam"", lr=0.001)\n\nfor epoch in range(num_epochs):\n    loss_acc = 0.\n    step = 0\n    start = time.time()\n    for x, y in train_set.get_input(training=True):\n        loss_value = optimizer.update(model, x, y)\n        loss_acc += loss_value\n        step += 1\n    print(\'training time {}\'.format(time.time() - start))\n    mean_loss = loss_acc / step\n    print(\'Training Loss {}\'.format(mean_loss))\n    cm = ConfusionMatrix([\'0\', \'1\'])\n    for x, y in valid_set.get_input():\n        with torch.no_grad():\n            y_ = np.argmax(to_host(model(x)), axis=1)\n            cm.add_batch(y, y_)\n    print(cm)\n    print(cm.get_all_metrics())\n\nprint(\'FINAL\')\ncm = ConfusionMatrix([\'0\', \'1\'])\nwith torch.no_grad():\n    for x, y in test_set.get_input():\n        y_ = np.argmax(to_host(model(x)), axis=1)\n        cm.add_batch(y, y_)\n\nprint(cm)\nprint(cm.get_all_metrics())\n'"
api-examples/layers-classify-tf.py,0,"b'import argparse\nimport baseline\nfrom eight_mile.utils import get_version\nfrom eight_mile.confusion import ConfusionMatrix\nimport baseline.tf.embeddings\nimport eight_mile.tf.layers as L\nfrom eight_mile.tf.layers import SET_TRAIN_FLAG, set_tf_log_level, set_tf_eager_mode\nfrom eight_mile.tf.optz import EagerOptimizer\nset_tf_eager_mode(True)\nimport tensorflow as tf\nfrom tensorflow.compat.v1 import count_nonzero\nimport logging\nimport numpy as np\nimport time\n\ndef get_logging_level(ll):\n    ll = ll.lower()\n    if ll == \'debug\':\n        return logging.DEBUG\n    if ll == \'info\':\n        return logging.INFO\n    return logging.WARNING\n\n\n\n#tf.config.gpu.set_per_process_memory_growth(True)\n\nNUM_PREFETCH = 2\nSHUF_BUF_SZ = 5000\n\n\ndef to_device(m):\n    return m\n\n\ndef to_host(o):\n    return o\n\n\nparser = argparse.ArgumentParser(description=\'Train a Layers model with TensorFlow API\')\nparser.add_argument(\'--model_type\', help=\'What type of model to build\', type=str, default=\'default\')\nparser.add_argument(\'--poolsz\', help=\'How many hidden units for pooling\', type=int, default=100)\nparser.add_argument(\'--stacksz\', help=\'How many hidden units for stacking\', type=int, nargs=\'+\')\nparser.add_argument(\'--name\', help=\'(optional) signature name\', type=str)\nparser.add_argument(\'--epochs\', help=\'Number of epochs to train\', type=int, default=2)\nparser.add_argument(\'--batchsz\', help=\'Batch size\', type=int, default=50)\nparser.add_argument(\'--filts\', help=\'Parallel convolution filter widths (if default model)\', type=int, default=[3, 4, 5], nargs=\'+\')\nparser.add_argument(\'--mxlen\', help=\'Maximum post length (number of words) during training\', type=int, default=100)\nparser.add_argument(\'--train\', help=\'Training file\', default=\'../data/stsa.binary.phrases.train\')\nparser.add_argument(\'--valid\', help=\'Validation file\', default=\'../data/stsa.binary.dev\')\nparser.add_argument(\'--test\', help=\'Testing file\', default=\'../data/stsa.binary.test\')\nparser.add_argument(\'--embeddings\', help=\'Pretrained embeddings file\', default=\'/data/embeddings/GoogleNews-vectors-negative300.bin\')\nparser.add_argument(\'--ll\', help=\'Log level\', type=str, default=\'info\')\nparser.add_argument(\'--lr\', help=\'Learning rate\', type=float, default=0.001)\nparser.add_argument(\'--tf_ll\', help=\'TensorFlow Log level\', type=str, default=\'warn\')\n\nargs = parser.parse_known_args()[0]\n\n\nlogging.basicConfig(level=get_logging_level(args.ll))\nset_tf_log_level(args.tf_ll)\n\nfeature_desc = {\n    \'word\': {\n        \'vectorizer\': baseline.Token1DVectorizer(mxlen=100, transform_fn=baseline.lowercase),\n        \'embed\': {\'file\': args.embeddings, \'type\': \'default\', \'unif\': 0.25}\n    }\n}\n\nvectorizers = {k: v[\'vectorizer\'] for k, v in feature_desc.items()}\nreader = baseline.TSVSeqLabelReader(vectorizers, clean_fn=baseline.TSVSeqLabelReader.do_clean)\n\ntrain_file = args.train\nvalid_file = args.valid\ntest_file = args.test\n\n\n# This builds a set of counters\nvocabs, labels = reader.build_vocab([train_file,\n                                     valid_file,\n                                     test_file])\n\n# This builds a set of embeddings objects, these are typically not DL-specific\n# but if they happen to be addons, they can be\nembeddings = dict()\nfor k, v in feature_desc.items():\n    embed_config = v[\'embed\']\n    embeddings_for_k = baseline.embeddings.load_embeddings(\'word\', embed_file=embed_config[\'file\'], known_vocab=vocabs[k],\n                                                           embed_type=embed_config.get(\'type\', \'default\'),\n                                                           unif=embed_config.get(\'unif\', 0.), use_mmap=True)\n\n    embeddings[k] = embeddings_for_k[\'embeddings\']\n    # Reset the vocab to the embeddings one\n    vocabs[k] = embeddings_for_k[\'vocab\']\n\n\nclass Data:\n\n    def __init__(self, ts, batchsz):\n        self.x, self.y = self._to_tensors(ts)\n        self.batchsz = batchsz\n\n    def _to_tensors(self, ts):\n        x = []\n        y = []\n        for sample in ts:\n            x.append(sample[\'word\'].squeeze())\n            y.append(sample[\'y\'].squeeze())\n        return np.stack(x), np.stack(y)\n\n    def get_input(self, training=False):\n        SET_TRAIN_FLAG(training)\n        dataset = tf.data.Dataset.from_tensor_slices((self.x, self.y))\n        dataset = dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n        dataset = dataset.batch(50)\n        dataset = dataset.map(lambda x, y: ({\'word\': x, \'lengths\': count_nonzero(x, axis=1)}, y))\n        dataset = dataset.prefetch(NUM_PREFETCH)\n        return dataset\n\n\ntrain_set = Data(reader.load(train_file, vocabs=vocabs, batchsz=1), args.batchsz)\nvalid_set = Data(reader.load(valid_file, vocabs=vocabs, batchsz=1), args.batchsz)\ntest_set = Data(reader.load(test_file, vocabs=vocabs, batchsz=1), args.batchsz)\n\nstacksz = len(args.filts) * args.poolsz\nnum_epochs = 2\n\nmodel = to_device(\n    L.EmbedPoolStackModel(2, L.EmbeddingsStack(embeddings), L.WithoutLength(L.ParallelConv(None, args.poolsz, args.filts)), L.Highway(stacksz))\n)\n\n\ndef loss(model, x, y):\n  y_ = model(x)\n  return tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n\n\n# This works with TF 2.0 and PyTorch:\noptimizer = EagerOptimizer(loss, optim=""adam"", lr=0.001)\n\n\n@tf.function\ndef train_step(optimizer, model, x, y):\n    loss_value = optimizer.update(model, x, y)\n    return loss_value\n\nfor epoch in range(num_epochs):\n    loss_acc = 0.\n    step = 0\n    start = time.time()\n    for x, y in train_set.get_input(training=True):\n        loss_value = train_step(optimizer, model, x, y)\n        loss_acc += loss_value\n        step += 1\n    \n    print(\'training time {}\'.format(time.time() - start))\n    mean_loss = loss_acc / step\n    print(\'Training Loss {}\'.format(mean_loss))\n    cm = ConfusionMatrix([\'0\', \'1\'])\n    for x, y in valid_set.get_input():\n        y_ = np.argmax(to_device(model(x)), axis=1)\n        cm.add_batch(y, y_)\n    print(cm)\n    print(cm.get_all_metrics())\n\nprint(\'FINAL\')\ncm = ConfusionMatrix([\'0\', \'1\'])\nfor x, y in test_set.get_input():\n    y_ = tf.argmax(to_device(model(x)), axis=1, output_type=tf.int32)\n    cm.add_batch(y, y_)\n\nprint(cm)\nprint(cm.get_all_metrics())\n'"
api-examples/lm-text.py,0,"b'import baseline as bl\nimport argparse\nimport os\nfrom baseline.utils import str2bool\nparser = argparse.ArgumentParser(description=\'Classify text with a model\')\nparser.add_argument(\'--model\', help=\'A classifier model\', required=True, type=str)\nparser.add_argument(\'--text\', help=\'raw value\', type=str)\nparser.add_argument(\'--device\', help=\'device\')\nparser.add_argument(\'--backend\', help=\'backend\', choices={\'tf\', \'pytorch\'}, default=\'tf\')\nparser.add_argument(\'--prefer_eager\', help=""If running in TensorFlow, should we prefer eager model"", type=str2bool)\n\n\nargs = parser.parse_known_args()[0]\n\nif args.backend == \'tf\':\n    from eight_mile.tf.layers import set_tf_eager_mode\n    set_tf_eager_mode(args.prefer_eager)\n\nif os.path.exists(args.text) and os.path.isfile(args.text):\n    texts = []\n    with open(args.text, \'r\') as f:\n        for line in f:\n            text = line.strip().split()\n            texts += [text]\n\nelse:\n    texts = args.text.split()\n\nprint(texts)\n\nm = bl.LanguageModelService.load(args.model, device=args.device)\nprint(m.predict(texts))\n'"
api-examples/preproc-tlm.py,0,"b'import argparse\nimport baseline\nfrom baseline.vectorizers import BPEVectorizer1D\nfrom eight_mile.utils import write_yaml, mlm_masking\nimport json\nimport struct\nimport logging\nimport numpy as np\nimport os\nlogger = logging.getLogger(\'baseline\')\ntry:\n    import tensorflow as tf\nexcept:\n    pass\n\n\ndef create_record(chunk, str_lookup, prefix, suffix, mask_value, vocab_size):\n    """"""Emit a record\n\n    :param chunk: A chunk of integer inputs\n    :param str_lookup: A lookup table from integers to strings\n    :param prefix: A prefix integer token\n    :param suffix: A suffix integer token\n    :param mask_value: An integer value representing a [MASK]\n    :param vocab_size: The total size of the vocab\n    :return: An object with `[xy]_str` and `[xy]` entries\n    """"""\n    ignore_prefix = False\n    ignore_suffix = False\n    if prefix:\n        chunk = [prefix] + chunk\n        ignore_prefix = True\n    if suffix:\n        chunk = [suffix] + chunk\n        ignore_suffix = True\n\n    inputs, labels = mlm_masking(np.array(chunk), mask_value, vocab_size, ignore_prefix, ignore_suffix)\n    return {\'x\': inputs, \'y\': labels, \'x_str\': [str_lookup[s] for s in inputs], \'y_str\': [str_lookup[s] for s in labels]}\n\n\ndef in_bytes(mb):\n    return mb * 1024 * 1024\n\n\nclass RollingWriter:\n    def __init__(self, name, fields, max_file_size_mb):\n        self.name = name\n        self.counter = 1\n        self.fields = fields\n        self.current_file_size = 0\n        self.writer = None\n        self.max_file_size = in_bytes(max_file_size_mb)\n        self._rollover_file()\n\n    def _open_file(self, filename):\n        return open(filename, \'w\')\n\n    def _rollover_file(self):\n        if self.writer:\n            self.writer.close()\n        filename = f\'{self.name}-{self.counter}.{self.suffix}\'\n        self.counter += 1\n        self.current_file_size = 0\n        logger.info(""Rolling over.  New file [%s]"", filename)\n        self.writer = self._open_file(filename)\n\n    @property\n    def suffix(self):\n        raise Exception(""Dont know suffix in ABC"")\n\n    def _write_line(self, str_val):\n        self.writer.write(str_val)\n        return len(str_val.encode(""utf8""))\n\n    def _write_line_rollover(self, l):\n        sz = self._write_line(l)\n        self.current_file_size += sz\n        if self.current_file_size > self.max_file_size:\n            self._rollover_file()\n\n    def close(self):\n        self.writer.close()\n\n\nclass TSVWriter(RollingWriter):\n    def __init__(self, name, fields, max_file_size_mb):\n        super().__init__(name, fields, max_file_size_mb)\n\n    def _to_str(self, value):\n        if isinstance(value, np.ndarray):\n            value = [str(v) for v in value]\n        return value\n\n    def write(self, record):\n        l = [\' \'.join(self._to_str(record[f])) for f in self.fields]\n        str_val = \'\\t\'.join(l) + \'\\n\'\n        self._write_line_rollover(str_val)\n\n    @property\n    def suffix(self):\n        return \'tsv\'\n\n\nclass JSONLWriter(RollingWriter):\n\n    def __init__(self, name, fields, max_file_size_mb):\n        super().__init__(name, fields, max_file_size_mb)\n\n    def write(self, record):\n        r = {}\n        for f in self.fields:\n            if isinstance(record[f], np.ndarray):\n                value = record[f].tolist()\n            else:\n                value = record[f]\n            r[f] = value\n        output = json.dumps(r) + \'\\n\'\n        self._write_line_rollover(output)\n\n    @property\n    def suffix(self):\n        return \'json\'\n\n\nclass TFRecordRollingWriter(RollingWriter):\n    def __init__(self, name, fields, max_file_size_mb):\n        try:\n            self.RecordWriterClass = tf.io.TFRecordWriter\n        except Exception as e:\n            raise Exception(""tfrecord package could not be loaded, pip install that first, along with crc32c"")\n        super().__init__(name, fields, max_file_size_mb)\n\n    @staticmethod\n    def _bytes_feature(value):\n        """"""Returns a bytes_list from a string / byte.""""""\n        if isinstance(value, type(tf.constant(0))):\n            value = value.numpy()  # BytesList won\'t unpack a string from an EagerTensor.\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n    @staticmethod\n    def _int64_feature(value):\n        """"""Returns an int64_list from a bool / enum / int / uint.""""""\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n    def _open_file(self, filename):\n        return self.RecordWriterClass(filename)\n\n    def _write_line(self, str_val):\n        self.writer.write(str_val)\n        return len(str_val)\n\n    def serialize_tf_example(self, record):\n        """"""\n        Creates a tf.Example message ready to be written to a file.\n        """"""\n        # Create a dictionary mapping the feature name to the tf.Example-compatible\n        # data type.\n        feature= {}\n        for f in self.fields:\n            if f.endswith(\'_str\'):\n                value = \' \'.join(record[f])\n                value = TFRecordRollingWriter._bytes_feature(value.encode(\'utf-8\'))\n            else:\n                value = TFRecordRollingWriter._int64_feature(record[f])\n            feature[f] = value\n\n        example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n        return example_proto.SerializeToString()\n\n    def write(self, record):\n        example_str = self.serialize_tf_example(record)\n        self._write_line_rollover(example_str)\n\n    @property\n    def suffix(self):\n        return \'tfrecord\'\n\n\ndef create_file_writer(fmt, name, fields, max_file_size_mb):\n    if fmt == \'tsv\':\n        return TSVWriter(name, fields, max_file_size_mb)\n    if fmt == \'json\':\n        return JSONLWriter(name, fields, max_file_size_mb)\n    return TFRecordRollingWriter(name, fields, max_file_size_mb)\n\n\nparser = argparse.ArgumentParser(description=\'Convert text into MLM fixed width contexts\')\n\nparser.add_argument(\'--text\', help=\'The text to classify as a string, or a path to a file with each line as an example\',\n                    type=str)\nparser.add_argument(\'--codes\', help=\'BPE codes\')\nparser.add_argument(\'--vocab\', help=\'BPE vocab\')\nparser.add_argument(""--nctx"", type=int, default=256, help=""Max input length"")\nparser.add_argument(""--fmt"", type=str, default=\'json\', choices=[\'json\', \'tsv\', \'tfrecord\'])\nparser.add_argument(""--fields"", type=str, nargs=""+"", default=[""x_str"", ""y_str""])\nparser.add_argument(""--output"", type=str, help=""Output base name, e.g. /path/to/output/record"")\nparser.add_argument(""--prefix"", type=str, help=""Prefix every line with this token"")\nparser.add_argument(""--suffix"", type=str, help=""Suffix every line with this token"")\nparser.add_argument(""--max_file_size"", type=int, default=100, help=""Shard size, defaults to 100MB"")\nparser.add_argument(""--stride"", type=int, help=""Tokens to stride before next read, defaults to `nctx`"")\nparser.add_argument(""--eos_on_eol"", type=baseline.str2bool, default=True)\nparser.add_argument(""--cased"", type=baseline.str2bool, default=True)\nargs = parser.parse_args()\nif not args.output:\n    args.output = f\'{args.text}.records\'\n\nprint(args.output)\ntransform = baseline.lowercase\nvectorizer = BPEVectorizer1D(transform_fn=transform, model_file=args.codes, vocab_file=args.vocab, mxlen=1024)\n\nlookup_indices = []\nwords = []\nindices2word = baseline.revlut(vectorizer.vocab)\nvocab_size = max(vectorizer.vocab.values()) + 1\nnctx = args.nctx\nmask_value = vectorizer.vocab[\'[MASK]\']\nprefix = suffix = None\nroot_dir = os.path.dirname(args.output)\nif not os.path.exists(root_dir):\n    os.makedirs(root_dir)\n\nif args.prefix:\n    nctx -= 1\n    prefix = vectorizer.vocab[args.prefix]\n\nif args.suffix:\n    nctx -= 1\n    suffix = vectorizer.vocab[args.suffix]\n\nfw = create_file_writer(args.fmt, args.output, args.fields, args.max_file_size)\nnum_samples = 0\nwith open(args.text, encoding=\'utf-8\') as rf:\n    for line in rf:\n        to_bpe = line.strip().split()\n        if args.eos_on_eol:\n            to_bpe += [\'<EOS>\']\n\n        output, available = vectorizer.run(to_bpe, vectorizer.vocab)\n        while available > 0:\n            if len(lookup_indices) == nctx:\n                record = create_record(lookup_indices, indices2word, prefix, suffix, mask_value, vocab_size)\n                fw.write(record)\n                num_samples += 1\n                lookup_indices = []\n            needed = nctx - len(lookup_indices)\n            if available >= needed:\n                lookup_indices += output[:needed].tolist()\n                output = output[needed:]\n                available -= needed\n                record = create_record(lookup_indices, indices2word, prefix, suffix, mask_value, vocab_size)\n                fw.write(record)\n                num_samples += 1\n                lookup_indices = []\n            # The amount available is less than what we need, so read the whole thing\n            else:\n                lookup_indices += output[:available].tolist()\n                available = 0\n\nfw.close()\nwrite_yaml({\'num_samples\': num_samples}, os.path.join(root_dir, \'md.yml\'))'"
api-examples/pretrain-discrim.py,25,"b'import logging\nimport time\nimport os\nfrom argparse import ArgumentParser\nfrom collections import namedtuple\nimport baseline\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DataLoader\nfrom eight_mile.utils import str2bool, write_json, revlut, print_table\nimport glob\nfrom baseline.pytorch.embeddings import *\nimport baseline.embeddings\nfrom eight_mile.optz import *\nfrom eight_mile.pytorch.optz import *\nfrom eight_mile.utils import Average, get_num_gpus_multiworker\nfrom eight_mile.pytorch.layers import checkpoint_for, rm_old_checkpoints, save_checkpoint, init_distributed\nlogger = logging.getLogger(__file__)\nfrom baseline.pytorch.lm import TransformerMaskedLanguageModel\nfrom eight_mile.pytorch.layers import TransformerEncoderStack, EmbeddingsStack, subsequent_mask\nfrom transformer_utils import MultiFileDatasetReader, TransformerDiscriminator, find_latest_checkpoint, \\\n    TensorWordDatasetReader, load_data_caching, get_lr_decay\n\n""""""Pre-train an discriminator Transformer model in PyTorch\n\nThis file uses Baseline to train a Transformer-based discriminative model\nmodel, similar to (https://openreview.net/pdf?id=r1xMH1BtvB)\n""""""\nLAMBDA = 50\nRow = namedtuple(\'Row\', \'original reconstructed guess\')\n\n\ndef print_batch(index2word, labels, recon_labels, logits):\n    j = 0\n\n    for orig, recon, guess in zip(labels, recon_labels, logits):\n        rows = []\n        orig = orig.tolist()\n        recon = recon.tolist()\n        guess = guess.squeeze()\n        guess = (guess > 0.5).tolist()\n\n        for i in range(10):\n            rows.append(Row(original=index2word[orig[i]], reconstructed=index2word[recon[i]], guess=str(guess[i])))\n        print_table(rows)\n        print(\'\\n\')\n        if j == 3:\n            return\n        j += 1\n\n\ndef create_generator(embeddings, d_model, d_ff, dropout, num_heads, num_layers, rpr_k, d_k):\n    """"""Produces an MLM generator model\n\n    :param embeddings: The pre-inited embeddings\n    :param d_model: The size of the model\n    :param d_ff: The feed-forward layer size\n    :param dropout: The amount of dropout\n    :param num_heads: The number of attention heads\n    :param num_layers: The number of layers\n    :param rpr_k: The relative position sizes\n    :param d_k: The head size if single headed\n    :return:\n    """"""\n    model = TransformerMaskedLanguageModel.create(embeddings,\n                                                  hsz=d_model,\n                                                  d_ff=d_ff,\n                                                  tie_weights=True,\n                                                  dropout=dropout,\n                                                  gpu=False,\n                                                  num_heads=num_heads,\n                                                  layers=num_layers,\n                                                  rpr_k=rpr_k,\n                                                  d_k=d_k,\n                                                  src_keys=[\'x\'], tgt_key=\'x\')\n    return model\n\n\ndef best_from(x_preds):\n    #return x_preds.argmax(axis=-1)\n    B, T, V = x_preds.shape\n    sample_dist = x_preds.exp().view(B * T, V)\n    output = torch.multinomial(sample_dist, num_samples=1).view(B, T)\n    #output = output.squeeze(0).item()\n    return output\n\n\ndef get_accuracy(preds, true_or_fake, logits):\n    flat_logits = logits.reshape(-1)\n    nz_preds = preds.view(-1)[flat_logits != 0]\n    nz_true_or_fake = true_or_fake.view(-1)[flat_logits != 0]\n\n    preds_true = (nz_preds > 0.5).squeeze().to(nz_true_or_fake.dtype)\n    num = torch.sum((nz_true_or_fake == preds_true).to(torch.float32))\n    denom = nz_true_or_fake.nelement()\n    return (num / denom).item()\n\n\ndef train():\n    parser = ArgumentParser()\n    parser.add_argument(""--basedir"", type=str)\n    parser.add_argument(""--train_file"", type=str, help=\'Optional file path to use for train file\')\n    parser.add_argument(""--valid_file"", type=str, help=\'Optional file path to use for valid file\')\n    parser.add_argument(""--streaming_load"", type=str2bool, default=True,\n                        help=""whether loading data in a streaming way or loading all data into memory once"")\n    parser.add_argument(""--gen_d_model"", type=int, default=256, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--discrim_d_model"", type=int, default=512, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--gen_d_ff"", type=int, default=1024, help=""FFN dimension"")\n    parser.add_argument(""--discrim_d_ff"", type=int, default=2048, help=""FFN dimension"")\n    parser.add_argument(""--gen_d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--discrim_d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--gen_num_heads"", type=int, default=8, help=""Number of heads"")\n    parser.add_argument(""--discrim_num_heads"", type=int, default=8, help=""Number of heads"")\n    parser.add_argument(""--gen_num_layers"", type=int, default=8, help=""Number of layers"")\n    parser.add_argument(""--discrim_num_layers"", type=int, default=8, help=""Number of layers"")\n    parser.add_argument(""--gen_dropout"", type=float, default=0.1, help=""Dropout"")\n    parser.add_argument(""--discrim_dropout"", type=float, default=0.1, help=""Dropout"")\n    parser.add_argument(\'--gen_rpr_k\', help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[8], nargs=\'+\')\n\n    parser.add_argument(\'--discrim_rpr_k\', help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[8], nargs=\'+\')\n\n    parser.add_argument(""--num_train_workers"", type=int, default=4, help=""Number train workers"")\n    parser.add_argument(""--num_valid_workers"", type=int, default=2, help=""Number valid workers"")\n    parser.add_argument(""--nctx"", type=int, default=64, help=""Max context length (for both encoder and decoder)"")\n    parser.add_argument(""--embed_type"", type=str, default=\'default\',\n                        help=""register label of the embeddings, so far support positional or learned-positional"")\n    parser.add_argument(""--pattern"", default=\'*.txt\', help=""Glob pattern for data"")\n    parser.add_argument(""--batch_size"", type=int, default=256, help=""Batch Size"")\n    parser.add_argument(""--dataset_key"", default=""reddit"",\n                        help=""dataset key for basedir"")\n    parser.add_argument(""--subword_model_file"", type=str, required=True)\n    parser.add_argument(""--subword_vocab_file"", type=str, required=True)\n    parser.add_argument(""--lr_scheduler"", type=str, default=\'cosine\', help=""The type of learning rate decay scheduler"")\n    parser.add_argument(""--lr_decay_steps"", type=int, help=""decay steps of lr scheduler"")\n    parser.add_argument(""--lr_decay_rate"", type=float, help=""decay rate of lr scheduler"")\n    parser.add_argument(""--lr_alpha"", type=float, help=""parameter alpha for cosine decay scheduler"")\n    parser.add_argument(""--optim"", default=""adam"", type=str, help=""Optimizer to use (defaults to adam)"")\n    parser.add_argument(""--lr"", type=float, default=4.0e-4, help=""Learning rate"")\n    parser.add_argument(""--clip"", type=float, default=0.25, help=""Clipping gradient norm"")\n    parser.add_argument(""--weight_decay"", type=float, default=0.0, help=""Weight decay"")\n    parser.add_argument(""--epochs"", type=int, default=20, help=""Num training epochs"")\n    parser.add_argument(""--restart_from"", type=str, help=""Option allows you to restart from the latest checkpoint in a directory"")\n    parser.add_argument(""--restart_tt"", type=str, choices=[\'step\', \'epoch\'],\n                        default=\'step\',\n                        help=""Optional param for legacy checkpoints (step|epoch)"")\n    parser.add_argument(""--warmup_steps"", type=int, default=10000, help=""Num warmup steps"")\n    parser.add_argument(""--saves_per_epoch"", type=int, default=100, help=""The number of checkpoints to save per epoch"")\n    parser.add_argument(""--print"", type=str2bool, default=True, help=""Print some output"")\n    parser.add_argument(""--device"", type=str,\n                        default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                        help=""Device (cuda or cpu)"")\n    parser.add_argument(""--distributed"",\n                        type=str2bool,\n                        default=False,\n                        help=""Are we doing distributed training?"")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""Local rank for distributed training (-1 means use the environment variables to find)"")\n\n    args = parser.parse_args()\n\n    if args.train_file and not args.valid_file:\n        logger.error(""If you provide a train_file, you must provide a valid_file"")\n        return\n\n    if not args.train_file and args.valid_file:\n        logger.error(""If you provide a valid_file, you must also provide a train_file"")\n        return\n\n    if args.basedir is None:\n        args.basedir = \'gd-{}-bpe-{}\'.format(args.dataset_key, os.getpid())\n    logging.basicConfig(\n        format=""%(name)s: %(levelname)s: %(message)s"",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN\n    )\n    num_gpus = get_num_gpus_multiworker()\n    args.distributed = args.distributed or num_gpus > 1\n    logger.info(f""Using {num_gpus} GPUs in this job."")\n\n    if args.distributed:\n        args.device = init_distributed(args.local_rank)\n\n    if args.streaming_load:\n        reader = MultiFileDatasetReader(args.nctx, args.subword_model_file, args.subword_vocab_file, args.pattern,\n                                        reader_type=""lang"")\n    else:\n        reader = TensorWordDatasetReader(args.nctx, \'bpe\', args.subword_model_file, args.subword_vocab_file)\n\n    # This just return the vocab from the BPE vectorizer\n    vocab = reader.build_vocab([])\n    gen_embed = baseline.embeddings.load_embeddings(\'x\', dsz=args.gen_d_model, known_vocab=vocab[\'x\'],\n                                                    embed_type=args.embed_type)\n    vocabs = gen_embed[\'vocab\']\n    index2word = revlut(vocabs)\n    discrim_embed = baseline.embeddings.load_embeddings(\'x\', dsz=args.discrim_d_model, known_vocab=vocab[\'x\'],\n                                                        embed_type=args.embed_type)\n\n    os.makedirs(args.basedir, exist_ok=True)\n    # We want to make sure to save our input vocab into the basedir for reuse later\n    write_json(vocabs, os.path.join(args.basedir, \'vocabs.json\'))\n    gen_embeddings = {\'x\': gen_embed[\'embeddings\']}\n    discrim_embeddings = {\'x\': discrim_embed[\'embeddings\']}\n    logger.info(""Loaded embeddings"")\n\n    if args.streaming_load:\n        train_set = reader.load(args.train_file, vocabs)\n        valid_set = reader.load(args.valid_file, vocabs)\n        train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=args.num_train_workers)\n        valid_loader = DataLoader(valid_set, batch_size=args.batch_size, num_workers=args.num_valid_workers)\n        steps_per_epoch = len(train_loader) // (args.batch_size*num_gpus)\n    else:\n        dataset = {\'train_file\': args.train_file, \'valid_file\': args.valid_file}\n        train_set = load_data_caching(\'bpe\', reader, dataset, \'train_file\', {\'x\': vocabs}, True, logger)\n        valid_set = load_data_caching(\'bpe\', reader, dataset, \'valid_file\', {\'x\': vocabs}, True, logger)\n\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_set) if args.distributed else None\n        train_loader = DataLoader(train_set, sampler=train_sampler, batch_size=args.batch_size, shuffle=(not args.distributed))\n        valid_loader = DataLoader(valid_set, batch_size=args.batch_size, shuffle=False)\n        steps_per_epoch = len(train_loader)\n\n    logger.info(""Loaded datasets"")\n    logger.info(""Using embedding type [%s]"", args.embed_type)\n\n    mask_value = vocabs.get(""[MASK]"", vocabs.get(""<MASK>"", -1))\n    if mask_value == -1:\n        logger.error(""We could not find a suitable masking token in the vocab"")\n        return\n    os.makedirs(args.basedir, exist_ok=True)\n    vocab_size = len(vocabs)\n\n    if len(args.gen_rpr_k) == 0 or args.gen_rpr_k[0] < 1:\n        gen_rpr_k = None\n    elif len(args.gen_rpr_k) == 1:\n        gen_rpr_k = args.gen_rpr_k[0]\n    else:\n        gen_rpr_k = args.gen_rpr_k\n\n    if len(args.gen_rpr_k) == 0 or args.discrim_rpr_k[0] < 1:\n        discrim_rpr_k = None\n    elif len(args.discrim_rpr_k) == 1:\n        discrim_rpr_k = args.discrim_rpr_k[0]\n    else:\n        discrim_rpr_k = args.discrim_rpr_k\n\n    gen_model = create_generator(gen_embeddings, args.gen_d_model, args.gen_d_ff, args.gen_dropout, args.gen_num_heads,\n                                 args.gen_num_layers, gen_rpr_k, args.gen_d_k)\n    discrim_model = TransformerDiscriminator(discrim_embeddings, args.discrim_d_model, args.discrim_d_ff,\n                                             args.discrim_dropout, args.discrim_num_heads, args.discrim_num_layers,\n                                             discrim_rpr_k, args.discrim_d_k)\n    gen_model.to(args.device)\n    gen_loss_fn = gen_model.create_loss()\n\n    discrim_model.to(args.device)\n    discrim_loss_fn = discrim_model.create_loss()\n    logger.info(""Loaded model and loss"")\n\n    update_on = steps_per_epoch // args.saves_per_epoch\n    report_on = update_on // 10\n    logger.info(f""Steps per epoch per GPU: {steps_per_epoch}. Saving checkpoint every {update_on} steps."")\n    lr_decay = get_lr_decay(args.lr_scheduler, args.lr, steps_per_epoch, args.epochs, logger,\n                            decay_steps=args.lr_decay_steps, decay_rate=args.lr_decay_rate, alpha=args.lr_alpha)\n    linear_warmup = WarmupLinearSchedulerPyTorch(args.warmup_steps, lr=args.lr)\n    lr_sched = CompositeLRScheduler(linear_warmup, lr_decay, lr=args.lr)\n\n    global_step = 0\n    start_epoch = 0\n    if args.restart_from:\n        if not os.path.isdir(args.restart_from):\n            raise Exception(f""Cannot restart from {args.restart_from}, directory not found"")\n        tick_type = args.restart_tt\n        discrim_latest = find_latest_checkpoint(args.restart_from, wildcard=f\'checkpoint-discrim-{tick_type}\')\n        step_num = int(discrim_latest.split(""-"")[-1])\n        gen_latest = find_latest_checkpoint(args.restart_from, wildcard=f\'checkpoint-gen-{tick_type}\')\n        discrim_model.load_state_dict(torch.load(discrim_latest))\n        gen_model.load_state_dict(torch.load(gen_latest))\n        if tick_type == \'step\':\n            start_epoch = step_num // steps_per_epoch\n            global_step = step_num\n        else:\n            start_epoch = step_num\n            global_step = steps_per_epoch * start_epoch\n\n    parameters = list(discrim_model.parameters()) + list(gen_model.parameters())\n    optz = OptimizerManager(parameters, global_step, optim=args.optim, lr=args.lr, lr_function=lr_sched, weight_decay=args.weight_decay)\n    logger.info(""Generator has {:,} parameters"".format(sum(p.numel() for p in gen_model.parameters() if p.requires_grad)))\n    logger.info(""Discriminator has {:,} parameters"".format(sum(p.numel() for p in discrim_model.parameters() if p.requires_grad)))\n    # Prepare model for distributed training if needed\n    if args.distributed:\n        # This program assume pure data parallelism, each model is on a single gpu\n        # If we wanted to support model and data parallelism we would need to update\n        # the selection of gpus based on rank, it would need to select multiple ids\n        # based on rank, here we select only a single gpu and use it for input and\n        # output.\n        gen_model = DistributedDataParallel(gen_model, device_ids=[args.device], output_device=args.device)\n        discrim_model = DistributedDataParallel(discrim_model, device_ids=[args.device], output_device=args.device)\n        logger.info(""Model located on %s"", args.device)\n\n    # This is the training loop\n    steps = global_step\n    model_base = os.path.join(args.basedir, \'checkpoint\')\n    discrim_base = f\'{model_base}-discrim\'\n    gen_base = f\'{model_base}-gen\'\n    for epoch in range(start_epoch, args.epochs):\n        gen_model.train()\n        discrim_model.train()\n        avg_gen_loss = Average(\'average_train_gen_loss\')\n        avg_discrim_loss = Average(\'average_train_discrim_loss\')\n        avg_discrim_acc = Average(\'average_train_discrim_acc\')\n        avg_train_loss = Average(\'average5_train_loss\')\n        metrics = {}\n        optz.zero_grad()\n        start = time.time()\n        for i, batch in enumerate(train_loader):\n            steps += 1\n            x, y = batch\n            noised_x = x.to(args.device)\n            # We are going to mask inplace and that will leave us with <PAD> anywhere that isnt MLM\n            labels = y.to(args.device, copy=True)\n            # Replace 15% of tokens\n            masked_indices = torch.bernoulli(torch.full(labels.shape, 0.15)).type(torch.bool)\n            # Anything not masked is 0 so no loss\n            labels[~masked_indices] = 0\n            # Of the masked items, mask 80% of them with [MASK]\n            indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).type(torch.bool) & masked_indices\n            noised_x[indices_replaced] = mask_value\n            # Replace 10% of them with random words, rest preserved for auto-encoding\n            indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).type(torch.bool) & masked_indices & ~indices_replaced\n            random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=args.device)\n            noised_x[indices_random] = random_words[indices_random]\n            labels = labels.transpose(0, 1).contiguous()\n            logits = gen_model({\'x\': noised_x}, None)[0]\n            gen_loss_step = gen_loss_fn(logits.transpose(0, 1).contiguous(), labels)\n            avg_gen_loss.update(gen_loss_step.item())\n            # Re-read labels from device, this clears the masked <PAD>\n            labels = y.to(args.device)\n\n            # The logits needs to be replaced with either argmax or sampling.  Which?\n            recon_labels = best_from(logits)\n            recon_labels[~masked_indices] = labels[~masked_indices]\n            true_or_fake = (recon_labels == labels).to(torch.float32).view(-1)\n            logits = discrim_model({\'x\': recon_labels})\n            discrim_loss_step = discrim_loss_fn(logits.view(-1), true_or_fake.view(-1))\n\n            total_loss_step = gen_loss_step + LAMBDA * discrim_loss_step\n            total_loss_step.backward()\n            avg_discrim_loss.update(discrim_loss_step.item())\n            avg_train_loss.update(total_loss_step.item())\n            avg_discrim_acc.update(get_accuracy(logits, true_or_fake, labels))\n            torch.nn.utils.clip_grad_norm_(parameters, args.clip)\n            optz.step()\n            optz.zero_grad()\n            if (i + 1) % report_on == 0:\n                logging.info(\'Loss g=%f, d=%f total=%f, Per token acc=%f\', avg_gen_loss.avg, avg_discrim_loss.avg, avg_train_loss.avg, avg_discrim_acc.avg)\n                if args.print:\n                    print_batch(index2word, labels, recon_labels, logits)\n\n            if (i + 1) % update_on == 0 and args.local_rank < 1:\n                elapsed = (time.time() - start)/60\n                logging.info(\'elapsed time this epoch %d min\', elapsed)\n                logging.info(\'elapsed step time %f steps/min\', i/elapsed)\n                save_checkpoint(gen_model, gen_base, steps, tick_type=\'step\')\n                save_checkpoint(discrim_model, discrim_base, steps, tick_type=\'step\')\n        # How much time elapsed in minutes\n        elapsed = (time.time() - start)/60\n        # This is the average training token-level loss across all machines\n        # This is the token-level training perplexity\n        metrics[\'train_elapsed_min\'] = elapsed\n        metrics[\'average_train_gen_loss\'] = avg_gen_loss.avg\n        metrics[\'average_train_discrim_loss\'] = avg_discrim_loss.avg\n        metrics[\'average_train_discrim_per_token_accuracy\'] = avg_discrim_acc.avg\n        metrics[\'average_train_loss\'] = avg_train_loss.avg\n\n        avg_valid_gen_loss = Average(\'average_valid_gen_loss\')\n        avg_valid_discrim_loss = Average(\'average_valid_discrim_loss\')\n        avg_valid_discrim_acc = Average(\'average_valid_discrim_acc\')\n        avg_valid_loss = Average(\'average_valid_loss\')\n        start = time.time()\n        gen_model.eval()\n        discrim_model.eval()\n        for batch in valid_loader:\n            with torch.no_grad():\n                x, y = batch\n                noised_x = x.to(args.device)\n                labels = y.to(args.device, copy=True)\n                # Replace 15% of tokens\n                masked_indices = torch.bernoulli(torch.full(labels.shape, 0.15)).type(torch.bool)\n                # Anything not masked is 0 so no loss\n                labels[~masked_indices] = 0\n                # Of the masked items, mask 80% of them with [MASK]\n                indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).type(torch.bool) & masked_indices\n                noised_x[indices_replaced] = mask_value\n                # Replace 10% of them with random work\n                indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).type(torch.bool) & masked_indices & ~indices_replaced\n                random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=args.device)\n                noised_x[indices_random] = random_words[indices_random]\n                labels = labels.transpose(0, 1).contiguous()\n                logits = gen_model({\'x\': noised_x}, None)[0]\n\n                gen_loss_step = gen_loss_fn(logits.transpose(0, 1).contiguous(), labels)\n                avg_valid_gen_loss.update(gen_loss_step.item())\n\n                labels = y.to(args.device)\n                recon_labels = best_from(logits)\n                recon_labels[~masked_indices] = labels[~masked_indices]\n                true_or_fake = (recon_labels == labels).to(torch.float32).view(-1)\n                logits = discrim_model({\'x\': recon_labels})\n                discrim_loss_step = discrim_loss_fn(logits.view(-1), true_or_fake.view(-1))\n                avg_valid_discrim_acc.update(get_accuracy(logits, true_or_fake, labels))\n                avg_valid_discrim_loss.update(discrim_loss_step.item())\n                total_loss_step = gen_loss_step + LAMBDA * discrim_loss_step\n                avg_valid_loss.update(total_loss_step.item())\n        elapsed = (time.time() - start)/60\n        metrics[\'valid_elapsed_min\'] = elapsed\n        metrics[\'average_valid_gen_loss\'] = avg_valid_gen_loss.avg\n        metrics[\'average_valid_discrim_loss\'] = avg_valid_discrim_loss.avg\n        metrics[\'average_valid_discrim_per_token_accuracy\'] = avg_valid_discrim_acc.avg\n        metrics[\'average_valid_loss\'] = avg_valid_loss.avg\n        logger.info(metrics)\n\n        if args.local_rank < 1:\n            save_checkpoint(discrim_model, discrim_base, epoch, tick_type=\'epoch\', save_npz=True)\n            save_checkpoint(gen_model, gen_base, epoch, tick_type=\'epoch\', save_npz=True)\n\n\nif __name__ == ""__main__"":\n    train()\n\n'"
api-examples/pretrain-paired.py,9,"b'import logging\nimport time\nimport os\nfrom argparse import ArgumentParser\nimport baseline\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DataLoader\nfrom eight_mile.utils import str2bool, write_json\nimport glob\nfrom baseline.pytorch.embeddings import *\nimport baseline.embeddings\nfrom eight_mile.optz import *\nfrom eight_mile.utils import Average, get_num_gpus_multiworker\nfrom eight_mile.pytorch.optz import *\nfrom eight_mile.pytorch.layers import save_checkpoint, init_distributed\nfrom transformer_utils import MultiFileDatasetReader, PairedModel, TripletLoss, AllLoss, TiedEmbeddingsSeq2SeqModel, \\\n    create_model, get_lr_decay\n\nlogger = logging.getLogger(__file__)\n\n""""""Pre-train a paired model in PyTorch\n\nThis file uses Baseline to train a Transformer-based ConveRT\nmodel (https://arxiv.org/pdf/1911.03688.pdf) or Seq2Seq with fastBPE with PyTorch on multiple GPUs.\n\n""""""\n\n\ndef train():\n    parser = ArgumentParser()\n    parser.add_argument(""--basedir"", type=str)\n    parser.add_argument(""--train_file"", type=str, help=\'Optional file path to use for train file\')\n    parser.add_argument(""--valid_file"", type=str, help=\'Optional file path to use for valid file\')\n    parser.add_argument(""--dataset_cache"", type=str, default=os.path.expanduser(\'~/.bl-data\'),\n                        help=""Path or url of the dataset cache"")\n    parser.add_argument(""--d_model"", type=int, default=512, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--d_ff"", type=int, default=2048, help=""FFN dimension"")\n    parser.add_argument(""--d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--num_heads"", type=int, default=8, help=""Number of heads"")\n    parser.add_argument(""--reduction_d_k"", type=int, default=64, help=""Dimensions of Key and Query in the single headed""\n                                                                      ""reduction layers"")\n    parser.add_argument(""--stacking_layers"", type=int, nargs=\'+\', default=[1024, 1024, 1024],\n                        help=""Hidden sizes of the dense stack (ff2 from the convert paper)"")\n    parser.add_argument(""--ff_pdrop"", type=float, default=0.1, help=""Dropout in the dense stack"")\n    parser.add_argument(""--num_train_workers"", type=int, default=4, help=""Number train workers"")\n    parser.add_argument(""--num_valid_workers"", type=int, default=2, help=""Number valid workers"")\n    parser.add_argument(""--num_layers"", type=int, default=6, help=""Number of layers"")\n    parser.add_argument(""--nctx"", type=int, default=64, help=""Max context length (for both encoder and decoder)"")\n    parser.add_argument(""--reader_type"", type=str, default=\'ntp\', choices=[\'ntp\', \'nsp\', \'preprocessed\'])\n    parser.add_argument(""--embed_type"", type=str, default=\'default\',\n                        help=""register label of the embeddings, so far support positional or learned-positional"")\n    parser.add_argument(""--pattern"", default=\'*.txt\', help=""Glob pattern for data"")\n    parser.add_argument(""--batch_size"", type=int, default=256, help=""Batch Size"")\n    parser.add_argument(""--dataset_key"", default=""reddit"",\n                        help=""dataset key for basedir"")\n    parser.add_argument(""--subword_model_file"", type=str, required=True)\n    parser.add_argument(""--subword_vocab_file"", type=str, required=True)\n    parser.add_argument(""--dropout"", type=float, default=0.1, help=""Dropout"")\n    parser.add_argument(""--lr_scheduler"", type=str, default=\'cosine\', help=""The type of learning rate decay scheduler"")\n    parser.add_argument(""--lr_decay_steps"", type=int, help=""decay steps of lr scheduler"")\n    parser.add_argument(""--lr_decay_rate"", type=float, help=""decay rate of lr scheduler"")\n    parser.add_argument(""--lr_alpha"", type=float, help=""parameter alpha for cosine decay scheduler"")\n    parser.add_argument(""--optim"", default=""adam"", type=str, help=""Optimizer to use (defaults to adam)"")\n    parser.add_argument(""--model_type"", default=""dual-encoder"", choices=[""dual-encoder"", ""encoder-decoder""])\n    parser.add_argument(""--lr"", type=float, default=4.0e-4, help=""Learning rate"")\n    parser.add_argument(""--clip"", type=float, default=0.25, help=""Clipping gradient norm"")\n    parser.add_argument(""--weight_decay"", type=float, default=0.0, help=""Weight decay"")\n    parser.add_argument(""--epochs"", type=int, default=20, help=""Num training epochs"")\n    parser.add_argument(""--restart_from"", type=str, help=""Option allows you to restart from a previous checkpoint"")\n    parser.add_argument(""--restart_tt"", type=str, help=""Optional param for legacy checkpoints (step|epoch)"")\n    parser.add_argument(""--warmup_steps"", type=int, default=10000, help=""Num warmup steps"")\n    parser.add_argument(""--loss"", type=str, default=\'all\', choices=[\'triplet\', \'all\'])\n    parser.add_argument(""--saves_per_epoch"", type=int, default=100, help=""The number of checkpoints to save per epoch"")\n    parser.add_argument(\'--rpr_k\', help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[8], nargs=\'+\')\n\n    parser.add_argument(""--device"", type=str,\n                        default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                        help=""Device (cuda or cpu)"")\n    parser.add_argument(""--distributed"",\n                        type=str2bool,\n                        default=False,\n                        help=""Are we doing distributed training?"")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""Local rank for distributed training (-1 means use the environment variables to find)"")\n\n    args = parser.parse_args()\n\n    if args.train_file and not args.valid_file:\n        logger.error(""If you provide a train_file, you must provide a valid_file"")\n        return\n\n    if not args.train_file and args.valid_file:\n        logger.error(""If you provide a valid_file, you must also provide a train_file"")\n        return\n\n    if args.basedir is None:\n        args.basedir = \'{}-{}-paired-{}-bpe-{}\'.format(args.model_type, args.reader_type, args.dataset_key, os.getpid())\n    logging.basicConfig(\n        format=""%(name)s: %(levelname)s: %(message)s"",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN\n    )\n    num_gpus = get_num_gpus_multiworker()\n    args.distributed = args.distributed or num_gpus > 1\n    logger.info(f""Using {num_gpus} GPUs in this job."")\n\n    if len(args.rpr_k) == 0 or args.rpr_k[0] < 1:\n        rpr_k = None\n    elif len(args.rpr_k) == 1:\n        rpr_k = args.rpr_k[0]\n    else:\n        rpr_k = args.rpr_k\n\n    if args.distributed:\n        args.device = init_distributed(args.local_rank)\n\n    reader = MultiFileDatasetReader(args.nctx, args.subword_model_file, args.subword_vocab_file, args.pattern, args.reader_type)\n    vocab = reader.build_vocab()\n\n    # If we are not using chars, then use \'x\' for both input and output\n    preproc_data = baseline.embeddings.load_embeddings(\'x\', dsz=args.d_model, known_vocab=vocab[\'x\'], embed_type=args.embed_type)\n    vocabs = preproc_data[\'vocab\']\n    os.makedirs(args.basedir, exist_ok=True)\n    # We want to make sure to save our input vocab into the basedir for reuse later\n    write_json(vocabs, os.path.join(args.basedir, \'vocabs.json\'))\n    embeddings = preproc_data[\'embeddings\']\n    logger.info(""Loaded embeddings"")\n\n    train_set = reader.load(args.train_file, vocabs)\n    valid_set = reader.load(args.valid_file, vocabs)\n\n    train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=args.num_train_workers)\n    valid_loader = DataLoader(valid_set, batch_size=args.batch_size, num_workers=args.num_valid_workers)\n    logger.info(""Loaded datasets"")\n\n    model = create_model(embeddings, d_model=args.d_model, d_ff=args.d_ff, dropout=args.dropout,\n                         num_heads=args.num_heads, num_layers=args.num_layers,\n                         model_type=args.model_type, rpr_k=rpr_k, d_k=args.d_k, reduction_d_k=args.reduction_d_k,\n                         stacking_layers=args.stacking_layers, ff_pdrop=args.ff_pdrop, logger=logger)\n    model.to(args.device)\n    loss_function = model.create_loss(args.loss)\n    loss_function.to(args.device)\n\n    logger.info(""Loaded model and loss"")\n\n    # according to pytorch, len(train_loader) will return len(train_set) when train_set is IterableDataset, so manually\n    # correct it here\n    steps_per_epoch = len(train_loader) // (args.batch_size*num_gpus)\n    update_on = steps_per_epoch // args.saves_per_epoch\n    report_on = update_on // 10\n    logger.info(f""Steps per epoch per GPU: {steps_per_epoch}. Saving checkpoint every {update_on} steps."")\n    lr_decay = get_lr_decay(args.lr_scheduler, args.lr, steps_per_epoch, args.epochs, logger,\n                            decay_steps=args.lr_decay_steps, decay_rate=args.lr_decay_rate, alpha=args.lr_alpha)\n    linear_warmup = WarmupLinearSchedulerPyTorch(args.warmup_steps, lr=args.lr)\n    lr_sched = CompositeLRScheduler(linear_warmup, lr_decay, lr=args.lr)\n\n    global_step = 0\n    start_epoch = 0\n    if args.restart_from:\n        model.load_state_dict(torch.load(args.restart_from))\n        vec = args.restart_from.split(""-"")\n\n        if args.restart_tt:\n            tick_type = args.restart_tt\n        else:\n            tick_type = vec[-2]\n        step_num = int(vec[-1].split(""."")[0])\n        if tick_type == \'epoch\':\n            start_epoch = step_num\n            global_step = start_epoch * steps_per_epoch\n\n        else:\n            start_epoch = step_num // steps_per_epoch\n            global_step = step_num\n\n        logger.info(""Restarting from a previous checkpoint %s.\\n\\tStarting at global_step=%d, epoch=%d"",\n                    args.restart_from, global_step, start_epoch+1)\n    optimizer = OptimizerManager(model, global_step, optim=args.optim, lr=args.lr, lr_function=lr_sched, weight_decay=args.weight_decay)\n    logger.info(""Model has {:,} parameters"".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n\n    # Prepare model for distributed training if needed\n    if args.distributed:\n        model = DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n        logger.info(""Model located on %d"", args.local_rank)\n\n    model_base = os.path.join(args.basedir, \'checkpoint\')\n\n    # This is the training loop\n    steps = global_step\n    for epoch in range(start_epoch, args.epochs):\n        avg_loss = Average(\'average_train_loss\')\n        metrics = {}\n        optimizer.zero_grad()\n        start = time.time()\n        model.train()\n        for i, batch in enumerate(train_loader):\n            steps += 1\n            x, y = batch\n            inputs = x.to(args.device)\n            labels = y.to(args.device)\n            loss = loss_function(inputs, labels)\n            loss.backward()\n            avg_loss.update(loss.item())\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n            optimizer.step()\n            optimizer.zero_grad()\n            if (i + 1) % report_on == 0:\n                logging.info(avg_loss)\n            if (i + 1) % update_on == 0 and args.local_rank < 1:\n                elapsed = (time.time() - start)/60\n                logging.info(\'elapsed time this epoch %d min\', elapsed)\n                logging.info(\'elapsed step time %f steps/min\', i/elapsed)\n                save_checkpoint(model, model_base, steps, tick_type=\'step\')\n\n        # How much time elapsed in minutes\n        elapsed = (time.time() - start)/60\n        train_avg_loss = avg_loss.avg\n        # This is the average training token-level loss across all machines\n        # This is the token-level training perplexity\n        metrics[\'train_elapsed_min\'] = elapsed\n        metrics[\'average_train_loss\'] = train_avg_loss\n        avg_valid_loss = Average(\'average_valid_loss\')\n        start = time.time()\n        model.eval()\n        for batch in valid_loader:\n            with torch.no_grad():\n                x, y = batch\n                inputs = x.to(args.device)\n                labels = y.to(args.device)\n                loss = loss_function(inputs, labels)\n                avg_valid_loss.update(loss.item())\n\n        valid_avg_loss = avg_valid_loss.avg\n\n        elapsed = (time.time() - start)/60\n        metrics[\'valid_elapsed_min\'] = elapsed\n\n        metrics[\'average_valid_loss\'] = valid_avg_loss\n        logger.info(metrics)\n        if args.local_rank < 1:\n            save_checkpoint(model, model_base, epoch, tick_type=\'epoch\')\n\n\nif __name__ == ""__main__"":\n    train()\n'"
api-examples/pretrain-tlm-tf.py,0,"b'import logging\nimport time\nimport os\nfrom argparse import ArgumentParser\nimport math\nfrom typing import Tuple\nimport baseline\nfrom eight_mile.utils import str2bool, write_json\nimport baseline.tf.embeddings\nimport baseline.embeddings\nfrom baseline.vectorizers import BPEVectorizer1D\nfrom eight_mile.utils import Average, get_num_gpus_multiworker, read_yaml\nfrom eight_mile.optz import *\nfrom eight_mile.tf.optz import *\nfrom baseline.tf.lm import SET_TRAIN_FLAG, TransformerLanguageModel, TransformerMaskedLanguageModel\nfrom eight_mile.tf.serialize import save_tlm_npz\nimport tensorflow as tf\nimport json\nlogger = logging.getLogger(__file__)\n\n\n""""""Pre-train a Transformer model in TensorFlow\n\nRead in preprocessed contexts generated by `preproc-tlm` and use a Transformer Language Model (TLM)\nto train them across multiple workers.\n\nSample preprocessed data can be found here: https://www.dropbox.com/s/jir4layu6nzjtqy/wt2.tar.gz\n\n""""""\n\n\nclass Loss:\n    def __init__(self, vocab_size, nctx):\n        self.vocab_size = vocab_size\n        self.nctx = nctx\n\n    def __call__(self, model, features, labels):\n        logits, _ = model(features, None)\n        loss_mask = tf.cast(labels != 0, tf.float32)\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n        losses = losses * loss_mask\n        losses = tf.reduce_sum(losses)\n        non_zero = tf.reduce_sum(loss_mask)\n        losses /= non_zero\n        return losses\n\n\ndef _parse_json(example):\n    j = json.loads(example.numpy())\n    return tf.constant(j[\'x\'], dtype=tf.int32), tf.constant(j[\'y\'], dtype=tf.int32)\n\nfeature_description = {\n    \'x\': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=0),\n    \'y\': tf.io.FixedLenSequenceFeature([], tf.int64, allow_missing=True, default_value=0),\n}\n\n\ndef _parse_tf_record(example_proto):\n    record = tf.io.parse_single_example(example_proto, feature_description)\n    return record[\'x\'], record[\'y\']\n\n\ndef decode_json(example):\n    return tf.py_function(_parse_json, [example], [tf.int32, tf.int32])\n\n\ndef get_dataset(directory, file_type, num_parallel_reads=1):\n    """"""Get a dataset as a tf.data.Dataset.  Input can be a bucket or a local file\n\n\n    :param directory: Either a bucket or a file\n    :param file_type: Currently supports ""json"" files or ""tfrecords""\n    :param num_parallel_reads: The number of parallel reads\n    :return: a `tf.data.Dataset`\n    """"""\n    pattern = os.path.join(directory, f\'*.{file_type}\')\n    files = tf.io.gfile.glob(pattern)\n    logger.debug(files)\n\n    if file_type == \'json\':\n        ds = tf.data.TextLineDataset(files, num_parallel_reads=num_parallel_reads)\n        return ds.map(decode_json)\n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=num_parallel_reads).map(_parse_tf_record)\n    return ds\n\n\ndef get_num_samples(sample_md):\n    yml = read_yaml(sample_md)\n    return yml[\'num_samples\']\n\n\ndef create_distribute_strategy(strategy_name, endpoint=None):\n    if strategy_name == \'tpu\':\n        if endpoint is None:\n            endpoint = \'grpc://\' + os.environ[\'COLAB_TPU_ADDR\']\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=endpoint)\n        tf.config.experimental_connect_to_cluster(resolver)\n        # This is the TPU initialization code that has to be at the beginning.\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        logger.info(""All devices: "", tf.config.list_logical_devices(\'TPU\'))\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    elif strategy_name == ""nccl"":\n        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\n            communication=tf.distribute.experimental.CollectiveCommunication.NCCL)\n    elif strategy_name == \'mirror\':\n        num_gpus = get_num_gpus_multiworker()\n        devices = [\'/device:GPU:{}\'.format(i) for i in range(num_gpus)]\n        strategy = tf.distribute.MirroredStrategy(devices)\n    else:\n        raise Exception(f""Unsupported strategy {strategy_name}"")\n    return strategy\n\n\ndef train():\n    parser = ArgumentParser()\n    parser.add_argument(""--basedir"", type=str)\n    parser.add_argument(""--train_dir"", type=str, required=True, help=\'Training directory\')\n    parser.add_argument(""--valid_dir"", type=str, required=True, help=\'Validation directory\')\n    parser.add_argument(""--train_md"", type=str, help=""Training metadata YAML, defaults to `{train_dir}/md.yml`"")\n    parser.add_argument(""--valid_md"", type=str, help=""Validation metadata YAML, defaults to `{valid_dir}/md.yml`"")\n    parser.add_argument(""--dataset_key"", default=""tlm"",\n                        help=""dataset key for basedir"")\n    parser.add_argument(""--embed_type"", type=str, default=\'default\',\n                        choices=[""default"", ""positional"", ""learned-positional""],\n                        help=""register label of the embeddings"")\n    parser.add_argument(""--d_model"", type=int, default=410, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--d_ff"", type=int, default=2100, help=""FFN dimension"")\n    parser.add_argument(""--d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--num_heads"", type=int, default=1, help=""Number of heads"")\n    parser.add_argument(""--num_layers"", type=int, default=6, help=""Number of layers"")\n    parser.add_argument(""--num_train_workers"", type=int, default=4, help=""Number train workers"")\n    parser.add_argument(""--num_valid_workers"", type=int, default=2, help=""Number valid workers"")\n    parser.add_argument(""--distribute"", type=str, default=""mirror"", choices=[""mirror"", ""tpu"", ""nccl""])\n    parser.add_argument(""--tpu_ep"", type=str, help=""The TPU endpoint if using `distribute=tpu`"")\n    parser.add_argument(""--nctx"", type=int, default=128, help=""Max input length"")\n    parser.add_argument(""--file_type"", default=\'tfrecord\', choices=[\'json\', \'tfrecord\'], help=""Glob pattern for data"")\n    parser.add_argument(""--batch_size"", type=int, default=8, help=""Batch Size"")\n    parser.add_argument(""--subword_model_file"", type=str, help=""The BPE model file"", required=True)\n    parser.add_argument(""--subword_vocab_file"", type=str, help=""The BPE subword vocab"", required=True)\n    parser.add_argument(""--dropout"", type=float, default=0.1, help=""Dropout"")\n    parser.add_argument(""--optim"", default=""adam"", type=str, help=""Optimizer to use (defaults to adam)"")\n    parser.add_argument(""--lr"", type=float, default=4.0e-4, help=""Learning rate"")\n    parser.add_argument(""--clip"", type=float, default=1, help=""Clipping gradient norm"")\n    parser.add_argument(""--weight_decay"", type=float, default=0.0, help=""Weight decay"")\n    parser.add_argument(""--epochs"", type=int, default=20, help=""Num training epochs"")\n    parser.add_argument(""--restart_from"", type=str, help=""Option allows you to restart from a previous checkpoint"")\n    parser.add_argument(""--restart_tt"", type=str, help=""Optional param for legacy checkpoints (step|epoch)"")\n    parser.add_argument(""--warmup_steps"", type=int, default=10000, help=""Num warmup steps"")\n    parser.add_argument(""--lr_scheduler"", type=str, help=""The type of learning rate decay scheduler"", default=\'cosine\')\n    parser.add_argument(""--lr_decay_steps"", type=int, help=""decay steps of lr scheduler"")\n    parser.add_argument(""--lr_decay_rate"", type=float, help=""decay rate of lr scheduler"")\n    parser.add_argument(""--lr_alpha"", type=float, help=""parameter alpha for cosine decay scheduler"")\n    parser.add_argument(""--mlm"", type=str2bool, default=True, help=""Use Masked Language Model (MLM) objective"")\n    parser.add_argument(""--saves_per_epoch"", type=int, default=100, help=""The number of checkpoints to save per epoch"")\n    parser.add_argument(\'--rpr_k\',\n                        help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[8], nargs=\'+\')\n    parser.add_argument(""--strategy"", help=""Training strategy, defaults to `mirror`"", choices=[""mirror""])\n    parser.add_argument(""--npz"", help=""Should we write out NPZ files?"", type=str2bool, default=False)\n    args = parser.parse_args()\n    SET_TRAIN_FLAG(True)\n\n    if args.basedir is None:\n        args.basedir = f\'lm-{args.dataset_key}-bpe-{os.getpid()}\'\n    logging.basicConfig(level=logging.INFO)\n    logger.info(f""Writing results to {args.basedir}"")\n    strategy = create_distribute_strategy(args.distribute, args.tpu_ep)\n    num_replicas = strategy.num_replicas_in_sync\n    logger.info(f""Using {num_replicas} replicas in this job."")\n    global_step = 1\n    vectorizer = BPEVectorizer1D(model_file=args.subword_model_file, vocab_file=args.subword_vocab_file, mxlen=args.nctx)\n    vocab = {\'x\': vectorizer.vocab}\n    preproc_data = baseline.embeddings.load_embeddings(\'x\', dsz=args.d_model, known_vocab=vocab[\'x\'],\n                                                       preserve_vocab_indices=True,\n                                                       embed_type=args.embed_type)\n    vocabs = preproc_data[\'vocab\']\n    vocab_size = max(vocabs.values())\n\n    def dataset_train_fn(input_context):\n        batch_size = input_context.get_per_replica_batch_size(args.batch_size)\n        ds = get_dataset(args.train_dir, args.file_type, args.num_train_workers).batch(batch_size)\n        return ds.shard(\n            input_context.num_input_pipelines, input_context.input_pipeline_id\n        )\n    train_loader = strategy.experimental_distribute_datasets_from_function(dataset_train_fn)\n\n    def dataset_test_fn(input_context):\n        batch_size = input_context.get_per_replica_batch_size(args.batch_size)\n        ds = get_dataset(args.valid_dir, args.file_type, args.num_train_workers).batch(batch_size)\n        return ds.shard(\n            input_context.num_input_pipelines, input_context.input_pipeline_id\n        )\n    valid_loader = strategy.experimental_distribute_datasets_from_function(dataset_test_fn)\n\n    train_md = args.train_md if args.train_md else os.path.join(args.train_dir, \'md.yml\')\n    num_train_samples = get_num_samples(train_md)\n    valid_md = args.valid_md if args.valid_md else os.path.join(args.valid_dir, \'md.yml\')\n    num_valid_samples = get_num_samples(valid_md)\n    os.makedirs(args.basedir, exist_ok=True)\n    # We want to make sure to save our input vocab into the basedir for reuse later\n    write_json(vocabs, os.path.join(args.basedir, \'vocabs.json\'))\n    embeddings = {\'x\': preproc_data[\'embeddings\']}\n    logger.info(""Loaded embeddings"")\n\n    logger.info(""Loaded datasets"")\n    logger.info(""Using embedding type [%s]"", args.embed_type)\n    if len(args.rpr_k) == 0 or args.rpr_k[0] < 1:\n        rpr_k = None\n    elif len(args.rpr_k) == 1:\n        rpr_k = args.rpr_k[0]\n    else:\n        rpr_k = args.rpr_k\n\n    TLM = TransformerMaskedLanguageModel if args.mlm else TransformerLanguageModel\n    model = TLM.create(embeddings,\n                       hsz=args.d_model,\n                       d_ff=args.d_ff,\n                       tie_weights=True,\n                       dropout=args.dropout,\n                       gpu=False,\n                       num_heads=args.num_heads,\n                       layers=args.num_layers,\n                       rpr_k=rpr_k,\n                       d_k=args.d_k,\n                       src_keys=[\'x\'], tgt_key=\'x\')\n\n    loss_function = Loss(vocab_size, args.nctx)\n\n    logger.info(""Loaded model and loss"")\n    steps_per_epoch = num_train_samples // args.batch_size\n    steps_per_valid_epoch = num_valid_samples // args.batch_size\n    update_on = steps_per_epoch // args.saves_per_epoch\n    report_on = max(10, update_on) // 10\n    logger.info(f""Steps per epoch: {steps_per_epoch}. Saving checkpoint every {update_on} steps."")\n\n    lr_decay = CosineDecaySchedulerTensorFlow(steps_per_epoch)\n    linear_warmup = WarmupLinearSchedulerTensorFlow(args.warmup_steps, lr=args.lr)\n    lr_sched = CompositeLRSchedulerTensorFlow(linear_warmup, lr_decay, lr=args.lr)\n    optimizer = EagerOptimizer(loss_function, global_step=global_step, lr=args.lr, optim=args.optim,\n                               learning_rate_decay_fn=lr_sched, weight_decay=args.weight_decay, clip=args.clip)\n    checkpoint = tf.train.Checkpoint(optimizer=optimizer.optimizer, model=model)\n    checkpoint_manager = tf.train.CheckpointManager(checkpoint,\n                                                    directory=args.basedir,\n                                                    max_to_keep=5)\n\n    if args.restart_from:\n        checkpoint.restore(checkpoint_manager.latest_checkpoint)\n\n    def _replicated_train_step(inputs):\n        """"""This runs on a single replica""""""\n        x, y = inputs\n        per_replica_loss = optimizer.update(model, {\'x\': x}, y, num_replicas)\n        return per_replica_loss\n\n    @tf.function\n    def _distributed_train_step(inputs: Tuple[tf.Tensor, tf.Tensor]):\n        """"""Runs across multiple replicas and aggregates the results.\n\n        :param inputs:\n        :return:\n        """"""\n        per_replica_loss = strategy.experimental_run_v2(_replicated_train_step, args=(inputs,))\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n\n    valid_loss_function = Loss(vocab_size, args.nctx)\n\n    def _replicated_test_step(inputs):\n        """"""This runs on a single replica""""""\n        x, y = inputs\n        per_replica_loss = valid_loss_function(model, {\'x\': x}, y) / num_replicas\n        return per_replica_loss\n\n    @tf.function\n    def _distributed_test_step(inputs: Tuple[tf.Tensor, tf.Tensor]):\n        """"""Runs across multiple replicas and aggregates the results.\n\n        :param inputs:\n        :return:\n        """"""\n        per_replica_loss = strategy.experimental_run_v2(_replicated_test_step, args=(inputs,))\n        return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n\n    # This is the training loop\n    steps = global_step\n    start_epoch = 0\n\n    with strategy.scope():\n\n        SET_TRAIN_FLAG(True)\n        for epoch in range(start_epoch, args.epochs):\n            avg_loss = Average(\'average_train_loss\')\n            metrics = {}\n            start = time.time()\n            train_iter = iter(train_loader)\n            for i in range(steps_per_epoch):\n                steps += 1\n                loss = _distributed_train_step(next(train_iter))\n                avg_loss.update(loss.numpy().item())\n                if (i + 1) % report_on == 0:\n                    logging.info(avg_loss)\n                if (i + 1) % update_on == 0:\n                    elapsed = (time.time() - start)/60\n                    logging.info(\'elapsed time this epoch %d min\', elapsed)\n                    logging.info(\'elapsed step time %f steps/min\', i/elapsed)\n                    checkpoint_manager.save()\n                    if args.npz:\n                        npz_checkpoint = os.path.join(args.basedir, f\'checkpoint-step-{steps}.npz\')\n                        save_tlm_npz(model, npz_checkpoint)\n\n            # How much time elapsed in minutes\n            elapsed = (time.time() - start)/60\n            train_token_loss = avg_loss.avg\n            # This is the average training token-level loss across all machines\n            # This is the token-level training perplexity\n            train_token_ppl = math.exp(train_token_loss)\n            metrics[\'train_elapsed_min\'] = elapsed\n            metrics[\'average_train_loss\'] = train_token_loss\n            metrics[\'train_ppl\'] = train_token_ppl\n            avg_valid_loss = Average(\'average_valid_loss\')\n            start = time.time()\n            SET_TRAIN_FLAG(False)\n            valid_iter = iter(valid_loader)\n            for i in range(steps_per_valid_epoch):\n                valid_loss = _distributed_test_step(next(valid_iter))\n                avg_valid_loss.update(valid_loss.numpy().item())\n\n            valid_token_loss = avg_valid_loss.avg\n            valid_token_ppl = math.exp(valid_token_loss)\n\n            elapsed = (time.time() - start)/60\n            metrics[\'valid_elapsed_min\'] = elapsed\n            metrics[\'average_valid_loss\'] = valid_token_loss\n            metrics[\'average_valid_word_ppl\'] = valid_token_ppl\n            logger.info(json.dumps(metrics, indent=4))\n\n\nif __name__ == ""__main__"":\n    train()\n\n'"
api-examples/pretrain-tlm.py,11,"b'import logging\nimport time\nimport os\nfrom argparse import ArgumentParser\nimport tempfile\nimport baseline\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DataLoader\nfrom eight_mile.utils import str2bool, write_json, Average, get_num_gpus_multiworker\nimport baseline.pytorch.embeddings\nimport baseline.embeddings\nfrom eight_mile.optz import *\nfrom eight_mile.pytorch.layers import save_checkpoint, init_distributed\nfrom eight_mile.pytorch.optz import *\nfrom baseline.pytorch.lm import TransformerLanguageModel, TransformerMaskedLanguageModel\nfrom transformer_utils import MultiFileDatasetReader, on_demand_mlm_masking, get_lr_decay\nfrom eight_mile.pytorch.serialize import load_tlm_npz\n\nlogger = logging.getLogger(__file__)\n\n\n""""""Pre-train a Transformer model in PyTorch\n\nThe datasets in this program are read in as an `IterableDataset`, typically one line per sample, which\nmakes it efficient to process even very large datasets that may not fit in core memory.  The datasets are\nassumed to be sharded over a set of files for training and validation.\n\nThe `preproc-tlm` script can be used upfront to generate pre-processed representations which allows the reader\nto simple ingest the sample without any on demand vectorization or masking.  This approach should be preferred\nwhere available.  To run the model in this manner, first run `preproc-tlm`, generating keys `x` and `y` containing\nthe numeric one-hot values for each token, and then in this script, pass `--preprocessed true`.\n\nIf the model is an MLM and the `preprocessed` value is false, on-demand MLM masking is performed.\n\n""""""\n\ndef train():\n    parser = ArgumentParser()\n    parser.add_argument(""--basedir"", type=str)\n    parser.add_argument(""--train_file"", type=str, required=True, help=\'File path to use for train file\')\n    parser.add_argument(""--valid_file"", type=str, required=True, help=\'File path to use for valid file\')\n    parser.add_argument(""--dataset_key"", default=""reddit"",\n                        help=""dataset key for basedir"")\n    parser.add_argument(""--embed_type"", type=str, default=\'default\',\n                        choices=[""default"", ""positional"", ""learned-positional""],\n                        help=""register label of the embeddings"")\n    parser.add_argument(""--d_model"", type=int, default=410, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--d_ff"", type=int, default=2100, help=""FFN dimension"")\n    parser.add_argument(""--d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--num_heads"", type=int, default=1, help=""Number of heads"")\n    parser.add_argument(""--num_layers"", type=int, default=6, help=""Number of layers"")\n    parser.add_argument(""--num_train_workers"", type=int, default=4, help=""Number train workers"")\n    parser.add_argument(""--nctx"", type=int, default=128, help=""Max input length"")\n    parser.add_argument(""--pattern"", default=\'*.txt\', help=""Glob pattern for data"")\n    parser.add_argument(""--batch_size"", type=int, default=8, help=""Batch Size"")\n    parser.add_argument(""--subword_model_file"", type=str, help=""The BPE model file"", required=True)\n    parser.add_argument(""--subword_vocab_file"", type=str, help=""The BPE subword vocab"", required=True)\n    parser.add_argument(""--dropout"", type=float, default=0.1, help=""Dropout"")\n    parser.add_argument(""--lr_scheduler"", type=str, default=\'cosine\', help=""The type of learning rate decay scheduler"")\n    parser.add_argument(""--lr_decay_steps"", type=int, help=""decay steps of lr scheduler"")\n    parser.add_argument(""--lr_decay_rate"", type=float, help=""decay rate of lr scheduler"")\n    parser.add_argument(""--lr_alpha"", type=float, help=""parameter alpha for cosine decay scheduler"")\n    parser.add_argument(""--optim"", default=""adam"", type=str, help=""Optimizer to use (defaults to adam)"")\n    parser.add_argument(""--lr"", type=float, default=4.0e-4, help=""Learning rate"")\n    parser.add_argument(""--clip"", type=float, default=1.0, help=""Clipping gradient norm"")\n    parser.add_argument(""--weight_decay"", type=float, default=0.0, help=""Weight decay"")\n    parser.add_argument(""--epochs"", type=int, default=20, help=""Num training epochs"")\n    parser.add_argument(""--restart_from"", type=str, help=""Option allows you to restart from a previous checkpoint"")\n    parser.add_argument(""--restart_tt"", type=str, help=""Optional param for legacy checkpoints (step|epoch)"")\n    parser.add_argument(""--warmup_steps"", type=int, default=10000, help=""Num warmup steps"")\n    parser.add_argument(""--saves_per_epoch"", type=int, default=100, help=""The number of checkpoints to save per epoch"")\n    parser.add_argument(""--mlm"", type=str2bool, default=True, help=""Use Masked Language Model (MLM) objective"")\n    parser.add_argument(""--preprocessed"", type=str2bool, default=True, help=""Has the data already been preprocessed?"")\n    parser.add_argument(\'--rpr_k\',\n                        help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[8], nargs=\'+\')\n    parser.add_argument(""--device"", type=str,\n                        default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                        help=""Device (cuda or cpu)"")\n    parser.add_argument(""--distributed"",\n                        type=str2bool,\n                        default=False,\n                        help=""Are we doing distributed training?"")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""Local rank for distributed training (-1 means use the environment variables to find)"")\n\n    args = parser.parse_args()\n\n    if args.basedir is None:\n        args.basedir = \'lm-{}-bpe-{}\'.format(args.dataset_key, os.getpid())\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n\n    num_gpus = get_num_gpus_multiworker()\n    args.distributed = args.distributed or num_gpus > 1\n    logger.info(f""Using {num_gpus} GPUs in this job."")\n\n    do_on_demand_masking = args.mlm and not args.preprocessed\n    if do_on_demand_masking:\n        logger.info(f""On-demand masking is turned on"")\n    if args.distributed:\n        args.device, updated_local_rank = init_distributed(args.local_rank)\n        args.local_rank = updated_local_rank\n\n    reader_type = ""lang"" if not args.preprocessed else ""preprocessed""\n    reader = MultiFileDatasetReader(args.nctx, args.subword_model_file, args.subword_vocab_file, args.pattern,\n                                    reader_type=reader_type)\n\n    # This looks a bit funny but the streaming reader ignores our vocab and gives us the one from the subword_model\n    # However, we do need to get counts from our dataset for validation so we can calculate the perplexity\n    vocab = reader.build_vocab([args.valid_file])\n    # If we are not using chars, then use \'x\' for both input and output\n    preproc_data = baseline.embeddings.load_embeddings(\'x\', dsz=args.d_model, known_vocab=vocab[\'x\'],\n                                                       preserve_vocab_indices=True,\n                                                       embed_type=args.embed_type)\n    vocabs = preproc_data[\'vocab\']\n\n    os.makedirs(args.basedir, exist_ok=True)\n    # We want to make sure to save our input vocab into the basedir for reuse later\n    write_json(vocabs, os.path.join(args.basedir, \'vocabs.json\'))\n    embeddings = {\'x\': preproc_data[\'embeddings\']}\n    logger.info(""Loaded embeddings"")\n\n    train_set = reader.load(args.train_file, vocabs)\n    valid_set = reader.load(args.valid_file, vocabs, distribute=False, shuffle=False)\n\n    train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=args.num_train_workers)\n    valid_loader = DataLoader(valid_set, batch_size=args.batch_size)\n    logger.info(""Loaded datasets"")\n    logger.info(""Using embedding type [%s]"", args.embed_type)\n\n    if args.mlm:\n        mask_from = vocabs\n        vocab_size = len(mask_from)\n        mask_value = mask_from.get(""[MASK]"")\n        if mask_value == -1:\n            logger.error(""We could not find a suitable masking token in the vocab"")\n            return\n\n    if len(args.rpr_k) == 0 or args.rpr_k[0] < 1:\n        rpr_k = None\n    elif len(args.rpr_k) == 1:\n        rpr_k = args.rpr_k[0]\n    else:\n        rpr_k = args.rpr_k\n\n    TLM = TransformerMaskedLanguageModel if args.mlm else TransformerLanguageModel\n    model = TLM.create(embeddings,\n                       hsz=args.d_model,\n                       d_ff=args.d_ff,\n                       tie_weights=True,\n                       dropout=args.dropout,\n                       gpu=False,\n                       num_heads=args.num_heads,\n                       layers=args.num_layers,\n                       rpr_k=rpr_k,\n                       d_k=args.d_k,\n                       src_keys=[\'x\'], tgt_key=\'x\')\n    model.to(args.device)\n    loss_function = model.create_loss()\n    loss_function.to(args.device)\n\n    logger.info(""Loaded model and loss"")\n\n    # according to pytorch, len(train_loader) will return len(train_set) when train_set is IterableDataset, so manually\n    # correct it here\n    steps_per_epoch = len(train_loader) // (args.batch_size*num_gpus)\n    update_on = steps_per_epoch // args.saves_per_epoch\n    report_on = max(10, update_on) // 10\n    logger.info(f""Steps per epoch per GPU: {steps_per_epoch}. Saving checkpoint every {update_on} steps."")\n    lr_decay = get_lr_decay(args.lr_scheduler, args.lr, steps_per_epoch, args.epochs, logger,\n                            decay_steps=args.lr_decay_steps, decay_rate=args.lr_decay_rate, alpha=args.lr_alpha)\n    linear_warmup = WarmupLinearSchedulerPyTorch(args.warmup_steps, lr=args.lr)\n    lr_sched = CompositeLRScheduler(linear_warmup, lr_decay, lr=args.lr)\n\n    global_step = 0\n    start_epoch = 0\n    if args.restart_from:\n\n        if args.restart_from.endswith(\'npz\'):\n            load_tlm_npz(model, args.restart_from)\n        else:\n            model.load_state_dict(torch.load(args.restart_from))\n        vec = args.restart_from.split(""-"")\n\n        if args.restart_tt:\n            tick_type = args.restart_tt\n        else:\n            tick_type = vec[-2]\n        step_num = int(vec[-1].split(""."")[0])\n        if tick_type == \'epoch\':\n            start_epoch = step_num\n            global_step = start_epoch * steps_per_epoch\n\n        else:\n            start_epoch = step_num // steps_per_epoch\n            global_step = step_num\n\n        logger.info(""Restarting from a previous checkpoint %s.\\n\\tStarting at global_step=%d, epoch=%d"",\n                    args.restart_from, global_step, start_epoch+1)\n\n    optimizer = OptimizerManager(model, global_step, optim=args.optim, lr=args.lr, lr_function=lr_sched, weight_decay=args.weight_decay)\n    logger.info(""Model has {:,} parameters"".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n\n    # Prepare model for distributed training if needed\n    if args.distributed:\n        # This program assume pure data parallelism, each model is on a single gpu\n        # If we wanted to support model and data parallelism we would need to update\n        # the selection of gpus based on rank, it would need to select multiple ids\n        # based on rank, here we select only a single gpu and use it for input and\n        # output.\n        model = DistributedDataParallel(model, device_ids=[args.device], output_device=args.device)\n        logger.info(""Model located on %s"", args.device)\n\n    # This is the training loop\n    steps = global_step\n    model_base = os.path.join(args.basedir, \'checkpoint\')\n\n    for epoch in range(start_epoch, args.epochs):\n        avg_loss = Average(\'average_train_loss\')\n        metrics = {}\n        optimizer.zero_grad()\n        start = time.time()\n        model.train()\n        for i, batch in enumerate(train_loader):\n            steps += 1\n            x, y = batch\n            inputs = x.to(args.device)\n            labels = y.to(args.device)\n            if do_on_demand_masking:\n                inputs, labels = on_demand_mlm_masking(inputs, labels, mask_value, vocab_size)\n            inputs = {\'x\': inputs}\n\n            labels = labels.transpose(0, 1).contiguous()\n            logits = model(inputs, None)[0].transpose(0, 1).contiguous()\n            if args.mlm:\n                loss = loss_function(logits, labels)\n            else:\n                shift_logits = logits[:-1]\n                shift_labels = labels[1:]\n                loss = loss_function(shift_logits, shift_labels)\n            loss.backward()\n            avg_loss.update(loss.item())\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n            optimizer.step()\n            optimizer.zero_grad()\n            if (i + 1) % report_on == 0:\n                logging.info(avg_loss)\n\n            if (i + 1) % update_on == 0 and args.local_rank < 1:\n                elapsed = (time.time() - start)/60\n                logging.info(\'elapsed time this epoch %d min\', elapsed)\n                logging.info(\'elapsed step time %f steps/min\', i/elapsed)\n                logging.info(\'LR: %f\',  optimizer.current_lr)\n                save_checkpoint(model, model_base, steps, tick_type=\'step\')\n\n        # How much time elapsed in minutes\n        elapsed = (time.time() - start)/60\n        train_token_loss = avg_loss.avg\n        # This is the average training token-level loss across all machines\n        # This is the token-level training perplexity\n        train_token_ppl = math.exp(train_token_loss)\n        metrics[\'train_elapsed_min\'] = elapsed\n        metrics[\'average_train_loss\'] = train_token_loss\n        metrics[\'train_ppl\'] = train_token_ppl\n        avg_valid_loss = Average(\'average_valid_loss\')\n        start = time.time()\n        model.eval()\n        for batch in valid_loader:\n            with torch.no_grad():\n                x, y = batch\n                inputs = x.to(args.device)\n                labels = y.to(args.device)\n\n                if do_on_demand_masking:\n                    inputs, labels = on_demand_mlm_masking(inputs, labels, mask_value, vocab_size)\n                inputs = {\'x\': inputs}\n                labels = labels.transpose(0, 1).contiguous()\n                logits = model(inputs, None)[0].transpose(0, 1).contiguous()\n                if args.mlm:\n                    loss = loss_function(logits, labels)\n                else:\n                    shift_logits = logits[:-1]\n                    shift_labels = labels[1:]\n                    loss = loss_function(shift_logits, shift_labels)\n                avg_valid_loss.update(loss.item())\n\n        valid_token_loss = avg_valid_loss.avg\n        valid_token_ppl = math.exp(valid_token_loss)\n\n        elapsed = (time.time() - start)/60\n        metrics[\'valid_elapsed_min\'] = elapsed\n        metrics[\'average_valid_loss\'] = valid_token_loss\n        metrics[\'average_valid_word_ppl\'] = valid_token_ppl\n        logger.info(metrics)\n\n        if args.local_rank < 1:\n            save_checkpoint(model, model_base, epoch, save_npz=True)\n\n\nif __name__ == ""__main__"":\n    train()\n\n'"
api-examples/pretrain-transformer-lm.py,14,"b'import logging\nimport time\nimport os\nfrom argparse import ArgumentParser\nimport tempfile\nimport baseline\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformer_utils import on_demand_mlm_masking\nfrom eight_mile.utils import str2bool, write_json, Offsets\nimport baseline.pytorch.embeddings\nimport baseline.embeddings\nfrom eight_mile.optz import *\nfrom eight_mile.utils import Average, get_num_gpus_multiworker\nfrom eight_mile.pytorch.layers import init_distributed, save_checkpoint\nfrom eight_mile.pytorch.optz import *\nfrom baseline.pytorch.lm import TransformerLanguageModel, TransformerMaskedLanguageModel\nfrom baseline.utils import DataDownloader\nfrom transformer_utils import TensorWordDatasetReader, TensorCharDatasetReader, load_data_caching, get_lr_decay\nimport numpy as np\nimport codecs\nfrom collections import Counter\n\nlogger = logging.getLogger(__file__)\n\n""""""Pre-train a Transformer model in PyTorch\n\nNOTE: Deprecated! This script reads the entire dataset into memory, which may be problematic for large datasets.  For\ntypical use-cases (BPE single-token, MLM loss models), use pretrain-tlm instead, using the `preproc-tlm`\nscript to generate fixed contexts upfront.  This is more efficient as it allows us to read and collate full\nrows of data without having the dataset in-core, and also because the masking is done upfront.\n\nThis file uses Baseline to train a Transformer with PyTorch on multiple GPUs.\nIt is inspired by: https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_train.py\nThis pretraining module has multiple configurations that allow it to support\n\n  * 3 types of pre-training tokenization\n    - word\n    - subword (based on BERT tokenizer)\n    - ELMo (Kim et al 2015) char method\n  * pretraining on several datasets including PTB, Wikitext 2 (including raw) and Wikitext 103 (including raw).\n\nIf you use `tokens=bpe`, it requires fastBPE.\nOtherwise, it depends only on six, numpy, pytorch, and baseline.\n\nBecause we are trying to pretrain a language model so we can do better on downstream tasks, it probably makes more\nsense to train on a full word model, not a model where rare words have already been replaced.\n\n""""""\nDATASETS = {\n    ""ptb"": {\n        ""train_file"": ""train.txt"",\n        ""valid_file"": ""valid.txt"",\n        ""test_file"": ""test.txt"",\n        ""download"": ""https://www.dropbox.com/s/5g8en2jc9951omu/ptb.tar.gz?dl=1"",\n        ""sha1"": ""56aacd9bd3aeffb34a9536e8de2341a8d6770f7b""\n    },\n    ""wikitext-2"": {\n        ""train_file"": ""train.txt"",\n        ""valid_file"": ""valid.txt"",\n        ""test_file"": ""test.txt"",\n        ""download"": ""https://www.dropbox.com/s/q4i2vxw1nkhsk8g/wikitext-2.tar.gz?dl=1""\n    },\n    ""wikitext-2-raw"": {\n        ""train_file"": ""wikitext-2-raw/wiki.train.raw"",\n        ""valid_file"": ""wikitext-2-raw/wiki.valid.raw"",\n        ""test_file"": ""wikitext-2-raw/wiki.test.raw"",\n        ""download"": ""https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip""\n    },\n    ""wikitext-103"": {\n        ""train_file"": ""wikitext-103/wiki.train.tokens"",\n        ""valid_file"": ""wikitext-103/wiki.valid.tokens"",\n        ""test_file"": ""wikitext-103/wiki.test.tokens"",\n        ""download"": ""https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip""\n    },\n    ""wikitext-103-raw"": {\n        ""train_file"": ""wikitext-103/wiki.train.raw"",\n        ""valid_file"": ""wikitext-103/wiki.valid.raw"",\n        ""test_file"": ""wikitext-103/wiki.test.raw"",\n        ""download"": ""https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip""\n    }\n}\n\nX_CHAR_EMBEDDINGS = {\n    ""dsz"": 16,\n    ""wsz"": 128,\n    ""keep_unused"": True,\n    ""cfiltsz"": [\n        [1, 32],\n        [2, 32],\n        [3, 64],\n        [4, 128],\n        [5, 256],\n        [6, 512],\n        [7, 1024]\n    ],\n    ""gating"": ""highway"",\n    ""num_gates"": 2,\n    ""projsz"": 512\n}\n\n\ndef create_reader(token_type, nctx, chars_per_word, subword_model_file, subword_vocab_file, subword_special_tokens):\n    if token_type == ""chars"":\n        logger.info(""Using character input"")\n        reader = TensorCharDatasetReader(nctx, chars_per_word)\n    elif token_type == ""words"":\n        logger.info(""Using word input"")\n        reader = TensorWordDatasetReader(nctx)\n    else:\n        logger.info(""Using subword ({}) input"".format(token_type))\n        reader = TensorWordDatasetReader(nctx, token_type, subword_model_file, subword_vocab_file,\n                                         subword_special_tokens)\n    return reader\n\n\ndef get_embed_and_vocab_cache(base_path, dataset_key, token_type, embed_type):\n    return os.path.join(base_path, \'preproc-{}-{}-{}.cache\'.format(dataset_key, token_type, embed_type))\n\n\ndef load_embed_and_vocab(token_type, reader, dataset, dataset_key, embed_type, d_model, caching):\n    base_path = os.path.dirname(dataset[\'train_file\'])\n    preproc_cache = get_embed_and_vocab_cache(base_path, dataset_key, token_type, embed_type)\n    if caching and os.path.exists(preproc_cache):\n        logger.info(""Loading cached preprocessing info [%s]"", preproc_cache)\n        preproc_data = torch.load(preproc_cache)\n        vectorizers_mxlen = preproc_data[\'vectorizers_mxlen\']\n        if token_type == \'chars\':\n            char_vectorizer = reader.vectorizers[\'x\']\n            tok_vectorizer = reader.vectorizers[\'y\']\n            char_vectorizer.max_seen_tok, char_vectorizer.max_seen_char = vectorizers_mxlen[\'x\']\n            tok_vectorizer.max_seen = vectorizers_mxlen[\'y\']\n        else:\n            reader.vectorizers[\'x\'].max_seen = vectorizers_mxlen[\'x\']\n    else:\n        vocab_sources = [dataset[\'train_file\'], dataset[\'valid_file\']]\n        vocabs = reader.build_vocab(vocab_sources)\n        valid_num_words = reader.num_words[dataset[\'valid_file\']]\n        vectorizers_mxlen = {}\n        if token_type == \'chars\':\n            vectorizers_mxlen[\'x\'] = (reader.vectorizers[\'x\'].max_seen_tok, reader.vectorizers[\'x\'].max_seen_char)\n            vectorizers_mxlen[\'y\'] = reader.vectorizers[\'y\'].max_seen\n        else:\n            vectorizers_mxlen[\'x\'] = reader.vectorizers[\'x\'].max_seen\n\n        logger.info(""Read vocabulary"")\n        embeddings = {}\n\n        # If we are not using chars, then use \'x\' for both input and output\n        tgt_key = \'x\'\n        if token_type == \'chars\':\n            # Write JSON file here and skip this step the second time\n            X_CHAR_EMBEDDINGS[\'embed_type\'] = embed_type\n            x_embedding = baseline.embeddings.load_embeddings(\'x\', known_vocab=vocabs[\'x\'], **X_CHAR_EMBEDDINGS)\n            vocabs[\'x\'] = x_embedding[\'vocab\']\n\n            y_embedding = baseline.embeddings.load_embeddings(\'y\', dsz=1, known_vocab=vocabs[\'y\'])\n            vocabs[\'y\'] = y_embedding[\'vocab\']\n\n            embeddings[\'x\'] = x_embedding[\'embeddings\']\n            embeddings[\'y\'] = y_embedding[\'embeddings\']\n            tgt_key = \'y\'\n        else:\n            x_embedding = baseline.embeddings.load_embeddings(\'x\',\n                                                              dsz=d_model,\n                                                              known_vocab=vocabs[\'x\'],\n                                                              embed_type=embed_type)\n            logger.info(""Using embedding type [%s]"", embed_type)\n            vocabs[\'x\'] = x_embedding[\'vocab\']\n            embeddings[\'x\'] = x_embedding[\'embeddings\']\n\n        preproc_data = {\'vocabs\': vocabs, \'embeddings\': embeddings, \'valid_num_words\': valid_num_words,\n                        \'tgt_key\': tgt_key, \'vectorizers_mxlen\': vectorizers_mxlen}\n        logger.info(""Saving preprocessing info [%s]"", preproc_cache)\n        torch.save(preproc_data, preproc_cache)\n    return preproc_data\n\n\ndef train():\n    parser = ArgumentParser()\n    parser.add_argument(""--basedir"", type=str)\n    parser.add_argument(""--dataset_key"", type=str, default=\'wikitext-2\', help=""key from DATASETS global"")\n    parser.add_argument(""--train_file"", type=str, help=\'Optional file path to use for train file\')\n    parser.add_argument(""--valid_file"", type=str, help=\'Optional file path to use for valid file\')\n    parser.add_argument(""--dataset_cache"", type=str, default=os.path.expanduser(\'~/.bl-data\'),\n                        help=""Path or url of the dataset cache"")\n    parser.add_argument(""--cache_features"", type=str2bool, default=True)\n    parser.add_argument(""--embed_type"", type=str, default=\'default\',\n                        help=""register label of the embeddings, so far support positional or learned-positional"")\n    parser.add_argument(""--d_model"", type=int, default=410, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--d_ff"", type=int, default=2100, help=""FFN dimension"")\n    parser.add_argument(""--d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--num_heads"", type=int, default=10, help=""Number of heads"")\n    parser.add_argument(""--num_layers"", type=int, default=16, help=""Number of layers"")\n    parser.add_argument(""--nctx"", type=int, default=256, help=""Max input length"")\n    parser.add_argument(""--batch_size"", type=int, default=8, help=""Batch Size"")\n    parser.add_argument(""--tokens"", choices=[""words"", ""chars"", ""bpe"", ""wordpiece""], default=""wordpiece"",\n                        help=""What tokens to use"")\n    parser.add_argument(""--subword_model_file"", type=str, help=""If using subwords, pass this"", default=\'bert-base-uncased\')\n    parser.add_argument(""--subword_vocab_file"", type=str, help=""If using subwords with separate vocab file, pass here"")\n    parser.add_argument(""--subword_special_tokens"", type=str, nargs=\'*\',\n                        help=""When using wordpiece vectorizer, this list provide special tokens to the never_split ""\n                             ""argument of BertTokenizer. These special tokens should also be in the customized vocab ""\n                             ""file so that they have their indices."")\n    parser.add_argument(""--dropout"", type=float, default=0.1, help=""Dropout"")\n    parser.add_argument(""--lr_scheduler"", type=str, help=""The type of learning rate decay scheduler"", default=\'cosine\')\n    parser.add_argument(""--lr_decay_steps"", type=int, help=""decay steps of lr scheduler"")\n    parser.add_argument(""--lr_decay_rate"", type=float, help=""decay rate of lr scheduler"")\n    parser.add_argument(""--lr_alpha"", type=float, help=""parameter alpha for cosine decay scheduler"")\n    parser.add_argument(""--optim"", default=""adam"", type=str, help=""Optimizer to use (defaults to adam)"")\n    parser.add_argument(""--lr"", type=float, default=4.0e-4, help=""Learning rate"")\n    parser.add_argument(""--clip"", type=float, default=0.25, help=""Clipping gradient norm"")\n    parser.add_argument(""--weight_decay"", type=float, default=0.0, help=""Weight decay"")\n    parser.add_argument(""--epochs"", type=int, default=20, help=""Num training epochs"")\n    parser.add_argument(""--restart_from"", type=str, help=""Option allows you to restart from a previous checkpoint"")\n    parser.add_argument(""--restart_tt"", type=str, help=""Optional param for legacy checkpoints (step|epoch)"")\n    parser.add_argument(""--warmup_steps"", type=int, default=1000, help=""Num warmup steps"")\n    parser.add_argument(""--saves_per_epoch"", type=int, default=5, help=""The number of checkpoints to save within an epoch"")\n    parser.add_argument(""--mlm"", type=str2bool, default=False, help=""Use Masked Language Model (MLM) objective"")\n    parser.add_argument(\'--rpr_k\', help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[8], nargs=\'+\')\n    parser.add_argument(""--device"", type=str,\n                        default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                        help=""Device (cuda or cpu)"")\n    parser.add_argument(""--distributed"",\n                        type=str2bool,\n                        default=False,\n                        help=""Are we doing distributed training?"")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""Local rank for distributed training (-1 means use the environment variables to find)"")\n    parser.add_argument(""--chars_per_word"",\n                        type=int,\n                        default=40,\n                        help=""How many max characters per word"")\n\n    args = parser.parse_args()\n\n    if args.train_file and not args.valid_file:\n        logger.error(""If you provide a train_file, you must provide a valid_file"")\n        return\n\n    if not args.train_file and args.valid_file:\n        logger.error(""If you provide a valid_file, you must also provide a train_file"")\n        return\n\n    if args.tokens == ""chars"" and args.mlm:\n        logger.error(""Character composition cannot currently be used with the MLM objective"")\n\n    if args.basedir is None:\n        args.basedir = \'transformer-{}-{}-{}\'.format(args.dataset_key, args.tokens, os.getpid())\n    logging.basicConfig(level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n    logger.info(""Cache directory [%s]"", args.dataset_cache)\n\n    num_gpus = get_num_gpus_multiworker()\n    args.distributed = args.distributed or num_gpus > 1\n    logger.info(f""Using {num_gpus} GPUs in this job."")\n\n    if args.distributed:\n        args.device = init_distributed(args.local_rank)\n\n    if args.train_file:\n        dataset = {\'train_file\': args.train_file, \'valid_file\': args.valid_file}\n    else:\n        dataset = DataDownloader(DATASETS[args.dataset_key], args.dataset_cache).download()\n    if args.subword_special_tokens is None:\n        special_tokens = ()\n    else:\n        special_tokens = tuple(args.subword_special_tokens)\n    reader = create_reader(args.tokens, args.nctx, args.chars_per_word, args.subword_model_file,\n                           args.subword_vocab_file, special_tokens)\n\n    preproc_data = load_embed_and_vocab(args.tokens, reader, dataset, args.dataset_key,\n                                        args.embed_type, args.d_model, args.cache_features)\n\n    vocabs = preproc_data[\'vocabs\']\n    if args.mlm:\n        mask_from = vocabs[\'x\']\n        vocab_size = len(mask_from)\n        mask_value = mask_from.get(""[MASK]"", mask_from.get(""<MASK>"", -1))\n        if mask_value == -1:\n            logger.error(""We could not find a suitable masking token in the vocab"")\n            return\n    os.makedirs(args.basedir, exist_ok=True)\n    # We want to make sure to save our input vocab into the basedir for reuse later\n    write_json(vocabs[\'x\'], os.path.join(args.basedir, \'vocabs.json\'))\n    embeddings = preproc_data[\'embeddings\']\n    valid_num_words = preproc_data[\'valid_num_words\']\n    tgt_key = preproc_data[\'tgt_key\']\n    logger.info(""Loaded embeddings"")\n\n    train_set = load_data_caching(args.tokens, reader, dataset, \'train_file\', vocabs, args.cache_features, logger)\n    valid_set = load_data_caching(args.tokens, reader, dataset, \'valid_file\', vocabs, args.cache_features, logger)\n    logger.info(""valid. tokens [%s], valid. words [%s]"", valid_set.tensors[-1].numel(), valid_num_words)\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set) if args.distributed else None\n    train_loader = DataLoader(train_set, sampler=train_sampler, batch_size=args.batch_size, shuffle=(not args.distributed))\n    valid_loader = DataLoader(valid_set, batch_size=args.batch_size, shuffle=False)\n    logger.info(""Loaded datasets"")\n\n    if len(args.rpr_k) == 0 or args.rpr_k[0] < 1:\n        rpr_k = None\n    elif len(args.rpr_k) == 1:\n        rpr_k = args.rpr_k[0]\n    else:\n        rpr_k = args.rpr_k\n\n    TLM = TransformerMaskedLanguageModel if args.mlm else TransformerLanguageModel\n    model = TLM.create(embeddings,\n                       hsz=args.d_model,\n                       d_ff=args.d_ff,\n                       tie_weights=(args.tokens != \'chars\'),\n                       dropout=args.dropout,\n                       gpu=False,\n                       num_heads=args.num_heads,\n                       layers=args.num_layers,\n                       rpr_k=rpr_k,\n                       d_k=args.d_k,\n                       src_keys=[\'x\'], tgt_key=tgt_key)\n    model.to(args.device)\n    loss_function = model.create_loss()\n    loss_function.to(args.device)\n\n    logger.info(""Loaded model and loss"")\n\n    # in this case (train_loader is not iterator) the division by number of gpus is automatically taken care of by\n    # torch.DataLoader\n    steps_per_epoch = len(train_loader)\n    update_on = steps_per_epoch // args.saves_per_epoch\n    report_on = update_on // 10\n    logger.info(f""Steps per epoch per GPU: {steps_per_epoch}. Saving a checkpoint every {update_on} steps."")\n    lr_decay = get_lr_decay(args.lr_scheduler, args.lr, steps_per_epoch, args.epochs, logger,\n                            decay_steps=args.lr_decay_steps, decay_rate=args.lr_decay_rate, alpha=args.lr_alpha)\n    linear_warmup = WarmupLinearSchedulerPyTorch(args.warmup_steps, lr=args.lr)\n    lr_sched = CompositeLRScheduler(linear_warmup, lr_decay, lr=args.lr)\n\n    global_step = 0\n    start_epoch = 0\n    if args.restart_from:\n        model.load_state_dict(torch.load(args.restart_from))\n        vec = args.restart_from.split(""-"")\n\n        if args.restart_tt:\n            tick_type = args.restart_tt\n        else:\n            tick_type = vec[-2]\n        step_num = int(vec[-1].split(""."")[0])\n        if tick_type == \'epoch\':\n            start_epoch = step_num\n            global_step = start_epoch * steps_per_epoch\n\n        else:\n            start_epoch = step_num // steps_per_epoch\n            global_step = step_num\n\n        logger.info(""Restarting from a previous checkpoint %s.\\n\\tStarting at global_step=%d, epoch=%d"",\n                    args.restart_from, global_step, start_epoch + 1)\n\n    optimizer = OptimizerManager(model, global_step, optim=args.optim, lr=args.lr, lr_function=lr_sched, weight_decay=args.weight_decay)\n    logger.info(""Model has {:,} parameters"".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n\n    # Prepare model for distributed training if needed\n    if args.distributed:\n        # This program assume pure data parallelism, each model is on a single gpu\n        # If we wanted to support model and data parallelism we would need to update\n        # the selection of gpus based on rank, it would need to select multiple ids\n        # based on rank, here we select only a single gpu and use it for input and\n        # output.\n        model = DistributedDataParallel(model, device_ids=[args.device], output_device=args.device)\n        logger.info(""Model located on %s"", args.device)\n\n    # This is the training loop\n    steps = global_step\n    model_base = os.path.join(args.basedir, \'checkpoint\')\n    for epoch in range(start_epoch, args.epochs):\n        avg_loss = Average(\'average_train_loss\')\n        metrics = {}\n        optimizer.zero_grad()\n\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n\n        start = time.time()\n        model.train()\n        for i, batch in enumerate(train_loader):\n            steps += 1\n            x, y = batch\n            inputs = x.to(args.device)\n            labels = y.to(args.device)\n            if args.mlm:\n                inputs, labels = on_demand_mlm_masking(inputs, labels, mask_value, vocab_size)\n            inputs = {\'x\': inputs}\n            labels = labels.transpose(0, 1).contiguous()\n            logits = model(inputs, None)[0].transpose(0, 1).contiguous()\n            if args.mlm:\n                loss = loss_function(logits, labels)\n            else:\n                shift_logits = logits[:-1]\n                shift_labels = labels[1:]\n                loss = loss_function(shift_logits, shift_labels)\n            loss.backward()\n            avg_loss.update(loss.item())\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n            optimizer.step()\n            optimizer.zero_grad()\n            if (i + 1) % report_on == 0:\n                logging.info(avg_loss)\n            if (i + 1) % update_on == 0 and args.local_rank < 1:\n                elapsed = (time.time() - start)/60\n                logging.info(\'elapsed time this epoch %d min\', elapsed)\n                logging.info(\'elapsed step time %f steps/min\', i/elapsed)\n                save_checkpoint(model, model_base, steps, tick_type=\'step\')\n\n        # How much time elapsed in minutes\n        elapsed = (time.time() - start)/60\n        train_token_loss = avg_loss.avg\n        # This is the average training token-level loss across all machines\n        # This is the token-level training perplexity\n        train_token_ppl = math.exp(train_token_loss)\n        metrics[\'train_elapsed_min\'] = elapsed\n        metrics[\'average_train_loss\'] = train_token_loss\n        metrics[\'train_ppl\'] = train_token_ppl\n        model_base = os.path.join(args.basedir, \'checkpoint\')\n        avg_valid_loss = Average(\'average_valid_loss\')\n        start = time.time()\n        model.eval()\n        for batch in valid_loader:\n            with torch.no_grad():\n                x, y = batch\n                inputs = x.to(args.device)\n                labels = y.to(args.device)\n                if args.mlm:\n                   inputs, labels = on_demand_mlm_masking(inputs, labels, mask_value, vocab_size)\n                inputs = {\'x\': inputs}\n                labels = labels.transpose(0, 1).contiguous()\n                logits = model(inputs, None)[0].transpose(0, 1).contiguous()\n                if args.mlm:\n                    loss = loss_function(logits, labels)\n                else:\n                    shift_logits = logits[:-1]\n                    shift_labels = labels[1:]\n                    loss = loss_function(shift_logits, shift_labels)\n                avg_valid_loss.update(loss.item())\n\n        valid_token_loss = avg_valid_loss.avg\n        valid_token_ppl = math.exp(valid_token_loss)\n\n        elapsed = (time.time() - start)/60\n        metrics[\'valid_elapsed_min\'] = elapsed\n\n        metrics[\'average_valid_loss\'] = valid_token_loss\n        if args.tokens in [\'bpe\', \'wordpiece\']:\n            metrics[\'valid_token_ppl\'] = valid_token_ppl\n            metrics[\'average_valid_word_ppl\'] = math.exp(valid_token_loss * valid_set.tensors[-1].numel() / valid_num_words)\n        else:\n            metrics[\'average_valid_word_ppl\'] = valid_token_ppl\n        logger.info(metrics)\n\n        if args.local_rank < 1:\n            save_checkpoint(model, model_base, epoch, save_npz=True)\n\n\nif __name__ == ""__main__"":\n    train()\n\n'"
api-examples/response-selection.py,7,"b'import argparse\nimport os\nfrom transformer_utils import *\nfrom baseline.pytorch.lm import TransformerLanguageModel\nfrom baseline.progress import create_progress_bar\nfrom baseline.utils import str2bool\nfrom torch.utils.data import DataLoader\nimport logging\nlogger = logging.getLogger(""baseline"")\n\n\nparser = argparse.ArgumentParser(""Load a dual-encoder model (conveRT) and do response selection on testing data"")\nparser.add_argument(""--d_model"", type=int, default=512, help=""Model dimension (and embedding dsz)"")\nparser.add_argument(""--d_ff"", type=int, default=2048, help=""FFN dimension"")\nparser.add_argument(""--d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\nparser.add_argument(""--num_heads"", type=int, default=8, help=""Number of heads"")\nparser.add_argument(""--num_ft_workers"", type=int, default=4, help=""Number train workers"")\nparser.add_argument(""--num_test_workers"", type=int, default=2, help=""Number valid workers"")\nparser.add_argument(""--num_layers"", type=int, default=6, help=""Number of layers"")\nparser.add_argument(""--nctx"", type=int, default=64, help=""Max context length (for both encoder and decoder)"")\nparser.add_argument(""--embed_type"", type=str, default=\'positional\',\n                    help=""register label of the embeddings, so far support positional or learned-positional"")\nparser.add_argument(""--stacking_layers"", type=int, nargs=\'+\', default=[1024, 1024, 1024])\nparser.add_argument(\'--rpr_k\', help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                    type=int, default=[3, 5, 48, 48, 48, 48], nargs=\'+\')\nparser.add_argument(""--reduction_d_k"", type=int, default=64, help=""Dimensions of Key and Query in the single headed""\n                                                                  ""reduction layers"")\nparser.add_argument(""--ckpt"", type=str, help=""path to the model checkpoint"")\nparser.add_argument(""--test_file"", type=str, help=""path to the testing data"")\nparser.add_argument(""--subword_model_file"", type=str, required=True)\nparser.add_argument(""--subword_vocab_file"", type=str, required=True)\nparser.add_argument(""--recall_k"", type=int, default=100, help=""select the response from how many candidates"")\nparser.add_argument(""--recall_top"", type=int, default=1, help=""whether the correct response is ranked top x"")\nparser.add_argument(""--device"", type=str,\n                    default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                    help=""Device (cuda or cpu)"")\nargs = parser.parse_args()\n\nreader = MultiFileDatasetReader(args.nctx, args.subword_model_file, args.subword_vocab_file, \'*.txt\', \'ntp\')\nvocab = reader.build_vocab()\n\npreproc_data = baseline.embeddings.load_embeddings(\'x\', dsz=args.d_model, known_vocab=vocab[\'x\'], embed_type=args.embed_type)\nvocabs = preproc_data[\'vocab\']\nembeddings = preproc_data[\'embeddings\']\nlogger.info(""Loaded embeddings"")\n\ntest_set = reader.load(args.test_file, vocabs)\nind2tok = {ind: tok for tok, ind in vocabs.items()}\n\n# use other samples in a batch as negative samples. Don\'t shuffle to compare with conveRT benchmarks\ntest_loader = DataLoader(test_set, batch_size=args.recall_k, num_workers=args.num_test_workers)\nlogger.info(""Loaded datasets"")\n\nmodel = create_model(embeddings,\n                     model_type=\'dual-encoder\',\n                     d_model=args.d_model,\n                     d_ff=args.d_ff,\n                     num_heads=args.num_heads,\n                     num_layers=args.num_layers,\n                     stacking_layers=args.stacking_layers,\n                     dropout=0.,\n                     rpr_k=args.rpr_k,\n                     d_k=args.d_k,\n                     reduction_d_k=args.reduction_d_k,\n                     ff_pdrop=0.,\n                     logger=logger)\n\nif os.path.isdir(args.ckpt):\n    checkpoint = find_latest_checkpoint(args.ckpt)\n    logger.warning(""Found latest checkpoint %s"", checkpoint)\nelse:\n    checkpoint = args.ckpt\nmodel.load_state_dict(torch.load(checkpoint, map_location=torch.device(\'cpu\')))\nmodel.to(args.device)\n\nnumerator = 0\ndenominator = 0\nmodel.eval()\npg = create_progress_bar(len(test_loader)//args.recall_k)\nfor batch in test_loader:\n    if batch[0].shape[0] != args.recall_k:\n        break\n    with torch.no_grad():\n        x, y = batch\n        inputs = x.to(args.device)\n        targets = y.to(args.device)\n        query = model.encode_query(inputs).unsqueeze(1)  # [B, 1, H]\n        response = model.encode_response(targets).unsqueeze(0)  # [1, B, H]\n        all_score = nn.CosineSimilarity(dim=-1)(query, response).to(\'cpu\')\n\n        _, indices = torch.topk(all_score, args.recall_top, dim=1)\n        correct = (indices == torch.arange(args.recall_k).unsqueeze(1).expand(-1, args.recall_top)).sum()\n        numerator += correct\n        print(f""Selected {correct} correct responses out of {args.recall_k}"")\n        denominator += args.recall_k\n    pg.update()\npg.done()\nacc = float(numerator)/denominator\n\nprint(f""{args.recall_top}@{args.recall_k} acc: {acc}"")\nwith open(\'./results.txt\', \'a\') as wf:\n    wf.write(f""Checkpoint: {checkpoint}; {args.recall_top}@{args.recall_k} accuracy: {acc}\\n"")\n'"
api-examples/score-text.py,0,"b'import os\nimport argparse\nimport baseline as bl\n\nparser = argparse.ArgumentParser(description=\'Score text with a language model.\')\nparser.add_argument(""--model"", help=\'The path to either the .zip file (or dir) created by training\', required=True)\nparser.add_argument(""--text"", help=""The text to score as a string, or a path to a file"", required=True)\nparser.add_argument(""--backend"", default=""tf"", help=""the dl backend framework the model was trained with"", choices=(""tensorflow"", ""tf"", ""pytorch"", ""pyt""))\nparser.add_argument(""--device"", help=""the device to run the model on"")\nparser.add_argument(""--prob"", default=""conditional"", choices=(""joint"", ""conditional""), help=""Should you score the whole string (joint) or the score of the last token given the previous (conditional)"")\nparser.add_argument(\'--prefer_eager\', help=""If running in TensorFlow, should we prefer eager model"", type=bl.str2bool, default=False)\nargs = parser.parse_args()\n\nif args.backend == \'tf\':\n    from eight_mile.tf.layers import set_tf_eager_mode\n    set_tf_eager_mode(args.prefer_eager)\n\nif os.path.exists(args.text) and os.path.isfile(args.text):\n    with open(args.text, \'r\') as f:\n        text = [f.read.strip().split()]\nelse:\n    text = [args.text.split()]\n\nif len(text) > 1:\n    raise ValueError(f""Currently only batch size 1 supported for LanguageModelServices, got {len(text)}"")\n\nm = bl.LanguageModelService.load(args.model, backend=args.backend, device=args.device)\n\nif args.prob == \'joint\':\n    scores = m.joint(text)\n    print(f""P({\' \'.join(text[0])}) = {scores[0]}"")\nelse:\n    *context, targets = list(zip(*text))\n    context = list(zip(*context))\n    scores = m.conditional(context, target=targets)\n    print(f""P({text[0][-1]} | {\' \'.join(text[0][:-1])}) = {scores[0]}"")\n'"
api-examples/tag-text.py,0,"b'from __future__ import print_function\n\nimport baseline as bl\nimport argparse\nimport os\nfrom baseline.utils import str2bool, read_conll\n\n\nparser = argparse.ArgumentParser(description=\'Tag text with a model\')\nparser.add_argument(\'--model\', help=\'A tagger model with extended features\', required=True, type=str)\nparser.add_argument(\'--text\', help=\'raw value\', type=str)\nparser.add_argument(\'--conll\', help=\'is file type conll?\', type=str2bool, default=False)\nparser.add_argument(\'--features\', help=\'(optional) features in the format feature_name:index (column # in conll) or \'\n                                       \'just feature names (assumed sequential)\', default=[], nargs=\'+\')\nparser.add_argument(\'--backend\', help=\'backend\', default=\'tf\')\nparser.add_argument(\'--device\', help=\'device\')\nparser.add_argument(\'--remote\', help=\'(optional) remote endpoint\', type=str) # localhost:8500\nparser.add_argument(\'--name\', help=\'(optional) signature name\', type=str)\nparser.add_argument(\'--preproc\', help=\'(optional) where to perform preprocessing\', choices={\'client\', \'server\'}, default=\'client\')\nparser.add_argument(\'--export_mapping\', help=\'mapping between features and the fields in the grpc/ REST \'\n                                                         \'request, eg: token:word ner:ner. This should match with the \'\n                                                         \'`exporter_field` definition in the mead config\',\n                    default=[], nargs=\'+\')\nparser.add_argument(\'--prefer_eager\', help=""If running in TensorFlow, should we prefer eager model"", type=str2bool)\nparser.add_argument(\'--batchsz\', default=64, help=""How many examples to run through the model at once"", type=int)\n\nargs = parser.parse_args()\n\nif args.backend == \'tf\':\n    from eight_mile.tf.layers import set_tf_eager_mode\n    set_tf_eager_mode(args.prefer_eager)\n\n\ndef create_export_mapping(feature_map_strings):\n    feature_map_strings = [x.strip() for x in feature_map_strings if x.strip()]\n    if not feature_map_strings:\n        return {}\n    else:\n        return {x[0]: x[1] for x in [y.split(\':\') for y in feature_map_strings]}\n\n\ndef feature_index_mapping(features):\n    if not features:\n        return {}\n    elif \':\' in features[0]:\n        return {feature.split(\':\')[0]: int(feature.split(\':\')[1]) for feature in features}\n    else:\n        return {feature: index for index, feature in enumerate(features)}\n\n\nif os.path.exists(args.text) and os.path.isfile(args.text):\n    texts = []\n    if args.conll:\n        feature_indices = feature_index_mapping(args.features)\n        for sentence in read_conll(args.text):\n            if feature_indices:\n                texts.append([{k: line[v] for k, v in feature_indices.items()} for line in sentence])\n            else:\n                texts.append([line[0] for line in sentence])\n    else:\n        with open(args.text, \'r\') as f:\n            for line in f:\n                text = line.strip().split()\n                texts += [text]\nelse:\n    texts = [args.text.split()]\n\nm = bl.TaggerService.load(args.model, backend=args.backend, remote=args.remote,\n                          name=args.name, preproc=args.preproc, device=args.device)\n\nbatched = [texts[i:i+args.batchsz] for i in range(0, len(texts), args.batchsz)]\n\nfor texts in batched:\n    for sen in m.predict(texts, export_mapping=create_export_mapping(args.export_mapping)):\n        for word_tag in sen:\n            print(""{} {}"".format(word_tag[\'text\'], word_tag[\'label\']))\n        print()\n'"
api-examples/tf-train-from-scratch.py,0,"b""import baseline as bl\n\nfrom eight_mile.tf.layers import set_tf_log_level, set_tf_eager_mode\nset_tf_eager_mode(False)\nimport baseline.tf.embeddings\nimport baseline.tf.classify\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport argparse\nimport logging\nlog = logging.getLogger('baseline.timing')\n\ndef get_logging_level(ll):\n    ll = ll.lower()\n    if ll == 'debug':\n        return logging.DEBUG\n    if ll == 'info':\n        return logging.INFO\n    return logging.WARNING\n\nparser = argparse.ArgumentParser(description='Train a Baseline model with TensorFlow Estimator API')\nparser.add_argument('--checkpoint_dir', help='Directory for model checkpoints', default='./checkpoints', type=str)\nparser.add_argument('--model_type', help='What type of model to build', type=str, default='default')\nparser.add_argument('--poolsz', help='How many hidden units for pooling', type=int, default=100)\nparser.add_argument('--stacksz', help='How many hidden units for stacking', type=int, nargs='+')\nparser.add_argument('--text', help='raw value', type=str)\nparser.add_argument('--backend', help='backend', default='tf')\nparser.add_argument('--remote', help='(optional) remote endpoint', type=str) # localhost:8500\nparser.add_argument('--name', help='(optional) signature name', type=str)\nparser.add_argument('--epochs', help='Number of epochs to train', type=int, default=2)\nparser.add_argument('--batchsz', help='Batch size', type=int, default=50)\nparser.add_argument('--filts', help='Parallel convolution filter widths (if default model)', type=int, default=[3, 4, 5], nargs='+')\nparser.add_argument('--mxlen', help='Maximum post length (number of words) during training', type=int, default=100)\nparser.add_argument('--train', help='Training file', default='../data/stsa.binary.phrases.train')\nparser.add_argument('--valid', help='Validation file', default='../data/stsa.binary.dev')\nparser.add_argument('--test', help='Testing file', default='../data/stsa.binary.test')\nparser.add_argument('--embeddings', help='Pretrained embeddings file', default='/data/embeddings/GoogleNews-vectors-negative300.bin')\nparser.add_argument('--ll', help='Log level', type=str, default='info')\nparser.add_argument('--tf_ll', help='TensorFlow Log level', type=str, default='warning')\nparser.add_argument('--lr', help='Learning rate', type=float, default=0.001)\nparser.add_argument('--optim', help='Optimizer (sgd, adam) (default is adam)', type=str, default='adam')\nargs = parser.parse_known_args()[0]\n\nlogging.basicConfig(level=get_logging_level(args.ll))\nset_tf_log_level(args.tf_ll)\n\npool_field = 'cmotsz' if args.model_type == 'default' else 'rnnsz'\n\nmodel_params = {\n    'model_type': args.model_type,\n    'filtsz': args.filts,\n    pool_field: args.poolsz\n}\n\nif args.stacksz is not None:\n    model_params['hsz'] = args.stacksz\n\n\nfeature_desc = {\n    'word': {\n        'vectorizer': bl.Token1DVectorizer(mxlen=args.mxlen, transform_fn=bl.lowercase),\n        'embed': {'file': args.embeddings, 'type': 'default', 'unif': 0.25}\n    }\n}\n# Create a reader that is using our vectorizers to parse a TSV file\n# with rows like:\n# <label>\\t<sentence>\\n\n\nvectorizers = {k: v['vectorizer'] for k, v in feature_desc.items()}\nreader = bl.TSVSeqLabelReader(vectorizers,\n                              clean_fn=bl.TSVSeqLabelReader.do_clean)\n\ntrain_file = args.train\nvalid_file = args.valid\ntest_file = args.test\n\n# This builds a set of counters\nvocabs, labels = reader.build_vocab([train_file,\n                                     valid_file,\n                                     test_file])\n\n# This builds a set of embeddings objects, these are typically not DL-specific\n# but if they happen to be addons, they can be\nembeddings = dict()\nfor k, v in feature_desc.items():\n    embed_config = v['embed']\n    embeddings_for_k = bl.load_embeddings('word',\n                                          embed_file=embed_config['file'],\n                                          known_vocab=vocabs[k],\n                                          embed_type=embed_config.get('type', 'default'),\n                                          unif=embed_config.get('unif', 0.),\n                                          use_mmap=True)\n\n    embeddings[k] = embeddings_for_k['embeddings']\n    # Reset the vocab to the embeddings one\n    vocabs[k] = embeddings_for_k['vocab']\n\n# Now create the model we want to train.  There are lots of HPs we can pass in\n# but for this simple example, use basic defaults\nmodel = bl.model.create_model(embeddings, labels, **model_params)\n\nts = reader.load(train_file, vocabs=vocabs, batchsz=args.batchsz)\nvs = reader.load(valid_file, vocabs=vocabs, batchsz=args.batchsz)\nes = reader.load(test_file, vocabs=vocabs, batchsz=args.batchsz)\n\nbl.train.fit(model, ts, vs, es, epochs=args.epochs,\n             optim=args.optim, eta=args.lr,\n             reporting=[r.step for r in (bl.reporting.LoggingReporting(),)],\n             fit_func='feed_dict')\n"""
api-examples/transformer-paired-response.py,12,"b'import numpy as np\nimport torch\nimport logging\nimport os\nimport glob\nfrom argparse import ArgumentParser\nimport baseline\nfrom transformer_utils import TiedEmbeddingsSeq2SeqModel, find_latest_checkpoint\n\nfrom baseline.utils import str2bool, read_json, Offsets, revlut\nfrom baseline.vectorizers import Token1DVectorizer, BPEVectorizer1D\n\nlogger = logging.getLogger(__file__)\n\n\ndef decode_sentence(model, vectorizer, query, word2index, index2word, device, max_response_length, sample=True):\n    vec, length = vectorizer.run(query, word2index)\n    toks = torch.from_numpy(vec).unsqueeze(0).to(device=device)\n    length = torch.from_numpy(np.array(length)).unsqueeze(0).to(device=device)\n    EOU = word2index.get(\'<EOU>\')\n    response = []\n    with torch.no_grad():\n        dst = [Offsets.GO]\n        for i in range(max_response_length):\n            dst_tensor = torch.zeros_like(toks).squeeze()\n            dst_tensor[:len(dst)] = torch.from_numpy(np.array(dst)).to(device=device)\n            predictions = model({\'x\': toks, \'src_len\': length, \'dst\': dst_tensor.unsqueeze(0)})\n            token_offset = len(dst) - 1\n            if not sample:\n                output = torch.argmax(predictions, -1).squeeze(0)\n                output = output[token_offset].item()\n            else:\n                # using a multinomial distribution to predict the word returned by the model\n                predictions = predictions.exp().squeeze(0)\n                output = torch.multinomial(predictions, num_samples=1).squeeze(0)[token_offset].item()\n\n\n            dst.append(output)\n            response.append(index2word.get(dst[-1], \'<ERROR>\'))\n            if output == Offsets.EOS or output == EOU:\n                break\n    return response\n\n\ndef create_model(embeddings, d_model, d_ff, num_heads, num_layers, rpr_k, d_k, activation, checkpoint_name):\n    if len(rpr_k) == 0 or rpr_k[0] < 1:\n        rpr_k = None\n    logger.info(""Creating tied encoder decoder model"")\n    hps = {""dsz"": d_model,\n           ""hsz"": d_model,\n           ""d_ff"": d_ff,\n           ""dropout"": 0.0,\n           ""num_heads"": num_heads,\n           ""layers"": num_layers,\n           ""encoder_type"": ""transformer"",\n           ""decoder_type"": ""transformer"",\n           ""src_lengths_key"": ""x_lengths"",\n           ""d_k"": d_k,\n           ""activation"": activation,\n           ""rpr_k"": rpr_k}\n    model = TiedEmbeddingsSeq2SeqModel(embeddings, **hps)\n    model.load_state_dict(torch.load(checkpoint_name))\n    model.eval()\n    print(model)\n    return model\n\n\ndef run():\n    parser = ArgumentParser()\n    parser.add_argument(""--basedir"", type=str)\n    parser.add_argument(""--checkpoint"", type=str, help=\'Checkpoint name or directory to load\')\n    parser.add_argument(""--sample"", type=str2bool, help=\'Sample from the decoder?  Defaults to `true`\', default=1)\n    parser.add_argument(""--vocab"", type=str, help=\'Vocab file to load\', required=False)\n    parser.add_argument(""--query"", type=str, default=\'hello how are you ?\')\n    parser.add_argument(""--dataset_cache"", type=str, default=os.path.expanduser(\'~/.bl-data\'),\n                        help=""Path or url of the dataset cache"")\n    parser.add_argument(""--d_model"", type=int, default=512, help=""Model dimension (and embedding dsz)"")\n    parser.add_argument(""--d_ff"", type=int, default=2048, help=""FFN dimension"")\n    parser.add_argument(""--d_k"", type=int, default=None, help=""Dimension per head.  Use if num_heads=1 to reduce dims"")\n    parser.add_argument(""--num_heads"", type=int, default=8, help=""Number of heads"")\n    parser.add_argument(""--num_layers"", type=int, default=6, help=""Number of layers"")\n    parser.add_argument(""--nctx"", type=int, default=64, help=""Max context length (for both encoder and decoder)"")\n    parser.add_argument(""--embed_type"", type=str, default=\'learned-positional\',\n                        help=""register label of the embeddings, so far support positional or learned-positional"")\n    parser.add_argument(""--subword_model_file"", type=str, required=True)\n    parser.add_argument(""--subword_vocab_file"", type=str, required=True)\n    parser.add_argument(""--activation"", type=str, default=\'relu\')\n    parser.add_argument(\'--rpr_k\', help=\'Relative attention positional sizes pass 0 if you dont want relative attention\',\n                        type=int, default=[3, 5, 48, 48, 48, 48], nargs=\'+\')\n\n    parser.add_argument(""--device"", type=str,\n                        default=""cuda"" if torch.cuda.is_available() else ""cpu"",\n                        help=""Device (cuda or cpu)"")\n    args = parser.parse_args()\n\n    if torch.cuda.device_count() == 1:\n        torch.cuda.set_device(0)\n        args.device = torch.device(""cuda"", 0)\n\n\n    vocab_file = args.vocab\n\n    if os.path.isdir(args.checkpoint):\n        vocab_file = os.path.join(args.checkpoint, \'vocabs.json\')\n        checkpoint = find_latest_checkpoint(args.checkpoint)\n        logger.warning(""Found latest checkpoint %s"", checkpoint)\n    else:\n        checkpoint = args.checkpoint\n\n\n    vocab = read_json(vocab_file)\n    # If we are not using chars, then use \'x\' for both input and output\n    preproc_data = baseline.embeddings.load_embeddings(\'x\', dsz=args.d_model, counts=False, known_vocab=vocab, embed_type=args.embed_type)\n    embeddings = preproc_data[\'embeddings\']\n\n    model = create_model(embeddings, d_model=args.d_model, d_ff=args.d_ff, num_heads=args.num_heads, num_layers=args.num_layers,\n                         rpr_k=args.rpr_k, d_k=args.d_k, checkpoint_name=checkpoint, activation=args.activation)\n    model.to(args.device)\n\n    vectorizer = BPEVectorizer1D(model_file=args.subword_model_file, vocab_file=args.subword_vocab_file, mxlen=args.nctx)\n    index2word = revlut(vocab)\n    print(\'[Query]\', args.query)\n    print(\'[Response]\', \' \'.join(decode_sentence(model, vectorizer, args.query.split(), vocab, index2word, args.device, max_response_length=args.nctx, sample=args.sample)))\n\nrun()\n'"
api-examples/transformer_utils.py,28,"b'from baseline.pytorch.torchy import vec_log_sum_exp\nfrom baseline.pytorch.seq2seq import Seq2SeqModel\nfrom eight_mile.utils import str2bool, write_yaml, read_yaml, Offsets\nfrom eight_mile.pytorch.layers import *\nfrom eight_mile.optz import create_lr_scheduler\nimport baseline.pytorch.embeddings\nimport baseline.embeddings\nimport random\nfrom baseline.progress import create_progress_bar\nfrom torch.utils.data.dataset import IterableDataset, TensorDataset\nfrom baseline.vectorizers import Token1DVectorizer, BPEVectorizer1D, Char2DVectorizer, WordpieceVectorizer1D\nimport codecs\nfrom collections import Counter\nimport glob\nimport json\n\ndef find_latest_checkpoint(checkpoint_dir: str, wildcard=""checkpoint"") -> str:\n    step_num = 0\n    for f in glob.glob(os.path.join(checkpoint_dir, f""{wildcard}*"")):\n        this_step_num = int(f.split(""-"")[-1])\n        if this_step_num > step_num:\n            checkpoint = f\n            step_num = this_step_num\n    return checkpoint\n\n\nclass TripletLoss(nn.Module):\n    """"""Provide a Triplet Loss using the reversed batch for negatives""""""\n    def __init__(self, model):\n        super().__init__()\n        self.score = nn.CosineSimilarity(dim=1)\n        self.model = model\n\n    def forward(self, inputs, targets):\n        # reverse the batch and use as a negative example\n        neg = targets.flip(0)\n        query = self.model.encode_query(inputs)\n        response = self.model.encode_response(targets)\n        neg_response = self.model.encode_response(neg)\n        pos_score = self.score(query, response)\n        neg_score = self.score(query, neg_response)\n        score = neg_score - pos_score\n        score = score.masked_fill(score < 0.0, 0.0).sum(0)\n        return score\n\n\nclass AllLoss(nn.Module):\n    def __init__(self, model, warmup_steps=10000):\n        r""""""Loss from here https://arxiv.org/pdf/1705.00652.pdf see section 4\n\n        We want to minimize the negative log prob of y given x\n\n        -log P(y|x)\n\n        P(y|x) P(x) = P(x, y)                             Chain Rule of Probability\n        P(y|x) = P(x, y) / P(x)                           Algebra\n        P(y|x) = P(x, y) / \\sum_\\hat(y) P(x, y = \\hat(y)) Marginalize over all possible ys to get the probability of x\n        P_approx(y|x) = P(x, y) / \\sum_i^k P(x, y_k)      Approximate the Marginalization by just using the ys in the batch\n\n        S(x, y) is the score (cosine similarity between x and y in this case) from our neural network\n        P(x, y) = e^S(x, y)\n\n        P(y|x) = e^S(x, y) / \\sum_i^k e^S(x, y_k)\n        log P(y|x) = log( e^S(x, y) / \\sum_i^k e^S(x, y_k))\n        log P(y|x) = S(x, y) - log \\sum_i^k e^S(x, y_k)\n        -log P(y|x) = -(S(x, y) - log \\sum_i^k e^S(x, y_k))\n        """"""\n        super().__init__()\n        self.score = nn.CosineSimilarity(dim=-1)\n        self.model = model\n        self.max_scale = math.sqrt(self.model.embedding_layers.get_dsz())\n        self.steps = 0\n        self.warmup_steps = warmup_steps\n\n    def forward(self, inputs, targets):\n        # This is the cosine distance annealing referred to in https://arxiv.org/pdf/1911.03688.pdf\n        fract = min(self.steps / self.warmup_steps, 1)\n        c = (self.max_scale-1) * fract + 1\n        self.steps += 1\n        # These will get broadcast to [B, B, H]\n        query = self.model.encode_query(inputs).unsqueeze(1)  # [B, 1, H]\n        response = self.model.encode_response(targets).unsqueeze(0)  # [1, B, H]\n        # all_scores is now a batch x batch matrix where index (i, j) is the score between\n        # the i^th x vector and the j^th y vector\n        all_score = c * self.score(query, response)  # [B, B]\n        # The diagonal has the scores of correct pair, (i, i)\n        pos_score = torch.diag(all_score)\n        # vec_log_sum_exp will calculate the batched log_sum_exp in a numerically stable way\n        # the result is a [B, 1] vector which we squeeze to make it [B] to match the diag\n        # Because we are minimizing the negative log we turned the division into a subtraction here\n        loss = pos_score - vec_log_sum_exp(all_score, -1).squeeze()\n        # Batch loss\n        loss = torch.sum(loss)\n        # minimize the negative loss\n        return -loss\n\n\nclass TwoHeadConcat(nn.Module):\n    """"""Use two parallel SingleHeadReduction, and concatenate the outputs. It is used in the conveRT\n    paper (https://arxiv.org/pdf/1911.03688.pdf)""""""\n\n    def __init__(self, d_model, dropout, scale=False, d_k=None):\n        """"""Two parallel 1-head self-attention, then concatenate the output\n        :param d_model: dim of the self-attention\n        :param dropout: dropout of the self-attention\n        :param scale: scale fo the self-attention\n        :param d_k: d_k of the self-attention\n        :return: concatenation of the two 1-head attention\n        """"""\n        super().__init__()\n        self.reduction1 = SingleHeadReduction(d_model, dropout, scale=scale, d_k=d_k)\n        self.reduction2 = SingleHeadReduction(d_model, dropout, scale=scale, d_k=d_k)\n\n    def forward(self, inputs: torch.Tensor):\n        x = inputs\n        encoding1 = self.reduction1(x)\n        encoding2 = self.reduction2(x)\n        x = torch.cat([encoding1, encoding2], dim=-1)\n        return x\n\n\nclass ConveRTFFN(nn.Module):\n    """"""Implementation of the FFN layer from the convert paper (https://arxiv.org/pdf/1911.03688.pdf)""""""\n    def __init__(self, insz, hszs, outsz, pdrop):\n        """"""\n        :param insz: input dim\n        :param hszs: list of hidden sizes\n        :param outsz: output dim\n        :param pdrop: dropout of each hidden layer\n        """"""\n        super().__init__()\n        self.dense_stack = DenseStack(insz,\n                                      hszs,\n                                      activation=\'gelu\',\n                                      pdrop_value=pdrop,\n                                      skip_connect=True,\n                                      layer_norm=True)\n        self.final = Dense(hszs[-1], outsz)\n        self.proj = Dense(insz, outsz) if insz != outsz else nn.Identity()\n        self.ln1 = nn.LayerNorm(insz, eps=1e-6)\n        self.ln2 = nn.LayerNorm(outsz, eps=1e-6)\n\n    def forward(self, inputs):\n        x = self.ln1(inputs)\n        x = self.dense_stack(x)\n        x = self.final(x)\n        x = x + self.proj(inputs)\n        return self.ln2(x)\n\n\nclass PairedModel(nn.Module):\n\n    def __init__(self, embeddings,\n                 d_model,\n                 d_ff,\n                 dropout,\n                 num_heads,\n                 num_layers,\n                 stacking_layers=None,\n                 d_out=512,\n                 d_k=None,\n                 weight_std=0.02,\n                 rpr_k=None,\n                 reduction_d_k=64,\n                 ff_pdrop=0.1):\n        super().__init__()\n        if stacking_layers is None:\n            stacking_layers = [d_model] * 3\n\n        self.weight_std = weight_std\n        stacking_layers = listify(stacking_layers)\n        transformer = TransformerEncoderStack(num_heads=num_heads, d_model=d_model,\n                                              pdrop=dropout, layers=num_layers, activation=\'gelu\', d_ff=d_ff,\n                                              d_k=d_k, rpr_k=rpr_k)\n        self.attention_layer = TwoHeadConcat(d_model, dropout, scale=False, d_k=reduction_d_k)\n        self.transformer_layers = transformer\n        self.embedding_layers = embeddings\n        self.ff1 = ConveRTFFN(2*d_model, stacking_layers, d_out, ff_pdrop)\n        self.ff2 = ConveRTFFN(2*d_model, stacking_layers, d_out, ff_pdrop)\n        self.apply(self.init_layer_weights)\n\n    def init_layer_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n            module.weight.data.normal_(mean=0.0, std=self.weight_std)\n        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def encode_query(self, query):\n        query_mask = (query != Offsets.PAD)\n        att_mask = query_mask.unsqueeze(1).unsqueeze(1)\n        embedded = self.embedding_layers(query)\n        encoded_query = self.transformer_layers((embedded, att_mask))\n        encoded_query = self.attention_layer((encoded_query, encoded_query, encoded_query, att_mask))\n        encoded_query = self.ff1(encoded_query)\n        return encoded_query\n\n    def encode_response(self, response):\n        response_mask = (response != Offsets.PAD)\n        att_mask = response_mask.unsqueeze(1).unsqueeze(1)\n        embedded = self.embedding_layers(response)\n        encoded_response = self.transformer_layers((embedded, att_mask))\n        encoded_response = self.attention_layer((encoded_response, encoded_response, encoded_response, att_mask))\n        encoded_response = self.ff2(encoded_response)\n        return encoded_response\n\n    def forward(self, query, response):\n        encoded_query = self.encode_query(query)\n        encoded_response = self.encode_response(response)\n        return encoded_query, encoded_response\n\n    def create_loss(self, loss_type=\'all\'):\n        if loss_type == \'all\':\n            return AllLoss(self)\n        return TripletLoss(self)\n\n\nclass TransformerDiscriminator(nn.Module):\n\n    def __init__(self, embeddings, d_model, d_ff, dropout, num_heads, num_layers, rpr_k, d_k, **kwargs):\n        super().__init__()\n        self.embeddings = EmbeddingsStack(embeddings, dropout)\n        self.weight_std = kwargs.get(\'weight_std\', 0.02)\n        assert self.embeddings.dsz == d_model\n        self.transformer = TransformerEncoderStack(num_heads, d_model=d_model, pdrop=dropout, scale=True,\n                                                   layers=num_layers, d_ff=d_ff, rpr_k=rpr_k, d_k=d_k)\n        self.proj_to_output = pytorch_linear(d_model, 1)\n\n        self.apply(self.init_layer_weights)\n        self.lengths_feature = kwargs.get(\'lengths_feature\', self.embeddings.keys()[0])\n\n    def init_layer_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n            module.weight.data.normal_(mean=0.0, std=self.weight_std)\n        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, features):\n        embedded = self.embeddings(features)\n        x = features[self.lengths_feature]\n        input_mask = torch.zeros(x.shape, device=x.device, dtype=torch.long).masked_fill(x != 0, 1).unsqueeze(1).unsqueeze(1)\n        transformer_out = self.transformer((embedded, input_mask))\n        binary = self.proj_to_output(transformer_out)\n        return torch.sigmoid(binary)\n\n    def create_loss(self):\n        class Loss(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.loss = nn.BCELoss()\n\n            def forward(self, input, target):\n                fake_loss = self.loss(input[target == 0], target[target == 0])\n                real_loss = self.loss(input[target != 0], target[target != 0])\n                return real_loss + fake_loss\n        return Loss()\n\n\nclass TensorDatasetReaderBase(object):\n    """"""Provide a base-class to do operations that are independent of token representation\n    """"""\n    def __init__(self, nctx, vectorizers):\n        self.vectorizers = vectorizers\n        self.nctx = nctx\n        self.num_words = {}\n\n    def build_vocab(self, files):\n        vocabs = {k: Counter({\'[CLS]\': 1}) for k in self.vectorizers.keys()}\n\n        for file in files:\n            if file is None:\n                continue\n            self.num_words[file] = 0\n            with codecs.open(file, encoding=\'utf-8\', mode=\'r\') as f:\n                sentences = []\n                for line in f:\n                    split_sentence = line.split() + [\'<EOS>\']\n                    self.num_words[file] += len(split_sentence)\n                    sentences += split_sentence\n                for k, vectorizer in self.vectorizers.items():\n                    vocabs[k].update(vectorizer.count(sentences))\n        return vocabs\n\n    def load_features(self, filename, vocabs):\n\n        features = dict()\n        with codecs.open(filename, encoding=\'utf-8\', mode=\'r\') as f:\n            sentences = []\n            for line in f:\n                sentences += line.strip().split() + [\'<EOS>\']\n            for k, vectorizer in self.vectorizers.items():\n                vec, valid_lengths = vectorizer.run(sentences, vocabs[k])\n                features[k] = vec[:valid_lengths]\n                shp = list(vectorizer.get_dims())\n                shp[0] = valid_lengths\n                features[\'{}_dims\'.format(k)] = tuple(shp)\n        return features\n\n\nclass TensorWordDatasetReader(TensorDatasetReaderBase):\n    """"""Read each word, and produce a tensor of x and y that are identical\n    """"""\n    def __init__(self, nctx, use_subword=None, model_file=None, vocab_file=None, special_tokens=None):\n        """"""Create a reader with a context window that reads words\n\n        :param nctx: The context window length\n        :param use_subword: If this is not none, it should be either \'bpe\' or \'wordpiece\'\n        """"""\n        self.use_subword = use_subword\n\n        if self.use_subword == \'bpe\':\n            vectorizer = BPEVectorizer1D(model_file=model_file, vocab_file=vocab_file)\n        elif self.use_subword == \'wordpiece\':\n            vectorizer = WordpieceVectorizer1D(embed_file=model_file, vocab_file=vocab_file,\n                                               special_tokens=special_tokens)\n        else:\n            vectorizer = Token1DVectorizer(transform_fn=baseline.lowercase)\n        super().__init__(nctx, {\'x\': vectorizer})\n\n    def build_vocab(self, files):\n        """"""Read the vocab file to get the tokens\n\n        :param files:\n        :return:\n        """"""\n        if self.use_subword is not None:\n            super().build_vocab(files)\n            return {\'x\': self.vectorizers[\'x\'].vocab}\n        return super().build_vocab(files)\n\n    def load(self, filename, vocabs):\n        features = self.load_features(filename, vocabs)\n        x_tensor = torch.tensor(features[\'x\'], dtype=torch.long)\n        num_sequences_word = (x_tensor.size(0) // self.nctx) * self.nctx\n        x_tensor = x_tensor.narrow(0, 0, num_sequences_word).view(-1, self.nctx)\n        return TensorDataset(x_tensor, x_tensor)\n\n\nclass TensorCharDatasetReader(TensorDatasetReaderBase):\n    """"""TensorCharDatasetReader reads in a vocab and then a dataset and returns as a `dict` of `string` to `ndarray`\n    """"""\n    def __init__(self, nctx, chars_per_word):\n        y_vectorizer = Token1DVectorizer(transform_fn=baseline.lowercase)\n        x_vectorizer = Char2DVectorizer(mxwlen=chars_per_word)\n        super(TensorCharDatasetReader, self).__init__(nctx, {\'x\': x_vectorizer, \'y\': y_vectorizer})\n        self.chars_per_word = chars_per_word\n\n    def load(self, filename, vocabs):\n        features = self.load_features(filename, vocabs)\n        y_tensor = torch.tensor(features[\'y\'], dtype=torch.long)\n        num_sequences_word = (y_tensor.size(0) // self.nctx) * self.nctx\n        y_tensor = y_tensor.narrow(0, 0, num_sequences_word).view(-1, self.nctx)\n\n        x_dataset = torch.tensor(features[\'x\'], dtype=torch.long)\n        x_tensor = torch.tensor(x_dataset, dtype=torch.long)\n        x_tensor = x_tensor.narrow(0, 0, num_sequences_word)\n        x_tensor = x_tensor.view(-1, self.nctx, self.chars_per_word)\n        return TensorDataset(x_tensor, y_tensor)\n\n\ndef load_data_caching(token_type, reader, dataset, file_key, vocabs, caching, logger):\n    cached_file = \'{}-{}.cache\'.format(dataset[file_key], token_type)\n    if caching and os.path.exists(cached_file):\n        logger.info(""Reloading %s from cached file [%s]"", file_key, cached_file)\n        loaded = torch.load(cached_file)\n    else:\n        # if build_vocab() was never run, need to run it to set reader.vectorizer.mxlen correctly\n        if reader.vectorizers[\'x\'].mxlen == -1:\n            _ = reader.build_vocab([dataset[file_key]])\n        loaded = reader.load(dataset[file_key], vocabs)\n        logger.info(""Caching %s to [%s]"", file_key, cached_file)\n        torch.save(loaded, cached_file)\n    return loaded\n\n\nclass MultiFileLoader(IterableDataset):\n\n    def __init__(self, directory, pattern, vocabs, vectorizer, nctx, last_turn_only=True, distribute=True, shuffle=True):\n        super().__init__()\n        self.vectorizer = vectorizer\n        self.pattern = pattern\n        self.nctx = nctx\n        self.directory = directory\n        self.vocab = vocabs\n        self.samples = 0\n        self.rank = 0\n        self.world_size = 1\n        self.shuffle = shuffle\n        self.last_turn_only = last_turn_only\n        self.distribute = distribute\n        if torch.distributed.is_initialized() and distribute:\n            self.rank = torch.distributed.get_rank()\n            self.world_size = torch.distributed.get_world_size()\n\n        if os.path.exists(f""{directory}/md.yml""):\n            f = read_yaml(f""{directory}/md.yml"")\n            self.samples = f[\'num_samples\']\n        else:\n            files = list(glob.glob(f""{directory}/{self.pattern}""))\n            pg = create_progress_bar(len(files))\n            for file in pg(files):\n                with open(file) as rf:\n                    for _ in rf:\n                        self.samples += 1\n            write_yaml({\'num_samples\': self.samples}, f""{directory}/md.yml"")\n\n    def __len__(self):\n        return self.samples\n\n    def _get_worker_info(self):\n        return torch.utils.data.get_worker_info() if self.distribute else None\n\n    def __iter__(self):\n        # Each node has the same worker_info, so the unique offsets for each is\n        # rank * num_workers + worker_id\n        # and the total available workers is world_size * num_workers\n        worker_info = self._get_worker_info()\n        files = sorted(list(glob.glob(f""{self.directory}/{self.pattern}"")))\n\n        if worker_info is None:\n            num_workers_per_node = 1\n            node_worker_id = 0\n        else:\n            num_workers_per_node = worker_info.num_workers\n            node_worker_id = worker_info.id\n        all_workers = (self.world_size * num_workers_per_node)\n        offset = self.rank * num_workers_per_node + node_worker_id\n        self.vectorizer.mxlen = self.nctx\n        read_file_order = list(range(offset, len(files), all_workers))\n        # If we have multiple files per worker, possibly shuffle the file read order\n        if self.shuffle:\n            read_file_order = np.random.permutation(read_file_order)\n\n        for file_idx in read_file_order:\n            file = files[file_idx]\n            with open(file) as rf:\n                lines = rf.readlines()\n                if self.shuffle:\n                    random.shuffle(lines)\n                for l in lines:\n                    response = self.process_line(l)\n                    yield response\n\n    def process_line(self, line):\n        """"""Read in a line and turn it into an entry\n\n        The entries will get collated by the data loader\n\n        :param line:\n        :return:\n        """"""\n\n\nclass NextTurnPredictionFileLoader(MultiFileLoader):\n\n    def process_line(self, line):\n        pair = line.strip().split(\'\\t\')\n        # Unfortunately, this occassionally happens, a bunch of blank turns etc.\n        if len(pair) != 2:\n            return None\n        q, r = pair\n        if q == \'\' or r == \'\':\n            return None\n        if self.last_turn_only:\n            turns = q.split(\'<EOU>\')\n            q = turns[-1] if turns[-1].strip() != \'\' else turns[-2]\n            if q.strip() == \'\':\n                return None\n            q_vec, q_valid_lengths = self.vectorizer.run(q.split(), self.vocab)\n        else:\n            q_vec, q_valid_lengths = self.vectorizer.run(reversed(q.split()), self.vocab)\n            q_vec = np.roll(q_vec[::-1], -(self.vectorizer.mxlen - q_valid_lengths))\n\n        r_vec, r_valid_lengths = self.vectorizer.run(r.split(), self.vocab)\n        return q_vec, r_vec\n\n\ndef on_demand_mlm_masking(inputs, labels, mask_value, vocab_size):\n    # Replace 15% of tokens\n    masked_indices = torch.bernoulli(torch.full(labels.shape, 0.15)).type(torch.bool)\n    # Anything not masked is 0 so no loss\n    labels[~masked_indices] = 0\n    # Of the masked items, mask 80% of them with [MASK]\n    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).type(torch.bool) & masked_indices\n    inputs[indices_replaced] = mask_value\n    # Replace 10% of them with random words, rest preserved for auto-encoding\n    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).type(\n        torch.bool) & masked_indices & ~indices_replaced\n    random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long, device=labels.device)\n    inputs[indices_random] = random_words[indices_random]\n    return inputs, labels\n\nclass SequencePredictionFileLoader(MultiFileLoader):\n\n    def process_line(self, line):\n        line = line.strip()\n        if not line:\n            return None\n\n        vec, valid_lengths = self.vectorizer.run(line.split(), self.vocab)\n        if valid_lengths < 2:\n            return None\n        return vec, vec\n\n\nclass PreprocessedFileLoader(MultiFileLoader):\n\n    def process_line(self, line):\n        obj = json.loads(line)\n        return np.array(obj[\'x\'], dtype=int), np.array(obj[\'y\'], dtype=int)\n\n\nclass NextSequencePredictionFileLoader(MultiFileLoader):\n\n    def process_line(self, line):\n        line = line.strip()\n        if not line:\n            return None\n        vec, valid_lengths = self.vectorizer.run(line.split(), self.vocab)\n        if valid_lengths < 2:\n            return None\n        pair_entry_length = self.vectorizer.mxlen//2\n        end_of_query = min(valid_lengths//2, pair_entry_length)\n        # Front half is all tokens up until the half_way marker\n        # Create a new query vector\n        query = np.zeros(pair_entry_length, dtype=np.int)\n        query[:end_of_query] = vec[:end_of_query]\n        # Repurpose the existing vector as the response vector\n        vec = vec[end_of_query:end_of_query+pair_entry_length]\n        return query, vec\n\n\nclass MultiFileDatasetReader:\n    """"""Provide a base-class to do operations that are independent of token representation\n    """"""\n\n    def __init__(self, nctx=64, model_file=None, vocab_file=None, pattern=\'*.txt\', reader_type=""ntp""):\n        self.nctx = nctx\n        self.pattern = pattern\n        self.reader_type = reader_type\n        self.vectorizer = BPEVectorizer1D(model_file=model_file, vocab_file=vocab_file, mxlen=nctx)\n\n    def build_vocab(self, _=None):\n        return {\'x\': self.vectorizer.vocab}\n\n    def load(self, directory, vocabs, distribute=True, shuffle=True):\n        reader_type = self.reader_type.lower()\n        if reader_type == ""ntp"":\n            return NextTurnPredictionFileLoader(directory, self.pattern, vocabs, self.vectorizer, self.nctx, distribute=distribute, shuffle=shuffle)\n        elif reader_type == ""nsp"":\n            return NextSequencePredictionFileLoader(directory, self.pattern, vocabs, self.vectorizer, 2*self.nctx, distribute=distribute, shuffle=shuffle)\n        elif reader_type == ""lang"":\n            print(""Using files as an LM"")\n            return SequencePredictionFileLoader(directory, self.pattern, vocabs, self.vectorizer, self.nctx, distribute=distribute, shuffle=shuffle)\n        return PreprocessedFileLoader(directory, self.pattern, vocabs, self.vectorizer, self.nctx, distribute=distribute, shuffle=shuffle)\n\n\nclass TiedEmbeddingsSeq2SeqModel(Seq2SeqModel):\n\n    def __init__(self, tied_embeddings, **kwargs):\n        super().__init__({\'x\': tied_embeddings}, tied_embeddings, **kwargs)\n\n    def input_tensor(self, key, batch_dict, perm_idx):\n        tensor = batch_dict[key]\n        tensor = self.drop_inputs(key, tensor)\n        tensor = tensor[perm_idx]\n        return tensor\n\n    def make_input(self, batch_dict, perm=False):\n        """"""Prepare the input.\n\n        :param batch_dict: `dict`: The data.\n        :param perm: `bool`: If True return the permutation index\n            so that you can undo the sort if you want.\n        """"""\n        example = dict({})\n\n        lengths = batch_dict[self.src_lengths_key]\n        lengths, perm_idx = lengths.sort(0, descending=True)\n\n        example[\'src_len\'] = lengths\n        for key in self.src_embeddings.keys():\n            example[key] = self.input_tensor(key, batch_dict, perm_idx)\n\n        if \'tgt\' in batch_dict:\n            tgt = batch_dict[\'tgt\']\n            example[\'dst\'] = torch.cat([torch.full((tgt.shape[0], 1), Offsets.GO, device=tgt.device, dtype=tgt.dtype), tgt[:, :-1]], 1)\n            example[\'tgt\'] = tgt\n            example[\'dst\'] = example[\'dst\'][perm_idx]\n            example[\'tgt\'] = example[\'tgt\'][perm_idx]\n        if perm:\n            return example, perm_idx\n        return example\n\n    def create_loss(self, _=None):\n        loss = super().create_loss()\n\n        class LossFn(nn.Module):\n            def __init__(self, model: nn.Module, l: nn.Module):\n                super().__init__()\n                self._loss = l\n                self.model = model\n\n            def forward(self, inputs, targets):\n                lengths = torch.sum(inputs != 0, 1)\n                in_ = self.model.make_input({""x"": inputs, ""x_lengths"": lengths,  ""tgt"": targets})\n                targets = in_[\'tgt\']\n                pred = self.model(in_)\n                return self._loss(pred, targets)\n        return LossFn(self, loss)\n\n\ndef create_model(embeddings, d_model, d_ff, dropout, num_heads, num_layers, model_type, rpr_k, d_k, reduction_d_k,\n                 stacking_layers, ff_pdrop, logger):\n    if model_type == ""encoder-decoder"":\n        logger.info(""Creating tied encoder decoder model"")\n        hps = {""dsz"": d_model,\n               ""hsz"": d_model,\n               ""d_ff"": d_ff,\n               ""dropout"": dropout,\n               ""num_heads"": num_heads,\n               ""layers"": num_layers,\n               ""encoder_type"": ""transformer"",\n               ""decoder_type"": ""transformer"",\n               ""src_lengths_key"": ""x_lengths"",\n               ""d_k"": d_k,\n               ""rpr_k"": rpr_k}\n        model = TiedEmbeddingsSeq2SeqModel(embeddings, **hps)\n    else:\n        model = PairedModel(embeddings, d_model, d_ff, dropout, num_heads, num_layers, rpr_k=rpr_k, d_k=d_k,\n                            reduction_d_k=reduction_d_k, stacking_layers=stacking_layers, ff_pdrop=ff_pdrop)\n\n    logger.info(model)\n    return model\n\n\ndef get_lr_decay(sched_type, lr, steps_per_epoch, n_epochs, logger, decay_steps=None, decay_rate=None, alpha=None):\n    if sched_type == \'cosine\':\n        decay_steps = decay_steps if decay_steps else steps_per_epoch * n_epochs\n        alpha = alpha if alpha else 0.\n        params = {\'decay_steps\': decay_steps, \'alpha\': alpha}\n    else:\n        decay_steps = decay_steps if decay_steps else steps_per_epoch\n        if not decay_rate:\n            if sched_type == \'exponential\':\n                decay_rate = 0.5\n            elif sched_type == \'invtime\':\n                decay_rate = 1.0\n        params = {\'decay_steps\': decay_steps, \'decay_rate\': decay_rate}\n    lr_decay = create_lr_scheduler(lr_scheduler_type=sched_type, lr=lr, **params)\n    logger.info(f""Using {sched_type} decay learning rate with params {params}."")\n    return lr_decay\n'"
baseline/__init__.py,0,"b""from baseline.version import __version__\nfrom baseline.utils import *\nfrom baseline.vectorizers import *\nfrom baseline.confusion import *\nfrom baseline.data import *\nfrom baseline.reader import *\nfrom baseline.progress import *\nfrom baseline.reporting import *\nfrom baseline.model import *\nfrom baseline.embeddings import *\nfrom baseline.services import *\nfrom baseline.train import *\nlogger = get_console_logger('baseline', env_key='BASELINE_LOG_LEVEL')\nreport_logger = get_console_logger('baseline.reporting', env_key='BASELINE_LOG_LEVEL')\n"""
baseline/bleu.py,0,b'import eight_mile.bleu\nfrom eight_mile.bleu import *\nfrom eight_mile.bleu import main\n\n__all__ = []\n__all__.extend(eight_mile.bleu.__all__)\n'
baseline/confusion.py,0,b'import eight_mile.confusion\nfrom eight_mile.confusion import *\n\n__all__ = []\n__all__.extend(eight_mile.confusion.__all__)\n'
baseline/conlleval.py,0,"b'""""""An implementation of conlleval.pl https://www.clips.uantwerpen.be/conll2000/chunking/output.html\n\nInput is a conll file with the rightmost columns being gold and predicted tags\nin that order. Sentences are separated by a blank line.\n\nSome parts of the perl script are not replicated (this doesn\'t generate a latex\ntable) but there are other improvements like it supports files in the `IOBES`\nformat.\n\nThe script produces the same output as conlleval.pl for BIO and IOB tagged file.\nWhen running on IOBES files it will produce the same F1 scores as if you\nconverted the file to BIO and run conlleval.pl however the accuracy will be\nslightly different IOBES accuracy will always equal to or lower than BIO score.\n\nThis difference is because if the error is on a B-, I-, or O token then the\nerror will be in both IOBES and BIO. In the conversion from IOBES to BIO then\nS- is converted to B- and E- to I- if the IOBES was correct then they will be\nconverted and will still be correct. If the token was wrong in that is said\nit was O or something then it will be wrong after the conversion too. If it was\nwrong in that the S- was tagged as B- or the E- was an I- (this is possible and\na common error) then when the gold is changed these will become correct. So it\nis only possible that the conversion will make some answers correct. It won\'t\nmake anything that was correct wrong.\n""""""\n\nimport sys\nimport argparse\nfrom itertools import chain\nfrom baseline.utils import to_chunks, per_entity_f1, conlleval_output, read_conll_sentences\n\n\ndef _read_conll_file(f, delim):\n    """"""Read a golds and predictions out of a conll file.\n\n    :param f: `file` The open file object.\n    :param delim: `str` The symbol that separates columns in the file.\n\n    :returns: `Tuple[List[List[str]], List[List[str]]]` The golds\n        and the predictions. They are aligned lists and each element\n        is a List of strings that are the list of tags.\n\n    Note:\n        the file should contain lines with items separated\n        by $delimiter characters (default space). The final\n        two items should contain the correct tag and the\n        guessed tag in that order. Sentences should be\n        separated from each other by empty lines.\n    """"""\n    golds = []\n    preds = []\n    for lines in read_conll_sentences(f, delim=delim):\n        golds.append([l[-2] for l in lines])\n        preds.append([l[-1] for l in lines])\n    return golds, preds\n\n\ndef _get_accuracy(golds, preds):\n    """"""Calculate the token level accuracy.\n\n    :param golds: `List[List[str]]` The list of golds of each example.\n    :param preds: `List[List[str]]` The list of predictions of each example.\n\n    :returns: `Tuple[float, int]` The Accuracy and the total number of tokens.\n    """"""\n    total = 0\n    correct = 0\n    for g, p in zip(chain(*golds), chain(*preds)):\n        if g == p:\n            correct += 1\n        total += 1\n    return correct / float(total) * 100, total\n\n\ndef _get_entites(golds, preds, span_type=\'iobes\', verbose=False):\n    """""" Convert the tags into sets of entities.\n\n    :param golds: `List[List[str]]` The list of gold tags.\n    :param preds: `List[List[str]]` The list of predicted tags.\n    :param span_type: `str` The span labeling scheme used.\n    :param verbose: `bool` Should warnings be printed when an illegal transistion is found.\n    """"""\n    golds = [set(to_chunks(g, span_type, verbose)) for g in golds]\n    preds = [set(to_chunks(p, span_type, verbose)) for p in preds]\n    return golds, preds\n\n\ndef main():\n    """"""Use as a cli tool like conlleval.pl""""""\n    usage = ""usage: %(prog)s [--span_type {bio,iobes,iob}] [-d delimiterTag] [-v] < file""\n    parser = argparse.ArgumentParser(description=""Calculate Span level F1 from the CoNLL-2000 shared task."", usage=usage)\n    parser.add_argument(""--span_type"", default=""iobes"", choices={""iobes"", ""iob"", ""bio""}, help=""What tag annotation scheme is this file using."")\n    parser.add_argument(""--delimiterTag"", ""-d"", default="" "", help=""The separator between items in the file."")\n    parser.add_argument(""--verbose"", \'-v\', action=""store_true"", help=""Output warnings when there is an illegal transition."")\n    args = parser.parse_args()\n\n    golds, preds = _read_conll_file(sys.stdin, args.delimiterTag)\n    acc, tokens = _get_accuracy(golds, preds)\n    golds, preds = _get_entites(golds, preds, args.span_type, args.verbose)\n    metrics = per_entity_f1(golds, preds)\n    metrics[\'acc\'] = acc\n    metrics[\'tokens\'] = tokens\n    print(conlleval_output(metrics))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
baseline/data.py,0,"b'import random\nimport logging\nimport numpy as np\nimport math\nfrom baseline.utils import exporter\n\n__all__ = []\nexport = exporter(__all__)\nlogger = logging.getLogger(\'baseline\')\n\n\n@export\nclass DataFeed(object):\n    """"""Data collection that, when iterated, produces an epoch of data\n\n    This class manages producing a dataset to the trainer, by iterating an epoch and producing\n    a single step at a time.  The data can be shuffled per epoch, if requested, otherwise it is\n    returned in the order of the dateset\n    """"""\n    def __init__(self):\n        self.steps = 0\n        self.shuffle = False\n\n    def _batch(self, i):\n        pass\n\n    def __getitem__(self, i):\n        return self._batch(i)\n\n    def __iter__(self):\n        shuffle = np.random.permutation(np.arange(self.steps)) if self.shuffle else np.arange(self.steps)\n        for i in range(self.steps):\n            si = shuffle[i]\n            yield self._batch(si)\n\n    def __len__(self):\n        return self.steps\n\n\n@export\nclass ExampleDataFeed(DataFeed):\n\n    """"""Abstract base class that works on a list of examples\n\n    """"""\n    def __init__(self, examples, batchsz, **kwargs):\n        """"""Constructor from a list of examples\n\n        Use the examples requested to provide data.  Options for batching and shuffling are supported,\n        along with some optional processing function pointers\n\n        :param examples: A list of examples\n        :param batchsz: Batch size per step\n        :param kwargs: See below\n\n        :Keyword Arguments:\n            * *shuffle* -- Shuffle the data per epoch? Defaults to `False`\n            * *vec_alloc* -- Allocate a new tensor.  Defaults to ``numpy.zeros``\n            * *vec_shape* -- Function to retrieve tensor shape.  Defaults to ``numpy.shape``\n            * *trim* -- Trim batches to the maximum length seen in the batch (defaults to `False`)\n                This can lead to batches being shorter than the maximum length provided to the system.\n                Not supported in all frameworks.\n            * *src_vec_trans* -- A transform function to use on the source tensor (`None`)\n            * *truncate* -- bool, If true the datastream will be cut short when\n                a full batch cannot be made, otherwise the final batch is smaller\n                than normal batches.\n        """"""\n        super(ExampleDataFeed, self).__init__()\n\n        self.examples = examples\n        self.batchsz = batchsz\n        self.shuffle = bool(kwargs.get(\'shuffle\', False))\n        self.truncate = bool(kwargs.get(\'truncate\', False))\n        if self.truncate:\n            self.steps = int(math.floor(len(self.examples) / float(batchsz)))\n        else:\n            self.steps = (len(self.examples) + batchsz - 1) // batchsz\n        self.trim = bool(kwargs.get(\'trim\', False))\n\n    def _batch(self, i):\n        """"""\n        Get a batch of data at step `i`\n        :param i: (``int``) step index\n        :return: A batch tensor x, batch tensor y\n        """"""\n        batch = self.examples.batch(i, self.batchsz, trim=self.trim)\n        return batch\n\n\n@export\nclass DictExamples(object):\n    """"""This object holds a list of dictionaries, and knows how to shuffle, sort and batch them\n    """"""\n    def __init__(self, example_list, do_shuffle=True, sort_key=None):\n        """"""Constructor\n\n        :param example_list:  A list of examples\n        :param do_shuffle: (``bool``) Shuffle the data? Defaults to `True`\n        :param do_sort: (``bool``) Sort the data.  Defaults to `True`\n        """"""\n        self.example_list = example_list\n\n        if do_shuffle:\n            random.shuffle(self.example_list)\n\n        if sort_key is not None:\n            self.example_list = sorted(self.example_list, key=lambda x: x[sort_key])\n\n        self.sort_key = sort_key\n\n    def __getitem__(self, i):\n        """"""Get a single example\n\n        :param i: (``int``) simple index\n        :return: an example\n        """"""\n        return self.example_list[i]\n\n    def __len__(self):\n        """"""Number of examples\n\n        :return: (``int``) length of data\n        """"""\n        return len(self.example_list)\n\n    def _trim_batch(self, batch, keys, max_src_len):\n        if max_src_len == 0:\n            return batch\n        for k in keys:\n            if len(batch[k].shape) == 3:\n                batch[k] = batch[k][:, 0:max_src_len, :]\n            elif len(batch[k].shape) == 2:\n                batch[k] = batch[k][:, :max_src_len]\n        return batch\n\n    def batch(self, start, batchsz, trim=False):\n\n        """"""Get a batch of data\n\n        :param start: (``int``) The step index\n        :param batchsz: (``int``) The batch size\n        :param trim: (``bool``) Trim to maximum length in a batch\n        :return batched dictionary\n        """"""\n        ex = self.example_list[start]\n        keys = ex.keys()\n        batch = {}\n\n        for k in keys:\n            batch[k] = []\n        sz = len(self.example_list)\n        idx = start * batchsz\n        max_src_len = 0\n\n        for i in range(batchsz):\n            if idx >= sz:\n                break\n\n            ex = self.example_list[idx]\n            for k in keys:\n                batch[k].append(ex[k])\n\n            # Trim all batches along the sort_key if it exists\n            if trim and self.sort_key is not None:\n                max_src_len = max(max_src_len, ex[self.sort_key])\n            idx += 1\n\n        for k in keys:\n            batch[k] = np.stack(batch[k])\n        return self._trim_batch(batch, keys, max_src_len) if trim else batch\n\n\n@export\nclass Seq2SeqExamples(object):\n\n    """"""Paired training examples\n    """"""\n    def __init__(self, example_list, do_shuffle=True, src_sort_key=None):\n        """"""Constructor\n\n        :param example_list: Training pair examples\n        :param do_shuffle: Shuffle the data (defaults to `True`)\n        :param do_sort: Sort the data (defaults to `True`)\n        """"""\n        self.example_list = example_list\n        if do_shuffle:\n            random.shuffle(self.example_list)\n        if src_sort_key is not None:\n            self.example_list = sorted(self.example_list, key=lambda x: x[src_sort_key])\n        self.src_sort_key = src_sort_key\n\n    def __getitem__(self, i):\n        """"""Get `ith` example\n\n        :param i: (``int``) index of example\n        :return: example dict\n        """"""\n        return self.example_list[i]\n\n    def __len__(self):\n        return len(self.example_list)\n\n    def _trim_batch(self, batch, max_src_len, max_tgt_len):\n        for k in batch.keys():\n            max_len = max_src_len\n            if k == \'tgt\':\n                max_len = max_tgt_len\n\n            if max_len == 0:\n                continue\n            if len(batch[k].shape) == 3:\n                batch[k] = batch[k][:, 0:max_len, :]\n            elif len(batch[k].shape) == 2:\n                batch[k] = batch[k][:, :max_len]\n\n        return batch\n\n    def batch(self, start, batchsz, trim=False):\n\n        """"""Get a batch of data\n\n        :param start: (``int``) The step index\n        :param batchsz: (``int``) The batch size\n        :param trim: (``bool``) Trim to maximum length in a batch\n        :param vec_alloc: A vector allocator\n        :param vec_shape: A vector shape function\n        :return: batched `x` word vector, `x` character vector, batched `y` vector, `length` vector, `ids`\n        """"""\n        ex = self.example_list[start]\n        keys = ex.keys()\n        batch = {}\n\n        for k in keys:\n            batch[k] = []\n        sz = len(self.example_list)\n        idx = start * batchsz\n        max_src_len = 0\n        max_tgt_len = 0\n        for i in range(batchsz):\n            if idx >= sz:\n                break\n\n            ex = self.example_list[idx]\n            for k in keys:\n                batch[k].append(ex[k])\n\n            # Trim all batches along the sort_key if it exists\n            if trim and self.src_sort_key is not None:\n                max_src_len = max(max_src_len, ex[self.src_sort_key])\n\n            if trim:\n                max_tgt_len = max(max_tgt_len, ex[\'tgt_lengths\'])\n\n            idx += 1\n\n        for k in keys:\n            batch[k] = np.stack(batch[k])\n        return self._trim_batch(batch, max_src_len, max_tgt_len) if trim else batch\n\n\n# This one is a little different at the moment\n@export\nclass SeqWordCharDataFeed(DataFeed):\n    """"""Data feed to return language modeling training data\n    """"""\n\n    def __init__(self, examples, nctx, batchsz, tgt_key=None):\n        """"""Constructor\n        :param examples: word tensor\n        :param nctx: Number of steps of BPTT\n        :param batchsz: Batch size\n        :param tgt_key: Which field to treat as the target key (this will share an embedding vocab with the source)\n        """"""\n        super().__init__()\n        self.examples = dict()\n        # This identifies which vector to use for targets\n        self.tgt_key = \'x\' if tgt_key is None else tgt_key\n        num_examples = examples[\'{}_dims\'.format(tgt_key)][0]\n        rest = num_examples // batchsz\n        self.steps = rest // nctx\n        # if num_examples is divisible by batchsz * nctx (equivalent to rest is divisible by nctx), we\n        # have a problem. reduce rest in that case.\n\n        if rest % nctx == 0:\n            rest = rest-1\n\n        for k in examples.keys():\n            if k.endswith(\'_dims\'):\n                continue\n            dim_key = \'{}_dims\'.format(k)\n            shp = examples[dim_key]\n            if len(shp) == 2:\n                width = shp[1]\n            else:\n                width = 1\n            trunc = batchsz * rest * width\n            vec = examples[k].reshape(-1)[:trunc]\n            self.examples[k] = vec.reshape((batchsz, rest * width))\n\n            logger.info(\'Truncating %s from %d to %d\', k, num_examples, trunc)\n            self.examples[k].flatten()\n            self.examples[dim_key] = shp\n        self.nctx = nctx\n        self.batchsz = batchsz\n\n    def _batch(self, i):\n\n        example = {}\n        for k in self.examples.keys():\n            if k.endswith(\'_dims\'):\n                continue\n            x = self.examples[k]\n            dims = self.examples[\'{}_dims\'.format(k)]\n            if len(dims) == 1:\n                width = 1\n            else:\n                width = dims[1]\n\n            example[k] = x[:, i*self.nctx * width:(i + 1) * self.nctx * width]\n\n            if len(dims) == 1:\n                reshape_dims = (self.batchsz, self.nctx)\n            else:\n                reshape_dims = (self.batchsz, self.nctx, width)\n            example[k] = example[k].reshape(reshape_dims)\n            if self.tgt_key == k:\n                example[\'y\'] = x[:, i*self.nctx * width + 1:(i + 1) * self.nctx * width + 1].reshape(reshape_dims)\n\n        return example\n        #return {\n        #    \'x\': self.x[:, i*self.nbptt:(i+1)*self.nbptt].reshape((self.batchsz, self.nbptt)),\n        #    \'xch\': self.xch[:, i*self.stride_ch:(i+1)*self.stride_ch].reshape((self.batchsz, self.nbptt, self.wsz)),\n        #    \'y\': self.x[:, i*self.nbptt+1:(i+1)*self.nbptt+1].reshape((self.batchsz, self.nbptt))\n        #}\n'"
baseline/embeddings.py,0,"b'import eight_mile.embeddings\nfrom eight_mile.embeddings import *\nfrom eight_mile.utils import exporter, optional_params, listify, idempotent_append, is_sequence\nfrom baseline.utils import import_user_module, AddonDownloader\nimport logging\n\n__all__ = []\n__all__.extend(eight_mile.embeddings.__all__)\nexport = exporter(__all__)\nlogger = logging.getLogger(""mead.layers"")\n\n\nMEAD_LAYERS_EMBEDDINGS = {}\nMEAD_LAYERS_EMBEDDINGS_LOADERS = {}\n\n\n@export\n@optional_params\ndef register_embeddings(cls, name=None):\n    """"""Register a function as a plug-in""""""\n    if name is None:\n        name = cls.__name__\n\n    if name in MEAD_LAYERS_EMBEDDINGS:\n        raise Exception(\n            ""Error: attempt to re-define previously registered handler {} (old: {}, new: {}) in registry"".format(\n                name, MEAD_LAYERS_EMBEDDINGS[name], cls\n            )\n        )\n\n    MEAD_LAYERS_EMBEDDINGS[name] = cls\n\n    if hasattr(cls, ""load""):\n        MEAD_LAYERS_EMBEDDINGS_LOADERS[name] = cls.load\n    return cls\n\n\n@export\ndef create_embeddings(**kwargs):\n    embed_type = kwargs.get(""embed_type"", ""default"")\n    Constructor = MEAD_LAYERS_EMBEDDINGS.get(embed_type)\n    return Constructor(**kwargs)\n\n\n\n@export\ndef load_embeddings(name, **kwargs):\n    """"""This method negotiates loading an embeddings sub-graph AND a corresponding vocabulary (lookup from word to int)\n\n    Embeddings and their addons may be downloaded from an http `GET` either via raw URL or using hub notation\n    (hub:v1:embeddings/hub:v1:addons)\n\n    This function behaves differently depending on its keyword arguments and the `embed_type`.\n    If the registered embeddings class contains a load method on it and we are given an `embed_file`,\n    we will assume that we need to load that file, and that the embeddings object wants its own load function used\n    for that.  This would be typical, e.g, for a user-defined sub-graph LM.\n\n    For cases where no `embed_file` is provided and there is a `create` method on this class, we  assume that the user\n    wants us to build a VSM (`baseline.embeddings.PretrainedEmbeddingsModel`) ourselves, and call\n    their create function, which will take in this VSM.\n\n    The VSM is then used to provide the vocabulary back, and the `create` function invokes the class constructor\n    with the sub-parts of VSM required to build the graph.\n\n    If there is no create method provided, and there is no load function provided, we simply invoke the\n    registered embeddings\' constructor with the args, and assume there is a `get_vocab()` method on the\n    provided implementation\n\n    :param name: A unique string name for these embeddings\n    :param kwargs:\n\n    :Keyword Arguments:\n    * *embed_type*  The key identifying the embedding type in the registry\n    :return:\n    """"""\n    embed_type = kwargs.pop(""embed_type"", ""default"")\n    # Dynamically load a module if its needed\n    for module in listify(kwargs.get(\'module\', kwargs.get(\'modules\', []))):\n        import_user_module(module, kwargs.get(\'data_download_cache\'))\n    embeddings_cls = MEAD_LAYERS_EMBEDDINGS[embed_type]\n\n    filename = kwargs.get(""embed_file"")\n\n    # If the embedding model has a load function, defer all the work to that.  Basically just pass the kwargs in\n    # and let it do its magic\n    if hasattr(embeddings_cls, ""load"") and filename is not None:\n        model = embeddings_cls.load(filename, **kwargs)\n        return {""embeddings"": model, ""vocab"": model.get_vocab()}\n    # If there isnt a load function, there must be a create() function where the first arg is a type of\n    # EmbeddingsModel\n    elif hasattr(embeddings_cls, ""create""):\n        unif = kwargs.pop(""unif"", 0.1)\n        known_vocab = kwargs.pop(""known_vocab"", None)\n        keep_unused = kwargs.pop(""keep_unused"", False)\n        normalize = kwargs.pop(""normalized"", False)\n        preserve_vocab_indices = bool(kwargs.get(\'preserve_vocab_indices\', False))\n\n        # if there is no filename, use random-init model\n        if filename is None:\n            dsz = kwargs.pop(""dsz"")\n            model = RandomInitVecModel(dsz, known_vocab=known_vocab, unif_weight=unif, counts=not preserve_vocab_indices)\n        # If there, is use the PretrainedEmbeddingsModel loader\n        else:\n            if is_sequence(filename):\n                model = PretrainedEmbeddingsStack(\n                    listify(filename),\n                    known_vocab=known_vocab,\n                    normalize=normalize,\n                    counts=not preserve_vocab_indices,\n                    **kwargs\n                )\n            else:\n                model = PretrainedEmbeddingsModel(\n                    filename,\n                    known_vocab=known_vocab,\n                    unif_weight=unif,\n                    keep_unused=keep_unused,\n                    normalize=normalize,\n                    **kwargs,\n                )\n\n        # Then call create(model, name, **kwargs)\n        return {""embeddings"": embeddings_cls.create(model, name, **kwargs), ""vocab"": model.get_vocab()}\n    # If we dont have a load function, but filename is none, we should just instantiate the class\n    model = embeddings_cls(name, **kwargs)\n    return {""embeddings"": model, ""vocab"": model.get_vocab()}\n\n'"
baseline/mime_type.py,0,b'import eight_mile.mime_type\nfrom eight_mile.mime_type import *\n\n__all__ = []\n__all__.extend(eight_mile.mime_type.__all__)\n'
baseline/model.py,0,"b'import logging\nimport numpy as np\nfrom baseline.utils import (\n    exporter, optional_params, listify, register, import_user_module, read_json\n)\n\n__all__ = []\nexport = exporter(__all__)\nlogger = logging.getLogger(\'baseline\')\n\nBASELINE_MODELS = {}\nBASELINE_LOADERS = {}\n\n\n@export\n@optional_params\ndef register_model(cls, task, name=None):\n    """"""Register a function as a plug-in""""""\n    if name is None:\n        name = cls.__name__\n\n    names = listify(name)\n\n    if task not in BASELINE_MODELS:\n        BASELINE_MODELS[task] = {}\n\n    if task not in BASELINE_LOADERS:\n        BASELINE_LOADERS[task] = {}\n\n    if hasattr(cls, \'create\'):\n        def create(*args, **kwargs):\n            return cls.create(*args, **kwargs)\n    else:\n        def create(*args, **kwargs):\n            return cls(*args, **kwargs)\n\n    for alias in names:\n        if alias in BASELINE_MODELS[task]:\n            raise Exception(\'Error: attempt to re-define previously registered handler {} (old: {}, new: {}) for task {} in registry\'.format(alias, BASELINE_MODELS[task], cls, task))\n\n        BASELINE_MODELS[task][alias] = create\n\n        if hasattr(cls, \'load\'):\n            BASELINE_LOADERS[task][alias] = cls.load\n    return cls\n\n\n@export\ndef create_model_for(activity, **kwargs):\n    model_type = kwargs.get(\'type\', kwargs.get(\'model_type\', \'default\'))\n    creator_fn = BASELINE_MODELS[activity][model_type]\n    logger.info(\'Calling model %s\', creator_fn)\n    input_ = kwargs.pop(\'features\', None)\n    output_ = kwargs.pop(\'labels\', None)\n    if output_ is not None:\n        return creator_fn(input_, output_, **kwargs)\n    return creator_fn(input_, **kwargs)\n\n\n@export\ndef create_model(embeddings, labels, **kwargs):\n    return create_model_for(\'classify\', features=embeddings, labels=labels, **kwargs)\n\n\n@export\ndef create_tagger_model(embeddings, labels, **kwargs):\n    return create_model_for(\'tagger\', features=embeddings, labels=labels, **kwargs)\n\n\n\nBASELINE_SEQ2SEQ_ENCODERS = {}\n\n@export\n@optional_params\ndef register_encoder(cls, name=None):\n    """"""Register a function as a plug-in""""""\n    return register(cls, BASELINE_SEQ2SEQ_ENCODERS, name, \'encoder\')\n\n\nBASELINE_SEQ2SEQ_DECODERS = {}\n\n@export\n@optional_params\ndef register_decoder(cls, name=None):\n    """"""Register a function as a plug-in""""""\n    return register(cls, BASELINE_SEQ2SEQ_DECODERS, name, \'decoder\')\n\n\nBASELINE_SEQ2SEQ_ARC_POLICY = {}\n\n@export\n@optional_params\ndef register_arc_policy(cls, name=None):\n    """"""Register a function as a plug-in""""""\n    return register(cls, BASELINE_SEQ2SEQ_ARC_POLICY, name, \'decoder\')\n\n\n@export\ndef create_seq2seq_decoder(tgt_embeddings, **kwargs):\n    decoder_type = kwargs.get(\'decoder_type\', \'default\')\n    Constructor = BASELINE_SEQ2SEQ_DECODERS.get(decoder_type)\n    return Constructor(tgt_embeddings, **kwargs)\n\n\n@export\ndef create_seq2seq_encoder(**kwargs):\n    encoder_type = kwargs.get(\'encoder_type\', \'default\')\n    Constructor = BASELINE_SEQ2SEQ_ENCODERS.get(encoder_type)\n    return Constructor(**kwargs)\n\n\n@export\ndef create_seq2seq_arc_policy(**kwargs):\n    arc_type = kwargs.get(\'arc_policy_type\', \'default\')\n    Constructor = BASELINE_SEQ2SEQ_ARC_POLICY.get(arc_type)\n    return Constructor()\n\n\n@export\ndef create_seq2seq_model(embeddings, labels, **kwargs):\n    return create_model_for(\'seq2seq\', embeddings, labels, **kwargs)\n\n\n@export\ndef create_lang_model(embeddings, **kwargs):\n    return create_model_for(\'lm\', embeddings, None, **kwargs)\n\n\n@export\ndef load_model_for(activity, filename, **kwargs):\n    # Sniff state to see if we need to import things\n    state = read_json(\'{}.state\'.format(filename))\n    if \'hub_modules\' in state:\n        for hub_module in state[\'hub_modules\']:\n            import_user_module(hub_module)\n    # There won\'t be a module for pytorch (there is no state file to load).\n    if \'module\' in state:\n        import_user_module(state[\'module\'])\n    # Allow user to override model type (for back compat with old api), backoff\n    # to the model type in the state file or to default.\n    # TODO: Currently in pytorch all models are always reloaded with the load\n    # classmethod with a default model class. This is fine given how simple pyt\n    # loading is but it could cause problems if a model has a custom load\n    model_type = kwargs.get(\'type\', kwargs.get(\'model_type\', state.get(\'type\', state.get(\'model_type\', \'default\'))))\n    creator_fn = BASELINE_LOADERS[activity][model_type]\n    logger.info(\'Calling model %s\', creator_fn)\n    return creator_fn(filename, **kwargs)\n\n\n@export\ndef load_model(filename, **kwargs):\n    return load_model_for(\'classify\', filename, **kwargs)\n\n\n@export\ndef load_tagger_model(filename, **kwargs):\n    return load_model_for(\'tagger\', filename, **kwargs)\n\n\n@export\ndef load_seq2seq_model(filename, **kwargs):\n    return load_model_for(\'seq2seq\', filename, **kwargs)\n\n\n@export\ndef load_lang_model(filename, **kwargs):\n    return load_model_for(\'lm\', filename, **kwargs)\n\n\n@export\nclass ClassifierModel:\n    """"""Text classifier\n\n    Provide an interface to DNN classifiers that use word lookup tables.\n    """"""\n    task_name = \'classify\'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    def save(self, basename):\n        """"""Save this model out\n\n        :param basename: Name of the model, not including suffixes\n        :return: None\n        """"""\n        pass\n\n    @classmethod\n    def load(cls, basename, **kwargs):\n        """"""Load the model from a basename, including directory\n\n        :param basename: Name of the model, not including suffixes\n        :param kwargs: Anything that is useful to optimize experience for a specific framework\n        :return: A newly created model\n        """"""\n        pass\n\n    def predict(self, batch_dict):\n        """"""Classify a batch of text with whatever features the model can use from the batch_dict.\n        The indices correspond to get_vocab().get(\'word\', 0)\n\n        :param batch_dict: This normally contains `x`, a `BxT` tensor of indices.  Some classifiers and readers may\n        provide other features\n\n        :return: A list of lists of tuples (label, value)\n        """"""\n        pass\n\n    # deprecated: use predict\n    def classify(self, batch_dict):\n        logger.warning(\'`classify` is deprecated, use `predict` instead.\')\n        return self.predict(batch_dict)\n\n    def get_labels(self):\n        """"""Return a list of labels, where the offset within the list is the location in a confusion matrix, etc.\n\n        :return: A list of the labels for the decision\n        """"""\n        pass\n\n\n@export\nclass TaggerModel:\n    """"""Structured prediction classifier, AKA a tagger\n\n    This class takes a temporal signal, represented as words over time, and characters of words\n    and generates an output label for each time.  This type of model is used for POS tagging or any\n    type of chunking (e.g. NER, POS chunks, slot-filling)\n    """"""\n    task_name = \'tagger\'\n\n    def __init__(self):\n        super().__init__()\n\n    def save(self, basename):\n        pass\n\n    @staticmethod\n    def load(basename, **kwargs):\n        pass\n\n    def predict(self, batch_dict):\n        pass\n\n    def get_labels(self):\n        pass\n\n\n@export\nclass LanguageModel(object):\n\n    task_name = \'lm\'\n\n    def __init__(self):\n        super().__init__()\n\n    @staticmethod\n    def load(basename, **kwargs):\n        pass\n\n    @classmethod\n    def create(cls, embeddings, **kwargs):\n        pass\n\n    def predict(self, batch_dict, **kwargs):\n        pass\n\n\n@export\nclass EncoderDecoderModel(object):\n\n    task_name = \'seq2seq\'\n\n    def save(self, model_base):\n        pass\n\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n\n    @staticmethod\n    def load(basename, **kwargs):\n        pass\n\n    @classmethod\n    def create(cls, src_embeddings, dst_embedding, **kwargs):\n        pass\n\n    def create_loss(self):\n        pass\n\n    def predict(self, source_dict, **kwargs):\n        pass\n\n    # deprecated: use predict\n    def run(self, source_dict, **kwargs):\n        logger.warning(\'`run` is deprecated, use `predict` instead.\')\n        return self.predict(source_dict, **kwargs)\n'"
baseline/progress.py,0,"b""import eight_mile.progress\nfrom eight_mile.progress import *\nfrom eight_mile.utils import exporter, optional_params, register\n\n__all__ = []\n__all__.extend(eight_mile.progress.__all__)\nexport = exporter(__all__)\n\nMEAD_LAYERS_PROGRESS = {}\n\n\n@export\n@optional_params\ndef register_progress(cls, name=None):\n    return register(cls, MEAD_LAYERS_PROGRESS, name, 'progress')\n\n\n@export\ndef create_progress_bar(steps, name='default', **kwargs):\n    return MEAD_LAYERS_PROGRESS[name](steps, **kwargs)\n\n\n@register_progress('jupyter')\nclass ProgressBarJupyterBaseline(ProgressBarJupyter): pass\n\n\n@register_progress('default')\nclass ProgressBarTerminalBaseline(ProgressBarTerminal): pass\n"""
baseline/reader.py,0,"b'import os\nimport re\nimport codecs\nfrom itertools import chain\nfrom collections import Counter\nimport numpy as np\nimport baseline.data\nfrom baseline.vectorizers import Dict1DVectorizer, GOVectorizer, Token1DVectorizer, create_vectorizer\nfrom baseline.utils import import_user_module, revlut, exporter, optional_params, Offsets, listify\n\n__all__ = []\nexport = exporter(__all__)\n\n\nBASELINE_READERS = {}\n\n\n@export\n@optional_params\ndef register_reader(cls, task, name=None):\n    """"""Register your own `Reader`\n\n    Use this pattern if you want to provide an override to a `Reader` class.\n\n    """"""\n    if name is None:\n        name = cls.__name__\n\n    if task not in BASELINE_READERS:\n        BASELINE_READERS[task] = {}\n\n    if name in BASELINE_READERS[task]:\n        raise Exception(\'Error: attempt to re-defined previously registered handler {} for task {} in registry\'.format(name, task))\n\n    BASELINE_READERS[task][name] = cls\n    return cls\n\n\n@export\ndef create_reader(task, vectorizers, trim, **kwargs):\n    name = kwargs.get(\'type\', kwargs.get(\'reader_type\', \'default\'))\n    Constructor = BASELINE_READERS[task][name]\n    return Constructor(vectorizers, trim, **kwargs)\n\n\n@export\ndef num_lines(filename):\n    """"""Counts the number of lines in a file.\n\n    :param filename: `str` The name of the file to count the lines of.\n    :returns: `int` The number of lines.\n    """"""\n    with codecs.open(filename, encoding=\'utf-8\', mode=\'r\') as f:\n        return sum(1 for _ in f)\n\n\ndef _filter_vocab(vocab, min_fs):\n    """"""Filter down the vocab based on rules in the vectorizers.\n\n    :param vocab: `dict[Counter]`: A dict of vocabs.\n    :param min_fs: `dict[int]: A dict of cutoffs.\n\n    Note:\n        Any key in the min_fs dict should appear in the vocab dict.\n\n    :returns: `dict[dict]`: A dict of new filtered vocabs.\n    """"""\n    for k, min_f in min_fs.items():\n        # If we don\'t filter then skip to save an iteration through the vocab\n        if min_f == -1:\n            continue\n        vocab[k] = dict(filter(lambda x: x[1] >= min_f, vocab[k].items()))\n    return vocab\n\n\ndef _read_from_col(col, files, col_splitter=r\'\\t\', word_splitter=r\'\\s\'):\n    """"""Read from a single column of a file.\n\n    :param col: `int`: The column to read from.\n    :param files: List[str]: A list of files to read from.\n    :param col_splitter: `str`: The regex that splits a line into columns.\n    :param word_splitter: `str`: The regex that will split a column into words.\n\n    :returns: List[str]: The text from the col of each file.\n    """"""\n    text = []\n    for file_name in files:\n        if file_name is None:\n            continue\n        with codecs.open(file_name, encoding=\'utf-8\', mode=\'r\') as f:\n            for line in f:\n                line = line.rstrip(\'\\n\')\n                if line == """":\n                    continue\n                cols = re.split(col_splitter, line)\n                text.append(re.split(word_splitter, cols[col]))\n    return text\n\n\ndef _build_vocab_for_col(col, files, vectorizers, text=None, col_splitter=r\'\\t\', word_splitter=r\'\\s\'):\n    """"""Build vocab from a single column in file. (separated by `\\t`).\n\n    Used to read a vocab from a single conll column, read a vocab from the\n    source or target of a seq2seq file, or reading from a vocab file.\n\n    :param col: `int`: The column to read from.\n    :param files: List[str]: A list of files to read from.\n    :param vectorizers: dict[str] -> Vectorizer: The vectorizer to use to count the column\n    :param text: List[str]: The text from the columns or None\n    :param col_splitter: `str`: The regex that splits a line into columns.\n    :param word_splitter: `str`: The regex that will split a column into words.\n\n    :returns: dict[str] -> dict[str] -> int: The vocabs.\n    """"""\n    text = _read_from_col(col, files, col_splitter, word_splitter) if text is None else text\n    vocab = {k: Counter() for k in vectorizers}\n    for t in text:\n        for k, vect in vectorizers.items():\n            vocab[k].update(vect.count(t))\n    return vocab\n\n\ndef _check_lens(vectorizers):\n    failures = set()\n    for k, vect in vectorizers.items():\n        mxlen = getattr(vect, \'mxlen\', None)\n        if mxlen == -1:\n            failures.add(k)\n    return failures\n\n\ndef _vocab_allowed(vectorizers):\n    fails = _check_lens(vectorizers)\n    if fails:\n        fail_str = ""When using a vocab file mxlen for vectorizers must not be `-1`\\n""\n        vect_str = ""\\n"".join(""\\t{}"".format(fails))\n        raise RuntimeError(fail_str + vect_str)\n\n\n@export\nclass ParallelCorpusReader(object):\n\n    def __init__(self, vectorizers, trim=False, truncate=False):\n        super().__init__()\n\n        self.src_vectorizers = {}\n        self.tgt_vectorizer = None\n        for k, vectorizer in vectorizers.items():\n            if k == \'tgt\':\n                self.tgt_vectorizer = GOVectorizer(vectorizer)\n            else:\n                self.src_vectorizers[k] = vectorizer\n        self.trim = trim\n        self.truncate = truncate\n\n    def build_vocabs(self, files, **kwargs):\n        pass\n\n    def load_examples(self, tsfile, vocab1, vocab2, shuffle, sort_key):\n        pass\n\n    def load(self, tsfile, vocab1, vocab2, batchsz, shuffle=False, sort_key=None):\n        examples = self.load_examples(tsfile, vocab1, vocab2, shuffle, sort_key)\n        return baseline.data.ExampleDataFeed(examples, batchsz,\n                                             shuffle=shuffle, trim=self.trim, sort_key=sort_key, truncate=self.truncate)\n\n\n@register_reader(task=\'seq2seq\', name=\'tsv\')\nclass TSVParallelCorpusReader(ParallelCorpusReader):\n\n    def __init__(self, vectorizers,\n                 trim=False, truncate=False, src_col_num=0, tgt_col_num=1, **kwargs):\n        super().__init__(vectorizers, trim, truncate)\n        self.src_col_num = src_col_num\n        self.tgt_col_num = tgt_col_num\n\n    def build_vocabs(self, files, **kwargs):\n        vocab_file = kwargs.get(\'vocab_file\')\n        if vocab_file is not None:\n            all_vects = self.src_vectorizers.copy()\n            all_vects[\'tgt\'] = self.tgt_vectorizer\n            _vocab_allowed(all_vects)\n            # Only read the file once\n            text = _read_from_col(0, listify(vocab_file))\n            src_vocab = _build_vocab_for_col(None, None, self.src_vectorizers, text=text)\n            tgt_vocab = _build_vocab_for_col(None, None, {\'tgt\': self.tgt_vectorizer}, text=text)\n            return src_vocab, tgt_vocab[\'tgt\']\n        src_vocab = _build_vocab_for_col(self.src_col_num, files, self.src_vectorizers)\n        tgt_vocab = _build_vocab_for_col(self.tgt_col_num, files, {\'tgt\': self.tgt_vectorizer})\n        min_f = kwargs.get(\'min_f\', {})\n        tgt_min_f = {\'tgt\': min_f.pop(\'tgt\', -1)}\n        src_vocab = _filter_vocab(src_vocab, min_f)\n        tgt_vocab = _filter_vocab(tgt_vocab, tgt_min_f)\n        return src_vocab, tgt_vocab[\'tgt\']\n\n    def load_examples(self, tsfile, src_vocabs, tgt_vocab, do_shuffle, src_sort_key):\n        ts = []\n        with codecs.open(tsfile, encoding=\'utf-8\', mode=\'r\') as f:\n            for line in f:\n                splits = re.split(""\\t"", line.strip())\n                src = list(filter(lambda x: len(x) != 0, re.split(""\\s+"", splits[0])))\n\n                example = {}\n                for k, vectorizer in self.src_vectorizers.items():\n                    example[k], length = vectorizer.run(src, src_vocabs[k])\n                    if length is not None:\n                        example[\'{}_lengths\'.format(k)] = length\n\n                tgt = list(filter(lambda x: len(x) != 0, re.split(""\\s+"", splits[1])))\n                example[\'tgt\'], example[\'tgt_lengths\'] = self.tgt_vectorizer.run(tgt, tgt_vocab)\n                ts.append(example)\n        return baseline.data.Seq2SeqExamples(ts, do_shuffle=do_shuffle, src_sort_key=src_sort_key)\n\n\n@export\n@register_reader(task=\'seq2seq\', name=\'default\')\nclass MultiFileParallelCorpusReader(ParallelCorpusReader):\n\n    def __init__(self, vectorizers, trim=False, truncate=False, **kwargs):\n        super().__init__(vectorizers, trim, truncate)\n        pair_suffix = kwargs[\'pair_suffix\']\n\n        self.src_suffix = pair_suffix[0]\n        self.tgt_suffix = pair_suffix[1]\n        if not self.src_suffix.startswith(\'.\'):\n            self.src_suffix = \'.\' + self.src_suffix\n        if not self.tgt_suffix.startswith(\'.\'):\n            self.tgt_suffix = \'.\' + self.tgt_suffix\n\n    def build_vocabs(self, files, **kwargs):\n        vocab_file = kwargs.get(\'vocab_file\')\n        if vocab_file is not None:\n            all_vects = self.src_vectorizers.copy()\n            all_vects[\'tgt\'] = self.tgt_vectorizer\n            _vocab_allowed(all_vects)\n            # Only read the file once.\n            text = _read_from_col(0, listify(vocab_file))\n            src_vocab = _build_vocab_for_col(None, None, self.src_vectorizers, text=text)\n            tgt_vocab = _build_vocab_for_col(None, None, {\'tgt\': self.tgt_vectorizer}, text=text)\n            return src_vocab, tgt_vocab[\'tgt\']\n        src_vocab = _build_vocab_for_col(0, [f + self.src_suffix for f in files], self.src_vectorizers)\n        tgt_vocab = _build_vocab_for_col(0, [f + self.tgt_suffix for f in files], {\'tgt\': self.tgt_vectorizer})\n        min_f = kwargs.get(\'min_f\', {})\n        tgt_min_f = {\'tgt\': min_f.pop(\'tgt\', -1)}\n        src_vocab = _filter_vocab(src_vocab, min_f)\n        tgt_vocab = _filter_vocab(tgt_vocab, tgt_min_f)\n        return src_vocab, tgt_vocab[\'tgt\']\n\n    def load_examples(self, tsfile, src_vocabs, tgt_vocab, do_shuffle, src_sort_key):\n        ts = []\n\n        with codecs.open(tsfile + self.src_suffix, encoding=\'utf-8\', mode=\'r\') as fsrc:\n            with codecs.open(tsfile + self.tgt_suffix, encoding=\'utf-8\', mode=\'r\') as ftgt:\n                for src, tgt in zip(fsrc, ftgt):\n                    example = {}\n                    src = re.split(""\\s+"", src.strip())\n                    for k, vectorizer in self.src_vectorizers.items():\n                        example[k], length = vectorizer.run(src, src_vocabs[k])\n                        if length is not None:\n                            example[\'{}_lengths\'.format(k)] = length\n                    tgt = re.split(""\\s+"", tgt.strip())\n                    example[\'tgt\'], example[\'tgt_lengths\'] = self.tgt_vectorizer.run(tgt, tgt_vocab)\n                    ts.append(example)\n        return baseline.data.Seq2SeqExamples(ts, do_shuffle=do_shuffle, src_sort_key=src_sort_key)\n\n\n@export\nclass SeqPredictReader(object):\n\n    def __init__(self, vectorizers, trim=False, truncate=False, mxlen=-1, **kwargs):\n        super().__init__()\n        self.vectorizers = vectorizers\n        self.trim = trim\n        self.truncate = truncate\n        label_vectorizer_spec = kwargs.get(\'label_vectorizer\', None)\n        if label_vectorizer_spec:\n            self.label_vectorizer = create_vectorizer(**label_vectorizer_spec)\n        else:\n            self.label_vectorizer = Dict1DVectorizer(fields=\'y\', mxlen=mxlen)\n        self.label2index = {\n            Offsets.VALUES[Offsets.PAD]: Offsets.PAD,\n            Offsets.VALUES[Offsets.GO]: Offsets.GO,\n            Offsets.VALUES[Offsets.EOS]: Offsets.EOS\n        }\n\n    def build_vocab(self, files, **kwargs):\n        pre_vocabs = None\n        pre_labels = None\n        vocabs = {k: Counter() for k in self.vectorizers.keys()}\n\n        vocab_file = kwargs.get(\'vocab_file\')\n        label_file = kwargs.get(\'label_file\')\n        if vocab_file:\n            _vocab_allowed(self.vectorizers)\n            pre_vocabs = _build_vocab_for_col(0, listify(vocab_file), self.vectorizers)\n        if label_file:\n            pre_labels = Counter(chain(*_read_from_col(0, listify(label_file))))\n            self.label2index = {l: i for i, l in enumerate(pre_labels)}\n\n        labels = Counter()\n\n        #if not pre_vocabs:\n        for file in files:\n            if file is None:\n                continue\n\n            examples = self.read_examples(file)\n            for example in examples:\n                labels.update(self.label_vectorizer.count(example))\n                for k, vectorizer in self.vectorizers.items():\n                    vocab_example = vectorizer.count(example)\n                    vocabs[k].update(vocab_example)\n\n        if pre_labels and not pre_vocabs:\n            return vocabs\n\n        vocabs = _filter_vocab(vocabs, kwargs.get(\'min_f\', {}))\n        base_offset = len(self.label2index)\n        labels.pop(Offsets.VALUES[Offsets.PAD], None)\n        for i, k in enumerate(labels.keys()):\n            self.label2index[k] = i + base_offset\n        if pre_vocabs:\n            vocabs = pre_vocabs\n        return vocabs\n\n    def read_examples(self):\n        pass\n\n    def load(self, filename, vocabs, batchsz, shuffle=False, sort_key=None):\n\n        ts = []\n        texts = self.read_examples(filename)\n\n        if sort_key is not None and not sort_key.endswith(\'_lengths\'):\n            sort_key += \'_lengths\'\n\n        for i, example_tokens in enumerate(texts):\n            example = {}\n            for k, vectorizer in self.vectorizers.items():\n                example[k], lengths = vectorizer.run(example_tokens, vocabs[k])\n                if lengths is not None:\n                    example[\'{}_lengths\'.format(k)] = lengths\n            example[\'y\'], lengths = self.label_vectorizer.run(example_tokens, self.label2index)\n            example[\'y_lengths\'] = lengths\n            example[\'ids\'] = i\n            ts.append(example)\n        examples = baseline.data.DictExamples(ts, do_shuffle=shuffle, sort_key=sort_key)\n        return baseline.data.ExampleDataFeed(examples, batchsz=batchsz, shuffle=shuffle, trim=self.trim, truncate=self.truncate), texts\n\n\n@export\n@register_reader(task=\'tagger\', name=\'default\')\nclass CONLLSeqReader(SeqPredictReader):\n\n    def __init__(self, vectorizers, trim=False, truncate=False, mxlen=-1, **kwargs):\n        super().__init__(vectorizers, trim, truncate, mxlen, **kwargs)\n        self.named_fields = kwargs.get(\'named_fields\', {})\n\n    def read_examples(self, tsfile):\n\n        tokens = []\n        examples = []\n\n        with codecs.open(tsfile, encoding=\'utf-8\', mode=\'r\') as f:\n            for i, line in enumerate(f):\n                states = re.split(""\\s"", line.strip())\n\n                token = dict()\n                if len(states) > 1:\n                    for j in range(len(states)):\n                        noff = j - len(states)\n                        if noff >= 0:\n                            noff = j\n                        field_name = self.named_fields.get(str(j),\n                                                           self.named_fields.get(str(noff), str(j)))\n                        token[field_name] = states[j]\n                    tokens.append(token)\n\n                else:\n                    if len(tokens) == 0:\n                        raise Exception(""Unexpected empty line ({}) in {}"".format(i, tsfile))\n                    examples.append(tokens)\n                    tokens = []\n            if len(tokens) > 0:\n                examples.append(tokens)\n        return examples\n\n\ndef _norm_ext(ext):\n    return ext if ext.startswith(\'.\') else \'.\' + ext\n\n\n# TODO: get rid of this class\n@export\n@register_reader(task=\'tagger\', name=\'parallel\')\nclass ParallelSeqReader(SeqPredictReader):\n    def __init__(self, vectorizers, trim=False, truncate=False, mxlen=-1, **kwargs):\n        # This works but its not helpful for most custom vectorizers\n        kwargs.get[\'label_vectorizer\'] = kwargs.get(\'label_vectorizer\', Token1DVectorizer(mxlen=mxlen))\n        super().__init__(vectorizers, trim, truncate, mxlen, **kwargs)\n        self.data = _norm_ext(kwargs.get(\'data_suffix\', \'in\'))\n        self.tag = _norm_ext(kwargs.get(\'label_suffix\', \'out\'))\n\n    def build_vocab(self, files, **kwargs):\n        vocabs = {k: Counter() for k in self.vectorizers.keys()}\n        labels = Counter()\n\n        for file_name in files:\n            if file_name is None:\n                continue\n            examples = self.read_examples(file_name + self.data)\n            tags = self.read_examples(file_name + self.tag)\n            for example, tag in zip(examples, tags):\n                labels.update(self.label_vectorizer.count(tag))\n                for k, vectorizer in self.vectorizers.items():\n                    vocab_example = vectorizer.count(example)\n                    vocabs[k].update(vocab_example)\n\n        vocabs = _filter_vocab(vocabs, kwargs.get(\'min_f\', {}))\n        base_offset = len(self.label2index)\n        for i, k in enumerate(labels.keys()):\n            self.label2index[k] = i + base_offset\n        return vocabs\n\n    def read_examples(self, file_name):\n        with codecs.open(file_name, encoding=\'utf-8\', mode=\'r\') as f:\n            return [l.strip().split() for l in f]\n\n    def load(self, filename, vocabs, batchsz, shuffle=False, sort_key=None):\n\n        ts = []\n        texts = self.read_examples(filename + self.data)\n        tag_texts = self.read_examples(filename + self.tag)\n\n        if sort_key is not None and not sort_key.endswith(\'_lengths\'):\n            sort_key += \'_lengths\'\n\n        raw_texts = []\n\n        for i, (example_tokens, tag_tokens) in enumerate(zip(texts, tag_texts)):\n            example = {}\n            for k, vectorizer in self.vectorizers.items():\n                example[k], lengths = vectorizer.run(example_tokens, vocabs[k])\n                if lengths is not None:\n                    example[\'{}_lengths\'.format(k)] = lengths\n            example[\'y\'], lengths = self.label_vectorizer.run(tag_tokens, self.label2index)\n            example[\'y_lengths\'] = lengths\n            example[\'ids\'] = i\n            ts.append(example)\n            raw_texts.append([{\'text\': t, \'y\': l} for t, l in zip(example_tokens, tag_tokens)])\n        examples = baseline.data.DictExamples(ts, do_shuffle=shuffle, sort_key=sort_key)\n        return baseline.data.ExampleDataFeed(examples, batchsz=batchsz, shuffle=shuffle, trim=self.trim, truncate=self.truncate), raw_texts\n\n\n@export\nclass SeqLabelReader(object):\n\n    def __init__(self):\n        pass\n\n    def build_vocab(self, files, **kwargs):\n        pass\n\n    def load(self, filename, index, batchsz, **kwargs):\n        pass\n\n\ndef _get_dir(files):\n    if isinstance(files, str):\n        if os.path.isdir(files):\n            base = files\n            files = filter(os.path.isfile, [os.path.join(base, x) for x in os.listdir(base)])\n        else:\n            files = [files]\n    return files\n\n\n@export\n@register_reader(task=\'classify\', name=\'default\')\nclass TSVSeqLabelReader(SeqLabelReader):\n\n    REPLACE = { ""\'s"": "" \'s "",\n                ""\'ve"": "" \'ve "",\n                ""n\'t"": "" n\'t "",\n                ""\'re"": "" \'re "",\n                ""\'d"": "" \'d "",\n                ""\'ll"": "" \'ll "",\n                "","": "" , "",\n                ""!"": "" ! "",\n                }\n\n    def __init__(self, vectorizers, trim=False, truncate=False, **kwargs):\n        super().__init__()\n\n        self.label2index = {}\n        self.vectorizers = vectorizers\n        self.clean_fn = kwargs.get(\'clean_fn\')\n        if self.clean_fn is None:\n            self.clean_fn = lambda x: x\n        self.trim = trim\n        self.truncate = truncate\n\n    SPLIT_ON = \'[\\t\\s]+\'\n\n    @staticmethod\n    def splits(text):\n        return list(filter(lambda s: len(s) != 0, re.split(\'\\s+\', text)))\n\n    @staticmethod\n    def do_clean(l):\n        l = l.lower()\n        l = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", l)\n        for k, v in TSVSeqLabelReader.REPLACE.items():\n            l = l.replace(k, v)\n        return l.strip()\n\n    @staticmethod\n    def label_and_sentence(line, clean_fn):\n        label_text = re.split(TSVSeqLabelReader.SPLIT_ON, line)\n        label = label_text[0]\n        text = label_text[1:]\n        text = \' \'.join(list(filter(lambda s: len(s) != 0, [clean_fn(w) for w in text])))\n        text = list(filter(lambda s: len(s) != 0, re.split(\'\\s+\', text)))\n        return label, text\n\n    def build_vocab(self, files, **kwargs):\n        """"""Take a directory (as a string), or an array of files and build a vocabulary\n\n        Take in a directory or an array of individual files (as a list).  If the argument is\n        a string, it may be a directory, in which case, all files in the directory will be loaded\n        to form a vocabulary.\n\n        :param files: Either a directory (str), or an array of individual files\n        :return:\n        """"""\n        vocab_file = kwargs.get(\'vocab_file\')\n        label_file = kwargs.get(\'label_file\')\n        if vocab_file is not None and label_file is not None:\n            _vocab_allowed(self.vectorizers)\n            vocab = _build_vocab_for_col(0, listify(vocab_file), self.vectorizers)\n            labels = Counter(chain(*_read_from_col(0, listify(label_file))))\n            self.label2index = {l: i for i, l in enumerate(labels)}\n            return vocab, self.get_labels()\n\n        label_idx = len(self.label2index)\n        if isinstance(files, str):\n            if os.path.isdir(files):\n                base = files\n                files = filter(os.path.isfile, [os.path.join(base, x) for x in os.listdir(base)])\n            else:\n                files = [files]\n        vocab = {k: Counter() for k in self.vectorizers.keys()}\n\n        for file_name in files:\n            if file_name is None:\n                continue\n            with codecs.open(file_name, encoding=\'utf-8\', mode=\'r\') as f:\n                for il, line in enumerate(f):\n                    label, text = TSVSeqLabelReader.label_and_sentence(line, self.clean_fn)\n                    if len(text) == 0:\n                        continue\n\n                    for k, vectorizer in self.vectorizers.items():\n                        vocab_file = vectorizer.count(text)\n                        vocab[k].update(vocab_file)\n\n                    if label not in self.label2index:\n                        self.label2index[label] = label_idx\n                        label_idx += 1\n\n        vocab = _filter_vocab(vocab, kwargs.get(\'min_f\', {}))\n\n        return vocab, self.get_labels()\n\n    def get_labels(self):\n        labels = [\'\'] * len(self.label2index)\n        for label, index in self.label2index.items():\n            labels[index] = label\n        return labels\n\n    def load_text(self, filename, vocabs, batchsz, **kwargs):\n\n        shuffle = kwargs.get(\'shuffle\', False)\n        sort_key = kwargs.get(\'sort_key\', None)\n        if sort_key is not None and not sort_key.endswith(\'_lengths\'):\n            sort_key += \'_lengths\'\n\n        examples = []\n        texts = []\n        with codecs.open(filename, encoding=\'utf-8\', mode=\'r\') as f:\n            for il, line in enumerate(f):\n                label, text = TSVSeqLabelReader.label_and_sentence(line, self.clean_fn)\n                texts.append(text)\n                if len(text) == 0:\n                    continue\n                y = self.label2index[label]\n                example_dict = dict()\n                for k, vectorizer in self.vectorizers.items():\n                    example_dict[k], lengths = vectorizer.run(text, vocabs[k])\n                    if lengths is not None:\n                        example_dict[\'{}_lengths\'.format(k)] = lengths\n\n                example_dict[\'y\'] = y\n                examples.append(example_dict)\n        return baseline.data.ExampleDataFeed(baseline.data.DictExamples(examples,\n                                                                        do_shuffle=shuffle,\n                                                                        sort_key=sort_key),\n                                             batchsz=batchsz, shuffle=shuffle, trim=self.trim, truncate=self.truncate), texts\n\n    def load(self, filename, vocabs, batchsz, **kwargs):\n    \n        shuffle = kwargs.get(\'shuffle\', False)\n        sort_key = kwargs.get(\'sort_key\', None)\n        if sort_key is not None and not sort_key.endswith(\'_lengths\'):\n            sort_key += \'_lengths\'\n    \n        examples = []\n    \n        with codecs.open(filename, encoding=\'utf-8\', mode=\'r\') as f:\n            for il, line in enumerate(f):\n                label, text = TSVSeqLabelReader.label_and_sentence(line, self.clean_fn)\n                if len(text) == 0:\n                    continue\n                y = self.label2index[label]\n                example_dict = dict()\n                for k, vectorizer in self.vectorizers.items():\n                    example_dict[k], lengths = vectorizer.run(text, vocabs[k])\n                    if lengths is not None:\n                        example_dict[\'{}_lengths\'.format(k)] = lengths\n            \n                example_dict[\'y\'] = y\n                examples.append(example_dict)\n        return baseline.data.ExampleDataFeed(baseline.data.DictExamples(examples,\n                                                                        do_shuffle=shuffle,\n                                                                        sort_key=sort_key),\n                                             batchsz=batchsz, shuffle=shuffle, trim=self.trim, truncate=self.truncate)\n\n\n@export\n@register_reader(task=\'lm\', name=\'default\')\nclass LineSeqReader(object):\n\n    def __init__(self, vectorizers, trim=False, **kwargs):\n        self.nctx = kwargs[\'nctx\']\n        self.vectorizers = vectorizers\n\n    def build_vocab(self, files, **kwargs):\n        vocab_file = kwargs.get(\'vocab_file\')\n        if vocab_file is not None:\n            _vocab_allowed(self.vectorizers)\n            return _build_vocab_for_col(0, listify(vocab_file), self.vectorizers)\n\n        vocabs = {k: Counter() for k in self.vectorizers.keys()}\n\n        for file in files:\n            if file is None:\n                continue\n\n            with codecs.open(file, encoding=\'utf-8\', mode=\'r\') as f:\n                sentences = []\n                for line in f:\n                    sentences += line.split() + [\'<EOS>\']\n                for k, vectorizer in self.vectorizers.items():\n                    vocabs[k].update(vectorizer.count(sentences))\n\n        vocabs = _filter_vocab(vocabs, kwargs.get(\'min_f\', {}))\n        return vocabs\n\n    def load(self, filename, vocabs, batchsz, tgt_key=\'x\'):\n\n        x = dict()\n        with codecs.open(filename, encoding=\'utf-8\', mode=\'r\') as f:\n            sentences = []\n            for line in f:\n                sentences += line.split() + [\'<EOS>\']\n            for k, vectorizer in self.vectorizers.items():\n                vec, valid_lengths = vectorizer.run(sentences, vocabs[k])\n                x[k] = vec[:valid_lengths]\n                shp = list(vectorizer.get_dims())\n                shp[0] = valid_lengths\n                x[\'{}_dims\'.format(k)] = tuple(shp)\n\n        return baseline.data.SeqWordCharDataFeed(x, self.nctx, batchsz, tgt_key=tgt_key)\n'"
baseline/remote.py,0,"b'import json\nfrom urllib.parse import urlparse\nfrom http.client import HTTPConnection\nimport numpy as np\nfrom baseline.utils import (\n    import_user_module, exporter, optional_params, register, listify\n)\n\n\n__all__ = []\nexport = exporter(__all__)\n\nBASELINE_REMOTES = {}\n\n\n@export\n@optional_params\ndef register_remote(cls, name=None):\n    """"""Register a class as a plug-in""""""\n    if name is None:\n        name = cls.__name__\n    names = listify(name)\n    for alias in names:\n        if alias in BASELINE_REMOTES:\n            raise Exception(\'Error: attempt to re-define previously registered hander {} (old: {}, new: {}) in registry\'.format(alias, BASELINE_REMOTES, cls))\n        BASELINE_REMOTES[alias] = cls\n    return cls\n\n\n@export\ndef create_remote(exporter_type, **kwargs):\n    remote = BASELINE_REMOTES[exporter_type]\n    return remote(**kwargs)\n\n\ndef verify_example(examples, keys):\n    missing_keys = [k for k in keys if k not in examples]\n    if missing_keys:\n        raise ValueError(""Data should have keys: {}\\n {} are missing."".format(keys, missing_keys))\n\n\nclass RemoteModel(object):\n    def __init__(\n            self,\n            remote,\n            name, signature,\n            labels=None,\n            beam=None,\n            lengths_key=None,\n            inputs=None,\n            version=None,\n            return_labels=None,\n    ):\n        """"""A remote model where the actual inference is done on a server.\n\n        :param remote: The remote endpoint\n        :param name:  The name of the model\n        :param signature: The model signature\n        :param labels: The labels (defaults to None)\n        :param beam: The beam width (defaults to None)\n        :param lengths_key: Which key is used for the length of the input\n            vector (defaults to None)\n        :param inputs: The inputs (defaults to empty list)\n        :param version: The model version (defaults to None)\n        :param return_labels: Whether the remote model returns class indices or\n            the class labels directly. This depends on the `return_labels`\n            parameter in exporters\n        """"""\n        inputs = [] if inputs is None else inputs\n        self.remote = remote\n        self.name = name\n        self.signature = signature\n        self.lengths_key = lengths_key\n        self.input_keys = inputs\n        self.beam = beam\n        self.labels = labels\n        self.version = version\n        self.return_labels = return_labels\n\n    def get_labels(self):\n        """"""Return the model\'s labels\n\n        :return: The model\'s labels\n        """"""\n        return self.labels\n\n    def predict(self, examples, **kwargs):\n        """"""Run inference on examples.""""""\n        pass\n\n    def deserialize_response(self, examples, predict_response):\n        """"""Convert the response into a standard format.""""""\n        pass\n\n    def create_request(self, examples):\n        """"""Convert the examples into the format expected by the server.""""""\n        pass\n\n\n@export\nclass RemoteModelREST(RemoteModel):\n\n    def __init__(\n            self,\n            remote,\n            name, signature,\n            labels=None,\n            beam=None,\n            lengths_key=None,\n            inputs=None,\n            version=None,\n            return_labels=None,\n    ):\n        """"""A remote model with REST transport\n\n        :param remote: The remote endpoint\n        :param name:  The name of the model\n        :param signature: The model signature\n        :param labels: The labels (defaults to None)\n        :param beam: The beam width (defaults to None)\n        :param lengths_key: Which key is used for the length of the input vector (defaults to None)\n        :param inputs: The inputs (defaults to empty list)\n        :param version: The model version (defaults to None)\n        :param return_labels: Whether the remote model returns class indices or the class labels directly. This depends\n        on the `return_labels` parameter in exporters\n        """"""\n        super(RemoteModelREST, self).__init__(\n            remote, name, signature, labels, beam, lengths_key, inputs, version, return_labels\n        )\n        url = urlparse(self.remote)\n        if len(url.netloc.split("":"")) != 2:\n            raise ValueError(""remote has to have the form <host_name>:<port>"")\n        self.hostname, self.port = url.netloc.split("":"")\n        v_str = \'/versions/{}\'.format(self.version) if self.version is not None else \'\'\n        path = url.path if url.path.endswith(""/"") else ""{}/"".format(url.path)\n        self.path = \'{}v1/models/{}{}:predict\'.format(path, self.name, v_str)\n        self.headers = {\'Content-type\': \'application/json\'}\n\n    def predict(self, examples, **kwargs):\n        """"""Run prediction over HTTP/REST.\n\n        :param examples: The input examples\n        :return: The outcomes\n        """"""\n\n        verify_example(examples, self.input_keys)\n\n        request = self.create_request(examples)\n        conn = HTTPConnection(self.hostname, self.port)\n        conn.request(\'POST\', self.path, json.dumps(request), self.headers)\n        response = conn.getresponse().read()\n        outcomes_list = json.loads(response)\n        if ""error"" in outcomes_list:\n            raise ValueError(""remote server returns error: {0}"".format(outcomes_list[""error""]))\n        outcomes_list = outcomes_list[""outputs""]\n        outcomes_list = self.deserialize_response(examples, outcomes_list)\n        return outcomes_list\n\n\n@export\nclass RemoteRESTSeq2Seq(RemoteModelREST):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""Read the JSON response and decode it according to the signature.\n\n        :param examples: Input examples\n        :param predict_response: an HTTP/REST output\n        """"""\n        num_ex = len(predict_response[\'classes\'])\n\n        # s2s returns int values.\n        tensor = np.array(predict_response)\n        tensor = tensor.transpose(1, 2, 0)\n        return tensor\n\n\n@export\nclass RemoteRESTTagger(RemoteModelREST):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""Read the JSON response and decode it according to the signature.\n\n        :param examples: Input examples\n        :param predict_response: an HTTP/REST output\n        """"""\n        num_ex = len(predict_response[\'classes\'])\n\n        classes = predict_response[\'classes\']\n        lengths = examples[self.lengths_key]\n        result = []\n        for i in range(num_ex):\n            length_i = lengths[i]\n            classes_i = classes[i]\n            if self.return_labels:\n                d = [np.array(classes_i[j]) for j in range(length_i)]\n            else:\n                d = [np.array(np.int32(classes_i[j])) for j in range(length_i)]\n            result.append(d)\n\n        return result\n\n\n@export\nclass RemoteRESTClassifier(RemoteModelREST):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""Read the JSON response and decode it according to the signature.\n\n        :param examples: Input examples\n        :param predict_response: an HTTP/REST output\n        """"""\n        num_ex = len(predict_response[\'classes\'])\n        scores = predict_response[\'scores\']\n        classes = predict_response[\'classes\']\n        result = []\n        for i in range(num_ex):\n            score_i = scores[i]\n            classes_i = classes[i]\n            if self.return_labels:\n                d = [(c, np.float32(s)) for c, s in zip(classes_i, score_i)]\n            else:\n                d = [(np.int32(c), np.float32(s)) for c, s in zip(classes_i, score_i)]\n            result.append(d)\n        return result\n\n\n@export\nclass RemoteRESTEmbeddings(RemoteModelREST):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""Read the JSON response and decode it according to the signature.\n\n        :param examples: Input examples\n        :param predict_response: an HTTP/REST output\n        """"""\n        num_ex = len(predict_response[\'classes\'])\n\n        return np.array(predict_response[\'scores\'])\n\n\n@export\nclass RemoteModelGRPC(RemoteModel):\n\n    def __init__(self, remote, name, signature, labels=None, beam=None, lengths_key=None, inputs=None, version=None, return_labels=False):\n        """"""A remote model with gRPC transport\n\n        When using this type of model, there is an external dependency on the `grpc` package, as well as the\n        TF serving protobuf stub files.  There is also currently a dependency on `tensorflow`\n\n        :param remote: The remote endpoint\n        :param name:  The name of the model\n        :param signature: The model signature\n        :param labels: The labels (defaults to None)\n        :param beam: The beam width (defaults to None)\n        :param lengths_key: Which key is used for the length of the input vector (defaults to None)\n        :param inputs: The inputs (defaults to empty list)\n        :param version: The model version (defaults to None)\n        :param return_labels: Whether the remote model returns class indices or the class labels directly. This depends\n        on the `return_labels` parameter in exporters\n        """"""\n        super(RemoteModelGRPC, self).__init__(\n            remote, name, signature, labels, beam, lengths_key, inputs, version, return_labels\n        )\n        self.predictpb = import_user_module(\'baseline.tensorflow_serving.apis.predict_pb2\')\n        self.servicepb = import_user_module(\'baseline.tensorflow_serving.apis.prediction_service_pb2_grpc\')\n        self.metadatapb = import_user_module(\'baseline.tensorflow_serving.apis.get_model_metadata_pb2\')\n        self.grpc = import_user_module(\'grpc\')\n        self.channel = self.grpc.insecure_channel(remote)\n\n    def decode_output(self, x):\n        return x.decode(\'ascii\') if self.return_labels else np.int32(x)\n\n    def get_labels(self):\n        """"""Return the model\'s labels\n\n        :return: The model\'s labels\n        """"""\n        return self.labels\n\n    def predict(self, examples, **kwargs):\n        """"""Run prediction over gRPC\n\n        :param examples: The input examples\n        :return: The outcomes\n        """"""\n        verify_example(examples, self.input_keys)\n\n        request = self.create_request(examples)\n        stub = self.servicepb.PredictionServiceStub(self.channel)\n        outcomes_list = stub.Predict(request)\n        outcomes_list = self.deserialize_response(examples, outcomes_list)\n\n        return outcomes_list\n\n    def create_request(self, examples):\n        # TODO: Remove TF dependency client side\n        import tensorflow as tf\n\n        request = self.predictpb.PredictRequest()\n        request.model_spec.name = self.name\n        request.model_spec.signature_name = self.signature\n        if self.version is not None:\n            request.model_spec.version.value = self.version\n\n        for feature in self.input_keys:\n            if isinstance(examples[feature], np.ndarray):\n                shape = examples[feature].shape\n            else:\n                shape = [1]\n\n            dtype = examples[feature].dtype.type\n            if issubclass(dtype, np.integer): dtype = tf.int32\n            elif issubclass(dtype, np.floating): dtype = tf.float32\n            else: dtype = tf.string\n\n            tensor_proto = tf.compat.v1.make_tensor_proto(examples[feature], shape=shape, dtype=dtype)\n            request.inputs[feature].CopyFrom(\n                tensor_proto\n            )\n\n        return request\n\n\n@export\nclass RemoteGRPCSeq2Seq(RemoteModelGRPC):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""\n        read the protobuf response from tensorflow serving and decode it according\n        to the signature.\n\n        here\'s the relevant piece of the proto:\n            map<string, TensorProto> inputs = 2;\n\n        the predict endpoint happens to have the ability to filter output for certain keys, but\n        we do not support this currently. There are two keys we want to extract: classes and scores.\n\n        :params predict_response: a PredictResponse protobuf object,\n                    as defined in tensorflow_serving proto files\n        """"""\n\n        example_len = predict_response.outputs.get(\'classes\').tensor_shape.dim[1].size\n        num_examples = predict_response.outputs.get(\'classes\').tensor_shape.dim[0].size\n\n        # s2s returns int values.\n        classes = predict_response.outputs.get(\'classes\')\n        shape = [dim.size for dim in classes.tensor_shape.dim]\n        results = np.reshape(np.array(classes.int_val), shape)\n        results = results.transpose(1, 2, 0)\n        return results\n\n\n@export\nclass RemoteGRPCTagger(RemoteModelGRPC):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""\n        read the protobuf response from tensorflow serving and decode it according\n        to the signature.\n\n        here\'s the relevant piece of the proto:\n            map<string, TensorProto> inputs = 2;\n\n        the predict endpoint happens to have the ability to filter output for certain keys, but\n        we do not support this currently. There are two keys we want to extract: classes and scores.\n\n        :params predict_response: a PredictResponse protobuf object,\n                    as defined in tensorflow_serving proto files\n        """"""\n\n        example_len = predict_response.outputs.get(\'classes\').tensor_shape.dim[1].size\n        num_examples = predict_response.outputs.get(\'classes\').tensor_shape.dim[0].size\n\n        if self.return_labels:\n            classes = predict_response.outputs.get(\'classes\').string_val\n        else:\n            classes = predict_response.outputs.get(\'classes\').int_val\n        lengths = examples[self.lengths_key]\n        result = []\n        for i in range(num_examples):\n            length = lengths[i]\n            tmp = [self.decode_output(x) for x in classes[example_len*i:example_len*(i+1)][:length]]\n            result.append(tmp)\n\n        return result\n\n\n@export\nclass RemoteGRPCClassifier(RemoteModelGRPC):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""\n        read the protobuf response from tensorflow serving and decode it according\n        to the signature.\n\n        here\'s the relevant piece of the proto:\n            map<string, TensorProto> inputs = 2;\n\n        the predict endpoint happens to have the ability to filter output for certain keys, but\n        we do not support this currently. There are two keys we want to extract: classes and scores.\n\n        :params predict_response: a PredictResponse protobuf object,\n                    as defined in tensorflow_serving proto files\n        """"""\n\n        example_len = predict_response.outputs.get(\'classes\').tensor_shape.dim[1].size\n        num_examples = predict_response.outputs.get(\'classes\').tensor_shape.dim[0].size\n\n        scores = predict_response.outputs.get(\'scores\').float_val\n        if self.return_labels:\n            classes = predict_response.outputs.get(\'classes\').string_val\n        else:\n            classes = predict_response.outputs.get(\'classes\').int_val\n        result = []\n        length = len(self.get_labels())\n        for i in range(num_examples):   # wrap in numpy because the local models send that dtype out\n            d = [(self.decode_output(c), np.float32(s)) for c, s in zip(classes[example_len*i:example_len*(i+1)][:length], scores[length*i:length*(i+1)][:length])]\n            result.append(d)\n        return result\n\n\n@export\nclass RemoteGRPCEmbeddings(RemoteModelGRPC):\n\n    def deserialize_response(self, examples, predict_response):\n        """"""\n        read the protobuf response from tensorflow serving and decode it according\n        to the signature.\n\n        here\'s the relevant piece of the proto:\n            map<string, TensorProto> inputs = 2;\n\n        the predict endpoint happens to have the ability to filter output for certain keys, but\n        we do not support this currently. There are two keys we want to extract: classes and scores.\n\n        :params predict_response: a PredictResponse protobuf object,\n                    as defined in tensorflow_serving proto files\n        """"""\n\n        example_len = predict_response.outputs.get(\'classes\').tensor_shape.dim[1].size\n        num_examples = predict_response.outputs.get(\'classes\').tensor_shape.dim[0].size\n        scores = predict_response.outputs.get(\'scores\')\n        shape = [dim.size for dim in scores.tensor_shape.dim]\n        return np.reshape(np.array(scores.float_val), shape)\n'"
baseline/reporting.py,0,"b'import os\nimport logging\nimport numpy as np\nfrom baseline.utils import exporter, optional_params, register\n\n\n__all__ = []\nexport = exporter(__all__)\nlogger = logging.getLogger(\'baseline\')\n\nBASELINE_REPORTING = {}\n\n\n@export\n@optional_params\ndef register_reporting(cls, name=None):\n    """"""Register a function as a plug-in""""""\n    return register(cls, BASELINE_REPORTING, name, \'reporting\')\n\n\n@export\nclass ReportingHook(object):\n    def __init__(self, **kwargs):\n        pass\n\n    def step(self, metrics, tick, phase, tick_type, **kwargs):\n        pass\n\n    def done(self, **kwargs):\n        pass\n\n    @staticmethod\n    def _infer_tick_type(phase, tick_type):\n        if tick_type is None:\n            tick_type = \'STEP\'\n            if phase in {\'Valid\', \'Test\'}:\n                tick_type = \'EPOCH\'\n            if phase == \'Export\':\n                tick_type = \'EXPORT\'\n        return tick_type\n\n\nclass EpochReportingHook(ReportingHook):\n    def step(self, metrics, tick, phase, tick_type=None, **kwargs):\n        tick_type = ReportingHook._infer_tick_type(phase, tick_type)\n        if tick_type == \'EPOCH\':\n            self._step(metrics, tick, phase, tick_type, **kwargs)\n\n\nclass StepReportingHook(ReportingHook):\n    def step(self, metrics, tick, phase, tick_type=None, **kwargs):\n        tick_type = ReportingHook._infer_tick_type(phase, tick_type)\n        if tick_type == \'STEP\':\n            self._step(metrics, tick, phase, tick_type, **kwargs)\n\n\n@register_reporting(name=\'console\')\nclass ConsoleReporting(EpochReportingHook):\n    def __init__(self, **kwargs):\n        super(ConsoleReporting, self).__init__(**kwargs)\n\n    def _step(self, metrics, tick, phase, tick_type=None, **kwargs):\n        """"""Write results to `stdout`\n\n        :param metrics: A map of metrics to scores\n        :param tick: The time (resolution defined by `tick_type`)\n        :param phase: The phase of training (`Train`, `Valid`, `Test`)\n        :param tick_type: The resolution of tick (`STEP`, `EPOCH`)\n        :return:\n        """"""\n        print(\'%s [%d] [%s]\' % (tick_type, tick, phase))\n        print(\'=================================================\')\n        for k, v in metrics.items():\n            if k not in [\'avg_loss\', \'perplexity\']:\n                v *= 100.\n            print(\'\\t%s=%.3f\' % (k, v))\n        print(\'-------------------------------------------------\')\n\n\n@register_reporting(name=\'step_logging\')\nclass StepLoggingReporting(StepReportingHook):\n    def __init__(self, **kwargs):\n        super(StepLoggingReporting, self).__init__(**kwargs)\n        self.log = logging.getLogger(\'baseline\')\n\n    def _step(self, metrics, tick, phase, tick_type=None, **kwargs):\n        """"""Write results to Python\'s `logging` module under `root`\n\n        :param metrics: A map of metrics to scores\n        :param tick: The time (resolution defined by `tick_type`)\n        :param phase: The phase of training (`Train`, `Valid`, `Test`)\n        :param tick_type: The resolution of tick (`STEP`, `EPOCH`)\n        :return:\n        """"""\n        msg = {\'tick_type\': tick_type, \'tick\': tick, \'phase\': phase }\n        for k, v in metrics.items():\n            msg[k] = v\n        self.log.info(msg)\n\n\n@register_reporting(name=\'logging\')\nclass LoggingReporting(EpochReportingHook):\n    def __init__(self, **kwargs):\n        super(LoggingReporting, self).__init__(**kwargs)\n        self.log = logging.getLogger(\'baseline.reporting\')\n\n    def _step(self, metrics, tick, phase, tick_type=None, **kwargs):\n        """"""Write results to Python\'s `logging` module under `baseline.reporting`\n\n        :param metrics: A map of metrics to scores\n        :param tick: The time (resolution defined by `tick_type`)\n        :param phase: The phase of training (`Train`, `Valid`, `Test`)\n        :param tick_type: The resolution of tick (`STEP`, `EPOCH`)\n        :return:\n        """"""\n        msg = {\'tick_type\': tick_type, \'tick\': tick, \'phase\': phase }\n        for k, v in metrics.items():\n            msg[k] = v\n        self.log.info(msg)\n\n\n@register_reporting(name=\'tensorboard\')\nclass TensorBoardReporting(ReportingHook):\n    """"""Log results to tensorboard.\n\n    Writes tensorboard logs to a directory specified in the `mead-settings`\n    section for tensorboard. Otherwise it defaults to `runs`.\n    """"""\n    def __init__(self, **kwargs):\n        super(TensorBoardReporting, self).__init__(**kwargs)\n        from tensorboardX import SummaryWriter\n        # Base dir is often the dir created to save the model into\n        base_dir = kwargs.get(\'base_dir\', \'.\')\n        log_dir = os.path.expanduser(kwargs.get(\'log_dir\', \'runs\'))\n        if not os.path.isabs(log_dir):\n            log_dir = os.path.join(base_dir, log_dir)\n        # Run dir is the name of an individual run\n        run_dir = kwargs.get(\'run_dir\')\n        pid = str(os.getpid())\n        run_dir = \'{}-{}\'.format(run_dir, pid) if run_dir is not None else pid\n        log_dir = os.path.join(log_dir, run_dir)\n        flush_secs = int(kwargs.get(\'flush_secs\', 2))\n        self._log = SummaryWriter(log_dir, flush_secs=flush_secs)\n\n    def step(self, metrics, tick, phase, tick_type=None, **kwargs):\n        tick_type = ReportingHook._infer_tick_type(phase, tick_type)\n        for metric in metrics.keys():\n            name = ""{}/{}/{}"".format(phase, tick_type, metric)\n            self._log.add_scalar(name, metrics[metric], tick)\n\n\n@register_reporting(name=\'visdom\')\nclass VisdomReporting(ReportingHook):\n    """"""\n    To use this:\n    - python -m visdom.server\n    - http://localhost:8097/\n    """"""\n    def __init__(self, **kwargs):\n        super(VisdomReporting, self).__init__(**kwargs)\n        import visdom\n        name = kwargs.get(\'name\', \'main\')\n        logger.info(\'Creating g_vis instance with env {}\'.format(name))\n        self._vis = visdom.Visdom(env=name, use_incoming_socket=False)\n        self._vis_win = {}\n\n    def step(self, metrics, tick, phase, tick_type=None, **kwargs):\n        """"""This method will write its results to visdom\n\n        :param metrics: A map of metrics to scores\n        :param tick: The time (resolution defined by `tick_type`)\n        :param phase: The phase of training (`Train`, `Valid`, `Test`)\n        :param tick_type: The resolution of tick (`STEP`, `EPOCH`)\n        :return:\n        """"""\n        tick_type = ReportingHook._infer_tick_type(phase, tick_type)\n        for metric in metrics.keys():\n            chart_id = \'({} - {}) {}\'.format(phase, tick_type, metric)\n            if chart_id not in self._vis_win:\n                logger.info(\'Creating visualization for %s\' % chart_id)\n                self._vis_win[chart_id] = self._vis.line(\n                    X=np.array([0]),\n                    Y=np.array([metrics[metric]]),\n                    opts=dict(\n                        fillarea=True,\n                        xlabel=\'Time\',\n                        ylabel=\'Metric\',\n                        title=chart_id,\n                    ),\n                )\n            else:\n                self._vis.line(\n                    X=np.array([tick]),\n                    Y=np.array([metrics[metric]]),\n                    win=self._vis_win[chart_id],\n                    update=\'append\'\n                )\n\n\n@export\ndef create_reporting(reporting_hooks, hook_settings, proc_info):\n    reporting = [LoggingReporting()]\n\n    for name in reporting_hooks:\n        ReportingClass = BASELINE_REPORTING[name]\n        reporting_args = hook_settings.get(name, {})\n        reporting_args.update(proc_info)\n        reporting.append(ReportingClass(**reporting_args))\n\n    return reporting\n'"
baseline/services.py,1,"b'import os\nimport pickle\nimport logging\nfrom typing import Optional, List\nfrom collections import defaultdict\nimport numpy as np\nimport baseline\nfrom baseline.utils import (\n    exporter,\n    unzip_files,\n    find_model_basename,\n    find_files_with_prefix,\n    import_user_module,\n    read_json,\n    is_sequence,\n    Offsets,\n    topk,\n    to_numpy,\n    revlut,\n    load_vectorizers,\n    load_vocabs,\n    lookup_sentence,\n    normalize_backend,\n)\nfrom baseline.model import load_model_for\n\n\nlogger = logging.getLogger(\'baseline\')\n__all__ = []\nexport = exporter(__all__)\n\n\nclass Service:\n\n    def __init__(self, vocabs=None, vectorizers=None, model=None, preproc=\'client\'):\n        self.vectorizers = vectorizers\n        self.model = model\n        self.vocabs = vocabs\n        self.preproc = preproc\n\n    def get_vocab(self, vocab_type=\'word\'):\n        return self.vocabs.get(vocab_type)\n\n    def get_labels(self):\n        return self.model.get_labels()\n\n    @classmethod\n    def signature_name(cls):\n        raise Exception(""Undefined signature name"")\n\n    @classmethod\n    def task_name(cls):\n        raise Exception(""Undefined task name"")\n\n    def predict(self, tokens, **kwargs):\n        pass\n\n    def format_output(self, predicted, **kwargs):\n        """"""Turn the results of self.model.predict into our output format\n\n        :param predicted: The results from self.model.predict\n        :returns: Formatted output, different for each task\n        """"""\n\n    def batch_input(self, tokens):\n        """"""Turn the input into a consistent format.\n        :param tokens: tokens in format List[str] or List[List[str]]\n        :return: List[List[str]]\n        """"""\n        # If the input is List[str] wrap it in list to make a batch of size one.\n        return (tokens,) if isinstance(tokens[0], str) else tokens\n\n    def prepare_vectorizers(self, tokens_batch):\n        """"""Batch the input tokens, and call reset and count method on each vectorizers to set up their mxlen.\n           This method is mainly for reducing repeated code blocks.\n\n        :param tokens_batch: input tokens in format or List[List[str]]\n        """"""\n        for vectorizer in self.vectorizers.values():\n            vectorizer.reset()\n            for tokens in tokens_batch:\n                _ = vectorizer.count(tokens)\n\n    def vectorize(self, tokens_batch):\n        """"""Turn the input into that batch dict for prediction.\n\n        :param tokens_batch: `List[List[str]]`: The input text batch.\n\n        :returns: dict[str] -> np.ndarray: The vectorized batch.\n        """"""\n        examples = defaultdict(list)\n        for i, tokens in enumerate(tokens_batch):\n            for k, vectorizer in self.vectorizers.items():\n                vec, length = vectorizer.run(tokens, self.vocabs[k])\n                examples[k].append(vec)\n                if length is not None:\n                    lengths_key = \'{}_lengths\'.format(k)\n                    examples[lengths_key].append(length)\n\n        for k in self.vectorizers.keys():\n            examples[k] = np.stack(examples[k])\n            lengths_key = \'{}_lengths\'.format(k)\n            if lengths_key in examples:\n                examples[lengths_key] = np.stack(examples[lengths_key])\n        return examples\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        """"""Load a model from a bundle.\n\n        This can be either a local model or a remote, exported model.\n\n        :returns a Service implementation\n        """"""\n        # can delegate\n        basehead = None\n\n        if os.path.isdir(bundle):\n            directory = bundle\n        elif os.path.isfile(bundle):\n            directory = unzip_files(bundle)\n        else:\n            directory = os.path.dirname(bundle)\n            basehead = os.path.basename(bundle)\n        model_basename = find_model_basename(directory, basehead)\n        suffix = model_basename.split(\'-\')[-1] + "".json""\n        vocabs = load_vocabs(directory, suffix)\n\n        be = normalize_backend(kwargs.get(\'backend\', \'tf\'))\n\n        remote = kwargs.get(""remote"", None)\n        name = kwargs.get(""name"", None)\n        if remote:\n            logging.debug(""loading remote model"")\n            beam = int(kwargs.get(\'beam\', 30))\n            model, preproc = Service._create_remote_model(\n                directory, be, remote, name, cls.task_name(), cls.signature_name(), beam,\n                preproc=kwargs.get(\'preproc\', \'client\'),\n                version=kwargs.get(\'version\'),\n                remote_type=kwargs.get(\'remote_type\'),\n            )\n            vectorizers = load_vectorizers(directory)\n            return cls(vocabs, vectorizers, model, preproc)\n\n        # Currently nothing to do here\n        # labels = read_json(os.path.join(directory, model_basename) + \'.labels\')\n\n        import_user_module(\'baseline.{}.embeddings\'.format(be))\n        try:\n            import_user_module(\'baseline.{}.{}\'.format(be, cls.task_name()))\n        except:\n            pass\n        model = load_model_for(cls.task_name(), model_basename, **kwargs)\n        vectorizers = load_vectorizers(directory)\n        return cls(vocabs, vectorizers, model, \'client\')\n\n    @staticmethod\n    def _create_remote_model(directory, backend, remote, name, task_name, signature_name, beam, **kwargs):\n        """"""Reads the necessary information from the remote bundle to instatiate\n        a client for a remote model.\n\n        :directory the location of the exported model bundle\n        :remote a url endpoint to hit\n        :name the model name, as defined in tf-serving\'s model.config\n        :signature_name  the signature to use.\n        :beam used for s2s and found in the kwargs. We default this and pass it in.\n\n        :returns a RemoteModel\n        """"""\n        from baseline.remote import create_remote\n        assets = read_json(os.path.join(directory, \'model.assets\'))\n        model_name = assets[\'metadata\'][\'exported_model\']\n        preproc = assets[\'metadata\'].get(\'preproc\', kwargs.get(\'preproc\', \'client\'))\n        labels = read_json(os.path.join(directory, model_name) + \'.labels\')\n        lengths_key = assets.get(\'lengths_key\', None)\n        inputs = assets.get(\'inputs\', [])\n        return_labels = bool(assets[\'metadata\'][\'return_labels\'])\n        version = kwargs.get(\'version\')\n\n        if backend not in {\'tf\'}:\n            raise ValueError(""only Tensorflow is currently supported for remote Services"")\n        import_user_module(\'baseline.{}.remote\'.format(backend))\n        exp_type = kwargs.get(\'remote_type\')\n        if exp_type is None:\n            exp_type = \'http\' if remote.startswith(\'http\') else \'grpc\'\n            exp_type = \'{}-preproc\'.format(exp_type) if preproc == \'server\' else exp_type\n            exp_type = f\'{exp_type}-{task_name}\'\n        model = create_remote(\n            exp_type,\n            remote=remote, name=name,\n            signature=signature_name,\n            labels=labels,\n            lengths_key=lengths_key,\n            inputs=inputs,\n            beam=beam,\n            return_labels=return_labels,\n            version=version,\n        )\n        return model, preproc\n\n@export\nclass ClassifierService(Service):\n    def __init__(self, vocabs=None, vectorizers=None, model=None, preproc=\'client\'):\n        super().__init__(vocabs, vectorizers, model, preproc)\n        if hasattr(self.model, \'return_labels\'):\n            self.return_labels = self.model.return_labels\n        else:\n            self.return_labels = True  # keeping the default classifier behavior\n        if not self.return_labels:\n            self.label_vocab = {index: label for index, label in enumerate(self.get_labels())}\n\n    @classmethod\n    def task_name(cls):\n        return \'classify\'\n\n    @classmethod\n    def signature_name(cls):\n        return \'predict_text\'\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        backend = kwargs.get(\'backend\', \'tf\')\n        if backend == \'onnx\':\n            return ONNXClassifierService.load(bundle, **kwargs)\n        return super().load(bundle, **kwargs)\n\n    def predict(self, tokens, preproc=None, raw=False, dense=False):\n        """"""Take tokens and apply the internal vocab and vectorizers.  The tokens should be either a batch of text\n        single utterance of type ``list``\n        """"""\n        if preproc is not None:\n            logger.warning(""Warning: Passing `preproc` to `ClassifierService.predict` is deprecated."")\n        if raw and not dense:\n            logger.warning(""Warning: `raw` parameter is deprecated pass `dense=True` to get back values as a single tensor"")\n            dense = True\n        tokens_batch = self.batch_input(tokens)\n        self.prepare_vectorizers(tokens_batch)\n        if self.preproc == ""client"":\n            examples = self.vectorize(tokens_batch)\n        elif self.preproc == \'server\':\n            # TODO: here we allow vectorizers even for preproc=server to get `word_lengths`.\n            # vectorizers should not be available when preproc=server.\n            featurized_examples = self.vectorize(tokens_batch)\n            examples = {\n                        \'tokens\': np.array(["" "".join(x) for x in tokens_batch]),\n                        self.model.lengths_key: featurized_examples[self.model.lengths_key]\n            }\n\n        outcomes_list = self.model.predict(examples, dense=dense)\n        return self.format_output(outcomes_list, dense=dense)\n\n    def format_output(self, predicted, dense=False, **kwargs):\n        if dense:\n            return predicted\n        results = []\n        for outcomes in predicted:\n            if self.return_labels:\n                results += [list(map(lambda x: (x[0], x[1].item()), sorted(outcomes, key=lambda tup: tup[1], reverse=True)))]\n            else:\n                results += [list(map(lambda x: (self.label_vocab[x[0].item()], x[1].item()),\n                                     sorted(outcomes, key=lambda tup: tup[1], reverse=True)))]\n        return results\n\n\n@export\nclass ONNXClassifierService(ClassifierService):\n\n    def __init__(self, vocabs=None, vectorizers=None, model=None, labels=None, lengths_key=None, **kwargs):\n        self.label_vocab = labels\n        self.lengths_key = lengths_key\n        super().__init__(vocabs, vectorizers, model, **kwargs)\n        self.return_labels = False\n\n    def get_labels(self):\n        return self.labels\n\n    def predict(self, tokens, **kwargs):\n        tokens_batch = self.batch_input(tokens)\n        self.prepare_vectorizers(tokens_batch)\n        # Hide the fact we can only do one at a time\n        examples = [self.vectorize([tokens]) for tokens in tokens_batch]\n        outcomes_list = np.concatenate([self.model.run(None, example)[0] for example in examples])\n        return self.format_output(outcomes_list, dense=kwargs.get(\'dense\', False))\n\n    def vectorize(self, tokens_batch):\n        """"""Turn the input into that batch dict for prediction.\n\n        :param tokens_batch: `List[List[str]]`: The input text batch.\n\n        :returns: dict[str] -> np.ndarray: The vectorized batch.\n        """"""\n        examples = defaultdict(list)\n        if self.lengths_key is None:\n            self.lengths_key = list(self.vectorizers.keys())[0]\n\n        for i, tokens in enumerate(tokens_batch):\n            for k, vectorizer in self.vectorizers.items():\n                vec, length = vectorizer.run(tokens, self.vocabs[k])\n                examples[k].append(vec)\n                if self.lengths_key == k and length:\n                    #examples[f\'{self.lengths_key}_lengths\'].append(length)\n                    examples[\'lengths\'].append(length)\n        for k in self.vectorizers.keys():\n            examples[k] = np.stack(examples[k])\n        if \'lengths\' in examples:\n            examples[\'lengths\'] = np.stack(examples[\'lengths\'])\n        return examples\n\n    def format_output(self, predicted, dense=False):\n        if dense:\n            return predicted\n        results = []\n        for outcomes in predicted:\n            outcomes = list(zip(self.label_vocab, outcomes))\n            results += [list(map(lambda x: (x[0], x[1].item()), sorted(outcomes, key=lambda tup: tup[1], reverse=True)))]\n        return results\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        """"""Load a model from a bundle.\n\n        This can be either a local model or a remote, exported model.\n\n        :returns a Service implementation\n        """"""\n        import onnxruntime as ort\n\n        # can delegate\n        if os.path.isdir(bundle):\n            directory = bundle\n        # Try and unzip if its a zip file\n        else:\n            directory = unzip_files(bundle)\n\n        model_basename = find_model_basename(directory)\n        # model_basename = model_basename.replace("".pyt"", """")\n        model_name = f""{model_basename}.onnx""\n\n        vocabs = load_vocabs(directory)\n        vectorizers = load_vectorizers(directory)\n\n        # Currently nothing to do here\n        labels = read_json(model_basename + \'.labels\')\n\n        model = ort.InferenceSession(model_name)\n        return cls(vocabs, vectorizers, model, labels)\n\n\n\n\n@export\nclass EmbeddingsService(Service):\n    @classmethod\n    def task_name(cls):\n        return \'servable-embeddings\'\n\n    @classmethod\n    def signature_name(cls):\n        return \'embed_text\'\n\n    def predict(self, tokens, preproc=None):\n        if preproc is not None:\n            logger.warning(""Warning: Passing `preproc` to `EmbeddingsService.predict` is deprecated."")\n        tokens_batch = self.batch_input(tokens)\n        self.prepare_vectorizers(tokens_batch)\n        if self.preproc == \'client\':\n            examples = self.vectorize(tokens_batch)\n        else:\n            examples = {\n                \'tokens\': np.array(["" "".join(x) for x in tokens_batch]),\n            }\n        return self.format_output(self.model.predict(examples))\n\n    def format_output(self, predicted):\n        return predicted\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        import_user_module(\'hub:v1:addons:create_servable_embeddings\')\n        return super().load(bundle, **kwargs)\n\n\n\n\n@export\nclass TaggerService(Service):\n\n    def __init__(self, vocabs=None, vectorizers=None, model=None, preproc=\'client\'):\n        super().__init__(vocabs, vectorizers, model, preproc)\n        if hasattr(self.model, \'return_labels\'):\n            self.return_labels = self.model.return_labels\n        else:\n            self.return_labels = False  # keeping the default tagger behavior\n        if not self.return_labels:\n            self.label_vocab = revlut(self.get_labels())\n\n    @classmethod\n    def task_name(cls):\n        return \'tagger\'\n\n    @classmethod\n    def signature_name(cls):\n        return \'tag_text\'\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        backend = kwargs.get(\'backend\', \'tf\')\n        if backend == \'onnx\':\n            return ONNXTaggerService.load(bundle, **kwargs)\n        return super().load(bundle, **kwargs)\n\n    def batch_input(self, tokens):\n        """"""Convert the input into a consistent format.\n\n        :return: List[List[dict[str] -> str]]\n        """"""\n        # Input is a list of strings. (assume strings are tokens)\n        if isinstance(tokens[0], str):\n            tokens_batch = []\n            for t in tokens:\n                tokens_batch.append({\'text\': t})\n            tokens_batch = [tokens_batch]\n        else:\n            # Better be a sequence, but it could be pre-batched, [[],[]]\n            # But what kind of object is at the first one then?\n            if is_sequence(tokens[0]):\n                tokens_batch = []\n                # Then what we have is [[\'The\', \'dog\',...], [\'I\', \'cannot\']]\n                # [[{\'text\': \'The\', \'pos\': \'DT\'}, ...\n\n                # For each of the utterances, we need to make a dictionary\n                if isinstance(tokens[0][0], str):\n                    for utt in tokens:\n                        utt_dict_seq = []\n                        for t in utt:\n                            utt_dict_seq += [dict({\'text\': t})]\n                        tokens_batch += [utt_dict_seq]\n                # Its already in List[List[dict]] form, do nothing\n                elif isinstance(tokens[0][0], dict):\n                    tokens_batch = tokens\n            # If its a dict, we just wrap it up\n            elif isinstance(tokens[0], dict):\n                tokens_batch = [tokens]\n            else:\n                raise Exception(\'Unknown input format\')\n\n        if len(tokens_batch) == 0:\n            return []\n        return tokens_batch\n\n    def predict(self, tokens, **kwargs):\n        """"""\n        Utility function to convert lists of sentence tokens to integer value one-hots which\n        are then passed to the tagger.  The resultant output is then converted back to label and token\n        to be printed.\n\n        This method is not aware of any input features other than words and characters (and lengths).  If you\n        wish to use other features and have a custom model that is aware of those, use `predict` directly.\n\n        :param tokens: (``list``) A list of tokens\n\n        """"""\n        preproc = kwargs.get(\'preproc\', None)\n        if preproc is not None:\n            logger.warning(""Warning: Passing `preproc` to `TaggerService.predict` is deprecated."")\n        export_mapping = kwargs.get(\'export_mapping\', {})  # if empty dict argument was passed\n        if not export_mapping:\n            export_mapping = {\'tokens\': \'text\'}\n        label_field = kwargs.get(\'label\', \'label\')\n        tokens_batch = self.batch_input(tokens)\n        self.prepare_vectorizers(tokens_batch)\n        # TODO: here we allow vectorizers even for preproc=server to get `word_lengths`.\n        # vectorizers should not be available when preproc=server.\n        examples = self.vectorize(tokens_batch)\n        if self.preproc == \'server\':\n            unfeaturized_examples = {}\n            for exporter_field in export_mapping:\n                unfeaturized_examples[exporter_field] = np.array(["" "".join([y[export_mapping[exporter_field]]\n                                                                   for y in x]) for x in tokens_batch])\n            unfeaturized_examples[self.model.lengths_key] = examples[self.model.lengths_key]  # remote model\n            examples = unfeaturized_examples\n\n        outcomes = self.model.predict(examples)\n        return self.format_output(outcomes, tokens_batch=tokens_batch, label_field=label_field)\n\n    def format_output(self, predicted, tokens_batch=None, label_field=\'label\', **kwargs):\n        assert tokens_batch is not None\n        outputs = []\n        for i, outcome in enumerate(predicted):\n            output = []\n            for j, token in enumerate(tokens_batch[i]):\n                new_token = dict()\n                new_token.update(token)\n                if self.return_labels:\n                    new_token[label_field] = outcome[j]\n                else:\n                    new_token[label_field] = self.label_vocab[outcome[j].item()]\n                output += [new_token]\n            outputs += [output]\n        return outputs\n\n\nclass ONNXTaggerService(TaggerService):\n\n    def __init__(self, vocabs=None, vectorizers=None, model=None, labels=None, lengths_key=None, **kwargs):\n        self.labels = labels\n        self.lengths_key = lengths_key\n        super().__init__(vocabs, vectorizers, model)\n\n    def get_vocab(self, vocab_type=\'word\'):\n        return self.vocabs.get(vocab_type)\n\n    def get_labels(self):\n        return self.labels\n\n    def predict(self, tokens, **kwargs):\n        tokens_batch = self.batch_input(tokens)\n        self.prepare_vectorizers(tokens_batch)\n        # Process each example in the batch by itself to hide the one at a time nature\n        examples = [self.vectorize([tokens]) for tokens in tokens_batch]\n        outcomes_list = np.concatenate([self.model.run(None, example)[0] for example in examples], axis=0)\n        return self.format_output(outcomes_list, tokens_batch, label_field=kwargs.get(\'label\', \'label\'))\n\n    def vectorize(self, tokens_batch):\n        """"""Turn the input into that batch dict for prediction.\n\n        :param tokens_batch: `List[List[str]]`: The input text batch.\n\n        :returns: dict[str] -> np.ndarray: The vectorized batch.\n        """"""\n        examples = defaultdict(list)\n        if self.lengths_key is None:\n            self.lengths_key = list(self.vectorizers.keys())[0]\n\n        for i, tokens in enumerate(tokens_batch):\n            for k, vectorizer in self.vectorizers.items():\n                vec, length = vectorizer.run(tokens, self.vocabs[k])\n                examples[k].append(vec)\n                if self.lengths_key == k and length:\n                    examples[\'lengths\'].append(length)\n        for k in self.vectorizers.keys():\n            examples[k] = np.stack(examples[k])\n        if \'lengths\' in examples:\n            examples[\'lengths\'] = np.stack(examples[\'lengths\'])\n        return examples\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        """"""Load a model from a bundle.\n\n        This can be either a local model or a remote, exported model.\n\n        :returns a Service implementation\n        """"""\n        import onnxruntime as ort\n        if os.path.isdir(bundle):\n            directory = bundle\n        else:\n            directory = unzip_files(bundle)\n\n        model_basename = find_model_basename(directory)\n        model_name = f""{model_basename}.onnx""\n\n        vocabs = load_vocabs(directory)\n        vectorizers = load_vectorizers(directory)\n\n        # Currently nothing to do here\n        labels = read_json(model_basename + \'.labels\')\n\n        model = ort.InferenceSession(model_name)\n        return cls(vocabs, vectorizers, model, labels)\n\n\n@export\nclass LanguageModelService(Service):\n\n    @classmethod\n    def task_name(cls):\n        return ""lm""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.idx_to_token = revlut(self.vocabs[self.model.tgt_key])\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        kwargs[\'batchsz\'] = 1\n        return super().load(bundle, **kwargs)\n\n    def conditional(self, context, target: Optional[List[str]] = None, limit: Optional[int] = None, raw: bool = False, **kwargs):\n        """"""Get the conditional probabilities of the next tokens.\n\n        :param context: The tokens\n        :param target: A list of values that you want the conditional prob of P(target | context)\n        :param limit: The number of (next word, score) pairs to return\n        :param raw: Should you just return the raw softmax values? This will override the limit argument\n\n        :returns: The conditional probs of a specific target word if `target` is defined, the top limit softmax scores\n            for the next step, or the raw softmax numpy array if `raw` is set\n        """"""\n        if kwargs.get(\'preproc\', None) is not None:\n            logger.warning(""Warning: Passing `preproc` to `LanguageModelService.predict` is deprecated."")\n        tokens_batch = self.batch_input(context)\n        self.prepare_vectorizers(tokens_batch)\n        batch_dict = self.vectorize(tokens_batch)\n        next_softmax = self.model.predict(batch_dict)[:, -1, :]\n        next_softmax = to_numpy(next_softmax)\n        if target is not None:\n            target_batch = [[t] for t in target]\n            self.prepare_vectorizers(target_batch)\n            target_batch = self.vectorize(target_batch)\n            target = target_batch[self.model.tgt_key]\n            return np.array([v.item() for v in next_softmax[np.arange(next_softmax.shape[0]), target]])\n        if raw:\n            return next_softmax\n        limit = next_softmax.shape[-1] if limit is None else limit\n        scores = [topk(limit, soft) for soft in next_softmax]\n        return [{self.idx_to_token[k]: v for k, v in score.items()} for score in scores]\n\n    @staticmethod\n    def pad_eos(tokens_batch):\n        """"""Add <EOS> tokens to both the beginning of each item in the batch.\n\n        Note:\n            When training the language models we have and <EOS> token between each sentence we use\n            that here to represent these tokens ending where they do. Because each sentence end with\n            <EOS> the next sentence always starts with the <EOS> so we add that here too.\n        """"""\n        return [[Offsets.VALUES[Offsets.EOS]] + t + [Offsets.VALUES[Offsets.EOS]] for t in tokens_batch]\n\n    def joint(self, tokens, **kwargs):\n        """"""Score tokens with a language model.\n\n        Note:\n            This is not quite the correct joint probability I think, the joint prob of P(wn, wn-1, ... w1)\n            is P(w_1) * \\pi_2^n P(w_i| w_i-1, ... w_1) but here we have the P(w_1 | <EOS>) which is the\n            prob of w_1 staring a string, not quite the same as the unconditional probability.\n\n        :tokens: A sequence of tokens\n\n        returns: The score based on the probability type\n        """"""\n        if kwargs.get(\'preproc\', None) is not None:\n            logger.warning(""Warning: Passing `preproc` to `LanguageModelService.predict` is deprecated."")\n        tokens_batch = self.batch_input(tokens)\n        tokens_batch = self.pad_eos(tokens_batch)\n        self.prepare_vectorizers(tokens_batch)\n        batch_dict = self.vectorize(tokens_batch)\n        softmax_tokens = self.model.predict(batch_dict)\n\n        # Get the targets, the first is a <EOS> seed so we skip that\n        values = batch_dict[self.model.tgt_key][:, 1:]\n\n        # Numpy doesn\'t have a gather so we can\'t do this very efficiently\n        # scores = torch.gather(softmax_tokens, -1, values.unsqueeze(-1))\n        scores = []\n        # The last softmax is what comes after the <EOS> so we don\'t grab from there\n        for soft, value in zip(softmax_tokens[:, :-1], values):\n            tokens = []\n            for tok, val in zip(soft, value):\n                tokens.append(to_numpy(tok)[val].item())\n            scores.append(tokens)\n        scores = np.array(scores)\n        return np.exp(np.sum(np.log(scores), axis=1))\n\n    # Do a greedy decode for now, everything else will be super slow\n    def predict(self, tokens, **kwargs):\n        mxlen = kwargs.get(\'mxlen\', 10)\n        mxwlen = kwargs.get(\'mxwlen\', 40)\n\n        for k, vectorizer in self.vectorizers.items():\n            if hasattr(vectorizer, \'mxlen\') and vectorizer.mxlen == -1:\n                vectorizer.mxlen = mxlen\n            if hasattr(vectorizer, \'mxwlen\') and vectorizer.mxwlen == -1:\n                vectorizer.mxwlen = mxwlen\n\n        token_buffer = tokens\n        tokens_seq = tokens\n        examples = dict()\n        for i in range(mxlen):\n\n            for k, vectorizer in self.vectorizers.items():\n                vectorizer.mxlen = len(token_buffer)\n                vec, length = vectorizer.run(token_buffer, self.vocabs[k])\n                if k in examples:\n                    examples[k] = np.append(examples[k], vec)\n                else:\n                    examples[k] = vec\n\n                if length is not None:\n                    lengths_key = \'{}_lengths\'.format(k)\n                    if lengths_key in examples:\n                        examples[lengths_key] += length\n                    else:\n                        examples[lengths_key] = np.array(length)\n            batch_dict = {k: v.reshape((1,) + v.shape) for k, v in examples.items()}\n            softmax_tokens = self.model.predict(batch_dict)\n            next_token = np.argmax(softmax_tokens[:, -1, :], axis=-1)[0]\n            token_str = self.idx_to_token.get(next_token, \'<PAD>\')\n            if token_str == \'<EOS>\':\n                break\n            if token_str != \'<PAD>\':\n                tokens_seq += [token_str]\n            token_buffer = [token_str]\n        return tokens_seq\n\n\n@export\nclass EncoderDecoderService(Service):\n\n    @classmethod\n    def task_name(cls):\n        return \'seq2seq\'\n\n    @classmethod\n    def signature_name(cls):\n        return \'suggest_text\'\n\n    def __init__(self, vocabs=None, vectorizers=None, model=None, preproc=\'client\'):\n        super(EncoderDecoderService, self).__init__(None, None, model, preproc)\n        self.src_vocabs = {}\n        self.tgt_vocab = None\n        for k, vocab in vocabs.items():\n            if k == \'tgt\':\n                self.tgt_vocab = vocab\n            else:\n                self.src_vocabs[k] = vocab\n\n        self.tgt_idx_to_token = revlut(self.tgt_vocab)\n        self.src_vectorizers = {}\n        self.tgt_vectorizer = None\n        for k, vectorizer, in vectorizers.items():\n            if k == \'tgt\':\n                self.tgt_vectorizer = vectorizer\n            else:\n                self.src_vectorizers[k] = vectorizer\n\n    def src_vocab(self, vocab_type):\n        return self.src_vocabs[vocab_type]\n\n    def get_tgt_vocab(self):\n        return self.tgt_vocab\n\n    @classmethod\n    def load(cls, bundle, **kwargs):\n        kwargs[\'predict\'] = kwargs.get(\'predict\', True)\n        kwargs[\'beam\'] = int(kwargs.get(\'beam\', 30))\n        return super().load(bundle, **kwargs)\n\n    def vectorize(self, tokens_batch):\n        examples = defaultdict(list)\n        for i, tokens in enumerate(tokens_batch):\n            for k, vectorizer in self.src_vectorizers.items():\n                vec, length = vectorizer.run(tokens, self.src_vocabs[k])\n                examples[k] += [vec]\n                if length is not None:\n                    lengths_key = \'{}_lengths\'.format(k)\n                    examples[lengths_key] += [length]\n\n        for k in self.src_vectorizers.keys():\n            examples[k] = np.stack(examples[k])\n            lengths_key = \'{}_lengths\'.format(k)\n            if lengths_key in examples:\n                examples[lengths_key] = np.array(examples[lengths_key])\n        return examples\n\n    def predict(self, tokens, K=1, **kwargs):\n        tokens_batch = self.batch_input(tokens)\n        for vectorizer in self.src_vectorizers.values():\n            vectorizer.reset()\n            for tokens in tokens_batch:\n                _ = vectorizer.count(tokens)\n        examples = self.vectorize(tokens_batch)\n\n        kwargs[\'beam\'] = int(kwargs.get(\'beam\', K))\n        outcomes = self.model.predict(examples, **kwargs)\n        return self.format_output(outcomes, K=K)\n\n    def format_output(self, predicted, K=1, **kwargs):\n        results = []\n        B = len(predicted)\n        for i in range(B):\n            N = len(predicted[i])\n            n_best_result = []\n            for n in range(min(K, N)):\n                n_best = predicted[i][n]\n                out = lookup_sentence(self.tgt_idx_to_token, n_best).split()\n                if K == 1:\n                    results += [out]\n                else:\n                    n_best_result += [out]\n            if K > 1:\n                results.append(n_best_result)\n        return results\n'"
baseline/train.py,0,"b'import time\nimport logging\nimport numpy as np\nimport eight_mile.optz\nfrom eight_mile.optz import *\nfrom eight_mile.utils import exporter, optional_params\nfrom baseline.utils import register\nimport math\n\n\n__all__ = []\n__all__.extend(eight_mile.optz.__all__)\nexport = exporter(__all__)\n\n\n@export\nclass Trainer:\n\n    def __init__(self):\n        self.train_epochs = 0\n        self.valid_epochs = 0\n        self.nsteps = None\n        self.nstep_agg = 0\n        self.nstep_div = 0\n        self.nstep_start = 0\n        self.log = logging.getLogger(\'baseline.timing\')\n\n    def report(self, step, metrics, start, phase, tt, reporting_fns, steps=1):\n        """"""Make a report (both metric and timinging).\n\n        :param step: `int` The step number of this report (epoch or nstep number).\n        :param metrics: `dict` The metrics to report.\n        :param start: `int` The starting time of this segment.\n        :param phase: `str` The phase type. {\'Train\', \'Valid\', \'Test\'}\n        :param tt: `str` The tick type. {\'STEP\', \'EPOCH\'}\n        :param reporting_fns: `List[Callable]` The list of reporting functions to call.\n        :param steps: `int` The number of steps in this segment, used to normalize the time.\n        """"""\n        elapsed = time.time() - start\n        for reporting in reporting_fns:\n            reporting(metrics, step, phase, tt)\n        self.log.debug({\n            \'tick_type\': tt, \'tick\': step, \'phase\': phase,\n            \'time\': elapsed / float(steps),\n            \'step/sec\': steps / float(elapsed)\n        })\n\n    def reset_nstep(self):\n        self.nstep_agg = 0\n        self.nstep_div = 0\n        self.nstep_start = time.time()\n\n    @staticmethod\n    def calc_metrics(agg, norm):\n        return {\'avg_loss\': agg / float(norm)}\n\n    def test(self, loader, reporting_fns):\n        pass\n\n    def train(self, loader, reporting_fns):\n        pass\n\n\n@export\nclass EpochReportingTrainer(Trainer):\n\n    def __init__(self):\n        super().__init__()\n\n    def train(self, ts, reporting_fns, **kwargs):\n        start_time = time.time()\n        self.nstep_start = start_time\n        metrics = self._train(ts, reporting_fns=reporting_fns, **kwargs)\n        self.train_epochs += 1\n        self.report(\n            self.train_epochs, metrics, start_time,\n            \'Train\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def test(self, vs, reporting_fns, phase=\'Valid\', **kwargs):\n        start_time = time.time()\n        metrics = self._test(vs, **kwargs)\n        epochs = 0\n        if phase == \'Valid\':\n            self.valid_epochs += 1\n            epochs = self.valid_epochs\n        self.report(\n            epochs, metrics, start_time,\n            phase, \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def _train(self, ts):\n        pass\n\n    def _test(self, vs, **kwargs):\n        pass\n\n\nBASELINE_TRAINERS = {}\n\n\n@export\n@optional_params\ndef register_trainer(cls, task, name=None):\n    """"""Register a function as a plug-in\n\n    Use this pattern if you want to provide an override to a `Trainer` class.\n\n    """"""\n    if task not in BASELINE_TRAINERS:\n        BASELINE_TRAINERS[task] = {}\n    if name is None:\n        name = cls.__name__\n    BASELINE_TRAINERS[task][name] = cls\n    return cls\n\n\nBASELINE_FIT_FUNC = {}\n\n\n@export\n@optional_params\ndef register_training_func(func, task, name=None):\n    """"""Register a training by-pass\n\n    Use this pattern if you want to change the entire training hook.  Your function will have to fulfill all the\n    behaviors that fit() normally handles\n\n    :param func:\n    :param name:\n    :return:\n    """"""\n    if task not in BASELINE_FIT_FUNC:\n        BASELINE_FIT_FUNC[task] = {}\n    if name is None:\n        name = \'default\'\n\n    if name in BASELINE_FIT_FUNC[task]:\n        raise Exception(\'Attempting to override the fit function without providing a suitable name\')\n\n    BASELINE_FIT_FUNC[task][name] = func\n    return func\n\n\n@export\ndef fit(model_params, ts, vs, es, **kwargs):\n    """"""This method delegates to the registered fit function for each DL framework.  It is possible to provide a by-pass\n    to our defined fit functions for each method (this is considered advanced usage).  In cases where the user wishes\n    to provide their own fit hook, the need to decorate the bypass hook with @register_training_func(name=\'myname\'),\n    and then pass in the `fit_func=\'myname\'` to this.  MEAD handles this automatically -- just pass fit_func: myname\n    in the mead config if you want your own bypass, in which case training is entirely delegate to the 3rd party code.\n\n    This use-case is expected to be extremely uncommon.  More common behavior would be to override the Trainer and use\n    the provided fit function.\n\n    :param model:\n    :param ts:\n    :param vs:\n    :param es:\n    :param kwargs:\n    :return:\n    """"""\n    if type(model_params) is dict:\n        task_name = model_params[\'task\']\n    else:\n        task_name = model_params.task_name\n    fit_func_name = kwargs.get(\'fit_func\', \'default\')\n    return BASELINE_FIT_FUNC[task_name][fit_func_name](model_params, ts, vs, es, **kwargs)\n\n\n@export\ndef create_trainer(model_params, **kwargs):\n    """"""Create the default trainer, or a user-defined one if `trainer_type` is not `default`\n\n    :param default_create_model_fn: The constructor for the default trainer (defined in each platform/task)\n    :param model: The model to train\n    :param kwargs:\n    :return:\n    """"""\n    trainer_type = kwargs.get(\'trainer_type\', \'default\')\n    if type(model_params) is dict:\n        task_name = model_params[\'task\']\n    else:\n        task_name = model_params.task_name\n    Constructor = BASELINE_TRAINERS[task_name][trainer_type]\n    return Constructor(model_params, **kwargs)\n\n\ndef calc_lr_params(train_params, num_steps):\n    # This belongs in the trainer!\n    if train_params.get(\'lr_scheduler_type\', None) == \'zaremba\':\n        first_range = int(train_params[\'start_decay_epoch\'] * num_steps)\n        train_params[\'boundaries\'] = [first_range] + list(\n            np.arange(\n                train_params[\'start_decay_epoch\'] + 1,\n                train_params[\'epochs\'] + 1,\n                dtype=np.int32\n            ) * num_steps\n        )\n'"
baseline/utils.py,0,"b'import six\nimport os\nimport re\nimport sys\nimport json\nimport gzip\nimport pickle\nimport hashlib\nimport logging\nimport zipfile\nimport platform\nimport shutil\nfrom functools import wraps\nfrom operator import lt, le, gt, ge\nfrom contextlib import contextmanager\nfrom typing import Dict, List, Set, Optional\nimport numpy as np\nimport collections\nimport eight_mile\nimport importlib\nfrom eight_mile.utils import *\nfrom baseline.progress import create_progress_bar\nimport addons\nfrom six.moves.urllib.request import urlretrieve\n\n__all__ = []\n__all__.extend(eight_mile.utils.__all__)\nlogger = logging.getLogger(\'baseline\')\n# These are inputs to models that shouldn\'t be saved out\nMAGIC_VARS = [\'sess\', \'tgt\', \'y\', \'lengths\', \'gpus\']\nMAGIC_VARS = [\'sess\', \'tgt\', \'y\', \'lengths\']\nMEAD_HUB_MODULES = []\nDEFAULT_DATA_CACHE = os.path.expanduser(\'~/.bl-data\')\nexport = exporter(__all__)\n\n\nexport(str2bool)\n\n@export\n@export\ndef import_user_module(module_name: str, data_download_cache: Optional[str] = None):\n    """"""Load a module that is in the python path\n    :param model_name: (``str``) - the name of the module\n    :return:\n    """"""\n    if not data_download_cache and os.path.exists(DEFAULT_DATA_CACHE):\n        data_download_cache = DEFAULT_DATA_CACHE\n    if data_download_cache:\n        if module_name.startswith(""hub:"") or module_name.startswith(""http""):\n            if module_name.startswith(""hub:""):\n                vec = module_name.split("":"")\n                version = vec[1]\n                addons_literal = vec[2]\n                rest = "":"".join(vec[3:])\n                if not rest.endswith("".py""):\n                    rest += "".py""\n                if addons_literal != ""addons"":\n                    raise Exception(""We only support downloading addons right now"")\n                module_name = f""http://raw.githubusercontent.com/mead-ml/hub/master/{version}/addons/{rest}""\n                if module_name in MEAD_HUB_MODULES:\n                    logger.warning(f""Skipping previously downloaded module: {module_name}"")\n                    return None\n                MEAD_HUB_MODULES.append(module_name)\n            module_name = AddonDownloader(module_name, data_download_cache, cache_ignore=True).download()\n\n    # TODO: get rid of this!\n    addon_path = os.path.dirname(os.path.realpath(addons.__file__))\n    idempotent_append(addon_path, sys.path)\n    if any(module_name.endswith(suffix) for suffix in importlib.machinery.SOURCE_SUFFIXES):\n        module_path = module_name\n        module_name, _ = parse_module_as_path(module_path)\n        # File based import from here https://docs.python.org/3.6/library/importlib.html#importing-a-source-file-directly\n        spec = importlib.util.spec_from_file_location(module_name, module_path)\n        mod = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(mod)\n        # Set this module in sys.modules so later we can import the module by name when pickling things.\n        sys.modules[module_name] = mod\n        return mod\n    mod = importlib.import_module(module_name)\n    return mod\n\n\n@export\ndef normalize_backend(name: str) -> str:\n    allowed_backends = {\'tf\', \'pytorch\'}\n    name = name.lower()\n    if name == \'tensorflow\':\n        name = \'tf\'\n    elif name == \'torch\' or name == \'pyt\':\n        name = \'pytorch\'\n    if name not in allowed_backends:\n        raise ValueError(""Supported backends are %s, got %s"" % (allowed_backends, name))\n    return name\n\n\n@export\ndef get_console_logger(name, level=None, env_key=\'LOG_LEVEL\'):\n    """"""A small default logging setup.\n\n    This is a default logging setup to print json formatted logging to\n    the console. This is used as a default for when baseline/mead is used\n    as an API. This can be overridden with the logging config.\n\n    The level defaults to `INFO` but can also be read from an env var\n    of you choice with a back off to `LOG_LEVEL`\n\n    :param name: `str` The logger to create.\n    :param level: `str` The level to look for.\n    :param env_key: `str` The env var to look in.\n\n    :returns: logging.Logger\n    """"""\n    if level is None:\n        level = os.getenv(env_key, os.getenv(\'LOG_LEVEL\', \'INFO\'))\n    level = get_logging_level(level)\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    formatter = JSONFormatter()\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    logger.propagate = False\n    return logger\n\n\n@contextmanager\ndef redirect(from_stream, to_stream):\n    original_from = from_stream.fileno()\n    saved_from = os.dup(original_from)\n    os.dup2(to_stream.fileno(), original_from)\n    try:\n        yield\n        os.dup2(saved_from, original_from)\n    except Exception as e:\n        os.dup2(saved_from, original_from)\n        raise(e)\n\n\n@export\n@contextmanager\ndef suppress_output():\n    with open(os.devnull, \'w\') as devnull, redirect(sys.stdout, devnull), redirect(sys.stderr, devnull):\n        yield\n\n\n@export\nclass Colors(object):\n    GREEN = \'\\033[32;1m\'\n    RED = \'\\033[31;1m\'\n    YELLOW = \'\\033[33;1m\'\n    BLACK = \'\\033[30;1m\'\n    CYAN = \'\\033[36;1m\'\n    RESTORE = \'\\033[0m\'\n\n\n@export\ndef color(msg: str, color: str) -> str:\n    if platform.system() == \'Windows\':\n        return msg\n    return f""{color}{msg}{Colors.RESTORE}""\n\n\nclass ColoredFormatter(logging.Formatter):\n    COLORS = {\n        \'WARNING\': Colors.YELLOW,\n        \'ERROR\': Colors.RED\n    }\n\n    def format(self, record):\n        if record.levelname in self.COLORS:\n            return color(super(ColoredFormatter, self).format(record), self.COLORS[record.levelname])\n        return super().format(record)\n\n\nclass JSONFormatter(ColoredFormatter):\n    """"""Format message as JSON if possible, log normally otherwise.""""""\n    def format(self, record):\n        try:\n            if isinstance(record.msg, (list, dict)):\n                return json.dumps(record.msg)\n        except TypeError:\n            pass\n        return super().format(record)\n\n\nclass MakeFileHandler(logging.FileHandler):\n    """"""A File logger that will create intermediate dirs if need be.""""""\n    def __init__(self, filename, mode=\'a\', encoding=None, delay=0):\n        log_dir = os.path.dirname(filename)\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        super().__init__(filename, mode, encoding, delay)\n\n\n@export\ndef lowercase(x):\n    return x.lower()\n\n\nUNREP_EMOTICONS = (\n    \':)\',\n    \':(((\',\n    \':D\',\n    \'=)\',\n    \':-)\',\n    \'=(\',\n    \'(=\',\n    \'=[[\',\n)\n\n\n@export\ndef web_cleanup(word):\n    if word.startswith(\'http\'): return \'URL\'\n    if word.startswith(\'@\'): return \'@@@@\'\n    if word.startswith(\'#\'): return \'####\'\n    if word == \'""\': return \',\'\n    if word in UNREP_EMOTICONS: return \';)\'\n    if word == \'<3\': return \'&lt;3\'\n    return word\n\n\n@export\ndef get_model_file(task, platform, basedir=None):\n    """"""Model name file helper to abstract different DL platforms (FWs)\n\n    :param dictionary:\n    :param task:\n    :param platform:\n    :return:\n    """"""\n    basedir = \'./\' if basedir is None else basedir\n    base = \'{}/{}-model\'.format(basedir, task)\n    rid = os.getpid()\n    if platform.startswith(\'pyt\'):\n        name = \'%s-%d.pyt\' % (base, rid)\n    else:\n        name = \'%s-%s-%d\' % (base, platform, rid)\n    logger.info(\'model file [%s]\' % name)\n    return name\n\n\n@export\ndef lookup_sentence(rlut: Dict[int, str], seq: List[str], reverse: bool = False, padchar: str = \'\') -> str:\n    """"""Lookup a sentence by id and return words\n\n    :param rlut: an index -> word lookup table\n    :param seq: A temporal sequence\n    :param reverse: (``bool``) Should reverse?\n    :param padchar: What padding character to use when replacing with words\n    :return:\n    """"""\n    s = seq[::-1] if reverse else seq\n    res = []\n    for idx in s:\n        idx = int(idx)\n        char = padchar\n        if idx == Offsets.EOS: break\n        if idx != Offsets.PAD and idx != Offsets.GO:\n            char = rlut[idx]\n        res.append(char)\n    return (\' \'.join(res)).strip()\n\n\n@export\ndef topk(k, probs):\n    """"""Get a sparse index (dictionary of top values).""""""\n    idx = np.argpartition(probs, probs.size-k)[-k:]\n    sort = idx[np.argsort(probs[idx])][::-1]\n    return dict(zip(sort, probs[sort]))\n\n\n@export\ndef beam_multinomial(k, probs):\n    """"""Prune all elements in a large probability distribution below the top K.\n\n    Renormalize the distribution with only top K, and then sample n times out of that.\n    """"""\n\n    tops = topk(k, probs)\n    i = 0\n    n = len(tops.keys())\n    ary = np.zeros((n))\n    idx = []\n    for abs_idx, v in tops.items():\n        ary[i] = v\n        idx.append(abs_idx)\n        i += 1\n\n    ary /= np.sum(ary)\n    sample_idx = np.argmax(np.random.multinomial(1, ary))\n    return idx[sample_idx]\n\n\n@export\ndef unzip_model(path):\n    path = unzip_files(path)\n    return os.path.join(path, [x[:-6] for x in os.listdir(path) if \'index\' in x][0])\n\n\n@export\ndef save_vectorizers(basedir, vectorizers, name=\'vectorizers\'):\n    save_md_file = os.path.join(basedir, \'{}-{}.pkl\'.format(name, os.getpid()))\n    with open(save_md_file, \'wb\') as f:\n        pickle.dump(vectorizers, f)\n    # Save out the vectorizer module names so we can automatically import them\n    # when reloading without going all the way to a pure json save\n    vectorizer_modules = [v.__class__.__module__ for v in vectorizers.values()]\n    module_file = os.path.join(basedir, \'{}-{}.json\'.format(name, os.getpid()))\n    write_json(vectorizer_modules, module_file)\n\n\n@export\ndef save_vocabs(basedir, embeds_or_vocabs, name=\'vocabs\'):\n    for k, embeds_or_vocabs in embeds_or_vocabs.items():\n        save_md = \'{}/{}-{}-{}.json\'.format(basedir, name, k, os.getpid())\n        # Its a vocab\n        if isinstance(embeds_or_vocabs, collections.Mapping):\n            write_json(embeds_or_vocabs, save_md)\n        # Type is embeds\n        else:\n            write_json(embeds_or_vocabs.vocab, save_md)\n\n\n@export\ndef load_vocabs(directory: str, suffix: Optional[str] = None):\n    vocab_fnames = find_files_with_prefix(directory, \'vocabs\', suffix)\n    vocabs = {}\n    for f in vocab_fnames:\n        logger.info(f)\n        k = f.split(\'-\')[-2]\n        vocab = read_json(f)\n        vocabs[k] = vocab\n    return vocabs\n\n\n@export\ndef load_vectorizers(directory: str, data_download_cache: Optional[str] = None):\n    vectorizers_fname = find_files_with_prefix(directory, \'vectorizers\')\n    # Find the module list for the vectorizer so we can import them without\n    # needing to bother the user with providing them\n    vectorizers_modules = [x for x in vectorizers_fname if \'json\' in x][0]\n    modules = read_json(vectorizers_modules)\n    for module in modules:\n        import_user_module(module, data_download_cache)\n    vectorizers_pickle = [x for x in vectorizers_fname if \'pkl\' in x][0]\n    with open(vectorizers_pickle, ""rb"") as f:\n        vectorizers = pickle.load(f)\n    return vectorizers\n\n\n@export\ndef unzip_files(zip_path):\n    if os.path.isdir(zip_path):\n        return zip_path\n    from eight_mile.utils import mime_type\n    if mime_type(zip_path) == \'application/zip\':\n        with open(zip_path, \'rb\') as f:\n            sha1 = hashlib.sha1(f.read()).hexdigest()\n            temp_dir = os.path.join(""/tmp/"", sha1)\n            if not os.path.exists(temp_dir):\n                logger.info(""unzipping model"")\n                with zipfile.ZipFile(zip_path, ""r"") as zip_ref:\n                    zip_ref.extractall(temp_dir)\n            if len(os.listdir(temp_dir)) == 1:  # a directory was zipped v files\n                temp_dir = os.path.join(temp_dir, os.listdir(temp_dir)[0])\n        return temp_dir\n    return zip_path\n\n\n@export\ndef find_model_basename(directory, basename=None):\n    if not basename:\n        basename = [x for x in os.listdir(directory) if \'model\' in x and \'-md\' not in x and \'wgt\' not in x and \'.assets\' not in x][0]\n    else:\n        globname = os.path.join(directory, basename)\n        if not os.path.isfile(globname):\n            import glob\n            out = glob.glob(f\'{globname}*\')\n            out = [x for x in out if \'model\' in x and \'-md\' not in x and \'wgt\' not in x and \'.assets\' not in x][0]\n            basename = out\n    path = os.path.join(directory, basename)\n    logger.info(path)\n    path = path.split(\'.\')[:-1]\n    return \'.\'.join(path)\n\n\n@export\ndef find_files_with_prefix(directory, prefix, suffix=None):\n\n    files_with_prefix = [os.path.join(directory, x) for x in os.listdir(directory) if x.startswith(prefix)]\n    if suffix:\n        files_with_prefix = [f for f in files_with_prefix if f.endswith(suffix)]\n    return files_with_prefix\n\n\n@export\ndef zip_files(basedir, limit_to_pid=True):\n    pid = str(os.getpid())\n    tgt_zip_base = os.path.abspath(basedir)\n    zip_name = os.path.basename(tgt_zip_base)\n    if limit_to_pid:\n        model_files = [x for x in os.listdir(basedir) if pid in x and os.path.isfile(os.path.join(basedir, x))]\n    else:\n        model_files = [x for x in os.listdir(basedir) if os.path.isfile(os.path.join(basedir, x))]\n    with zipfile.ZipFile(""{}-{}.zip"".format(tgt_zip_base, pid), ""w"") as z:\n        for f in model_files:\n            abs_f = os.path.join(basedir, f)\n            z.write(abs_f, os.path.join(zip_name, f))\n            os.remove(abs_f)\n\n\n@export\ndef zip_model(path):\n    """"""zips the model files""""""\n    logger.info(""zipping model files"")\n    model_files = [x for x in os.listdir(""."") if path[2:] in x]\n    z = zipfile.ZipFile(""{}.zip"".format(path), ""w"")\n    for f in model_files:\n        z.write(f)\n        os.remove(f)\n    z.close()\n\n\n@export\ndef verbose_output(verbose, confusion_matrix):\n    if verbose is None:\n        return\n    do_print = bool(verbose.get(""console"", False))\n    outfile = verbose.get(""file"", None)\n    if do_print:\n        logger.info(confusion_matrix)\n    if outfile is not None:\n        confusion_matrix.save(outfile)\n\n\nLESS_THAN_METRICS = {""avg_loss"", ""loss"", ""perplexity"", ""ppl""}\n\n\n@export\ndef get_metric_cmp(metric, user_cmp=None, less_than_metrics=LESS_THAN_METRICS):\n    if user_cmp is not None:\n        return _try_user_cmp(user_cmp)\n    if metric in less_than_metrics:\n        return lt, six.MAXSIZE\n    return gt, -six.MAXSIZE - 1\n\n\ndef _try_user_cmp(user_cmp):\n    user_cmp = user_cmp.lower()\n    if user_cmp in {""lt"", ""less"", ""less than"", ""<"", ""less_than""}:\n        return lt, six.MAXSIZE\n    if user_cmp in {""le"", ""lte"", ""<=""}:\n        return le, six.MAXSIZE\n    if user_cmp in {""ge"", ""gte"", "">=""}:\n        return ge, -six.MAXSIZE - 1\n    return gt, -six.MAXSIZE - 1\n\n\n@export\ndef show_examples(model, es, rlut1, rlut2, vocab, mxlen, sample, prob_clip, max_examples, reverse):\n    """"""Expects model.predict to return [B, K, T].""""""\n    try:\n        si = np.random.randint(0, len(es))\n        batch_dict = es[si]\n    except:\n        batch_dict = next(iter(es))\n\n    lengths_key = model.src_lengths_key\n    src_field = lengths_key.split(\'_\')[0]\n    src_array = batch_dict[src_field]\n    if max_examples > 0:\n        max_examples = min(max_examples, src_array.shape[0])\n\n    for i in range(max_examples):\n        example = {}\n        # Batch first, so this gets a single example at once\n        for k, v in batch_dict.items():\n            example[k] = v[i, np.newaxis]\n\n        logger.info(\'========================================================================\')\n        sent = lookup_sentence(rlut1, example[src_field].squeeze(), reverse=reverse)\n        logger.info(\'[OP] %s\' % sent)\n        sent = lookup_sentence(rlut2, example[\'tgt\'].squeeze())\n        logger.info(\'[Actual] %s\' % sent)\n        dst_i = model.predict(example)[0][0]\n        sent = lookup_sentence(rlut2, dst_i)\n        logger.info(\'Guess: %s\' % sent)\n        logger.info(\'------------------------------------------------------------------------\')\n\n\n@export\ndef convert_seq2seq_golds(indices, lengths, rlut, subword_fix=lambda x: x):\n    """"""Convert indices to words and format like a bleu reference corpus.\n\n    :param indices: The indices of the gold sentence. Should be in the shape\n        `[B, T]`. Iterating though axis=1 should yield ints.\n    :param lengths: The length of the gold sentences.\n    :param rlut: `dict[int] -> str` A lookup table from indices to words.\n\n    :returns: List[List[List[str]]] Shape is [B, 1, T] where T is the number of\n        words in that gold sentence\n    """"""\n    golds = []\n    for idx, l in zip(indices, lengths):\n        gold = idx[:l]\n        gold_str = lookup_sentence(rlut, gold)\n        gold = subword_fix(gold_str).split()\n        golds.append([gold])\n    return golds\n\n\n@export\ndef convert_seq2seq_preds(indices, rlut, subword_fix=lambda x: x):\n    """"""Convert indices to words and format like a bleu hypothesis corpus.\n\n    :param indices: The indices of the predicted sentence. Should be in the\n        shape `[B, T]`. Iterating though axis=1 should yield ints.\n    :param rlut: `dict[int] -> str` A lookup table from indices to words.\n\n    :returns: List[List[str]] Shape is [B, T] where T is the number of\n        words in that predicted sentence\n    """"""\n    preds = []\n    for idx in indices:\n        pred_str = lookup_sentence(rlut, idx)\n        pred = subword_fix(pred_str).split()\n        preds.append(pred)\n    return preds\n\n\n@export\ndef undo_bpe(seq: str) -> str:\n    """"""Undo the BPE splits to make Bleu comparable.\n\n    :param seq: The string with encoded tokens in it.\n\n    :returns: The string with BPE splits collapsed.\n    """"""\n    # BPE token is @@ this removes it if it is at the end of a word or the end\n    # of the sequence.\n    return re.sub(r""@@( | ?$)"", """", seq)\n\n@export\ndef undo_wordpiece(seq: str) -> str:\n    """"""Undo the WordPiece splits to make Bleu comparable.  Use BERT-style detok\n    :param seq: The string with encoded tokens in it.\n\n    :returns: The string with BPE splits collapsed.\n    """"""\n    return re.sub(r""\\s+##"", """", seq)\n\n@export\ndef undo_sentence_piece(seq):\n    """"""Undo the sentence Piece splits to make Bleu comparable.\n    TODO: in what context does this actually work?  it doesnt do replacement as above\n    """"""\n\n    return seq.replace(""\\u2581"", """")\n\nDATA_CACHE_CONF = ""data-cache.json""\n\n@export\ndef delete_old_copy(file_name):\n    if os.path.exists(file_name):\n        if os.path.isfile(file_name):\n            os.remove(file_name)\n        else:\n            shutil.rmtree(file_name)\n    return file_name\n\n\n@export\ndef extract_gzip(file_loc):\n    temp_file = delete_old_copy(""{}.1"".format(file_loc))\n    with gzip.open(file_loc, \'rb\') as f_in:\n        with open(temp_file, \'wb\') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    if mime_type(temp_file) == ""application/x-tar"":\n        return extract_tar(temp_file)\n    else:\n        shutil.move(temp_file, file_loc)\n        return file_loc\n\n\n@export\ndef extract_tar(file_loc):\n    import tarfile\n    temp_file = delete_old_copy(""{}.1"".format(file_loc))\n    with tarfile.open(file_loc, ""r"") as tar_ref:\n        tar_ref.extractall(temp_file)\n    if len(os.listdir(temp_file)) != 1:\n        raise RuntimeError(""tar extraction unsuccessful"")\n    return os.path.join(temp_file, os.listdir(temp_file)[0])\n\n\n@export\ndef extract_zip(file_loc):\n    temp_file = delete_old_copy(""{}.1"".format(file_loc))\n    with zipfile.ZipFile(file_loc, ""r"") as zip_ref:\n        zip_ref.extractall(temp_file)\n    return temp_file\n\n\n@export\ndef extractor(filepath, cache_dir, extractor_func):\n    with open(filepath, \'rb\') as f:\n        sha1 = hashlib.sha1(f.read()).hexdigest()\n    logger.info(""extracting file.."")\n    path_to_save = filepath if extractor_func is None else extractor_func(filepath)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    path_to_save_sha1 = os.path.join(cache_dir, sha1)\n    delete_old_copy(path_to_save_sha1)\n    shutil.move(path_to_save, path_to_save_sha1)\n    logger.info(""downloaded data saved in {}"".format(path_to_save_sha1))\n    return path_to_save_sha1\n\n\n@export\ndef web_downloader(url, path_to_save=None):\n    # Use a class to simulate the nonlocal keyword in 2.7\n    class Context: pg = None\n    def _report_hook(count, block_size, total_size):\n        if Context.pg is None:\n            length = int((total_size + block_size - 1) / float(block_size)) if total_size != -1 else 1\n            Context.pg = create_progress_bar(length)\n        Context.pg.update()\n\n    if not path_to_save:\n        path_to_save = ""/tmp/data.dload-{}"".format(os.getpid())\n    try:\n        path_to_save, _ = urlretrieve(url, path_to_save, reporthook=_report_hook)\n        Context.pg.done()\n    except Exception as e:  # this is too broad but there are too many exceptions to handle separately\n        raise RuntimeError(""failed to download data from [url]: {} [to]: {}"".format(url, path_to_save))\n    return path_to_save\n\n\n@export\ndef update_cache(key, data_download_cache):\n    dcache = read_json(os.path.join(data_download_cache, DATA_CACHE_CONF))\n    if key not in dcache:\n        return\n    del dcache[key]\n    write_json(dcache, os.path.join(data_download_cache, DATA_CACHE_CONF))\n\n\ndef _verify_file(file_loc):\n    # dropbox doesn\'t give 404 in case the file does not exist, produces an HTML. The actual files are never HTMLs.\n    if not os.path.exists(file_loc):\n        return False\n\n    if os.path.isfile(file_loc) and mime_type(file_loc) == ""text/html"":\n        return False\n\n    return True\n\n\n@export\ndef is_file_correct(file_loc, data_dcache=None, key=None):\n    """"""check if the file location mentioned in the json file is correct, i.e.,\n    exists and not corrupted. This is needed when the direct download link/ path for a file\n    changes and the user is unaware. This is not tracked by sha1 either. If it returns False, delete the corrupted file.\n    Additionally, if the file location is a URL, i.e. exists in the cache, delete it so that it can be re-downloaded.\n\n    Keyword arguments:\n    file_loc -- location of the file\n    data_dcache -- data download cache location (default None, for local system file paths)\n    key -- URL for download (default None, for local system file paths)\n    """"""\n    if _verify_file(file_loc):\n        return True\n    # Some files are prefixes (the datasset.json has `train` and the data has `train.fr` and `train.en`)\n    dir_name = os.path.dirname(file_loc)\n    # When we are using this for checking embeddings file_loc is a url so we need this check.\n    if os.path.exists(dir_name):\n        files = [os.path.join(dir_name, f) for f in os.listdir(dir_name) if os.path.join(dir_name, f).startswith(file_loc)]\n        if files and all(_verify_file(f) for f in files):\n            return True\n    delete_old_copy(file_loc)\n    if key is not None:  # cache file validation\n        update_cache(key, data_dcache)\n    return False\n\n\n@export\ndef is_dir_correct(dir_loc, dataset_desc, data_dcache, key, ignore_file_check=False):\n    """"""check if the directory extracted from the zip location mentioned in the datasets json file is correct, i.e.,\n    all files inside exist and are not corrupted. If not, we will update the cache try to re-download them.\n\n    Keyword arguments:\n    dir_loc -- location of the directory\n    dataset_desc -- to know the individual file locations inside the directory\n    data_dcache -- data download cache location\n    key -- URL for download\n    ignore_file_check --to handle enc_dec datasets, see later.\n    """"""\n\n    if not os.path.exists(dir_loc) or not os.path.isdir(dir_loc):\n        update_cache(key, data_dcache)\n        return False\n    if ignore_file_check:  # for enc_dec tasks there\'s no direct downloads\n        return True\n    files = [os.path.join(dir_loc, dataset_desc[k]) for k in dataset_desc if k.endswith(""_file"")]\n    for f in files:\n        if not is_file_correct(f, key, data_dcache):\n            return False\n    return True\n\n\n@export\nclass Downloader:\n    ZIPD = {\'application/gzip\': extract_gzip, \'application/zip\': extract_zip}\n\n    def __init__(self, data_download_cache, cache_ignore):\n        super().__init__()\n        self.cache_ignore = cache_ignore\n        self.data_download_cache = data_download_cache\n\n    def download(self):\n        pass\n\n\n@export\nclass SingleFileDownloader(Downloader):\n    def __init__(self, dataset_file, data_download_cache, cache_ignore=False):\n        super().__init__(data_download_cache, cache_ignore)\n        self.dataset_file = dataset_file\n        self.data_download_cache = data_download_cache\n\n    def download(self):\n        file_loc = self.dataset_file\n        if is_file_correct(file_loc):\n            return file_loc\n        elif validate_url(file_loc):  # is it a web URL? check if exists in cache\n            url = file_loc\n            dcache_path = os.path.join(self.data_download_cache, DATA_CACHE_CONF)\n            dcache = read_json(dcache_path)\n            if url in dcache and is_file_correct(dcache[url], self.data_download_cache, url) and not self.cache_ignore:\n                logger.info(""file for {} found in cache, not downloading"".format(url))\n                return dcache[url]\n            else:  # download the file in the cache, update the json\n                cache_dir = self.data_download_cache\n                logger.info(""using {} as data/embeddings cache"".format(cache_dir))\n                temp_file = web_downloader(url)\n                dload_file = extractor(filepath=temp_file, cache_dir=cache_dir,\n                                       extractor_func=Downloader.ZIPD.get(mime_type(temp_file), None))\n                dcache.update({url: dload_file})\n                write_json(dcache, os.path.join(self.data_download_cache, DATA_CACHE_CONF))\n                return dload_file\n        raise RuntimeError(""the file [{}] is not in cache and can not be downloaded"".format(file_loc))\n\n\n@export\nclass AddonDownloader(Downloader):\n    ADDON_SUBPATH = \'addons\'\n    """"""Grab addons and write them to the download cache\n    """"""\n    def __init__(self, dataset_file, data_download_cache, cache_ignore=False):\n        super().__init__(data_download_cache, cache_ignore)\n        self.dataset_file = dataset_file\n        self.data_download_cache = data_download_cache\n\n    def download(self):\n        file_loc = self.dataset_file\n        if is_file_correct(file_loc):\n            return file_loc\n        elif validate_url(file_loc):  # is it a web URL? check if exists in cache\n            url = file_loc\n            dcache_path = os.path.join(self.data_download_cache, DATA_CACHE_CONF)\n            dcache = read_json(dcache_path)\n            # If the file already exists in the cache\n            if url in dcache and is_file_correct(dcache[url], self.data_download_cache, url) and not self.cache_ignore:\n                logger.info(""file for {} found in cache, not downloading"".format(url))\n                return dcache[url]\n            # Otherwise, we want it to be placed in ~/.bl-cache/addons\n            else:  # download the file in the cache, update the json\n                cache_dir = self.data_download_cache\n                addon_path = os.path.join(cache_dir, AddonDownloader.ADDON_SUBPATH)\n                if not os.path.exists(addon_path):\n                    os.makedirs(addon_path)\n                path_to_save = os.path.join(addon_path, os.path.basename(file_loc))\n                logger.info(""using {} as data/addons cache"".format(cache_dir))\n                web_downloader(url, path_to_save)\n                dcache.update({url: path_to_save})\n                write_json(dcache, os.path.join(self.data_download_cache, DATA_CACHE_CONF))\n                return path_to_save\n        raise RuntimeError(""the file [{}] is not in cache and can not be downloaded"".format(file_loc))\n\n\n@export\nclass DataDownloader(Downloader):\n    def __init__(self, dataset_desc, data_download_cache, enc_dec=False, cache_ignore=False):\n        super().__init__(data_download_cache, cache_ignore)\n        self.dataset_desc = dataset_desc\n        self.data_download_cache = data_download_cache\n        self.enc_dec = enc_dec\n\n    def download(self):\n        dload_bundle = self.dataset_desc.get(""download"", None)\n        if dload_bundle is not None:  # download a zip/tar/tar.gz directory, look for train, dev test files inside that.\n            dcache_path = os.path.join(self.data_download_cache, DATA_CACHE_CONF)\n            dcache = read_json(dcache_path)\n            if dload_bundle in dcache and \\\n                    is_dir_correct(dcache[dload_bundle], self.dataset_desc, self.data_download_cache, dload_bundle,\n                                   self.enc_dec) and not self.cache_ignore:\n                download_dir = dcache[dload_bundle]\n                logger.info(""files for {} found in cache, not downloading"".format(dload_bundle))\n                return {k: os.path.join(download_dir, self.dataset_desc[k]) for k in self.dataset_desc\n                        if k.endswith(""_file"")}\n            else:  # try to download the bundle and unzip\n                if not validate_url(dload_bundle):\n                    raise RuntimeError(""can not download from the given url"")\n                else:\n                    cache_dir = self.data_download_cache\n                    temp_file = web_downloader(dload_bundle)\n\n                    download_dir = extractor(filepath=temp_file, cache_dir=cache_dir,\n                                             extractor_func=Downloader.ZIPD.get(mime_type(temp_file), None))\n                    if ""sha1"" in self.dataset_desc:\n                        if os.path.split(download_dir)[-1] != self.dataset_desc[""sha1""]:\n                            raise RuntimeError(""The sha1 of the downloaded file does not match with the provided one"")\n                    dcache.update({dload_bundle: download_dir})\n                    write_json(dcache, os.path.join(self.data_download_cache, DATA_CACHE_CONF))\n                    return {k: os.path.join(download_dir, self.dataset_desc[k]) for k in self.dataset_desc\n                            if k.endswith(""_file"")}\n        else:  # we have download links to every file or they exist\n            if not self.enc_dec:\n                return {k: SingleFileDownloader(self.dataset_desc[k], self.data_download_cache).download()\n                        for k in self.dataset_desc if k.endswith(""_file"") and self.dataset_desc[k]}\n            else:\n                return {k: self.dataset_desc[k] for k in self.dataset_desc if k.endswith(""_file"")}\n                # these files can not be downloaded because there\'s a post processing on them.\n\n\n@export\nclass EmbeddingDownloader(Downloader):\n    def __init__(self, embedding_file, embedding_dsz, embedding_sha1, data_download_cache, cache_ignore=False, unzip_file=True):\n        super().__init__(data_download_cache, cache_ignore)\n        self.embedding_file = embedding_file\n        self.embedding_key = embedding_dsz\n        self.data_download_cache = data_download_cache\n        self.unzip_file = unzip_file\n        self.sha1 = embedding_sha1\n\n    @staticmethod\n    def _get_embedding_file(loc, key):\n        if os.path.isfile(loc):\n            logger.info(""embedding file location: {}"".format(loc))\n            return loc\n        else:  # This is a directory, return the actual file\n            files = [x for x in os.listdir(loc) if str(key) in x]\n            if len(files) == 0:\n                raise RuntimeError(""No embedding file found for the given key [{}]"".format(key))\n            elif len(files) > 1:\n                logger.info(""multiple embedding files found for the given key [{}], choosing {}"".format(key, files[0]))\n            embed_file_loc = os.path.join(loc, files[0])\n            return embed_file_loc\n\n    def download(self):\n        if is_file_correct(self.embedding_file):\n            logger.info(""embedding file location: {}"".format(self.embedding_file))\n            return self.embedding_file\n        dcache_path = os.path.join(self.data_download_cache, DATA_CACHE_CONF)\n        dcache = read_json(dcache_path)\n        if self.embedding_file in dcache and not self.cache_ignore:\n            download_loc = dcache[self.embedding_file]\n            logger.info(""files for {} found in cache"".format(self.embedding_file))\n            return self._get_embedding_file(download_loc, self.embedding_key)\n        else:  # try to download the bundle and unzip\n            url = self.embedding_file\n            if not validate_url(url):\n                raise RuntimeError(""can not download from the given url"")\n            else:\n                cache_dir = self.data_download_cache\n                temp_file = web_downloader(url)\n                unzip_fn = Downloader.ZIPD.get(mime_type(temp_file)) if self.unzip_file else None\n                download_loc = extractor(filepath=temp_file, cache_dir=cache_dir,\n                                         extractor_func=unzip_fn)\n                if self.sha1 is not None:\n                    if os.path.split(download_loc)[-1] != self.sha1:\n                        raise RuntimeError(""The sha1 of the downloaded file does not match with the provided one"")\n                dcache.update({url: download_loc})\n                write_json(dcache, os.path.join(self.data_download_cache, DATA_CACHE_CONF))\n                return self._get_embedding_file(download_loc, self.embedding_key)\n'"
baseline/vectorizers.py,0,"b'import collections\nimport tempfile\nimport unicodedata\nfrom typing import Tuple\nimport numpy as np\nfrom baseline.utils import exporter, optional_params, listify, register, Offsets, import_user_module, validate_url, urlretrieve\n\n\n__all__ = []\nexport = exporter(__all__)\n\n\n@export\nclass Vectorizer(object):\n\n    def __init__(self):\n        pass\n\n    def run(self, tokens, vocab):\n        pass\n\n    def count(self, tokens):\n        pass\n\n    def get_dims(self) -> Tuple[int]:\n        pass\n\n    def iterable(self, tokens):\n        pass\n\n    def reset(self):\n        pass\n\n\nMEAD_VECTORIZERS = {}\n\n\n@export\n@optional_params\ndef register_vectorizer(cls, name=None):\n    """"""Register a function as a plug-in""""""\n    return register(cls, MEAD_VECTORIZERS, name, \'vectorizer\')\n\n\n@export\ndef identity_trans_fn(x):\n    return x\n\n\n@export\nclass AbstractVectorizer(Vectorizer):\n\n    def __init__(self, transform_fn=None):\n        super().__init__()\n        self.transform_fn = identity_trans_fn if transform_fn is None else transform_fn\n\n    def iterable(self, tokens):\n        for tok in tokens:\n            yield self.transform_fn(tok)\n\n    def _next_element(self, tokens, vocab):\n        for atom in self.iterable(tokens):\n            value = vocab.get(atom)\n            if value is None:\n                value = vocab.get(\'<UNK>\', -1)\n                if value == -1:\n                    break\n            yield value\n\n\n@export\n@register_vectorizer(name=\'token1d\')\nclass Token1DVectorizer(AbstractVectorizer):\n\n    def __init__(self, **kwargs):\n        super().__init__(kwargs.get(\'transform_fn\'))\n        self.time_reverse = kwargs.get(\'rev\', False)\n        self.mxlen = kwargs.get(\'mxlen\', -1)\n        self.max_seen = 0\n\n    def count(self, tokens):\n        seen = 0\n        counter = collections.Counter()\n        for tok in self.iterable(tokens):\n            counter[tok] += 1\n            seen += 1\n        self.max_seen = max(self.max_seen, seen)\n        return counter\n\n    def reset(self):\n        self.mxlen = -1\n        self.max_seen = -1\n\n    def run(self, tokens, vocab):\n\n        if self.mxlen < 0:\n            self.mxlen = self.max_seen\n\n        vec1d = np.zeros(self.mxlen, dtype=int)\n        i = 0\n        for i, atom in enumerate(self._next_element(tokens, vocab)):\n            if i == self.mxlen:\n                i -= 1\n                break\n            vec1d[i] = atom\n        valid_length = i + 1\n\n        if self.time_reverse:\n            vec1d = vec1d[::-1]\n            return vec1d, None\n        return vec1d, valid_length\n\n    def get_dims(self):\n        return self.mxlen,\n\n\n@export\nclass GOVectorizer(Vectorizer):\n\n    def __init__(self, vectorizer):\n        self.vectorizer = vectorizer\n\n    def iterable(self, tokens):\n        raise Exception(""Not implemented"")\n\n    def count(self, tokens):\n        counter = self.vectorizer.count(tokens)\n        counter[\'<GO>\'] += 1\n        return counter\n\n    def run(self, tokens, vocab):\n        vec1d, valid_length = self.vectorizer.run(tokens, vocab)\n        vec1d = np.concatenate([[Offsets.GO], vec1d, [Offsets.PAD]])\n        vec1d[valid_length+1] = Offsets.EOS\n        return vec1d, valid_length + 2\n\n    def get_dims(self):\n        return self.vectorizer.get_dims()[0] + 2,\n\n\ndef _token_iterator(vectorizer, tokens):\n    for tok in tokens:\n        token = []\n        for field in vectorizer.fields:\n            if isinstance(tok, dict):\n                token += [vectorizer.transform_fn(tok[field])]\n            else:\n                token += [vectorizer.transform_fn(tok)]\n        yield vectorizer.delim.join(token)\n\n\n@export\n@register_vectorizer(name=\'dict1d\')\nclass Dict1DVectorizer(Token1DVectorizer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.fields = listify(kwargs.get(\'fields\', \'text\'))\n        self.delim = kwargs.get(\'token_delim\', \'@@\')\n\n    def iterable(self, tokens):\n        return _token_iterator(self, tokens)\n\n\n\n@export\nclass AbstractCharVectorizer(AbstractVectorizer):\n\n    def __init__(self, transform_fn=None):\n        super().__init__(transform_fn)\n\n    def _next_element(self, tokens, vocab):\n        OOV = vocab[\'<UNK>\']\n        EOW = vocab.get(\'<EOW>\', vocab.get(\' \', Offsets.PAD))\n        for token in self.iterable(tokens):\n            for ch in token:\n                yield vocab.get(ch, OOV)\n            yield EOW\n\n\n@export\n@register_vectorizer(name=\'char2d\')\nclass Char2DVectorizer(AbstractCharVectorizer):\n\n    def __init__(self, **kwargs):\n        super().__init__(kwargs.get(\'transform_fn\'))\n        self.mxlen = kwargs.get(\'mxlen\', -1)\n        self.mxwlen = kwargs.get(\'mxwlen\', -1)\n        self.max_seen_tok = 0\n        self.max_seen_char = 0\n\n    def count(self, tokens):\n        seen_tok = 0\n        counter = collections.Counter()\n        for token in self.iterable(tokens):\n            self.max_seen_char = max(self.max_seen_char, len(token))\n            seen_tok += 1\n            for ch in token:\n                counter[ch] += 1\n            counter[\'<EOW>\'] += 1\n        self.max_seen_tok = max(self.max_seen_tok, seen_tok)\n        return counter\n\n    def reset(self):\n        self.mxlen = -1\n        self.mxwlen = -1\n        self.max_seen_tok = 0\n        self.max_seen_char = 0\n\n    def run(self, tokens, vocab):\n\n        if self.mxlen < 0:\n            self.mxlen = self.max_seen_tok\n        if self.mxwlen < 0:\n            self.mxwlen = self.max_seen_char\n\n        EOW = vocab.get(\'<EOW>\', vocab.get(\' \', Offsets.PAD))\n\n        vec2d = np.zeros((self.mxlen, self.mxwlen), dtype=int)\n        i = 0\n        j = 0\n        over = False\n        for atom in self._next_element(tokens, vocab):\n            if over:\n                # If if we have gone over mxwlen burn tokens until we hit end of word\n                if atom == EOW:\n                    over = False\n                continue\n            if i == self.mxlen:\n                break\n            if atom == EOW:\n                i += 1\n                j = 0\n                continue\n            elif j == self.mxwlen:\n                over = True\n                i += 1\n                j = 0\n                continue\n            else:\n                vec2d[i, j] = atom\n                j += 1\n        valid_length = i\n        return vec2d, valid_length\n\n    def get_dims(self):\n        return self.mxlen, self.mxwlen\n\n\n@export\n@register_vectorizer(name=\'dict2d\')\nclass Dict2DVectorizer(Char2DVectorizer):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.fields = listify(kwargs.get(\'fields\', \'text\'))\n        self.delim = kwargs.get(\'token_delim\', \'@@\')\n\n    def iterable(self, tokens):\n        return _token_iterator(self, tokens)\n\n\n@export\n@register_vectorizer(name=\'char1d\')\nclass Char1DVectorizer(AbstractCharVectorizer):\n\n    def __init__(self, **kwargs):\n        super().__init__(kwargs.get(\'transform_fn\'))\n        self.mxlen = kwargs.get(\'mxlen\', -1)\n        self.time_reverse = kwargs.get(\'rev\', False)\n        self.max_seen_tok = 0\n\n    def count(self, tokens):\n        seen_tok = 0\n        counter = collections.Counter()\n        for token in self.iterable(tokens):\n            seen_tok += 1\n            for ch in token:\n                counter[ch] += 1\n                seen_tok += 1\n            counter[\'<EOW>\'] += 1\n            seen_tok += 1\n\n        self.max_seen_tok = max(self.max_seen_tok, seen_tok)\n        return counter\n\n    def reset(self):\n        self.mxlen = -1\n        self.max_seen_tok = 0\n\n    def run(self, tokens, vocab):\n\n        if self.mxlen < 0:\n            self.mxlen = self.max_seen_tok\n\n        vec1d = np.zeros(self.mxlen, dtype=int)\n        for i, atom in enumerate(self._next_element(tokens, vocab)):\n            if i == self.mxlen:\n                i -= 1\n                break\n            vec1d[i] = atom\n        if self.time_reverse:\n            vec1d = vec1d[::-1]\n            return vec1d, None\n        return vec1d, i + 1\n\n    def get_dims(self):\n        return self.mxlen,\n\n\n@register_vectorizer(name=\'ngram\')\nclass TextNGramVectorizer(Token1DVectorizer):\n    def __init__(self, filtsz=3, joiner=\'@@\', transform_fn=None, pad=\'<PAD>\', **kwargs):\n        super().__init__(**kwargs)\n        self.filtsz = filtsz\n        self.pad = pad\n        self.joiner = joiner\n        self.transform_fn = identity_trans_fn if transform_fn is None else transform_fn\n\n    def iterable(self, tokens):\n        nt = len(tokens)\n        valid_range = nt - self.filtsz + 1\n        for i in range(valid_range):\n            chunk = tokens[i:i+self.filtsz]\n            yield self.joiner.join(chunk)\n\n    def get_padding(self):\n        return [self.pad] * (self.filtsz // 2)\n\n    def run(self, tokens, vocab):\n        if self.mxlen < 0:\n            self.mxlen = self.max_seen\n        zp = self.get_padding()\n        vec2d = np.zeros(self.mxlen, dtype=int)\n        padded_tokens = zp + tokens + zp\n        for i, atom in enumerate(self._next_element(padded_tokens, vocab)):\n            if i == self.mxlen:\n                break\n            vec2d[i] = atom\n\n        lengths = min(self.mxlen, len(tokens))\n        if self.time_reverse:\n            vec2d = vec2d[::-1]\n            return vec2d, None\n        return vec2d, lengths\n\n\n@register_vectorizer(name=\'dict-ngram\')\nclass DictTextNGramVectorizer(TextNGramVectorizer):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.fields = listify(kwargs.get(\'fields\', \'text\'))\n        if len(self.fields) > 1:\n            raise Exception(""Multifield N-grams arent supported right now"")\n        self.delim = kwargs.get(\'token_delim\', \'@@\')\n\n    def get_padding(self):\n        return [{self.fields[0]: self.pad}] * (self.filtsz // 2)\n\n    def _tx(self, tok):\n        if tok == \'<PAD>\' or tok == \'<UNK>\':\n            return tok\n        return self.transform_fn(tok)\n\n    def iterable(self, tokens):\n        nt = len(tokens)\n        if isinstance(tokens[0], collections.Mapping):\n            token_list = [self._tx(tok[self.fields[0]]) for tok in tokens]\n        else:\n            token_list = [self._tx(tok) for tok in tokens]\n\n        for i in range(nt - self.filtsz + 1):\n            chunk = token_list[i:i+self.filtsz]\n            yield self.joiner.join(chunk)\n\n\n\nclass SavableFastBPE:\n    def __init__(self, codes_path, vocab_path):\n        from fastBPE import fastBPE\n        self.codes = open(codes_path, \'rb\').read()\n        self.vocab = open(vocab_path, \'rb\').read()\n        self.bpe = fastBPE(codes_path, vocab_path)\n\n    def __getstate__(self):\n        return {\'codes\': self.codes, \'vocab\': self.vocab}\n\n    def __setstate__(self, state):\n        with tempfile.NamedTemporaryFile() as codes, tempfile.NamedTemporaryFile() as vocab:\n            codes.write(state[\'codes\'])\n            vocab.write(state[\'vocab\'])\n            self.bpe = fastBPE(codes.name, vocab.name)\n\n    def apply(self, sentences):\n        return self.bpe.apply(sentences)\n\n\n@export\n@register_vectorizer(name=\'bpe1d\')\nclass BPEVectorizer1D(AbstractVectorizer):\n    """"""Define a Baseline Vectorizer for BPE using fastBPE (https://github.com/glample/fastBPE)\n\n    If you use tokens=bpe, this vectorizer is used, and so then there is a\n    dependency on fastBPE\n\n    To use BPE, we assume that a Dictionary of codes and vocab was already created\n\n    """"""\n    def __init__(self, **kwargs):\n        """"""Loads a BPE tokenizer""""""\n        super().__init__(kwargs.get(\'transform_fn\'))\n        self.max_seen = 128\n        self.model_file = kwargs.get(\'model_file\')\n        self.vocab_file = kwargs.get(\'vocab_file\')\n        self.tokenizer = SavableFastBPE(self.model_file, self.vocab_file)\n        self.mxlen = kwargs.get(\'mxlen\', -1)\n        self.vocab = {k: i for i, k in enumerate(self.read_vocab(self.vocab_file))}\n        self.emit_begin_toks = listify(kwargs.get(\'emit_begin_tok\', []))\n        self.emit_end_toks = listify(kwargs.get(\'emit_end_tok\', []))\n\n    def read_vocab(self, s):\n        vocab = [] + Offsets.VALUES + [\'[CLS]\', \'[MASK]\']\n        with open(s, ""r"") as f:\n            for line in f.readlines():\n                token = line.split()[0].strip()\n                vocab.append(token)\n        return vocab\n\n    def count(self, tokens):\n        seen = 0\n        counter = collections.Counter()\n        for tok in self.iterable(tokens):\n            counter[tok] += 1\n            seen += 1\n        self.max_seen = max(self.max_seen, seen)\n        return counter\n\n    def iterable(self, tokens):\n        for t in self.emit_begin_toks:\n            yield t\n\n        for t in tokens:\n            if t in Offsets.VALUES:\n                yield t\n            elif t == \'<unk>\':\n                yield Offsets.VALUES[Offsets.UNK]\n            elif t == \'<eos>\':\n                yield Offsets.VALUES[Offsets.EOS]\n            else:\n                subwords = self.tokenizer.apply([self.transform_fn(t)])[0].split()\n                for x in subwords:\n                    yield x\n        for t in self.emit_end_toks:\n                yield t\n\n    def _next_element(self, tokens, vocab):\n        for atom in self.iterable(tokens):\n            value = vocab.get(atom, vocab.get(Offsets.VALUES[Offsets.UNK]))  # This shouldnt actually happen\n            yield value\n\n    def run(self, tokens, vocab):\n        if self.mxlen < 0:\n            self.mxlen = self.max_seen\n        vec1d = np.zeros(self.mxlen, dtype=np.long)\n        for i, atom in enumerate(self._next_element(tokens, vocab)):\n            if i == self.mxlen:\n                i -= len(self.emit_end_toks)\n                for j, x in enumerate(self.emit_end_toks):\n                    vec1d[i + j] = vocab.get(x)\n                i = self.mxlen - 1\n                break\n            vec1d[i] = atom\n        valid_length = i + 1\n        return vec1d, valid_length\n\n    def get_dims(self):\n        return self.mxlen,\n\n\n@export\n@register_vectorizer(name=\'bpe-label-dict1d\')\nclass BPELabelDict1DVectorizer(BPEVectorizer1D):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.field = kwargs.get(\'fields\', kwargs.get(\'field\', \'text\'))\n        self.label = kwargs.get(\'label\', \'label\')\n        self.emit_begin_tok = kwargs.get(\'emit_begin_tok\')\n        self.emit_end_tok = kwargs.get(\'emit_end_tok\')\n\n    def iterable(self, tokens):\n        if self.emit_begin_tok:\n            yield self.emit_begin_tok\n        for t in tokens:\n            t_word = t[self.field]\n            t_label = t[self.label]\n            if t_word in Offsets.VALUES:\n                yield t_label\n            elif t == \'<unk>\':\n                yield t_label\n            elif t == \'<eos>\':\n                yield t_label\n            else:\n                subwords = self.tokenizer.apply([t_word])[0].split()\n                subwords = [Offsets.VALUES[Offsets.PAD]] * len(subwords)\n                subwords[0] = t_label\n                for x in subwords:\n                    yield x\n        if self.emit_end_tok:\n            yield self.emit_end_tok\n\n    def run(self, tokens, vocab):\n        return super().run(tokens, vocab)\n\n\n@export\n@register_vectorizer(name=\'bpe-dict1d\')\nclass BPEDict1DVectorizer(BPEVectorizer1D):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.field = kwargs.get(\'fields\', kwargs.get(\'field\', \'text\'))\n        self.delim = kwargs.get(\'token_delim\', \'~~\')\n\n    def iterable(self, tokens):\n        return super().iterable([t[self.field] for t in tokens])\n\n\n# This code is borrowed from the official BERT rep and slightly modified\n# https://github.com/google-research/bert/\ndef convert_by_vocab(vocab, items):\n    """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef convert_to_unicode(text):\n    """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n    if isinstance(text, str):\n        return text\n    elif isinstance(text, bytes):\n        return text.decode(""utf-8"", ""ignore"")\n    else:\n        raise ValueError(""Unsupported string type: %s"" % (type(text)))\n\n\ndef printable_text(text):\n    """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n    if isinstance(text, str):\n        return text\n    elif isinstance(text, bytes):\n        return text.decode(""utf-8"", ""ignore"")\n    else:\n        raise ValueError(""Unsupported string type: %s"" % (type(text)))\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nBERT_VOCAB = None\n\n\n@export\ndef load_bert_vocab(vocab_file):\n    global BERT_VOCAB\n    if BERT_VOCAB is not None:\n        return BERT_VOCAB\n\n    if validate_url(vocab_file):\n        print(f\'Downloading {vocab_file}\')\n        vocab_file, _ = urlretrieve(vocab_file)\n\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, ""r"") as rf:\n        for line in rf:\n            token = convert_to_unicode(line)\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    BERT_VOCAB = vocab\n    return vocab\n\n\nclass FullTokenizer(object):\n    """"""Runs end-to-end tokenization.""""""\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_bert_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True):\n        """"""Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer:\n    """"""Runs WordPiece tokenziation.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n\n\n@register_vectorizer(name=\'wordpiece1d\')\nclass WordpieceVectorizer1D(AbstractVectorizer):\n\n    def __init__(self, **kwargs):\n        super().__init__(kwargs.get(\'transform_fn\'))\n        self.max_seen = 128\n        self.tokenizer = WordpieceTokenizer(load_bert_vocab(kwargs.get(\'vocab_file\')))\n        self.mxlen = kwargs.get(\'mxlen\', -1)\n        self.dtype = kwargs.get(\'dtype\', \'int\')\n\n    def iterable(self, tokens):\n        yield \'[CLS]\'\n        for tok in tokens:\n            if tok == \'<unk>\':\n                yield \'[UNK]\'\n            elif tok == \'<EOS>\':\n                yield \'[SEP]\'\n            else:\n                for subtok in self.tokenizer.tokenize(self.transform_fn(tok)):\n                    yield subtok\n        yield \'[SEP]\'\n\n    def _next_element(self, tokens, vocab):\n        for atom in self.iterable(tokens):\n            value = vocab.get(atom)\n            if value is None:\n                value = vocab[\'[UNK]\']\n            yield value\n\n    def run(self, tokens, vocab):\n        if self.mxlen < 0:\n            self.mxlen = self.max_seen\n        vec1d = np.zeros(self.mxlen, dtype=self.dtype)\n        for i, atom in enumerate(self._next_element(tokens, vocab)):\n            if i == self.mxlen:\n                i -= 1\n                break\n            vec1d[i] = atom\n        valid_length = i + 1\n        return vec1d, valid_length\n\n    def get_dims(self):\n        return self.mxlen,\n\n\n@register_vectorizer(name=\'wordpiece-label-dict1d\')\nclass WordpieceLabelDict1DVectorizer(WordpieceVectorizer1D):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.field = kwargs.get(\'fields\', kwargs.get(\'field\', \'text\'))\n        self.label = kwargs.get(\'label\', \'label\')\n\n    def iterable(self, tokens):\n        yield Offsets.VALUES[Offsets.PAD]\n        for t in tokens:\n            t_word = t[self.field]\n            t_label = t[self.label]\n            subwords = [x for x in self.tokenizer.tokenize(t_word)]\n            subwords = [Offsets.VALUES[Offsets.PAD]] * len(subwords)\n            # TODO: The tokenizer sometimes cuts up the token and leaves nothing\n            # how to handle this since we cannot get anything for it\n            if len(subwords):\n                subwords[0] = t_label\n            for x in subwords:\n                yield x\n        yield Offsets.VALUES[Offsets.PAD]\n\n    def run(self, tokens, vocab):\n        return super().run(tokens, vocab)\n\n    def count(self, tokens):\n        seen = 0\n        counter = collections.Counter()\n        for tok in self.iterable(tokens):\n            counter[tok] += 1\n            seen += 1\n        self.max_seen = max(self.max_seen, seen)\n        return counter\n\n\n@register_vectorizer(name=\'wordpiece-dict1d\')\nclass WordpieceDict1DVectorizer(WordpieceVectorizer1D):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.field = kwargs.get(\'fields\', kwargs.get(\'field\', \'text\'))\n        self.delim = kwargs.get(\'token_delim\', \'~~\')\n\n    def iterable(self, tokens):\n        yield \'[CLS]\'\n        for t in tokens:\n            tok = t[self.field]\n            for subtok in self.tokenizer.tokenize(tok):\n                yield subtok\n        yield \'[SEP]\'\n\n\n@export\ndef create_vectorizer(**kwargs):\n    vec_type = kwargs.get(\'vectorizer_type\', kwargs.get(\'type\', \'token1d\'))\n    # Dynamically load a module if its needed\n\n    for module in listify(kwargs.get(\'module\', kwargs.get(\'modules\', []))):\n        import_user_module(module, kwargs.get(\'data_download_cache\'))\n    Constructor = MEAD_VECTORIZERS.get(vec_type)\n    return Constructor(**kwargs)\n\n'"
baseline/version.py,0,"b'__version__ = ""2.0.4""\n'"
baseline/w2v.py,0,b'import eight_mile.w2v\nfrom eight_mile.w2v import *\n\n__all__ = []\n__all__.extend(eight_mile.w2v.__all__)\n'
layers/setup.py,0,"b'import ast\nfrom setuptools import setup, find_packages\n\n\ndef get_version(file_name, version_name=""__version__""):\n    with open(file_name) as f:\n        tree = ast.parse(f.read())\n        for node in ast.walk(tree):\n            if isinstance(node, ast.Assign):\n                if node.targets[0].id == version_name:\n                    return node.value.s\n    raise ValueError(f""Unable to find an assignment to the variable {version_name} in file {file_name}"")\n\n\nclass About(object):\n    NAME = ""mead-layers""\n    AUTHOR = ""mead-ml""\n    VERSION = get_version(""eight_mile/version.py"")\n    EMAIL = ""mead.baseline@gmail.com""\n    URL = ""https://www.github.com/{}/{}"".format(AUTHOR, NAME)\n    DOWNLOAD_URL = ""{}/archive/{}.tar.gz"".format(URL, VERSION)\n    LICENSE = ""Apache 2.0""\n    DESCRIPTION = ""Reusable Deep-Learning layers for NLP""\n\n\ndef main():\n    setup(\n        name=About.NAME,\n        version=About.VERSION,\n        description=About.DESCRIPTION,\n        long_description=open(""README.md"").read(),\n        long_description_content_type=""text/markdown"",\n        author=About.AUTHOR,\n        author_email=About.EMAIL,\n        license=About.LICENSE,\n        url=About.URL,\n        download_url=About.DOWNLOAD_URL,\n        packages=find_packages(exclude=[""tests""]),\n        include_package_data=False,\n        install_requires=[""numpy""],\n        extras_require={\n            ""test"": [""pytest"", ""mock"", ""contextdecorator"", ""pytest-forked""],\n            ""yaml"": [""pyyaml""],\n            ""tf2"": [""tensorflow_addons""],\n            ""plot"": [""matplotlib""],\n        },\n        entry_points={""console_scripts"": [""bleu = eight_mile.bleu:main"" ""conlleval = eight_mile.conlleval:main""]},\n        classifiers={\n            ""Development Status :: 3 - Alpha"",\n            ""Environment :: Console"",\n            ""Intended Audience :: Developers"",\n            ""Intended Audience :: Science/Research"",\n            ""License :: OSI Approved :: Apache Software License"",\n            ""Natural Language :: English"",\n            ""Operating System :: OS Independent"",\n            ""Programming Language :: Python :: 3.5"",\n            ""Programming Language :: Python :: 3.6"",\n            ""Programming Language :: Python :: 3.7"",\n            ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        },\n        keywords=[""deep-learning"", ""nlp"", ""pytorch"", ""tensorflow""],\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
mead/__init__.py,0,"b'from mead.tasks import *\nfrom mead.utils import *\nfrom mead.exporters import *\nfrom mead.preprocessors import *\n\nfrom baseline.utils import get_console_logger\nlogger = get_console_logger(\'mead\', env_key=""MEAD_LOG_LEVEL"")\n'"
mead/clean.py,0,"b'import os\nimport re\nimport shutil\nimport glob\nfrom itertools import chain\n\nframeworks = [\'tf\', \'pytorch\']\nframeworks = \'({})\'.format(""|"".join(frameworks))\n\ntasks = [\'classify\', \'tagger\', \'lm\', \'seq2seq\']\ntasks = \'({})\'.format(""|"".join(tasks))\n\npid = r\'[0-9]{2,5}\'\n\npytorch_model = re.compile(r"".*\\.pyt$"")\nlog = re.compile(r"".*\\.log$"")\ntf_model_dir = re.compile(r""tf-{}-{}"".format(tasks, pid))\nmodel_file = re.compile(r""{}-model-{}-{}.*"".format(tasks, frameworks, pid))\ncheckpoint = re.compile(""checkpoint"")\nconll = re.compile(""^conll(-(bio|iobes)-)?results.conll$"")\ntwpos = re.compile(""^twposresults.conll$"")\n\npyc = re.compile(r"".*\\.pyc$"")\npycache = re.compile(r""^.*/__pycache__/.*$"")\ntest_file = re.compile(r""^.*/test_data/.*$"")\ncm_file = re.compile(r""^.*-cm.csv$"")\n\nres = [\n    log,\n    pytorch_model,\n    tf_model_dir,\n    model_file,\n    checkpoint,\n    conll,\n    twpos,\n    cm_file,\n]\n\ndef delete(file_):\n    try:\n        if os.path.isdir(file_):\n            shutil.rmtree(file_)\n        else:\n            os.remove(file_)\n    except OSError:\n        pass\n\n\ndef main():\n    for path, dirs, files in os.walk(\'.\'):\n        for file_name in chain(files, dirs):\n            if file_name == ""Dockerfile.pyt"":\n                continue\n            file_ = os.path.join(path, file_name)\n            # Skip test files\n            if test_file.match(file_):\n                continue\n            # Delete files that are generated by training\n            if any(r.match(file_name) for r in res):\n                delete(file_)\n            # Delete .pyc files not in __pycache__ (created by python 27)\n            if pyc.match(file_name) and not pycache.match(file_):\n                delete(file_)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
mead/eval.py,0,"b'from copy import deepcopy\nimport argparse\nimport logging\nimport baseline\nfrom baseline.reporting import create_reporting\nfrom baseline.reader import create_reader\nfrom baseline.train import create_trainer\nfrom baseline.utils import read_config_stream, str2bool, import_user_module\nfrom baseline.services import ClassifierService, TaggerService, EncoderDecoderService, LanguageModelService\nfrom mead.utils import configure_logger, convert_path, parse_extra_args\nfrom mead.tasks import merge_reporting_with_settings, Backend\n\n\nDEFAULT_SETTINGS_LOC = \'config/mead-settings.json\'\nDEFAULT_LOGGING_LOC = \'config/logging.json\'\nlogger = logging.getLogger(\'mead\')\nSERVICES = {\n    \'classify\': ClassifierService,\n    \'tagger\': TaggerService,\n    \'seq2seq\': EncoderDecoderService,\n    \'lm\': LanguageModelService\n}\n\n\ndef feature_index_mapping(features):\n    if not features:\n        return {}\n    elif \':\' in features[0]:\n        return {feature.split(\':\')[0]: int(feature.split(\':\')[1]) for feature in features}\n    else:\n        return {feature: index for index, feature in enumerate(features)}\n\n\ndef get_service(task, services=SERVICES):\n    return services[task]\n\n\ndef get_vectorizers(task, model):\n    vectorizers = model.vectorizers\n    if task == \'seq2seq\':\n        vectorizers = deepcopy(model.src_vectorizers)\n        vectorizers[\'tgt\'] = deepcopy(model.tgt_vectorizer)\n    return vectorizers\n\n\ndef patch_reader(task, model, reader):\n    if task == \'classify\':\n        reader.label2index = {l: i for i, l in enumerate(model.model.labels)}\n    if task == \'tagger\':\n        reader.label2index = model.model.labels\n        reader.label_vectorizer.mxlen = model.vectorizers[list(model.vectorizers.keys())[0]].mxlen\n    if task == \'seq2seq\':\n        # This might be skippable when PR #318 is merged\n        reader.tgt_vectorizer.vectorizer.mxlen = model.tgt_vectorizer.mxlen\n    return reader\n\n\ndef load_data(task, reader, model, dataset, batchsz):\n    if task == \'seq2seq\':\n        data = reader.load(dataset, model.src_vocabs, model.tgt_vocab, batchsz)\n    elif task == \'lm\':\n        data = reader.load(dataset, model.vocabs, batchsz, tgt_key=model.model.tgt_key)\n    elif task == \'classify\':\n        if hasattr(reader, \'load_text\'):\n            data = reader.load_text(dataset, model.vocabs, batchsz)\n        else:\n            data = reader.load(dataset, model.vocabs, batchsz)\n    else:\n        data = reader.load(dataset, model.vocabs, batchsz)\n    if not isinstance(data, tuple):\n        data = (data, )\n    return data\n\n\ndef process_reader_options(reader_options):\n    if \'clean_fn\' in reader_options:\n        reader_options[\'clean_fn\'] = eval(reader_options[\'clean_fn\'])\n    return reader_options\n\n\ndef get_trainer(model, trainer, verbose, backend, **kwargs):\n    if backend == \'tf\':\n        with model.model.sess.graph.as_default():\n            trainer = create_trainer(\n                model.model,\n                type=trainer,\n                verbose=verbose,\n                eval_mode=True,\n                basedir=\'\',\n                **kwargs\n            )\n    else:\n        trainer = create_trainer(\n            model.model, type=trainer, verbose=verbose, eval_mode=True, basedir=\'\', **kwargs\n        )\n    return trainer\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Evaluate on a dataset\')\n    parser.add_argument(\'--model\', required=True)\n    parser.add_argument(\'--dataset\', required=True)\n    parser.add_argument(\'--settings\', default=DEFAULT_SETTINGS_LOC, type=convert_path)\n    parser.add_argument(\'--modules\', nargs=""+"", default=[])\n    parser.add_argument(\'--reporting\', nargs=""+"")\n    parser.add_argument(\'--logging\', default=DEFAULT_LOGGING_LOC, type=convert_path)\n    parser.add_argument(\'--task\', default=\'classify\', choices={\'classify\', \'tagger\', \'seq2seq\', \'lm\'})\n    parser.add_argument(\'--backend\', default=\'tf\')\n    parser.add_argument(\'--reader\', default=\'default\')\n    parser.add_argument(\'--trim\', default=True, type=str2bool)\n    parser.add_argument(\'--batchsz\', default=50)\n    parser.add_argument(\'--trainer\', default=\'default\')\n    parser.add_argument(\'--output\', default=None)\n    parser.add_argument(\'--remote\')\n    parser.add_argument(\n        \'--features\',\n        help=\'(optional) features in the format feature_name:index (column # in conll) or \'\n        \'just feature names (assumed sequential)\',\n        default=[],\n        nargs=\'+\',\n    )\n    parser.add_argument(\'--device\', default=\'cpu\')\n    # our parse_extra_args doesn\'t handle lists :/\n    parser.add_argument(\'--pair_suffix\', nargs=\'+\', default=[])\n    args, extra_args = parser.parse_known_args()\n\n    args.batchsz = args.batchsz if args.task != \'lm\' else 1\n\n    named_fields = {str(v): k for k, v in feature_index_mapping(args.features).items()}\n\n    reader_options = parse_extra_args([\'reader\'], extra_args)[\'reader\']\n    reader_options = process_reader_options(reader_options)\n    verbose_options = parse_extra_args([\'verbose\'], extra_args)[\'verbose\']\n    trainer_options = parse_extra_args([\'trainer\'], extra_args)[\'trainer\']\n    if \'span_type\' not in trainer_options:\n        trainer_options[\'span_type\'] = \'iobes\'\n    model_options = parse_extra_args([\'model\'], extra_args)[\'model\']\n\n    args.logging = read_config_stream(args.logging)\n    configure_logger(args.logging)\n\n    try:\n        args.settings = read_config_stream(args.settings)\n    except:\n        logger.warning(\'Warning: no mead-settings file was found at [{}]\'.format(args.settings))\n        args.settings = {}\n\n    backend = Backend(args.backend)\n    backend.load(args.task)\n    for module in args.modules:\n        import_user_module(module)\n\n    reporting = parse_extra_args(args.reporting if args.reporting is not None else [], extra_args)\n    reporting_hooks, reporting = merge_reporting_with_settings(reporting, args.settings)\n    reporting_fns = [x.step for x in create_reporting(reporting_hooks, reporting, {\'task\': args.task})]\n\n    service = get_service(args.task)\n    model = service.load(args.model, backend=args.backend, remote=args.remote, device=args.device, **model_options)\n\n    vectorizers = get_vectorizers(args.task, model)\n\n    reader = create_reader(\n        args.task,\n        vectorizers,\n        args.trim,\n        type=args.reader,\n        named_fields=named_fields,\n        pair_suffix=args.pair_suffix,\n        **reader_options\n    )\n    reader = patch_reader(args.task, model, reader)\n\n    data, txts = load_data(args.task, reader, model, args.dataset, args.batchsz)\n\n    if args.task == \'seq2seq\':\n        trainer_options[\'tgt_rlut\'] = {v: k for k, v in model.tgt_vocab.items()}\n\n    trainer = get_trainer(\n        model,\n        args.trainer,\n        verbose_options,\n        backend.name,\n        gpu=args.device != \'cpu\',\n        nogpu=args.device == \'cpu\',\n        **trainer_options\n    )\n    if args.task == \'classify\':\n        _ = trainer.test(data, reporting_fns=reporting_fns, phase=\'Test\', verbose=verbose_options,\n                         output=args.output, txts=txts, **model_options)\n    elif args.task == \'tagger\':\n        _ = trainer.test(data, reporting_fns=reporting_fns, phase=\'Test\', verbose=verbose_options,\n                         conll_output=args.output, txts=txts, **model_options)\n    else:\n        _ = trainer.test(data, reporting_fns=reporting_fns, phase=\'Test\', verbose=verbose_options, **model_options)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
mead/export.py,0,"b'import os\nimport logging\nimport argparse\nfrom itertools import chain\nfrom baseline.utils import (\n    unzip_files,\n    str2bool,\n    read_config_file,\n    normalize_backend,\n    read_config_stream\n)\n\nimport mead\nfrom mead.exporters import create_exporter\nfrom mead.utils import (\n    convert_path,\n    configure_logger,\n    get_export_params,\n    parse_extra_args,\n    create_feature_exporter_field_map,\n    parse_and_merge_overrides,\n)\n\n\nDEFAULT_SETTINGS_LOC = \'config/mead-settings.json\'\nDEFAULT_LOGGING_LOC = \'config/logging.json\'\nlogger = logging.getLogger(\'mead\')\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Export a model\')\n    parser.add_argument(\'--config\', help=\'configuration for an experiment\', required=True, type=convert_path)\n    parser.add_argument(\'--settings\', help=\'configuration for mead\', required=False, default=DEFAULT_SETTINGS_LOC, type=convert_path)\n    parser.add_argument(\'--modules\', help=\'modules to load\', default=[], nargs=\'+\', required=False)\n    parser.add_argument(\'--datasets\', help=\'json library of dataset labels\')\n    parser.add_argument(\'--vecs\', help=\'index of vectorizers: local file, remote URL or hub mead-ml/ref\', default=\'config/vecs.json\', type=convert_path)\n    parser.add_argument(\'--logging\', help=\'json file for logging\', default=\'config/logging.json\', type=convert_path)\n    parser.add_argument(\'--task\', help=\'task to run\', choices=[\'classify\', \'tagger\', \'seq2seq\', \'lm\'])\n    parser.add_argument(\'--exporter_type\', help=""exporter type (default \'default\')"", default=None)\n    parser.add_argument(\'--return_labels\', help=\'if true, the exported model returns actual labels else \'\n                                                \'the indices for labels vocab (default False)\', default=None)\n    parser.add_argument(\'--model\', help=\'model name\', required=True, type=unzip_files)\n    parser.add_argument(\'--model_version\', help=\'model_version\', default=None)\n    parser.add_argument(\'--output_dir\', help=""output dir (default \'./models\')"", default=None)\n    parser.add_argument(\'--project\', help=\'Name of project, used in path first\', default=None)\n    parser.add_argument(\'--name\', help=\'Name of the model, used second in the path\', default=None)\n    parser.add_argument(\'--beam\', help=\'beam_width\', default=30, type=int)\n    parser.add_argument(\'--is_remote\', help=\'if True, separate items for remote server and client. If False bundle everything together (default True)\', default=None)\n    parser.add_argument(\'--backend\', help=\'The deep learning backend to use\')\n    parser.add_argument(\'--reporting\', help=\'reporting hooks\', nargs=\'+\')\n    parser.add_argument(\'--use_version\', help=\'Should we use the version?\', type=str2bool, default=True)\n    parser.add_argument(\'--zip\', help=\'Should we zip the results?\', type=str2bool, default=False)\n\n    args, overrides = parser.parse_known_args()\n    configure_logger(args.logging)\n\n    config_params = read_config_stream(args.config)\n    config_params = parse_and_merge_overrides(config_params, overrides, pre=\'x\')\n\n    try:\n        args.settings = read_config_stream(args.settings)\n    except Exception:\n        logger.warning(\'Warning: no mead-settings file was found at [{}]\'.format(args.settings))\n        args.settings = {}\n\n    task_name = config_params.get(\'task\', \'classify\') if args.task is None else args.task\n\n    # Remove multigpu references\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = """"\n    os.environ[\'NV_GPU\'] = """"\n    if \'gpus\' in config_params.get(\'train\', {}):\n        del config_params[\'train\'][\'gpus\']\n\n    if task_name == \'seq2seq\' and \'beam\' not in config_params:\n         config_params[\'beam\'] = args.beam\n\n    config_params[\'modules\'] = config_params.get(\'modules\', []) + args.modules\n    if args.backend is not None:\n        config_params[\'backend\'] = normalize_backend(args.backend)\n\n    cmd_hooks = args.reporting if args.reporting is not None else []\n    config_hooks = config_params.get(\'reporting\') if config_params.get(\'reporting\') is not None else []\n    reporting = parse_extra_args(set(chain(cmd_hooks, config_hooks)), overrides)\n    config_params[\'reporting\'] = reporting\n\n    args.vecs = read_config_stream(args.vecs)\n\n    task = mead.Task.get_task_specific(task_name, args.settings)\n\n    output_dir, project, name, model_version, exporter_type, return_labels, is_remote = get_export_params(\n        config_params.get(\'export\', {}),\n        args.output_dir,\n        args.project, args.name,\n        args.model_version,\n        args.exporter_type,\n        args.return_labels,\n        args.is_remote,\n    )\n    # Here we reuse code in `.read_config` which needs a dataset index (when used with mead-train)\n    # but when used with mead-export it is not needed. This is a dummy dataset index that will work\n    # It means we don\'t need to pass it in\n    datasets = [{\'label\': config_params[\'dataset\']}]\n    task.read_config(config_params, datasets, args.vecs, exporter_type=exporter_type)\n    feature_exporter_field_map = create_feature_exporter_field_map(config_params[\'features\'])\n    exporter = create_exporter(task, exporter_type, return_labels=return_labels,\n                               feature_exporter_field_map=feature_exporter_field_map)\n    exporter.run(args.model, output_dir, project, name, model_version, remote=is_remote, use_version=args.use_version, zip_results=args.zip)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
mead/exporters.py,0,"b'import os\nfrom baseline.utils import exporter, optional_params\n\n__all__ = []\nexport = exporter(__all__)\n\n\n@export\nclass Exporter(object):\n\n    def __init__(self, task, **kwargs):\n        super(Exporter, self).__init__()\n        self.task = task\n\n    @classmethod\n    def preproc_type(cls):\n        return \'client\'\n\n    def _run(self, model_file, output_dir, project=None, name=None, model_version=None, **kwargs):\n        raise NotImplementedError\n\n    def run(self, model_file, output_dir, project=None, name=None, model_version=None, **kwargs):\n        client_loc, server_loc = self._run(\n            model_file,\n            output_dir,\n            project=project,\n            name=name,\n            model_version=model_version,\n            **kwargs\n        )\n        if model_version is None:\n            try:\n                model_version = int(os.path.basename(client_loc))\n            except ValueError:\n                pass\n        msg = {\n            ""client_bundle"": client_loc,\n            ""server_bundle"": server_loc,\n            ""project"": project,\n            ""name"": name,\n            ""version"": model_version\n        }\n        for rep in self.task.reporting:\n            rep.step(msg, 0, \'Export\', \'EXPORT\')\n        self.task._close_reporting_hooks()\n        return client_loc, server_loc\n\n\nBASELINE_EXPORTERS = {}\n\n\n@export\n@optional_params\ndef register_exporter(cls, task, name=None):\n    """"""Register an exporter\n\n    Use this pattern if you want to provide an override to a `Exporter` class.\n\n    """"""\n    if name is None:\n        name = cls.__name__\n\n    if task not in BASELINE_EXPORTERS:\n        BASELINE_EXPORTERS[task] = {}\n\n    if name in BASELINE_EXPORTERS[task]:\n        raise Exception(\'Error: attempt to re-defined previously registered handler {} in exporter registry\'.format(name))\n\n    BASELINE_EXPORTERS[task][name] = cls\n    return cls\n\n\ndef create_exporter(task, name=None, **kwargs):\n    return BASELINE_EXPORTERS[task.task_name()][name](task, **kwargs)\n'"
mead/preprocessors.py,0,"b'from baseline.utils import register, exporter, optional_params\n\n\n__all__ = []\nexport = exporter(__all__)\n\nBASELINE_PREPROCESSORS = {}\n\n\n@export\nclass Preprocessor(object):\n    """"""\n    Generic class for creating vectorizers using tensorflow/pyt ops, to be used for exporting models when the service gets\n    a string instead of a vectorized input.\n    """"""\n\n    def __init__(self, feature, vectorizer, index, vocab, **kwargs):\n        self.feature = feature\n        self.vectorizer = vectorizer\n        self.index = index\n        self.vocab = vocab\n\n    def preproc(self, tf_example):\n        """"""\n        Create a preprocessor chain inside of the tensorflow graph.\n        """"""\n        pass\n\n\n@export\n@optional_params\ndef register_preprocessor(cls, name=None):\n    """"""Register a function as a plug-in""""""\n    return register(cls, BASELINE_PREPROCESSORS, name, \'preprocessor\')\n\n\n@export\ndef create_preprocessors(**kwargs):\n    preprocessor_type = kwargs[\'preprocessor_type\']  # fail early\n    Constructor = BASELINE_PREPROCESSORS.get(preprocessor_type)\n    if Constructor is None:\n        raise NotImplementedError(\'no preproc exporter found for type {}\'.format(preprocessor_type))\n    return Constructor(**kwargs)\n'"
mead/tasks.py,0,"b'import os\nimport json\nimport logging\nfrom typing import List\nfrom collections import Counter\nimport numpy as np\nimport baseline\nfrom copy import deepcopy\nfrom baseline.reporting import create_reporting\nfrom baseline.utils import (\n    exporter,\n    get_version,\n    revlut,\n    is_sequence,\n    Offsets,\n    get_env_gpus,\n    import_user_module,\n    listify,\n    SingleFileDownloader,\n    EmbeddingDownloader,\n    DataDownloader,\n    show_examples,\n    normalize_backend,\n    str2bool,\n)\n\nfrom mead.utils import (\n    index_by_label,\n    get_mead_settings,\n    print_dataset_info,\n    read_config_file_or_json,\n    get_dataset_from_key\n)\n\nfrom baseline.train import calc_lr_params\n\n__all__ = []\nexport = exporter(__all__)\nlogger = logging.getLogger(\'mead\')\n\n\ndef merge_reporting_with_settings(reporting, settings):\n    default_reporting = settings.get(\'reporting_hooks\', {})\n    # Add default reporting information to the reporting settings.\n    for report_type in default_reporting:\n        if report_type in reporting:\n            for report_arg, report_val in default_reporting[report_type].items():\n                if report_arg not in reporting[report_type]:\n                    reporting[report_type][report_arg] = report_val\n    reporting_hooks = list(reporting.keys())\n    for settings in reporting.values():\n        for module in listify(settings.get(\'module\', settings.get(\'modules\', []))):\n            import_user_module(module)\n    return reporting_hooks, reporting\n\n\nclass Backend(object):\n    """"""Simple object to represent a deep-learning framework backend""""""\n    def __init__(self, name=None, params=None, exporter=None):\n        """"""Initialize the backend, optional with constructor args\n\n        :param name: (``str``) Name of the framework: currently one of (`tensorflow`, `pytorch`)\n        :param params: (``dict``) A dictionary of framework-specific user-data to pass through keyword args to each sub-module\n        :param exporter: A framework-specific exporter to facilitate exporting to runtime deployment\n        """"""\n        self.name = normalize_backend(name)\n        self.params = params\n\n    def load(self, task_name=None):\n        if self.name == \'tf\':\n            prefer_eager = self.params.get(\'prefer_eager\', False)\n            from eight_mile.tf.layers import set_tf_eager_mode, set_tf_log_level, set_tf_eager_debug\n            set_tf_eager_mode(prefer_eager)\n            set_tf_log_level(os.getenv(""MEAD_TF_LOG_LEVEL"", ""ERROR""))\n            set_tf_eager_debug(str2bool(os.getenv(""MEAD_TF_EAGER_DEBUG"", ""FALSE"")))\n\n        base_pkg_name = \'baseline.{}\'.format(self.name)\n        # Backends may not be downloaded to the cache, they must exist locally\n        mod = import_user_module(base_pkg_name)\n        import_user_module(\'baseline.{}.optz\'.format(self.name))\n        import_user_module(\'baseline.{}.embeddings\'.format(self.name))\n        import_user_module(\'mead.{}.exporters\'.format(self.name))\n        if task_name is not None:\n            import_user_module(\'{}.{}\'.format(base_pkg_name, task_name))\n        self.transition_mask = mod.transition_mask\n\n\nTASK_REGISTRY = {}\n@export\ndef register_task(cls):\n    TASK_REGISTRY[cls.task_name()] = cls\n    return cls\n\n@export\ndef get_task_registry():\n    return TASK_REGISTRY\n\n\ndef assert_unique_feature_names(names: List[str]) -> None:\n    """"""Check if all the feature names are unique.\n\n    :param names: The feature names\n    :raises ValueError: If there are duplicated names\n    """"""\n    counts = Counter(names)\n    dups = [n for n, c in counts.items() if c > 1]\n    if dups:\n        raise ValueError(f""Features names must be unique, found duplicates {dups}"")\n\n\n@export\nclass Task(object):\n    """"""Basic building block for a task of NLP problems, e.g. `tagger`, `classify`, etc.\n    """"""\n\n    def _create_backend(self, **kwargs):\n        """"""This method creates and returns a `Backend` object\n\n        :return:\n        """"""\n        pass\n\n    def __init__(self, mead_settings_config=None):\n        super().__init__()\n        self.config_params = None\n        self.mead_settings_config = get_mead_settings(mead_settings_config)\n        if \'datacache\' not in self.mead_settings_config:\n            self.data_download_cache = os.path.expanduser(""~/.bl-data"")\n            self.mead_settings_config[\'datacache\'] = self.data_download_cache\n        else:\n            self.data_download_cache = os.path.expanduser(self.mead_settings_config[\'datacache\'])\n        logger.info(""using %s as data/embeddings cache"", self.data_download_cache)\n\n    @classmethod\n    def task_name(cls):\n        """"""This classmethod returns the official name of this task, e.g., `classify` for classification\n\n        :return: (``str``) - String name of this task\n        """"""\n        pass\n\n    def _create_vectorizers(self, vecs_set=None):\n        """"""Read the `features` section of the mead config.  This sections contains both embedding info and vectorizers\n        Then use the vectorizer sub-section to instantiate the vectorizers and return them in a ``dict`` with name\n        keyed off of the `features->name` and value of `vectorizer`\n\n        :return: (``dict``) - A dictionary of the vectorizers keyed by feature name\n        """"""\n        self.vectorizers = {}\n\n        features = self.config_params[\'features\']\n        assert_unique_feature_names([f[\'name\'] for f in features])\n        self.primary_key = features[0][\'name\']\n        for feature in self.config_params[\'features\']:\n            key = feature[\'name\']\n            if \'-\' in key:\n                raise ValueError(\'Feature names cannot contain ""-"". Found feature named ""{}""\'.format(key))\n            if feature.get(\'primary\', False) is True:\n                self.primary_key = key\n\n            vectorizer_section = feature.get(\'vectorizer\', {})\n            vecs_global_config = {\'type\': \'token1d\'}\n            if \'label\' in vectorizer_section:\n                vecs_global_config = vecs_set.get(vectorizer_section[\'label\'])\n\n            vectorizer_section = {**vecs_global_config, **vectorizer_section}\n            vectorizer_section[\'data_download_cache\'] = self.data_download_cache\n            vec_file = vectorizer_section.get(\'file\')\n            if vec_file:\n                vec_file = SingleFileDownloader(vec_file, self.data_download_cache).download()\n                vectorizer_section[\'file\'] = vec_file\n            vectorizer_section[\'mxlen\'] = vectorizer_section.get(\'mxlen\', self.config_params.get(\'preproc\', {}).get(\'mxlen\', -1))\n            vectorizer_section[\'mxwlen\'] = vectorizer_section.get(\'mxwlen\', self.config_params.get(\'preproc\', {}).get(\'mxwlen\', -1))\n            if \'transform\' in vectorizer_section:\n                vectorizer_section[\'transform_fn\'] = eval(vectorizer_section[\'transform\'])\n            vectorizer = baseline.create_vectorizer(**vectorizer_section)\n            self.vectorizers[key] = vectorizer\n\n\n    @staticmethod\n    def get_task_specific(task, mead_config):\n        """"""Get the task from the task registry associated with the name\n\n        :param task: The task name\n        :return:\n        """"""\n        config = TASK_REGISTRY[task](mead_config)\n        return config\n\n    def read_config(self, config_params, datasets_index, vecs_index, **kwargs):\n        """"""\n        Read the config file and the datasets index\n\n        Between the config file and the dataset index, we have enough information\n        to configure the backend and the models.  We can also initialize the data readers\n\n        :param config_file: The config file\n        :param datasets_index: The index of datasets\n        :return:\n        """"""\n        datasets_index = read_config_file_or_json(datasets_index, \'datasets\')\n        datasets_set = index_by_label(datasets_index)\n        vecs_index = read_config_file_or_json(vecs_index, \'vecs\')\n        vecs_set = index_by_label(vecs_index)\n        self.config_params = config_params\n        config_file = deepcopy(config_params)\n        basedir = self.get_basedir()\n        if basedir is not None and not os.path.exists(basedir):\n            logger.info(\'Creating: %s\', basedir)\n            os.makedirs(basedir)\n        self.config_params[\'train\'][\'basedir\'] = basedir\n        # Read GPUS from env variables now so that the reader has access\n        if self.config_params[\'train\'].get(\'gpus\', -1) == -1:\n            self.config_params[\'train\'][\'gpus\'] = len(get_env_gpus())\n        self._setup_task(**kwargs)\n        self._load_user_modules()\n        self.dataset = get_dataset_from_key(self.config_params[\'dataset\'], datasets_set)\n        # replace dataset in config file by the latest dataset label, this will be used by some reporting hooks\n        config_file[\'dataset\'] = self.dataset[\'label\']\n        self._configure_reporting(config_params.get(\'reporting\', {}), config_file=config_file, **kwargs)\n        self.reader = self._create_task_specific_reader(vecs_set)\n\n    def _load_user_modules(self):\n        # User modules can be downloaded from hub or HTTP automatically if they are defined in form\n        # http://path/to/module_name.py\n        # hub:v1:addons:module_name\n        if \'modules\' in self.config_params:\n            for addon in self.config_params[\'modules\']:\n                import_user_module(addon, self.data_download_cache)\n\n    def initialize(self, embeddings_index):\n        """"""\n        Load the vocabulary using the readers and then load any embeddings required\n\n        :param embeddings_index: The index of embeddings\n        :return:\n        """"""\n        pass\n\n    def _create_task_specific_reader(self, vecs_set=None):\n        self._create_vectorizers(vecs_set)\n        reader_params = self.config_params[\'reader\'] if \'reader\' in self.config_params else self.config_params[\'loader\']\n        reader_params[\'clean_fn\'] = reader_params.get(\'clean_fn\', self.config_params.get(\'preproc\', {}).get(\'clean_fn\'))\n        if reader_params[\'clean_fn\'] is not None and self.config_params[\'dataset\'] != \'SST2\':\n            logger.warning(\'Warning: A reader preprocessing function (%s) is active, it is recommended that all data preprocessing is done outside of baseline to insure data at inference time matches data at training time.\', reader_params[\'clean_fn\'])\n        reader_params[\'mxlen\'] = self.vectorizers[self.primary_key].mxlen\n        if self.config_params[\'train\'].get(\'gpus\', 1) > 1:\n            reader_params[\'truncate\'] = True\n        return baseline.reader.create_reader(self.task_name(), self.vectorizers, self.config_params[\'preproc\'].get(\'trim\', False), **reader_params)\n\n    @staticmethod\n    def _get_min_f(config):\n        read = config[\'reader\'] if \'reader\' in config else config[\'loader\']\n        backoff = read.get(\'min_f\', config.get(\'preproc\', {}).get(\'min_f\', -1))\n        return {f[\'name\']: f.get(\'min_f\', backoff) for f in config[\'features\']}\n\n    def _setup_task(self, **kwargs):\n        """"""\n        This method provides the task-specific setup\n        :return:\n        """"""\n        self.backend = self._create_backend(**kwargs)\n\n    def _load_dataset(self):\n        """"""This hook is responsible for creating and initializing the ``DataFeed`` objects to be used for train, dev\n        and test phases.  This method should yield a `self.train_data`, `self.valid_data` and `self.test_data` on this\n        class\n\n        :return: Nothing\n        """"""\n        pass\n\n    def _reorganize_params(self):\n        """"""This hook create the model used for training, using the `model` section of the mead config.  The model is\n        returned, not stored as a field of the class\n\n        :return: A representation\n        """"""\n        pass\n\n    def train(self, checkpoint=None):\n        """"""This method delegates to several sub-hooks in order to complete training.\n        1. call `_load_dataset()` which initializes the `DataFeed` fields of this class\n        2. call `baseline.save_vectorizers()` which write out the bound `vectorizers` fields to a file in the `basedir`\n        3. call `baseline.train.fit()` which executes the training procedure and  yields a saved model\n        4. call `baseline.zip_files()` which zips all files in the `basedir` with the same `PID` as this process\n        5. call `_close_reporting_hooks()` which lets the reporting hooks know that the job is finished\n        :return: Nothing\n        """"""\n        self._reorganize_params()\n        baseline.save_vectorizers(self.get_basedir(), self.vectorizers)\n        self._load_dataset()\n\n        model_params = self.config_params[\'model\']\n        model_params[\'features\'] = self._get_features()\n        model_params[\'labels\'] = self._get_labels()\n        model_params[\'task\'] = self.task_name()\n        train_params = self.config_params[\'train\']\n        train_params[\'checkpoint\'] = checkpoint\n        baseline.train.fit(model_params, self.train_data, self.valid_data, self.test_data, **train_params)\n        baseline.zip_files(self.get_basedir())\n        self._close_reporting_hooks()\n\n    def _configure_reporting(self, reporting, config_file, **kwargs):\n        """"""Configure all `reporting_hooks` specified in the mead settings or overridden at the command line\n\n        :param reporting:\n        :param kwargs:\n        :return:\n        """"""\n        # If there is an nstep request in config or we are doing seq2seq/lm log steps to console\n        if \'nsteps\' in self.config_params[\'train\'] or self.__class__.task_name() in {\'seq2seq\', \'lm\'}:\n            reporting[\'step_logging\'] = reporting.get(\'step_logging\', {})\n\n        reporting_hooks, reporting = merge_reporting_with_settings(reporting, self.mead_settings_config)\n\n        self.reporting = create_reporting(reporting_hooks,\n                                          reporting,\n                                          {\'config_file\': config_file, \'task\': self.__class__.task_name(), \'base_dir\': self.get_basedir()})\n\n        self.config_params[\'train\'][\'reporting\'] = [x.step for x in self.reporting]\n        logging.basicConfig(level=logging.DEBUG)\n\n    def _close_reporting_hooks(self):\n        """"""Tell all reporting objects they are done\n\n        :return: Nothing\n        """"""\n        for x in self.reporting:\n            x.done()\n\n    def _create_embeddings(self, embeddings_set, vocabs, features):\n        """"""Creates a set of arbitrary sub-graph, DL-framework-specific embeddings by delegating to wired sub-module.\n\n        As part of this process, we take in an index of embeddings by name, a ``dict`` of ``Counter`` objects (keyed by\n        feature name), containing the number of times each token has been seen, and a `features` list which is a\n        sub-section of the mead config containing the `embeddings` section for each feature.\n        This method\'s job is to either create a sub-graph from a pretrained model, or to create a new random\n        initialized sub-graph, taking into account the input vocabulary counters.  The embeddings model has control\n        to determine the actual word indices and sub-graph for the embeddings, both of which are returned from this\n        method.  If some sort of feature selection is\n        performed, such as low count removal that would be required via the delegated methods\n\n        :param embeddings_set: The embeddings index passed to mead driver\n        :param vocabs: A set of known ``Counter``s for each vocabulary consisting of a token key and count for each\n        :param features: The `features` sub-section of the mead config\n        :return: Returns a ``tuple`` comprised of a ``dict`` of (`feature name`, `Embedding`) and an updated vocab\n        """"""\n\n\n        embeddings_map = {}\n        out_vocabs = {}\n\n\n        for feature in features:\n            # Get the block from the features section with key `embeddings`\n            embeddings_section = feature[\'embeddings\']\n\n            # The name is at the top level for the feature block of mead config\n            name = feature[\'name\']\n\n            # Get the label out of the embeddings section in the features block of mead config\n            embed_label = embeddings_section.get(\'label\', embeddings_section.get(\'labels\'))\n\n            # Get the type of embedding out of the embeddings section in the features block of mead config\n            embed_type = embeddings_section.get(\'type\', \'default\')\n            is_stacked = is_sequence(embed_label)\n            if is_stacked:\n                if embed_type != \'default\':\n                    logger.warning(""You have requested a stack of pretrained embeddings but didnt request \'default\' or representation"")\n            # Backwards compat, copy from main block if not present locally\n            embeddings_section[\'unif\'] = embeddings_section.get(\'unif\', self.config_params.get(\'unif\', 0.1))\n\n            # Backwards compat, copy from main block if not present locally\n            embeddings_section[\'keep_unused\'] = embeddings_section.get(\'keep_unused\',\n                                                                       self.config_params.get(\'keep_unused\', False))\n\n            # Overlay any backend parameters\n\n            # Also, if we are in eager mode, we might have to place the embeddings explicitly on the CPU\n            embeddings_section[\'cpu_placement\'] = bool(embeddings_section.get(\'cpu_placement\', False))\n            if self.backend.params is not None:\n                # If we are in eager mode\n                if bool(self.backend.params.get(\'prefer_eager\', False)):\n                    train_block = self.config_params[\'train\']\n                    optimizer_type = train_block.get(\'optim\', \'sgd\')\n                    # If the optimizer cannot handle embeddings on GPU\n                    if optimizer_type not in [\'sgd\', \'adam\', \'adamw\']:\n                        logger.warning(""Running in eager mode with [%s] optimizer, forcing CPU placement"", optimizer_type)\n                        embeddings_section[\'cpu_placement\'] = True\n                    elif optimizer_type == \'sgd\' and float(train_block.get(\'mom\', 0.0)) > 0:\n                        logger.warning(""Running in eager mode with momentum on, forcing CPU placement"")\n                        embeddings_section[\'cpu_placement\'] = True\n                for k, v in self.backend.params.items():\n                    embeddings_section[k] = v\n            if embed_label is not None:\n                # Allow local overrides to uniform initializer\n\n                embed_labels = listify(embed_label)\n\n                embed_files = []\n                for embed_label in embed_labels:\n\n                    embeddings_global_config_i = embeddings_set[embed_label]\n                    if \'type\' in embeddings_global_config_i:\n                        embed_type_i = embeddings_global_config_i[\'type\']\n                        embed_type = embed_type_i\n                        if embed_type_i != \'default\' and is_stacked:\n                            raise Exception(""Stacking embeddings only works for \'default\' pretrained word embeddings"")\n\n                    embed_file = embeddings_global_config_i.get(\'file\')\n                    unzip_file = embeddings_global_config_i.get(\'unzip\', True)\n                    embed_dsz = embeddings_global_config_i[\'dsz\']\n                    embed_sha1 = embeddings_global_config_i.get(\'sha1\')\n                    # Should we grab vocab here too?\n\n                    embed_model = embeddings_global_config_i.get(\'model\', {})\n                    if \'dsz\' not in embed_model and not is_stacked:\n                        embed_model[\'dsz\'] = embed_dsz\n\n                    embeddings_section = {**embed_model, **embeddings_section}\n                    try:\n                        # We arent necessarily going to get an `embed_file`. For instance, using the HuggingFace\n                        # models in the Hub addon, the `embed_file` should be downloaded using HuggingFace\'s library,\n                        # not by us.  In this case we want it to be None and we dont want to download it\n                        if embed_file:\n                            embed_file = EmbeddingDownloader(embed_file, embed_dsz, embed_sha1, self.data_download_cache, unzip_file=unzip_file).download()\n                            embed_files.append(embed_file)\n                        else:\n                            embed_files.append(None)\n                    except Exception as e:\n                        if is_stacked:\n                            raise e\n                        logger.warning(f""We were not able to download {embed_file}, passing to the addon"")\n                        embed_files.append(embed_file)\n                # If we have stacked embeddings (which only works with `default` model, we need to pass the list\n                # If not, grab the first item\n                embed_file = embed_files if is_stacked else embed_files[0]\n                embedding_bundle = baseline.embeddings.load_embeddings(name,\n                                                                       embed_file=embed_file,\n                                                                       known_vocab=vocabs[name],\n                                                                       embed_type=embed_type,\n                                                                       data_download_cache=self.data_download_cache,\n                                                                       **embeddings_section)\n\n                embeddings_map[name] = embedding_bundle[\'embeddings\']\n                out_vocabs[name] = embedding_bundle[\'vocab\']\n            else:  # if there is no label given, assume we need random initialization vectors\n                dsz = embeddings_section.pop(\'dsz\')\n                embedding_bundle = baseline.embeddings.load_embeddings(name,\n                                                                       dsz=dsz,\n                                                                       known_vocab=vocabs[name],\n                                                                       embed_type=embed_type,\n                                                                       data_download_cache=self.data_download_cache,\n                                                                       **embeddings_section)\n                embeddings_map[name] = embedding_bundle[\'embeddings\']\n                out_vocabs[name] = embedding_bundle[\'vocab\']\n\n        return embeddings_map, out_vocabs\n\n    def _get_features(self):\n        pass\n\n    def _get_labels(self):\n        pass\n\n    def get_basedir(self):\n        """"""Return the base directory if provided, or CWD\n        """"""\n        return self.config_params.get(\'basedir\', \'./{}\'.format(self.task_name()))\n\n    @staticmethod\n    def _get_batchsz(config):\n        train = config[\'train\']\n        # Use this if statement to short circuit the last lookup so \'batchsz\' isn\'t required in the config\n        bsz = train[\'batchsz\'] if \'batchsz\' in train else config[\'batchsz\']\n        vbsz = train.get(\'valid_batchsz\', config.get(\'valid_batchsz\', bsz))\n        tbsz = train.get(\'test_batchsz\', config.get(\'test_batchsz\', 1))\n        return bsz, vbsz, tbsz\n\n\n@export\n@register_task\nclass ClassifierTask(Task):\n\n    def __init__(self, mead_settings_config, **kwargs):\n        super(ClassifierTask, self).__init__(mead_settings_config, **kwargs)\n\n    @classmethod\n    def task_name(cls):\n        return \'classify\'\n\n    def _create_backend(self, **kwargs):\n        backend = Backend(self.config_params.get(\'backend\', \'tf\'), kwargs)\n        backend.load(self.task_name())\n\n        return backend\n\n    def _setup_task(self, **kwargs):\n        super(ClassifierTask, self)._setup_task(**kwargs)\n        if self.config_params.get(\'preproc\', {}).get(\'clean\', False) is True:\n            self.config_params.get(\'preproc\', {})[\'clean_fn\'] = baseline.TSVSeqLabelReader.do_clean\n            logger.info(\'Clean\')\n        else:\n            self.config_params.setdefault(\'preproc\', {})\n            self.config_params[\'preproc\'][\'clean_fn\'] = None\n\n    def initialize(self, embeddings):\n        embeddings = read_config_file_or_json(embeddings, \'embeddings\')\n        embeddings_set = index_by_label(embeddings)\n        self.dataset = DataDownloader(self.dataset, self.data_download_cache).download()\n        print_dataset_info(self.dataset)\n\n        vocab_sources = [self.dataset[\'train_file\'], self.dataset[\'valid_file\']]\n        # TODO: make this optional\n        if \'test_file\' in self.dataset:\n            vocab_sources.append(self.dataset[\'test_file\'])\n\n        vocab, self.labels = self.reader.build_vocab(vocab_sources,\n                                                     min_f=Task._get_min_f(self.config_params),\n                                                     vocab_file=self.dataset.get(\'vocab_file\'),\n                                                     label_file=self.dataset.get(\'label_file\'))\n        self.embeddings, self.feat2index = self._create_embeddings(embeddings_set, vocab, self.config_params[\'features\'])\n        baseline.save_vocabs(self.get_basedir(), self.feat2index)\n\n    def _get_features(self):\n        return self.embeddings\n\n    def _get_labels(self):\n        return self.labels\n\n    def _reorganize_params(self):\n        train_params = self.config_params[\'train\']\n        train_params[\'batchsz\'] = train_params[\'batchsz\'] if \'batchsz\' in train_params else self.config_params[\'batchsz\']\n        train_params[\'test_batchsz\'] = train_params.get(\'test_batchsz\', self.config_params.get(\'test_batchsz\', 1))\n        unif = self.config_params.get(\'unif\', 0.1)\n        model = self.config_params[\'model\']\n        model[\'unif\'] = model.get(\'unif\', unif)\n        lengths_key = model.get(\'lengths_key\', self.primary_key)\n        if lengths_key is not None:\n            if not lengths_key.endswith(\'_lengths\'):\n                lengths_key = \'{}_lengths\'.format(lengths_key)\n            model[\'lengths_key\'] = lengths_key\n        if self.backend.params is not None:\n            for k, v in self.backend.params.items():\n                model[k] = v\n        ##return baseline.model.create_model(self.embeddings, self.labels, **model)\n\n    def _load_dataset(self):\n        read = self.config_params[\'reader\'] if \'reader\' in self.config_params else self.config_params[\'loader\']\n        sort_key = read.get(\'sort_key\')\n        bsz, vbsz, tbsz = Task._get_batchsz(self.config_params)\n        self.train_data = self.reader.load(\n            self.dataset[\'train_file\'],\n            self.feat2index,\n            bsz,\n            shuffle=True,\n            sort_key=sort_key,\n        )\n        self.valid_data = self.reader.load(\n            self.dataset[\'valid_file\'],\n            self.feat2index,\n            vbsz,\n        )\n        self.test_data = None\n        if \'test_file\' in self.dataset:\n            self.test_data = self.reader.load(\n                self.dataset[\'test_file\'],\n                self.feat2index,\n                tbsz,\n            )\n\n\n@export\n@register_task\nclass TaggerTask(Task):\n\n    def __init__(self, mead_settings_config, **kwargs):\n        super(TaggerTask, self).__init__(mead_settings_config, **kwargs)\n\n    @classmethod\n    def task_name(cls):\n        return \'tagger\'\n\n    def _create_backend(self, **kwargs):\n        backend = Backend(self.config_params.get(\'backend\', \'tf\'), kwargs)\n        if \'preproc\' not in self.config_params:\n            self.config_params[\'preproc\'] = {}\n        if backend.name == \'pytorch\':\n            self.config_params[\'preproc\'][\'trim\'] = True\n        else:\n            self.config_params[\'preproc\'][\'trim\'] = False\n\n        backend.load(self.task_name())\n\n        return backend\n\n    def initialize(self, embeddings):\n        self.dataset = DataDownloader(self.dataset, self.data_download_cache).download()\n        print_dataset_info(self.dataset)\n        embeddings = read_config_file_or_json(embeddings, \'embeddings\')\n        embeddings_set = index_by_label(embeddings)\n        vocab_sources = [self.dataset[\'train_file\'], self.dataset[\'valid_file\']]\n        # TODO: make this optional\n        if \'test_file\' in self.dataset:\n            vocab_sources.append(self.dataset[\'test_file\'])\n\n        vocabs = self.reader.build_vocab(vocab_sources, min_f=Task._get_min_f(self.config_params),\n                                         vocab_file\n                                         =self.dataset.get(\'vocab_file\'))\n        self.embeddings, self.feat2index = self._create_embeddings(embeddings_set, vocabs, self.config_params[\'features\'])\n        baseline.save_vocabs(self.get_basedir(), self.feat2index)\n\n    def _reorganize_params(self):\n        train_params = self.config_params[\'train\']\n        train_params[\'batchsz\'] = train_params[\'batchsz\'] if \'batchsz\' in train_params else self.config_params[\'batchsz\']\n        train_params[\'test_batchsz\'] = train_params.get(\'test_batchsz\', self.config_params.get(\'test_batchsz\', 1))\n        labels = self.reader.label2index\n        span_type = self.config_params[\'train\'].get(\'span_type\')\n        constrain = bool(self.config_params[\'model\'].get(\'constrain_decode\', False))\n        if span_type is None and constrain:\n            logger.warning(""Constrained Decoding was set but no span type could be found so no Constraints will be applied."")\n        self.config_params[\'model\'][\'span_type\'] = span_type\n        if span_type is not None and constrain:\n            self.config_params[\'model\'][\'constraint_mask\'] = self.backend.transition_mask(\n                labels, span_type, Offsets.GO, Offsets.EOS, Offsets.PAD\n            )\n\n        model = self.config_params[\'model\']\n        unif = self.config_params.get(\'unif\', 0.1)\n        model[\'unif\'] = model.get(\'unif\', unif)\n\n        lengths_key = model.get(\'lengths_key\', self.primary_key)\n        if lengths_key is not None:\n            if not lengths_key.endswith(\'_lengths\'):\n                lengths_key = \'{}_lengths\'.format(lengths_key)\n            model[\'lengths_key\'] = lengths_key\n\n        if self.backend.params is not None:\n            for k, v in self.backend.params.items():\n                model[k] = v\n        #return baseline.model.create_tagger_model(self.embeddings, labels, **self.config_params[\'model\'])\n\n    def _load_dataset(self):\n        # TODO: get rid of sort_key=self.primary_key in favor of something explicit?\n        bsz, vbsz, tbsz = Task._get_batchsz(self.config_params)\n        self.train_data, _ = self.reader.load(\n            self.dataset[\'train_file\'],\n            self.feat2index,\n            bsz,\n            shuffle=True,\n            sort_key=\'{}_lengths\'.format(self.primary_key)\n        )\n        self.valid_data, _ = self.reader.load(\n            self.dataset[\'valid_file\'],\n            self.feat2index,\n            vbsz,\n            sort_key=None\n        )\n        self.test_data = None\n        self.txts = None\n        if \'test_file\' in self.dataset:\n            self.test_data, self.txts = self.reader.load(\n                self.dataset[\'test_file\'],\n                self.feat2index,\n                tbsz,\n                shuffle=False,\n                sort_key=None\n            )\n\n    def _get_features(self):\n        return self.embeddings\n\n    def _get_labels(self):\n        return self.reader.label2index\n\n    def train(self, checkpoint=None):\n        self._load_dataset()\n        baseline.save_vectorizers(self.get_basedir(), self.vectorizers)\n        self._reorganize_params()\n        conll_output = self.config_params.get(""conll_output"", None)\n        model_params = self.config_params[\'model\']\n        model_params[\'features\'] = self._get_features()\n        model_params[\'labels\'] = self._get_labels()\n        model_params[\'task\'] = self.task_name()\n        train_params = self.config_params[\'train\']\n        train_params[\'checkpoint\'] = checkpoint\n        train_params[\'conll_output\'] = conll_output\n        train_params[\'txts\'] = self.txts\n\n        baseline.train.fit(model_params, self.train_data, self.valid_data, self.test_data, **train_params)\n        baseline.zip_files(self.get_basedir())\n        self._close_reporting_hooks()\n\n\n\n@export\n@register_task\nclass EncoderDecoderTask(Task):\n\n    def __init__(self, mead_settings_config, **kwargs):\n        super(EncoderDecoderTask, self).__init__(mead_settings_config, **kwargs)\n\n    @classmethod\n    def task_name(cls):\n        return \'seq2seq\'\n\n    def _create_backend(self, **kwargs):\n        backend = Backend(self.config_params.get(\'backend\', \'tf\'), kwargs)\n        if \'preproc\' not in self.config_params:\n            self.config_params[\'preproc\'] = {}\n        self.config_params[\'preproc\'][\'show_ex\'] = show_examples\n        if backend.name == \'pytorch\':\n            self.config_params[\'preproc\'][\'trim\'] = True\n        else:\n\n            self.config_params[\'preproc\'][\'trim\'] = False  # TODO: For datasets on ONLY\n            from mead.tf.exporters import Seq2SeqTensorFlowExporter\n            backend.exporter = Seq2SeqTensorFlowExporter\n        backend.load(self.task_name())\n\n        return backend\n\n    def initialize(self, embeddings):\n        embeddings = read_config_file_or_json(embeddings, \'embeddings\')\n        embeddings_set = index_by_label(embeddings)\n        self.dataset = DataDownloader(self.dataset, self.data_download_cache).download()\n        print_dataset_info(self.dataset)\n        vocab_sources = [self.dataset[\'train_file\'], self.dataset[\'valid_file\']]\n        # TODO: make this optional\n        if \'test_file\' in self.dataset:\n            vocab_sources.append(self.dataset[\'test_file\'])\n        vocab1, vocab2 = self.reader.build_vocabs(vocab_sources,\n                                                  min_f=Task._get_min_f(self.config_params),\n                                                  vocab_file=self.dataset.get(\'vocab_file\'))\n\n        # To keep the config file simple, share a list between source and destination (tgt)\n        features_src = []\n        features_tgt = None\n        for feature in self.config_params[\'features\']:\n            if feature[\'name\'] == \'tgt\':\n                features_tgt = feature\n            else:\n                features_src += [feature]\n\n        self.src_embeddings, self.feat2src = self._create_embeddings(embeddings_set, vocab1, features_src)\n        # For now, dont allow multiple vocabs of output\n        baseline.save_vocabs(self.get_basedir(), self.feat2src)\n        self.tgt_embeddings, self.feat2tgt = self._create_embeddings(embeddings_set, {\'tgt\': vocab2}, [features_tgt])\n        baseline.save_vocabs(self.get_basedir(), self.feat2tgt)\n        self.tgt_embeddings = self.tgt_embeddings[\'tgt\']\n        self.feat2tgt = self.feat2tgt[\'tgt\']\n\n    def _load_dataset(self):\n        bsz, vbsz, tbsz = Task._get_batchsz(self.config_params)\n        self.train_data = self.reader.load(\n            self.dataset[\'train_file\'],\n            self.feat2src, self.feat2tgt,\n            bsz,\n            shuffle=True,\n            sort_key=\'{}_lengths\'.format(self.primary_key)\n        )\n\n        self.valid_data = self.reader.load(\n            self.dataset[\'valid_file\'],\n            self.feat2src, self.feat2tgt,\n            vbsz,\n            shuffle=True\n        )\n        self.test_data = None\n        if \'test_file\' in self.dataset:\n            self.test_data = self.reader.load(\n                self.dataset[\'test_file\'],\n                self.feat2src, self.feat2tgt,\n                tbsz,\n            )\n\n    def _reorganize_params(self):\n        train_params = self.config_params[\'train\']\n        train_params[\'batchsz\'] = train_params[\'batchsz\'] if \'batchsz\' in train_params else self.config_params[\'batchsz\']\n        train_params[\'test_batchsz\'] = train_params.get(\'test_batchsz\', self.config_params.get(\'test_batchsz\', 1))\n\n        self.config_params[\'model\'][""unif""] = self.config_params[""unif""]\n        model = self.config_params[\'model\']\n        unif = self.config_params.get(\'unif\', 0.1)\n        model[\'unif\'] = model.get(\'unif\', unif)\n        lengths_key = model.get(\'src_lengths_key\', self.primary_key)\n        if lengths_key is not None:\n            if not lengths_key.endswith(\'_lengths\'):\n                lengths_key = \'{}_lengths\'.format(lengths_key)\n            model[\'src_lengths_key\'] = lengths_key\n        if self.backend.params is not None:\n            for k, v in self.backend.params.items():\n                model[k] = v\n        #return baseline.model.create_seq2seq_model(self.src_embeddings, self.tgt_embeddings, **self.config_params[\'model\'])\n\n    def _get_features(self):\n        return self.src_embeddings\n\n    def _get_labels(self):\n        return self.tgt_embeddings\n\n    def train(self, checkpoint=None):\n\n        num_ex = self.config_params[\'num_valid_to_show\']\n\n        rlut1 = revlut(self.feat2src[self.primary_key])\n        rlut2 = revlut(self.feat2tgt)\n        if num_ex > 0:\n            logger.info(\'Showing examples\')\n            preproc = self.config_params.get(\'preproc\', {})\n            show_ex_fn = preproc[\'show_ex\']\n            self.config_params[\'train\'][\'after_train_fn\'] = lambda model: show_ex_fn(model,\n                                                                                     self.valid_data, rlut1, rlut2,\n                                                                                     self.feat2tgt,\n                                                                                     preproc[\'mxlen\'], False, 0,\n                                                                                     num_ex, reverse=False)\n        self.config_params[\'train\'][\'tgt_rlut\'] = rlut2\n        return super().train(checkpoint)\n\n\n@export\n@register_task\nclass LanguageModelingTask(Task):\n\n    def __init__(self, mead_settings_config, **kwargs):\n        super().__init__(mead_settings_config, **kwargs)\n\n    @classmethod\n    def task_name(cls):\n        return \'lm\'\n\n    def _create_task_specific_reader(self, vecs_set=None):\n        self._create_vectorizers(vecs_set)\n\n        reader_params = self.config_params[\'reader\'] if \'reader\' in self.config_params else self.config_params[\'loader\']\n        reader_params[\'nctx\'] = reader_params.get(\'nctx\', reader_params.get(\'nbptt\', self.config_params.get(\'nctx\', self.config_params.get(\'nbptt\', 35))))\n        reader_params[\'clean_fn\'] = reader_params.get(\'clean_fn\', self.config_params.get(\'preproc\', {}).get(\'clean_fn\'))\n        if reader_params[\'clean_fn\'] is not None and self.config_params[\'dataset\'] != \'SST2\':\n            logger.warning(\'Warning: A reader preprocessing function (%s) is active, it is recommended that all data preprocessing is done outside of baseline to insure data at inference time matches data at training time.\', reader_params[\'clean_fn\'])\n        reader_params[\'mxlen\'] = self.vectorizers[self.primary_key].mxlen\n        if self.config_params[\'train\'].get(\'gpus\', 1) > 1:\n            reader_params[\'truncate\'] = True\n        return baseline.reader.create_reader(self.task_name(), self.vectorizers, self.config_params.get(\'preproc\', {}).get(\'trim\', False), **reader_params)\n\n    def _create_backend(self, **kwargs):\n        backend = Backend(self.config_params.get(\'backend\', \'tf\'), kwargs)\n        if backend.name == \'pytorch\':\n            self.config_params.get(\'preproc\', {})[\'trim\'] = True\n\n        backend.load(self.task_name())\n        return backend\n\n    def initialize(self, embeddings):\n        embeddings = read_config_file_or_json(embeddings, \'embeddings\')\n        embeddings_set = index_by_label(embeddings)\n        self.dataset = DataDownloader(self.dataset, self.data_download_cache).download()\n        print_dataset_info(self.dataset)\n        vocab_sources = [self.dataset[\'train_file\'], self.dataset[\'valid_file\']]\n        # TODO: make this optional\n        if \'test_file\' in self.dataset:\n            vocab_sources.append(self.dataset[\'test_file\'])\n        vocabs = self.reader.build_vocab(vocab_sources,\n                                         min_f=Task._get_min_f(self.config_params),\n                                         vocab_file=self.dataset.get(\'vocab_file\'))\n        self.embeddings, self.feat2index = self._create_embeddings(embeddings_set, vocabs, self.config_params[\'features\'])\n        baseline.save_vocabs(self.get_basedir(), self.feat2index)\n\n    def _load_dataset(self):\n        read = self.config_params[\'reader\'] if \'reader\' in self.config_params else self.config_params[\'loader\']\n        tgt_key = read.get(\'tgt_key\', self.primary_key)\n        bsz, vbsz, tbsz = Task._get_batchsz(self.config_params)\n        self.train_data = self.reader.load(\n            self.dataset[\'train_file\'],\n            self.feat2index,\n            bsz,\n            tgt_key=tgt_key\n        )\n        self.valid_data = self.reader.load(\n            self.dataset[\'valid_file\'],\n            self.feat2index,\n            vbsz,\n            tgt_key=tgt_key\n        )\n        self.test_data = None\n        if \'test_file\' in self.dataset:\n            self.test_data = self.reader.load(\n                self.dataset[\'test_file\'],\n                self.feat2index,\n                1,\n                tgt_key=tgt_key\n            )\n\n\n    def _reorganize_params(self):\n\n        train_params = self.config_params[\'train\']\n        train_params[\'batchsz\'] = train_params[\'batchsz\'] if \'batchsz\' in train_params else self.config_params[\'batchsz\']\n        train_params[\'test_batchsz\'] = train_params.get(\'test_batchsz\', self.config_params.get(\'test_batchsz\', 1))\n        model = self.config_params[\'model\']\n        unif = self.config_params.get(\'unif\', 0.1)\n        model[\'unif\'] = model.get(\'unif\', unif)\n        model[\'batchsz\'] = train_params[\'batchsz\']\n        model[\'tgt_key\'] = self.config_params.get(\'reader\',\n                                                  self.config_params.get(\'loader\', {})).get(\'tgt_key\', self.primary_key)\n        model[\'src_keys\'] = listify(self.config_params.get(\'reader\', self.config_params.get(\'loader\', {})).get(\'src_keys\', list(self.embeddings.keys())))\n\n        if self.backend.params is not None:\n            for k, v in self.backend.params.items():\n                model[k] = v\n\n    def _get_features(self):\n        return self.embeddings\n\n    def _get_labels(self):\n        return None\n\n    def train(self, checkpoint=None):\n\n        self._reorganize_params()\n        self._load_dataset()\n        # Dont do this here!  We need to move train_data elsewhere\n        calc_lr_params(self.config_params[\'train\'], self.train_data.steps)\n        baseline.save_vectorizers(self.get_basedir(), self.vectorizers)\n\n        model_params = self.config_params[\'model\']\n        model_params[\'task\'] = self.task_name()\n        model_params[\'features\'] = self._get_features()\n        train_params = self.config_params[\'train\']\n        train_params[\'checkpoint\'] = checkpoint\n        baseline.train.fit(model_params, self.train_data, self.valid_data, self.test_data, **train_params)\n        baseline.zip_files(self.get_basedir())\n        self._close_reporting_hooks()\n\n    @staticmethod\n    def _num_steps_per_epoch(num_examples, nctx, batchsz):\n        rest = num_examples // batchsz\n        return rest // nctx\n'"
mead/trainer.py,0,"b'import os\nimport logging\nimport argparse\nimport copy\nfrom itertools import chain\nfrom eight_mile.utils import read_config_stream, str2bool\nfrom baseline.utils import import_user_module, normalize_backend\nimport mead\nfrom mead.utils import convert_path, parse_extra_args, configure_logger, parse_and_merge_overrides\n\nDEFAULT_SETTINGS_LOC = \'config/mead-settings.json\'\nDEFAULT_DATASETS_LOC = \'config/datasets.json\'\nDEFAULT_LOGGING_LOC = \'config/logging.json\'\nDEFAULT_EMBEDDINGS_LOC = \'config/embeddings.json\'\nDEFAULT_VECTORIZERS_LOC = \'config/vecs.json\'\n\nlogger = logging.getLogger(\'mead\')\n\n\ndef update_datasets(datasets_config, config_params, train, valid, test):\n    """"""Take an existing datasets index file and update to include a record with train/valid/test overrides\n\n    If the label provided in the dataset is found in the dataset index, it will use that as a template and\n    individually override the provided `train`, `valid` and `test` params to update that record.\n\n    If the label does not exist, it creates a dummy record and augments that record with the provided `train`,\n    `valid`, and optionally, the `test`\n\n    :param datasets_config: The datasets config to update\n    :param config_params: The mead config\n    :param train: (`str`) An override train set or `None`. If `dataset` key doesnt exist, cannot be `None`\n    :param valid: (`str`) An override valid set or `None` If `dataset` key doesnt exist, cannot be `None`\n    :param test: (`str`) An override test set or None\n    :return: None\n    """"""\n\n    for file_name in [train, valid, test]:\n        if not os.path.exists(train):\n            raise Exception(\'No such file exists for override: {}\'.format(file_name))\n\n    original_dataset_label = config_params[\'dataset\']\n\n    original_record = [entry for entry in datasets_config if entry[\'label\'] == original_dataset_label]\n    if not original_record:\n        if not train or not valid:\n            raise Exception(\'No template label provided, so you must provide at least train and valid params!\')\n        updated_record = {\'label\': original_record, \'train_file\': None, \'valid_file\': None, \'test_file\': None}\n    else:\n        if len(original_record) != 1:\n            logger.warning(\'Warning: multiple templates found for dataset override, using first!\')\n        updated_record = copy.deepcopy(original_record[0])\n        if \'sha1\' in updated_record:\n            logger.info(\'Ignoring SHA1 due to user override\')\n            del updated_record[\'sha1\']\n        if \'download\' in updated_record:\n            if not train or not valid:\n                raise Exception(\'Cannot override downloadable dataset without providing file \'\n                                \'locations for both training and validation\')\n            if not test and \'test_file\' in updated_record:\n                del updated_record[\'test_file\']\n            del updated_record[\'download\']\n    new_dataset_label = \'{}.{}\'.format(original_dataset_label, os.getpid())\n    updated_record[\'label\'] = new_dataset_label\n\n    if train:\n        updated_record[\'train_file\'] = train\n    if valid:\n        updated_record[\'valid_file\'] = valid\n    if test:\n        updated_record[\'test_file\'] = test\n\n    logger.warning(updated_record)\n    config_params[\'dataset\'] = new_dataset_label\n    logger.info(""The dataset key for this override is {}"".format(new_dataset_label))\n    datasets_config.append(updated_record)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Train a text classifier\')\n    parser.add_argument(\'--config\', help=\'JSON/YML Configuration for an experiment: local file or remote URL\', type=convert_path, default=""$MEAD_CONFIG"")\n    parser.add_argument(\'--settings\', help=\'JSON/YML Configuration for mead\', default=DEFAULT_SETTINGS_LOC, type=convert_path)\n    parser.add_argument(\'--task_modules\', help=\'tasks to load, must be local\', default=[], nargs=\'+\', required=False)\n    parser.add_argument(\'--datasets\', help=\'index of dataset labels: local file, remote URL or mead-ml/hub ref\', type=convert_path)\n    parser.add_argument(\'--modules\', help=\'modules to load: local files, remote URLs or mead-ml/hub refs\', default=[], nargs=\'+\', required=False)\n    parser.add_argument(\'--mod_train_file\', help=\'override the training set\')\n    parser.add_argument(\'--mod_valid_file\', help=\'override the validation set\')\n    parser.add_argument(\'--mod_test_file\', help=\'override the test set\')\n    parser.add_argument(\'--fit_func\', help=\'override the fit function\')\n    parser.add_argument(\'--embeddings\', help=\'index of embeddings: local file, remote URL or mead-ml/hub ref\', type=convert_path)\n    parser.add_argument(\'--vecs\', help=\'index of vectorizers: local file, remote URL or hub mead-ml/ref\', type=convert_path)\n    parser.add_argument(\'--logging\', help=\'json file for logging\', default=DEFAULT_LOGGING_LOC, type=convert_path)\n    parser.add_argument(\'--task\', help=\'task to run\', choices=[\'classify\', \'tagger\', \'seq2seq\', \'lm\'])\n    parser.add_argument(\'--gpus\', help=\'Number of GPUs (defaults to number available)\', type=int, default=-1)\n    parser.add_argument(\'--basedir\', help=\'Override the base directory where models are stored\', type=str)\n    parser.add_argument(\'--reporting\', help=\'reporting hooks\', nargs=\'+\')\n    parser.add_argument(\'--backend\', help=\'The deep learning backend to use\')\n    parser.add_argument(\'--checkpoint\', help=\'Restart training from this checkpoint\')\n    parser.add_argument(\'--prefer_eager\', help=""If running in TensorFlow, should we prefer eager model"", type=str2bool)\n    args, overrides = parser.parse_known_args()\n    config_params = read_config_stream(args.config)\n    config_params = parse_and_merge_overrides(config_params, overrides, pre=\'x\')\n    if args.basedir is not None:\n        config_params[\'basedir\'] = args.basedir\n\n    # task_module overrides are not allowed via hub or HTTP, must be defined locally\n    for task in args.task_modules:\n        import_user_module(task)\n\n    task_name = config_params.get(\'task\', \'classify\') if args.task is None else args.task\n    args.logging = read_config_stream(args.logging)\n    configure_logger(args.logging, config_params.get(\'basedir\', \'./{}\'.format(task_name)))\n\n    try:\n        args.settings = read_config_stream(args.settings)\n    except:\n        logger.warning(\'Warning: no mead-settings file was found at [{}]\'.format(args.settings))\n        args.settings = {}\n\n    args.datasets = args.settings.get(\'datasets\', convert_path(DEFAULT_DATASETS_LOC)) if args.datasets is None else args.datasets\n    args.datasets = read_config_stream(args.datasets)\n    if args.mod_train_file or args.mod_valid_file or args.mod_test_file:\n        logging.warning(\'Warning: overriding the training/valid/test data with user-specified files\'\n                        \' different from what was specified in the dataset index.  Creating a new key for this entry\')\n        update_datasets(args.datasets, config_params, args.mod_train_file, args.mod_valid_file, args.mod_test_file)\n\n    args.embeddings = args.settings.get(\'embeddings\', convert_path(DEFAULT_EMBEDDINGS_LOC)) if args.embeddings is None else args.embeddings\n    args.embeddings = read_config_stream(args.embeddings)\n\n    args.vecs = args.settings.get(\'vecs\', convert_path(DEFAULT_VECTORIZERS_LOC)) if args.vecs is None else args.vecs\n    args.vecs = read_config_stream(args.vecs)\n\n    if args.gpus:\n        # why does it go to model and not to train?\n        config_params[\'train\'][\'gpus\'] = args.gpus\n    if args.fit_func:\n        config_params[\'train\'][\'fit_func\'] = args.fit_func\n    if args.backend:\n        config_params[\'backend\'] = normalize_backend(args.backend)\n\n    config_params[\'modules\'] = list(set(chain(config_params.get(\'modules\', []), args.modules)))\n\n    cmd_hooks = args.reporting if args.reporting is not None else []\n    config_hooks = config_params.get(\'reporting\') if config_params.get(\'reporting\') is not None else []\n    reporting = parse_extra_args(set(chain(cmd_hooks, config_hooks)), overrides)\n    config_params[\'reporting\'] = reporting\n\n    logger.info(\'Task: [{}]\'.format(task_name))\n\n    task = mead.Task.get_task_specific(task_name, args.settings)\n\n    task.read_config(config_params, args.datasets, args.vecs, reporting_args=overrides, prefer_eager=args.prefer_eager)\n    task.initialize(args.embeddings)\n    task.train(args.checkpoint)\n\nif __name__ == ""__main__"":\n    main()\n'"
mead/utils.py,0,"b'import os\nimport json\nimport shutil\nimport logging\nimport logging.config\nimport hashlib\nfrom typing import Optional, List, Dict\nfrom datetime import datetime\nimport argparse\nfrom copy import deepcopy\nfrom itertools import chain\nfrom collections import OrderedDict, MutableMapping\nfrom baseline.utils import exporter, str2bool, read_config_file, write_json, get_logging_level, validate_url, zip_files, delete_old_copy\n\n__all__ = []\nexport = exporter(__all__)\nlogger = logging.getLogger(\'mead\')\n\nKNOWN_DATE_FMTS = [\n    \'%Y%m%d\',\n    \'%Y-%m-%d\',\n    \'%Y/%m/%d\',\n    \'%Y\',\n    \'%Y%m\',\n    \'%Y-%m\',\n    \'%Y/%m\',\n    \'%Y%m%d_%H%M\',\n    \'%Y-%m-%d_%H-%M\',\n    \'%Y/%m/%d_%M/%H\'\n]\n\n\n@export\ndef configure_logger(logger_config, basedir=None):\n    """"""Use the logger file (logging.json) to configure the log, but overwrite the filename to include the PID\n\n    There are reporting and timing loggers that are configured, the latter being used for speed testing.\n\n    :param logger_config: The logging configuration JSON or file containing JSON\n    :return: A dictionary config derived from the logger_file, with the reporting handler suffixed with PID\n    """"""\n\n    config = read_config_file_or_json(logger_config, \'logger\')\n    config[\'handlers\'][\'reporting_file_handler\'][\'filename\'] = \'reporting-{}.log\'.format(os.getpid())\n    config[\'handlers\'][\'timing_file_handler\'][\'filename\'] = \'timing-{}.log\'.format(os.getpid())\n    if basedir is not None:\n        # Create a copy of the file handler so we can keep the log in cwd for back compat\n        config[\'handlers\'][\'reporting_basedir_handler\'] = deepcopy(config[\'handlers\'][\'reporting_file_handler\'])\n        # Update the file to be in the basedir of the model\n        config[\'handlers\'][\'reporting_basedir_handler\'][\'filename\'] = os.path.join(basedir, config[\'handlers\'][\'reporting_basedir_handler\'][\'filename\'])\n        config[\'handlers\'][\'reporting_basedir_handler\'][\'class\'] = \'baseline.utils.MakeFileHandler\'\n        # Add this handler to the logger\n        config[\'loggers\'][\'baseline.reporting\'][\'handlers\'].append(\'reporting_basedir_handler\')\n        # Same as above\n        config[\'handlers\'][\'timing_basedir_handler\'] = deepcopy(config[\'handlers\'][\'timing_file_handler\'])\n        config[\'handlers\'][\'timing_basedir_handler\'][\'filename\'] = os.path.join(basedir, config[\'handlers\'][\'timing_basedir_handler\'][\'filename\'])\n        config[\'handlers\'][\'timing_basedir_handler\'][\'class\'] = \'baseline.utils.MakeFileHandler\'\n        config[\'loggers\'][\'baseline.timing\'][\'handlers\'].append(\'timing_basedir_handler\')\n    level = os.getenv(\'LOG_LEVEL\', \'INFO\')\n    config[\'loggers\'][\'baseline\'][\'level\'] = get_logging_level(os.getenv(\'BASELINE_LOG_LEVEL\', level))\n    config[\'loggers\'][\'mead\'][\'level\'] = get_logging_level(os.getenv(\'MEAD_LOG_LEVEL\', level))\n    config[\'handlers\'][\'reporting_console_handler\'][\'level\'] = get_logging_level(os.getenv(\'REPORTING_LOG_LEVEL\', level))\n    config[\'handlers\'][\'timing_console_handler\'][\'level\'] = get_logging_level(os.getenv(\'TIMING_LOG_LEVEL\', level))\n\n    logging.config.dictConfig(config)\n\n\n@export\ndef print_dataset_info(dataset):\n    logger.info(""[train file]: {}"".format(dataset[\'train_file\']))\n    logger.info(""[valid file]: {}"".format(dataset[\'valid_file\']))\n    if \'test_file\' in dataset:\n        logger.info(""[test file]: {}"".format(dataset[\'test_file\']))\n    vocab_file = dataset.get(\'vocab_file\')\n    if vocab_file is not None:\n        logger.info(""[vocab file]: {}"".format(vocab_file))\n    label_file = dataset.get(\'label_file\')\n    if label_file is not None:\n        logger.info(""[label file]: {}"".format(label_file))\n\n\n@export\ndef read_config_file_or_json(config, name=\'\'):\n    if isinstance(config, (dict, list)):\n        return config\n    config = os.path.expanduser(config)\n    if os.path.exists(config):\n        return read_config_file(config)\n    raise Exception(\'Expected {} config file or a JSON object.\'.format(name))\n\n\ndef parse_date(s, known_fmts: List[str] = KNOWN_DATE_FMTS):\n    for fmt in known_fmts:\n        try:\n            dt = datetime.strptime(s, fmt)\n            return dt\n        except:\n            continue\n    raise Exception(""Couldn\'t parse datestamp {}"".format(s))\n\n\n@export\ndef flatten(dictionary, sep=\'.\'):\n    """"""Flatten a nested dict.\n\n    :param dictionary: the dictionary to flatten.\n    :param sep: The separator between old nested keys.\n    :returns: The flattened dict.\n    """"""\n\n    def _flatten(dictionary, sep, prev):\n        """"""This is the recursive function that actually does the work of flattening..\n\n        :param dictionary: the dictionary to flatten.\n        :param sep: The separator between old nested keys.\n        :param prev: A recurrent state param that tracks key above you.\n        :returns: The flattened dict.\n        """"""\n        flat = {}\n        prev = [] if prev is None else prev\n        for k, v in dictionary.items():\n            if isinstance(v, MutableMapping):\n                flat.update(_flatten(v, sep=sep, prev=list(chain(prev, [k]))))\n            else:\n                flat[sep.join(chain(prev, [k]))] = v\n        return flat\n\n    return _flatten(dictionary, sep, [])\n\n\n@export\ndef unflatten(dictionary, sep: str = "".""):\n    """"""Turn a flattened dict into a nested dict.\n\n    :param dictionary: The dict to unflatten.\n    :param sep: This character represents a nesting level in a flattened key\n    :returns: The nested dict\n    """"""\n    nested = {}\n    for k, v in dictionary.items():\n        keys = k.split(sep)\n        it = nested\n        for key in keys[:-1]:\n            # If key is in `it` we get the value otherwise we get a new dict.\n            # .setdefault will also set the new dict to the value of `it[key]`.\n            # assigning to `it` will move us a step deeper into the nested dict.\n            it = it.setdefault(key, {})\n        it[keys[-1]] = v\n    return nested\n\n\ndef _infer_numeric_or_str(value: str):\n    """"""Convert value to an int, float or leave it as a string.\n\n    :param value: The cli value to parse\n    :returns: The value converted to int or float if possible\n    """"""\n    for func in (int, float):\n        try:\n            return func(value)\n        except ValueError:\n            continue\n    return value\n\n\n@export\ndef parse_overrides(overrides, pre):\n    """"""Find override parameters in the cli args.\n\n    Note:\n        If you use the same cli flag multiple time the values will be\n        aggregated into a list. For example\n        `--x:a 1 --x:a 2 --x:a 3` will give back `{\'a\': [1, 2, 3]}`\n\n    :param overrides: The cli flags and values.\n    :param pre: only look at keys that start with `--{pre}:`\n    :returns: The key value pairs from the command line args.\n    """"""\n    pre = f\'--{pre}:\'\n    parser = argparse.ArgumentParser()\n    for key in set(filter(lambda x: pre in x, overrides)):\n        # Append action collect each value into a list allowing us to override a\n        # yaml list by repeating the key.\n        parser.add_argument(key, action=\'append\', type=_infer_numeric_or_str)\n    args = parser.parse_known_args(overrides)[0]\n    return {k.split("":"")[1]: v[0] if len(v) == 1 else v for k, v in vars(args).items()}\n\n\n@export\ndef parse_and_merge_overrides(base, overrides, pre):\n    """"""Parse extra cli args and use them to override the original config.\n\n    :param base: The base config that will be overridden\n    :param overrides: The cli args holding override values\n    :param pre: The key used to find cli flags.\n    :returns: The base config overridden with values from overrides.\n    """"""\n    overrides = parse_overrides(overrides, pre)\n    base = flatten(base)\n    base.update(overrides)\n    return unflatten(base)\n\n\n@export\ndef get_dataset_from_key(dataset_key, datasets_set):\n\n    # This is the previous behavior\n    if dataset_key in datasets_set:\n        return datasets_set[dataset_key]\n\n    last_date = parse_date(\'1900\')\n    last_k = None\n    for k, v in datasets_set.items():\n\n        if dataset_key in k:\n            try:\n                parts = k.split(\':\')\n                dt = parse_date(parts[-1])\n                dataset_name = "":"".join(parts[:-1])\n            except Exception:\n                continue\n            if dt > last_date and dataset_name == dataset_key:\n                last_date = dt\n                last_k = k\n\n    if last_k is None:\n        raise Exception(""No dataset could be found with key {}"".format(dataset_key))\n\n    return datasets_set[last_k]\n\n\n@export\ndef get_mead_settings(mead_settings_config):\n    if mead_settings_config is None:\n        return {}\n    return read_config_file_or_json(mead_settings_config, \'mead settings\')\n\n\n@export\ndef index_by_label(object_list):\n    objects = {x[\'label\']: x for x in object_list}\n    return objects\n\n\n@export\ndef convert_path(path, loc=None):\n    """"""If the provided path doesn\'t exist search for it relative to loc (or this file).""""""\n    if os.path.isfile(path) or path.startswith(""$"") or validate_url(path):\n        return path\n    if path.startswith(""http""):\n        return path\n    if path.startswith(""hub:""):\n        return path\n    if loc is None:\n        loc = os.path.dirname(os.path.realpath(__file__))\n    return os.path.join(loc, path)\n\n\ndef _infer_type_or_str(x):\n    try:\n        return str2bool(x)\n    except:\n        try:\n            return int(x)\n        except ValueError:\n            try:\n                return float(x)\n            except ValueError:\n                return x\n\n\n@export\ndef parse_extra_args(base_args, extra_args):\n    """"""Parse extra command line arguments based on based names.\n    Note:\n        special args should be in the form --{base_name}:{special_name}\n    :param base_args: List[str], A list of base argument names.\n    :param extra_args: List[str], A list of special arguments and values.\n    :returns:\n        dict, The parsed special settings in the form\n        {\n            ""base"": {\n                ""special"": val,\n                ...\n            },\n            ...\n        }\n    """"""\n    found_args = []\n    for arg in base_args:\n        key = ""{}:"".format(arg)\n        for extra_arg in extra_args:\n            if key in extra_arg:\n                found_args.append(extra_arg)\n    parser = argparse.ArgumentParser()\n    for key in found_args:\n        parser.add_argument(key, type=_infer_type_or_str)\n    args = parser.parse_known_args(extra_args)[0]\n    settings = {arg: {} for arg in base_args}\n    args = vars(args)\n    for key in args:\n        base, extra = key.split("":"")\n        settings[base][extra] = args[key]\n    return settings\n\n\n@export\ndef order_json(data):\n    """"""Sort json to a consistent order.\n    When you hash json that has the some content but is different orders you get\n    different fingerprints.\n    In:  hashlib.sha1(json.dumps({\'a\': 12, \'b\':14}).encode(\'utf-8\')).hexdigest()\n    Out: \'647aa7508f72ece3f8b9df986a206d95fd9a2caf\'\n    In:  hashlib.sha1(json.dumps({\'b\': 14, \'a\':12}).encode(\'utf-8\')).hexdigest()\n    Out: \'a22215982dc0e53617be08de7ba9f1a80d232b23\'\n    This function sorts json by key so that hashes are consistent.\n    Note:\n        In our configs we only have lists where the order doesn\'t matter so we\n        can sort them for consistency. This would have to change if we add a\n        config field that needs order we will need to refactor this.\n    :param data: dict, The json data.\n    :returns:\n        collections.OrderedDict: The data in a consistent order (keys sorted alphabetically).\n    """"""\n    new = OrderedDict()\n    for (key, value) in sorted(data.items(), key=lambda x: x[0]):\n        if isinstance(value, dict):\n            value = order_json(value)\n        new[key] = value\n    return new\n\n\nKEYS = {\n    (\'conll_output\',),\n    (\'visdom\',),\n    (\'visdom_name\',),\n    (\'model\', \'gpus\'),\n    (\'test_thresh\',),\n    (\'reporting\',),\n    (\'num_valid_to_show\',),\n    (\'train\', \'verbose\'),\n    (\'train\', \'model_base\'),\n    (\'train\', \'model_zip\'),\n    (\'train\', \'nsteps\'),\n    (\'test_batchsz\',),\n    (\'basedir\',),\n}\n\n\n@export\ndef remove_extra_keys(config, keys=KEYS):\n    """"""Remove config items that don\'t effect the model.\n    We base most things off of the sha1 hash of the model configs but there\n    is a problem. Some things in the config file don\'t effect the model such\n    as the name of the `conll_output` file or if you are using `visdom`\n    reporting. This strips out these kind of things so that as long as the model\n    parameters match the sha1 will too.\n    :param config: dict, The json data.\n    :param keys: Set[Tuple[str]], The keys to remove.\n    :returns:\n        dict, The config with certain keys removed.\n    """"""\n    c = deepcopy(config)\n    for key in keys:\n        x = c\n        for k in key[:-1]:\n            x = x.get(k)\n            if x is None:\n                break\n        else:\n            _ = x.pop(key[-1], None)\n    return c\n\n\n@export\ndef hash_config(config):\n    """"""Hash a json config with sha1.\n    :param config: dict, The config to hash.\n    :returns:\n        str, The sha1 hash.\n    """"""\n    stripped_config = remove_extra_keys(config)\n    sorted_config = order_json(stripped_config)\n    json_bytes = json.dumps(sorted_config).encode(\'utf-8\')\n    return hashlib.sha1(json_bytes).hexdigest()\n\n\ndef _listdir(model_dir):\n    try:\n        return os.listdir(model_dir)\n    except OSError:\n        return []\n\n\n@export\ndef find_model_version(model_dir):\n    """"""Find the next usable model version when exporting.\n\n    :param model_dir: `str` The directory we are exporting to.\n\n    :returns: `str` The model version.\n    """"""\n    return str(max(chain([0], map(int, filter(lambda x: x.isdigit(), _listdir(model_dir))))) + 1)\n\n\n@export\ndef get_output_paths(\n        output_dir: str,\n        project: Optional[str] = None,\n        name: Optional[str] = None,\n        version: Optional[str] = None,\n        remote: bool = False,\n        make_server: bool = True,\n        use_version: bool = True,\n    ):\n    """"""Create the output paths for export.\n\n    Old behavior:\n    if remote == True:\n        f""{output_dir}/client/{basename(output_dir)}/{version}""\n        f""{output_dir}/server/{basename(output_dir)}/{version}""\n    else:\n        f""{output_dir}/{version}""\n\n    New Behavior:\n    if remote == True:\n        f""{output_dir}/client/{project}/{name}/{version}""\n        f""{output_dir}/server/{project}/{name}/{version}""\n    else:\n        f""{output_dir}/{project}/{name}/{version}""\n\n    If either project or name is None then they are skipped in the path.\n\n    If you want to create output that looks the same as the old versions\n    then set both project and name to None. This will skip them if remote is\n    false or it will add the basename of output_dir to the path if remote.\n\n    :param output_dir: `str` The base of these paths.\n    :param project: `str` The first subdir in the created path.\n    :param name: `str` The second subdir in the created path.\n    :param version: `str` The model version.\n    :param remote: `bool` Should we create separate client and server bundles?\n    :param maker_server: `bool` When false don\'t actually make the directory that the\n        server part will go in. This is because TF really wants to make it itself.\n    :param use_version: Should we use\n    :returns: `Tuple[str, str]` The client and server output dirs.\n    """"""\n    # In this case we use the basename to simulate the old behavior.\n    if remote and project is None and name is None:\n        project = os.path.basename(output_dir)\n    project = project if project is not None else \'\'\n    name = name if name is not None else \'\'\n    client = \'client\' if remote else \'\'\n    server = \'server\' if remote else \'\'\n    server_path = [output_dir, server, project, name]\n    client_path = [output_dir, client, project, name]\n    # Sniff the dir and see what version we should use\n    if use_version:\n        version = find_model_version(os.path.join(*server_path)) if version is None else version\n        server_path.append(version)\n        client_path.append(version)\n    server_path = os.path.join(*server_path)\n    client_path = os.path.join(*client_path)\n    if remote:\n        os.makedirs(client_path)\n    if make_server:\n        os.makedirs(server_path)\n    return client_path, server_path\n\n\n@export\ndef get_export_params(\n        config: Dict,\n        output_dir: Optional[str] = None,\n        project: Optional[str] = None,\n        name: Optional[str] = None,\n        model_version: Optional[str] = None,\n        exporter_type: Optional[str] = None,\n        return_labels: Optional[bool] = None,\n        is_remote: Optional[bool] = None\n):\n    """"""Combine export parameters from the config file and cli arguments.\n\n    :param config: The export block of the config.\n    :param output_dir: The base of export paths. (defaults to \'./models\')\n    :param project: The name of the project this model is for.\n    :param name: The name of this model (often the use case for it, `ner`, `intent` etc).\n    :param model_version: The version of this model.\n    :param exporter_type: The name of the exporter to use (defaults to \'default\')\n    :param return_labels: Should labels be returned? (defaults to False)\n    :param is_remote: Should the bundle be split into client and server dirs.\n    :param user_version\n\n    :returns: `Tuple[str, str, str, str, str, bool, bool]`\n        The output_dir, project, name, model_version, exporter_type, return_labels, and remote\n    """"""\n    project = project if project is not None else config.get(\'project\')\n    name = name if name is not None else config.get(\'name\')\n    output_dir = output_dir if output_dir is not None else config.get(\'output_dir\', \'./models\')\n    output_dir = os.path.expanduser(output_dir)\n    model_version = model_version if model_version is not None else config.get(\'model_version\')\n    exporter_type = exporter_type if exporter_type is not None else config.get(\'type\', config.get(\'exporter_type\', \'default\'))\n    return_labels = return_labels if return_labels is not None else config.get(\'return_labels\', False)\n    return_labels = str2bool(return_labels)\n    is_remote = is_remote if is_remote is not None else config.get(\'is_remote\', True)\n    is_remote = str2bool(is_remote)\n    return output_dir, project, name, model_version, exporter_type, return_labels, is_remote\n\n\ndef create_metadata(inputs, outputs, sig_name, model_name, lengths_key=None, beam=None, return_labels=False, preproc=\'client\'):\n    meta = {\n        \'inputs\': inputs,\n        \'outputs\': outputs,\n        \'signature_name\': sig_name,\n        \'metadata\': {\n            \'exported_model\': str(model_name),\n            \'exported_time\': str(datetime.utcnow()),\n            \'return_labels\': return_labels,\n            \'preproc\': preproc,\n        }\n    }\n    if lengths_key:\n        meta[\'lengths_key\'] = lengths_key\n    if beam:\n        meta[\'beam\'] = beam\n\n    return meta\n\n\ndef save_to_bundle(output_path, directory, assets=None, zip_results=False):\n    """"""Save files to the exported bundle.\n\n    :vocabs\n    :vectorizers\n    :labels\n    :assets\n    :output_path  the bundle output_path. vocabs, vectorizers know how to save themselves.\n    """"""\n    for filename in os.listdir(directory):\n        if filename.startswith(\'vocabs\') or \\\n           filename.endswith("".labels"") or \\\n           filename.startswith(\'vectorizers\'):\n            shutil.copy(os.path.join(directory, filename), os.path.join(output_path, filename))\n\n    if assets:\n        asset_file = os.path.join(output_path, \'model.assets\')\n        write_json(assets, asset_file)\n\n    if zip_results:\n        zip_files(output_path, False)\n        delete_old_copy(output_path)\n\ndef create_feature_exporter_field_map(feature_section, default_exporter_field=\'tokens\'):\n    feature_exporter_field_map = {}\n    for feature_desc in feature_section:\n        if feature_desc.get(\'exporter_field\') is None:\n            feature_exporter_field_map[feature_desc[\'name\']] = default_exporter_field\n        else:\n            feature_exporter_field_map[feature_desc[\'name\']] = feature_desc[\'exporter_field\']\n    return feature_exporter_field_map\n'"
scripts/bump.py,0,"b'import os\nimport re\nimport argparse\n\n\nVERSION_REGEX = re.compile(r\'\'\'^__version__ = [\'""](?P<major>\\d+)\\.(?P<minor>\\d+).(?P<patch>\\d+)(?:dev(?P<dev>\\d*))?[""\']$\'\'\')\n\n\ndef parse_version(regex, data):\n    return regex.match(data)\n\n\ndef bump_version(data, version):\n    match = parse_version(VERSION_REGEX, data)\n    if match is None:\n        raise ValueError(""Version string could not be parsed."")\n    current = match.group(version)\n    if current is None:\n        bumped = 0\n    elif current == \'\':\n        bumped = 1\n    else:\n        bumped = int(current) + 1\n    return set_version(match, version, bumped)\n\n\ndef set_version(match, version, value):\n    if version == \'major\':\n        return set_major(match, value)\n    elif version == \'minor\':\n        return set_minor(match, value)\n    elif version == \'patch\':\n        return set_patch(match, value)\n    return set_dev(match, value)\n\n\ndef set_major(match, number):\n    return ""{}.{}.{}"".format(number, 0, 0)\n\ndef set_minor(match, number):\n    return ""{}.{}.{}"".format(\n        match.group(\'major\'),\n        number, 0\n    )\n\ndef set_patch(match, number):\n    return ""{}.{}.{}"".format(\n        match.group(\'major\'),\n        match.group(\'minor\'),\n        number\n    )\n\ndef set_dev(match, number):\n    if number == 0:\n        number = \'\'\n    return ""{}.{}.{}dev{}"".format(\n        match.group(\'major\'),\n        match.group(\'minor\'),\n        match.group(\'patch\'),\n        number\n    )\n\n\ndef projects_to_file(name):\n    loc = os.path.realpath(os.path.dirname(__file__))\n    if name == \'baseline\':\n        return os.path.realpath(os.path.join(loc, \'..\', \'python\', \'baseline\', \'version.py\'))\n    elif name == \'xpctl\':\n        return os.path.realpath(os.path.join(loc, \'..\', \'python\', \'xpctl\', \'version.py\'))\n    elif name == \'hpctl\':\n        return os.path.realpath(os.path.join(loc, \'..\', \'python\', \'hpctl\', \'hpctl\', \'version.py\'))\n    return name\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Bump versions."")\n    parser.add_argument(""file"")\n    parser.add_argument(""type"", choices={\'major\', \'minor\', \'patch\', \'dev\'})\n    parser.add_argument(""--test"", action=""store_true"")\n    args = parser.parse_args()\n    file_name = projects_to_file(args.file)\n    with open(file_name) as f:\n        data = f.read().strip(""\\n"")\n    new_version = bump_version(data, args.type)\n    result = \'__version__ = ""{}""\\n\'.format(new_version)\n    if args.test:\n        print(""The found version string:"")\n        print(""\\t{}"".format(data))\n        print(""The version string after a {} bump:"".format(args.type))\n        print(""\\t{}"".format(result))\n        return\n    with open(file_name, \'w\') as f:\n        f.write(result)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/compare_calibrations.py,0,"b'""""""Plot and compare the metrics from various calibrated models.\n\nThis script creates the following:\n\n    * A csv file with columns for the Model Type (the label), and the various calibration metrics\n    * A grid of graphs, the first row is confidence histograms for each model, the second row is\n      the reliability diagram for that model.\n    * If the problem as binary it creates calibration curves for each model all plotted on the same graph.\n\nMatplotlib is required to use this script. The `tabulate` package is recommended but not required.\n\nThe input of this script is pickle files created by `$MEAD-BASELINE/api-examples/analyze_calibration.py`\n""""""\n\nimport csv\nimport pickle\nimport argparse\nfrom collections import Counter\nfrom eight_mile.calibration import (\n    expected_calibration_error,\n    maximum_calibration_error,\n    reliability_diagram,\n    reliability_curve,\n    confidence_histogram,\n    Bins,\n)\nimport matplotlib.pyplot as plt\n\n\nparser = argparse.ArgumentParser(description=""Compare calibrated models by grouping visualizations and creating a table."")\nparser.add_argument(""--stats"", nargs=""+"", default=[], required=True, help=""A list of pickles created by the analyze_calibration.py script to compare."")\nparser.add_argument(""--labels"", nargs=""+"", default=[], required=True, help=""A list of labels to assign to each pickle, should have the same number of arguments as --stats"")\nparser.add_argument(""--metrics-output"", ""--metrics_output"", default=""table.csv"", help=""Filename to save the resulting metrics into as a csv"")\nparser.add_argument(""--curve-output"", ""--curve_output"", default=""curve.png"", help=""Filename to save the reliability curves graph to."")\nparser.add_argument(""--diagram-output"", ""--diagram_output"", default=""diagram.png"", help=""Filename to save the reliability diagrams and confidence histograms too."")\nparser.add_argument(""--figsize"", default=10, type=int, help=""The size of the figure, controls how tall the figure is."")\nargs = parser.parse_args()\n\n# Make sure the labels and stats are aligned\nif len(args.stats) != len(args.labels):\n    raise ValueError(f""You need a label for each calibration stat you load. Got {len(args.stats)} stats and {len(args.labels)} labels"")\n\n# Make sure the labels are unique\ncounts = Counter(args.labels)\nif any(v != 1 for v in counts.values()):\n    raise ValueError(f""All labels must be unique, found duplicates of {[k for k, v in counts.items() if v != 1]}"")\n\n# Load the calibration stats\nstats = []\nfor file_name in args.stats:\n    with open(file_name, ""rb"") as f:\n        stats.append(pickle.load(f))\n\n# Make sure there is the same number of bins for each model\nfor field in stats[0]:\n    if not isinstance(stats[0][field], Bins):\n        continue\n    lengths = []\n    for stat in stats:\n        if stat[field] is None:\n            continue\n        lengths.append(len(stat[field].accs))\n    if len(set(lengths)) != 1:\n        raise ValueError(f""It is meaningless to compare calibrations with different numbers of bins: Mismatch was found for {field}"")\n\n\ndef get_metrics(data, model_type):\n    return {\n        ""Model Type"": model_type,\n        ""ECE"": expected_calibration_error(data.accs, data.confs, data.counts) * 100,\n        ""MCE"": maximum_calibration_error(data.accs, data.confs, data.counts) * 100,\n    }\n\n# Calculate the metrics based on the multiclass calibration bins\nmetrics = [get_metrics(stat[\'multiclass\'], label) for stat, label in zip(stats, args.labels)]\n\n# Print the metrics\ntry:\n    # If you have tabulate installed it prints a nice postgres style table\n    from tabulate import tabulate\n    print(tabulate(metrics, headers=""keys"", floatfmt="".3f"", tablefmt=""psql""))\nexcept ImportError:\n    for metric in metrics:\n        for k, v in metric.items():\n            if isinstance(v, float):\n                print(f""{k}: {v:.3f}"")\n            else:\n                print(f""{k}: {v}"")\n\n# Write the metrics to a csv to look at later\nwith open(args.metrics_output, ""w"", newline="""") as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=list(metrics[0].keys()), quoting=csv.QUOTE_MINIMAL, delimiter="","", dialect=""unix"")\n    writer.writeheader()\n    writer.writerows(metrics)\n\n# Plot the histograms and graphs for each model\nf, ax = plt.subplots(2, len(metrics), figsize=(args.figsize * len(metrics) // 2, args.figsize), sharey=True, sharex=True)\nfor i, (stat, label) in enumerate(zip(stats, args.labels)):\n    # If you are the first model you get y_labels, everyone else just uses yours\n    if i == 0:\n        confidence_histogram(\n            stat[\'histogram\'].edges,\n            stat[\'histogram\'].counts,\n            acc=stat[\'acc\'],\n            avg_conf=stat[\'conf\'],\n            title=f""{label}\\nConfidence Distribution"",\n            x_label=None,\n            ax=ax[0][i],\n        )\n        reliability_diagram(\n            stat[\'multiclass\'].accs,\n            stat[\'multiclass\'].confs,\n            stat[\'multiclass\'].edges,\n            num_classes=stat[\'num_classes\'],\n            ax=ax[1][i]\n        )\n    else:\n        confidence_histogram(\n            stat[\'histogram\'].edges,\n            stat[\'histogram\'].counts,\n            acc=stat[\'acc\'],\n            avg_conf=stat[\'conf\'],\n            title=f""{label}\\nConfidence Distribution"",\n            y_label=None,\n            x_label=None,\n            ax=ax[0][i],\n        )\n        reliability_diagram(\n            stat[\'multiclass\'].accs,\n            stat[\'multiclass\'].confs,\n            stat[\'multiclass\'].edges,\n            num_classes=stat[\'num_classes\'],\n            y_label=None,\n            ax=ax[1][i]\n        )\n\nf.savefig(args.diagram_output)\nplt.show()\n\n# Plot reliability curves for binary classification models\nif stats[0][\'num_classes\'] == 2:\n    f, ax = plt.subplots(1, 1, figsize=(args.figsize, args.figsize))\n    for stat, label, color in zip(stats, args.labels, plt.rcParams[\'axes.prop_cycle\'].by_key()[\'color\']):\n        reliability_curve(\n            stat[\'binary\'].accs,\n            stat[\'binary\'].confs,\n            color=color,\n            label=label,\n            ax=ax\n        )\n    f.savefig(args.curve_output)\n    plt.show()\n'"
scripts/download_all.py,0,"b'import os\nimport argparse\nfrom baseline.utils import read_json\nfrom mead.utils import index_by_label, convert_path\nfrom baseline.utils import EmbeddingDownloader, DataDownloader\n\n\nparser = argparse.ArgumentParser(description=""Download all data and embeddings."")\nparser.add_argument(""--cache"", default=""~/.bl-data"", type=os.path.expanduser, help=""Location of the data cache"")\nparser.add_argument(\'--datasets\', help=\'json library of dataset labels\', default=\'config/datasets.json\', type=convert_path)\nparser.add_argument(\'--embeddings\', help=\'json library of embeddings\', default=\'config/embeddings.json\', type=convert_path)\nargs = parser.parse_args()\n\n\ndatasets = read_json(args.datasets)\ndatasets = index_by_label(datasets)\n\nfor name, d in datasets.items():\n    print(name)\n    try:\n        DataDownloader(d, args.cache).download()\n    except Exception as e:\n        print(e)\n\n\nemb = read_json(args.embeddings)\nemb = index_by_label(emb)\n\nfor name, e in emb.items():\n    print(name)\n    try:\n        EmbeddingDownloader(e[\'file\'], e[\'dsz\'], e.get(\'sha1\'), args.cache).download()\n    except Exception as e:\n        print(e)\n'"
scripts/lr_compare.py,0,"b'import os\nimport json\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_learning_rates(ys, names):\n    fig, ax = plt.subplots(1, 1)\n    ax.set_title(\'Learning Rates from baseline.\')\n    ax.set_xlabel(\'Steps\')\n    ax.set_ylabel(\'Learning Rates\')\n    for y, name in zip(ys, names):\n        ax.plot(np.arange(len(y)), y, label=name)\n    ax.legend()\n    return fig\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Compare various schedules"")\n    parser.add_argument(""--index"", default=\'.lrs/cache.index\', help=""The location of the lr index"")\n    args = parser.parse_args()\n\n    dir_ = os.path.dirname(args.index)\n    index = {}\n    with open(args.index) as f:\n        for line in f:\n            line = json.loads(line)\n            try:\n                index[line[\'name\']] = np.load(os.path.join(dir_, line[\'file\']))\n            except:\n                pass\n\n    keys = index.keys()\n\n    plot_learning_rates([index[k] for k in keys], keys)\n    plt.tight_layout()\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/lr_find.py,0,"b'import six\nimport argparse\nfrom copy import deepcopy\nfrom itertools import chain\nfrom baseline.utils import read_config_stream, normalize_backend, str2bool\nimport mead\nfrom mead.utils import convert_path, parse_extra_args\nfrom baseline.train import register_training_func, create_trainer\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n@register_training_func(\'classify\', name=\'lr-find\')\n@register_training_func(\'tagger\', name=\'lr-find\')\ndef fit(model, ts, vs, es, **kwargs):\n    num_iters = kwargs.get(\'num_iters\', 5)\n    kwargs[\'warmup_steps\'] = num_iters * len(ts)\n    kwargs[\'lr\'] = kwargs.get(\'max_lr\', 10.)\n    trainer = create_trainer(model, **kwargs)\n    lrs = []\n    losses = []\n    use_val = kwargs.get(\'use_val\', False)\n    beta = kwargs.get(\'smooth_beta\', 0.05)\n    log = kwargs.get(\'log_scale\', True)\n    best_loss = six.MAXSIZE\n    diverge_threshold = kwargs.get(\'diverge_threshold\', 5)\n    stop = False\n    i = 0\n    be = kwargs.get(\'backend\', \'tf\')\n    if be == \'tf\':\n        import tensorflow as tf\n        tables = tf.compat.v1.tables_initializer()\n        model.sess.run(tables)\n        model.sess.run(tf.compat.v1.global_variables_initializer())\n        model.set_saver(tf.compat.v1.train.Saver())\n\n\n    for _ in range(num_iters):\n        if stop:\n            break\n        for batch in ts:\n            i += 1\n            train_metrics = trainer.train([batch], [])\n            if use_val:\n                val_metrics = trainer.test(vs, [])\n                loss = val_metrics[\'avg_loss\']\n            else:\n                loss = train_metrics[\'avg_loss\']\n\n            if losses and beta > 0:\n                loss = beta * loss + (1 - beta) * losses[-1]\n                loss /= (1 - beta ** i)\n\n            losses.append(loss)\n            if be == \'tf\':\n                lrs.append(model.sess.run(""OptimizeLoss/lr:0""))\n            else:\n                lrs.append(trainer.optimizer.current_lr)\n\n            if loss < best_loss:\n                best_loss = loss\n\n            if loss > diverge_threshold * best_loss:\n                print(""Stopping Early"")\n                stop = True\n                break\n\n    plt.plot(lrs, losses)\n    if log:\n        plt.xscale(\'log\')\n        plt.xlabel(\'Learning Rate (log scale)\')\n    else:\n        plt.xlabel(\'Learning Rate\')\n    if use_val:\n        plt.ylabel(\'Average Validation Loss\')\n    else:\n        plt.ylabel(\'Per Batch Training Loss\')\n    plt.show()\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Train a text classifier\')\n    parser.add_argument(\'--config\', help=\'JSON Configuration for an experiment\', type=convert_path, default=""$MEAD_CONFIG"")\n    parser.add_argument(\'--settings\', help=\'JSON Configuration for mead\', default=\'config/mead-settings.json\', type=convert_path)\n    parser.add_argument(\'--datasets\', help=\'json library of dataset labels\', default=\'config/datasets.json\', type=convert_path)\n    parser.add_argument(\'--embeddings\', help=\'json library of embeddings\', default=\'config/embeddings.json\', type=convert_path)\n    parser.add_argument(\'--logging\', help=\'json file for logging\', default=\'config/logging.json\', type=convert_path)\n    parser.add_argument(\'--task\', help=\'task to run\', choices=[\'classify\', \'tagger\', \'seq2seq\', \'lm\'])\n    parser.add_argument(\'--backend\', help=\'The deep learning backend to use\')\n\n    parser.add_argument(\'--num_iters\', type=int, default=5)\n    parser.add_argument(\'--max_lr\', type=float, default=10)\n    parser.add_argument(\'--smooth\', type=float, default=0.05)\n    parser.add_argument(\'--use_val\', type=str2bool, default=False)\n    parser.add_argument(\'--log\', type=str2bool, default=True)\n    parser.add_argument(\'--diverge_threshold\', type=int, default=5)\n\n    args, reporting_args = parser.parse_known_args()\n\n    config_params = read_config_stream(args.config)\n    try:\n        args.settings = read_config_stream(args.settings)\n    except:\n        print(\'Warning: no mead-settings file was found at [{}]\'.format(args.config))\n        args.settings = {}\n    args.datasets = read_config_stream(args.datasets)\n    args.embeddings = read_config_stream(args.embeddings)\n    args.logging = read_config_stream(args.logging)\n\n    if args.backend is not None:\n        config_params[\'backend\'] = normalize_backend(args.backend)\n\n    config_params[\'reporting\'] = {}\n    config_params[\'train\'][\'fit_func\'] = ""lr-find""\n    config_params[\'train\'][\'lr_scheduler_type\'] = \'warmup_linear\'\n    config_params[\'train\'][\'smooth_beta\'] = args.smooth\n    config_params[\'train\'][\'use_val\'] = args.use_val\n    config_params[\'train\'][\'log_scale\'] = args.log\n    config_params[\'train\'][\'diverge_threshold\'] = args.diverge_threshold\n    config_params[\'train\'][\'be\'] = config_params[\'backend\']\n\n    task_name = config_params.get(\'task\', \'classify\') if args.task is None else args.task\n    print(\'Task: [{}]\'.format(task_name))\n    task = mead.Task.get_task_specific(task_name, args.logging, args.settings)\n    task.read_config(config_params, args.datasets, reporting_args=reporting_args, config_file=deepcopy(config_params))\n    task.initialize(args.embeddings)\n    task.train()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/lr_visualize.py,0,"b'import os\nimport math\nimport json\nimport inspect\nimport argparse\nfrom hashlib import sha1\nfrom collections import defaultdict\nimport numpy as np\nimport baseline as bl\nimport matplotlib.pyplot as plt\nfrom lr_compare import plot_learning_rates\n\n\n# Collect possible Schedulers\nOPTIONS = {}\nWARMUP = {}\nbase_classes = (\n    bl.train.LearningRateScheduler.__name__,\n    bl.train.WarmupLearningRateScheduler.__name__\n)\n# Collect all schedulers except base classes and separate out the warmup ones\nfor item_name in dir(bl.train):\n    if item_name in base_classes: continue\n    item = getattr(bl.train, item_name)\n    try:\n        if issubclass(item, bl.train.LearningRateScheduler):\n            OPTIONS[item_name] = item\n        if issubclass(item, bl.train.WarmupLearningRateScheduler):\n            WARMUP[item_name] = item\n    except:\n        pass\nREST = {}\nfor k, v in OPTIONS.items():\n    if k not in WARMUP:\n        REST[k] = v\n\n# Collect the args and defaults that the schedulers use.\n# This currently misses the `lr` because it is kwargs only as well\n# As `values` because that is never a defaulted argument\n# (we skip over ones without defaults)\nARGS = defaultdict(list)\nDEFAULTS = defaultdict(list)\nfor k, v in OPTIONS.items():\n    az = inspect.getargspec(v).args\n    ds = inspect.getargspec(v).defaults\n    if az is None or ds is None: continue\n    for a, d in zip(reversed(az), reversed(ds)):\n        ARGS[a].append(k)\n        DEFAULTS[a].append(d)\n\nfor a, ds in DEFAULTS.items():\n    ds_set = set(ds)\n    if len(ds_set) == 1:\n        # If the only default values is None set to None.\n        if None in ds_set:\n            DEFAULTS[a] = None\n        # Set to the only default.\n        else:\n            DEFAULTS[a] = ds_set.pop()\n    else:\n        # Remove None and grab the last default, no reason arbitrary but\n        # consistant\n        if None in ds: ds.remove(None)\n        DEFAULTS[a] = ds[-1]\n\n\nnew_defaults = defaultdict(lambda: None)\nfor k, v in DEFAULTS.items():\n    new_defaults[k] = v\n\nDEFAULTS = new_defaults\n\n\ndef exp_steps(y, lr, decay_rate, decay_steps):\n    """"""Calculate how many steps are needed to get to some value.""""""\n    return decay_steps * (math.log(y / lr) / math.log(decay_rate))\n\n\ndef inv_steps(y, lr, decay_rate, decay_steps):\n    """"""Calculate how many steps are needed to get to some value.""""""\n    return decay_steps * (((lr / y) - 1.0) / decay_rate)\n\n\ndef get_steps(type_, args, double_warm=False):\n    """"""\n    Calculate the number of steps to plot. Most of these calculations are\n    bespoke to the scheduler.\n    """"""\n    if type_ == \'ConstantScheduler\':\n        return 1000\n    # WarmupLinear\n    if type_ in WARMUP:\n        # If we are just warm up double it to show its fine\n        return args[\'warmup_steps\'] * 2 if double_warm else args[\'warmup_steps\']\n    # PiecewiseDecay, Zaremba\n    if args[\'boundaries\'] is not None:\n        # If we are bound based how bounds plus some extra\n        gap = np.max(np.diff(args[\'boundaries\']))\n        return args[\'boundaries\'][-1] + gap\n    if type_ == \'CyclicLRScheduler\':\n        # Cyclic, show some cycles\n        return args[\'decay_steps\'] * args[\'cycles\']\n    # Show it decay once\n    # Cosine, InverseTimeDecay, Exponential\n    if type_ == \'ExponentialDecayScheduler\':\n        return exp_steps(args[\'min_lr\'], args[\'lr\'], args[\'decay_rate\'], args[\'decay_steps\'])\n    if type_ == \'InverseTimeDecayScheduler\':\n        return inv_steps(args[\'min_lr\'], args[\'lr\'], args[\'decay_rate\'], args[\'decay_steps\'])\n    if type_ == \'CosineDecayScheduler\':\n        return args[\'decay_steps\']\n\n\n# def plot_learning_rates(ys, names):\n#     fig, ax = plt.subplots(1, 1)\n#     ax.set_title(\'Learning Rates from baseline.\')\n#     ax.set_xlabel(\'Steps\')\n#     ax.set_ylabel(\'Learning Rates\')\n#     for y, name in zip(ys, names):\n#         ax.plot(np.arange(len(y)), y, label=name)\n#     ax.legend()\n#     return fig\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""Plot a learning rate schedule."")\n    # Begin lr scheduler based arguments\n    parser.add_argument(\n        ""type"",\n        choices=OPTIONS.keys(),\n        help=""The scheduler to visualize.""\n    )\n    parser.add_argument(\n        ""--lr"",\n        type=float, default=1.0,\n        help=""The initial Learning Rate.""\n    )\n    parser.add_argument(\n        ""--warmup_steps"",\n        type=int, default=DEFAULTS[\'warmup_steps\'],\n        help=""The number of steps for a warmup. Used in {}."".format("", "".join(ARGS[\'warmup_steps\']))\n    )\n    parser.add_argument(\n        ""--max_lr"",\n        type=float, default=DEFAULTS[\'max_lr\'],\n        help=""The maximum learning rate for a cyclical one. Used in {}."".format("", "".join(ARGS[\'max_lr\']))\n    )\n    parser.add_argument(\n        ""--decay_steps"",\n        type=int, default=DEFAULTS[\'decay_steps\'],\n        help=""The number of steps to take before a decay. Used in {}."".format("", "".join(ARGS[\'decay_steps\']))\n    )\n    parser.add_argument(""--boundaries"", nargs=""+"", default=DEFAULTS[\'boundaries\'])\n    parser.add_argument(""--values"", nargs=""+"", default=DEFAULTS[\'values\'])\n    parser.add_argument(\n        ""--decay_rate"",\n        type=float, default=DEFAULTS[\'decay_rate\'],\n        help=""How much to decay. Used in {}."".format("", "".join(ARGS[\'decay_rate\']))\n    )\n    parser.add_argument(\n        ""--staircase"",\n        action=""store_true"", default=DEFAULTS[\'staircase\'],\n        help=""Should you decay in a stepwise fashion? Used in {}."".format("", "".join(ARGS[\'staircase\']))\n    )\n    parser.add_argument(\n        ""--alpha"",\n        type=float, default=DEFAULTS[\'alpha\'],\n        help=""Alpha. Used in {}."".format("", "".join(ARGS[\'alpha\']))\n    )\n    parser.add_argument(\n        ""--warm"",\n        choices=WARMUP.keys(),\n        default=DEFAULTS[\'warm\'],\n        help=""The Warmup Scheduler to use. Used in {}."".format("", "".join(ARGS[\'warm\']))\n    )\n    parser.add_argument(\n        ""--rest"",\n        choices=REST.keys(),\n        default=DEFAULTS[\'rest\'],\n        help=""The Scheduler to use after warmup. Used in {}."".format("", "".join(ARGS[\'rest\']))\n    )\n    # Begin Visualization only arguments\n    parser.add_argument(\n        ""--steps"", type=int,\n        help=""Override the number of steps to plot.""\n    )\n    parser.add_argument(\n        ""--cycles"",\n        type=int, default=6,\n        help=""Override the number of cycles to plot.""\n    )\n    parser.add_argument(\n        ""--min_lr"",\n        type=float, default=1e-3,\n        help=""When calculating the number of steps to show how small should the learning rate get before we stop.""\n    )\n    parser.add_argument(\n        ""--out_file"", default=\'.lrs/cache.index\', help=""Where to save the results for later plotting.""\n    )\n    args = parser.parse_args()\n    args = vars(args)\n\n    # Build the sub schedulers for the Composite Scheduler\n    if args[\'type\'] == \'CompositeLRScheduler\':\n        if args[\'warm\'] is None or args[\'rest\'] is None:\n            raise RuntimeError(""Warmup and Rest Scheduler are required when the Scheduler is CompositeLRScheduler"")\n        args[\'warm\'] = WARMUP[args[\'warm\']](**args)\n        args[\'rest\'] = REST[args[\'rest\']](**args)\n\n    if args[\'boundaries\'] is not None:\n        if args[\'values\'] is not None:\n            if len(args[\'boundaries\']) != len(args[\'values\']):\n                raise RuntimeError(""Bounds and Value list must be aligned"")\n\n    lr = OPTIONS[args[\'type\']](**args)\n\n    # Calculate the number of steps you should show.\n    if args[\'steps\'] is None:\n        if args[\'type\'] == \'CompositeLRScheduler\':\n            warm = get_steps(type(args[\'warm\']).__name__, args, double_warm=False)\n            rest = get_steps(type(args[\'rest\']).__name__, args)\n            args[\'steps\'] = warm + rest\n        else:\n            args[\'steps\'] = get_steps(args[\'type\'], args, double_warm=True)\n\n\n    # Plot the schedule\n    steps = np.arange(0, args[\'steps\'])\n    ys = np.stack(lr(s) for s in steps)\n    fig = plot_learning_rates([ys], [str(lr)])\n    plt.show()\n\n    # Save the lr values to a cache, this is .lrs/{hash of lr values}.npy and save\n    # information about it into the cache index\n    if args[\'out_file\'] is not None:\n        dir_ = os.path.dirname(args[\'out_file\'])\n        try: os.makedirs(dir_)\n        except: pass\n        with open(args[\'out_file\'], \'a\') as f:\n            lr_hash = sha1(ys.tostring()).hexdigest()\n            file_name = ""{}.npy"".format(lr_hash)\n            index = {\'name\': str(lr), \'file\': file_name}\n            f.write(json.dumps(index) + ""\\n"")\n            np.save(os.path.join(dir_, file_name), ys)\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/speed_tests.py,0,"b'import argparse\nfrom mead.utils import convert_path\nfrom speed_test.run import run, add\nfrom speed_test.report import report, query, explore\n\n\ndef print_help(p):\n    def ph(*args, **kwargs):\n        p.print_help()\n    return ph\n\n\ndef main():\n    parser = argparse.ArgumentParser(prog=\'Baseline Speed Test.\', description=""Run speed tests for baseline."")\n    ph = print_help(parser)\n    parser.set_defaults(func=ph)\n    subparsers = parser.add_subparsers()\n\n    run_parser = subparsers.add_parser(\'run\', description=\'Run a speed test.\')\n    run_parser.set_defaults(func=run)\n    run_parser.add_argument(\'--config\', default=\'./speed_configs\')\n    run_parser.add_argument(\'--frameworks\', default=None, nargs=\'+\')\n    run_parser.add_argument(\'--single\', action=\'store_true\')\n    run_parser.add_argument(\'--no_crf\', action=\'store_true\')\n    run_parser.add_argument(\'--no_attn\', action=\'store_true\')\n    run_parser.add_argument(\'--logging\', default=""config/logging.json"", type=convert_path)\n    run_parser.add_argument(\'--datasets\', default=""config/datasets.json"", type=convert_path)\n    run_parser.add_argument(\'--embeddings\', default=""config/embeddings.json"", type=convert_path)\n    run_parser.add_argument(\'--settings\', default=""config/mead-settings.json"", type=convert_path)\n    run_parser.add_argument(\'--db\', default=\'speed.db\')\n    run_parser.add_argument(\'--trials\', default=5, type=int)\n    run_parser.add_argument(\'--gpu\', default=0, type=int)\n    run_parser.add_argument(\'--verbose\', action=\'store_true\')\n\n    query_parser = subparsers.add_parser(\'query\', description=\'Get results about a setup.\')\n    query_parser.set_defaults(func=query)\n    query_parser.add_argument(\'--db\', default=\'speed.db\')\n    query_parser.add_argument(\'--task\', default=\'classify\')\n    query_parser.add_argument(\'--dataset\', default=\'SST2\')\n    query_parser.add_argument(\'--frameworks\', nargs=\'+\')\n    query_parser.add_argument(\'--models\', nargs=\'+\')\n    query_parser.add_argument(\'--framework_versions\', \'-fwv\', nargs=\'+\')\n    query_parser.add_argument(\'--baseline_versions\', \'-blv\', nargs=\'+\')\n\n    explore_parser = subparsers.add_parser(\'explore\', description=\'Explore the database.\')\n    explore_parser.set_defaults(func=explore)\n    explore_parser.add_argument(\'--db\', required=True)\n\n    report_parser = subparsers.add_parser(\'report\', description=\'Generate a report based on a speed test.\')\n    report_parser.set_defaults(func=report)\n    report_parser.add_argument(\'--db\', default=\'speed.db\')\n    report_parser.add_argument(\'--out\', default=\'speed_test\')\n\n    add_parser = subparsers.add_parser(\'add\', description=\'Add a run to the database\')\n    add_parser.set_defaults(func=add)\n    add_parser.add_argument(\'--config\', required=True)\n    add_parser.add_argument(\'--log\', required=True)\n    add_parser.add_argument(\'--db\', required=True)\n    add_parser.add_argument(\'--gpu\', default=0, type=int)\n\n    args = parser.parse_args()\n    args.func(args)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/test_bump.py,0,"b'import pytest\nfrom bump import bump_version\n\ndef cv(M, m, p, d=None):\n    if d is None:\n        return \'__version__ = ""{}.{}.{}""\'.format(M, m, p)\n    return \'__version__ = ""{}.{}.{}dev{}""\'.format(M, m, p, d)\n\n\ndef test_major_is_updated():\n    M = 1\n    gold_M = 2\n    v = cv(M, 12, 3)\n    m, _, _ = bump_version(v, \'major\').split(""."")\n    assert int(m) == gold_M\n\ndef test_major_resets_minor():\n    gold_m = 0\n    m = 15\n    v = cv(12, m, 3)\n    _, m, _ = bump_version(v, \'major\').split(""."")\n    assert int(m) == gold_m\n\ndef test_major_resets_patch():\n    gold_p = 0\n    p = 15\n    v = cv(12, 1, p)\n    _, _, p = bump_version(v, \'major\').split(""."")\n    assert int(p) == gold_p\n\ndef test_major_resets_dev():\n    v = cv(12, 1, 11, \'\')\n    assert \'dev\' in v\n    res = bump_version(v, \'major\').split(""."")\n    assert \'dev\' not in res\n\n    v = cv(12, 1, 15, 12)\n    assert \'dev\' in v\n    res = bump_version(v, \'major\')\n    assert \'dev\' not in res\n\n    _, _, p = res.split(\'.\')\n    assert int(p) == 0\n\ndef test_minor_is_updated():\n    m = 14\n    gold_m = 15\n    v = cv(12, m, 13)\n    _, x, _ = bump_version(v, \'minor\').split(\'.\')\n    assert int(x) == gold_m\n\ndef test_minor_ignores_major():\n    M = 12\n    gold_M = M\n    v = cv(M, 16, 12)\n    x, _, _ = bump_version(v, \'minor\').split(\'.\')\n    assert int(x) == gold_M\n\ndef test_minor_resets_patch():\n    gold_p = 0\n    v = cv(12, 15, 16)\n    _, _, x = bump_version(v, \'minor\').split(\'.\')\n    assert int(x) == gold_p\n\ndef test_minor_resets_dev():\n    v = cv(12, 1, 11, \'\')\n    assert \'dev\' in v\n    res = bump_version(v, \'minor\').split(""."")\n    assert \'dev\' not in res\n\n    v = cv(12, 1, 15, 12)\n    assert \'dev\' in v\n    res = bump_version(v, \'minor\')\n    assert \'dev\' not in res\n\n    _, _, p = res.split(\'.\')\n    assert int(p) == 0\n\ndef test_patch_is_updated():\n    p = 14\n    gold_p = 15\n    v = cv(11, 13, p)\n    _, _, x = bump_version(v, \'patch\').split(""."")\n    assert int(x) == gold_p\n\ndef test_patch_ignores_major():\n    gold_M = 1\n    v = cv(gold_M, 13, 11)\n    x, _, _ = bump_version(v, \'patch\').split(""."")\n    assert int(x) == gold_M\n\ndef test_patch_ignores_minor():\n    gold_m = 1\n    v = cv(13, gold_m, 11)\n    _, x, _ = bump_version(v, \'patch\').split(""."")\n    assert int(x) == gold_m\n\ndef test_patch_resets_dev():\n    p = 1\n    gold_p = 2\n    v = cv(12, 1, p, \'\')\n    assert \'dev\' in v\n    res = bump_version(v, \'patch\').split(""."")\n    assert \'dev\' not in res\n\n    v = cv(12, 1, p, 12)\n    assert \'dev\' in v\n    res = bump_version(v, \'patch\')\n    assert \'dev\' not in res\n\n    _, _, x = res.split(\'.\')\n    assert int(x) == gold_p\n\ndef test_dev_is_updated_from_none():\n    d = None\n    v = cv(1, 1, 1, d)\n    res = bump_version(v, \'dev\')\n    assert res.endswith(\'dev\')\n\ndef test_dev_is_updated_from_number():\n    d = 12\n    gold_d = 13\n    v = cv(1, 1, 1, d)\n    _, x = bump_version(v, \'dev\').split(\'dev\')\n    assert int(x) == gold_d\n\ndef test_dev_is_updated_from_empty():\n    d = \'\'\n    gold_d = 1\n    v = cv(1, 1, 1, d)\n    _, x = bump_version(v, \'dev\').split(\'dev\')\n    assert int(x) == gold_d\n\ndef test_dev_ignores_major():\n    gold_M = 1\n    v = cv(gold_M, 13, 11)\n    x, _, _ = bump_version(v, \'dev\').split(""."")\n    assert int(x) == gold_M\n\ndef test_dev_ignores_minor():\n    gold_m = 1\n    v = cv(1, gold_m, 13, 11)\n    _, x, _ = bump_version(v, \'dev\').split(""."")\n    assert int(x) == gold_m\n\ndef test_dev_ignores_patch():\n    gold_p = 1\n    v = cv(12, 13, gold_p, 13)\n    _, _, x = bump_version(v, \'dev\').split(""."")\n    x, _ = x.split(\'dev\')\n    assert int(x) == gold_p\n'"
tests/test_beam_pytorch.py,4,"b'import os\nfrom collections import namedtuple\nimport pytest\nimport numpy as np\nfrom mock import patch, MagicMock\n\ntorch = pytest.importorskip(""torch"")\nfrom eight_mile.utils import Offsets\nfrom eight_mile.pytorch.layers import BeamSearchBase, repeat_batch, vec_log_sum_exp\n\nB = 1\nV = 3  # It has to be at least 3 for Offsets.EOS\nK = 2\n\n\n""""""\nOriginally the probs in path one for index 1 were `[0.4, 0.3, 0.3]`\n\nThis lead to a mis-match between the tensorflow and pytorch results.\nAs you can see below when there are multiple values that are the same\nin tf then `tf.math.top_k` will select the first one of that value.\nWhen there are duplicates pytorch `.topk` will select the last one.\n\nFrom the tensorflow docs:\n    `If two elements are equal, the lower-index element appears first.`\n\nPytorch docs don\'t explain what they do.\n\nThis resulted in pytorch yielding the paths\n\n`[0, 2, 2, 2]` and `[0, 0, 0, 2]` which we wrong based on our gold\nwhich was assuming a select the first approach.\n\n(Pdb) flat_scores\n<tf.Tensor: id=161, shape=(1, 6), dtype=float32, numpy=\narray([[-1.2729657, -1.5606477, -1.5606477, -2.5257287, -2.8134108,\n        -2.8134108]], dtype=float32)>\n(Pdb) n\n> /home/blester/dev/work/baseline/python/eight_mile/tf/layers.py(1988)__call__()\n-> probs = tf.reshape(probs, (bsz, -1))\n(Pdb) best_idx\n<tf.Tensor: id=164, shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)>\n\n(Pdb) flat_scores\ntensor([[-1.2730, -1.5606, -1.5606, -2.5257, -2.8134, -2.8134]])\n(Pdb) best_idx\ntensor([[0, 2]])\n""""""\n\n\nPATH_1 = [[0.7, 0.2, 0.1], [0.4, 0.35, 0.25], [0.6, 0.3, 0.1], [0.1, 0.1, 0.9]]\nBEST_1_1ST = [0, 0, 0, 2]\nBEST_1_2ND = [0, 1, 0, 2]\n\nPATH_2 = [[0.1, 0.2, 0.7], [0.7, 0.2, 0.1], [0.7, 0.2, 0.1], [0.2, 0.2, 0.6]]\nBEST_2_1ST = [[2, 2, 2, 2]]\nBEST_2_2ND = [[1, 0, 0, 2]]\n\n\nclass MockBeamSearch(BeamSearchBase):\n    def __init__(self, path_dist):\n        super().__init__(beam=K)\n        self.path_dist = path_dist\n        self.i = 0\n\n    def init(self, encoder_outputs):\n        """"""Tile batches for encoder inputs and the likes.""""""\n\n    def step(self, paths, _):\n        probs = np.array(self.path_dist[self.i], dtype=np.float32)\n        self.i += 1\n        probs = probs.reshape((B, V))\n        single = np.log(probs)\n        single = torch.from_numpy(single)\n        return repeat_batch(single, self.K), None\n\n    def update(self, beams, _):\n        """"""Select the correct hidden states and outputs to used based on the best performing beams.""""""\n        return None\n\n\ndef test_beam_easy():\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    probs = np.zeros((B, V), dtype=np.float32)\n    probs[0, 1] = 0.33\n    probs[0, 0] = 0.33\n    probs[0, 2] = 0.33\n    encoder.output = torch.from_numpy(probs).log()\n    mbs = MockBeamSearch(PATH_1)\n    paths, lengths, probs = mbs(encoder)\n    paths = paths.squeeze()\n    assert np.allclose(paths[0].numpy(), np.array(BEST_1_1ST))\n    assert np.allclose(paths[1].numpy(), np.array(BEST_1_2ND))\n\n\ndef test_beam_lengths():\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    probs = np.zeros((B, V), dtype=np.float32)\n    probs[0, 1] = 0.33\n    probs[0, 0] = 0.33\n    probs[0, 2] = 0.33\n    encoder.output = torch.from_numpy(probs).log()\n    mbs = MockBeamSearch(PATH_2)\n    paths, lengths, probs = mbs(encoder)\n    paths = paths.squeeze()\n    assert np.allclose(paths[0].numpy(), np.array(BEST_2_1ST))\n    assert np.allclose(paths[1].numpy(), np.array(BEST_2_2ND))\n'"
tests/test_beam_tensorflow.py,0,"b'import os\nfrom collections import namedtuple\nimport pytest\nimport numpy as np\nfrom eight_mile.utils import get_version\n\ntf = pytest.importorskip(""tensorflow"")\npytestmark = pytest.mark.skipif(get_version(tf) < 2, reason=""TF1.X"")\nfrom eight_mile.tf.layers import BeamSearchBase, repeat_batch\n\nB = 1\nV = 3  # It has to be at least 3 for Offsets.EOS\nK = 2\n\n""""""\nOriginally the probs in path one for index 1 were `[0.4, 0.3, 0.3]`\n\nThis lead to a mis-match between the tensorflow and pytorch results.\nAs you can see below when there are multiple values that are the same\nin tf then `tf.math.top_k` will select the first one of that value.\nWhen there are duplicates pytorch `.topk` will select the last one.\n\nFrom the tensorflow docs:\n    `If two elements are equal, the lower-index element appears first.`\n\nPytorch docs don\'t explain what they do.\n\nThis resulted in pytorch yielding the paths\n\n`[0, 2, 2, 2]` and `[0, 0, 0, 2]` which we wrong based on our gold\nwhich was assuming a select the first approach.\n\n(Pdb) flat_scores\n<tf.Tensor: id=161, shape=(1, 6), dtype=float32, numpy=\narray([[-1.2729657, -1.5606477, -1.5606477, -2.5257287, -2.8134108,\n        -2.8134108]], dtype=float32)>\n(Pdb) n\n> /home/blester/dev/work/baseline/python/eight_mile/tf/layers.py(1988)__call__()\n-> probs = tf.reshape(probs, (bsz, -1))\n(Pdb) best_idx\n<tf.Tensor: id=164, shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)>\n\n(Pdb) flat_scores\ntensor([[-1.2730, -1.5606, -1.5606, -2.5257, -2.8134, -2.8134]])\n(Pdb) best_idx\ntensor([[0, 2]])\n""""""\n\nPATH_1 = [[0.7, 0.2, 0.1], [0.4, 0.35, 0.25], [0.6, 0.3, 0.1], [0.1, 0.1, 0.9]]\nBEST_1_1ST = [0, 0, 0, 2]\nBEST_1_2ND = [0, 1, 0, 2]\n\nPATH_2 = [[0.1, 0.2, 0.7], [0.7, 0.2, 0.1], [0.7, 0.2, 0.1], [0.2, 0.2, 0.6]]\nBEST_2_1ST = [[2, 2, 2, 2]]\nBEST_2_2ND = [[1, 0, 0, 2]]\n\n\nclass MockBeamSearch(BeamSearchBase):\n    def __init__(self, path_dist):\n        super().__init__(beam=K)\n        self.path_dist = path_dist\n        self.i = 0\n\n    def init(self, encoder_outputs):\n        """"""Tile batches for encoder inputs and the likes.""""""\n\n    def step(self, paths, _):\n        probs = np.array(self.path_dist[self.i], dtype=np.float32)\n        self.i += 1\n        probs = probs.reshape((B, V))\n        single = np.log(probs)\n        single = tf.convert_to_tensor(single)\n        return repeat_batch(single, self.K), None\n\n    def update(self, beams, _):\n        """"""Select the correct hidden states and outputs to used based on the best performing beams.""""""\n        return None\n\n\ndef test_beam_easy():\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    probs = np.zeros((B, V), dtype=np.float32)\n    probs[0, 1] = 0.33\n    probs[0, 0] = 0.33\n    probs[0, 2] = 0.33\n    encoder.output = np.log(probs)\n    mbs = MockBeamSearch(PATH_1)\n    paths, lengths, probs = mbs(encoder)\n    paths = paths.numpy().squeeze()\n    assert np.allclose(paths[0], np.array(BEST_1_1ST))\n    assert np.allclose(paths[1], np.array(BEST_1_2ND))\n\n\ndef test_beam_lengths():\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    probs = np.zeros((B, V), dtype=np.float32)\n    probs[0, 1] = 0.33\n    probs[0, 0] = 0.33\n    probs[0, 2] = 0.33\n    encoder.output = np.log(probs)\n    mbs = MockBeamSearch(PATH_2)\n    paths, lengths, probs = mbs(encoder)\n    paths = paths.numpy().squeeze()\n    assert np.allclose(paths[0], np.array(BEST_2_1ST))\n    assert np.allclose(paths[1], np.array(BEST_2_2ND))\n'"
tests/test_bleu.py,0,"b'import string\nimport random\nfrom collections import Counter\nimport pytest\nfrom mock import patch, call\nimport numpy as np\nfrom eight_mile.bleu import (\n    n_grams,\n    count_n_grams,\n    find_closest,\n    corpora_lengths,\n    max_gold_n_gram_counts,\n    count_matches,\n    count_possible,\n    geometric_mean,\n    brevity_penalty,\n    _read_references,\n    _read_lines,\n)\n\n\ndef random_str(len_=None, min_=5, max_=21):\n    if len_ is None:\n        len_ = np.random.randint(min_, max_)\n    choices = list(string.ascii_letters + string.digits)\n    return """".join([np.random.choice(choices) for _ in range(len_)])\n\n\ndef test_find_closest_above():\n    pred_len = np.random.randint(10, 20)\n    input_lens = pred_len + np.random.randint(5, 10, size=np.random.randint(2, 4))\n    gold = np.min(input_lens)\n    input_ = [[""""] * input_len for input_len in input_lens]\n    res = find_closest(pred_len, input_)\n    assert res == gold\n\n\ndef test_find_closest_below():\n    pred_len = np.random.randint(10, 20)\n    input_lens = pred_len - np.random.randint(5, 10, size=np.random.randint(2, 4))\n    gold = np.max(input_lens)\n    input_ = [[""""] * input_len for input_len in input_lens]\n    res = find_closest(pred_len, input_)\n    assert res == gold\n\n\ndef test_find_closest_tie():\n    pred_len = np.random.randint(10, 20)\n    offset = np.random.randint(5, 10)\n    above = pred_len + offset\n    below = pred_len - offset\n    input_ = [[""""] * input_len for input_len in (above, below)]\n    gold = below\n    res = find_closest(pred_len, input_)\n    assert res == gold\n\n\ndef test_corpora_lengths():\n    pred_lens = np.random.randint(2, 20, size=np.random.randint(100, 200))\n    gold_pred = np.sum(pred_lens)\n    gold_lens = np.random.randint(2, 20, size=len(pred_lens))\n    gold_gold = np.sum(gold_lens)\n    preds = [[""""] * p for p in pred_lens]\n    golds = [[[""""] * g] for g in gold_lens]\n    with patch(""eight_mile.bleu.find_closest"") as find_patch:\n        find_patch.side_effect = gold_lens\n        pred_guess, gold_guess = corpora_lengths(preds, golds)\n    assert pred_guess == gold_pred\n    assert gold_guess == gold_gold\n\n\ndef test_max_gold_counts():\n    # Create a gold that has strings and a max count for each\n    gold = Counter()\n    for _ in range(np.random.randint(5, 10)):\n        gold[random_str()] = np.random.randint(10, 20)\n    # For each word create a counter that will have the max value for it\n    # With some probability (0.5) add other words to the counter with\n    # A smaller value (than gold for that string) for the count\n    counters = []\n    for word, count in gold.items():\n        counter = Counter()\n        counter[word] = count\n        for word2, count2 in gold.items():\n            if word == word2:\n                continue\n            if np.random.rand() > 0.5:\n                counter[word2] = count2 - np.random.randint(1, count2)\n        counters.append(counter)\n    random.shuffle(counters)\n    # Have the reduce work on all of these counters.\n    with patch(""eight_mile.bleu.count_n_grams"") as count_mock:\n        count_mock.side_effect = counters\n        res = max_gold_n_gram_counts([""""] * len(gold), None)\n    assert res == gold\n\n\ndef test_max_gold_counts_calls():\n    input_ = [chr(i + 97) * l for i, l in enumerate(np.random.randint(10, 20, size=np.random.randint(5, 10)))]\n    n = np.random.randint(2, 6)\n    golds = [call(i, n) for i in input_]\n    with patch(""eight_mile.bleu.count_n_grams"") as count_mock:\n        _ = max_gold_n_gram_counts(input_, n)\n    assert count_mock.call_args_list == golds\n\n\ndef test_n_grams_int():\n    input_ = [""a"", ""b"", ""c""]\n    n = 3\n    gold = [(""a"",), (""b"",), (""c"",), (""a"", ""b""), (""b"", ""c""), (""a"", ""b"", ""c"")]\n    res = n_grams(input_, n)\n    assert list(res) == gold\n\n\ndef test_n_grams_tuple():\n    input_ = [""a"", ""b"", ""c""]\n    n = (1, 3)\n    gold = [(""a"",), (""b"",), (""c"",), (""a"", ""b"", ""c"")]\n    res = n_grams(input_, n)\n    assert list(res) == gold\n\n\ndef test_geometric_mean():\n    input_ = [0.82, 0.061, 0.22]\n    gold = 0.22242765817194177\n    res = geometric_mean(input_)\n    np.testing.assert_allclose(res, gold)\n\n\ndef test_geometric_mean_whole():\n    p = np.random.randint(1, 10, size=np.random.randint(4, 10))\n    gold = np.power(np.prod(p), 1.0 / len(p))\n    res = geometric_mean(p)\n    np.testing.assert_allclose(res, gold)\n\n\ndef test_geometric_mean_zero():\n    p = np.random.randint(1, 100, size=np.random.randint(4, 10))\n    p[np.random.randint(0, len(p))] = 0\n    res = geometric_mean(p)\n    assert res == 0.0\n\n\ndef test_geometric_mean_neg():\n    p = np.random.randint(1, 100, size=np.random.randint(4, 10))\n    p[np.random.randint(0, len(p))] = np.random.randint(3, 30) * -1\n    res = geometric_mean(p)\n    assert res == 0.0\n\n\ndef test_brevity_penalty_not_applied():\n    def test():\n        p = np.random.randint(100, 1000)\n        g = p - np.random.randint(1, p)\n        assert p > g, ""Your test is wrong""\n        res, _ = brevity_penalty(p, g)\n        assert res == 1.0\n\n    for _ in range(100):\n        test()\n\n\ndef test_brevity_penalty_value():\n    p = np.random.randint(100, 1000)\n    g = p + np.random.randint(100, 200)\n    gold = np.exp(1 - (g / p))\n    res, _ = brevity_penalty(p, g)\n    np.testing.assert_allclose(res, gold)\n\n\ndef test_brevity_penalty_ratio():\n    p = np.random.randint(100, 1000)\n    g = np.random.randint(100, 1000)\n    gold = p / float(g)\n    _, res = brevity_penalty(p, g)\n    assert res == gold\n\n\ndef test_count_matches():\n    a = Counter()\n    b = Counter()\n    total = 0\n    upper = np.random.randint(3, 6)\n    gold = np.zeros(upper)\n    for i in range(np.random.randint(10, 20)):\n        key = tuple([i] * np.random.randint(1, upper))\n        if np.random.rand() > 0.5:\n            a_v = np.random.randint(5, 10)\n            b_v = np.random.randint(5, 10)\n            a[key] = a_v\n            b[key] = b_v\n            gold[len(key) - 1] += min(a_v, b_v)\n        else:\n            c = a if np.random.rand() > 0.5 else b\n            c[key] = np.random.randint(5, 10)\n    res = count_matches(a, b, np.zeros(upper))\n    np.testing.assert_equal(res, gold)\n\n\ndef test_count_possible():\n    input_ = [""a"", ""b"", ""c""]\n    n = 3\n    gold = [3, 2, 1]\n    res = count_possible(input_, np.zeros(n))\n    np.testing.assert_equal(res, np.array(gold))\n\n\ndef test_read_lines():\n    gold = [[random_str() for _ in range(np.random.randint(1, 100))] for _ in range(np.random.randint(25, 40))]\n    input_ = ["" "".join(g) for g in gold]\n    res = _read_lines(input_, False)\n    assert res == gold\n\n\ndef test_read_lines_lowercase():\n    gold = [[random_str() for _ in range(np.random.randint(1, 100))] for _ in range(np.random.randint(25, 40))]\n    input_ = ["" "".join(g) for g in gold]\n    gold = [[x.lower() for x in g] for g in gold]\n    res = _read_lines(input_, True)\n    assert res == gold\n\n\ndef test_read_references_group_references():\n    input1 = [\n        "" "".join([random_str() for _ in range(np.random.randint(1, 100))]) for _ in range(np.random.randint(25, 40))\n    ]\n    input2 = ["" "".join([random_str() for _ in range(np.random.randint(1, 100))]) for _ in range(len(input1))]\n    gold = [(input1[i], input2[i]) for i in range(len(input1))]\n    with patch(""eight_mile.bleu._read_lines"") as read_patch:\n        with patch(""eight_mile.bleu.open""):\n            read_patch.side_effect = (input1, input2)\n            res = _read_references(["""", """"], False)\n            assert res == gold\n'"
tests/test_calc_feats.py,0,"b'import random\nimport pytest\nimport numpy as np\nfrom eight_mile.utils import calc_nfeats\n\n\ndef test_use_nfeats():\n    filtsz = [random.randint(1, 10) for _ in range(random.randint(2, 6))]\n    input_nfeat = random.randint(1, 100)\n    gold_nfeats = [input_nfeat] * len(filtsz)\n    _, nfeat = calc_nfeats(filtsz, None, None, nfeats=input_nfeat)\n    assert nfeat == gold_nfeats\n\n\ndef test_use_nfeats_filtsz_unchanged():\n    gold_filtsz = [random.randint(1, 10) for _ in range(random.randint(2, 6))]\n    nfeat = random.randint(1, 100)\n    filtsz, _ = calc_nfeats(gold_filtsz, None, None, nfeats=nfeat)\n    assert filtsz == gold_filtsz\n\n\ndef test_use_nfeats_none():\n    filtsz = [random.randint(1, 10) for _ in range(random.randint(2, 6))]\n    with pytest.raises(AssertionError):\n        calc_nfeats(filtsz)\n\n\ndef test_use_nfeats_list():\n    filtsz = [random.randint(1, 10) for _ in range(random.randint(2, 6))]\n    nfeats = [random.randint(1, 10) for _ in range(len(filtsz))]\n    with pytest.raises(AssertionError):\n        _, nfeat = calc_nfeats(filtsz, None, None, nfeats=nfeats)\n\n\ndef test_extract_tuple():\n    filt_feat = [(random.randint(1, 10), random.randint(10, 20)) for _ in range(random.randint(2, 6))]\n    gold_filtsz = tuple(filter_and_size[0] for filter_and_size in filt_feat)\n    gold_nfeats = tuple(filter_and_size[1] for filter_and_size in filt_feat)\n    filtsz, nfeats = calc_nfeats(filt_feat)\n    assert filtsz == gold_filtsz\n    assert nfeats == gold_nfeats\n\n\ndef test_feat_factor_manual():\n    gold_filtsz = [1, 2, 3, 4, 5]\n    feat_factor = 10\n    gold_nfeats = [10, 20, 30, 40, 50]\n    filtsz, nfeats = calc_nfeats(gold_filtsz, feat_factor, float(""Infinity""))\n    assert filtsz == gold_filtsz\n    assert nfeats == gold_nfeats\n\n\ndef test_feat_factor_capped():\n    gold_filtsz = [1, 2, 3, 4, 5]\n    feat_factor = 10\n    gold_nfeats = [10, 20, 30, 30, 30]\n    filtsz, nfeats = calc_nfeats(gold_filtsz, feat_factor, 30)\n    assert filtsz == gold_filtsz\n    assert nfeats == gold_nfeats\n\n\ndef test_feat_factor():\n    gold_filtsz = [random.randint(1, 10) for _ in range(random.randint(2, 6))]\n    feat_factor = random.randint(10, 25)\n    max_feat = random.randint(30, 40)\n    gold_nfeats = np.minimum(np.array(gold_filtsz) * feat_factor, max_feat)\n    filtsz, nfeats = calc_nfeats(gold_filtsz, feat_factor, max_feat)\n    np.testing.assert_equal(filtsz, gold_filtsz)\n    np.testing.assert_equal(nfeats, gold_nfeats)\n\n\ndef test_feat_factor_max_none():\n    filtsz = [random.randint(1, 10) for _ in range(random.randint(2, 6))]\n    feat_factor = 10\n    with pytest.raises(AssertionError):\n        calc_nfeats(filtsz, nfeat_factor=feat_factor, max_feat=None)\n'"
tests/test_cm.py,0,"b'import math\nfrom itertools import chain\nimport pytest\nimport numpy as np\nfrom eight_mile.confusion import ConfusionMatrix\n\n\nY_TRUE = [2, 0, 2, 2, 0, 1, 3, 1, 3, 3, 3, 3, 4]\nY_PRED = [0, 0, 2, 2, 0, 2, 3, 3, 3, 1, 3, 2, 4]\nLABELS = [""0"", ""1"", ""2"", ""3"", ""4""]\n\n\nCLASS_PREC = [0.666667, 0.0, 0.5, 0.75, 1.0]\nCLASS_RECALL = [1.0, 0.0, 0.666667, 0.6, 1.0]\nCLASS_F1 = [0.8, 0.0, 0.571429, 0.666667, 1.0]\nCLASS_SUPPORT = [2, 2, 3, 5, 1]\nCLASS_MCC = 0.5\nTOL = 1e-6\n\n\ndef make_mc_cm():\n    cm = ConfusionMatrix(LABELS)\n    for y_t, y_p in zip(Y_TRUE, Y_PRED):\n        cm.add(y_t, y_p)\n    return cm\n\n\ndef test_create_cm():\n    gold = make_mc_cm()\n    cm = ConfusionMatrix.create(Y_TRUE, Y_PRED)\n    np.testing.assert_equal(gold._cm, cm._cm)\n\n\ndef test_mc_support():\n    cm = make_mc_cm()\n    support = cm.get_support()\n    np.testing.assert_allclose(support, CLASS_SUPPORT, TOL)\n\n\ndef test_mc_precision():\n    cm = make_mc_cm()\n    prec = cm.get_precision()\n    np.testing.assert_allclose(prec, CLASS_PREC, TOL)\n    wp = cm.get_weighted_precision()\n    np.testing.assert_allclose(wp, 0.5833333, TOL)\n    mp = cm.get_mean_precision()\n    np.testing.assert_allclose(mp, 0.5833333, TOL)\n\n\ndef test_mc_recall():\n    cm = make_mc_cm()\n    recall = cm.get_recall()\n    np.testing.assert_allclose(recall, CLASS_RECALL, TOL)\n    wr = cm.get_weighted_recall()\n    np.testing.assert_allclose(wr, 0.6153846, TOL)\n    mr = cm.get_mean_recall()\n    np.testing.assert_allclose(mr, 0.65333333, TOL)\n\n\ndef test_mc_f1():\n    cm = make_mc_cm()\n    f1 = cm.get_class_f()\n    np.testing.assert_allclose(f1, CLASS_F1, TOL)\n    wf1 = cm.get_weighted_f()\n    np.testing.assert_allclose(wf1, 0.5882784, TOL)\n\n\ndef test_mcc_example():\n    Y_TRUE = [1, 1, 1, 0]\n    Y_PRED = [1, 0, 1, 1]\n    MCC_GOLD = -0.3333333333333333\n    cm = ConfusionMatrix.create(Y_TRUE, Y_PRED)\n    np.testing.assert_allclose(cm.get_mcc(), MCC_GOLD, TOL)\n\n\ndef test_binary_mcc():\n    Y_TRUE = [1, 1, 1, 0]\n    Y_PRED = [1, 0, 1, 1]\n    MCC_GOLD = -0.3333333333333333\n    cm = ConfusionMatrix([0, 1])\n    cm.add_batch(Y_TRUE, Y_PRED)\n    np.testing.assert_allclose(cm.get_mcc(), MCC_GOLD, TOL)\n\n\ndef test_mc_mcc():\n    cm = make_mc_cm()\n    np.testing.assert_allclose(cm.get_r_k(), CLASS_MCC)\n\n\ndef explicit_mcc(golds, preds):\n    """"""This is the way to calculate MCC directly from a confusion matrix as described here:\n        ""https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\n    """"""\n    TP, TN, FP, FN = 0, 0, 0, 0\n    for g, p in zip(golds, preds):\n        if g == p and g == 0:\n            TP += 1\n        elif g == p and p == 1:\n            TN += 1\n        elif g != p and p == 0:\n            FP += 1\n        elif g != p and p == 1:\n            FN += 1\n    num = TP * TN - FP * FN\n    denom = float(math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n    denom = denom if denom != 0.0 else 1.0\n    return num / denom\n\n\ndef author_mcc(cm):\n    """"""Calculate Matthews Correlation Coefficient as described in this original paper\n        https://www.ncbi.nlm.nih.gov/pubmed/1180967\n    """"""\n    N = np.sum(cm, dtype=np.float32)\n    S = np.sum(cm, axis=0, dtype=np.float64)[0] / N\n    P = np.sum(cm, axis=1, dtype=np.float64)[0] / N\n    TP = cm[0, 0]\n    num = TP / N - S * P\n    denom = np.sqrt(P * S * (1 - P) * (1 - S))\n    denom = denom if denom != 0.0 else 1.0\n    return num / denom\n\n\ndef test_mcc():\n    def test():\n        golds = np.concatenate((np.arange(2), np.random.randint(0, 2, size=np.random.randint(4, 100))), axis=0)\n        preds = np.random.randint(0, 2, size=len(golds))\n        cm = ConfusionMatrix.create(golds, preds)\n        np.testing.assert_allclose(cm.get_mcc(), explicit_mcc(golds, preds), TOL)\n        np.testing.assert_allclose(cm.get_mcc(), author_mcc(cm._cm), TOL, TOL)\n\n    for _ in range(100):\n        test()\n\n\ndef test_mcc_perfect():\n    golds = np.concatenate((np.arange(2), np.random.randint(0, 2, size=np.random.randint(4, 100))), axis=0)\n    preds = np.copy(golds)\n    cm = ConfusionMatrix.create(golds, preds)\n    np.testing.assert_allclose(cm.get_mcc(), 1.0, TOL)\n\n\ndef test_mcc_inverse():\n    golds = np.concatenate((np.arange(2), np.random.randint(0, 2, size=np.random.randint(4, 100))), axis=0)\n    preds = 1 - golds\n    cm = ConfusionMatrix.create(golds, preds)\n    np.testing.assert_allclose(cm.get_mcc(), -1.0, TOL)\n\n\ndef explicit_r_k(cm):\n    """"""This is the way to calculate MCC directly from a confusion matrix as described here:\n        ""https://en.wikipedia.org/wiki/Matthews_correlation_coefficient#Multiclass_case\n    """"""\n    num = 0.0\n    for k in range(len(cm)):\n        for l in range(len(cm)):\n            for m in range(len(cm)):\n                num += cm[k, k] * cm[l, m] - cm[k, l] * cm[m, k]\n    denom1 = 0\n    for k in range(len(cm)):\n        k_l = 0\n        for l in range(len(cm)):\n            k_l += cm[k, l]\n        not_k_l = 0\n        for k2 in range(len(cm)):\n            if k == k2:\n                continue\n            for l2 in range(len(cm)):\n                not_k_l += cm[k2, l2]\n        denom1 += k_l * not_k_l\n    denom2 = 0\n    for k in range(len(cm)):\n        k_l = 0\n        for l in range(len(cm)):\n            k_l += cm[l, k]\n        not_k_l = 0\n        for k2 in range(len(cm)):\n            if k == k2:\n                continue\n            for l2 in range(len(cm)):\n                not_k_l += cm[l2, k2]\n        denom2 += k_l * not_k_l\n    denom = np.sqrt(denom1) * np.sqrt(denom2)\n    denom = denom if denom != 0.0 else 1\n    return num / denom\n\n\ndef test_r_k():\n    def test():\n        C = np.random.randint(3, 11)\n        golds = np.concatenate((np.arange(C), np.random.randint(0, C, size=np.random.randint(4, 100))), axis=0)\n        preds = np.random.randint(0, C, size=len(golds))\n        cm = ConfusionMatrix.create(golds, preds)\n        np.testing.assert_allclose(cm.get_r_k(), explicit_r_k(cm._cm), TOL)\n\n    for _ in range(100):\n        test()\n\n\ndef test_r_k_perfect():\n    """"""Perfect correlation results in a score of 1.""""""\n    C = np.random.randint(2, 11)\n    golds = np.concatenate((np.arange(C), np.random.randint(0, C, size=np.random.randint(4, 100))), axis=0)\n    preds = np.copy(golds)\n    cm = ConfusionMatrix.create(golds, preds)\n    np.testing.assert_allclose(cm.get_r_k(), 1.0)\n\n\ndef test_r_k_inverse():\n    """"""The worst value for R k ranges from -1 to 0 depending on the distribution of the true labels.""""""\n    C = np.random.randint(2, 11)\n    golds = np.concatenate((np.arange(C), np.random.randint(0, C, size=np.random.randint(4, 100))), axis=0)\n    preds = golds + 1 % C\n    cm = ConfusionMatrix.create(golds, preds)\n    assert -1.0 <= cm.get_r_k() <= 0.0\n'"
tests/test_conll.py,0,"b'import os\nimport random\nimport textwrap\nfrom io import StringIO\nimport pytest\nfrom eight_mile.utils import (\n    sniff_conll_file,\n    read_conll,\n    read_conll_docs,\n    read_conll_docs_md,\n    read_conll_sentences,\n    read_conll_sentences_md,\n    convert_conll_file,\n    write_conll,\n)\n\n\nfile_loc = os.path.realpath(os.path.dirname(__file__))\nTEST_FILE = os.path.join(file_loc, ""test_data"", ""test.conll"")\n\ngold_sentences = [\n    [[""a"", ""1"", ""2""], [""b"", ""3"", ""4""], [""c"", ""5"", ""6""]],\n    [[""d"", ""7"", ""8""], [""e"", ""9"", ""10""]],\n    [[""g"", ""11"", ""12""], [""h"", ""13"", ""14""], [""#"", ""ex"", ""44""], [""i"", ""15"", ""16""], [""j"", ""17"", ""18""]],\n    [[""k"", ""19"", ""20""]],\n    [[""l"", ""21"", ""22""], [""m"", ""23"", ""24""], [""n"", ""25"", ""26""]],\n    [[""o"", ""27"", ""28""], [""p"", ""29"", ""30""]],\n]\n\ngold_documents = [\n    [gold_sentences[0], gold_sentences[1]],\n    [gold_sentences[2], gold_sentences[3]],\n    [gold_sentences[4], gold_sentences[5]],\n]\n\ncomments = [\n    ""# begin doc"",\n    ""# This is a comment"",\n    ""# This is the second sentence"",\n    ""# This is an extra comment"",\n    ""# end doc"",\n    ""# begin doc"",\n    ""# 2 2"",\n    ""# begin doc"",\n]\n\ngold_documents_with_comments = [\n    [\n        [comments[1].split()] + gold_sentences[0],\n        [comments[2].split(), comments[3].split()] + gold_sentences[1],\n        [comments[4].split()],\n    ],\n    [gold_sentences[2], [comments[6].split()] + gold_sentences[3]],\n    [gold_sentences[4], gold_sentences[5]],\n]\n\n\nsentence_comments = [\n    [comments[0], comments[1]],\n    [comments[2], comments[3]],\n    [comments[4], comments[5]],\n    [comments[6]],\n    [comments[7]],\n    [],\n]\n\ndoc_comments = [[comments[0]], [comments[4], comments[5]], [comments[7]]]\n\ndoc_sent_comments = [[[comments[1]], [comments[2], comments[3]]], [[], [comments[6]]], [[], []]]\n\n\ndef test_read_conll_sentences():\n    for p, g in zip(read_conll_sentences(TEST_FILE), gold_sentences):\n        assert p == g\n\n\ndef test_read_conll_sentences_hash_token():\n    text_tokens = [[""# This is actually a comment""], [""a"", ""1"", ""2""], [""#"", ""3"", ""4""], [""c"", ""5"", ""6""]]\n    gold = [text_tokens[1:]]\n    text = StringIO(""\\n"".join("" "".join([t for t in tt]) for tt in text_tokens))\n    for p, g in zip(read_conll_sentences(text), gold):\n        assert p == g\n\n\ndef test_read_conll_sentences_no_comments():\n    text_tokens = [[""#"", ""1"", ""2""], [""b"", ""3"", ""4""], [""c"", ""5"", ""6""]]\n    gold = [text_tokens]\n    text = StringIO(""\\n"".join("" "".join([t for t in tt]) for tt in text_tokens))\n    for p, g in zip(read_conll_sentences(text, allow_comments=False), gold):\n        assert p == g\n\n\ndef test_read_conll_sentences_diff_comment_string():\n    text_tokens = [[""#"", ""1"", ""2""], [""b"", ""3"", ""4""], [""c"", ""5"", ""6""]]\n    gold = [text_tokens]\n    text = StringIO(""\\n"".join("" "".join([t for t in tt]) for tt in text_tokens))\n    for p, g in zip(read_conll_sentences(text, comment_pattern=""# comment: ""), gold):\n        assert p == g\n\n\ndef test_read_conll_sentences_md():\n    for (p, pm), g, m in zip(read_conll_sentences_md(TEST_FILE), gold_sentences, sentence_comments):\n        assert p == g\n        assert pm == m\n\n\ndef test_read_conll_sentences_md_hash_token():\n    text_tokens = [[""# This is actually a comment""], [""a"", ""1"", ""2""], [""#"", ""3"", ""4""], [""c"", ""5"", ""6""]]\n    gold = [(text_tokens[1:], [text_tokens[0]])]\n    text = StringIO(""\\n"".join("" "".join([t for t in tt]) for tt in text_tokens))\n    for (p, pm), (g, gm) in zip(read_conll_sentences_md(text), gold):\n        assert p == g\n\n\ndef test_read_conll_sentences_md_hash_token():\n    text_tokens = [\n        [""# comment: This is actually a comment""],\n        [""#"", ""-1"", ""0""],\n        [""a"", ""1"", ""2""],\n        [""#"", ""3"", ""4""],\n        [""c"", ""5"", ""6""],\n    ]\n    gold = [(text_tokens[1:], [text_tokens[0]])]\n    text = StringIO(""\\n"".join("" "".join([t for t in tt]) for tt in text_tokens))\n    for (p, pm), (g, gm) in zip(read_conll_sentences_md(text, comment_pattern=""# comment: ""), gold):\n        assert p == g\n\n\ndef test_read_conll_metadata_comments_conflict():\n    with pytest.raises(ValueError):\n        next(read_conll(StringIO(""a""), metadata=True, allow_comments=False))\n\n\ndef test_read_conll_docs():\n    for d, g in zip(read_conll_docs(TEST_FILE), gold_documents):\n        assert d == g\n\n\ndef test_read_conll_docs_ignore_comments():\n    for d, g in zip(read_conll_docs(TEST_FILE, allow_comments=False), gold_documents_with_comments):\n        assert d == g\n\n\ndef test_read_conll_docs_md():\n    for (d, dm, dsm), g, gd, gsm in zip(read_conll_docs_md(TEST_FILE), gold_documents, doc_comments, doc_sent_comments):\n        assert d == g\n        assert dm == gd\n        assert dsm == gsm\n\n\ndef test_sniff_conll():\n    files = StringIO()\n    for _ in range(random.randint(0, 4)):\n        files.write(""#{}\\n"".format(random.random()))\n    gold = random.randint(2, 12)\n    files.write("" "".join([""a""] * gold) + ""\\n"")\n    files.seek(0)\n    res = sniff_conll_file(files)\n    assert res == gold\n\n\ndef test_sniff_reset():\n    files = StringIO()\n    files.write(""We shouldn\'t see this\\n"")\n    start = files.tell()\n    gold = ""#This is the gold!""\n    files.write(gold + ""\\n"")\n    for _ in range(random.randint(0, 3)):\n        files.write(""#\\n"")\n    files.write(""a a\\n"")\n    files.seek(start)\n    _ = sniff_conll_file(files)\n    res = files.readline().rstrip()\n    assert res == gold\n\n\ndef test_read_write_with_delim_none():\n    data = textwrap.dedent(""""""\n        a 1 2\n        b 1 2\n\n        c 1 2\n        d 1 2\n    """""".lstrip(""\\n""))\n    in_ = StringIO(data)\n    out_ = StringIO()\n    convert_conll_file(in_, out_, lambda x: x, delim=None)\n    out_.seek(0)\n    assert out_.read().strip(""\\n"") == data.strip(""\\n"")\n\n\ndef test_write_conll():\n    data = read_conll(TEST_FILE)\n    out_ = StringIO()\n    write_conll(out_, data, delim="" "")\n    out_.seek(0)\n    with open(TEST_FILE) as gold:\n        assert out_.read().strip(""\\n"") == gold.read().strip(""\\n"")\n\n\ndef test_write_conll_with_delims():\n    delim = random.choice([""~~"", ""|"", ""\\t""])\n    data = read_conll(TEST_FILE)\n    out_ = StringIO()\n    write_conll(out_, data, delim=delim)\n    out_.seek(0)\n    with open(TEST_FILE) as gold:\n        assert out_.read().strip(""\\n"") == gold.read().strip(""\\n"").replace("" "", delim)\n'"
tests/test_crf_pytorch.py,60,"b'import os\nimport math\nimport json\nimport pytest\nimport numpy as np\nfrom mock import patch, MagicMock\n\ntorch = pytest.importorskip(""torch"")\nfrom eight_mile.utils import Offsets\nfrom eight_mile.pytorch.layers import (\n    CRF,\n    Viterbi,\n    ViterbiLogSoftmaxNorm,\n    transition_mask,\n    script_viterbi,\n    ViterbiBatchSize1,\n    vec_log_sum_exp,\n)\n\n\ndef explicit_log_sum_exp(xs):\n    """"""Log Sum Exp on a dict of values.""""""\n    max_x = max(xs.values())\n    total = 0\n    for x in xs.values():\n        total += math.exp(x - max_x)\n    return max_x + math.log(total)\n\n\ndef explicit_score_gold(emiss, trans, golds, start, end):\n    score = 0\n    for e, g in zip(emiss, golds):\n        score += e[g]\n    for i in range(len(golds)):\n        from_ = start if i == 0 else golds[i - 1]\n        to = golds[i]\n        score += trans[(from_, to)]\n    score += trans[(golds[-1], end)]\n    return score\n\n\ndef explicit_forward(emiss, trans, start, end):\n    """"""Best path through a lattice on the log semiring with explicit looping.""""""\n    trellis = dict.fromkeys(emiss[0].keys(), -1e4)\n    trellis[start] = 0\n\n    for e in emiss:\n        new_trellis = {}\n        for next_state in trellis:\n            score = {}\n            for prev_state in trellis:\n                score[prev_state] = trellis[prev_state] + e[next_state] + trans[(prev_state, next_state)]\n            new_trellis[next_state] = explicit_log_sum_exp(score)\n        trellis = new_trellis\n    for state in trellis:\n        trellis[state] += trans[(state, end)]\n    return explicit_log_sum_exp(trellis)\n\n\ndef explicit_nll(emiss, trans, golds, start, end):\n    f = explicit_forward(emiss, trans, start, end)\n    g = explicit_score_gold(emiss, trans, golds, start, end)\n    return f - g\n\n\ndef explicit_viterbi(emiss, trans, start, end):\n    """"""Best path through a lattice on the viterbi semiring with explicit looping.""""""\n    backpointers = []\n    trellis = dict.fromkeys(emiss[0].keys(), -1e4)\n    trellis[start] = 0\n\n    for e in emiss:\n        new_trellis = {}\n        backpointer = {}\n        for next_state in trellis:\n            score = {}\n            for prev_state in trellis:\n                score[prev_state] = trellis[prev_state] + e[next_state] + trans[(prev_state, next_state)]\n            new_trellis[next_state] = max(score.values())\n            backpointer[next_state] = max(score, key=lambda x: score[x])  # argmax\n        trellis = new_trellis\n        backpointers.append(backpointer)\n    for state in trellis:\n        trellis[state] += trans[(state, end)]\n    score = max(trellis.values())\n    state = max(trellis, key=lambda x: trellis[x])\n    states = [state]\n    for t in reversed(range(0, len(emiss))):\n        states.append(backpointers[t][states[-1]])\n    return list(reversed(states[:-1])), score\n\n\ndef build_trans(t):\n    """"""Convert the transition tensor to a dict.\n\n    :param t: `torch.FloatTensor` [H, H]: transition scores in the\n        form [to, from]\n\n    :returns: `dict` transition scores in the form [from, to]\n    """"""\n    trans = {}\n    for i in range(t.size(0)):\n        for j in range(t.size(1)):\n            trans[(i, j)] = t[j, i].item()\n    return trans\n\n\ndef build_emission(emission):\n    """"""Convert the emission scores into a list of dicts\n\n    :param emission: `torch.FloatTensor` [T, H]: emission scores\n\n    :returns: `List[dict]`\n    """"""\n    es = []\n    for emiss in emission:\n        e_ = {}\n        for i in range(emiss.size(0)):\n            e_[i] = emiss[i].item()\n        es.append(e_)\n    return es\n\n\n@pytest.fixture\ndef generate_batch():\n    """"""Generate a batch of data.\n\n    Creates lengths such that at least one half of the batch is the maximum\n    length.\n\n    :returns: unary [B, T, H], tags [T, B], lengths [B]\n    """"""\n    B = np.random.randint(5, 11)\n    T = np.random.randint(15, 21)\n    H = np.random.randint(22, 41)\n    scores = torch.rand(B, T, H)\n    tags = torch.randint(1, H, (B, T))\n    lengths = torch.randint(1, T, (B,))\n    lengths[torch.randint(0, B, (B // 2,))] = T\n    for s, l in zip(scores, lengths):\n        s[l:] = 0\n    return scores.transpose(0, 1), tags.transpose(0, 1), lengths\n\n\n@pytest.fixture\ndef generate_examples_and_batch():\n    """"""A good test for these systems are do they produce the same results for a\n    batch of data as when you feed the example in one by one.\n\n    This function generates two single examples and then batches them together.\n    """"""\n    T = np.random.randint(15, 21)\n    H = np.random.randint(22, 41)\n    diff = np.random.randint(1, T // 2)\n\n    item1 = torch.rand(T, 1, H)\n    tags1 = torch.randint(1, H, (T, 1))\n    lengths1 = torch.tensor([T])\n\n    item2 = torch.rand(T - diff, 1, H)\n    tags2 = torch.randint(1, H, (T - diff, 1))\n    lengths2 = torch.tensor([T - diff])\n\n    packed_input = torch.zeros(T, 2, H)\n    packed_tags = torch.zeros(T, 2, dtype=torch.long)\n    packed_input[:, 0, :] = item1.squeeze(1)\n    packed_input[: T - diff, 1, :] = item2.squeeze(1)\n    packed_tags[:, 0] = tags1.squeeze(1)\n    packed_tags[: T - diff, 1] = tags2.squeeze(1)\n    lengths = torch.cat([lengths1, lengths2], dim=0)\n    return item1, tags1, lengths1, item2, tags2, lengths2, packed_input, packed_tags, lengths\n\n\ndef test_score_sentence(generate_batch):\n    unary, tags, lengths = generate_batch\n    h = unary.size(2)\n    crf = CRF(h, batch_first=False)\n    trans = torch.rand(h, h)\n    crf.transitions_p.data = trans.unsqueeze(0)\n    sentence_score = crf.score_sentence(unary, tags, lengths)\n\n    new_trans = build_trans(trans)\n    unary = unary.transpose(0, 1)\n    tags = tags.transpose(0, 1)\n    scores = []\n    for u, t, l in zip(unary, tags, lengths):\n        emiss = build_emission(u[:l])\n        golds = t[:l].tolist()\n        scores.append(explicit_score_gold(emiss, new_trans, golds, Offsets.GO, Offsets.EOS))\n    gold_scores = np.array(scores)\n    np.testing.assert_allclose(sentence_score.detach().numpy(), gold_scores, rtol=1e-6)\n\n\ndef test_score_sentence_batch_stable(generate_examples_and_batch):\n    i1, t1, l1, i2, t2, l2, i, t, l = generate_examples_and_batch\n    h = i1.size(2)\n    crf = CRF(h, batch_first=False)\n    crf.transitions_p.data = torch.rand(1, h, h)\n    score1 = crf.score_sentence(i1, t1, l1)\n    score2 = crf.score_sentence(i2, t2, l2)\n    one_x_one = torch.cat([score1, score2], dim=0)\n    batched = crf.score_sentence(i, t, l)\n    np.testing.assert_allclose(one_x_one.detach().numpy(), batched.detach().numpy())\n\n\ndef test_score_sentence_shape(generate_batch):\n    unary, tags, lengths = generate_batch\n    h = unary.size(2)\n    crf = CRF(h, batch_first=False)\n    score = crf.score_sentence(unary, tags, lengths)\n    assert score.shape == torch.Size([unary.size(1)])\n\n\ndef test_neg_log_loss(generate_batch):\n   unary, tags, lengths = generate_batch\n   h = unary.size(2)\n   crf = CRF(h, batch_first=False)\n   trans = torch.rand(h, h)\n   crf.transitions_p.data = trans.unsqueeze(0)\n   nll = crf.neg_log_loss(unary, tags, lengths)\n\n   new_trans = build_trans(trans)\n   unary = unary.transpose(0, 1)\n   tags = tags.transpose(0, 1)\n   scores = []\n   for u, t, l in zip(unary, tags, lengths):\n       emiss = build_emission(u[:l])\n       golds = t[:l].tolist()\n       scores.append(explicit_nll(emiss, new_trans, golds, Offsets.GO, Offsets.EOS))\n   gold_scores = np.mean(np.array(scores))\n   np.testing.assert_allclose(nll.detach().numpy(), gold_scores, rtol=1e-6)\n\n\ndef test_neg_log_loss_batch_stable(generate_examples_and_batch):\n   i1, t1, l1, i2, t2, l2, i, t, l = generate_examples_and_batch\n   h = i1.size(2)\n   crf = CRF(h, batch_first=False)\n   crf.transitions_p.data = torch.rand(1, h, h)\n   nll1 = crf.neg_log_loss(i1, t1, l1)\n   nll2 = crf.neg_log_loss(i2, t2, l2)\n   one_x_one = (nll1 + nll2) / 2\n   batched = crf.neg_log_loss(i, t, l)\n   np.testing.assert_allclose(one_x_one.detach().numpy(), batched.detach().numpy())\n\n\ndef test_forward(generate_batch):\n    unary, _, lengths = generate_batch\n    h = unary.size(2)\n    crf = CRF(h, batch_first=False)\n    trans = torch.rand(h, h)\n    crf.transitions_p.data = trans.unsqueeze(0)\n    forward = crf.forward((unary, lengths))\n\n    new_trans = build_trans(trans)\n    unary = unary.transpose(0, 1)\n    scores = []\n    for u, l in zip(unary, lengths):\n        emiss = build_emission(u[:l])\n        scores.append(explicit_forward(emiss, new_trans, Offsets.GO, Offsets.EOS))\n    gold_scores = np.array(scores)\n    np.testing.assert_allclose(forward.detach().numpy(), gold_scores, rtol=1e-6)\n\n\ndef test_forward_batch_stable(generate_examples_and_batch):\n    i1, _, l1, i2, _, l2, i, _, l = generate_examples_and_batch\n    h = i1.size(2)\n    crf = CRF(h, batch_first=False)\n    crf.transitions_p.data = torch.rand(1, h, h)\n    fw1 = crf.forward((i1, l1))\n    fw2 = crf.forward((i2, l2))\n    one_x_one = torch.cat([fw1, fw2], dim=0)\n    batched = crf.forward((i, l))\n    np.testing.assert_allclose(one_x_one.detach().numpy(), batched.detach().numpy())\n\n\ndef test_forward_shape(generate_batch):\n    unary, _, lengths = generate_batch\n    h = unary.size(2)\n    crf = CRF(h, batch_first=False)\n    fwd = crf.forward((unary, lengths))\n    assert fwd.shape == torch.Size([unary.size(1)])\n\n\ndef test_decode_batch_stable(generate_examples_and_batch):\n   i1, _, l1, i2, _, l2, i, _, l = generate_examples_and_batch\n   h = i1.size(2)\n   crf = CRF(h, batch_first=False)\n   crf.transitions_p.data = torch.rand(1, h, h)\n   p1, s1 = crf.decode(i1, l1)\n   p2, s2 = crf.decode(i2, l2)\n   pad = torch.zeros(p1.size(0) - p2.size(0), 1, dtype=torch.long)\n   one_x_one_p = torch.cat([p1, torch.cat([p2, pad], dim=0)], dim=1)\n   one_x_one_s = torch.cat([s1, s2], dim=0)\n   batched_p, batched_s =  crf.decode(i, l)\n   np.testing.assert_allclose(one_x_one_s.detach().numpy(), batched_s.detach().numpy())\n   for p1, p2 in zip(one_x_one_p, batched_p):\n       np.testing.assert_allclose(p1.detach().numpy(), p2.detach().numpy())\n\n\ndef test_decode_shape_crf(generate_batch):\n    unary, _, lengths = generate_batch\n    h = unary.size(2)\n    crf = CRF(h, batch_first=False)\n    paths, scores = crf.decode(unary, lengths)\n    assert scores.shape == torch.Size([unary.size(1)])\n    assert paths.shape == torch.Size([unary.size(0), unary.size(1)])\n\n\ndef test_mask_is_applied():\n    h = np.random.randint(22, 41)\n    loc = np.random.randint(h)\n    constraint = torch.zeros(h, h, dtype=torch.uint8)\n    constraint[Offsets.GO, loc] = 1\n    crf = CRF(h, constraint_mask=constraint)\n    t = crf.transitions.detach().numpy()\n    assert t[0, Offsets.GO, loc] == -1e4\n\n\ndef test_mask_not_applied():\n    h = np.random.randint(22, 41)\n    crf = CRF(h)\n    t = crf.transitions.detach().numpy()\n    assert t[0, Offsets.GO, np.random.randint(h)] != -1e4\n\n\ndef test_mask_flips():\n    h = np.random.randint(22, 41)\n    mask = (np.random.rand(h, h) < 0.5).astype(np.uint8)\n    with patch(""eight_mile.pytorch.layers.transition_mask_np"") as mask_mock:\n        mask_mock.return_value = mask\n        pyt_mask = transition_mask(None, None, None, None, None)\n    mask2 = pyt_mask.numpy()\n    assert (mask & mask2).sum() == 0\n    assert (mask | mask2).sum() == h * h\n\n\ndef test_mask_same_after_update(generate_batch):\n    from torch.optim import SGD\n\n    unary, tags, lengths = generate_batch\n    h = unary.size(2)\n    constraint = torch.rand(h, h) < 0.5\n    crf = CRF(h, constraint_mask=constraint, batch_first=False)\n    opt = SGD(crf.parameters(), lr=10)\n    m1 = crf.constraint_mask.numpy()\n    t1 = crf.transitions_p.detach().clone().numpy()\n    l = crf.neg_log_loss(unary, tags, lengths)\n    l = torch.mean(l)\n    l.backward()\n    opt.step()\n    m2 = crf.constraint_mask.numpy()\n    t2 = crf.transitions_p.detach().numpy()\n    np.testing.assert_allclose(m1, m2)\n    with pytest.raises(AssertionError):\n        np.testing.assert_allclose(t1, t2)\n\n\ndef test_viterbi(generate_batch):\n    unary, _, lengths = generate_batch\n    h = unary.size(2)\n    trans = torch.rand(h, h)\n    pyt_path, pyt_scores = Viterbi(Offsets.GO, Offsets.EOS)(unary, trans.unsqueeze(0), lengths)\n\n    new_trans = build_trans(trans)\n    unary = unary.transpose(0, 1)\n    paths = []\n    scores = []\n    for u, l in zip(unary, lengths):\n        emiss = build_emission(u[:l])\n        p, s = explicit_viterbi(emiss, new_trans, Offsets.GO, Offsets.EOS)\n        scores.append(s)\n        paths.append(p)\n    gold_scores = np.array(scores)\n    np.testing.assert_allclose(pyt_scores.detach().numpy(), gold_scores, rtol=1e-6)\n    pyt_path = pyt_path.transpose(0, 1)\n    for pp, l, p in zip(pyt_path, lengths, paths):\n        assert pp[:l].tolist() == p\n\n\ndef test_viterbi_score_equals_sentence_score(generate_batch):\n    """"""Test that the scores from viterbi decoding are the same scores that you get when looking up those returned paths.""""""\n    unary, _, lengths = generate_batch\n    h = unary.size(2)\n    trans = torch.rand(h, h)\n    crf = CRF(h)\n\n    p, viterbi_scores = Viterbi(Offsets.GO, Offsets.EOS)(unary, crf.transitions, lengths)\n    gold_scores = crf.score_sentence(unary, p, lengths)\n    np.testing.assert_allclose(viterbi_scores.detach().numpy(), gold_scores.detach().numpy())\n\n\ndef test_viterbi_script(generate_batch):\n    unary, _, lengths = generate_batch\n    h = unary.size(2)\n    trans = torch.rand(h, h)\n\n    # pyt_path, pyt_scores = ViterbiBatchSize1(Offsets.GO, Offsets.EOS)(unary, trans.unsqueeze(0), lengths)\n\n    new_trans = build_trans(trans)\n    batch_first_unary = unary.transpose(0, 1)\n    for u, l in zip(batch_first_unary, lengths):\n        emiss = build_emission(u[:l])\n        p, s = explicit_viterbi(emiss, new_trans, Offsets.GO, Offsets.EOS)\n        ps, ss = script_viterbi(u[:l], trans, Offsets.GO, Offsets.EOS)\n\n        np.testing.assert_allclose(ps.numpy().tolist(), p, rtol=1e-6)\n        np.testing.assert_allclose(ss.item(), s, rtol=1e-6)\n\n\ndef test_viterbi_batch_stable(generate_examples_and_batch):\n    i1, _, l1, i2, _, l2, i, _, l = generate_examples_and_batch\n    h = i1.size(2)\n    trans = torch.rand(1, h, h)\n    p1, s1 = Viterbi(Offsets.GO, Offsets.EOS)(i1, trans, l1)\n    p2, s2 = Viterbi(Offsets.GO, Offsets.EOS)(i2, trans, l2)\n    pad = torch.zeros(p1.size(0) - p2.size(0), 1, dtype=torch.long)\n    one_x_one_p = torch.cat([p1, torch.cat([p2, pad], dim=0)], dim=1)\n    one_x_one_s = torch.cat([s1, s2], dim=0)\n    batched_p, batched_s = Viterbi(Offsets.GO, Offsets.EOS)(i, trans, l)\n    np.testing.assert_allclose(one_x_one_s.detach().numpy(), batched_s.detach().numpy())\n    np.testing.assert_allclose(one_x_one_p.detach().numpy(), batched_p.detach().numpy())\n\n\ndef test_viterbi_degenerates_to_argmax(generate_batch):\n    scores, _, l = generate_batch\n    h = scores.size(2)\n    # Then transitions are all zeros then it just greedily selects the best\n    # state at that given emission. This is the same as doing argmax.\n    trans = torch.zeros((1, h, h))\n    viterbi = Viterbi(Offsets.GO, Offsets.EOS)\n    p, s = viterbi(scores, trans, l)\n    s_gold, p_gold = torch.max(scores, 2)\n    # Mask out the argmax results from past the lengths\n    for i, sl in enumerate(l):\n        s_gold[sl:, i] = 0\n        p_gold[sl:, i] = 0\n    s_gold = torch.sum(s_gold, 0)\n    np.testing.assert_allclose(p.detach().numpy(), p_gold.detach().numpy())\n    np.testing.assert_allclose(s.detach().numpy(), s_gold.detach().numpy())\n\n\ndef test_decode_shape(generate_batch):\n    unary, _, lengths = generate_batch\n    h = unary.size(2)\n    trans = torch.rand(1, h, h)\n    viterbi = Viterbi(Offsets.GO, Offsets.EOS)\n    paths, scores = viterbi(unary, trans, lengths)\n    assert scores.shape == torch.Size([unary.size(1)])\n    assert paths.shape == torch.Size([unary.size(0), unary.size(1)])\n\n\ndef test_vec_log_sum_exp():\n    vec = torch.rand(1, np.random.randint(5, 31))\n    ours = vec_log_sum_exp(vec, 1).squeeze()\n    xs = {}\n    for i in range(vec.size(1)):\n        xs[i] = vec[0, i].item()\n    gold = explicit_log_sum_exp(xs)\n    np.testing.assert_allclose(ours, gold, rtol=1e-6)\n\n\ndef test_vec_log_sum_exp_zeros():\n    l = np.random.randint(1, 21)\n    in_ = torch.zeros(1, l)\n    lse = vec_log_sum_exp(in_, 1).squeeze()\n    np.testing.assert_allclose(lse.detach().numpy(), math.log(l))\n\n\ndef test_vec_log_sum_exp_ones():\n    l = np.random.randint(1, 21)\n    in_ = torch.ones(1, l)\n    lse = vec_log_sum_exp(in_, 1).squeeze()\n    np.testing.assert_allclose(lse.detach().numpy(), math.log(l * math.e))\n\n\ndef test_vec_log_sum_exp_shape():\n    dim = torch.randint(0, 3, (1,)).item()\n    shape = torch.randint(1, 21, (3,))\n    in_ = torch.rand(*shape)\n    out = vec_log_sum_exp(in_, dim)\n    shape[dim] = 1\n    for i in range(len(shape)):\n        assert out.size(i) == shape[i]\n\n\ndef test_vec_log_sum_exp_batch_stable():\n    h = np.random.randint(22, 41)\n    i1 = torch.rand(1, h, h)\n    i2 = torch.rand(1, h, h)\n    i = torch.cat([i1, i2], dim=0)\n    lse1 = vec_log_sum_exp(i1, 2)\n    lse2 = vec_log_sum_exp(i2, 2)\n    one_x_one = torch.cat([lse1, lse2], dim=0)\n    lse = vec_log_sum_exp(i, 2)\n    np.testing.assert_allclose(one_x_one.numpy(), lse.numpy())\n'"
tests/test_crf_tensorflow.py,0,"b'import os\nimport json\nimport pytest\n\npytest.skip(""There is now a crf object we should be testing on instead of the tagger"", allow_module_level=True)\nimport numpy as np\n\ntf = pytest.importorskip(""tensorflow"")\nfrom baseline.model import create_tagger_model, load_tagger_model\nfrom baseline.embeddings import load_embeddings\nfrom baseline.utils import transition_mask as np_transition_mask\nfrom baseline.tf.tfy import transition_mask\n\n\nHSZ = 100\nWSZ = 30\nS = ""<GO>""\nE = ""<EOS>""\nP = ""<PAD>""\nSPAN_TYPE = ""IOB2""\nLOC = os.path.dirname(os.path.realpath(__file__))\n\n\n@pytest.fixture(scope=""module"")\ndef set_cpu():\n    os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n    yield\n    del os.environ[""CUDA_VISIBLE_DEVICES""]\n\n\n@pytest.fixture\ndef label_vocab():\n    LOC = os.path.dirname(os.path.realpath(__file__))\n    vocab_loc = os.path.join(LOC, ""test_data"", ""crf_vocab"")\n    return json.load(open(vocab_loc))\n\n\n@pytest.fixture\ndef embeds():\n    import baseline.tf.embeddings\n\n    embeds = {}\n    embeds[""word""] = load_embeddings(""word"", dsz=HSZ, known_vocab={chr(i): i for i in range(100)})[""embeddings""]\n    embeds[""char""] = load_embeddings(""char"", dsz=WSZ, known_vocab={chr(i): i for i in range(100)})[""embeddings""]\n    return embeds\n\n\n@pytest.fixture\ndef save_file():\n    base = ""del""\n    path = os.path.join(LOC, base)\n    yield path\n    os.remove(os.path.join(LOC, ""checkpoint""))\n    for file_name in os.listdir(LOC):\n        if file_name.startswith(base):\n            os.remove(os.path.join(LOC, file_name))\n\n\n@pytest.fixture\ndef model(label_vocab, embeds, mask):\n    from baseline.tf import tagger\n\n    model = create_tagger_model(\n        embeds, label_vocab, crf=True, constraint_mask=mask, hsz=HSZ, cfiltsz=[3], wsz=WSZ, layers=2, rnntype=""blstm""\n    )\n    model.create_loss()\n    model.sess.run(tf.compat.v1.global_variables_initializer())\n    return model\n\n\n@pytest.fixture\ndef mask(label_vocab):\n    return transition_mask(label_vocab, SPAN_TYPE, label_vocab[S], label_vocab[E], label_vocab[P])\n\n\ndef test_mask_used(label_vocab, model):\n    transition = model.sess.run(model.A)\n    assert transition[label_vocab[""O""], label_vocab[S]] == -1e4\n\n\ndef test_mask_is_transpose(label_vocab, model):\n    transition = model.sess.run(model.mask)\n    np_mask = np_transition_mask(label_vocab, SPAN_TYPE, label_vocab[S], label_vocab[E], label_vocab[P])\n    np.testing.assert_allclose(transition.T, np_mask)\n\n\ndef test_persists_save(model, save_file):\n    model.save_using(tf.compat.v1.train.Saver())\n    t1 = model.sess.run(model.A)\n    model.save(save_file)\n    m2 = load_tagger_model(save_file)\n    t2 = model.sess.run(m2.A)\n    np.testing.assert_allclose(t1, t2)\n\n\ndef test_skip_mask(label_vocab, embeds, mask):\n    from baseline.tf import tagger\n\n    model = create_tagger_model(embeds, label_vocab, crf=True, hsz=HSZ, cfiltsz=[3], wsz=WSZ, layers=2, rnntype=""blstm"")\n    model.create_loss()\n    model.sess.run(tf.compat.v1.global_variables_initializer())\n    transition = model.sess.run(model.A)\n    assert transition[label_vocab[""O""], label_vocab[S]] != -1e4\n'"
tests/test_decay.py,1,"b'import six\nimport pytest\nimport numpy as np\nfrom mock import patch, MagicMock\nfrom eight_mile.optz import (\n    create_lr_scheduler,\n    CosineDecayScheduler,\n    CyclicLRScheduler,\n    ExponentialDecayScheduler,\n    WarmupLinearScheduler,\n    ConstantScheduler,\n    PiecewiseDecayScheduler,\n    ZarembaDecayScheduler,\n    InverseTimeDecayScheduler,\n    CompositeLRScheduler,\n)\n\n\n@pytest.fixture\ndef piecewise():\n    min_ = np.random.randint(1, 5)\n    max_ = np.random.randint(min_ + 2, min_ + 7)\n    bounds = [min_, max_]\n    vals = np.random.uniform(size=len(bounds) + 1)\n    return bounds, vals\n\n\ndef test_zaremba_with_nones():\n    eta = np.random.rand()\n    zd = ZarembaDecayScheduler(lr=eta)\n    for step in np.random.randint(0, 1000000, size=100):\n        assert zd(step) == eta\n\n\ndef test_piecewise_start(piecewise):\n    b, v = piecewise\n    p = PiecewiseDecayScheduler(b, v)\n    lr = p(0)\n    assert lr == v[0]\n\n\ndef test_piecewise_mid(piecewise):\n    b, v = piecewise\n    p = PiecewiseDecayScheduler(b, v)\n    step = np.random.randint(np.min(b) + 1, np.max(b))\n    lr = p(step)\n    assert lr == v[1]\n\n\ndef test_piecewise_lsat(piecewise):\n    b, v = piecewise\n    p = PiecewiseDecayScheduler(b, v)\n    step = np.random.randint(np.max(b) + 3, np.max(b) + 100)\n    lr = p(step)\n    assert lr == v[-1]\n\n\ndef test_staircase_decay_flat():\n    steps = np.random.randint(900, 1001)\n    sd = ExponentialDecayScheduler(steps, np.random.rand(), lr=np.random.rand(), staircase=True)\n    stair_one_one = sd(np.random.randint(steps - 100, steps))\n    stair_one_two = sd(np.random.randint(steps - 100, steps))\n    stair_two = sd(np.random.randint(steps + 1, steps + 10))\n    assert stair_one_one == stair_one_two\n    assert stair_one_two != stair_two\n\n\ndef test_staircase_value():\n    sd = ExponentialDecayScheduler(1000, 0.9, lr=1.0, staircase=True)\n    gold = 1.0\n    test = sd(100)\n    np.testing.assert_allclose(test, gold)\n    gold = 0.9\n    test = sd(1001)\n    np.testing.assert_allclose(test, gold)\n\n\ndef test_exp_values():\n    sd = ExponentialDecayScheduler(1000, 0.9, lr=1.0)\n    gold = 0.9895192582062144\n    test = sd(100)\n    np.testing.assert_allclose(test, gold)\n    gold = 0.8999051805311098\n    test = sd(1001)\n    np.testing.assert_allclose(test, gold)\n\n\ndef test_warmup_peaks():\n    steps = np.random.randint(100, 1000)\n    lr = np.random.rand()\n    wls = WarmupLinearScheduler(steps, lr=lr)\n    peak = wls(steps)\n    assert peak == lr\n    past = wls(steps + np.random.randint(100, 10000))\n    assert past == lr\n\n\ndef test_warmup_increases():\n    steps = np.random.randint(100, 1000)\n    lr = np.random.rand()\n    wls = WarmupLinearScheduler(steps, lr=lr)\n    lrs = [wls(s) for s in range(steps)]\n    last = -1\n    for lr in lrs:\n        assert lr > last\n        last = lr\n\n\ndef test_cyclic_lr():\n    bounds = 1000\n    min_eta = 1e-5\n    max_eta = 1e-2\n    clr = CyclicLRScheduler(max_eta, bounds, lr=min_eta)\n    start = clr(0)\n    up = clr(bounds / 2.0)\n    mid = clr(bounds)\n    down = clr(bounds + (bounds / 2.0))\n    end = clr(2 * bounds)\n    late = clr(3 * bounds)\n    assert start == min_eta\n    assert up > start\n    assert up < mid\n    assert mid == max_eta\n    assert down < mid\n    assert down > end\n    assert end == min_eta\n    assert late == max_eta\n\n\ndef test_cosine_lr():\n    cd = CosineDecayScheduler(1000, lr=0.1)\n    iters = [0, 100, 900, 1000, 1001]\n    golds = [0.1, 0.09755283, 0.002447176, 0.0, 0.0]\n    for i, gold in zip(iters, golds):\n        np.testing.assert_allclose(cd(i), gold, rtol=1e-6)\n\n\ndef test_constant_lr():\n    lr = np.random.rand()\n    lrs = ConstantScheduler(lr=lr)\n    for x in np.random.randint(0, 10000000, size=np.random.randint(100, 1000)):\n        assert lrs(x) == lr\n\n\ndef test_inverse_time_values():\n    eta = 1.0\n    steps = np.random.randint(1, 100)\n    ti = InverseTimeDecayScheduler(steps, 1.0, lr=eta)\n    for i in range(1, 5):\n        lr = ti(i * steps)\n        assert lr == eta / (i + 1)\n\n\ndef test_inverse_time_is_flat():\n    steps = np.random.randint(2, 100)\n    ti = InverseTimeDecayScheduler(steps, np.random.rand(), staircase=True, lr=np.random.rand())\n    before = steps - np.random.randint(1, steps)\n    after = steps + np.random.randint(1, steps)\n    after2 = steps + np.random.randint(1, steps)\n    lr_before = ti(before)\n    lr_after = ti(after)\n    lr_after2 = ti(after2)\n    assert lr_before != lr_after\n    assert lr_after == lr_after2\n\n\ndef test_composite_calls_warm():\n    warmup_steps = np.random.randint(50, 101)\n    warm = MagicMock()\n    warm.warmup_steps = warmup_steps\n    rest = MagicMock()\n    lr = CompositeLRScheduler(warm=warm, rest=rest)\n    step = np.random.randint(0, warmup_steps)\n    _ = lr(step)\n    warm.assert_called_once_with(step)\n    rest.assert_not_called()\n\n\ndef test_composite_calls_rest():\n    warmup_steps = np.random.randint(50, 101)\n    warm = MagicMock()\n    warm.warmup_steps = warmup_steps\n    rest = MagicMock()\n    lr = CompositeLRScheduler(warm=warm, rest=rest)\n    step = np.random.randint(warmup_steps + 1, six.MAXSIZE)\n    _ = lr(step)\n    warm.assert_not_called()\n    rest.assert_called_once_with(step - warmup_steps)\n\n\ndef test_composite_error():\n    pytest.importorskip(""torch"")\n    from eight_mile.pytorch.optz import CompositeLRSchedulerPyTorch\n\n    with pytest.raises(AssertionError):\n        _ = create_lr_scheduler(**{""lr_scheduler_type"": [""exponential"", ""zaremba""]})\n'"
tests/test_decoders_pytorch.py,14,"b'import os\nimport math\nimport json\nimport pytest\nimport numpy as np\nfrom eight_mile.embeddings import RandomInitVecModel\nfrom collections import namedtuple\nimport string\ntorch = pytest.importorskip(\'torch\')\nfrom eight_mile.utils import Offsets\n\n\ndef test_rnn_decode_shapes():\n    from baseline.pytorch.embeddings import LookupTableEmbeddingsModel\n    from baseline.pytorch.seq2seq.decoders import RNNDecoder\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    batchsz = 2\n    temporal = 7\n    temporal_output = 4\n    hsz = 20\n    dsz = 10\n    layers = 1\n    # Always pick the right path\n    wv = RandomInitVecModel(\n        dsz, {k: 1 for k in list(string.ascii_letters)}\n    )\n    assert len(string.ascii_letters) + len(Offsets.VALUES) == wv.get_vsz()\n    encoder.output = np.random.randn(batchsz, temporal, hsz)\n    encoder.output = torch.from_numpy(encoder.output).float()\n    encoder.hidden = (torch.from_numpy(np.random.randn(layers, batchsz, hsz)).float(),\n                      torch.from_numpy(np.random.randn(layers, batchsz, hsz)).float())\n    encoder.output = encoder.output\n    encoder.src_mask = torch.zeros(batchsz, temporal).byte()\n    tgt_embed = LookupTableEmbeddingsModel.create(wv, \'output\')\n    decoder = RNNDecoder(tgt_embed, hsz=hsz, tie_weights=False)\n    decode_start = torch.full((batchsz, temporal_output), Offsets.GO, dtype=torch.long)\n    output = decoder(encoder, decode_start)\n    assert output.shape[0] == batchsz\n    assert output.shape[1] == temporal_output\n    assert output.shape[2] == wv.get_vsz()\n\n\n\ndef test_rnn_attn_decode_shapes():\n    from baseline.pytorch.embeddings import LookupTableEmbeddingsModel\n    from baseline.pytorch.seq2seq.decoders import RNNDecoderWithAttn\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    batchsz = 2\n    temporal = 7\n    temporal_output = 4\n    hsz = 20\n    dsz = 10\n    layers = 1\n    # Always pick the right path\n    wv = RandomInitVecModel(\n        dsz, {k: 1 for k in list(string.ascii_letters)}\n    )\n\n    assert len(string.ascii_letters) + len(Offsets.VALUES) == wv.get_vsz()\n    encoder.output = np.random.randn(batchsz, temporal, hsz)\n    encoder.output = torch.from_numpy(encoder.output).float()\n    encoder.hidden = (torch.from_numpy(np.random.randn(layers, batchsz, hsz)).float(),\n                  torch.from_numpy(np.random.randn(layers, batchsz, hsz)).float())\n    encoder.output = encoder.output\n    encoder.src_mask = torch.zeros(batchsz, temporal).byte()\n\n    tgt_embed = LookupTableEmbeddingsModel.create(wv, \'output\')\n    decoder = RNNDecoderWithAttn(tgt_embed, hsz=hsz, tie_weights=False)\n    decode_start = torch.full((batchsz, temporal_output), Offsets.GO, dtype=torch.long)\n    output = decoder(encoder, decode_start)\n    assert output.shape[0] == batchsz\n    assert output.shape[1] == temporal_output\n    assert output.shape[2] == wv.get_vsz()\n'"
tests/test_decoders_tensorflow.py,0,"b'import os\nimport math\nimport json\nimport pytest\nimport numpy as np\nfrom eight_mile.utils import get_version\nfrom eight_mile.embeddings import RandomInitVecModel\nfrom collections import namedtuple\nimport string\ntf = pytest.importorskip(\'tensorflow\')\npytestmark = pytest.mark.skipif(get_version(tf) < 2, reason=\'TF1.X\')\nfrom eight_mile.utils import Offsets\n\n\ndef test_rnn_decode_shapes():\n    from baseline.tf.embeddings import LookupTableEmbeddingsModel\n    from baseline.tf.seq2seq.decoders.v2 import RNNDecoder\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    batchsz = 2\n    temporal = 7\n    temporal_output = 4\n    hsz = 20\n    dsz = 10\n    layers = 1\n    # Always pick the right path\n    wv = RandomInitVecModel(\n        dsz, {k: 1 for k in list(string.ascii_letters)}\n    )\n    assert len(string.ascii_letters) + len(Offsets.VALUES) == wv.get_vsz()\n    encoder.output = tf.cast(np.random.randn(batchsz, temporal, hsz), dtype=tf.float32)\n    encoder.hidden = (tf.cast(np.random.randn(layers, batchsz, hsz), dtype=tf.float32),\n                      tf.cast(np.random.randn(layers, batchsz, hsz), dtype=tf.float32))\n    encoder.src_mask = np.zeros((batchsz, temporal), dtype=np.uint8)\n    tgt_embed = LookupTableEmbeddingsModel.create(wv, \'output\')\n    decoder = RNNDecoder(tgt_embed, hsz=hsz, tie_weights=False)\n    decode_start = np.full((batchsz, temporal_output), Offsets.GO, dtype=np.int64)\n    output = decoder(encoder, decode_start)\n    assert output.shape[0] == batchsz\n    assert output.shape[1] == temporal_output\n    assert output.shape[2] == wv.get_vsz()\n\n\n\ndef test_rnn_attn_decode_shapes():\n    from baseline.tf.embeddings import LookupTableEmbeddingsModel\n    from baseline.tf.seq2seq.decoders.v2 import RNNDecoderWithAttn\n    # Always pick the right path\n    encoder = namedtuple(""EncoderOutput"", ""output src_mask"")\n    batchsz = 2\n    temporal = 7\n    temporal_output = 4\n    hsz = 20\n    dsz = 10\n    layers = 1\n    # Always pick the right path\n    wv = RandomInitVecModel(\n        dsz, {k: 1 for k in list(string.ascii_letters)}\n    )\n    assert len(string.ascii_letters) + len(Offsets.VALUES) == wv.get_vsz()\n    encoder.output = tf.cast(np.random.randn(batchsz, temporal, hsz), dtype=tf.float32)\n    encoder.hidden = (tf.cast(np.random.randn(layers, batchsz, hsz), dtype=tf.float32),\n                      tf.cast(np.random.randn(layers, batchsz, hsz), dtype=tf.float32))\n    encoder.src_mask = np.zeros((batchsz, temporal), dtype=np.uint8)\n    tgt_embed = LookupTableEmbeddingsModel.create(wv, \'output\')\n    decoder = RNNDecoderWithAttn(tgt_embed, hsz=hsz, attn_type=\'sdpx\', tie_weights=False)\n    decode_start = np.full((batchsz, temporal_output), Offsets.GO, dtype=np.int64)\n    output = decoder(encoder, decode_start)\n    assert output.shape[0] == batchsz\n    assert output.shape[1] == temporal_output\n    assert output.shape[2] == wv.get_vsz()\n'"
tests/test_embeddings.py,0,"b'import os\nimport math\nimport random\nimport string\nfrom functools import partial\nimport pytest\nimport numpy as np\nfrom eight_mile.embeddings import *\nfrom eight_mile.utils import Offsets\n\nloc = os.path.dirname(os.path.realpath(__file__))\nGLOVE_FILE = os.path.join(loc, ""test_data"", ""glove_test.txt"")\nW2V_FILE = os.path.join(loc, ""test_data"", ""w2v_test.bin"")\n\n\ndef random_model():\n    """"""Randomly pick a model for testing.""""""\n    files = [GLOVE_FILE, W2V_FILE]\n    f = random.choice(files)\n    return partial(PretrainedEmbeddingsModel, f)\n\n\ndef test_glove_vsz():\n    # Demo data = 10\n    gold_vsz = 10 + len(Offsets.VALUES)\n    wv = PretrainedEmbeddingsModel(GLOVE_FILE, keep_unused=True)\n    assert wv.get_vsz() == gold_vsz\n\n\ndef test_glove_dsz():\n    gold_dsz = 50\n    wv = PretrainedEmbeddingsModel(GLOVE_FILE, keep_unused=True)\n    assert wv.get_dsz() == gold_dsz\n\n\ndef test_w2v_vsz():\n    gold_vsz = 10 + len(Offsets.VALUES)\n    wv = PretrainedEmbeddingsModel(W2V_FILE, keep_unused=True)\n    assert wv.get_vsz() == gold_vsz\n\n\ndef test_w2v_dsz():\n    gold_dsz = 300\n    wv = PretrainedEmbeddingsModel(W2V_FILE, keep_unused=True)\n    assert wv.get_dsz() == gold_dsz\n\n\n# def test_rand_vsz():\n#     ref_wv = random_model()(keep_unused=True)\n#     vocab = ref_wv.vocab\n#     gold_dsz = ref_wv.get_dsz()\n#     wv = RandomInitVecModel(gold_dsz, vocab)\n#     assert wv.get_vsz() == len(vocab)\n\n\ndef test_rand_dsz():\n    ref_wv = random_model()(keep_unused=True)\n    vocab = ref_wv.vocab\n    gold_dsz = ref_wv.get_dsz()\n    wv = RandomInitVecModel(gold_dsz, vocab)\n    assert wv.get_dsz() == gold_dsz\n\n\ndef test_random_vector_range():\n    gold_weight = 0.4\n    wv = RandomInitVecModel(300, {k: 1 for k in list(string.ascii_letters)}, unif_weight=gold_weight)\n    assert np.min(wv.weights) >= -gold_weight\n    assert np.max(wv.weights) <= gold_weight\n\n\ndef test_round_trip():\n    gold_weight = 0.4\n    input_model = RandomInitVecModel(300, {k: 1 for k in list(string.ascii_letters)}, unif_weight=gold_weight)\n    write_word2vec_file(""test.bin"", input_model.vocab, input_model.weights)\n    output_model = PretrainedEmbeddingsModel(""test.bin"", keep_unused=True)\n    # This is a bit weird...\n    assert output_model.vsz == input_model.vsz\n    for word in input_model.vocab:\n        if word not in Offsets.VALUES:\n            assert np.allclose(input_model[word], output_model[word])\n\n\n# def test_valid_lookup():\n#     wv = random_model()(keep_unused=True)\n#     key = \'<PAD>\'\n#     while key == \'<PAD>\':\n#         key = random.choice(list(wv.vocab.keys()))\n#     res = wv.lookup(key)\n#     assert res is not None\n#     assert res.shape == (wv.get_dsz(),)\n#     with pytest.raises(AssertionError):\n#         np.testing.assert_allclose(res, np.zeros((wv.get_dsz(),)))\n\n\ndef test_nullv_lookup():\n    wv = random_model()(keep_unused=True)\n    key = ""zzzzzzzzzzz""\n    assert key not in wv.vocab\n    res = wv.lookup(key, nullifabsent=False)\n    np.testing.assert_allclose(res, np.zeros((wv.get_dsz())))\n\n\ndef test_none_lookup():\n    wv = random_model()(keep_unused=True)\n    key = ""zzzzzzzzzzz""\n    assert key not in wv.vocab\n    res = wv.lookup(key)\n    assert res is None\n\n\ndef test_mmap_glove():\n    wv_file = PretrainedEmbeddingsModel(GLOVE_FILE, keep_unused=True)\n    wv_mmap = PretrainedEmbeddingsModel(GLOVE_FILE, keep_unused=True, use_mmap=True)\n    np.testing.assert_allclose(wv_file.weights, wv_mmap.weights)\n\n\ndef test_mmap_w2v():\n    wv_file = PretrainedEmbeddingsModel(W2V_FILE, keep_unused=True)\n    wv_mmap = PretrainedEmbeddingsModel(W2V_FILE, keep_unused=True, use_mmap=True)\n    np.testing.assert_allclose(wv_file.weights, wv_mmap.weights)\n\n\ndef test_normalize_e2e():\n    wv = random_model()(normalize=True, keep_unused=True)\n    norms = np.sqrt(np.sum(np.square(wv.weights), 1))\n    for norm in norms:\n        assert norm == 0 or np.allclose(norm, 1, rtol=1e-4)\n\n\ndef test_normalize():\n    wv = random_model()(keep_unused=True)\n    normed = norm_weights(wv.weights)\n    gold_norms = np.zeros_like(wv.weights)\n    for i in range(len(gold_norms)):\n        norm = np.sqrt(np.sum(np.square(wv.weights[i])))\n        gold_norms[i] = wv.weights[i] if norm == 0.0 else wv.weights[i] / norm\n    np.testing.assert_allclose(normed, gold_norms)\n\n\n# def test_vocab_truncation():\n#     model = random_model()\n#     wv = model(keep_unused=True)\n#     gold = wv.vocab\n#     keys = list(gold.keys())\n#     removed = \'<PAD>\'\n#     while removed == \'<PAD>\':\n#         removed = random.choice(keys)\n#     gold.pop(removed)\n#     wv = model(known_vocab=gold)\n#     assert set(gold.keys()) == set(wv.vocab.keys())\n#     assert removed not in wv.vocab\n\n\ndef test_vocab_not_truncated():\n    model = random_model()\n    wv = model(keep_unused=True)\n    gold = wv.vocab\n    keys = list(gold.keys())\n    removed = ""<PAD>""\n    while removed == ""<PAD>"":\n        removed = random.choice(keys)\n    gold.pop(removed)\n    assert set(keys) != set(gold)\n    wv = model(known_vocab=gold, keep_unused=True)\n    assert set(keys) == set(wv.vocab.keys())\n    assert removed in wv.vocab\n\n\ndef test_extra_vocab():\n    model = random_model()\n    wv = model(keep_unused=True)\n    vocab = wv.vocab\n    extra_keys = [""AAAAAAAAAAAA"", ""ZZZZZZZZZZ""]\n    for key in extra_keys:\n        vocab[key] = 12\n    wv = model(known_vocab=vocab)\n    for key in extra_keys:\n        assert key in wv.vocab\n    for key in extra_keys:\n        np.testing.assert_allclose(wv.lookup(key), np.zeros((wv.get_dsz(),)))\n\n\ndef test_extra_vocab_weights():\n    """"""Test that words not in the pretrained vocab are initialized correctly.""""""\n    weight = 0.5\n    model = random_model()\n    wv = model(keep_unused=True, unif_weight=weight)\n    vocab = wv.vocab\n    extra_keys = [""AAAAAAAAAAAA"", ""ZZZZZZZZZZ""]\n    for key in extra_keys:\n        vocab[key] = 12\n    wv = model(known_vocab=vocab)\n    for key in extra_keys:\n        vec = wv.lookup(key)\n        assert np.min(vec) >= -weight\n        assert np.max(vec) <= weight\n\n\ndef test_rand_no_counts():\n    """"""Test that unattested word are not removed without counts.""""""\n    vocab = {""A"": 0, ""B"": 0, ""C"": 13, ""D"": 55}\n    wv = RandomInitVecModel(random.randint(1, 301), vocab, counts=False)\n    assert wv.get_vsz() == max(vocab.values()) + 1\n\n\ndef test_rand_counts():\n    """"""Test that unattested word are removed with counts.""""""\n    vocab = {""A"": 0, ""B"": 0, ""C"": 13, ""D"": 55}\n    wv = RandomInitVecModel(12, vocab, counts=True)\n    gold = {""C"", ""D""}\n    for g in gold:\n        assert g in wv.vocab\n'"
tests/test_hash_utils.py,0,"b""import random\nfrom copy import deepcopy\nimport mock\nimport pytest\nfrom mead.utils import remove_extra_keys, order_json, hash_config\n\n\n@pytest.fixture\ndef data():\n    return {\n        'a': 1,\n        'b': {\n            'c': 2,\n            'd': 3,\n        },\n        'e': 4,\n    }\n\n\n@pytest.fixture\ndef mixed():\n    return {\n        'e': 4,\n        'b': {\n            'd': 3,\n            'c': 2,\n        },\n        'a': 1,\n    }\n\n\ndef test_hash_config_fixes_json(data):\n    with mock.patch('mead.utils.remove_extra_keys') as strip_mock:\n        strip_mock.return_value = data\n        with mock.patch('mead.utils.order_json') as order_mock:\n           order_mock.return_value = data\n           hash_config(data)\n    strip_mock.assert_called_once_with(data)\n    order_mock.assert_called_once_with(data)\n\n\ndef test_order_json_e2e(data, mixed):\n    res = order_json(mixed)\n    assert res == data\n\n\ndef test_order_json_doesnt_sorts_list():\n    gold = list(range(10))\n    random.shuffle(gold)\n    data = {'a': gold}\n    res = order_json(data)\n    assert res['a'] == gold\n\n\ndef test_remove_first_layer(data):\n    gold = deepcopy(data)\n    del gold['a']\n    keys = {('a',)}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_multiple_first(data):\n    gold = {'a': 1}\n    keys = {('b',), ('e',)}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_nested_key(data):\n    gold = deepcopy(data)\n    del gold['b']['c']\n    keys = {('b', 'c')}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_multiple_nested(data):\n    gold = deepcopy(data)\n    gold['b'] = {}\n    keys = {('b', 'c'), ('b', 'd')}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_mixed_keys(data):\n    gold = deepcopy(data)\n    del gold['b']['c']\n    del gold['a']\n    keys = {('b', 'c'), ('a',)}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_single_missing(data):\n    gold = deepcopy(data)\n    keys = {('x',)}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_nested_missing_first(data):\n    gold = deepcopy(data)\n    keys = {('x', 'c')}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_nested_missing_last(data):\n    gold = deepcopy(data)\n    keys = {('b', 'x')}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_nested_missing_both(data):\n    gold = deepcopy(data)\n    keys = {('y', 'x')}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n\n\ndef test_remove_one_missing_one_good(data):\n    gold = deepcopy(data)\n    del gold['a']\n    keys = {('x', 'y'), ('a',)}\n    res = remove_extra_keys(data, keys)\n    assert res == gold\n"""
tests/test_iobes.py,0,"b'import random\nfrom mock import patch\nfrom eight_mile.utils import (\n    to_chunks,\n    to_chunks_iobes,\n    convert_iob_to_bio,\n    convert_iob_to_iobes,\n    convert_bio_to_iob,\n    convert_bio_to_iobes,\n    convert_iobes_to_bio,\n    convert_iobes_to_iob,\n)\n\n# Most of these tests follow this general format. A random sequence of entity spans are generated,\n# These entities are used to generate a tag sequence in some given format. These generated entities\n# are used as gold for the `to_chunks` code. Generating multiple tag sequences from a single\n# random set of spans can be used for gold data for the converting functions.\n\n\ndef generate_spans(ents=[""X"", ""Y""], max_span=5, min_span=0, max_spanl=3, min_spanl=1, min_space=0, max_space=1):\n    """"""Generate a series of entity spans. They are be touching each other but don\'t overlap.""""""\n    i = 0\n    n_spans = random.randint(min_span, max_span)\n    spans = []\n    for _ in range(n_spans):\n        ent = random.choice(ents)\n        span_length = random.randint(min_spanl, max_spanl)\n        span_gap = random.randint(min_space, max_space)\n        span_start = i + span_gap\n        span_end = span_start + span_length\n        i = span_end\n        span = ""@"".join([ent] + list(map(str, range(span_start, span_end))))\n        spans.append(span)\n    end = random.randint(min_space, max_space)\n    return spans, i + end\n\n\ndef parse_span(span):\n    parts = span.split(""@"")\n    return parts[0], list(map(int, parts[1:]))\n\n\ndef generate_iobes(spans):\n    """"""Convert a set of spans to an IOBES sequence.""""""\n    spans, length = spans\n    text = [""O""] * length\n    for span in spans:\n        ent, locs = parse_span(span)\n        if len(locs) == 1:\n            text[locs[0]] = ""S-{}"".format(ent)\n            continue\n        for i, loc in enumerate(locs):\n            if i == 0:\n                text[loc] = ""B-{}"".format(ent)\n            elif i == len(locs) - 1:\n                text[loc] = ""E-{}"".format(ent)\n            else:\n                text[loc] = ""I-{}"".format(ent)\n    return text\n\n\ndef generate_bio(spans):\n    """"""Convert a set of spans to a BIO sequence.""""""\n    spans, length = spans\n    text = [""O""] * length\n    for span in spans:\n        ent, locs = parse_span(span)\n        for i, loc in enumerate(locs):\n            if i == 0:\n                text[loc] = ""B-{}"".format(ent)\n            else:\n                text[loc] = ""I-{}"".format(ent)\n    return text\n\n\ndef generate_iob(spans):\n    """"""Convert a set of spans to an IOB sequence.""""""\n    spans, length = spans\n    text = [""O""] * length\n    for span in spans:\n        ent, locs = parse_span(span)\n        for i, loc in enumerate(locs):\n            text[loc] = ""I-{}"".format(ent)\n    for span in spans:\n        ent, locs = parse_span(span)\n        if locs[0] != 0 and text[locs[0] - 1][2:] == ent:\n            text[locs[0]] = ""B-{}"".format(ent)\n    return text\n\n\ndef test_iob_bio():\n    def test():\n        spans = generate_spans()\n        iob = generate_iob(spans)\n        gold_bio = generate_bio(spans)\n        bio = convert_iob_to_bio(iob)\n        assert bio == gold_bio\n\n    for _ in range(100):\n        test()\n\ndef test_bio_iob():\n    def test():\n        spans = generate_spans()\n        bio = generate_bio(spans)\n        gold_iob = generate_iob()\n        iob = convert_bio_to_iob(bio)\n        assert iob == gold_iob\n\n\ndef test_bio_iob():\n    def test():\n        spans = generate_spans()\n        bio = generate_bio(spans)\n        gold_iob = generate_iob()\n        iob = convert_bio_to_iob(bio)\n        assert iob == gold_iob\n\n\ndef test_bio_iobes():\n    def test():\n        spans = generate_spans()\n        bio = generate_bio(spans)\n        gold_iobes = generate_iobes(spans)\n        iobes = convert_bio_to_iobes(bio)\n        assert iobes == gold_iobes\n\n    for _ in range(100):\n        test()\n\n\ndef test_iobes_bio():\n    def test():\n        spans = generate_spans()\n        iobes = generate_iobes(spans)\n        gold_bio = generate_bio(spans)\n        bio = convert_iobes_to_bio(iobes)\n        assert bio == gold_bio\n\n    for _ in range(100):\n        test()\n\n\ndef test_iobes_iob():\n    def test():\n        spans = generate_spans()\n        iobes = generate_iobes(spans)\n        gold_iob = generate_iob(spans)\n        iob = convert_iobes_to_iob(iobes)\n        assert iob == gold_iob\n\n\ndef test_iob_bio_cycle():\n    def test():\n        spans = generate_spans()\n        gold_iob = generate_iob(spans)\n        res = convert_bio_to_iob(convert_iob_to_bio(gold_iob))\n        assert res == gold_iob\n\n    for _ in range(100):\n        test()\n\n\ndef test_bio_iob_cycle():\n    def test():\n        spans = generate_spans()\n        gold_bio = generate_bio(spans)\n        res = convert_iob_to_bio(convert_bio_to_iob(gold_bio))\n        assert res == gold_bio\n\n    for _ in range(100):\n        test()\n\n\ndef test_iobes_bio_cycle():\n    def test():\n        spans = generate_spans()\n        gold_iobes = generate_iobes(spans)\n        res = convert_bio_to_iobes(convert_iobes_to_bio(gold_iobes))\n        assert res == gold_iobes\n\n    for _ in range(100):\n        test()\n\n\ndef test_bio_iobes_cycle():\n    def test():\n        spans = generate_spans()\n        gold_bio = generate_bio(spans)\n        res = convert_iobes_to_bio(convert_bio_to_iobes(gold_bio))\n        assert res == gold_bio\n\n    for _ in range(100):\n        test()\n\n\ndef test_iobes_iob_cycle():\n    def test():\n        spans = generate_spans()\n        gold_iobes = generate_iobes(spans)\n        res = convert_iob_to_iobes(convert_iobes_to_iob(gold_iobes))\n        assert res == gold_iobes\n\n    for _ in range(100):\n        test()\n\n\ndef test_iob_iobes_cycle():\n    def test():\n        spans = generate_spans()\n        gold_iob = generate_iob(spans)\n        res = convert_iobes_to_iob(convert_iob_to_iobes(gold_iob))\n        assert res == gold_iob\n\n    for _ in range(100):\n        test()\n\n\ndef test_to_chunks_iobes():\n    def test():\n        spans = generate_spans()\n        gold = spans[0]\n        iobes = generate_iobes(spans)\n        chunks = to_chunks_iobes(iobes)\n        assert chunks == gold\n\n    for _ in range(100):\n        test()\n\n\ndef test_to_chunks_iobes_delim():\n    def test():\n        delim = random.choice([""@"", ""#"", ""%""])\n        spans = generate_spans()\n        gold = spans[0]\n        gold = [g.replace(""@"", delim) for g in gold]\n        iobes = generate_iobes(spans)\n        chunks = to_chunks_iobes(iobes, delim=delim)\n        assert chunks == gold\n\n    for _ in range(100):\n        test()\n\n\ndef test_to_chunks_calls_iobes():\n    seq = [random.randint(0, 5) for _ in range(random.randint(1, 6))]\n    verbose = random.choice([True, False])\n    delim = random.choice([""@"", ""#"", ""%""])\n    with patch(""eight_mile.utils.to_chunks_iobes"") as iobes_patch:\n        to_chunks(seq, ""iobes"", verbose, delim)\n    iobes_patch.assert_called_once_with(seq, verbose, delim)\n\n\ndef test_to_chunks_bio():\n    def test():\n        spans = generate_spans()\n        gold = spans[0]\n        iobes = generate_bio(spans)\n        chunks = to_chunks(iobes, ""bio"")\n        assert chunks == gold\n\n    for _ in range(100):\n        test()\n\n\ndef test_to_chunks_bio_delim():\n    def test():\n        delim = random.choice([""@"", ""#"", ""!""])\n        spans = generate_spans()\n        gold = spans[0]\n        gold = [g.replace(""@"", delim) for g in gold]\n        iobes = generate_bio(spans)\n        chunks = to_chunks(iobes, ""bio"", delim=delim)\n        assert chunks == gold\n\n    for _ in range(100):\n        test()\n\n\ndef test_to_chunks_iob():\n    def test():\n        spans = generate_spans()\n        gold = spans[0]\n        iobes = generate_iob(spans)\n        chunks = to_chunks(iobes, ""iob"")\n        assert chunks == gold\n\n    for _ in range(100):\n        test()\n\n\ndef test_to_chunks_iob_delim():\n    def test():\n        delim = random.choice([""@"", ""#"", ""$""])\n        spans = generate_spans()\n        gold = spans[0]\n        gold = [g.replace(""@"", delim) for g in gold]\n        iobes = generate_iob(spans)\n        chunks = to_chunks(iobes, ""iob"", delim=delim)\n        assert chunks == gold\n\n    for _ in range(100):\n        test()\n\n\n# Tests By Hand\ndef test_iob_bio_i_after_o():\n    in_ = [""O"", ""I-X"", ""O""]\n    gold = [""O"", ""B-X"", ""O""]\n    res = convert_iob_to_bio(in_)\n    assert res == gold\n\n\ndef test_bio_iob_b_after_diff():\n    in_ = [""O"", ""B-X"", ""B-Y"", ""O""]\n    gold = [""O"", ""I-X"", ""I-Y"", ""O""]\n    res = convert_bio_to_iob(in_)\n    assert res == gold\n\n\ndef test_bio_iob_b_after_same():\n    in_ = [""O"", ""B-X"", ""B-X"", ""O""]\n    gold = [""O"", ""I-X"", ""B-X"", ""O""]\n    res = convert_bio_to_iob(in_)\n    assert res == gold\n\n\ndef test_iob_bio_i_after_b_diff():\n    in_ = [""O"", ""B-X"", ""I-Y"", ""O""]\n    gold = [""O"", ""B-X"", ""B-Y"", ""O""]\n    res = convert_iob_to_bio(in_)\n    assert res == gold\n\n\ndef test_iob_bio_i_after_b_same():\n    in_ = [""O"", ""B-X"", ""I-X"", ""O""]\n    gold = [""O"", ""B-X"", ""I-X"", ""O""]\n    res = convert_iob_to_bio(in_)\n    assert res == gold\n\n\ndef test_iob_bio_i_after_i_diff():\n    in_ = [""O"", ""I-X"", ""I-Y"", ""O""]\n    gold = [""O"", ""B-X"", ""B-Y"", ""O""]\n    res = convert_iob_to_bio(in_)\n    assert res == gold\n\n\ndef test_iob_bio_i_after_i_same():\n    in_ = [""O"", ""I-X"", ""I-X"", ""O""]\n    gold = [""O"", ""B-X"", ""I-X"", ""O""]\n    res = convert_iob_to_bio(in_)\n    assert res == gold\n\n\ndef test_iob_bio_i_first():\n    in_ = [""I-X"", ""O""]\n    gold = [""B-X"", ""O""]\n    res = convert_iob_to_bio(in_)\n    assert res == gold\n\n\ndef test_bio_iobes_single_b():\n    in_ = [""O"", ""B-X"", ""O""]\n    gold = [""O"", ""S-X"", ""O""]\n    res = convert_bio_to_iobes(in_)\n    assert res == gold\n\n\ndef test_bio_iobes_i_to_e_at_end():\n    in_ = [""O"", ""B-X"", ""I-X""]\n    gold = [""O"", ""B-X"", ""E-X""]\n    res = convert_bio_to_iobes(in_)\n    assert res == gold\n\n\ndef test_bio_iobes_i_to_e_at_o():\n    in_ = [""O"", ""B-X"", ""I-X"", ""O""]\n    gold = [""O"", ""B-X"", ""E-X"", ""O""]\n    res = convert_bio_to_iobes(in_)\n    assert res == gold\n\n\ndef test_bio_iobes_i_to_e_at_b():\n    in_ = [""O"", ""B-X"", ""I-X"", ""B-Y"", ""I-Y""]\n    gold = [""O"", ""B-X"", ""E-X"", ""B-Y"", ""E-Y""]\n    res = convert_bio_to_iobes(in_)\n    assert res == gold\n'"
tests/test_label_first_data_utils.py,0,"b'from six import StringIO\n\nimport random\nimport string\nimport pytest\nimport numpy as np\nfrom eight_mile.utils import read_label_first_data, write_label_first_data\n\n\ndef random_str(len_=None, min_=5, max_=21):\n    if len_ is None:\n        len_ = np.random.randint(min_, max_)\n    choices = list(string.ascii_letters + string.digits)\n    return """".join([np.random.choice(choices) for _ in range(len_)])\n\n\ndef generate_data():\n    labels = [random_str() for _ in range(random.randint(5, 50))]\n    texts = [[random_str() for _ in range(random.randint(1, 20))] for _ in range(len(labels))]\n    return labels, texts\n\n\ndef test_label_first_data_round_trip():\n    data = StringIO()\n    labels, texts = generate_data()\n    write_label_first_data(data, labels, texts)\n    data.seek(0)\n    l, ts = read_label_first_data(data)\n\n\ndef test_label_first_data_read_tabs():\n    data = StringIO()\n    data.write(\n        """"""\n1\\tdata\n2\\tdata data\n3\\tdata\\tdata\\tdata\n    """""".lstrip()\n    )\n    gold_labels = list(""123"")\n    gold_texts = [[""data""] * i for i in range(1, 4)]\n    data.seek(0)\n    l, t = read_label_first_data(data)\n    assert l == gold_labels\n    assert t == gold_texts\n\n\ndef test_label_first_data_read_space_at_end():\n    data = StringIO()\n    # Note: The end of the first line in this example has a space after it\n    # This is needed for this test\n    data.write(\n        """"""\n1 data \n2 data data\\t\n3 data data\\tdata\\t\\t\\t\n    """""".lstrip()\n    )\n    gold_labels = list(""123"")\n    gold_texts = [[""data""] * i for i in range(1, 4)]\n    data.seek(0)\n    l, t = read_label_first_data(data)\n    assert l == gold_labels\n    assert t == gold_texts\n\n\ndef test_label_first_data_read_empty_row():\n    data = StringIO()\n    data.write(\n        """"""\n1\\tdata\n\n2\\tdata data\n    """""".lstrip()\n    )\n    gold_labels = list(""12"")\n    gold_texts = [[""data""] * i for i in range(1, 3)]\n    data.seek(0)\n    l, t = read_label_first_data(data)\n    assert l == gold_labels\n    assert t == gold_texts\n\n\ndef test_label_first_data_read_empty_example():\n    data = StringIO()\n    data.write(\n        """"""\n1\\tdata data\n2\n3 data\n    """""".lstrip()\n    )\n    data.seek(0)\n    with pytest.raises(ValueError):\n        l, t = read_label_first_data(data)\n\n\ndef test_label_first_data_read_single_token():\n    data = StringIO()\n    data.write(\n        """"""\n1 1\n2 2\n3 3\n    """""".lstrip()\n    )\n    data.seek(0)\n    l, t = read_label_first_data(data)\n    assert l == list(""123"")\n    assert t == [[item] for item in ""123""]\n\n\ndef test_write_label_first_data():\n    gold = """"""\n1 data\n2 data data\n3 data data data\n5 data data data data data\n4 data data data data\n    """""".strip()\n    labels = list(""12354"")\n    texts = [[""data""] * int(l) for l in labels]\n    data = StringIO()\n    write_label_first_data(data, labels, texts)\n    data.seek(0)\n    assert data.read() == gold\n'"
tests/test_layers_pytorch.py,17,"b'import pytest\nimport numpy as np\n\ntorch = pytest.importorskip(""torch"")\nfrom eight_mile.utils import Offsets\nfrom eight_mile.pytorch.layers import *\n\nC = 10\nB = 50\nT = 20\nH = 8\nL = 2\nTRIALS = 100\n\n\n@pytest.fixture\ndef lengths():\n    lengths = torch.randint(1, T, size=(B,)).long()\n    return lengths\n\n\n@pytest.fixture\ndef logits(lengths):\n    logits = torch.rand(B, T, C)\n    for i, l in enumerate(lengths):\n        logits[i, l:, :] = 0\n    return logits\n\n\n@pytest.fixture\ndef labels(lengths):\n    lab = torch.randint(1, C, size=(B, T)).long()\n    for i, l in enumerate(lengths):\n        lab[i, l:] = 0\n    return lab\n\n@pytest.fixture\ndef encoder_input(lengths):\n    hidden = torch.rand(B, T, C)\n    lengths, perm_idx = lengths.sort(0, descending=True)\n    return hidden[perm_idx], lengths\n\ndef generate_data_with_zeros(lengths):\n    data = torch.randint(1, 100, (len(lengths), torch.max(lengths)))\n    for i, length in enumerate(lengths):\n        data[i, length:] = 0\n        if length // 2 > 0:\n            extra_zeros = torch.randint(0, length.item() - 1, size=((length // 2).item(),))\n            data[i, extra_zeros] = 0\n    return data\n\n\ndef test_infer_lengths(lengths):\n    def test():\n        data = generate_data_with_zeros(lengths)\n        infered = infer_lengths(data, dim=1)\n        np.testing.assert_equal(infered.numpy(), lengths.numpy())\n\n    for _ in range(TRIALS):\n        test()\n\n\ndef test_infer_lengths_t_first(lengths):\n    def test():\n        data = generate_data_with_zeros(lengths)\n        data = data.transpose(0, 1)\n        infered = infer_lengths(data, dim=0)\n        np.testing.assert_equal(infered.numpy(), lengths.numpy())\n\n    for _ in range(TRIALS):\n        test()\n\n\ndef test_infer_legnths_multi_dim():\n    data = torch.rand(10, 11, 12)\n    with pytest.raises(ValueError):\n        infer_lengths(data)\n\n\ndef raw_loss(logits, labels, loss):\n    B, T, H = logits.size()\n    crit = loss(reduce=False, ignore_index=Offsets.PAD)\n    total_size = labels.nelement()\n    res = crit(logits.view(total_size, -1), labels.view(total_size))\n    return res.view(B, T)\n\n\ndef test_batch_sequence_loss(logits, labels):\n    loss = torch.nn.CrossEntropyLoss\n    raw = raw_loss(logits, labels, loss)\n    gold = torch.mean(torch.sum(raw, dim=1))\n    crit = SequenceLoss(LossFn=loss, avg=""batch"")\n    res = crit(logits, labels)\n    np.testing.assert_allclose(res.numpy(), gold.numpy(), rtol=1e-6)\n\n\ndef test_token_sequence_loss(logits, labels, lengths):\n    loss = torch.nn.CrossEntropyLoss\n    raw = raw_loss(logits, labels, loss)\n    gold = torch.sum(raw) / torch.sum(lengths).to(logits.dtype)\n    crit = SequenceLoss(LossFn=loss, avg=""token"")\n    res = crit(logits, labels)\n    np.testing.assert_allclose(res.numpy(), gold.numpy(), rtol=1e-6)\n\n\ndef test_gru_sequence_shapes(encoder_input):\n\n    z = GRUEncoderSequence(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    seq_len = torch.max(encoder_input[1]).item()\n    np.testing.assert_equal(out.shape, (B, seq_len, H))\n\n    z = BiGRUEncoderSequence(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    np.testing.assert_equal(out.shape, (B, seq_len, H))\n\n\n\ndef test_gru_hidden_shapes(encoder_input):\n\n    z = GRUEncoderHidden(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    np.testing.assert_equal(out.shape, (B, H))\n\n    z = BiGRUEncoderHidden(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    np.testing.assert_equal(out.shape, (B, H))\n\n\ndef test_gru_all_shapes(encoder_input):\n\n    seq_len = torch.max(encoder_input[1]).item()\n    z = GRUEncoderAll(C, H, L, batch_first=True)\n    seq, s = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, seq_len, H))\n    np.testing.assert_equal(s.shape, (L, B, H))\n\n    z = BiGRUEncoderAll(C, H, L, batch_first=True)\n    seq, s = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, seq_len, H))\n    np.testing.assert_equal(s.shape, (L, B, H))\n\n\ndef test_lstm_sequence_shapes(encoder_input):\n\n    z = LSTMEncoderSequence(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    seq_len = torch.max(encoder_input[1]).item()\n    np.testing.assert_equal(out.shape, (B, seq_len, H))\n\n    z = BiLSTMEncoderSequence(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    np.testing.assert_equal(out.shape, (B, seq_len, H))\n\n\ndef test_lstm_hidden_shapes(encoder_input):\n\n    z = LSTMEncoderHidden(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    np.testing.assert_equal(out.shape, (B, H))\n\n    z = BiLSTMEncoderHidden(C, H, L, batch_first=True)\n    out = z(encoder_input)\n    np.testing.assert_equal(out.shape, (B, H))\n\n\n\ndef test_lstm_shc_shapes(encoder_input):\n\n    seq_len = torch.max(encoder_input[1]).item()\n    z = LSTMEncoderSequenceHiddenContext(C, H, L, batch_first=True)\n    seq, (s1, s2) = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, seq_len, H))\n    np.testing.assert_equal(s1.shape, (B, H))\n    np.testing.assert_equal(s2.shape, (B, H))\n\n    z = BiLSTMEncoderSequenceHiddenContext(C, H, L, batch_first=True)\n    seq, (s1, s2) = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, seq_len, H))\n    np.testing.assert_equal(s1.shape, (B, H))\n    np.testing.assert_equal(s2.shape, (B, H))\n\n\ndef test_lstm_all_shapes(encoder_input):\n\n    seq_len = torch.max(encoder_input[1]).item()\n    z = LSTMEncoderAll(C, H, L, batch_first=True)\n    seq, (s1, s2) = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, seq_len, H))\n    np.testing.assert_equal(s1.shape, (L, B, H))\n    np.testing.assert_equal(s2.shape, (L, B, H))\n\n    z = BiLSTMEncoderAll(C, H, L, batch_first=True)\n    seq, (s1, s2) = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, seq_len, H))\n    np.testing.assert_equal(s1.shape, (L, B, H))\n    np.testing.assert_equal(s2.shape, (L, B, H))\n\n\ndef test_conv_shapes(encoder_input):\n\n    z = WithoutLength(ConvEncoder(C, H, 4))\n    seq = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, T, H))\n\n    z = WithoutLength(ConvEncoder(C, H, 3))\n    seq = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, T, H))\n\n    z = WithoutLength(ConvEncoderStack(C, H, 3, nlayers=3))\n    seq = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, T, H))\n\n    z = WithoutLength(ConvEncoderStack(C, H, 4, nlayers=3))\n    seq = z(encoder_input)\n    np.testing.assert_equal(seq.shape, (B, T, H))\n'"
tests/test_layers_tf1.py,0,"b'import pytest\nimport numpy as np\nfrom eight_mile.utils import get_version\ntf = pytest.importorskip(\'tensorflow\')\npytestmark = pytest.mark.skipif(get_version(tf) >= 2, reason=""TF2.X"")\nfrom eight_mile.utils import Offsets\nfrom eight_mile.tf.layers import infer_lengths\n\nB = 10\nT = 15\nTRIALS = 100\n\n@pytest.fixture\ndef lengths():\n    lengths = np.random.randint(1, T, size=(B,)).astype(np.int32)\n    return lengths\n\n\ndef generate_data_with_zeros(lengths):\n    data = np.random.randint(1, 100, (len(lengths), np.max(lengths))).astype(np.int32)\n    for i, length in enumerate(lengths):\n        data[i, length:] = 0\n        if length // 2 > 0:\n            extra_zeros = np.random.randint(0, length.item() - 1, size=((length // 2).item(),))\n            data[i, extra_zeros] = 0\n    return data\n\n\ndef test_infer_lengths(lengths):\n    def test():\n        tf.compat.v1.reset_default_graph()\n        with tf.compat.v1.Session() as sess:\n            data = generate_data_with_zeros(lengths)\n            data = tf.constant(data)\n            infered = infer_lengths(data, axis=1)\n            infered = sess.run(infered)\n        np.testing.assert_allclose(infered, lengths)\n\n    for _ in range(TRIALS):\n        test()\n\n\ndef test_infer_lengths_t_first(lengths):\n    def test():\n        tf.compat.v1.reset_default_graph()\n        with tf.compat.v1.Session() as sess:\n            data = generate_data_with_zeros(lengths)\n            print(data)\n            data = tf.constant(data)\n            data = tf.transpose(data)\n            print(data)\n            infered = infer_lengths(data, axis=0)\n            infered = sess.run(infered)\n            print(infered)\n        np.testing.assert_allclose(infered, lengths)\n\n    for _ in range(TRIALS):\n        test()\n\n\ndef test_infer_lengths_multi_dim():\n    data = tf.random.uniform((10, 11, 12))\n    with pytest.raises(ValueError):\n        infer_lengths(data, axis=1)\n'"
tests/test_layers_tf2.py,0,"b'import pytest\nimport numpy as np\nfrom eight_mile.utils import get_version\ntf = pytest.importorskip(\'tensorflow\')\npytestmark = pytest.mark.skipif(get_version(tf) < 2, reason=""TF1.X"")\nfrom eight_mile.utils import Offsets\nfrom eight_mile.tf.layers import infer_lengths\n\nB = 10\nT = 15\nTRIALS = 100\n\n@pytest.fixture\ndef lengths():\n    lengths = np.random.randint(1, T, size=(B,)).astype(np.int32)\n    return lengths\n\n\ndef generate_data_with_zeros(lengths):\n    data = np.random.randint(1, 100, (len(lengths), np.max(lengths))).astype(np.int32)\n    for i, length in enumerate(lengths):\n        data[i, length:] = 0\n        if length // 2 > 0:\n            extra_zeros = np.random.randint(0, length.item() - 1, size=((length // 2).item(),))\n            data[i, extra_zeros] = 0\n    return data\n\n\ndef test_infer_lengths(lengths):\n    def test():\n        data = generate_data_with_zeros(lengths)\n        data = tf.convert_to_tensor(data)\n        infered = infer_lengths(data, axis=1)\n        np.testing.assert_allclose(infered.numpy(), lengths)\n\n    for _ in range(TRIALS):\n        test()\n\n\ndef test_infer_lengths_t_first(lengths):\n    def test():\n        data = generate_data_with_zeros(lengths)\n        data = tf.convert_to_tensor(data)\n        data = tf.transpose(data)\n        infered = infer_lengths(data, axis=0)\n        np.testing.assert_allclose(infered.numpy(), lengths)\n\n    for _ in range(TRIALS):\n        test()\n\n\ndef test_infer_lengths_multi_dim():\n    data = tf.random.uniform((10, 11, 12))\n    with pytest.raises(ValueError):\n        infer_lengths(data, axis=1)\n'"
tests/test_lr_sched_tf1.py,0,"b'import os\nimport json\nimport pytest\nimport numpy as np\nfrom eight_mile.utils import get_version\n\ntf = pytest.importorskip(""tensorflow"")\npytestmark = pytest.mark.skipif(get_version(tf) >= 2, reason=""TF2.0"")\nfrom eight_mile.optz import (\n    create_lr_scheduler,\n    ConstantScheduler,\n    WarmupLinearScheduler,\n    CyclicLRScheduler,\n    PiecewiseDecayScheduler,\n    ZarembaDecayScheduler,\n    CosineDecayScheduler,\n    InverseTimeDecayScheduler,\n    ExponentialDecayScheduler,\n)\nimport numpy as np\n\n\n@pytest.fixture(scope=""module"")\ndef set_cpu():\n    os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n    yield\n    del os.environ[""CUDA_VISIBLE_DEVICES""]\n\n\nINIT_LR = 1.2\nNUM_STEPS = 1000\nCYCLIC_LR_CONFIG = {""lr_scheduler_type"": ""clr"", ""decay_steps"": 10, ""lr"": INIT_LR, ""max_lr"": INIT_LR * 3}\n\nINVTIME_LR_CONFIG = {""lr_scheduler_type"": ""invtime"", ""decay_rate"": 0.05, ""decay_steps"": 1, ""lr"": INIT_LR}\n\n\nEXP_LR_CONFIG = {""lr_scheduler_type"": ""exponential"", ""decay_rate"": 0.5, ""decay_steps"": 100, ""lr"": INIT_LR}\n\nLINEAR_WARMUP_LR_CONFIG = {""lr_scheduler_type"": ""warmup_linear"", ""warmup_steps"": 20, ""lr"": INIT_LR}\n\nCOMPOSITE_LR_CONFIG = LINEAR_WARMUP_LR_CONFIG.copy()\nCOMPOSITE_LR_CONFIG.update(EXP_LR_CONFIG)\nCOMPOSITE_LR_CONFIG[""lr_scheduler_type""] = [""warmup_linear"", ""exponential""]\n\nBOUNDS = [1, 2, 4, 10, 50]\nZAREMBA_DECAY_RATE = 1.2\nZAREMBA_DECAY_VALUES = [INIT_LR / (ZAREMBA_DECAY_RATE ** i) for i in range(len(BOUNDS) + 1)]\nPW_ZAREMBA_LR_CONFIG = {\n    ""lr_scheduler_type"": ""piecewise"",\n    ""boundaries"": BOUNDS,\n    ""values"": ZAREMBA_DECAY_VALUES,\n    ""lr"": INIT_LR,\n}\n\nZAREMBA_LR_CONFIG = {\n    ""lr_scheduler_type"": ""zaremba"",\n    ""boundaries"": BOUNDS,\n    ""decay_rate"": ZAREMBA_DECAY_RATE,\n    ""lr"": INIT_LR,\n}\n\nSGDR_LR_CONFIG = {""lr_scheduler_type"": ""sgdr"", ""first_decay_steps"": 100}\n\n\ndef test_zaremba():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(**ZAREMBA_LR_CONFIG)\n    bl_zaremba = ZarembaDecayScheduler(**ZAREMBA_LR_CONFIG)\n    lr_var = tf.compat.v1.placeholder(tf.float32, shape=(), name=""lr"")\n    step_var = tf.compat.v1.placeholder(tf.int32, shape=(), name=""step"")\n\n    gph = lr_sched(lr_var, step_var)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    lrs = []\n    lrs_bl = []\n    expect_lrs = []\n    current_lr = INIT_LR\n    for step in range(NUM_STEPS):\n        lr = sess.run(gph, feed_dict={lr_var: INIT_LR, step_var: step})\n        lr_bl = bl_zaremba(step)\n        lrs += [lr]\n        lrs_bl += [lr_bl]\n        if step in BOUNDS:\n            b = BOUNDS.index(step)\n            current_lr = ZAREMBA_DECAY_VALUES[b]\n        expect_lrs += [current_lr]\n    np.allclose(expect_lrs, lrs)\n    np.allclose(expect_lrs, lrs_bl)\n\n\ndef test_piecewise():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(**PW_ZAREMBA_LR_CONFIG)\n    bl_piecewise = PiecewiseDecayScheduler(**PW_ZAREMBA_LR_CONFIG)\n    lr_var = tf.compat.v1.placeholder(tf.float32, shape=(), name=""lr"")\n    step_var = tf.compat.v1.placeholder(tf.int32, shape=(), name=""step"")\n\n    gph = lr_sched(lr_var, step_var)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    lrs = []\n    lrs_bl = []\n    expect_lrs = []\n    current_lr = INIT_LR\n    for step in range(NUM_STEPS):\n        lr = sess.run(gph, feed_dict={lr_var: INIT_LR, step_var: step})\n        lrs += [lr]\n        lr_bl = bl_piecewise(step)\n        lrs_bl += [lr_bl]\n        if step in BOUNDS:\n            b = BOUNDS.index(step)\n            current_lr = ZAREMBA_DECAY_VALUES[b]\n        expect_lrs += [current_lr]\n    np.allclose(expect_lrs, lrs)\n    np.allclose(expect_lrs, lrs_bl)\n\n\ndef test_invtime():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(**INVTIME_LR_CONFIG)\n    bl_invtime = InverseTimeDecayScheduler(**INVTIME_LR_CONFIG)\n    decay_rate = INVTIME_LR_CONFIG[""decay_rate""]\n\n    lr_var = tf.compat.v1.placeholder(tf.float32, shape=(), name=""lr"")\n    step_var = tf.compat.v1.placeholder(tf.int32, shape=(), name=""step"")\n\n    gph = lr_sched(lr_var, step_var)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    lrs = []\n    lrs_bl = []\n    for step in range(NUM_STEPS):\n        lr = sess.run(gph, feed_dict={lr_var: INIT_LR, step_var: step})\n        lrs += [lr]\n        lr_bl = bl_invtime(step)\n        lrs_bl += [lr_bl]\n    inv_times = [INIT_LR / (1.0 + decay_rate * t) for t in range(NUM_STEPS)]\n    assert np.allclose(inv_times, lrs)\n    assert np.allclose(inv_times, lrs_bl)\n\n\ndef test_exp():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(**EXP_LR_CONFIG)\n    bl_exp = ExponentialDecayScheduler(**EXP_LR_CONFIG)\n    decay_rate = EXP_LR_CONFIG[""decay_rate""]\n\n    lr_var = tf.compat.v1.placeholder(tf.float32, shape=(), name=""lr"")\n    step_var = tf.compat.v1.placeholder(tf.int32, shape=(), name=""step"")\n\n    gph = lr_sched(lr_var, step_var)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    lrs = []\n    lrs_bl = []\n    for step in range(NUM_STEPS):\n        lr = sess.run(gph, feed_dict={lr_var: INIT_LR, step_var: step})\n        lrs += [lr]\n        lr_bl = bl_exp(step)\n        lrs_bl += [lr_bl]\n    inv_times = [(INIT_LR * decay_rate ** (t / 100.0)) for t in range(NUM_STEPS)]\n    assert np.allclose(inv_times, lrs)\n    assert np.allclose(inv_times, lrs_bl)\n\n\ndef test_linear_warmup():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(**LINEAR_WARMUP_LR_CONFIG)\n    warmup_steps = LINEAR_WARMUP_LR_CONFIG[""warmup_steps""]\n\n    lr_var = tf.compat.v1.placeholder(tf.float32, shape=(), name=""lr"")\n    step_var = tf.compat.v1.placeholder(tf.int32, shape=(), name=""step"")\n\n    gph = lr_sched(lr_var, step_var)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    lrs = []\n    for step in range(NUM_STEPS):\n        lr = sess.run(gph, feed_dict={lr_var: INIT_LR, step_var: step})\n        lrs += [lr]\n\n    expected_lrs = [INIT_LR * min(1.0, step / warmup_steps) for step in range(NUM_STEPS)]\n    assert np.allclose(expected_lrs, lrs)\n\n\ndef test_composite_warmup():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    warmup_steps = COMPOSITE_LR_CONFIG[""warmup_steps""]\n    decay_rate = EXP_LR_CONFIG[""decay_rate""]\n    with tf.compat.v1.Session() as sess:\n        lr_sched = create_lr_scheduler(**COMPOSITE_LR_CONFIG)\n        lr_var = tf.compat.v1.placeholder(tf.float32, name=""lr"")\n        step_var = tf.compat.v1.placeholder(tf.int32, name=""step"")\n\n        out = lr_sched(lr_var, step_var)\n        sess.run(tf.compat.v1.global_variables_initializer())\n\n        lrs = [sess.run(out, {lr_var: INIT_LR, step_var: step}) for step in range(NUM_STEPS)]\n\n        warmup_expected = [INIT_LR * min(1.0, step / warmup_steps) for step in range(NUM_STEPS)]\n        exp_expected = [(INIT_LR * decay_rate ** (t / 100.0)) for t in range(NUM_STEPS)]\n\n    for step in range(NUM_STEPS):\n        if step < warmup_steps:\n            assert np.allclose(lrs[step], warmup_expected[step])\n        else:\n            assert np.allclose(lrs[step], exp_expected[step - warmup_steps])\n\n\ndef test_constant():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(lr=INIT_LR, lr_scheduler_type=""default"")\n    bl_const = ConstantScheduler(lr=INIT_LR)\n\n    lr_var = tf.compat.v1.placeholder(tf.float32, shape=(), name=""lr"")\n    step_var = tf.compat.v1.placeholder(tf.int32, shape=(), name=""step"")\n\n    gph = lr_sched(lr_var, step_var)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    for step in range(NUM_STEPS):\n        lr = sess.run(gph, feed_dict={lr_var: INIT_LR, step_var: step})\n        assert np.isclose(INIT_LR, lr)\n        assert np.isclose(INIT_LR, bl_const(step))\n\n\ndef test_cyclic():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(**CYCLIC_LR_CONFIG)\n    bl_const = CyclicLRScheduler(**CYCLIC_LR_CONFIG)\n\n    lr_var = tf.compat.v1.placeholder(tf.float32, shape=(), name=""lr"")\n    step_var = tf.compat.v1.placeholder(tf.int32, shape=(), name=""step"")\n\n    gph = lr_sched(lr_var, step_var)\n    sess.run(tf.compat.v1.global_variables_initializer())\n\n    for step in range(NUM_STEPS):\n        lr = sess.run(gph, feed_dict={lr_var: INIT_LR, step_var: step})\n        lr_bl = bl_const(step)\n        assert np.isclose(lr, lr_bl)\n'"
tests/test_lr_sched_tf2.py,0,"b'import os\nimport json\nimport pytest\nimport numpy as np\nfrom eight_mile.utils import get_version\ntf = pytest.importorskip(""tensorflow"")\npytestmark = pytest.mark.skipif(get_version(tf) < 2, reason=""TF1.X"")\n\nfrom eight_mile.optz import (\n    create_lr_scheduler,\n    ConstantScheduler,\n    WarmupLinearScheduler,\n    CyclicLRScheduler,\n    PiecewiseDecayScheduler,\n    ZarembaDecayScheduler,\n    CosineDecayScheduler,\n    InverseTimeDecayScheduler,\n    ExponentialDecayScheduler,\n)\n\nimport numpy as np\n\n\n@pytest.fixture(scope=""module"")\ndef set_cpu():\n    os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n    yield\n    del os.environ[""CUDA_VISIBLE_DEVICES""]\n\n\nINIT_LR = 1.2\nNUM_STEPS = 1000\nCYCLIC_LR_CONFIG = {""lr_scheduler_type"": ""clr"", ""decay_steps"": 10, ""lr"": INIT_LR, ""max_lr"": INIT_LR * 3}\n\nINVTIME_LR_CONFIG = {""lr_scheduler_type"": ""invtime"", ""decay_rate"": 0.05, ""decay_steps"": 1, ""lr"": INIT_LR}\n\n\nEXP_LR_CONFIG = {""lr_scheduler_type"": ""exponential"", ""decay_rate"": 0.5, ""decay_steps"": 100, ""lr"": INIT_LR}\n\nLINEAR_WARMUP_LR_CONFIG = {""lr_scheduler_type"": ""warmup_linear"", ""warmup_steps"": 20, ""lr"": INIT_LR}\n\nCOMPOSITE_LR_CONFIG = LINEAR_WARMUP_LR_CONFIG.copy()\nCOMPOSITE_LR_CONFIG.update(EXP_LR_CONFIG)\nCOMPOSITE_LR_CONFIG[""lr_scheduler_type""] = [""warmup_linear"", ""exponential""]\n\nBOUNDS = [1, 2, 4, 10, 50]\nZAREMBA_DECAY_RATE = 1.2\nZAREMBA_DECAY_VALUES = [INIT_LR / (ZAREMBA_DECAY_RATE ** i) for i in range(len(BOUNDS) + 1)]\nPW_ZAREMBA_LR_CONFIG = {\n    ""lr_scheduler_type"": ""piecewise"",\n    ""boundaries"": BOUNDS,\n    ""values"": ZAREMBA_DECAY_VALUES,\n    ""lr"": INIT_LR,\n}\n\nZAREMBA_LR_CONFIG = {\n    ""lr_scheduler_type"": ""zaremba"",\n    ""boundaries"": BOUNDS,\n    ""decay_rate"": ZAREMBA_DECAY_RATE,\n    ""lr"": INIT_LR,\n}\n\nSGDR_LR_CONFIG = {""lr_scheduler_type"": ""sgdr"", ""first_decay_steps"": 100}\n\n\ndef test_zaremba():\n    from eight_mile.tf import optz\n\n    lr_sched = create_lr_scheduler(**ZAREMBA_LR_CONFIG)\n    bl_zaremba = ZarembaDecayScheduler(**ZAREMBA_LR_CONFIG)\n\n    lrs = []\n    lrs_bl = []\n    expect_lrs = []\n    current_lr = INIT_LR\n    for step in range(NUM_STEPS):\n        lr = lr_sched(step)\n        lr_bl = bl_zaremba(step)\n        lrs += [lr]\n        lrs_bl += [lr_bl]\n        if step in BOUNDS:\n            b = BOUNDS.index(step)\n            current_lr = ZAREMBA_DECAY_VALUES[b]\n        expect_lrs += [current_lr]\n    np.allclose(expect_lrs, lrs)\n    np.allclose(expect_lrs, lrs_bl)\n\n\ndef test_piecewise():\n    from eight_mile.tf import optz\n\n    lr_sched = create_lr_scheduler(**PW_ZAREMBA_LR_CONFIG)\n    bl_piecewise = PiecewiseDecayScheduler(**PW_ZAREMBA_LR_CONFIG)\n\n    lrs = []\n    lrs_bl = []\n    expect_lrs = []\n    current_lr = INIT_LR\n    for step in range(NUM_STEPS):\n        lr = lr_sched(step)\n        lrs += [lr]\n        lr_bl = bl_piecewise(step)\n        lrs_bl += [lr_bl]\n        if step in BOUNDS:\n            b = BOUNDS.index(step)\n            current_lr = ZAREMBA_DECAY_VALUES[b]\n        expect_lrs += [current_lr]\n    np.allclose(expect_lrs, lrs)\n    np.allclose(expect_lrs, lrs_bl)\n\n\ndef test_invtime():\n    from eight_mile.tf import optz\n\n    lr_sched = create_lr_scheduler(**INVTIME_LR_CONFIG)\n    bl_invtime = InverseTimeDecayScheduler(**INVTIME_LR_CONFIG)\n    decay_rate = INVTIME_LR_CONFIG[""decay_rate""]\n\n    lrs = []\n    lrs_bl = []\n    for step in range(NUM_STEPS):\n        lr = lr_sched(step)\n        lrs += [lr]\n        lr_bl = bl_invtime(step)\n        lrs_bl += [lr_bl]\n    inv_times = [INIT_LR / (1.0 + decay_rate * t) for t in range(NUM_STEPS)]\n    assert np.allclose(inv_times, lrs)\n    assert np.allclose(inv_times, lrs_bl)\n\n\ndef test_exp():\n    from eight_mile.tf import optz\n\n    lr_sched = create_lr_scheduler(**EXP_LR_CONFIG)\n    bl_exp = ExponentialDecayScheduler(**EXP_LR_CONFIG)\n    decay_rate = EXP_LR_CONFIG[""decay_rate""]\n\n    lrs = []\n    lrs_bl = []\n    for step in range(NUM_STEPS):\n        lr = lr_sched(step)\n        lrs += [lr]\n        lr_bl = bl_exp(step)\n        lrs_bl += [lr_bl]\n    inv_times = [(INIT_LR * decay_rate ** (t / 100.0)) for t in range(NUM_STEPS)]\n    assert np.allclose(inv_times, lrs)\n    assert np.allclose(inv_times, lrs_bl)\n\n\ndef test_linear_warmup():\n    from eight_mile.tf import optz\n\n    lr_sched = create_lr_scheduler(**LINEAR_WARMUP_LR_CONFIG)\n    warmup_steps = LINEAR_WARMUP_LR_CONFIG[""warmup_steps""]\n\n    lrs = []\n    for step in range(NUM_STEPS):\n        lr = lr_sched(step)\n        lrs += [lr]\n\n    expected_lrs = [INIT_LR * min(1.0, step / warmup_steps) for step in range(NUM_STEPS)]\n    assert np.allclose(expected_lrs, lrs)\n\n\ndef test_composite_warmup():\n    from eight_mile.tf import optz\n\n    warmup_steps = COMPOSITE_LR_CONFIG[""warmup_steps""]\n    decay_rate = EXP_LR_CONFIG[""decay_rate""]\n    lr_sched = create_lr_scheduler(**COMPOSITE_LR_CONFIG)\n    lrs = [lr_sched(step) for step in range(NUM_STEPS)]\n\n    warmup_expected = [INIT_LR * min(1.0, step / warmup_steps) for step in range(NUM_STEPS)]\n    exp_expected = [(INIT_LR * decay_rate ** (t / 100.0)) for t in range(NUM_STEPS)]\n\n    for step in range(NUM_STEPS):\n        if step < warmup_steps:\n            assert np.allclose(lrs[step], warmup_expected[step])\n        else:\n            assert np.allclose(lrs[step], exp_expected[step - warmup_steps])\n\n\ndef test_constant():\n    from eight_mile.tf import optz\n\n    lr_sched = create_lr_scheduler(lr=INIT_LR, lr_scheduler_type=""default"")\n    bl_const = ConstantScheduler(lr=INIT_LR)\n\n    for step in range(NUM_STEPS):\n        lr = lr_sched(step)\n        assert np.isclose(INIT_LR, lr)\n        assert np.isclose(INIT_LR, bl_const(step))\n\n\ndef test_cyclic():\n    from eight_mile.tf import optz\n\n    tf.compat.v1.reset_default_graph()\n    sess = tf.compat.v1.Session()\n\n    lr_sched = create_lr_scheduler(**CYCLIC_LR_CONFIG)\n    bl_const = CyclicLRScheduler(**CYCLIC_LR_CONFIG)\n\n    for step in range(NUM_STEPS):\n        lr = lr_sched(step)\n        lr_bl = bl_const(step)\n        assert np.isclose(lr, lr_bl)\n'"
tests/test_mead_tasks.py,0,"b""import pytest\nimport numpy as np\nfrom mead.tasks import Task\n\n\ndef test_get_min_f_loader_backoff():\n    name = 'a'\n    gold_val = np.random.randint(5, 10)\n    fake = np.random.randint(10, 100)\n    c = {\n        'features': [{'name': name}],\n        'loader': {\n            'min_f': gold_val\n        },\n        'preproc': {\n            'min_f': fake\n        },\n    }\n    gold = {name: gold_val}\n    cutoffs = Task._get_min_f(c)\n    assert cutoffs == gold\n\n\ndef test_get_min_f_preproc_backoff():\n    name = 'a'\n    gold_val = np.random.randint(5, 10)\n    c = {\n        'features': [{'name': name}],\n        'loader': {\n        },\n        'preproc': {\n            'min_f': gold_val\n        },\n    }\n    gold = {name: gold_val}\n    cutoffs = Task._get_min_f(c)\n    assert cutoffs == gold\n\n\ndef test_get_min_f_no_backoff():\n    name = 'a'\n    default = -1\n    c = {\n        'features': [{'name': name}],\n        'loader': {\n        },\n    }\n    gold = {name: default}\n    cutoffs = Task._get_min_f(c)\n    assert cutoffs == gold\n\n\ndef test_get_min_f_for_each_feature():\n    names = ['a', 'b']\n    gold_vals = np.random.randint(5, 10, size=len(names))\n    c = {\n        'features': [{'name': n, 'min_f': mf} for n, mf in zip(names, gold_vals)],\n        'loader': {},\n    }\n    gold = {n: m for n, m in zip(names, gold_vals)}\n    cutoffs = Task._get_min_f(c)\n    assert cutoffs == gold\n\n\ndef test_get_min_f_each_feat_preset():\n    names = np.random.choice(np.arange(1000), replace=False, size=np.random.randint(10, 100))\n    c = {\n        'features': [{'name': n for n in names}],\n        'loader': {},\n    }\n    cutoffs = Task._get_min_f(c)\n    for name in names:\n        assert name in names\n\n\n"""
tests/test_mead_utils.py,0,"b'import os\nimport string\nimport random\nfrom itertools import chain\nfrom collections import namedtuple\nfrom mock import patch, call\nimport pytest\nfrom baseline.utils import str2bool\nfrom mead.utils import (\n    KEYS,\n    convert_path,\n    get_output_paths,\n    get_export_params,\n    find_model_version,\n    get_dataset_from_key\n)\n\n\nCHARS = list(chain(string.ascii_letters, string.digits))\n\n\n@pytest.fixture\ndef file_name():\n    file_ = ""test_file""\n    with open(file_, ""w""):\n        pass\n    yield file_\n    os.remove(file_)\n\n\ndef test_no_loc():\n    file_name = ""test""\n    gold = os.path.realpath(os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."", ""mead"", file_name))\n    path = convert_path(file_name)\n    assert path == gold\n\n\ndef test_loc():\n    file_name = ""test""\n    start = ""/dev""\n    gold = os.path.join(start, file_name)\n    path = convert_path(file_name, start)\n    assert path == gold\n\n\ndef test_file_exists(file_name):\n    start = ""/dev""\n    gold = file_name\n    wrong = os.path.join(start, file_name)\n    path = convert_path(file_name, start)\n    assert path == gold\n    assert path != wrong\n\n\ndef rand_str(length=None, min_=3, max_=10):\n    if length is None:\n        length = random.randint(min_, max_)\n    return \'\'.join([random.choice(CHARS) for _ in range(length)])\n\n\ndef test_find_version():\n    gold = random.randint(5, 15)\n    items = list(map(str, range(gold)))\n    gold = str(gold)\n    with patch(\'mead.utils._listdir\') as list_patch:\n        list_patch.return_value = items\n        res = find_model_version(None)\n    assert res == gold\n\n\ndef test_find_version_gappy():\n    gold = random.randint(5, 15)\n    items = list(map(str, random.sample(range(gold), gold // 2)))\n    items.append(str(gold - 1))\n    gold = str(gold)\n    with patch(\'mead.utils._listdir\') as list_patch:\n        list_patch.return_value = items\n        res = find_model_version(None)\n    assert res == gold\n\n\ndef test_find_version_non_int():\n    gold = random.randint(5, 15)\n    items = list(map(str, random.sample(range(gold), gold // 2)))\n    items.append(str(gold - 1))\n    gold = str(gold)\n    items.insert(random.randint(0, len(items)), rand_str())\n    with patch(\'mead.utils._listdir\') as list_patch:\n        list_patch.return_value = items\n        res = find_model_version(None)\n    assert res == gold\n\n\ndef test_find_version_none():\n    gold = ""1""\n    items = []\n    with patch(\'mead.utils._listdir\') as list_patch:\n        list_patch.return_value = items\n        res = find_model_version(None)\n    assert res == gold\n\n\ndef test_find_version_non_int_only():\n    gold = ""1""\n    items = [rand_str for _ in range(random.randint(1, 3))]\n    with patch(\'mead.utils._listdir\') as list_patch:\n        list_patch = items\n        res = find_model_version(None)\n    assert res == gold\n\n\nnames = namedtuple(""d"", ""dir base proj name version"")\n\n\n@pytest.fixture\ndef d():\n    return make_data()\n\n\ndef make_data():\n    data = []\n    data.append(os.path.join(*[rand_str() for _ in range(random.randint(1, 4))]))\n    data.append(os.path.basename(data[-1]))\n    data.append(rand_str())\n    data.append(rand_str())\n    data.append(str(random.randint(1, 4)))\n    return names(*data)\n\n\n@pytest.fixture\ndef m_patch():\n    with patch(\'mead.utils.os.makedirs\') as m_patch:\n        yield m_patch\n\n\ndef test_get_output_paths_old_remote(d, m_patch):\n    gc = os.path.join(d.dir, \'client\', d.base, d.version)\n    gs = os.path.join(d.dir, \'server\', d.base, d.version)\n    c, s = get_output_paths(d.dir, None, None, d.version, True)\n    assert c == gc\n    assert s == gs\n\n\ndef test_get_output_paths_old(d, m_patch):\n    gc = os.path.join(d.dir, d.version)\n    gs = os.path.join(d.dir, d.version)\n    c, s = get_output_paths(d.dir, None, None, d.version, False)\n    assert c == gc\n    assert s == gs\n    assert c == s\n\n\ndef test_get_output_paths_project_name(d, m_patch):\n    g = os.path.join(d.dir, d.proj, d.name, d.version)\n    c, s = get_output_paths(d.dir, d.proj, d.name, d.version, False)\n    assert c == g\n    assert c == s\n\n\ndef test_get_output_paths_project_name_remote(d, m_patch):\n    gc = os.path.join(d.dir, \'client\', d.proj, d.name, d.version)\n    gs = os.path.join(d.dir, \'server\', d.proj, d.name, d.version)\n    c, s = get_output_paths(d.dir, d.proj, d.name, d.version, True)\n    assert c == gc\n    assert s == gs\n\n\ndef test_get_output_paths_project(d, m_patch):\n    g = os.path.join(d.dir, d.proj, d.version)\n    c, s = get_output_paths(d.dir, d.proj, None, d.version, False)\n    assert c == g\n    assert c == s\n\n\ndef test_get_output_paths_project_remote(d, m_patch):\n    gc = os.path.join(d.dir, ""client"", d.proj, d.version)\n    gs = os.path.join(d.dir, ""server"", d.proj, d.version)\n    c, s = get_output_paths(d.dir, d.proj, None, d.version, True)\n    assert c == gc\n    assert s == gs\n\n\ndef test_get_output_paths_name(d, m_patch):\n    g = os.path.join(d.dir, d.name, d.version)\n    c, s = get_output_paths(d.dir, None, d.name, d.version, False)\n    assert c == g\n    assert c == s\n\n\ndef test_get_output_paths_name_remote(d, m_patch):\n    gc = os.path.join(d.dir, ""client"", d.name, d.version)\n    gs = os.path.join(d.dir, ""server"", d.name, d.version)\n    c, s = get_output_paths(d.dir, None, d.name, d.version, True)\n    assert c == gc\n    assert s == gs\n\n\ndef test_get_output_paths_no_version(d, m_patch):\n    g = os.path.join(d.dir, d.proj, d.name, d.version)\n    with patch(\'mead.utils.find_model_version\') as v_patch:\n        v_patch.return_value = d.version\n        c, s = get_output_paths(d.dir, d.proj, d.name, None, False)\n    assert c == g\n    assert c == s\n\n\ndef test_get_output_paths_no_version_remote(d, m_patch):\n    gc = os.path.join(d.dir, ""client"", d.proj, d.name, d.version)\n    gs = os.path.join(d.dir, ""server"", d.proj, d.name, d.version)\n    with patch(\'mead.utils.find_model_version\') as v_patch:\n        v_patch.return_value = d.version\n        c, s = get_output_paths(d.dir, d.proj, d.name, None, True)\n    assert c == gc\n    assert s == gs\n\n\ndef test_get_output_paths_make_server(d, m_patch):\n    g = os.path.join(d.dir, d.proj, d.name, d.version)\n    _, _ = get_output_paths(d.dir, d.proj, d.name, d.version, False, True)\n    m_patch.assert_called_once_with(g)\n\n\ndef test_get_output_paths_no_make_server(d, m_patch):\n    _, _ = get_output_paths(d.dir, d.proj, d.name, d.version, False, False)\n    m_patch.assert_not_called()\n\n\ndef test_get_output_paths_make_server_remote(d, m_patch):\n    gs = os.path.join(d.dir, ""server"", d.proj, d.name, d.version)\n    gc = os.path.join(d.dir, ""client"", d.proj, d.name, d.version)\n    _, _ = get_output_paths(d.dir, d.proj, d.name, d.version, True, True)\n    assert m_patch.call_args_list == [call(gc), call(gs)]\n\n\ndef test_get_output_paths_no_make_server_remote(d, m_patch):\n    gc = os.path.join(d.dir, ""client"", d.proj, d.name, d.version)\n    _, _ = get_output_paths(d.dir, d.proj, d.name, d.version, True, False)\n    m_patch.assert_called_once_with(gc)\n\n\ndef test_get_export_input_override():\n    project = rand_str()\n    name = rand_str()\n    output_dir = os.path.join(rand_str(), rand_str())\n    model_version = str(random.randint(1, 5))\n    exporter_type = rand_str()\n    return_labels = random.choice([True, False])\n    is_remote = random.choice([True, False])\n    config = {\n        \'project\': rand_str(),\n        \'name\': rand_str(),\n        \'output_dir\': os.path.join(rand_str(), rand_str()),\n        \'model_version\': str(random.randint(1, 5)),\n        \'exporter_type\': rand_str(),\n        \'return_labels\': not return_labels,\n        \'is_remote\': not is_remote,\n    }\n    o, p, n, v, e, l, r = get_export_params(config, output_dir, project, name, model_version, exporter_type, return_labels, is_remote)\n    assert o == output_dir\n    assert p == project\n    assert n == name\n    assert v == model_version\n    assert e == exporter_type\n    assert l == return_labels\n    assert r == is_remote\n\n\ndef test_get_export_defaults():\n    o, p, n, v, e, l, r = get_export_params({})\n    assert o == \'./models\'\n    assert p is None\n    assert n is None\n    assert v is None\n    assert e == \'default\'\n    assert l is False\n    assert r is True\n\n\ndef test_get_export_config():\n    config = {\n        \'project\': rand_str(),\n        \'name\': rand_str(),\n        \'output_dir\': os.path.join(rand_str(), rand_str()),\n        \'model_version\': str(random.randint(1, 5)),\n        \'exporter_type\': rand_str(),\n        \'return_labels\': random.choice([\'true\', \'false\']),\n        \'is_remote\': random.choice([\'true\', \'false\']),\n    }\n    o, p, n, v, e, l, r = get_export_params(config)\n    assert o == config[\'output_dir\']\n    assert p == config[\'project\']\n    assert n == config[\'name\']\n    assert v == config[\'model_version\']\n    assert e == config[\'exporter_type\']\n    assert l == str2bool(config[\'return_labels\'])\n    assert r == str2bool(config[\'is_remote\'])\n\n\ndef test_get_export_type_in_config():\n    config = {\'type\': rand_str()}\n    _, _, _, _, e, _, _ = get_export_params(config)\n    assert e == config[\'type\']\n\n\ndef test_get_export_output_expanded():\n    output_dir = ""~/example""\n    gold_output_dir = os.path.expanduser(output_dir)\n    o, _, _, _, _, _, _ = get_export_params({}, output_dir)\n    assert o == gold_output_dir\n\n\ndef test_get_export_str2bool_called():\n    return_labels = random.choice([\'true\', \'false\'])\n    is_remote = random.choice([\'true\', \'false\'])\n    with patch(\'mead.utils.str2bool\') as b_patch:\n        _ = get_export_params({}, return_labels=return_labels, is_remote=is_remote)\n        assert b_patch.call_args_list == [call(return_labels), call(is_remote)]\n\n\ndef choice(in_, config, key):\n    c = random.randint(1, 3)\n    if c == 1:\n        return in_, in_\n    elif c == 2:\n        return None, config[key]\n    else:\n        del config[key]\n        return None, None\n\n\ndef test_get_export_params():\n    def test():\n        in_ = make_data()\n        c = make_data()\n        config = {\n            \'output_dir\': c.dir,\n            \'project\': c.proj,\n            \'name\': c.name,\n            \'model_version\': c.version,\n            \'exporter_type\': rand_str(),\n            \'return_labels\': random.choice([\'true\', \'false\']),\n            \'is_remote\': random.choice([\'true\', \'false\']),\n        }\n        in_output, gold_output = choice(in_.dir, config, \'output_dir\')\n        gold_output = \'./models\' if gold_output is None else gold_output\n        in_project, gold_project = choice(in_.proj, config, \'project\')\n        in_name, gold_name = choice(in_.name, config, \'name\')\n        in_version, gold_version = choice(in_.version, config, \'model_version\')\n        in_export, gold_export = choice(rand_str(), config, \'exporter_type\')\n        gold_export = gold_export if gold_export is not None else \'default\'\n        in_labels, gold_labels = choice(random.choice([\'true\', \'false\']), config, \'return_labels\')\n        gold_labels = str2bool(gold_labels) if gold_labels is not None else False\n        in_remote, gold_remote = choice(random.choice([\'true\', \'false\']), config, \'is_remote\')\n        gold_remote = str2bool(gold_remote) if gold_remote is not None else True\n        o, p, n, v, e, l, r = get_export_params(\n            config,\n            in_output,\n            in_project, in_name,\n            in_version,\n            in_export,\n            in_labels,\n            in_remote,\n        )\n        assert o == gold_output\n        assert p == gold_project\n        assert n == gold_name\n        assert v == gold_version\n        assert e == gold_export\n        assert l == gold_labels\n        assert r == gold_remote\n\n    for _ in range(100):\n        test()\n\n\ndef test_ensure_keys_are_tuples():\n    for key in KEYS:\n        assert isinstance(key, tuple)\n        for k in key:\n            assert isinstance(k, str)\n\ndef test_dataset_formats():\n    keys = {\'1\': 1,\n            \'1:1978\': 7,\n            \'2:1996\': 2,\n            \'2:20190327\': 3,\n            \'2:2019-03-28\': 42\n    }\n\n    # Test exact match first\n    assert get_dataset_from_key(\'1\', keys) == 1\n    # Test where not exact that we get last date\n    assert get_dataset_from_key(\'2\', keys) == 42\n    # Test where we do not get last date that we get an exception\n    try:\n        j = get_dataset_from_key(\'3\', keys)\n        assert j is None\n    except:\n        pass\n\n\ndef test_dataset_formats_with_hours():\n    keys = {\n        \'1:20200312\': 2,\n        \'1:20200312_0545\': 3,\n    }\n    assert get_dataset_from_key(\'1\', keys) == 3\n    keys[\'1:20200312_0435\'] = 4\n    assert get_dataset_from_key(\'1\', keys) == 3\n    keys[\'1:20200313\'] = 5\n    assert get_dataset_from_key(\'1\', keys) == 5\n    keys[\'1:2020-03-13_23-12\'] = 6\n    assert get_dataset_from_key(\'1\', keys) == 6\n\n\ndef test_dataset_hard_match():\n    """"""Test that we only match datasets that are exact match % date, ignore ones that have extra before or after.""""""\n    keys = {\n        \'prefix:postfix:20190101\': \'bad\',\n        \'prefix:20190103\': \'gold\',\n        \'prefix:postfix:20190402\': \'bad\',\n        \'pre-prefix:prefix:20190801\': \'bad\'\n    }\n    assert get_dataset_from_key(\'prefix\', keys) == \'gold\'\n\n\ndef test_dataset_handle_no_date():\n    """"""Test that we can handle a dataset we are looking at that doesn\'t have a date on it.""""""\n    keys = {\n        \'prefix:20190405\': \'bad\',\n        \'prefix:not-a-date\': \'bad\',\n        \'prefix:20190506\': \'gold\'\n    }\n    assert get_dataset_from_key(\'prefix\', keys) == \'gold\'\n'"
tests/test_model.py,0,"b'import pytest\nfrom baseline import model\nfrom mock import patch, MagicMock\nimport numpy as np\n\nFULL_CONFIG = {\n        ""mxlen"": 60,\n        ""mxwlen"": 40,\n        ""maxw"": 40,  # add this so this works for both classifiers and taggers\n    }\n\nNO_LENGTH_CONFIG = {\n        # ""mxlen"": 60,\n        # ""mxwlen"": 40,\n    }\n\nTOKENS = [\'this\', \'is\', \'a\', \'test\']\nCLASS_RESPONSE =  [[(\'test\', 1), (\'test\', 1)]]\nTAG_RESPONSE = [[np.array((1)),np.array((1)),np.array((1)),np.array((1))]]\n\ndef create_dummy_classifier(mxlen=None, mxwlen=None, zero_alloc=None):\n    """"""\n    fixture to return a dummy classifier.\n    """"""\n    m = model.ClassifierModel()\n    m.classify = MagicMock(name=\'classify_method\')\n    m.classify.return_value = CLASS_RESPONSE\n\n    if mxlen:\n        m.mxlen = mxlen\n    if mxwlen:\n        m.mxwlen = mxwlen\n    if zero_alloc:\n        m.zero_alloc = zero_alloc\n\n    return m\n\ndef create_dummy_tagger(mxlen=None, mxwlen=None, zero_alloc=None):\n    """"""\n    fixture to return a dummy tagger.\n    """"""\n    m = model.TaggerModel()\n    m.predict = MagicMock(name=\'predict_method\')\n    m.predict.return_value = TAG_RESPONSE\n\n    m.get_labels = MagicMock(name=\'get_labels_method\')\n    m.get_labels.return_value = {\'one\': 1, \'two\': 2, \'three\': 3, \'four\': 4}\n\n    if mxlen:\n        m.mxlen = mxlen\n    if mxwlen:\n        m.maxw = mxwlen\n    if zero_alloc:\n        m.zero_alloc = zero_alloc\n\n    return m\n\n# @patch(\'baseline.model.WordCharLength\')\n# def test_classify_config_settings(WordCharLength):\n#     """"""\n#     ensure that the classifier will read from the config in\n#     classify_text.\n#     """"""\n#     classifier = create_dummy_classifier()\n\n#     WordCharLength.predict.return_value = {}\n\n#     result = classifier.classify_text(TOKENS, **FULL_CONFIG)\n\n#     classifier.classify.assert_called_once()\n#     WordCharLength.assert_called_with(classifier, 60, 40, np.zeros)\n\n# @patch(\'baseline.model.WordCharLength\')\n# def test_classify_self_settings(WordCharLength):\n#     """"""\n#     ensure that the classifier will read from the classifier in\n#     classify_text.\n#     """"""\n\n#     classifier = create_dummy_classifier(mxlen=999, mxwlen=666)\n\n#     WordCharLength.predict.return_value = {}\n\n#     result = classifier.classify_text(TOKENS, **NO_LENGTH_CONFIG)\n\n#     classifier.classify.assert_called_once()\n#     WordCharLength.assert_called_with(classifier, 999, 666, np.zeros)\n\n# @patch(\'baseline.model.WordCharLength\')\n# def test_classify_token_settings(WordCharLength):\n#     """"""\n#     ensure that the classifier will read from the tokens in\n#     classify_text.\n#     """"""\n#     # mxlen and mxwlen not set in class. it should be determined from tokens.\n#     classifier = create_dummy_classifier(mxlen=None, mxwlen=None)\n\n#     WordCharLength.predict.return_value = {}\n\n#     result = classifier.classify_text(TOKENS, **NO_LENGTH_CONFIG)\n\n#     classifier.classify.assert_called_once()\n#     WordCharLength.assert_called_with(classifier, 4, 4, np.zeros)\n\n\n# @patch(\'baseline.model.WordCharLength\')\n# def test_tagger_config_settings(WordCharLength):\n#     """"""\n#     ensure that the tagger will read from the config in\n#     classify_text.\n#     """"""\n#     tagger = create_dummy_tagger()\n\n#     WordCharLength.predict.return_value = {\'length\': [4]}\n\n#     result = tagger.predict_text(TOKENS, **FULL_CONFIG)\n\n#     tagger.predict.assert_called_once()\n#     WordCharLength.assert_called_with(tagger, 60, 40, np.zeros)\n\n# @patch(\'baseline.model.WordCharLength\')\n# def test_tagger_self_settings(WordCharLength):\n#     """"""\n#     ensure that the classifier will read from the classifier in\n#     classify_text.\n#     """"""\n#     tagger = create_dummy_tagger(mxlen=999, mxwlen=666)\n\n#     WordCharLength.predict.return_value = {\'length\': [4]}\n\n#     result = tagger.predict_text(TOKENS, **NO_LENGTH_CONFIG)\n\n#     tagger.predict.assert_called_once()\n#     WordCharLength.assert_called_with(tagger, 999, 666, np.zeros)\n\n# @patch(\'baseline.model.WordCharLength\')\n# def test_tagger_token_settings(WordCharLength):\n#     """"""\n#     ensure that the classifier will read from the tokens in\n#     classify_text.\n#     """"""\n#     # mxlen and mxwlen not set in class. it should be determined from tokens.\n#     tagger = create_dummy_tagger(mxlen=None, mxwlen=None)\n\n#     WordCharLength.predict.return_value = {\'length\': [4]}\n\n#     result = tagger.predict_text(TOKENS, **NO_LENGTH_CONFIG)\n\n#     tagger.predict.assert_called_once()\n#     WordCharLength.assert_called_with(tagger, 4, 4, np.zeros)\n'"
tests/test_parallel_conv.py,0,"b'try:\n    import os\n    import random\n    import unittest\n    from mock import patch, MagicMock\n    import numpy as np\n    import tensorflow as tf\n    import pytest\n    from eight_mile.utils import get_version\n\n    pytestmark = pytest.mark.skipif(get_version(tf) >= 2, reason=""tf2.0"")\n    from baseline.tf.tfy import parallel_conv, char_word_conv_embeddings, highway_conns, skip_conns\nexcept ImportError:\n    raise unittest.SkipTest(""Failed to import tensorflow"")\n\n\nclass ParallelConvTest(tf.test.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n\n    @classmethod\n    def tearDownClass(cls):\n        del os.environ[""CUDA_VISIBLE_DEVICES""]\n\n    def setUp(self):\n        tf.reset_default_graph()\n        self.batchsz = random.randint(5, 65)\n        self.seqsz = random.randint(5, 11)\n        self.embedsz = random.randint(100, 301)\n        self.num_filt = random.randint(2, 6)\n        self.filtsz = set()\n        while len(self.filtsz) != self.num_filt:\n            self.filtsz.add(random.randint(2, 7))\n        self.filtsz = list(self.filtsz)\n        self.motsz = random.randint(128, 257)\n        self.nfeat_factor = random.randint(1, 4)\n        self.max_feat = random.randint(100, 301)\n        self.input = np.random.uniform(size=(self.batchsz, self.seqsz, self.embedsz)).astype(np.float32)\n        self.p = tf.compat.v1.placeholder(tf.float32, shape=(None, self.seqsz, self.embedsz))\n\n    def test_output_batch_shape_int_arg(self):\n        conv = parallel_conv(self.p, self.filtsz, self.embedsz, self.motsz)\n        with self.test_session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n            self.assertEqual(conv.eval({self.p: self.input}).shape[0], self.batchsz)\n\n    def test_output_batch_shape_list_arg(self):\n        motsz = [self.motsz] * len(self.filtsz)\n        conv = parallel_conv(self.p, self.filtsz, self.embedsz, motsz)\n        with self.test_session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n            self.assertEqual(conv.eval({self.p: self.input}).shape[0], self.batchsz)\n\n    def test_output_feature_shape_int_arg(self):\n        conv = parallel_conv(self.p, self.filtsz, self.embedsz, self.motsz)\n        with self.test_session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n            self.assertEqual(conv.eval({self.p: self.input}).shape[1], self.motsz * self.num_filt)\n\n    def test_output_feature_shape_list_arg(self):\n        motsz = [self.nfeat_factor * fsz for fsz in self.filtsz]\n        conv = parallel_conv(self.p, self.filtsz, self.embedsz, motsz)\n        with self.test_session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n            self.assertEqual(conv.eval({self.p: self.input}).shape[1], sum(motsz))\n\n    def test_shape_available_int(self):\n        """"""The previous tests test the shape of the actual output tensor. This\n        tests the output shape information available when building the graph.\n        When tf.squeeze is used on a tensor with a None dimension all shape info\n        is lost because it is unknown if the None dimension is 1 and should be\n        squeezed out. This cased error later because layers needed shape information\n        to set the right size. This test makes sure that needed size information\n        is present. This test would have caught the break in the classify method.\n        """"""\n        conv = parallel_conv(self.p, self.filtsz, self.embedsz, self.motsz)\n        conv_shape = conv.get_shape().as_list()\n        self.assertEqual(conv_shape, [None, self.motsz * len(self.filtsz)])\n\n    def test_shape_available_list(self):\n        """"""Same as the `test_shape_available_int` commnet.""""""\n        motsz = [self.nfeat_factor * fsz for fsz in self.filtsz]\n        conv = parallel_conv(self.p, self.filtsz, self.embedsz, motsz)\n        conv_shape = conv.get_shape().as_list()\n        self.assertEqual(conv_shape, [None, sum(motsz)])\n\n    def test_conv_called(self):\n        with patch(""baseline.tf.tfy.tf.nn.conv2d"") as conv_mock:\n            conv_mock.return_value = tf.zeros((self.batchsz, 1, self.seqsz, self.motsz))\n            conv = parallel_conv(self.p, self.filtsz, self.embedsz, self.motsz)\n            self.assertEqual(conv_mock.call_count, self.num_filt)\n\n    # def test_list_and_number_args_equal(self):\n    #     with tf.variable_scope(""TEST""):\n    #         conv1 = parallel_conv(self.p, self.filtsz, self.embedsz, self.motsz)\n    #     with tf.variable_scope(""TEST"", reuse=True):\n    #         conv2 = parallel_conv(self.p, self.filtsz, self.embedsz, [self.motsz] * len(self.filtsz))\n    #     with self.test_session() as sess:\n    #         sess.run(tf.compat.v1.global_variables_initializer())\n    #         np.testing.assert_allclose(conv1.eval({self.p: self.input}), conv2.eval({self.p: self.input}))\n\n    # @patch(\'baseline.tf.tfy.parallel_conv\')\n    # @patch(\'baseline.tf.tfy.skip_conns\')\n    # def test_char_word_call_correct(self, skip_mock, conv_mock):\n    #    conv_ret = MagicMock()\n    #    conv_mock.return_value = conv_ret\n    #    _, _ = char_word_conv_embeddings(self.p, self.filtsz, self.embedsz, self.motsz)\n    #    conv_mock.assert_called_once_with(self.p, self.filtsz, self.embedsz, self.motsz, tf.nn.tanh)\n    #    skip_mock.assert_called_once_with(conv_ret, self.motsz * len(self.filtsz), 1)\n\n    # @patch(\'baseline.tf.tfy.parallel_conv\')\n    # @patch(\'baseline.tf.tfy.highway_conns\')\n    # def test_char_word_var_fm_call_correct(self, skip_mock, conv_mock):\n    #    conv_ret = MagicMock()\n    #    conv_mock.return_value = conv_ret\n    #    nfeats = [min(self.nfeat_factor * fsz, self.max_feat) for fsz in self.filtsz]\n    #    _, _ = char_word_conv_embeddings(self.p, self.filtsz, self.embedsz, self.nfeat_factor, gating=highway_conns)\n    #    conv_mock.assert_called_once_with(self.p, self.filtsz, self.embedsz, nfeats, tf.nn.tanh)\n    #    skip_mock.assert_called_once_with(conv_ret, sum(nfeats), 2)\n\n\nif __name__ == ""__main__"":\n    tf.test.main()\n'"
tests/test_parse_extra_args.py,0,"b'import pytest\nfrom mead.utils import parse_extra_args\n\n\ndef test_all_base_name_appear():\n    base_names = [\'a\', \'b\']\n    reporting = parse_extra_args(base_names, [])\n    for name in base_names:\n        assert name in reporting\n\n\ndef test_all_special_names_grabbed():\n    base_names = [\'a\', \'b\']\n    special_names = [\'a:one\', \'a:two\']\n    gold_special_names = [\'one\', \'two\']\n    reporting = parse_extra_args(base_names, special_names)\n    for gold in gold_special_names:\n        assert gold in reporting[\'a\']\n\n\ndef test_nothing_if_no_special():\n    base_names = [\'a\', \'b\']\n    special_names = [\'a:one\', \'a:two\']\n    reporting = parse_extra_args(base_names, special_names)\n    assert {} == reporting[\'b\']\n\n\ndef test_special_names_multiple_bases():\n    base_names = [\'a\', \'b\']\n    special_names = [\'a:one\', \'b:two\']\n    reporting = parse_extra_args(base_names, special_names)\n    assert \'one\' in reporting[\'a\']\n    assert \'two\' not in reporting[\'a\']\n    assert \'two\' in reporting[\'b\']\n    assert \'one\' not in reporting[\'b\']\n\n\ndef test_special_shared_across_base():\n    base_names = [\'a\', \'b\']\n    special_names = [\'--b:one\', \'b\', \'--a:one\', \'a\']\n    reporting = parse_extra_args(base_names, special_names)\n    for name in base_names:\n        assert \'one\' in reporting[name]\n        assert reporting[name][\'one\'] == name\n\n\n# This depends on `argparse` atm which should be mocked out eventually\ndef test_values_are_grabbed():\n    base_names = [\'a\', \'b\']\n    special_names = [\'--a:xxx\', \'xxx\', \'--a:yyy\', \'yyy\', \'--b:zzz\', \'zzz\']\n    reporting = parse_extra_args(base_names, special_names)\n    for name in base_names:\n        for special, value in reporting[name].items():\n            assert special == value\n\ndef test_extra_things_ignored():\n    base_names = [\'b\']\n    special_names = [\'a:one\']\n    reporting = parse_extra_args(base_names, special_names)\n    assert {} == reporting[\'b\']\n\n\ndef test_no_base_names():\n    base_names = []\n    special_names = [""--visdom:name"", ""sst2""]\n    reporting = parse_extra_args(base_names, special_names)\n    assert {} == reporting\n'"
tests/test_pytorch_masks.py,8,"b'import pytest\n\ntorch = pytest.importorskip(""torch"")\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\nfrom eight_mile.pytorch.layers import sequence_mask, subsequent_mask\n\n\n@pytest.fixture\ndef lengths():\n    batch_size = np.random.randint(5, 10)\n    max_seq = np.random.randint(15, 20)\n    lengths = torch.LongTensor(np.random.randint(1, max_seq, size=[batch_size]))\n    seq_len = torch.max(lengths).item()\n    return batch_size, lengths, seq_len\n\n\ndef test_mask_shape(lengths):\n    bsz, lengths, seq_len = lengths\n    mask = sequence_mask(lengths)\n    assert mask.size(0) == bsz\n    assert mask.size(1) == seq_len\n\n\ndef test_mask_valid_locs(lengths):\n    bsz, lengths, seq_len = lengths\n    mask = sequence_mask(lengths)\n    np_mask = np.zeros((bsz, seq_len))\n    for i in range(bsz):\n        for j in range(seq_len):\n            if j < lengths.data[i]:\n                np_mask[i, j] = 1\n    np.testing.assert_allclose(mask.data.numpy(), np_mask)\n\n\ndef test_mask_mxlen(lengths):\n    bsz, lengths, seq_len = lengths\n    extra = np.random.randint(2, 11)\n    mask = sequence_mask(lengths, seq_len + extra)\n    np_mask = np.zeros((bsz, seq_len + extra))\n    for i in range(bsz):\n        for j in range(seq_len + extra):\n            if j < lengths.data[i]:\n                np_mask[i, j] = 1\n    np.testing.assert_allclose(mask.data.numpy(), np_mask)\n\n\ndef test_attention_masked_valid_probs(lengths):\n    bsz, lengths, seq_len = lengths\n    mask = sequence_mask(lengths)\n    scores = torch.rand(bsz, seq_len)\n    score_mask = scores.masked_fill(mask, -1e9)\n    attention_weights = F.softmax(score_mask, dim=1)\n    for row in attention_weights:\n        np.testing.assert_allclose(torch.sum(row).numpy(), 1.0, rtol=1e-5)\n\n\ndef test_attention_masked_ignores_pad(lengths):\n    bsz, lengths, seq_len = lengths\n    mask = sequence_mask(lengths)\n    scores = torch.rand(bsz, seq_len)\n    score_mask = scores.masked_fill(mask, -1e9)\n    attention_weights = F.softmax(score_mask, dim=1)\n    for row, length in zip(attention_weights, lengths):\n        if length.item() == seq_len:\n            continue\n        masked = row[: length.item()]\n        np.testing.assert_allclose(masked.data.numpy(), 0.0)\n\n\ndef test_seq_mask_valid_count(lengths):\n    bsz, lengths, _ = lengths\n    mask = sequence_mask(lengths)\n    gold = lengths.sum()\n    assert mask.sum() == gold.sum()\n\n\ndef test_subsequent_mask_shape():\n    T = np.random.randint(2, 50)\n    gold = (1, 1, T, T)\n    mask = subsequent_mask(T)\n    assert mask.shape == gold\n\n\ndef test_subsequent_mask_valid_count():\n    T = np.random.randint(4, 50)\n    gold = (T * (T + 1)) / 2\n    mask = subsequent_mask(T).numpy()\n    assert np.sum(mask) == gold\n\n\ndef test_subsequent_mask_valid_loc():\n    T = np.random.randint(4, 100)\n    mask = subsequent_mask(T).numpy().squeeze()\n\n    def test(T, mask):\n        i, j = np.random.randint(0, T, size=2)\n        if i < j:\n            assert mask[i, j] == 0\n        else:\n            assert mask[i, j] == 1\n\n    for _ in range(100):\n        test(T, mask)\n'"
tests/test_pytorch_transformer.py,11,"b'import copy\nimport pytest\nimport numpy as np\nfrom mock import MagicMock\n\ntorch = pytest.importorskip(""torch"")\nfrom eight_mile.pytorch.layers import (\n    SeqScaledDotProductAttention,\n    SeqDotProductAttention,\n    SeqDotProductRelativeAttention,\n    SeqScaledDotProductRelativeAttention,\n    sequence_mask,\n    subsequent_mask,\n)\n\n\n@pytest.fixture(autouse=True)\ndef no_grad():\n    with torch.no_grad():\n        yield\n\n\n@pytest.fixture\ndef qkv():\n    B, H, T, D = map(int, np.random.randint(5, 10, size=4))\n    q = torch.rand(B, H, T, D)\n    k = torch.rand(B, H, T, D)\n    v = torch.rand(B, H, T, D)\n    return q, k, v\n\n\n@pytest.fixture\ndef ra_inputs():\n    B, H, T, D = map(int, np.random.randint(5, 10, size=4))\n    q = torch.rand(B, H, T, D)\n    k = torch.rand(B, H, T, D)\n    v = torch.rand(B, H, T, D)\n    ek = torch.rand(T, T, D)\n    ev = torch.rand(T, T, D)\n    return q, k, v, ek, ev, None\n\n\ndef test_rel_attn_shapes(ra_inputs):\n    ra = SeqScaledDotProductRelativeAttention()\n    output = ra(ra_inputs)\n    assert output.shape == ra_inputs[0].shape\n\n\ndef test_sdpa_values(qkv):\n    sdpa = SeqScaledDotProductAttention(0.0)\n    attn_values(sdpa, qkv)\n\n\ndef test_dpa_values(qkv):\n    dpa = SeqDotProductAttention(0.0)\n    attn_values(dpa, qkv)\n\n\ndef attn_values(attn, qkv):\n    q, k, v = qkv\n    B, H, T, _ = q.shape\n    q = q.zero_()\n    res = attn((q, k, v, None))\n    res = res.numpy()\n    gold = v.numpy()\n    for b in range(B):\n        for h in range(H):\n            for t in range(T):\n                np.testing.assert_allclose(res[b, h, t, :], np.mean(gold, axis=2)[b, h, :], rtol=1e-5)\n\n\ndef test_sdpa_values_seq_mask(qkv):\n    sdpa = SeqScaledDotProductAttention(0.0)\n    attn_values_seq_mask(sdpa, qkv)\n\n\ndef test_dpa_values_seq_mask(qkv):\n    dpa = SeqDotProductAttention(0.0)\n    attn_values_seq_mask(dpa, qkv)\n\n\ndef attn_values_seq_mask(attn, qkv):\n    q, k, v = qkv\n    B, H, T, _ = q.shape\n    q = q.zero_()\n    lens = torch.from_numpy(np.random.randint(1, T, size=B))\n    mask = sequence_mask(lens, T).unsqueeze(1).unsqueeze(1)\n    res = attn((q, k, v, mask))\n    res = res.numpy()\n    gold = v.numpy()\n    for b in range(B):\n        for h in range(H):\n            for t in range(T):\n                np.testing.assert_allclose(\n                    res[b, h, t, :], np.mean(gold[:, :, : lens[b], :], axis=2)[b, h, :], atol=1e-5\n                )\n\n\ndef test_sdpa_values_sub_mask(qkv):\n    sdpa = SeqScaledDotProductAttention(0.0)\n    attn_values_sub_mask(sdpa, qkv)\n\n\ndef test_dpa_values_sub_mask(qkv):\n    dpa = SeqDotProductAttention(0.0)\n    attn_values_sub_mask(dpa, qkv)\n\n\ndef attn_values_sub_mask(attn, qkv):\n    q, k, v = qkv\n    B, H, T, _ = q.shape\n    q = q.zero_()\n    mask = subsequent_mask(T)\n    res = attn((q, k, v, mask))\n    res = res.numpy()\n    gold = v.numpy()\n    for b in range(B):\n        for h in range(H):\n            for t in range(T):\n                np.testing.assert_allclose(res[b, h, t, :], np.mean(gold[:, :, : t + 1, :], axis=2)[b, h, :], atol=1e-5)\n'"
tests/test_pytorch_variational_dropout.py,4,"b'import random\nimport pytest\nimport numpy as np\n\ntorch = pytest.importorskip(""torch"")\nfrom eight_mile.pytorch.layers import VariationalDropout\n\n\n@pytest.fixture\ndef input_():\n    SEQ_LEN = random.randint(20, 101)\n    BATCH_SIZE = random.randint(5, 21)\n    FEAT_SIZE = random.choice([128, 256, 300])\n    return torch.randn((SEQ_LEN, BATCH_SIZE, FEAT_SIZE))\n\n\n@pytest.fixture\ndef pdrop():\n    return random.choice(np.arange(0.2, 0.8, 0.1))\n\n\ndef test_dropout_same_across_seq(input_, pdrop):\n    """"""Test the dropout masks are the same across a sequence.""""""\n    vd = VariationalDropout(pdrop)\n    dropped = vd(input_)\n    first_mask = (dropped[0, :, :] == 0).detach().numpy()\n    for drop in dropped:\n        mask = drop == 0\n        np.testing.assert_allclose(first_mask, mask.detach().numpy())\n\n\ndef test_dropout_same_across_seq_batch_first(input_, pdrop):\n    """"""Test the dropout masks are the same across a sequence.""""""\n    vd = VariationalDropout(pdrop, batch_first=True)\n    dropped = vd(input_)\n    first_mask = (dropped[:, 0, :] == 0).detach().numpy()\n    for i in range(dropped.shape[1]):\n        mask = dropped[:, i] == 0\n        np.testing.assert_allclose(first_mask, mask.detach().numpy())\n\n\ndef test_dropout_probs(input_, pdrop):\n    """"""Test that approximately the right number of units are dropped.""""""\n    vd = VariationalDropout(pdrop)\n    dropped = vd(input_)\n    initial_non_zero = np.count_nonzero(input_.numpy())\n    dropped_non_zero = np.count_nonzero(dropped.detach().numpy())\n    np.testing.assert_allclose((dropped_non_zero / float(initial_non_zero)), (1 - pdrop), atol=1e-1)\n\n\ndef test_dropout_scaling(input_, pdrop):\n    """"""Test that the dropout layer scales correctly.""""""\n    vd = VariationalDropout(pdrop)\n    dropped = vd(input_)\n    re_dropped = input_.masked_fill(dropped == 0, 0)\n    dropped_np = dropped.detach().numpy()\n    re_dropped = re_dropped.numpy()\n    np.testing.assert_allclose(re_dropped * (1 / float(pdrop)), dropped_np)\n\n\ndef test_gradient_flow(input_, pdrop):\n    """"""Test that a gradient can flow through the Variational Dropout.""""""\n    input_.requires_grad_()\n    vd = VariationalDropout(pdrop)\n    dropped = vd(input_)\n    output = torch.mean(dropped)\n    loss = torch.pow(torch.sub(torch.Tensor([1.0]), output), 2)\n    assert input_.grad is None\n    loss.backward()\n    assert input_.grad is not None\n'"
tests/test_pytorch_weight_sharing.py,10,"b'import pytest\nimport numpy as np\n\ntorch = pytest.importorskip(""torch"")\nimport torch.nn as nn\nfrom eight_mile.pytorch.layers import pytorch_linear\n\n\nclass TiedWeights(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.tgt_embeddings = nn.Embedding(100, 10)  # vsz, dsz\n        self.preds = pytorch_linear(10, 100)  # hsz, output_sz\n        self.preds.weight = self.tgt_embeddings.weight  # tied weights\n\n    def forward(self, input_vec):\n        return self.preds(self.tgt_embeddings(input_vec))\n\n\ndef test_weight_tying():\n    model = TiedWeights()\n    model.train()\n\n    loss_fn = nn.MSELoss()\n\n    optim = torch.optim.Adam([p for p in model.parameters() if p.requires_grad])\n    steps = 0\n    epoch_loss, epoch_accuracy = 0, 0\n    for idx in range(100):\n        inputs, targets = random_ints(10), random_floats(10)\n\n        likelihood = model(inputs)\n\n        loss = loss_fn(likelihood, targets)\n\n        loss.backward()\n        optim.step()\n\n    assert torch.equal(model.tgt_embeddings.weight, model.preds.weight)\n\n\ndef test_weight_tying_cuda():\n    if not torch.cuda.is_available():\n        pytest.skip(""Cuda not available"")\n\n    model = TiedWeights().cuda()\n    model.train()\n\n    loss_fn = nn.MSELoss()\n\n    optim = torch.optim.Adam([p for p in model.parameters() if p.requires_grad])\n    steps = 0\n    epoch_loss, epoch_accuracy = 0, 0\n    for idx in range(100):\n        inputs, targets = random_ints(10), random_floats(10)\n        inputs = inputs.cuda()\n        targets = targets.cuda()\n\n        likelihood = model(inputs)\n\n        loss = loss_fn(likelihood, targets)\n\n        loss.backward()\n        optim.step()\n\n    assert torch.equal(model.tgt_embeddings.weight, model.preds.weight)\n\n\ndef random_ints(size):\n    return torch.tensor(np.random.randint(0, 100, size=size))\n\n\ndef random_floats(size):\n    return torch.rand(size, 100)\n    # return torch.tensor(np.random.randint(0, 100, size=size))\n'"
tests/test_read_files.py,0,"b'import os\nimport json\nimport mock\nimport pytest\nfrom eight_mile.utils import read_config_file, read_json, read_yaml, read_config_stream\n\n\n@pytest.fixture\ndef gold_data():\n    return {""a"": 1, ""b"": {""c"": 2}}\n\n\n@pytest.fixture\ndef env(gold_data):\n    env_name = ""TEST""\n    os.environ[env_name] = json.dumps(gold_data)\n    yield ""$"" + env_name\n    del os.environ[env_name]\n\n\ndata_loc = os.path.realpath(os.path.dirname(__file__))\ndata_loc = os.path.join(data_loc, ""test_data"")\n\n\ndef test_read_json(gold_data):\n    data = read_json(os.path.join(data_loc, ""test_json.json""))\n    assert data == gold_data\n\n\ndef test_read_json_default_value():\n    gold_default = {}\n    data = read_json(os.path.join(data_loc, ""not_there.json""))\n    assert data == gold_default\n\n\ndef test_read_json_given_default():\n    gold_default = ""default""\n    data = read_json(os.path.join(data_loc, ""not_there.json""), gold_default)\n    assert data == gold_default\n\n\ndef test_read_json_strict():\n    with pytest.raises(IOError):\n        read_json(os.path.join(""not_there.json""), strict=True)\n\n\ndef test_read_yaml(gold_data):\n    pytest.importorskip(""yaml"")\n    data = read_yaml(os.path.join(data_loc, ""test_yaml.yml""))\n    assert data == gold_data\n\n\ndef test_read_yaml_default_value():\n    pytest.importorskip(""yaml"")\n    gold_default = {}\n    data = read_yaml(os.path.join(data_loc, ""not_there.yml""))\n    assert data == gold_default\n\n\ndef test_read_yaml_given_default():\n    pytest.importorskip(""yaml"")\n    gold_default = ""default""\n    data = read_yaml(os.path.join(""not_there.yml""), gold_default)\n    assert data == gold_default\n\n\ndef test_read_yaml_strict():\n    pytest.importorskip(""yaml"")\n    with pytest.raises(IOError):\n        read_yaml(os.path.join(""not_there.yml""), strict=True)\n\n\ndef test_read_config_json_dispatch():\n    file_name = ""example.json""\n    with mock.patch(""eight_mile.utils.read_json"") as read_patch:\n        read_config_file(file_name)\n    read_patch.assert_called_once_with(file_name, strict=True)\n\n\ndef test_read_config_ymal_dispatch():\n    pytest.importorskip(""yaml"")\n    file_name = ""example.yml""\n    with mock.patch(""eight_mile.utils.read_yaml"") as read_patch:\n        read_config_file(file_name)\n    read_patch.assert_called_once_with(file_name, strict=True)\n\n\ndef test_read_config_stream_file():\n    file_name = os.path.join(data_loc, ""test_json.json"")\n    with mock.patch(""eight_mile.utils.read_config_file"") as read_patch:\n        read_config_stream(file_name)\n    read_patch.assert_called_once_with(file_name)\n\n\ndef test_read_config_stream_env(env, gold_data):\n    data = read_config_stream(env)\n    assert data == gold_data\n\n\ndef test_read_config_stream_str(gold_data):\n    input_ = json.dumps(gold_data)\n    data = read_config_stream(input_)\n    assert data == gold_data\n'"
tests/test_readers.py,0,"b'import os\nimport random\nfrom copy import deepcopy\nfrom collections import Counter\nimport pytest\nimport numpy as np\nfrom mock import MagicMock, patch, call\nfrom baseline.reader import (\n    _filter_vocab,\n    num_lines,\n    _read_from_col,\n    _build_vocab_for_col,\n    _check_lens,\n    TSVSeqLabelReader,\n    TSVParallelCorpusReader,\n    MultiFileParallelCorpusReader,\n)\n\n\n@pytest.fixture\ndef data():\n    keys = [\'1\', \'2\']\n    words = [\'a\', \'b\', \'c\']\n    vocab = {\n        \'1\': {\n            \'a\': 2,\n            \'b\': 4,\n            \'c\': 8,\n        },\n        \'2\': {\n            \'a\': 2,\n            \'b\': 4,\n            \'c\': 8,\n        },\n    }\n    return keys, words, vocab\n\n\nTEST_LOC = os.path.join(os.path.realpath(os.path.dirname(__file__)), \'test_data\')\n\n\ndef test_filters_one(data):\n    keys, words, vocab = data\n    gold = deepcopy(vocab)\n    min_f = {keys[0]: 5, keys[1]: 2}\n    vocab = _filter_vocab(vocab, min_f)\n    assert \'a\' not in vocab[keys[0]]\n    assert \'b\' not in vocab[keys[0]]\n    assert \'c\' in vocab[keys[0]]\n    assert vocab[keys[1]] == gold[keys[1]]\n\n\ndef test_filters_both(data):\n    keys, words, vocab = data\n    min_f = dict.fromkeys(keys, 6)\n    vocab = _filter_vocab(vocab, min_f)\n    assert \'a\' not in vocab[keys[0]] and \'a\' not in vocab[keys[1]]\n    assert \'b\' not in vocab[keys[0]] and \'b\' not in vocab[keys[1]]\n    assert \'c\' in vocab[keys[0]] and \'c\' in vocab[keys[1]]\n\n\ndef test_no_filters(data):\n    keys, words, vocab = data\n    min_f = dict.fromkeys(keys, -1)\n    with patch(\'baseline.reader.filter\') as filt_mock:\n        filt_mock.return_value = [(\'a\', 1)]\n        _ = _filter_vocab(vocab, min_f)\n    filt_mock.assert_not_called()\n\n\ndef test_one_filters(data):\n    keys, words, vocab = data\n    min_f = {keys[0]: -1, keys[1]: 1}\n    with patch(\'baseline.reader.filter\') as filt_mock:\n        filt_mock.return_value = [(\'a\', 1)]\n        _ = _filter_vocab(vocab, min_f)\n    filt_mock.assert_called_once()\n\n\ndef test_num_lines():\n    gold = random.randint(0, 100)\n    file_name = ""replace with a random name""\n    with patch(\'baseline.reader.codecs.open\') as open_patch:\n        file_mock = MagicMock()\n        iter_mock = MagicMock()\n        file_mock.__enter__.return_value = iter_mock\n        iter_mock.__iter__.return_value = range(gold)\n        open_patch.return_value = file_mock\n        lines = num_lines(file_name)\n        assert lines == gold\n        open_patch.assert_called_once_with(file_name, encoding=\'utf-8\', mode=\'r\')\n\n\ndef test_num_lines_closes_file():\n    gold = random.randint(0, 100)\n    file_name = ""replace with a random name""\n    with patch(\'baseline.reader.codecs.open\') as open_patch:\n        file_mock = MagicMock()\n        iter_mock = MagicMock()\n        file_mock.__enter__.return_value = iter_mock\n        iter_mock.__iter__.return_value = range(gold)\n        open_patch.return_value = file_mock\n        lines = num_lines(file_name)\n        file_mock.__exit__.assert_called_once()\n\n\ndef test_read_cols_single_col_file():\n    gold = [[\'one\'], [\'two\'], [\'three\'], [\'four\'], [\'five\']]\n    data = _read_from_col(0, [os.path.join(TEST_LOC, \'single_col.txt\')])\n    assert data == gold\n\n\ndef test_read_cols_multi_col_file():\n    gold = [[\'one2\'], [\'two2\'], [\'three2\'], [\'four2\'], [\'five2\']]\n    data = _read_from_col(1, [os.path.join(TEST_LOC, \'multi_col.txt\')])\n    assert data == gold\n\n\ndef test_read_cols_multi_word_col():\n    gold = [[\'one\', \'two\'], [\'three\', \'four\'], [\'five\', \'six\'], [\'seven\', \'eight\'], [\'nine\', \'ten\']]\n    data = _read_from_col(2, [os.path.join(TEST_LOC, \'multi_col.txt\')])\n    assert data == gold\n\n\ndef test_vocab_col_calls_counts():\n    dummy = [\'text\']\n    with patch(\'baseline.reader._read_from_col\') as read_patch:\n        read_patch.return_value = dummy\n        vects = {\'a\': MagicMock(), \'b\': MagicMock(), \'c\': MagicMock()}\n        _ = _build_vocab_for_col(0, None, vects)\n        for vect in vects.values():\n            vect.count.assert_called_once_with(dummy[0])\n\n\ndef test_vocab_col_calls_read():\n    file_name = \'fake\'\n    col = np.random.randint(0, 5)\n    with patch(\'baseline.reader._read_from_col\') as read_patch:\n        _ = _build_vocab_for_col(col, file_name, {})\n        read_patch.assert_called_once_with(col, file_name, r\'\\t\', r\'\\s\')\n\n\ndef test_vocab_col_uses_text():\n    file_name = \'fake\'\n    fake_text = [\'fake\', \'text\']\n    col = np.random.randint(0, 5)\n    with patch(\'baseline.reader._read_from_col\') as read_patch:\n        _ = _build_vocab_for_col(col, file_name, {}, fake_text)\n        read_patch.assert_not_called()\n\n\nclass VectMock(MagicMock):\n    def count(self, x):\n        return Counter(x)\n\n\ndef test_tsv_unstruct_build_vocab():\n    gold_vocab = {\'vect\': {\'a\': 3, \'b\': 2, \'c\': 1, \'d\': 1, \'e\': 1, \'f\': 1, \'g\': 1, \'h\': 1, \'i\': 1}}\n    gold_labels = {\'1\', \'2\', \'3\'}\n    file_name = \'tsv_unstruct_file.tsv\'\n    vocab, labels = TSVSeqLabelReader({\'vect\': VectMock()}).build_vocab(os.path.join(TEST_LOC, file_name))\n    assert vocab == gold_vocab\n    assert set(labels) == gold_labels\n\n\nGOLD_SOURCE = {\'a\': 3, \'c\': 2, \'e\': 1}\nGOLD_TARGET = {\'b\': 3, \'d\': 2, \'f\': 1, \'<GO>\': 3}\n\n\ndef test_tsv_parallel_build_vocab():\n    reader = TSVParallelCorpusReader(vectorizers={\'tgt\': VectMock(), \'src\': VectMock()})\n    src_vocab, tgt_vocab = reader.build_vocabs([os.path.join(TEST_LOC, \'tsv_parallel.tsv\')])\n    assert src_vocab[\'src\'] == GOLD_SOURCE\n    assert tgt_vocab == GOLD_TARGET\n\n\ndef test_parallel_multifile_build_vocab():\n    reader = MultiFileParallelCorpusReader(vectorizers={\'tgt\': VectMock(), \'src\': VectMock()}, pair_suffix=[\'1\', \'2\'])\n    files = os.path.join(TEST_LOC, \'multi_parallel\')\n    src_vocab, tgt_vocab = reader.build_vocabs([files])\n    assert src_vocab[\'src\'] == GOLD_SOURCE\n    assert tgt_vocab == GOLD_TARGET\n\n\ndef test_check_for_neg_one():\n    gold = set()\n    vects = {}\n    for i in range(np.random.randint(2, 10)):\n        vect = MagicMock()\n        if np.random.rand() > 0.5:\n            vect.mxlen = -1\n            gold.add(i)\n        else:\n            vect.mxlen = 100\n        vects[i] = vect\n    fails = _check_lens(vects)\n    assert fails == gold\n'"
tests/test_reporting_hooks.py,0,"b'import os\nimport string\nimport pytest\nfrom mock import patch\n\npytest.importorskip(""tensorboardX"")\nimport numpy as np\nimport baseline.reporting\nfrom baseline.reporting import TensorBoardReporting\nfrom baseline.reporting import ReportingHook\n\n\ndef random_str(len_=None, min_=5, max_=21):\n    if len_ is None:\n        len_ = np.random.randint(min_, max_)\n    choices = list(string.ascii_letters + string.digits)\n    return """".join([np.random.choice(choices) for _ in range(len_)])\n\n\n@pytest.fixture\ndef patches():\n    pid = np.random.randint(0, 10)\n    with patch(""baseline.reporting.os.getpid"") as pid_patch:\n        pid_patch.return_value = pid\n        with patch(""tensorboardX.SummaryWriter"") as write_patch:\n            yield pid_patch, pid, write_patch\n\n\ndef test_no_base_dir(patches):\n    p_patch, pid, w_patch = patches\n    log_dir = random_str()\n    _ = TensorBoardReporting(log_dir=log_dir, flush_secs=2)\n    gold = os.path.join(""."", log_dir, str(pid))\n    w_patch.assert_called_once_with(gold, flush_secs=2)\n\n\ndef test_relative_log_dir(patches):\n    p_patch, pid, w_patch = patches\n    log_dir = random_str()\n    base_dir = random_str()\n    _ = TensorBoardReporting(log_dir=log_dir, flush_secs=2, base_dir=base_dir)\n    gold = os.path.join(base_dir, log_dir, str(pid))\n    w_patch.assert_called_once_with(gold, flush_secs=2)\n\n\ndef test_user_log_dir(patches):\n    p_patch, pid, w_patch = patches\n    log_dir = random_str()\n    log_dir = os.path.join(""~"", log_dir)\n    base_dir = random_str()\n    _ = TensorBoardReporting(log_dir=log_dir, flush_secs=2, base_dir=base_dir)\n    gold = os.path.join(os.path.expanduser(log_dir), str(pid))\n    w_patch.assert_called_once_with(gold, flush_secs=2)\n\n\ndef test_absolute_log_dir(patches):\n    p_patch, pid, w_patch = patches\n    log_dir = random_str()\n    log_dir = os.path.join(""/example"", log_dir)\n    base_dir = random_str()\n    _ = TensorBoardReporting(log_dir=log_dir, flush_secs=2, base_dir=base_dir)\n    gold = os.path.join(log_dir, str(pid))\n    w_patch.assert_called_once_with(gold, flush_secs=2)\n\n\ndef test_no_run_dir(patches):\n    p_patch, pid, w_patch = patches\n    _ = TensorBoardReporting(flush_secs=2)\n    gold = os.path.join(""."", ""runs"", str(pid))\n    w_patch.assert_called_once_with(gold, flush_secs=2)\n\n\ndef test_run_dir(patches):\n    p_patch, pid, w_patch = patches\n    run_dir = random_str()\n    _ = TensorBoardReporting(flush_secs=2, run_dir=run_dir)\n    gold = os.path.join(""."", ""runs"", ""{}-{}"".format(run_dir, pid))\n    w_patch.assert_called_once_with(gold, flush_secs=2)\n\n\ndef test_infer_type_train():\n    hook = ReportingHook()\n    gold = ""STEP""\n    phase = ""Train""\n    tt = hook._infer_tick_type(phase, None)\n    assert tt == gold\n\n\ndef test_infer_type_test():\n    hook = ReportingHook()\n    gold = ""EPOCH""\n    phase = ""Test""\n    tt = hook._infer_tick_type(phase, None)\n    assert tt == gold\n\n\ndef test_infer_type_valid():\n    hook = ReportingHook()\n    gold = ""EPOCH""\n    phase = ""Valid""\n    tt = hook._infer_tick_type(phase, None)\n    assert tt == gold\n\n\ndef test_infer_type_override_train():\n    hook = ReportingHook()\n    gold = ""EPOCH""\n    phase = ""Train""\n    tt = hook._infer_tick_type(phase, gold)\n    assert tt == gold\n\n\ndef test_infer_type_override_valid():\n    hook = ReportingHook()\n    gold = ""STEP""\n    phase = ""Valid""\n    tt = hook._infer_tick_type(phase, gold)\n    assert tt == gold\n\n\ndef test_infer_type_override_test():\n    hook = ReportingHook()\n    gold = ""STEP""\n    phase = ""Test""\n    tt = hook._infer_tick_type(phase, gold)\n    assert tt == gold\n'"
tests/test_rnn_dropout.py,0,"b'import os\nimport pytest\n\npytest.skip(""This has been broken for a while, will fix soon, BL"", allow_module_level=True)\nimport numpy as np\n\ntf = pytest.importorskip(""tensorflow"")\nfrom baseline.tf.tfy import rnn_cell_w_dropout, lstm_cell_w_dropout\n\n\n@pytest.fixture(scope=""module"")\ndef set_cpu():\n    os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n    yield\n    del os.environ[""CUDA_VISIBLE_DEVICES""]\n\n\ndef test_static_dropout_lstm_cell():\n    with tf.device(""/cpu:0""):\n        sess = tf.compat.v1.Session()\n        x = np.random.randn(1, 10, 50).astype(np.float32)\n        with sess.graph.as_default():\n            with tf.variable_scope(""DropoutIsOn""):\n                rnn_drop_cell = lstm_cell_w_dropout(100, 0.9999999999, training=True)\n                rnn_drop, _ = tf.nn.dynamic_rnn(\n                    rnn_drop_cell, x, sequence_length=np.array([10], dtype=np.int), dtype=tf.float32\n                )\n            with tf.variable_scope(""DropoutIsOff""):\n                rnn_no_drop_cell = lstm_cell_w_dropout(100, 0.9999999999, training=False)\n                rnn_no_drop, _ = tf.nn.dynamic_rnn(\n                    rnn_no_drop_cell, x, sequence_length=np.array([10], dtype=np.int), dtype=tf.float32\n                )\n        sess.run(tf.compat.v1.global_variables_initializer())\n        out_ten = sess.run(rnn_drop)\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) < 20\n        out_ten = sess.run(rnn_no_drop)\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) > 20\n\n\ndef test_static_dropout_rnn_cell():\n    with tf.device(""/cpu:0""):\n        sess = tf.compat.v1.Session()\n        x = np.random.randn(1, 10, 50).astype(np.float32)\n        with sess.graph.as_default():\n            with tf.variable_scope(""DropoutIsOn""):\n                rnn_drop_cell = rnn_cell_w_dropout(100, 0.9999999999, ""gru"", training=True)\n                rnn_drop, _ = tf.nn.dynamic_rnn(\n                    rnn_drop_cell, x, sequence_length=np.array([10], dtype=np.int), dtype=tf.float32\n                )\n            with tf.variable_scope(""DropoutIsOff""):\n                rnn_no_drop_cell = rnn_cell_w_dropout(100, 0.9999999999, ""gru"", training=False)\n                rnn_no_drop, _ = tf.nn.dynamic_rnn(\n                    rnn_no_drop_cell, x, sequence_length=np.array([10], dtype=np.int), dtype=tf.float32\n                )\n        sess.run(tf.compat.v1.global_variables_initializer())\n        out_ten = sess.run(rnn_drop)\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) < 20\n        out_ten = sess.run(rnn_no_drop)\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) > 20\n\n\ndef test_placeholder_dropout_lstm_cell():\n    with tf.device(""/cpu:0""):\n        sess = tf.compat.v1.Session()\n        x = np.random.randn(1, 10, 50).astype(np.float32)\n        with sess.graph.as_default():\n            train_flag = tf.compat.v1.placeholder_with_default(False, shape=(), name=""TEST_TRAIN_FLAG"")\n            with tf.variable_scope(""DropoutMightBeOn""):\n                rnn_cell = lstm_cell_w_dropout(100, 0.9999999999, training=train_flag)\n                rnn, _ = tf.nn.dynamic_rnn(rnn_cell, x, sequence_length=np.array([10], dtype=np.int), dtype=tf.float32)\n\n        sess.run(tf.compat.v1.global_variables_initializer())\n        out_ten = sess.run(rnn, {train_flag: True})\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) < 20\n        out_ten = sess.run(rnn)\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) > 20\n\n\ndef test_placeholder_dropout_rnn_cell():\n    with tf.device(""/cpu:0""):\n        sess = tf.compat.v1.Session()\n        x = np.random.randn(1, 10, 50).astype(np.float32)\n        with sess.graph.as_default():\n            train_flag = tf.compat.v1.placeholder_with_default(False, shape=(), name=""TEST_TRAIN_FLAG"")\n            with tf.variable_scope(""DropoutMightBeOn""):\n                rnn_cell = rnn_cell_w_dropout(100, 0.9999999999, ""gru"", training=train_flag)\n                rnn, _ = tf.nn.dynamic_rnn(rnn_cell, x, sequence_length=np.array([10], dtype=np.int), dtype=tf.float32)\n\n        sess.run(tf.compat.v1.global_variables_initializer())\n        out_ten = sess.run(rnn, {train_flag: True})\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) < 20\n        out_ten = sess.run(rnn)\n        assert len(out_ten[np.nonzero(out_ten)].squeeze()) > 20\n'"
tests/test_sample.py,0,"b'import pytest\nimport numpy as np\nfrom baseline.utils import topk\n\n@pytest.fixture\ndef setup():\n    input_ = np.random.rand(100)\n    k = np.random.randint(2, 20)\n    top = np.random.choice(np.arange(len(input_)), size=k, replace=False)\n    add = np.max(input_) + np.arange(1, len(top) + 1)\n    input_[top] = add\n    return input_, top, k\n\n\ndef test_k_drawn(setup):\n    input_, top, k = setup\n    result = topk(k, input_)\n    assert len(result) == k\n\n\ndef test_k_are_correct(setup):\n    input_, top, k = setup\n    result = topk(k, input_)\n    for x in top:\n        assert x in result\n\n\ndef test_k_in_order(setup):\n    input_, top, k = setup\n    result = topk(k, input_)\n    start = -1e4\n    for x in top:\n        assert result[x] > start\n        start = result[x]\n\n\ndef test_k_values_are_correct(setup):\n    input_, top, k = setup\n    result = topk(k, input_)\n    for k, v in result.items():\n        assert v == input_[k]\n'"
tests/test_tf_ema.py,0,"b'import os\nimport pytest\nimport numpy as np\ntf = pytest.importorskip(\'tensorflow\')\nfrom eight_mile.utils import get_version\npytestmark = pytest.mark.skipif(get_version(tf) >= 2, reason=\'tf2.0\')\n\nfrom baseline.tf.tfy import _add_ema\n\n\n@pytest.fixture(scope=""module"")\ndef set_cpu():\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n    yield\n    del os.environ[\'CUDA_VISIBLE_DEVICES\']\n\n\nclass Model:\n    def __init__(self):\n        self.sess = tf.compat.v1.Session()\n        with self.sess.graph.as_default():\n            self.w = tf.Variable(100, dtype=tf.float32)\n            self.w2 = tf.Variable(12, dtype=tf.float32)\n            self.w3 = tf.Variable(64, dtype=tf.float32, trainable=False)\n            self.train_vars = [self.w, self.w2]\n            self.x = tf.compat.v1.placeholder(tf.float32, [None])\n            self.y = tf.compat.v1.placeholder(tf.float32, [None])\n            self.y_ = tf.multiply(self.x, self.w)\n\n            self.loss = tf.reduce_sum(tf.square(tf.subtract(self.y, self.y_)))\n            self.opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n\n\n@pytest.fixture\ndef model():\n    tf.reset_default_graph()\n    m = Model()\n    with m.sess.graph.as_default():\n       ema_op, load, restore = _add_ema(m, 0.99)\n       with tf.control_dependencies([ema_op]):\n            train_op = m.opt.minimize(m.loss)\n    yield m, train_op, load, restore\n    m.sess.close()\n\n\ndef test_ema_is_graph(model):\n    m, _, _, _ = model\n    for var in m.train_vars:\n        m.sess.graph.get_tensor_by_name(""{}/ExponentialMovingAverage:0"".format(var.name[:-2]))\n    with pytest.raises(KeyError):\n        m.sess.graph.get_tensor_by_name(""{}/ExponentialMovingAverage:0"".format(m.w3.name[:-2]))\n    assert True\n\n\ndef test_trainable_in_backup(model):\n    m, _, _, _ = model\n    for var in m.train_vars:\n        m.sess.graph.get_tensor_by_name(""BackupVariables/{}"".format(var.name))\n    assert True\n\n\ndef test_restore(model):\n    m, _, _, restore = model\n    m.sess.run(tf.compat.v1.global_variables_initializer())\n    old = m.sess.run(m.w)\n    m.sess.run(tf.assign(m.w, tf.constant(3.0)))\n    over = m.sess.run(m.w)\n    m.sess.run(restore)\n    new = m.sess.run(m.w)\n    assert old != over\n    assert over != new\n    assert old == new\n\n\ndef test_load_saves_before_write(model):\n    m, _, load, _ = model\n    m.sess.run(tf.compat.v1.global_variables_initializer())\n    old_backup = m.sess.run(m.sess.graph.get_tensor_by_name(""BackupVariables/Variable:0""))\n    m.sess.run(tf.assign(m.w, tf.constant(3.0)))\n    gold = m.sess.run(m.w)\n    m.sess.run(load)\n    new_backup = m.sess.run(m.sess.graph.get_tensor_by_name(""BackupVariables/Variable:0""))\n    assert old_backup != new_backup\n    assert new_backup == gold\n\n\n# def test_load_twice_does_not_overwirte(model):\n#     m, _, load, _ = model\n#     m.sess.run(tf.compat.v1.global_variables_initializer())\n#     m.sess.run(tf.assign(m.w, tf.constant(3.0)))\n#     gold = m.sess.run(m.w)\n#     m.sess.run(load)\n#     wrong = m.sess.run(m.w)\n#     m.sess.run(load)\n#     backup = m.sess.run(m.sess.graph.get_tensor_by_name(""BackupVariables/Variable:0""))\n#     assert backup != wrong\n#     assert backup == gold\n\n\ndef test_ema_update_on_step(model):\n    m, train_op, load, _ = model\n    m.sess.run(tf.compat.v1.global_variables_initializer())\n    m.sess.run([train_op], {m.x: [1], m.y: [0]})\n    m.sess.run([train_op], {m.x: [1], m.y: [0]})\n    w = m.sess.run(m.w)\n    m.sess.run(load)\n    new_w = m.sess.run(m.w)\n    with pytest.raises(AssertionError):\n        np.testing.assert_allclose(w, new_w)\n'"
tests/test_tf_transformer.py,0,"b'import os\nimport pytest\nimport numpy as np\n\ntf = pytest.importorskip(""tensorflow"")\nfrom eight_mile.utils import get_version\nfrom eight_mile.tf.layers import SeqDotProductAttention, SeqScaledDotProductAttention, subsequent_mask\n\n\n@pytest.fixture(scope=""module"")\ndef set_cpu():\n    os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n    yield\n    del os.environ[""CUDA_VISIBLE_DEVICES""]\n\n\n@pytest.fixture\ndef qkv():\n    with tf.device(""/cpu:0""):\n        dim = np.random.randint(5, 10, size=4)\n        q = tf.random.normal(shape=dim)\n        k = tf.random.normal(shape=dim)\n        v = tf.random.normal(shape=dim)\n    return q, k, v\n\n\ndef test_attn_value(qkv):\n    q, k, v = qkv\n    with tf.device(""/cpu:0""):\n        q = tf.zeros_like(q)\n        dot_product_attention = SeqDotProductAttention(0.0)\n        res = dot_product_attention((q, k, v, None))\n        if get_version(tf) < 2:\n            with tf.compat.v1.Session() as sess:\n                res, gold = sess.run([res, v])\n        else:\n            res, gold = res.numpy(), v.numpy()\n        B, H, T, _ = q.get_shape().as_list()\n        for b in range(B):\n            for h in range(H):\n                for t in range(T):\n                    np.testing.assert_allclose(res[b, h, t, :], np.mean(gold, axis=2)[b, h, :], atol=1e-5)\n\n\n@pytest.mark.skipif(get_version(tf) < 2, reason=""needs tf2"")\ndef test_attn_value_seq_mask(qkv):\n    q, k, v = qkv\n    with tf.device(""/cpu:0""):\n        B, H, T, _ = q.get_shape().as_list()\n        q = tf.zeros_like(q)\n        lens = np.random.randint(1, T, size=B).astype(np.int32)\n        tf_lens = tf.constant(lens)\n        mask = tf.expand_dims(tf.expand_dims(tf.sequence_mask(tf_lens, T, dtype=tf.float32), 1), 1)\n        dot_product_attention = SeqDotProductAttention(0.0)\n        res = dot_product_attention((q, k, v, mask))\n        res, gold = res.numpy(), v.numpy()\n        for b in range(B):\n            for h in range(H):\n                for t in range(T):\n                    np.testing.assert_allclose(\n                        res[b, h, t, :], np.mean(gold[:, :, : lens[b], :], axis=2)[b, h, :], atol=1e-5\n                    )\n\n\n@pytest.mark.skipif(get_version(tf) < 2, reason=""needs tf2"")\ndef test_attn_value_sub_mask(qkv):\n    q, k, v = qkv\n    with tf.device(""/cpu:0""):\n        B, H, T, _ = q.get_shape().as_list()\n        q = tf.zeros_like(q)\n        mask = subsequent_mask(T)\n        dot_product_attention = SeqDotProductAttention(0.0)\n        res = dot_product_attention((q, k, v, mask))\n        res, gold = res.numpy(), v.numpy()\n        for b in range(B):\n            for h in range(H):\n                for t in range(T):\n                    np.testing.assert_allclose(\n                        res[b, h, t, :], np.mean(gold[:, :, : t + 1, :], axis=2)[b, h, :], atol=1e-5\n                    )\n\n\ndef test_scaled_attn_value(qkv):\n    q, k, v = qkv\n    with tf.device(""/cpu:0""):\n        q = tf.zeros_like(q)\n        scaled_dot_product_attention = SeqScaledDotProductAttention(0.0)\n        res = scaled_dot_product_attention((q, k, v, None))\n        if get_version(tf) < 2:\n            with tf.compat.v1.Session() as sess:\n                res, gold = sess.run([res, v])\n        else:\n            res, gold = res.numpy(), v.numpy()\n        B, H, T, _ = q.get_shape().as_list()\n        for b in range(B):\n            for h in range(H):\n                for t in range(T):\n                    np.testing.assert_allclose(res[b, h, t, :], np.mean(gold, axis=2)[b, h, :], atol=1e-5)\n\n\n@pytest.mark.skipif(get_version(tf) < 2, reason=""needs tf2"")\ndef test_scaled_attn_value_seq_mask(qkv):\n    q, k, v = qkv\n    with tf.device(""/cpu:0""):\n        B, H, T, _ = q.get_shape().as_list()\n        q = tf.zeros_like(q)\n        lens = np.random.randint(1, T, size=B).astype(np.int32)\n        tf_lens = tf.constant(lens)\n        mask = tf.expand_dims(tf.expand_dims(tf.sequence_mask(tf_lens, T, dtype=tf.float32), 1), 1)\n        scaled_dot_product_attention = SeqScaledDotProductAttention(0.0)\n        res = scaled_dot_product_attention((q, k, v, mask))\n        res, gold = res.numpy(), v.numpy()\n        for b in range(B):\n            for h in range(H):\n                for t in range(T):\n                    np.testing.assert_allclose(\n                        res[b, h, t, :], np.mean(gold[:, :, : lens[b], :], axis=2)[b, h, :], atol=1e-5\n                    )\n\n\n@pytest.mark.skipif(get_version(tf) < 2, reason=""needs tf2"")\ndef test_scaled_attn_value_sub_mask(qkv):\n    q, k, v = qkv\n    with tf.device(""/cpu:0""):\n        B, H, T, _ = q.get_shape().as_list()\n        q = tf.zeros_like(q)\n        mask = subsequent_mask(T)\n        scaled_dot_product_attention = SeqScaledDotProductAttention(0.0)\n        res = scaled_dot_product_attention((q, k, v, mask))\n        res, gold = res.numpy(), v.numpy()\n        for b in range(B):\n            for h in range(H):\n                for t in range(T):\n                    np.testing.assert_allclose(\n                        res[b, h, t, :], np.mean(gold[:, :, : t + 1, :], axis=2)[b, h, :], atol=1e-5\n                    )\n'"
tests/test_tf_weight_sharing.py,0,"b'import os\nimport pytest\nimport numpy as np\ntf = pytest.importorskip(\'tensorflow\')\nfrom eight_mile.utils import get_version\npytestmark = pytest.mark.skipif(get_version(tf) >= 2, reason=\'tf2.0\')\n\nfrom baseline.tf.tfy import tie_weight\n\n\n@pytest.fixture(scope=""module"")\ndef set_cpu():\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n    yield\n    del os.environ[\'CUDA_VISIBLE_DEVICES\']\n\n\ndef test_sharing():\n    # For some reason I can\'t get this to stop trying to use the gpu which causes it to fail\n    with tf.device(\'/cpu:0\'):\n        input_ = tf.compat.v1.placeholder(tf.int32, shape=[None])\n        weight = tf.get_variable(""weight"", shape=[100, 200], initializer=tf.random_normal_initializer())\n        embed = tf.nn.embedding_lookup(weight, input_)\n\n\n        tie_shape = [weight.get_shape()[-1], weight.get_shape()[0]]\n        with tf.variable_scope(""Share"", custom_getter=tie_weight(weight, tie_shape)):\n            layer = tf.layers.Dense(100, use_bias=False, kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n            out = layer(embed)\n\n        loss = tf.reduce_mean(tf.square(out - 0))\n        train_op = tf.compat.v1.train.GradientDescentOptimizer(1).minimize(loss)\n\n        with tf.compat.v1.Session() as sess:\n            sess.run(tf.compat.v1.global_variables_initializer())\n            weights = sess.run([weight, layer.kernel])\n            np.testing.assert_allclose(weights[0], weights[1].T)\n            print(""** Weights Start the Same"")\n            for _ in range(100):\n                sess.run(train_op, {input_: np.random.randint(0, 100, size=10)})\n\n            weights = sess.run([weight, layer.kernel])\n            np.testing.assert_allclose(weights[0], weights[1].T)\n            print(""** Weights Stay the Same"")\n'"
tests/test_tlm_serialization.py,8,"b'import os\nimport math\nimport json\nimport pytest\nimport numpy as np\nfrom mock import patch, MagicMock\n\ntorch = pytest.importorskip(""torch"")\n\n\ndata_loc = os.path.realpath(os.path.dirname(__file__))\ndata_loc = os.path.join(data_loc, ""test_data"")\n\nfrom baseline.pytorch.lm import TransformerLanguageModel\nimport baseline.pytorch.embeddings\nimport baseline.embeddings\nimport torch\nfrom eight_mile.optz import *\nfrom eight_mile.pytorch.optz import *\nfrom eight_mile.pytorch.layers import TransformerEncoderStack as PytTransformerEncoderStack, Dense, subsequent_mask\nfrom eight_mile.pytorch.serialize import save_tlm_npz, load_tlm_npz\nimport numpy as np\n\n\nfile_loc = os.path.realpath(os.path.dirname(__file__))\n\n\ndef _call_model(m, inputs):\n    from eight_mile.pytorch.layers import sequence_mask\n    m.eval()\n    lengths = inputs.get(\'lengths\')\n    x = m.embeddings(inputs)\n    max_seqlen = x.shape[1]\n    mask = sequence_mask(lengths, max_seqlen).to(x.device).unsqueeze(1).unsqueeze(1)\n    return m.generator((x, mask))\n\n\ndef _round_trip(embed_type, rpr_k=None):\n    test_file = os.path.join(file_loc, ""test_data"", ""blah.npz"")\n    d_model = 40\n    vocab_x = {\'a\':1, \'aardvark\':100, \'beandip\':42, \'cheerio\':86, \'dumdum\':129, \'eoyre\':3}\n    embeddings = {}\n    vocabs = {\'x\': vocab_x}\n    src_x_embedding = baseline.embeddings.load_embeddings(\'x\',\n                                                          dsz=d_model,\n                                                          known_vocab=vocab_x,\n                                                          embed_type=embed_type)\n    embeddings[\'x\'] = src_x_embedding[\'embeddings\']\n\n    src_model = TransformerLanguageModel.create(embeddings,\n                                                hsz=d_model,\n                                                dropout=0.1,\n                                                gpu=False,\n                                                num_heads=4,\n                                                layers=2,\n                                                rpr_k=rpr_k,\n                                                src_keys=[\'x\'], tgt_key=\'x\')\n\n    save_tlm_npz(src_model, test_file)\n\n    dst_x_embedding = baseline.embeddings.load_embeddings(\'x\',\n                                                          dsz=d_model,\n                                                          known_vocab=vocab_x,\n                                                          embed_type=embed_type)\n    embeddings[\'x\'] = dst_x_embedding[\'embeddings\']\n    dst_model = TransformerLanguageModel.create(embeddings,\n                                                hsz=d_model,\n                                                dropout=0.1,\n                                                gpu=False,\n                                                num_heads=4,\n                                                layers=2,\n                                                rpr_k=rpr_k,\n                                                src_keys=[\'x\'], tgt_key=\'x\')\n    load_tlm_npz(dst_model, test_file)\n\n    B = 4\n    T = 7\n    a_batch = torch.randint(0, 9, (B, T)).long()\n    a_lengths = torch.randint(0, T, (B,)).long()\n    out_pyt1 = _call_model(src_model, {\'x\': a_batch, \'lengths\': a_lengths}).detach().numpy()\n    out_pyt2 = _call_model(dst_model, {\'x\': a_batch, \'lengths\': a_lengths}).detach().numpy()\n    return np.allclose(out_pyt1, out_pyt2, atol=1e-6)\n\n\ndef test_round_trip():\n    assert _round_trip(\'positional\')\n    assert _round_trip(\'learned-positional\')\n    assert _round_trip(\'default\', rpr_k=[3, 5])\n'"
tests/test_torchy.py,8,"b""import pytest\nimport numpy as np\ntorch = pytest.importorskip('torch')\nfrom baseline.utils import Offsets\nfrom baseline.pytorch.torchy import SequenceCriterion\n\nC = 10\nB = 50\nS = 20\n\n\n@pytest.fixture\ndef lengths():\n    lengths = torch.randint(1, S, size=(B,)).long()\n    return lengths\n\n\n@pytest.fixture\ndef logits(lengths):\n    logits = torch.rand(B, S, C)\n    for i, l in enumerate(lengths):\n        logits[i, l:, :] = 0\n    return logits\n\n\n@pytest.fixture\ndef labels(lengths):\n    lab = torch.randint(1, C, size=(B, S)).long()\n    for i, l in enumerate(lengths):\n        lab[i, l:] = 0\n    return lab\n\n\ndef raw_loss(logits, labels, loss):\n    B, T, H = logits.size()\n    crit = loss(reduce=False, ignore_index=Offsets.PAD)\n    total_size = labels.nelement()\n    res = crit(logits.view(total_size, -1), labels.view(total_size))\n    return res.view(B, T)\n\n\ndef test_batch_sequence_loss(logits, labels):\n    loss = torch.nn.CrossEntropyLoss\n    raw = raw_loss(logits, labels, loss)\n    gold = torch.mean(torch.sum(raw, dim=1))\n    crit = SequenceCriterion(LossFn=loss, avg='batch')\n    res = crit(logits, labels)\n    np.testing.assert_allclose(res.numpy(), gold.numpy(), rtol=1e-6)\n\n\ndef test_token_sequence_loss(logits, labels, lengths):\n    loss = torch.nn.CrossEntropyLoss\n    raw = raw_loss(logits, labels, loss)\n    gold = torch.sum(raw) / torch.sum(lengths).to(logits.dtype)\n    crit = SequenceCriterion(LossFn=loss, avg='token')\n    res = crit(logits, labels)\n    np.testing.assert_allclose(res.numpy(), gold.numpy(), rtol=1e-6)\n"""
tests/test_transition_masks.py,0,"b'import pytest\nfrom eight_mile.utils import transition_mask\n\nIOBv = {""<PAD>"": 0, ""<GO>"": 1, ""<EOS>"": 2, ""B-X"": 3, ""I-X"": 4, ""B-X-Y"": 5, ""I-X-Y"": 6, ""O"": 7}\n\nBIOv = IOBv\n\nIOBESv = {\n    ""<PAD>"": 0,\n    ""<GO>"": 1,\n    ""<EOS>"": 2,\n    ""B-X"": 3,\n    ""I-X"": 4,\n    ""E-X"": 5,\n    ""S-X"": 6,\n    ""B-X-Y"": 7,\n    ""I-X-Y"": 8,\n    ""E-X-Y"": 9,\n    ""S-X-Y"": 10,\n    ""O"": 11,\n}\n\n\n@pytest.fixture\ndef IOB():\n    return transition_mask(IOBv, ""IOB"", IOBv[""<GO>""], IOBv[""<EOS>""], IOBv[""<PAD>""])\n\n\n@pytest.fixture\ndef BIO():\n    return transition_mask(IOBv, ""BIO"", IOBv[""<GO>""], IOBv[""<EOS>""], IOBv[""<PAD>""])\n\n\n@pytest.fixture\ndef IOBES():\n    return transition_mask(IOBESv, ""IOBES"", IOBESv[""<GO>""], IOBESv[""<EOS>""], IOBESv[""<PAD>""])\n\n\ndef test_IOB_shape(IOB):\n    assert IOB.shape == (len(IOBv), len(IOBv))\n\n\ndef test_BIO_shape(BIO):\n    assert BIO.shape == (len(IOBv), len(IOBv))\n    mask = transition_mask(IOBv, ""IOB2"", IOBv[""<GO>""], IOBv[""<EOS>""], IOBv[""<PAD>""])\n    assert mask.shape == (len(IOBv), len(IOBv))\n\n\ndef test_IOBES_shape(IOBES):\n    assert IOBES.shape == (len(IOBESv), len(IOBESv))\n\n\ndef test_IOB_I_B_mismatch(IOB):\n    assert IOB[IOBv[""B-X""], IOBv[""I-X-Y""]] == 0\n\n\ndef test_ION_I_I_match(IOB):\n    assert IOB[IOBv[""I-X""], IOBv[""I-X""]] == 1\n\n\ndef test_IOB_I_I_mismatch(IOB):\n    assert IOB[IOBv[""I-X-Y""], IOBv[""I-X""]] == 1\n\n\ndef test_IOB_to_pad(IOB):\n    assert IOB[IOBv[""<PAD>""], IOBv[""O""]] == 1\n    assert IOB[IOBv[""<PAD>""], IOBv[""I-X""]] == 1\n    assert IOB[IOBv[""<PAD>""], IOBv[""B-X""]] == 1\n\n\ndef test_IOB_to_end(IOB):\n    assert IOB[IOBv[""<EOS>""], IOBv[""O""]] == 1\n    assert IOB[IOBv[""<EOS>""], IOBv[""I-X""]] == 1\n    assert IOB[IOBv[""<EOS>""], IOBv[""B-X""]] == 1\n\n\ndef test_BIO_from_start(BIO):\n    assert BIO[BIOv[""B-X""], BIOv[""<GO>""]] == 1\n    assert BIO[BIOv[""I-X""], BIOv[""<GO>""]] == 0\n    assert BIO[BIOv[""O""], BIOv[""<GO>""]] == 1\n\n\ndef test_IOBES_to_start(IOBES):\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""B-X""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""I-X""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""E-X""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""S-X""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""O""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""<PAD>""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""<GO>""]] == 0\n\n\ndef test_IOBES_from_end(IOBES):\n    assert IOBES[IOBESv[""B-X""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""I-X""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""S-X""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""O""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""<PAD>""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""<EOS>""]] == 0\n    assert IOBES[IOBESv[""<EOS>""], IOBESv[""<EOS>""]] == 0\n\n\ndef test_IOBES_from_pad(IOBES):\n    assert IOBES[IOBESv[""B-X""], IOBESv[""<PAD>""]] == 0\n    assert IOBES[IOBESv[""I-X""], IOBESv[""<PAD>""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""<PAD>""]] == 0\n    assert IOBES[IOBESv[""S-X""], IOBESv[""<PAD>""]] == 0\n    assert IOBES[IOBESv[""O""], IOBESv[""<PAD>""]] == 0\n    assert IOBES[IOBESv[""<GO>""], IOBESv[""<PAD>""]] == 0\n    assert IOBES[IOBESv[""<PAD>""], IOBESv[""<PAD>""]] == 1\n    assert IOBES[IOBESv[""<EOS>""], IOBESv[""<PAD>""]] == 1\n\n\ndef test_IOBES_O(IOBES):\n    assert IOBES[IOBESv[""B-X""], IOBESv[""O""]] == 1\n    assert IOBES[IOBESv[""I-X""], IOBESv[""O""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""O""]] == 0\n    assert IOBES[IOBESv[""S-X""], IOBESv[""O""]] == 1\n    assert IOBES[IOBESv[""O""], IOBESv[""O""]] == 1\n\n\ndef test_IOBES_B(IOBES):\n    assert IOBES[IOBESv[""I-X""], IOBESv[""B-X""]] == 1\n    assert IOBES[IOBESv[""E-X""], IOBESv[""B-X""]] == 1\n    assert IOBES[IOBESv[""I-X-Y""], IOBESv[""B-X""]] == 0\n    assert IOBES[IOBESv[""E-X-Y""], IOBESv[""B-X""]] == 0\n    assert IOBES[IOBESv[""S-X""], IOBESv[""B-X""]] == 0\n    assert IOBES[IOBESv[""B-X""], IOBESv[""B-X""]] == 0\n    assert IOBES[IOBESv[""O""], IOBESv[""B-X""]] == 0\n\n\ndef test_IOBES_I(IOBES):\n    assert IOBES[IOBESv[""I-X""], IOBESv[""I-X""]] == 1\n    assert IOBES[IOBESv[""E-X""], IOBESv[""I-X""]] == 1\n    assert IOBES[IOBESv[""I-X-Y""], IOBESv[""I-X""]] == 0\n    assert IOBES[IOBESv[""E-X-Y""], IOBESv[""I-X""]] == 0\n    assert IOBES[IOBESv[""S-X""], IOBESv[""I-X""]] == 0\n    assert IOBES[IOBESv[""B-X""], IOBESv[""I-X""]] == 0\n    assert IOBES[IOBESv[""O""], IOBESv[""I-X""]] == 0\n\n\ndef test_IOBES_from_E(IOBES):\n    assert IOBES[IOBESv[""I-X""], IOBESv[""E-X""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""E-X""]] == 0\n    assert IOBES[IOBESv[""S-X""], IOBESv[""E-X""]] == 1\n    assert IOBES[IOBESv[""B-X""], IOBESv[""E-X""]] == 1\n    assert IOBES[IOBESv[""O""], IOBESv[""E-X""]] == 1\n\n\ndef test_IOBES_to_E(IOBES):\n    assert IOBES[IOBESv[""E-X""], IOBESv[""B-X""]] == 1\n    assert IOBES[IOBESv[""E-X""], IOBESv[""I-X""]] == 1\n    assert IOBES[IOBESv[""E-X""], IOBESv[""E-X""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""B-X-Y""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""I-X-Y""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""E-X-Y""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""S-X""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""S-X-Y""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""O""]] == 0\n\n\ndef test_IOBES_S(IOBES):\n    assert IOBES[IOBESv[""B-X""], IOBESv[""S-X""]] == 1\n    assert IOBES[IOBESv[""I-X""], IOBESv[""S-X""]] == 0\n    assert IOBES[IOBESv[""E-X""], IOBESv[""S-X""]] == 0\n    assert IOBES[IOBESv[""S-X""], IOBESv[""S-X""]] == 1\n    assert IOBES[IOBESv[""O""], IOBESv[""S-X""]] == 1\n'"
tests/test_utils.py,6,"b'import os\nimport string\nimport random\nfrom copy import deepcopy\nfrom itertools import chain\nimport pytest\nimport numpy as np\nfrom mock import patch\nfrom eight_mile.utils import (\n    get_env_gpus,\n    idempotent_append,\n    parse_module_as_path,\n    to_numpy,\n    get_version,\n    remove_extensions,\n    split_extensions,\n)\n\n\n@pytest.fixture\ndef cuda_visible():\n    gpus = [""2"", ""4""]\n    os.environ[""CUDA_VISIBLE_DEVICES""] = "","".join(gpus)\n    yield gpus\n    del os.environ[""CUDA_VISIBLE_DEVICES""]\n\n\n@pytest.fixture\ndef nv_gpu():\n    gpus = [""5"", ""6""]\n    os.environ[""NV_GPU""] = "","".join(gpus)\n    yield gpus\n    del os.environ[""NV_GPU""]\n\n\n@pytest.fixture\ndef remove_envs():\n    os.environ.pop(""CUDA_VISIBLE_DEVICES"", None)\n    os.environ.pop(""NV_GPU"", None)\n\n\ndef test_visible(cuda_visible):\n    gpus = get_env_gpus()\n    assert gpus == cuda_visible\n\n\n# def test_nv_gpu(nv_gpu):\n#    gpus = get_env_gpus()\n#    assert gpus == nv_gpu\n\n\ndef test_visible_first(cuda_visible, nv_gpu):\n    gpus = get_env_gpus()\n    assert gpus != nv_gpu\n    assert gpus == cuda_visible\n\n\n# def test_none(remove_envs):\n#     gold = [\'0\']\n#     gpus = get_env_gpus()\n#     assert gpus == gold\n\n\ndef test_idempotent_add_missing():\n    element = random.randint(0, 5)\n    data = set(random.randint(0, 10) for _ in range(random.randint(5, 10)))\n    data.discard(element)\n    data = list(data)\n    assert element not in data\n    idempotent_append(element, data)\n    assert element in data\n    assert data[-1] == element\n\n\ndef test_idempotent_add_there():\n    element = random.randint(0, 5)\n    data = set(random.randint(0, 10) for _ in range(random.randint(5, 10)))\n    data.add(element)\n    data = list(data)\n    random.shuffle(data)\n    # Make sure element isn\'t at the end in this example\n    data.append(None)\n    assert element in data\n    element_idx = data.index(element)\n    data_len = len(data)\n    idempotent_append(element, data)\n    assert element in data\n    assert data.index(element) == element_idx\n    assert data[-1] != element\n    assert len(data) == data_len\n\n\ndef test_idempotent_add_last():\n    # Because the last tests forces not to be last this tests always forces last\n    element = random.randint(0, 5)\n    data = set(random.randint(0, 10) for _ in range(random.randint(5, 10)))\n    data.discard(element)\n    data = list(data)\n    data.append(element)\n    assert element in data\n    element_idx = data.index(element)\n    data_len = len(data)\n    idempotent_append(element, data)\n    assert element in data\n    assert data.index(element) == element_idx\n    assert data[-1] == element\n    assert len(data) == data_len\n\n\nCHARS = list(chain(string.ascii_letters, string.digits))\n\n\ndef rand_str(length=None, min_=3, max_=10):\n    if length is None:\n        length = random.randint(min_, max_)\n    return """".join([random.choice(CHARS) for _ in range(length)])\n\n\ndef test_parse_module_as_path_file():\n    file_base = rand_str()\n    file_ext = rand_str()\n    file_name = ""{}.{}"".format(file_base, file_ext)\n    n, d = parse_module_as_path(file_name)\n    assert n == file_base\n    assert d == """"\n\n\ndef test_parse_module_as_path_relative():\n    file_base = rand_str()\n    file_ext = rand_str()\n    file_name = ""{}.{}"".format(file_base, file_ext)\n    path = os.path.join(*[rand_str() for _ in range(random.randint(1, 5))])\n    gold_path = rand_str()\n    assert not os.path.isabs(path)\n    module_name = os.path.join(path, file_name)\n    with patch(""eight_mile.utils.os.path.realpath"") as real_patch:\n        with patch(""eight_mile.utils.os.path.expanduser"") as user_patch:\n            real_patch.return_value = gold_path\n            user_patch.return_value = path\n            n, d = parse_module_as_path(module_name)\n            real_patch.assert_called_once_with(path)\n    assert n == file_base\n    assert d == gold_path\n\n\ndef test_parse_module_as_path_absolute():\n    file_base = rand_str()\n    file_ext = rand_str()\n    file_name = ""{}.{}"".format(file_base, file_ext)\n    path = os.path.join(*[rand_str() for _ in range(random.randint(1, 5))])\n    path = ""/"" + path\n    assert os.path.isabs(path)\n    module_name = os.path.join(path, file_name)\n    with patch(""eight_mile.utils.os.path.realpath"") as real_patch:\n        with patch(""eight_mile.utils.os.path.expanduser"") as user_patch:\n            real_patch.return_value = path\n            user_patch.return_value = path\n            n, d = parse_module_as_path(module_name)\n            real_patch.assert_called_once_with(path)\n    assert n == file_base\n    assert d == path\n\n\ndef test_to_numpy_tf2():\n    tf = pytest.importorskip(\'tensorflow\')\n    if get_version(tf) < 2:\n        pytest.skip(""TF1.X"")\n    gold = np.random.rand(*np.random.randint(1, 10, size=np.random.randint(1, 5)))\n    tensor = tf.convert_to_tensor(gold)\n    np_ = to_numpy(tensor)\n    np.testing.assert_allclose(np_, gold)\n\n\ndef test_to_numpy_pyt_gpu():\n    torch = pytest.importorskip(\'torch\')\n    if not torch.cuda.is_available():\n        pytest.skip(""No GPU Found"")\n    gold = np.random.rand(*np.random.randint(1, 10, size=np.random.randint(1, 5)))\n    tensor = torch.from_numpy(gold).cuda()\n    np_ = to_numpy(tensor)\n    np.testing.assert_allclose(np_, gold)\n\ndef test_to_numpy_pyt_detch():\n    torch = pytest.importorskip(\'torch\')\n    gold = np.random.rand(*np.random.randint(1, 10, size=np.random.randint(1, 5)))\n    tensor = torch.from_numpy(gold)\n    tensor.requires_grad = True\n    np_ = to_numpy(tensor)\n    np.testing.assert_allclose(np_, gold)\n\ndef test_to_numpy_pyt_detch_gpu():\n    torch = pytest.importorskip(\'torch\')\n    if not torch.cuda.is_available():\n        pytest.skip(""No GPU Found"")\n    gold = np.random.rand(*np.random.randint(1, 10, size=np.random.randint(1, 5)))\n    tensor = torch.from_numpy(gold).cuda()\n    tensor.requires_grad = True\n    np_ = to_numpy(tensor)\n    np.testing.assert_allclose(np_, gold)\n\ndef test_to_numpy_pyt():\n    torch = pytest.importorskip(\'torch\')\n    gold = np.random.rand(*np.random.randint(1, 10, size=np.random.randint(1, 5)))\n    tensor = torch.from_numpy(gold)\n    np_ = to_numpy(tensor)\n    np.testing.assert_allclose(np_, gold)\n\n\ndef test_remove_ext():\n    exts = {""."" + rand_str() for _ in range(np.random.randint(2, 4))}\n    ext_list = list(exts)\n    gold = rand_str()\n    path = deepcopy(gold)\n    for _ in range(np.random.randint(1, 2)):\n        path += random.choice(ext_list)\n    res = remove_extensions(path, exts)\n    assert res == gold\n\n\ndef test_remove_ext_not_in_middle():\n    gold = ""example.bio.more-stuff""\n    exts = {"".bio""}\n    path = deepcopy(gold)\n    res = remove_extensions(path, exts)\n    assert res == gold\n\n\ndef test_split_ext():\n    exts = {""."" + rand_str() for _ in range(np.random.randint(2, 4))}\n    ext_list = list(exts)\n    gold_path = rand_str()\n    gold_exts = """".join(random.choice(ext_list) for _ in range(np.random.randint(1, 4)))\n    path = gold_path + gold_exts\n    path, ext = split_extensions(path, exts)\n    assert path == gold_path\n    assert ext == gold_exts\n\n\ndef test_split_ext_not_in_middle():\n    gold = ""example.bio.more-stuff""\n    exts = {"".bio""}\n    path = deepcopy(gold)\n    res, ext = split_extensions(path, exts)\n    assert res == gold\n    assert ext == \'\'\n'"
tests/test_vectorizers.py,0,"b'import string\nimport pytest\nimport numpy as np\nfrom baseline.utils import Offsets\nfrom baseline.vectorizers import Char1DVectorizer, Char2DVectorizer, TextNGramVectorizer, DictTextNGramVectorizer\n\n\n@pytest.fixture\ndef vocab():\n    vocab = {k: i for i, k in enumerate(Offsets.VALUES)}\n    vocab[\'<EOW>\'] = len(vocab)\n    for i, k in enumerate(string.ascii_lowercase, len(vocab)): vocab[k] = i\n    return vocab\n\n\ndef test_char_2d_shapes(vocab):\n    mxlen, mxwlen = np.random.randint(1, 100, size=2)\n    gold_shape = (mxlen, mxwlen)\n    vect = Char2DVectorizer(mxlen=mxlen, mxwlen=mxwlen)\n    res, _ = vect.run([\'\'], vocab)\n    assert res.shape == gold_shape\n\n\ndef test_char_2d_cuts_off_mxlen(vocab):\n    mxlen = 2; mxwlen = 4\n    input_ = [\'a\', \'b\', \'c\']\n    vect = Char2DVectorizer(mxlen=mxlen, mxwlen=mxwlen)\n    res, _ = vect.run(input_, vocab)\n    assert res.shape[0] == mxlen\n    for i, char in enumerate(input_[:mxlen]):\n        assert res[i, 0] == vocab[char]\n    values = set(res.flatten().tolist())\n    for char in input_[mxlen:]:\n        assert vocab[char] not in values\n\n\ndef test_char_2d_cuts_off_mxwlen(vocab):\n    mxlen = 2; mxwlen = 4\n    input_ = [\'aaaabbbb\', \'cccc\']\n    gold = np.array([[vocab[\'a\']] * mxwlen, [vocab[\'c\']] * mxwlen], dtype=int)\n    vect = Char2DVectorizer(mxlen=mxlen, mxwlen=mxwlen)\n    res, _ = vect.run(input_, vocab)\n    np.testing.assert_equal(res, gold)\n\n\ndef test_char_2d_valid_length(vocab):\n    mxlen, mxwlen = np.random.randint(3, 15, size=2)\n    my_len = np.random.randint(1, mxlen)\n    input_ = [\'a\'] * my_len\n    vect = Char2DVectorizer(mxlen=mxlen, mxwlen=mxwlen)\n    _, lens = vect.run(input_, vocab)\n    assert lens == my_len\n\n\ndef test_char_2d_valid_length_cutoff(vocab):\n    mxlen, mxwlen = np.random.randint(3, 15, size=2)\n    my_len = mxlen + np.random.randint(5, 10)\n    input_ = [\'a\'] * my_len\n    vect = Char2DVectorizer(mxlen=mxlen, mxwlen=mxwlen)\n    _, lens = vect.run(input_, vocab)\n    assert my_len > mxlen\n    assert lens == mxlen\n\n\ndef test_char_2d_run_values(vocab):\n    mxlen, mxwlen = np.random.randint(3, 15, size=2)\n    input_ = [chr(i + 97) * mxwlen for i in range(mxlen)]\n    vect = Char2DVectorizer(mxlen=mxlen, mxwlen=mxwlen)\n    res, _ = vect.run(input_, vocab)\n    for i, word in enumerate(input_):\n        for j, char in enumerate(word):\n            assert res[i, j] == vocab[char]\n\n\ndef test_char_1d_shape(vocab):\n    mxlen = np.random.randint(3, 15)\n    input_ = [\'a\']\n    vect = Char1DVectorizer(mxlen=mxlen)\n    res, _ = vect.run(input_, vocab)\n    assert res.shape == (mxlen,)\n\n\ndef test_char_1d_cut_off_mxlen(vocab):\n    mxlen = np.random.randint(3, 15)\n    extra = np.random.randint(5, 10)\n    input_ = [\'a\' * mxlen + \'b\' * extra]\n    vect = Char1DVectorizer(mxlen=mxlen)\n    res, _ = vect.run(input_, vocab)\n    assert res.shape == (mxlen,)\n    assert all(res == vocab[\'a\'])\n    assert vocab[\'b\'] not in res\n\n\ndef test_char_1d_no_eow(vocab):\n    del vocab[\'<EOW>\']\n    mxlen = np.random.randint(3, 15)\n    input_ = [\'a\']\n    vect = Char1DVectorizer(mxlen=mxlen)\n    res, _ = vect.run(input_, vocab)\n    assert res[0] == vocab[\'a\']\n    assert res[1] == Offsets.PAD\n\n\ndef test_text_ngrams():\n    tokens = [""The"", ""brown"", ""dog"", ""walked"", ""past"", ""the"", ""white"", ""spotted"", ""one"", "".""]\n\n    n = 3\n    l = 4\n\n    v = TextNGramVectorizer(filtsz=n)\n    v.mxlen = l\n    cnt = v.count([\'<PAD>\'] + tokens + [\'<PAD>\'])\n    vocab = {}\n    for i, word in enumerate(cnt.keys()):\n        vocab[word] = i\n\n    a, length = v.run(tokens, vocab)\n    assert np.allclose(a, np.arange(0, 4))\n    assert length == l\n\n    tokens = [{""text"": t} for t in tokens]\n\n    v = DictTextNGramVectorizer(filtsz=n)\n    v.mxlen = 100\n\n    a, length = v.run(tokens, vocab)\n    assert np.allclose(a[:length], np.arange(0, len(tokens)))\n'"
baseline/pytorch/__init__.py,2,b'from baseline.pytorch.torchy import *\nfrom baseline.pytorch.transformer import *\n'
baseline/pytorch/embeddings.py,6,"b'from baseline.embeddings import register_embeddings, create_embeddings\nfrom eight_mile.pytorch.embeddings import *\nfrom eight_mile.pytorch.serialize import load_tlm_npz, tlm_load_state_dict\nfrom eight_mile.utils import read_json, mime_type\nfrom baseline.vectorizers import load_bert_vocab\n\n\nclass PyTorchEmbeddingsModel(PyTorchEmbeddings):\n    """"""A subclass of embeddings layers to prep them for registration and creation via baseline.\n\n    In tensorflow this layer handles the creation of placeholders and things like that so the\n    embeddings layer can just be tensor in tensor out but in pytorch all it does is strip the\n    unused `name` input and register them.\n    """"""\n    def __init__(self, _=None, **kwargs):\n        super().__init__(**kwargs)\n\n    @classmethod\n    def create(cls, model, name, **kwargs):\n        kwargs.pop(""dsz"", None)\n        return cls(name, vsz=model.vsz, dsz=model.dsz, weights=model.weights, **kwargs)\n\n\n@register_embeddings(name=\'default\')\nclass LookupTableEmbeddingsModel(PyTorchEmbeddingsModel, LookupTableEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'char-conv\')\nclass CharConvEmbeddingsModel(PyTorchEmbeddingsModel, CharConvEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'char-lstm\')\nclass CharLSTMEmbeddingsModel(PyTorchEmbeddingsModel, CharLSTMEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'char-transformer\')\nclass CharTransformerEmbeddingsModel(PyTorchEmbeddingsModel, CharTransformerEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'positional\')\nclass PositionalLookupTableEmbeddingsModel(PyTorchEmbeddingsModel, PositionalLookupTableEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'learned-positional\')\nclass LearnedPositionalLookupTableEmbeddingsModel(PyTorchEmbeddingsModel, LearnedPositionalLookupTableEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'learned-positional-w-bias\')\nclass LearnedPositionalLookupTableEmbeddingsWithBiasModel(PyTorchEmbeddingsModel, LearnedPositionalLookupTableEmbeddingsWithBias):\n    pass\n\n\n@register_embeddings(name=\'bert-lookup-table-embeddings\')\nclass BERTLookupTableEmbeddingsModel(PyTorchEmbeddingsModel, BERTLookupTableEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'positional-char-conv\')\nclass PositionalCharConvEmbeddingsModel(PyTorchEmbeddingsModel, PositionalCharConvEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'learned-positional-char-conv\')\nclass LearnedPositionalCharConvEmbeddingsModel(PyTorchEmbeddingsModel, LearnedPositionalCharConvEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'positional-char-lstm\')\nclass PositionalCharLSTMEmbeddingsModel(PyTorchEmbeddingsModel, PositionalCharLSTMEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'learned-positional-char-lstm\')\nclass LearnedPositionalCharLSTMEmbeddingsModel(PyTorchEmbeddingsModel, LearnedPositionalCharLSTMEmbeddings):\n    pass\n\n\nclass TransformerLMEmbeddings(PyTorchEmbeddings):\n    """"""Support embeddings trained with the TransformerLanguageModel class\n\n    This method supports either subword or word embeddings, not characters\n\n    """"""\n    def __init__(self, **kwargs):\n        super().__init__()\n        # You dont actually have to pass this if you are using the `load_bert_vocab` call from your\n        # tokenizer.  In this case, a singleton variable will contain the vocab and it will be returned\n        # by `load_bert_vocab`\n        # If you trained your model with MEAD/Baseline, you will have a `*.json` file which would want to\n        # reference here\n        vocab_file = kwargs.get(\'vocab_file\')\n        if vocab_file and vocab_file.endswith(\'.json\'):\n            self.vocab = read_json(kwargs.get(\'vocab_file\'))\n        else:\n            self.vocab = load_bert_vocab(kwargs.get(\'vocab_file\'))\n        self.cls_index = self.vocab[\'[CLS]\']\n        self.vsz = max(self.vocab.values()) + 1\n        self.d_model = int(kwargs.get(\'dsz\', kwargs.get(\'d_model\', 768)))\n        self.init_embed(**kwargs)\n        self.proj_to_dsz = pytorch_linear(self.dsz, self.d_model) if self.dsz != self.d_model else _identity\n        self.init_transformer(**kwargs)\n\n    @property\n    def dsz(self):\n        return self.embeddings.output_dim\n\n    def embed(self, input):\n        return self.embeddings({\'x\': input})\n\n    def init_embed(self, **kwargs):\n        # If you are using BERT, you probably want to use either\n        # `learned-positional` with a token type feature\n        # or `learned-positional-w-bias` if you dont care about the token type\n        embed_type = kwargs.get(\'word_embed_type\', \'learned-positional\')\n        x_embedding = create_embeddings(vsz=self.vsz, dsz=self.d_model, embed_type=embed_type)\n\n        embeddings = {\'x\': x_embedding}\n        # This is for BERT support when we are using 2 features\n        #token_type_vsz = kwargs.get(\'token_type_vsz\')\n        #if token_type_vsz:\n        #    tt_embedding = LookupTableEmbeddings(vsz=token_type_vsz, dsz=self.dsz)\n        #    embeddings[\'tt\'] = tt_embedding\n        # For bert, make sure this is `sum-layer-norm`\n        reduction = kwargs.get(\'embeddings_reduction\', kwargs.get(\'reduction\', \'concat\'))\n        embeddings_dropout = kwargs.get(\'embeddings_dropout\', 0.1)\n        self.embeddings = EmbeddingsStack(embeddings, dropout_rate=embeddings_dropout, reduction=reduction)\n\n    def init_transformer(self, **kwargs):\n        num_layers = int(kwargs.get(\'layers\', 12))\n        num_heads = int(kwargs.get(\'num_heads\', 12))\n        pdrop = kwargs.get(\'dropout\', 0.1)\n        ff_pdrop = kwargs.get(\'ffn_dropout\', 0.1)\n        d_ff = int(kwargs.get(\'d_ff\', 3072))\n        d_k = kwargs.get(\'d_k\')\n        rpr_k = kwargs.get(\'rpr_k\')\n        layer_norms_after = kwargs.get(\'layer_norms_after\', False)\n        layer_norm_eps = kwargs.get(\'layer_norm_eps\', 1e-12)\n        activation = kwargs.get(\'activation\', \'gelu\')\n        self.transformer = TransformerEncoderStack(num_heads, d_model=self.d_model, pdrop=pdrop, scale=True,\n                                                   layers=num_layers, d_ff=d_ff, rpr_k=rpr_k, d_k=d_k,\n                                                   activation=activation,\n                                                   ffn_pdrop=ff_pdrop,\n                                                   layer_norms_after=layer_norms_after, layer_norm_eps=layer_norm_eps)\n        self.mlm = kwargs.get(\'mlm\', False)\n        self.finetune = kwargs.get(\'finetune\', True)\n\n    def _model_mask(self, nctx):\n        """"""This function creates the mask that controls which token to be attended to depending on the model. A causal\n        LM should have a subsequent mask; and a masked LM should have no mask.""""""\n        if self.mlm:\n            return torch.ones((1, 1, nctx, nctx), dtype=torch.long)\n        else:\n            return subsequent_mask(nctx)\n\n    def forward(self, x):\n        # the following line masks out the attention to padding tokens\n        input_mask = torch.zeros(x.shape, device=x.device, dtype=torch.long).masked_fill(x != 0, 1).unsqueeze(1).unsqueeze(1)\n        # the following line builds mask depending on whether it is a causal lm or masked lm\n        input_mask = input_mask & self._model_mask(x.shape[1]).type_as(input_mask)\n        embedding = self.embed(x)\n        embedding = self.proj_to_dsz(embedding)\n        transformer_out = self.transformer((embedding, input_mask))\n        z = self.get_output(x, transformer_out)\n        return z\n\n    def get_output(self, inputs, z):\n        return z if self.finetune else z.detach()\n\n    def get_vocab(self):\n        return self.vocab\n\n    def get_vsz(self):\n        return self.vsz\n\n    def get_dsz(self):\n        return self.d_model\n\n    @classmethod\n    def load(cls, embeddings, **kwargs):\n        c = cls(""tlm-words-embed"", **kwargs)\n\n        if embeddings.endswith(\'.bin\'):\n            # HuggingFace checkpoint, convert on the fly\n            from eight_mile.pytorch.serialize import load_tlm_transformers_bin, BERT_HF_FT_LAYER_MAP\n            unmatch = load_tlm_transformers_bin(c, embeddings, replace_layers=BERT_HF_FT_LAYER_MAP)\n            if unmatch[\'missing\'] or unmatch[\'unexpected\']:\n                raise Exception(""Unable to load the HuggingFace checkpoint"")\n        if mime_type(embeddings) == \'application/zip\':\n            load_tlm_npz(c, embeddings)\n        else:\n            tlm_load_state_dict(c, embeddings)\n        return c\n\n\n@register_embeddings(name=\'tlm-words-embed\')\nclass TransformerLMEmbeddingsModel(PyTorchEmbeddingsModel, TransformerLMEmbeddings):\n    """"""Register embedding model for usage in mead""""""\n    pass\n\n\ndef _identity(x):\n    return x\n\n\ndef _mean_pool(inputs, embeddings):\n    mask = (inputs != 0)\n    seq_lengths = mask.sum(1).float()\n    embeddings = embeddings.masked_fill(mask.unsqueeze(-1) == False, 0.)\n    return embeddings.sum(1)/seq_lengths.unsqueeze(-1)\n\n\ndef _max_pool(inputs, embeddings):\n    mask = (inputs != 0)\n    embeddings = embeddings.masked_fill(mask.unsqueeze(-1) == False, 0.)\n    return torch.max(embeddings, 1, False)[0]\n\n\n@register_embeddings(name=\'tlm-words-embed-pooled\')\nclass TransformerLMPooledEmbeddingsModel(TransformerLMEmbeddingsModel):\n\n    def __init__(self, name, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n        pooling = kwargs.get(\'pooling\', \'cls\')\n        if pooling == \'max\':\n            self.pooling_op = _max_pool\n        elif pooling == \'mean\':\n            self.pooling_op = _mean_pool\n        elif pooling == \'sqrt_length\':\n            self.pooling_op = self._sqrt_length_pool\n        else:\n            self.pooling_op = self._cls_pool\n\n    def _sqrt_length_pool(self, inputs, embeddings):\n        mask = (inputs != 0)\n        lengths = mask.sum(1)\n        sqrt_length = lengths.float()\n        embeddings = embeddings.masked_fill(mask.unsqueeze(-1) == False, 0.)\n        embeddings = embeddings.sum(1) * sqrt_length.sqrt().unsqueeze(-1)\n        return embeddings\n\n    def _cls_pool(self, inputs, tensor):\n        # Would prefer\n        # tensor[inputs == self.cls_index]\n        # but ONNX export fails\n        B = tensor.shape[0]\n        mask = (inputs == self.cls_index).unsqueeze(-1).expand_as(tensor)\n        pooled = tensor.masked_select(mask).view(B, -1)\n        return pooled\n\n    def get_output(self, inputs, z):\n        z = self.pooling_op(inputs, z)\n        return z if self.finetune else z.detach()\n'"
baseline/pytorch/optz.py,0,b'import eight_mile.optz\nfrom eight_mile.optz import *\n\n__all__ = []\n__all__.extend(eight_mile.optz.__all__)\n'
baseline/pytorch/remote.py,0,"b'import numpy as np\nfrom baseline.remote import RemoteModelREST, RemoteModelGRPC, register_remote\n\n\ndef _convert(data):\n    if isinstance(data, np.ndarray):\n        return data\n    return np.array(data)\n\n\n@register_remote(\'http\')\nclass RemoteModelRESTPytorch(RemoteModelREST):\n    """"""JSON schema:\n\n        {\n            ""signature_name"": ""name"",\n            ""inputs"": {\n                ""data"": [\n                    [...],\n                    [...],\n                    ...\n                ],\n                ""shapes"": [\n                    [...],\n                    [...],\n                    ...\n                ]\n                ""lengths"": [...]\n            }\n        }\n\n        data should be flattened (np.ravel)\n        shapes should be the call needed to reshape the tensor (should be same size as data)\n\n    """"""\n    def predict(self, examples, **kwargs):\n        """"""The pytorch server can only handle batch size of 1 because the JIT\'d\n        `pack_padded_sequence jits that batch size. So we send a request per\n        example.\n        """"""\n        results = []\n        example_input = examples[self.input_keys[0]]\n        batch_size = len(example_input)\n        for i in range(batch_size):\n            example = {k: np.array([v[i]]) for k, v in examples.items()}\n            example_output = super().predict(example, **kwargs)\n            results.append(example_output[0])\n        return results\n\n    def create_request(self, examples):\n        request = {}\n        request[\'signature_name\'] = self.signature\n        request[\'inputs\'] = {}\n        request[\'inputs\'][\'data\'] = [_convert(examples[x]).ravel().tolist() for x in self.input_keys]\n        request[\'inputs\'][\'shapes\'] = [list(examples[x].shape) for x in self.input_keys]\n        request[\'inputs\'][\'lengths\'] = examples[self.lengths_key].tolist()\n        return request\n\n\n@register_remote(\'grpc\')\nclass RemoteModelGRPCPytorch(RemoteModelGRPC):\n\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError(\'Pytorch GRPC service is not implemented.\')\n\n\n@register_remote(\'grpc-preproc\')\nclass RemoteModelGRPCPytorchPreproc(RemoteModelGRPCPytorch):\n\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError(\'Pytorch does not support string tensors so Server side preproc is not supported.\')\n\n\n@register_remote(\'http-preproc\')\nclass RemoteModelHTTPPytorchPreproc(RemoteModelRESTPytorch):\n\n    def __init__(self, *args, **kwargs):\n        raise NotImplementedError(\'Pytorch does not support string tensors so Server side preproc is not supported.\')\n'"
baseline/pytorch/torchy.py,8,"b'import math\nimport copy\nimport os\nimport logging\nimport numpy as np\nimport torch\nimport torch.autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom baseline.utils import lookup_sentence, get_version, Offsets\nfrom eight_mile.pytorch.layers import *\n\nPYT_MAJOR_VERSION = get_version(torch)\n\nBaseLayer = nn.Module\nTensorDef = torch.Tensor\n\n\nclass SequenceCriterion(nn.Module):\n\n    def __init__(self, LossFn=nn.NLLLoss, avg=\'token\'):\n        super(SequenceCriterion, self).__init__()\n        if avg == \'token\':\n            # self.crit = LossFn(ignore_index=Offsets.PAD, reduction=\'elementwise-mean\')\n            self.crit = LossFn(ignore_index=Offsets.PAD, size_average=True)\n            self._norm = self._no_norm\n        else:\n            self.crit = LossFn(ignore_index=Offsets.PAD, size_average=False)\n            self._norm = self._batch_norm\n\n    def _batch_norm(self, loss, inputs):\n        return loss / inputs.size()[0]\n\n    def _no_norm(self, loss, inputs):\n        return loss\n\n    def forward(self, inputs, targets):\n        """"""Evaluate some loss over a sequence.\n\n        :param inputs: torch.FloatTensor, [B, .., C] The scores from the model. Batch First\n        :param targets: torch.LongTensor, The labels.\n\n        :returns: torch.FloatTensor, The loss.\n        """"""\n        total_sz = targets.nelement()\n        loss = self.crit(inputs.view(total_sz, -1), targets.view(total_sz))\n        return self._norm(loss, inputs)\n\n\ndef pytorch_rnn_cell(insz, hsz, rnntype, nlayers, dropout):\n\n    if rnntype == \'gru\':\n        rnn = StackedGRUCell(nlayers, insz, hsz, dropout)\n    else:\n        rnn = StackedLSTMCell(nlayers, insz, hsz, dropout)\n    return rnn\n\n\ndef pytorch_conv1d(in_channels, out_channels, fsz, unif=0, padding=0, initializer=None):\n    c = nn.Conv1d(in_channels, out_channels, fsz, padding=padding)\n    if unif > 0:\n        c.weight.data.uniform_(-unif, unif)\n    elif initializer == ""ortho"":\n        nn.init.orthogonal(c.weight)\n    elif initializer == ""he"" or initializer == ""kaiming"":\n        nn.init.kaiming_uniform(c.weight)\n    else:\n        nn.init.xavier_uniform_(c.weight)\n    return c\n\n\ndef tie_weight(to_layer, from_layer):\n    """"""Assigns a weight object to the layer weights.\n\n    This method exists to duplicate baseline functionality across packages.\n\n    :param to_layer: the pytorch layer to assign weights to  \n    :param from_layer: pytorch layer to retrieve weights from  \n    """"""\n    to_layer.weight = from_layer.weight\n\n\ndef pytorch_clone_module(module_, N):\n    return nn.ModuleList([copy.deepcopy(module_) for _ in range(N)])\n\n\ndef append2seq(seq, modules):\n\n    for i, module in enumerate(modules):\n        seq.add_module(\'%s-%d\' % (str(module).replace(\'.\', \'dot\'), i), module)\n'"
baseline/pytorch/transformer.py,1,b'from eight_mile.pytorch.layers import *\n\n'
baseline/tensorflow_serving/__init__.py,0,b''
baseline/tf/__init__.py,0,b'from baseline.tf.tfy import *\nfrom baseline.tf.transformer import *\n'
baseline/tf/embeddings.py,0,"b'import logging\nfrom baseline.embeddings import register_embeddings, create_embeddings\nfrom eight_mile.tf.embeddings import *\nfrom eight_mile.tf.serialize import load_tlm_npz\nfrom eight_mile.utils import read_json\nfrom baseline.vectorizers import load_bert_vocab\nfrom baseline.utils import MEAD_HUB_MODULES\nimport tensorflow as tf\n\n\nlogger = logging.getLogger(\'baseline\')\n\n\nclass TensorFlowEmbeddingsMixin(tf.keras.layers.Layer):\n    """"""This provides a base for TensorFlow embeddings sub-graphs that includes the placeholders\n\n    """"""\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        """"""Constructor\n        """"""\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self._record_state(**kwargs)\n\n    def detached_ref(self):\n        """"""This will detach any attached input and reference the same sub-graph otherwise\n\n        TODO: this should not longer be required and can be removed\n\n        :return:\n        """"""\n        if getattr(self, \'_weights\', None) is not None:\n            return type(self)(name=self.name, weights=self._weights, **self._state)\n        if hasattr(self, \'embed\') and getattr(self.init_embed, \'_weights\') is not None:\n            return type(self)(name=self.name, weights=self.init_embed._weights, **self._state)\n        raise Exception(\'You must initialize `weights` in order to use this method\')\n\n    def call(self, x):\n        if x is None:\n            x = self.create_placeholder(self.name)\n        self.x = x\n\n        return super().encode(x)\n\n    @classmethod\n    def create_placeholder(cls, name):\n        """"""Create a placeholder with name `name`\n\n        :param name: (``str``) The name of the placeholder\n        :return: The placeholder\n        """"""\n        pass\n\n    @classmethod\n    def create(cls, model, name, **kwargs):\n        """"""Instantiate this sub-graph from the generalized representation from `baseline.w2v`\n\n        :param name: The name of the embeddings\n        :param model: The `baseline.w2v` model\n        :param kwargs:\n        :return:\n        """"""\n        kwargs.pop(\'dsz\', None)\n        return cls(name=name, vsz=model.vsz, dsz=model.dsz, weights=model.weights, **kwargs)\n\n    def _record_state(self, **kwargs):\n        w = kwargs.pop(\'weights\', None)\n        self._state = copy.deepcopy(kwargs)\n\n    def save_md(self, target):\n        """"""Save the metadata associated with this embedding as a JSON file\n\n        :param target: The name of the output file\n        :return:\n        """"""\n        write_json(self.get_config(), target)\n\n    def get_config(self):\n        #config = super(TensorFlowEmbeddings, self).get_config()\n        config = {}\n        config[\'dsz\'] = int(self.get_dsz())\n        config[\'vsz\'] = int(self.get_vsz())\n        config[\'module\'] = self.__class__.__module__\n        config[\'class\'] = self.__class__.__name__\n        config[\'mead_hub_modules\'] = MEAD_HUB_MODULES\n        config.update(self._state)\n        return config\n\n\n@register_embeddings(name=\'default\')\nclass LookupTableEmbeddingsModel(TensorFlowEmbeddingsMixin, LookupTableEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None], name=name)\n\n\n@register_embeddings(name=\'char-conv\')\nclass CharConvEmbeddingsModel(TensorFlowEmbeddingsMixin, CharConvEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None, None], name=name)\n\n\n@register_embeddings(name=\'char-transformer\')\nclass CharTransformerModel(TensorFlowEmbeddingsMixin, CharTransformerEmbeddings):\n    pass\n\n\n@register_embeddings(name=\'char-lstm\')\nclass CharLSTMEmbeddingsModel(TensorFlowEmbeddingsMixin, CharLSTMEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None, None], name=name)\n\n\n@register_embeddings(name=\'positional\')\nclass PositionalLookupTableEmbeddingsModel(TensorFlowEmbeddingsMixin, PositionalLookupTableEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None], name=name)\n\n\n@register_embeddings(name=\'learned-positional\')\nclass LearnedPositionalLookupTableEmbeddingsModel(TensorFlowEmbeddingsMixin, LearnedPositionalLookupTableEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None], name=name)\n\n\n@register_embeddings(name=\'learned-positional-w-bias\')\nclass LearnedPositionalLookupTableEmbeddingsWithBiasModel(TensorFlowEmbeddingsMixin, LearnedPositionalLookupTableEmbeddingsWithBias):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None], name=name)\n\n\n@register_embeddings(name=\'positional-char-conv\')\nclass PositionalCharConvEmbeddingsModel(TensorFlowEmbeddingsMixin, PositionalCharConvEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None, None], name=name)\n\n\n@register_embeddings(name=\'learned-positional-char-conv\')\nclass PositionalCharConvEmbeddingsModel(TensorFlowEmbeddingsMixin, LearnedPositionalCharConvEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None, None], name=name)\n\n\n@register_embeddings(name=\'positional-char-lstm\')\nclass PositionalCharLSTMEmbeddingsModel(TensorFlowEmbeddingsMixin, PositionalCharLSTMEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None, None], name=name)\n\n\n@register_embeddings(name=\'learned-positional-char-lstm\')\nclass LearnedPositionalCharLSTMEmbeddingsModel(TensorFlowEmbeddingsMixin, LearnedPositionalCharLSTMEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None, None], name=name)\n\n\nclass TransformerLMEmbeddings(TensorFlowEmbeddings):\n    def __init__(self, name=None, **kwargs):\n        super().__init__(name=name)\n        # You dont actually have to pass this if you are using the `load_bert_vocab` call from your\n        # tokenizer.  In this case, a singleton variable will contain the vocab and it will be returned\n        # by `load_bert_vocab`\n        # If you trained your model with MEAD/Baseline, you will have a `*.json` file which would want to\n        # reference here\n        vocab_file = kwargs.get(\'vocab_file\')\n        if vocab_file and vocab_file.endswith(\'.json\'):\n            self.vocab = read_json(kwargs.get(\'vocab_file\'))\n        else:\n            self.vocab = load_bert_vocab(kwargs.get(\'vocab_file\'))\n\n        self.cls_index = self.vocab[\'[CLS]\']\n        self.vsz = max(self.vocab.values()) + 1\n        self.d_model = int(kwargs.get(\'dsz\', kwargs.get(\'d_model\', 768)))\n        self.init_embed(**kwargs)\n        self.proj_to_dsz = tf.keras.layers.Dense(self.dsz, self.d_model) if self.dsz != self.d_model else _identity\n        self.init_transformer(**kwargs)\n\n    @property\n    def dsz(self):\n        return self.embeddings.output_dim\n\n    def embed(self, input):\n        return self.embeddings({\'x\': input})\n\n    def init_embed(self, **kwargs):\n        # If you are using BERT, you probably want to use either\n        # `learned-positional` with a token type feature\n        # or `learned-positional-w-bias` if you dont care about the token type\n        embed_type = kwargs.get(\'word_embed_type\', \'learned-positional\')\n        x_embedding = create_embeddings(vsz=self.vsz, dsz=self.d_model, embed_type=embed_type, name=\'x\')\n\n        embeddings = {\'x\': x_embedding}\n        # This is for BERT support when we are using 2 features\n        #token_type_vsz = kwargs.get(\'token_type_vsz\')\n        #if token_type_vsz:\n        #    tt_embedding = LookupTableEmbeddings(vsz=token_type_vsz, dsz=self.dsz, name=\'tt\')\n        #    embeddings[\'tt\'] = tt_embedding\n        # For bert, make sure this is `sum-layer-norm`\n        reduction = kwargs.get(\'embeddings_reduction\', kwargs.get(\'reduction\', \'concat\'))\n        embeddings_dropout = kwargs.get(\'embeddings_dropout\', 0.1)\n        self.embeddings = EmbeddingsStack(embeddings, dropout_rate=embeddings_dropout, reduction=reduction)\n\n    def init_transformer(self, **kwargs):\n        num_layers = int(kwargs.get(\'layers\', 12))\n        num_heads = int(kwargs.get(\'num_heads\', 12))\n        pdrop = kwargs.get(\'dropout\', 0.1)\n        d_ff = int(kwargs.get(\'d_ff\', 3072))\n        d_k = kwargs.get(\'d_k\')\n        rpr_k = kwargs.get(\'rpr_k\')\n        layer_norms_after = kwargs.get(\'layer_norms_after\', False)\n        layer_norm_eps = kwargs.get(\'layer_norm_eps\', 1e-12)\n        activation = kwargs.get(\'activation\', \'gelu\')\n        self.transformer = TransformerEncoderStack(num_heads, d_model=self.d_model, pdrop=pdrop, scale=True,\n                                                   layers=num_layers, d_ff=d_ff, rpr_k=rpr_k, d_k=d_k,\n                                                   activation=activation,\n                                                   layer_norms_after=layer_norms_after, layer_norm_eps=layer_norm_eps)\n        self.mlm = kwargs.get(\'mlm\', False)\n        self.finetune = kwargs.get(\'finetune\', True)\n\n    def _model_mask(self, nctx):\n        """"""This function creates the mask that controls which token to be attended to depending on the model. A causal\n        LM should have a subsequent mask; and a masked LM should have no mask.""""""\n        if self.mlm:\n            return tf.fill([1, 1, nctx, nctx], 1.)\n        else:\n            return subsequent_mask(nctx)\n\n    def encode(self, x):\n        # the following line masks out the attention to padding tokens\n        input_mask = tf.expand_dims(tf.expand_dims(tf.cast(tf.not_equal(x, 0), tf.float32), 1), 1)\n        # the following line builds mask depending on whether it is a causal lm or masked lm\n        input_mask = tf.multiply(input_mask, self._model_mask(x.shape[1]))\n        embedding = self.embed(x)\n        embedding = self.proj_to_dsz(embedding)\n        transformer_out = self.transformer((embedding, input_mask))\n        z = self.get_output(x, transformer_out)\n        return z\n\n    def get_output(self, inputs, z):\n        return tf.stop_gradient(z)\n\n    def get_vocab(self):\n        return self.vocab\n\n    def get_vsz(self):\n        return self.vsz\n\n    def get_dsz(self):\n        return self.d_model\n\n    @classmethod\n    def load(cls, embeddings, **kwargs):\n        c = cls(""tlm-words-embed"", **kwargs)\n        # pass random data through to initialize the graph\n        B = 1\n        T = 8\n        data_sample = tf.ones([B, T], dtype=tf.int32)\n        #data_sample_tt = tf.zeros([B, T], dtype=tf.int32)\n        _ = c(data_sample)\n        load_tlm_npz(c, embeddings)\n        return c\n\n    #def detached_ref(self):\n    #    return self\n\n\nclass TransformerLMPooledEmbeddings(TransformerLMEmbeddings):\n\n    def __init__(self, name, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n        pooling = kwargs.get(\'pooling\', \'cls\')\n        if pooling == \'max\':\n            self.pooling_op = _max_pool\n        elif pooling == \'mean\':\n            self.pooling_op = _mean_pool\n        else:\n            self.pooling_op = self._cls_pool\n\n    def _cls_pool(self, inputs, tensor):\n        pooled = tensor[tf.equal(inputs, self.cls_index)]\n        return pooled\n\n    def get_output(self, inputs, z):\n        return self.pooling_op(inputs, z)\n\n\n@register_embeddings(name=\'tlm-words-embed\')\nclass TransformerLMEmbeddingsModel(TensorFlowEmbeddingsMixin, TransformerLMEmbeddings):\n\n    @classmethod\n    def create_placeholder(cls, name):\n        return tf.compat.v1.placeholder(tf.int32, [None, None], name=name)\n\n\n@register_embeddings(name=\'tlm-words-embed-pooled\')\nclass TransformerLMPooledEmbeddingsModel(TensorFlowEmbeddingsMixin, TransformerLMPooledEmbeddings):\n    pass\n\n\ndef _identity(x):\n    return x\n\n\ndef _mean_pool(inputs, embeddings):\n    mask = tf.not_equal(inputs, 0)\n    seq_lengths = tf.reduce_sum(tf.cast(mask, tf.int8), axis=1, keepdims=True)\n    embeddings = tf.where(tf.expand_dims(mask, -1), embeddings, 0.)\n    return tf.reduce_sum(embeddings, 1, False) / tf.cast(seq_lengths, embeddings.dtype)\n\n\ndef _max_pool(inputs, embeddings):\n    mask = tf.not_equal(inputs, 0)\n    embeddings = tf.where(tf.expand_dims(mask, -1), embeddings, 0.)\n    return tf.reduce_max(embeddings, 1, False)\n'"
baseline/tf/optz.py,0,b'import eight_mile.optz\nfrom eight_mile.optz import *\n\n__all__ = []\n__all__.extend(eight_mile.optz.__all__)\n'
baseline/tf/remote.py,0,"b""import numpy as np\nfrom baseline.remote import (\n    RemoteModelREST,\n    RemoteModelGRPC,\n    register_remote,\n    RemoteRESTClassifier,\n    RemoteRESTTagger,\n    RemoteRESTSeq2Seq,\n    RemoteRESTEmbeddings,\n    RemoteGRPCClassifier,\n    RemoteGRPCTagger,\n    RemoteGRPCSeq2Seq,\n    RemoteGRPCEmbeddings,\n)\n\n\nclass RemoteRESTTensorFlowMixin(RemoteModelREST):\n\n    def create_request(self, examples):\n        inputs = {}\n        for feature in self.input_keys:\n            tensor = examples[feature]\n            if isinstance(tensor, np.ndarray):\n                inputs[feature] = tensor.tolist()\n            else:\n                inputs[feature] = tensor\n        request = {'signature_name': self.signature, 'inputs': inputs}\n        return request\n\n@register_remote('http-classify')\nclass RemoteRESTTensorFlowClassifier(RemoteRESTTensorFlowMixin, RemoteRESTClassifier): pass\n\n@register_remote('http-tagger')\nclass RemoteRESTTensorFlowTagger(RemoteRESTTensorFlowMixin, RemoteRESTTagger): pass\n\n@register_remote('http-seq2seq')\nclass RemoteRESTTensorFlowSeq2Seq(RemoteRESTTensorFlowMixin, RemoteRESTSeq2Seq): pass\n\n@register_remote('http-servable-embeddings')\nclass RemoteRESTTensorFlowEmbeddings(RemoteRESTTensorFlowMixin, RemoteRESTEmbeddings): pass\n\n\n@register_remote('grpc')\nclass RemoteGRPCTensorFlowMixin(RemoteModelGRPC): pass\n\n@register_remote('grpc-classify')\nclass RemoteGRPCTensorFlowClassifier(RemoteGRPCTensorFlowMixin, RemoteGRPCClassifier): pass\n\n@register_remote('grpc-tagger')\nclass RemoteGRPCTensorFlowTagger(RemoteGRPCTensorFlowMixin, RemoteGRPCTagger): pass\n\n@register_remote('grpc-seq2seq')\nclass RemoteGRPCTensorFlowSeq2Seq(RemoteGRPCTensorFlowMixin, RemoteGRPCSeq2Seq): pass\n\n@register_remote('grpc-servable-embeddings')\nclass RemoteGRPCTensorFlowEmbeddings(RemoteGRPCTensorFlowMixin, RemoteGRPCEmbeddings): pass\n\n\n@register_remote('grpc-preproc')\nclass RemoteGRPCTensorFlowPreprocMixin(RemoteModelGRPC):\n\n    def create_request(self, examples):\n        # TODO: Remove TF dependency client side\n        import tensorflow as tf\n\n        request = self.predictpb.PredictRequest()\n        request.model_spec.name = self.name\n        request.model_spec.signature_name = self.signature\n        if self.version is not None:\n            request.model_spec.version.value = self.version\n\n        for key in examples:\n            if key.endswith('lengths'):\n                shape = examples[key].shape\n                tensor_proto = tf.contrib.util.make_tensor_proto(examples[key], shape=shape, dtype=tf.int32)\n                request.inputs[key].CopyFrom(\n                    tensor_proto\n                )\n            else:\n                request.inputs[key].CopyFrom(\n                    tf.contrib.util.make_tensor_proto(examples[key], shape=[len(examples[key]), 1])\n                )\n        return request\n\n@register_remote('grpc-preproc-classify')\nclass RemoteGRPCTensorFlowPreprocClassifier(RemoteGRPCTensorFlowPreprocMixin, RemoteGRPCClassifier): pass\n\n@register_remote('grpc-preproc-tagger')\nclass RemoteGRPCTensorFlowPreprocTagger(RemoteGRPCTensorFlowPreprocMixin, RemoteGRPCTagger): pass\n\n@register_remote('grpc-preproc-seq2seq')\nclass RemoteGRPCTensorFlowPreprocSeq2Seq(RemoteGRPCTensorFlowPreprocMixin, RemoteGRPCSeq2Seq): pass\n\n@register_remote('grpc-preproc-servable-embeddings')\nclass RemoteGRPCTensorFlowPreprocEmbeddings(RemoteGRPCTensorFlowPreprocMixin, RemoteGRPCEmbeddings): pass\n\n\n@register_remote('http-preproc')\nclass RemoteRESTTensorFlowPreprocMixin(RemoteModelREST):\n\n    def create_request(self, examples):\n        inputs = {}\n        if isinstance(examples['tokens'], np.ndarray):\n            inputs['tokens'] = examples['tokens'].tolist()\n        else:\n            inputs['tokens'] = examples['tokens']\n        for feature in self.input_keys:\n            if feature.endswith('lengths'):\n                if isinstance(examples[feature], np.ndarray):\n                    inputs[feature] = examples[feature].tolist()\n                else:\n                    inputs[feature] = examples[feature]\n        return {'signature_name': self.signature, 'inputs': inputs}\n\n@register_remote('http-preproc-classify')\nclass RemoteRESTTensorFlowPreprocClassifier(RemoteRESTTensorFlowPreprocMixin, RemoteRESTClassifier): pass\n\n@register_remote('http-preproc-tagger')\nclass RemoteRESTTensorFlowPreprocTagger(RemoteRESTTensorFlowPreprocMixin, RemoteRESTTagger): pass\n\n@register_remote('http-preproc-seq2seq')\nclass RemoteRESTTensorFlowPreprocSeq2Seq(RemoteRESTTensorFlowPreprocMixin, RemoteRESTSeq2Seq): pass\n\n@register_remote('http-preproc-servable-embeddings')\nclass RemoteRESTTensorFlowPreprocEmbeddings(RemoteRESTTensorFlowPreprocMixin, RemoteRESTEmbeddings): pass\n"""
baseline/tf/tfy.py,0,"b'import numpy as np\nimport tensorflow as tf\nfrom baseline.utils import transition_mask as transition_mask_np, listify, read_json, is_sequence, import_user_module\nfrom eight_mile.tf.layers import *\nfrom functools import wraps\n\nBaseLayer = tf.keras.layers.Layer\nTensorDef = tf.Tensor\n\ndef reload_embeddings(embeddings_dict, basename):\n    embeddings = {}\n    for key, cls in embeddings_dict.items():\n        embed_args = read_json(\'{}-{}-md.json\'.format(basename, key))\n        module = embed_args.pop(\'module\')\n        name = embed_args.pop(\'name\', None)\n        assert name is None or name == key\n        mod = import_user_module(module)\n        embed_args[\'name\'] = key\n        Constructor = getattr(mod, cls)\n        embeddings[key] = Constructor(**embed_args)\n    return embeddings\n\n\ndef _add_ema(model, decay):\n    """"""Create ops needed to track EMA when training.\n\n    :param model: The model with a `.sess` we want to track.\n    :param decay: float, Decay to use in the EMA\n\n    :returns:\n        ema_op: The update op. This applies the ema to each variable. Should be\n           set as a control dependency on the training op.\n        load: Op to copy emas to the variables.\n        restore_var: Op to copy the original variables back from the EMA ones.\n\n    Note:\n        If you run the load op multiple times then the backup variables will be\n        replaced by the ema variables.\n\n        Currently there was a bug I haven\'t been able to fix. I haven\'t found why\n        but sometimes when you run it with a tf.cond you get this error.\n        `tensorflow.python.framework.errors_impl.InvalidArgumentError: Retval[0] does not have value`\n        The stop gap is to remove this which means if you run load multiple times\n        it will over write the backup variables with ema values.\n\n        The load op is set up to automatically save the normal parameters when\n        you load the ema\'s in.\n    """"""\n    ema = tf.train.ExponentialMovingAverage(decay=decay)\n    model_vars = model.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n    with tf.variable_scope(""BackupVariables""):\n        backup_vars = [\n            tf.get_variable(\n                var.op.name,\n                dtype=var.value().dtype,\n                trainable=False,\n                initializer=var.initialized_value()\n            ) for var in model_vars\n        ]\n    ema_op = ema.apply(model_vars)\n\n    save_back_up = tf.group(*(\n        tf.assign(back, var.read_value())\n        for var, back in zip(model_vars, backup_vars)\n    ), name=\'save_backups\')\n\n    with tf.control_dependencies([save_back_up]):\n        load = tf.group(*(\n            tf.assign(var, ema.average(var).read_value())\n            for var in model_vars\n        ), name=""load_ema"")\n\n    restore_vars = tf.group(*(\n        tf.assign(var, back.read_value())\n        for var, back in zip(model_vars, backup_vars)\n    ), name=""restore_backups"")\n\n    return ema_op, load, restore_vars\n\n\ndef transition_mask(vocab, span_type, s_idx, e_idx, pad_idx=None):\n    """"""Create a CRF Mask.\n    Returns a mask with invalid moves as 0 and valid moves as 1.\n    """"""\n    mask = transition_mask_np(vocab, span_type, s_idx, e_idx, pad_idx).T\n    inv_mask = (mask == 0).astype(np.float32)\n    return mask, inv_mask\n\n\ndef dense_layer(output_layer_depth):\n    output_layer = tf.layers.Dense(output_layer_depth, use_bias=False, dtype=tf.float32, name=""dense"")\n    return output_layer\n\n\ndef reload_embeddings_from_state(embeddings_dict, basename):\n    embeddings = {}\n    for key, class_name in embeddings_dict.items():\n        embed_args = read_json(\'{}-{}-md.json\'.format(basename, key))\n        module = embed_args.pop(\'module\')\n        name = embed_args.pop(\'name\', None)\n        assert name is None or name == key\n        mod = import_user_module(module)\n        Constructor = getattr(mod, class_name)\n        embeddings[key] = Constructor(key, **embed_args)\n    return embeddings\n\n\ndef tie_weight(weight, tie_shape):\n    """"""Higher order function to share weights between two layers.\n\n    Tensorflow will take a custom_getter inside of a variable scope.\n    This method creates a getter that looks for a match in shapes. If they match,\n    The weights are transposed and shared.\n\n    """"""\n    def tie_getter(getter, name, *args, **kwargs):\n        if kwargs[\'shape\'] == tie_shape:\n            return tf.transpose(weight)\n        return getter(""{}"".format(name), *args, **kwargs)\n    return tie_getter\n\n\ndef rnn_cell_w_dropout(hsz, pdrop, rnntype, st=None, variational=False, training=False):\n\n    """"""Produce a single RNN cell with dropout\n    :param hsz: (``int``) The number of hidden units per LSTM\n    :param rnntype: (``str``): `lstm` or `gru`\n    :param pdrop: (``int``) The probability of dropping a unit value during dropout\n    :param st: (``bool``) state is tuple? defaults to `None`\n    :param variational: (``bool``) Variational recurrence is on\n    :param training: (``bool``) Are we training?  Defaults to ``False``\n    :return: a cell\n    """"""\n    output_keep_prob = tf.contrib.framework.smart_cond(training, lambda: 1.0 - pdrop, lambda: 1.0)\n    state_keep_prob = tf.contrib.framework.smart_cond(training, lambda: 1.0 - pdrop if variational else 1.0, lambda: 1.0)\n    cell = rnn_cell(hsz, rnntype, st)\n    output = tf.contrib.rnn.DropoutWrapper(cell,\n                                           output_keep_prob=output_keep_prob,\n                                           state_keep_prob=state_keep_prob,\n                                           variational_recurrent=variational,\n                                           dtype=tf.float32)\n    return output\n\n\ndef multi_rnn_cell_w_dropout(hsz, pdrop, rnntype, num_layers, variational=False, training=False):\n    """"""Produce a stack of RNNs with dropout performed on all but the last layer.\n\n    :param hsz: (``int``) The number of hidden units per RNN\n    :param pdrop: (``int``) The probability of dropping a unit value during dropout\n    :param rnntype: (``str``) The type of RNN to use - `lstm` or `gru`\n    :param num_layers: (``int``) The number of layers of RNNs to stack\n    :param training: (``bool``) Are we training? Defaults to ``False``\n    :return: a stacked cell\n    """"""\n    if variational:\n        return tf.contrib.rnn.MultiRNNCell(\n            [rnn_cell_w_dropout(hsz, pdrop, rnntype, variational=variational, training=training) for _ in range(num_layers)],\n            state_is_tuple=True\n        )\n    return tf.contrib.rnn.MultiRNNCell(\n        [rnn_cell_w_dropout(hsz, pdrop, rnntype, training=training) if i < num_layers - 1 else rnn_cell_w_dropout(hsz, 1.0, rnntype) for i in range(num_layers)],\n        state_is_tuple=True\n    )\n\n\ndef stacked_dense(inputs, init, hszs=[], pdrop_value=0.5):\n    return DenseStack(None, hszs, pdrop_value=pdrop_value, init=init)(inputs)\n\n'"
baseline/tf/transformer.py,0,"b""from baseline.tf.tfy import *\nfrom eight_mile.tf.layers import subsequent_mask\n\n\ndef transformer_encoder(x, src_mask, scope, num_heads, pdrop, scale=True, activation_type='relu', d_ff=None):\n    d_model = get_shape_as_list(x)[-1]\n    return TransformerEncoder(d_model, num_heads, pdrop, scale, activation_type, d_ff, name=scope)((x, src_mask))\n\n\ndef transformer_decoder(src, tgt, src_mask, tgt_mask, scope, num_heads, pdrop, scale=True, activation_type='relu', d_ff=None):\n    d_model = get_shape_as_list(tgt)[-1]\n    return TransformerDecoder(d_model, num_heads, pdrop, scale, activation_type, d_ff)((src, tgt, src_mask, tgt_mask))\n\n\ndef transformer_encoder_stack(x, src_mask, num_heads, pdrop, scale=True, layers=1, activation_type='relu', scope='TransformerEncoder', d_ff=None):\n    d_model = get_shape_as_list(x)[-1]\n    return TransformerEncoderStack(d_model, num_heads, pdrop, scale, layers, activation_type, d_ff=d_ff, name=scope)((x, src_mask))\n\n\ndef transformer_decoder_stack(tgt, src, src_mask, tgt_mask, num_heads, pdrop, scale=True, layers=1, activation_type='relu', scope='TransformerDecoder', d_ff=None):\n    d_model = get_shape_as_list(tgt)[-1]\n    return TransformerDecoderStack(d_model, num_heads, pdrop, scale, layers, activation_type, d_ff=d_ff, name=scope)((tgt, src, src_mask, tgt_mask))\n"""
layers/eight_mile/__init__.py,0,b'from eight_mile.version import __version__\n'
layers/eight_mile/bleu.py,0,"b'r""""""Multi-bleu in python matching Moses http://www.statmt.org/moses/?n=Moses.SupportTools#ntoc5\n\nReference files and prediction files must be sentence aligned. To use multiple\nreferences use multiple reference files (all sentence aligned)\n\nAll text should already be tokenized (so that calling `.split()` on the line produces correct tokens)\n\nSlight differences between this and multi-ble.pl:\n\n * multi-bleu.pl crashes when the hypothesis corpus is completely empty. We don\'t want training to crash\n   so we set the brevity penalty to `0` in this case. This makes sense because as the reference corpus grows\n   arbitrary large the length ratio (gold / pred) approaches infinity and the limit of e^(1 - x) as\n   x -> infinity is 0. When the hypothesis corpus is we also return a np.nan for the length ratio for a\n   visual cue that the lengths were weird when using the cli tool.\n\n * If your reference file doesn\'t have a newline at the end of the file but your hypothesis does\n   then multi-blue.pl will give include this newline as an empty example and your score will\n   be lower than it should be. This code will ignore that line and give the correct score.\n\n   For example:\n   ```\n       $ diff ref pred\n       1000c1000\n       < 52 95 73 83 93 87 88 81 93\n       \\ No newline at end of file\n       ---\n       > 52 95 73 83 93 87 88 81 93\n\n       $ diff ref pred_no_newline\n       $\n\n       $ bleu ref < pred\n       BLEU = 100.00, 100.0/100.0/100.0/100.0 (BP=1.000, ratio=1.000, hyp_len=7533, ref_len=7533)\n\n       $ bleu ref < pred_no_newline\n       BLEU = 100.00, 100.0/100.0/100.0/100.0 (BP=1.000, ratio=1.000, hyp_len=7533, ref_len=7533)\n\n       $ perl multi-bleu.pl ref < pred\n       BLEU = 99.98, 100.0/100.0/100.0/100.0 (BP=1.000, ratio=1.000, hyp_len=7533, ref_len=7533)\n\n       $ perl multi-bleu.pl ref < pred_no_newline\n       BLEU = 100.00, 100.0/100.0/100.0/100.0 (BP=1.000, ratio=1.000, hyp_len=7533, ref_len=7533)\n    ```\n""""""\nimport sys\nimport argparse\nfrom operator import or_\nfrom itertools import chain\nfrom functools import reduce\nfrom collections import Counter\nfrom typing import List, Tuple, Union, Iterable, TextIO, Counter as CounterType, NamedTuple\nimport numpy as np\n\n\n__all__ = [""Bleu"", ""bleu""]\n\n\nHypothesis = List[str]\n# This is [B, T] where B is the number of hypotheses and T is the number of words in that hypothesis.\nHypothesisCorpus = List[Hypothesis]\n\nReference = List[str]\nReferences = List[Reference]\n# This is [B, R, T] where B is the number of references, R is the number of gold references we have\n# for a particular example, and T is the number of words in that reference.\nReferenceCorpus = List[References]\n\n\nclass Bleu(NamedTuple):\n    """"""A collection of the information returned from the bleu calculation""""""\n\n    bleu: float\n    precision: np.ndarray\n    brevity_penalty: float\n    length_ratio: float\n    pred_length: int\n    gold_length: int\n\n\ndef n_grams(tokens: List[str], n: Union[int, Tuple[int]]) -> Iterable[Tuple[str]]:\n    """"""Create a list for n grams for each value up to and including n.\n\n    :param tokens: The sequence to create n_grams on.\n    :param n: If an int the largest n_gram to create otherwise the size of\n        n_grams to create.\n\n    :returns: The n_grams ordered by size.\n    """"""\n    n = range(n + 1) if isinstance(n, int) else n\n    return chain(*(zip(*[tokens[i:] for i in range(n_)]) for n_ in n))\n\n\ndef count_n_grams(tokens: List[str], n: Union[int, Tuple[int]]) -> CounterType[Tuple[str]]:\n    """"""Count the n_grams in a sequence.\n\n    :param tokens: The sequence to create n_grams on.\n    :param n: If an int the largest n_gram to create otherwise the size of\n        n_grams to create.\n\n    :returns: The counts of each n_gram.\n    """"""\n    return Counter(n_grams(tokens, n))\n\n\ndef find_closest(pred_len: int, golds: References) -> int:\n    """"""Find the gold sentence that has the most similar length to pred.\n\n    Note:\n        When there are multiple sentence with the same difference in length\n        to the pred length the shortest one is selected.\n\n    :param pred_len: The length of the predicted sentence.\n    :param golds: The gold sentences.\n\n    :returns: The length of the selected gold sentence.\n    """"""\n    best_diff = sys.maxsize\n    best_len = sys.maxsize\n    for gold in golds:\n        gold_len = len(gold)\n        diff = abs(pred_len - gold_len)\n        if diff < best_diff:\n            best_diff = diff\n            best_len = gold_len\n        elif diff == best_diff:\n            best_len = gold_len if gold_len < best_len else best_len\n    return best_len\n\n\ndef corpora_lengths(preds: HypothesisCorpus, golds: ReferenceCorpus) -> Tuple[int, int]:\n    """"""Calculate the length of the two corpora.\n\n    The length of the pred corpus is just the sum of lengths, the length\n    of the gold corpus is the sum of the lengths of a single gold selected\n    from a set of golds. This single gold is selected to be the length that\n    is closest to the pred lengths (in the event of ties select the shorter\n    gold sentence).\n\n    :param preds: A list of sentences generated by the model.\n    :param golds: A list of gold references. A gold reference can have multiple gold sentences\n\n    :returns: The length of the predicted corpus and the length of the gold corpus.\n    """"""\n    pred_len = 0\n    gold_len = 0\n    for pred, gold in zip(preds, golds):\n        pred_len += len(pred)\n        gold_len += find_closest(len(pred), gold)\n    return pred_len, gold_len\n\n\ndef max_gold_n_gram_counts(golds: References, n: int) -> CounterType[Tuple[str]]:\n    """"""Find the maximum n gram count for the gold sequences.\n\n    :param golds: The gold sentences.\n    :param n: The max size of n-grams we are using.\n\n    :returns: The maximum count for any n-gram that appears in a gold sentence.\n    """"""\n    # or_ is the union of counters so result is max count for each n gram\n    # that appears in the any of the gold sentences.\n    return reduce(or_, map(lambda g: count_n_grams(g, n), golds))\n\n\ndef count_matches(\n    pred_counts: CounterType[Tuple[str]], gold_counts: CounterType[Tuple[str]], matches: np.ndarray\n) -> np.ndarray:\n    """"""Aggregate the number of matches for each n gram length.\n\n    :param pred_counts: The counts for n grams found in the predicted sentence.\n    :param gold_counts: The max counts for n grams found in the gold sentences.\n    :param matches: The number of matches found so far grouped by n-gram size.\n\n    :returns: The number of matches so far grouped by n-gram size.\n    """"""\n    # & is the intersection of counters, selecting the min of each n gram\n    # that appears in both of the counters.\n    overlap = pred_counts & gold_counts\n    for n_gram, count in overlap.items():\n        matches[len(n_gram) - 1] += count\n    return matches\n\n\ndef count_possible(pred: Hypothesis, total: np.ndarray) -> np.ndarray:\n    """"""Count the total number of possible matches.\n\n    We recalculate the total possible rather than use the counts calculated\n    before because the counts aren\'t binned by length.\n\n    :param pred: The predicted sentence.\n    :param total: The current total number of possible matches grouped by size.\n\n    :returns: The new number of possible n-gram matches of far grouped by size.\n    """"""\n    for n in range(len(total)):\n        # Possible number of n_grams for a sequence is the length of the sequence\n        # minus the length of the n_gram plus 1. In this case n is already\n        # one less than length of n_gram so just subtract that.\n        total_grams = len(pred) - n\n        if total_grams > 0:\n            total[n] += total_grams\n    return total\n\n\ndef geometric_mean(precision: np.ndarray) -> float:\n    """"""Calculate the geometric mean of the precision.\n\n    Geometric mean is the nth root of the product of n numbers. This\n    can be expressed as the e^(arithmetic mean of the logs). The geometric\n    mean only applies to positive number and any 0 is the values makes the\n    answer trivially zero to checkout for that first to avoid log(0).\n\n    :param precision: The precision for each n-gram size.\n\n    :returns: The geometric_mean of the values.\n    """"""\n    if np.min(precision) <= 0:\n        return 0.0\n    return np.exp(np.mean(np.log(precision))).item()\n\n\ndef brevity_penalty(pred_len: int, gold_len: int) -> Tuple[float, float]:\n    """"""Calculate the brevity penalty.\n\n    Note:\n        multi-bleu.pl crashes when the hypothesis corpus is completely empty.\n        We don\'t want training to crash then there are no hypotheses so we set\n        the brevity penalty to `0` because as the reference corpus grows arbitrary\n        large the length ration (gold / pred) approaches infinity and the limit of\n        e^(1 - x) as x -> infinity is 0.\n\n        We also return a np.nan for the length ratio for a visual cue that the\n        lengths were weird when using the cli tool.\n\n    :param pred_len: The length of the model prediction corpus\n    :param gold_len: The length of the gold corpus\n\n    :returns: The brevity penalty and the ratio of predicted length to gold length.\n    """"""\n    if pred_len == 0:\n        return 0, np.nan\n    ratio = pred_len / float(gold_len)\n    # If ratio is <= 1.0 then pred_len <= gold_len so penalty applies.\n    # Penalty is defined as e^(1 - (gold / pred)). (1 / (p / g)) = (g / p)\n    bp = np.exp(1 - (1.0 / ratio)).item() if ratio <= 1.0 else 1.0\n    return bp, ratio\n\n\ndef bleu(preds: HypothesisCorpus, golds: ReferenceCorpus, n: int = 4) -> Bleu:\n    """"""Calculate BLEU score\n\n    This implementation is designed to match the output of `multi-bleu,pl` from\n    http://www.statmt.org/moses/?n=Moses.SupportTools#ntoc5\n\n    :param preds: A list of sentences generated by the model.\n    :param golds: A list of gold references where each reference can contain multiple sentences.\n    :param n: The max size n-gram to use.\n\n    :returns: (\n        bleu score,\n        precision per n_gram size,\n        brevity penalty,\n        ration of prediction length to reference lengths,\n        length of the predictions,\n        length of the references\n    )\n    """"""\n    matches = np.zeros(n)\n    total = np.zeros(n)\n    pred_len, gold_len = corpora_lengths(preds, golds)\n    for pred, gold in zip(preds, golds):\n        max_gold_counts = max_gold_n_gram_counts(gold, n)\n        pred_counts = count_n_grams(pred, n)\n        matches = count_matches(pred_counts, max_gold_counts, matches)\n        total = count_possible(pred, total)\n\n    precision = np.array([matches[i] / float(total[i]) if total[i] > 0 else 0.0 for i in range(n)])\n    geo_mean = geometric_mean(precision)\n    bp, len_ratio = brevity_penalty(pred_len, gold_len)\n    b = geo_mean * bp * 100\n    return Bleu(b, precision * 100, bp, len_ratio, pred_len, gold_len)\n\n\ndef _read_references(reference_files: List[str], lc: bool) -> ReferenceCorpus:\n    """"""Read from multiple reference files.\n\n    :param reference_file: A list of reference files to read from.\n    :param lc: Should the input be lowercased?\n\n    :return: The reference corpus.\n    """"""\n    references: List[HypothesisCorpus] = []\n    for ref in reference_files:\n        with open(ref) as f:\n            references.append(_read_lines(f, lc))\n    return list(zip(*references))\n\n\n# Slightly strange file reader that read from pre opened files to facilitate\n# reading the predictions from stdin. Won\'t be used elsewhere.\ndef _read_lines(f: TextIO, lc: bool) -> HypothesisCorpus:\n    """"""Read from a file that is open.\n\n    The expected file format is each line contains a single example and each example\n    is a list of tokens joined on whitespace\n\n    :param f: The file to read from.\n    :param lc: Should the input be lowercased?\n\n    :return: The list of examples\n    """"""\n    if lc:\n        return list(map(lambda x: list(map(lambda y: y.lower(), x.split())), f))\n    return list(map(lambda x: x.split(), f))\n\n\ndef main():\n    """"""Use as a cli tools like multibleu.pl""""""\n    usage = ""%(prog)s [-lc] [-n] reference < hypothesis\\nReads the references from reference or reference0, reference1, ...\\n""\n    parser = argparse.ArgumentParser(description=""Calculate Bleu score."", usage=usage)\n    parser.add_argument(""-lc"", help=""lowercase the input"", action=""store_true"")\n    parser.add_argument(""-n"", type=int, default=4, help=""The number of ngrams to use."")\n    parser.add_argument(""reference"", nargs=""+"")\n    args = parser.parse_args()\n\n    golds: ReferenceCorpus = _read_references(args.reference, args.lc)\n    preds: HypothesisCorpus = _read_lines(sys.stdin, args.lc)\n\n    b = bleu(preds, golds, args.n)\n    precision_str = ""/"".join([""{:.1f}""] * len(b.precision)).format(*b.precision)\n\n    print(\n        f""BLEU = {b.bleu:.2f}, {precision_str} (BP={b.brevity_penalty:.3f}, ratio={b.length_ratio:.3f}, hyp_len={b.pred_length}, ref_len={b.gold_length})""\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
layers/eight_mile/confusion.py,0,"b'import csv\nfrom itertools import chain\nimport numpy as np\nfrom eight_mile.utils import exporter\nfrom collections import OrderedDict\n\n\n__all__ = []\nexport = exporter(__all__)\n\n\n@export\nclass ConfusionMatrix:\n    """"""Confusion matrix with metrics\n\n    This class accumulates classification output, and tracks it in a confusion matrix.\n    Metrics are available that use the confusion matrix\n    """"""\n\n    def __init__(self, labels):\n        """"""Constructor with input labels\n\n        :param labels: Either a dictionary (`k=int,v=str`) or an array of labels\n        """"""\n        if isinstance(labels, dict):\n            self.labels = []\n            for i in range(len(labels)):\n                self.labels.append(labels[i])\n        else:\n            self.labels = labels\n        nc = len(self.labels)\n        self._cm = np.zeros((nc, nc), dtype=np.int)\n\n    def add(self, truth, guess):\n        """"""Add a single value to the confusion matrix based off `truth` and `guess`\n\n        :param truth: The real `y` value (or ground truth label)\n        :param guess: The guess for `y` value (or assertion)\n        """"""\n\n        self._cm[truth, guess] += 1\n\n    def __str__(self):\n        values = []\n        width = max(8, max(len(x) for x in self.labels) + 1)\n        for i, label in enumerate([""""] + self.labels):\n            values += [""{:>{width}}"".format(label, width=width + 1)]\n        values += [""\\n""]\n        for i, label in enumerate(self.labels):\n            values += [""{:>{width}}"".format(label, width=width + 1)]\n            for j in range(len(self.labels)):\n                values += [""{:{width}d}"".format(self._cm[i, j], width=width + 1)]\n            values += [""\\n""]\n        values += [""\\n""]\n        return """".join(values)\n\n    def save(self, outfile):\n        ordered_fieldnames = OrderedDict([(""labels"", None)] + [(l, None) for l in self.labels])\n        with open(outfile, ""w"") as f:\n            dw = csv.DictWriter(f, delimiter="","", fieldnames=ordered_fieldnames)\n            dw.writeheader()\n            for index, row in enumerate(self._cm):\n                row_dict = {l: row[i] for i, l in enumerate(self.labels)}\n                row_dict.update({""labels"": self.labels[index]})\n                dw.writerow(row_dict)\n\n    def reset(self):\n        """"""Reset the matrix\n        """"""\n        self._cm *= 0\n\n    def get_correct(self):\n        """"""Get the diagonals of the confusion matrix\n\n        :return: (``int``) Number of correct classifications\n        """"""\n        return self._cm.diagonal().sum()\n\n    def get_total(self):\n        """"""Get total classifications\n\n        :return: (``int``) total classifications\n        """"""\n        return self._cm.sum()\n\n    def get_acc(self):\n        """"""Get the accuracy\n\n        :return: (``float``) accuracy\n        """"""\n        return float(self.get_correct()) / self.get_total()\n\n    def get_recall(self):\n        """"""Get the recall\n\n        :return: (``float``) recall\n        """"""\n        total = np.sum(self._cm, axis=1)\n        total = (total == 0) + total\n        return np.diag(self._cm) / total.astype(float)\n\n    def get_support(self):\n        return np.sum(self._cm, axis=1)\n\n    def get_precision(self):\n        """"""Get the precision\n        :return: (``float``) precision\n        """"""\n\n        total = np.sum(self._cm, axis=0)\n        total = (total == 0) + total\n        return np.diag(self._cm) / total.astype(float)\n\n    def get_mean_precision(self):\n        """"""Get the mean precision across labels\n\n        :return: (``float``) mean precision\n        """"""\n        return np.mean(self.get_precision())\n\n    def get_weighted_precision(self):\n        return np.sum(self.get_precision() * self.get_support()) / float(self.get_total())\n\n    def get_mean_recall(self):\n        """"""Get the mean recall across labels\n\n        :return: (``float``) mean recall\n        """"""\n        return np.mean(self.get_recall())\n\n    def get_weighted_recall(self):\n        return np.sum(self.get_recall() * self.get_support()) / float(self.get_total())\n\n    def get_weighted_f(self, beta=1):\n        return np.sum(self.get_class_f(beta) * self.get_support()) / float(self.get_total())\n\n    def get_macro_f(self, beta=1):\n        """"""Get the macro F_b, with adjustable beta (defaulting to F1)\n\n        :param beta: (``float``) defaults to 1 (F1)\n        :return: (``float``) macro F_b\n        """"""\n        if beta < 0:\n            raise Exception(""Beta must be greater than 0"")\n        return np.mean(self.get_class_f(beta))\n\n    def get_class_f(self, beta=1):\n        p = self.get_precision()\n        r = self.get_recall()\n\n        b = beta * beta\n        d = b * p + r\n        d = (d == 0) + d\n\n        return (b + 1) * p * r / d\n\n    def get_f(self, beta=1):\n        """"""Get 2 class F_b, with adjustable beta (defaulting to F1)\n\n        :param beta: (``float``) defaults to 1 (F1)\n        :return: (``float``) 2-class F_b\n        """"""\n        p = self.get_precision()[1]\n        r = self.get_recall()[1]\n        if beta < 0:\n            raise Exception(""Beta must be greater than 0"")\n        d = beta * beta * p + r\n        if d == 0:\n            return 0\n        return (beta * beta + 1) * p * r / d\n\n    def get_r_k(self):\n        """"""Calculate the R k correlation coefficient from here\n            https://www.sciencedirect.com/science/article/abs/pii/S1476927104000799?via%3Dihub\n        See this blog post for an explanation of metric, how this calculation is equivalent and\n        how it reduces to MCC for 2 classes.\n            www.blester125.com/blog/rk.html\n        """"""\n        samples = np.sum(self._cm)\n        correct = np.trace(self._cm)\n        true = np.sum(self._cm, axis=1, dtype=np.float64)\n        guess = np.sum(self._cm, axis=0, dtype=np.float64)\n\n        cov_guess_true = correct * samples - np.dot(guess, true)\n        cov_true_true = samples * samples - np.dot(true, true)\n        cov_guess_guess = samples * samples - np.dot(guess, guess)\n\n        denom = np.sqrt(cov_guess_guess * cov_true_true)\n        denom = denom if denom != 0.0 else 1.0\n        return cov_guess_true / denom\n\n    def get_mcc(self):\n        """"""Get the Mathews correlation coefficient.\n        R k reduces to the Matthews Correlation Coefficient for two classes.\n        People are often more familiar with the name MCC than R k so we provide this alias\n        """"""\n        return self.get_r_k()\n\n    def get_all_metrics(self):\n        """"""Make a map of metrics suitable for reporting, keyed by metric name\n\n        :return: (``dict``) Map of metrics keyed by metric names\n        """"""\n        metrics = {""acc"": self.get_acc()}\n        # If 2 class, assume second class is positive AKA 1\n        if len(self.labels) == 2:\n            metrics[""precision""] = self.get_precision()[1]\n            metrics[""recall""] = self.get_recall()[1]\n            metrics[""f1""] = self.get_f(1)\n            metrics[""mcc""] = self.get_mcc()\n        else:\n            metrics[""mean_precision""] = self.get_mean_precision()\n            metrics[""mean_recall""] = self.get_mean_recall()\n            metrics[""macro_f1""] = self.get_macro_f(1)\n            metrics[""weighted_precision""] = self.get_weighted_precision()\n            metrics[""weighted_recall""] = self.get_weighted_recall()\n            metrics[""weighted_f1""] = self.get_weighted_f(1)\n            metrics[""r_k""] = self.get_r_k()\n        return metrics\n\n    def add_batch(self, truth, guess):\n        """"""Add a batch of data to the confusion matrix\n\n        :param truth: The truth tensor\n        :param guess: The guess tensor\n        :return:\n        """"""\n        for truth_i, guess_i in zip(truth, guess):\n            self.add(truth_i, guess_i)\n\n    @classmethod\n    def create(cls, truth, guess):\n        """"""Build a Confusion Matrix from truths and guesses.\n        If there are classes missing from the union of golds and preds they will\n        be missing from the confusion matrix\n        The classes are added in sorted order, if they are ints already this means\n        they were probably already encoded and we will hopefully respect the order\n        they are in.\n        :param truth: List[Union[int. str]] The correct labels\n        :param guess: List[Union[int. str]] The predicted labels\n        :returns: ConfusionMatrix\n        """"""\n        label_index = {i: k for i, k in enumerate(sorted(set(chain(truth, guess))))}\n        rev_lut = {v: k for k, v in label_index.items()}\n        cm = cls(label_index)\n        for g, p in zip(truth, guess):\n            cm.add(rev_lut[g], rev_lut[p])\n        return cm\n'"
layers/eight_mile/conlleval.py,0,"b'""""""An implementation of conlleval.pl https://www.clips.uantwerpen.be/conll2000/chunking/output.html\n\nInput is a conll file with the rightmost columns being gold and predicted tags\nin that order. Sentences are separated by a blank line.\n\nSome parts of the perl script are not replicated (this doesn\'t generate a latex\ntable) but there are other improvements like it supports files in the `IOBES`\nformat.\n\nThe script produces the same output as conlleval.pl for BIO and IOB tagged file.\nWhen running on IOBES files it will produce the same F1 scores as if you\nconverted the file to BIO and run conlleval.pl however the accuracy will be\nslightly different IOBES accuracy will always equal to or lower than BIO score.\n\nThis difference is because if the error is on a B-, I-, or O token then the\nerror will be in both IOBES and BIO. In the conversion from IOBES to BIO then\nS- is converted to B- and E- to I- if the IOBES was correct then they will be\nconverted and will still be correct. If the token was wrong in that is said\nit was O or something then it will be wrong after the conversion too. If it was\nwrong in that the S- was tagged as B- or the E- was an I- (this is possible and\na common error) then when the gold is changed these will become correct. So it\nis only possible that the conversion will make some answers correct. It won\'t\nmake anything that was correct wrong.\n""""""\n\nimport sys\nimport argparse\nfrom itertools import chain\nfrom eight_mile.utils import to_chunks, per_entity_f1, conlleval_output, read_conll_sentences\n\n\ndef _read_conll_file(f, delim):\n    """"""Read a golds and predictions out of a conll file.\n\n    :param f: `file` The open file object.\n    :param delim: `str` The symbol that separates columns in the file.\n\n    :returns: `Tuple[List[List[str]], List[List[str]]]` The golds\n        and the predictions. They are aligned lists and each element\n        is a List of strings that are the list of tags.\n\n    Note:\n        the file should contain lines with items separated\n        by $delimiter characters (default space). The final\n        two items should contain the correct tag and the\n        guessed tag in that order. Sentences should be\n        separated from each other by empty lines.\n    """"""\n    golds = []\n    preds = []\n    for lines in read_conll_sentences(f, delim=delim):\n        golds.append([l[-2] for l in lines])\n        preds.append([l[-1] for l in lines])\n    return golds, preds\n\n\ndef _get_accuracy(golds, preds):\n    """"""Calculate the token level accuracy.\n\n    :param golds: `List[List[str]]` The list of golds of each example.\n    :param preds: `List[List[str]]` The list of predictions of each example.\n\n    :returns: `Tuple[float, int]` The Accuracy and the total number of tokens.\n    """"""\n    total = 0\n    correct = 0\n    for g, p in zip(chain(*golds), chain(*preds)):\n        if g == p:\n            correct += 1\n        total += 1\n    return correct / float(total) * 100, total\n\n\ndef _get_entites(golds, preds, span_type=""iobes"", verbose=False):\n    """""" Convert the tags into sets of entities.\n\n    :param golds: `List[List[str]]` The list of gold tags.\n    :param preds: `List[List[str]]` The list of predicted tags.\n    :param span_type: `str` The span labeling scheme used.\n    :param verbose: `bool` Should warnings be printed when an illegal transistion is found.\n    """"""\n    golds = [set(to_chunks(g, span_type, verbose)) for g in golds]\n    preds = [set(to_chunks(p, span_type, verbose)) for p in preds]\n    return golds, preds\n\n\ndef main():\n    """"""Use as a cli tool like conlleval.pl""""""\n    usage = ""usage: %(prog)s [--span_type {bio,iobes,iob}] [-d delimiterTag] [-v] < file""\n    parser = argparse.ArgumentParser(\n        description=""Calculate Span level F1 from the CoNLL-2000 shared task."", usage=usage\n    )\n    parser.add_argument(\n        ""--span_type"",\n        default=""iobes"",\n        choices={""iobes"", ""iob"", ""bio""},\n        help=""What tag annotation scheme is this file using."",\n    )\n    parser.add_argument(""--delimiterTag"", ""-d"", default="" "", help=""The separator between items in the file."")\n    parser.add_argument(\n        ""--verbose"", ""-v"", action=""store_true"", help=""Output warnings when there is an illegal transition.""\n    )\n    args = parser.parse_args()\n\n    golds, preds = _read_conll_file(sys.stdin, args.delimiterTag)\n    acc, tokens = _get_accuracy(golds, preds)\n    golds, preds = _get_entites(golds, preds, args.span_type, args.verbose)\n    metrics = per_entity_f1(golds, preds)\n    metrics[""acc""] = acc\n    metrics[""tokens""] = tokens\n    print(conlleval_output(metrics))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
layers/eight_mile/embeddings.py,0,"b'import io\nimport copy\nimport logging\nimport collections\nimport contextlib\nimport numpy as np\nfrom eight_mile.utils import optional_params, exporter, write_json, read_config_file, Offsets, mime_type, revlut\n__all__ = []\nexport = exporter(__all__)\nlogger = logging.getLogger(""mead.layers"")\n\n\n@export\ndef norm_weights(word_vectors):\n    norms = np.linalg.norm(word_vectors, axis=1, keepdims=True)\n    norms = (norms == 0) + norms\n    return word_vectors / norms\n\n\n@export\ndef write_word2vec_file(filename, vocab, word_vectors):\n    """"""Write out a binary word2vec file\n\n    This function allows either a list of vocabulary of the same size\n    as the vectors, in which case it does a simple write, or a vocabulary\n    dict which may contain ""magic"" Offset values that baseline uses.  If\n    those are populated offsets in the dict, they will occupy the low indices\n    (first values), so to prune them, remove them from the vocab and shift the\n    indices down\n\n    :param filename: (`str`) The filename\n    :param vocab: (`Dict[str, int]` or `List[str]`) A list of vocabulary\n    :param word_vectors: The word vectors to write for each vocab\n    :return: None\n    """"""\n\n    offset = 0\n    # Check for case where we have a dict and possibly magic values to prune\n    if isinstance(vocab, collections.Mapping):\n        vocab_copy = copy.deepcopy(vocab)\n\n        for v in Offsets.VALUES:\n            if v in vocab_copy:\n                del vocab_copy[v]\n                offset += 1\n        vocab_list = [0] * len(vocab_copy)\n        for word, idx in vocab_copy.items():\n            vocab_list[idx - offset] = word\n    # Otherwise its just a list dont do anything weird\n    else:\n        vocab_list = vocab\n\n    # Writing the file is pretty simple, just need the vocab size and depth\n    # Then write each line\n    with io.open(filename, ""wb"") as f:\n        word_vectors_offset = word_vectors[offset:]\n        vsz = len(vocab_list)\n        dsz = word_vectors[0].shape[0]\n        f.write(bytes(""{} {}\\n"".format(vsz, dsz), encoding=""utf-8""))\n        assert len(vocab_list) == len(word_vectors_offset)\n        for word, vector in zip(vocab_list, word_vectors_offset):\n            vec_str = vector.tobytes()\n            f.write(bytes(""{} "".format(word), encoding=""utf-8"") + vec_str)\n\n\n@export\nclass EmbeddingsModel(object):\n    def __init__(self):\n        super().__init__()\n\n    def get_dsz(self):\n        pass\n\n    def get_vsz(self):\n        pass\n\n    def get_vocab(self):\n        pass\n\n    def save_md(self, target):\n        pass\n\n\ndef pool_vec(embeddings, tokens, operation=np.mean):\n    if type(tokens) is str:\n        tokens = tokens.split()\n    try:\n        return operation([embeddings.lookup(t, False) for t in tokens], 0)\n    except:\n        return embeddings.weights[0]\n\n\n@export\nclass WordEmbeddingsModel(EmbeddingsModel):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.vocab = kwargs.get(""vocab"")\n        self.vsz = kwargs.get(""vsz"")\n        self.dsz = kwargs.get(""dsz"")\n        self.weights = kwargs.get(""weights"")\n        if ""md_file"" in kwargs:\n            md = read_config_file(kwargs[""md_file""])\n            self.vocab = md[""vocab""]\n            self.vsz = md[""vsz""]\n            self.dsz = md[""dsz""]\n        if ""weights_file"" in kwargs:\n            self.weights = np.load(kwargs[""weights_file""]).get(""arr_0"")\n\n        if self.weights is not None:\n            if self.vsz is None:\n                self.vsz = self.weights.shape[0]\n            else:\n                assert self.vsz == self.weights.shape[0]\n            if self.dsz is None:\n                self.dsz = self.weights.shape[1]\n            else:\n                assert self.dsz == self.weights.shape[1]\n\n        elif self.vsz is not None and self.dsz is not None:\n            self.weights = np.zeros((self.vsz, self.dsz))\n\n    def get_dsz(self):\n        return self.dsz\n\n    def get_vsz(self):\n        return self.vsz\n\n    def get_vocab(self):\n        return self.vocab\n\n    def get_weights(self):\n        return self.weights\n\n    def save_md(self, target):\n        write_json({""vsz"": self.get_vsz(), ""dsz"": self.get_dsz(), ""vocab"": self.get_vocab()}, target)\n\n    def save_weights(self, target):\n        np.savez(target, self.weights)\n\n    def lookup(self, word, nullifabsent=True):\n        if word in self.vocab:\n            return self.weights[self.vocab[word]]\n        if nullifabsent:\n            return None\n        return self.nullv\n\n    def __getitem__(self, word):\n        return self.lookup(word, nullifabsent=False)\n\n\n@export\nclass PretrainedEmbeddingsModel(WordEmbeddingsModel):\n    def __init__(self, filename, known_vocab=None, unif_weight=None, keep_unused=False, normalize=False, **kwargs):\n        super().__init__()\n\n        if (known_vocab is None or not known_vocab) and keep_unused is False:\n            logger.warning(\n                ""Warning: known_vocab=None or is Empty, keep_unused=False. Setting keep_unused=True, all vocab will be preserved""\n            )\n            keep_unused = True\n        uw = 0.0 if unif_weight is None else unif_weight\n        self.vocab = {}\n        # Set the start offset to one past the last special token\n        idx = Offsets.OFFSET\n\n        word_vectors, self.dsz, known_vocab, idx = self._read_vectors(filename, idx, known_vocab, keep_unused, **kwargs)\n        self.nullv = np.zeros(self.dsz, dtype=np.float32)\n        special_tokens = [self.nullv]\n        for i in range(1, len(Offsets.VALUES)):\n            special_tokens.append(np.random.uniform(-uw, uw, self.dsz).astype(np.float32))\n        word_vectors = special_tokens + word_vectors\n        # Add ""well-known"" values to the vocab\n        for i, name in enumerate(Offsets.VALUES):\n            self.vocab[name] = i\n\n        if known_vocab is not None:\n            # Remove ""well-known"" values\n            for name in Offsets.VALUES:\n                known_vocab.pop(name, 0)\n            unknown = {v: cnt for v, cnt in known_vocab.items() if cnt > 0}\n            for v in unknown:\n                word_vectors.append(np.random.uniform(-uw, uw, self.dsz).astype(np.float32))\n                self.vocab[v] = idx\n                idx += 1\n\n        self.weights = np.array(word_vectors)\n        if normalize is True:\n            self.weights = norm_weights(self.weights)\n\n        self.vsz = self.weights.shape[0]\n        assert self.weights.dtype == np.float32\n\n    def _read_vectors(self, filename, idx, known_vocab, keep_unused, **kwargs):\n        use_mmap = bool(kwargs.get(""use_mmap"", False))\n        read_fn = self._read_word2vec_file\n        is_glove_file = mime_type(filename) == ""text/plain""\n        if use_mmap:\n            if is_glove_file:\n                read_fn = self._read_text_mmap\n            else:\n                read_fn = self._read_word2vec_mmap\n        elif is_glove_file:\n            read_fn = self._read_text_file\n\n        return read_fn(filename, idx, known_vocab, keep_unused)\n\n    def _read_word2vec_file(self, filename, idx, known_vocab, keep_unused):\n        word_vectors = []\n        with io.open(filename, ""rb"") as f:\n            header = f.readline()\n            vsz, dsz = map(int, header.split())\n            width = 4 * dsz\n            for i in range(vsz):\n                word = self._readtospc(f)\n                raw = f.read(width)\n                if word in self.vocab:\n                    continue\n                if keep_unused is False and word not in known_vocab:\n                    continue\n                if known_vocab and word in known_vocab:\n                    known_vocab[word] = 0\n                vec = np.fromstring(raw, dtype=np.float32)\n                word_vectors.append(vec)\n                self.vocab[word] = idx\n                idx += 1\n        return word_vectors, dsz, known_vocab, idx\n\n    @staticmethod\n    def _read_word2vec_line_mmap(m, width, start):\n        current = start + 1\n        while m[current : current + 1] != b"" "":\n            current += 1\n        vocab = m[start:current].decode(""utf-8"").strip("" \\n"")\n        raw = m[current + 1 : current + width + 1]\n        value = np.fromstring(raw, dtype=np.float32)\n        return vocab, value, current + width + 1\n\n    def _read_word2vec_mmap(self, filename, idx, known_vocab, keep_unused):\n        import mmap\n\n        word_vectors = []\n        with io.open(filename, ""rb"") as f:\n            with contextlib.closing(mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)) as m:\n                header_end = m[:50].find(b""\\n"")\n                vsz, dsz = map(int, (m[:header_end]).split(b"" ""))\n                width = 4 * dsz\n                current = header_end + 1\n                for i in range(vsz):\n                    word, vec, current = self._read_word2vec_line_mmap(m, width, current)\n                    if word in self.vocab:\n                        continue\n                    if keep_unused is False and word not in known_vocab:\n                        continue\n                    if known_vocab and word in known_vocab:\n                        known_vocab[word] = 0\n\n                    word_vectors.append(vec)\n                    self.vocab[word] = idx\n                    idx += 1\n                return word_vectors, dsz, known_vocab, idx\n\n    @staticmethod\n    def _readtospc(f):\n\n        s = bytearray()\n        ch = f.read(1)\n\n        while ch != b"" "":\n            s.extend(ch)\n            ch = f.read(1)\n        s = s.decode(""utf-8"")\n        # Only strip out normal space and \\n not other spaces which are words.\n        return s.strip("" \\n"")\n\n    def _read_text_file(self, filename, idx, known_vocab, keep_unused):\n        word_vectors = []\n\n        with io.open(filename, ""r"", encoding=""utf-8"") as f:\n            for i, line in enumerate(f):\n                line = line.rstrip(""\\n "")\n                values = line.split("" "")\n                word = values[0]\n                if i == 0 and len(values) == 2:\n                    print(""VSZ: {}, DSZ: {}"".format(word, values[1]))\n                    continue\n                if word in self.vocab:\n                    continue\n                if keep_unused is False and word not in known_vocab or word in self.vocab:\n                    continue\n                if known_vocab and word in known_vocab:\n                    known_vocab[word] = 0\n                vec = np.asarray(values[1:], dtype=np.float32)\n                word_vectors.append(vec)\n                self.vocab[word] = idx\n                idx += 1\n        dsz = vec.shape[0]\n        return word_vectors, dsz, known_vocab, idx\n\n    def _read_text_mmap(self, filename, idx, known_vocab, keep_unused):\n        import mmap\n\n        word_vectors = []\n        with io.open(filename, ""r"", encoding=""utf-8"") as f:\n            with contextlib.closing(mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)) as m:\n                for i, line in enumerate(iter(m.readline, b"""")):\n                    line = line.rstrip(b""\\n"")\n                    values = line.split(b"" "")\n                    if i == 0 and len(values) == 2:\n                        print(""VSZ: {}, DSZ: {}"".format(values[0], values[1]))\n                        continue\n                    if len(values) == 0:\n                        break\n                    word = values[0].decode(""utf-8"").strip("" \\n"")\n                    if word in self.vocab:\n                        continue\n                    if keep_unused is False and word not in known_vocab:\n                        continue\n                    if known_vocab and word in known_vocab:\n                        known_vocab[word] = 0\n                    vec = np.asarray(values[1:], dtype=np.float32)\n                    word_vectors.append(vec)\n                    self.vocab[word] = idx\n                    idx += 1\n                dsz = vec.shape[0]\n                return word_vectors, dsz, known_vocab, idx\n\n\n@export\nclass PretrainedEmbeddingsStack(EmbeddingsModel):\n    def __init__(self, filenames, known_vocab, counts=True, unif_weight=None, normalize=False, **kwargs):\n        uw = 0.0 if unif_weight is None else unif_weight\n\n        self.vocab = dict()\n        for i, name in enumerate(Offsets.VALUES):\n            self.vocab[name] = i\n        self.vsz = Offsets.OFFSET\n\n        if counts is True:\n            for name in Offsets.VALUES:\n                known_vocab.pop(name, 0)\n            attested = [v for v, cnt in known_vocab.items() if cnt > 0]\n            for k, v in enumerate(attested):\n                self.vocab[v] = k + Offsets.OFFSET\n                self.vsz += 1\n        else:\n            self.vocab = known_vocab\n            self.vsz = max(self.vocab.values()) + 1\n\n        index2word = revlut(self.vocab)\n        # vocab = word2index\n        embeddings = []\n\n        for file in filenames:\n            embeddings.append(PretrainedEmbeddingsModel(file, known_vocab))\n\n        self.dsz = sum([embedding.dsz for embedding in embeddings])\n        self.weights = np.random.uniform(-uw, uw, (self.vsz, self.dsz)).astype(np.float32)\n\n        for i in range(len(self.vocab.keys())):\n                w = index2word[i]\n                e = []\n                for emb in embeddings:\n                    e.append(emb.lookup(w, False))\n                self.weights[i] = np.concatenate(e)\n        if normalize is True:\n            self.weights = norm_weights(self.weights)\n\n    def __getitem__(self, word):\n        return self.lookup(word, nullifabsent=False)\n\n    def lookup(self, word, nullifabsent=True):\n        if word in self.vocab:\n            return self.weights[self.vocab[word]]\n        if nullifabsent:\n            return None\n        return np.zeros(self.dsz, dtype=np.float32)\n\n    def get_vocab(self):\n        return self.vocab\n\n    def get_dsz(self):\n        return self.dsz\n\n    def get_vsz(self):\n        return self.vsz\n\n    def save_md(self, target):\n        write_json({""vsz"": self.get_vsz(), ""dsz"": self.get_dsz(), ""vocab"": self.get_vocab()}, target)\n\n\n\n@export\nclass RandomInitVecModel(EmbeddingsModel):\n    def __init__(self, dsz, known_vocab, counts=True, unif_weight=None):\n        super().__init__()\n        uw = 0.0 if unif_weight is None else unif_weight\n        self.vocab = dict()\n        for i, name in enumerate(Offsets.VALUES):\n            self.vocab[name] = i\n        self.dsz = dsz\n        self.vsz = Offsets.OFFSET\n\n        if counts is True:\n            for name in Offsets.VALUES:\n                known_vocab.pop(name, 0)\n            attested = [v for v, cnt in known_vocab.items() if cnt > 0]\n            for k, v in enumerate(attested):\n                self.vocab[v] = k + Offsets.OFFSET\n                self.vsz += 1\n        else:\n            self.vocab = known_vocab\n            self.vsz = max(self.vocab.values()) + 1\n\n        self.weights = np.random.uniform(-uw, uw, (self.vsz, self.dsz)).astype(np.float32)\n\n        self.nullv = np.zeros(self.dsz, dtype=np.float32)\n\n        self.weights[0] = self.nullv\n        for i in range(1, len(Offsets.VALUES)):\n            self.weights[i] = np.random.uniform(-uw, uw, self.dsz).astype(np.float32)\n\n    def __getitem__(self, word):\n        return self.lookup(word, nullifabsent=False)\n\n    def lookup(self, word, nullifabsent=True):\n        if word in self.vocab:\n            return self.weights[self.vocab[word]]\n        if nullifabsent:\n            return None\n        return self.nullv\n\n    def get_vocab(self):\n        return self.vocab\n\n    def get_dsz(self):\n        return self.dsz\n\n    def get_vsz(self):\n        return self.vsz\n\n    def save_md(self, target):\n        write_json({""vsz"": self.get_vsz(), ""dsz"": self.get_dsz(), ""vocab"": self.get_vocab()}, target)\n\n\n\n'"
layers/eight_mile/optz.py,0,"b'import math\nimport numpy as np\nfrom eight_mile.utils import exporter, listify\nfrom eight_mile.utils import optional_params, register\n\n\n__all__ = []\nexport = exporter(__all__)\n\nMEAD_LAYERS_LR_SCHEDULERS = {}\nexport = exporter(__all__)\n\n\n@export\n@optional_params\ndef register_lr_scheduler(cls, name=None):\n    return register(cls, MEAD_LAYERS_LR_SCHEDULERS, name, ""lr_scheduler"")\n\n\n@export\ndef create_lr_scheduler(**kwargs):\n    """"""Create a learning rate scheduler.\n\n    :Keyword Arguments:\n      * *lr_scheduler_type* `str` or `list` The name of the learning rate scheduler\n          if list then the first scheduler should be a warmup scheduler.\n    """"""\n    sched_type = kwargs.get(""lr_scheduler_type"")\n    if sched_type is None:\n        return None\n    sched_type = listify(sched_type)\n    if len(sched_type) == 2:\n        warm = MEAD_LAYERS_LR_SCHEDULERS.get(sched_type[0])(**kwargs)\n        assert isinstance(warm, WarmupLearningRateScheduler), ""First LR Scheduler must be a warmup scheduler.""\n        rest = MEAD_LAYERS_LR_SCHEDULERS.get(sched_type[1])(**kwargs)\n        return MEAD_LAYERS_LR_SCHEDULERS.get(""composite"")(warm=warm, rest=rest, **kwargs)\n    Constructor = MEAD_LAYERS_LR_SCHEDULERS.get(sched_type[0])\n    return Constructor(**kwargs)\n\n\n@export\nclass LearningRateScheduler:\n    def __init__(self, **kwargs):\n        self.lr = kwargs.get(""lr"", kwargs.get(""eta"", 1.0))\n\n    @staticmethod\n    def _identity(x):\n        return x\n\n\n@export\nclass WarmupLearningRateScheduler(LearningRateScheduler):\n    def __init__(self, warmup_steps=16000, **kwargs):\n        super().__init__(**kwargs)\n        self._warmup_steps = warmup_steps\n\n    @property\n    def warmup_steps(self):\n        return self._warmup_steps\n\n\n@export\nclass ConstantScheduler(LearningRateScheduler):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def __call__(self, global_step):\n        return self.lr\n\n\n@export\nclass WarmupLinearScheduler(WarmupLearningRateScheduler):\n    def __call__(self, global_step):\n        lr_factor = min(1.0, global_step / float(self.warmup_steps))\n        return self.lr * lr_factor\n\n\n@export\nclass CyclicLRScheduler(LearningRateScheduler):\n    def __init__(self, max_lr=1e-2, decay_steps=1000, **kwargs):\n        super().__init__(**kwargs)\n        self.max_lr = max_lr\n        self.decay_steps = decay_steps\n\n    def __call__(self, global_step):\n        cycle = np.floor(1.0 + global_step / (2.0 * self.decay_steps))\n        x = np.abs(global_step / self.decay_steps - 2.0 * cycle + 1.0)\n        new_lr = self.lr + (self.max_lr - self.lr) * np.maximum(0.0, 1.0 - x)\n        return new_lr\n\n\n@export\nclass PiecewiseDecayScheduler(LearningRateScheduler):\n    def __init__(self, boundaries, values, **kwargs):\n        super().__init__(**kwargs)\n        self.boundaries = boundaries\n        self.values = values\n\n    def __call__(self, global_step):\n        pos = np.searchsorted(self.boundaries, global_step)\n        return self.values[pos]\n\n\n@export\nclass ZarembaDecayScheduler(PiecewiseDecayScheduler):\n    def __init__(self, boundaries=None, decay_rate=None, **kwargs):\n        lr = kwargs.get(""lr"", kwargs.get(""eta"", 1.0))\n\n        if boundaries is None or decay_rate is None:\n            boundaries = []\n            values = [lr]\n        else:\n            values = [lr / (decay_rate ** i) for i in range(len(boundaries) + 1)]\n        super().__init__(boundaries, values, **kwargs)\n\n\n@export\nclass CosineDecayScheduler(LearningRateScheduler):\n    def __init__(self, decay_steps=1000, alpha=0.0, **kwargs):\n        super().__init__(**kwargs)\n        self.decay_steps = decay_steps\n        self.alpha = alpha\n\n    def __call__(self, global_step):\n        global_step = min(global_step, self.decay_steps)\n        cosine_decay = 0.5 * (1 + np.cos(np.pi * global_step / self.decay_steps))\n        decayed = (1 - self.alpha) * cosine_decay + self.alpha\n        return self.lr * decayed\n\n\n@export\nclass InverseTimeDecayScheduler(LearningRateScheduler):\n    def __init__(self, decay_steps=16000, decay_rate=0.05, staircase=False, **kwargs):\n        super().__init__(**kwargs)\n        self.decay_steps = decay_steps\n        self.decay_rate = decay_rate\n        self.wrap_fn = math.floor if staircase else LearningRateScheduler._identity\n\n    def __call__(self, global_step):\n        t = self.wrap_fn(global_step / self.decay_steps)\n        return self.lr / (1.0 + self.decay_rate * t)\n\n\n@export\nclass ExponentialDecayScheduler(LearningRateScheduler):\n    def __init__(self, decay_steps=16000, decay_rate=0.5, staircase=False, **kwargs):\n        super().__init__(**kwargs)\n        self.decay_steps = decay_steps\n        self.decay_rate = decay_rate\n        self.wrap_fn = math.floor if staircase else LearningRateScheduler._identity\n\n    def __call__(self, global_step):\n        t = self.wrap_fn(global_step / float(self.decay_steps))\n        return self.lr * self.decay_rate ** t\n\n\n@export\nclass CompositeLRScheduler(LearningRateScheduler):\n    def __init__(self, warm=None, rest=None, **kwargs):\n        super().__init__(**kwargs)\n        self.warm = warm\n        self.rest = rest\n\n    def __call__(self, global_step):\n        if global_step < self.warm.warmup_steps:\n            return self.warm(global_step)\n        return self.rest(global_step - self.warm.warmup_steps)\n'"
layers/eight_mile/progress.py,0,"b'import re\nimport sys\nimport six\nfrom eight_mile.utils import exporter\n\n\n__all__ = []\nexport = exporter(__all__)\n\n\n@export\nclass Progress(object):\n    """"""Progress hook\n\n    Provide interface for progress updates\n    """"""\n\n    def __init__(self):\n        pass\n\n    def update(self, step=1):\n        """"""Update progress by number of `steps`\n\n        :param step: The number of tasks completed\n        :return:\n        """"""\n        pass\n\n    def done(self):\n        """"""Triggered when all tasks are completed\n\n        :return:\n        """"""\n        pass\n\n    def __call__(self, real_iter):\n        return self.__iter__(real_iter)\n\n    def __iter__(self, real_iter):\n        for x in real_iter:\n            yield x\n            self.update()\n        self.done()\n\n\n@export\nclass ProgressBarJupyter(Progress):\n    """"""Simple Jupyter progress bar\n\n    Writes a progress bar to an ipython widget\n\n    """"""\n\n    def __init__(self, total):\n        super(ProgressBarJupyter, self).__init__()\n        from ipywidgets import FloatProgress\n        from IPython.display import display\n\n        self.progress = FloatProgress(min=0, max=total)\n        display(self.progress)\n\n    def update(self, step=1):\n        self.progress.value += step\n\n    def done(self):\n        """"""Close the widget\n        """"""\n        self.progress.close()\n\n\n# Modifed from here\n# http://stackoverflow.com/questions/3160699/python-progress-bar#3160819\n@export\nclass ProgressBarTerminal(Progress):\n    """"""Simple terminal-based progress bar\n\n    Writes a progress bar to the terminal, using a designated `symbol` (which defaults to `=`)\n\n    """"""\n\n    DEFAULT = ""Progress: %(bar)s %(percent)3d%%""\n    FULL = ""%(bar)s %(current)d/%(total)d (%(percent)3d%%) %(remaining)d to go""\n\n    def __init__(self, total, width=40, fmt=DEFAULT, symbol=""=""):\n        super(ProgressBarTerminal, self).__init__()\n        assert len(symbol) == 1\n\n        self.total = total\n        self.width = width\n        self.symbol = symbol\n        self.fmt = re.sub(r""(?P<name>%\\(.+?\\))d"", r""\\g<name>%dd"" % len(str(total)), fmt)\n\n        self.current = 0\n        self.print_ = six.print_ if sys.stdout.isatty() else lambda *args, **kwargs: None\n        # Force initial print of pg for when steps are long.\n        self.update(step=0)\n\n    def update(self, step=1):\n        """"""Update progress bar by number of `steps`\n\n        :param step: The number of tasks completed\n        :return:\n        """"""\n        self.current += step\n        percent = self.current / float(self.total)\n        size = int(self.width * percent)\n        remaining = self.total - self.current\n        bar = ""["" + self.symbol * size + "" "" * (self.width - size) + ""]""\n\n        args = {\n            ""total"": self.total,\n            ""bar"": bar,\n            ""current"": self.current,\n            ""percent"": percent * 100,\n            ""remaining"": remaining,\n        }\n        self.print_(""\\r"" + self.fmt % args, end="""")\n\n    def done(self):\n        """"""All tasks are finished, complete the progress bar\n\n        :return:\n        """"""\n        self.current = self.total\n        self.update(step=0)\n        self.print_("""")\n'"
layers/eight_mile/utils.py,1,"b'import re\nimport io\nimport os\nimport sys\nimport json\nimport logging\nimport inspect\nimport importlib\nimport collections\nfrom itertools import chain\nfrom binascii import hexlify\nfrom functools import partial\nfrom collections import Counter\nfrom typing import List, Tuple, Union, Optional, Dict, Any, Set, Pattern, TextIO\nfrom functools import partial, update_wrapper, wraps\nimport numpy as np\nfrom six.moves.urllib.request import urlretrieve\n\n\nlogger = logging.getLogger(""mead.layers"")\n\n__all__ = [""exporter"", ""optional_params"", ""parameterize""]\nCONLL_EXTS = {"".bio"", "".iobes"", "".iob"", "".conll""}\n\n\ndef optional_params(func):\n    """"""Allow a decorator to be called without parentheses if no kwargs are given.\n\n    parameterize is a decorator, function is also a decorator.\n    """"""\n\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n        """"""If a decorator is called with only the wrapping function just execute the real decorator.\n           Otherwise return a lambda that has the args and kwargs partially applied and read to take a function as an argument.\n\n        *args, **kwargs are the arguments that the decorator we are parameterizing is called with.\n\n        the first argument of *args is the actual function that will be wrapped\n        """"""\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            return func(args[0])\n        return lambda x: func(x, *args, **kwargs)\n\n    return wrapped\n\n\ndef parameterize(func):\n    """"""Allow as decorator to be called with arguments, returns a new decorator that should be called with the function to be wrapped.""""""\n\n    @wraps(func)\n    def decorator(*args, **kwargs):\n        return lambda x: func(x, *args, **kwargs)\n\n    return decorator\n\n\n@parameterize\ndef exporter(obj, all_list: List[str] = None):\n    """"""Add a function or class to the __all__.\n\n    When exporting something with out using as a decorator do it like so:\n        `func = exporter(func)`\n    """"""\n    all_list.append(obj.__name__)\n    return obj\n\n\nexport = exporter(__all__)\n\n\n@export\ndef register(cls, registry, name=None, error=""""):\n    if name is None:\n        name = cls.__name__\n    if name in registry:\n        raise Exception(\n            ""Error: attempt to re-define previously registered {} {} (old: {}, new: {})"".format(\n                error, name, registry[name], cls\n            )\n        )\n    if hasattr(cls, ""create""):\n        registry[name] = cls.create\n    else:\n        registry[name] = cls\n    return cls\n\n\n@export\nclass Offsets:\n    """"""Support pre 3.4""""""\n\n    PAD, GO, EOS, UNK, OFFSET = range(0, 5)\n    VALUES = [""<PAD>"", ""<GO>"", ""<EOS>"", ""<UNK>""]\n\n\n@export\ndef get_logging_level(level: str) -> int:\n    """"""Get the logging level as a logging module constant.\n\n    :param level: `str` The log level to get.\n\n    :returns: The log level, defaults to `INFO`\n    """"""\n    return getattr(logging, level.upper(), logging.INFO)\n\n\n@export\ndef sequence_mask(lengths, max_len: int = -1):\n    if max_len < 0:\n        max_len = np.max(lengths)\n    row = np.arange(0, max_len).reshape(1, -1)\n    col = np.reshape(lengths, (-1, 1))\n    return (row < col).astype(np.uint8)\n\n\n@export\ndef calc_nfeats(\n    filtsz: Union[List[Tuple[int, int]], List[int]],\n    nfeat_factor: Optional[int] = None,\n    max_feat: Optional[int] = None,\n    nfeats: Optional[int] = None,\n) -> Tuple[List[int], List[int]]:\n    """"""Calculate the output sizes to use for multiple parallel convolutions.\n\n    If filtsz is a List of Lists of ints then we assume that each element represents\n        a filter size, feature size pair. This is the format used by ELMo\n    If filtsz is a List of ints we assume each element represents a filter size\n    If nfeat_factor and max_feat are set we calculate the nfeat size based on the\n        nfeat_factor and the filter size capped by max_feat. This is the method used\n        in Kim et. al. 2015 (https://arxiv.org/abs/1508.06615)\n    Otherwise nfeats must be set and we assume this is output size to use for all of\n        the parallel convs and return the feature size expanded to list the same length\n        as filtsz\n\n    :param filtsz: The filter sizes to use in parallel\n    :param nfeat_factor: How to scale the feat size as you grow the filters\n    :param max_feat: The cap on the feature size\n    :param nfeats: A fall back constant feature size\n    :returns: Associated arrays where the first one is the filter sizes and the second\n        one has the corresponding number of feats as the output\n    """"""\n    # If this is a list, then its a tuple of (filtsz, nfeats)\n    if is_sequence(filtsz[0]):\n        filtsz, nfeats = zip(*filtsz)\n    # If we get a nfeat factor, we multiply that by each filter, and thresh at max_feat\n    elif nfeat_factor is not None:\n        assert max_feat is not None, ""If using `nfeat_factor`, `max_feat` must not be None""\n        nfeats = [min(nfeat_factor * fsz, max_feat) for fsz in filtsz]\n    # Otherwise its just a scalar\n    else:\n        assert nfeats is not None, ""When providing only `filtsz` and not `nfeat_factor` `nfeats` must be specified""\n        assert isinstance(\n            nfeats, int\n        ), ""If you want to use custom nfeat sizes do `filtsz = zip(filtsz, nfeats)` then call this function""\n        nfeats = [nfeats] * len(filtsz)\n    return filtsz, nfeats\n\n\n@export\ndef transition_mask(vocab: Dict[str, int], span_type: str, s_idx: int, e_idx: int, pad_idx: Optional[int] = None):\n    """"""Create a CRF mask.\n\n    Returns a mask with invalid moves as 0 and valid as 1.\n\n    :param vocab: dict, Label vocabulary mapping name to index.\n    :param span_type: str, The sequence labeling formalism {IOB, IOB2, BIO, or IOBES}\n    :param s_idx: int, What is the index of the GO symbol?\n    :param e_idx: int, What is the index of the EOS symbol?\n    :param pad_idx: int, What is the index of the PAD symbol?\n\n    Note:\n        In this mask the PAD symbol is between the last symbol and EOS, PADS can\n        only move to pad and the EOS. Any symbol that can move to an EOS can also\n        move to a pad.\n    """"""\n    rev_lut = {v: k for k, v in vocab.items()}\n    start = rev_lut[s_idx]\n    end = rev_lut[e_idx]\n    pad = None if pad_idx is None else rev_lut[pad_idx]\n    if span_type.upper() == ""IOB"":\n        mask = iob_mask(vocab, start, end, pad)\n    if span_type.upper() in (""IOB2"", ""BIO""):\n        mask = iob2_mask(vocab, start, end, pad)\n    if span_type.upper() == ""IOBES"":\n        mask = iobes_mask(vocab, start, end, pad)\n    return mask\n\n\n@export\ndef iob_mask(vocab, start, end, pad=None):\n    small = 0\n    mask = np.ones((len(vocab), len(vocab)), dtype=np.float32)\n    for from_ in vocab:\n        for to in vocab:\n            # Can\'t move to start\n            if to is start:\n                mask[vocab[to], vocab[from_]] = small\n            # Can\'t move from end\n            if from_ is end:\n                mask[vocab[to], vocab[from_]] = small\n            # Can only move from pad to pad or end\n            if from_ is pad:\n                if not (to is pad or to is end):\n                    mask[vocab[to], vocab[from_]] = small\n            elif from_ is start:\n                # Can\'t move from start to a B\n                if to.startswith(""B-""):\n                    mask[vocab[to], vocab[from_]] = small\n            else:\n                if from_.startswith(""B-""):\n                    # Can\'t move from a B to a B of another type\n                    if to.startswith(""B-""):\n                        from_type = from_.split(""-"", maxsplit=1)[1]\n                        to_type = to.split(""-"", maxsplit=1)[1]\n                        if from_type != to_type:\n                            mask[vocab[to], vocab[from_]] = small\n                elif from_.startswith(""I-""):\n                    # Can\'t move from an I to a B of another type\n                    if to.startswith(""B-""):\n                        from_type = from_.split(""-"", maxsplit=1)[1]\n                        to_type = to.split(""-"", maxsplit=1)[1]\n                        if from_type != to_type:\n                            mask[vocab[to], vocab[from_]] = small\n                elif from_.startswith(""O""):\n                    # Can\'t move from an O to a B\n                    if to.startswith(""B-""):\n                        mask[vocab[to], vocab[from_]] = small\n    return mask\n\n\n@export\ndef iob2_mask(vocab, start, end, pad=None):\n    small = 0\n    mask = np.ones((len(vocab), len(vocab)), dtype=np.float32)\n    for from_ in vocab:\n        for to in vocab:\n            # Can\'t move to start\n            if to is start:\n                mask[vocab[to], vocab[from_]] = small\n            # Can\'t move from end\n            if from_ is end:\n                mask[vocab[to], vocab[from_]] = small\n            # Can only move from pad to pad or end\n            if from_ is pad:\n                if not (to is pad or to is end):\n                    mask[vocab[to], vocab[from_]] = small\n            elif from_ is start:\n                # Can\'t move from start to a I\n                if to.startswith(""I-""):\n                    mask[vocab[to], vocab[from_]] = small\n            else:\n                if from_.startswith(""B-""):\n                    # Can\'t move from a B to an I of a different type\n                    if to.startswith(""I-""):\n                        from_type = from_.split(""-"", maxsplit=1)[1]\n                        to_type = to.split(""-"", maxsplit=1)[1]\n                        if from_type != to_type:\n                            mask[vocab[to], vocab[from_]] = small\n                elif from_.startswith(""I-""):\n                    # Can\'t move from an I to an I of another type\n                    if to.startswith(""I-""):\n                        from_type = from_.split(""-"", maxsplit=1)[1]\n                        to_type = to.split(""-"", maxsplit=1)[1]\n                        if from_type != to_type:\n                            mask[vocab[to], vocab[from_]] = small\n                elif from_.startswith(""O""):\n                    # Can\'t move from an O to an I\n                    if to.startswith(""I-""):\n                        mask[vocab[to], vocab[from_]] = small\n    return mask\n\n\n@export\ndef iobes_mask(vocab, start, end, pad=None):\n    small = 0\n    mask = np.ones((len(vocab), len(vocab)), dtype=np.float32)\n    for from_ in vocab:\n        for to in vocab:\n            # Can\'t move to start\n            if to is start:\n                mask[vocab[to], vocab[from_]] = small\n            # Can\'t move from end\n            if from_ is end:\n                mask[vocab[to], vocab[from_]] = small\n            # Can only move from pad to pad or to end\n            if from_ is pad:\n                if not (to is pad or to is end):\n                    mask[vocab[to], vocab[from_]] = small\n            elif from_ is start:\n                # Can\'t move from start to I or E\n                if to.startswith(""I-"") or to.startswith(""E-""):\n                    mask[vocab[to], vocab[from_]] = small\n            else:\n                if from_.startswith(""B-""):\n                    # Can\'t move from B to B, S, O, End, or Pad\n                    if to.startswith((""B-"", ""S-"", ""O"")) or to is end or to is pad:\n                        mask[vocab[to], vocab[from_]] = small\n                    # Can only move to matching I or E\n                    elif to.startswith(""I-"") or to.startswith(""E-""):\n                        from_type = from_.split(""-"", maxsplit=1)[1]\n                        to_type = to.split(""-"", maxsplit=1)[1]\n                        if from_type != to_type:\n                            mask[vocab[to], vocab[from_]] = small\n                elif from_.startswith(""I-""):\n                    # Can\'t move from I to B, S, O, End or Pad\n                    if to.startswith((""B-"", ""S-"", ""O"")) or to is end or to is pad:\n                        mask[vocab[to], vocab[from_]] = small\n                    # Can only move to matching I or E\n                    elif to.startswith(""I-"") or to.startswith(""E-""):\n                        from_type = from_.split(""-"", maxsplit=1)[1]\n                        to_type = to.split(""-"", maxsplit=1)[1]\n                        if from_type != to_type:\n                            mask[vocab[to], vocab[from_]] = small\n                elif from_.startswith((""E-"", ""S-"", ""O"")):\n                    # Can\'t move from E to I or E\n                    # Can\'t move from S to I or E\n                    # Can\'t move from O to I or E\n                    if to.startswith(""I-"") or to.startswith(""E-""):\n                        mask[vocab[to], vocab[from_]] = small\n    return mask\n\n\n@export\ndef get_version(pkg):\n    s = ""."".join(pkg.__version__.split(""."")[:2])\n    return float(s)\n\n\n@export\ndef revlut(lut: Dict) -> Dict:\n    return {v: k for k, v in lut.items()}\n\n\n@export\ndef str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (""yes"", ""true"", ""t"", ""y"", ""1""):\n        return True\n    elif v.lower() in (""no"", ""false"", ""f"", ""n"", ""0""):\n        return False\n    else:\n        raise Exception(""Boolean value expected."")\n\n\n@export\ndef is_sequence(x) -> bool:\n    if isinstance(x, str):\n        return False\n    return isinstance(x, (collections.Sequence, collections.MappingView))\n\n\n@export\ndef listify(x: Union[List[Any], Any]) -> List[Any]:\n    """"""Take a scalar or list and make it a list iff not already a sequence or numpy array\n\n    :param x: The input to convert\n    :return: A list\n    """"""\n    if is_sequence(x) or isinstance(x, np.ndarray):\n        return x\n    return [x] if x is not None else []\n\n\n@export\ndef read_json(filepath: str, default_value: Optional[Any] = None, strict: bool = False) -> Dict:\n    """"""Read a JSON file in.  If no file is found and default value is set, return that instead.  Otherwise error\n\n    :param filepath: str, A file to load\n    :param default_value: If the file doesn\'t exist, return return this. Defaults to an empty dict.\n    :param strict: bool, If true raise an error on file not found.\n\n    :return: dict, The read JSON object\n    """"""\n    if not os.path.exists(filepath):\n        if strict:\n            raise IOError(""No file {} found"".format(filepath))\n        return default_value if default_value is not None else {}\n    with open(filepath) as f:\n        return json.load(f)\n\n\n@export\ndef read_yaml(filepath: str, default_value: Optional[Any] = None, strict: bool = False) -> Dict:\n    """"""Read a YAML file in.  If no file is found and default value is set, return that instead.  Otherwise error\n\n    :param filepath: str, A file to load\n    :param default_value: If the file doesn\'t exist, return return this. Defaults to an empty dict.\n    :param strict: bool, If true raise an error on file not found.\n\n    :return: dict, The read yaml object\n    """"""\n    if not os.path.exists(filepath):\n        if strict:\n            raise IOError(""No file {} found"".format(filepath))\n        return default_value if default_value is not None else {}\n    with open(filepath) as f:\n        import yaml\n        from distutils.version import LooseVersion\n\n        if LooseVersion(yaml.__version__) >= LooseVersion(""5.1""):\n            return yaml.load(f, Loader=yaml.FullLoader)\n        return yaml.load(f)\n\n\n@export\ndef read_config_file(config_file: str) -> Dict:\n    """"""Read config file, optionally supports YAML, if dependency was already installed.  O.W. JSON plz\n\n    :param config_file: (``str``) A path to a config file which should be a JSON file, or YAML if pyyaml is installed\n    :return: (``dict``) An object\n    """"""\n    try:\n        return read_yaml(config_file, strict=True)\n    except:\n        return read_json(config_file, strict=True)\n\n\n@export\ndef validate_url(url: str) -> bool:\n    regex = re.compile(\n        r""^(?:http|ftp)s?://""  # http:// or https://\n        r""(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|""  # domain...\n        r""localhost|""  # localhost...\n        r""\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})""  # ...or ip\n        r""(?::\\d+)?""  # optional port\n        r""(?:/?|[/?]\\S+)$"",\n        re.IGNORECASE,\n    )\n    return re.match(regex, url) is not None\n\n\n\n@export\ndef read_config_stream(config_stream) -> Dict:\n    """"""Read config stream.  May be a path to a YAML or JSON file, or a str containing JSON or the name\n    of an env variable, or even a JSON object directly\n\n    :param config_stream:\n    :return:\n    """"""\n    if isinstance(config_stream, (dict, list)) or config_stream is None:\n        return config_stream\n    if os.path.exists(config_stream) and os.path.isfile(config_stream):\n        logger.info(""Reading config file \'{}\'"".format(config_stream))\n        return read_config_file(config_stream)\n    config = config_stream\n    if config_stream.startswith(""hub:""):\n        vec = config_stream.split("":"")\n        version = vec[1]\n        rest = "":"".join(vec[2:])\n        config_stream = f""http://raw.githubusercontent.com/mead-ml/hub/master/{version}/{rest}.yml""\n\n    if config_stream.startswith(""$""):\n        logger.info(""Reading config from \'{}\'"".format(config_stream))\n        config = os.getenv(config_stream[1:])\n    \n    else:\n        if validate_url(config_stream):\n            path_to_save, _ = urlretrieve(config_stream)\n            return read_config_stream(path_to_save)\n        else:\n            logger.info(""No file found \'{}...\', loading as string"".format(config_stream[:12]))\n    return json.loads(config)\n\n\n@export\ndef write_sentence_conll(handle, sentence, gold, txt, idx2label):\n\n    if len(txt) != len(sentence):\n        txt = txt[: len(sentence)]\n\n    try:\n        for word, truth, guess in zip(txt, gold, sentence):\n            handle.write(""%s %s %s\\n"" % (word[""text""], idx2label[truth], idx2label[guess]))\n        handle.write(""\\n"")\n    except:\n        logger.error(""ERROR: Failed to write lines... closing file"")\n        handle.close()\n\n\n@export\ndef ls_props(thing):\n    """"""List all of the properties on some object\n\n    :param thing: Some object\n    :return: The list of properties\n    """"""\n    return [x for x in dir(thing) if isinstance(getattr(type(thing), x, None), property)]\n\n\n@export\ndef idempotent_append(element: Any, data: List[Any]) -> List[Any]:\n    """"""Append to a list if that element is not already in the list.\n\n    :param element: The element to add to the list.\n    :param data: `List` the list to add to.\n    :returns: `List` the list with the element in it.\n    """"""\n    if element not in data:\n        data.append(element)\n    return data\n\n\n@export\ndef parse_module_as_path(module_name):\n    """"""Convert a path to a file to a format that it can be imported.\n\n    :param module_name: The module as a path.\n    :returns: `Tuple[str, str]` the module name (without a file ext) and the\n        absolute path of the dir the file lives in (or \'\' if the module_name\n        is just a filename).\n    """"""\n    module_dir, module_name = os.path.split(module_name)\n    module_dir = os.path.realpath(os.path.expanduser(module_dir)) if module_dir else module_dir\n    module_name, _ = os.path.splitext(module_name)\n    return module_name, module_dir\n\n@export\ndef fill_y(nc, yidx):\n    """"""Convert a `B` sparse array to a dense one, to expand labels\n\n    :param nc: (``int``) The number of labels\n    :param yidx: The sparse array of the labels\n    :return: A dense array\n    """"""\n    xidx = np.arange(0, yidx.shape[0], 1)\n    dense = np.zeros((yidx.shape[0], nc), dtype=int)\n    dense[xidx, yidx] = 1\n    return dense\n\n\n@export\n@optional_params\ndef str_file(func, **kwargs):\n    """"""A decorator to automatically open arguments that are files.\n\n    If there are kwargs then they are name=mode. When the function is\n    called if the argument name is a string then the file is opened with\n    mode.\n\n    If there are no kwargs then it is assumed the first argument is a\n    file that should be opened as \'r\'\n    """"""\n    possible_files = kwargs\n    # We need to have the generator check out here so that the inner function is\n    # either a generator (has a yield) or not. If we were to try to have this if\n    # inside of the open_files then we would always return a generator.\n    if inspect.isgeneratorfunction(func):\n\n        @wraps(func)\n        def open_files(*args, **kwargs):\n            # If no arg names are given then assume the first arg is a file\n            # you want to read from.\n            if not possible_files:\n                if isinstance(args[0], str):\n                    with io.open(args[0], mode=""r"", encoding=""utf-8"") as f:\n                        # Call the function with the file instead we need to\n                        # yield from it until it is done other wise the file\n                        # will be closed after the first yield.\n                        for x in func(f, *args[1:], **kwargs):\n                            yield x\n                else:\n                    for x in func(*args, **kwargs):\n                        yield x\n            else:\n                # Otherwise we have multiple files we want to open\n                to_close = []\n                # Get a dict representation of what it will look like if we\n                # call func with *args and **kwargs\n                arg = inspect.getcallargs(func, *args, **kwargs)\n                try:\n                    for f, mode in possible_files.items():\n                        if isinstance(arg[f], str):\n                            # Replace strings with the opened files\n                            arg[f] = io.open(arg[f], mode=mode, encoding=None if ""b"" in mode else ""utf-8"")\n                            to_close.append(f)\n                    # Call the function with the files instead\n                    for x in func(**arg):\n                        yield x\n                finally:\n                    # Make sure to close the files\n                    for f in to_close:\n                        arg[f].close()\n\n    else:\n\n        @wraps(func)\n        def open_files(*args, **kwargs):\n            # If no arg names are given then assume the first arg is a file\n            # you want to read from.\n            if not possible_files:\n                if isinstance(args[0], str):\n                    with io.open(args[0], mode=""r"", encoding=""utf-8"") as f:\n                        # Call the function with the file instead\n                        return func(f, *args[1:], **kwargs)\n                else:\n                    return func(*args, **kwargs)\n            else:\n                # Otherwise we have multiple files we want to open\n                to_close = []\n                # Get a dict representation of what it will look like if we\n                # call func with *args and **kwargs\n                arg = inspect.getcallargs(func, *args, **kwargs)\n                try:\n                    for f, mode in possible_files.items():\n                        if isinstance(arg[f], str):\n                            # Replace strings with the opened files\n                            arg[f] = io.open(arg[f], mode=mode, encoding=None if ""b"" in mode else ""utf-8"")\n                            to_close.append(f)\n                    # Call the function with the files instead\n                    return func(**arg)\n                finally:\n                    # Make sure to close the files\n                    for f in to_close:\n                        arg[f].close()\n\n    return open_files\n\n\n@export\n@str_file(filepath=""w"")\ndef write_json(content, filepath):\n    json.dump(content, filepath, indent=True)\n\n\n@export\n@str_file(filepath=""w"")\ndef write_yaml(content, filepath):\n    import yaml\n\n    yaml.dump(content, filepath, default_flow_style=False)\n\n\n@export\ndef normalize_indices(xs, length):\n    """"""Normalize negative indices into positive.\n\n    :param xs: `List[int]` The indices.\n    :param length: `int` The length of the thing to be indexed\n\n    :returns: `List[int]` The indices converted to positive only.\n    """"""\n    return list(map(lambda x: length + x if x < 0 else x, xs))\n\n\n@export\ndef convert_iob_to_bio(seq: List[str]) -> List[str]:\n    """"""Convert a sequence of IOB tags to BIO tags.\n\n    The difference between IOB and BIO (also called IOB2) is that in IOB\n    the B- prefix is only used to separate two chunks of the same type\n    while in BIO the B- prefix is used to start every chunk.\n\n    :param seq: `List[str]` The list of IOB tags.\n\n    :returns: `List[str] The list of BIO tags.\n    """"""\n    new = []\n    prev = ""O""\n    for token in seq:\n        # Only I- needs to be changed\n        if token.startswith(""I-""):\n            # If last was O or last was a different type force B\n            if prev == ""O"" or token[2:] != prev[2:]:\n                token = ""B-"" + token[2:]\n        new.append(token)\n        prev = token\n    return new\n\n\n@export\ndef convert_bio_to_iob(seq: List[str]) -> List[str]:\n    """"""Convert a sequence of BIO tags to IOB tags.\n\n    The difference between BIO and IOB is that in IOB the B- prefix is only\n    used to separate two chunks of the same type while in BIO the B- prefix\n    starts every chunk. To convert we only need to look at the B- tokens.\n    If they are following a chunk of the same type we leave it as a B-\n    otherwise it converts it back to an I-\n\n    :param seq: `List[str]` The list of BIO tags.\n\n    :returns: `List[str]` The list of IOB tags.\n    """"""\n    new = []\n    prev_ty = ""O""\n    for token in seq:\n        ty = ""O"" if token == ""O"" else token[2:]\n        # In IOB, `B-` is only needed if the previous type is the same as ours\n        if token.startswith(""B-""):\n            # If we are different than the type before us convert to `I-`\n            if prev_ty != ty:\n                token = ""I-"" + ty\n        new.append(token)\n        prev_ty = ty\n    return new\n\n\n@export\ndef convert_bio_to_iobes(seq: List[str]) -> List[str]:\n    """"""Convert a sequence of BIO tags to IOBES tags.\n\n    The difference between BIO and IOBES tags is that in IOBES the end\n    of a multi-token entity is marked with the E- prefix while in BIO\n    it would end with an I- prefix.\n\n    The other difference is that a single token entity in BIO is a\n    just a B- whereas in IOBES it uses the special S- prefix.\n\n    :param seq: `List[str]` The list of BIO tags.\n\n    :returns: `List[str]` The list of IOBES tags.\n    """"""\n    new = []\n    # Get tag bigrams with a fake O at the end so that the final real\n    # token is processed correctly (Final I turned to E, etc.)\n    for c, n in zip(seq, chain(seq[1:], [""O""])):\n        if c.startswith(""B-""):\n            # If the next token is an I of the same class this is a\n            # multi token span and should stay as B\n            if n == c.replace(""B-"", ""I-""):\n                new.append(c)\n            # If the next token is anything else this is a single token\n            # span and should become a S\n            else:\n                new.append(c.replace(""B-"", ""S-""))\n        elif c.startswith(""I-""):\n            # If the next token is also an I of this type then we are\n            # in the middle of a span and should stay I\n            if n == c:\n                new.append(c)\n            # If next is anything else we are the end and thus E\n            else:\n                new.append(c.replace(""I-"", ""E-""))\n        # Pass O through\n        else:\n            new.append(c)\n    return new\n\n\n@export\ndef convert_iobes_to_bio(seq: List[str]) -> List[str]:\n    """"""Convert a sequence of IOBES tags to BIO tags\n\n    :param seq: `List[str]` The list of IOBES tags.\n\n    :returns: `List[str]` The list of BIO tags.\n    """"""\n    # Use re over `.replace` to make sure it only acts on the beginning of the token.\n    return list(map(lambda x: re.sub(r""^S-"", ""B-"", re.sub(r""^E-"", ""I-"", x)), seq))\n\n\n@export\ndef convert_iob_to_iobes(seq: List[str]) -> List[str]:\n    """"""Convert a sequence of IOB tags to IOBES tags.\n\n    :param seq: `List[str]` The list of IOB tags.\n\n    :returns: `List[str]` The list of IOBES tags.\n    """"""\n    return convert_bio_to_iobes(convert_iob_to_bio(seq))\n\n\n@export\ndef convert_iobes_to_iob(seq: List[str]) -> List[str]:\n    """"""Convert a sequence of IOBES tags to IOB tags.\n\n    :param seq: `List[str]` The list of IOBES tags.\n\n    :returns: `List[str]` The list of IOB tags.\n    """"""\n    return convert_bio_to_iob(convert_iobes_to_bio(seq))\n\n\n@export\n@str_file\ndef sniff_conll_file(f, delim=None, comment_pattern=""#"", doc_pattern=""# begin doc""):\n    """"""Figure out how many columns are in a conll file.\n\n    :param file_name: `str` The name of the file.\n    :param delim: `str` The token between columns in the file.\n\n    :returns: `int` The number of columns in the file.\n    """"""\n    start = f.tell()\n    for line in f:\n        line = line.rstrip(""\\n"")\n        if line.startswith((comment_pattern, doc_pattern)):\n            continue\n        parts = line.split(delim)\n        if len(parts) > 1:\n            f.seek(start)\n            return len(parts)\n\n@export\ndef split_extensions(path: str, exts: Set[str]) -> Tuple[str, str]:\n    all_ext = []\n    path, ext = os.path.splitext(path)\n    while ext in exts:\n        all_ext.append(ext)\n        path, ext = os.path.splitext(path)\n    return path + ext, """".join(all_ext[::-1])\n\n@export\ndef remove_extensions(path: str, exts: Set[str]) -> str:\n    path, _ = split_extensions(path, exts)\n    return path\n\n\n@export\ndef remove_conll_extensions(path: str, exts: Set[str] = CONLL_EXTS) -> str:\n    return remove_extensions(path, exts)\n\n@export\ndef split_conll_extensions(path: str, exts: Set[str] = CONLL_EXTS) -> Tuple[str, str]:\n    return split_extensions(path, exts)\n\n\n@export\n@str_file\ndef read_conll(f, doc_pattern=None, delim=None, metadata=False, allow_comments=False, comment_pattern=""#""):\n    """"""Read from a conll file.\n\n    :param f: `str` The file to read from.\n    :param doc_pattern: `str` A pattern that matches the line that signals the\n        beginning of a new document in the conll file. When None just the\n        conll sentences are returned.\n    :param delim: `str` The token between columns in the file\n    :param metadata: `bool` Should meta data (lines starting with `#` before a\n        sentence) be returned with our sentences.\n    :param allow_comments: `bool` Are comments (lines starting with `#`) allowed in the file.\n\n    :returns: `Generator` The sentences or documents from the file.\n    """"""\n    if metadata and not allow_comments:\n        raise ValueError(\n            ""You have metadata set to `True` but allow_comments set to `False` you can\'t extract ""\n            ""metadata from a file that doesn\'t allow comments.""\n        )\n    if doc_pattern is not None:\n        if metadata:\n            for x in read_conll_docs_md(f, doc_pattern, delim=delim, comment_pattern=comment_pattern):\n                yield x\n        else:\n            for x in read_conll_docs(\n                f, doc_pattern, delim=delim, allow_comments=allow_comments, comment_pattern=comment_pattern\n            ):\n                yield x\n    else:\n        if metadata:\n            for x in read_conll_sentences_md(f, delim=delim, comment_pattern=comment_pattern):\n                yield x\n        else:\n            for x in read_conll_sentences(\n                f, delim=delim, allow_comments=allow_comments, comment_pattern=comment_pattern\n            ):\n                yield x\n\n\n@export\n@str_file(wf=""w"")\ndef write_conll(wf: Union[str, TextIO], sentences: List[List[List[str]]], delim: Optional[str] = None) -> None:\n    delim = "" "" if delim is None else delim\n    wf.write(""\\n\\n"".join([""\\n"".join([delim.join(row) for row in sentence]) for sentence in sentences]) + ""\\n\\n"")\n\n\n@export\n@str_file\ndef read_conll_sentences(f, delim=None, allow_comments=True, comment_pattern=""#""):\n    """"""Read sentences from a conll file.\n\n    :param f: `str` The file to read from.\n    :param delim: `str` The token between columns in the file.\n\n    Note:\n        If you have a sentence where the first token is `#` it will get eaten by the\n        metadata. If this happens you need to set `allow_comments=True` and not have\n        comments in the file. If you have comments in the file and set this then\n        they will show up in the sentences\n\n    :returns: `Generator[List[List[str]]]` A list of rows representing a sentence.\n    """"""\n    sentence = []\n    for line in f:\n        line = line.rstrip()\n        # Comments are not allowed in the middle of a sentence so if we find a line that\n        # starts with # but we are in a sentence it must be a # as a token so we should\n        # not skip it\n        if allow_comments and not sentence and line.startswith(comment_pattern):\n            continue\n\n        # Blank lines signal the end of a sentence\n        if len(line) == 0:\n            # If we built a sentence yield it, this check allows multiple blank lines in a row\n            if sentence:\n                yield sentence\n            # Reset the sentence\n            sentence = []\n            continue\n        # This is a normal row, we split and take the tokens.\n        sentence.append(line.split(delim))\n    # If we have a sentence then the file didn\'t end with a new line, we yield the sentence\n    # so we don\'t lose it\n    if sentence:\n        yield sentence\n\n\n@export\n@str_file\ndef read_conll_sentences_md(f, delim=None, comment_pattern=""#""):\n    """"""Read sentences from a conll file.\n\n    :param f: `str` The file to read from.\n    :param delim: `str` The token between columns in the file.\n\n    Note:\n        If there are document annotations in the conll file then they will show\n        up in the meta data for what would be the first sentence of that doc\n\n        If you have a sentence where the first token is `#` it will show up in the\n        metadata. If this happens you\'ll need to update you comments to use a different\n        comment pattern, something like `# comment:` I recommend having a space in\n        you patten so it can\'t show up as a conll token\n\n    :returns: `Generator[Tuple[List[List[str]], List[List[str]]]`\n        The first element is the list or rows, the second is a list of comment\n        lines that preceded that sentence in the file.\n    """"""\n    sentence, meta = [], []\n    for line in f:\n        line = line.rstrip()\n        # Comments are not allowed in the middle of a sentence so if we find a line that\n        # starts with # but we are in a sentence it must be a # as a token so we should\n        # not skip it. If this is a comment we track it in our meta data list\n        if not sentence and line.startswith(comment_pattern):\n            meta.append(line)\n            continue\n        if len(line) == 0:\n            if sentence:\n                yield sentence, meta\n            sentence, meta = [], []\n            continue\n        sentence.append(line.split(delim))\n    if sentence:\n        yield sentence, meta\n\n\n@export\n@str_file\ndef read_conll_docs(f, doc_pattern=""# begin doc"", delim=None, allow_comments=True, comment_pattern=""#""):\n    """"""Read sentences from a conll file.\n\n    :param f: `str` The file to read from.\n    :param doc_pattern: `str` The beginning of lines that represent new documents\n    :param delim: `str` The token between columns in the file.\n\n    Note:\n        If you have a sentence where the first token is `#` it will show up in the\n        metadata. If this happens you\'ll need to update you comments to use a different\n        comment pattern, something like `# comment:` I recommend having a space in\n        you patten so it can\'t show up as a conll token\n\n    :returns: `Generator[List[List[List[str]]]]`\n        A document which is a list of sentences.\n    """"""\n    doc, sentence = [], []\n    for line in f:\n        line = line.rstrip()\n        if line.startswith(doc_pattern):\n            if doc:\n                if sentence:\n                    doc.append(sentence)\n                yield doc\n                doc, sentence = [], []\n            continue\n        elif allow_comments and not sentence and line.startswith(comment_pattern):\n            continue\n        if len(line) == 0:\n            if sentence:\n                doc.append(sentence)\n                sentence = []\n            continue\n        sentence.append(line.split(delim))\n    if doc or sentence:\n        if sentence:\n            doc.append(sentence)\n        yield doc\n\n\n@export\n@str_file\ndef read_conll_docs_md(f, doc_pattern=""# begin doc"", delim=None, comment_pattern=""#""):\n    """"""Read sentences from a conll file.\n\n    :param f: `str` The file to read from.\n    :param doc_pattern: `str` The beginning of lines that represent new documents\n    :param delim: `str` The token between columns in the file.\n\n    Note:\n        If you have a sentence where the first token is `#` it will show up in the\n        metadata. If this happens you\'ll need to update you comments to use a different\n        comment pattern, something like `# comment:` I recommend having a space in\n        you patten so it can\'t show up as a conll token\n\n    :returns: `Generator[Tuple[List[List[List[str]]], List[str]  List[List[str]]]`\n        The first element is a document, the second is a list of comments\n        lines that preceded the document break (includes the document line)\n        since the last sentence. The last is a list of comments for each\n        list in the document.\n    """"""\n    doc, sentence, doc_meta, sent_meta, meta = [], [], [], [], []\n    for line in f:\n        line = line.rstrip()\n        if line.startswith(doc_pattern):\n            new_doc_meta = meta\n            meta = []\n            new_doc_meta.append(line)\n            if doc:\n                if sentence:\n                    doc.append(sentence)\n                    sentence = []\n                yield doc, doc_meta, sent_meta\n                doc, sentence, sent_meta = [], [], []\n            doc_meta = new_doc_meta\n            continue\n        elif not sentence and line.startswith(comment_pattern):\n            meta.append(line)\n            continue\n        if len(line) == 0:\n            if sentence:\n                doc.append(sentence)\n                sent_meta.append(meta)\n                sentence, meta = [], []\n            continue\n        sentence.append(line.split(delim))\n    if doc or sentence:\n        if sentence:\n            doc.append(sentence)\n            sent_meta.append(meta)\n            meta = []\n        yield doc, doc_meta, sent_meta\n\n\n@export\n@str_file(ifile=""r"", ofile=""w"")\ndef convert_conll_file(ifile, ofile, convert, fields=[-1], delim=None):\n    """"""Convert the tagging scheme in a conll file.\n\n    This function assumes the that columns that one wishes to convert are\n    the right model columns.\n\n    :param ifile: `str` The input file name.\n    :param ofile: `str` The output file name.\n    :param convert: `Callable(List[str]) -> List[str]` The function that\n        transforms a sequence in one tag scheme to another scheme.\n    :param fields: `List[int]` The columns to convert.\n    :param delim: `str` The symbol that separates the columns.\n    """"""\n    output_delim = \' \' if delim is None else delim\n    conll_length = sniff_conll_file(ifile, delim)\n    fields = set(normalize_indices(fields, conll_length))\n    for lines, md in read_conll_sentences_md(ifile, delim=delim):\n        lines = zip(*(convert(l) if i in fields else l for i, l in enumerate(zip(*lines))))\n        # Write out meta data\n        if md:\n            ofile.write(""\\n"".join(md) + ""\\n"")\n        # Write out the lines\n        ofile.write(""\\n"".join(output_delim.join(l).rstrip() for l in lines) + ""\\n\\n"")\n\n\n@export\n@str_file(ifile=""r"", ofile=""w"")\ndef convert_iob_conll_to_bio(ifile, ofile, fields=[-1], delim=None):\n    """"""Convert a conll file from iob to bio.""""""\n    convert_conll_file(ifile, ofile, convert_iob_to_bio, fields, delim)\n\n\n@export\n@str_file(ifile=""r"", ofile=""w"")\ndef convert_bio_conll_to_iob(ifile, ofile, fields=[-1], delim=None):\n    """"""Convert a conll file from bio to iob.""""""\n    convert_conll_file(ifile, ofile, convert_bio_to_iob, fields, delim)\n\n\n@export\n@str_file(ifile=""r"", ofile=""w"")\ndef convert_iob_conll_to_iobes(ifile, ofile, fields=[-1], delim=None):\n    """"""Convert a conll file from iob to iobes.""""""\n    convert_conll_file(ifile, ofile, convert_iob_to_iobes, fields, delim)\n\n\n@export\n@str_file(ifile=""r"", ofile=""w"")\ndef convert_iobes_conll_to_iob(ifile, ofile, fields=[-1], delim=None):\n    """"""Convert a conll file from iob to iobes.""""""\n    convert_conll_file(ifile, ofile, convert_iobes_to_iob, fields, delim)\n\n\n@export\n@str_file(ifile=""r"", ofile=""w"")\ndef convert_bio_conll_to_iobes(ifile, ofile, fields=[-1], delim=None):\n    """"""Convert a conll file from bio to iobes.""""""\n    convert_conll_file(ifile, ofile, convert_bio_to_iobes, fields, delim)\n\n\n@export\n@str_file(ifile=""r"", ofile=""w"")\ndef convert_iobes_conll_to_bio(ifile, ofile, fields=[-1], delim=None):\n    """"""Convert a conll file from iobes to bio. Useful for formatting output to use `conlleval.pl`.""""""\n    convert_conll_file(ifile, ofile, convert_iobes_to_bio, fields, delim)\n\n\n@export\ndef to_spans(\n    sequence: List[int], lut: Dict[int, str], span_type: str, verbose: bool = False, delim: str = ""@""\n) -> List[str]:\n    """"""Turn a sequence into a list of chunks.\n\n    :param sequence: `List[int]` The tag sequence.\n    :param lut: `Dict[int] -> str` A mapping for integers to tag names.\n    :param span_type: `str` The tagging scheme.\n    :param verbose: `bool` Should we output warning on illegal transitions.\n    :param delim: `str` The symbol the separates output chunks from their indices.\n\n    :returns: `List[str]` The list of entities in the order they appear. The\n        entities are in the form {chunk_type}{delim}{index}{delim}{index}...\n        for example LOC@3@4@5 means a Location chunk was at indices 3, 4, and 5\n        in the original sequence.\n    """"""\n    sequence = [lut[y] for y in sequence]\n    return to_chunks(sequence, span_type, verbose, delim)\n\n@export\ndef to_chunks(sequence: List[str], span_type: str, verbose: bool = False, delim: str = ""@"") -> List[str]:\n    """"""Turn a sequence of tags into a list of chunks.\n\n    :param sequence: `List[str]` The tag sequence.\n    :param span_type: `str` The tagging scheme.\n    :param verbose: `bool` Should we output warning on illegal transitions.\n    :param delim: `str` The symbol the separates output chunks from their indices.\n\n    :returns: `List[str]` The list of entities in the order they appear. The\n        entities are in the form {chunk_type}{delim}{index}{delim}{index}...\n        for example LOC@3@4@5 means a Location chunk was at indices 3, 4, and 5\n        in the original sequence.\n    """"""\n    span_type = span_type.lower()\n    if span_type == ""iobes"":\n        return to_chunks_iobes(sequence, verbose, delim)\n    if span_type == ""token"":\n        # For token level tasks we force each token to be it\'s own span\n        # Normally it is fine to pass token level annotations through the iob\n        # span code to produce spans of size 1 but if your data has a label of `O`\n        # like twpos does then you will lose tokens and mess up the calculations\n        return [f""{s}@{i}"" for i, s in enumerate(sequence)]\n\n    strict_iob2 = span_type in (""iob2"", ""bio"")\n    iobtype = 2 if strict_iob2 else 1\n    chunks = []\n    current = None\n    for i, label in enumerate(sequence):\n        if not label.startswith(""I-"") and not label == ""O"":\n            if current is not None:\n                chunks.append(delim.join(current))\n            current = [label.replace(""B-"", """"), ""%d"" % i]\n        elif label.startswith(""I-""):\n            if current is not None:\n                base = label.replace(""I-"", """")\n                if base == current[0]:\n                    current.append(""%d"" % i)\n                else:\n                    chunks.append(delim.join(current))\n                    if iobtype == 2 and verbose:\n                        logger.warning(""Warning: I doesn\'t agree with previous B/I @ %d"" % i)\n\n                    current = [base, ""%d"" % i]\n            else:\n                current = [label.replace(""I-"", """"), ""%d"" % i]\n                if iobtype == 2 and verbose:\n                    logger.warning(""Warning: I without previous chunk @ %d"" % i)\n        else:\n            if current is not None:\n                chunks.append(delim.join(current))\n            current = None\n    if current is not None:\n        chunks.append(delim.join(current))\n    return chunks\n\n\n@export\ndef to_chunks_iobes(sequence: List[str], verbose: bool = False, delim: str = ""@"") -> List[str]:\n    """"""Turn a sequence of IOBES tags into a list of chunks.\n\n    :param sequence: `List[str]` The tag sequence.\n    :param verbose: `bool` Should we output warning on illegal transitions.\n    :param delim: `str` The symbol the separates output chunks from their indices.\n\n    :returns: `List[str]` The list of entities in the order they appear. The\n        entities are in the form {chunk_type}{delim}{index}{delim}{index}...\n        for example LOC@3@4@5 means a Location chunk was at indices 3, 4, and 5\n        in the original sequence.\n    """"""\n    chunks = []\n    current = None\n    for i, label in enumerate(sequence):\n        # This indicates a multi-word chunk start\n        if label.startswith(""B-""):\n            # Flush existing chunk\n            if current is not None:\n                chunks.append(delim.join(current))\n            # Create a new chunk\n            current = [label.replace(""B-"", """"), ""%d"" % i]\n            if verbose:\n                # Look ahead to make sure this `B-Y` shouldn\'t be a `S-Y`\n                # We only check for `B`, `S` and `O` because a mismatched `I` or `E`\n                # will already warn looking backwards\n                if i < len(sequence) - 1:\n                    nxt = sequence[i + 1]\n                    if nxt == ""O"" or nxt.startswith((""B"", ""S"")):\n                        logger.warning(""Warning: Single B token chunk @ %d"", i)\n                elif i == len(sequence) - 1:\n                    logger.warning(""Warning: B as final token"")\n        # This indicates a single word chunk\n        elif label.startswith(""S-""):\n            # Flush existing chunk, and since this is self-contained, we will clear current\n            if current is not None:\n                chunks.append(delim.join(current))\n                current = None\n            base = label.replace(""S-"", """")\n            # Write this right into the chunks since self-contained\n            chunks.append(delim.join([base, ""%d"" % i]))\n        # Indicates we are inside of a chunk already\n        elif label.startswith(""I-""):\n            # This should always be the case!\n            if current is not None:\n                base = label.replace(""I-"", """")\n                if base == current[0]:\n                    current.append(""%d"" % i)\n                else:\n                    # Doesn\'t match previous entity, flush the old one and start a new one\n                    chunks.append(delim.join(current))\n                    if verbose:\n                        logger.warning(""Warning: I doesn\'t agree with previous B/I @ %d"" % i)\n                    current = [base, ""%d"" % i]\n            else:\n                if verbose:\n                    logger.warning(""Warning: I without previous chunk @ %d"" % i)\n                current = [label.replace(""I-"", """"), ""%d"" % i]\n            if verbose and i == len(sequence) - 1:\n                logger.warning(""Warning: I as final token"")\n        # We are at the end of a chunk, so flush current\n        elif label.startswith(""E-""):\n            # Flush current chunk\n            if current is not None:\n                base = label.replace(""E-"", """")\n                if base == current[0]:\n                    current.append(""%d"" % i)\n                    chunks.append(delim.join(current))\n                    current = None\n                else:\n                    chunks.append(delim.join(current))\n                    if verbose:\n                        logger.warning(""Warning: E doesn\'t agree with previous B/I @ %d"", i)\n                    current = [base, ""%d"" % i]\n                    chunks.append(delim.join(current))\n                    current = None\n            # This should never happen\n            else:\n                current = [label.replace(""E-"", """"), ""%d"" % i]\n                if verbose:\n                    logger.warning(""Warning: E without previous chunk @ %d"" % i)\n                chunks.append(delim.join(current))\n                current = None\n        # Outside\n        else:\n            if current is not None:\n                chunks.append(delim.join(current))\n            current = None\n    # If something is left, flush\n    if current is not None:\n        chunks.append(delim.join(current))\n    return chunks\n\n\n@export\ndef span_f1(golds: List[Set[str]], preds: List[Set[str]]) -> float:\n    """"""Calculate Span level F1 score.\n\n    :param golds: `List[set[str]]` The list of the set of gold chunks.\n    :param preds: `List[set[str]]` The list of the set of predicted chunks.\n\n    :returns: `float` The f1 score.\n    """"""\n    overlap = sum(len(g & p) for g, p in zip(golds, preds))\n    gold_total = sum(len(g) for g in golds)\n    pred_total = sum(len(p) for p in preds)\n    return f_score(overlap, gold_total, pred_total)\n\n\n@export\ndef per_entity_f1(golds: List[Set[str]], preds: List[Set[str]], delim: str = ""@"") -> Dict[str, float]:\n    """"""Calculate Span level F1 with break downs per entity type.\n\n    :param golds: `List[set[str]]` The list of the set of gold chunks.\n    :param preds: `List[set[str]]` The list of the set of predicted chunks.\n    :param delim: `str` The symbol that separates an entity from its indices.\n\n    :returns: `dict` The metrics at a global level and fine grained entity\n        level performance.\n\n    Note:\n        This function returns most of the metrics needed for the\n        `conlleval_output`. `acc` and `tokens` (the token level accuracy\n        and the number of tokens respectively) need to be added.\n    """"""\n    metrics = {}\n    overlap = Counter()\n    gold_total = Counter()\n    pred_total = Counter()\n    types = set()\n    for g, p in zip(golds, preds):\n        overlaps = g & p\n        overlap[""total""] += len(overlaps)\n        gold_total[""total""] += len(g)\n        pred_total[""total""] += len(p)\n        for o in overlaps:\n            ent = o.split(delim)[0]\n            overlap[ent] += 1\n            types.add(ent)\n        for o in g:\n            ent = o.split(delim)[0]\n            gold_total[ent] += 1\n            types.add(ent)\n        for o in p:\n            ent = o.split(delim)[0]\n            pred_total[ent] += 1\n            types.add(ent)\n    metrics[""overlap""] = overlap[""total""]\n    metrics[""gold_total""] = gold_total[""total""]\n    metrics[""pred_total""] = pred_total[""total""]\n    metrics[""precision""] = precision(overlap[""total""], pred_total[""total""]) * 100\n    metrics[""recall""] = recall(overlap[""total""], gold_total[""total""]) * 100\n    metrics[""f1""] = f_score(overlap[""total""], gold_total[""total""], pred_total[""total""]) * 100\n    metrics[""types""] = []\n    for t in sorted(types):\n        metrics[""types""].append(\n            {\n                ""ent"": t,\n                ""precision"": precision(overlap[t], pred_total[t]) * 100,\n                ""recall"": recall(overlap[t], gold_total[t]) * 100,\n                ""f1"": f_score(overlap[t], gold_total[t], pred_total[t]) * 100,\n                ""count"": pred_total[t],\n            }\n        )\n    return metrics\n\n\n@export\ndef conlleval_output(results: Dict[str, Union[float, int]]) -> str:\n    """"""Create conlleval formated output.\n\n    :param results: `dict` The metrics. results should have the following keys.\n        tokens: `int` The total number of tokens processed.\n        acc: `float` The token level accuracy.\n        gold_total: `int` The total number of gold entities.\n        pred_total: `int` The total number of predicted entities.\n        overlap: `int` The number of exact match entites.\n        precision: `float` The precision of all entities.\n        recall: `float` The recall of all entities.\n        f1: `float` The f1 score of all entities.\n        types: `List[dict]` A list of metrics for each entity type. Keys should include:\n            ent: `str` The name of the entity.\n            precision: `float` The precision of this entity type.\n            recall: `float` The recall of this this entity type.\n            f1: `float` The f1 score of this entity type.\n            count: `int` The number of predicted entities of this type.\n\n    :returns: `str` The formatted string ready for printing.\n\n    Note:\n        Both the metrics in the results dict and acc are expected to already be\n        multiplied by 100. The result won\'t look correct and a lot of the\n        metric will be cut off if they are not.\n\n        Metrics per type are output in the order they appear in the list.\n        conlleval.pl outputs the types in sorted order. To match this the list\n        in `results[\'types\'] should be sorted.\n    """"""\n    s = (\n        ""processed {tokens} tokens with {gold_total} phrases; found: {pred_total} phrases; correct: {overlap}.\\n""\n        ""accuracy: {acc:>{length}.2f}%; precision: {precision:>6.2f}%; recall: {recall:>6.2f}%; FB1: {f1:>6.2f}\\n""\n    )\n    t = []\n    longest_ent = max(len(max(results[""types""], key=lambda x: len(x[""ent""]))[""ent""]), 17)\n    for type_metric in results[""types""]:\n        t.append(\n            ""{ent:>{longest_ent}}: precision: {precision:>6.2f}%; recall: {recall:>6.2f}%; FB1: {f1:>6.2f}  {count}"".format(\n                longest_ent=longest_ent, **type_metric\n            )\n        )\n    s = s + ""\\n"".join(t)\n    s = s.format(length=longest_ent - 11, **results)\n    return s\n\n\n@export\ndef precision(overlap_count: int, guess_count: int) -> float:\n    """"""Compute the precision in a zero safe way.\n\n    :param overlap_count: `int` The number of true positives.\n    :param guess_count: `int` The number of predicted positives (tp + fp)\n\n    :returns: `float` The precision.\n    """"""\n    if guess_count == 0:\n        return 0.0\n    return 0.0 if guess_count == 0 else overlap_count / float(guess_count)\n\n\n@export\ndef recall(overlap_count: int, gold_count: int) -> float:\n    """"""Compute the recall in a zero safe way.\n\n    :param overlap_count: `int` The number of true positives.\n    :param gold_count: `int` The number of gold positives (tp + fn)\n\n    :returns: `float` The recall.\n    """"""\n    return 0.0 if gold_count == 0 else overlap_count / float(gold_count)\n\n\n@export\ndef f_score(overlap_count: int, gold_count: int, guess_count: int, f: int = 1) -> float:\n    """"""Compute the f1 score.\n\n    :param overlap_count: `int` The number of true positives.\n    :param gold_count: `int` The number of gold positives (tp + fn)\n    :param guess_count: `int` The number of predicted positives (tp + fp)\n    :param f: `int` The beta term to weight precision vs recall.\n\n    :returns: `float` The f score\n    """"""\n    beta_sq = f * f\n    if guess_count == 0:\n        return 0.0\n    p = precision(overlap_count, guess_count)\n    r = recall(overlap_count, gold_count)\n    if p == 0.0 or r == 0.0:\n        return 0.0\n    f = (1.0 + beta_sq) * (p * r) / (beta_sq * p + r)\n    return f\n\n\n@export\ndef get_env_gpus() -> List[str]:\n    return os.getenv(""CUDA_VISIBLE_DEVICES"", os.getenv(""NVIDIA_VISIBLE_DEVICES"", os.getenv(""NV_GPU"", ""0""))).split("","")\n\n\n@export\ndef get_num_gpus_multiworker() -> int:\n    """"""Get the number of GPUs in multi-worker distributed training\n\n    :return:\n    """"""\n    return int(os.environ.get(""WORLD_SIZE"", 1))\n\n@export\ndef ngrams(sentence: List[str], filtsz: int = 3, joiner: str = ""@@"") -> List[str]:\n    """"""Generate ngrams over a sentence\n\n    :param sentence: (`List[str]`) Some tokens\n    :param filtsz: The ngram width\n    :param joiner: A string to join ngrams\n    :return: (`List[str]`) A list of ngrams\n    """"""\n    chunks = []\n    nt = len(sentence)\n    for i in range(nt - filtsz + 1):\n        chunk = sentence[i : i + filtsz]\n        chunks += [joiner.join(chunk)]\n    return chunks\n\n\n@export\n@str_file\ndef read_label_first_data(f):\n    """"""Read data from a file where the first token in the label and each line is a single example.\n\n    :param f: `Union[str, IO]` The file to read from.\n    :return: `Tuple[List[str], List[List[str]]]` The labels and text\n    """"""\n    labels, texts = [], []\n    for line in f:\n        line = line.rstrip()\n        if line:\n            label, text = line.split(maxsplit=1)\n            labels.append(label)\n            texts.append(text.split())\n    return labels, texts\n\n\n@export\n@str_file(w=""w"")\ndef write_label_first_data(w, labels, texts):\n    """"""Read data to a file where the first token in the label and each line is a single example.\n\n    :param w: `Union[str, IO]` The file to write the results in\n    :param labels: `List[str]` The labels for the examples\n    :param texts: `List[List[str]]` The text examples\n    """"""\n    w.write(""\\n"".join("" "".join(chain((l,), t)) for l, t in zip(labels, texts)))\n\n\nclass MN:\n    GZIP = b""1f8b""\n    TAR = b""7573746172""\n    TAR_START = 257\n    ZIP = b""504b0304""\n\n\ndef check_mn(b: bytes, mn: bytes = None, start: int = 0) -> bool:\n    if hexlify(b[start : start + 20])[: len(mn)] == mn:\n        return True\n    return False\n\n\ndef int2byte(x: int) -> bytes:\n    """"""Takes an int in the 8-bit range and returns a single-character byte""""""\n    return bytes((x,))\n\n\n_text_characters: bytes = (b"""".join(int2byte(i) for i in range(32, 127)) + b""\\n\\r\\t\\f\\b"")\n\n\n# Borrowed from: https://eli.thegreenplace.net/2011/10/19/perls-guess-if-file-is-text-or-binary-implemented-in-python\ndef is_text_file(block: bytes):\n    """""" Uses heuristics to guess whether the given file is text or binary,\n        by reading a single block of bytes from the file.\n        If more than 30% of the chars in the block are non-text, or there\n        are NUL (\'\\x00\') bytes in the block, assume this is a binary file.\n    """"""\n    if b""\\x00"" in block:\n        # Files with null bytes are binary\n        return False\n    elif not block:\n        # An empty file is considered a valid text file\n        return True\n\n    # Use translate\'s \'deletechars\' argument to efficiently remove all\n    # occurrences of _text_characters from the block\n    nontext = block.translate(None, _text_characters)\n    return float(len(nontext)) / len(block) <= 0.30\n\n\ncheck_gzip = partial(check_mn, mn=MN.GZIP)\ncheck_tar = partial(check_mn, mn=MN.TAR, start=MN.TAR_START)\ncheck_zip = partial(check_mn, mn=MN.ZIP)\n\n\nclass RE(object):\n    HTML = re.compile(b""(<!doctype html>|<html.*?>)"")\n    BIN = re.compile(b""\\d+? \\d+?$"", re.MULTILINE)\n\n\ndef check_re(b: bytes, regex: Pattern = None) -> bool:\n    return True if regex.match(b) else False\n\n\ncheck_html = partial(check_re, regex=RE.HTML)\n\n\n@export\ndef mime_type(file_name: str) -> str:\n    b = open(file_name, ""rb"").read(1024)\n    if check_gzip(b):\n        return ""application/gzip""\n    if check_tar(b):\n        return ""application/x-tar""\n    if check_zip(b):\n        return ""application/zip""\n    if check_html(b):\n        return ""text/html""\n    if is_text_file(b):\n        return ""text/plain""\n    return ""application/w2v""\n\n\n@export\ndef to_numpy(x):\n    if isinstance(x, np.ndarray):\n        return x\n    try:\n        import torch\n        if isinstance(x, torch.Tensor):\n            x = x.detach()\n            x = x.to(\'cpu\')\n    except ImportError:\n        pass\n    return x.numpy()\n\n\n@export\ndef mlm_masking(inputs, mask_value, vocab_size, ignore_prefix, ignore_suffix):\n    labels = np.copy(inputs)\n    masked_indices = np.random.binomial(size=len(inputs), n=1, p=0.15)\n    masked_indices[np.random.randint(1, len(inputs)-1)] = 1\n    if ignore_prefix:\n        masked_indices[0] = 0\n    if ignore_suffix:\n        masked_indices[-1] = 0\n    # Anything not masked is 0 so no loss\n    labels[masked_indices == 0] = 0\n    # Of the masked items, mask 80% of them with [MASK]\n    indices_replaced = np.random.binomial(size=len(inputs), n=1, p=0.8)\n    indices_replaced = indices_replaced & masked_indices\n    inputs[indices_replaced == 1] = mask_value\n    indices_random = np.random.binomial(size=len(inputs), n=1, p=0.5)\n    # Replace 10% of them with random words, rest preserved for auto-encoding\n    indices_random = indices_random & masked_indices & ~indices_replaced\n    random_words = np.random.randint(low=len(Offsets.VALUES) + 3, high=vocab_size-1, size=len(inputs))\n    inputs[indices_random == 1] = random_words[indices_random == 1]\n    return inputs, labels\n\n# https://stackoverflow.com/questions/5909873/how-can-i-pretty-print-ascii-tables-with-python\n@export\ndef print_table(rows: collections.namedtuple, columns: Optional[Set[str]] = None) -> None:\n    """"""Pretty print a table\n\n    :param rows: Some rows\n    :param columns: A set of columns to include in the output.\n    """"""\n    columns = columns if columns is not None else set(rows[0]._fields)\n    fields = [f for f in rows[0]._fields if f in columns]\n    if len(rows) > 1:\n        lens = [\n            len(str(max(chain((getattr(x, field) for x in rows), [field]), key=lambda x: len(str(x)))))\n            for field in fields\n        ]\n        formats = []\n        hformats = []\n        for i, field in enumerate(fields):\n            if isinstance(getattr(rows[0], field), int):\n                formats.append(""%%%dd"" % lens[i])\n            else:\n                formats.append(""%%-%ds"" % lens[i])\n            hformats.append(""%%-%ds"" % lens[i])\n        pattern = "" | "".join(formats)\n        hpattern = "" | "".join(hformats)\n        separator = ""-+-"".join([\'-\' * n for n in lens])\n        print(hpattern % tuple(fields))\n        print(separator)\n        for line in rows:\n            print(pattern % tuple(getattr(line, field) for field in fields))\n    elif len(rows) == 1:\n        row = rows[0]\n        hwidth = len(max(fields, key=lambda x: len(x)))\n        for field in fields:\n            print(""%*s = %s"" % (hwidth, field, getattr(row, field)))\n\n\n@export\nclass Average:\n    def __init__(self, name, fmt=\':f\'):\n        self.name = name\n        self.fmt = fmt\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = \'{name} {val\' + self.fmt + \'} ({avg\' + self.fmt + \'})\'\n        return fmtstr.format(**self.__dict__)\n\n'"
layers/eight_mile/version.py,0,"b'__version__ = ""2.0.4""\n'"
mead/pytorch/__init__.py,1,b'from mead.pytorch.exporters import *\n'
mead/pytorch/exporters.py,9,"b'import os\nimport logging\nimport torch\nimport torch.nn as nn\nimport baseline as bl\nfrom eight_mile.pytorch.layers import CRF, ViterbiBatchSize1\nfrom baseline.utils import (\n    exporter,\n    Offsets,\n    write_json,\n    load_vectorizers,\n    load_vocabs,\n    find_model_basename,\n)\nfrom baseline.model import load_model_for\nfrom baseline.vectorizers import (\n    GOVectorizer,\n    Dict1DVectorizer,\n    Char2DVectorizer,\n    Dict2DVectorizer,\n    Char1DVectorizer,\n    Token1DVectorizer,\n)\nfrom mead.utils import (\n    get_output_paths,\n    create_metadata,\n    save_to_bundle,\n)\nfrom mead.exporters import Exporter, register_exporter\n\n__all__ = []\nexport = exporter(__all__)\nlogger = logging.getLogger(\'mead\')\n\n\nVECTORIZER_SHAPE_MAP = {\n    Token1DVectorizer: [1, 100],\n    GOVectorizer: [1, 100],\n    Dict1DVectorizer: [1, 100],\n    Char2DVectorizer: [1, 100, 50],\n    Dict2DVectorizer: [1, 100, 50],\n    Char1DVectorizer: [1, 100],\n}\n\nFAKE_SENTENCE = """"""\nCommon starlings may be kept as pets or as laboratory animals . Austrian <unk> Konrad Lorenz wrote of them in his book King Solomon \'s Ring as "" the poor man \'s dog "" and "" something to love "" , because nestlings are easily obtained from the wild and after careful hand rearing they are straightforward to look after . They adapt well to captivity , and thrive on a diet of standard bird feed and <unk> . Several birds may be kept in the same cage , and their <unk> makes them easy to train or study . The only disadvantages are their <unk> and indiscriminate defecation habits and the need to take precautions against diseases that may be transmitted to humans . As a laboratory bird , the common starling is second in numbers only to the domestic <unk> . \n""""""\n\ndef create_fake_data(shapes, vectorizers, order, min_=0, max_=50,):\n    data = {\n        k: torch.randint(min_, max_, shapes[type(v)]) for k, v in vectorizers.items()\n    }\n    ordered_data = tuple(data[k] for k in order.embeddings)\n    lengths = torch.LongTensor([data[list(data.keys())[0]].shape[1]])\n    return ordered_data, lengths\n\n\ndef create_data_dict(vocabs, vectorizers, transpose=False, min_=0, max_=50, default_size=100):\n    data = {}\n    lengths = None\n    for k, v in vectorizers.items():\n        data[k], feature_length = vectorizers[k].run(FAKE_SENTENCE.split(), vocabs[k])\n        data[k] = torch.LongTensor(data[k]).unsqueeze(0)\n        if not lengths:\n            lengths = [feature_length]\n        # TODO: use the vectorizers, thats their job!!\n        # data[k][0][0] = 101\n\n    lengths = torch.LongTensor(lengths)\n\n    if transpose:\n        for k in vectorizers.keys():\n            if len(data[k].shape) > 1:\n                data[k] = data[k].transpose(0, 1)\n    data[\'lengths\'] = lengths\n    return data\n\n\n@export\nclass PytorchONNXExporter(Exporter):\n    def __init__(self, task, **kwargs):\n        super().__init__(task, **kwargs)\n        self.transpose = kwargs.get(\'transpose\', False)\n        self.tracing = kwargs.get(\'tracing\', True)\n        self.default_size = int(kwargs.get(\'default_size\', 100))\n\n    def apply_model_patches(self, model):\n        return model\n\n    def _run(self, basename, output_dir, project=None, name=None, model_version=None, use_version=False, zip_results=True, **kwargs):\n        logger.warning(""Pytorch exporting is experimental and is not guaranteed to work for plugin models."")\n        client_output, server_output = get_output_paths(\n            output_dir,\n            project, name,\n            model_version,\n            kwargs.get(\'remote\', True),\n            use_version=use_version\n        )\n        logger.info(""Saving vectorizers and vocabs to %s"", client_output)\n        logger.info(""Saving serialized model to %s"", server_output)\n        model, vectorizers, vocabs, model_name = self.load_model(basename)\n        model = self.apply_model_patches(model)\n        data = create_data_dict(vocabs, vectorizers, transpose=self.transpose, default_size=self.default_size)\n\n        inputs = [k for k, _ in model.embeddings.items()] + [\'lengths\']\n\n        dynamics = {\'output\': {1: \'sequence\'}}\n        for k, _ in model.embeddings.items():\n            if k == \'char\':\n                dynamics[k] = {1: \'sequence\', 2: \'chars\'}\n            else:\n                dynamics[k] = {1: \'sequence\'}\n\n        meta = create_metadata(\n            inputs, [\'output\'],\n            self.sig_name,\n            model_name, model.lengths_key\n        )\n\n        if not self.tracing:\n            model = torch.jit.script(model)\n\n        logger.info(""Exporting Model."")\n        print(inputs)\n\n        torch.onnx.export(model, data,\n                          verbose=True,\n                          dynamic_axes=dynamics,\n                          f=f\'{server_output}/{model_name}.onnx\',\n                          input_names=inputs,\n                          output_names=[\'output\'],\n                          opset_version=11,\n                          example_outputs=torch.ones((1, len(model.labels))))\n\n        logger.info(""Saving metadata."")\n        save_to_bundle(client_output, basename, assets=meta, zip_results=zip_results)\n        logger.info(\'Successfully exported model to %s\', output_dir)\n        return client_output, server_output\n\n    def load_model(self, model_dir):\n        model_name = find_model_basename(model_dir)\n        vectorizers = load_vectorizers(model_dir)\n        vocabs = load_vocabs(model_dir)\n        model = load_model_for(self.task.task_name(), model_name, device=\'cpu\')\n        model = model.cpu()\n        model.eval()\n        model_name = os.path.basename(model_name)\n        return model, vectorizers, vocabs, model_name\n\n\n@export\n@register_exporter(task=\'classify\', name=\'default\')\nclass ClassifyPytorchONNXExporter(PytorchONNXExporter):\n    def __init__(self, task, **kwargs):\n        super().__init__(task)\n        self.sig_name = \'predict_text\'\n\n\n@export\n@register_exporter(task=\'tagger\', name=\'default\')\nclass TaggerPytorchONNXExporter(PytorchONNXExporter):\n    def __init__(self, task, **kwargs):\n        super().__init__(task)\n        self.sig_name = \'tag_text\'\n\n    def apply_model_patches(self, model):\n        if hasattr(model, \'decoder\'):\n            if isinstance(model.decoder, CRF):\n                model.decoder.viterbi = ViterbiBatchSize1(model.decoder.viterbi.start_idx,\n                                                          model.decoder.viterbi.end_idx)\n        return model\n'"
mead/tf/__init__.py,0,b'from mead.tf.exporters import *\nfrom mead.tf.preproc_exporters import *\nfrom mead.tf.preprocessors import *\n'
mead/tf/exporters.py,0,"b'import os\nimport shutil\nimport logging\nimport datetime\nfrom collections import namedtuple\nimport baseline\nfrom baseline.tf.embeddings import *\nfrom baseline.utils import (\n    exporter,\n    Offsets,\n    ls_props,\n    read_json,\n    write_json,\n    transition_mask\n)\nfrom baseline.model import load_tagger_model, load_model, load_seq2seq_model\nfrom mead.exporters import Exporter, register_exporter\nfrom mead.utils import save_to_bundle, create_metadata, get_output_paths\n\n\n__all__ = []\nexport = exporter(__all__)\n\n\nFIELD_NAME = \'text/tokens\'\nASSET_FILE_NAME = \'model.assets\'\nlogger = logging.getLogger(\'mead\')\nSignatureOutput = namedtuple(""SignatureOutput"", (""classes"", ""scores""))\n\n\ndef get_tf_index_from_unzipped(dir_path):\n    return os.path.join(dir_path, [x[:-6] for x in os.listdir(dir_path) if \'index\' in x][0])\n\n\n@export\nclass TensorFlowExporter(Exporter):\n\n    def __init__(self, task, **kwargs):\n        super(TensorFlowExporter, self).__init__(task, **kwargs)\n\n    def _run(self, basename, output_dir, project=None, name=None, model_version=None, **kwargs):\n        basename = get_tf_index_from_unzipped(basename)\n\n        with tf.compat.v1.Graph().as_default():\n            config_proto = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n            with tf.compat.v1.Session(config=config_proto) as sess:\n                sig_input, sig_output, sig_name, assets = self._create_rpc_call(sess, basename)\n                client_output, server_output = get_output_paths(\n                    output_dir,\n                    project, name,\n                    model_version,\n                    kwargs.get(\'remote\', True), make_server=False\n                )\n                logger.info(\'Saving vectorizers and vocabs to %s\', client_output)\n                logger.info(\'Saving serialized model to %s\' % server_output)\n                try:\n                    builder = self._create_saved_model_builder(sess, server_output, sig_input, sig_output, sig_name)\n                    create_bundle(builder, client_output, basename, assets)\n                    logger.info(\'Successfully exported model to %s\' % output_dir)\n                except AssertionError as e:\n                    # model already exists\n                    raise e\n                except Exception as e:\n                    import traceback\n                    traceback.print_exc()\n                    # export process broke.\n                    # TODO(MB): we should remove the directory, if one has been saved already.\n                    raise e\n        return client_output, server_output\n\n    def _create_saved_model_builder(self, sess, output_path, sig_input, sig_output, sig_name):\n        """"""\n        create the SavedModelBuilder with standard endpoints.\n\n        we reuse the classify constants from tensorflow to define the predict\n        endpoint so that we can call the output by classes/scores.\n        """"""\n        builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(output_path)\n\n        classes_output_tensor = tf.compat.v1.saved_model.utils.build_tensor_info(\n            sig_output.classes)\n\n        output_def_map = {\n            tf.compat.v1.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES: classes_output_tensor\n        }\n        if sig_output.scores is not None:\n            scores_output_tensor = tf.compat.v1.saved_model.utils.build_tensor_info(sig_output.scores)\n            output_def_map[tf.compat.v1.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES] = scores_output_tensor\n\n        prediction_signature = (\n            tf.compat.v1.saved_model.signature_def_utils.build_signature_def(\n                inputs=sig_input,\n                outputs=output_def_map,  # we reuse classify constants here.\n                method_name=tf.compat.v1.saved_model.signature_constants.PREDICT_METHOD_NAME\n            )\n        )\n\n        legacy_init_op = tf.group(tf.compat.v1.tables_initializer(), name=\'legacy_init_op\')\n        definition = dict({})\n        definition[sig_name] = prediction_signature\n        builder.add_meta_graph_and_variables(\n            sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n            signature_def_map=definition,\n            legacy_init_op=legacy_init_op)\n\n        return builder\n\n    def _create_rpc_call(self, sess, basename, **kwargs):\n        pass\n\n\n@export\n@register_exporter(task=\'classify\', name=\'default\')\nclass ClassifyTensorFlowExporter(TensorFlowExporter):\n\n    def __init__(self, task, **kwargs):\n        super(ClassifyTensorFlowExporter, self).__init__(task, **kwargs)\n        self.return_labels = kwargs.get(\'return_labels\', True)\n\n    def _create_model(self, sess, basename, **kwargs):\n        model = load_model(basename, sess=sess, **kwargs)\n        values, indices = tf.nn.top_k(model.probs, len(model.labels))\n        # Restore the checkpoint\n        if self.return_labels:\n            class_tensor = tf.constant(model.labels)\n            table = tf.contrib.lookup.index_to_string_table_from_tensor(class_tensor)\n            classes = table.lookup(tf.to_int64(indices))\n            return model, classes, values\n        else:\n            return model, indices, values\n\n    def _create_rpc_call(self, sess, basename, **kwargs):\n        model, classes, values = self._create_model(sess, basename)\n\n        predict_tensors = {}\n        predict_tensors[model.lengths_key] = tf.compat.v1.saved_model.utils.build_tensor_info(model.lengths)\n\n        for k, v in model.embeddings.items():\n            try:\n                predict_tensors[k] = tf.compat.v1.saved_model.utils.build_tensor_info(v.x)\n            except:\n                raise Exception(\'Unknown attribute in signature: {}\'.format(v))\n\n        sig_input = predict_tensors\n        sig_output = SignatureOutput(classes, values)\n        sig_name = \'predict_text\'\n\n        assets = create_assets(\n            basename,\n            sig_input, sig_output, sig_name,\n            model.lengths_key,\n            return_labels=self.return_labels,\n            preproc=self.preproc_type()\n        )\n        return sig_input, sig_output, sig_name, assets\n\n\n@export\n@register_exporter(task=\'tagger\', name=\'default\')\nclass TaggerTensorFlowExporter(TensorFlowExporter):\n\n    def __init__(self, task, **kwargs):\n        super(TaggerTensorFlowExporter, self).__init__(task, **kwargs)\n        self.return_labels = kwargs.get(\'return_labels\', False)  # keep default behavior\n\n    def _create_model(self, sess, basename, **kwargs):\n        model = load_tagger_model(basename, sess=sess, **kwargs)\n        softmax_output = tf.nn.softmax(model.probs)\n        values, _ = tf.nn.top_k(softmax_output, 1)\n        indices = model.best\n        if self.return_labels:\n            labels = read_json(basename + \'.labels\')\n            list_of_labels = [\'\'] * len(labels)\n            for label, idval in labels.items():\n                list_of_labels[idval] = label\n            class_tensor = tf.constant(list_of_labels)\n            table = tf.contrib.lookup.index_to_string_table_from_tensor(class_tensor)\n            classes = table.lookup(tf.to_int64(indices))\n            return model, classes, values\n        else:\n            return model, indices, values\n\n    def _create_rpc_call(self, sess, basename, **kwargs):\n        model, classes, values = self._create_model(sess, basename)\n\n        predict_tensors = {}\n        predict_tensors[model.lengths_key] = tf.compat.v1.saved_model.utils.build_tensor_info(model.lengths)\n\n        for k, v in model.embeddings.items():\n            try:\n                predict_tensors[k] = tf.compat.v1.saved_model.utils.build_tensor_info(v.x)\n            except:\n                raise Exception(\'Unknown attribute in signature: {}\'.format(v))\n\n        sig_input = predict_tensors\n        sig_output = SignatureOutput(classes, values)\n        sig_name = \'tag_text\'\n        assets = create_assets(\n            basename,\n            sig_input, sig_output, sig_name,\n            model.lengths_key,\n            return_labels=self.return_labels,\n            preproc=self.preproc_type()\n        )\n        return sig_input, sig_output, sig_name, assets\n\n\n@export\n@register_exporter(task=\'seq2seq\', name=\'default\')\nclass Seq2SeqTensorFlowExporter(TensorFlowExporter):\n    """"""\n    seq2seq has a source and tgt embedding layer.\n\n    Please note that Tensorflow creates a state file where\n    the target embeddings are not a dictionary. In order to reuse\n    the embedding creation logic, I initialize a list of tuples\n    as if items() was called on a dictionary. This also hides\n    the correct key for target embeddings, so that is stored\n    as a constant alongside the state keys.\n\n    Perhaps this should be modified in the model, but this would decrease\n    the readablity of the state file. It also doesn\'t make sense\n    to generalize target embeddings to a dict since it\'s doubtful\n    that we will ever target more than one embedding.\n\n    :see baseline.python.tf.seq2seq.model:L411\n    """"""\n    SOURCE_STATE_EMBED_KEY = \'src_embeddings\'\n    TARGET_STATE_EMBED_KEY = \'tgt_embedding\'\n    TARGET_EMBED_KEY = \'tgt\'\n\n    def __init__(self, task, **kwargs):\n        super(Seq2SeqTensorFlowExporter, self).__init__(task, **kwargs)\n        self.return_labels = kwargs.get(\'return_labels\', False)\n\n    def _create_model(self, sess, basename, **kwargs):\n        model = load_seq2seq_model(\n            basename,\n            sess=sess, predict=True,\n            beam=self.task.config_params.get(\'beam\', 30),\n            **kwargs\n        )\n        return model, model.decoder.best, None\n\n    def _create_rpc_call(self, sess, basename):\n        model, classes, values = self._create_model(sess, basename)\n\n        predict_tensors = {}\n        predict_tensors[model.src_lengths_key] = tf.compat.v1.saved_model.utils.build_tensor_info(model.src_len)\n\n        for k, v in model.src_embeddings.items():\n            try:\n                predict_tensors[k] = tf.compat.v1.saved_model.utils.build_tensor_info(v.x)\n            except:\n                raise Exception(\'Unknown attribute in signature: {}\'.format(v))\n\n        sig_input = predict_tensors\n        sig_output = SignatureOutput(classes, values)\n        sig_name = \'suggest_text\'\n        assets = create_assets(\n            basename,\n            sig_input, sig_output, sig_name,\n            model.src_lengths_key,\n            beam=model.decoder.beam_width,\n            return_labels=self.return_labels,\n            preproc=self.preproc_type(),\n        )\n\n        return sig_input, sig_output, sig_name, assets\n\n\ndef create_bundle(builder, output_path, basename, assets=None):\n    """"""Creates the output files for an exported model.\n\n    :builder the tensorflow saved_model builder.\n    :output_path the path to export a model. this includes the model_version name.\n    :assets a dictionary of assets to save alongside the model.\n    """"""\n    builder.save()\n\n    model_name = os.path.basename(basename)\n    directory = os.path.realpath(os.path.dirname(basename))\n    save_to_bundle(output_path, directory, assets)\n\n\ndef create_assets(basename, sig_input, sig_output, sig_name, lengths_key=None, beam=None, return_labels=False, preproc=\'client\'):\n    """"""Save required variables for running an exported model from baseline\'s services.\n\n    :basename the base model name. e.g. /path/to/tagger-26075\n    :sig_input the input dictionary\n    :sig_output the output namedTuple\n    :lengths_key the lengths_key from the model.\n        used to translate batch output. Exported models will return a flat list,\n        and it needs to be reshaped into per-example lists. We use this key to tell\n        us which feature holds the sequence lengths.\n\n    """"""\n    inputs = [k for k in sig_input]\n    outputs =  sig_output._fields\n    model_name = basename.split(""/"")[-1]\n    directory = basename.split(""/"")[:-1]\n    metadata = create_metadata(inputs, outputs, sig_name, model_name, lengths_key, beam=beam,\n                               return_labels=return_labels, preproc=preproc)\n    return metadata\n'"
mead/tf/preproc_exporters.py,0,"b'import os\nimport json\nimport baseline\nfrom mead.exporters import register_exporter\nfrom mead.preprocessors import create_preprocessors\nfrom baseline.tf.embeddings import *\nfrom baseline.utils import exporter, read_json\nfrom collections import namedtuple\nfrom mead.tf.exporters import ClassifyTensorFlowExporter, TaggerTensorFlowExporter, create_assets\n\n__all__ = []\nexport = exporter(__all__)\n\nSignatureOutput = namedtuple(""SignatureOutput"", (""classes"", ""scores""))\n\n\ndef get_string_size(tensor):\n    """"""Get the number of tokens in a string sperated by space.""""""\n    split = tf.string_split(tf.reshape(tensor, [-1]))\n    return tf.shape(split)[1]\n\n\ndef get_mxlen(tensors):\n    """"""Find the longest string in a batch.""""""\n    sizes = tf.map_fn(\n        get_string_size, tensors,\n        dtype=tf.int32,\n        back_prop=False\n    )\n    return tf.reduce_max(sizes)\n\ndef get_lengths(tensors):\n    return tf.map_fn(\n        get_string_size, tensors, dtype=tf.int32, back_prop=False\n    )\n\n\ndef get_lengths(tensors):\n    return tf.map_fn(\n        get_string_size, tensors, dtype=tf.int32, back_prop=False\n    )\n\ndef peek_lengths_key(model_file, field_map):\n    """"""Check if there is lengths key to use when finding the length. (defaults to tokens)""""""\n    peek_state = read_json(model_file + "".state"")\n    lengths = peek_state.get(\'lengths_key\', \'tokens\')\n    lengths = lengths.replace(\'_lengths\', \'\')\n    return field_map.get(lengths, lengths)\n\n\nclass PreProcessorController(object):\n    def __init__(self, model_base_dir, pid, features, feature_exporter_field_map, length_field=\'tokens\'):\n        self.length_field = length_field\n        saved_vectorizers = self.get_vectorizers(model_base_dir, pid)\n        feature_names = [feature[\'name\'] for feature in features]\n        feature_vectorizer_mapping = {feature[\'name\']: feature[\'vectorizer\'][\'type\'] for feature in features}\n        self.preprocessors = dict()\n        indices, vocabs = self.create_vocabs(model_base_dir, pid, feature_names)\n        self.feature_exporter_field_map = feature_exporter_field_map\n        self.FIELD_NAMES = list(set(self.feature_exporter_field_map.values()))\n        for feature in feature_vectorizer_mapping:\n            self.preprocessors[feature] = create_preprocessors(preprocessor_type=feature_vectorizer_mapping[feature],\n                                                               feature=feature,\n                                                               vectorizer=saved_vectorizers[feature],\n                                                               index=indices[feature],\n                                                               vocab=vocabs[feature])\n\n    @staticmethod\n    def get_vectorizers(model_base_dir, pid):\n        """"""\n        :model_file the path-like object to the model and model name.\n        :vocab_suffixes the list of vocab types. e.g. \'word\', \'char\', \'ner\'.\n        """"""\n        import pickle\n        return pickle.load(open(os.path.join(model_base_dir, ""vectorizers-{}.pkl"".format(pid)), ""rb""))\n\n    def _create_example(self):\n        serialized_tf_example = tf.compat.v1.placeholder(tf.string, name=\'tf_example\')\n        feature_configs = {f: tf.FixedLenFeature(shape=[], dtype=tf.string) for f in self.FIELD_NAMES}\n        tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n        return tf_example\n\n    def preproc(self, tf_example):\n        preprocessed_inputs = {}\n        for feature in self.preprocessors:\n            preprocessed_inputs[feature] = self.preprocessors[feature].preproc(tf_example, self.mxlen)\n        return preprocessed_inputs\n\n    def create_preprocessed_input(self, tf_example):\n        """"""\n        Create a preprocessor chain inside of the tensorflow graph.\n        """"""\n        self.mxlen = get_mxlen(tf_example[self.length_field])\n\n        types = {f: tf.int64 for f in self.preprocessors}\n        return tf.map_fn(\n            self.preproc, tf_example,\n            dtype=types, back_prop=False\n        )\n\n    def create_vocabs(self, model_base_dir, pid, features):\n        """"""\n        :model_file the path-like object to the model and model name.\n        :vocab_suffixes the list of vocab types. e.g. \'word\', \'char\', \'ner\'.\n        """"""\n        indices = {}\n        vocabs = {}\n        for feature in features:\n            feature_vocab_file = os.path.join(model_base_dir, ""vocabs-{}-{}.json"".format(feature, pid))\n            if os.path.exists(feature_vocab_file):\n                indices[feature], vocabs[feature] = self._read_vocab(feature_vocab_file, feature)\n        return indices, vocabs\n\n    @staticmethod\n    def _read_vocab(vocab_file, feature_name):\n        with open(vocab_file, \'r\') as f:\n            vocab = json.load(f)\n\n        # Make a vocab list\n        vocab_list = [\'\'] * (len(vocab) + 1)\n\n        for v, i in vocab.items():\n            vocab_list[i] = v\n\n        tok2index = tf.contrib.lookup.index_table_from_tensor(\n            tf.constant(vocab_list),\n            default_value=Offsets.UNK,\n            dtype=tf.string,\n            name=\'%s2index\' % feature_name\n        )\n        return tok2index, vocab\n\n    def run(self):\n        tf_example = self._create_example()\n        preprocessed = self.create_preprocessed_input(tf_example)\n        lengths = get_lengths(tf_example[self.length_field])\n        return tf_example, preprocessed, lengths\n\n\n@export\n@register_exporter(task=\'classify\', name=\'preproc\')\nclass ClassifyTensorFlowPreProcExporter(ClassifyTensorFlowExporter):\n\n    @classmethod\n    def preproc_type(cls):\n        return \'server\'\n\n    def __init__(self, task, **kwargs):\n        super(ClassifyTensorFlowPreProcExporter, self).__init__(task, **kwargs)\n        self.feature_exporter_field_map = kwargs.get(\'feature_exporter_field_map\', {\'tokens\': \'text\'})\n        self.return_labels = kwargs.get(\'return_labels\', False)\n\n    def _create_rpc_call(self, sess, model_file, **kwargs):\n        model_base_dir = os.path.split(model_file)[0]\n        pid = model_file.split(""-"")[-1]\n\n        lengths = peek_lengths_key(model_file, self.feature_exporter_field_map)\n\n        pc = PreProcessorController(model_base_dir, pid, self.task.config_params[\'features\'],\n                                    self.feature_exporter_field_map, lengths)\n        tf_example, preprocessed, lengths = pc.run()\n        # Create a dict of embedding names to sub-graph outputs to wire in as embedding inputs\n        embedding_inputs = {}\n        for feature in preprocessed:\n            embedding_inputs[feature] = preprocessed[feature]\n        model, classes, values = self._create_model(sess, model_file, lengths=lengths, **embedding_inputs)\n        sig_input = {x: tf.saved_model.utils.build_tensor_info(tf_example[x]) for x in pc.FIELD_NAMES}\n        if model.lengths is not None:\n            sig_input.update({model.lengths_key: tf.saved_model.utils.build_tensor_info(model.lengths)})\n        sig_output = SignatureOutput(classes, values)\n        sig_name = \'predict_text\'\n        assets = create_assets(\n            model_file,\n            sig_input, sig_output, sig_name,\n            model.lengths_key,\n            return_labels=self.return_labels,\n            preproc=self.preproc_type(),\n        )\n        return sig_input, sig_output, sig_name, assets\n\n\n@export\n@register_exporter(task=\'tagger\', name=\'preproc\')\nclass TaggerTensorFlowPreProcExporter(TaggerTensorFlowExporter):\n\n    @classmethod\n    def preproc_type(cls):\n        return \'server\'\n\n    def __init__(self, task, **kwargs):\n        super(TaggerTensorFlowPreProcExporter, self).__init__(task, **kwargs)\n        self.feature_exporter_field_map = kwargs.get(\'feature_exporter_field_map\', {\'tokens\': \'text\'})\n        self.return_labels = kwargs.get(\'return_labels\', False)\n\n    def _create_rpc_call(self, sess, model_file, **kwargs):\n        model_base_dir = os.path.split(model_file)[0]\n        pid = model_file.split(""-"")[-1]\n        lengths = peek_lengths_key(model_file, self.feature_exporter_field_map)\n        pc = PreProcessorController(model_base_dir, pid, self.task.config_params[\'features\'],\n                                    self.feature_exporter_field_map, lengths)\n        tf_example, preprocessed, lengths = pc.run()\n        # Create a dict of embedding names to sub-graph outputs to wire in as embedding inputs\n        embedding_inputs = {}\n        for feature in preprocessed:\n            embedding_inputs[feature] = preprocessed[feature]\n        model, classes, values = self._create_model(sess, model_file, lengths=lengths, **embedding_inputs)\n        sig_input = {x: tf.saved_model.utils.build_tensor_info(tf_example[x]) for x in pc.FIELD_NAMES}\n        sig_input.update({model.lengths_key: tf.saved_model.utils.build_tensor_info(model.lengths)})\n        sig_output = SignatureOutput(classes, values)\n        sig_name = \'tag_text\'\n        assets = create_assets(\n            model_file,\n            sig_input, sig_output, sig_name,\n            model.lengths_key,\n            return_labels=self.return_labels,\n            preproc=self.preproc_type(),\n        )\n        return sig_input, sig_output, sig_name, assets\n'"
mead/tf/preprocessors.py,0,"b'import tensorflow as tf\nfrom baseline.utils import exporter\nfrom mead.preprocessors import Preprocessor, register_preprocessor\n\n__all__ = []\nexport = exporter(__all__)\n\n\n@export\nclass TensorFlowPreprocessor(Preprocessor):\n    """"""\n    Generic class for creating vectorizers using tensorflow ops, to be used for exporting models when the service gets\n    a string instead of a vectorized input.\n    """"""\n\n    def __init__(self, feature, vectorizer, index, vocab, **kwargs):\n        super(TensorFlowPreprocessor, self).__init__(feature, vectorizer, index, vocab, **kwargs)\n        self.FIELD_NAME = kwargs.get(\'FIELD_NAME\', \'tokens\')\n        self.upchars = tf.constant([chr(i) for i in range(65, 91)])\n        self.lchars = tf.constant([chr(i) for i in range(97, 123)])\n        self.upchars_lut = tf.contrib.lookup.index_table_from_tensor(mapping=self.upchars, num_oov_buckets=1, default_value=-1)\n\n    def lowercase(self, raw_post):\n        split_chars = tf.string_split(tf.reshape(raw_post, [-1]), delimiter="""").values\n        upchar_inds = self.upchars_lut.lookup(split_chars)\n        return tf.reduce_join(tf.map_fn(lambda x: tf.cond(x[0] > 25,\n                                                          lambda: x[1],\n                                                          lambda: self.lchars[x[0]]),\n                                        (upchar_inds, split_chars), dtype=tf.string))\n\n    def preproc(self, tf_example, mxlen):\n        """"""\n        Create a preprocessor chain inside of the tensorflow graph.\n        """"""\n        pass\n\n    def resize_sen(self, raw, mxlen):\n        """"""\n        Splits and rejoins a string to ensure that tokens meet\n        the required max len.\n        """"""\n        raw_tokens = tf.string_split(tf.reshape(raw, [-1])).values\n        # sentence length > mxlen\n        raw_post = tf.reduce_join(raw_tokens[:mxlen], separator="" "")\n        return raw_post\n\n    @staticmethod\n    def reshape_indices(indices, shape):\n        reshaped = tf.sparse_reset_shape(indices, new_shape=shape)\n        # Now convert to a dense representation\n        x = tf.sparse_tensor_to_dense(reshaped)\n        return x\n\n\n@export\n@register_preprocessor(name=\'token1d\')\nclass Token1DPreprocessor(TensorFlowPreprocessor):\n\n    def __init__(self, feature, vectorizer, index, vocab, **kwargs):\n        super().__init__(feature, vectorizer, index, vocab, **kwargs)\n        self.hard_mxlen = self.vectorizer.mxlen\n        # 10000 is just a large max, six.MAXSIZE was causing issues\n        self.hard_mxlen = self.hard_mxlen if self.hard_mxlen != -1 else 10000\n\n        transform_fn = self.vectorizer.transform_fn.__name__\n        if transform_fn not in [""identity_trans_fn"", ""lowercase""]:\n            raise NotImplementedError(""can not export arbitrary transform functions"")\n        self.do_lowercase = transform_fn == ""lowercase""\n\n    def create_word_vectors_from_post(self, raw_post, mxlen):\n        # vocab has only lowercase words\n        word2index = self.index\n        if self.do_lowercase:\n            raw_post = self.lowercase(raw_post)\n        word_tokens = tf.string_split(tf.reshape(raw_post, [-1]))\n        word_indices = word2index.lookup(word_tokens)\n        # Reshape them out to the proper length\n        reshaped_words = tf.sparse_reshape(word_indices, shape=[-1])\n        return self.reshape_indices(reshaped_words, [mxlen])\n\n    def preproc(self, post_mappings, mxlen):\n        # Split the input string, assuming that whitespace is splitter\n        # The client should perform any required tokenization for us and join on \' \'\n        mxlen = tf.minimum(mxlen, self.hard_mxlen)\n        raw_post = post_mappings[self.FIELD_NAME]\n        raw_post = self.resize_sen(raw_post, mxlen)\n        return self.create_word_vectors_from_post(raw_post, mxlen)\n\n\n@export\n@register_preprocessor(name=\'char2d\')\nclass Char2DPreprocessor(TensorFlowPreprocessor):\n    def __init__(self, feature, vectorizer, index, vocab, **kwargs):\n        super().__init__(feature, vectorizer, index, vocab, **kwargs)\n        self.hard_mxlen = self.vectorizer.mxlen\n        # 10000 is just a large max, six.MAXSIZE was causing issues\n        self.hard_mxlen = self.hard_mxlen if self.hard_mxlen != -1 else 10000\n        self.mxwlen = self.vectorizer.mxwlen\n        transform_fn = self.vectorizer.transform_fn.__name__\n        if transform_fn not in [""identity_trans_fn"", ""lowercase""]:\n            raise NotImplementedError(""can not export arbitrary transform functions"")\n        self.do_lowercase = transform_fn == ""lowercase""\n\n    def create_char_vectors_from_post(self, raw_post, mxlen):\n        char2index = self.index\n        if self.do_lowercase:\n            raw_post = self.lowercase(raw_post)\n        raw_post = tf.string_split(tf.reshape(raw_post, [-1]))\n        culled_word_token_vals = tf.substr(raw_post.values, 0, self.mxwlen)\n        char_tokens = tf.string_split(culled_word_token_vals, delimiter=\'\')\n        char_indices = char2index.lookup(char_tokens)\n        return self.reshape_indices(char_indices, [mxlen, self.mxwlen])\n\n    def preproc(self, post_mappings, mxlen):\n        mxlen = tf.minimum(mxlen, self.hard_mxlen)\n        raw_post = post_mappings[self.FIELD_NAME]\n        raw_post = self.resize_sen(raw_post, mxlen)\n        return self.create_char_vectors_from_post(raw_post, mxlen)\n\n\n@export\n@register_preprocessor(name=\'dict1d\')\nclass Dict1DPreprocessor(Token1DPreprocessor):\n    def __init__(self, feature, vectorizer, index, vocab, **kwargs):\n        super(Dict1DPreprocessor, self).__init__(feature, vectorizer, index, vocab, **kwargs)\n\n\n@export\n@register_preprocessor(name=\'dict2d\')\nclass Dict2DPreprocessor(Char2DPreprocessor):\n    def __init__(self, feature, vectorizer, index, vocab, **kwargs):\n        super(Dict2DPreprocessor, self).__init__(feature, vectorizer, index, vocab, **kwargs)\n'"
scripts/speed_test/__init__.py,0,b''
scripts/speed_test/report.py,0,"b'from __future__ import print_function\nfrom six.moves import shlex_quote\n\nimport os\nimport shutil\nimport sqlite3\nimport tempfile\nfrom subprocess import check_call, call\nfrom collections import defaultdict\nfrom itertools import repeat, chain\nimport numpy as np\nimport mead\nfrom mead.utils import convert_path\nfrom baseline.utils import read_config_file\n\n\nclass Version(object):\n    def __init__(self, major, minor, patch):\n        self.major = major\n        self.minor = minor\n        self.patch = patch\n    def __str__(self):\n        return ""."".join(map(str, [self.major, self.minor, self.patch]))\n\n\ndef explore(args):\n    if not os.path.exists(args.db):\n        print(""Unable to find database `{}`"".format(args.db))\n        return\n    conn = sqlite3.connect(args.db)\n    create_results(conn)\n    call([\'sqlite3\', \'-column\', \'-header\', shlex_quote(args.db)])\n\n\ndef query(args):\n    if not os.path.exists(args.db):\n        print(""Unable to find database `{}`"".format(args.db))\n        return\n    conn = sqlite3.connect(args.db)\n    task = args.task\n    dataset = args.dataset\n    frameworks = args.frameworks if args.frameworks is not None else get_frameworks(conn, task, dataset)\n    models = args.models if args.models is not None else get_models(conn, task, dataset)\n    create_results(conn)\n    results = defaultdict(list)\n    for fw in frameworks:\n        for model in models:\n            m, s, t, d, f, ms = get_results_by_version(\n                conn, task, dataset, fw, model, \'Train\',\n            )\n            results[\'framework\'].extend(f)\n            results[\'model\'].extend(ms)\n            results[\'task\'].extend(t)\n            results[\'dataset\'].extend(d)\n            results[\'mean\'].extend(m)\n            results[\'std\'].extend(s)\n    keys = results.keys()\n    for k in keys:\n        if k == \'mean\' or k == \'std\':\n            print("" {:>10} "".format(k), end=\'\')\n        else:\n            print("" {:<10} "".format(k), end=\'\')\n    print()\n    for k in keys:\n        print("" "" + ""-"" * 10 + "" "", end=\'\')\n    print()\n    for i in range(len(results[\'mean\'])):\n        for k in keys:\n            if isinstance(results[k][i], float):\n                print("" {:=10.5f} "".format(results[k][i]), end=\'\')\n            else:\n                print("" {:<10} "".format(results[k][i]), end=\'\')\n        print()\n\n\nPREAMBLE = r\'\'\'\n\\documentclass{standalone}\n\\usepackage{multirow}\n\\begin{document}\n\'\'\'\n\nEND = r\'\'\'\n\\end{document}\n\'\'\'\n\nMARKDOWN = \'\'\'\n# Speed Tests\n\nThis is a speed test of different baseline models and frameworks.\n\n\'\'\'\n\n\nresults_query = \'\'\'\nCREATE TABLE results AS\nSELECT\n    sub.runs as runs,\n    sub.latest as latest,\n    sub.mean as mean,\n    SUM((speed.time - sub.mean) * (speed.time - sub.mean)) / (COUNT(1) - 1) as var,\n    speed.task as task,\n    speed.dataset as dataset,\n    speed.model as model,\n    speed.framework as framework,\n    speed.phase as phase,\n    speed.framework_major as framework_major,\n    speed.framework_minor as framework_minor,\n    speed.framework_patch as framework_patch,\n    speed.baseline_major as baseline_major,\n    speed.baseline_minor as baseline_minor,\n    speed.baseline_patch as baseline_patch,\n    speed.cuda_major as cuda_major,\n    speed.cuda_minor as cuda_minor,\n    speed.cuda_patch as cuda_patch,\n    speed.cudnn_major as cudnn_major,\n    speed.cudnn_minor as cudnn_minor,\n    speed.cudnn_patch as cudnn_patch,\n    speed.python_major as python_major,\n    speed.python_minor as python_minor,\n    speed.python_patch as python_patch,\n    speed.gpu_name, speed.gpu_mem,\n    speed.cpu_name, speed.cpu_mem, speed.cpu_cores,\n    speed.config\nFROM speed JOIN (\n    SELECT\n        MAX(timestamp) as latest,\n        COUNT(time) as runs,\n        AVG(time) as mean,\n        task, framework, dataset, model, phase,\n        framework_major, framework_minor, framework_patch,\n        baseline_major, baseline_minor, baseline_patch,\n        cuda_major, cuda_minor, cuda_patch,\n        cudnn_major, cudnn_minor, cudnn_patch,\n        python_major, python_minor, python_patch,\n        gpu_name, gpu_mem,\n        cpu_name, cpu_mem, cpu_cores,\n        config\n    FROM speed GROUP BY\n        phase, framework, dataset, model, task,\n        framework_major, framework_minor, framework_patch,\n        baseline_major, baseline_minor, baseline_patch,\n        cuda_major, cuda_minor, cuda_patch,\n        cudnn_major, cudnn_minor, cudnn_patch,\n        python_major, python_minor, python_patch,\n        gpu_name, gpu_mem,\n        cpu_name, cpu_mem, cpu_cores,\n        config\n) AS sub ON\n    speed.task = sub.task AND\n    speed.framework = sub.framework AND\n    speed.dataset = sub.dataset AND\n    speed.model = sub.model AND\n    speed.phase = sub.phase AND\n    speed.framework_major = sub.framework_major AND\n    speed.framework_minor = sub.framework_minor AND\n    speed.framework_patch = sub.framework_patch AND\n    speed.baseline_major = sub.baseline_major AND\n    speed.baseline_minor = sub.baseline_minor AND\n    speed.baseline_patch = sub.baseline_patch AND\n    speed.cuda_major = sub.cuda_major AND\n    speed.cuda_minor = sub.cuda_minor AND\n    speed.cuda_patch = sub.cuda_patch AND\n    speed.cudnn_major = sub.cudnn_major AND\n    speed.cudnn_minor = sub.cudnn_minor AND\n    speed.cudnn_patch = sub.cudnn_patch AND\n    speed.python_major = sub.python_major AND\n    speed.python_minor = sub.python_minor AND\n    speed.python_patch = sub.python_patch AND\n    speed.gpu_name = sub.gpu_name AND speed.gpu_mem = sub.gpu_mem AND\n    speed.cpu_name = sub.cpu_name AND speed.cpu_mem = sub.cpu_mem AND speed.cpu_cores = sub.cpu_cores AND\n    speed.config = sub.config\nGROUP BY\n    speed.phase,\n    speed.framework,\n    speed.dataset,\n    speed.model,\n    speed.task,\n    speed.framework_major,\n    speed.framework_minor,\n    speed.framework_patch,\n    speed.baseline_major,\n    speed.baseline_minor,\n    speed.baseline_patch,\n    speed.cuda_major,\n    speed.cuda_minor,\n    speed.cuda_patch,\n    speed.cudnn_major,\n    speed.cudnn_minor,\n    speed.cudnn_patch,\n    speed.python_major,\n    speed.python_minor,\n    speed.python_patch,\n    speed.gpu_name, speed.gpu_mem,\n    speed.cpu_name, speed.cpu_mem, speed.cpu_cores,\n    speed.config\nHAVING runs >= 2\nORDER BY speed.task, speed.dataset, speed.model, speed.framework, speed.phase,\nframework_major, framework_minor, framework_patch,\nbaseline_major, baseline_minor, baseline_patch,\ncuda_major, cuda_minor, cuda_patch,\ncudnn_major, cudnn_minor, cudnn_patch,\npython_major, python_minor, python_patch,\nlatest;\n\'\'\'\n\n\ndef create_results(conn):\n    try:\n        c = conn.cursor()\n        c.execute(\'\'\'\n            DROP TABLE results;\n        \'\'\')\n        conn.commit()\n    except sqlite3.OperationalError:\n        conn.rollback()\n    finally:\n        c.close()\n    try:\n        c = conn.cursor()\n        c.execute(results_query)\n        conn.commit()\n    except Exception as e:\n        conn.rollback()\n        raise(e)\n    finally:\n        c.close()\n\n\ndef get_tasks(conn):\n    tasks = conn.execute(\'\'\'\n        SELECT DISTINCT task FROM speed ORDER BY task;\n    \'\'\').fetchall()\n    return [t[0] for t in tasks]\n\n\ndef get_datasets(conn, task):\n    datasets = conn.execute(\'\'\'\n        SELECT DISTINCT dataset FROM speed WHERE task = ? ORDER BY dataset;\n    \'\'\', (task,)\n    ).fetchall()\n    return [d[0] for d in datasets]\n\n\ndef get_frameworks(conn, task, dataset):\n    frameworks = conn.execute(\'\'\'\n        SELECT DISTINCT framework FROM speed\n        WHERE task = ? AND dataset = ?\n        ORDER BY framework;\n        \'\'\', (task, dataset)\n    ).fetchall()\n    return [f[0] for f in frameworks]\n\n\ndef get_models(conn, task, dataset):\n    models = conn.execute(\'\'\'\n        SELECT DISTINCT model FROM speed\n        WHERE task = ? AND dataset = ?\n        ORDER BY model;\n        \'\'\', (task, dataset)\n    ).fetchall()\n    return [m[0] for m in models]\n\n\ndef get_phases(conn, task, dataset):\n    phases = conn.execute(\'\'\'\n        SELECT DISTINCT phase FROM speed\n        WHERE task = ? AND dataset = ?\n        ORDER BY phase;\n        \'\'\', (task, dataset)\n    ).fetchall()\n    phases = {p[0] for p in phases}\n    return phases\n\n\ndef get_results(conn, task, dataset, framework, model, phase):\n    mean, var = conn.execute(\'\'\'\n        SELECT mean, var FROM results\n        WHERE task = ? AND dataset = ? AND framework = ? AND model = ? AND phase = ?\n        ORDER BY\n            framework_major DESC, framework_minor DESC, framework_patch DESC,\n            baseline_major DESC, baseline_minor DESC, baseline_patch DESC,\n            cuda_major DESC, cuda_minor DESC, cuda_patch DESC,\n            cudnn_major DESC, cudnn_minor DESC, cudnn_patch DESC,\n            python_major DESC, python_minor DESC, python_patch DESC\n        LIMIT 1;\n        \'\'\',\n        (task, dataset, framework, model, phase)\n    ).fetchall()[0]\n    return mean, np.sqrt(var)\n\n\ndef get_results_by_version(conn, task, dataset, framework, model, phase):\n    results = conn.execute(\'\'\'\n        SELECT\n            mean, var, task, dataset, framework, model\n        FROM results\n        WHERE task = ? AND dataset = ? AND framework = ? AND model = ? AND phase = ?\n        ORDER BY\n            framework_major DESC, framework_minor DESC, framework_patch DESC,\n            baseline_major DESC, baseline_minor DESC, baseline_patch DESC,\n            cuda_major DESC, cuda_minor DESC, cuda_patch DESC,\n            cudnn_major DESC, cudnn_minor DESC, cudnn_patch DESC,\n            python_major DESC, python_minor DESC, python_patch DESC\n        LIMIT 1;\n        \'\'\',\n        (task, dataset, framework, model, phase)\n    ).fetchall()\n    mean = [r[0] for r in results]\n    std = [np.sqrt(r[1]) for r in results]\n    task = [r[2] for r in results]\n    dataset = [r[3] for r in results]\n    framework = [r[4] for r in results]\n    model = [r[5] for r in results]\n    return mean, std, task, dataset, framework, model\n\n\ndef get_env(conn, task, dataset, framework, model, phase):\n    env = conn.execute(\'\'\'\n        SELECT\n            framework_major, framework_minor, framework_patch,\n            baseline_major, baseline_minor, baseline_patch,\n            cuda_major, cuda_minor, cuda_patch,\n            cudnn_major, cudnn_minor, cudnn_patch,\n            python_major, python_minor, python_patch,\n            gpu_name, gpu_mem,\n            cpu_name, cpu_mem, cpu_cores\n        FROM results\n        WHERE task = ? AND dataset = ? AND framework = ? AND model = ? AND phase = ?\n        ORDER BY\n            framework_major DESC, framework_minor DESC, framework_patch DESC,\n            baseline_major DESC, baseline_minor DESC, baseline_patch DESC,\n            cuda_major DESC, cuda_minor DESC, cuda_patch DESC,\n            cudnn_major DESC, cudnn_minor DESC, cudnn_patch DESC,\n            python_major DESC, python_minor DESC, python_patch DESC\n        LIMIT 1;\n        \'\'\',\n        (task, dataset, framework, model, phase)\n    ).fetchall()[0]\n    _env = {}\n    _env[\'framework\'] = Version(env[0], env[1], env[2])\n    _env[\'baseline\'] = Version(env[3], env[4], env[5])\n    _env[\'cuda\'] = Version(env[6], env[7], env[8])\n    _env[\'cudnn\'] = Version(env[9], env[10], env[11])\n    _env[\'python\'] = Version(env[12], env[13], env[14])\n    _env[\'gpu_name\'] = env[15]\n    _env[\'gpu_mem\'] = env[16]\n    _env[\'cpu_name\'] = env[17]\n    _env[\'cpu_mem\'] = env[18]\n    _env[\'cpu_cores\'] = env[19]\n    return _env\n\n\ndef create_table(out, task, table):\n    dir_name = ""images_{}"".format(out)\n    tmp = tempfile.mkdtemp()\n    curr = os.getcwd()\n    os.chdir(tmp)\n    pdf = create_latex(task, table)\n    pic = create_png(pdf)\n    pic_file = os.path.join(tmp, pic)\n    os.chdir(curr)\n    try:\n        os.mkdir(dir_name)\n    except:\n        pass\n    pic = os.path.join(dir_name, pic)\n    os.rename(pic_file, pic)\n    shutil.rmtree(tmp)\n    return pic\n\n\ndef create_latex(file_name, table):\n    latex = ""\\n"".join([PREAMBLE, table, END])\n    with open(file_name, \'w\') as f:\n        f.write(latex)\n    res = check_call(\'pdflatex {}\'.format(file_name), shell=True, stdout=open(os.devnull, \'w\'))\n    return file_name + \'.pdf\'\n\n\ndef create_png(file_name):\n    pic = file_name[:-3] + \'png\'\n    res = check_call(\'convert -density 300 {} -quality 90 {}\'.format(file_name, pic), shell=True)\n    res = check_call(\'convert {0} -background white -alpha remove {0}\'.format(pic), shell=True)\n    return pic\n\n\ndef build_table(conn, task, dataset, models, frameworks, phases):\n    cols = (\'c\' * len(frameworks) + ""|"")\n    table_start = r\'\\begin{tabular}{|c|c|%s|}\\hline\' % cols\n\n    table_header = [\'\\multicolumn{2}{|c|}{%s}\' % dataset]\n    for fw in frameworks:\n        table_header.append(fw)\n    table_header = ""&"".join(table_header) + r""\\\\ \\hline""\n\n    rows = []\n    for phase in phases:\n        label = \'\\multirow{%d}{*}{%s}\' % (len(models), phase)\n        first = True\n        for model in models:\n            if first:\n                model_row = [phase]\n                first = False\n            else:\n                model_row = [\' \']\n            model_row.append(model)\n            for framework in frameworks:\n                mean, std = get_results(conn, task, dataset, framework, model, phase)\n                model_row.append(""${:.2f} \\pm {:.2f}$"".format(mean, std))\n            rows.append(""&"".join(model_row) + r""\\\\ \\cline{2-2}"")\n        rows.append(""\\hline"")\n    table_end = ""\\hline\\n\\end{tabular}""\n    x = ""\\n"".join([\n        table_start,\n        table_header,\n        *rows,\n        table_end\n    ])\n    return x\n\n\ndef create_envs(conn, task, dataset, models, frameworks):\n    md = [\'\\n\\n<details><summary>Environment</summary>\\n<p>\\nThese results were runs on the following configurations:\\n\\n\']\n    for framework in frameworks:\n        for model in models:\n            env = get_env(conn, task, dataset, framework, model, \'Train\')\n            md.append(\' * {} - {}\'.format(framework, model))\n            for k, v in env.items():\n                md.append(\'    * {} - {}\'.format(k, v))\n    md.append(""\\n\\n</p>\\n</details>\\n\\n"")\n    return md\n\n\ndef report(args):\n    if not os.path.exists(args.db):\n        print(""Unable to find database `{}`"".format(args.db))\n        return\n    conn = sqlite3.connect(args.db)\n    create_results(conn)\n    tasks = get_tasks(conn)\n    markdown = [MARKDOWN]\n    for task in tasks:\n        datasets = get_datasets(conn, task)\n        for dataset in datasets:\n            frameworks = get_frameworks(conn, task, dataset)\n            models = get_models(conn, task, dataset)\n            phases = get_phases(conn, task, dataset)\n            table = build_table(conn, task, dataset, models, frameworks, phases)\n            pic = create_table(args.out, task, table)\n            markdown.append(""\\n## {}"".format(task))\n            markdown.append("""")\n            markdown.append(""![](./{})"".format(pic))\n            markdown.append("""")\n            envs = create_envs(conn, task, dataset, models, frameworks)\n            markdown.extend(envs)\n    with open(args.out + "".md"", \'w\') as f:\n        f.write(""\\n"".join(markdown))\n'"
scripts/speed_test/run.py,3,"b'import os\nimport re\nimport sys\nimport json\nimport shutil\nimport sqlite3\nimport tempfile\nimport datetime\nfrom pprint import pprint\nfrom copy import deepcopy\nfrom collections import namedtuple\nfrom subprocess import check_output\nfrom multiprocessing import Process, cpu_count, Manager\nimport numpy as np\nimport mead\nimport baseline\nfrom baseline.progress import create_progress_bar\nfrom baseline.utils import read_config_file, suppress_output\nfrom xpctl.helpers import order_json\n\nVersion = namedtuple(\'Version\', \'major minor patch\')\nsimple_version_regex = re.compile(\'^(\\d+)\\.(\\d+)\\.(\\d+)\')\n\nFRAMEWORKS = [\'tensorflow\', \'pytorch\']\nFULL_FRAMEWORKS = [\'tensorflow\', \'pytorch\']\nTASKS = [\'classify\', \'tagger\', \'seq2seq\', \'lm\']\nPHASES = [\'Train\', \'Valid\', \'Test\']\n\n\ndef run_model(si, config_params, logs, settings, datasets, embeddings, task_name, dir_, gpu):\n    """"""Run a model and collect system information.""""""\n    os.chdir(dir_)\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n    config_params[\'reporting\'] = {}\n    with suppress_output():\n        task = mead.Task.get_task_specific(task_name, logs, settings)\n        task.read_config(config_params, datasets)\n        task.initialize(embeddings)\n        task.train()\n\n        si[\'framework_version\'] = get_framework_version(config_params[\'backend\'])\n        si[\'cuda\'], si[\'cudnn\'] = get_cuda_version(config_params[\'backend\'])\n        si[\'gpu_name\'], si[\'gpu_mem\'] = get_gpu_info(gpu)\n        si[\'cpu_name\'], si[\'cpu_mem\'], si[\'cpu_cores\'] = get_cpu_info()\n        si[\'python\'] = get_python_version()\n        si[\'baseline\'] = version_str_to_tuple(baseline.__version__)\n\n\ndef parse_logs(file_name):\n    """"""Read the timing logs out of the log file.""""""\n    data = []\n    with open(file_name) as f:\n        for line in f:\n            entry = json.loads(line)\n            if \'phase\' in entry and entry[\'phase\'] == \'Test\':\n                continue\n            if \'time\' in entry:\n                data.append(entry)\n    return data\n\n\ndef create_db(name=""speed.db""):\n    """"""Generate/Connect to the database to save results.""""""\n    conn = sqlite3.connect(name, isolation_level=\'EXCLUSIVE\')\n    try:\n        c = conn.cursor()\n        c.execute(\'\'\'\n            CREATE TABLE speed(\n                time FLOAT,\n                phase TEXT,\n                framework TEXT,\n                framework_major INTEGER,\n                framework_minor INTEGER,\n                framework_patch INTEGER,\n                dataset TEXT,\n                model TEXT,\n                task TEXT,\n                config TEXT,\n                cuda_major INTEGER,\n                cuda_minor INTEGER,\n                cuda_patch INTEGER,\n                cudnn_major INTEGER,\n                cudnn_minor INTEGER,\n                cudnn_patch INTEGER,\n                gpu_name TEXT,\n                gpu_mem FLOAT,\n                cpu_name TEXT,\n                cpu_mem FLOAT,\n                cpu_cores INTEGER,\n                python_major INTEGER,\n                python_minor INTEGER,\n                python_patch INTEGER,\n                baseline_major INTEGER,\n                baseline_minor INTEGER,\n                baseline_patch INTEGER,\n                timestamp DATE\n        );\'\'\')\n        conn.commit()\n    except sqlite3.OperationalError:\n        conn.rollback()\n    finally:\n        c.close()\n    return conn\n\n\ndef save_data(conn, speeds, config, si):\n    """"""Write data to the db.""""""\n    framework = config[\'backend\']\n    task = config[\'task\']\n    config_str = json.dumps(order_json(config)).encode(\'utf-8\')\n    task, dataset, model = get_run_info(config)\n    fw_v = si[\'framework_version\']\n    bl_v = si[\'baseline\']\n    c_v = si[\'cuda\']\n    nn_v = si[\'cudnn\']\n    p_v = si[\'python\']\n\n    try:\n        c = conn.cursor()\n        for speed in speeds:\n            c.execute(\'\'\'\n                INSERT INTO speed (\n                    time, phase, framework, dataset, model, task, config,\n                    framework_major, framework_minor, framework_patch,\n                    baseline_major, baseline_minor, baseline_patch,\n                    cuda_major, cuda_minor, cuda_patch,\n                    cudnn_major, cudnn_minor, cudnn_patch,\n                    python_major, python_minor, python_patch,\n                    gpu_name, gpu_mem, cpu_name, cpu_mem, cpu_cores,\n                    timestamp\n                )\n                VALUES (\n                    ?, ?, ?, ?, ?, ?, ?,\n                    ?, ?, ?,\n                    ?, ?, ?,\n                    ?, ?, ?,\n                    ?, ?, ?,\n                    ?, ?, ?,\n                    ?, ?, ?, ?, ?,\n                    STRFTIME(\'%s\', \'now\')\n                );\n                \'\'\', (\n                    speed[\'time\'], speed[\'phase\'], framework, dataset, model, task, config_str,\n                    fw_v.major, fw_v.minor, fw_v.patch,\n                    bl_v.major, bl_v.minor, bl_v.patch,\n                    c_v.major, c_v.minor, c_v.patch,\n                    nn_v.major, nn_v.minor, nn_v.patch,\n                    p_v.major, p_v.minor, p_v.patch,\n                    si[\'gpu_name\'], si[\'gpu_mem\'], si[\'cpu_name\'], si[\'cpu_mem\'], si[\'cpu_cores\'],\n                )\n            )\n        conn.commit()\n    except Exception as e:\n        conn.rollback()\n        raise(e)\n    finally:\n        c.close()\n\n\ndef get_run_info(config):\n    """"""Get task, dataset, and model info from a config.""""""\n    task = config[\'task\']\n    dataset = config[\'dataset\']\n    model = config[\'model\'][\'model_type\']\n    if task == \'classify\':\n        model = \'conv\' if model == \'default\' else model\n        if model == \'lstm\':\n            if config[\'model\'].get(\'rnn_type\') == \'blstm\':\n                model = \'blstm\'\n    elif task == \'tagger\':\n        model = \'lstm\' if model == \'default\' else model\n        if bool(config[\'model\'].get(\'crf\', True)):\n            model = \'crf\'\n    elif task == \'seq2seq\':\n        model = \'vanilla\' if model == \'default\' else model\n    elif task == \'lm\':\n        model = \'word\' if model == \'default\' else model\n    return task, dataset, model\n\n\ndef get_framework_version(framework):\n    """"""Get the version of a framework without importing them all at the same time.""""""\n    if framework == ""tensorflow"":\n        import tensorflow\n        version = tensorflow.__version__\n    elif framework == ""pytorch"":\n        import torch\n        version = torch.__version__\n    return version_str_to_tuple(version)\n\n\ndef version_str_to_tuple(version):\n    m = simple_version_regex.match(version)\n    if m is None:\n        return Version(0, 0, 0)\n    return Version(int(m.group(1)), int(m.group(2)), int(m.group(3)))\n\n\ndef get_cuda_version(framework):\n    if framework == \'pytorch\':\n        import torch\n        cuda = version_str_to_tuple(torch.version.cuda)\n        cudnn = torch.backends.cudnn.version()\n        cudnn_major = cudnn // 1000\n        cudnn = cudnn % 1000\n        cudnn_minor = cudnn // 100\n        cudnn_patch = cudnn % 100\n        cudnn = Version(cudnn_major, cudnn_minor, cudnn_patch)\n        return cuda, cudnn\n    return file_based_cuda_version()\n\n\ndef file_based_cuda_version():\n    """"""Find the version of CUDA and CUDNN.""""""\n    loc = os.getenv(\'CUDA_HOME\', os.path.join(os.sep, \'usr\', \'local\', \'cuda\'))\n    cuda_version = open(os.path.join(loc, \'version.txt\')).read().split()[-1]\n    cudnn = open(os.path.join(loc, \'include\', \'cudnn.h\')).read()\n    cudnn_major = re.search(r\'CUDNN_MAJOR (\\d)\', cudnn).groups()[0]\n    cudnn_minor = re.search(r\'CUDNN_MINOR (\\d)\', cudnn).groups()[0]\n    cudnn_patch = re.search(r\'CUDNN_PATCHLEVEL (\\d)\', cudnn).groups()[0]\n    cudnn_version = Version(cudnn_major, cudnn_minor, cudnn_patch)\n    return version_str_to_tuple(cuda_version), cudnn_version\n\n\ndef get_gpu_info(gpu):\n    """"""Get name and memory info about a GPU.""""""\n    gpu_info = check_output(\'nvidia-smi --display=MEMORY -q --id={}\'.format(gpu), shell=True).decode(\'utf-8\')\n    mem = re.search(r""^\\s+Total\\s+: (\\d+ MiB)\\s*$"", gpu_info, re.M)\n    gpu_mem = mem.groups()[0]\n    gpu_mem = int(gpu_mem[:-3])\n\n    gpu_dir = \'/proc/driver/nvidia/gpus\'\n    gpu_file = sorted(os.listdir(gpu_dir))[int(gpu)]\n    gpu_file = os.path.join(gpu_dir, gpu_file, \'information\')\n    gpu_name = open(gpu_file).read().split(""\\n"")[0].split("":"")[1].strip()\n    return gpu_name, gpu_mem\n\n\ndef get_cpu_info():\n    """"""Get CPU name, memory, and core count.""""""\n    cpu_cores = cpu_count()\n\n    cpu_info = open(\'/proc/cpuinfo\').read().split(""\\n\\n"")[0]\n\n    cpu_name = re.search(r""^model name\\s+: (.*)$"", cpu_info, re.M).groups()[0]\n\n    mem_info = check_output(\'free -m\', shell=True).decode(\'utf-8\')\n    mem_info = mem_info.split(\'\\n\')[1]\n    cpu_mem = int(mem_info.split()[1])\n\n    return cpu_name, cpu_mem, cpu_cores\n\n\ndef get_python_version():\n    v = sys.version_info\n    version = Version(v.major, v.minor, v.micro)\n    return version\n\n\ndef edit_classify_config(config, frameworks=FRAMEWORKS):\n    """"""Rotate frameworks.""""""\n    configs = []\n    for fw in frameworks:\n        c = deepcopy(config)\n        c[\'backend\'] = fw\n        configs.append(c)\n    return configs\n\n\ndef edit_tagger_config(config, frameworks=FULL_FRAMEWORKS, no_crf=False):\n    """"""Rotate frameworks and optionally remove the CRF.""""""\n    configs = []\n    for fw in frameworks:\n        c = deepcopy(config)\n        c[\'backend\'] = fw\n        configs.append(c)\n    if not no_crf:\n        new_configs = []\n        # Remove the CRF\n        for config in configs:\n            c = deepcopy(config)\n            c[\'model\'][\'crf\'] = False\n            c[\'model\'][\'crf_mask\'] = False\n            new_configs.append(c)\n            new_configs.append(config)\n        configs = new_configs\n    return configs\n\n\ndef edit_lm_config(config, frameworks=FULL_FRAMEWORKS):\n    """"""Rotate frameworks.""""""\n    configs = []\n    for fw in frameworks:\n        c = deepcopy(config)\n        c[\'backend\'] = fw\n        configs.append(c)\n    return configs\n\n\ndef edit_seq2seq_config(config, frameworks=FULL_FRAMEWORKS, no_attn=False):\n    """"""Rotate frameworks and optionally remove attention.""""""\n    configs = []\n    for fw in frameworks:\n        c = deepcopy(config)\n        c[\'backend\'] = fw\n        configs.append(c)\n    if not no_attn:\n        new_configs = []\n        # Run the non attention version\n        for config in configs:\n            c = deepcopy(config)\n            c[\'model\'][\'model_type\'] = \'default\'\n            new_configs.append(c)\n            new_configs.append(config)\n        configs = new_configs\n    return configs\n\n\ndef edit_config(config, frameworks=None, no_crf=False, no_attn=False):\n    """"""Expand a config to test all the needed versions.""""""\n    task = config[\'task\']\n    if task == \'classify\':\n        frameworks = frameworks if frameworks is not None else FRAMEWORKS\n        return edit_classify_config(config, frameworks)\n    if task == \'tagger\':\n        frameworks = frameworks if frameworks is not None else FULL_FRAMEWORKS\n        return edit_tagger_config(config, frameworks, no_crf=no_crf)\n    if task == \'lm\':\n        frameworks = frameworks if frameworks is not None else FULL_FRAMEWORKS\n        return edit_lm_config(config, frameworks)\n    if task == \'seq2seq\':\n        frameworks = frameworks if frameworks is not None else FULL_FRAMEWORKS\n        return edit_seq2seq_config(config, frameworks, no_attn=no_attn)\n\n\ndef get_configs(path):\n    """"""Get configs from disk, if it\'s a dir read all configs in it.""""""\n    configs = []\n    if os.path.isdir(path):\n        for file_name in os.listdir(path):\n            configs.append(read_config_file(os.path.join(path, file_name)))\n    else:\n        configs.append(read_config_file(path))\n    return configs\n\n\ndef run(args):\n    # Limit it to a single GPU.\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(args.gpu)\n\n    conn = create_db(args.db)\n    m = Manager()\n\n    logs = args.logging\n    datasets = args.datasets\n    embeddings = args.embeddings\n    settings = args.settings\n\n    # So we don\'t litter the fs\n    dir_ = tempfile.mkdtemp(prefix=\'baseline-speed-test-\')\n\n    try:\n        configs = get_configs(args.config)\n        if not args.single:\n            full_configs = []\n            for config in configs:\n                full_configs.extend(edit_config(config, args.frameworks, args.no_crf, args.no_attn))\n            configs = full_configs\n        if args.verbose:\n            for config in configs:\n                pprint(config)\n                print()\n            print()\n        steps = len(configs)\n        pg = create_progress_bar(steps)\n        for config in configs:\n            write_config = deepcopy(config)\n            config[\'train\'][\'epochs\'] = args.trials\n            task_name = config[\'task\']\n\n            system_info = m.dict()\n            p = Process(\n                target=run_model,\n                args=(\n                    system_info,\n                    config,\n                    logs,\n                    settings,\n                    datasets,\n                    embeddings,\n                    task_name,\n                    dir_,\n                    int(args.gpu)\n                )\n            )\n            p.start()\n            pid = p.pid\n            p.join()\n            log_file = os.path.join(dir_, \'timing-{}.log\'.format(pid))\n            speeds = parse_logs(log_file)\n\n            save_data(conn, speeds, write_config, system_info)\n            pg.update()\n        pg.done()\n    finally:\n        shutil.rmtree(dir_)\n\n\ndef add(args):\n    conn = create_db(args.db)\n    config = read_config_file(args.config)\n    speeds = parse_logs(args.log)\n    if not speeds:\n        return\n    si = {}\n    si[\'framework_version\'] = get_framework_version(config[\'backend\'])\n    si[\'cuda\'], si[\'cudnn\'] = get_cuda_version(config[\'backend\'])\n    si[\'gpu_name\'], si[\'gpu_mem\'] = get_gpu_info(args.gpu)\n    si[\'cpu_name\'], si[\'cpu_mem\'], si[\'cpu_cores\'] = get_cpu_info()\n    si[\'python\'] = get_python_version()\n    si[\'baseline\'] = version_str_to_tuple(baseline.__version__)\n\n    save_data(conn, speeds, config, si)\n'"
baseline/pytorch/classify/__init__.py,2,b'from baseline.pytorch.classify.model import *\nfrom baseline.pytorch.classify.train import *\n'
baseline/pytorch/classify/model.py,9,"b'import logging\nfrom baseline.model import ClassifierModel, register_model\nfrom baseline.pytorch.torchy import *\nfrom baseline.utils import listify, write_json\nfrom eight_mile.pytorch.layers import *\nimport torch.backends.cudnn as cudnn\nimport os\ncudnn.benchmark = True\n\nlogger = logging.getLogger(\'baseline\')\n\n\nclass ClassifierModelBase(nn.Module, ClassifierModel):\n    """"""Base for all baseline implementations of token-based classifiers\n\n    This class provides a loose skeleton around which the baseline models\n    are built.  It is built on the PyTorch `nn.Module` base, and fulfills the `ClassifierModel` interface.\n    To override this class, the use would typically override the `create_layers` function which will\n    create and attach all sub-layers of this model into the class, and the `forward` function which will\n    give the model some implementation to call on forward.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.gpu = False\n\n    @classmethod\n    def load(cls, filename: str, **kwargs) -> \'ClassifierModelBase\':\n        device = kwargs.get(\'device\')\n        if not os.path.exists(filename):\n            filename += \'.pyt\'\n        model = torch.load(filename, map_location=device)\n        model.gpu = False if device == \'cpu\' else model.gpu\n        return model\n\n    def save(self, outname: str):\n        logger.info(\'saving %s\' % outname)\n        torch.save(self, outname)\n        basename, _ = os.path.splitext(outname)\n        write_json(self.labels, basename + "".labels"")\n\n    @classmethod\n    def create(cls, embeddings, labels, **kwargs) -> \'ClassifierModelBase\':\n\n        model = cls()\n        model.pdrop = kwargs.get(\'pdrop\', 0.5)\n        model.lengths_key = kwargs.get(\'lengths_key\')\n        model.gpu = not bool(kwargs.get(\'nogpu\', False))\n        model.labels = labels\n        model.create_layers(embeddings, **kwargs)\n        logger.info(model)\n        return model\n\n    def cuda(self, device=None):\n        self.gpu = True\n        return super().cuda(device=device)\n\n    def create_loss(self):\n        return nn.NLLLoss()\n\n    def make_input(self, batch_dict, perm=False, numpy_to_tensor=False):\n\n        """"""Transform a `batch_dict` into something usable in this model\n\n        :param batch_dict: (``dict``) A dictionary containing all inputs to the embeddings for this model\n        :return:\n        """"""\n        example_dict = dict({})\n        perm_idx = None\n\n        # Allow us to track a length, which is needed for BLSTMs\n        if self.lengths_key is not None:\n            lengths = batch_dict[self.lengths_key]\n            if numpy_to_tensor:\n                lengths = torch.from_numpy(lengths)\n            lengths, perm_idx = lengths.sort(0, descending=True)\n            if self.gpu:\n                lengths = lengths.cuda()\n            example_dict[\'lengths\'] = lengths\n\n        for key in self.embeddings.keys():\n            tensor = batch_dict[key]\n            if numpy_to_tensor:\n                tensor = torch.from_numpy(tensor)\n            if perm_idx is not None:\n                tensor = tensor[perm_idx]\n            if self.gpu:\n                tensor = tensor.cuda()\n            example_dict[key] = tensor\n\n        y = batch_dict.get(\'y\')\n        if y is not None:\n            if numpy_to_tensor:\n                y = torch.from_numpy(y)\n            if perm_idx is not None:\n                y = y[perm_idx]\n            if self.gpu:\n                y = y.cuda()\n            example_dict[\'y\'] = y\n\n        if perm:\n            return example_dict, perm_idx\n\n        return example_dict\n\n    def predict_batch(self, batch_dict: Dict[str, TensorDef], **kwargs) -> TensorDef:\n        numpy_to_tensor = bool(kwargs.get(\'numpy_to_tensor\', True))\n        examples, perm_idx = self.make_input(batch_dict, perm=True, numpy_to_tensor=numpy_to_tensor)\n        with torch.no_grad():\n            probs = self(examples).exp()\n            probs = unsort_batch(probs, perm_idx)\n        return probs\n\n    def predict(self, batch_dict: Dict[str, TensorDef], raw: bool = False, dense: bool = False, **kwargs):\n        probs = self.predict_batch(batch_dict, **kwargs)\n        if raw and not dense:\n            logger.warning(\n                ""Warning: `raw` parameter is deprecated pass `dense=True` to get back values as a single tensor"")\n\n            dense = True\n        if dense:\n            return probs\n        results = []\n        batchsz = probs.size(0)\n        for b in range(batchsz):\n            outcomes = [(self.labels[id_i], prob_i) for id_i, prob_i in enumerate(probs[b])]\n            results.append(outcomes)\n        return results\n\n    def get_labels(self) -> List[str]:\n        return self.labels\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        """"""This method defines the model itself, and must be overloaded by derived classes\n\n        This function will update `self` with the layers required to execute the `call()` method\n\n        :param embeddings: The input feature indices\n        :param kwargs:\n        :return:\n        """"""\n\n\nclass EmbedPoolStackClassifier(ClassifierModelBase):\n\n    """"""Provides a simple but effective base for most `ClassifierModel`s\n\n   This class provides a common base for classifiers by identifying and codifying\n   and idiomatic pattern where a typical classifier may be though of as a composition\n   between a stack of embeddings, followed by a pooling operation yielding a fixed length\n   tensor, followed by one or more dense layers, and ultimately, a projection to the output space.\n\n   To provide an useful interface to sub-classes, we override the `create_layers` to provide a hook\n   for each layer identified in this idiom, and leave the implementations up to the sub-class.\n\n   We also fully implement the `forward` method.\n\n   """"""\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.pool_model = self.init_pool(self.embeddings.output_dim, **kwargs)\n        self.stack_model = self.init_stacked(self.pool_model.output_dim, **kwargs)\n        self.output_layer = self.init_output(self.stack_model.output_dim, **kwargs)\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n\n        :Keyword Arguments: See below\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n        * *embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        reduction = kwargs.get(\'embeddings_reduction\', \'concat\')\n        embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n        return EmbeddingsStack(embeddings, embeddings_dropout, reduction=reduction)\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce a pooling operation that will be used in the model\n\n        :param input_dim: The input dimension size\n        :param kwargs:\n        :return: A pooling operation\n        """"""\n\n    def init_stacked(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce a stacking operation that will be used in the model\n\n        :param input_dim: The input dimension size\n        :param kwargs:\n        :return: A stacking operation (or None)\n        """"""\n        hszs = listify(kwargs.get(\'hsz\', []))\n        if not hszs:\n            return PassThru(input_dim)\n        return DenseStack(input_dim, hszs, pdrop_value=self.pdrop)\n\n    def init_output(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce the final output layer in the model\n\n        :param input_dim: The input hidden size\n        :param kwargs:\n        :return:\n        """"""\n        return Dense(input_dim, len(self.labels), activation=kwargs.get(\'output_activation\', \'log_softmax\'))\n\n    def forward(self, inputs: Dict[str, TensorDef]) -> TensorDef:\n        """"""Forward execution of the model.  Sub-classes typically shouldnt need to override\n\n        :param inputs: An input dictionary containing the features and the primary key length\n        :return: A tensor\n        """"""\n        lengths = inputs.get(""lengths"")\n        embedded = self.embeddings(inputs)\n        embedded = (embedded, lengths)\n        pooled = self.pool_model(embedded)\n        stacked = self.stack_model(pooled)\n        return self.output_layer(stacked)\n\n\n@register_model(task=\'classify\', name=\'default\')\nclass ConvModel(EmbedPoolStackClassifier):\n    """"""Current default model for `baseline` classification.  Parallel convolutions of varying receptive field width\n    """"""\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Do parallel convolutional filtering with varied receptive field widths, followed by max-over-time pooling\n\n        :param input_dim: Embedding output size\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *cmotsz* -- (``int``) The number of convolutional feature maps for each filter\n            These are MOT-filtered, leaving this # of units per parallel filter\n        * *filtsz* -- (``list``) This is a list of filter widths to use\n\n        :return: A pooling layer\n        """"""\n        cmotsz = kwargs[\'cmotsz\']\n        filtsz = kwargs[\'filtsz\']\n        return WithoutLength(WithDropout(ParallelConv(input_dim, cmotsz, filtsz, ""relu"", input_fmt=""bth""), self.pdrop))\n\n\n@register_model(task=\'classify\', name=\'lstm\')\nclass LSTMModel(EmbedPoolStackClassifier):\n    """"""A simple single-directional single-layer LSTM. No layer-stacking.\n    """"""\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""LSTM with dropout yielding a final-state as output\n\n        :param input_dim: The input word embedding depth\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *rnnsz* -- (``int``) The number of hidden units (defaults to `hsz`)\n        * *rnntype/rnn_type* -- (``str``) The RNN type, defaults to `lstm`, other valid values: `blstm`\n        * *hsz* -- (``int``) backoff for `rnnsz`, typically a result of stacking params.  This keeps things simple so\n          its easy to do things like residual connections between LSTM and post-LSTM stacking layers\n\n        :return: A pooling layer\n        """"""\n        unif = kwargs.get(\'unif\')\n        hsz = kwargs.get(\'rnnsz\', kwargs.get(\'hsz\', 100))\n        if type(hsz) is list:\n            hsz = hsz[0]\n        weight_init = kwargs.get(\'weight_init\', \'uniform\')\n        rnntype = kwargs.get(\'rnn_type\', kwargs.get(\'rnntype\', \'lstm\'))\n        if rnntype == \'blstm\':\n            return BiLSTMEncoderHidden(input_dim, hsz, 1, self.pdrop, unif=unif, batch_first=True, initializer=weight_init)\n        return LSTMEncoderHidden(input_dim, hsz, 1, self.pdrop, unif=unif, batch_first=True, initializer=weight_init)\n\n\nclass NBowModelBase(EmbedPoolStackClassifier):\n    """"""Neural Bag-of-Words Model base class.  Defines stacking of fully-connected layers, but leaves pooling to derived\n    """"""\n\n    def init_stacked(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce a stacking operation that will be used in the model, defaulting to a single layer\n\n        :param input_dim: The input dimension size\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *hsz* -- (``List[int]``) The number of hidden units (defaults to 100)\n        """"""\n        kwargs.setdefault(\'hsz\', [100])\n        return super().init_stacked(input_dim, **kwargs)\n\n\n@register_model(task=\'classify\', name=\'nbow\')\nclass NBowModel(NBowModelBase):\n    """"""Neural Bag-of-Words average pooling (standard) model""""""\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Do average pooling on input embeddings, yielding a `dsz` output layer\n\n        :param input_dim: The word embedding depth\n        :param kwargs: None\n        :return: The average pooling representation\n        """"""\n        return MeanPool1D(input_dim)\n\n\n@register_model(task=\'classify\', name=\'nbowmax\')\nclass NBowMaxModel(NBowModelBase):\n    """"""Max-pooling model for Neural Bag-of-Words.  Sometimes does better than avg pooling\n    """"""\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Do max pooling on input embeddings, yielding a `dsz` output layer\n\n        :param input_dim: The word embedding depth\n        :param kwargs: None\n        :return: The max pooling representation\n        """"""\n        return MaxPool1D(input_dim)\n\n\n@register_model(task=\'classify\', name=\'fine-tune\')\nclass FineTuneModelClassifier(ClassifierModelBase):\n    """"""Fine-tune based on pre-pooled representations""""""\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n\n        :Keyword Arguments: See below\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n        * *embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        reduction = kwargs.get(\'embeddings_reduction\', \'concat\')\n        embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n        return EmbeddingsStack(embeddings, embeddings_dropout, reduction=reduction)\n\n    def init_stacked(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce a stacking operation that will be used in the model\n\n        :param input_dim: The input dimension size\n        :param kwargs:\n        :return: A stacking operation (or None)\n        """"""\n        hszs = listify(kwargs.get(\'hsz\', []))\n        if not hszs:\n            return PassThru(input_dim)\n        return DenseStack(input_dim, hszs, pdrop_value=self.pdrop)\n\n    def init_output(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce the final output layer in the model\n\n        :param input_dim: The input hidden size\n        :param kwargs:\n        :return:\n        """"""\n        return Dense(input_dim, len(self.labels), activation=kwargs.get(\'output_activation\', \'log_softmax\'))\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.stack_model = self.init_stacked(self.embeddings.output_dim, **kwargs)\n        self.output_layer = self.init_output(self.stack_model.output_dim, **kwargs)\n\n    def forward(self, inputs):\n        base_layers = self.embeddings(inputs)\n        stacked = self.stack_model(base_layers)\n        return self.output_layer(stacked)\n\n\n@register_model(task=\'classify\', name=\'composite\')\nclass CompositePoolingModel(ClassifierModelBase):\n    """"""Fulfills pooling contract by aggregating pooling from a set of sub-models and concatenates each""""""\n\n    def init_pool(self, dsz, **kwargs):\n        SubModels = [eval(model) for model in kwargs.get(\'sub\')]\n        sub_models = [SM.init_pool(self, dsz, **kwargs) for SM in SubModels]\n        return CompositePooling(sub_models)\n\n    def make_input(self, batch_dict):\n        """"""Because the sub-model could contain an LSTM, make sure to sort lengths descending\n\n        :param batch_dict:\n        :return:\n        """"""\n        inputs = super().make_input(batch_dict)\n        lengths = inputs[\'lengths\']\n        lengths, perm_idx = lengths.sort(0, descending=True)\n        for k, value in inputs.items():\n            inputs[k] = value[perm_idx]\n        return inputs\n\n'"
baseline/pytorch/classify/train.py,6,"b'import six\nimport logging\nimport torch\nimport torch.autograd\nimport os\n\nfrom eight_mile.confusion import ConfusionMatrix\nfrom eight_mile.utils import listify\nfrom eight_mile.pytorch.optz import OptimizerManager\n\nfrom baseline.progress import create_progress_bar\nfrom baseline.utils import verbose_output, get_model_file, get_metric_cmp\nfrom baseline.train import EpochReportingTrainer, create_trainer, register_trainer, register_training_func\nfrom baseline.model import create_model_for\nfrom torch.utils.data import DataLoader\nlogger = logging.getLogger(\'baseline\')\n\n\ndef _add_to_cm(cm, y, pred):\n    _, best = pred.max(1)\n    yt = y.cpu().int()\n    yp = best.cpu().int()\n    cm.add_batch(yt.data.numpy(), yp.data.numpy())\n\n\n@register_trainer(task=\'classify\', name=\'default\')\nclass ClassifyTrainerPyTorch(EpochReportingTrainer):\n\n    def __init__(self, model, **kwargs):\n\n        if type(model) is dict:\n            model = create_model_for(\'classify\', **model)\n        super(ClassifyTrainerPyTorch, self).__init__()\n        if type(model) is dict:\n            model = create_model_for(\'classify\', **model)\n        self.clip = float(kwargs.get(\'clip\', 5))\n        self.labels = model.labels\n        self.gpus = int(kwargs.get(\'gpus\', 1))\n        if self.gpus == -1:\n            self.gpus = len(os.getenv(\'CUDA_VISIBLE_DEVICES\', os.getenv(\'NV_GPU\', \'0\')).split(\',\'))\n\n        self.optimizer = OptimizerManager(model, **kwargs)\n        self.model = model\n        if self.gpus > 0 and self.model.gpu:\n            self.crit = model.create_loss().cuda()\n            if self.gpus > 1:\n                self.model = torch.nn.DataParallel(model).cuda()\n            else:\n                self.model.cuda()\n        else:\n            logger.warning(""Requested training on CPU.  This will be slow."")\n            self.crit = model.create_loss()\n            self.model = model\n        self.nsteps = kwargs.get(\'nsteps\', six.MAXSIZE)\n\n    def _get_pytorch_model(self):\n        return self.model.module if self.gpus > 1 else self.model\n\n    def save(self, model_file):\n        self._get_pytorch_model().save(model_file)\n\n    def _make_input(self, batch_dict, **kwargs):\n        return self._get_pytorch_model().make_input(batch_dict, **kwargs)\n\n    @staticmethod\n    def _get_batchsz(batch_dict):\n        return len(batch_dict[\'y\'])\n\n    def _test(self, loader, **kwargs):\n        self.model.eval()\n        total_loss = 0\n        total_norm = 0\n        steps = len(loader)\n        pg = create_progress_bar(steps)\n        cm = ConfusionMatrix(self.labels)\n        verbose = kwargs.get(""verbose"", None)\n        output = kwargs.get(\'output\')\n        txts = kwargs.get(\'txts\')\n        handle = None\n        line_number = 0\n        if output is not None and txts is not None:\n            handle = open(output, ""w"")\n\n        for batch_dict in pg(loader):\n            example = self._make_input(batch_dict)\n            ys = example.pop(\'y\')\n            pred = self.model(example)\n            loss = self.crit(pred, ys)\n            if handle is not None:\n                for p, y in zip(pred, ys):\n                    handle.write(\'{}\\t{}\\t{}\\n\'.format("" "".join(txts[line_number]), self.model.labels[p], self.model.labels[y]))\n                    line_number += 1\n            batchsz = self._get_batchsz(batch_dict)\n            total_loss += loss.item() * batchsz\n            total_norm += batchsz\n            _add_to_cm(cm, ys, pred)\n\n        metrics = cm.get_all_metrics()\n        metrics[\'avg_loss\'] = total_loss / float(total_norm)\n        verbose_output(verbose, cm)\n        if handle is not None:\n            handle.close()\n\n        return metrics\n\n    def _train(self, loader, **kwargs):\n        self.model.train()\n        reporting_fns = kwargs.get(\'reporting_fns\', [])\n        steps = len(loader)\n        pg = create_progress_bar(steps)\n        cm = ConfusionMatrix(self.labels)\n        epoch_loss = 0\n        epoch_div = 0\n        for batch_dict in pg(loader):\n            self.optimizer.zero_grad()\n            example = self._make_input(batch_dict)\n            y = example.pop(\'y\')\n            pred = self.model(example)\n            loss = self.crit(pred, y)\n            batchsz = self._get_batchsz(batch_dict)\n            report_loss = loss.item() * batchsz\n            epoch_loss += report_loss\n            epoch_div += batchsz\n            self.nstep_agg += report_loss\n            self.nstep_div += batchsz\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n            _add_to_cm(cm, y, pred)\n            self.optimizer.step()\n\n            if (self.optimizer.global_step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    self.optimizer.global_step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = cm.get_all_metrics()\n        metrics[\'avg_loss\'] = epoch_loss / float(epoch_div)\n        return metrics\n\n\n@register_training_func(\'classify\')\ndef fit(model_params, ts, vs, es, **kwargs):\n    """"""\n    Train a classifier using PyTorch\n    :param model_params: The model to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs: See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) -- Stop after eval data is not improving. Default to True\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *optim* --\n           Optimizer to use, defaults to `sgd`\n        * *eta, lr* (``float``) --\n           Learning rate, defaults to 0.01\n        * *mom* (``float``) --\n           Momentum (SGD only), defaults to 0.9 if optim is `sgd`\n    :return:\n    """"""\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = kwargs.get(\'verbose\', {\'console\': kwargs.get(\'verbose_console\', False), \'file\': kwargs.get(\'verbose_file\', None)})\n    epochs = int(kwargs.get(\'epochs\', 20))\n    model_file = get_model_file(\'classify\', \'pytorch\', kwargs.get(\'basedir\'))\n    output = kwargs.get(\'output\')\n    txts = kwargs.get(\'txts\')\n\n    num_loader_workers = int(kwargs.get(\'num_loader_workers\', 0))\n    pin_memory = bool(kwargs.get(\'pin_memory\', True))\n    ts = DataLoader(ts, num_workers=num_loader_workers, batch_size=None, pin_memory=pin_memory)\n    vs = DataLoader(vs, batch_size=None, pin_memory=pin_memory)\n    es = DataLoader(es, batch_size=None, pin_memory=pin_memory) if es is not None else None\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        logger.info(\'Doing early stopping on [%s] with patience [%d]\', early_stopping_metric, patience)\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    logger.info(\'reporting %s\', reporting_fns)\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n\n    for epoch in range(epochs):\n        trainer.train(ts, reporting_fns)\n        test_metrics = trainer.test(vs, reporting_fns)\n\n        if do_early_stopping is False:\n            trainer.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            logger.info(\'New best %.3f\', best_metric)\n            trainer.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            logger.info(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        logger.info(\'Best performance on %s: %.3f at epoch %d\', early_stopping_metric, best_metric, last_improved)\n\n    if es is not None:\n        logger.info(\'Reloading best checkpoint\')\n        model = torch.load(model_file)\n        trainer = create_trainer(model, **kwargs)\n        test_metrics = trainer.test(es, reporting_fns, phase=\'Test\', verbose=verbose, output=output, txts=txts)\n    return test_metrics\n'"
baseline/pytorch/lm/__init__.py,2,b'from baseline.pytorch.lm.model import *\nfrom baseline.pytorch.lm.train import *'
baseline/pytorch/lm/model.py,10,"b'from baseline.pytorch.torchy import *\nfrom baseline.pytorch.transformer import TransformerEncoderStack, subsequent_mask, MultiHeadedAttention\nfrom baseline.model import LanguageModel, register_model\nimport torch.autograd\nimport os\n\n\nclass LanguageModelBase(nn.Module, LanguageModel):\n    def __init__(self):\n        super().__init__()\n\n    def save(self, outname):\n        torch.save(self, outname)\n        basename, _ = os.path.splitext(outname)\n\n    def create_loss(self):\n        return SequenceCriterion(LossFn=nn.CrossEntropyLoss)\n\n    @classmethod\n    def load(cls, filename, **kwargs):\n        device = kwargs.get(\'device\')\n        if not os.path.exists(filename):\n            filename += \'.pyt\'\n        model = torch.load(filename, map_location=device)\n        model.gpu = False if device == \'cpu\' else model.gpu\n        return model\n\n    def zero_state(self, batchsz):\n        return None\n\n    @property\n    def requires_state(self):\n        pass\n\n    def make_input(self, batch_dict, numpy_to_tensor=False):\n        example_dict = dict({})\n        for key in self.src_keys:\n\n            tensor = batch_dict[key]\n            if numpy_to_tensor:\n                tensor = torch.from_numpy(tensor)\n\n            if self.gpu:\n                tensor = tensor.cuda()\n\n            example_dict[key] = tensor\n\n        y = batch_dict.get(\'y\')\n        if y is not None:\n            if numpy_to_tensor:\n                y = torch.from_numpy(y)\n            if self.gpu:\n                y = y.cuda()\n            example_dict[\'y\'] = y\n        return example_dict\n\n    @classmethod\n    def create(cls, embeddings, **kwargs):\n\n        lm = cls()\n        lm.gpu = kwargs.get(\'gpu\', True)\n        lm.tgt_key = kwargs.get(\'tgt_key\')\n        if lm.tgt_key is None:\n            raise Exception(\'Need a `tgt_key` to know which source vocabulary should be used for destination \')\n\n        lm.src_keys = kwargs.get(\'src_keys\', embeddings.keys())\n        lm.create_layers(embeddings, **kwargs)\n        return lm\n\n    def create_layers(self, embeddings, **kwargs):\n        """"""This method defines the model itself, and must be overloaded by derived classes\n\n        This function will update `self` with the layers required to execute the `call()` method\n\n        :param embeddings: The input feature indices\n        :param kwargs:\n        :return:\n        """"""\n\n    def predict(self, batch_dict, **kwargs):\n        numpy_to_tensor = bool(kwargs.get(\'numpy_to_tensor\', True))\n        batch_dict = self.make_input(batch_dict, numpy_to_tensor=numpy_to_tensor)\n        hidden = batch_dict.get(\'h\')\n        step_softmax, _ = self(batch_dict, hidden)\n        return F.softmax(step_softmax, dim=-1)\n\n\nclass AbstractGeneratorLanguageModel(LanguageModelBase):\n\n    def create_layers(self, embeddings, **kwargs):\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.embeddings_proj = self.init_embeddings_proj(**kwargs)\n        self.generator = self.init_generate(**kwargs)\n        self.output_layer = self.init_output(embeddings, **kwargs)\n\n    def forward(self, input: Dict[str, TensorDef], hidden: TensorDef) -> Tuple[TensorDef, TensorDef]:\n        emb = self.embed(input)\n        output, hidden = self.generate(emb, hidden)\n        return self.output_layer(output), hidden\n\n    def embed(self, input):\n        embedded_dropout = self.embeddings(input)\n        return self.embeddings_proj(embedded_dropout)\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n\n        :Keyword Arguments: See below\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n        * *embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        reduction = kwargs.get(\'embeddings_reduction\', \'concat\')\n        embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n        return EmbeddingsStack({k: embeddings[k] for k in self.src_keys}, embeddings_dropout, reduction=reduction)\n\n    def init_embeddings_proj(self, **kwargs):\n        input_sz = self.embeddings.output_dim\n        hsz = kwargs.get(\'hsz\', kwargs.get(\'d_model\'))\n        if hsz != input_sz:\n            proj = pytorch_linear(input_sz, hsz)\n            print(\'Applying a transform from {} to {}\'.format(input_sz, hsz))\n        else:\n            proj = nn.Identity()\n        return proj\n\n    def init_generate(self, **kwargs):\n        pass\n\n    def generate(self, emb, hidden):\n        return self.generator((emb, hidden))\n\n    def init_output(self, embeddings, **kwargs):\n        self.vsz = embeddings[self.tgt_key].get_vsz()\n        hsz = kwargs.get(\'hsz\', kwargs.get(\'d_model\'))\n        unif = float(kwargs.get(\'unif\', 0.0))\n        do_weight_tying = bool(kwargs.get(\'tie_weights\', False))\n        output_bias = kwargs.get(\'output_bias\', False)\n        if do_weight_tying:\n            output = WeightTieDense(embeddings[self.tgt_key], output_bias)\n        else:\n            output = pytorch_linear(hsz, self.vsz, unif)\n        return output\n\n\n@register_model(task=\'lm\', name=\'default\')\nclass RNNLanguageModel(AbstractGeneratorLanguageModel):\n\n    def __init__(self):\n        super().__init__()\n\n    def zero_state(self, batchsz):\n        weight = next(self.parameters()).data\n        return (torch.autograd.Variable(weight.new(self.num_layers, batchsz, self.hsz).zero_()),\n                torch.autograd.Variable(weight.new(self.num_layers, batchsz, self.hsz).zero_()))\n\n    @property\n    def requires_state(self):\n        True\n\n    def init_generate(self, **kwargs):\n        pdrop = float(kwargs.get(\'dropout\', 0.5))\n        self.num_layers = kwargs.get(\'layers\', kwargs.get(\'num_layers\', 1))\n        self.hsz = kwargs.get(\'hsz\', kwargs.get(\'d_model\'))\n        return WithDropoutOnFirst(LSTMEncoderWithState(self.hsz, self.hsz, self.num_layers, pdrop, batch_first=True),\n                                  pdrop,\n                                  kwargs.get(\'variational\', False))\n\n\n@register_model(task=\'lm\', name=\'transformer\')\nclass TransformerLanguageModel(AbstractGeneratorLanguageModel):\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def requires_state(self):\n        False\n\n    def init_layer_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding, nn.LayerNorm)):\n            module.weight.data.normal_(mean=0.0, std=self.weight_std)\n        if isinstance(module, (nn.Linear, nn.LayerNorm)) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def init_generate(self, **kwargs):\n        pdrop = float(kwargs.get(\'dropout\', 0.1))\n        layers = kwargs.get(\'layers\', kwargs.get(\'num_layers\', 1))\n        d_model = int(kwargs.get(\'d_model\', kwargs.get(\'hsz\')))\n        num_heads = kwargs.get(\'num_heads\', 4)\n        d_ff = int(kwargs.get(\'d_ff\', 4 * d_model))\n        rpr_k = kwargs.get(\'rpr_k\')\n        d_k = kwargs.get(\'d_k\')\n        scale = bool(kwargs.get(\'scale\', True))\n        activation = kwargs.get(\'activation\', \'gelu\')\n        layer_norm_eps = kwargs.get(\'layer_norm_eps\', 1e-12)\n        layer_norms_after = kwargs.get(\'layer_norms_after\', False)\n        return TransformerEncoderStack(num_heads, d_model=d_model, pdrop=pdrop, scale=scale,\n                                       layers=layers, d_ff=d_ff, rpr_k=rpr_k, d_k=d_k,\n                                       activation=activation, layer_norm_eps=layer_norm_eps, layer_norms_after=layer_norms_after)\n\n    def create_layers(self, embeddings, **kwargs):\n        super().create_layers(embeddings, **kwargs)\n        self.weight_std = kwargs.get(\'weight_std\', 0.02)\n        self.apply(self.init_layer_weights)\n\n    def create_mask(self, bth):\n        T = bth.shape[1]\n        mask = subsequent_mask(T).type_as(bth)\n        return mask\n\n    def generate(self, bth, _):\n        mask = self.create_mask(bth)\n        return self.generator((bth, mask)), None\n\n\n@register_model(task=\'lm\', name=\'transformer-mlm\')\nclass TransformerMaskedLanguageModel(TransformerLanguageModel):\n\n    def create_mask(self, bth):\n        T = bth.shape[1]\n        mask = torch.ones((1, 1, T, T)).type_as(bth)\n        return mask\n'"
baseline/pytorch/lm/train.py,7,"b'import time\nimport logging\nfrom baseline.pytorch.torchy import *\nfrom eight_mile.utils import listify, revlut\nfrom eight_mile.pytorch.optz import OptimizerManager\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.train import Trainer, create_trainer, register_trainer, register_training_func\nfrom baseline.model import create_model_for\nfrom torch.utils.data import DataLoader\nlogger = logging.getLogger(\'baseline\')\n\n\n@register_trainer(task=\'lm\', name=\'default\')\nclass LanguageModelTrainerPyTorch(Trainer):\n\n    def __init__(self, model, **kwargs):\n        super().__init__()\n        if type(model) is dict:\n            model = create_model_for(\'lm\', **model)\n        self.model = model\n        self.clip = float(kwargs.get(\'clip\', 5))\n        self.gpus = kwargs.get(\'gpus\', 1)\n        if self.gpus > 0:\n            self.crit = model.create_loss().cuda()\n            if self.gpus > 1:\n                self.model = torch.nn.DataParallel(model).cuda()\n            else:\n                self.model.cuda()\n        else:\n            logger.warning(""Requested training on CPU.  This will be slow."")\n            self.crit = model.create_loss()\n\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n        self.optimizer = OptimizerManager(self.model, **kwargs)\n\n    def repackage_hidden(self, h):\n        """"""Wraps hidden states in new Variables, to detach them from their history.""""""\n        if isinstance(h, torch.Tensor):\n            return h.detach()\n        else:\n            return tuple(self.repackage_hidden(v) for v in h)\n\n    def save(self, model_file):\n        self._get_pytorch_model().save(model_file)\n\n    def _get_pytorch_model(self):\n        return self.model.module if self.gpus > 1 else self.model\n\n    @staticmethod\n    def _get_dims(loader):\n        batch_dict = loader.dataset[0]\n        return batch_dict[\'y\'].shape\n\n    @staticmethod\n    def _num_toks(batch_dict):\n        return np.prod(batch_dict[\'y\'].shape)\n\n    def calc_metrics(self, agg, norm):\n        metrics = super().calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\'])\n        return metrics\n\n    def test(self, vs, reporting_fns, phase=\'Valid\', **kwargs):\n        epoch = 0\n        if phase == \'Valid\':\n            self.valid_epochs += 1\n            epoch = self.valid_epochs\n        start = time.time()\n        self.model.eval()\n        total_loss = 0\n        total_toks = 0\n        batchsz, nctx = self._get_dims(vs)\n        hidden = self._get_pytorch_model().zero_state(batchsz)\n\n        for batch_dict in vs:\n            inputs = self._get_pytorch_model().make_input(batch_dict)\n            y = inputs.pop(\'y\')\n            output, hidden = self.model(inputs, hidden)\n            toks = self._num_toks(batch_dict)\n            total_loss += self.crit(output, y).item() * toks\n            total_toks += toks\n            if hidden is not None:\n                hidden = self.repackage_hidden(hidden)\n        metrics = self.calc_metrics(total_loss, total_toks)\n        self.report(\n            epoch, metrics, start,\n            phase, \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def train(self, ts, reporting_fns):\n        start = time.time()\n        self.nstep_start = start\n        self.model.train()\n        epoch_loss = 0\n        epoch_toks = 0\n        batchsz, nctx = self._get_dims(ts)\n        hidden = self._get_pytorch_model().zero_state(batchsz)\n\n        for batch_dict in ts:\n            if hidden is not None:\n                hidden = self.repackage_hidden(hidden)\n            inputs = self._get_pytorch_model().make_input(batch_dict)\n            y = inputs.pop(\'y\')\n            self.optimizer.zero_grad()\n            output, hidden = self.model(inputs, hidden)\n            loss = self.crit(output, y)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n            self.optimizer.step()\n            toks = self._num_toks(batch_dict)\n            report_loss = loss.item() * toks\n            epoch_loss += report_loss\n            epoch_toks += toks\n            self.nstep_agg += report_loss\n            self.nstep_div += toks\n            if (self.optimizer.global_step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    self.optimizer.global_step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = self.calc_metrics(epoch_loss, epoch_toks)\n        self.train_epochs += 1\n        self.report(\n            self.train_epochs, metrics, start,\n            \'Train\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n\n@register_training_func(\'lm\')\ndef fit(model, ts, vs, es, **kwargs):\n    epochs = int(kwargs[\'epochs\']) if \'epochs\' in kwargs else 5\n    patience = int(kwargs[\'patience\']) if \'patience\' in kwargs else epochs\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    model_file = get_model_file(\'lm\', \'pytorch\', kwargs.get(\'basedir\'))\n\n    num_loader_workers = int(kwargs.get(\'num_loader_workers\', 0))\n    pin_memory = bool(kwargs.get(\'pin_memory\', True))\n    ts = DataLoader(ts, num_workers=num_loader_workers, batch_size=None, pin_memory=pin_memory)\n    vs = DataLoader(vs, batch_size=None, pin_memory=pin_memory)\n    es = DataLoader(es, batch_size=None, pin_memory=pin_memory) if es is not None else None\n\n    best_metric = 10000\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'avg_loss\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        logger.info(\'Doing early stopping on [%s] with patience [%d]\', early_stopping_metric, patience)\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    logger.info(\'reporting %s\', reporting_fns)\n\n    after_train_fn = kwargs.get(\'after_train_fn\', None)\n    trainer = create_trainer(model, **kwargs)\n    last_improved = 0\n    for epoch in range(epochs):\n\n        trainer.train(ts, reporting_fns)\n        if after_train_fn is not None:\n            after_train_fn(model)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            logger.info(\'New best %.3f\', best_metric)\n            trainer.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            logger.info(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        logger.info(\'Best performance on %s: %.3f at epoch %d\', early_stopping_metric, best_metric, last_improved)\n\n    if es is not None:\n        logger.info(\'Reloading best checkpoint\')\n        model = torch.load(model_file)\n        trainer = create_trainer(model, **kwargs)\n        test_metrics = trainer.test(es, reporting_fns, phase=\'Test\')\n    return test_metrics\n'"
baseline/pytorch/seq2seq/__init__.py,4,b'from baseline.pytorch.seq2seq.model import *\nfrom baseline.pytorch.seq2seq.train import *\nfrom baseline.pytorch.seq2seq.encoders import *\nfrom baseline.pytorch.seq2seq.decoders import *\n'
baseline/pytorch/seq2seq/decoders.py,15,"b'import math\nfrom functools import partial\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.autograd import Variable\nfrom baseline.utils import Offsets, exporter\nfrom eight_mile.pytorch.layers import repeat_batch, gnmt_length_penalty, BeamSearchBase, rnn_cell, WeightTieDense\nfrom baseline.pytorch.transformer import subsequent_mask, TransformerDecoderStack\nfrom baseline.model import register_arc_policy, register_decoder, create_seq2seq_arc_policy\nfrom baseline.pytorch.seq2seq.encoders import TransformerEncoderOutput\nfrom baseline.pytorch.torchy import (\n    tie_weight,\n    pytorch_linear,\n    LuongDotProductAttention,\n    BahdanauAttention,\n    ScaledDotProductAttention,\n    LuongGeneralAttention,\n)\n\n__all__ = []\nexport = exporter(__all__)\n\n\nclass ArcPolicy(torch.nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, encoder_outputs, hsz, beam_width=1):\n        pass\n\n\nclass AbstractArcPolicy(ArcPolicy):\n\n    def __init__(self):\n        super().__init__()\n\n    def get_state(self, encoder_outputs):\n        pass\n\n    def forward(self, encoder_output, hsz, beam_width=1):\n        h_i = self.get_state(encoder_output)\n        context = encoder_output.output\n        if beam_width > 1:\n            with torch.no_grad():\n                context = repeat_batch(context, beam_width)\n                if type(h_i) is tuple:\n                    h_i = repeat_batch(h_i[0], beam_width, dim=1), repeat_batch(h_i[1], beam_width, dim=1)\n                else:\n                    h_i = repeat_batch(h_i, beam_width, dim=1)\n        batch_size = context.shape[0]\n        h_size = (batch_size, hsz)\n        with torch.no_grad():\n            init_zeros = context.data.new(*h_size).zero_()\n        return h_i, init_zeros, context\n\n\n@register_arc_policy(name=\'default\')\nclass TransferLastHiddenPolicy(AbstractArcPolicy):\n\n    def __init__(self):\n        super().__init__()\n\n    def get_state(self, encoder_outputs):\n        return encoder_outputs.hidden\n\n\n@register_arc_policy(name=\'no_arc\')\nclass NoArcPolicy(AbstractArcPolicy):\n\n    def __init__(self):\n        super().__init__()\n\n    def get_state(self, encoder_outputs):\n        final_encoder_state = encoder_outputs.hidden\n        if isinstance(final_encoder_state, tuple):\n            s1, s2 = final_encoder_state\n            return s1 * 0, s2 * 0\n        return final_encoder_state * 0\n\n\n@register_decoder(name=\'vanilla\')\nclass RNNDecoder(torch.nn.Module):\n\n    def __init__(self, tgt_embeddings, **kwargs):\n        """"""Construct an RNN decoder.  It provides the input size, the rest is up to the impl.\n\n        The default implementation provides an RNN cell, followed by a linear projection, out to a softmax\n\n        :param input_dim: The input size\n        :param kwargs:\n        :return: void\n        """"""\n        super().__init__()\n        self.hsz = kwargs[\'hsz\']\n        self.arc_policy = create_seq2seq_arc_policy(**kwargs)\n        self.tgt_embeddings = tgt_embeddings\n        rnntype = kwargs.get(\'rnntype\', \'lstm\')\n        layers = kwargs.get(\'layers\', 1)\n        feed_input = kwargs.get(\'feed_input\', True)\n        dsz = tgt_embeddings.get_dsz()\n        if feed_input:\n            self.input_i = self._feed_input\n            dsz += self.hsz\n        else:\n            self.input_i = self._basic_input\n        pdrop = kwargs.get(\'dropout\', 0.5)\n        self.decoder_rnn = rnn_cell(dsz, self.hsz, rnntype, layers, pdrop)\n        self.dropout = torch.nn.Dropout(pdrop)\n        self.init_attn(**kwargs)\n\n        do_weight_tying = bool(kwargs.get(\'tie_weights\', True))\n\n        if do_weight_tying:\n            if self.hsz != self.tgt_embeddings.get_dsz():\n                raise ValueError(""weight tying requires hsz == embedding dsz, got {} hsz and {} dsz"".format(self.hsz, self.tgt_embeddings.get_dsz()))\n            self.preds = WeightTieDense(self.tgt_embeddings)\n        else:\n            self.preds = pytorch_linear(self.hsz, self.tgt_embeddings.get_vsz())\n\n    @staticmethod\n    def _basic_input(dst_embed_i, _):\n        """"""\n        In this function the destination embedding is passed directly to into the decoder.  The output of previous H\n        is ignored.  This is implemented using a bound method to a field in the class for speed so that this decision\n        is handled at initialization, not as a conditional in the training or inference\n\n        :param embed_i: The embedding at i\n        :param _: Ignored\n        :return: basic input\n        """"""\n        return dst_embed_i.squeeze(0)\n\n    @staticmethod\n    def _feed_input(embed_i, attn_output_i):\n        """"""\n        In this function the destination embedding is concatenated with the previous attentional output and\n        passed to the decoder. This is implemented using a bound method to a field in the class for speed\n        so that this decision is handled at initialization, not as a conditional in the training or inference\n\n        :param embed_i: The embedding at i\n        :param output_i: This is the last H state\n        :return: an input that is a concatenation of previous state and destination embedding\n        """"""\n        embed_i = embed_i.squeeze(0)\n        return torch.cat([embed_i, attn_output_i], 1)\n\n    def forward(self, encoder_outputs, dst):\n        src_mask = encoder_outputs.src_mask\n        h_i, output_i, context_bth = self.arc_policy(encoder_outputs, self.hsz)\n        output_tbh, _ = self.decode_rnn(context_bth, h_i, output_i, dst.transpose(0, 1), src_mask)\n        pred = self.output(output_tbh)\n        return pred.transpose(0, 1).contiguous()\n\n    def decode_rnn(self, context_bth, h_i, output_i, dst_bth, src_mask):\n        embed_out_bth = self.tgt_embeddings(dst_bth)\n\n        outputs = []\n\n        for i, embed_i in enumerate(embed_out_bth.split(1)):\n            # Input feeding would use previous attentional output in addition to destination embeddings\n            embed_i = self.input_i(embed_i, output_i)\n            output_i, h_i = self.decoder_rnn(embed_i, h_i)\n            output_i = self.attn(output_i, context_bth, src_mask)\n            output_i = self.dropout(output_i)\n            # Attentional outputs\n            outputs.append(output_i)\n\n        outputs_tbh = torch.stack(outputs)\n        return outputs_tbh, h_i\n\n    def attn(self, output_t, context, src_mask=None):\n        return output_t\n\n    def init_attn(self, **kwargs):\n        pass\n\n    def output(self, x):\n        pred = F.log_softmax(self.preds(x.view(x.size(0)*x.size(1), -1)), dim=-1)\n        pred = pred.view(x.size(0), x.size(1), -1)\n        return pred\n\n    class BeamSearch(BeamSearchBase):\n\n        def __init__(self, parent, **kwargs):\n            super().__init__(**kwargs)\n            self.parent = parent\n\n        def init(self, encoder_outputs):\n            """"""Tile batches for encoder inputs and the likes.""""""\n            src_mask = repeat_batch(encoder_outputs.src_mask, self.K)\n            h_i, dec_out, context = self.parent.arc_policy(encoder_outputs, self.parent.hsz, self.K)\n            return h_i, dec_out, context, src_mask\n\n        def step(self, paths, extra):\n            """"""Calculate the probs of the next output and update state.""""""\n            h_i, dec_out, context, src_mask = extra\n            last = paths[:, :, -1].view(1, -1)\n            dec_out, h_i = self.parent.decode_rnn(context, h_i, dec_out, last, src_mask)\n            probs = self.parent.output(dec_out)\n            dec_out = dec_out.squeeze(0)\n            return probs, (h_i, dec_out, context, src_mask)\n\n        def update(self, beams, extra):\n            """"""Select the correct hidden states and outputs to used based on the best performing beams.""""""\n            h_i, dec_out, context, src_mask = extra\n            h_i = tuple(hc[:, beams, :] for hc in h_i)\n            dec_out = dec_out[beams, :]\n            return h_i, dec_out, context, src_mask\n\n    def beam_search(self, encoder_outputs, **kwargs):\n        alpha = kwargs.get(\'alpha\')\n        if alpha is not None:\n            kwargs[\'length_penalty\'] = partial(gnmt_length_penalty, alpha=alpha)\n        return RNNDecoder.BeamSearch(parent=self, **kwargs)(encoder_outputs)\n\n\n@register_decoder(name=\'default\')\nclass RNNDecoderWithAttn(RNNDecoder):\n\n    def __init__(self, tgt_embeddings, **kwargs):\n        super().__init__(tgt_embeddings, **kwargs)\n\n    def init_attn(self, **kwargs):\n        attn_type = kwargs.get(\'attn_type\', \'bahdanau\').lower()\n        if attn_type == \'dot\':\n            self.attn_module = LuongDotProductAttention(self.hsz)\n        elif attn_type == \'concat\' or attn_type == \'bahdanau\':\n            self.attn_module = BahdanauAttention(self.hsz)\n        elif attn_type == \'sdp\':\n            self.attn_module = ScaledDotProductAttention(self.hsz)\n        else:\n            self.attn_module = LuongGeneralAttention(self.hsz)\n\n    def attn(self, output_t, context, src_mask=None):\n        return self.attn_module(output_t, context, context, src_mask)\n\n\n@register_decoder(name=\'transformer\')\nclass TransformerDecoderWrapper(torch.nn.Module):\n\n    def __init__(self, tgt_embeddings, dropout=0.5, layers=1, hsz=None, num_heads=4, scale=True, **kwargs):\n        super().__init__()\n        self.tgt_embeddings = tgt_embeddings\n        dsz = self.tgt_embeddings.get_dsz()\n        if hsz is None:\n            hsz = dsz\n\n        d_ff = int(kwargs.get(\'d_ff\', 4 * hsz))\n        rpr_k = kwargs.get(\'rpr_k\')\n        d_k = kwargs.get(\'d_k\')\n        activation = kwargs.get(\'activation\', \'relu\')\n        scale = bool(kwargs.get(\'scale\', True))\n\n        self.transformer_decoder = TransformerDecoderStack(num_heads, d_model=hsz, d_ff=d_ff,\n                                                           pdrop=dropout, scale=scale,\n                                                           layers=layers, rpr_k=rpr_k, d_k=d_k, activation_type=activation)\n\n        self.proj_to_dsz = self._identity\n        self.proj_to_hsz = self._identity\n        if hsz != dsz:\n            self.proj_to_hsz = pytorch_linear(dsz, hsz)\n            self.proj_to_dsz = pytorch_linear(hsz, dsz)\n            del self.proj_to_dsz.weight\n            self.proj_to_dsz.weight = torch.nn.Parameter(self.proj_to_hsz.weight.transpose(0, 1), requires_grad=True)\n\n        do_weight_tying = bool(kwargs.get(\'tie_weights\', True))\n        if do_weight_tying:\n            if hsz != self.tgt_embeddings.get_dsz():\n                raise ValueError(""weight tying requires hsz == embedding dsz, got {} hsz and {} dsz"".format(self.hsz, self.tgt_embeddings.get_dsz()))\n            self.preds = WeightTieDense(self.tgt_embeddings)\n        else:\n            self.preds = pytorch_linear(dsz, self.tgt_embeddings.get_vsz())\n\n    def _identity(self, x):\n        return x\n\n    def forward(self, encoder_output, dst):\n        embed_out_bth = self.tgt_embeddings(dst)\n        embed_out_bth = self.proj_to_hsz(embed_out_bth)\n        context_bth = encoder_output.output\n        T = embed_out_bth.shape[1]\n        dst_mask = subsequent_mask(T).type_as(embed_out_bth)\n        src_mask = encoder_output.src_mask.unsqueeze(1).unsqueeze(1)\n        output = self.transformer_decoder((embed_out_bth, context_bth, src_mask, dst_mask))\n        output = self.proj_to_dsz(output)\n        prob = self.output(output)\n        return prob\n\n    def output(self, x):\n        pred = F.log_softmax(self.preds(x.view(x.size(0)*x.size(1), -1)), dim=-1)\n        pred = pred.view(x.size(0), x.size(1), -1)\n        return pred\n\n    class BeamSearch(BeamSearchBase):\n\n        def __init__(self, parent, **kwargs):\n            super().__init__(**kwargs)\n            self.parent = parent\n\n        def init(self, encoder_outputs):\n            """"""Tile for the batch of the encoder inputs.""""""\n            encoder_outputs = TransformerEncoderOutput(\n                repeat_batch(encoder_outputs.output, self.K),\n                repeat_batch(encoder_outputs.src_mask, self.K)\n            )\n            return encoder_outputs\n\n        def step(self, paths, extra):\n            """"""Calculate the probs for the last item based on the full path.""""""\n            B, K, T = paths.size()\n            assert K == self.K\n            return self.parent(extra, paths.view(B * K, T))[:, -1], extra\n\n        def update(self, beams, extra):\n            """"""There is no state for the transformer so just pass it.""""""\n            return extra\n\n    def beam_search(self, encoder_outputs, **kwargs):\n        return TransformerDecoderWrapper.BeamSearch(parent=self, **kwargs)(encoder_outputs)\n'"
baseline/pytorch/seq2seq/encoders.py,5,"b'from collections import namedtuple\nfrom baseline.pytorch.torchy import sequence_mask, pytorch_linear, pytorch_lstm\nfrom baseline.pytorch.transformer import TransformerEncoderStack\nfrom baseline.model import register_encoder\nfrom eight_mile.pytorch.layers import *\nimport torch\n\n\nRNNEncoderOutput = namedtuple(""RNNEncoderOutput"", (""output"", ""hidden"", ""src_mask""))\n\n\ndef _make_src_mask(output, lengths):\n    T = output.shape[1]\n    src_mask = sequence_mask(lengths, T).type_as(lengths.data)\n    return src_mask\n\n\n@register_encoder(name=\'default\')\nclass RNNEncoder(torch.nn.Module):\n\n    def __init__(self, dsz=None, hsz=None, rnntype=\'blstm\', layers=1, pdrop=0.5, residual=False, create_src_mask=True, **kwargs):\n        super().__init__()\n        self.residual = residual\n        hidden = hsz if hsz is not None else dsz\n        Encoder = LSTMEncoderAll if rnntype == \'lstm\' else BiLSTMEncoderAll\n        self.rnn = Encoder(dsz, hidden, layers, pdrop, batch_first=True)\n        self.src_mask_fn = _make_src_mask if create_src_mask is True else lambda x, y: None\n\n    def forward(self, btc, lengths):\n        output, hidden = self.rnn((btc, lengths))\n        return RNNEncoderOutput(output=output + btc if self.residual else output,\n                                hidden=hidden,\n                                src_mask=self.src_mask_fn(output, lengths))\n\n\nTransformerEncoderOutput = namedtuple(""TransformerEncoderOutput"", (""output"", ""src_mask""))\n\n\n@register_encoder(name=\'transformer\')\nclass TransformerEncoderWrapper(torch.nn.Module):\n\n    def __init__(self, dsz, hsz=None, num_heads=4, layers=1, dropout=0.5, **kwargs):\n        super().__init__()\n        if hsz is None:\n            hsz = dsz\n        self.proj = pytorch_linear(dsz, hsz) if hsz != dsz else self._identity\n        d_ff = int(kwargs.get(\'d_ff\', 4 * hsz))\n        rpr_k = kwargs.get(\'rpr_k\')\n        d_k = kwargs.get(\'d_k\')\n        activation = kwargs.get(\'activation\', \'relu\')\n        scale = bool(kwargs.get(\'scale\', True))\n        self.transformer = TransformerEncoderStack(num_heads, d_model=hsz, d_ff=d_ff,\n                                                   pdrop=dropout, scale=scale, layers=layers,\n                                                   rpr_k=rpr_k, d_k=d_k, activation=activation)\n\n    def _identity(self, x):\n        return x\n\n    def forward(self, bth, lengths):\n        T = bth.shape[1]\n        src_mask = sequence_mask(lengths, T).type_as(lengths.data)\n        bth = self.proj(bth)\n        output = self.transformer((bth, src_mask.unsqueeze(1).unsqueeze(1)))\n        return TransformerEncoderOutput(output=output, src_mask=src_mask)\n'"
baseline/pytorch/seq2seq/model.py,10,"b'import os\nimport logging\nfrom baseline.utils import write_json\nfrom baseline.pytorch.torchy import *\nfrom baseline.pytorch.transformer import *\nfrom baseline.model import EncoderDecoderModel, register_model, create_seq2seq_encoder, create_seq2seq_decoder\nfrom baseline.pytorch.seq2seq.encoders import *\nfrom baseline.pytorch.seq2seq.decoders import *\n\nlogger = logging.getLogger(\'baseline\')\n\n\nclass EncoderDecoderModelBase(nn.Module, EncoderDecoderModel):\n\n    def __init__(self, src_embeddings, tgt_embedding, **kwargs):\n        super().__init__()\n        self.beam_sz = kwargs.get(\'beam\', 1)\n        self.gpu = kwargs.get(\'gpu\', True)\n        src_dsz = self.init_embed(src_embeddings, tgt_embedding)\n        self.src_lengths_key = kwargs.get(\'src_lengths_key\')\n        self.dropin_values = kwargs.get(\'dropin\', {})\n        self.encoder = self.init_encoder(src_dsz, **kwargs)\n        self.decoder = self.init_decoder(tgt_embedding, **kwargs)\n\n    def init_embed(self, src_embeddings, tgt_embedding, **kwargs):\n        """"""This is the hook for providing embeddings.  It takes in a dictionary of `src_embeddings` and a single\n        tgt_embedding` of type `PyTorchEmbedding`\n\n        :param src_embeddings: (``dict``) A dictionary of PyTorchEmbeddings, one per embedding\n        :param tgt_embedding: (``PyTorchEmbeddings``) A single PyTorchEmbeddings object\n        :param kwargs:\n        :return: Return the aggregate embedding input size\n        """"""\n        self.src_embeddings = EmbeddingsStack(src_embeddings, reduction=kwargs.get(\'embeddings_reduction\', \'concat\'))\n        return self.src_embeddings.output_dim\n\n    def init_encoder(self, input_sz, **kwargs):\n        # This is a hack since TF never needs this one, there is not a general constructor param, so shoehorn\n        kwargs[\'dsz\'] = input_sz\n        return create_seq2seq_encoder(**kwargs)\n\n    def init_decoder(self, tgt_embedding, **kwargs):\n        return create_seq2seq_decoder(tgt_embedding, **kwargs)\n\n    def encode(self, input, lengths):\n        """"""\n\n        :param input:\n        :param lengths:\n        :return:\n        """"""\n        embed_in_seq = self.embed(input)\n        return self.encoder(embed_in_seq, lengths)\n\n    def decode(self, encoder_outputs, dst):\n        return self.decoder(encoder_outputs, dst)\n\n    def save(self, model_file):\n        """"""Save the model out\n\n        :param model_file: (``str``) The filename\n        :return:\n        """"""\n        torch.save(self, model_file)\n\n    def create_loss(self):\n        """"""Create a loss function.\n\n        :return:\n        """"""\n        return SequenceCriterion()\n\n    @classmethod\n    def load(cls, filename, **kwargs):\n        """"""Load a model from file\n\n        :param filename: (``str``) The filename\n        :param kwargs:\n        :return:\n        """"""\n        device = kwargs.get(\'device\')\n        if not os.path.exists(filename):\n            filename += \'.pyt\'\n        model = torch.load(filename, map_location=device)\n        model.gpu = False if device == \'cpu\' else model.gpu\n        return model\n\n    @classmethod\n    def create(cls, src_embeddings, tgt_embedding, **kwargs):\n        model = cls(src_embeddings, tgt_embedding, **kwargs)\n        logger.info(model)\n        return model\n\n    def drop_inputs(self, key, x):\n        v = self.dropin_values.get(key, 0)\n\n        if not self.training or v == 0:\n            return x\n\n        mask_pad = x != Offsets.PAD\n        mask_drop = x.new(x.size(0), x.size(1)).bernoulli_(v).byte()\n        x.masked_fill_(mask_pad & mask_drop, Offsets.UNK)\n        return x\n\n    def input_tensor(self, key, batch_dict, perm_idx, numpy_to_tensor=False):\n        tensor = batch_dict[key]\n        if numpy_to_tensor:\n            tensor = torch.from_numpy(tensor)\n\n        tensor = self.drop_inputs(key, tensor)\n        tensor = tensor[perm_idx]\n        if self.gpu:\n            tensor = tensor.cuda()\n        return tensor\n\n    def make_input(self, batch_dict, perm=False, numpy_to_tensor=False):\n        """"""Prepare the input.\n\n        :param batch_dict: `dict`: The data.\n        :param perm: `bool`: If True return the permutation index\n            so that you can undo the sort if you want.\n        """"""\n        example = dict({})\n\n        lengths = batch_dict[self.src_lengths_key]\n        if numpy_to_tensor:\n            lengths = torch.from_numpy(lengths)\n        lengths, perm_idx = lengths.sort(0, descending=True)\n\n        if self.gpu:\n            lengths = lengths.cuda()\n        example[\'src_len\'] = lengths\n        for key in self.src_embeddings.keys():\n            example[key] = self.input_tensor(key, batch_dict, perm_idx, numpy_to_tensor=numpy_to_tensor)\n\n        if \'tgt\' in batch_dict:\n            tgt = batch_dict[\'tgt\']\n            if numpy_to_tensor:\n                tgt = torch.from_numpy(tgt)\n            example[\'dst\'] = tgt[:, :-1]\n            example[\'tgt\'] = tgt[:, 1:]\n            example[\'dst\'] = example[\'dst\'][perm_idx]\n            example[\'tgt\'] = example[\'tgt\'][perm_idx]\n            if self.gpu:\n                example[\'dst\'] = example[\'dst\'].cuda()\n                example[\'tgt\'] = example[\'tgt\'].cuda()\n        if perm:\n            return example, perm_idx\n        return example\n\n    def embed(self, input):\n        return self.src_embeddings(input)\n\n    def forward(self, input: Dict[str, torch.Tensor]):\n        src_len = input[\'src_len\']\n        encoder_outputs = self.encode(input, src_len)\n        output = self.decode(encoder_outputs, input[\'dst\'])\n        # Return as B x T x H\n        return output\n\n    def predict(self, batch_dict, **kwargs):\n        """"""Predict based on the batch.\n\n        If `make_input` is True then run make_input on the batch_dict.\n        This is false for being used during dev eval where the inputs\n        are already transformed.\n        """"""\n        self.eval()\n        make = kwargs.get(\'make_input\', True)\n        if make:\n            numpy_to_tensor = bool(kwargs.get(\'numpy_to_tensor\', True))\n            inputs, perm_idx = self.make_input(batch_dict, perm=True, numpy_to_tensor=numpy_to_tensor)\n        else:\n            inputs = batch_dict\n        encoder_outputs = self.encode(inputs, inputs[\'src_len\'])\n        outs, lengths, scores = self.decoder.beam_search(encoder_outputs, **kwargs)\n        if make:\n            outs = unsort_batch(outs, perm_idx)\n            lengths = unsort_batch(lengths, perm_idx)\n            scores = unsort_batch(scores, perm_idx)\n        return outs.cpu().numpy()\n\n\n@register_model(task=\'seq2seq\', name=[\'default\', \'attn\'])\nclass Seq2SeqModel(EncoderDecoderModelBase):\n\n    def __init__(self, src_embeddings, tgt_embedding, **kwargs):\n        """"""This base model is extensible for attention and other uses.  It declares minimal fields allowing the\n        subclass to take over most of the duties for drastically different implementations\n\n        :param src_embeddings: (``dict``) A dictionary of PyTorchEmbeddings\n        :param tgt_embedding: (``PyTorchEmbeddings``) A single PyTorchEmbeddings object\n        :param kwargs:\n        """"""\n        super().__init__(src_embeddings, tgt_embedding, **kwargs)\n'"
baseline/pytorch/seq2seq/train.py,6,"b'import time\nimport logging\nimport torch\nimport numpy as np\nfrom baseline.progress import create_progress_bar\nfrom eight_mile.utils import listify\nfrom baseline.utils import get_model_file, get_metric_cmp, convert_seq2seq_golds, convert_seq2seq_preds\nfrom baseline.train import Trainer, create_trainer, register_trainer, register_training_func\nfrom eight_mile.pytorch.optz import OptimizerManager\nfrom eight_mile.bleu import bleu\nfrom baseline.model import create_model_for\nfrom torch.utils.data import DataLoader\n\nlogger = logging.getLogger(\'baseline\')\n\n\n@register_trainer(task=\'seq2seq\', name=\'default\')\nclass Seq2SeqTrainerPyTorch(Trainer):\n\n    def __init__(self, model, **kwargs):\n        super().__init__()\n        if type(model) is dict:\n            model = create_model_for(\'seq2seq\', **model)\n\n        self.clip = float(kwargs.get(\'clip\', 5))\n        self.model = model\n        self.optimizer = OptimizerManager(self.model, **kwargs)\n        self._input = model.make_input\n        self._predict = model.predict\n        self.tgt_rlut = kwargs[\'tgt_rlut\']\n        self.gpus = kwargs.get(\'gpus\', 1)\n        self.bleu_n_grams = int(kwargs.get(""bleu_n_grams"", 4))\n\n        if self.gpus > 0:\n            self.crit = model.create_loss().cuda()\n            if self.gpus > 1:\n                self.model = torch.nn.DataParallel(model).cuda()\n            else:\n                self.model.cuda()\n        else:\n            logger.warning(""Requested training on CPU.  This will be slow."")\n            self.crit = model.create_loss()\n\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n\n    @staticmethod\n    def _num_toks(tgt_lens):\n        return torch.sum(tgt_lens).item()\n\n    def save(self, model_file):\n        self._get_pytorch_model().save(model_file)\n\n    def _get_pytorch_model(self):\n        return self.model.module if self.gpus > 1 else self.model\n\n    def calc_metrics(self, agg, norm):\n        metrics = super(Seq2SeqTrainerPyTorch, self).calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\'])\n        return metrics\n\n    def test(self, vs, reporting_fns, phase, **kwargs):\n        if phase == \'Test\':\n            return self._evaluate(vs, reporting_fns, **kwargs)\n\n        self.model.eval()\n        total_loss = total_toks = 0\n        steps = len(vs)\n        self.valid_epochs += 1\n        preds = []\n        golds = []\n\n        start = time.time()\n        pg = create_progress_bar(steps)\n        for batch_dict in pg(vs):\n            input_ = self._input(batch_dict)\n            tgt = input_[\'tgt\']\n            tgt_lens = batch_dict[\'tgt_lengths\']\n            pred = self.model(input_)\n            loss = self.crit(pred, tgt)\n            toks = self._num_toks(tgt_lens)\n            total_loss += loss.item() * toks\n            total_toks += toks\n            greedy_preds = [p[0] for p in self._predict(input_, beam=1, make_input=False)]\n            preds.extend(convert_seq2seq_preds(greedy_preds, self.tgt_rlut))\n            golds.extend(convert_seq2seq_golds(tgt.cpu().numpy(), tgt_lens, self.tgt_rlut))\n\n        metrics = self.calc_metrics(total_loss, total_toks)\n        metrics[\'bleu\'] = bleu(preds, golds, self.bleu_n_grams)[0]\n        self.report(\n            self.valid_epochs, metrics, start,\n            phase, \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def _evaluate(self, es, reporting_fns, **kwargs):\n        self.model.eval()\n        pg = create_progress_bar(len(es))\n        preds = []\n        golds = []\n        start = time.time()\n        for batch_dict in pg(es):\n            tgt = batch_dict[\'tgt\']\n            tgt_lens = batch_dict[\'tgt_lengths\']\n            pred = [p[0] for p in self._predict(batch_dict, numpy_to_tensor=False, **kwargs)]\n            preds.extend(convert_seq2seq_preds(pred, self.tgt_rlut))\n            golds.extend(convert_seq2seq_golds(tgt, tgt_lens, self.tgt_rlut))\n        metrics = {\'bleu\': bleu(preds, golds, self.bleu_n_grams)[0]}\n        self.report(\n            0, metrics, start, \'Test\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def train(self, ts, reporting_fns):\n        self.model.train()\n\n        epoch_loss = 0\n        epoch_toks = 0\n\n        start = time.time()\n        self.nstep_start = start\n        for batch_dict in ts:\n\n            start_time = time.time()\n            self.optimizer.zero_grad()\n            input_ = self._input(batch_dict)\n            tgt = input_[\'tgt\']\n            pred = self.model(input_)\n            loss = self.crit(pred, tgt)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n            self.optimizer.step()\n            tgt_lens = batch_dict[\'tgt_lengths\']\n            tok_count = self._num_toks(tgt_lens)\n            reporting_loss = loss.item() * tok_count\n            epoch_loss += reporting_loss\n            epoch_toks += tok_count\n            self.nstep_agg += reporting_loss\n            self.nstep_div += tok_count\n\n            if (self.optimizer.global_step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    self.optimizer.global_step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = self.calc_metrics(epoch_loss, epoch_toks)\n        self.train_epochs += 1\n        self.report(\n            self.train_epochs, metrics, start,\n            \'Train\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n\n@register_training_func(\'seq2seq\')\ndef fit(model_params, ts, vs, es=None, **kwargs):\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    epochs = int(kwargs.get(\'epochs\', 20))\n    model_file = get_model_file(\'seq2seq\', \'pytorch\', kwargs.get(\'basedir\'))\n\n\n    num_loader_workers = int(kwargs.get(\'num_loader_workers\', 0))\n    pin_memory = bool(kwargs.get(\'pin_memory\', True))\n    ts = DataLoader(ts, num_workers=num_loader_workers, batch_size=None, pin_memory=pin_memory)\n    vs = DataLoader(vs, batch_size=None, pin_memory=pin_memory)\n    es = DataLoader(es, batch_size=None, pin_memory=pin_memory) if es is not None else None\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'perplexity\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        logger.info(\'Doing early stopping on [%s] with patience [%d]\', early_stopping_metric, patience)\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    logger.info(\'reporting %s\', reporting_fns)\n\n    after_train_fn = kwargs.get(\'after_train_fn\', None)\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n    for epoch in range(epochs):\n        trainer.train(ts, reporting_fns)\n\n        if after_train_fn is not None:\n            after_train_fn(trainer.model)\n\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            logger.info(\'New best %.3f\', best_metric)\n            trainer.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            logger.info(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        logger.info(\'Best performance on %s: %.3f at epoch %d\', early_stopping_metric, best_metric, last_improved)\n\n    if es is not None:\n        model = torch.load(model_file)\n        trainer = Seq2SeqTrainerPyTorch(model, **kwargs)\n        test_metrics = trainer.test(es, reporting_fns, phase=\'Test\')\n    return test_metrics\n'"
baseline/pytorch/tagger/__init__.py,2,b'from baseline.pytorch.tagger.model import *\nfrom baseline.pytorch.tagger.train import *\n'
baseline/pytorch/tagger/model.py,11,"b'import logging\nfrom baseline.pytorch.torchy import *\nfrom baseline.utils import Offsets, write_json\nfrom baseline.model import TaggerModel\nfrom baseline.model import register_model\nimport torch.autograd\nimport os\n\nlogger = logging.getLogger(\'baseline\')\n\n\nclass TaggerModelBase(nn.Module, TaggerModel):\n    """"""Base class for tagger models\n\n    This class provides the model base for tagging.  To create a tagger, overload `create_layers()` and `forward()`.\n    Most implementations should be able to subclass the `AbstractEncoderTaggerModel`, which inherits from this and imposes\n    additional structure\n    """"""\n    def __init__(self):\n        """"""Constructor""""""\n        super().__init__()\n        self.gpu = False\n\n    def save(self, outname: str):\n        """"""Save out the model\n\n        :param outname: The name of the checkpoint to write\n        :return:\n        """"""\n        torch.save(self, outname)\n        basename, _ = os.path.splitext(outname)\n        write_json(self.labels, basename + "".labels"")\n\n    def cuda(self, device=None):\n        self.gpu = True\n        return super().cuda(device=device)\n\n\n    @staticmethod\n    def load(filename: str, **kwargs) -> \'TaggerModelBase\':\n        """"""Create and load a tagger model from file\n\n        """"""\n        device = kwargs.get(\'device\')\n        if not os.path.exists(filename):\n            filename += \'.pyt\'\n        model = torch.load(filename, map_location=device)\n        model.gpu = False if device == \'cpu\' else model.gpu\n        return model\n\n    def drop_inputs(self, key, x):\n        """"""Do dropout on inputs, using the dropout value (or none if not set)\n        This works by applying a dropout mask with the probability given by a\n        value within the `dropin_values: Dict[str, float]`, keyed off the text name\n        of the feature\n\n        :param key: The feature name\n        :param x: The tensor to drop inputs for\n        :return: The dropped out tensor\n        """"""\n        v = self.dropin_values.get(key, 0)\n        if not self.training or v == 0:\n            return x\n\n        mask_pad = x != Offsets.PAD\n        mask_drop = x.new(x.size(0), x.size(1)).bernoulli_(v).to(mask_pad.dtype)\n        x.masked_fill_(mask_pad & mask_drop, Offsets.UNK)\n        return x\n\n    def input_tensor(self, key, batch_dict, perm_idx, numpy_to_tensor=False):\n        """"""Given a batch of input, and a key, prepare and noise the input\n\n        :param key: The key of the tensor\n        :param batch_dict: The batch of data as a dictionary of tensors\n        :param perm_idx: The proper permutation order to get lengths descending\n        :param numpy_to_tensor: Should this tensor be converted to a `torch.Tensor`\n        :return:\n        """"""\n        tensor = batch_dict[key]\n        if numpy_to_tensor:\n            tensor = torch.from_numpy(tensor)\n\n        tensor = self.drop_inputs(key, tensor)\n        tensor = tensor[perm_idx]\n        if self.gpu:\n            tensor = tensor.cuda()\n        return tensor\n\n    def make_input(self, batch_dict: Dict[str, TensorDef], perm: bool = False, numpy_to_tensor: bool = False) -> Dict[str, TensorDef]:\n        """"""Transform a `batch_dict` into format suitable for tagging\n\n        :param batch_dict: A dictionary containing all inputs to the embeddings for this model\n        :param perm: Should we sort data by length descending?\n        :param numpy_to_tensor: Do we need to convert the input from numpy to a torch.Tensor?\n        :return: A dictionary representation of this batch suitable for processing\n        """"""\n        example_dict = dict({})\n        lengths = batch_dict[self.lengths_key]\n        if numpy_to_tensor:\n            lengths = torch.from_numpy(lengths)\n\n        lengths, perm_idx = lengths.sort(0, descending=True)\n        if self.gpu:\n            lengths = lengths.cuda()\n\n        example_dict[\'lengths\'] = lengths\n        for key in self.embeddings.keys():\n            example_dict[key] = self.input_tensor(key, batch_dict, perm_idx, numpy_to_tensor=numpy_to_tensor)\n        y = batch_dict.get(\'y\')\n        if y is not None:\n            if numpy_to_tensor:\n                y = torch.from_numpy(y)\n            y = y[perm_idx]\n            if self.gpu:\n                y = y.cuda()\n            example_dict[\'y\'] = y\n\n        ids = batch_dict.get(\'ids\')\n        if ids is not None:\n            if numpy_to_tensor:\n                ids = torch.from_numpy(ids)\n            ids = ids[perm_idx]\n            if self.gpu:\n                ids = ids.cuda()\n            example_dict[\'ids\'] = ids\n        if perm:\n            return example_dict, perm_idx\n        return example_dict\n\n    def get_labels(self) -> List[str]:\n        """"""Get the labels (names of each class)\n\n        :return: (`List[str]`) The labels\n        """"""\n        return self.labels\n\n    def predict(self, batch_dict: Dict[str, TensorDef], **kwargs) -> TensorDef:\n        """"""Take in a batch of data, and predict the tags\n\n        :param batch_dict: A batch of features that is to be predicted\n        :param kwargs: See Below\n\n        :Keyword Arguments:\n\n        * *numpy_to_tensor* (``bool``) Should we convert input from numpy to `torch.Tensor` Defaults to `True`\n        :return: A batch-sized tensor of predictions\n        """"""\n        numpy_to_tensor = bool(kwargs.get(\'numpy_to_tensor\', True))\n        inputs, perm_idx = self.make_input(batch_dict, perm=True, numpy_to_tensor=numpy_to_tensor)\n        outputs = self(inputs)\n        return unsort_batch(outputs, perm_idx)\n\n    @classmethod\n    def create(cls, embeddings: Dict[str, TensorDef], labels: List[str], **kwargs) -> \'TaggerModelBase\':\n        """"""Create a tagger from the inputs.  Most classes shouldnt extend this\n\n        :param embeddings: A dictionary containing the input feature indices\n        :param labels: A list of the labels (tags)\n        :param kwargs: See below\n\n        :Keyword Arguments:\n\n        * *lengths_key* (`str`) Which feature identifies the length of the sequence\n        * *activation* (`str`) What type of activation function to use (defaults to `tanh`)\n        * *dropout* (`str`) What fraction dropout to apply\n        * *dropin* (`str`) A dictionarwith feature keys telling what fraction of word masking to apply to each feature\n\n        :return:\n        """"""\n        model = cls()\n        model.lengths_key = kwargs.get(\'lengths_key\')\n        model.activation_type = kwargs.get(\'activation\', \'tanh\')\n        model.pdrop = float(kwargs.get(\'dropout\', 0.5))\n        model.dropin_values = kwargs.get(\'dropin\', {})\n        model.labels = labels\n        model.gpu = not bool(kwargs.get(\'nogpu\', False))\n        model.create_layers(embeddings, **kwargs)\n        return model\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        """"""This method defines the model itself, and must be overloaded by derived classes\n\n        This function will update `self` with the layers required to execute the `call()` method\n\n        :param embeddings: The input feature indices\n        :param kwargs:\n        :return:\n        """"""\n    def compute_loss(self, inputs):\n        """"""Define a loss function from the inputs, which includes the gold tag values as `inputs[\'y\']`\n\n        :param inputs:\n        :return:\n        """"""\n\n\nclass AbstractEncoderTaggerModel(TaggerModelBase):\n    """"""Class defining a typical flow for taggers.  Most taggers should extend this class\n\n    This class provides the model base for tagging by providing specific hooks for each phase.  There are\n    4 basic steps identified in this class:\n\n    1. embed\n    2. encode (transduction)\n    3. proj (projection to the final number of labels)\n    4. decode\n\n    There is an `init_* method for each of this phases, allowing you to\n    define and return a custom layer.\n\n    The actual forward method is defined as a combination of these 3 steps, which includes a\n    projection from the encoder output to the number of labels.\n\n    Decoding in taggers refers to the process of selecting the best path through the labels and is typically\n    implemented either as a constrained greedy decoder or as a CRF layer\n    """"""\n    def __init__(self):\n        """"""Constructor""""""\n        super().__init__()\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n\n        :Keyword Arguments: See below\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        return EmbeddingsStack(embeddings, self.pdrop, reduction=kwargs.get(\'embeddings_reduction\', \'concat\'))\n\n    def init_encode(self, input_dim, **kwargs) -> BaseLayer:\n        """"""Provide a layer object that represents the `encode` phase of the model\n        :param input_dim: The hidden input size\n        :param kwargs:\n        :return: The encoder\n        """"""\n\n    def init_proj(self, **kwargs) -> BaseLayer:\n        """"""Provide a projection from the encoder output to the number of labels\n\n        This projection typically will not include any activation, since its output is the logits that\n        the decoder is built on\n\n        :param kwargs:\n        :return: A projection from the encoder output size to the final number of labels\n        """"""\n        return Dense(self.encoder.output_dim, len(self.labels))\n\n    def init_decode(self, **kwargs) -> BaseLayer:\n        """"""Define a decoder from the inputs\n\n        :param kwargs: See below\n        :keyword Arguments:\n        * *crf* (``bool``) Should we create a CRF as the decoder\n        * *constraint_mask* (``tensor``) A constraint mask to apply to the transitions\n        * *reduction* (``str``) How to reduce the loss, defaults to `batch`\n        :return: A decoder layer\n        """"""\n        use_crf = bool(kwargs.get(\'crf\', False))\n        constraint_mask = kwargs.get(\'constraint_mask\')\n        if constraint_mask is not None:\n            constraint_mask = constraint_mask.unsqueeze(0)\n\n        if use_crf:\n            decoder = CRF(len(self.labels), constraint_mask=constraint_mask, batch_first=True)\n        else:\n            decoder = TaggerGreedyDecoder(\n                len(self.labels),\n                constraint_mask=constraint_mask,\n                batch_first=True,\n                reduction=kwargs.get(\'reduction\', \'batch\')\n            )\n        return decoder\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        """"""This class overrides this method to produce the outline of steps for a transduction tagger\n\n        :param embeddings: The input embeddings dict\n        :param kwargs:\n        :return:\n        """"""\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.encoder = self.init_encode(self.embeddings.output_dim, **kwargs)\n        self.proj_layer = self.init_proj(**kwargs)\n        self.decoder = self.init_decode(**kwargs)\n\n    def transduce(self, inputs: Dict[str, TensorDef]) -> TensorDef:\n        """"""This operation performs embedding of the input, followed by encoding and projection to logits\n\n        :param inputs: The feature indices to embed\n        :return: Transduced (post-encoding) output\n        """"""\n        lengths = inputs[""lengths""]\n        embedded = self.embeddings(inputs)\n        embedded = (embedded, lengths)\n        transduced = self.proj_layer(self.encoder(embedded))\n        return transduced\n\n    def decode(self, tensor: TensorDef, lengths: TensorDef) -> TensorDef:\n        """"""Take in the transduced (encoded) input and decode it\n\n        :param tensor: Transduced input\n        :param lengths: Valid lengths of the transduced input\n        :return: A best path through the output\n        """"""\n        return self.decoder((tensor, lengths))\n\n    def forward(self, inputs: Dict[str, TensorDef]) -> TensorDef:\n        """"""Take the input and produce the best path of labels out\n\n        :param inputs: The feature indices for the input\n        :return: The most likely path through the output labels\n        """"""\n        transduced = self.transduce(inputs)\n        path = self.decode(transduced, inputs.get(""lengths""))\n        return path\n\n    def compute_loss(self, inputs):\n        """"""Provide the loss by requesting it from the decoder\n\n        :param inputs: A batch of inputs\n        :return:\n        """"""\n        tags = inputs[\'y\']\n        lengths = inputs[\'lengths\']\n        unaries = self.transduce(inputs)\n        return self.decoder.neg_log_loss(unaries, tags, lengths)\n\n\n@register_model(task=\'tagger\', name=\'default\')\nclass RNNTaggerModel(AbstractEncoderTaggerModel):\n    """"""RNN-based tagger implementation: this is the default tagger for mead-baseline\n\n    Overload the encoder, typically as a BiLSTM\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Override the base method to produce an RNN transducer\n\n        :param input_dim: The size of the input\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *rnntype* (``str``) The type of RNN, defaults to `blstm`\n        * *layers* (``int``) The number of layers to stack\n        * *hsz* (``int``) The number of hidden units for each layer in the encoder\n        * *dropout* (``float``) The dropout rate\n        * *weight_init* (``str``) The weight initializer, defaults to `uniform`\n        * *unif* (``float``) A value for the weight initializer\n        :return: An encoder\n        """"""\n        rnntype = kwargs.get(\'rnntype\', \'blstm\')\n        nlayers = int(kwargs.get(\'layers\', 1))\n        unif = kwargs.get(\'unif\', 0)\n        hsz = int(kwargs[\'hsz\'])\n        pdrop = float(kwargs.get(\'dropout\', 0.5))\n        weight_init = kwargs.get(\'weight_init\', \'uniform\')\n        Encoder = LSTMEncoderSequence if rnntype == \'lstm\' else BiLSTMEncoderSequence\n        return Encoder(input_dim, hsz, nlayers, pdrop, unif=unif, initializer=weight_init, batch_first=True)\n\n\n@register_model(task=\'tagger\', name=\'transformer\')\nclass TransformerTaggerModel(AbstractEncoderTaggerModel):\n    """"""Transformer-based tagger model\n\n    Overload the encoder using a length-aware Transformer\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Override the base method to produce an RNN transducer\n\n        :param input_dim: The size of the input\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *num_heads* (``int``) The number of heads for multi-headed attention\n        * *layers* (``int``) The number of layers to stack\n        * *hsz* (``int``) The number of hidden units for each layer in the encoder\n        * *dropout* (``float``) The dropout rate, defaults\n        * *d_ff* (``int``) The feed-forward layer size\n        * *rpr_k* (``list`` or ``int``) The relative attention sizes.  If its a list, one scalar per layer, if its\n          a scalar, apply same size to each layer\n        :return: An encoder\n        """"""\n        layers = int(kwargs.get(\'layers\', 1))\n        num_heads = int(kwargs.get(\'num_heads\', 4))\n        pdrop = float(kwargs.get(\'dropout\', 0.5))\n        scale = False\n        hsz = int(kwargs[\'hsz\'])\n        rpr_k = kwargs.get(\'rpr_k\', 100)\n        d_ff = kwargs.get(\'d_ff\')\n        encoder = TransformerEncoderStackWithLengths(num_heads, hsz, pdrop, scale, layers, d_ff=d_ff, rpr_k=rpr_k, input_sz=input_dim)\n        return encoder\n\n\n@register_model(task=\'tagger\', name=\'cnn\')\nclass CNNTaggerModel(AbstractEncoderTaggerModel):\n    """"""Convolutional (AKA TDNN) tagger\n\n    Overload the encoder using a conv layer\n\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Override the base method to produce an RNN transducer\n\n        :param input_dim: The size of the input\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *layers* (``int``) The number of layers to stack\n        * *hsz* (``int``) The number of hidden units for each layer in the encoder\n        * *dropout* (``float``) The dropout rate, defaults\n        * *activation_type* (``str``) Defaults to `relu`\n        * *wfiltsz* (``int``) The 1D filter size for the convolution\n        :return: An encoder\n        """"""\n        layers = int(kwargs.get(\'layers\', 1))\n        pdrop = float(kwargs.get(\'dropout\', 0.5))\n        filtsz = kwargs.get(\'wfiltsz\', 5)\n        activation_type = kwargs.get(\'activation_type\', \'relu\')\n        hsz = int(kwargs[\'hsz\'])\n        return WithoutLength(ConvEncoderStack(input_dim, hsz, filtsz, layers, pdrop, activation_type))\n\n\n@register_model(task=\'tagger\', name=\'pass\')\nclass PassThruTaggerModel(AbstractEncoderTaggerModel):\n    """"""A Pass-thru implementation of the encoder\n\n    When we fine-tune our taggers from things like BERT embeddings, we might want to just pass through our\n    embedding result directly to the output decoder.  This model provides a mechanism for this by providing\n    a simple identity layer\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Identity layer encoder\n\n        :param input_dim: The input dims\n        :param kwargs: None\n        :return: An encoder\n        """"""\n        return WithoutLength(PassThru(input_dim))\n'"
baseline/pytorch/tagger/train.py,7,"b'import six\nimport logging\nfrom baseline.progress import create_progress_bar\nfrom baseline.train import EpochReportingTrainer, create_trainer, register_trainer, register_training_func\nfrom eight_mile.utils import listify, to_spans, f_score, revlut, write_sentence_conll\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.pytorch.torchy import *\nfrom eight_mile.pytorch.optz import OptimizerManager\nfrom eight_mile.utils import span_f1, per_entity_f1, conlleval_output\nfrom baseline.model import create_model_for\nfrom torch.utils.data import DataLoader\n\nlogger = logging.getLogger(\'baseline\')\n\n\n@register_trainer(task=\'tagger\', name=\'default\')\nclass TaggerTrainerPyTorch(EpochReportingTrainer):\n\n    def __init__(self, model, **kwargs):\n        super(TaggerTrainerPyTorch, self).__init__()\n        if type(model) is dict:\n            model = create_model_for(\'tagger\', **model)\n        self.grad_accum = int(kwargs.get(\'grad_accum\', 1))\n        self.gpus = int(kwargs.get(\'gpus\', 1))\n        # By default support IOB1/IOB2\n        self.span_type = kwargs.get(\'span_type\', \'iob\')\n        self.verbose = kwargs.get(\'verbose\', False)\n\n        logger.info(\'Setting span type %s\', self.span_type)\n        self.model = model\n        self.idx2label = revlut(self.model.labels)\n        self.clip = float(kwargs.get(\'clip\', 5))\n        self.optimizer = OptimizerManager(self.model, **kwargs)\n        if self.gpus > 1:\n            logger.info(""Trainer for PyTorch tagger currently doesnt support multiple GPUs.  Setting to 1"")\n            self.gpus = 1\n        if self.gpus > 0 and self.model.gpu:\n            self.model = model.cuda()\n        else:\n            logger.warning(""Requested training on CPU.  This will be slow."")\n\n        self.nsteps = kwargs.get(\'nsteps\', six.MAXSIZE)\n\n    def save(self, model_file):\n        self.model.save(model_file)\n\n    @staticmethod\n    def _get_batchsz(batch_dict):\n        return batch_dict[\'y\'].shape[0]\n\n    def process_output(self, guess, truth, sentence_lengths, ids, handle=None, txts=None):\n\n        # For acc\n        correct_labels = 0\n        total_labels = 0\n        truth_n = truth.cpu().numpy()\n        # For f1\n        gold_chunks = []\n        pred_chunks = []\n\n        # For each sentence\n        for b in range(len(guess)):\n            sentence = guess[b]\n            if isinstance(sentence, torch.Tensor):\n                sentence = sentence.cpu().numpy()\n            sentence_length = sentence_lengths[b]\n            gold = truth_n[b, :sentence_length]\n            sentence = sentence[:sentence_length]\n\n            valid_guess = sentence[gold != Offsets.PAD]\n            valid_gold = gold[gold != Offsets.PAD]\n            valid_sentence_length = np.sum(gold != Offsets.PAD)\n            correct_labels += np.sum(np.equal(valid_guess, valid_gold))\n            total_labels += valid_sentence_length\n            gold_chunks.append(set(to_spans(valid_gold, self.idx2label, self.span_type, self.verbose)))\n            pred_chunks.append(set(to_spans(valid_guess, self.idx2label, self.span_type, self.verbose)))\n\n            # Should we write a file out?  If so, we have to have txts\n            if handle is not None and txts is not None:\n                txt_id = ids[b]\n                txt = txts[txt_id]\n                write_sentence_conll(handle, valid_guess, valid_gold, txt, self.idx2label)\n\n        return correct_labels, total_labels, gold_chunks, pred_chunks\n\n    def _test(self, ts, **kwargs):\n\n        self.model.eval()\n        total_sum = 0\n        total_correct = 0\n\n        gold_spans = []\n        pred_spans = []\n\n        metrics = {}\n        steps = len(ts)\n        conll_output = kwargs.get(\'conll_output\', None)\n        txts = kwargs.get(\'txts\', None)\n        handle = None\n        if conll_output is not None and txts is not None:\n            handle = open(conll_output, ""w"")\n        pg = create_progress_bar(steps)\n        for batch_dict in pg(ts):\n\n            inputs = self.model.make_input(batch_dict)\n            y = inputs.pop(\'y\')\n            lengths = inputs[\'lengths\']\n            ids = inputs[\'ids\']\n            with torch.no_grad():\n                pred = self.model(inputs)\n            correct, count, golds, guesses = self.process_output(pred, y.data, lengths, ids, handle, txts)\n            total_correct += correct\n            total_sum += count\n            gold_spans.extend(golds)\n            pred_spans.extend(guesses)\n\n        total_acc = total_correct / float(total_sum)\n        metrics[\'acc\'] = total_acc\n        metrics[\'f1\'] = span_f1(gold_spans, pred_spans)\n        if self.verbose:\n            # TODO: Add programmatic access to these metrics?\n            conll_metrics = per_entity_f1(gold_spans, pred_spans)\n            conll_metrics[\'acc\'] = total_acc * 100\n            conll_metrics[\'tokens\'] = total_sum.item()\n            logger.info(conlleval_output(conll_metrics))\n        return metrics\n\n    def _train(self, ts, **kwargs):\n        self.model.train()\n        reporting_fns = kwargs.get(\'reporting_fns\', [])\n        epoch_loss = 0\n        epoch_norm = 0\n        steps = len(ts)\n        pg = create_progress_bar(steps)\n        self.optimizer.zero_grad()\n\n        for i, batch_dict in enumerate(pg(ts)):\n            inputs = self.model.make_input(batch_dict)\n            loss = self.model.compute_loss(inputs)\n            loss.backward()\n\n            if (i+1) % self.grad_accum == 0 or (i+1) == steps:\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n\n            bsz = self._get_batchsz(batch_dict)\n            report_loss = loss.item() * bsz\n            epoch_loss += report_loss\n            epoch_norm += bsz\n            self.nstep_agg += report_loss\n            self.nstep_div += bsz\n            if (self.optimizer.global_step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    self.optimizer.global_step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = self.calc_metrics(epoch_loss, epoch_norm)\n        return metrics\n\n\n@register_training_func(\'tagger\')\ndef fit(model_params, ts, vs, es, **kwargs):\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    epochs = int(kwargs.get(\'epochs\', 20))\n    model_file = get_model_file(\'tagger\', \'pytorch\', kwargs.get(\'basedir\'))\n    conll_output = kwargs.get(\'conll_output\', None)\n    txts = kwargs.get(\'txts\', None)\n\n\n    num_loader_workers = int(kwargs.get(\'num_loader_workers\', 0))\n    pin_memory = bool(kwargs.get(\'pin_memory\', True))\n    ts = DataLoader(ts, num_workers=num_loader_workers, batch_size=None, pin_memory=pin_memory)\n    vs = DataLoader(vs, batch_size=None, pin_memory=pin_memory)\n    es = DataLoader(es, batch_size=None, pin_memory=pin_memory) if es is not None else None\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        logger.info(\'Doing early stopping on [%s] with patience [%d]\', early_stopping_metric, patience)\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    logger.info(\'reporting %s\', reporting_fns)\n\n    after_train_fn = kwargs.get(\'after_train_fn\', None)\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n    for epoch in range(epochs):\n\n        trainer.train(ts, reporting_fns)\n        if after_train_fn is not None:\n            after_train_fn(trainer.model)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            logger.info(\'New best %.3f\', best_metric)\n            trainer.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            logger.info(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        logger.info(\'Best performance on %s: %.3f at epoch %d\', early_stopping_metric, best_metric, last_improved)\n\n    if es is not None:\n        logger.info(\'Reloading best checkpoint\')\n        model = torch.load(model_file)\n        trainer = create_trainer(model, **kwargs)\n        test_metrics = trainer.test(es, reporting_fns, conll_output=conll_output, txts=txts, phase=\'Test\')\n    return test_metrics\n'"
baseline/tensorflow_serving/apis/__init__.py,0,b''
baseline/tensorflow_serving/apis/classification_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/classification.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom baseline.tensorflow_serving.apis import input_pb2 as tensorflow__serving_dot_apis_dot_input__pb2\nfrom baseline.tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/classification.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n,tensorflow_serving/apis/classification.proto\\x12\\x12tensorflow.serving\\x1a#tensorflow_serving/apis/input.proto\\x1a#tensorflow_serving/apis/model.proto\\""%\\n\\x05\\x43lass\\x12\\r\\n\\x05label\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05score\\x18\\x02 \\x01(\\x02\\""=\\n\\x0f\\x43lassifications\\x12*\\n\\x07\\x63lasses\\x18\\x01 \\x03(\\x0b\\x32\\x19.tensorflow.serving.Class\\""T\\n\\x14\\x43lassificationResult\\x12<\\n\\x0f\\x63lassifications\\x18\\x01 \\x03(\\x0b\\x32#.tensorflow.serving.Classifications\\""t\\n\\x15\\x43lassificationRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12(\\n\\x05input\\x18\\x02 \\x01(\\x0b\\x32\\x19.tensorflow.serving.Input\\""\\x85\\x01\\n\\x16\\x43lassificationResponse\\x12\\x31\\n\\nmodel_spec\\x18\\x02 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12\\x38\\n\\x06result\\x18\\x01 \\x01(\\x0b\\x32(.tensorflow.serving.ClassificationResultB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_input__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_CLASS = _descriptor.Descriptor(\n  name=\'Class\',\n  full_name=\'tensorflow.serving.Class\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'label\', full_name=\'tensorflow.serving.Class.label\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'tensorflow.serving.Class.score\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=142,\n  serialized_end=179,\n)\n\n\n_CLASSIFICATIONS = _descriptor.Descriptor(\n  name=\'Classifications\',\n  full_name=\'tensorflow.serving.Classifications\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'classes\', full_name=\'tensorflow.serving.Classifications.classes\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=181,\n  serialized_end=242,\n)\n\n\n_CLASSIFICATIONRESULT = _descriptor.Descriptor(\n  name=\'ClassificationResult\',\n  full_name=\'tensorflow.serving.ClassificationResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'classifications\', full_name=\'tensorflow.serving.ClassificationResult.classifications\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=244,\n  serialized_end=328,\n)\n\n\n_CLASSIFICATIONREQUEST = _descriptor.Descriptor(\n  name=\'ClassificationRequest\',\n  full_name=\'tensorflow.serving.ClassificationRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.ClassificationRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorflow.serving.ClassificationRequest.input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=330,\n  serialized_end=446,\n)\n\n\n_CLASSIFICATIONRESPONSE = _descriptor.Descriptor(\n  name=\'ClassificationResponse\',\n  full_name=\'tensorflow.serving.ClassificationResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.ClassificationResponse.model_spec\', index=0,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'result\', full_name=\'tensorflow.serving.ClassificationResponse.result\', index=1,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=449,\n  serialized_end=582,\n)\n\n_CLASSIFICATIONS.fields_by_name[\'classes\'].message_type = _CLASS\n_CLASSIFICATIONRESULT.fields_by_name[\'classifications\'].message_type = _CLASSIFICATIONS\n_CLASSIFICATIONREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_CLASSIFICATIONREQUEST.fields_by_name[\'input\'].message_type = tensorflow__serving_dot_apis_dot_input__pb2._INPUT\n_CLASSIFICATIONRESPONSE.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_CLASSIFICATIONRESPONSE.fields_by_name[\'result\'].message_type = _CLASSIFICATIONRESULT\nDESCRIPTOR.message_types_by_name[\'Class\'] = _CLASS\nDESCRIPTOR.message_types_by_name[\'Classifications\'] = _CLASSIFICATIONS\nDESCRIPTOR.message_types_by_name[\'ClassificationResult\'] = _CLASSIFICATIONRESULT\nDESCRIPTOR.message_types_by_name[\'ClassificationRequest\'] = _CLASSIFICATIONREQUEST\nDESCRIPTOR.message_types_by_name[\'ClassificationResponse\'] = _CLASSIFICATIONRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nClass = _reflection.GeneratedProtocolMessageType(\'Class\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASS,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Class)\n  ))\n_sym_db.RegisterMessage(Class)\n\nClassifications = _reflection.GeneratedProtocolMessageType(\'Classifications\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONS,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Classifications)\n  ))\n_sym_db.RegisterMessage(Classifications)\n\nClassificationResult = _reflection.GeneratedProtocolMessageType(\'ClassificationResult\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONRESULT,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ClassificationResult)\n  ))\n_sym_db.RegisterMessage(ClassificationResult)\n\nClassificationRequest = _reflection.GeneratedProtocolMessageType(\'ClassificationRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONREQUEST,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ClassificationRequest)\n  ))\n_sym_db.RegisterMessage(ClassificationRequest)\n\nClassificationResponse = _reflection.GeneratedProtocolMessageType(\'ClassificationResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONRESPONSE,\n  __module__ = \'tensorflow_serving.apis.classification_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ClassificationResponse)\n  ))\n_sym_db.RegisterMessage(ClassificationResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/classification_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tensorflow_serving/apis/get_model_metadata_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/get_model_metadata.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import any_pb2 as google_dot_protobuf_dot_any__pb2\nfrom tensorflow.core.protobuf import meta_graph_pb2 as tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2\nfrom baseline.tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/get_model_metadata.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow_serving/apis/get_model_metadata.proto\\x12\\x12tensorflow.serving\\x1a\\x19google/protobuf/any.proto\\x1a)tensorflow/core/protobuf/meta_graph.proto\\x1a#tensorflow_serving/apis/model.proto\\""\\xae\\x01\\n\\x0fSignatureDefMap\\x12L\\n\\rsignature_def\\x18\\x01 \\x03(\\x0b\\x32\\x35.tensorflow.serving.SignatureDefMap.SignatureDefEntry\\x1aM\\n\\x11SignatureDefEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x18.tensorflow.SignatureDef:\\x02\\x38\\x01\\""d\\n\\x17GetModelMetadataRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12\\x16\\n\\x0emetadata_field\\x18\\x02 \\x03(\\t\\""\\xe2\\x01\\n\\x18GetModelMetadataResponse\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12L\\n\\x08metadata\\x18\\x02 \\x03(\\x0b\\x32:.tensorflow.serving.GetModelMetadataResponse.MetadataEntry\\x1a\\x45\\n\\rMetadataEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12#\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x14.google.protobuf.Any:\\x02\\x38\\x01\\x42\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY = _descriptor.Descriptor(\n  name=\'SignatureDefEntry\',\n  full_name=\'tensorflow.serving.SignatureDefMap.SignatureDefEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.SignatureDefMap.SignatureDefEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.SignatureDefMap.SignatureDefEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=277,\n  serialized_end=354,\n)\n\n_SIGNATUREDEFMAP = _descriptor.Descriptor(\n  name=\'SignatureDefMap\',\n  full_name=\'tensorflow.serving.SignatureDefMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'signature_def\', full_name=\'tensorflow.serving.SignatureDefMap.signature_def\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_SIGNATUREDEFMAP_SIGNATUREDEFENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=180,\n  serialized_end=354,\n)\n\n\n_GETMODELMETADATAREQUEST = _descriptor.Descriptor(\n  name=\'GetModelMetadataRequest\',\n  full_name=\'tensorflow.serving.GetModelMetadataRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.GetModelMetadataRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'metadata_field\', full_name=\'tensorflow.serving.GetModelMetadataRequest.metadata_field\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=356,\n  serialized_end=456,\n)\n\n\n_GETMODELMETADATARESPONSE_METADATAENTRY = _descriptor.Descriptor(\n  name=\'MetadataEntry\',\n  full_name=\'tensorflow.serving.GetModelMetadataResponse.MetadataEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.GetModelMetadataResponse.MetadataEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.GetModelMetadataResponse.MetadataEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=616,\n  serialized_end=685,\n)\n\n_GETMODELMETADATARESPONSE = _descriptor.Descriptor(\n  name=\'GetModelMetadataResponse\',\n  full_name=\'tensorflow.serving.GetModelMetadataResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.GetModelMetadataResponse.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'metadata\', full_name=\'tensorflow.serving.GetModelMetadataResponse.metadata\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_GETMODELMETADATARESPONSE_METADATAENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=459,\n  serialized_end=685,\n)\n\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_protobuf_dot_meta__graph__pb2._SIGNATUREDEF\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY.containing_type = _SIGNATUREDEFMAP\n_SIGNATUREDEFMAP.fields_by_name[\'signature_def\'].message_type = _SIGNATUREDEFMAP_SIGNATUREDEFENTRY\n_GETMODELMETADATAREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_GETMODELMETADATARESPONSE_METADATAENTRY.fields_by_name[\'value\'].message_type = google_dot_protobuf_dot_any__pb2._ANY\n_GETMODELMETADATARESPONSE_METADATAENTRY.containing_type = _GETMODELMETADATARESPONSE\n_GETMODELMETADATARESPONSE.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_GETMODELMETADATARESPONSE.fields_by_name[\'metadata\'].message_type = _GETMODELMETADATARESPONSE_METADATAENTRY\nDESCRIPTOR.message_types_by_name[\'SignatureDefMap\'] = _SIGNATUREDEFMAP\nDESCRIPTOR.message_types_by_name[\'GetModelMetadataRequest\'] = _GETMODELMETADATAREQUEST\nDESCRIPTOR.message_types_by_name[\'GetModelMetadataResponse\'] = _GETMODELMETADATARESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nSignatureDefMap = _reflection.GeneratedProtocolMessageType(\'SignatureDefMap\', (_message.Message,), dict(\n\n  SignatureDefEntry = _reflection.GeneratedProtocolMessageType(\'SignatureDefEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _SIGNATUREDEFMAP_SIGNATUREDEFENTRY,\n    __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.SignatureDefMap.SignatureDefEntry)\n    ))\n  ,\n  DESCRIPTOR = _SIGNATUREDEFMAP,\n  __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.SignatureDefMap)\n  ))\n_sym_db.RegisterMessage(SignatureDefMap)\n_sym_db.RegisterMessage(SignatureDefMap.SignatureDefEntry)\n\nGetModelMetadataRequest = _reflection.GeneratedProtocolMessageType(\'GetModelMetadataRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _GETMODELMETADATAREQUEST,\n  __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.GetModelMetadataRequest)\n  ))\n_sym_db.RegisterMessage(GetModelMetadataRequest)\n\nGetModelMetadataResponse = _reflection.GeneratedProtocolMessageType(\'GetModelMetadataResponse\', (_message.Message,), dict(\n\n  MetadataEntry = _reflection.GeneratedProtocolMessageType(\'MetadataEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _GETMODELMETADATARESPONSE_METADATAENTRY,\n    __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.GetModelMetadataResponse.MetadataEntry)\n    ))\n  ,\n  DESCRIPTOR = _GETMODELMETADATARESPONSE,\n  __module__ = \'tensorflow_serving.apis.get_model_metadata_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.GetModelMetadataResponse)\n  ))\n_sym_db.RegisterMessage(GetModelMetadataResponse)\n_sym_db.RegisterMessage(GetModelMetadataResponse.MetadataEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY.has_options = True\n_SIGNATUREDEFMAP_SIGNATUREDEFENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_GETMODELMETADATARESPONSE_METADATAENTRY.has_options = True\n_GETMODELMETADATARESPONSE_METADATAENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/get_model_metadata_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tensorflow_serving/apis/inference_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/inference.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom baseline.tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom baseline.tensorflow_serving.apis import input_pb2 as tensorflow__serving_dot_apis_dot_input__pb2\nfrom baseline.tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\nfrom baseline.tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/inference.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n\\\'tensorflow_serving/apis/inference.proto\\x12\\x12tensorflow.serving\\x1a,tensorflow_serving/apis/classification.proto\\x1a#tensorflow_serving/apis/input.proto\\x1a#tensorflow_serving/apis/model.proto\\x1a(tensorflow_serving/apis/regression.proto\\""W\\n\\rInferenceTask\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12\\x13\\n\\x0bmethod_name\\x18\\x02 \\x01(\\t\\""\\xdc\\x01\\n\\x0fInferenceResult\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12I\\n\\x15\\x63lassification_result\\x18\\x02 \\x01(\\x0b\\x32(.tensorflow.serving.ClassificationResultH\\x00\\x12\\x41\\n\\x11regression_result\\x18\\x03 \\x01(\\x0b\\x32$.tensorflow.serving.RegressionResultH\\x00\\x42\\x08\\n\\x06result\\""s\\n\\x15MultiInferenceRequest\\x12\\x30\\n\\x05tasks\\x18\\x01 \\x03(\\x0b\\x32!.tensorflow.serving.InferenceTask\\x12(\\n\\x05input\\x18\\x02 \\x01(\\x0b\\x32\\x19.tensorflow.serving.Input\\""N\\n\\x16MultiInferenceResponse\\x12\\x34\\n\\x07results\\x18\\x01 \\x03(\\x0b\\x32#.tensorflow.serving.InferenceResultB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_classification__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_input__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_regression__pb2.DESCRIPTOR,])\n\n\n\n\n_INFERENCETASK = _descriptor.Descriptor(\n  name=\'InferenceTask\',\n  full_name=\'tensorflow.serving.InferenceTask\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.InferenceTask.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'method_name\', full_name=\'tensorflow.serving.InferenceTask.method_name\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=225,\n  serialized_end=312,\n)\n\n\n_INFERENCERESULT = _descriptor.Descriptor(\n  name=\'InferenceResult\',\n  full_name=\'tensorflow.serving.InferenceResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.InferenceResult.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'classification_result\', full_name=\'tensorflow.serving.InferenceResult.classification_result\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'regression_result\', full_name=\'tensorflow.serving.InferenceResult.regression_result\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'result\', full_name=\'tensorflow.serving.InferenceResult.result\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=315,\n  serialized_end=535,\n)\n\n\n_MULTIINFERENCEREQUEST = _descriptor.Descriptor(\n  name=\'MultiInferenceRequest\',\n  full_name=\'tensorflow.serving.MultiInferenceRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tasks\', full_name=\'tensorflow.serving.MultiInferenceRequest.tasks\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorflow.serving.MultiInferenceRequest.input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=537,\n  serialized_end=652,\n)\n\n\n_MULTIINFERENCERESPONSE = _descriptor.Descriptor(\n  name=\'MultiInferenceResponse\',\n  full_name=\'tensorflow.serving.MultiInferenceResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'results\', full_name=\'tensorflow.serving.MultiInferenceResponse.results\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=654,\n  serialized_end=732,\n)\n\n_INFERENCETASK.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_INFERENCERESULT.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_INFERENCERESULT.fields_by_name[\'classification_result\'].message_type = tensorflow__serving_dot_apis_dot_classification__pb2._CLASSIFICATIONRESULT\n_INFERENCERESULT.fields_by_name[\'regression_result\'].message_type = tensorflow__serving_dot_apis_dot_regression__pb2._REGRESSIONRESULT\n_INFERENCERESULT.oneofs_by_name[\'result\'].fields.append(\n  _INFERENCERESULT.fields_by_name[\'classification_result\'])\n_INFERENCERESULT.fields_by_name[\'classification_result\'].containing_oneof = _INFERENCERESULT.oneofs_by_name[\'result\']\n_INFERENCERESULT.oneofs_by_name[\'result\'].fields.append(\n  _INFERENCERESULT.fields_by_name[\'regression_result\'])\n_INFERENCERESULT.fields_by_name[\'regression_result\'].containing_oneof = _INFERENCERESULT.oneofs_by_name[\'result\']\n_MULTIINFERENCEREQUEST.fields_by_name[\'tasks\'].message_type = _INFERENCETASK\n_MULTIINFERENCEREQUEST.fields_by_name[\'input\'].message_type = tensorflow__serving_dot_apis_dot_input__pb2._INPUT\n_MULTIINFERENCERESPONSE.fields_by_name[\'results\'].message_type = _INFERENCERESULT\nDESCRIPTOR.message_types_by_name[\'InferenceTask\'] = _INFERENCETASK\nDESCRIPTOR.message_types_by_name[\'InferenceResult\'] = _INFERENCERESULT\nDESCRIPTOR.message_types_by_name[\'MultiInferenceRequest\'] = _MULTIINFERENCEREQUEST\nDESCRIPTOR.message_types_by_name[\'MultiInferenceResponse\'] = _MULTIINFERENCERESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nInferenceTask = _reflection.GeneratedProtocolMessageType(\'InferenceTask\', (_message.Message,), dict(\n  DESCRIPTOR = _INFERENCETASK,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceTask)\n  ))\n_sym_db.RegisterMessage(InferenceTask)\n\nInferenceResult = _reflection.GeneratedProtocolMessageType(\'InferenceResult\', (_message.Message,), dict(\n  DESCRIPTOR = _INFERENCERESULT,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceResult)\n  ))\n_sym_db.RegisterMessage(InferenceResult)\n\nMultiInferenceRequest = _reflection.GeneratedProtocolMessageType(\'MultiInferenceRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _MULTIINFERENCEREQUEST,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceRequest)\n  ))\n_sym_db.RegisterMessage(MultiInferenceRequest)\n\nMultiInferenceResponse = _reflection.GeneratedProtocolMessageType(\'MultiInferenceResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _MULTIINFERENCERESPONSE,\n  __module__ = \'tensorflow_serving.apis.inference_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceResponse)\n  ))\n_sym_db.RegisterMessage(MultiInferenceResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/inference_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tensorflow_serving/apis/input_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/input.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.example import example_pb2 as tensorflow_dot_core_dot_example_dot_example__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/input.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n#tensorflow_serving/apis/input.proto\\x12\\x12tensorflow.serving\\x1a%tensorflow/core/example/example.proto\\""4\\n\\x0b\\x45xampleList\\x12%\\n\\x08\\x65xamples\\x18\\x01 \\x03(\\x0b\\x32\\x13.tensorflow.Example\\""e\\n\\x16\\x45xampleListWithContext\\x12%\\n\\x08\\x65xamples\\x18\\x01 \\x03(\\x0b\\x32\\x13.tensorflow.Example\\x12$\\n\\x07\\x63ontext\\x18\\x02 \\x01(\\x0b\\x32\\x13.tensorflow.Example\\""\\xa1\\x01\\n\\x05Input\\x12;\\n\\x0c\\x65xample_list\\x18\\x01 \\x01(\\x0b\\x32\\x1f.tensorflow.serving.ExampleListB\\x02(\\x01H\\x00\\x12S\\n\\x19\\x65xample_list_with_context\\x18\\x02 \\x01(\\x0b\\x32*.tensorflow.serving.ExampleListWithContextB\\x02(\\x01H\\x00\\x42\\x06\\n\\x04kindB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_example_dot_example__pb2.DESCRIPTOR,])\n\n\n\n\n_EXAMPLELIST = _descriptor.Descriptor(\n  name=\'ExampleList\',\n  full_name=\'tensorflow.serving.ExampleList\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'examples\', full_name=\'tensorflow.serving.ExampleList.examples\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=98,\n  serialized_end=150,\n)\n\n\n_EXAMPLELISTWITHCONTEXT = _descriptor.Descriptor(\n  name=\'ExampleListWithContext\',\n  full_name=\'tensorflow.serving.ExampleListWithContext\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'examples\', full_name=\'tensorflow.serving.ExampleListWithContext.examples\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'context\', full_name=\'tensorflow.serving.ExampleListWithContext.context\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=152,\n  serialized_end=253,\n)\n\n\n_INPUT = _descriptor.Descriptor(\n  name=\'Input\',\n  full_name=\'tensorflow.serving.Input\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'example_list\', full_name=\'tensorflow.serving.Input.example_list\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\')), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'example_list_with_context\', full_name=\'tensorflow.serving.Input.example_list_with_context\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\')), file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'kind\', full_name=\'tensorflow.serving.Input.kind\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=256,\n  serialized_end=417,\n)\n\n_EXAMPLELIST.fields_by_name[\'examples\'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE\n_EXAMPLELISTWITHCONTEXT.fields_by_name[\'examples\'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE\n_EXAMPLELISTWITHCONTEXT.fields_by_name[\'context\'].message_type = tensorflow_dot_core_dot_example_dot_example__pb2._EXAMPLE\n_INPUT.fields_by_name[\'example_list\'].message_type = _EXAMPLELIST\n_INPUT.fields_by_name[\'example_list_with_context\'].message_type = _EXAMPLELISTWITHCONTEXT\n_INPUT.oneofs_by_name[\'kind\'].fields.append(\n  _INPUT.fields_by_name[\'example_list\'])\n_INPUT.fields_by_name[\'example_list\'].containing_oneof = _INPUT.oneofs_by_name[\'kind\']\n_INPUT.oneofs_by_name[\'kind\'].fields.append(\n  _INPUT.fields_by_name[\'example_list_with_context\'])\n_INPUT.fields_by_name[\'example_list_with_context\'].containing_oneof = _INPUT.oneofs_by_name[\'kind\']\nDESCRIPTOR.message_types_by_name[\'ExampleList\'] = _EXAMPLELIST\nDESCRIPTOR.message_types_by_name[\'ExampleListWithContext\'] = _EXAMPLELISTWITHCONTEXT\nDESCRIPTOR.message_types_by_name[\'Input\'] = _INPUT\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nExampleList = _reflection.GeneratedProtocolMessageType(\'ExampleList\', (_message.Message,), dict(\n  DESCRIPTOR = _EXAMPLELIST,\n  __module__ = \'tensorflow_serving.apis.input_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ExampleList)\n  ))\n_sym_db.RegisterMessage(ExampleList)\n\nExampleListWithContext = _reflection.GeneratedProtocolMessageType(\'ExampleListWithContext\', (_message.Message,), dict(\n  DESCRIPTOR = _EXAMPLELISTWITHCONTEXT,\n  __module__ = \'tensorflow_serving.apis.input_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ExampleListWithContext)\n  ))\n_sym_db.RegisterMessage(ExampleListWithContext)\n\nInput = _reflection.GeneratedProtocolMessageType(\'Input\', (_message.Message,), dict(\n  DESCRIPTOR = _INPUT,\n  __module__ = \'tensorflow_serving.apis.input_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Input)\n  ))\n_sym_db.RegisterMessage(Input)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_INPUT.fields_by_name[\'example_list\'].has_options = True\n_INPUT.fields_by_name[\'example_list\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\'))\n_INPUT.fields_by_name[\'example_list_with_context\'].has_options = True\n_INPUT.fields_by_name[\'example_list_with_context\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'(\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/input_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tensorflow_serving/apis/model_management_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/model_management.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom baseline.tensorflow_serving.config import model_server_config_pb2 as tensorflow__serving_dot_config_dot_model__server__config__pb2\nfrom baseline.tensorflow_serving.util import status_pb2 as tensorflow__serving_dot_util_dot_status__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/model_management.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n.tensorflow_serving/apis/model_management.proto\\x12\\x12tensorflow.serving\\x1a\\x33tensorflow_serving/config/model_server_config.proto\\x1a$tensorflow_serving/util/status.proto\\""L\\n\\x13ReloadConfigRequest\\x12\\x35\\n\\x06\\x63onfig\\x18\\x01 \\x01(\\x0b\\x32%.tensorflow.serving.ModelServerConfig\\""G\\n\\x14ReloadConfigResponse\\x12/\\n\\x06status\\x18\\x01 \\x01(\\x0b\\x32\\x1f.tensorflow.serving.StatusProtoB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_config_dot_model__server__config__pb2.DESCRIPTOR,tensorflow__serving_dot_util_dot_status__pb2.DESCRIPTOR,])\n\n\n\n\n_RELOADCONFIGREQUEST = _descriptor.Descriptor(\n  name=\'ReloadConfigRequest\',\n  full_name=\'tensorflow.serving.ReloadConfigRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'config\', full_name=\'tensorflow.serving.ReloadConfigRequest.config\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=161,\n  serialized_end=237,\n)\n\n\n_RELOADCONFIGRESPONSE = _descriptor.Descriptor(\n  name=\'ReloadConfigResponse\',\n  full_name=\'tensorflow.serving.ReloadConfigResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'tensorflow.serving.ReloadConfigResponse.status\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=239,\n  serialized_end=310,\n)\n\n_RELOADCONFIGREQUEST.fields_by_name[\'config\'].message_type = tensorflow__serving_dot_config_dot_model__server__config__pb2._MODELSERVERCONFIG\n_RELOADCONFIGRESPONSE.fields_by_name[\'status\'].message_type = tensorflow__serving_dot_util_dot_status__pb2._STATUSPROTO\nDESCRIPTOR.message_types_by_name[\'ReloadConfigRequest\'] = _RELOADCONFIGREQUEST\nDESCRIPTOR.message_types_by_name[\'ReloadConfigResponse\'] = _RELOADCONFIGRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nReloadConfigRequest = _reflection.GeneratedProtocolMessageType(\'ReloadConfigRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _RELOADCONFIGREQUEST,\n  __module__ = \'tensorflow_serving.apis.model_management_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ReloadConfigRequest)\n  ))\n_sym_db.RegisterMessage(ReloadConfigRequest)\n\nReloadConfigResponse = _reflection.GeneratedProtocolMessageType(\'ReloadConfigResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _RELOADCONFIGRESPONSE,\n  __module__ = \'tensorflow_serving.apis.model_management_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ReloadConfigResponse)\n  ))\n_sym_db.RegisterMessage(ReloadConfigResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/model_management_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tensorflow_serving/apis/model_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/model.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/model.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n#tensorflow_serving/apis/model.proto\\x12\\x12tensorflow.serving\\x1a\\x1egoogle/protobuf/wrappers.proto\\""_\\n\\tModelSpec\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12,\\n\\x07version\\x18\\x02 \\x01(\\x0b\\x32\\x1b.google.protobuf.Int64Value\\x12\\x16\\n\\x0esignature_name\\x18\\x03 \\x01(\\tB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_wrappers__pb2.DESCRIPTOR,])\n\n\n\n\n_MODELSPEC = _descriptor.Descriptor(\n  name=\'ModelSpec\',\n  full_name=\'tensorflow.serving.ModelSpec\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'tensorflow.serving.ModelSpec.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'version\', full_name=\'tensorflow.serving.ModelSpec.version\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'signature_name\', full_name=\'tensorflow.serving.ModelSpec.signature_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=91,\n  serialized_end=186,\n)\n\n_MODELSPEC.fields_by_name[\'version\'].message_type = google_dot_protobuf_dot_wrappers__pb2._INT64VALUE\nDESCRIPTOR.message_types_by_name[\'ModelSpec\'] = _MODELSPEC\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nModelSpec = _reflection.GeneratedProtocolMessageType(\'ModelSpec\', (_message.Message,), dict(\n  DESCRIPTOR = _MODELSPEC,\n  __module__ = \'tensorflow_serving.apis.model_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.ModelSpec)\n  ))\n_sym_db.RegisterMessage(ModelSpec)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/model_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tensorflow_serving/apis/model_service_pb2.py,0,"b""# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/model_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom baseline.tensorflow_serving.apis import get_model_status_pb2 as tensorflow__serving_dot_apis_dot_get__model__status__pb2\nfrom baseline.tensorflow_serving.apis import model_management_pb2 as tensorflow__serving_dot_apis_dot_model__management__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name='tensorflow_serving/apis/model_service.proto',\n  package='tensorflow.serving',\n  syntax='proto3',\n  serialized_pb=_b('\\n+tensorflow_serving/apis/model_service.proto\\x12\\x12tensorflow.serving\\x1a.tensorflow_serving/apis/get_model_status.proto\\x1a.tensorflow_serving/apis/model_management.proto2\\xe7\\x01\\n\\x0cModelService\\x12g\\n\\x0eGetModelStatus\\x12).tensorflow.serving.GetModelStatusRequest\\x1a*.tensorflow.serving.GetModelStatusResponse\\x12n\\n\\x19HandleReloadConfigRequest\\x12\\'.tensorflow.serving.ReloadConfigRequest\\x1a(.tensorflow.serving.ReloadConfigResponseB\\x03\\xf8\\x01\\x01\\x62\\x06proto3')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_get__model__status__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__management__pb2.DESCRIPTOR,])\n\n\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\\370\\001\\001'))\n\n_MODELSERVICE = _descriptor.ServiceDescriptor(\n  name='ModelService',\n  full_name='tensorflow.serving.ModelService',\n  file=DESCRIPTOR,\n  index=0,\n  options=None,\n  serialized_start=164,\n  serialized_end=395,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name='GetModelStatus',\n    full_name='tensorflow.serving.ModelService.GetModelStatus',\n    index=0,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_get__model__status__pb2._GETMODELSTATUSREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_get__model__status__pb2._GETMODELSTATUSRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name='HandleReloadConfigRequest',\n    full_name='tensorflow.serving.ModelService.HandleReloadConfigRequest',\n    index=1,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_model__management__pb2._RELOADCONFIGREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_model__management__pb2._RELOADCONFIGRESPONSE,\n    options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_MODELSERVICE)\n\nDESCRIPTOR.services_by_name['ModelService'] = _MODELSERVICE\n\n# @@protoc_insertion_point(module_scope)\n"""
baseline/tensorflow_serving/apis/model_service_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom baseline.tensorflow_serving.apis import get_model_status_pb2 as tensorflow__serving_dot_apis_dot_get__model__status__pb2\nfrom baseline.tensorflow_serving.apis import model_management_pb2 as tensorflow__serving_dot_apis_dot_model__management__pb2\n\n\nclass ModelServiceStub(object):\n  """"""ModelService provides methods to query and update the state of the server, e.g.\n  which models/versions are being served.\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.GetModelStatus = channel.unary_unary(\n        \'/tensorflow.serving.ModelService/GetModelStatus\',\n        request_serializer=tensorflow__serving_dot_apis_dot_get__model__status__pb2.GetModelStatusRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_get__model__status__pb2.GetModelStatusResponse.FromString,\n        )\n    self.HandleReloadConfigRequest = channel.unary_unary(\n        \'/tensorflow.serving.ModelService/HandleReloadConfigRequest\',\n        request_serializer=tensorflow__serving_dot_apis_dot_model__management__pb2.ReloadConfigRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_model__management__pb2.ReloadConfigResponse.FromString,\n        )\n\n\nclass ModelServiceServicer(object):\n  """"""ModelService provides methods to query and update the state of the server, e.g.\n  which models/versions are being served.\n  """"""\n\n  def GetModelStatus(self, request, context):\n    """"""Gets status of model. If the ModelSpec in the request does not specify\n    version, information about all versions of the model will be returned. If\n    the ModelSpec in the request does specify a version, the status of only\n    that version will be returned.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def HandleReloadConfigRequest(self, request, context):\n    """"""Reloads the set of served models. The new config supersedes the old one, so if a\n    model is omitted from the new config it will be unloaded and no longer served.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_ModelServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'GetModelStatus\': grpc.unary_unary_rpc_method_handler(\n          servicer.GetModelStatus,\n          request_deserializer=tensorflow__serving_dot_apis_dot_get__model__status__pb2.GetModelStatusRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_get__model__status__pb2.GetModelStatusResponse.SerializeToString,\n      ),\n      \'HandleReloadConfigRequest\': grpc.unary_unary_rpc_method_handler(\n          servicer.HandleReloadConfigRequest,\n          request_deserializer=tensorflow__serving_dot_apis_dot_model__management__pb2.ReloadConfigRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_model__management__pb2.ReloadConfigResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.serving.ModelService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
baseline/tensorflow_serving/apis/predict_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/predict.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nfrom baseline.tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/predict.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n%tensorflow_serving/apis/predict.proto\\x12\\x12tensorflow.serving\\x1a&tensorflow/core/framework/tensor.proto\\x1a#tensorflow_serving/apis/model.proto\\""\\xe2\\x01\\n\\x0ePredictRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12>\\n\\x06inputs\\x18\\x02 \\x03(\\x0b\\x32..tensorflow.serving.PredictRequest.InputsEntry\\x12\\x15\\n\\routput_filter\\x18\\x03 \\x03(\\t\\x1a\\x46\\n\\x0bInputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\""\\xd0\\x01\\n\\x0fPredictResponse\\x12\\x31\\n\\nmodel_spec\\x18\\x02 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12\\x41\\n\\x07outputs\\x18\\x01 \\x03(\\x0b\\x32\\x30.tensorflow.serving.PredictResponse.OutputsEntry\\x1aG\\n\\x0cOutputsEntry\\x12\\x0b\\n\\x03key\\x18\\x01 \\x01(\\t\\x12&\\n\\x05value\\x18\\x02 \\x01(\\x0b\\x32\\x17.tensorflow.TensorProto:\\x02\\x38\\x01\\x42\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow_dot_core_dot_framework_dot_tensor__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_PREDICTREQUEST_INPUTSENTRY = _descriptor.Descriptor(\n  name=\'InputsEntry\',\n  full_name=\'tensorflow.serving.PredictRequest.InputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictRequest.InputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=295,\n  serialized_end=365,\n)\n\n_PREDICTREQUEST = _descriptor.Descriptor(\n  name=\'PredictRequest\',\n  full_name=\'tensorflow.serving.PredictRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.PredictRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'inputs\', full_name=\'tensorflow.serving.PredictRequest.inputs\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'output_filter\', full_name=\'tensorflow.serving.PredictRequest.output_filter\', index=2,\n      number=3, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTREQUEST_INPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=139,\n  serialized_end=365,\n)\n\n\n_PREDICTRESPONSE_OUTPUTSENTRY = _descriptor.Descriptor(\n  name=\'OutputsEntry\',\n  full_name=\'tensorflow.serving.PredictResponse.OutputsEntry\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.key\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.PredictResponse.OutputsEntry.value\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\')),\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=505,\n  serialized_end=576,\n)\n\n_PREDICTRESPONSE = _descriptor.Descriptor(\n  name=\'PredictResponse\',\n  full_name=\'tensorflow.serving.PredictResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.PredictResponse.model_spec\', index=0,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'outputs\', full_name=\'tensorflow.serving.PredictResponse.outputs\', index=1,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_PREDICTRESPONSE_OUTPUTSENTRY, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=368,\n  serialized_end=576,\n)\n\n_PREDICTREQUEST_INPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTREQUEST_INPUTSENTRY.containing_type = _PREDICTREQUEST\n_PREDICTREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_PREDICTREQUEST.fields_by_name[\'inputs\'].message_type = _PREDICTREQUEST_INPUTSENTRY\n_PREDICTRESPONSE_OUTPUTSENTRY.fields_by_name[\'value\'].message_type = tensorflow_dot_core_dot_framework_dot_tensor__pb2._TENSORPROTO\n_PREDICTRESPONSE_OUTPUTSENTRY.containing_type = _PREDICTRESPONSE\n_PREDICTRESPONSE.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_PREDICTRESPONSE.fields_by_name[\'outputs\'].message_type = _PREDICTRESPONSE_OUTPUTSENTRY\nDESCRIPTOR.message_types_by_name[\'PredictRequest\'] = _PREDICTREQUEST\nDESCRIPTOR.message_types_by_name[\'PredictResponse\'] = _PREDICTRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nPredictRequest = _reflection.GeneratedProtocolMessageType(\'PredictRequest\', (_message.Message,), dict(\n\n  InputsEntry = _reflection.GeneratedProtocolMessageType(\'InputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTREQUEST_INPUTSENTRY,\n    __module__ = \'tensorflow_serving.apis.predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest.InputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTREQUEST,\n  __module__ = \'tensorflow_serving.apis.predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictRequest)\n  ))\n_sym_db.RegisterMessage(PredictRequest)\n_sym_db.RegisterMessage(PredictRequest.InputsEntry)\n\nPredictResponse = _reflection.GeneratedProtocolMessageType(\'PredictResponse\', (_message.Message,), dict(\n\n  OutputsEntry = _reflection.GeneratedProtocolMessageType(\'OutputsEntry\', (_message.Message,), dict(\n    DESCRIPTOR = _PREDICTRESPONSE_OUTPUTSENTRY,\n    __module__ = \'tensorflow_serving.apis.predict_pb2\'\n    # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse.OutputsEntry)\n    ))\n  ,\n  DESCRIPTOR = _PREDICTRESPONSE,\n  __module__ = \'tensorflow_serving.apis.predict_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.PredictResponse)\n  ))\n_sym_db.RegisterMessage(PredictResponse)\n_sym_db.RegisterMessage(PredictResponse.OutputsEntry)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n_PREDICTREQUEST_INPUTSENTRY.has_options = True\n_PREDICTREQUEST_INPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n_PREDICTRESPONSE_OUTPUTSENTRY.has_options = True\n_PREDICTRESPONSE_OUTPUTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b(\'8\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/predict_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tensorflow_serving/apis/prediction_service_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/prediction_service.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom baseline.tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom baseline.tensorflow_serving.apis import get_model_metadata_pb2 as tensorflow__serving_dot_apis_dot_get__model__metadata__pb2\nfrom baseline.tensorflow_serving.apis import inference_pb2 as tensorflow__serving_dot_apis_dot_inference__pb2\nfrom baseline.tensorflow_serving.apis import predict_pb2 as tensorflow__serving_dot_apis_dot_predict__pb2\nfrom baseline.tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/prediction_service.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n0tensorflow_serving/apis/prediction_service.proto\\x12\\x12tensorflow.serving\\x1a,tensorflow_serving/apis/classification.proto\\x1a\\x30tensorflow_serving/apis/get_model_metadata.proto\\x1a\\\'tensorflow_serving/apis/inference.proto\\x1a%tensorflow_serving/apis/predict.proto\\x1a(tensorflow_serving/apis/regression.proto2\\xfc\\x03\\n\\x11PredictionService\\x12\\x61\\n\\x08\\x43lassify\\x12).tensorflow.serving.ClassificationRequest\\x1a*.tensorflow.serving.ClassificationResponse\\x12X\\n\\x07Regress\\x12%.tensorflow.serving.RegressionRequest\\x1a&.tensorflow.serving.RegressionResponse\\x12R\\n\\x07Predict\\x12\\"".tensorflow.serving.PredictRequest\\x1a#.tensorflow.serving.PredictResponse\\x12g\\n\\x0eMultiInference\\x12).tensorflow.serving.MultiInferenceRequest\\x1a*.tensorflow.serving.MultiInferenceResponse\\x12m\\n\\x10GetModelMetadata\\x12+.tensorflow.serving.GetModelMetadataRequest\\x1a,.tensorflow.serving.GetModelMetadataResponseB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_classification__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_inference__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_predict__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_regression__pb2.DESCRIPTOR,])\n\n\n\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n\n_PREDICTIONSERVICE = _descriptor.ServiceDescriptor(\n  name=\'PredictionService\',\n  full_name=\'tensorflow.serving.PredictionService\',\n  file=DESCRIPTOR,\n  index=0,\n  options=None,\n  serialized_start=291,\n  serialized_end=799,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name=\'Classify\',\n    full_name=\'tensorflow.serving.PredictionService.Classify\',\n    index=0,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_classification__pb2._CLASSIFICATIONREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_classification__pb2._CLASSIFICATIONRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Regress\',\n    full_name=\'tensorflow.serving.PredictionService.Regress\',\n    index=1,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_regression__pb2._REGRESSIONREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_regression__pb2._REGRESSIONRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Predict\',\n    full_name=\'tensorflow.serving.PredictionService.Predict\',\n    index=2,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_predict__pb2._PREDICTREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_predict__pb2._PREDICTRESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'MultiInference\',\n    full_name=\'tensorflow.serving.PredictionService.MultiInference\',\n    index=3,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_inference__pb2._MULTIINFERENCEREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_inference__pb2._MULTIINFERENCERESPONSE,\n    options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'GetModelMetadata\',\n    full_name=\'tensorflow.serving.PredictionService.GetModelMetadata\',\n    index=4,\n    containing_service=None,\n    input_type=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2._GETMODELMETADATAREQUEST,\n    output_type=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2._GETMODELMETADATARESPONSE,\n    options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_PREDICTIONSERVICE)\n\nDESCRIPTOR.services_by_name[\'PredictionService\'] = _PREDICTIONSERVICE\n\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/prediction_service_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom baseline.tensorflow_serving.apis import classification_pb2 as tensorflow__serving_dot_apis_dot_classification__pb2\nfrom baseline.tensorflow_serving.apis import get_model_metadata_pb2 as tensorflow__serving_dot_apis_dot_get__model__metadata__pb2\nfrom baseline.tensorflow_serving.apis import inference_pb2 as tensorflow__serving_dot_apis_dot_inference__pb2\nfrom baseline.tensorflow_serving.apis import predict_pb2 as tensorflow__serving_dot_apis_dot_predict__pb2\nfrom baseline.tensorflow_serving.apis import regression_pb2 as tensorflow__serving_dot_apis_dot_regression__pb2\n\n\nclass PredictionServiceStub(object):\n  """"""open source marker; do not remove\n  PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Classify = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Classify\',\n        request_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.FromString,\n        )\n    self.Regress = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Regress\',\n        request_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.FromString,\n        )\n    self.Predict = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/Predict\',\n        request_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.FromString,\n        )\n    self.MultiInference = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/MultiInference\',\n        request_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.FromString,\n        )\n    self.GetModelMetadata = channel.unary_unary(\n        \'/tensorflow.serving.PredictionService/GetModelMetadata\',\n        request_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.SerializeToString,\n        response_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.FromString,\n        )\n\n\nclass PredictionServiceServicer(object):\n  """"""open source marker; do not remove\n  PredictionService provides access to machine-learned models loaded by\n  model_servers.\n  """"""\n\n  def Classify(self, request, context):\n    """"""Classify.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Regress(self, request, context):\n    """"""Regress.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Predict(self, request, context):\n    """"""Predict -- provides access to loaded TensorFlow model.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def MultiInference(self, request, context):\n    """"""MultiInference API for multi-headed models.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def GetModelMetadata(self, request, context):\n    """"""GetModelMetadata - provides access to metadata for loaded models.\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_PredictionServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Classify\': grpc.unary_unary_rpc_method_handler(\n          servicer.Classify,\n          request_deserializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_classification__pb2.ClassificationResponse.SerializeToString,\n      ),\n      \'Regress\': grpc.unary_unary_rpc_method_handler(\n          servicer.Regress,\n          request_deserializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_regression__pb2.RegressionResponse.SerializeToString,\n      ),\n      \'Predict\': grpc.unary_unary_rpc_method_handler(\n          servicer.Predict,\n          request_deserializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_predict__pb2.PredictResponse.SerializeToString,\n      ),\n      \'MultiInference\': grpc.unary_unary_rpc_method_handler(\n          servicer.MultiInference,\n          request_deserializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_inference__pb2.MultiInferenceResponse.SerializeToString,\n      ),\n      \'GetModelMetadata\': grpc.unary_unary_rpc_method_handler(\n          servicer.GetModelMetadata,\n          request_deserializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataRequest.FromString,\n          response_serializer=tensorflow__serving_dot_apis_dot_get__model__metadata__pb2.GetModelMetadataResponse.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'tensorflow.serving.PredictionService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
baseline/tensorflow_serving/apis/regression_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: tensorflow_serving/apis/regression.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom baseline.tensorflow_serving.apis import input_pb2 as tensorflow__serving_dot_apis_dot_input__pb2\nfrom baseline.tensorflow_serving.apis import model_pb2 as tensorflow__serving_dot_apis_dot_model__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'tensorflow_serving/apis/regression.proto\',\n  package=\'tensorflow.serving\',\n  syntax=\'proto3\',\n  serialized_pb=_b(\'\\n(tensorflow_serving/apis/regression.proto\\x12\\x12tensorflow.serving\\x1a#tensorflow_serving/apis/input.proto\\x1a#tensorflow_serving/apis/model.proto\\""\\x1b\\n\\nRegression\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x02\\""G\\n\\x10RegressionResult\\x12\\x33\\n\\x0bregressions\\x18\\x01 \\x03(\\x0b\\x32\\x1e.tensorflow.serving.Regression\\""p\\n\\x11RegressionRequest\\x12\\x31\\n\\nmodel_spec\\x18\\x01 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12(\\n\\x05input\\x18\\x02 \\x01(\\x0b\\x32\\x19.tensorflow.serving.Input\\""}\\n\\x12RegressionResponse\\x12\\x31\\n\\nmodel_spec\\x18\\x02 \\x01(\\x0b\\x32\\x1d.tensorflow.serving.ModelSpec\\x12\\x34\\n\\x06result\\x18\\x01 \\x01(\\x0b\\x32$.tensorflow.serving.RegressionResultB\\x03\\xf8\\x01\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[tensorflow__serving_dot_apis_dot_input__pb2.DESCRIPTOR,tensorflow__serving_dot_apis_dot_model__pb2.DESCRIPTOR,])\n\n\n\n\n_REGRESSION = _descriptor.Descriptor(\n  name=\'Regression\',\n  full_name=\'tensorflow.serving.Regression\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'tensorflow.serving.Regression.value\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=138,\n  serialized_end=165,\n)\n\n\n_REGRESSIONRESULT = _descriptor.Descriptor(\n  name=\'RegressionResult\',\n  full_name=\'tensorflow.serving.RegressionResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'regressions\', full_name=\'tensorflow.serving.RegressionResult.regressions\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=167,\n  serialized_end=238,\n)\n\n\n_REGRESSIONREQUEST = _descriptor.Descriptor(\n  name=\'RegressionRequest\',\n  full_name=\'tensorflow.serving.RegressionRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.RegressionRequest.model_spec\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'input\', full_name=\'tensorflow.serving.RegressionRequest.input\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=240,\n  serialized_end=352,\n)\n\n\n_REGRESSIONRESPONSE = _descriptor.Descriptor(\n  name=\'RegressionResponse\',\n  full_name=\'tensorflow.serving.RegressionResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model_spec\', full_name=\'tensorflow.serving.RegressionResponse.model_spec\', index=0,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'result\', full_name=\'tensorflow.serving.RegressionResponse.result\', index=1,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=354,\n  serialized_end=479,\n)\n\n_REGRESSIONRESULT.fields_by_name[\'regressions\'].message_type = _REGRESSION\n_REGRESSIONREQUEST.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_REGRESSIONREQUEST.fields_by_name[\'input\'].message_type = tensorflow__serving_dot_apis_dot_input__pb2._INPUT\n_REGRESSIONRESPONSE.fields_by_name[\'model_spec\'].message_type = tensorflow__serving_dot_apis_dot_model__pb2._MODELSPEC\n_REGRESSIONRESPONSE.fields_by_name[\'result\'].message_type = _REGRESSIONRESULT\nDESCRIPTOR.message_types_by_name[\'Regression\'] = _REGRESSION\nDESCRIPTOR.message_types_by_name[\'RegressionResult\'] = _REGRESSIONRESULT\nDESCRIPTOR.message_types_by_name[\'RegressionRequest\'] = _REGRESSIONREQUEST\nDESCRIPTOR.message_types_by_name[\'RegressionResponse\'] = _REGRESSIONRESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nRegression = _reflection.GeneratedProtocolMessageType(\'Regression\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSION,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.Regression)\n  ))\n_sym_db.RegisterMessage(Regression)\n\nRegressionResult = _reflection.GeneratedProtocolMessageType(\'RegressionResult\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSIONRESULT,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.RegressionResult)\n  ))\n_sym_db.RegisterMessage(RegressionResult)\n\nRegressionRequest = _reflection.GeneratedProtocolMessageType(\'RegressionRequest\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSIONREQUEST,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.RegressionRequest)\n  ))\n_sym_db.RegisterMessage(RegressionRequest)\n\nRegressionResponse = _reflection.GeneratedProtocolMessageType(\'RegressionResponse\', (_message.Message,), dict(\n  DESCRIPTOR = _REGRESSIONRESPONSE,\n  __module__ = \'tensorflow_serving.apis.regression_pb2\'\n  # @@protoc_insertion_point(class_scope:tensorflow.serving.RegressionResponse)\n  ))\n_sym_db.RegisterMessage(RegressionResponse)\n\n\nDESCRIPTOR.has_options = True\nDESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b(\'\\370\\001\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
baseline/tensorflow_serving/apis/regression_pb2_grpc.py,0,b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\n'
baseline/tf/classify/__init__.py,0,b'from baseline.tf.classify.train import *\nfrom baseline.tf.classify.model import *\n'
baseline/tf/classify/model.py,0,"b'import os\nimport copy\nimport logging\nfrom itertools import chain\nimport tensorflow as tf\n\nfrom baseline.tf.embeddings import *\nfrom eight_mile.tf.layers import *\nfrom baseline.version import __version__\n\nfrom baseline.utils import (\n    fill_y,\n    listify,\n    ls_props,\n    read_json,\n    write_json,\n    MAGIC_VARS,\n    MEAD_HUB_MODULES\n)\nfrom baseline.model import ClassifierModel, register_model\nfrom baseline.tf.tfy import (\n    TRAIN_FLAG,\n    reload_embeddings,\n    new_placeholder_dict,\n    tf_device_wrapper,\n    create_session,\n    BaseLayer,\n    TensorDef\n)\n\n\nlogger = logging.getLogger(\'baseline\')\n\n\nclass ClassifierModelBase(tf.keras.Model, ClassifierModel):\n    """"""Base for all baseline implementations of token-based classifiers\n\n    This class provides a loose skeleton around which the baseline models\n    are built.  It is built on the Keras Model base, and fulfills the `ClassifierModel` interface.\n    To override this class, the use would typically override the `create_layers` function which will\n    create and attach all sub-layers of this model into the class, and the `call` function which will\n    give the model some implementation to call on forward.\n    """"""\n    def __init__(self, name=None):\n        """"""Base\n        """"""\n        super().__init__(name=name)\n        self._unserializable = []\n\n    def set_saver(self, saver):\n        self.saver = saver\n\n    def save_values(self, basename):\n        """"""Save tensor files out\n\n        :param basename: Base name of model\n        :return:\n        """"""\n        if not tf.executing_eagerly():\n            self.saver.save(self.sess, basename, write_meta_graph=False)\n        else:\n            self.save_weights(f""{basename}.wgt"")\n\n    def save_md(self, basename):\n        """"""This method saves out a `.state` file containing meta-data from these classes and any info\n        registered by a user-defined derived class as a `property`. Also write the `graph` and `saver` and `labels`\n\n        :param basename:\n        :return:\n        """"""\n        write_json(self._state, \'{}.state\'.format(basename))\n        write_json(self.labels, \'{}.labels\'.format(basename))\n        for key, embedding in self.embeddings.items():\n            if hasattr(embedding, \'save_md\'):\n                embedding.save_md(\'{}-{}-md.json\'.format(basename, key))\n\n    def _record_state(self, embeddings: Dict[str, BaseLayer], **kwargs):\n        embeddings_info = {}\n        for k, v in embeddings.items():\n            embeddings_info[k] = v.__class__.__name__\n\n        blacklist = set(chain(\n            self._unserializable,\n            MAGIC_VARS,\n            embeddings.keys(),\n            (f\'{k}_lengths\' for k in embeddings.keys())\n        ))\n        self._state = {k: v for k, v in kwargs.items() if k not in blacklist}\n        self._state.update({\n            \'version\': __version__,\n            \'module\': self.__class__.__module__,\n            \'class\': self.__class__.__name__,\n            \'embeddings\': embeddings_info,\n            \'hub_modules\': MEAD_HUB_MODULES\n        })\n\n    def save(self, basename, **kwargs):\n        """"""Save meta-data and actual data for a model\n\n        :param basename: (``str``) The model basename\n        :param kwargs:\n        :return: None\n        """"""\n        self.save_md(basename)\n        self.save_values(basename)\n\n    def create_test_loss(self):\n        with tf.name_scope(""test_loss""):\n            loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(self.y, ""float""))\n            all_loss = tf.reduce_mean(loss)\n        return all_loss\n\n    def create_loss(self):\n        """"""The loss function is currently provided here, although this is not a great place for it\n        as it provides a coupling between the model and its loss function.  Just here for convenience at the moment.\n\n        :return:\n        """"""\n        with tf.name_scope(""loss""):\n            loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(self.y, ""float""))\n            all_loss = tf.reduce_mean(loss)\n        return all_loss\n\n    def predict_batch(self, batch_dict):\n\n        """"""This method provides a basic routine to run ""inference"" or predict outputs based on data.\n        It runs the `x` tensor in (`BxT`), and turns dropout off, running the network all the way to a softmax\n        output. You can use this method directly if you have vector input, or you can use the `ClassifierService`\n        which can convert directly from text durign its `transform`.  That method calls this one underneath.\n\n        :param batch_dict: (``dict``) Contains any inputs to embeddings for this model\n        :return: Each outcome as a ``list`` of tuples `(label, probability)`\n        """"""\n\n        batch_dict = self.make_input(batch_dict)\n        if not tf.executing_eagerly():\n            probs = self.sess.run(self.probs, batch_dict)\n        else:\n            probs = tf.nn.softmax(self(batch_dict)).numpy()\n        return probs\n\n    def predict(self, batch_dict, raw=False, dense=False):\n\n        """"""This method provides a basic routine to run ""inference"" or predict outputs based on data.\n        It runs the `x` tensor in (`BxT`), and turns dropout off, running the network all the way to a softmax\n        output. You can use this method directly if you have vector input, or you can use the `ClassifierService`\n        which can convert directly from text durign its `transform`.  That method calls this one underneath.\n\n        :param batch_dict: (``dict``) Contains any inputs to embeddings for this model\n        :return: Each outcome as a ``list`` of tuples `(label, probability)`\n        """"""\n\n        probs = self.predict_batch(batch_dict)\n        if raw and not dense:\n            logger.warning(""Warning: `raw` parameter is deprecated pass `dense=True` to get back values as a single tensor"")\n            dense = True\n        if dense:\n            return probs\n        results = []\n        batchsz = probs.shape[0]\n        for b in range(batchsz):\n            outcomes = [(self.labels[id_i], prob_i) for id_i, prob_i in enumerate(probs[b])]\n            results.append(outcomes)\n        return results\n\n    def make_input(self, batch_dict, train=False):\n        """"""Transform a `batch_dict` into a TensorFlow `feed_dict`\n\n        :param batch_dict: (``dict``) A dictionary containing all inputs to the embeddings for this model\n        :param train: (``bool``) Are we training.  Defaults to False\n        :return:\n        """"""\n        y = batch_dict.get(\'y\', None)\n        if not tf.executing_eagerly():\n            batch_for_model = new_placeholder_dict(train)\n\n            for k in self.embeddings.keys():\n                batch_for_model[""{}:0"".format(k)] = batch_dict[k]\n\n            # Allow us to track a length, which is needed for BLSTMs\n            if self.lengths_key is not None:\n                batch_for_model[self.lengths] = batch_dict[self.lengths_key]\n\n            if y is not None:\n                batch_for_model[self.y] = fill_y(len(self.labels), y)\n\n        else:\n            SET_TRAIN_FLAG(train)\n            batch_for_model = {}\n            for k in self.embeddings.keys():\n                batch_for_model[k] = batch_dict[k]\n\n            # Allow us to track a length, which is needed for BLSTMs\n            if self.lengths_key is not None:\n                batch_for_model[""lengths""] = batch_dict[self.lengths_key]\n\n        return batch_for_model\n\n    def get_labels(self) -> List[str]:\n        """"""Get the string labels back\n\n        :return: labels\n        """"""\n        return self.labels\n\n    @classmethod\n    @tf_device_wrapper\n    def load(cls, basename: str, **kwargs) -> \'ClassifierModelBase\':\n        """"""Reload the model from a graph file and a checkpoint\n\n        The model that is loaded is independent of the pooling and stacking layers, making this class reusable\n        by sub-classes.\n\n        :param basename: The base directory to load from\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *sess* -- An optional tensorflow session.  If not passed, a new session is\n            created\n\n        :return: A restored model\n        """"""\n        _state = read_json(""{}.state"".format(basename))\n        if __version__ != _state[\'version\']:\n            logger.warning(""Loaded model is from baseline version %s, running version is %s"", _state[\'version\'], __version__)\n\n        if not tf.executing_eagerly():\n            _state[\'sess\'] = kwargs.pop(\'sess\', create_session())\n            with _state[\'sess\'].graph.as_default():\n                embeddings_info = _state.pop(\'embeddings\')\n                embeddings = reload_embeddings(embeddings_info, basename)\n                # If there is a kwarg that is the same name as an embedding object that\n                # is taken to be the input of that layer. This allows for passing in\n                # subgraphs like from a tf.split (for data parallel) or preprocessing\n                # graphs that convert text to indices\n                for k in embeddings_info:\n                    if k in kwargs:\n                        _state[k] = kwargs[k]\n                labels = read_json(""{}.labels"".format(basename))\n                model = cls.create(embeddings, labels, **_state)\n                model._state = _state\n                if kwargs.get(\'init\', True):\n                    model.sess.run(tf.compat.v1.global_variables_initializer())\n                model.saver = tf.compat.v1.train.Saver()\n                model.saver.restore(model.sess, basename)\n        else:\n            embeddings_info = _state.pop(\'embeddings\')\n            embeddings = reload_embeddings(embeddings_info, basename)\n            # If there is a kwarg that is the same name as an embedding object that\n            # is taken to be the input of that layer. This allows for passing in\n            # subgraphs like from a tf.split (for data parallel) or preprocessing\n            # graphs that convert text to indices\n            for k in embeddings_info:\n                if k in kwargs:\n                    _state[k] = kwargs[k]\n                # TODO: convert labels into just another vocab and pass number of labels to models.\n            labels = read_json(""{}.labels"".format(basename))\n            model = cls.create(embeddings, labels, **_state)\n            model._state = _state\n            model.load_weights(f""{basename}.wgt"")\n        return model\n\n    @property\n    def lengths_key(self) -> str:\n        return self._lengths_key\n\n    @lengths_key.setter\n    def lengths_key(self, value: str):\n        self._lengths_key = value\n\n    @classmethod\n    def create(cls, embeddings: Dict[str, BaseLayer], labels: List[str], **kwargs) -> \'ClassifierModelBase\':\n        """"""The main method for creating all :class:`ClassifierBasedModel` types.\n\n        This method typically instantiates a model with pooling and optional stacking layers.\n        Many of the arguments provided are reused by each implementation, but some sub-classes need more\n        information in order to properly initialize.  For this reason, the full list of keyword args are passed\n        to the :method:`pool` and :method:`stacked` methods.\n\n        :param embeddings: This is a dictionary of embeddings, mapped to their numerical indices in the lookup table\n        :param labels: This is a list of the `str` labels\n        :param kwargs: There are sub-graph specific Keyword Args allowed for e.g. embeddings. See below for known args:\n\n        :Keyword Arguments:\n        * *gpus* -- (``int``) How many GPUs to split training across.  If called this function delegates to\n            another class `ClassifyParallelModel` which creates a parent graph and splits its inputs across each\n            sub-model, by calling back into this exact method (w/o this argument), once per GPU\n        * *model_type* -- The string name for the model (defaults to `default`)\n        * *sess* -- An optional tensorflow session.  If not passed, a new session is\n            created\n        * *lengths_key* -- (``str``) Specifies which `batch_dict` property should be used to determine the temporal length\n            if this is not set, it defaults to either `word`, or `x` if `word` is also not a feature\n        * *finetune* -- Are we doing fine-tuning of word embeddings (defaults to `True`)\n        * *mxlen* -- The maximum signal (`x` tensor temporal) length (defaults to `100`)\n        * *dropout* -- This indicates how much dropout should be applied to the model when training.\n        * *filtsz* -- This is actually a top-level param due to an unfortunate coupling between the pooling layer\n            and the input, which, for convolution, requires input padding.\n\n        :return: A fully-initialized tensorflow classifier\n        """"""\n        model = cls(name=kwargs.get(\'name\'))\n        #embeddings_ = {}\n        #for k, embedding in embeddings.items():\n        #    embeddings_[k] = embedding #.detached_ref()\n\n        model.lengths_key = kwargs.get(\'lengths_key\')\n\n        if not tf.executing_eagerly():\n\n            inputs = {}\n            if model.lengths_key is not None:\n                model._unserializable.append(model.lengths_key)\n                model.lengths = kwargs.get(\'lengths\', tf.compat.v1.placeholder(tf.int32, [None], name=""lengths""))\n                inputs[\'lengths\'] = model.lengths\n            else:\n                model.lengths = None\n\n        model._record_state(embeddings, **kwargs)\n\n        nc = len(labels)\n        if not tf.executing_eagerly():\n            model.y = kwargs.get(\'y\', tf.compat.v1.placeholder(tf.int32, [None, nc], name=""y""))\n            for k, embedding in embeddings.items():\n                x = kwargs.get(k, embedding.create_placeholder(name=k))\n                inputs[k] = x\n\n            model.sess = kwargs.get(\'sess\', create_session())\n\n        model.pdrop_value = kwargs.get(\'dropout\', 0.5)\n        model.labels = labels\n        model.create_layers(embeddings, **kwargs)\n\n        if not tf.executing_eagerly():\n            model.logits = tf.identity(model(inputs), name=""logits"")\n            model.best = tf.argmax(model.logits, 1, name=""best"")\n            model.probs = tf.nn.softmax(model.logits, name=""probs"")\n        return model\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        """"""This method defines the model itself, and must be overloaded by derived classes\n\n        This function will update `self` with the layers required to execute the `call()` method\n\n        :param embeddings: The input feature indices\n        :param kwargs:\n        :return:\n        """"""\n\n\nclass EmbedPoolStackClassifier(ClassifierModelBase):\n    """"""Provides a simple but effective base for most `ClassifierModel`s\n\n    This class provides a common base for classifiers by identifying and codifying\n    and idiomatic pattern where a typical classifier may be though of as a composition\n    between a stack of embeddings, followed by a pooling operation yielding a fixed length\n    tensor, followed by one or more dense layers, and ultimately, a projection to the output space.\n\n    To provide an useful interface to sub-classes, we override the `create_layers` to provide a hook\n    for each layer identified in this idiom, and leave the implementations up to the sub-class.\n\n    We also fully implement the `call` method.\n\n    """"""\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.pool_model = self.init_pool(self.embeddings.output_dim, **kwargs)\n        self.stack_model = self.init_stacked(self.pool_model.output_dim, **kwargs)\n        self.output_layer = self.init_output(**kwargs)\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n        * *embeddings_name* (``str``) Optional override to Keras default names\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        reduction = kwargs.get(\'embeddings_reduction\', \'concat\')\n        name = kwargs.get(\'embeddings_name\')\n        return EmbeddingsStack(embeddings, self.pdrop_value, reduction=reduction, name=name)\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce a pooling operation that will be used in the model\n\n        :param input_dim: The input dimension size\n        :param kwargs:\n        :return: A pooling operation\n        """"""\n\n    def init_stacked(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce a stacking operation that will be used in the model\n\n        :param input_dim: The input dimension size\n        :param kwargs: See below\n\n        :keyword arguments:\n\n        * *hsz* (``list``), defaults to nothing, in which case this function is pass-through\n        * *stacked_name* (``str``) Optional override to stacking name\n\n        :return: A stacking operation (or None)\n        """"""\n        hszs = listify(kwargs.get(\'hsz\', []))\n        if not hszs:\n            return PassThru(input_dim)\n        name = kwargs.get(\'stacked_name\')\n        return DenseStack(input_dim, hszs, pdrop_value=self.pdrop_value, name=name)\n\n    def init_output(self, **kwargs):\n        """"""Provide a projection from the encoder output to the number of labels\n\n        This projection typically will not include any activation, since its output is the logits that\n        the decoder is built on\n\n        :param kwargs: See below\n\n        :keyword arguments:\n        * *output_name* (``str``) Optional override to default Keras layer name\n        :return: A projection from the encoder output size to the final number of labels\n        """"""\n        name = kwargs.get(\'output_name\')\n        return tf.keras.layers.Dense(len(self.labels), name=name)\n\n    def call(self, inputs: Dict[str, TensorDef]) -> TensorDef:\n        """"""Forward execution of the model.  Sub-classes typically shouldnt need to override\n\n        :param inputs: An input dictionary containing the features and the primary key length\n        :return: A tensor\n        """"""\n        lengths = inputs.get(""lengths"")\n        embedded = self.embeddings(inputs)\n        embedded = (embedded, lengths)\n        pooled = self.pool_model(embedded)\n        stacked = self.stack_model(pooled)\n        return self.output_layer(stacked)\n\n\n@register_model(task=\'classify\', name=\'default\')\nclass ConvModel(EmbedPoolStackClassifier):\n    """"""Current default model for `baseline` classification.  Parallel convolutions of varying receptive field width\n    """"""\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Do parallel convolutional filtering with varied receptive field widths, followed by max-over-time pooling\n\n        :param input_dim: Embedding output size\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *cmotsz* -- (``int``) The number of convolutional feature maps for each filter\n            These are MOT-filtered, leaving this # of units per parallel filter\n        * *filtsz* -- (``list``) This is a list of filter widths to use\n        * *pool_name* -- (``str``) Optional name to override default Keras layer name\n        :return: A pooling layer\n        """"""\n        cmotsz = kwargs[\'cmotsz\']\n        filtsz = kwargs[\'filtsz\']\n        name = kwargs.get(\'pool_name\')\n        return WithoutLength(WithDropout(ParallelConv(input_dim, cmotsz, filtsz, name=name), self.pdrop_value))\n\n\n@register_model(task=\'classify\', name=\'lstm\')\nclass LSTMModel(EmbedPoolStackClassifier):\n    """"""A simple single-directional single-layer LSTM. No layer-stacking.\n    """"""\n\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self._vdrop = None\n\n    @property\n    def vdrop(self):\n        return self._vdrop\n\n    @vdrop.setter\n    def vdrop(self, value):\n        self._vdrop = value\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""LSTM with dropout yielding a final-state as output\n\n        :param input_dim: The input word embedding depth\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *rnnsz* -- (``int``) The number of hidden units (defaults to `hsz`)\n        * *rnntype/rnn_type* -- (``str``) The RNN type, defaults to `lstm`, other valid values: `blstm`\n        * *hsz* -- (``int``) backoff for `rnnsz`, typically a result of stacking params.  This keeps things simple so\n          its easy to do things like residual connections between LSTM and post-LSTM stacking layers\n        * *pool_name* -- (``str``) Optional name to override default Keras layer name\n\n        :return: A pooling layer\n        """"""\n        hsz = kwargs.get(\'rnnsz\', kwargs.get(\'hsz\', 100))\n        vdrop = bool(kwargs.get(\'variational\', False))\n        if type(hsz) is list:\n            hsz = hsz[0]\n\n        rnntype = kwargs.get(\'rnn_type\', kwargs.get(\'rnntype\', \'lstm\'))\n        nlayers = int(kwargs.get(\'layers\', 1))\n        name = kwargs.get(\'pool_name\')\n        if rnntype == \'blstm\':\n            return BiLSTMEncoderHidden(None, hsz, nlayers, self.pdrop_value, vdrop, name=name)\n        return LSTMEncoderHidden(None, hsz, nlayers, self.pdrop_value, vdrop, name=name)\n\n\nclass NBowModelBase(EmbedPoolStackClassifier):\n    """"""Neural Bag-of-Words Model base class.  Defines stacking of fully-connected layers, but leaves pooling to derived\n    """"""\n\n    def init_stacked(self, **kwargs):\n        """"""Produce a stacking operation that will be used in the model, defaulting to a single layer\n\n        :param input_dim: The input dimension size\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *hsz* -- (``List[int]``) The number of hidden units (defaults to 100)\n        * *stacked_name* -- (``str``) Optional name to override default Keras layer name\n        """"""\n        kwargs.setdefault(\'hsz\', [100])\n        return super().stacked(**kwargs)\n\n\n@register_model(task=\'classify\', name=\'nbow\')\nclass NBowModel(NBowModelBase):\n    """"""Neural Bag-of-Words average pooling (standard) model""""""\n\n    def init_pool(self, input_dim: int, **kwargs):\n        """"""Do average pooling on input embeddings, yielding a `dsz` output layer\n\n        :param input_dim: The word embedding depth\n        :param kwargs: See below\n\n        :keyword arguments:\n\n        * *pool_name* -- (``str``) Optional name to override default Keras layer name\n\n\n        :return: The average pooling representation\n        """"""\n        name = kwargs.get(\'pool_name\')\n        return MeanPool1D(input_dim, name=name)\n\n\n@register_model(task=\'classify\', name=\'nbowmax\')\nclass NBowMaxModel(NBowModelBase):\n    """"""Max-pooling model for Neural Bag-of-Words.  Sometimes does better than avg pooling\n    """"""\n\n    def init_pool(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Do max pooling on input embeddings, yielding a `dsz` output layer\n\n        :param input_dim: The word embedding depth\n        :param kwargs: See below\n\n        :keyword arguments:\n        * *pool_name* -- (``str``) Optional name to override default Keras layer name\n\n        :return: The max pooling representation\n        """"""\n        name = kwargs.get(\'pool_name\')\n        return WithoutLength(tf.keras.layers.GlobalMaxPooling1D(name=name))\n\n\n@register_model(task=\'classify\', name=\'fine-tune\')\nclass FineTuneModelClassifier(ClassifierModelBase):\n    """"""Fine-tune based on pre-pooled representations""""""\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n        * *embeddings_name* (``str``) Optional override to Keras default names\n        * *embeddings_dropout* (``float``) how much dropout post-reduction (defaults to 0.0)\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        reduction = kwargs.get(\'embeddings_reduction\', \'concat\')\n        embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n        name = kwargs.get(\'embeddings_name\')\n        return EmbeddingsStack(embeddings, embeddings_dropout, reduction=reduction, name=name)\n\n    def init_stacked(self, input_dim: int, **kwargs) -> BaseLayer:\n        """"""Produce a stacking operation that will be used in the model\n\n        :param input_dim: The input dimension size\n        :param kwargs: See below\n\n        :keyword arguments:\n\n        * *hsz* (``list``), defaults to nothing, in which case this function is pass-through\n        * *stacked_name* (``str``) Optional override to stacking name\n\n        :return: A stacking operation (or None)\n        """"""\n        hszs = listify(kwargs.get(\'hsz\', []))\n        if not hszs:\n            return PassThru(input_dim)\n        name = kwargs.get(\'stacked_name\')\n        return DenseStack(input_dim, hszs, pdrop_value=self.pdrop_value, name=name)\n\n    def init_output(self, **kwargs):\n        """"""Provide a projection from the encoder output to the number of labels\n\n        This projection typically will not include any activation, since its output is the logits that\n        the decoder is built on\n\n        :param kwargs: See below\n\n        :keyword arguments:\n        * *output_name* (``str``) Optional override to default Keras layer name\n        :return: A projection from the encoder output size to the final number of labels\n        """"""\n        name = kwargs.get(\'output_name\')\n        return tf.keras.layers.Dense(len(self.labels), name=name)\n\n    def create_layers(self, embeddings, **kwargs):\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.stack_model = self.init_stacked(self.embeddings.output_dim, **kwargs)\n        self.output_layer = self.init_output(**kwargs)\n\n    def call(self, inputs):\n        base_layers = self.embeddings(inputs)\n        stacked = self.stack_model(base_layers)\n        return self.output_layer(stacked)\n\n\n@register_model(task=\'classify\', name=\'composite\')\nclass CompositePoolingModel(EmbedPoolStackClassifier):\n    """"""Fulfills pooling contract by aggregating pooling from a set of sub-models and concatenates each\n    """"""\n\n    def pool(self, dsz, **kwargs):\n        """"""Cycle each sub-model and call its pool method, then concatenate along final dimension\n\n        :param word_embeddings: The input graph\n        :param dsz: The number of input units\n        :param init: The initializer operation\n        :param kwargs:\n        :return: A pooled composite output\n        """"""\n        SubModels = [eval(model) for model in kwargs.get(\'sub\')]\n        models = []\n        for SubClass in SubModels:\n            models.append(SubClass.pool(self, dsz, **kwargs))\n        return CompositeModel(models)\n'"
baseline/tf/classify/train.py,0,"b'""""""Train a classifier with TensorFlow\n\nThis module supports several different ways of training a model\n\n1. feed_dict\n2. datasets (`default` default for non-eager)\n3. eager mode (`default` for eager)\n4. distributed eager mode (`distributed`)\n""""""\nfrom baseline.tf.classify.training import *\n'"
baseline/tf/lm/__init__.py,0,b'from baseline.tf.lm.train import *\nfrom baseline.tf.lm.model import *'
baseline/tf/lm/model.py,0,"b'""""""Language model baselines in TensorFlow\n""""""\nfrom itertools import chain\nfrom baseline.tf.tfy import *\nfrom baseline.version import __version__\nfrom baseline.model import LanguageModel, register_model\nfrom baseline.tf.embeddings import *\nfrom baseline.tf.tfy import new_placeholder_dict, TRAIN_FLAG, lstm_cell_w_dropout\nfrom baseline.utils import read_json, write_json, MAGIC_VARS\n\n\nclass LanguageModelBase(tf.keras.Model, LanguageModel):\n    """"""Base for all baseline implementations of LMs\n\n    This class provides a loose skeleton around which the baseline models\n    are built.  This essentially consists of dividing up the network into a logical separation between ""embedding"",\n    or composition of lookup tables to build a vector representation of a temporal input, ""decoding"",\n    or the conversion of temporal data to a decoded representation, and ""output"" --\n    a projection to output space and a softmax\n    """"""\n    def __init__(self):\n        """"""Construct a base LM\n        """"""\n        super().__init__()\n        self.saver = None\n        self.hsz = None\n        self.probs = None\n        self._unserializable = []\n\n    def save_values(self, basename):\n        """"""Save tensor files out\n\n        :param basename: Base name of model\n        :return:\n        """"""\n        if not tf.executing_eagerly():\n            self.saver.save(self.sess, basename, write_meta_graph=False)\n        else:\n            self.save_weights(f""{basename}.wgt"")\n\n    def save_md(self, basename):\n        """"""This method saves out a `.state` file containing meta-data from these classes and any info\n        registered by a user-defined derived class as a `property`. Also write the `graph` and `saver` and `labels`\n\n        :param basename:\n        :return:\n        """"""\n\n        write_json(self._state, basename + \'.state\')\n        for key, embedding in self.embeddings.items():\n            embedding.save_md(basename + \'-{}-md.json\'.format(key))\n\n    def _record_state(self, embeddings, **kwargs):\n        """"""\n        First, write out the embedding names, so we can recover those.  Then do a deepcopy on the model init params\n        so that it can be recreated later.  Anything that is a placeholder directly on this model needs to be removed\n\n        :param kwargs:\n        :return:\n        """"""\n        embeddings_info = {}\n        for k, v in embeddings.items():\n            embeddings_info[k] = v.__class__.__name__\n\n        blacklist = set(chain(self._unserializable, MAGIC_VARS, embeddings.keys()))\n        self._state = {k: v for k, v in kwargs.items() if k not in blacklist}\n        self._state.update({\n            \'version\': __version__,\n            \'module\': self.__class__.__module__,\n            \'class\': self.__class__.__name__,\n            \'embeddings\': embeddings_info,\n        })\n\n    def set_saver(self, saver):\n        """"""Connect a `tf.Saver` to the model\n\n        :param saver: A saver\n        :return: None\n        """"""\n        self.saver = saver\n\n    def save(self, basename):\n        """"""Save the model\n\n        :param basename: The model prefix\n        :return:\n        """"""\n        self.save_md(basename)\n        self.save_values(basename)\n\n    def _create_loss(self, scope):\n        with tf.compat.v1.variable_scope(scope):\n            targets = tf.reshape(self.y, [-1])\n            bt_x_v = tf.nn.log_softmax(tf.reshape(self.logits, [-1, self.vsz]), axis=-1)\n            one_hots = tf.one_hot(targets, self.vsz)\n            example_loss = -tf.reduce_sum(one_hots * bt_x_v, axis=-1)\n            loss = tf.reduce_mean(example_loss)\n            return loss\n\n    def create_loss(self):\n        """"""Create training loss operator\n\n        :return: loss\n        """"""\n        return self._create_loss(scope=\'loss\')\n\n    def create_test_loss(self):\n        """"""Create test loss operator\n\n        :return: loss\n        """"""\n        return self._create_loss(scope=\'test_loss\')\n\n    def make_input(self, batch_dict, train=False):\n        """"""When we are running with `DataFeed`s, need to transform to `feed_dict`s\n\n        :param batch_dict: The batch for a step\n        :param train: (`bool`) Are we training (or evaluating)?\n        :return: A `feed_dict`\n        """"""\n        if not tf.executing_eagerly():\n            batch_dict_for_model = new_placeholder_dict(train)\n\n            for key in self.src_keys:\n                batch_dict_for_model[""{}:0"".format(key)] = batch_dict[key]\n\n            y = batch_dict.get(\'y\')\n            if y is not None:\n                batch_dict_for_model[self.y] = batch_dict[\'y\']\n\n        else:\n            SET_TRAIN_FLAG(train)\n\n            batch_dict_for_model = {}\n            for key in self.src_keys:\n                batch_dict_for_model[key] = batch_dict[key]\n\n        return batch_dict_for_model\n\n    def predict(self, batch_dict):\n        """"""Do prediction from a `batch_dict`\n\n        :param batch_dict: A step of data\n        :return: The softmax output for this step\n        """"""\n        batch_dict = self.make_input(batch_dict)\n        if not tf.executing_eagerly():\n            step_softmax = self.sess.run(self.probs, batch_dict)\n        else:\n            # FIXME: This is not really the proper handling for eager mode\n            # We want to be able to pass in the last hidden state and emit the current one right?\n            step_softmax = tf.nn.softmax(self(batch_dict, None)[0])\n\n        return step_softmax\n\n    @classmethod\n    def create(cls, embeddings, **kwargs):\n        """"""Create the language model\n\n        :param embeddings: A set of embeddings used\n        :param kwargs: see below\n\n        :Keyword Arguments:\n\n        * *tgt_key* (`str`) -- Which vocabulary is the destination vocabulary\n          (for example, you might have character inputs, or character + word inputs.  The outputs need to be specified)\n        * *sess* (`tf.compat.v1.Session`) -- Optionally, pass in a session (or one will be created)\n        * *pdrop* (`float`) -- The dropout probability\n        * *y* -- Optional target.  If this is not passed in, a placeholder gets created\n        * *hsz* (`int`) -- Number of hidden units per layers\n        * *unif* (`float`) -- set the weights initializer to small random uniform values\n\n        :return: The created model\n        """"""\n        lm = cls()\n        lm.src_keys = kwargs.get(\'src_keys\', embeddings.keys())\n        lm.tgt_key = kwargs.get(\'tgt_key\')\n        if lm.tgt_key is None:\n            raise Exception(\'Need a `tgt_key` to know which source vocabulary should be used for destination\')\n\n        lm._unserializable.append(lm.tgt_key)\n        lm._record_state(embeddings, **kwargs)\n        inputs = {}\n\n        if not tf.executing_eagerly():\n\n            for k, embedding in embeddings.items():\n                x = kwargs.get(k, embedding.create_placeholder(name=k))\n                inputs[k] = x\n\n            lm.y = kwargs.get(\'y\', tf.compat.v1.placeholder(tf.int32, [None, None], name=""y""))\n            lm.sess = kwargs.get(\'sess\', tf.compat.v1.Session())\n        lm.create_layers(embeddings, **kwargs)\n\n        if not tf.executing_eagerly():\n            if lm.requires_state:\n                lm.zero_state(inputs)\n                lm.logits, lm.final_state = lm(inputs, lm.initial_state)\n            else:\n                lm.logits, _ = lm(inputs, None)\n            lm.probs = tf.nn.softmax(lm.logits, name=""softmax"")\n\n        return lm\n\n    def call(self, inputs: Dict[str, TensorDef], hidden: TensorDef) -> Tuple[TensorDef, TensorDef]:\n        """"""Take the input and produce the best path of labels out\n\n        :param inputs: The feature indices for the input\n        :return: The output and hidden units\n        """"""\n\n    def create_layers(self, embeddings, **kwargs):\n        """"""This method defines the model itself, and must be overloaded by derived classes\n\n        This function will update `self` with the layers required to execute the `call()` method\n\n        :param embeddings: The input feature indices\n        :param kwargs:\n        :return:\n        """"""\n\n    @classmethod\n    def load(cls, basename, **kwargs):\n        """"""Reload the model from a graph file and a checkpoint\n\n        The model that is loaded is independent of the pooling and stacking layers, making this class reusable\n        by sub-classes.\n\n        :param basename: The base directory to load from\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *sess* -- An optional tensorflow session.  If not passed, a new session is\n            created\n\n        :return: A restored model\n        """"""\n        _state = read_json(basename + \'.state\')\n\n        if not tf.executing_eagerly():\n            _state[\'sess\'] = kwargs.pop(\'sess\', create_session())\n            embeddings_info = _state.pop(""embeddings"")\n\n            with _state[\'sess\'].graph.as_default():\n                embeddings = reload_embeddings(embeddings_info, basename)\n                for k in embeddings_info:\n                    if k in kwargs:\n                        _state[k] = kwargs[k]\n\n                _state[\'model_type\'] = kwargs.get(\'model_type\', \'default\')\n                model = cls.create(embeddings, **_state)\n                model._state = _state\n\n                do_init = kwargs.get(\'init\', True)\n                if do_init:\n                    init = tf.compat.v1.global_variables_initializer()\n                    model.sess.run(init)\n\n                model.saver = tf.compat.v1.train.Saver()\n                model.saver.restore(model.sess, basename)\n        else:\n            _state = read_json(basename + \'.state\')\n            _state[\'model_type\'] = kwargs.get(\'model_type\', \'default\')\n            embeddings = {}\n            embeddings_dict = _state.pop(""embeddings"")\n\n            for key, class_name in embeddings_dict.items():\n                md = read_json(\'{}-{}-md.json\'.format(basename, key))\n                embed_args = dict({\'vsz\': md[\'vsz\'], \'dsz\': md[\'dsz\']})\n                Constructor = eval(class_name)\n                embeddings[key] = Constructor(key, **embed_args)\n\n            model = cls.create(embeddings, **_state)\n            model._state = _state\n            model.load_weights(f""{basename}.wgt"")\n\n        return model\n\n    @property\n    def requires_state(self):\n        pass\n\n\nclass AbstractGeneratorModel(LanguageModelBase):\n\n    def create_layers(self, embeddings, **kwargs):\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.embeddings_proj = self.init_embeddings_proj(**kwargs)\n        self.generator = self.init_generate(**kwargs)\n        self.output_layer = self.init_output(embeddings, **kwargs)\n\n    def call(self, inputs: Dict[str, TensorDef], hidden: TensorDef) -> Tuple[TensorDef, TensorDef]:\n        emb = self.embed(inputs)\n        output, hidden = self.generate(emb, hidden)\n        return self.output_layer(output), hidden\n\n    def embed(self, input):\n        embedded_dropout = self.embeddings(input)\n        return self.embeddings_proj(embedded_dropout)\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n\n        :Keyword Arguments: See below\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n        * *embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        reduction = kwargs.get(\'embeddings_reduction\', \'concat\')\n        embeddings_dropout = float(kwargs.get(\'embeddings_dropout\', 0.0))\n        return EmbeddingsStack({k: embeddings[k] for k in self.src_keys}, embeddings_dropout, reduction=reduction)\n\n    def init_embeddings_proj(self, **kwargs):\n        input_sz = self.embeddings.output_dim\n        hsz = kwargs.get(\'hsz\', kwargs.get(\'d_model\'))\n        if hsz != input_sz:\n            proj = tf.keras.layers.Dense(hsz)\n            print(\'Applying a transform from {} to {}\'.format(input_sz, hsz))\n        else:\n            proj = PassThru(hsz)\n        return proj\n\n    def init_generate(self, **kwargs):\n        pass\n\n    def generate(self, emb, hidden):\n        return self.generator((emb, hidden))\n\n    def init_output(self, embeddings, **kwargs):\n        self.vsz = embeddings[self.tgt_key].get_vsz()\n        do_weight_tying = bool(kwargs.get(\'tie_weights\', False))\n        output_bias = kwargs.get(\'output_bias\', False)\n        if do_weight_tying:\n            output = WeightTieDense(embeddings[self.tgt_key], use_bias=output_bias)\n        else:\n            output = tf.keras.layers.Dense(self.vsz)\n        return output\n\n\n@register_model(task=\'lm\', name=\'default\')\nclass RNNLanguageModel(AbstractGeneratorModel):\n    """"""RNN-based Language Model built on base class\n    """"""\n    def __init__(self):\n        """"""Construct an RNNLM\n        """"""\n        super().__init__()\n        self.rnntype = \'lstm\'\n        self.initial_state = None\n\n    @property\n    def requires_state(self):\n        return True\n\n    def zero_state(self, inputs):\n        batchsz = get_shape_as_list(inputs[self.src_keys[0]])[0]\n        self.initial_state = self.generator.layer.zero_state(batchsz)\n\n    def init_generate(self, **kwargs):\n        """"""LSTM-based method for decoding\n\n        :param inputs: The outputs of the embeddings\n        :param kwargs: See above\n\n        :return: The layer\n        """"""\n        pdrop = float(kwargs.get(\'dropout\', 0.5))\n        layers = kwargs.get(\'layers\', kwargs.get(\'num_layers\', 1))\n        self.hsz = kwargs.get(\'hsz\', kwargs.get(\'d_model\'))\n        return WithDropoutOnFirst(LSTMEncoderWithState(self.hsz, self.hsz, layers, pdrop, batch_first=True),\n                                  pdrop,\n                                  kwargs.get(\'variational\', False))\n\n\n@register_model(task=\'lm\', name=\'transformer\')\nclass TransformerLanguageModel(AbstractGeneratorModel):\n    """"""Transformer-based Language Model built on base class\n    """"""\n    def __init__(self):\n        """"""Construct an TLM\n        """"""\n        super().__init__()\n\n    @property\n    def requires_state(self):\n        return False\n\n    def init_generate(self, **kwargs):\n        pdrop = float(kwargs.get(\'dropout\', 0.1))\n        layers = kwargs.get(\'layers\', kwargs.get(\'num_layers\', 1))\n        d_model = int(kwargs.get(\'d_model\', kwargs.get(\'hsz\')))\n        num_heads = kwargs.get(\'num_heads\', 4)\n        d_ff = int(kwargs.get(\'d_ff\', 4 * d_model))\n        rpr_k = kwargs.get(\'rpr_k\')\n        d_k = kwargs.get(\'d_k\')\n        scale = bool(kwargs.get(\'scale\', True))\n        activation = kwargs.get(\'activation\', \'gelu\')\n        layer_norm_eps = kwargs.get(\'layer_norm_eps\', 1e-12)\n        layer_norms_after = kwargs.get(\'layer_norms_after\', False)\n        return TransformerEncoderStack(num_heads, d_model=d_model, pdrop=pdrop, scale=scale,\n                                       layers=layers, d_ff=d_ff, rpr_k=rpr_k, d_k=d_k,\n                                       activation=activation, layer_norm_eps=layer_norm_eps,\n                                       layer_norms_after=layer_norms_after)\n\n    def create_mask(self, bth):\n        max_seqlen = get_shape_as_list(bth)[1]\n        mask = subsequent_mask(max_seqlen)\n        return mask\n\n    def generate(self, bth, _):\n        mask = self.create_mask(bth)\n        return self.generator((bth, mask)), None\n\n\n@register_model(task=\'lm\', name=\'transformer-mlm\')\nclass TransformerMaskedLanguageModel(TransformerLanguageModel):\n\n    def create_mask(self, bth):\n        nctx = get_shape_as_list(bth)[1]\n        return tf.fill([1, 1, nctx, nctx], 1.)\n\n'"
baseline/tf/lm/train.py,0,b'from baseline.tf.lm.training import *'
baseline/tf/seq2seq/__init__.py,0,b'from baseline.tf.seq2seq.model import *\nfrom baseline.tf.seq2seq.train import *\nfrom baseline.tf.seq2seq.encoders import *\nfrom baseline.tf.seq2seq.decoders import *\n'
baseline/tf/seq2seq/model.py,0,"b'import logging\nfrom itertools import chain\nfrom baseline.tf.seq2seq.encoders import *\nfrom baseline.tf.seq2seq.decoders import *\nfrom baseline.tf.tfy import *\nfrom baseline.model import EncoderDecoderModel, register_model, create_seq2seq_decoder, create_seq2seq_encoder, create_seq2seq_arc_policy\nfrom baseline.utils import ls_props, read_json, MAGIC_VARS\nfrom baseline.tf.embeddings import *\nfrom baseline.version import __version__\n\nlogger = logging.getLogger(\'baseline\')\n\n\ndef _temporal_cross_entropy_loss(logits, labels, label_lengths, mx_seq_length):\n    """"""Do cross-entropy loss accounting for sequence lengths\n\n    :param logits: a `Tensor` with shape `[timesteps, batch, timesteps, vocab]`\n    :param labels: an integer `Tensor` with shape `[batch, timesteps]`\n    :param label_lengths: The actual length of the target text.  Assume right-padded\n    :param mx_seq_length: The maximum length of the sequence\n    :return:\n    """"""\n\n    # The labels actual length is 100, and starts with <GO>\n    # labels = tf.Print(labels, [tf.shape(labels)], message=""Label Shape: "")\n    # logits = tf.Print(logits, [tf.shape(logits)], message=""Logits Shape: "")\n    labels = tf.transpose(labels, perm=[1, 0])\n    # labels = tf.Print(labels, [tf.shape(labels)], message=""Label.T Shape: "")\n    # TxB loss mask\n    labels = labels[:mx_seq_length, :]\n    # labels = tf.Print(labels, [tf.shape(labels)], message=""Label cut Shape: "")\n    logits = logits[:mx_seq_length]\n    # logits = tf.Print(logits, [tf.shape(logits)], message=""logits cut Shape: "")\n    logit_length = tf.to_int32(tf.shape(logits)[0])\n    # logit_length = tf.Print(logit_length, [logit_length], message=\'Length of logits\')\n    timesteps = tf.to_int32(tf.shape(labels)[0])\n    # The labels no longer include <GO> so go is not useful.  This means that if the length was 100 before, the length\n    # of labels is now 99 (and that is the max allowed)\n    pad_size = timesteps - logit_length\n    logits = tf.pad(logits, [[0, pad_size], [0, 0], [0, 0]])\n    #logits = logits[0:mx_seq_length, :, :]\n    with tf.name_scope(""Loss""):\n        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=labels)\n\n        # BxT loss mask\n        loss_mask = tf.cast(tf.sequence_mask(tf.cast(label_lengths, tf.int32), timesteps), tf.float32)\n        # TxB losses * TxB loss_mask\n        losses = losses * tf.transpose(loss_mask, [1, 0])\n\n        losses = tf.reduce_sum(losses)\n        losses /= tf.cast(tf.reduce_sum(label_lengths), tf.float32)\n        return losses\n\nif not tf.executing_eagerly():\n    class EncoderDecoderModelBase(EncoderDecoderModel, tf.keras.Model):\n\n        def __init__(self, name=None):\n            super().__init__(name=name)\n\n        def create_loss(self):\n            with tf.variable_scope(\'loss\'):\n                # We do not want to count <GO> in our assessment, we do want to count <EOS>\n                return _temporal_cross_entropy_loss(self.decoder.preds[:-1, :, :],\n                                                    self.tgt_embedding.x[:, 1:],\n                                                    self.tgt_len - 1,\n                                                    self.mx_tgt_len - 1)\n\n        def create_test_loss(self):\n            with tf.variable_scope(\'test_loss\'):\n                # We do not want to count <GO> in our assessment, we do want to count <EOS>\n                return _temporal_cross_entropy_loss(self.decoder.preds[:-1, :, :],\n                                                    self.tgt_embedding.x[:, 1:],\n                                                    self.tgt_len - 1,\n                                                    self.mx_tgt_len - 1)\n\n        def __init__(self):\n            super().__init__()\n            self.saver = None\n            self._unserializable = [\'src_len\', \'tgt_len\', \'mx_tgt_len\']\n\n        @classmethod\n        @tf_device_wrapper\n        def load(cls, basename, **kwargs):\n            _state = read_json(\'{}.state\'.format(basename))\n            if __version__ != _state[\'version\']:\n                logger.warning(""Loaded model is from baseline version %s, running version is %s"", _state[\'version\'], __version__)\n            if \'predict\' in kwargs:\n                _state[\'predict\'] = kwargs[\'predict\']\n            if \'beam\' in kwargs:\n                _state[\'beam\'] = kwargs[\'beam\']\n            _state[\'sess\'] = kwargs.get(\'sess\', create_session())\n\n            with _state[\'sess\'].graph.as_default():\n\n                src_embeddings_info = _state.pop(\'src_embeddings\')\n                src_embeddings = reload_embeddings(src_embeddings_info, basename)\n                for k in src_embeddings_info:\n                    if k in kwargs:\n                        _state[k] = kwargs[k]\n                tgt_embedding_info = _state.pop(\'tgt_embedding\')\n                tgt_embedding = reload_embeddings(tgt_embedding_info, basename)[\'tgt\']\n\n                model = cls.create(src_embeddings, tgt_embedding, **_state)\n                model._state = _state\n                if kwargs.get(\'init\', True):\n                    model.sess.run(tf.compat.v1.global_variables_initializer())\n                model.saver = tf.compat.v1.train.Saver()\n                #reload_checkpoint(model.sess, basename, [\'OptimizeLoss/\'])\n                model.saver.restore(model.sess, basename)\n                return model\n\n        def embed(self, inputs):\n            """"""This method performs ""embedding"" of the inputs.  The base method here then concatenates along depth\n            dimension to form word embeddings\n\n            :return: A 3-d vector where the last dimension is the concatenated dimensions of all embeddings\n            """"""\n            return self.src_embeddings(inputs)\n\n        def save_md(self, basename):\n            state = {k: v for k, v in self._state.items()}\n\n            write_json(state, \'{}.state\'.format(basename))\n            for key, embedding in self.src_embeddings.items():\n                embedding.save_md(\'{}-{}-md.json\'.format(basename, key))\n            self.tgt_embedding.save_md(\'{}-{}-md.json\'.format(basename, \'tgt\'))\n\n        def _record_state(self, src_embeddings, tgt_embedding, **kwargs):\n            src_embeddings_info = {}\n            for k, v in src_embeddings.items():\n                src_embeddings_info[k] = v.__class__.__name__\n\n            blacklist = set(chain(self._unserializable, MAGIC_VARS, src_embeddings.keys()))\n            self._state = {k: v for k, v in kwargs.items() if k not in blacklist}\n            self._state.update({\n                \'version\': __version__,\n                \'module\': self.__class__.__module__,\n                \'class\': self.__class__.__name__,\n                \'src_embeddings\': src_embeddings_info,\n                \'tgt_embedding\': {\'tgt\': tgt_embedding.__class__.__name__}\n            })\n\n        @classmethod\n        def create(cls, src_embeddings, tgt_embedding, **kwargs):\n            model = cls()\n\n            model._record_state(src_embeddings, tgt_embedding, **kwargs)\n            model.src_embeddings = EmbeddingsStack(src_embeddings)\n            model.tgt_embedding = tgt_embedding\n\n            model.src_len = kwargs.pop(\'src_len\', tf.compat.v1.placeholder(tf.int32, [None], name=""src_len""))\n            model.tgt_len = kwargs.pop(\'tgt_len\', tf.compat.v1.placeholder(tf.int32, [None], name=""tgt_len""))\n            model.mx_tgt_len = kwargs.pop(\'mx_tgt_len\', tf.compat.v1.placeholder(tf.int32, name=""mx_tgt_len""))\n            model.src_lengths_key = kwargs.get(\'src_lengths_key\')\n            model.id = kwargs.get(\'id\', 0)\n            model.sess = kwargs.get(\'sess\', create_session())\n            model.pdrop_value = kwargs.get(\'dropout\', 0.5)\n            model.dropin_value = kwargs.get(\'dropin\', {})\n            model.num_layers = kwargs.get(\'layers\', 1)\n            model.hsz = kwargs[\'hsz\']\n\n            inputs = {k: kwargs.get(k, src_embeddings[k].create_placeholder(k)) for k in src_embeddings.keys()}\n            embed_in = model.embed(inputs)\n            encoder_output = model.encode(embed_in, **kwargs)\n            model.decode(encoder_output, **kwargs)\n            return model\n\n        def set_saver(self, saver):\n            self.saver = saver\n\n        @property\n        def src_lengths_key(self):\n            return self._src_lengths_key\n\n        @src_lengths_key.setter\n        def src_lengths_key(self, value):\n            self._src_lengths_key = value\n\n        def create_encoder(self, **kwargs):\n            return create_seq2seq_encoder(**kwargs)\n\n        def create_decoder(self, **kwargs):\n            return create_seq2seq_decoder(self.tgt_embedding, **kwargs)\n\n        def decode(self, encoder_output, **kwargs):\n            self.decoder = self.create_decoder(pdrop=self.pdrop_value, **kwargs)\n            predict = kwargs.get(\'predict\', False)\n            if predict:\n                self.decoder((encoder_output, self.src_len), **kwargs)\n            else:\n                tgt = kwargs.get(\'tgt\')\n                self.decoder((encoder_output, tgt, self.src_len, self.tgt_len), **kwargs)\n\n        def encode(self, embed_in, **kwargs):\n            self.encoder = self.create_encoder(pdrop=self.pdrop_value, **kwargs)\n            return self.encoder((embed_in, self.src_len))\n\n        def save(self, model_base):\n            self.save_md(model_base)\n            self.saver.save(self.sess, model_base, write_meta_graph=False)\n\n        def predict(self, batch_dict, **kwargs):\n            feed_dict = self.make_input(batch_dict)\n            vec = self.sess.run(self.decoder.best, feed_dict=feed_dict)\n            # Vec is either [T, B] or [T, B, K]\n            if len(vec.shape) == 2:\n                # Add a fake K\n                vec = np.expand_dims(vec, axis=2)\n            # convert to (B x K x T)\n            return vec.transpose(1, 2, 0)\n\n        def step(self, batch_dict):\n            """"""\n            Generate probability distribution over output V for next token\n            """"""\n            feed_dict = self.make_input(batch_dict)\n            x = self.sess.run(self.decoder.probs, feed_dict=feed_dict)\n            return x\n\n        @property\n        def dropin_value(self):\n            return self._dropin_value\n\n        @dropin_value.setter\n        def dropin_value(self, dict_value):\n            self._dropin_value = dict_value\n\n        def drop_inputs(self, key, x, do_dropout):\n            v = self.dropin_value.get(key, 0)\n            if do_dropout and v > 0.0:\n\n                #do_drop = (np.random.random() < v)\n                #if do_drop:\n                #    drop_indices = np.where(x != Offsets.PAD)\n                #    x[drop_indices[0], drop_indices[1]] = Offsets.PAD\n                drop_indices = np.where((np.random.random(x.shape) < v) & (x != Offsets.PAD))\n                x[drop_indices[0], drop_indices[1]] = Offsets.UNK\n            return x\n\n        def make_input(self, batch_dict, train=False):\n            feed_dict = new_placeholder_dict(train)\n\n            for key in self.src_embeddings.keys():\n                feed_dict[""{}:0"".format(key)] = self.drop_inputs(key, batch_dict[key], train)\n\n            if self.src_lengths_key is not None:\n                feed_dict[self.src_len] = batch_dict[self.src_lengths_key]\n\n            tgt = batch_dict.get(\'tgt\')\n            if tgt is not None:\n                feed_dict[self.tgt_embedding.x] = batch_dict[\'tgt\']\n                feed_dict[self.tgt_len] = batch_dict[\'tgt_lengths\']\n                feed_dict[self.mx_tgt_len] = np.max(batch_dict[\'tgt_lengths\'])\n\n            return feed_dict\n\n\n    @register_model(task=\'seq2seq\', name=[\'default\', \'attn\'])\n    class Seq2Seq(EncoderDecoderModelBase):\n\n        def __init__(self):\n            super(Seq2Seq, self).__init__()\n            self._vdrop = False\n\n        @property\n        def vdrop(self):\n            return self._vdrop\n\n        @vdrop.setter\n        def vdrop(self, value):\n            self._vdrop = value\n\n\nelse:\n    logger = logging.getLogger(\'baseline\')\n\n\n    class EncoderDecoderModelBase(tf.keras.Model, EncoderDecoderModel):\n\n        @property\n        def src_lengths_key(self):\n            return self._src_lengths_key\n\n        @src_lengths_key.setter\n        def src_lengths_key(self, value):\n            self._src_lengths_key = value\n\n        def make_input(self, batch_dict: Dict[str, TensorDef], train: bool = False) -> Dict[str, TensorDef]:\n            """"""Transform a `batch_dict` into format suitable for tagging\n            :param batch_dict: (``dict``) A dictionary containing all inputs to the embeddings for this model\n            :param train: (``bool``) Are we training.  Defaults to False\n            :return: A dictionary representation of this batch suitable for processing\n            """"""\n            if not tf.executing_eagerly():\n                batch_for_model = new_placeholder_dict(train)\n\n                for k in self.src_embeddings.keys():\n                    batch_for_model[""{}:0"".format(k)] = self.drop_inputs(k, batch_dict[k], train)\n\n                # Allow us to track a length, which is needed for BLSTMs\n                batch_for_model[self.lengths] = batch_dict[self.lengths_key]\n\n                if self.src_lengths_key is not None:\n                    batch_for_model[self.src_len] = batch_dict[self.src_lengths_key]\n\n                tgt = batch_dict.get(\'tgt\')\n                if tgt is not None:\n                    batch_for_model[self.tgt_embedding.x] = batch_dict[\'tgt\']\n                    batch_for_model[self.tgt_len] = batch_dict[\'tgt_lengths\']\n                    batch_for_model[self.mx_tgt_len] = np.max(batch_dict[\'tgt_lengths\'])\n\n            else:\n                SET_TRAIN_FLAG(train)\n                batch_for_model = {}\n                for k in self.src_embeddings.keys():\n                    batch_for_model[k] = batch_dict[k] #self.drop_inputs(k, batch_dict[k], train)\n\n                if self.src_lengths_key is not None:\n                    batch_for_model[""src_len""] = batch_dict[self.src_lengths_key]\n\n                if \'tgt\' in batch_dict:\n                    tgt = batch_dict[\'tgt\']\n                    batch_for_model[\'dst\'] = tgt[:, :-1]\n                    batch_for_model[\'tgt\'] = tgt[:, 1:]\n\n            return batch_for_model\n\n\n        def drop_inputs(self, key, x, do_dropout):\n            """"""Do dropout on inputs, using the dropout value (or none if not set)\n            This works by applying a dropout mask with the probability given by a\n            value within the `dropin_value: Dict[str, float]`, keyed off the text name\n            of the feature\n            :param key: The feature name\n            :param x: The tensor to drop inputs for\n            :param do_dropout: A `bool` specifying if dropout is turned on\n            :return: The dropped out tensor\n            """"""\n            v = self.dropin_values.get(key, 0)\n            if do_dropout and v > 0.0:\n                drop_indices = np.where((np.random.random(x.shape) < v) & (x != Offsets.PAD))\n                x[drop_indices[0], drop_indices[1]] = Offsets.UNK\n            return x\n\n        def __init__(self, src_embeddings, tgt_embedding, **kwargs):\n            super().__init__()\n            self.beam_sz = kwargs.get(\'beam\', 1)\n            self._unserializable = [\'src_len\', \'tgt_len\', \'mx_tgt_len\']\n\n            # These will get reinitialized as Layers\n            self._record_state(src_embeddings, tgt_embedding, **kwargs)\n            src_dsz = self.init_embed(src_embeddings, tgt_embedding)\n            self.dropin_values = kwargs.get(\'dropin\', {})\n\n            self.encoder = self.init_encoder(src_dsz, **kwargs)\n            self.decoder = self.init_decoder(tgt_embedding, **kwargs)\n            self.src_lengths_key = kwargs.get(\'src_lengths_key\')\n\n        def init_embed(self, src_embeddings, tgt_embedding, **kwargs):\n            """"""This is the hook for providing embeddings.  It takes in a dictionary of `src_embeddings` and a single\n            tgt_embedding` of type `PyTorchEmbedding`\n            :param src_embeddings: (``dict``) A dictionary of PyTorchEmbeddings, one per embedding\n            :param tgt_embedding: (``PyTorchEmbeddings``) A single PyTorchEmbeddings object\n            :param kwargs:\n            :return: Return the aggregate embedding input size\n            """"""\n            self.src_embeddings = EmbeddingsStack(src_embeddings, reduction=kwargs.get(\'embeddings_reduction\', \'concat\'))\n            self.tgt_embedding = tgt_embedding\n\n            return self.src_embeddings.output_dim\n\n        def init_encoder(self, input_sz, **kwargs):\n            # This is a hack since TF never needs this one, there is not a general constructor param, so shoehorn\n            kwargs[\'dsz\'] = input_sz\n            return create_seq2seq_encoder(**kwargs)\n\n        def init_decoder(self, tgt_embedding, **kwargs):\n            return create_seq2seq_decoder(tgt_embedding, **kwargs)\n\n        def encode(self, input, lengths):\n            """"""\n\n            :param input:\n            :param lengths:\n            :return:\n            """"""\n            embed_in_seq = self.embed(input)\n            return self.encoder((embed_in_seq, lengths))\n\n        def decode(self, encoder_outputs, dst):\n            return self.decoder(encoder_outputs, dst)\n\n        def save_md(self, basename):\n            state = {k: v for k, v in self._state.items()}\n\n            write_json(state, \'{}.state\'.format(basename))\n            for key, embedding in self.src_embeddings.items():\n                embedding.save_md(\'{}-{}-md.json\'.format(basename, key))\n            self.tgt_embedding.save_md(\'{}-{}-md.json\'.format(basename, \'tgt\'))\n\n        def _record_state(self, src_embeddings, tgt_embedding, **kwargs):\n            src_embeddings_info = {}\n            for k, v in src_embeddings.items():\n                src_embeddings_info[k] = v.__class__.__name__\n\n            blacklist = set(chain(self._unserializable, MAGIC_VARS, src_embeddings.keys()))\n            self._state = {k: v for k, v in kwargs.items() if k not in blacklist}\n            self._state.update({\n                \'version\': __version__,\n                \'module\': self.__class__.__module__,\n                \'class\': self.__class__.__name__,\n                \'src_embeddings\': src_embeddings_info,\n                \'tgt_embedding\': {\'tgt\': tgt_embedding.__class__.__name__}\n            })\n\n        def save(self, model_base):\n            self.save_md(model_base)\n            self.save_values(model_base)\n\n        def save_values(self, basename):\n            """"""Save tensor files out\n\n            :param basename: Base name of model\n            :return:\n            """"""\n            self.save_weights(f""{basename}.wgt"")\n\n        @classmethod\n        @tf_device_wrapper\n        def load(cls, basename, **kwargs):\n            _state = read_json(\'{}.state\'.format(basename))\n            if __version__ != _state[\'version\']:\n                logger.warning(""Loaded model is from baseline version %s, running version is %s"", _state[\'version\'], __version__)\n            if \'predict\' in kwargs:\n                _state[\'predict\'] = kwargs[\'predict\']\n            if \'beam\' in kwargs:\n                _state[\'beam\'] = kwargs[\'beam\']\n\n            src_embeddings_info = _state.pop(\'src_embeddings\')\n            src_embeddings = reload_embeddings(src_embeddings_info, basename)\n            for k in src_embeddings_info:\n                if k in kwargs:\n                    _state[k] = kwargs[k]\n            tgt_embedding_info = _state.pop(\'tgt_embedding\')\n            tgt_embedding = reload_embeddings(tgt_embedding_info, basename)[\'tgt\']\n\n            model = cls.create(src_embeddings, tgt_embedding, **_state)\n            model._state = _state\n            model.load_weights(f""{basename}.wgt"")\n            return model\n\n        @classmethod\n        def create(cls, src_embeddings, tgt_embedding, **kwargs):\n            model = cls(src_embeddings, tgt_embedding, **kwargs)\n            logger.info(model)\n            return model\n\n        def embed(self, input):\n            return self.src_embeddings(input)\n\n        def call(self, input):\n            src_len = input[\'src_len\']\n            encoder_outputs = self.encode(input, src_len)\n            output = self.decode(encoder_outputs, input[\'dst\'])\n            # Return as B x T x H\n            return output\n\n        def predict(self, inputs, **kwargs):\n            """"""Predict based on the batch.\n\n            If `make_input` is True then run make_input on the batch_dict.\n            This is false for being used during dev eval where the inputs\n            are already transformed.\n            """"""\n            SET_TRAIN_FLAG(False)\n            make = kwargs.get(\'make_input\', True)\n            if make:\n                inputs = self.make_input(inputs)\n\n            encoder_outputs = self.encode(inputs, inputs[\'src_len\'])\n            #outs = self.greedy_decode(encoder_outputs, **kwargs)\n            #return outs\n            outs, lengths, scores = self.decoder.beam_search(encoder_outputs, **kwargs)\n            return outs.numpy()\n\n        def greedy_decode(self, encoder_outputs, **kwargs):\n            mxlen = kwargs.get(\'mxlen\', 100)\n            B = get_shape_as_list(encoder_outputs.output)[0]\n            #log_probs = np.zeros(B)\n            paths = np.full((B, 1, 1), Offsets.GO)\n\n            src_mask = encoder_outputs.src_mask\n            h_i, dec_out, context = self.decoder.arc_policy((encoder_outputs, self.decoder.hsz, 1))\n\n            for i in range(mxlen - 1):\n                """"""Calculate the probs of the next output and update state.""""""\n                # Our RNN decoder is now batch-first, so we need to expand the time dimension\n                last = tf.reshape(paths[:, :, -1], (-1, 1))\n                dec_out, h_i = self.decoder.decode_rnn(context, h_i, dec_out, last, src_mask)\n                probs = self.decoder.output(dec_out)\n                # Collapse over time\n                dec_out = tf.squeeze(dec_out, 1)\n                best = tf.argmax(probs, -1)\n                paths = tf.concat([paths, tf.expand_dims(best, -1)], axis=2)\n\n            return paths[:, :, 1:]\n\n    @register_model(task=\'seq2seq\', name=[\'default\', \'attn\'])\n    class Seq2SeqModel(EncoderDecoderModelBase):\n\n        def __init__(self, src_embeddings, tgt_embedding, **kwargs):\n            """"""This base model is extensible for attention and other uses.  It declares minimal fields allowing the\n            subclass to take over most of the duties for drastically different implementations\n\n            :param src_embeddings: (``dict``) A dictionary of PyTorchEmbeddings\n            :param tgt_embedding: (``PyTorchEmbeddings``) A single PyTorchEmbeddings object\n            :param kwargs:\n            """"""\n            super().__init__(src_embeddings, tgt_embedding, **kwargs)\n'"
baseline/tf/seq2seq/train.py,0,b'from baseline.tf.seq2seq.training import *\n'
baseline/tf/tagger/__init__.py,0,b'from baseline.tf.tagger.model import *\nfrom baseline.tf.tagger.train import *'
baseline/tf/tagger/model.py,0,"b'""""""Provides tagger models for TensorFlow\n""""""\nfrom baseline.model import TaggerModel\nfrom itertools import chain\nfrom baseline.tf.tfy import *\nfrom eight_mile.tf.layers import *\nfrom baseline.utils import ls_props, read_json, write_json, MAGIC_VARS\nfrom baseline.tf.embeddings import *\nfrom baseline.version import __version__\nfrom baseline.model import register_model\nfrom baseline.utils import listify, Offsets\n\nlogger = logging.getLogger(\'baseline\')\n\n\nclass TaggerModelBase(tf.keras.Model, TaggerModel):\n    """"""Base class for tagger models\n\n    This class provides the model base for tagging.  To create a tagger, overload `create_layers()` and `forward()`.\n    Most implementations should be able to subclass the `AbstractEncoderTaggerModel`, which inherits from this and imposes\n    additional structure\n    """"""\n    def __init__(self):\n        """"""Create a tagger, nothing marked as unserializable\n        """"""\n        super().__init__()\n        self._unserializable = []\n        self._lengths_key = None\n        self._dropin_value = None\n        self.saver = None\n\n    @property\n    def lengths_key(self):\n        """"""Property for the name of the field that is used for lengths keying\n\n        :return: (`str`) The name of the field\n        """"""\n        return self._lengths_key\n\n    @lengths_key.setter\n    def lengths_key(self, value):\n        """"""Set the lengths key\n\n        :param value: (`str`) The value to set the lengths key to\n        :return: None\n        """"""\n        self._lengths_key = value\n\n    def save_values(self, basename):\n        """"""Save tensor files out\n\n        :param basename: Base name of model\n        :return:\n        """"""\n        if not tf.executing_eagerly():\n            self.saver.save(self.sess, basename, write_meta_graph=False)\n        else:\n            self.save_weights(f""{basename}.wgt"")\n\n    def save_md(self, basename):\n        """"""\n        This method saves out a `.state` file containing meta-data from these classes and any info\n        registered by a user-defined derived class as a `property`. Also write the `graph` and `saver` and `labels`\n\n        :param basename: The name of the model prefix\n        :return: None\n        """"""\n        write_json(self._state, \'{}.state\'.format(basename))\n        write_json(self.labels, \'{}.labels\'.format(basename))\n        for key, embedding in self.embeddings.items():\n            embedding.save_md(\'{}-{}-md.json\'.format(basename, key))\n\n    def _record_state(self, embeddings, **kwargs):\n        """"""\n        First, write out the embedding names, so we can recover those.  Then do a deepcopy on the model init params\n        so that it can be recreated later.  Anything that is a placeholder directly on this model needs to be removed\n\n        :param kwargs:\n        :return:\n        """"""\n        embeddings_info = {}\n        for k, v in embeddings.items():\n            embeddings_info[k] = v.__class__.__name__\n\n        blacklist = set(chain(\n            self._unserializable,\n            MAGIC_VARS,\n            embeddings.keys(),\n            (f""{k}_lengths"" for k in embeddings.keys())\n        ))\n        self._state = {k: v for k, v in kwargs.items() if k not in blacklist}\n        self._state.update({\n            \'version\': __version__,\n            \'module\': self.__class__.__module__,\n            \'class\': self.__class__.__name__,\n            \'embeddings\': embeddings_info,\n        })\n        if \'constraint_mask\' in kwargs:\n            self._state[\'constraint_mask\'] = True\n\n    @property\n    def dropin_value(self):\n        """"""Dropout on the input as a `Dict[str, float]`, one per embedding (keyed off the feature name)\n\n        :return: `Dict[str, float]` containing this information\n        """"""\n        return self._dropin_value\n\n    @dropin_value.setter\n    def dropin_value(self, dict_value):\n        """"""Set dictionary for dropout on the input values\n\n        :param dict_value: `Dict[str, float]` containing this information\n        :return: None\n        """"""\n        self._dropin_value = dict_value\n\n    def drop_inputs(self, key, x, do_dropout):\n        """"""Do dropout on inputs, using the dropout value (or none if not set)\n        This works by applying a dropout mask with the probability given by a\n        value within the `dropin_value: Dict[str, float]`, keyed off the text name\n        of the feature\n\n        :param key: The feature name\n        :param x: The tensor to drop inputs for\n        :param do_dropout: A `bool` specifying if dropout is turned on\n        :return: The dropped out tensor\n        """"""\n        v = self.dropin_value.get(key, 0)\n        if do_dropout and v > 0.0:\n            drop_indices = np.where((np.random.random(x.shape) < v) & (x != Offsets.PAD))\n            x[drop_indices[0], drop_indices[1]] = Offsets.UNK\n        return x\n\n    def make_input(self, batch_dict: Dict[str, TensorDef], train: bool = False) -> Dict[str, TensorDef]:\n        """"""Transform a `batch_dict` into format suitable for tagging\n\n        :param batch_dict: (``dict``) A dictionary containing all inputs to the embeddings for this model\n        :param train: (``bool``) Are we training.  Defaults to False\n        :return: A dictionary representation of this batch suitable for processing\n        """"""\n        y = batch_dict.get(\'y\', None)\n        if not tf.executing_eagerly():\n            batch_for_model = new_placeholder_dict(train)\n\n            for k in self.embeddings.keys():\n                batch_for_model[""{}:0"".format(k)] = self.drop_inputs(k, batch_dict[k], train)\n\n            # Allow us to track a length, which is needed for BLSTMs\n            batch_for_model[self.lengths] = batch_dict[self.lengths_key]\n\n            if y is not None:\n                batch_for_model[self.y] = y\n        else:\n            SET_TRAIN_FLAG(train)\n            batch_for_model = {}\n            for k in self.embeddings.keys():\n                batch_for_model[k] = self.drop_inputs(k, batch_dict[k], train)\n\n            # Allow us to track a length, which is needed for BLSTMs\n            if self.lengths_key is not None:\n                batch_for_model[""lengths""] = batch_dict[self.lengths_key]\n\n        return batch_for_model\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        """"""This method defines the model itself, and must be overloaded by derived classes\n\n        This function will update `self` with the layers required to execute the `call()` method\n\n        :param embeddings: The input feature indices\n        :param kwargs:\n        :return:\n        """"""\n\n    def save(self, basename: str):\n        """"""Save a model, using the parameter as a prefix\n\n        :param basename: The prefix for the model including the directories and the base of the name to use\n        :return: None\n        """"""\n        self.save_md(basename)\n        self.save_values(basename)\n\n    @classmethod\n    @tf_device_wrapper\n    def load(cls, basename, **kwargs):\n        """"""Reload the model from a graph file and a checkpoint\n        The model that is loaded is independent of the pooling and stacking layers, making this class reusable\n        by sub-classes.\n\n        :param basename: The base directory to load from\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *sess* -- An optional tensorflow session.  If not passed, a new session is\n            created\n\n        :return: A restored model\n        """"""\n        _state = read_json(""{}.state"".format(basename))\n        # FIXME: Somehow not writing this anymore\n        #if __version__ != _state[\'version\']:\n        #    logger.warning(""Loaded model is from baseline version %s, running version is %s"", _state[\'version\'], __version__)\n        if not tf.executing_eagerly():\n\n            _state[\'sess\'] = kwargs.pop(\'sess\', create_session())\n            embeddings_info = _state.pop(""embeddings"")\n\n            with _state[\'sess\'].graph.as_default():\n                embeddings = reload_embeddings(embeddings_info, basename)\n                for k in embeddings_info:\n                    if k in kwargs:\n                        _state[k] = kwargs[k]\n                labels = read_json(""{}.labels"".format(basename))\n                # FIXME: referring to the `constraint_mask` in the base where its not mentioned isnt really clean\n                if _state.get(\'constraint_mask\') is not None:\n                    # Dummy constraint values that will be filled in by the check pointing\n                    _state[\'constraint_mask\'] = [np.zeros((len(labels), len(labels))) for _ in range(2)]\n                model = cls.create(embeddings, labels, **_state)\n                model._state = _state\n                model.create_loss()\n                if kwargs.get(\'init\', True):\n                    model.sess.run(tf.compat.v1.global_variables_initializer())\n                model.saver = tf.compat.v1.train.Saver()\n                model.saver.restore(model.sess, basename)\n        else:\n            embeddings_info = _state.pop(\'embeddings\')\n            embeddings = reload_embeddings(embeddings_info, basename)\n\n            for k in embeddings_info:\n                if k in kwargs:\n                    _state[k] = kwargs[k]\n            # TODO: convert labels into just another vocab and pass number of labels to models.\n            labels = read_json(""{}.labels"".format(basename))\n            # FIXME: referring to the `constraint_mask` in the base where its not mentioned isnt really clean\n            if _state.get(\'constraint_mask\') is not None:\n                # Dummy constraint values that will be filled in by the check pointing\n                _state[\'constraint_mask\'] = [np.zeros((len(labels), len(labels))) for _ in range(2)]\n            model = cls.create(embeddings, labels, **_state)\n            model._state = _state\n            model.load_weights(f""{basename}.wgt"")\n        return model\n\n    def save_using(self, saver):\n        """"""Method to wire up the `tf.Saver`\n\n        :param saver: The `tf.Saver`\n        :return: None\n        """"""\n        self.saver = saver\n\n    def get_labels(self) -> List[str]:\n        """"""Get the labels (names of each class)\n\n        :return: (`List[str]`) The labels\n        """"""\n        return self.labels\n\n    def predict(self, batch_dict: Dict[str, TensorDef]) -> TensorDef:\n        """"""Take in a batch of data, and predict the tags\n\n        :param batch_dict: A `Dict[str, tensor]` that is to be predicted\n        :return: A batch-sized list of predictions\n        """"""\n        lengths = batch_dict[self.lengths_key]\n        batch_dict = self.make_input(batch_dict)\n        if not tf.executing_eagerly():\n            return self.sess.run(self.best, feed_dict=batch_dict)\n        else:\n            return self(batch_dict).numpy()\n\n    def call(self, inputs: Dict[str, TensorDef]) -> TensorDef:\n        """"""Take the input and produce the best path of labels out\n\n        :param inputs: The feature indices for the input\n        :return: The most likely path through the output labels\n        """"""\n\n    @classmethod\n    def create(cls, embeddings, labels, **kwargs) -> \'TaggerModelBase\':\n        """"""Create the model\n        :param embeddings: A `dict` of input embeddings\n        :param labels: The output labels for sequence tagging\n        :param kwargs: See below\n        :Keyword Arguments:\n\n        * *lengths_key* (`str`) -- What is the name of the key that is used to get the full temporal length\n        * *dropout* (`float`) -- The probability of dropout\n        * *dropin* (`Dict[str, float]`) -- For each feature, how much input to dropout\n        * *sess* -- An optional `tf.compat.v1.Session`, if not provided, this will be created\n        * *span_type* -- (`str`) The type of input span\n        * *username* -- (`str`) A username, defaults to the name of the user on this machine\n        * *label* -- (`str`) An optional, human-readable label name.  Defaults to sha1 of this configuration\n        * *variational* -- (`bool`) Should we do variational dropout\n        * *rnntype* -- (`str`) -- The type of RNN (if this is an RNN), defaults to \'blstm\'\n        * *layers* -- (`int`) -- The number of layers to apply on the encoder\n        * *hsz* -- (`int`) -- The number of hidden units for the encoder\n\n        :return: A newly created tagger\n        """"""\n        model = cls()\n\n        model.lengths_key = kwargs.get(\'lengths_key\')\n\n        if not tf.executing_eagerly():\n\n            inputs = {}\n            for k, embedding in embeddings.items():\n                x = kwargs.get(k, embedding.create_placeholder(name=k))\n                inputs[k] = x\n            model._unserializable.append(model.lengths_key)\n            model.lengths = kwargs.get(\'lengths\', tf.compat.v1.placeholder(tf.int32, [None], name=""lengths""))\n            inputs[\'lengths\'] = model.lengths\n            model.y = kwargs.get(\'y\', tf.compat.v1.placeholder(tf.int32, [None, None], name=""y""))\n            model.sess = kwargs.get(\'sess\', create_session())\n\n        model._record_state(embeddings, **kwargs)\n        model.labels = labels\n        # This only exists to make exporting easier\n        model.pdrop_value = kwargs.get(\'dropout\', 0.5)\n        model.dropin_value = kwargs.get(\'dropin\', {})\n\n        model.labels = labels\n        model.span_type = kwargs.get(\'span_type\')\n\n        model.create_layers(embeddings, **kwargs)\n        if not tf.executing_eagerly():\n            model.best = model(inputs)\n        return model\n\n    def create_loss(self):\n        """"""Create the loss function and return it\n\n        :return: The loss function\n        """"""\n\n\nclass AbstractEncoderTaggerModel(TaggerModelBase):\n    """"""Class defining a typical flow for taggers.  Most taggers should extend this class\n\n    This class provides the model base for tagging by providing specific hooks for each phase.  There are\n    4 basic steps identified in this class:\n\n    1. embed\n    2. encode (transduction)\n    3. proj (projection to the final number of labels)\n    4. decode\n\n    There is an `init_* method for each of this phases, allowing you to\n    define and return a custom layer.\n\n    The actual forward method is defined as a combination of these 3 steps, which includes a\n    projection from the encoder output to the number of labels.\n\n    Decoding in taggers refers to the process of selecting the best path through the labels and is typically\n    implemented either as a constrained greedy decoder or as a CRF layer\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def create_layers(self, embeddings: Dict[str, TensorDef], **kwargs):\n        self.embeddings = self.init_embed(embeddings, **kwargs)\n        self.encoder = self.init_encode(**kwargs)\n        self.proj_layer = self.init_proj(**kwargs)\n        self.decoder = self.init_decode(**kwargs)\n\n    def call(self, inputs: Dict[str, TensorDef]) -> TensorDef:\n        """"""Take the input and produce the best path of labels out\n\n        :param inputs: The feature indices for the input\n        :return: The most likely path through the output labels\n        """"""\n        self.probs = self.transduce(inputs)\n        return self.decode(self.probs, inputs.get(""lengths""))\n\n    def create_loss(self):\n        """"""Create the loss function and return it\n\n        :return: The loss function\n        """"""\n        return self.decoder.neg_log_loss(self.probs, self.y, self.lengths)\n\n    def init_embed(self, embeddings: Dict[str, TensorDef], **kwargs) -> BaseLayer:\n        """"""This method creates the ""embedding"" layer of the inputs, with an optional reduction\n\n        :param embeddings: A dictionary of embeddings\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *embeddings_reduction* (defaults to `concat`) An operator to perform on a stack of embeddings\n        * *embeddings_name* (``str``) Optional override to Keras default names\n        :return: The output of the embedding stack followed by its reduction.  This will typically be an output\n          with an additional dimension which is the hidden representation of the input\n        """"""\n        reduction = kwargs.get(\'embeddings_reduction\', \'concat\')\n        name = kwargs.get(\'embeddings_name\')\n        return EmbeddingsStack(embeddings, self.pdrop_value, reduction=reduction, name=name)\n\n    def init_encode(self, **kwargs) -> BaseLayer:\n        """"""Provide a layer object that represents the `encode` phase of the model\n       :param kwargs:\n       :return: The encoder\n       """"""\n\n    def init_proj(self, **kwargs) -> BaseLayer:\n        """"""Provide a projection from the encoder output to the number of labels\n\n        This projection typically will not include any activation, since its output is the logits that\n        the decoder is built on\n\n        :param kwargs: See below\n\n        :keyword arguments:\n        * *proj_name* (``str``) Optional override to default Keras layer name\n        :return: A projection from the encoder output size to the final number of labels\n        """"""\n        name = kwargs.get(\'proj_name\')\n        return tf.keras.layers.Dense(len(self.labels), name=name)\n\n    def init_decode(self, **kwargs) -> BaseLayer:\n        """"""Provide a layer object that represents the `decode` phase of the model\n        This will typically produce a CRF layer, or a greedy decoder\n\n        :param kwargs: See below\n\n        :keyword arguments:\n        * *crf* (``bool``) Is it a CRF?\n        * *constraint_mask* (``bool``) Is there a CRF mask?\n        * *decode_name* (``str``) Optional TF graph name to use, defaults to Keras layer default\n        :return: Some decoder for the model\n        """"""\n        self.crf = bool(kwargs.get(\'crf\', False))\n        self.constraint_mask = kwargs.get(\'constraint_mask\')\n        name = kwargs.get(\'decode_name\')\n        if self.crf:\n            return CRF(len(self.labels), self.constraint_mask, name=name)\n        return TaggerGreedyDecoder(len(self.labels), self.constraint_mask, name=name)\n\n    def transduce(self, inputs: Dict[str, TensorDef]) -> TensorDef:\n        """"""This operation performs embedding of the input, followed by encoding and projection to logits\n\n        :param inputs: The feature indices to embed\n        :return: Transduced (post-encoding) output\n        """"""\n        lengths = inputs[""lengths""]\n        embedded = self.embeddings(inputs)\n        embedded = (embedded, lengths)\n        transduced = self.proj_layer(self.encoder(embedded))\n        return transduced\n\n    def decode(self, tensor: TensorDef, lengths: TensorDef) -> TensorDef:\n        """"""Take in the transduced (encoded) input and decode it\n\n        :param tensor: Transduced input\n        :param lengths: Valid lengths of the transduced input\n        :return: A best path through the output\n        """"""\n        path, self.path_scores = self.decoder((tensor, lengths))\n        return path\n\n\n@register_model(task=\'tagger\', name=\'default\')\nclass RNNTaggerModel(AbstractEncoderTaggerModel):\n    """"""RNN-based tagger implementation: this is the default tagger for mead-baseline\n\n    Overload the encoder, typically as a BiLSTM\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, **kwargs):\n        """"""Override the base method to produce an RNN transducer\n\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *rnntype* (``str``) The type of RNN, defaults to `blstm`\n        * *layers* (``int``) The number of layers to stack\n        * *hsz* (``int``) The number of hidden units for each layer in the encoder\n        * *variational* (``bool``) Variational dropout\n        * *encode_name* (``str``) A Keras layer name to provide to the encoder, default to Keras layer name\n        :return: An encoder\n        """"""\n        self.vdrop = kwargs.get(\'variational\', False)\n        rnntype = kwargs.get(\'rnntype\', \'blstm\')\n        nlayers = int(kwargs.get(\'layers\', 1))\n        hsz = int(kwargs[\'hsz\'])\n        name = kwargs.get(\'encode_name\')\n        Encoder = BiLSTMEncoderSequence if rnntype == \'blstm\' else LSTMEncoderSequence\n        return Encoder(None, hsz, nlayers, self.pdrop_value, self.vdrop, name=name)\n\n    @property\n    def vdrop(self):\n        return self._vdrop\n\n    @vdrop.setter\n    def vdrop(self, value):\n        self._vdrop = value\n\n\n@register_model(task=\'tagger\', name=\'transformer\')\nclass TransformerTaggerModel(AbstractEncoderTaggerModel):\n    """"""Transformer-based tagger model\n\n    Overload the encoder using a length-aware Transformer\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, **kwargs):\n        """"""Override the base method to produce an RNN transducer\n\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *num_heads* (``int``) The number of heads for multi-headed attention\n        * *layers* (``int``) The number of layers to stack\n        * *hsz* (``int``) The number of hidden units for each layer in the encoder\n        * *dropout* (``float``) The dropout rate, defaults\n        * *d_ff* (``int``) The feed-forward layer size\n        * *rpr_k* (``list`` or ``int``) The relative attention sizes.  If its a list, one scalar per layer, if its\n          a scalar, apply same size to each layer\n        :return: An encoder\n        """"""\n        layers = int(kwargs.get(\'layers\', 1))\n        num_heads = int(kwargs.get(\'num_heads\', 4))\n        pdrop = float(kwargs.get(\'dropout\', 0.5))\n        scale = False\n        hsz = int(kwargs[\'hsz\'])\n        rpr_k = kwargs.get(\'rpr_k\', 100)\n        d_ff = kwargs.get(\'d_ff\')\n        encoder = TransformerEncoderStackWithLengths(num_heads, hsz, pdrop, scale, layers, d_ff=d_ff, rpr_k=rpr_k)\n        return encoder\n\n\n@register_model(task=\'tagger\', name=\'cnn\')\nclass CNNTaggerModel(AbstractEncoderTaggerModel):\n    """"""Convolutional (AKA TDNN) tagger\n\n    Overload the encoder using a conv layer\n\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, **kwargs):\n        """"""Override the base method to produce an RNN transducer\n\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *layers* (``int``) The number of layers to stack\n        * *hsz* (``int``) The number of hidden units for each layer in the encoder\n        * *dropout* (``float``) The dropout rate, defaults\n        * *wfiltsz* (``int``) The 1D filter size for the convolution\n        :return: An encoder\n        """"""\n        nlayers = kwargs.get(\'layers\', 1)\n        hsz = int(kwargs[\'hsz\'])\n        filts = kwargs.get(\'wfiltsz\', None)\n        if filts is None:\n            filts = 5\n\n        cnnout = ConvEncoderStack(None, hsz, filts, self.pdrop_value, nlayers)\n        return cnnout\n\n\n@register_model(task=\'tagger\', name=\'pass\')\nclass PassThruTaggerModel(AbstractEncoderTaggerModel):\n    """"""A Pass-thru implementation of the encoder\n\n    When we fine-tune our taggers from things like BERT embeddings, we might want to just pass through our\n    embedding result directly to the output decoder.  This model provides a mechanism for this by providing\n    a simple identity layer\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def init_encode(self, **kwargs) -> BaseLayer:\n        """"""Identity layer encoder\n\n        :param kwargs: None\n        :return: An encoder\n        """"""\n        input_dim = self.embeddings.output_dim\n        return WithoutLength(PassThru(input_dim))\n'"
baseline/tf/tagger/train.py,0,"b'""""""Train a tagger with TensorFlow\n\nThis module supports 2 different ways of training a model\n\n1. feed_dict\n2. datasets (`default` if not eager)\n3. eager mode (`default` if eager)\n4. distributed eager mode (`distributed`)\n""""""\nfrom baseline.tf.tagger.training import *\n\n\n'"
layers/eight_mile/calibration/__init__.py,0,b'from eight_mile.calibration.calibration import *\nfrom eight_mile.calibration.metrics import *\ntry:\n    from eight_mile.calibration.plot import *\nexcept ImportError:\n    pass\n'
layers/eight_mile/calibration/calibration.py,0,"b'from typing import Optional\nfrom collections import namedtuple\nimport numpy as np\n\n__all__ = [""calibration_bins"", ""multiclass_calibration_bins"", ""binary_calibration_bins"", ""average_confidence"", ""Bins""]\n\n\nBins = namedtuple(""Bins"", ""accs confs counts edges"")\n\n\ndef multiclass_calibration_bins(truth: np.ndarray, probs: np.ndarray, bins: int, class_weights: Optional[np.ndarray] = None) -> Bins:\n    """"""Calculate the binned confidence and accuracy for a multiclass problem.\n\n    :param truth: A 1D array of the true labels for some examples.\n    :param probs: A 1D array of the probabilities from a model. Each row represents an example in the dataset.\n        and each column represents the probably assigned by the model to each class for that example.\n    :param bins: The number of bins to use when aggregating.\n    :param class_weights: A 1D array of scores that can add extra weight to examples of specific classes.\n\n    :returns: The metrics aggregated by bins\n    """"""\n    preds = np.argmax(probs, axis=1)\n    pred_probs = np.max(probs, axis=1)\n    credit = truth == preds\n    if class_weights is not None:\n        weighted_labels = class_weights[truth]\n        credit = credit * weighted_labels\n    return binary_calibration_bins(credit, pred_probs, bins=bins)\n\n\ncalibration_bins = multiclass_calibration_bins\n\n\ndef binary_calibration_bins(credit: np.ndarray, probs: np.ndarray, bins: int) -> Bins:\n    """"""Calculate the accuracy and confidence inside bins. This is similar to the sklearn function.\n\n    Note:\n        This is different than the multiclass function. This will look at the scores for a specific\n        class even if it is below the 1/num_classes threshold that will cause the multiclass\n        version to not select this class.\n\n    :param credit: How much credit the model gets for this example. If it is wrong the value will be zero\n        If it is right one. You can also give different classes different weights by passing real values.\n    :param probs: The probabilities assigned to the positive class by the model.\n    :param bins: The number of bins to use when aggregating.\n\n    :returns: The metrics aggregated by bins\n    """"""\n    bins = np.linspace(0.0, 1.0, num=bins + 1, endpoint=True)\n\n    bin_idx = np.digitize(probs, bins) - 1\n\n    bin_conf_sum = np.bincount(bin_idx, weights=probs, minlength=len(bins))\n    bin_acc_sum = np.bincount(bin_idx, weights=credit, minlength=len(bins))\n    bin_counts = np.bincount(bin_idx, minlength=len(bins))\n\n    mask = bin_counts == 0\n    denom = bin_counts + mask\n\n    bin_mean_conf = bin_conf_sum / denom\n    bin_mean_acc = bin_acc_sum / denom\n\n    return Bins(bin_mean_acc[:-1], bin_mean_conf[:-1], bin_counts[:-1], bins[:-1])\n\n\ndef average_confidence(probs: np.ndarray) -> float:\n    """"""Calculate the average (maximum) confidence for a collection of predictions\n\n    :param probs: `[B, C]` A matrix of probabilities, each row is an example and\n        each column is a class.\n    """"""\n    if probs.ndim == 1:\n        probs = np.expand_dims(probs, axis=-1)\n    return np.mean(np.max(probs, axis=1))\n'"
layers/eight_mile/pytorch/__init__.py,0,b''
layers/eight_mile/pytorch/embeddings.py,16,"b'import math\nfrom collections import OrderedDict\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom eight_mile.utils import Offsets, is_sequence, calc_nfeats\nfrom eight_mile.pytorch.layers import *\n\n\nclass PyTorchEmbeddings(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n    def get_vsz(self):\n        pass\n\n    def get_dsz(self):\n        pass\n\n    @property\n    def output_dim(self):\n        return self.get_dsz()\n\n    def encode(self, x):\n        return self(x)\n\n\nclass LookupTableEmbeddings(PyTorchEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.vsz = kwargs.get(""vsz"")\n        self.dsz = kwargs.get(""dsz"")\n        self.finetune = kwargs.get(""finetune"", True)\n        self.dropin = kwargs.get(""dropin"", 0.0)\n\n        weights = kwargs.get(""weights"")\n        if weights is None:\n            self.embeddings = nn.Embedding(self.vsz, self.dsz, padding_idx=Offsets.PAD)\n        else:\n            self.embeddings = pytorch_embedding(weights, self.finetune)\n            # This makes sure that if you init with a weight and not vsz it will still be available\n            self.vsz, self.dsz = weights.shape\n\n    def get_vsz(self):\n        return self.vsz\n\n    def get_dsz(self):\n        return self.dsz\n\n    def forward(self, x):\n        if not self.dropin:\n            return self.embeddings(x)\n\n        mask = self.embeddings.weight.data.new().resize_((self.embeddings.weight.size(0),\n                                                          1)).bernoulli_(1 - self.dropin).expand_as(self.embeddings.weight) / (1 - self.dropin)\n        masked_embed_weight = mask * self.embeddings.weight\n        output = torch.nn.functional.embedding(x, masked_embed_weight,\n                                               self.embeddings.padding_idx, self.embeddings.max_norm, self.embeddings.norm_type,\n                                               self.embeddings.scale_grad_by_freq, self.embeddings.sparse)\n        return output\n\n    def extra_repr(self):\n        return f""finetune=False"" if not self.finetune else """"\n\n\nclass CharConvEmbeddings(PyTorchEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.nfeat_factor = kwargs.get(""nfeat_factor"")\n        self.cfiltsz = kwargs.get(""cfiltsz"", kwargs.get(""filtsz"", [3]))\n        self.max_feat = kwargs.get(""max_feat"", 30)\n        self.gating = kwargs.get(""gating"", ""skip"")\n        self.num_gates = kwargs.get(""num_gates"", 1)\n        self.activation = kwargs.get(""activation"", ""tanh"")\n        self.wsz = kwargs.get(""wsz"", 30)\n        self.projsz = kwargs.get(""projsz"", 0)\n        self.pdrop = kwargs.get(""pdrop"", 0.5)\n        self.filtsz, self.nfeats = calc_nfeats(self.cfiltsz, self.nfeat_factor, self.max_feat, self.wsz)\n        self.conv_outsz = int(np.sum(self.nfeats))\n        self.outsz = self.conv_outsz\n        if self.projsz > 0:\n            self.outsz = self.projsz\n        self.proj = pytorch_linear(self.conv_outsz, self.outsz)\n\n        self.embeddings = LookupTableEmbeddings(**kwargs)\n        self.char_comp = WithDropout(\n            ParallelConv(self.embeddings.output_dim, self.nfeats, self.filtsz, self.activation), self.pdrop\n        )\n\n        GatingConnection = SkipConnection if self.gating == ""skip"" else Highway\n        self.gating_seq = nn.Sequential(\n            OrderedDict(\n                [(""gate-{}"".format(i), GatingConnection(self.char_comp.output_dim)) for i in range(self.num_gates)]\n            )\n        )\n\n    def get_dsz(self):\n        return self.outsz\n\n    def get_vsz(self):\n        return self.vsz\n\n    def forward(self, xch):\n\n        # For starters we need to perform embeddings for each character\n        # (TxB) x W -> (TxB) x W x D\n        _0, _1, W = xch.shape\n        char_vecs = self.embeddings(xch.view(-1, W))\n        # (TxB) x D x W\n        # char_vecs = char_embeds.transpose(1, 2).contiguous()\n\n        #        pytorch_activation(self.activation_type)\n        mots = self.char_comp(char_vecs)\n        gated = self.gating_seq(mots)\n        if self.projsz:\n            gated = self.proj(gated)\n        return gated.view(_0, _1, self.get_dsz())\n\n\nclass CharLSTMEmbeddings(PyTorchEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.embed = LookupTableEmbeddings(**kwargs)\n        self.lstmsz = kwargs.get(""lstmsz"", 50)\n        layers = kwargs.get(""layers"", 1)\n        pdrop = kwargs.get(""pdrop"", 0.5)\n        unif = kwargs.get(""unif"", 0)\n        weight_init = kwargs.get(""weight_init"", ""uniform"")\n        self.char_comp = BiLSTMEncoderHidden(\n            self.embed.output_dim, self.lstmsz, layers, pdrop, unif=unif, initializer=weight_init\n        )\n\n    def forward(self, xch):\n        B, T, W = xch.shape\n        flat_chars = xch.view(-1, W)\n        char_embeds = self.embed(flat_chars)\n\n        # Calculate the lengths of each word\n        lengths = torch.sum(flat_chars != Offsets.PAD, dim=1)\n\n        # Sort the input to appease the cuDNN gods\n        sorted_word_lengths, perm_idx = lengths.sort(0, descending=True)\n        sorted_feats = char_embeds[perm_idx].transpose(0, 1).contiguous()\n\n        # cuDNN throws an error if there is an input with a length of 0, this happens when the ""word""\n        # is actually a ""<PAD>"" so there are no characters to run the LSTM over. Here we just say\n        # that the lengths is 1. This will make cudnn happy and we will just get junk in that spot\n        patched_lengths = sorted_word_lengths.masked_fill(sorted_word_lengths == 0, 1)\n\n        # Run the LSTM\n        hidden = self.char_comp((sorted_feats, patched_lengths))\n\n        # Create a mask that is true when the sorted length is 0 (where the word was a pad) so that\n        # we can mask out the junk that the lstm created because we needed a length of 1\n        hidden = hidden.masked_fill((sorted_word_lengths == 0).unsqueeze(-1), 0)\n\n        # Undo the sort so that the representations of the words are in the correct part of the sentence.\n        results = unsort_batch(hidden, perm_idx)\n\n        return results.reshape((B, T, -1))\n\n    def get_dsz(self):\n        return self.lstmsz\n\n    def get_vsz(self):\n        return self.embed.get_vsz()\n\n\nclass CharTransformerEmbeddings(PyTorchEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.embed = LookupTableEmbeddings(**kwargs)\n        self.d_model = kwargs.get(""wsz"", 30)\n        self.num_heads = kwargs.get(""num_heads"", 3)\n        self.rpr_k = kwargs.get(""rpr_k"", 10)\n        layers = kwargs.get(""layers"", 1)\n        pdrop = kwargs.get(""pdrop"", 0.5)\n        self.char_comp = TransformerEncoderStackWithLengths(\n            self.num_heads, self.d_model, pdrop, False, layers, rpr_k=self.rpr_k, input_sz=self.embed.output_dim\n        )\n\n    def forward(self, xch):\n        B, T, W = xch.shape\n        flat_chars = xch.view(-1, W)\n        char_embeds = self.embed(flat_chars)\n\n        # Calculate the lengths of each word\n        lengths = torch.sum(flat_chars != Offsets.PAD, dim=1)\n        results = self.char_comp((char_embeds, lengths))\n        # B,T,H output, how to pool this\n        pooled = torch.max(results, -2, keepdims=False)[0]\n        return pooled.reshape((B, T, -1))\n\n    def get_dsz(self):\n        return self.d_model\n\n    def get_vsz(self):\n        return self.embed.get_vsz()\n\n\nclass PositionalMixin(nn.Module):\n    """"""A Mixin that provides functionality to generate positional embeddings to be added to the normal embeddings.\n\n    Note, mixins need to be before the base case when used, i.e.\n        `Embedding(Mixin, BaseEmbed)` NOT `Embedding(BaseEmbed, Mixin)`\n    """"""\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def positional(self, length):\n        pass\n\n    def extra_repr(self):\n        return f""mxlen={self.mxlen}""\n\n\nclass SinusoidalPositionalMixin(PositionalMixin):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # This could get us in trouble, if in doubt, pick something big\n        self.mxlen = kwargs.get(""mxlen"", 1000)\n        max_timescale = kwargs.get(""max_timescale"", 1.0e4)\n\n        word_dsz = self.get_dsz()\n\n        log_timescale_increment = math.log(max_timescale) / word_dsz\n        inv_timescales = torch.exp(torch.arange(0, word_dsz, 2).float() * -log_timescale_increment)\n\n        pe = torch.zeros(self.mxlen, word_dsz)\n        position = torch.arange(0, self.mxlen).float().unsqueeze(1)\n        pe[:, 0::2] = torch.sin(position * inv_timescales)\n        pe[:, 1::2] = torch.cos(position * inv_timescales)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(""pe"", pe)\n\n    def positional(self, length):\n        return self.pe[:, :length]\n\n\nclass LearnedPositionalMixin(PositionalMixin):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.mxlen = int(kwargs.get(""mxlen"", 512))\n        self.pos_embeddings = nn.Embedding(self.mxlen, self.get_dsz())\n\n    def positional(self, length):\n        return self.pos_embeddings(\n            torch.arange(length, dtype=torch.long, device=self.pos_embeddings.weight.device)\n        ).unsqueeze(0)\n\n\nclass BERTLookupTableEmbeddings(LookupTableEmbeddings):\n    """"""\n    BERT style embeddings with a 0 token type\n\n    TODO: Get rid of this, we dont need it anymore\n    If you want to use BERT with token types, make a `LearnedPositionalLookupTableEmbeddings` feature\n    and a `LookupTableEmbeddings` feature (for the token type)\n    and put them in an `EmbeddingsStack` with an embeddings_reduction=\'sum-layer-norm\' on the model\n\n    Otherwise, if you do not plan on setting the token type, use the `LearnedPositionalLookupTableEmbeddingsWithBias`,\n    which will add the BERT token_type=0 weights into the pos + word_embed and is more efficient\n    than this class, since it doesnt do any memory allocation on the fly\n    """"""\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(\'dropout\', 0.1))\n        self.mxlen = int(kwargs.get(\'mxlen\', 512))\n        self.tok_type_vsz = kwargs[\'tok_type_vsz\']\n        self.pos_embeddings = nn.Embedding(self.mxlen, self.get_dsz())\n        self.tok_embeddings = nn.Embedding(self.tok_type_vsz, self.get_dsz())\n        self.ln = nn.LayerNorm(self.get_dsz(), eps=1e-12)\n\n    def forward(self, x):\n        zeros = torch.zeros_like(x)\n        x = super().forward(x)\n        x = x + self.positional(x.size(1)) + self.tok_embeddings(zeros)\n        x = self.ln(x)\n        return self.dropout(x)\n\n    def positional(self, length):\n        return self.pos_embeddings(\n            torch.arange(length, dtype=torch.long, device=self.pos_embeddings.weight.device)\n        ).unsqueeze(0)\n\n\nclass LearnedPositionalLookupTableEmbeddingsWithBias(LookupTableEmbeddings):\n    """"""Learned positional lookup table embeddings wih a bias and layer norm\n\n    This is just a typical learned positional embedding but with a learnable\n    bias and a layer norm.  This is equivalent to BERT embeddings when the\n    token_type is not set.\n\n    If you are using BERT but you have no interest in using token type embeddings\n    (IOW if you are setting all the values of that feature zero anyhow), using this\n    object is faster and simpler than having a separate vectorizer for token type.\n\n    If you have a need for token type embeddings, you will want to create 2 sets of embeddings,\n    one that acts on the tokens, of type `LearnedPositionalLookupTableEmbeddings` and one of the type\n    `LookupTableEmbeddings` for the token type feature\n\n    """"""\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(\'dropout\', 0.0))\n        self.mxlen = int(kwargs.get(\'mxlen\', 512))\n        self.pos_embeddings = nn.Embedding(self.mxlen, self.get_dsz())\n        self.bias = nn.Parameter(torch.zeros(self.get_dsz()))\n\n    def forward(self, x):\n        x = super().forward(x)\n        x = x + self.positional(x.size(1)) + self.bias\n        return x\n\n    def positional(self, length):\n        return self.pos_embeddings(\n            torch.arange(length, dtype=torch.long, device=self.pos_embeddings.weight.device)\n        ).unsqueeze(0)\n\n\nclass PositionalLookupTableEmbeddings(SinusoidalPositionalMixin, LookupTableEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(""dropout"", 0.0))\n        self.scale = math.sqrt(self.get_dsz())\n\n    def forward(self, x):\n        """"""Add a positional encoding to the embedding, followed by dropout\n\n        :param x: The temporal signal in, to which the positional embeddings are applied\n        :return: Embedded output\n        """"""\n        x = super().forward(x) * self.scale\n        x = x + self.positional(x.size(1))\n        return self.dropout(x)\n\n\nclass LearnedPositionalLookupTableEmbeddings(LearnedPositionalMixin, LookupTableEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def forward(self, x):\n        T = x.size(1)\n        x = super().forward(x)\n        pos = self.positional(T)\n        return self.dropout(x + pos)\n\n\nclass PositionalCharConvEmbeddings(SinusoidalPositionalMixin, CharConvEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(""dropout"", 0.0))\n        self.scale = math.sqrt(self.get_dsz())\n\n    def forward(self, xch):\n        """"""Add a positional encoding to the embedding, followed by dropout\n\n        :param xch: The temporal signal in, to which the positional embeddings are applied\n        :return: Embedded output\n        """"""\n        xch = super().forward(xch) * self.scale\n        xch = xch + self.positional(xch.size(1))\n        return self.dropout(xch)\n\n\nclass LearnedPositionalCharConvEmbeddings(LearnedPositionalMixin, CharConvEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def forward(self, xch):\n        """"""Add a positional encoding to the embedding, followed by dropout\n\n        :param xch: The temporal signal in, to which the positional embeddings are applied\n        :return: Embedded output\n        """"""\n        xch = super().forward(xch)\n        xch = xch + self.positional(xch.size(1))\n        return self.dropout(xch)\n\n\nclass PositionalCharLSTMEmbeddings(SinusoidalPositionalMixin, CharLSTMEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(""dropout"", 0.0))\n        self.scale = math.sqrt(self.get_dsz())\n\n    def forward(self, xch):\n        xch = super().forward(xch) * self.scale\n        xch = xch + self.positional(xch.size(1))\n        return self.dropout(xch)\n\n\nclass LearnedPositionalCharLSTMEmbeddings(LearnedPositionalMixin, CharLSTMEmbeddings):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.dropout = nn.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def forward(self, xch):\n        xch = super().forward(xch)\n        xch = xch + self.positional(xch.size(1))\n        return self.dropout(xch)\n'"
layers/eight_mile/pytorch/layers.py,302,"b'import copy\nimport math\nimport logging\nfrom typing import Dict, List, Optional, Tuple, Union\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.jit as jit\nimport torch.autograd\n\nfrom eight_mile.utils import listify, Offsets, is_sequence\nfrom eight_mile.utils import transition_mask as transition_mask_np\n\nMASK_FALSE = False\nlogger = logging.getLogger(""mead.layers"")\n\n\ndef sequence_mask(lengths: torch.Tensor, max_len: int = -1) -> torch.Tensor:\n    """"""Generate a sequence mask of shape `BxT` based on the given lengths\n\n    :param lengths: A `B` tensor containing the lengths of each example\n    :param max_len: The maximum width (length) allowed in this mask (default to None)\n    :return: A mask\n    """"""\n    lens = lengths.cpu()\n    if max_len < 0:\n        max_len_v = torch.max(lens)\n    else:\n        max_len_v = max_len\n    # 1 x T\n    row = torch.arange(0, max_len_v).type_as(lens).view(1, -1)\n    # B x 1\n    col = lens.view(-1, 1)\n    # Broadcast to B x T, compares increasing number to max\n    mask = row < col\n    return mask\n\n\ndef vec_log_sum_exp(vec: torch.Tensor, dim: int) -> torch.Tensor:\n    """"""Vectorized version of log-sum-exp\n\n    :param vec: Vector\n    :param dim: What dimension to operate on\n    :return:\n    """"""\n    max_scores, idx = torch.max(vec, dim, keepdim=True)\n    max_scores_broadcast = max_scores.expand_as(vec)\n    return max_scores + torch.log(torch.sum(torch.exp(vec - max_scores_broadcast), dim, keepdim=True))\n\n\ndef unsort_batch(batch: torch.Tensor, perm_idx: torch.Tensor) -> torch.Tensor:\n    """"""Undo the sort on a batch of tensors done for packing the data in the RNN.\n\n    :param batch: The batch of data batch first `[B, ...]`\n    :param perm_idx: The permutation index returned from the torch.sort.\n\n    :returns: The batch in the original order.\n    """"""\n    # Add ones to the shape of the perm_idx until it can broadcast to the batch\n    perm_idx = perm_idx.to(batch.device)\n    diff = len(batch.shape) - len(perm_idx.shape)\n    extra_dims = [1] * diff\n    perm_idx = perm_idx.view([-1] + extra_dims)\n    return batch.scatter_(0, perm_idx.expand_as(batch), batch)\n\n\ndef infer_lengths(tensor, dim=1):\n    """"""Infer the lengths of an input based on the idea the Offsets.PAD was used as the padding token.\n\n    :param tensor: The data to infer the length of, should be either [B, T] or [T, B]\n    :param dim: The dimension which contains the sequential signal\n\n    :returns: A Tensor of shape `[B]` that has the lengths for example item in the batch\n    """"""\n    if len(tensor.shape) != 2:\n        raise ValueError(f""infer_lengths only works with tensors wit two dims right now, got {len(tensor.shape)}"")\n    offsets = torch.arange(1, tensor.shape[dim] + 1, device=tensor.device, dtype=tensor.dtype).unsqueeze(1 - dim)\n    non_pad_loc = (tensor != Offsets.PAD).to(tensor.dtype)\n    return torch.argmax(non_pad_loc * offsets, dim=dim) + 1\n\n\ndef tensor_and_lengths(inputs) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    """"""Return either the unpacked inputs (2), or a `Tuple` of the input with None\n\n    TODO: this function should probably be changed to always return the lengths second.\n    To do this, we just need a sentinel value, e.g. <PAD> (0).  The problem with doing this is\n    that it might be possible to generate <PAD> in the middle of the tensor which would make that\n    length invalid.\n\n    :param inputs: Either a sequence of the `(tensor, length)` or just the `tensor`\n    :return: A `Tuple` of `(tensor, length)` or `(tensor, None)`\n    """"""\n    if isinstance(inputs, (list, tuple)):\n        in_tensor, lengths = inputs\n    else:\n        in_tensor = inputs\n        lengths = None\n\n    return in_tensor, lengths\n\n\nclass VariationalDropout(nn.Module):\n    """"""Inverted dropout that applies the same mask at each time step.""""""\n\n    def __init__(self, pdrop: float = 0.5, batch_first: bool = False):\n        """"""Variational Dropout\n\n        :param pdrop: the percentage to drop\n        """"""\n        super().__init__()\n        self.pdrop = pdrop\n        self.batch_first = batch_first\n\n    def extra_repr(self):\n        return ""p=%.1f"" % self.pdrop\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return input\n        # Create a mask that covers a single time step\n        if self.batch_first:\n            dim0 = input.size(0)\n            dim1 = 1\n        else:\n            dim0 = 1\n            dim1 = input.size(1)\n        mask = torch.zeros(dim0, dim1, input.size(2)).bernoulli_(1 - self.pdrop).to(input.device)\n        mask = mask / self.pdrop\n        # Broadcast the mask over the sequence\n        return mask * input\n\n\nclass SequenceLoss(nn.Module):\n    """"""Computes the loss over a sequence""""""\n\n    def __init__(self, LossFn: nn.Module = nn.NLLLoss, avg: str = ""token""):\n        """"""A class that applies a Loss function to sequence via the folding trick.\n\n        :param LossFn: A loss function to apply (defaults to `nn.NLLLoss`)\n        :param avg: A divisor to apply, valid values are `token` and `batch`\n        """"""\n        super().__init__()\n        self.avg = avg\n        if avg == ""token"":\n            self.crit = LossFn(ignore_index=Offsets.PAD, reduction=""mean"")\n            self._norm = self._no_norm\n        else:\n            self.crit = LossFn(ignore_index=Offsets.PAD, reduction=""sum"")\n            self._norm = self._batch_norm\n\n    def _batch_norm(self, loss, inputs):\n        return loss / inputs.size()[0]\n\n    def _no_norm(self, loss, inputs):\n        return loss\n\n    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        """"""Evaluate some loss over a sequence.\n        :param inputs: torch.FloatTensor, [B, .., C] The scores from the model. Batch First\n        :param targets: torch.LongTensor, The labels.\n        :returns: torch.FloatTensor, The loss.\n        """"""\n        total_sz = targets.nelement()\n        loss = self.crit(inputs.view(total_sz, -1), targets.view(total_sz))\n        return self._norm(loss, inputs)\n\n    def extra_repr(self):\n        return f""reduction={self.avg}""\n\n\nclass MeanPool1D(nn.Module):\n    """"""Do a mean pool while accounting for the length of a sequence\n    """"""\n\n    def __init__(self, outsz, batch_first=True):\n        """"""Set up pooling module\n\n        :param outsz: The output dim, for dowstream access\n        :param batch_first: Is this module batch first or time first?\n        """"""\n        super().__init__()\n        self.batch_first = batch_first\n        self.reduction_dim = 1 if self.batch_first else 0\n        self.output_dim = outsz\n        self.requires_length = True\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""Apply mean pooling on the valid inputs\n\n        :param inputs: A tuple of `(input, lengths)`\n        :return: Pooled output\n        """"""\n        tensor, lengths = tensor_and_lengths(inputs)\n        # Regardless of whether the input is `[B, T, H]` or `[T, B, H]` the shape after\n        # the sum is `[B, H]` so the lengths (of shape `[B]`) should be unsqueezed to\n        # `[B, 1]` in order to broadcast\n        return torch.sum(tensor, self.reduction_dim, keepdim=False) / torch.unsqueeze(lengths, -1).to(tensor.dtype).to(\n            tensor.device\n        )\n\n    def extra_repr(self):\n        return f""batch_first={self.batch_first}""\n\n\nclass MaxPool1D(nn.Module):\n    """"""Do a max-pooling operation with or without a length given\n    """"""\n\n    def __init__(self, outsz, batch_first=True):\n        super().__init__()\n        self.batch_first = batch_first\n        self.reduction_dim = 1 if self.batch_first else 0\n        self.output_dim = outsz\n\n    def forward(self, inputs: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]) -> torch.Tensor:\n        """"""If we are given a tuple as input, we will use the length, otherwise we will do an operation without masking\n\n        :param inputs: either a tuple of `(input, lengths)` or a tensor `input`\n        :return: A pooled tensor\n        """"""\n        tensor, lengths = tensor_and_lengths(inputs)\n        if lengths is not None:\n            # If tensor = `[B, T, H]`\n            #    mask = `[B, T, 1]`\n            # If tensor = `[T, B, H]`\n            #    mask = `[T, B, 1]`\n            # So it will mask all the values in H past the right length\n            mask = sequence_mask(lengths).to(tensor.device)\n            mask = mask if self.batch_first else bth2tbh(mask)\n            # Fill masked with very negative so it never gets selected\n            tensor = tensor.masked_fill(mask.unsqueeze(-1) == MASK_FALSE, -1e4)\n        dmax, _ = torch.max(tensor, self.reduction_dim, keepdim=False)\n        return dmax\n\n    def extra_repr(self) -> str:\n        return f""batch_first={self.batch_first}""\n\n\n# Torch only added this module in 1.4.0, shim\nclass GeLU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return torch.nn.functional.gelu(x)\n\n\ndef get_activation(name: str = ""relu"") -> nn.Module:\n    """"""Get back an `nn.Module` by string name of the activation operator\n\n    :param name: A string name of the operation\n    :return: A module associated with that string\n    """"""\n    if name is None or name == ""ident"":\n        return nn.Identity()\n    if name == ""tanh"":\n        return nn.Tanh()\n    if name == ""gelu"":\n        return GeLU()\n    if name == ""hardtanh"":\n        return nn.Hardtanh()\n    if name == ""leaky_relu"":\n        return nn.LeakyReLU()\n    if name == ""prelu"":\n        return nn.PReLU()\n    if name == ""sigmoid"":\n        return nn.Sigmoid()\n    if name == ""log_sigmoid"":\n        return nn.LogSigmoid()\n    if name == ""log_softmax"":\n        return nn.LogSoftmax(dim=-1)\n    if name == ""softmax"":\n        return nn.Softmax(dim=-1)\n    return nn.ReLU()\n\n\ndef _cat_dir(h: torch.Tensor) -> torch.Tensor:\n    """"""Concat forward and backword state vectors.\n\n    The shape of the hidden is `[#layers * #dirs, B, H]`. The docs say you can\n    separate directions with `h.view(#l, #dirs, B, H)` with the forward dir being\n    index 0 and backwards dir being 1.\n\n    This means that before separating with the view the forward dir are the even\n    indices in the first dim while the backwards dirs are the odd ones. Here we select\n    the even and odd values and concatenate them\n\n    :param h: The hidden shape as it comes back from PyTorch modules\n    """"""\n    return torch.cat([h[0 : h.size(0) : 2], h[1 : h.size(0) : 2]], dim=-1)\n\n\ndef concat_state_dirs(state):\n    """"""Convert the bidirectional out of an RNN so the forward and backward values are a single vector.""""""\n    if isinstance(state, tuple):\n        return tuple(_cat_dir(h) for h in state)\n    return _cat_dir(state)\n\n\nclass Conv1DSame(nn.Module):\n    """"""Perform a 1D convolution with output size same as input size\n\n    To make this operation work as expected, we cannot just use `padding=kernel_size//2` inside\n    of the convolution operation.  Instead, we zeropad the input using the `ConstantPad1d` module\n\n    """"""\n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, bias: bool = True):\n        """"""Create a 1D conv to produce the same output size as input\n\n        :param in_channels: The number of input feature maps\n        :param out_channels: The number of output feature maps\n        :param kernel_size: The kernel size\n        :param bias: Is bias on?\n        """"""\n        super().__init__()\n        start_pad = kernel_size // 2\n        end_pad = start_pad - 1 if kernel_size % 2 == 0 else start_pad\n        self.conv = nn.Sequential(\n            nn.ConstantPad1d((start_pad, end_pad), 0.),\n            nn.Conv1d(in_channels, out_channels, kernel_size, bias=bias)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Do convolution1d on an input tensor, `[B, C, T]`\n\n        :param x: The input tensor of shape `[B, C, T]`\n        :return: The output tensor of shape `[B, H, T]`\n        """"""\n        return self.conv(x)\n\n\nclass ConvEncoder(nn.Module):\n    """"""1D Convolutional layer encoder with given activation function, optional dropout\n\n    This module takes in a temporal signal of either shape `[B, C, T]` or `[B, T, C]`, depending on the constructor\n    and produces an output signal of the same orientation (`[B, H, T]` or `[B, T, H]`, respectively).  We default\n    to `[B, T, H]` orientation to make it more convenient for typical layout, but this requires transposing the last\n    2 dims before and after the convolution operation.\n\n    """"""\n\n    def __init__(self, insz: int, outsz: int, filtsz: int, pdrop: float = 0.0, activation: str = ""relu"", hidden_last=True):\n        """"""Construct the encoder with optional dropout, given activation, and orientation\n\n        :param insz: The number of input feature maps\n        :param outsz: The number of output feature maps (or hidden size)\n        :param filtsz: The kernel size\n        :param pdrop: The amount of dropout to apply, this defaults to 0\n        :param activation: The activation function by name, defaults to `relu`\n        :param hidden_last: PyTorch only! If `True` the orientatiation is `[B, T, H]`, o.w. `[B, H, T]` expected\n        """"""\n        super().__init__()\n        self.output_dim = outsz\n\n        conv = Conv1DSame(insz, outsz, filtsz)\n        act = get_activation(activation)\n        dropout = nn.Dropout(pdrop)\n\n        if hidden_last:\n            self.conv = nn.Sequential(BTH2BHT(), conv, act, dropout, BHT2BTH())\n        else:\n            self.conv = nn.Sequential(conv, act, dropout)\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return self.conv(input)\n\n\nclass ConvEncoderStack(nn.Module):\n    """"""Create a stack of convolutional encoders with residual connections between, using the `ConvEncoder` underneath\n\n    This creates an encoder stack of convolutions, finally returning the last temporal output.  Each layer uses zero-padding\n    which causes the output of the convolution at each layer to be the same length.\n\n    As in the `ConvEncoder` we support input tensor shapes of `[B, C, T]` or `[B, T, C]` depending on the constructor\n    initialization, and transpose underneath the input and output of the stack if the orientation is defaulted to\n    `[B, T, C]`\n    """"""\n\n    def __init__(self, insz: int, outsz: int, filtsz: int, nlayers: int = 1, pdrop: float = 0.0, activation: str = ""relu"", hidden_last=True):\n        """"""Construct the encoder stack\n\n        :param insz: The input number of feature maps\n        :param outsz: The output number of feature maps\n        :param filtsz: The kernel size\n        :param nlayers: The number of layers in the stack (defaults to a single layer)\n        :param pdrop: The amount of dropout to apply (defaults to `0`)\n        :param activation: The activation function to use as a string, defaults to `relu`\n        :param hidden_last: PyTorch only! If `True` the orientatiation is `[B, T, H]`, o.w. `[B, H, T]` expected\n        """"""\n        super().__init__()\n\n        if hidden_last:\n            first_layer = nn.Sequential(BTH2BHT(), ConvEncoder(insz, outsz, filtsz, pdrop, activation, hidden_last=False))\n        else:\n            first_layer = ConvEncoder(insz, outsz, filtsz, pdrop, activation, hidden_last=False)\n\n        subsequent_layer = ResidualBlock(ConvEncoder(outsz, outsz, filtsz, pdrop, activation, hidden_last=False))\n\n        self.layers = nn.ModuleList([first_layer] + [copy.deepcopy(subsequent_layer) for _ in range(nlayers - 1)])\n        if hidden_last:\n            self.layers.append(BHT2BTH())\n        self.output_dim = outsz\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        """"""Apply a stack of 1D convolutions with residual connections between them\n\n        :param input: A tensor of shape `[B, T, C]` or `[B, C, T]` depending on value of `hidden_last`\n        :return: A tensor of shape `[B, T, H]` or `[B, H, T]` depending on the value of `hidden_last`\n        """"""\n        x = input\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\ndef bth2bht(t: torch.Tensor) -> torch.Tensor:\n    """"""Transpose the 2nd and 3rd dim of a tensor""""""\n    return t.transpose(1, 2).contiguous()\n\n\nclass BTH2BHT(nn.Module):\n    """"""Utility layer to convert from `[B, T, H]` to `[B, H, T]`\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        return bth2bht(t)\n\n\ndef tbh2bht(t: torch.Tensor) -> torch.Tensor:\n    """"""Permute the dimensions, first goes to third, second goes to first, last moves to second""""""\n    return t.permute(1, 2, 0).contiguous()\n\n\nclass TBH2BHT(nn.Module):\n    """"""Utility layer to convert from `[T, B, H]` to `[B, H, T]`\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        return tbh2bht(t)\n\n\ndef tbh2bth(t: torch.Tensor) -> torch.Tensor:\n    """"""Transpose the first 2 dims""""""\n    return t.transpose(0, 1).contiguous()\n\n\nclass TBH2BTH(nn.Module):\n    """"""Utility layer to convert from `[T, B, H]` to `[B, T, H]`\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        return tbh2bth(t)\n\n\ndef bth2tbh(t: torch.Tensor) -> torch.Tensor:\n    """"""Transpose the first 2 dims""""""\n    return t.transpose(0, 1).contiguous()\n\n\nclass BTH2TBH(nn.Module):\n    """"""Utility layer to convert from `[B, T, H]` to `[T, B, H]`\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        return bth2tbh(t)\n\n\ndef bht2bth(t: torch.Tensor) -> torch.Tensor:\n    return t.transpose(1, 2).contiguous()\n\n\nclass BHT2BTH(nn.Module):\n    """"""Utility layer to convert from `[B, H, T]` to `[B, T, H]`\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, t: torch.Tensor) -> torch.Tensor:\n        return bht2bth(t)\n\n\nclass ParallelConv(nn.Module):\n    """"""Layer of parallel convolutions with varying filter sizes followed by max over time pooling\n\n    This module takes an input tensor of any orientation based on its constructor, and pools its\n    output to shape `[B, H]`, where `H` is `outsz * len(filtsz)`\n    """"""\n\n    def __init__(self, insz: int, outsz: int, filtsz: List[int], activation: str = ""relu"", input_fmt: str = ""bth""):\n        """"""\n        Constructor for a parallel convolution from any orientation tensor input\n\n        :param insz: The number of input feature maps\n        :param outsz: The number of output feature maps\n        :param filtsz: The kernel size as a list of parallel filters to apply, e.g. `[3, 4, 5]`\n        :param activation: An activation function by name to apply\n        :param input_fmt: A string for the orientation.  Valid values are `bth` or `btc` meaning hidden units last,\n        `bht` or `bct` meaning the temporal dim last or `tbh` or `tbc` meaning the hidden units last and the temporal dim\n        first\n        """"""\n        super().__init__()\n        self.requires_length = False\n        convs = []\n        outsz_filts = outsz\n        self.input_fmt = input_fmt.lower()\n\n        if type(outsz) == int:\n            outsz_filts = len(filtsz) * [outsz]\n\n        self.output_dim = sum(outsz_filts)\n        for i, fsz in enumerate(filtsz):\n            pad = fsz // 2\n            conv = nn.Sequential(nn.Conv1d(insz, outsz_filts[i], fsz, padding=pad), get_activation(activation))\n            convs.append(conv)\n            # Add the module so its managed correctly\n        self.convs = nn.ModuleList(convs)\n\n    def transform_input(self, t: torch.Tensor) -> torch.Tensor:\n\n        if self.input_fmt == ""bth"" or self.input_fmt == ""btc"":\n            return bth2bht(t)\n        elif self.input_fmt == ""tbh"" or self.input_fmt == ""tbc"":\n            return tbh2bht(t)\n        else:\n            return t\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        """"""Transform the input to `[B, C, T]` from any orientation and perform parallel 1D convs and max over time pool\n\n        :param inputs: An input tensor of any format specified in the constructor\n        :return: A `[B, H]` tensor representing the pooled outputs\n        """"""\n        mots = []\n        input_bct = self.transform_input(inputs)\n\n        for conv in self.convs:\n            # In Conv1d, data BxCxT, max over time\n            conv_out = conv(input_bct)\n            mot, _ = conv_out.max(2)\n            mots.append(mot)\n        mots = torch.cat(mots, 1)\n        return mots  # self.conv_drop(mots)\n\n\nclass Highway(nn.Module):\n    """"""Highway layer as defined in https://arxiv.org/abs/1505.00387\n\n    """"""\n\n    def __init__(self, input_size: int, **kwargs):\n        """"""Highway layer constructor\n\n        :param input_size: The input hidden size\n        :param kwargs:\n        """"""\n        super().__init__()\n        self.proj = nn.Linear(input_size, input_size)\n        self.transform = nn.Linear(input_size, input_size)\n        self.transform.bias.data.fill_(-2.0)\n        self.output_dim = input_size\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        """"""Take a tensor in and produce the highway layer output\n\n        :param input: Input tensor\n        :return: output tensor\n        """"""\n        proj_result = torch.relu(self.proj(input))\n        proj_gate = torch.sigmoid(self.transform(input))\n        gated = (proj_gate * proj_result) + ((1 - proj_gate) * input)\n        return gated\n\n\ndef pytorch_linear(in_sz: int, out_sz: int, unif: float = 0, initializer: str = None, bias: bool = True):\n    """"""Utility function that wraps a linear (AKA dense) layer creation, with options for weight init and bias""""""\n    l = nn.Linear(in_sz, out_sz, bias=bias)\n    if unif > 0:\n        l.weight.data.uniform_(-unif, unif)\n    elif initializer == ""ortho"":\n        nn.init.orthogonal(l.weight)\n    elif initializer == ""he"" or initializer == ""kaiming"":\n        nn.init.kaiming_uniform(l.weight)\n    else:\n        nn.init.xavier_uniform_(l.weight)\n    if bias:\n        l.bias.data.zero_()\n    return l\n\n\nclass StackedLSTMCell(nn.Module):\n    """"""A stacked LSTM cells applied at a timestep\n    """"""\n\n    def __init__(self, num_layers: int, input_size: int, rnn_size: int, dropout: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n\n        for i in range(num_layers):\n            self.layers.append(nn.LSTMCell(input_size=input_size, hidden_size=rnn_size, bias=False))\n            input_size = rnn_size\n\n    def forward(self, input: torch.Tensor, hidden: torch.Tensor):\n        """"""Apply a stack of LSTMs\n\n        :param input: The input to the first LSTM `[B, H]`\n        :param hidden: The previous `(h, c)` where `h=(h_0, h_1,..)`, `c=(c_0, c_1,..)`\n        :return: The output and hidden `(h, c)` where `h=(h_0, h_1,..)`, `c=(c_0, c_1,..)`\n        """"""\n        h_0, c_0 = hidden\n        hs, cs = [], []\n        for i, layer in enumerate(self.layers):\n            h_i, c_i = layer(input, (h_0[i], c_0[i]))\n            input = h_i\n            if i != self.num_layers - 1:\n                input = self.dropout(input)\n            hs.append(h_i)\n            cs.append(c_i)\n\n        hs = torch.stack(hs)\n        cs = torch.stack(cs)\n\n        return input, (hs, cs)\n\n\nclass StackedGRUCell(nn.Module):\n    """"""A stacked GRU cells applied at a timestep\n    """"""\n\n    def __init__(self, num_layers: int, input_size: int, rnn_size: int, dropout: float):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n\n        for i in range(num_layers):\n            self.layers.append(nn.GRUCell(input_size=input_size, hidden_size=rnn_size))\n            input_size = rnn_size\n\n    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Apply a stack of GRUs\n\n        :param input: The input to the first LSTM `[B, H]`\n        :param hidden: The previous `h` where `h=(h_0, h_1,..)`\n        :return: The output and hidden `h` where `h=(h_0, h_1,..)`\n        """"""\n        h_0 = hidden\n        hs = []\n        for i, layer in enumerate(self.layers):\n            h_i = layer(input, (h_0[i]))\n            input = h_i\n            if i != self.num_layers:\n                input = self.dropout(input)\n            hs.append(h_i)\n\n        hs = torch.stack(hs)\n\n        return input, hs\n\n\nclass Dense(nn.Module):\n    """"""Dense (Linear) layer with optional activation given\n\n    This module is the equivalent of the tf.keras.layer.Dense, module with optional activations applied\n    """"""\n\n    def __init__(\n        self,\n        insz: int,\n        outsz: int,\n        activation: Optional[str] = None,\n        unif: float = 0,\n        initializer: Optional[str] = None,\n    ):\n        """"""Constructor for ""dense"" or ""linear"" layer, with optional activation applied\n\n        :param insz: The number of hidden units in the input\n        :param outsz: The number of hidden units in the output\n        :param activation: The activation function by name, defaults to `None`, meaning no activation is applied\n        :param unif: An optional initialization value which can set the linear weights.  If given, biases will init to 0\n        :param initializer: An initialization scheme by string name: `ortho`, `kaiming` or `he`, `xavier` or `glorot`\n        """"""\n        super().__init__()\n        self.layer = pytorch_linear(insz, outsz, unif, initializer)\n        self.activation = get_activation(activation)\n        self.output_dim = outsz\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        """"""Run a linear projection over the input, followed by an optional activation given by constructor\n\n        :param input: the input tensor\n        :return: the transformed output\n        """"""\n        return self.activation(self.layer(input))\n\n\nclass WeightTieDense(nn.Module):\n    """"""Do weight tying from the input parameter\n    """"""\n\n    def __init__(self, tie: nn.Module, bias=False):\n        super().__init__()\n        self.tie = tie\n        self.weight, self.transform = self._get_weight(tie)\n        if bias:\n            bias = torch.nn.Parameter(torch.zeros(self.transform(self.weight).shape[0]))\n        else:\n            bias = None\n        self.register_parameter(""bias"", bias)\n\n    def _get_weight(self, tie: nn.Module):\n        emb = getattr(tie, ""embeddings"", None)\n        if emb is not None:\n            return getattr(emb, ""weight""), self._identity\n        return getattr(tie, ""weight""), self._transpose\n\n    def _identity(self, x: torch.Tensor) -> torch.Tensor:\n        return x\n\n    def _transpose(self, x: torch.Tensor) -> torch.Tensor:\n        return x.transpose(0, 1).contiguous()\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        return F.linear(input, self.transform(self.weight), self.bias)\n\n\nclass ResidualBlock(nn.Module):\n    """"""Create a residual block by wrapping an layer with a residual connection""""""\n\n    def __init__(self, layer: Optional[nn.Module] = None, **kwargs):\n        """"""Wrap an layer with a residual connection\n\n        :param layer: This layer will be applied to the input and added to the input\n        :param kwargs:\n        """"""\n        super().__init__()\n        self.layer = layer\n        if self.layer is not None and hasattr(layer, ""output_dim""):\n            self.output_dim = layer.output_dim\n\n    def forward(self, input: torch.Tensor) -> torch.Tensor:\n        """"""Apply a residual block\n\n        :param input: A tensor to use as input and to add to output\n        :return: The residual connection output\n        """"""\n        return input + self.layer(input)\n\n\nclass SkipConnection(ResidualBlock):\n\n    """"""Subclass of ResidualBlock(Dense) with an activation function given\n    """"""\n\n    def __init__(self, input_size: int, activation: str = ""relu""):\n        """"""Create a `SkipConnection`\n\n        :param input_size: The input dimension size\n        :param activation: A string activation name\n        """"""\n        super().__init__(None)\n        self.layer = Dense(input_size, input_size, activation=activation)\n        self.output_dim = input_size\n\n\ndef rnn_cell(insz: int, hsz: int, rnntype: str, nlayers: int, dropout: float):\n    """"""This is a wrapper function around a stacked RNN cell\n\n    :param insz: The input dimensions\n    :param hsz: The hidden dimensions\n    :param rnntype: An RNN type `gru` or `lstm`\n    :param nlayers: The number of layers to stack\n    :param dropout: The amount of dropout\n    :return:\n    """"""\n    if rnntype == ""gru"":\n        rnn = StackedGRUCell(nlayers, insz, hsz, dropout)\n    else:\n        rnn = StackedLSTMCell(nlayers, insz, hsz, dropout)\n    return rnn\n\n\ndef pytorch_lstm(\n    insz: int,\n    hsz: int,\n    rnntype: str,\n    nlayers: int,\n    dropout: float,\n    unif: float = 0,\n    batch_first: bool = False,\n    initializer: str = None,\n) -> torch.nn.LSTM:\n    """"""Wrapper around `torch.nn.LSTM`, mainly for weight initialization options\n\n    :param insz: The input dimension\n    :param hsz: The number of hidden units\n    :param rnntype: A string description of the type of LSTM: `bi?lstm` or `lstm`\n    :param nlayers: The number of layers\n    :param dropout: How much dropout to apply\n    :param unif: if uniform initialization, what range?\n    :param batch_first: Should we do the RNN batch first or time first\n    :param initializer: An optional string representing a style of initialization `ortho`, `he`/`kaiming`, `xavier`/`glorot`\n    :return: An LSTM\n    """"""\n    if nlayers == 1:\n        dropout = 0.0\n    ndir = 2 if rnntype.startswith(""b"") else 1\n    layer_hsz = hsz // ndir\n    rnn = torch.nn.LSTM(\n        insz, layer_hsz, nlayers, dropout=dropout, bidirectional=True if ndir > 1 else False, batch_first=batch_first\n    )  # , bias=False)\n    if initializer == ""ortho"":\n        nn.init.orthogonal(rnn.weight_hh_l0)\n        nn.init.orthogonal(rnn.weight_ih_l0)\n    elif initializer == ""he"" or initializer == ""kaiming"":\n        nn.init.kaiming_uniform(rnn.weight_hh_l0)\n        nn.init.kaiming_uniform(rnn.weight_ih_l0)\n    elif unif > 0:\n        for weight in rnn.parameters():\n            weight.data.uniform_(-unif, unif)\n    else:\n        nn.init.xavier_uniform_(rnn.weight_hh_l0)\n        nn.init.xavier_uniform_(rnn.weight_ih_l0)\n\n    return rnn\n\n\nclass LSTMEncoderBase(nn.Module):\n    """"""The LSTM encoder is a base for a set of encoders producing various outputs.\n\n    All LSTM encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`)\n\n    *PyTorch Note*: In PyTorch, its more common for the input shape to be temporal length first (`[T, B, H]`) and this\n    is the PyTorch default.  There is an extra parameter in all of these models called `batch_first` which controls this.\n    Currently, the default is time first (`batch_first=False`), which differs from TensorFlow.  To match the TF impl,\n    set `batch_first=True`.\n\n    *PyTorch Note*:\n    Most `LSTMEncoder` variants just define the `forward`.  This module cannot provide the same utility as the\n    TensorFlow `LSTMEncoder` base right now, because because the JIT isnt handling subclassing of forward properly.\n\n    """"""\n\n    def __init__(\n        self,\n        insz: int,\n        hsz: int,\n        nlayers: int,\n        pdrop: float = 0.0,\n        requires_length: bool = True,\n        batch_first: bool = False,\n        unif: float = 0,\n        initializer: str = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input\n        :param hsz: The number of hidden units per LSTM\n        :param nlayers: The number of layers of LSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param batch_first: PyTorch only! Should we do batch first input or time-first input? Defaults to `False` (differs from TF!)\n        :param unif: PyTorch only! Initialization parameters for RNN\n        :param initializer: PyTorch only! A string describing optional initialization type for RNN\n        """"""\n        super().__init__()\n        self.requires_length = requires_length\n        self.batch_first = batch_first\n        self.nlayers = nlayers\n        if nlayers == 1:\n            pdrop = 0.0\n        self.rnn = torch.nn.LSTM(insz, hsz, nlayers, dropout=pdrop, bidirectional=False, batch_first=batch_first)\n        if initializer == ""ortho"":\n            nn.init.orthogonal(self.rnn.weight_hh_l0)\n            nn.init.orthogonal(self.rnn.weight_ih_l0)\n        elif initializer == ""he"" or initializer == ""kaiming"":\n            nn.init.kaiming_uniform(self.rnn.weight_hh_l0)\n            nn.init.kaiming_uniform(self.rnn.weight_ih_l0)\n        elif unif > 0:\n            for weight in self.rnn.parameters():\n                weight.data.uniform_(-unif, unif)\n        else:\n            nn.init.xavier_uniform_(self.rnn.weight_hh_l0)\n            nn.init.xavier_uniform_(self.rnn.weight_ih_l0)\n        self.output_dim = hsz\n\n    # def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n    #    tbc, lengths = tensor_and_lengths(inputs)\n    #    packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n    #    output, hidden = self.rnn(packed)\n    #    output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n    #    return self.output_fn(output, hidden)\n\n    # def output_fn(self, output, state):\n    #    return output, self.extract_top_state(state)\n\n    def extract_top_state(self, state: Tuple[torch.Tensor, torch.Tensor]) -> List[torch.Tensor]:\n        """"""Get a view of the top state of shape [B, H]`\n\n        :param state:\n        :return:\n        """"""\n        # Select the topmost state with -1 and the only direction is forward (select with 0)\n        top = []\n        for s in state:\n            top.append(s.view(self.nlayers, 1, -1, self.output_dim)[-1, 0])\n\n        return top\n\n\nclass LSTMEncoderSequence(LSTMEncoderBase):\n\n    """"""LSTM encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n    *PyTorch Note:* The input shape of is either `[B, T, C]` or `[T, B, C]` depending on the value of `batch_first`,\n    and defaults to `[T, B, C]` for consistency with other PyTorch modules. The output shape is of the same orientation.\n    """"""\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""Take in a tuple of `(sequence, lengths)` and produce and output tensor of the last layer of LSTMs\n\n        The value `S` here is defined as `max(lengths)`, `S <= T`\n\n        :param inputs: sequence of shapes `[B, T, C]` or `[T, B, C]` and a lengths of shape `[B]`\n        :return: A tensor of shape `[B, S, H]` or `[S, B, H]` depending on setting of `batch_first`\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output\n\n\nclass LSTMEncoderWithState(nn.Module):\n\n    """"""LSTM encoder producing the hidden state and the output, where the input doesnt require any padding\n\n    PyTorch note: This type of encoder doesnt inherit the `LSTMEncoderWithState` base\n    """"""\n\n    def __init__(\n        self,\n        insz: int,\n        hsz: int,\n        nlayers: int,\n        pdrop: float = 0.0,\n        batch_first: bool = False,\n        unif: float = 0,\n        initializer: str = None,\n        **kwargs,\n    ):\n        """"""\n        :param insz: The size of the input\n        :param hsz: The number of hidden units per LSTM\n        :param nlayers: The number of layers of LSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param batch_first: PyTorch only! do batch first or time-first input? Defaults to `False` (differs from TF!)\n        :param unif: PyTorch only! Initialization parameters for RNN\n        :param initializer: PyTorch only! A string describing optional initialization type for RNN\n\n        """"""\n        super().__init__()\n        self.requires_length = False\n        self.requires_state = True\n        self.batch_first = batch_first\n        self.nlayers = nlayers\n        if nlayers == 1:\n            pdrop = 0.0\n        self.rnn = torch.nn.LSTM(insz, hsz, nlayers, dropout=pdrop, bidirectional=False, batch_first=batch_first)\n        if initializer == ""ortho"":\n            nn.init.orthogonal(self.rnn.weight_hh_l0)\n            nn.init.orthogonal(self.rnn.weight_ih_l0)\n        elif initializer == ""he"" or initializer == ""kaiming"":\n            nn.init.kaiming_uniform(self.rnn.weight_hh_l0)\n            nn.init.kaiming_uniform(self.rnn.weight_ih_l0)\n        elif unif > 0:\n            for weight in self.rnn.parameters():\n                weight.data.uniform_(-unif, unif)\n        else:\n            nn.init.xavier_uniform_(self.rnn.weight_hh_l0)\n            nn.init.xavier_uniform_(self.rnn.weight_ih_l0)\n        self.output_dim = hsz\n\n    def forward(self, input_and_prev_h: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n\n        :param input_and_prev_h: The input at this timestep and the previous hidden unit or `None`\n        :return: Raw `torch.nn.LSTM` output\n        """"""\n        inputs, hidden = input_and_prev_h\n        output, hidden = self.rnn(inputs, hidden)\n        return output, hidden  ##concat_state_dirs(hidden)\n\n\nclass LSTMEncoderAll(LSTMEncoderBase):\n    """"""LSTM encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a tuple of hidden vector `[L, B, H]` and context vector `[L, B, H]`, respectively\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H]` or `[B, H, S]` , and tuple of hidden `[L, B, H]` and context `[L, B, H]`\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output, hidden\n\n\nclass LSTMEncoderHidden(LSTMEncoderBase):\n\n    """"""LSTM encoder that returns the top hidden state\n\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor of shape `[B, H]` representing the last RNNs hidden state\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return self.extract_top_state(hidden)[0]\n\n\n# TODO: this module only exists in pytorch.  Do we eliminate it or put it in both?\nclass LSTMEncoderSequenceHiddenContext(LSTMEncoderBase):\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output, self.extract_top_state(hidden)\n\n\nclass BiLSTMEncoderBase(nn.Module):\n    """"""BiLSTM encoder base for a set of encoders producing various outputs.\n\n    All BiLSTM encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`).  Because its bidirectional, half of the hidden units given in the\n    constructor will be applied to the forward direction and half to the backward direction, and these will get\n    concatenated.\n\n    *PyTorch Note*: In PyTorch, its more common for the input shape to be temporal length first (`[T, B, H]`) and this\n    is the PyTorch default.  There is an extra parameter in all of these models called `batch_first` which controls this.\n    Currently, the default is time first (`batch_first=False`), which differs from TensorFlow.  To match the TF impl,\n    set `batch_first=True`.\n\n    *PyTorch Note*:\n    Most `BiLSTMEncoder` variants just define the `forward`.  This module cannot provide the same utility as the\n    TensorFlow `BiLSTMEncoder` base right now, because because the JIT isnt handling subclassing of forward properly.\n\n    """"""\n\n    def __init__(\n        self,\n        insz: int,\n        hsz: int,\n        nlayers: int,\n        pdrop: float = 0.0,\n        requires_length: bool = True,\n        batch_first: bool = False,\n        unif: float = 0,\n        initializer: str = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input\n        :param hsz: The number of hidden units per BiLSTM (`hsz//2` used for each direction and concatenated)\n        :param nlayers: The number of layers of BiLSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param batch_first: Should we do batch first input or time-first input? Defaults to `False` (differs from TF!)\n        :param unif: PyTorch only! Initialization parameters for RNN\n        :param initializer: PyTorch only! A string describing optional initialization type for RNN\n        """"""\n        super().__init__()\n        self.requires_length = requires_length\n        self.batch_first = batch_first\n        self.nlayers = nlayers\n        if nlayers == 1:\n            pdrop = 0.0\n        self.rnn = torch.nn.LSTM(insz, hsz // 2, nlayers, dropout=pdrop, bidirectional=True, batch_first=batch_first)\n        if initializer == ""ortho"":\n            nn.init.orthogonal(self.rnn.weight_hh_l0)\n            nn.init.orthogonal(self.rnn.weight_ih_l0)\n        elif initializer == ""he"" or initializer == ""kaiming"":\n            nn.init.kaiming_uniform(self.rnn.weight_hh_l0)\n            nn.init.kaiming_uniform(self.rnn.weight_ih_l0)\n        elif unif > 0:\n            for weight in self.rnn.parameters():\n                weight.data.uniform_(-unif, unif)\n        else:\n            nn.init.xavier_uniform_(self.rnn.weight_hh_l0)\n            nn.init.xavier_uniform_(self.rnn.weight_ih_l0)\n        self.output_dim = hsz\n\n    def extract_top_state(self, state):\n        # Select the topmost state with -1 and the only direction is forward (select with 0)\n        return tuple(s.view(self.nlayers, 1, -1, self.output_dim)[-1, 0] for s in state)\n\n\n# TODO: this module only exists in pytorch.  Do we eliminate it or put it in both?\nclass BiLSTMEncoderSequenceHiddenContext(BiLSTMEncoderBase):\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output, self.extract_top_state(concat_state_dirs(hidden))\n\n\nclass BiLSTMEncoderAll(BiLSTMEncoderBase):\n    """"""BiLSTM encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a tuple of hidden vector `[L, B, H]` and context vector `[L, B, H]`, respectively\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H] or `[B, H, S]` , and tuple of hidden `[L, B, H]` and context `[L, B, H]`\n        """"""\n        tensor, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tensor, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output, concat_state_dirs(hidden)\n\n\nclass BiLSTMEncoderSequence(BiLSTMEncoderBase):\n\n    """"""BiLSTM encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n\n    *PyTorch Note:* The input shape of is either `[B, T, C]` or `[T, B, C]` depending on the value of `batch_first`,\n    and defaults to `[T, B, C]` for consistency with other PyTorch modules. The output shape is of the same orientation.\n    """"""\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""Take in a tuple of `(sequence, lengths)` and produce and output tensor of the last layer of LSTMs\n\n        The value `S` here is defined as `max(lengths)`, `S <= T`\n\n        :param inputs: sequence of shapes `[B, T, C]` or `[T, B, C]` and a lengths of shape `[B]`\n        :return: A tensor of shape `[B, S, H]` or `[S, B, H]` depending on setting of `batch_first`\n        """"""\n        tensor, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tensor, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output\n\n\nclass BiLSTMEncoderHidden(BiLSTMEncoderBase):\n\n    """"""BiLSTM encoder that returns the top hidden state\n\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n    def forward(self, inputs):\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor of shape `[B, H]` representing the last RNNs hidden state\n        """"""\n        tensor, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tensor, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return self.extract_top_state(concat_state_dirs(hidden))[0]\n\n# TODO: Add this to TF or remove\nclass BiLSTMEncoderHiddenContext(BiLSTMEncoderBase):\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return self.extract_top_state(concat_state_dirs(hidden))\n\n\nclass GRUEncoderBase(nn.Module):\n    """"""The GRU encoder is a base for a set of encoders producing various outputs.\n\n    All GRU encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`)\n\n    *PyTorch Note*: In PyTorch, its more common for the input shape to be temporal length first (`[T, B, H]`) and this\n    is the PyTorch default.  There is an extra parameter in all of these models called `batch_first` which controls this.\n    Currently, the default is time first (`batch_first=False`), which differs from TensorFlow.  To match the TF impl,\n    set `batch_first=True`.\n\n    *PyTorch Note*:\n    Most `GRUEncoder` variants just define the `forward`.  This module cannot provide the same utility as the\n    TensorFlow `GRUEncoder` base right now, because because the JIT isnt handling subclassing of forward properly.\n\n    """"""\n\n    def __init__(\n            self,\n            insz: int,\n            hsz: int,\n            nlayers: int,\n            pdrop: float = 0.0,\n            requires_length: bool = True,\n            batch_first: bool = False,\n            unif: float = 0,\n            initializer: str = None,\n            **kwargs,\n    ):\n        """"""Produce a stack of GRUs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input\n        :param hsz: The number of hidden units per GRU\n        :param nlayers: The number of layers of GRUs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param batch_first: PyTorch only! Should we do batch first input or time-first input? Defaults to `False` (differs from TF!)\n        :param unif: PyTorch only! Initialization parameters for RNN\n        :param initializer: PyTorch only! A string describing optional initialization type for RNN\n        """"""\n        super().__init__()\n        self.requires_length = requires_length\n        self.batch_first = batch_first\n        self.nlayers = nlayers\n        if nlayers == 1:\n            pdrop = 0.0\n        self.rnn = torch.nn.GRU(insz, hsz, nlayers, dropout=pdrop, bidirectional=False, batch_first=batch_first)\n        if initializer == ""ortho"":\n            nn.init.orthogonal_(self.rnn.weight_ih_l0)\n            nn.init.orthogonal_(self.rnn.weight_hh_l0)\n        elif initializer == ""he"" or initializer == ""kaiming"":\n            nn.init.kaiming_uniform_(self.rnn.weight_ih_l0)\n            nn.init.kaiming_uniform_(self.rnn.weight_hh_l0)\n        elif unif > 0:\n            for weight in self.rnn.parameters():\n                weight.data.uniform_(-unif, unif)\n        else:\n            nn.init.xavier_uniform_(self.rnn.weight_ih_l0)\n            nn.init.xavier_uniform_(self.rnn.weight_hh_l0)\n        self.output_dim = hsz\n\n    def extract_top_state(self, state: torch.Tensor) -> torch.Tensor:\n        return state[-1]\n\n\nclass GRUEncoderSequence(GRUEncoderBase):\n\n    """"""GRU encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n    *PyTorch Note:* The input shape of is either `[B, T, C]` or `[T, B, C]` depending on the value of `batch_first`,\n    and defaults to `[T, B, C]` for consistency with other PyTorch modules. The output shape is of the same orientation.\n    """"""\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""Take in a tuple of the sequence tensor `[T, B, H]` or `[B, T, H]` and its length, produce output sequence\n\n        :param inputs: A tuple of the sequence tensor and its length\n        :return: A sequence tensor of shape `[T, B, H]` or `[B, T, H]`\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output\n\n\nclass GRUEncoderAll(GRUEncoderBase):\n    """"""GRU encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a hidden vector `[L, B, H]`\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H]` or `[B, H, S]` , and a hidden tensor `[L, B, H]`\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output, hidden\n\n\nclass GRUEncoderHidden(GRUEncoderBase):\n\n    """"""GRU encoder that returns the top hidden state\n\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor of shape `[B, H]` representing the last RNNs hidden state\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return self.extract_top_state(hidden)\n\n\nclass BiGRUEncoderBase(nn.Module):\n    """"""BiGRU encoder base for a set of encoders producing various outputs.\n\n    All BiGRU encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`).  Because its bidirectional, half of the hidden units given in the\n    constructor will be applied to the forward direction and half to the backward direction, and these will get\n    concatenated.\n\n    *PyTorch Note*: In PyTorch, its more common for the input shape to be temporal length first (`[T, B, H]`) and this\n    is the PyTorch default.  There is an extra parameter in all of these models called `batch_first` which controls this.\n    Currently, the default is time first (`batch_first=False`), which differs from TensorFlow.  To match the TF impl,\n    set `batch_first=True`.\n\n    *PyTorch Note*:\n    Most `BiGRUEncoder` variants just define the `forward`.  This module cannot provide the same utility as the\n    TensorFlow `BiGRUEncoder` base right now, because because the JIT isnt handling subclassing of forward properly.\n\n    """"""\n    def __init__(\n            self,\n            insz: int,\n            hsz: int,\n            nlayers: int,\n            pdrop: float = 0.0,\n            requires_length: bool = True,\n            batch_first: bool = False,\n            unif: float = 0,\n            initializer: str = None,\n            **kwargs,\n    ):\n        """"""Produce a stack of GRUs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input\n        :param hsz: The number of hidden units per BiGRU (`hsz//2` used for each direction and concatenated)\n        :param nlayers: The number of layers of BiGRUs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param batch_first: Should we do batch first input or time-first input? Defaults to `False` (differs from TF!)\n        :param unif: PyTorch only! Initialization parameters for RNN\n        :param initializer: PyTorch only! A string describing optional initialization type for RNN\n        """"""\n\n        super().__init__()\n        self.requires_length = requires_length\n        self.batch_first = batch_first\n        self.nlayers = nlayers\n        if nlayers == 1:\n            pdrop = 0.0\n        self.rnn = torch.nn.GRU(insz, hsz // 2, nlayers, dropout=pdrop, bidirectional=True, batch_first=batch_first)\n        if initializer == ""ortho"":\n            nn.init.orthogonal(self.rnn.weight_hh_l0)\n            nn.init.orthogonal(self.rnn.weight_ih_l0)\n        elif initializer == ""he"" or initializer == ""kaiming"":\n            nn.init.kaiming_uniform(self.rnn.weight_hh_l0)\n            nn.init.kaiming_uniform(self.rnn.weight_ih_l0)\n        elif unif > 0:\n            for weight in self.rnn.parameters():\n                weight.data.uniform_(-unif, unif)\n        else:\n            nn.init.xavier_uniform_(self.rnn.weight_hh_l0)\n            nn.init.xavier_uniform_(self.rnn.weight_ih_l0)\n        self.output_dim = hsz\n\n    def extract_top_state(self, state: torch.Tensor) -> torch.Tensor:\n        # Select the topmost state with -1 and the only direction is forward (select with 0)\n        return state[-1]\n\n# TODO: normalize across backends or remove\nclass BiGRUEncoderSequenceHiddenContext(BiGRUEncoderBase):\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output, self.extract_top_state(_cat_dir(hidden))\n\n\nclass BiGRUEncoderAll(BiGRUEncoderBase):\n    """"""BiGRU encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a hidden vector `[L, B, H]`\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H] or `[B, H, S]` , and a hidden vector `[L, B, H]`\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output, _cat_dir(hidden)\n\n\nclass BiGRUEncoderSequence(BiGRUEncoderBase):\n\n    """"""BiGRU encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n    *PyTorch Note:* The input shape of is either `[B, T, C]` or `[T, B, C]` depending on the value of `batch_first`,\n    and defaults to `[T, B, C]` for consistency with other PyTorch modules. The output shape is of the same orientation.\n    """"""\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""Take in a tuple of `(sequence, lengths)` and produce and output tensor of the last layer of GRUs\n\n        The value `S` here is defined as `max(lengths)`, `S <= T`\n\n        :param inputs: sequence of shapes `[B, T, C]` or `[T, B, C]` and a lengths of shape `[B]`\n        :return: A tensor of shape `[B, S, H]` or `[S, B, H]` depending on setting of `batch_first`\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return output\n\n\nclass BiGRUEncoderHidden(BiGRUEncoderBase):\n\n    """"""GRU encoder that returns the top hidden state\n\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n    def forward(self, inputs):\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor of shape `[B, H]` representing the last RNNs hidden state\n        """"""\n        tbc, lengths = inputs\n        packed = torch.nn.utils.rnn.pack_padded_sequence(tbc, lengths, batch_first=self.batch_first)\n        output, hidden = self.rnn(packed)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=self.batch_first)\n        return self.extract_top_state(_cat_dir(hidden))\n\nclass Reduction(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n        pass\n\n\nclass ConcatReduction(Reduction):\n    def __init__(self, output_dims: List[int], axis=-1):\n        super().__init__()\n        self.axis = axis\n        self.output_dim = sum(output_dims)\n\n    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n        return torch.cat(inputs, self.axis)\n\n\nclass SumReduction(Reduction):\n    def __init__(self, output_dims: List[int]):\n        super().__init__()\n        # We could actually project if we needed, or at least should validate\n        self.output_dim = output_dims[0]\n\n    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n        return sum(inputs)\n\n\nclass SumLayerNormReduction(Reduction):\n\n    def __init__(self, output_dims: List[int], layer_norm_eps: float = 1.0e-12):\n        super().__init__()\n        self.output_dim = output_dims[0]\n        self.ln = nn.LayerNorm(self.output_dim, eps=layer_norm_eps)\n\n    def forward(self, inputs: List[torch.Tensor]) -> torch.Tensor:\n        output = sum(inputs)\n        return self.ln(output)\n\n\nclass EmbeddingsStack(nn.Module):\n    def __init__(\n        self,\n        embeddings_dict: Dict[str, nn.Embedding],\n        dropout_rate: float = 0.0,\n        requires_length: bool = False,\n        reduction: Optional[Union[str, nn.Module]] = \'concat\',\n        **kwargs,\n    ):\n        """"""Takes in a dictionary where the keys are the input tensor names, and the values are the embeddings\n        :param embeddings_dict: dictionary of each feature embedding\n        :param dropout_rate: The dropout rate (0.0 means no dropout, 1.0 means complete)\n        """"""\n\n        super().__init__()\n\n        self._keys: List[str] = []\n        embeddings_list = []\n        output_dims = []\n        for k, embedding in embeddings_dict.items():\n\n            embeddings_list.append(embedding)\n            self._keys.append(k)\n            output_dims += [embedding.get_dsz()]\n\n        self.embeddings: nn.ModuleList = nn.ModuleList(embeddings_list)\n        # TODO: should we make a registry of options?\n        if isinstance(reduction, str):\n            if reduction == \'sum\':\n                self.reduction = SumReduction(output_dims)\n            elif reduction == \'sum-layer-norm\':\n                self.reduction = SumLayerNormReduction(output_dims, layer_norm_eps=kwargs.get(\'layer_norm_eps\', 1.0e-12))\n            else:\n                self.reduction = ConcatReduction(output_dims)\n        else:\n            self.reduction = reduction\n        self.dsz = self.reduction.output_dim\n        self.dropout = nn.Dropout(dropout_rate)\n        self.requires_length = requires_length\n\n    def __getitem__(self, item: str) -> nn.Module:\n        idx = self._keys.index(item)\n        if idx < 0:\n            raise Exception(f""Invalid item ({item})"")\n        return self.embeddings[idx]\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        """"""This method performs ""embedding"" of the inputs.  The base method here then concatenates along depth\n        dimension to form word embeddings\n        :return: A 3-d vector where the last dimension is the concatenated dimensions of all embeddings\n        """"""\n        all_embeddings_out = []\n        i = 0\n        for embedding in self.embeddings:\n            k = self._keys[i]\n            x = inputs[k]\n            embeddings_out = embedding(x)\n            all_embeddings_out.append(embeddings_out)\n            i += 1\n        word_embeddings = self.reduction(all_embeddings_out)\n        return self.dropout(word_embeddings)\n\n    def keys(self):\n        return self._keys\n\n    @property\n    def output_dim(self):\n        return self.dsz\n\n    def items(self):\n        for k, v in zip(self.keys(), self.embeddings):\n            yield k, v\n\n\nclass DenseStack(nn.Module):\n    """"""A stack of one or more hidden layers\n    """"""\n\n    def __init__(\n        self,\n        insz: int,\n        hsz: Union[int, List[int]],\n        activation: Union[str, List[str]] = ""relu"",\n        pdrop_value: float = 0.5,\n        init=None,\n        skip_connect=False,\n        layer_norm=False,\n        **kwargs,\n    ):\n        """"""Stack 1 or more hidden layers, optionally (forming an MLP)\n\n        :param insz: The number of input units\n        :param hsz: The number of hidden units\n        :param activation: The name of the activation function to use\n        :param pdrop_value: The dropout probability\n        :param init: The initializer\n        :param skip_connect: whether use skip connection when insz is equal to outsz for a layer\n        :param layer_norm: whether use layer norm in each layer\n\n        """"""\n        super().__init__()\n        hszs = listify(hsz)\n        self.output_dim = hsz[-1]\n        activations = listify(activation)\n        if len(activations) == 1:\n            activations = activations * len(hszs)\n        if len(activations) != len(hszs):\n            raise ValueError(""Number of activations must match number of hidden sizes in a stack!"")\n        current = insz\n        layer_stack = []\n        if layer_norm:\n            layer_norm_eps = kwargs.get(\'layer_norm_eps\', 1e-6)\n        for hsz, activation in zip(hszs, activations):\n            if skip_connect and current == hsz:\n                layer = SkipConnection(current, activation)\n            else:\n                layer = Dense(current, hsz, activation)\n            if layer_norm:\n                layer = nn.Sequential(layer, nn.LayerNorm(hsz, eps=layer_norm_eps))\n            layer_stack.append(WithDropout(layer, pdrop_value))\n            current = hsz\n        self.layer_stack = nn.Sequential(*layer_stack)\n        self.requires_length = False\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        """"""Stack 1 or more hidden layers, optionally (forming an MLP)\n\n        :param inputs: The fixed representation of the model\n\n        :Keyword Arguments:\n        * *hsz* -- (``int``) The number of hidden units (defaults to `100`)\n\n        :return: The final layer\n        """"""\n        return self.layer_stack(inputs)\n\n\nclass VectorSequenceAttention(nn.Module):\n    def __init__(self, hsz: int):\n        super().__init__()\n        self.hsz = hsz\n        self.W_c = nn.Linear(2 * self.hsz, hsz, bias=False)\n\n    def forward(self, query_t, keys_bth, values_bth, keys_mask=None):\n        # Output(t) = B x H x 1\n        # Keys = B x T x H\n        # a = B x T x 1\n        a = self._attention(query_t, keys_bth, keys_mask)\n        attended = self._update(a, query_t, values_bth)\n\n        return attended\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        pass\n\n    def _update(self, a, query_t, values_bth):\n        # a = B x T\n        # Want to apply over context, scaled by a\n        # (B x 1 x T) (B x T x H) = (B x 1 x H)\n        a = a.view(a.size(0), 1, a.size(1))\n        c_t = torch.bmm(a, values_bth).squeeze(1)\n\n        attended = torch.cat([c_t, query_t], -1)\n        attended = torch.tanh(self.W_c(attended))\n        return attended\n\ndef dot_product_attention_weights(query_t: torch.Tensor,\n                                  keys_bth: torch.Tensor,\n                                  keys_mask: torch.Tensor) -> torch.Tensor:\n    a = keys_bth @ query_t.unsqueeze(2)\n    a = a.squeeze(2).masked_fill(keys_mask == MASK_FALSE, -1e9)\n    a = F.softmax(a, dim=-1)\n    return a\n\n\ndef dot_product_attention_weights_lengths(query_t: torch.Tensor,\n                                          keys_bth: torch.Tensor,\n                                          keys_lengths: torch.Tensor) -> torch.Tensor:\n    mask = sequence_mask(keys_lengths, keys_bth.shape[1]).to(keys_bth.device)\n    return dot_product_attention_weights(query_t, keys_bth, mask)\n\n\nclass LuongDotProductAttention(VectorSequenceAttention):\n    def __init__(self, hsz):\n        super().__init__(hsz)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        return dot_product_attention_weights(query_t, keys_bth, keys_mask)\n\n\nclass ScaledDotProductAttention(VectorSequenceAttention):\n    def __init__(self, hsz):\n        super().__init__(hsz)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        a = (keys_bth @ query_t.unsqueeze(2)) / math.sqrt(self.hsz)\n        a = a.squeeze(2).masked_fill(keys_mask == MASK_FALSE, -1e9)\n        a = F.softmax(a, dim=-1)\n        return a\n\n\nclass LuongGeneralAttention(VectorSequenceAttention):\n    def __init__(self, hsz):\n        super().__init__(hsz)\n        self.W_a = nn.Linear(self.hsz, self.hsz, bias=False)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        a = keys_bth @ self.W_a(query_t).unsqueeze(2)\n        a = a.squeeze(2).masked_fill(keys_mask == MASK_FALSE, -1e9)\n        a = F.softmax(a, dim=-1)\n        return a\n\n\nclass BahdanauAttention(VectorSequenceAttention):\n    def __init__(self, hsz):\n        super().__init__(hsz)\n        self.hsz = hsz\n        self.W_a = nn.Linear(self.hsz, self.hsz, bias=False)\n        self.E_a = nn.Linear(self.hsz, self.hsz, bias=False)\n        self.v = nn.Linear(self.hsz, 1, bias=False)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        B, T, H = keys_bth.shape\n        q = self.W_a(query_t.view(-1, self.hsz)).view(B, 1, H)\n        u = self.E_a(keys_bth).view(B, T, H)\n        z = torch.tanh(q + u)\n        a = self.v(z.view(-1, self.hsz)).view(B, T)\n        a.masked_fill(keys_mask == MASK_FALSE, -1e9)\n        a = F.softmax(a, dim=-1)\n        return a\n\n    def _update(self, a, query_t, values_bth):\n        query_t = query_t.view(-1, self.hsz)\n        # a = B x T\n        # Want to apply over context, scaled by a\n        # (B x 1 x T) (B x T x H) = (B x 1 x H) -> (B x H)\n        a = a.view(a.size(0), 1, a.size(1))\n        c_t = (a @ values_bth).squeeze(1)\n        # (B x 2H)\n        attended = torch.cat([c_t, query_t], -1)\n        attended = self.W_c(attended)\n        return attended\n\n\nclass FineTuneModel(nn.Module):\n    def __init__(self, nc, embeddings, stack_model=None):\n        super().__init__()\n        if isinstance(embeddings, dict):\n            self.finetuned = EmbeddingsStack(embeddings)\n        else:\n            self.finetuned = embeddings\n        self.stack_model = stack_model\n        output_dim = self.finetuned.output_dim if stack_model is None else stack_model.output_dim\n        self.output_layer = Dense(output_dim, nc, activation=""log_softmax"")\n\n    def forward(self, inputs):\n        base_layers = self.finetuned(inputs)\n        stacked = self.stack_model(base_layers) if self.stack_model is not None else base_layers\n        return self.output_layer(stacked)\n\n\nclass CompositePooling(nn.Module):\n    """"""Composite pooling allows for multiple sub-modules during pooling to be used in parallel\n    """"""\n\n    def __init__(self, models):\n        """"""\n        Note, this currently requires that each submodel is an eight_mile model with an `output_dim` attr\n        """"""\n        super().__init__()\n        self.models = nn.ModuleList(models)\n        self.output_dim = sum(m.output_dim for m in self.models)\n        self.requires_length = any(getattr(m, ""requires_length"", False) for m in self.models)\n\n    def forward(self, inputs):\n        inputs, lengths = tensor_and_lengths(inputs)\n        pooled = []\n        for sub_model in self.models:\n            if getattr(sub_model, ""requires_length"", False):\n                pooled.append(sub_model((inputs, lengths)))\n            else:\n                pooled.append(sub_model(inputs))\n        return torch.cat(pooled, -1)\n\n\nclass EmbedPoolStackModel(nn.Module):\n    """"""This provides an idiom for classification consisting of multiple phases\n\n    In the first phase, we embed the input tensors, and subsequently pool them to\n    a fixed width representation.  Finally, we allow multiple hidden ""stacking""\n    layers, ultimately ending in a projection to the output space\n\n    """"""\n\n    def __init__(\n        self,\n        nc: int,\n        embeddings: nn.Module,\n        pool_model: nn.Module,\n        stack_model: Optional[nn.Module] = None,\n        output_model: Optional[nn.Module] = None,\n    ):\n        super().__init__()\n        self.embed_model = embeddings\n        self.pool_model = pool_model\n        self.stack_model = stack_model if stack_model else nn.Identity()\n        output_dim = self.pool_model.output_dim if stack_model is None else stack_model.output_dim\n        self.output_layer = Dense(output_dim, nc, activation=""log_softmax"") if output_model is None else output_model\n\n    def forward(self, inputs: Dict[str, torch.Tensor]):\n        lengths = inputs[""lengths""]\n        embedded = self.embed_model(inputs)\n        embedded = (embedded, lengths)\n        pooled = self.pool_model(embedded)\n        stacked = self.stack_model(pooled)\n        return self.output_layer(stacked)\n\n\nclass PassThru(nn.Module):\n\n    def __init__(self, input_dim):\n        super().__init__()\n        self.output_dim = input_dim\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        return inputs\n\n\nclass WithoutLength(nn.Module):\n    """"""Wrapper layer to remove lengths from the input\n    """"""\n\n    def __init__(self, layer: nn.Module):\n        super().__init__()\n        self.layer = layer\n        self.output_dim = self.layer.output_dim if hasattr(self.layer, ""output_dim"") else 0\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        return self.layer(inputs[0])\n\n\nclass WithDropout(nn.Module):\n    """"""Wrapper for any layer that surrounds it with dropout""""""\n\n    def __init__(self, layer: nn.Module, pdrop: float = 0.5, variational=False):\n        """"""Create a dropout wrapper around the given layer\n\n        :param layer: Some sort of layer\n        :param pdrop: A dropout value\n        """"""\n        super().__init__()\n        self.layer = layer\n        self.dropout = VariationalDropout(pdrop) if variational else nn.Dropout(pdrop)\n        self.output_dim = self.layer.output_dim if hasattr(self.layer, ""output_dim"") else 0\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        """"""Apply the layer followed by dropout\n\n        :param inputs: input tensor\n        :return: output transformed by the held layer and subsequent dropout\n        """"""\n        return self.dropout(self.layer(inputs))\n\n\nclass WithDropoutOnFirst(nn.Module):\n    """"""Wrapper for any layer that surrounds it with dropout\n\n    This exists primarily for the LSTMEncoderWithState to allow dropout on the output while\n    passing back the hidden state\n    """"""\n\n    def __init__(self, layer: nn.Module, pdrop: float = 0.5, variational=False):\n        """"""Create a dropout wrapper around the given layer\n\n        :param layer: Some sort of layer\n        :param pdrop: A dropout value\n        """"""\n        super().__init__()\n        self.layer = layer\n        self.dropout = VariationalDropout(pdrop) if variational else nn.Dropout(pdrop)\n        self.output_dim = self.layer.output_dim if hasattr(self.layer, ""output_dim"") else 0\n\n    def forward(self, inputs: Tuple[torch.Tensor]) -> torch.Tensor:\n        """"""Apply the layer followed by dropout\n\n        :param inputs: input tensor\n        :return: output transformed by the held layer and subsequent dropout\n        """"""\n        outputs = self.layer(inputs)\n        return self.dropout(outputs[0]), outputs[1]\n\n\ndef transition_mask(vocab, span_type, s_idx, e_idx, pad_idx=None):\n    """"""Create a mask to enforce span sequence transition constraints.\n\n    Returns a Tensor with valid transitions as a 0 and invalid as a 1 for easy use with `masked_fill`\n    """"""\n    np_mask = transition_mask_np(vocab, span_type, s_idx, e_idx, pad_idx=pad_idx)\n    return torch.from_numpy(np_mask) == 0\n\n\n@torch.jit.script\ndef inplace_assign(data: torch.Tensor, index: torch.Tensor, new_data: torch.Tensor) -> torch.Tensor:\n    new_data = new_data.unsqueeze(0)\n    index = index.expand(1, new_data.size(1))\n    data.scatter_(0, index, new_data)\n    return data\n\n\n@torch.jit.script\ndef i2t(i: int) -> torch.Tensor:\n    return torch.tensor(i).unsqueeze(0)\n\n\n@torch.jit.script\ndef script_viterbi(\n    unary: torch.Tensor, trans: torch.Tensor, start_idx: int, end_idx: int\n) -> Tuple[torch.Tensor, torch.Tensor]:\n\n    seq_len: int = unary.size(0)\n    num_tags: int = unary.size(1)\n    fill_value: float = -1e4\n    alphas = torch.full((num_tags,), fill_value, dtype=unary.dtype, device=unary.device)\n    broadcast_idx = torch.full((num_tags,), start_idx, dtype=torch.long)\n    alphas.scatter_(0, broadcast_idx, torch.zeros((num_tags,)))\n    alphas.unsqueeze_(0)\n    backpointers: torch.Tensor = torch.zeros(num_tags, dtype=torch.long).unsqueeze(0)\n    for i in range(seq_len):\n        unary_t = unary[i, :]\n        next_tag_var = alphas + trans\n        viterbi, best_tag_ids = torch.max(next_tag_var, 1)\n        backpointers = torch.cat([backpointers, best_tag_ids.unsqueeze(0)], 0)\n        alphas = (viterbi + unary_t).unsqueeze(0)\n\n    terminal_vars = alphas.squeeze(0) + trans[end_idx, :]\n    path_score, best_tag_id = torch.max(terminal_vars, 0)\n    best_path = best_tag_id.unsqueeze(0)\n\n    for i in range(unary.size(0)):\n        t = seq_len - i - 1\n        best_tag_id = backpointers[t + 1, best_tag_id]\n        best_path = torch.cat([best_path, best_tag_id.unsqueeze(0)], -1)\n\n    new_path_vec = best_path.flip(0)\n    return new_path_vec[1:], path_score\n\n\nclass ViterbiBatchSize1(nn.Module):\n    def __init__(self, start_idx: int, end_idx: int):\n        super().__init__()\n        self.start_idx = start_idx\n        self.end_idx = end_idx\n\n    def forward(self, unary: torch.Tensor, trans: torch.Tensor, _: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        unary = unary.squeeze(1)\n        trans = trans.squeeze(0)\n        path, score = script_viterbi(unary, trans, self.start_idx, self.end_idx)\n        return path.unsqueeze(1), score\n\n\nclass Viterbi(nn.Module):\n    def __init__(self, start_idx: int, end_idx: int):\n        super().__init__()\n        self.start_idx = start_idx\n        self.end_idx = end_idx\n        # r, start_idx: int, end_idx: int, norm = lambda x, y: x\n\n    def forward(\n        self, unary: torch.Tensor, trans: torch.Tensor, lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Do Viterbi decode on a batch.\n\n        :param unary: torch.FloatTensor: [T, B, N]\n        :param trans: torch.FloatTensor: [1, N, N]\n        :param norm: Callable: This function should take the initial and a dim to\n            normalize along.\n\n        :return: torch.LongTensor: [T, B] the padded paths\n        :return: torch.FloatTensor: [B] the path scores\n        """"""\n        seq_len, batch_size, tag_size = unary.size()\n        min_length = torch.min(lengths)\n        backpointers = []\n\n        # Alphas: [B, 1, N]\n        alphas = torch.full((batch_size, 1, tag_size), -1e4, device=unary.device)\n        alphas[:, 0, self.start_idx] = 0\n        # alphas = self.norm(alphas)\n\n        for i, unary_t in enumerate(unary):\n            next_tag_var = alphas + trans\n            viterbi, best_tag_ids = torch.max(next_tag_var, 2)\n            backpointers.append(best_tag_ids)\n            new_alphas = viterbi + unary_t\n            new_alphas.unsqueeze_(1)\n            # This part generates a warning\n            if i >= min_length:\n                mask = (i < lengths).view(-1, 1, 1)\n                alphas = alphas.masked_fill(mask, 0) + new_alphas.masked_fill(mask == MASK_FALSE, 0)\n            else:\n                alphas = new_alphas\n\n        # Add end tag\n        terminal_var = alphas.squeeze(1) + trans[:, self.end_idx, :]\n        path_score, best_tag_id = torch.max(terminal_var, 1)\n        # Flip lengths\n        rev_len = seq_len - lengths - 1\n\n        best_path = [best_tag_id]\n        for i in range(len(backpointers)):\n            t = len(backpointers) - i - 1\n            backpointer_t = backpointers[t]\n            # Get new best tag candidate\n            new_best_tag_id = backpointer_t.gather(1, best_tag_id.unsqueeze(1)).squeeze(1)\n            # We are going backwards now, if flipped length was passed\n            # these you aren\'t in your real results yet\n            mask = i > rev_len\n            best_tag_id = best_tag_id.masked_fill(mask, 0) + new_best_tag_id.masked_fill(mask == MASK_FALSE, 0)\n            best_path.append(best_tag_id)\n        _ = best_path.pop()\n        best_path.reverse()\n        best_path = torch.stack(best_path)\n        # Mask out the extra tags (This might be pointless given thathatt anything that\n        # will use this as a dense tensor downstream will mask it itself?)\n        seq_mask = sequence_mask(lengths, seq_len).to(best_path.device).transpose(0, 1)\n        best_path = best_path.masked_fill(seq_mask == MASK_FALSE, 0)\n        return best_path, path_score\n\n\nclass ViterbiLogSoftmaxNorm(Viterbi):\n    def forward(\n        self, unary: torch.Tensor, trans: torch.Tensor, lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Do Viterbi decode on a batch.\n\n        :param unary: torch.FloatTensor: [T, B, N]\n        :param trans: torch.FloatTensor: [1, N, N]\n        :param norm: Callable: This function should take the initial and a dim to\n            normalize along.\n\n        :return: torch.LongTensor: [T, B] the padded paths\n        :return: torch.FloatTensor: [B] the path scores\n        """"""\n        seq_len, batch_size, tag_size = unary.size()\n        min_length = torch.min(lengths)\n        backpointers = []\n\n        # Alphas: [B, 1, N]\n        alphas = torch.full((batch_size, 1, tag_size), -1e4, device=unary.device)\n        alphas[:, 0, self.start_idx] = 0\n        alphas = F.log_softmax(alphas, dim=-1)\n\n        for i, unary_t in enumerate(unary):\n            next_tag_var = alphas + trans\n            viterbi, best_tag_ids = torch.max(next_tag_var, 2)\n            backpointers.append(best_tag_ids)\n            new_alphas = viterbi + unary_t\n            new_alphas.unsqueeze_(1)\n            if i >= min_length:\n                mask = (i < lengths).view(-1, 1, 1)\n                alphas = alphas.masked_fill(mask, 0) + new_alphas.masked_fill(mask == MASK_FALSE, 0)\n            else:\n                alphas = new_alphas\n\n        # Add end tag\n        terminal_var = alphas.squeeze(1) + trans[:, self.end_idx, :]\n        path_score, best_tag_id = torch.max(terminal_var, 1)\n        # Flip lengths\n        rev_len = seq_len - lengths - 1\n\n        best_path = [best_tag_id]\n        for i in range(len(backpointers)):\n            t = len(backpointers) - i - 1\n            backpointer_t = backpointers[t]\n            # Get new best tag candidate\n            new_best_tag_id = backpointer_t.gather(1, best_tag_id.unsqueeze(1)).squeeze(1)\n            # We are going backwards now, if flipped length was passed\n            # these you aren\'t in your real results yet\n            mask = i > rev_len\n            best_tag_id = best_tag_id.masked_fill(mask, 0) + new_best_tag_id.masked_fill(mask == MASK_FALSE, 0)\n            best_path.append(best_tag_id)\n        _ = best_path.pop()\n        best_path.reverse()\n        best_path = torch.stack(best_path)\n        # Mask out the extra tags (This might be pointless given that anything that\n        # will use this as a dense tensor downstream will mask it itself?)\n        seq_mask = sequence_mask(lengths).to(best_path.device).transpose(0, 1)\n        best_path = best_path.masked_fill(seq_mask == MASK_FALSE, 0)\n        return best_path, path_score\n\n\ndef ident(x):\n    return x\n\n\nclass TaggerGreedyDecoder(nn.Module):\n    def __init__(\n        self,\n        num_tags: int,\n        constraint_mask: Optional[torch.Tensor] = None,\n        batch_first: bool = True,\n        reduction: str = ""batch"",\n    ):\n        """"""A Greedy decoder and loss module for taggers.\n\n        :param num_tags: `int` The number of output classes\n        :param constraint_mask: `Tensor[1, N, N]` A mask with valid transitions as 1 and invalid as 0\n        :param batch_first: `bool` Should the batch dimensions be first?\n        :param reduction: `str` Should the loss be calculated at the token level or batch level\n        """"""\n        super().__init__()\n        self.num_tags = num_tags\n\n        if constraint_mask is not None:\n            constraint_mask = F.log_softmax(\n                torch.zeros(constraint_mask.shape).masked_fill(constraint_mask, -1e4), dim=1\n            )\n            self.register_buffer(""constraint_mask"", constraint_mask)\n        else:\n            self.constraint_mask = None\n        # FIXME: we cant do it like this if using TorchScript\n        self.to_batch_first = ident if batch_first else tbh2bth\n        self.to_time_first = bth2tbh if batch_first else ident\n        self.batch_first = batch_first\n        self.loss = SequenceLoss(LossFn=nn.CrossEntropyLoss, avg=reduction)\n        self.viterbi = ViterbiLogSoftmaxNorm(Offsets.GO, Offsets.EOS)\n\n    @property\n    def transitions(self):\n        return self.constraint_mask\n\n    def neg_log_loss(self, inputs, tags, lengths):\n        unaries = self.to_batch_first(inputs)\n        tags = self.to_batch_first(tags)\n        return self.loss(unaries, tags)\n\n    def forward(self, inputs) -> torch.Tensor:\n        unaries, lengths = tensor_and_lengths(inputs)\n        # If there is a constraint mask do a masked viterbi\n        if self.constraint_mask is not None:\n            probv = self.to_time_first(unaries)\n            probv = F.log_softmax(probv, dim=-1)\n            preds, scores = self.viterbi(probv, self.constraint_mask, lengths)\n            if self.batch_first:\n                return tbh2bth(preds)  # , scores\n            else:\n                return preds\n        else:\n            # Decoding doesn\'t care about batch/time first\n            _, preds = torch.max(unaries, -1)\n            mask = sequence_mask(lengths, unaries.shape[1]).to(preds.device)\n            # The mask gets generated as batch first\n            mask = mask if self.batch_first else mask.transpose(0, 1)\n            preds = preds.masked_fill(mask == MASK_FALSE, 0)\n        return preds  # , None\n\n    def extra_repr(self) -> str:\n        str_ = f""n_tags={self.num_tags}, batch_first={self.batch_first}""\n        if self.constraint_mask is not None:\n            str_ += "", constrained=True""\n        return str_\n\n\nclass CRF(nn.Module):\n    def __init__(\n        self,\n        num_tags: int,\n        constraint_mask: Optional[torch.Tensor] = None,\n        batch_first: bool = True,\n        idxs: Tuple[int, int] = (Offsets.GO, Offsets.EOS),\n    ):\n        """"""Initialize the object.\n        :param num_tags: int, The number of tags in your output (emission size)\n        :param constraint: torch.ByteTensor, Constraints on the transitions [1, N, N]\n        :param idxs: Tuple(int. int), The index of the start and stop symbol\n            in emissions.\n        :param batch_first: bool, if the input [B, T, ...] or [T, B, ...]\n\n        Note:\n            if idxs is none then the CRF adds these symbols to the emission\n            vectors and n_tags is assumed to be the number of output tags.\n            if idxs is not none then the first element is assumed to be the\n            start index and the second idx is assumed to be the end index. In\n            this case n_tags is assumed to include the start and end symbols.\n        """"""\n        super().__init__()\n        self.start_idx, self.end_idx = idxs\n        self.num_tags = num_tags\n        if constraint_mask is not None:\n            self.register_buffer(""constraint_mask"", constraint_mask)\n        else:\n            self.constraint_mask = None\n\n        self.transitions_p = nn.Parameter(torch.Tensor(1, self.num_tags, self.num_tags).zero_())\n        self.batch_first = batch_first\n        self.viterbi = Viterbi(self.start_idx, self.end_idx)\n\n    def extra_repr(self) -> str:\n        str_ = ""n_tags=%d, batch_first=%s"" % (self.num_tags, self.batch_first)\n        if self.constraint_mask is not None:\n            str_ += "", constrained=True""\n        return str_\n\n    @property\n    def transitions(self):\n        if self.constraint_mask is not None:\n            return self.transitions_p.masked_fill(self.constraint_mask, -1e4)\n        return self.transitions_p\n\n    def neg_log_loss(self, unary, tags, lengths):\n        """"""Neg Log Loss with a Batched CRF.\n\n        :param unary: torch.FloatTensor: [T, B, N] or [B, T, N]\n        :param tags: torch.LongTensor: [T, B] or [B, T]\n        :param lengths: torch.LongTensor: [B]\n\n        :return: torch.FloatTensor: [B]\n        """"""\n        # Convert from [B, T, N] -> [T, B, N]\n        if self.batch_first:\n            unary = unary.transpose(0, 1)\n            tags = tags.transpose(0, 1)\n        _, batch_size, _ = unary.size()\n        fwd_score = self._forward_alg(unary, lengths)\n        gold_score = self.score_sentence(unary, tags, lengths)\n\n        loss = fwd_score - gold_score\n        batch_loss = torch.mean(loss)\n        return batch_loss\n\n    def score_sentence(self, unary: torch.Tensor, tags: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n        """"""Score a batch of sentences.\n\n        :param unary: torch.FloatTensor: [T, B, N]\n        :param tags: torch.LongTensor: [T, B]\n        :param lengths: torch.LongTensor: [B]\n        :param min_length: torch.LongTensor: []\n\n        :return: torch.FloatTensor: [B]\n        """"""\n        batch_size = lengths.shape[0]\n        assert lengths.shape[0] == unary.shape[1]\n\n        trans = self.transitions.squeeze(0)  # [N, N]\n        start = torch.full((1, batch_size), self.start_idx, dtype=tags.dtype, device=tags.device)  # [1, B]\n        tags = torch.cat([start, tags], 0)  # [T + 1, B]\n\n        # Unfold gives me all slices of size 2 (this tag next tag) from dimension T\n        tag_pairs = tags.unfold(0, 2, 1)\n        # Move the pair dim to the front and split it into two\n        indices = tag_pairs.permute(2, 0, 1).chunk(2)\n        trans_score = trans[[indices[1], indices[0]]].squeeze(0)\n        # Pull out the values of the tags from the unary scores.\n        unary_score = unary.gather(2, tags[1:].unsqueeze(-1)).squeeze(-1)\n        mask = sequence_mask(lengths).transpose(0, 1).to(tags.device)\n        scores = unary_score + trans_score\n        scores = scores.masked_fill(mask == MASK_FALSE, 0)\n        scores = scores.sum(0)\n\n        eos_scores = trans[self.end_idx, tags.gather(0, lengths.unsqueeze(0)).squeeze(0)]\n        scores = scores + eos_scores\n        return scores\n\n    def _forward_alg(self, unary: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n        """"""For CRF forward on a batch.\n\n        :param unary: torch.FloatTensor: [T, B, N]\n        :param lengths: torch.LongTensor: [B]\n\n        :return: torch.FloatTensor: [B]\n        """"""\n        # alphas: [B, 1, N]\n        min_length = torch.min(lengths)\n        batch_size = lengths.shape[0]\n        lengths.shape[0] == unary.shape[1]\n        alphas = torch.full((batch_size, 1, self.num_tags), -1e4, device=unary.device)\n        alphas[:, 0, self.start_idx] = 0.0\n        # alphas.requires_grad = True\n\n        trans = self.transitions  # [1, N, N]\n\n        for i, unary_t in enumerate(unary):\n            # unary_t: [B, N]\n            unary_t = unary_t.unsqueeze(2)  # [B, N, 1]\n            # Broadcast alphas along the rows of trans\n            # Broadcast trans along the batch of alphas\n            # [B, 1, N] + [1, N, N] -> [B, N, N]\n            # Broadcast unary_t along the cols of result\n            # [B, N, N] + [B, N, 1] -> [B, N, N]\n            scores = alphas + trans + unary_t\n            new_alphas = vec_log_sum_exp(scores, 2).transpose(1, 2)\n            # If we haven\'t reached your length zero out old alpha and take new one.\n            # If we are past your length, zero out new_alpha and keep old one.\n\n            if i >= min_length:\n                mask = (i < lengths).view(-1, 1, 1)\n                alphas = alphas.masked_fill(mask, 0) + new_alphas.masked_fill(mask == MASK_FALSE, 0)\n            else:\n                alphas = new_alphas\n\n        terminal_vars = alphas + trans[:, self.end_idx]\n        alphas = vec_log_sum_exp(terminal_vars, 2)\n        return alphas.view(batch_size)\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        unary, lengths = inputs\n        if self.training:\n            if self.batch_first:\n                unary = unary.transpose(0, 1)\n            forward = self._forward_alg(unary, lengths)\n            # if self.batch_first:\n            #    forward = forward.transpose(0, 1)\n            return forward\n        with torch.no_grad():\n            return self.decode(unary, lengths)[0]\n\n    @jit.export\n    def decode(self, unary: torch.Tensor, lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Do Viterbi decode on a batch.\n\n        :param unary: torch.FloatTensor: [T, B, N] or [B, T, N]\n        :param lengths: torch.LongTensor: [B]\n\n        :return: torch.LongTensor: [B] the paths\n        :return: torch.FloatTensor: [B] the path score\n        """"""\n        if self.batch_first:\n            unary = unary.transpose(0, 1)\n        trans = self.transitions  # [1, N, N]\n        path, score = self.viterbi(unary, trans, lengths)\n        if self.batch_first:\n            path = path.transpose(0, 1)\n        return path, score\n\n\nclass SequenceModel(nn.Module):\n    def __init__(self, nc: int, embeddings: nn.Module, transducer: nn.Module, decoder: Optional[nn.Module] = None):\n        super().__init__()\n        self.embed_model = embeddings\n        self.transducer_model = transducer\n        # TODO: make this a separate model!\n        if transducer.output_dim != nc:\n            self.proj_layer = Dense(transducer.output_dim, nc)\n        else:\n            self.proj_layer = nn.Identity()\n        self.decoder_model = decoder\n\n    def transduce(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        lengths = inputs[""lengths""]\n\n        embedded = self.embed_model(inputs)\n        embedded = (embedded, lengths)\n        # transduced = self.transducer_model(embedded)\n        transduced = self.proj_layer(self.transducer_model(embedded))\n        return transduced\n\n    def decode(self, transduced: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n        return self.decoder_model((transduced, lengths))\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        pass\n\n\nclass TagSequenceModel(SequenceModel):\n    def __init__(self, nc: int, embeddings: nn.Module, transducer: nn.Module, decoder: Optional[nn.Module] = None):\n        decoder_model = CRF(nc, batch_first=True) if decoder is None else decoder\n        super().__init__(nc, embeddings, transducer, decoder_model)\n\n    def neg_log_loss(self, unary: torch.Tensor, tags: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n        return self.decoder_model.neg_log_loss(unary, tags, lengths)\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n        transduced = self.transduce(inputs)\n        path = self.decode(transduced, inputs[""lengths""])\n        return path\n\n\nclass LangSequenceModel(nn.Module):\n    def __init__(\n        self,\n        nc: int,\n        embeddings: nn.Module,\n        transducer: nn.Module,\n        decoder: Optional[nn.Module] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__()\n        self.embed_model = embeddings\n        self.transducer_model = transducer\n        if hasattr(transducer, ""requires_state"") and transducer.requires_state:\n            self._call = self._call_with_state\n            self.requires_state = True\n        else:\n            self._call = self._call_without_state\n            self.requires_state = False\n        self.output_layer = nn.Linear(self.transducer_model.output_dim, nc)\n        self.decoder_model = decoder\n\n    def forward(self, inputs: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        return self._call(inputs)\n\n    def _call_with_state(self, inputs: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n\n        h = inputs[""h""]\n\n        embedded = self.embed_model(inputs)\n        transduced, hidden = self.transducer_model((embedded, h))\n        transduced = self.output_layer(transduced)\n        return transduced, hidden\n\n    def _call_without_state(self, inputs: Dict[str, torch.Tensor]) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        embedded = self.embed_model(inputs)\n        transduced = self.transducer_model((embedded, None))\n        transduced = self.output_layer(transduced)\n        return transduced, None\n\n\ndef pytorch_embedding(weights: torch.Tensor, finetune: bool = True) -> nn.Embedding:\n    """"""Creation function for making an nn.Embedding with the given weights\n\n    :param weights: The weights to use\n    :param finetune: Should we fine-tune the embeddings or freeze them\n    """"""\n    lut = nn.Embedding(weights.shape[0], weights.shape[1], padding_idx=Offsets.PAD)\n    del lut.weight\n    lut.weight = nn.Parameter(torch.FloatTensor(weights), requires_grad=finetune)\n    return lut\n\n\ndef subsequent_mask(size: int):\n    """"""\n    Creates a lower triangular mask to mask future\n\n    :param size: Temporal length\n    :return: A tensor of type `uint8` that is 1s along diagonals and below, zero  o.w\n    """"""\n    attn_shape = (1, 1, size, size)\n    sub_mask = np.tril(np.ones(attn_shape)).astype(""uint8"")\n    return torch.from_numpy(sub_mask)\n\n\nclass SequenceSequenceAttention(nn.Module):\n    def __init__(self, hsz: int = None, pdrop: float = 0.1, **kwargs):\n        super().__init__()\n        self.hsz = hsz\n        self.dropout = nn.Dropout(pdrop)\n        self.attn = None\n\n    def forward(self, qkvm: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        query, key, value, mask = qkvm\n        a = self._attention(query, key, mask)\n        self.attn = a\n        a = self.dropout(a)\n        return self._update(a, value)\n\n    def _attention(self, query: torch.Tensor, key: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        pass\n\n    def _update(self, a: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n        """"""Attention weights are applied for each value, but in a series of efficient matrix operations.\n\n        In the case of self-attention, the key and query (used to create the attention weights)\n        and values are all low order projections of the same input.\n\n        :param a: The attention weights [B, H, T, T]\n        :param values: The values [B, H, T, D]\n        :returns: A tensor of shape [B, H, T, D]\n        """"""\n        return torch.matmul(a, value)\n\n\nclass SeqScaledDotProductAttention(SequenceSequenceAttention):\n    def __init__(self, pdrop: float = 0.1, **kwargs):\n        super().__init__(pdrop=pdrop, **kwargs)\n\n    def _attention(self, query: torch.Tensor, key: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        """"""Scaled dot product attention, as defined in https://arxiv.org/abs/1706.03762\n\n        We apply the query to the keys to receive our weights via softmax in a series of efficient\n        matrix operations. In the case of self-attntion the key and query are all low order\n        projections of the same input.\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :return: A tensor that is (BxHxTxT)\n        """"""\n        # (., H, T, T) = (., H, T, D) x (., H, D, T)\n        d_k = query.size(-1)\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == MASK_FALSE, -1e9)\n        return F.softmax(scores, dim=-1)\n\n\nclass SeqDotProductAttention(SequenceSequenceAttention):\n    def __init__(self, pdrop: float = 0.1, **kwargs):\n        super().__init__(pdrop=pdrop, **kwargs)\n\n    def _attention(self, query: torch.Tensor, key: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        if mask is not None:\n            scores = scores.masked_fill(mask == MASK_FALSE, -1e9)\n        return F.softmax(scores, dim=-1)\n\n\nclass SequenceSequenceRelativeAttention(nn.Module):\n    """"""This form of attention is specified in Shaw et al 2018: https://www.aclweb.org/anthology/N18-2074.pdf\n\n    """"""\n\n    def __init__(self, hsz: int = None, pdrop: float = 0.1, **kwargs):\n        super().__init__()\n        self.hsz = hsz\n        self.dropout = nn.Dropout(pdrop)\n        self.attn = None\n\n    def forward(\n        self, q_k_v_ek_ev_m: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]\n    ) -> torch.Tensor:\n        """"""Take in a tuple of tensors corresponding to the query, key, value, edges_key, edges_value and mask variables\n\n        :param q_k_v_ek_ev_m: A tuple consisting of query, key, value, `edges_key`, `edges_value` and `mask` respectively\n        :return: An updated value Tensor\n        """"""\n        query, key, value, edges_key, edges_value, mask = q_k_v_ek_ev_m\n        a = self._attention(query, key, edges_key, mask)\n        self.attn = a\n        a = self.dropout(a)\n        return self._update(a, value, edges_value)\n\n    def _attention(\n        self, query: torch.Tensor, key: torch.Tensor, edges_key: torch.Tensor, mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        pass\n\n    def _update(self, a: torch.Tensor, value: torch.Tensor, edges_value: torch.Tensor) -> torch.Tensor:\n        """"""Attention weights are applied for each value, but in a series of efficient matrix operations.\n\n        In the case of self-attention, the key and query (used to create the attention weights)\n        and values are all low order projections of the same input.\n\n        :param a: The attention weights [B, H, T, T]\n        :param value: The values [B, H, T, D]\n        :param edge_value: The edge values [T, T, D]\n        :returns: A tensor of shape [B, H, T, D]\n        """"""\n        B, H, T, D = value.shape\n        updated_values = torch.matmul(a, value)\n        a = a.view(B * H, T, T).transpose(0, 1)  # (T, BxH, T)\n        t = torch.matmul(a, edges_value)  # (T, BxH, D)\n        update_edge_values = t.transpose(0, 1).view(B, H, T, D)\n        return updated_values + update_edge_values\n\n\nclass SeqScaledDotProductRelativeAttention(SequenceSequenceRelativeAttention):\n    def __init__(self, pdrop: float = 0.1, **kwargs):\n        super().__init__(pdrop=pdrop, **kwargs)\n\n    def _attention(\n        self, query: torch.Tensor, key: torch.Tensor, edges_key: torch.Tensor, mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        """"""Scaled dot product attention, as defined in https://arxiv.org/abs/1706.03762\n\n        We apply the query to the keys to receive our weights via softmax in a series of efficient\n        matrix operations. In the case of self-attntion the key and query are all low order\n        projections of the same input.\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :param edges_key: a matrix of relative embeddings between each word in a sequence [TxTxD]\n        :return: A tensor that is (BxHxTxT)\n        """"""\n        # (., H, T, T) = (., H, T, D) x (., H, D, T)\n        B, H, T, d_k = query.shape\n        scores_qk = torch.matmul(query, key.transpose(-2, -1))\n        tbhd = query.reshape(B * H, T, d_k).transpose(0, 1)\n        scores_qek = torch.matmul(tbhd, edges_key.transpose(-2, -1))\n        scores_qek = scores_qek.transpose(0, 1).view(B, H, T, T)\n        scores = (scores_qk + scores_qek) / math.sqrt(d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == MASK_FALSE, -1e9)\n        return F.softmax(scores, dim=-1)\n\n\nclass SeqDotProductRelativeAttention(SequenceSequenceRelativeAttention):\n    def __init__(self, pdrop: float = 0.1, **kwargs):\n        super().__init__(pdrop=pdrop, **kwargs)\n\n    def _attention(\n        self, query: torch.Tensor, key: torch.Tensor, edges_key: torch.Tensor, mask: Optional[torch.Tensor] = None\n    ) -> torch.Tensor:\n        B, H, T, d_k = query.shape\n        scores_qk = torch.matmul(query, key.transpose(-2, -1))\n        tbhd = query.reshape(B * H, T, d_k).transpose(0, 1)\n        scores_qek = torch.matmul(tbhd, edges_key.transpose(-2, -1))\n        scores_qek = scores_qek.transpose(0, 1).view(B, H, T, T)\n        scores = scores_qk + scores_qek\n        if mask is not None:\n            scores = scores.masked_fill(mask == MASK_FALSE, -1e9)\n        return F.softmax(scores, dim=-1)\n\n\nclass SeqBahdanauAttention(SequenceSequenceAttention):\n    def __init__(self, hsz: int, pdrop: float = 0.1, **kwargs):\n        super().__init__(hsz, pdrop=pdrop, **kwargs)\n        self.V = pytorch_linear(self.hsz, 1, bias=False)\n\n    def _attention(self, query: torch.Tensor, key: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        # [B, H, T, 1, D] + [B, H, 1, T, D] = [B, H, T, T, D]\n        additive = query.unsqueeze(-2) + key.unsqueeze(-3)\n        non_linear = torch.tanh(additive)\n        # [B, H, T, T, D] @ [D, 1] = [B, H, T, T, 1]\n        scores = self.V(non_linear)\n        # [B, H, T, T]\n        scores = scores.squeeze(-1)\n        return F.softmax(scores, dim=-1)\n\n\nclass MultiHeadedAttention(nn.Module):\n    """"""\n    Multi-headed attention from https://arxiv.org/abs/1706.03762 via http://nlp.seas.harvard.edu/2018/04/03/attention.html\n\n    Multi-headed attention provides multiple looks of low-order projections K, Q and V using an attention function\n    (specifically `scaled_dot_product_attention` in the paper.  This allows multiple relationships to be illuminated\n    via attention on different positional and representational information from each head.\n\n    The number of heads `h` times the low-order projection dim `d_k` is equal to `d_model` (which is asserted upfront).\n    This means that each weight matrix can be simply represented as a linear transformation from `d_model` to `d_model`,\n    and partitioned into heads after the fact.\n\n    Finally, an output projection is applied which brings the output space back to `d_model`, in preparation for the\n    sub-sequent `FFN` sub-layer.\n\n    There are 3 uses of multi-head attention in the Transformer.\n    For encoder-decoder layers, the queries come from the previous decoder layer, and the memory keys come from\n    the encoder.  For encoder layers, the K, Q and V all come from the output of the previous layer of the encoder.\n    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using\n    future values\n    """"""\n\n    def __init__(\n        self, num_heads: int, d_model: int, dropout: float = 0.1, scale: bool = False, d_k: Optional[int] = None\n    ):\n        """"""Constructor for multi-headed attention\n\n        :param h: The number of heads\n        :param d_model: The model hidden size\n        :param dropout (``float``): The amount of dropout to use\n        :param scale: Should we scale the dot product attention\n        :param d_k: The low-order project per head.  This is normally `d_model // num_heads` unless set explicitly\n        """"""\n        super().__init__()\n        if d_k is None:\n            self.d_k = d_model // num_heads\n            if d_model % num_heads != 0:\n                raise Exception(f""d_model ({d_model}) must be evenly divisible by num_heads ({num_heads})"")\n        else:\n            self.d_k = d_k\n        self.h = num_heads\n        self.w_Q = Dense(d_model, self.d_k * self.h)\n        self.w_K = Dense(d_model, self.d_k * self.h)\n        self.w_V = Dense(d_model, self.d_k * self.h)\n        self.w_O = Dense(self.d_k * self.h, d_model)\n        if scale:\n            self.attn_fn = SeqScaledDotProductAttention(dropout)\n        else:\n            self.attn_fn = SeqDotProductAttention(dropout)\n        self.attn = None\n\n    def forward(self, qkvm: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""Low-order projections of query, key and value into multiple heads, then attention application and dropout\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param value: a set of values from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :return: Multi-head attention output, result of attention application to sequence (B, T, d_model)\n        """"""\n        query, key, value, mask = qkvm\n        batchsz = query.size(0)\n\n        # (B, H, T, D)\n        query = self.w_Q(query).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n        key = self.w_K(key).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n        value = self.w_V(value).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n\n        x = self.attn_fn((query, key, value, mask))\n        self.attn = self.attn_fn.attn\n\n        x = x.transpose(1, 2).contiguous().view(batchsz, -1, self.h * self.d_k)\n        return self.w_O(x)\n\n\nclass MultiHeadedRelativeAttention(nn.Module):\n    """"""\n    Multi-headed relative attention from Shaw et al 2018 (https://www.aclweb.org/anthology/N18-2074.pdf)\n\n    This method follows the same approach of MultiHeadedAttention, but it computes Relative Position Representations (RPR)\n    which are used as part of the attention computations.  To facilitate this, the model has its own internal\n    embeddings lookup table, and it has an updated computation for both the attention weights and the application\n    of those weights to follow them.\n\n    """"""\n\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        rpr_k: int,\n        dropout: float = 0.1,\n        scale: bool = False,\n        d_k: Optional[int] = None,\n    ):\n        """"""Constructor for multi-headed attention\n\n        :param h: The number of heads\n        :param d_model: The model hidden size\n        :param dropout (``float``): The amount of dropout to use\n        :param scale: Should we scale the dot product attention\n        :param d_k: The low-order project per head.  This is normally `d_model // num_heads` unless set explicitly\n        """"""\n        super().__init__()\n\n        if d_k is None:\n            self.d_k = d_model // num_heads\n            if d_model % num_heads != 0:\n                raise Exception(f""d_model ({d_model}) must be evenly divisible by num_heads ({num_heads})"")\n        else:\n            self.d_k = d_k\n\n        self.rpr_k = rpr_k\n        self.rpr_key = nn.Embedding(2 * rpr_k + 1, self.d_k)\n        self.rpr_value = nn.Embedding(2 * rpr_k + 1, self.d_k)\n\n        self.h = num_heads\n        self.w_Q = Dense(d_model, self.d_k * self.h)\n        self.w_K = Dense(d_model, self.d_k * self.h)\n        self.w_V = Dense(d_model, self.d_k * self.h)\n        self.w_O = Dense(self.d_k * self.h, d_model)\n        if scale:\n            self.attn_fn = SeqScaledDotProductRelativeAttention(dropout)\n        else:\n            self.attn_fn = SeqDotProductRelativeAttention(dropout)\n        self.attn = None\n\n    def make_rpr(self, seq_len, device) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Create a matrix shifted by self.rpr_k and bounded between 0 and 2*self.rpr_k to provide 0-based indexing for embedding\n        """"""\n        seq = torch.arange(seq_len).to(device)\n        window_len = 2 * self.rpr_k\n        edges = seq.view(1, -1) - seq.view(-1, 1) + self.rpr_k\n        edges = torch.clamp(edges, 0, window_len)\n        return self.rpr_key(edges), self.rpr_value(edges)\n\n    def forward(self, qkvm: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""Low-order projections of query, key and value into multiple heads, then attention application and dropout\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param value: a set of values from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :return: Multi-head attention output, result of attention application to sequence (B, T, d_model)\n        """"""\n        query, key, value, mask = qkvm\n        batchsz = query.size(0)\n        seq_len = query.size(1)\n\n        # (B, H, T, D)\n        query = self.w_Q(query).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n        key = self.w_K(key).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n        value = self.w_V(value).view(batchsz, -1, self.h, self.d_k).transpose(1, 2)\n\n        rpr_key, rpr_value = self.make_rpr(seq_len, query.device)\n        x = self.attn_fn((query, key, value, rpr_key, rpr_value, mask))\n        self.attn = self.attn_fn.attn\n\n        x = x.transpose(1, 2).contiguous().view(batchsz, -1, self.h * self.d_k)\n        return self.w_O(x)\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: float,\n        scale: bool = True,\n        activation_type: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[int] = None,\n        ffn_pdrop: Optional[float] = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6\n    ):\n        super().__init__()\n        # to properly execute BERT models, we have to follow T2T and do layer norms after\n        self.layer_norms_after = layer_norms_after\n        self.d_model = d_model\n        self.d_ff = d_ff if d_ff is not None else 4 * d_model\n        if rpr_k is not None:\n            self.self_attn = MultiHeadedRelativeAttention(num_heads, d_model, rpr_k, pdrop, scale, d_k=d_k)\n        else:\n            self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale, d_k=d_k)\n        self.ffn = nn.Sequential(\n            Dense(self.d_model, self.d_ff),\n            get_activation(activation_type),\n            nn.Dropout(ffn_pdrop),\n            Dense(self.d_ff, self.d_model),\n        )\n        # Slightly late for a name change\n        # LN1 = ln_x\n        # LN2 = ln_attn_output\n        self.ln1 = nn.LayerNorm(self.d_model, eps=layer_norm_eps)\n        self.ln2 = nn.LayerNorm(self.d_model, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(pdrop)\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""\n        :param inputs: `(x, mask)`\n        :return: The output tensor\n        """"""\n        x, mask = inputs\n\n        if not self.layer_norms_after:\n            x = self.ln1(x)\n        h = self.self_attn((x, x, x, mask))\n        x = x + self.dropout(h)\n        x = self.ln2(x)\n        x = x + self.dropout(self.ffn(x))\n        if self.layer_norms_after:\n            x = self.ln1(x)\n        return x\n\n\nclass TransformerDecoder(nn.Module):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: float,\n        scale: bool = True,\n        activation_type: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[int] = None,\n        ffn_pdrop: Optional[float] = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6\n    ):\n        super().__init__()\n        self.d_model = d_model\n        self.layer_norms_after = layer_norms_after\n        self.d_ff = d_ff if d_ff is not None else 4 * d_model\n        if rpr_k is not None:\n            self.self_attn = MultiHeadedRelativeAttention(num_heads, d_model, rpr_k, pdrop, scale, d_k=d_k)\n            self.src_attn = MultiHeadedRelativeAttention(num_heads, d_model, rpr_k, pdrop, scale, d_k=d_k)\n\n        else:\n            self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale, d_k=d_k)\n            self.src_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale, d_k=d_k)\n\n        self.ffn = nn.Sequential(\n            Dense(self.d_model, self.d_ff),\n            nn.Dropout(ffn_pdrop),\n            get_activation(activation_type),\n            Dense(self.d_ff, self.d_model),\n        )\n\n        self.ln1 = nn.LayerNorm(self.d_model, eps=layer_norm_eps)\n        self.ln2 = nn.LayerNorm(self.d_model, eps=layer_norm_eps)\n        self.ln3 = nn.LayerNorm(self.d_model, eps=layer_norm_eps)\n        self.dropout = nn.Dropout(pdrop)\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n\n        x, memory, src_mask, tgt_mask = inputs\n        if not self.layer_norms_after:\n            x = self.ln1(x)\n        x = x + self.dropout(self.self_attn((x, x, x, tgt_mask)))\n\n        x = self.ln2(x)\n        x = x + self.dropout(self.src_attn((x, memory, memory, src_mask)))\n\n        x = self.ln3(x)\n        x = x + self.dropout(self.ffn(x))\n        if self.layer_norms_after:\n            x = self.ln1(x)\n        return x\n\n\nclass TransformerEncoderStack(nn.Module):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: float,\n        scale: bool = True,\n        layers: int = 1,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        ffn_pdrop: Optional[float] = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        **kwargs,\n    ):\n        super().__init__()\n        self.encoders = nn.ModuleList()\n        self.ln = nn.Identity() if layer_norms_after else nn.LayerNorm(d_model, eps=layer_norm_eps)\n        self.output_dim = d_model\n\n        if not is_sequence(rpr_k):\n            rpr_k = [rpr_k] * layers\n\n        for i in range(layers):\n            self.encoders.append(\n                TransformerEncoder(\n                    num_heads, d_model, pdrop, scale, activation, d_ff, d_k,\n                    rpr_k=rpr_k[i], ffn_pdrop=ffn_pdrop,\n                    layer_norms_after=layer_norms_after, layer_norm_eps=layer_norm_eps\n                )\n            )\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        x, mask = inputs\n        for layer in self.encoders:\n            x = layer((x, mask))\n        return self.ln(x)\n\n\nclass TransformerEncoderStackWithLengths(TransformerEncoderStack):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: bool,\n        scale: bool = True,\n        layers: int = 1,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        input_sz: Optional[int] = None,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        **kwargs,\n    ):\n        super().__init__(num_heads, d_model, pdrop, scale, layers, activation, d_ff, d_k, rpr_k,\n                         layer_norms_after=layer_norms_after, layer_norm_eps=layer_norm_eps)\n        self.proj = WithDropout(pytorch_linear(input_sz, d_model), pdrop)\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n\n        x, lengths = inputs\n        x = self.proj(x)\n        max_seqlen = x.shape[1]\n        mask = sequence_mask(lengths, max_seqlen).to(x.device)\n        return super().forward((x, mask.unsqueeze(1).unsqueeze(1)))\n\n\nclass TransformerEncoderStackWithTimeMask(TransformerEncoderStack):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: bool,\n        scale: bool = True,\n        layers: int = 1,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        input_sz: Optional[int] = None,\n        layer_norms_after = False,\n        **kwargs,\n    ):\n        super().__init__(num_heads, d_model, pdrop, scale, layers, activation, d_ff, d_k, rpr_k, layer_norms_after=layer_norms_after)\n        self.proj = WithDropout(pytorch_linear(input_sz, d_model), pdrop)\n\n    def forward(self, inputs: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        x, lengths = inputs\n        x = self.proj(x)\n        max_seqlen = x.shape[1]\n        mask = subsequent_mask(max_seqlen).to(x.device)\n        return super().forward((x, mask.unsqueeze(1).unsqueeze(1)))\n\n\nclass TransformerDecoderStack(nn.Module):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: float,\n        scale: bool = True,\n        layers: int = 1,\n        activation_type: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        ffn_pdrop: Optional[float] = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6\n\n    ):\n        super().__init__()\n        self.decoders = nn.ModuleList()\n        self.ln = nn.Identity() if layer_norms_after else nn.LayerNorm(d_model, eps=layer_norm_eps)\n\n        if not is_sequence(rpr_k):\n            rpr_k = [rpr_k] * layers\n\n        for i in range(layers):\n            self.decoders.append(\n                TransformerDecoder(num_heads, d_model, pdrop, scale, activation_type, d_ff,\n                                   d_k=d_k, rpr_k=rpr_k[i], ffn_pdrop=ffn_pdrop,\n                                   layer_norms_after=layer_norms_after, layer_norm_eps=layer_norm_eps)\n            )\n\n    def forward(self, inputs):\n        x, memory, src_mask, tgt_mask = inputs\n        for layer in self.decoders:\n            x = layer((x, memory, src_mask, tgt_mask))\n        return self.ln(x)\n\n\ndef update_lengths(lengths, eoses, idx):\n    """"""Update the length of a generated tensor based on the first EOS found.\n\n    This is useful for a decoding situation where tokens after an EOS\n    can be something other than EOS. This also makes sure that a second\n    generated EOS doesn\'t affect the lengths.\n\n    :param lengths: `torch.LongTensor`: The lengths where zero means an\n        unfinished sequence.\n    :param eoses:  `torch.ByteTensor`: A mask that has 1 for sequences that\n        generated an EOS.\n    :param idx: `int`: What value to fill the finished lengths with (normally\n        the current decoding timestep).\n\n    :returns: `torch.Tensor`: The updated lengths tensor (same shape and type).\n    """"""\n    # If a length is 0 it has never had a length set so it is eligible to have\n    # this EOS be the length.\n    updatable_lengths = lengths == 0\n    # If this length can be updated AND this token is an eos\n    lengths_mask = updatable_lengths & eoses\n    return lengths.masked_fill(lengths_mask, idx)\n\n\ndef gnmt_length_penalty(lengths, alpha=0.8):\n    """"""Calculate a length penalty from https://arxiv.org/pdf/1609.08144.pdf\n\n    The paper states the penalty as (5 + |Y|)^a / (5 + 1)^a. This is implemented\n    as ((5 + |Y|) / 6)^a for a (very) tiny performance boost\n\n    :param lengths: `torch.LongTensor`: [B, K] The lengths of the beams.\n    :param alpha: `float`: A hyperparameter. See Table 2 for a search on this\n        parameter.\n\n    :returns:\n        `torch.FloatTensor`: [B, K, 1] The penalties.\n    """"""\n    lengths = lengths.to(torch.float)\n    penalty = torch.pow(((5 + lengths) / 6), alpha)\n    return penalty.unsqueeze(-1)\n\n\ndef no_length_penalty(lengths):\n    """"""A dummy function that returns a no penalty (1).""""""\n    return torch.ones_like(lengths).to(torch.float).unsqueeze(-1)\n\n\ndef repeat_batch(t, K, dim=0):\n    """"""Repeat a tensor while keeping the concept of a batch.\n\n    :param t: `torch.Tensor`: The tensor to repeat.\n    :param K: `int`: The number of times to repeat the tensor.\n    :param dim: `int`: The dimension to repeat in. This should be the\n        batch dimension.\n\n    :returns: `torch.Tensor`: The repeated tensor. The new shape will be\n        batch size * K at dim, the rest of the shapes will be the same.\n\n    Example::\n\n        >>> a = torch.arange(10).view(2, -1)\n        >>> a\n\ttensor([[0, 1, 2, 3, 4],\n\t\t[5, 6, 7, 8, 9]])\n\t>>> a.repeat(2, 1)\n\ttensor([[0, 1, 2, 3, 4],\n\t\t[5, 6, 7, 8, 9],\n\t\t[0, 1, 2, 3, 4],\n\t\t[5, 6, 7, 8, 9]])\n\t>>> repeat_batch(a, 2)\n\ttensor([[0, 1, 2, 3, 4],\n\t\t[0, 1, 2, 3, 4],\n\t\t[5, 6, 7, 8, 9],\n\t\t[5, 6, 7, 8, 9]])\n    """"""\n    shape = t.shape\n    tiling = [1] * (len(shape) + 1)\n    tiling[dim + 1] = K\n    tiled = t.unsqueeze(dim + 1).repeat(tiling)\n    old_bsz = shape[dim]\n    new_bsz = old_bsz * K\n    new_shape = list(shape[:dim]) + [new_bsz] + list(shape[dim + 1 :])\n    return tiled.view(new_shape)\n\n\nclass BeamSearchBase:\n    def __init__(self, beam=1, length_penalty=None, **kwargs):\n        self.length_penalty = length_penalty if length_penalty else no_length_penalty\n        self.K = beam\n\n    def init(self, encoder_outputs):\n        pass\n\n    def step(self, paths, extra):\n        pass\n\n    def update(self, beams, extra):\n        pass\n\n    def __call__(self, encoder_outputs, **kwargs):\n        """"""Perform batched Beam Search.\n\n        Note:\n            The paths and lengths generated do not include the <GO> token.\n\n        :param encoder_outputs: `namedtuple` The outputs of the encoder class.\n        :param init: `Callable(ecnoder_outputs: encoder_outputs, K: int)` -> Any: A\n            callable that is called once at the start of the search to initialize\n            things. This returns a blob that is passed to other callables.\n        :param step: `Callable(paths: torch.LongTensor, extra) -> (probs: torch.FloatTensor, extra):\n            A callable that is does a single decoding step. It returns the log\n            probabilities over the vocabulary in the last dimension. It also returns\n            any state the decoding process needs.\n        :param update: `Callable(beams: torch.LongTensor, extra) -> extra:\n            A callable that is called to edit the decoding state based on the selected\n            best beams.\n        :param length_penalty: `Callable(lengths: torch.LongTensor) -> torch.floatTensor\n            A callable that generates a penalty based on the lengths. Lengths is\n            [B, K] and the returned penalty should be [B, K, 1] (or [B, K, V] to\n            have token based penalties?)\n\n        :Keyword Arguments:\n        * *beam* -- `int`: The number of beams to use.\n        * *mxlen* -- `int`: The max number of steps to run the search for.\n\n        :returns:\n            tuple(preds: torch.LongTensor, lengths: torch.LongTensor, scores: torch.FloatTensor)\n            preds: The predicted values: [B, K, max(lengths)]\n            lengths: The length of each prediction [B, K]\n            scores: The score of each path [B, K]\n        """"""\n        mxlen = kwargs.get(""mxlen"", 100)\n        bsz = encoder_outputs.output.shape[0]\n        device = encoder_outputs.output.device\n        with torch.no_grad():\n            extra = self.init(encoder_outputs)\n            paths = torch.full((bsz, self.K, 1), Offsets.GO, dtype=torch.long, device=device)\n            # This tracks the log prob of each beam. This is distinct from score which\n            # is based on the log prob and penalties.\n            log_probs = torch.zeros((bsz, self.K), dtype=torch.float, device=device)\n            # Tracks the lengths of the beams, unfinished beams have lengths of zero.\n            lengths = torch.zeros((bsz, self.K), dtype=torch.long, device=device)\n\n            for i in range(mxlen - 1):\n                probs, extra = self.step(paths, extra)\n                V = probs.shape[-1]\n                probs = probs.view((bsz, self.K, V))  # [B, K, V]\n                if i > 0:\n                    # This mask is for all beams that are done.\n                    done_mask = (lengths != 0).unsqueeze(-1)  # [B, K, 1]\n                    # Can creating this mask be moved out of the loop? It never changes but we don\'t have V\n                    # This mask selects the EOS token\n                    eos_mask = torch.zeros((1, 1, V), dtype=done_mask.dtype, device=device)\n                    eos_mask[:, :, Offsets.EOS] = 1\n                    # This mask selects the EOS token of only the beams that are done.\n                    mask = done_mask & eos_mask\n                    # Put all probability mass on the EOS token for finished beams.\n                    # Otherwise as the other beams get longer they will all give\n                    # up and eventually select this beam and all outputs become\n                    # the same.\n                    probs = probs.masked_fill(done_mask, -np.inf)\n                    probs = probs.masked_fill(mask, 0)\n                    probs = log_probs.unsqueeze(-1) + probs  # [B, K, V]\n                    # Calculate the score of the beam based on the current length.\n                    path_scores = probs / self.length_penalty(lengths.masked_fill(lengths == 0, i + 1))\n                else:\n                    # On the first step we only look at probabilities for the first beam.\n                    # If we don\'t then the probs will be the same for each beam\n                    # This means the same token will be selected for each beam\n                    # And we won\'t get any diversity.\n                    # Using only the first beam ensures K different starting points.\n                    path_scores = probs[:, 0, :]\n\n                flat_scores = path_scores.view(bsz, -1)  # [B, K * V]\n                best_scores, best_idx = flat_scores.topk(self.K, 1)\n                # Get the log_probs of the best scoring beams\n                log_probs = probs.view(bsz, -1).gather(1, best_idx).view(bsz, self.K)\n\n                best_beams = best_idx / V  # Get which beam it came from\n                best_idx = best_idx % V  # Get the index of the word regardless of which beam it is.\n\n                # Best Beam index is relative within the batch (only [0, K)).\n                # This makes the index global (e.g. best beams for the second\n                # batch example is in [K, 2*K)).\n                offsets = torch.arange(bsz, dtype=torch.long, device=device) * self.K\n                offset_beams = best_beams + offsets.unsqueeze(-1)\n                flat_beams = offset_beams.view(bsz * self.K)\n                # Select the paths to extend based on the best beams\n                flat_paths = paths.view(bsz * self.K, -1)\n                new_paths = flat_paths[flat_beams, :].view(bsz, self.K, -1)\n                # Add the selected outputs to the paths\n                paths = torch.cat([new_paths, best_idx.unsqueeze(-1)], dim=2)\n\n                # Select the lengths to keep tracking based on the valid beams left.\n                lengths = lengths.view(-1)[flat_beams].view((bsz, self.K))\n\n                extra = self.update(flat_beams, extra)\n\n                # Updated lengths based on if we hit EOS\n                last = paths[:, :, -1]\n                eoses = last == Offsets.EOS\n                lengths = update_lengths(lengths, eoses, i + 1)\n                if (lengths != 0).all():\n                    break\n            else:\n                # This runs if the loop didn\'t break meaning one beam hit the max len\n                # Add an EOS to anything that hasn\'t hit the end. This makes the scores real.\n                probs, extra = self.step(paths, extra)\n\n                V = probs.size(-1)\n                probs = probs.view((bsz, self.K, V))\n                probs = probs[:, :, Offsets.EOS]  # Select the score of EOS\n                # If any of the beams are done mask out the score of this EOS (they already had an EOS)\n                probs = probs.masked_fill((lengths != 0), 0)\n                log_probs = log_probs + probs\n                end_tokens = torch.full((bsz, self.K, 1), Offsets.EOS, device=device, dtype=paths.dtype)\n                paths = torch.cat([paths, end_tokens], dim=2)\n                lengths = update_lengths(lengths, torch.ones_like(lengths) == 1, mxlen)\n                lengths = update_lengths(lengths, torch.ones_like(lengths) == 1, mxlen)\n                best_scores = log_probs / self.length_penalty(lengths).squeeze(-1)\n\n        # Slice off the Offsets.GO token\n        paths = paths[:, :, 1:]\n        return paths, lengths, best_scores\n\n\ndef checkpoint_for(model_base, epoch, tick_type=\'epoch\'):\n    return \'{}-{}-{}\'.format(model_base, tick_type, epoch+1)\n\n\ndef rm_old_checkpoints(base_path, current_epoch, last_n=10):\n    for i in range(0, current_epoch-last_n):\n        checkpoint_i = checkpoint_for(base_path, i)\n        for extension in (\'.pth\', \'.npz\'):\n            checkpoint_name = checkpoint_i + extension\n            if os.path.exists(checkpoint_name):\n                os.remove(checkpoint_name)\n\n\n\ndef save_checkpoint(model: torch.nn.Module, model_base: str, count: int, tick_type: str = \'epoch\', save_npz: bool = False):\n    from eight_mile.pytorch.serialize import save_tlm_npz\n    checkpoint_name = checkpoint_for(model_base, count, tick_type=tick_type)\n    # Its possible due to how its called that we might save the same checkpoint twice if we dont check first\n    if os.path.exists(checkpoint_name):\n        logger.info(""Checkpoint already exists: %s"", checkpoint_name)\n        return\n    logger.info(""Creating checkpoint: %s"", checkpoint_name)\n    if hasattr(model, \'module\'):\n        torch.save(model.module.state_dict(), checkpoint_name+\'.pth\')\n        if save_npz:\n            save_tlm_npz(model.module, checkpoint_name+\'.npz\')\n    else:\n        torch.save(model.state_dict(), checkpoint_name+\'.pth\')\n        if save_npz:\n            save_tlm_npz(model, checkpoint_name+\'.npz\')\n    if tick_type == \'epoch\':\n        rm_old_checkpoints(model_base, count)\n\n\ndef init_distributed(local_rank):\n    if local_rank == -1:\n        # https://github.com/kubeflow/pytorch-operator/issues/128\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n        logger.info(""Setting local rank to RANK env variable"")\n        local_rank = int(os.environ[\'RANK\'])\n    logger.warning(""Local rank (%d)"", local_rank)\n    # In an env like k8s with kubeflow each worker will only see a single gpu\n    # with an id of 0. If the gpu count is 1 then we are probably in an env like\n    # that so we should just use the first (and only) gpu avaiable\n    if torch.cuda.device_count() == 1:\n        torch.cuda.set_device(0)\n        device = torch.device(""cuda"", 0)\n    # This program assumes multiprocess/multi-device on a single node. Each\n    # process gets a rank (via cli or ENV variable) and uses that rank to select\n    # which gpu to use. This only makes sense on a single node, if you had 4\n    # processes on 2 nodes where each node has 2 GPUs then the ranks would be\n    # 0, 1, 2, 3 but the gpus numbers would be node 0: 0, 1 and node 1: 0, 1\n    # and this assignment to gpu 3 would fail. On a single node with 4 processes\n    # and 4 gpus the rank and gpu ids will align and this will work\n    else:\n        torch.cuda.set_device(local_rank)\n        device = torch.device(""cuda"", local_rank)\n    torch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\n    return device, local_rank\n\nclass SingleHeadReduction(nn.Module):\n    """"""\n    Implementation of the ""self_attention_head"" layer from the conveRT paper (https://arxiv.org/pdf/1911.03688.pdf)\n    """"""\n    def __init__(\n            self, d_model: int, dropout: float = 0.0, scale: bool = True, d_k: Optional[int] = None\n    ):\n        """"""\n        :param d_model: The model hidden size\n        :param dropout (``float``): The amount of dropout to use\n        :param scale: should we scale the dot product attention\n        :param d_k: The low-order project per head.  This is normally `d_model // num_heads` unless set explicitly\n        """"""\n        super().__init__()\n        self.d_model = d_model\n        if d_k is None:\n            self.d_k = d_model\n        else:\n            self.d_k = d_k\n        self.w_Q = Dense(d_model, self.d_k)\n        self.w_K = Dense(d_model, self.d_k)\n        if scale:\n            self.attn_fn = SeqScaledDotProductAttention(dropout)\n        else:\n            self.attn_fn = SeqDotProductAttention(dropout)\n        self.attn = None\n\n    def forward(self, qkvm: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n        """"""According to conveRT model\'s graph, they project token encodings to lower-dimensional query and key in single\n        head, use them to calculate the attention score matrix that has dim [B, T, T], then sum over the query dim to\n        get a tensor with [B, 1, T] (meaning the amount of attentions each token gets from all other tokens), scale it\n        by sqrt of sequence lengths, then use it as the weight to weighted sum the token encoding to get the sentence\n        encoding. we implement it in an equivalent way that can best make use of the eight_mile codes: do the matrix\n        multiply with value first, then sum over the query dimension.\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param value: a set of values from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :return: sentence-level encoding with dim [B, d_model]\n        """"""\n        query, key, value, mask = qkvm\n        batchsz = query.size(0)\n        seq_mask = mask.squeeze()  # [B, T]\n        seq_lengths = seq_mask.sum(dim=1)\n\n        # (B, H, T, D), still have num_heads = 1 to use the attention function defined in eight_miles\n        query = self.w_Q(query).view(batchsz, -1, 1, self.d_k).transpose(1, 2)\n        key = self.w_K(key).view(batchsz, -1, 1, self.d_k).transpose(1, 2)\n        value = value.view(batchsz, -1, 1, self.d_model).transpose(1, 2)\n        x = self.attn_fn((query, key, value, mask))  # [B, 1, T, D]\n        self.attn = self.attn_fn.attn\n\n        x = x.squeeze(1)  # [B, T, D]\n        x = x * seq_mask.unsqueeze(-1)\n        x = x.sum(dim=1)  # [B, D]\n        x = x * seq_lengths.float().sqrt().unsqueeze(-1)\n        return x\n'"
layers/eight_mile/pytorch/optz.py,10,"b'import math\nimport logging\nimport torch\nimport torch.autograd\nfrom eight_mile.optz import create_lr_scheduler, register_lr_scheduler\nfrom eight_mile.optz import (\n    ConstantScheduler,\n    WarmupLinearScheduler,\n    CyclicLRScheduler,\n    PiecewiseDecayScheduler,\n    ZarembaDecayScheduler,\n    CosineDecayScheduler,\n    InverseTimeDecayScheduler,\n    ExponentialDecayScheduler,\n    CompositeLRScheduler,\n)\n\nlogger = logging.getLogger(""mead.layers"")\n\n\n@register_lr_scheduler(name=""default"")\nclass ConstantSchedulerPyTorch(ConstantScheduler):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n\n@register_lr_scheduler(name=""warmup_linear"")\nclass WarmupLinearSchedulerPyTorch(WarmupLinearScheduler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\n@register_lr_scheduler(name=""clr"")\nclass CyclicLRSchedulerPyTorch(CyclicLRScheduler):\n    def __init__(self, *args, **kwargs):\n        super().__init(*args, **kwargs)\n\n\n@register_lr_scheduler(name=""piecewise"")\nclass PiecewiseDecaySchedulerPyTorch(PiecewiseDecayScheduler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\n@register_lr_scheduler(name=""zaremba"")\nclass ZarembaDecaySchedulerPyTorch(ZarembaDecayScheduler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\n@register_lr_scheduler(name=""cosine"")\nclass CosineDecaySchedulerPyTorch(CosineDecayScheduler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\n@register_lr_scheduler(name=""invtime"")\nclass InverseTimeDecaySchedulerPytorch(InverseTimeDecayScheduler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\n@register_lr_scheduler(name=""exponential"")\nclass ExponentialDecaySchedulerPyTorch(ExponentialDecayScheduler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n\n@register_lr_scheduler(name=""composite"")\nclass CompositeLRSchedulerPyTorch(CompositeLRScheduler):\n    pass\n\n\nclass AdamW(torch.optim.Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(""Adam does not support sparse gradients, please consider SparseAdam instead"")\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[""step""] = 0\n                    # Exponential moving average of gradient values\n                    state[""exp_avg""] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[""exp_avg_sq""] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                state[""step""] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)\n                denom = exp_avg_sq.sqrt().add_(group[""eps""])\n\n                bias_correction1 = 1 - beta1 ** state[""step""]\n                bias_correction2 = 1 - beta2 ** state[""step""]\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n                p.data.addcdiv_(exp_avg, denom, value=-step_size)\n\n                if group[""weight_decay""] != 0.0:\n                    p.data.add_(p.data, alpha=-group[""weight_decay""] * group[""lr""])\n\n        return loss\n\n\nclass OptimizerManager:\n    def __init__(self, model_or_params, global_step=0, **kwargs):\n        if isinstance(model_or_params, torch.nn.Module):\n            parameters = model_or_params.parameters()\n        else:\n            parameters = model_or_params\n        self.global_step = global_step\n        if ""lr_function"" in kwargs:\n            self.lr_function = kwargs[""lr_function""]\n        else:\n            if ""lr_scheduler_type"" not in kwargs:\n                kwargs[""lr_scheduler_type""] = ""default""\n            self.lr_function = create_lr_scheduler(**kwargs)\n        self._init_optimizer(parameters, **kwargs)\n        self.current_lr = 0\n\n    @property\n    def global_step(self):\n        return self._global_step\n\n    @global_step.setter\n    def global_step(self, value):\n        self._global_step = value\n\n    def _init_optimizer(self, parameters, **kwargs):\n        wd = float(kwargs.get(""weight_decay"", 0))\n        optim = kwargs.get(""optim"", ""sgd"")\n        self.current_lr = kwargs.get(""eta"", kwargs.get(""lr"", 0.01))\n        if optim == ""adadelta"":\n            logger.info(""adadelta(eta=%f, wd=%f)"", self.current_lr, wd)\n            self.optimizer = torch.optim.Adadelta(parameters, lr=self.current_lr, weight_decay=wd)\n        elif optim.startswith(""adam""):\n            beta1 = kwargs.get(""beta1"", 0.9)\n            beta2 = kwargs.get(""beta2"", 0.999)\n            eps = kwargs.get(""epsilon"", 1e-8)\n            if optim == ""adam"":\n                logger.info(\n                    ""adam(eta=%f, beta1=%f, beta2=%f, epsilon=%f, wd=%f)"", self.current_lr, beta1, beta2, eps, wd\n                )\n                self.optimizer = torch.optim.Adam(\n                    parameters, lr=self.current_lr, betas=(beta1, beta2), eps=eps, weight_decay=wd\n                )\n            elif optim == ""adamw"":\n                logger.info(\n                    ""adamw(eta=%f, beta1=%f, beta2=%f, epsilon=%f, wd=%f)"", self.current_lr, beta1, beta2, eps, wd\n                )\n                self.optimizer = AdamW(\n                    parameters,\n                    lr=self.current_lr,\n                    betas=(beta1, beta2),\n                    eps=eps,\n                    weight_decay=wd,\n                )\n        elif optim == ""rmsprop"":\n            mom = kwargs.get(""mom"", 0.0)\n            logger.info(""rmsprop(eta=%f, wd=%f, mom=%f)"", self.current_lr, wd, mom)\n            self.optimizer = torch.optim.RMSprop(parameters, lr=self.current_lr, weight_decay=wd, momentum=mom)\n        elif optim == ""asgd"":\n            logger.info(""asgd(eta=%f, wd=%f)"", self.current_lr, wd)\n            self.optimizer = torch.optim.ASGD(parameters, lr=self.current_lr, weight_decay=wd)\n        else:\n            mom = kwargs.get(""mom"", 0.9)\n            logger.info(""sgd(eta=%f, mom=%f, wd=%f)"", self.current_lr, mom, wd)\n            self.optimizer = torch.optim.SGD(parameters, lr=self.current_lr, momentum=mom, weight_decay=wd)\n\n    def _identity(self, _):\n        return self.current_lr\n\n    def step(self):\n        """"""Runs at every step and updates the learning rate\n\n        :return:\n        """"""\n        self.optimizer.step()\n        self.current_lr = self.update_lr()\n        self.global_step += 1\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def update_lr(self):\n        lr = self.lr_function(self.global_step)\n        for p in self.optimizer.param_groups:\n            p[""lr""] = lr\n        return lr\n\n\nclass EagerOptimizer(object):\n    def __init__(self, loss, optimizer=None, **kwargs):\n        self.loss = loss\n        if optimizer:\n            self.optimizer = optimizer\n        else:\n            self.optimizer_args = kwargs\n            self.optimizer = None\n\n    def update(self, model, x, y):\n        if not self.optimizer:\n            self.optimizer = OptimizerManager(model, **self.optimizer_args)\n        self.optimizer.zero_grad()\n        l = self.loss(model, x, y)\n        l.backward()\n        self.optimizer.step()\n        return float(l)\n\n    def update_with_hidden(self, model, h, x, y):\n        if not self.optimizer:\n            self.optimizer = OptimizerManager(model, **self.optimizer_args)\n        self.optimizer.zero_grad()\n        l, h = self.loss(model, h, x, y)\n        l.backward()\n        self.optimizer.step()\n        return float(l), h\n'"
layers/eight_mile/pytorch/serialize.py,11,"b'import torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, List\nfrom eight_mile.pytorch.layers import Dense, TransformerEncoderStack, TransformerEncoder, EmbeddingsStack\nfrom eight_mile.pytorch.embeddings import LookupTableEmbeddings, LearnedPositionalLookupTableEmbeddingsWithBias\n\n# BERT HuggingFace Tokenizers checkpoints can be converted into MEAD Baseline Transformer checkpoints\n# With a simple name change\nBERT_HF_LAYER_MAP = {\n    ## FFN weights\n    \'bert.encoder.layer.{}.intermediate.dense.weight\': \'generator.encoders.{}.ffn.0.layer.weight\',\n    \'bert.encoder.layer.{}.intermediate.dense.bias\': \'generator.encoders.{}.ffn.0.layer.bias\',\n    \'bert.encoder.layer.{}.output.dense.weight\': \'generator.encoders.{}.ffn.3.layer.weight\',\n    \'bert.encoder.layer.{}.output.dense.bias\': \'generator.encoders.{}.ffn.3.layer.bias\',\n\n    ## MHA weights\n    \'bert.encoder.layer.{}.attention.self.key.weight\': \'generator.encoders.{}.self_attn.w_K.layer.weight\',\n    \'bert.encoder.layer.{}.attention.self.key.bias\': \'generator.encoders.{}.self_attn.w_K.layer.bias\',\n    \'bert.encoder.layer.{}.attention.self.query.weight\': \'generator.encoders.{}.self_attn.w_Q.layer.weight\',\n    \'bert.encoder.layer.{}.attention.self.query.bias\': \'generator.encoders.{}.self_attn.w_Q.layer.bias\',\n    \'bert.encoder.layer.{}.attention.self.value.weight\': \'generator.encoders.{}.self_attn.w_V.layer.weight\',\n    \'bert.encoder.layer.{}.attention.self.value.bias\': \'generator.encoders.{}.self_attn.w_V.layer.bias\',\n    \'bert.encoder.layer.{}.attention.output.dense.weight\': \'generator.encoders.{}.self_attn.w_O.layer.weight\',\n    \'bert.encoder.layer.{}.attention.output.dense.bias\': \'generator.encoders.{}.self_attn.w_O.layer.bias\',\n\n    ## LN weights\n    # The names in of layer norm our transformers are a bit unspecific\n    # think of ln1 as ln_x and ln2 as ln_attn_output\n    \'bert.encoder.layer.{}.output.LayerNorm.beta\': \'generator.encoders.{}.ln1.bias\',\n    \'bert.encoder.layer.{}.output.LayerNorm.gamma\': \'generator.encoders.{}.ln1.weight\',\n    \'bert.encoder.layer.{}.attention.output.LayerNorm.beta\': \'generator.encoders.{}.ln2.bias\',\n    \'bert.encoder.layer.{}.attention.output.LayerNorm.gamma\': \'generator.encoders.{}.ln2.weight\'\n}\n\nBERT_HF_FT_LAYER_MAP = {\n    ## FFN weights\n    \'bert.encoder.layer.{}.intermediate.dense.weight\': \'transformer.encoders.{}.ffn.0.layer.weight\',\n    \'bert.encoder.layer.{}.intermediate.dense.bias\': \'transformer.encoders.{}.ffn.0.layer.bias\',\n    \'bert.encoder.layer.{}.output.dense.weight\': \'transformer.encoders.{}.ffn.3.layer.weight\',\n    \'bert.encoder.layer.{}.output.dense.bias\': \'transformer.encoders.{}.ffn.3.layer.bias\',\n\n    ## MHA weights\n    \'bert.encoder.layer.{}.attention.self.key.weight\': \'transformer.encoders.{}.self_attn.w_K.layer.weight\',\n    \'bert.encoder.layer.{}.attention.self.key.bias\': \'transformer.encoders.{}.self_attn.w_K.layer.bias\',\n    \'bert.encoder.layer.{}.attention.self.query.weight\': \'transformer.encoders.{}.self_attn.w_Q.layer.weight\',\n    \'bert.encoder.layer.{}.attention.self.query.bias\': \'transformer.encoders.{}.self_attn.w_Q.layer.bias\',\n    \'bert.encoder.layer.{}.attention.self.value.weight\': \'transformer.encoders.{}.self_attn.w_V.layer.weight\',\n    \'bert.encoder.layer.{}.attention.self.value.bias\': \'transformer.encoders.{}.self_attn.w_V.layer.bias\',\n    \'bert.encoder.layer.{}.attention.output.dense.weight\': \'transformer.encoders.{}.self_attn.w_O.layer.weight\',\n    \'bert.encoder.layer.{}.attention.output.dense.bias\': \'transformer.encoders.{}.self_attn.w_O.layer.bias\',\n\n    ## LN weights\n    # The names in of layer norm our transformers are a bit unspecific\n    # think of ln1 as ln_x and ln2 as ln_attn_output\n    \'bert.encoder.layer.{}.output.LayerNorm.beta\': \'transformer.encoders.{}.ln1.bias\',\n    \'bert.encoder.layer.{}.output.LayerNorm.gamma\': \'transformer.encoders.{}.ln1.weight\',\n    \'bert.encoder.layer.{}.attention.output.LayerNorm.beta\': \'transformer.encoders.{}.ln2.bias\',\n    \'bert.encoder.layer.{}.attention.output.LayerNorm.gamma\': \'transformer.encoders.{}.ln2.weight\'\n}\n\nBERT_HF_EMBED_MAP = {\n    ## Embedding weights\n    \'bert.embeddings.word_embeddings.weight\': \'embeddings.embeddings.0.embeddings.weight\',\n    \'bert.embeddings.position_embeddings.weight\': \'embeddings.embeddings.0.pos_embeddings.weight\',\n    \'bert.embeddings.token_type_embeddings.weight\': \'embeddings.embeddings.1.embeddings.weight\',\n    \'bert.embeddings.LayerNorm.beta\': \'embeddings.reduction.ln.bias\',\n    \'bert.embeddings.LayerNorm.gamma\': \'embeddings.reduction.ln.weight\',\n}\n\n\ndef convert_transformers_keys(num_layers: int, d: Dict, replace_layer_map: Dict = BERT_HF_LAYER_MAP, replace_embed_map: Dict = BERT_HF_EMBED_MAP) -> Dict:\n    m = {}\n    for i in range(num_layers):\n        for k, v in replace_layer_map.items():\n            m[v.format(i)] = d[k.format(i)]\n\n    for k, v in replace_embed_map.items():\n        m[v] = d[k]\n\n\n    return m\n\n\ndef tlm_load_state_dict(module: nn.Module, checkpoint_file: str):\n    """"""\n\n    :param tlm: Safely loads the state dict for a transformer encoder\n    :param checkpoint_file: The file name\n    :return: None\n    """"""\n    from_str = \'transformer\'\n    to_str = \'generator\'\n    if hasattr(module, \'transformer\'):\n        from_str = \'generator\'\n        to_str = \'transformer\'\n    ckpt_dict = torch.load(checkpoint_file)\n    renamed = {}\n    for k, v in ckpt_dict.items():\n        renamed[k.replace(from_str, to_str)] = v\n    unmatch = module.load_state_dict(renamed, strict=False)\n    if unmatch.missing_keys or len(unmatch.unexpected_keys) > 2:\n        print(""Warning: Embedding doesn\'t match with the checkpoint being loaded."")\n        print(f""missing keys: {unmatch.missing_keys}\\n unexpected keys: {unmatch.unexpected_keys}"")\n\n\ndef to_weight_array(pytorch_layer: nn.Module, name: str) -> Dict:\n    """"""Convert a {`LayerNorm`, `Linear`, `layers.Dense`} to `weights` and `bias` arrays\n\n    :param pytorch_layer: A layer to get weights for\n    :param name: The name of this layer to serialize\n    :return: A Dictionary containing `weights` and `bias` keys\n    """"""\n    if isinstance(pytorch_layer, Dense):\n        pytorch_layer = pytorch_layer.layer\n    weights = pytorch_layer.weight.cpu().detach().numpy()\n    bias = pytorch_layer.bias.cpu().detach().numpy()\n    return {f""{name}/weights"": weights, f""{name}/bias"": bias}\n\n\ndef from_weight_array(pytorch_layer: nn.Module, d: Dict, name: str):\n    """"""Read in {`LayerNorm`, `Linear`, `layers.Dense`} from `weights` and `bias` fields\n\n    :param pytorch_layer: A layer to get weights for\n    :param d: A Dict containing the arrays by key\n    :param name: The name of this layer\n    :return: None\n    """"""\n    if isinstance(pytorch_layer, Dense):\n        pytorch_layer = pytorch_layer.layer\n    device = pytorch_layer.weight.device\n    pytorch_layer.weight = nn.Parameter(torch.from_numpy(d[f""{name}/weights""]).to(device=device), requires_grad=True)\n    pytorch_layer.bias = nn.Parameter(torch.from_numpy(d[f""{name}/bias""]).to(device=device), requires_grad=True)\n\n\ndef to_ffn_array(pytorch_ffn: nn.Sequential, name: str) -> Dict:\n    """"""Convert a `FFN` layer to a set of arrays\n\n    :param pytorch_ffn: An `FFN` layer for Transformers\n    :param name: The name of the layer\n    :return: A Dict containing the arrays by key\n    """"""\n    d = {}\n    d.update(to_weight_array(pytorch_ffn[0], f""{name}/expansion""))\n    d.update(to_weight_array(pytorch_ffn[3], f""{name}/squeeze""))\n    return d\n\n\ndef from_ffn_array(pytorch_ffn: nn.Sequential, d: Dict, name: str):\n    """"""Restore an `FFN` layer\'s weights from a set of arrays\n\n    :param pytorch_ffn: An `FFN` layer for Transformers\n    :param d: A Dict containing the arrays by key\n    :param name: The name of the layer\n    :return: None\n    """"""\n    from_weight_array(pytorch_ffn[0], d, f""{name}/expansion"")\n    from_weight_array(pytorch_ffn[3], d, f""{name}/squeeze"")\n\n\ndef to_attn_array(pytorch_attn: nn.Module, name: str) -> Dict:\n    """"""Convert a self-attention module to a set of arrays\n\n    :param pytorch_attn: The self-attention layer of the transformer encoder, could be MultiHeadedAttention or\n    MultiHeadedRelativeAttention\n    :param name: The name of the layer\n    :return: A Dict containing the arrays by key\n    """"""\n    d = {}\n\n    d.update(to_weight_array(pytorch_attn.w_Q, f""{name}/w_Q""))\n    d.update(to_weight_array(pytorch_attn.w_K, f""{name}/w_K""))\n    d.update(to_weight_array(pytorch_attn.w_V, f""{name}/w_V""))\n    d.update(to_weight_array(pytorch_attn.w_O, f""{name}/w_O""))\n\n    if hasattr(pytorch_attn, \'rpr_key\'):\n        rpr_key_weights = pytorch_attn.rpr_key.weight.cpu().detach().numpy()\n        rpr_value_weights = pytorch_attn.rpr_value.weight.cpu().detach().numpy()\n        d.update({f""{name}/rpr_key"": rpr_key_weights})\n        d.update({f""{name}/rpr_value"": rpr_value_weights})\n\n    return d\n\n\ndef from_attn_array(pytorch_attn: nn.Module, d: Dict, name: str):\n    """"""Restore the self-attention module from a set of arrays\n\n    :param pytorch_attn: A self-attention module, could be MultiHeadedAttention or MultiHeadedRelativeAttention\n    :param d: A Dict of arrays by key\n    :param name: The name of the layer\n    """"""\n    from_weight_array(pytorch_attn.w_Q, d, f""{name}/w_Q"")\n    from_weight_array(pytorch_attn.w_K, d, f""{name}/w_K"")\n    from_weight_array(pytorch_attn.w_V, d, f""{name}/w_V"")\n    from_weight_array(pytorch_attn.w_O, d, f""{name}/w_O"")\n\n    if hasattr(pytorch_attn, \'rpr_key\'):\n        device = pytorch_attn.rpr_key.weight.device\n        pytorch_attn.rpr_key.weight = torch.nn.Parameter(torch.from_numpy(d[f""{name}/rpr_key""]).to(device=device))\n        pytorch_attn.rpr_value.weight = torch.nn.Parameter(torch.from_numpy(d[f""{name}/rpr_value""]).to(device=device))\n\n\ndef to_encoder_array(pytorch_encoder: TransformerEncoder, name: str) -> Dict:\n    """"""Convert a `TransformerEncoder` layer to an set of numpy arrays\n\n    :param pytorch_encoder: A `TransformerEncoder` layer\n    :param name: The layer name\n    :return: A Dict of arrays by key\n    """"""\n    d = {}\n    d.update(to_weight_array(pytorch_encoder.ln1, f""{name}/ln1""))\n    d.update(to_weight_array(pytorch_encoder.ln2, f""{name}/ln2""))\n    d.update(to_attn_array(pytorch_encoder.self_attn, f""{name}/attn""))\n    d.update(to_ffn_array(pytorch_encoder.ffn, f""{name}/ffn""))\n    return d\n\n\ndef from_encoder_array(pytorch_encoder: TransformerEncoder, d: Dict, name: str):\n    """"""Restore a `TransformerEncoder` layer from a set of numpy arrays\n\n    :param pytorch_encoder: A `TransformerEncoder` layer\n    :param d: A Dict of arrays by key\n    :param name: The layer name\n    :return: None\n    """"""\n    from_weight_array(pytorch_encoder.ln1, d, f""{name}/ln1"")\n    from_weight_array(pytorch_encoder.ln2, d, f""{name}/ln2"")\n    from_attn_array(pytorch_encoder.self_attn, d, f""{name}/attn"")\n    from_ffn_array(pytorch_encoder.ffn, d, f""{name}/ffn"")\n\n\ndef to_embed_array(pytorch_embed: nn.Module, name: str) -> Dict:\n    """"""Convert positional embedding to a `weights` array, if it\'s learned positional embedding,\n    save the pos_weight as well\n\n    :param pytorch_embed: An embedding module\n    :param name: A layer name\n    :return: A Dict containing the embedding `weights`\n    """"""\n    d = {}\n    weights = pytorch_embed.embeddings.weight.cpu().detach().numpy()\n    d.update({f""{name}/weights"": weights})\n    if hasattr(pytorch_embed, \'pos_embeddings\'):\n        pos_weights = pytorch_embed.pos_embeddings.weight.cpu().detach().numpy()\n        d.update({f""{name}/pos_weights"": pos_weights})\n    return d\n\n\ndef from_embed_array(pytorch_embed: nn.Module, d: Dict, name: str):\n    """"""Restore a positional embedding from a `weights` array, if it\'s a learned positional embedding, the pos_weights\n    is also restored\n\n    :param pytorch_embed: An embedding module\n    :param d: A Dict containing a `weights` array to restore\n    :param name: name of the layer\n    :return: None\n    """"""\n    device = pytorch_embed.embeddings.weight.device\n    pytorch_embed.embeddings.weight = torch.nn.Parameter(torch.from_numpy(d[f""{name}/weights""]).to(device=device), requires_grad=True)\n    if hasattr(pytorch_embed, \'pos_embeddings\'):\n        pytorch_embed.pos_embeddings.weight = torch.nn.Parameter(torch.from_numpy(d[f""{name}/pos_weights""]).to(device=device),\n                                                                 requires_grad=True)\n\n\ndef to_tlm_array(pytorch_tlm: nn.Module, embeddings_keys: List[str] = None, name: str = ""TLM"") -> Dict:\n    """"""Convert a Transformer LM-type module to a set of weights in a Dict\n\n    :param pytorch_tlm: A Transformer LM-type module\n    :param embeddings_keys: A key to get the embeddings from, defaults to `None` in which case, gets all keys\n    :param name: A name for this TLM\n    :return: A Dict containing all the keys to restore from Embeddings and the TransformerEncoderStack\n    """"""\n    d = {}\n    transformer = pytorch_tlm.transformer if hasattr(pytorch_tlm, \'transformer\') else pytorch_tlm.generator\n    d.update(to_encoder_stack_array(transformer, name=f""{name}/TransformerEncoderStack""))\n    keys_to_write = embeddings_keys if embeddings_keys else list(pytorch_tlm.embeddings.keys())\n\n    for embeddings_key in keys_to_write:\n        d.update(to_embed_array(pytorch_tlm.embeddings[embeddings_key], name=f""{name}/Embeddings/{embeddings_key}""))\n\n    if hasattr(pytorch_tlm.embeddings.reduction, \'ln\'):\n        d.update(to_weight_array(pytorch_tlm.embeddings.reduction.ln, name=f""{name}/Embeddings/reduction/ln""))\n    return d\n\n\ndef save_tlm_npz(pytorch_tlm: nn.Module, npz: str, embeddings_keys: List[str] = None, name: str = ""TLM"", verbose: bool = False):\n    """"""Save a TLM to an NPZ file\n\n    :param pytorch_tlm: A Transformer LM-type module\n    :param npz: A file to save\n    :param embeddings_keys: A key to get embeddings from.  Defaults to `None`, in which case, all embeddings are written\n    :param name: A name for this TLM\n    :param verbose: whether output \n    :return: None\n    """"""\n    d = to_tlm_array(pytorch_tlm, embeddings_keys, name)\n    if verbose:\n        print(d.keys())\n    np.savez(npz, **d)\n\n\ndef to_encoder_stack_array(\n    pytorch_encoder_stack: TransformerEncoderStack, name: str = ""TransformerEncoderStack""\n) -> Dict:\n    """"""Convert a `TransformerEncoderStack` to a set of weigths\n\n    :param pytorch_encoder_stack: A transformer encoder stack\n    :param name: A name\n    :return: A Dict containing a set of weights\n    """"""\n    d = {}\n    if isinstance(pytorch_encoder_stack.ln, nn.LayerNorm):\n        d.update(to_weight_array(pytorch_encoder_stack.ln, f""{name}/ln""))\n    for i, enc_pyt in enumerate(pytorch_encoder_stack.encoders):\n        d.update(to_encoder_array(enc_pyt, f""{name}/{i}""))\n    return d\n\n\ndef from_encoder_stack_array(\n    pytorch_encoder_stack: TransformerEncoderStack, d: Dict, name: str = ""TransformerEncoderStack""\n):\n    """"""Restore weights from a `TransformerEncoderStack`\n\n    :param pytorch_encoder_stack: A transformer encoder stack\n    :param d: A Dict containing sets of arrays\n    :param name: A name for this primitive\n    :return: None\n    """"""\n    if isinstance(pytorch_encoder_stack.ln, nn.LayerNorm):\n        from_weight_array(pytorch_encoder_stack.ln, d, f""{name}/ln"")\n    for i, enc_pyt in enumerate(pytorch_encoder_stack.encoders):\n        from_encoder_array(enc_pyt, d, f""{name}/{i}"")\n\n\ndef from_tlm_array(pytorch_tlm: nn.Module, d: Dict, embeddings_keys: List[str] = None, name: str = ""TLM""):\n    """"""Restore a TLM-like model (possibly a `nn.Module` for fine-tuning)\n\n    We just populate the `TransformerEncoderStack` and the embeddings from weights, all other values remain\n    uninitialized\n\n    :param pytorch_tlm: A TLM-like model\n    :param d: A Dict of weights to restore for each layer\n    :param embeddings_keys: Name of embeddings to restore, defaults to `None`, in which case all embeddings are restored\n    :param name: A name for this primitive\n    :return:\n    """"""\n    transformer = pytorch_tlm.transformer if hasattr(pytorch_tlm, \'transformer\') else pytorch_tlm.generator\n    from_encoder_stack_array(transformer, d, name=f""{name}/TransformerEncoderStack"")\n    keys_to_restore = embeddings_keys if embeddings_keys else list(pytorch_tlm.embeddings.keys())\n    for embeddings_key in keys_to_restore:\n        from_embed_array(pytorch_tlm.embeddings[embeddings_key], d, f""{name}/Embeddings/{embeddings_key}"")\n        if isinstance(pytorch_tlm.embeddings[embeddings_key], LearnedPositionalLookupTableEmbeddingsWithBias):\n            tt = LookupTableEmbeddings(vsz=2, dsz=pytorch_tlm.embeddings.output_dim)\n            from_embed_array(tt, d, f""{name}/Embeddings/tt"")\n            pytorch_tlm.embeddings[embeddings_key].bias = nn.Parameter(tt.embeddings.weight[0])\n        else:\n            from_embed_array(pytorch_tlm.embeddings[embeddings_key], d, f""{name}/Embeddings/{embeddings_key}"")\n    if hasattr(pytorch_tlm.embeddings.reduction, \'ln\'):\n        from_weight_array(pytorch_tlm.embeddings.reduction.ln, d, f""{name}/Embeddings/reduction/ln"")\n\n\n\ndef load_tlm_npz(pytorch_tlm: nn.Module, npz: str, embeddings_keys: List[str] = None, name: str = ""TLM""):\n    """"""Restore a TLM-like model (possibly a `nn.Module` for fine-tuning\n\n    We just populate the `TransformerEncoderStack` and the embeddings from weights, all other values remain\n    uninitialized\n\n    :param pytorch_tlm: A TLM-like model\n    :param npz: A file to restore the weights from\n    :param embeddings_key: Name of embeddings to restore, defaults to `None` in which case we restore all embeddings\n    :param name: A name for this primitive\n    :return:\n    """"""\n    d = np.load(npz)\n    from_tlm_array(pytorch_tlm, d, embeddings_keys, name)\n\n\ndef load_tlm_transformers_bin(pytorch_tlm: nn.Module, bin_file: str, replace_layers=BERT_HF_LAYER_MAP, replace_embeds=BERT_HF_EMBED_MAP):\n    """"""For BERT transformer from HuggingFace, we need a TLM with EmbeddingsStack with 2 features and LN reduce\n\n    The Transformer architecture used by BERT mirrors T2T (with layer norms coming after each transformer layer and\n    a layer norm immediately following a sum reduce of learned-positional embeddings, the word LUT embeddings and\n    a token-type embedding.  For many cases, the token-type embeddings is uninteresting, and should be simply set\n    to 0.  For some cases, setting the token-type is critical.  In MEAD, to support the token type we need a\n    `LookupTableEmbeddings`, and for the token itself, we need a `LearnedPositionalEmbedding` (which adds the two\n    features of the token LUT and the position LUT together.  MEAD composes multiple features with the\n    `EmbeddingsStack` primitive, and provides a reduction operator.  This is equivalent to BERT if we supply\n    the `EmbeddingsStack` with 2 features and the `sum-layer-norm` reduction.  To do this, 2 vectorizers must be supplied\n    to MEAD, one for the usual wordpiece tokenizers, and another tokenizer which has to produce the exact same number\n    of tokens but with a token type value.  For example, for sentence-based token type delimiting, we would want to\n    have a vectorizer that starts with 0, and every time it sees a `[SEP]` in the token itself, produces an incremented\n    value, e.g. 0.... 1..... 2....\n\n    For applications where the token-type is uninteresting, we can actually make a more efficient representation,\n    which we do by creating a single `LearnedPositionalLookupTableEmbeddingsWithBias` embedding object as the single\n    feature to the `EmbeddingsStack`.  This operation looks like a `LearnedPositionalLookupTableEmbeddings` object\n    but it has a learnable bias parameter which is initialized to LUT index 0 of the BERT pretrained checkpoint.\n    Additionally, because this object\'s goal is to replace the compositional approach of having 2 features, it also has\n    a LayerNorm operator under the hood.  If you use this object, under normal circumstances, you do not need to bother\n    providing `EmbeddingsStack` with any reduction as there is only 1 embedding.\n\n    We want to support both types of models being passed in here by the user.  The way we do this, is to check the\n    number of features in the `EmbeddingsStack`.  If its just 1, we can just monkey-patch the object as though it was\n    the 2-feature equivalent, and after its loaded, restore the single feature so that when it goes back to the user\n    they have what the put in originally\n\n    :param pytorch_tlm: A Transformer LM\n    :param bin_file: A HuggingFace PyTorch BERT checkpoint\n    :param replace_layers: The mapping from HuggingFace Transformer keys to MEAD Transformer keys\n    :param replace_embeds: The mapping from HuggingFace Embeddings key to MEAD Embeddings Keys\n    :return:\n    """"""\n    d = torch.load(bin_file)\n    transformer = pytorch_tlm.transformer if hasattr(pytorch_tlm, \'transformer\') else pytorch_tlm.generator\n    num_layers = len(transformer.encoders)\n    mapped_keys = convert_transformers_keys(num_layers, d, replace_layers, replace_embeds)\n    old_embeddings_stack = None\n    k_0 = pytorch_tlm.embeddings.keys()[0]\n\n    # There are 2 options to consider here\n    # Option 1: the user doesnt care about token type embeddings, which means that the embedding type will just be\n    #   the usual LP embeddings with an added bias term set to weight 0\n    # Option 2: the user does care about token types and has provided a token type feature (presumed to be in the\n    #   second key of the embeddings stack\n\n    if isinstance(pytorch_tlm.embeddings[k_0], LearnedPositionalLookupTableEmbeddingsWithBias):\n        old_embeddings_stack = pytorch_tlm.embeddings\n        # we need to temporarily monkey patch the embeddings to load them, and then we can reset them to what they were\n        d = {k_0: pytorch_tlm.embeddings[k_0], \'tt\':  LookupTableEmbeddings(vsz=2, dsz=old_embeddings_stack.output_dim)}\n        pytorch_tlm.embeddings = EmbeddingsStack(d, reduction=\'sum-layer-norm\')\n    unknown_keys = pytorch_tlm.load_state_dict(mapped_keys, strict=False)\n    missing_keys = [key for key in unknown_keys.missing_keys if key != \'embeddings.embeddings.0.bias\']\n\n    if old_embeddings_stack:\n        old_embeddings_stack[k_0].bias = nn.Parameter(pytorch_tlm.embeddings[\'tt\'].embeddings.weight[0])\n        old_embeddings_stack.reduction.ln = pytorch_tlm.embeddings.reduction.ln\n        pytorch_tlm.embeddings = old_embeddings_stack\n\n    return {\'missing\': missing_keys, \'unexpected\': unknown_keys.unexpected_keys}\n'"
layers/eight_mile/tf/__init__.py,0,b''
layers/eight_mile/tf/embeddings.py,0,"b'import math\nimport copy\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom eight_mile.utils import write_json, Offsets, is_sequence, calc_nfeats\nfrom eight_mile.tf.layers import *\n\n\nFLOAT32 = 4\nGB2 = 1024 * 1024 * 1024 * 2\nlogger = logging.getLogger(""baseline"")\n\n\nclass TensorFlowEmbeddings(tf.keras.layers.Layer):\n    """"""This provides a base for TensorFlow embeddings sub-graphs\n\n    """"""\n\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        # tf.kers.layers.Layer has a validation step that only allows certain kwargs\n        # to be passed into it. These are not documented and you need to look into the\n        # code to find this. For now just don\'t pass in out kwargs\n        super().__init__(trainable=trainable, name=name, dtype=dtype)\n        self.W = None\n\n    def get_dsz(self):\n        """"""Get the number of output dimension of this operation\n\n        :return:\n        """"""\n        pass\n\n    def get_vsz(self):\n        """"""Get the number of words (including <PAD>) in the vocabulary\n\n        :return:\n        """"""\n        pass\n\n    def get_weights(self):\n        raise NotImplementedError\n\n    def encode(self, x):\n        """"""This defines the computation of the sub-graph for this object and returns the output node\n\n        :return:\n        """"""\n        pass\n\n    @property\n    def output_dim(self):\n        return self.get_dsz()\n\n    def call(self, x):\n        return self.encode(x)\n\n    def get_feed_dict(self):\n        """"""Return a feed dict that is needed to initialize this embeddings.""""""\n        return {}\n\n\nclass LookupTableEmbeddings(TensorFlowEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, cpu_placement=False, **kwargs):\n        """"""Create a lookup-table based embedding.\n\n        :param name: The name of the feature/placeholder, and a key for the scope\n        :param kwargs:\n\n        :Keyword Arguments: See below\n        * *vsz* -- (``int``) this is the vocabulary (input) size of the lookup table\n        * *dsz* -- (``int``) the output dimension size of this embedding\n        * *finetune* -- (``bool``) (default is `True`) should we allow the sub-graph to learn updated weights\n        * *weights* -- (``numpy.ndarray``) Optional `vsz x dsz` weight matrix for initialization\n        * *unif* -- (``float``) (defaults to `0.1`) If the weights should be created, what is the random initialization range\n        """"""\n        trainable = kwargs.get(""finetune"", trainable)\n        # The layers have a filter of allowed keywords and the docs don\'t list what they are\n        # you need to look in code. We are just not passing kwargs for now.\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self.vsz = kwargs.get(""vsz"")\n        self.dsz = kwargs.get(""dsz"")\n        self.finetune = kwargs.get(""finetune"", trainable)\n        self.scope = kwargs.get(""scope"", ""LUT"")\n        self.dropin = kwargs.get(""dropin"", 0.0)\n        self._weights = kwargs.get(""weights"")\n        self.drop = tf.keras.layers.Dropout(rate=self.dropin, noise_shape=(self.get_vsz(), 1))\n        self.cpu_placement = cpu_placement\n        if self._weights is None:\n            unif = kwargs.get(""unif"", 0.1)\n            self._weights = np.random.uniform(-unif, unif, (self.vsz, self.dsz))\n        else:\n            self.vsz, self.dsz = self._weights.shape\n\n    def build(self, input_shape):\n\n        if self.cpu_placement:\n            with tf.device(""cpu:0""):\n                self.W = self.add_weight(\n                        name=f""{self.scope}/Weight"",\n                        shape=(self.vsz, self.dsz),\n                        initializer=tf.constant_initializer(self._weights),\n                        trainable=self.finetune,\n                )\n        else:\n            self.W = self.add_weight(\n                name=f""{self.scope}/Weight"",\n                shape=(self.vsz, self.dsz),\n                initializer=tf.constant_initializer(self._weights),\n                trainable=self.finetune,\n            )\n        super().build(input_shape)\n\n    def encode(self, x):\n        """"""Build a simple Lookup Table and set as input `x` if it exists, or `self.x` otherwise.\n\n        :param x: An optional input sub-graph to bind to this operation or use `self.x` if `None`\n        :return: The sub-graph output\n        """"""\n        self.x = x\n        e0 = tf.tensor_scatter_nd_update(\n            self.W, tf.constant(Offsets.PAD, dtype=tf.int32, shape=[1, 1]), tf.zeros(shape=[1, self.dsz])\n        )\n        with tf.control_dependencies([e0]):\n            # The ablation table (4) in https://arxiv.org/pdf/1708.02182.pdf shows this has a massive impact\n            embedding_w_dropout = self.drop(self.W, training=TRAIN_FLAG())\n            word_embeddings = tf.nn.embedding_lookup(embedding_w_dropout, self.x)\n\n        return word_embeddings\n\n    def get_vsz(self):\n        return self.vsz\n\n    def get_dsz(self):\n        return self.dsz\n\n    def get_weights(self):\n        return self.W\n\n\nclass CharConvEmbeddings(TensorFlowEmbeddings):\n    """"""dos Santos embeddings extended to parallel filters (AKA Kim character-aware neural language model inputs)\n\n    """"""\n\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        trainable = kwargs.get(""finetune"", trainable)\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self.cpu_placement = bool(kwargs.get(\'cpu_placement\', False))\n        self.scope = kwargs.get(""scope"", ""CharConv"")\n        self.finetune = kwargs.get(""finetune"", trainable)\n        self.nfeat_factor = kwargs.get(""nfeat_factor"", None)\n        self.cfiltsz = kwargs.get(""cfiltsz"", kwargs.get(""filtsz"", [3]))\n        self.max_feat = kwargs.get(""max_feat"", 30)\n        gating = kwargs.get(""gating"", ""skip"")\n        num_gates = kwargs.get(""num_gates"", 1)\n        activation = kwargs.get(""activation"", ""tanh"")\n        self.wsz = kwargs.get(""wsz"", 30)\n        self.projsz = kwargs.get(""projsz"")\n        self.x = None\n        # These are the actual final filter sizes and num features\n        filtsz, nfeats = calc_nfeats(self.cfiltsz, self.nfeat_factor, self.max_feat, self.wsz)\n\n        self.embed = LookupTableEmbeddings(name=f""{self.name}/CharLUT"", finetune=self.finetune, **kwargs)\n        dsz = self.embed.output_dim\n        self.parallel_conv = ParallelConv(dsz, nfeats, filtsz, activation)\n        self.gating_fns = tf.keras.Sequential()\n        for _ in range(num_gates):\n            if gating == \'skip\':\n                self.gating_fns.add(SkipConnection(self.parallel_conv.output_dim, activation))\n            else:\n                self.gating_fns.add(Highway(self.parallel_conv.output_dim))\n\n        self.outsz = self.parallel_conv.output_dim\n        if self.projsz is not None:\n            self.outsz = self.projsz\n            self.proj = tf.keras.layers.Dense(self.outsz, bias_initializer=tf.constant_initializer(0.0))\n\n\n\n    @property\n    def dsz(self):\n        return self.outsz\n\n    def encode(self, x):\n        self.x = x\n\n        mxlen = tf.shape(self.x)[1]\n        mxwlen = tf.shape(self.x)[-1]\n        char_bt_x_w = tf.reshape(self.x, [-1, mxwlen])\n        cembed = self.embed(char_bt_x_w)\n        cmot = self.parallel_conv(cembed)\n        cmot = self.gating_fns(cmot)\n        if self.projsz:\n            cmot = self.proj(cmot)\n        word_char = tf.reshape(cmot, [-1, mxlen, self.outsz])\n        return word_char\n\n    def get_vsz(self):\n        return self.embed.get_vsz()\n\n    def get_dsz(self):\n        return self.outsz\n\n\nclass CharLSTMEmbeddings(TensorFlowEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        trainable = kwargs.get(""finetune"", trainable)\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self.scope = kwargs.get(""scope"", ""CharLUT"")\n        self.finetune = kwargs.get(""finetune"", trainable)\n        self.lstmsz = kwargs.get(""lstmsz"", 50)\n        self.lstm_layers = kwargs.get(""layers"", 1)\n        self.pdrop = kwargs.get(""pdrop"", 0.5)\n        self.rnn_type = kwargs.get(""rnn_type"", ""blstm"")\n        self.x = None\n        self.embed = LookupTableEmbeddings(name=f""{self.name}/CharLUT"", finetune=self.finetune, **kwargs)\n        self.lstm = BiLSTMEncoderHidden(\n            self.embed.output_dim,\n            self.lstmsz,\n            self.lstm_layers,\n            pdrop=self.pdrop,\n            requires_length=True,\n            name=f""{self.name}/blstm"",\n        )\n\n    def encode(self, x):\n        self.x = x\n        shape = tf.shape(x)\n        B = shape[0]\n        T = shape[1]\n        W = shape[2]\n        flat_chars = tf.reshape(x, [-1, W])\n        embed_chars = self.embed(flat_chars)\n\n        # Calculate the lengths of each word\n        word_lengths = tf.reduce_sum(tf.cast(tf.not_equal(flat_chars, Offsets.PAD), tf.int32), axis=1)\n\n        # cuDNN throws an error if there is an input with a length of 0, this happens when the ""word""\n        # is actually a ""<PAD>"" so there are no characters to run the LSTM over. Here we just say\n        # that the lengths is 1. This will make cudnn happy and we will just get junk in that spot\n        patched_lengths = tf.math.maximum(word_lengths, 1)\n\n        # Run the LSTM\n        result = self.lstm((embed_chars, patched_lengths))\n\n        # Create a mask that is true when the length is 0 (where the word was a pad) so that\n        # we can mask out the junk that the lstm created because we needed a length of 1\n        result = tf.multiply(result, tf.expand_dims(tf.cast(tf.not_equal(word_lengths, 0), tf.float32), -1))\n\n        return tf.reshape(result, (B, T, self.lstmsz))\n\n    def call(self, inputs):\n        return self.encode(inputs)\n\n    def get_dsz(self):\n        return self.lstmsz\n\n    def get_vsz(self):\n        return self.embed.get_vsz()\n\n\nclass CharTransformerEmbeddings(TensorFlowEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        trainable = kwargs.get(""finetune"", trainable)\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self.scope = kwargs.get(""scope"", ""CharLUT"")\n        self.finetune = kwargs.get(""finetune"", trainable)\n        self.embed = LookupTableEmbeddings(name=f""{self.name}/CharLUT"", finetune=self.finetune, **kwargs)\n        self.d_model = kwargs.get(""wsz"", 30)\n        self.num_heads = kwargs.get(""num_heads"", 3)\n        self.rpr_k = kwargs.get(""rpr_k"", 10)\n        layers = kwargs.get(""layers"", 1)\n        pdrop = kwargs.get(""pdrop"", 0.5)\n        self.char_comp = TransformerEncoderStackWithLengths(\n            self.num_heads,\n            self.d_model,\n            pdrop,\n            False,\n            layers,\n            rpr_k=self.rpr_k,\n            input_sz=self.embed.output_dim,\n            name=f""{self.name}/transformer"",\n        )\n\n    def encode(self, x):\n        self.x = x\n        shape = tf.shape(x)\n        B = shape[0]\n        T = shape[1]\n        W = shape[2]\n        flat_chars = tf.reshape(x, [-1, W])\n        embed_chars = self.embed(flat_chars)\n\n        # Calculate the lengths of each word\n        lengths = tf.reduce_sum(tf.cast(tf.not_equal(flat_chars, Offsets.PAD), tf.int32), axis=1)\n\n        # Run the LSTM\n        result = self.char_comp((embed_chars, lengths))\n\n        pooled = tf.reduce_max(result, -2, keepdims=False)\n\n        return tf.reshape(pooled, (B, T, self.d_model))\n\n    def call(self, inputs):\n        return self.encode(inputs)\n\n    def get_dsz(self):\n        return self.d_model\n\n    def get_vsz(self):\n        return self.embed.get_vsz()\n\n\nclass PositionalMixin(tf.keras.layers.Layer):\n    def positional(self, length):\n        pass\n\n\nclass SinusoidalPositionalMixin(PositionalMixin):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n\n        max_timescale = kwargs.get(""max_timescale"", 1.0e4)\n        # Match the mxlen pytorch has because it precomputes the timing signal\n        mxlen = kwargs.get(\'mxlen\', 10000)\n\n        word_dsz = self.get_dsz()\n        log_timescale_increment = math.log(max_timescale) / float(word_dsz)\n        inv_timescales = np.exp(np.arange(0, word_dsz, 2, dtype=np.float32) * -log_timescale_increment)\n\n        pe = np.zeros((mxlen, word_dsz), dtype=np.float32)\n        position = np.expand_dims(np.arange(0, mxlen, dtype=np.float32), 1)\n\n        pe[:, 0::2] = np.sin(position * inv_timescales)\n        pe[:, 1::2] = np.cos(position * inv_timescales)\n\n        self.pe = tf.expand_dims(pe, 0)\n\n    def positional(self, length):\n        return self.pe[:, :length]\n\n\nclass LearnedPositionalMixin(PositionalMixin):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        trainable = kwargs.get(""finetune"", trainable)\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self.mxlen = int(kwargs.get(""mxlen"", 512))\n        self.pos_weights = kwargs.get(""pos_weights"")\n        if self.pos_weights is None:\n            unif = float(kwargs.get(""unif"", 0.1))\n            self.pos_weights = np.random.uniform(-unif, unif, (self.mxlen, self.get_dsz()))\n\n    def build(self, input_shape):\n        self.pos = self.add_weight(\n            name=""pos"",\n            initializer=tf.constant_initializer(self.pos_weights),\n            shape=[self.mxlen, self.get_dsz()],\n            trainable=self.finetune,\n        )\n        super().build(input_shape)\n\n    def positional(self, length):\n        return tf.expand_dims(tf.nn.embedding_lookup(self.pos, tf.range(length, dtype=tf.int32)), 0)\n\n\nclass PositionalLookupTableEmbeddings(SinusoidalPositionalMixin, LookupTableEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.scale = math.sqrt(self.get_dsz())\n        self.dropout = tf.keras.layers.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def encode(self, x):\n        x = super().encode(x) * tf.constant(self.scale)\n        T = tf.shape(x)[1]\n        pos = self.positional(T)\n        return self.dropout(x + pos, training=TRAIN_FLAG())\n\n\nclass LearnedPositionalLookupTableEmbeddings(LearnedPositionalMixin, LookupTableEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.dropout = tf.keras.layers.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def encode(self, x):\n        x = super().encode(x)\n        T = tf.shape(x)[1]\n        pos = self.positional(T)\n        return self.dropout(x + pos, training=TRAIN_FLAG())\n\n\nclass LearnedPositionalLookupTableEmbeddingsWithBias(LearnedPositionalMixin, LookupTableEmbeddings):\n    """"""Learned positional lookup table embeddings wih a bias and layer norm\n\n    This is just a typical learned positional embedding but with a learnable\n    bias and a layer norm.  This is equivalent to BERT embeddings when the\n    token_type is not set\n\n    """"""\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        self.bias = self.add_weight(\n            name=""bias"",\n            initializer=tf.constant_initializer(0.0),\n            shape=[1, self.get_dsz()],\n            trainable=self.finetune,\n        )\n\n    def encode(self, x):\n        x = super().encode(x)\n        T = tf.shape(x)[1]\n        pos = self.positional(T)\n        x = x + pos + self.bias\n        return x\n\n\nclass PositionalCharConvEmbeddings(SinusoidalPositionalMixin, CharConvEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.scale = math.sqrt(self.get_dsz())\n        self.dropout = tf.keras.layers.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def encode(self, x):\n        x = super().encode(x) * tf.constant(self.scale)\n        T = tf.shape(x)[1]\n        pos = self.positional(T)\n        return self.dropout(x + pos, training=TRAIN_FLAG())\n\n\nclass LearnedPositionalCharConvEmbeddings(LearnedPositionalMixin, CharConvEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.dropout = tf.keras.layers.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def encode(self, x):\n        x = super().encode(x)\n        T = tf.shape(x)[1]\n        pos = self.positional(T)\n        return self.dropout(x + pos, training=TRAIN_FLAG())\n\n\nclass PositionalCharLSTMEmbeddings(SinusoidalPositionalMixin, CharLSTMEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        trainable = kwargs.get(""finetune"", trainable)\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self.scale = math.sqrt(self.get_dsz())\n        self.dropout = tf.keras.layers.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def encode(self, x):\n        x = super().encode(x) * tf.constant(self.scale)\n        T = tf.shape(x)[1]\n        pos = self.positional(T)\n        return self.dropout(x + pos, training=TRAIN_FLAG())\n\n\nclass LearnedPositionalCharLSTMEmbeddings(LearnedPositionalMixin, CharLSTMEmbeddings):\n    def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n        trainable = kwargs.get(""finetune"", trainable)\n        super().__init__(trainable=trainable, name=name, dtype=dtype, **kwargs)\n        self.dropout = tf.keras.layers.Dropout(kwargs.get(""dropout"", 0.0))\n\n    def encode(self, x):\n        x = super().encode(x)\n        T = tf.shape(x)[1]\n        pos = self.positional(T)\n        return self.dropout(x + pos, training=TRAIN_FLAG())\n\n\n# All the train functions don\'t have the large lut init codes anymore and it needs a placeholder\n# So lets skip it for now\n# https://stackoverflow.com/questions/43288147/how-do-i-use-a-very-large-2m-word-embedding-in-tensorflow\n# class LargeLookupTableEmbeddings(LookupTableEmbeddings):\n#     """"""Create a really large embedding matrix in tensorflow.\n\n#     Tensorflow has a limit on the size that a op can be (2GB). When we have very\n#     large embedding lookuptables (for example when we don\'t prune the vocab) we\n#     hit this limit and can\'t have the embeddings in the graph. This is due to a\n#     limit in the size that a thing can be in a protocol buffer (how tensorflow\n#     serializes the graph).\n\n#     Here we get around it with a placeholder. The place holder will be in the\n#     graph and it will know it needs to have a size of [vsz, dsz] but it doesn\'t\n#     have the actual values so it can be serialized into a protocol buffer since\n#     it is small.\n\n#     We then have a variable that is initialized with the value of the\n#     placeholder. This is filled in with value during the `sess.run` of\n#     `tf.compat.v1.global_variables_initialzier` with a feed_dict. Values are then saved\n#     into the checkpoint and can be reloaded from there.\n\n#     ```\n#     sess.run(tf.compat.v1.global_variables_initializer(), {e.W_place: e._weights})\n#     ```\n\n#     The future of this with tf 2 is unclear because it uses a placeholder to\n#     get around the size limit and placeholders are not cool in tf2\n#     """"""\n#     def __init__(self, trainable=True, name=None, dtype=tf.float32, **kwargs):\n#         super().__init__(trainable, name, dtype, **kwargs)\n#         self.W_place = tf.placeholder(tf.float32, shape=(self.vsz, self.dsz))\n\n#     def build(self, input_shape):\n#         self.W = tf.get_variable(f""{self.scope}/weight"", initializer=self.W_place)\n\n#     def get_feed_dict(self):\n#         """"""Feed dict mapping the numpy weights to the placeholder.""""""\n#         return {self.W_place: self._weights}\n'"
layers/eight_mile/tf/layers.py,18,"b'import logging\nimport tensorflow as tf\nimport numpy as np\nfrom eight_mile.utils import listify, Offsets, wraps, get_version, is_sequence\nfrom typing import Optional, Union, List, Dict, Any, Tuple\nimport contextlib\nimport math\n\nBASELINE_TF_TRAIN_FLAG = None\nLOGGER = logging.getLogger(\'mead.layers\')\n\n@contextlib.contextmanager\ndef autograph_options(options):\n    old_opts = tf.config.optimizer.get_experimental_options()\n    tf.config.optimizer.set_experimental_options(options)\n    try:\n        yield\n    finally:\n        tf.config.optimizer.set_experimental_options(old_opts)\n\ndef set_tf_eager_mode(prefer_eager: bool = False):\n    tf_version = get_version(tf)\n    if prefer_eager and tf_version < 2:\n        LOGGER.info(\'user requesting eager on 1.x\')\n        tf.compat.v1.enable_eager_execution()\n    elif not prefer_eager and tf_version >= 2:\n        LOGGER.info(\'User requesting eager disabled on 2.x\')\n        tf.compat.v1.disable_eager_execution()\n\n\ndef set_tf_eager_debug(debug: bool = False):\n    if tf.executing_eagerly():\n        if debug:\n            tf.config.experimental_run_functions_eagerly(debug)\n\n\ndef set_tf_log_level(ll):\n    # 0     | DEBUG            | [Default] Print all messages\n    # 1     | INFO             | Filter out INFO messages\n    # 2     | WARNING          | Filter out INFO & WARNING messages\n    # 3     | ERROR            | Filter out all messages\n    import os\n\n    TF_VERSION = get_version(tf)\n    if TF_VERSION < 2:\n        import tensorflow.compat.v1.logging as tf_logging\n    else:\n        from absl import logging as tf_logging\n    tf_ll = tf_logging.WARN\n    tf_cpp_ll = 1\n    ll = ll.lower()\n    if ll == ""debug"":\n        tf_ll = tf_logging.DEBUG\n        tf_cpp_ll = 0\n    if ll == ""info"":\n        tf_cpp_ll = 0\n        tf_ll = tf_logging.INFO\n    if ll == ""error"":\n        tf_ll = tf_logging.ERROR\n        tf_cpp_ll = 2\n    tf_logging.set_verbosity(tf_ll)\n    os.environ[""TF_CPP_MIN_LOG_LEVEL""] = f""{tf_cpp_ll}""\n\n\ndef SET_TRAIN_FLAG(X):\n    global BASELINE_TF_TRAIN_FLAG\n    BASELINE_TF_TRAIN_FLAG = X\n\n\ndef TRAIN_FLAG():\n    """"""Create a global training flag on first use""""""\n    global BASELINE_TF_TRAIN_FLAG\n    if BASELINE_TF_TRAIN_FLAG is not None:\n        return BASELINE_TF_TRAIN_FLAG\n\n    BASELINE_TF_TRAIN_FLAG = tf.compat.v1.placeholder_with_default(False, shape=(), name=""TRAIN_FLAG"")\n    return BASELINE_TF_TRAIN_FLAG\n\n\ndef infer_lengths(tensor, axis=1):\n    """"""Infer the lengths of an input based on the idea the Offsets.PAD was used as the padding token.\n\n    :param tensor: The data to infer the length of, should be either [B, T] or [T, B]\n    :param axis: The dimension which contains the sequential signal\n\n    :returns: A Tensor of shape `[B]` that has the lengths for example item in the batch\n    """"""\n    if len(tensor.get_shape()) != 2:\n        raise ValueError(f""infer_lengths only works with tensors with two dims right now, got {len(tensor.get_shape())}"")\n    offsets = tf.expand_dims(tf.range(1, tf.shape(tensor)[axis] + 1, dtype=tensor.dtype), 1 - axis)\n    non_pad_loc = tf.cast(tf.not_equal(tensor, Offsets.PAD), tensor.dtype)\n    return tf.argmax(non_pad_loc * offsets, axis=axis) + 1\n\n\n# Mapped\ndef tensor_and_lengths(inputs):\n    if isinstance(inputs, (list, tuple)):\n        in_tensor, lengths = inputs\n    else:\n        in_tensor = inputs\n        lengths = None  ##tf.reduce_sum(tf.cast(tf.not_equal(inputs, 0), tf.int32), axis=1)\n\n    return in_tensor, lengths\n\n\n# Get rid of this?\ndef new_placeholder_dict(train):\n    global BASELINE_TF_TRAIN_FLAG\n\n    if train:\n        return {BASELINE_TF_TRAIN_FLAG: 1}\n    return {}\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + tf.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * tf.pow(x, 3))))\n\n\ndef swish(x):\n    return x * tf.nn.sigmoid(x)\n\n\ndef masked_fill(t, mask, value):\n    return t * (1 - tf.cast(mask, t.dtype)) + value * tf.cast(mask, t.dtype)\n\n\n# https://stackoverflow.com/questions/41897212/how-to-sort-a-multi-dimensional-tensor-using-the-returned-indices-of-tf-nn-top-k\ndef gather_k(a, b, best_idx, k):\n    shape_a = get_shape_as_list(a)\n    auxiliary_indices = tf.meshgrid(\n        *[tf.range(d) for d in (tf.unstack(shape_a[: (a.get_shape().ndims - 1)]) + [k])], indexing=""ij""\n    )\n\n    sorted_b = tf.gather_nd(b, tf.stack(auxiliary_indices[:-1] + [best_idx], axis=-1))\n    return sorted_b\n\n\ndef get_shape_as_list(x):\n    """"""\n    This function makes sure we get a number whenever possible, and otherwise, gives us back\n    a graph operation, but in both cases, presents as a list.  This makes it suitable for a\n    bunch of different operations within TF, and hides away some details that we really dont care about, but are\n    a PITA to get right...\n\n    Borrowed from Alec Radford:\n    https://github.com/openai/finetune-transformer-lm/blob/master/utils.py#L38\n    """"""\n    try:\n        ps = x.get_shape().as_list()\n    except:\n        ps = x.shape\n    ts = tf.shape(x)\n    return [ts[i] if ps[i] is None else ps[i] for i in range(len(ps))]\n\n\ndef bth2bht(t):\n    return tf.transpose(t, [0, 2, 1])\n\n\ndef ident(t):\n    return t\n\n\ndef tbh2bht(t):\n    return tf.tranpose(t, [0, 2, 1])\n\n\ndef tbh2bth(t):\n    return tf.transpose(t, [1, 0, 2])\n\n\ndef bth2tbh(t):\n    return t.transpose(t, [1, 0, 2])\n\n\n# Mapped\ndef get_activation(name: str = ""relu""):\n    if name is None or name == ""ident"":\n        return tf.nn.identity\n    if name == ""softmax"":\n        return tf.nn.softmax\n    if name == ""tanh"":\n        return tf.nn.tanh\n    if name == ""sigmoid"":\n        return tf.nn.sigmoid\n    if name == ""gelu"":\n        return gelu\n    if name == ""swish"":\n        return swish\n        return tf.identity\n    if name == ""leaky_relu"":\n        return tf.nn.leaky_relu\n    return tf.nn.relu\n\n\nclass WithoutLength(tf.keras.layers.Layer):\n    """"""Wrapper layer to remove lengths from the input\n    """"""\n\n    def __init__(self, layer: tf.keras.layers.Layer, name=None):\n        super().__init__(name=name)\n        self.layer = layer\n        self.output_dim = self.layer.output_dim if hasattr(self.layer, ""output_dim"") else 0\n\n    def call(self, inputs):\n        output = self.layer(inputs[0])\n        return output\n\n# Mapped\nclass ConvEncoder(tf.keras.layers.Layer):\n    def __init__(self, insz: Optional[int], outsz: int, filtsz: int, pdrop: float = 0.0, activation: str = ""relu"", name=None):\n        super().__init__(name=name)\n        self.output_dim = outsz\n        self.conv = tf.keras.layers.Conv1D(filters=outsz, kernel_size=filtsz, padding=""same"")\n        self.act = get_activation(activation)\n        self.dropout = tf.keras.layers.Dropout(pdrop)\n\n    def call(self, inputs):\n        conv_out = self.act(self.conv(inputs))\n        return self.dropout(conv_out, TRAIN_FLAG())\n\n\nclass ConvEncoderStack(tf.keras.layers.Layer):\n    def __init__(\n        self, insz: Optional[int], outsz: int, filtsz: int, nlayers: int = 1, pdrop: float = 0.0, activation: str = ""relu"", name=None\n    ):\n        super().__init__(name=name)\n        self.layers = []\n        first_layer = ConvEncoder(insz, outsz, filtsz, pdrop, activation)\n        self.layers.append(first_layer)\n        for i in range(nlayers - 1):\n            subsequent_layer = ResidualBlock(ConvEncoder(insz, outsz, filtsz, pdrop, activation))\n            self.layers.append(subsequent_layer)\n\n    def call(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nclass PassThru(tf.keras.layers.Layer):\n\n    def __init__(self, input_dim):\n        super().__init__()\n        self.output_dim = input_dim\n\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n        return inputs\n\n\n# Mapped\nclass ParallelConv(tf.keras.layers.Layer):\n    DUMMY_AXIS = 1\n    TIME_AXIS = 2\n    FEATURE_AXIS = 3\n\n    def __init__(\n        self,\n        insz: Optional[int],\n        outsz: Union[int, List[int]],\n        filtsz: List[int],\n        activation: str = ""relu"",\n        name: Optional[str] = None,\n        **kwargs,\n    ):\n        """"""Do parallel convolutions with multiple filter widths and max-over-time pooling.\n\n        :param insz: The input size (not required, can pass `None`)\n        :param outsz: The output size(s).  Normally this is an int, but it can be a stack of them\n        :param filtsz: The list of filter widths to use.\n        :param activation: (``str``) The name of the activation function to use (`default=\'relu`)\n        :param name: An optional name\n        """"""\n        super().__init__(name=name)\n        self.Ws = []\n        self.bs = []\n        self.activation = get_activation(activation)\n        self.insz = insz\n        self.filtsz = filtsz\n        motsz = outsz\n        if not is_sequence(outsz):\n            motsz = [outsz] * len(filtsz)\n        self.motsz = motsz\n        self.output_dim = sum(motsz)\n\n\n    def build(self, input_shape):\n        insz = self.insz if self.insz is not None else tf.shape(input_shape)[-1]\n        for fsz, cmotsz in zip(self.filtsz, self.motsz):\n            kernel_shape = [1, int(fsz), int(insz), int(cmotsz)]\n            self.Ws.append(self.add_weight(f""cmot-{fsz}/W"", shape=kernel_shape))\n            self.bs.append(\n                self.add_weight(f""cmot-{fsz}/b"", shape=[cmotsz], initializer=tf.constant_initializer(0.0))\n            )\n\n\n    def call(self, inputs):\n        """"""\n        :param inputs: The inputs in the shape [B, T, H].\n        :return: Combined result\n        """"""\n        mots = []\n        expanded = tf.expand_dims(inputs, ParallelConv.DUMMY_AXIS)\n        for W, b in zip(self.Ws, self.bs):\n            conv = tf.nn.conv2d(expanded, W, strides=[1, 1, 1, 1], padding=""SAME"", name=""CONV"")\n            activation = self.activation(tf.nn.bias_add(conv, b), ""activation"")\n            mot = tf.reduce_max(activation, [ParallelConv.TIME_AXIS], keepdims=True)\n            mots.append(mot)\n        combine = tf.reshape(tf.concat(values=mots, axis=ParallelConv.FEATURE_AXIS), [-1, self.output_dim])\n        return combine\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], self.output_dim\n\n    @property\n    def requires_length(self):\n        return False\n\n\ndef lstm_cell(hsz: int, forget_bias: float = 1.0, **kwargs):\n    """"""Produce a single cell with no dropout\n    :param hsz: (``int``) The number of hidden units per LSTM\n    :param forget_bias: (``int``) Defaults to 1\n    :return: a cell\n    """"""\n    num_proj = kwargs.get(""projsz"")\n    if num_proj and num_proj == hsz:\n        num_proj = None\n    cell = tf.contrib.rnn.LSTMCell(hsz, forget_bias=forget_bias, state_is_tuple=True, num_proj=num_proj)\n    skip_conn = bool(kwargs.get(""skip_conn"", False))\n    return tf.nn.rnn_cell.ResidualWrapper(cell) if skip_conn else cell\n\n\ndef lstm_cell_w_dropout(\n    hsz: int, pdrop: float, forget_bias: float = 1.0, variational: bool = False, training: bool = False, **kwargs\n):\n    """"""Produce a single cell with dropout\n    :param hsz: (``int``) The number of hidden units per LSTM\n    :param pdrop: (``int``) The probability of keeping a unit value during dropout\n    :param forget_bias: (``int``) Defaults to 1\n    :param variational (``bool``) variational recurrence is on\n    :param training (``bool``) are we training? (defaults to ``False``)\n    :return: a cell\n    """"""\n    output_keep_prob = tf.contrib.framework.smart_cond(training, lambda: 1.0 - pdrop, lambda: 1.0)\n    state_keep_prob = tf.contrib.framework.smart_cond(\n        training, lambda: 1.0 - pdrop if variational else 1.0, lambda: 1.0\n    )\n    num_proj = kwargs.get(""projsz"")\n    cell = tf.contrib.rnn.LSTMCell(hsz, forget_bias=forget_bias, state_is_tuple=True, num_proj=num_proj)\n    skip_conn = bool(kwargs.get(""skip_conn"", False))\n    cell = tf.nn.rnn_cell.ResidualWrapper(cell) if skip_conn else cell\n    output = tf.contrib.rnn.DropoutWrapper(\n        cell,\n        output_keep_prob=output_keep_prob,\n        state_keep_prob=state_keep_prob,\n        variational_recurrent=variational,\n        dtype=tf.float32,\n    )\n    return output\n\n\nclass LSTMEncoder2(tf.keras.layers.Layer):\n    """"""The LSTM encoder is a base for a set of encoders producing various outputs.\n\n    All LSTM encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`)\n    """"""\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int = 1,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        requires_length: bool = True,\n        name: Optional[str] = None,\n        dropout_in_single_layer: bool = False,\n        skip_conn: bool = False,\n        projsz: Optional[int] = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input or `None`\n        :param hsz: The number of hidden units per LSTM\n        :param nlayers: The number of layers of LSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! Provide a graph layer name\n        :param dropout_in_single_layer: TF only! If we have a single layer, should we dropout (defaults to `False`)\n        :param skip_conn: TF only! This parameter isnt currently supported in TF Keras implementation\n        :param projsz: TF only! This parameter isnt currently supported in TF Keras implementation\n        """"""\n        super().__init__(name=name)\n        self.output_dim = hsz\n        self._requires_length = requires_length\n        self.rnns = []\n        for _ in range(nlayers - 1):\n            self.rnns.append(\n                tf.keras.layers.LSTM(\n                    hsz,\n                    return_sequences=True,\n                    recurrent_dropout=pdrop if variational else 0.0,\n                    dropout=pdrop if not variational else 0.0,\n                )\n            )\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        self.rnns.append(\n            tf.keras.layers.LSTM(\n                hsz,\n                return_sequences=True,\n                return_state=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n            )\n        )\n\n    def output_fn(self, output, state):\n        return output, state\n\n    def call(self, inputs):\n        """"""RNNs over input sequence of `[B, T, C]` and lengths `[B]`, output `[B, S, H]` where `S = max(lengths)`\n\n        :param inputs: A tuple of `(sequence, lengths)`, `sequence` shape `[B, T, C]`, lengths shape = `[B]`\n        :return: Output depends on the subclass handling\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        for rnn in self.rnns:\n            outputs = rnn(inputs, mask=mask)\n            inputs = outputs\n        rnnout, h, c = outputs\n        return self.output_fn(rnnout, (h, c))\n\n    @property\n    def requires_length(self) -> bool:\n        return self._requires_length\n\n\nclass LSTMEncoderWithState2(tf.keras.layers.Layer):\n\n    """"""LSTM encoder producing the hidden state and the output, where the input doesnt require any padding\n    """"""\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int = 1,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        name: Optional[str] = None,\n        dropout_in_single_layer: bool = False,\n        skip_conn: bool = False,\n        projsz: Optional[int] = None,\n        **kwargs,\n    ):\n        super().__init__(name=name)\n\n        self._requires_length = False\n        self.hsz = hsz\n        self.rnns = []\n        for _ in range(nlayers - 1):\n            self.rnns.append(\n                tf.keras.layers.LSTM(\n                    hsz,\n                    return_sequences=True,\n                    return_state=True,\n                    recurrent_dropout=pdrop if variational else 0.0,\n                    dropout=pdrop if not variational else 0.0,\n                )\n            )\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        self.rnns.append(\n            tf.keras.layers.LSTM(\n                hsz,\n                return_sequences=True,\n                return_state=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n            )\n        )\n        self.requires_state = True\n\n    @property\n    def output_dim(self):\n        return self.hsz\n\n    def call(self, inputs):\n        """"""The format of the output here is\n\n        output: `[B, T, H]`\n        hidden: List[(h, c), (h, c), ...]`\n        :param inputs:\n        :return:\n        """"""\n        inputs, hidden_state_input = inputs\n\n        hidden_outputs = []\n        initial_state = None\n        for i, rnn in enumerate(self.rnns):\n            if hidden_state_input is not None:\n                hidden_state = hidden_state_input[i]\n                initial_state = (hidden_state[0], hidden_state[1])\n            outputs, h, c = rnn(inputs, initial_state=initial_state)\n            hidden_outputs.append((h, c))\n            inputs = outputs\n        return outputs, hidden_outputs\n\n    def zero_state(self, batchsz: int):\n        """"""Zero state for LSTM with batch size given\n\n        :param batchsz: The batch size\n        """"""\n        num_rnns = len(self.rnns)\n        zstate = []\n        for rnn in self.rnns:\n            zstate.append(\n                #rnn.get_initial_state(tf.zeros(()))\n                (tf.zeros((batchsz, self.hsz), dtype=np.float32), tf.zeros((batchsz, self.hsz), dtype=tf.float32))\n            )\n\n        return zstate\n\n\nclass LSTMEncoderSequence2(LSTMEncoder2):\n\n    """"""LSTM encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n    """"""\n\n    def output_fn(self, output, state):\n        """"""Return sequence `[B, S, H]` where `S = max(lengths)`\n\n        :param output: The sequence\n        :param state: The hidden state\n        :return: The sequence `[B, S, H]`\n        """"""\n        return output\n\n\nclass LSTMEncoderHidden2(LSTMEncoder2):\n\n    """"""LSTM encoder that returns the top hidden state\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n    """"""\n\n    def output_fn(self, output, state):\n        """"""Get the last hidden layer\n\n        :param output:\n        :param state:\n        :return: hidden unit tensor of shape `[B, H]`\n        """"""\n        return state[0]\n\n\nclass LSTMEncoderHiddenContext2(LSTMEncoder2):\n\n    def output_fn(self, output, state):\n        """"""Return last hidden state `(h, c)`\n\n        :param output: The sequence\n        :param state: The hidden state\n        :return: The last hidden state `(h, c)`\n        """"""\n        return state\n\n\nclass GRUEncoder(tf.keras.layers.Layer):\n\n    """"""GRU encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n    """"""\n\n    def __init__(\n            self,\n            insz: Optional[int],\n            hsz: int,\n            nlayers: int = 1,\n            pdrop: float = 0.0,\n            variational: bool = False,\n            requires_length: bool = True,\n            name: Optional[str] = None,\n            dropout_in_single_layer: bool = False,\n            **kwargs,\n    ):\n        """"""Produce a stack of GRUs with dropout performed on all but the last layer.\n        :param insz: An optional input size for parity with other layer backends.  Can pass `None`\n        :param hsz: The number of hidden units per GRU\n        :param nlayers: The number of layers of GRUs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: variational recurrence is on, defaults to `False`\n        :param requires_length: Does the input require an input length (defaults to ``True``)\n        :param name: TF only! Put a name in the graph for this layer. Optional, defaults to `None`\n        :param dropout_in_single_layer: TF only! If there is a single layer, should we do dropout, defaults to `False`\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.rnns = []\n        self.output_dim = hsz\n\n        for _ in range(nlayers - 1):\n            self.rnns.append(\n                tf.keras.layers.GRU(\n                    hsz,\n                    return_sequences=True,\n                    recurrent_dropout=pdrop if variational else 0.0,\n                    dropout=pdrop if not variational else 0.0,\n                )\n            )\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        self.rnns.append(\n            tf.keras.layers.GRU(\n                hsz,\n                return_sequences=True,\n                return_state=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n            )\n        )\n\n    def output_fn(self, output, state):\n        return output, state\n\n    def call(self, inputs):\n        """"""RNNs over input sequence of `[B, T, C]` and lengths `[B]`, output `[B, S, H]` where `S = max(lengths)`\n\n        :param inputs: A tuple of `(sequence, lengths)`, `sequence` shape `[B, T, C]`, lengths shape = `[B]`\n        :return: Output depends on the subclass handling\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        for rnn in self.rnns:\n            outputs = rnn(inputs, mask=mask)\n            inputs = outputs\n        rnnout, h = outputs\n        return self.output_fn(rnnout, h)\n\n    @property\n    def requires_length(self) -> bool:\n        return self._requires_length\n\n\nclass GRUEncoderAll(tf.keras.layers.Layer):\n    """"""GRU encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a hidden vector `[L, B, H]`\n    """"""\n    def __init__(\n            self,\n            insz: Optional[int],\n            hsz: int,\n            nlayers: int = 1,\n            pdrop: float = 0.0,\n            variational: bool = False,\n            requires_length: bool = True,\n            name: Optional[str] = None,\n            dropout_in_single_layer=False,\n            **kwargs,\n    ):\n        """"""Produce a stack of GRUs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input (or `None`)\n        :param hsz: The number of hidden units per GRU\n        :param nlayers: The number of layers of GRUs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: TF only! apply variational dropout\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! A name to give the layer in the graph\n        :param dropout_in_single_layer: TF only! If its a single layer cell, should we do dropout?  Default to `False`\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.rnns = []\n        self.output_dim = hsz\n\n        for _ in range(nlayers - 1):\n            rnn = tf.keras.layers.GRU(\n                hsz,\n                return_sequences=True,\n                return_state=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n            )\n            self.rnns.append(rnn)\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        rnn = tf.keras.layers.GRU(\n            hsz,\n            return_sequences=True,\n            return_state=True,\n            recurrent_dropout=pdrop if variational else 0.0,\n            dropout=pdrop if not variational else 0.0,\n            )\n\n        # This concat mode only works on the sequences, we still are getting 4 objects back for the state\n        self.rnns.append(rnn)\n\n    def output_fn(self, rnnout, state):\n        return rnnout, state\n\n    def call(self, inputs):\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H] or `[B, H, S]` , and a hidden vector `[L, B, H]`\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        # (num_layers * num_directions, batch, hidden_size):\n        hs = []\n        for rnn in self.rnns:\n            outputs, h = rnn(inputs, mask=mask)\n            hs.append(h)\n            inputs = outputs\n\n        h = tf.stack(hs)\n        return self.output_fn(outputs, h)\n\n    @property\n    def requires_length(self):\n        return self._requires_length\n\n\nclass GRUEncoderSequence(GRUEncoder):\n\n    """"""GRU encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n    """"""\n\n    def output_fn(self, output, state):\n        """"""Return sequence `(BxTxC)`\n\n        :param output: The sequence\n        :param state: The hidden state\n        :return: The sequence `(BxTxC)`\n        """"""\n        return output\n\n\nclass GRUEncoderHidden(GRUEncoder):\n\n    """"""BiGRU encoder that returns the top hidden state\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n    """"""\n\n    def output_fn(self, output, state):\n        """"""Return last hidden state `h`\n\n        :param output: The sequence\n        :param state: The hidden state\n        :return: The last hidden state `(h, c)`\n        """"""\n        return state\n\n\nclass LSTMEncoder1(tf.keras.layers.Layer):\n    """"""The LSTM encoder is a base for a set of encoders producing various outputs.\n\n    All LSTM encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`)\n    """"""\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int = 1,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        requires_length: bool = True,\n        name: Optional[str] = None,\n        dropout_in_single_layer: bool = False,\n        skip_conn: bool = False,\n        projsz: Optional[int] = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input or `None`\n        :param hsz: The number of hidden units per LSTM\n        :param nlayers: The number of layers of LSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! Provide a graph layer name\n        :param dropout_in_single_layer: TF only! If we have a single layer, should we dropout (defaults to `False`)\n        :param skip_conn: TF only! Should there be residual connections between RNNs\n        :param projsz: TF only! Should we do `LSTMP` operation to a different output size\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.output_dim = hsz\n\n        if variational or dropout_in_single_layer:\n            self.rnn = tf.contrib.rnn.MultiRNNCell(\n                [\n                    lstm_cell_w_dropout(\n                        hsz, pdrop, variational=variational, training=TRAIN_FLAG(), skip_conn=skip_conn, projsz=projsz\n                    )\n                    for _ in range(nlayers)\n                ],\n                state_is_tuple=True,\n            )\n        else:\n            self.rnn = tf.contrib.rnn.MultiRNNCell(\n                [\n                    lstm_cell_w_dropout(hsz, pdrop, training=TRAIN_FLAG(), skip_conn=skip_conn, projsz=projsz)\n                    if i < nlayers - 1\n                    else lstm_cell(hsz, skip_conn=skip_conn, projsz=projsz)\n                    for i in range(nlayers)\n                ],\n                state_is_tuple=True,\n            )\n\n    def call(self, inputs):\n        """"""RNNs over input sequence of `[B, T, C]` and lengths `[B]`, output `[B, S, H]` where `S = max(lengths)`\n\n        :param inputs: A tuple of `(sequence, lengths)`, `sequence` shape `[B, T, C]`, lengths shape = `[B]`\n        :return: Output depends on the subclass handling\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        ns = tf.contrib.framework.get_name_scope()\n        with tf.name_scope(ns), tf.variable_scope(ns):\n            rnnout, hidden = tf.nn.dynamic_rnn(self.rnn, inputs, sequence_length=lengths, dtype=tf.float32)\n        state = (hidden[-1].h, hidden[-1].c)\n        return self.output_fn(rnnout, state)\n\n    def output_fn(self, output, state):\n        return output, state\n\n    @property\n    def requires_length(self):\n        return self._requires_length\n\n\nclass LSTMEncoderSequence1(LSTMEncoder1):\n\n    """"""LSTM encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n    """"""\n\n    def output_fn(self, output, state):\n        """"""Return sequence `[B, S, H]` where `S = max(lengths)`\n\n        :param output: The sequence\n        :param state: The hidden state\n        :return: The sequence `[B, S, H]`\n        """"""\n        return output\n\n\nclass LSTMEncoderAllLegacy(LSTMEncoder1):\n    """"""LSTM encoder that passes along the full output and hidden states for each layer\n\n    *TF Note*: This module does not change the underlying TF output, we only support this to make it easier to use\n    with existing TF primitives, especially in seq2seq\n    """"""\n    def call(self, inputs):\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H]` , and tuple of hidden `[L, B, H]` and context `[L, B, H]`\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        ns = tf.contrib.framework.get_name_scope()\n        with tf.name_scope(ns), tf.variable_scope(ns):\n            rnnout, encoder_state = tf.nn.dynamic_rnn(self.rnn, inputs, sequence_length=lengths, dtype=tf.float32)\n\n        return self.output_fn(rnnout, encoder_state)\n\n    def output_fn(self, output, hidden):\n        return output, hidden\n\n\nclass LSTMEncoderAll1(LSTMEncoderAllLegacy):\n    """"""LSTM encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a tuple of hidden vector `[L, B, H]` and context vector `[L, B, H]`, respectively\n\n    *TF Note*: This module reorganizes the underlying TF output, which means you must be careful if using\n    this with another tf 1 `RNNCell` implementation\n    """"""\n\n    def output_fn(self, output, hidden):\n\n        h = []\n        c = []\n        for i in range(len(hidden)):\n            h.append(hidden[i].h)\n            c.append(hidden[i].c)\n\n        encoder_state = tf.stack(h), tf.stack(c)\n        return output, encoder_state\n\n\nclass LSTMEncoderHidden1(LSTMEncoder1):\n\n    """"""LSTM encoder that returns the top hidden state\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n    """"""\n\n    def output_fn(self, output, state):\n        """"""Get the last hidden layer\n\n        :param output:\n        :param state:\n        :return: hidden unit tensor of shape `[B, H]`\n        """"""\n        return state[0]\n\n\n# TODO: make this available from PyTorch or get rid of it\nclass LSTMEncoderHiddenContext1(LSTMEncoder1):\n\n    def output_fn(self, output, state):\n        return state\n\n\nclass LSTMEncoderWithState1(LSTMEncoder1):\n\n    """"""LSTM encoder producing the hidden state and the output, where the input doesnt require any padding\n    """"""\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int = 1,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        name: Optional[str] = None,\n        dropout_in_single_layer: bool = True,\n        **kwargs,\n    ):\n        super().__init__(\n            insz=insz,\n            hsz=hsz,\n            nlayers=nlayers,\n            pdrop=pdrop,\n            variational=variational,\n            requires_length=False,\n            name=name,\n            dropout_in_single_layer=dropout_in_single_layer,\n            **kwargs,\n        )\n        self.requires_state = True\n\n    def zero_state(self, batchsz: int):\n        """"""Zero state for LSTM with batch size given\n\n        :param batchsz: The batch size\n        """"""\n        return self.rnn.zero_state(batchsz, tf.float32)\n\n    def call(self, inputs):\n        inputs, hidden = inputs\n        ns = tf.contrib.framework.get_name_scope()\n        with tf.name_scope(ns), tf.variable_scope(ns):\n            rnnout, hidden = tf.nn.dynamic_rnn(self.rnn, inputs, initial_state=hidden, dtype=tf.float32)\n        return rnnout, hidden  # (hidden[-1].h, hidden[-1].c)\n\n\nclass LSTMEncoderAll2(tf.keras.layers.Layer):\n    """"""LSTM encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a tuple of hidden vector `[L, B, H]` and context vector `[L, B, H]`, respectively\n\n    *PyTorch note*: Takes a vector of shape `[B, T, C]` or `[B, C, T]`, depending on input specification\n    of `batch_first`. Also note that in PyTorch, this defaults to `True`\n\n    """"""\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int = 1,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        requires_length: bool = True,\n        name: Optional[str] = None,\n        dropout_in_single_layer=False,\n        skip_conn: bool = False,\n        projsz: Optional[int] = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input or `None`\n        :param hsz: The number of hidden units per LSTM\n        :param nlayers: The number of layers of LSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: TF only! Do variational dropout\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! Provide a graph layer name\n        :param dropout_in_single_layer: TF only! If we have a single layer, should we dropout (defaults to `False`)\n        :param skip_conn: TF only! Not supported with tf.keras.layers implementation\n        :param projsz: TF only! Not supported with tf.keras.layers implementation\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.output_dim = hsz\n        self.rnns = []\n        for _ in range(nlayers - 1):\n            rnn = tf.keras.layers.LSTM(\n                hsz,\n                return_sequences=True,\n                return_state=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n            )\n            self.rnns.append(rnn)\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        rnn = tf.keras.layers.LSTM(\n            hsz,\n            return_sequences=True,\n            return_state=True,\n            recurrent_dropout=pdrop if variational else 0.0,\n            dropout=pdrop if not variational else 0.0,\n        )\n\n        # This concat mode only works on the sequences, we still are getting 4 objects back for the state\n        self.rnns.append(rnn)\n\n    def output_fn(self, rnnout, state):\n        return rnnout, state\n\n    def call(self, inputs):\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H]` , and tuple of hidden `[L, B, H]` and context `[L, B, H]`\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        hs = []\n        cs = []\n        for rnn in self.rnns:\n            outputs, h, c = rnn(inputs, mask=mask)\n            hs.append(h)\n            cs.append(c)\n            inputs = outputs\n\n        h = tf.stack(hs)\n        c = tf.stack(cs)\n        return self.output_fn(outputs, (h, c))\n\n    @property\n    def requires_length(self):\n        return self._requires_length\n\n\nclass BiLSTMEncoderAll2(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int = 1,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        requires_length: bool = True,\n        name: Optional[str] = None,\n        dropout_in_single_layer: bool = False,\n        skip_conn: bool = False,\n        projsz: Optional[int] = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input (or `None`)\n        :param hsz: The number of hidden units per BiLSTM (`hsz//2` used for each direction and concatenated)\n        :param nlayers: The number of layers of BiLSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: TF only! apply variational dropout\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! A name to give the layer in the graph\n        :param dropout_in_single_layer: TF only! If its a single layer cell, should we do dropout?  Default to `False`\n        :param skip_conn: TF 1 only!  Not supported on this implementation\n        :param projsz: TF 1 only! Not supported on this implementation\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.output_dim = hsz\n        self.rnns = []\n        for _ in range(nlayers - 1):\n            rnn = tf.keras.layers.LSTM(\n                hsz // 2,\n                return_sequences=True,\n                return_state=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n            )\n            self.rnns.append(tf.keras.layers.Bidirectional(rnn))\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        rnn = tf.keras.layers.LSTM(\n            hsz // 2,\n            return_sequences=True,\n            return_state=True,\n            recurrent_dropout=pdrop if variational else 0.0,\n            dropout=pdrop if not variational else 0.0,\n        )\n\n        # This concat mode only works on the sequences, we still are getting 4 objects back for the state\n        self.rnns.append(tf.keras.layers.Bidirectional(rnn, merge_mode=""concat""))\n\n    def output_fn(self, rnnout, state):\n        return rnnout, state\n\n    def call(self, inputs):\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H], and tuple of hidden `[L, B, H]` and context `[L, B, H]`\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        # (num_layers * num_directions, batch, hidden_size):\n        hs = []\n        cs = []\n        for rnn in self.rnns:\n            outputs, h1, c1, h2, c2 = rnn(inputs, mask=mask)\n            h = tf.stack([h1, h2])\n            c = tf.stack([c1, c2])\n            hs.append(h)\n            cs.append(c)\n            inputs = outputs\n\n        _, B, H = get_shape_as_list(h)\n        h = tf.reshape(tf.stack(hs), [-1, B, H * 2])\n        c = tf.reshape(tf.stack(cs), [-1, B, H * 2])\n        return self.output_fn(outputs, (h, c))\n\n    @property\n    def requires_length(self) -> bool:\n        return self._requires_length\n\n\nclass BiLSTMEncoder2(tf.keras.layers.Layer):\n    """"""BiLSTM encoder base for a set of encoders producing various outputs.\n\n    All BiLSTM encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`).  Because its bidirectional, half of the hidden units given in the\n    constructor will be applied to the forward direction and half to the backward direction, and these will get\n    concatenated.\n    """"""\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        requires_length: bool = True,\n        name: Optional[str] = None,\n        dropout_in_single_layer: bool = False,\n        skip_conn: bool = False,\n        projsz: Optional[int] = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input (or `None`)\n        :param hsz: The number of hidden units per BiLSTM (`hsz//2` used for each direction and concatenated)\n        :param nlayers: The number of layers of BiLSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: TF only! apply variational dropout\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! A name to give the layer in the graph\n        :param dropout_in_single_layer: TF only! If its a single layer cell, should we do dropout?  Default to `False`\n        :param skip_conn: TF 1 only!  Not supported on this implementation\n        :param projsz: TF 1 only! Not supported on this implementation\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.rnns = []\n        self.output_dim = hsz\n        for _ in range(nlayers - 1):\n            rnn = tf.keras.layers.LSTM(\n                hsz // 2,\n                return_sequences=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n            )\n            self.rnns.append(tf.keras.layers.Bidirectional(rnn))\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        rnn = tf.keras.layers.LSTM(\n            hsz // 2,\n            return_sequences=True,\n            return_state=True,\n            recurrent_dropout=pdrop if variational else 0.0,\n            dropout=pdrop if not variational else 0.0,\n        )\n\n        # This concat mode only works on the sequences, we still are getting 4 objects back for the state\n        self.rnns.append(tf.keras.layers.Bidirectional(rnn, merge_mode=""concat""))\n\n    def output_fn(self, rnnout, state):\n        return rnnout, state\n\n    def call(self, inputs):\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        for rnn in self.rnns:\n            outputs = rnn(inputs, mask=mask)\n            inputs = outputs\n\n        rnnout, h_fwd, c_fwd, h_bwd, c_bwd = outputs\n        return self.output_fn(rnnout, ((h_fwd, c_fwd), (h_bwd, c_bwd)))\n\n    @property\n    def requires_length(self):\n        return self._requires_length\n\n\nclass BiLSTMEncoderSequence2(BiLSTMEncoder2):\n\n    """"""BiLSTM encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n    """"""\n\n    def output_fn(self, rnnout, state):\n        return rnnout\n\n\nclass BiLSTMEncoderHidden2(BiLSTMEncoder2):\n    """"""BiLSTM encoder that returns the top hidden state\n\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n    """"""\n\n    def output_fn(self, rnnout, state):\n        return tf.concat([state[0][0], state[1][0]], axis=-1)\n\n\nclass BiLSTMEncoderHiddenContext2(BiLSTMEncoder2):\n\n    def output_fn(self, rnnout, state):\n        return tuple(tf.concat([state[0][i], state[1][i]], axis=-1) for i in range(2))\n\n\nclass BiGRUEncoder(tf.keras.layers.Layer):\n    """"""BiGRU encoder base for a set of encoders producing various outputs.\n\n    All BiGRU encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`).  Because its bidirectional, half of the hidden units given in the\n    constructor will be applied to the forward direction and half to the backward direction, and these will get\n    concatenated.\n    """"""\n    def __init__(\n            self,\n            insz: Optional[int],\n            hsz: int,\n            nlayers: int,\n            pdrop: float = 0.0,\n            variational: bool = False,\n            requires_length: bool = True,\n            name: Optional[str] = None,\n            dropout_in_single_layer: bool = False,\n            **kwargs,\n    ):\n        """"""Produce a stack of BiGRUs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input (or `None`)\n        :param hsz: The number of hidden units per BiLSTM (`hsz//2` used for each direction and concatenated)\n        :param nlayers: The number of layers of BiLSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: TF only! apply variational dropout\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! A name to give the layer in the graph\n        :param dropout_in_single_layer: TF only! If its a single layer cell, should we do dropout?  Default to `False`\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.rnns = []\n        self.output_dim = hsz\n        for _ in range(nlayers - 1):\n            rnn = tf.keras.layers.GRU(\n                hsz // 2,\n                return_sequences=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n                )\n            self.rnns.append(tf.keras.layers.Bidirectional(rnn))\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        rnn = tf.keras.layers.GRU(\n            hsz // 2,\n            return_sequences=True,\n            return_state=True,\n            recurrent_dropout=pdrop if variational else 0.0,\n            dropout=pdrop if not variational else 0.0,\n            )\n\n        # This concat mode only works on the sequences, we still are getting 2 objects back for the state\n        self.rnns.append(tf.keras.layers.Bidirectional(rnn, merge_mode=""concat""))\n\n    def output_fn(self, rnnout, state):\n        return rnnout, state\n\n    def call(self, inputs):\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        for rnn in self.rnns:\n            outputs = rnn(inputs, mask=mask)\n            inputs = outputs\n\n        rnnout, h_fwd, h_bwd = outputs\n        return self.output_fn(rnnout, (h_fwd, h_bwd))\n\n    @property\n    def requires_length(self):\n        return self._requires_length\n\n\nclass BiGRUEncoderSequence(BiGRUEncoder):\n\n    """"""BiGRU encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n\n    """"""\n    def output_fn(self, rnnout, state):\n        return rnnout\n\n\nclass BiGRUEncoderHidden(BiGRUEncoder):\n\n    """"""BiGRU encoder that returns the top hidden state\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n    """"""\n\n    def output_fn(self, _, state):\n        return tf.concat([state[0], state[1]], axis=-1)\n\n\nclass BiGRUEncoderAll(tf.keras.layers.Layer):\n    """"""BiGRU encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a hidden vector `[L, B, H]`\n    """"""\n    def __init__(\n            self,\n            insz: Optional[int],\n            hsz: int,\n            nlayers: int = 1,\n            pdrop: float = 0.0,\n            variational: bool = False,\n            requires_length: bool = True,\n            name: Optional[str] = None,\n            dropout_in_single_layer: bool = False,\n            **kwargs,\n    ):\n        """"""Produce a stack of BiGRUs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input (or `None`)\n        :param hsz: The number of hidden units per BiGRU (`hsz//2` used for each direction and concatenated)\n        :param nlayers: The number of layers of BiGRUs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: TF only! apply variational dropout\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! A name to give the layer in the graph\n        :param dropout_in_single_layer: TF only! If its a single layer cell, should we do dropout?  Default to `False`\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.rnns = []\n        self.output_dim = hsz\n\n        for _ in range(nlayers - 1):\n            rnn = tf.keras.layers.GRU(\n                hsz // 2,\n                return_sequences=True,\n                return_state=True,\n                recurrent_dropout=pdrop if variational else 0.0,\n                dropout=pdrop if not variational else 0.0,\n                )\n            self.rnns.append(tf.keras.layers.Bidirectional(rnn))\n        if nlayers == 1 and not dropout_in_single_layer and not variational:\n            pdrop = 0.0\n        rnn = tf.keras.layers.GRU(\n            hsz // 2,\n            return_sequences=True,\n            return_state=True,\n            recurrent_dropout=pdrop if variational else 0.0,\n            dropout=pdrop if not variational else 0.0,\n            )\n\n        # This concat mode only works on the sequences, we still are getting 4 objects back for the state\n        self.rnns.append(tf.keras.layers.Bidirectional(rnn, merge_mode=""concat""))\n\n    def output_fn(self, rnnout, state):\n        return rnnout, state\n\n    def call(self, inputs):\n        """"""\n        :param inputs: A tuple containing the input tensor `[B, T, C]` or `[B, H, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H] or `[B, H, S]` , and a hidden vector `[L, B, H]`\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        mask = tf.sequence_mask(lengths)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        # (num_layers * num_directions, batch, hidden_size):\n        hs = []\n        for rnn in self.rnns:\n            outputs, h1, h2 = rnn(inputs, mask=mask)\n            h = tf.stack([h1, h2])\n            hs.append(h)\n            inputs = outputs\n\n        _, B, H = get_shape_as_list(h)\n        h = tf.reshape(tf.stack(hs), [-1, B, H * 2])\n        return self.output_fn(outputs, h)\n\n    @property\n    def requires_length(self) -> bool:\n        return self._requires_length\n\n\nclass BiLSTMEncoder1(tf.keras.layers.Layer):\n    """"""BiLSTM encoder base for a set of encoders producing various outputs.\n\n    All BiLSTM encoders inheriting this class will trim the input to the max length given in the batch.  For example,\n    if the input sequence is `[B, T, C]` and the `S = max(lengths)` then the resulting sequence, if produced, will\n    be length `S` (or more precisely, `[B, S, H]`).  Because its bidirectional, half of the hidden units given in the\n    constructor will be applied to the forward direction and half to the backward direction, and these will get\n    concatenated.\n    """"""\n\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: int,\n        nlayers: int,\n        pdrop: float = 0.0,\n        variational: bool = False,\n        requires_length: bool = True,\n        name: Optional[str] = None,\n        skip_conn: bool = False,\n        projsz: Optional[int] = None,\n        **kwargs,\n    ):\n        """"""Produce a stack of LSTMs with dropout performed on all but the last layer.\n\n        :param insz: The size of the input (or `None`)\n        :param hsz: The number of hidden units per BiLSTM (`hsz//2` used for each direction and concatenated)\n        :param nlayers: The number of layers of BiLSTMs to stack\n        :param pdrop: The probability of dropping a unit value during dropout, defaults to 0\n        :param variational: TF only! apply variational dropout\n        :param requires_length: Does this encoder require an input length in its inputs (defaults to `True`)\n        :param name: TF only! A name to give the layer in the graph\n        :param dropout_in_single_layer: TF only! If its a single layer cell, should we do dropout?  Default to `False`\n        :param skip_conn: TF 1 only!  Do residual connections between RNN layers\n        :param projsz: TF 1 only! Do LSTMP operation to this output size\n        """"""\n        super().__init__(name=name)\n        self._requires_length = requires_length\n        self.layers = nlayers\n        self.output_dim = hsz\n\n        hsz = hsz // 2\n        if variational:\n            self.fwd_rnn = tf.contrib.rnn.MultiRNNCell(\n                [\n                    lstm_cell_w_dropout(\n                        hsz, pdrop, variational=variational, training=TRAIN_FLAG(), skip_conn=skip_conn, projsz=projsz\n                    )\n                    for _ in range(nlayers)\n                ],\n                state_is_tuple=True,\n            )\n            self.bwd_rnn = tf.contrib.rnn.MultiRNNCell(\n                [\n                    lstm_cell_w_dropout(\n                        hsz, pdrop, variational=variational, training=TRAIN_FLAG(), skip_conn=skip_conn, projsz=projsz\n                    )\n                    for _ in range(nlayers)\n                ],\n                state_is_tuple=True,\n            )\n        else:\n            self.fwd_rnn = tf.contrib.rnn.MultiRNNCell(\n                [\n                    lstm_cell_w_dropout(hsz, pdrop, training=TRAIN_FLAG(), skip_conn=skip_conn, projsz=projsz)\n                    if i < nlayers - 1\n                    else lstm_cell(hsz, skip_conn=skip_conn, projsz=projsz)\n                    for i in range(nlayers)\n                ],\n                state_is_tuple=True,\n            )\n            self.bwd_rnn = tf.contrib.rnn.MultiRNNCell(\n                [\n                    lstm_cell_w_dropout(hsz, pdrop, training=TRAIN_FLAG(), skip_conn=skip_conn, projsz=projsz)\n                    if i < nlayers - 1\n                    else lstm_cell(hsz)\n                    for i in range(nlayers)\n                ],\n                state_is_tuple=True,\n            )\n\n    def output_fn(self, rnnout, state):\n        return rnnout, state\n\n    def call(self, inputs):\n        inputs, lengths = tensor_and_lengths(inputs)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        ns = tf.contrib.framework.get_name_scope()\n        with tf.name_scope(ns), tf.variable_scope(ns):\n            rnnout, (fwd_state, backward_state) = tf.nn.bidirectional_dynamic_rnn(\n                self.fwd_rnn, self.bwd_rnn, inputs, sequence_length=lengths, dtype=tf.float32\n            )\n        rnnout = tf.concat(axis=2, values=rnnout)\n        return self.output_fn(\n            rnnout, ((fwd_state[-1].h, fwd_state[-1].c), (backward_state[-1].h, backward_state[-1].c))\n        )\n\n    @property\n    def requires_length(self) -> bool:\n        return self._requires_length\n\n\nclass BiLSTMEncoderAllLegacy(BiLSTMEncoder1):\n    """"""BiLSTM encoder that passes along the full output and hidden states for each layer\n\n    *TF Note*: This module reorganizes the underlying TF output, which means you must be careful if using\n    this with another tf 1 `RNNCell` implementation\n    """"""\n\n    def call(self, inputs):\n        """"""\n        :param inputs: A 1tuple containing the input tensor `[B, T, C]` and a length `[B]`\n        :return: An output tensor `[B, S, H], and tuple of hidden `[L, B, H]` and context `[L, B, H]`\n        """"""\n        inputs, lengths = tensor_and_lengths(inputs)\n        max_length = tf.reduce_max(lengths)\n        inputs = inputs[:, :max_length, :]\n        ns = tf.contrib.framework.get_name_scope()\n        with tf.name_scope(ns), tf.variable_scope(ns):\n            rnnout, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n                self.fwd_rnn, self.bwd_rnn, inputs, sequence_length=lengths, dtype=tf.float32\n            )\n        rnnout = tf.concat(axis=2, values=rnnout)\n\n        return self.output_fn(rnnout, encoder_state)\n\n    def output_fn(self, rnnout, encoder_state):\n        fwd_state, bwd_state = encoder_state\n        encoder_state = []\n        for i in range(self.layers):\n            h = tf.concat([fwd_state[i].h, bwd_state[i].h], -1)\n            c = tf.concat([fwd_state[i].c, bwd_state[i].c], -1)\n            encoder_state.append(tf.contrib.rnn.LSTMStateTuple(h=h, c=c))\n        encoder_state = tuple(encoder_state)\n        return rnnout, encoder_state\n\n\n\nclass BiLSTMEncoderAll1(BiLSTMEncoderAllLegacy):\n    """"""BiLSTM encoder that passes along the full output and hidden states for each layer\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]`\n\n    This returns a 2-tuple of outputs `[B, S, H]` where `S = max(lengths)`, for the output vector sequence,\n    and a tuple of hidden vector `[L, B, H]` and context vector `[L, B, H]`, respectively\n\n    *TF Note*: This module reorganizes the underlying TF output, which means you must be careful if using\n    this with another tf 1 `RNNCell` implementation\n    """"""\n\n    def output_fn(self, out, encoder_state):\n        fwd_state, bwd_state = encoder_state\n        hs = []\n        cs = []\n        for i in range(self.layers):\n            h = tf.concat([fwd_state[i].h, bwd_state[i].h], -1)\n            c = tf.concat([fwd_state[i].c, bwd_state[i].c], -1)\n            hs.append(h)\n            cs.append(c)\n        encoder_state = (tf.stack(hs), tf.stack(cs))\n        return out, encoder_state\n\n\nclass BiLSTMEncoderSequence1(BiLSTMEncoder1):\n\n    """"""BiLSTM encoder to produce the transduced output sequence.\n\n    Takes a tuple of tensor, shape `[B, T, C]` and a lengths of shape `[B]` and produce an output sequence of\n    shape `[B, S, H]` where `S = max(lengths)`.  The lengths of the output sequence may differ from the input\n    sequence if the `max(lengths)` given is shorter than `T` during execution.\n    """"""\n    def output_fn(self, rnnout, state):\n        return rnnout\n\n\nclass BiLSTMEncoderHidden1(BiLSTMEncoder1):\n\n    """"""BiLSTM encoder that returns the top hidden state\n\n\n    Takes a tuple containing a tensor input of shape `[B, T, C]` and lengths of shape `[B]` and\n    returns a hidden unit tensor of shape `[B, H]`\n    """"""\n    def output_fn(self, rnnout, state):\n        return tf.concat([state[0][0], state[1][0]], axis=-1)\n\n\nclass BiLSTMEncoderHiddenContext1(BiLSTMEncoder1):\n\n    def output_fn(self, rnnout, state):\n        return state\n\n\nif get_version(tf) < 2:\n    LSTMEncoder = LSTMEncoder1\n    LSTMEncoderSequence = LSTMEncoderSequence1\n    LSTMEncoderWithState = LSTMEncoderWithState1\n    LSTMEncoderHidden = LSTMEncoderHidden1\n    LSTMEncoderHiddenContext = LSTMEncoderHiddenContext1\n    LSTMEncoderAll = LSTMEncoderAll1\n    BiLSTMEncoder = BiLSTMEncoder1\n    BiLSTMEncoderSequence = BiLSTMEncoderSequence1\n    BiLSTMEncoderHidden = BiLSTMEncoderHidden1\n    BiLSTMEncoderHiddenContext = BiLSTMEncoderHiddenContext1\n    BiLSTMEncoderAll = BiLSTMEncoderAll1\n    from tensorflow.contrib.crf import crf_decode, crf_log_norm, crf_unary_score, crf_binary_score\n\n    def crf_sequence_score(inputs, tag_indices, sequence_lengths, transition_params):\n        """"""Computes the unnormalized score for a tag sequence.\n\n        This is a patched version of the contrib\n        where we dont do any length 1 sequence optimizations.  This was causing a very odd error\n        where the true branch of smart_cond was being executed despite the predicate evaluating to 0.\n        This probably makes it even slower than usual :(\n\n        Args:\n          inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials\n              to use as input to the CRF layer.\n          tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which we\n              compute the unnormalized score.\n          sequence_lengths: A [batch_size] vector of true sequence lengths.\n          transition_params: A [num_tags, num_tags] transition matrix.\n        Returns:\n          sequence_scores: A [batch_size] vector of unnormalized sequence scores.\n        """"""\n\n        # Compute the scores of the given tag sequence.\n        unary_scores = crf_unary_score(tag_indices, sequence_lengths, inputs)\n        binary_scores = crf_binary_score(tag_indices, sequence_lengths, transition_params)\n        sequence_scores = unary_scores + binary_scores\n        return sequence_scores\n\n\nelse:\n    LSTMEncoder = LSTMEncoder2\n    LSTMEncoderSequence = LSTMEncoderSequence2\n    LSTMEncoderWithState = LSTMEncoderWithState2\n    LSTMEncoderHidden = LSTMEncoderHidden2\n    LSTMEncoderHiddenContext = LSTMEncoderHiddenContext2\n    LSTMEncoderAll = LSTMEncoderAll2\n    BiLSTMEncoder = BiLSTMEncoder2\n    BiLSTMEncoderSequence = BiLSTMEncoderSequence2\n    BiLSTMEncoderHidden = BiLSTMEncoderHidden2\n    BiLSTMEncoderHiddenContext = BiLSTMEncoderHiddenContext2\n    BiLSTMEncoderAll = BiLSTMEncoderAll2\n    from tensorflow_addons.text.crf import crf_decode, crf_sequence_score, crf_log_norm\n\n\n\nclass Reduction(tf.keras.layers.Layer):\n\n    def __init__(self):\n        super().__init__()\n\n    def call(self, inputs: List[tf.Tensor]) -> tf.Tensor:\n        pass\n\n\nclass ConcatReduction(Reduction):\n    def __init__(self, output_dims: List[int], axis=-1):\n        super().__init__()\n        self.axis = axis\n        self.output_dim = sum(output_dims)\n\n    def call(self, inputs: List[tf.Tensor]) -> tf.Tensor:\n        return tf.concat(values=inputs, axis=-1)\n\n\nclass SumReduction(Reduction):\n    def __init__(self, output_dims: List[int]):\n        super().__init__()\n        # We could actually project if we needed, or at least should validate\n        self.output_dim = output_dims[0]\n\n    def call(self, inputs: List[tf.Tensor]) -> tf.Tensor:\n        return tf.add_n(inputs)\n\n\nclass SumLayerNormReduction(Reduction):\n\n    def __init__(self, output_dims: List[int], layer_norm_eps: float = 1.0e-12):\n        super().__init__()\n        self.ln = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n        self.output_dim = output_dims[0]\n\n    def call(self, inputs: List[tf.Tensor]) -> tf.Tensor:\n        outputs = tf.add_n(inputs)\n        return self.ln(outputs)\n\nclass EmbeddingsStack(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        embeddings_dict: Dict[str, tf.keras.layers.Layer],\n        dropout_rate: float = 0.0,\n        requires_length: bool = False,\n        reduction: Optional[Union[str, tf.keras.layers.Layer]] = \'concat\',\n        name: Optional[str] = None,\n        **kwargs,\n    ):\n        """"""Takes in a dictionary where the keys are the input tensor names, and the values are the embeddings\n\n        :param embeddings_dict: (``dict``) dictionary of each feature embedding\n        """"""\n\n        super().__init__(name=name)\n        self.embeddings = embeddings_dict\n\n        output_dims = []\n        for embedding in embeddings_dict.values():\n            output_dims += [embedding.get_dsz()]\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n        self._requires_length = requires_length\n        if isinstance(reduction, str):\n            if reduction == \'sum\':\n                self.reduction = SumReduction(output_dims)\n            elif reduction == \'sum-layer-norm\':\n                self.reduction = SumLayerNormReduction(output_dims, layer_norm_eps=kwargs.get(\'layer_norm_eps\', 1.0e-12))\n            else:\n                self.reduction = ConcatReduction(output_dims)\n        else:\n            self.reduction = reduction\n        self.dsz = self.reduction.output_dim\n\n    def keys(self):\n        return self.embeddings.keys()\n\n    def items(self):\n        return self.embeddings.items()\n\n    def __getitem__(self, item: str):\n        return self.embeddings[item]\n\n    def call(self, inputs: Dict[str, tf.Tensor]) -> tf.Tensor:\n        """"""This method performs ""embedding"" of the inputs.  The base method here then concatenates along depth\n        dimension to form word embeddings\n\n        :return: A 3-d vector where the last dimension is the concatenated dimensions of all embeddings\n        """"""\n        all_embeddings_out = []\n        for k, embedding in self.embeddings.items():\n            x = inputs[k]\n            embeddings_out = embedding(x)\n            all_embeddings_out.append(embeddings_out)\n        word_embeddings = self.reduction(all_embeddings_out)\n        return self.dropout(word_embeddings, TRAIN_FLAG())\n\n    #@property\n    #def dsz(self) -> int:\n    #    total_dsz = 0\n    #    for embeddings in self.embeddings.values():\n    #        total_dsz += embeddings.get_dsz()\n    #    return total_dsz\n\n    def keys(self):\n        return self.embeddings.keys()\n\n    @property\n    def requires_length(self) -> bool:\n        return self._requires_length\n\n    @property\n    def output_dim(self) -> bool:\n        return self.dsz\n\n\nclass WeightTieDense(tf.keras.layers.Layer):\n    def __init__(self, tied, name=""weight-tied"", use_bias=False):\n        super().__init__(name=name)\n        self.tied = tied\n        self.use_bias = use_bias\n\n    def _add_bias(self, W):\n        if self.use_bias:\n            self.bias = self.add_weight(\'bias\', shape=[tf.shape(W)[0], ], initializer=\'zeros\', regularizer=None,\n                                        constraint=None, dtype=self.W.dtype, trainable=True)\n        else:\n            self.bias = None\n\n    def build(self, input_shape):\n        emb = getattr(self.tied, ""embedding_layer"", None)\n        if emb is not None:\n            self.W = getattr(emb, ""W"")\n            self._add_bias(self.W)\n            super().build(input_shape)\n            return\n        W = getattr(self.tied, ""W"", None)\n        if W is not None:\n            self.W = W\n            self._add_bias(self.W)\n            super().build(input_shape)\n            return\n        self.W = getattr(self.tied, ""kernel"")\n        self._add_bias(self.W)\n        super().build()\n\n    def call(self, inputs):\n        shape = tf.shape(inputs)\n        inputs = tf.reshape(inputs, [-1, shape[-1]])\n        outs = tf.matmul(inputs, self.W, transpose_b=True)\n        if self.use_bias:\n            outs = tf.nn.bias_add(outs, self.bias)\n        new_shape = tf.concat([shape[:-1], tf.constant([-1])], axis=0)\n        return tf.reshape(outs, new_shape)\n\n\nclass DenseStack(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        insz: Optional[int],\n        hsz: Union[int, List[int]],\n        activation: Union[str, List[str]] = ""relu"",\n        pdrop_value: float = 0.5,\n        init: Optional[Any] = None,\n        name: Optional[str] = None,\n        skip_connect = False,\n        layer_norm = False,\n        **kwargs,\n    ):\n        """"""Stack 1 or more hidden layers, optionally (forming an MLP)\n\n        :param hsz: The number of hidden units\n        :param activation: The name of the activation function to use\n        :param pdrop_value: The dropout probability\n        :param init: The tensorflow initializer\n\n        """"""\n        super().__init__(name=name)\n        hszs = listify(hsz)\n        self.output_dim = hszs[-1]\n        activations = listify(activation)\n        if len(activations) == 1:\n            activations = activations * len(hszs)\n        if len(activations) != len(hszs):\n            raise ValueError(""Number of activations must match number of hidden sizes in a stack!"")\n        if layer_norm:\n            layer_norm_eps = kwargs.get(\'layer_norm_eps\', 1e-6)\n        if skip_connect:\n            if not insz:\n                raise ValueError(""In order to use skip connection, insz must be provided in DenseStack!"")\n            current = insz\n        layer_stack = []\n        for hsz, activation in zip(hszs, activations):\n            if skip_connect and current == hsz:\n                layer_stack.append(SkipConnection(hsz, activation))\n                current = hsz\n            else:\n                layer_stack.append(tf.keras.layers.Dense(hsz, activation))\n            if layer_norm:\n                layer_stack.append(tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps))\n        self.layer_stack = layer_stack\n        self.dropout = tf.keras.layers.Dropout(pdrop_value)\n\n    def call(self, inputs):\n        """"""Stack 1 or more hidden layers, optionally (forming an MLP)\n\n        :param inputs: The fixed representation of the model\n        :param training: (``bool``) A boolean specifying if we are training or not\n        :param init: The tensorflow initializer\n        :param kwargs: See below\n\n        :Keyword Arguments:\n        * *hsz* -- (``int``) The number of hidden units (defaults to `100`)\n\n        :return: The final layer\n        """"""\n        x = inputs\n        for layer in self.layer_stack:\n            x = layer(x)\n            x = self.dropout(x, TRAIN_FLAG())\n        return x\n\n    @property\n    def requires_length(self) -> bool:\n        return False\n\n\nclass WithDropout(tf.keras.layers.Layer):\n    """"""This is a utility wrapper that applies dropout after executing the layer it wraps\n\n    For variational dropout, we use `SpatialDropout1D` as described in:\n    https://github.com/keras-team/keras/issues/7290\n    """"""\n    def __init__(self, layer: tf.keras.layers.Layer, pdrop: float = 0.5, variational: bool = False):\n        super().__init__()\n        self.layer = layer\n        self.dropout = tf.keras.layers.SpatialDropout1D(pdrop) if variational else tf.keras.layers.Dropout(pdrop)\n\n    def call(self, inputs):\n        return self.dropout(self.layer(inputs), TRAIN_FLAG())\n\n    @property\n    def output_dim(self) -> int:\n        return self.layer.output_dim\n\n\nclass WithDropoutOnFirst(tf.keras.layers.Layer):\n    """"""Wrapper for any layer that surrounds it with dropout\n\n    This exists primarily for the LSTMEncoderWithState to allow dropout on the output while\n    passing back the hidden state\n\n    For variational dropout, we use `SpatialDropout1D` as described in:\n    https://github.com/keras-team/keras/issues/7290\n    """"""\n    def __init__(self, layer: tf.keras.layers.Layer, pdrop: float = 0.5, variational: bool = False):\n        super().__init__()\n        self.layer = layer\n        self.dropout = tf.keras.layers.SpatialDropout1D(pdrop) if variational else tf.keras.layers.Dropout(pdrop)\n\n    def call(self, inputs):\n        outputs = self.layer(inputs)\n        return self.dropout(outputs[0], TRAIN_FLAG()), outputs[1]\n\n    @property\n    def output_dim(self) -> int:\n        return self.layer.output_dim\n\n\nclass Highway(tf.keras.layers.Layer):\n    def __init__(self, input_size: int, name: Optional[str] = None, **kwargs):\n        super().__init__(name=name)\n        self.proj = tf.keras.layers.Dense(input_size, activation=""relu"")\n        self.transform = tf.keras.layers.Dense(\n            input_size, bias_initializer=tf.keras.initializers.Constant(value=-2.0), activation=""sigmoid""\n        )\n        self.output_dim = input_size\n\n    def call(self, inputs):\n        proj_result = self.proj(inputs)\n        proj_gate = self.transform(inputs)\n        gated = (proj_gate * proj_result) + ((1 - proj_gate) * inputs)\n        return gated\n\n    @property\n    def requires_length(self):\n        return False\n\n\nclass ResidualBlock(tf.keras.layers.Layer):\n    def __init__(self, layer: Optional[tf.keras.layers.Layer] = None, name: Optional[str] = None, **kwargs):\n        super().__init__(name=name)\n        self.layer = layer\n\n    def call(self, inputs):\n        return inputs + self.layer(inputs)\n\n    @property\n    def requires_length(self) -> bool:\n        return False\n\n\nclass SkipConnection(ResidualBlock):\n    def __init__(self, input_size: int, activation: str = ""relu""):\n        super(SkipConnection, self).__init__(tf.keras.layers.Dense(input_size, activation=activation))\n\n\nclass TimeDistributedProjection(tf.keras.layers.Layer):\n    def __init__(self, num_outputs, name=None):\n        """"""Set up a low-order projection (embedding) by flattening the batch and time dims and matmul\n\n        TODO: Avoid where possible, Dense should work in most cases\n\n        :param name: The name for this scope\n        :param num_outputs: The number of feature maps out\n        """"""\n        super().__init__(True, name)\n        self.output_dim = num_outputs\n        self.W = None\n        self.b = None\n\n    def build(self, input_shape):\n\n        nx = int(input_shape[-1])\n        self.W = self.add_weight(""W"", [nx, self.output_dim])\n        self.b = self.add_weight(""b"", [self.output_dim], initializer=tf.constant_initializer(0.0))\n        super().build(input_shape)\n\n    def call(self, inputs):\n        """"""Low-order projection (embedding) by flattening the batch and time dims and matmul\n\n        :param inputs: The input tensor\n        :return: An output tensor having the same dims as the input, except the last which is `output_dim`\n        """"""\n        input_shape = get_shape_as_list(inputs)\n        collapse = tf.reshape(inputs, [-1, input_shape[-1]])\n        c = tf.matmul(collapse, self.W) + self.b\n        c = tf.reshape(c, input_shape[:-1] + [self.output_dim])\n        return c\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], self.output_dim\n\n    @property\n    def requires_length(self) -> bool:\n        return False\n\n\nclass SequenceSequenceAttention(tf.keras.layers.Layer):\n    def __init__(self, hsz: Optional[int] = None, pdrop: float = 0.1, name: str = None):\n        super().__init__(name=name)\n        self.hsz = hsz\n        self.dropout = tf.keras.layers.Dropout(pdrop)\n        self.attn = None\n\n    def call(self, qkvm):\n        query, key, value, mask = qkvm\n        a = self._attention(query, key, mask)\n        self.attn = a\n        a = self.dropout(a, training=TRAIN_FLAG())\n        return self._update(a, value)\n\n    def _attention(self, queries, keys, mask=None):\n        pass\n\n    def _update(self, a, value):\n        """"""Attention weights are applied for each value, but in a series of efficient matrix operations.\n\n        In the case of self-attention, the key and query (used to create the attention weights)\n        and values are all low order projections of the same input.\n\n        :param a: The attention weights [B, H, T, T]\n        :param values: The values [B, H, T, D]\n        :returns: A tensor of shape [B, H, T, D]\n        """"""\n        return tf.matmul(a, value)\n\n\nclass SeqScaledDotProductAttention(SequenceSequenceAttention):\n    def __init__(self, pdrop: float = 0.1, name: str = ""scaled_dot_product_attention"", **kwargs):\n        super().__init__(pdrop, name=name, **kwargs)\n\n    def _attention(self, query, key, mask=None):\n        """"""Scaled dot product attention, as defined in https://arxiv.org/abs/1706.03762\n\n        We apply the query to the keys to receive our weights via softmax in a series of efficient\n        matrix operations. In the case of self-attntion the key and query are all low order\n        projections of the same input.\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :return: A tensor that is (BxHxTxT)\n        """"""\n        # Check why this was set to 2 before\n        d_k = tf.shape(query)[-1]\n        scores = tf.matmul(query, key, transpose_b=True)\n        scores *= tf.math.rsqrt(tf.cast(d_k, tf.float32))\n\n        if mask is not None:\n            scores = masked_fill(scores, tf.equal(mask, 0), -1e9)\n\n        return tf.nn.softmax(scores, name=""attention_weights"")\n\n\nclass SequenceSequenceRelativeAttention(tf.keras.layers.Layer):\n    """"""This form of attention is specified in Shaw et al 2018: https://www.aclweb.org/anthology/N18-2074.pdf\n    """"""\n\n    def __init__(self, hsz: int = None, pdrop: float = 0.1, name=None, **kwargs):\n        super().__init__(name=name)\n        self.hsz = hsz\n        self.dropout = tf.keras.layers.Dropout(pdrop)\n        self.attn = None\n\n    def call(self, q_k_v_ek_ev_m):\n        query, key, value, edges_key, edges_value, mask = q_k_v_ek_ev_m\n        a = self._attention(query, key, edges_key, mask)\n        self.attn = a\n        a = self.dropout(a, training=TRAIN_FLAG())\n        return self._update(a, value, edges_value)\n\n    def _attention(self, query, key, edges_key, mask=None):\n        pass\n\n    def _update(self, a, value, edges_value):\n        """"""Attention weights are applied for each value, but in a series of efficient matrix operations.\n\n        In the case of self-attention, the key and query (used to create the attention weights)\n        and values are all low order projections of the same input.\n\n        :param a: The attention weights [B, H, T, T]\n        :param value: The values [B, H, T, D]\n        :param edge_value: The edge values [T, T, D]\n        :returns: A tensor of shape [B, H, T, D]\n        """"""\n        B, H, T, D = get_shape_as_list(value)\n        updated_values = tf.matmul(a, value)\n        # (T, BxH, T)\n        a = tf.transpose(tf.reshape(a, [B * H, T, T]), [1, 0, 2])\n        t = tf.matmul(a, edges_value)  # (T, BxH, D)\n        t = tf.transpose(t, [1, 0, 2])\n        update_edge_values = tf.reshape(t, [B, H, T, D])\n        return updated_values + update_edge_values\n\n\nclass SeqScaledDotProductRelativeAttention(SequenceSequenceRelativeAttention):\n    def __init__(self, pdrop: float = 0.1, name: str = ""scaled_dot_product_rel_attention"", **kwargs):\n        super().__init__(pdrop=pdrop, name=name, **kwargs)\n\n    def _attention(self, query, key, edges_key, mask=None):\n        """"""Scaled dot product attention, as defined in https://arxiv.org/abs/1706.03762\n\n        We apply the query to the keys to receive our weights via softmax in a series of efficient\n        matrix operations. In the case of self-attntion the key and query are all low order\n        projections of the same input.\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :param edges_key: a matrix of relative embeddings between each word in a sequence [TxTxD]\n        :return: A tensor that is (BxHxTxT)\n        """"""\n        # (., H, T, T) = (., H, T, D) x (., H, D, T)\n        B, H, T, d_k = get_shape_as_list(query)\n        scores_qk = tf.matmul(query, key, transpose_b=True)\n\n        tbhd = tf.transpose(tf.reshape(query, [B * H, T, d_k]), [1, 0, 2])\n        scores_qek = tf.matmul(tbhd, edges_key, transpose_b=True)\n        scores_qek = tf.transpose(scores_qek, [1, 0, 2])\n        scores_qek = tf.reshape(scores_qek, [B, H, T, T])\n        scores = (scores_qk + scores_qek) / math.sqrt(d_k)\n\n        if mask is not None:\n            scores = masked_fill(scores, tf.equal(mask, 0), -1e9)\n\n        return tf.nn.softmax(scores, name=""rel_attention_weights"")\n\n\nclass SeqDotProductRelativeAttention(SequenceSequenceRelativeAttention):\n    def __init__(self, pdrop: float = 0.1, name: str = ""dot_product_rel_attention"", **kwargs):\n        super().__init__(pdrop=pdrop, name=name, **kwargs)\n\n    def _attention(self, query, key, edges_key, mask=None):\n        """"""Scaled dot product attention, as defined in https://arxiv.org/abs/1706.03762\n\n        We apply the query to the keys to receive our weights via softmax in a series of efficient\n        matrix operations. In the case of self-attntion the key and query are all low order\n        projections of the same input.\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :param edges_key: a matrix of relative embeddings between each word in a sequence [TxTxD]\n        :return: A tensor that is (BxHxTxT)\n        """"""\n        # (., H, T, T) = (., H, T, D) x (., H, D, T)\n        B, H, T, d_k = get_shape_as_list(query)\n        scores_qk = tf.matmul(query, key, transpose_b=True)\n\n        tbhd = tf.transpose(tf.reshape(query, [B * H, T, d_k]), [1, 0, 2])\n        scores_qek = tf.matmul(tbhd, edges_key, transpose_b=True)\n        scores_qek = tf.transpose(scores_qek, [1, 0, 2])\n        scores_qek = tf.reshape(scores_qek, [B, H, T, T])\n        scores = scores_qk + scores_qek\n\n        if mask is not None:\n            scores = masked_fill(scores, tf.equal(mask, 0), -1e9)\n\n        return tf.nn.softmax(scores, name=""rel_attention_weights"")\n\n\nclass SeqDotProductAttention(SequenceSequenceAttention):\n    def __init__(self, pdrop: float = 0.1, name: str = ""dot_product_attention"", **kwargs):\n        super().__init__(pdrop, name=name, **kwargs)\n\n    def _attention(self, query, key, mask=None):\n        scores = tf.matmul(query, key, transpose_b=True)\n\n        if mask is not None:\n            scores = masked_fill(scores, tf.equal(mask, 0), -1e9)\n\n        return tf.nn.softmax(scores, name=""attention_weights"")\n\n\nclass MultiHeadedAttention(tf.keras.layers.Layer):\n    """"""\n    Multi-headed attention from https://arxiv.org/abs/1706.03762 via http://nlp.seas.harvard.edu/2018/04/03/attention.html\n\n    Multi-headed attention provides multiple looks of low-order projections K, Q and V using an attention function\n    (specifically `scaled_dot_product_attention` in the paper.  This allows multiple relationships to be illuminated\n    via attention on different positional and representational information from each head.\n\n    The number of heads `h` times the low-order projection dim `d_k` is equal to `d_model` (which is asserted upfront).\n    This means that each weight matrix can be simply represented as a linear transformation from `d_model` to `d_model`,\n    and partitioned into heads after the fact.\n\n    Finally, an output projection is applied which brings the output space back to `d_model`, in preparation for the\n    sub-sequent `FFN` sub-layer.\n\n    There are 3 uses of multi-head attention in the Transformer.\n    For encoder-decoder layers, the queries come from the previous decoder layer, and the memory keys come from\n    the encoder.  For encoder layers, the K, Q and V all come from the output of the previous layer of the encoder.\n    And for self-attention in the decoder, K, Q and V all come from the decoder, but here it is masked to prevent using\n    future values\n    """"""\n\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        dropout: float = 0.1,\n        scale: bool = False,\n        d_k: Optional[int] = None,\n        name: str = None,\n    ):\n        """"""Constructor for multi-headed attention\n\n        :param h: The number of heads\n        :param d_model: The model hidden size\n        :param dropout (``float``): The amount of dropout to use\n        :param attn_fn: A function to apply attention, defaults to SDP\n        """"""\n        super().__init__(name=name)\n\n        if d_k is None:\n            self.d_k = d_model // num_heads\n            if d_model % num_heads != 0:\n                raise Exception(f""d_model ({d_model}) must be evenly divisible by num_heads ({num_heads})"")\n        else:\n            self.d_k = d_k\n\n        self.h = num_heads\n        self.w_Q = tf.keras.layers.Dense(units=self.d_k * self.h, name=""query_projection"")\n        self.w_K = tf.keras.layers.Dense(units=self.d_k * self.h, name=""key_projection"")\n        self.w_V = tf.keras.layers.Dense(units=self.d_k * self.h, name=""value_projection"")\n        self.w_O = tf.keras.layers.Dense(units=self.d_k * self.h, name=""output_projection"")\n        if scale:\n            self.attn_fn = SeqScaledDotProductAttention(dropout)\n        else:\n            self.attn_fn = SeqDotProductAttention(dropout)\n        self.attn = None\n\n    def call(self, qkvm):\n        query, key, value, mask = qkvm\n        batchsz = get_shape_as_list(query)[0]\n\n        # (B, T, H, D) -> (B, H, T, D)\n        query = tf.transpose(tf.reshape(self.w_Q(query), [batchsz, -1, self.h, self.d_k]), [0, 2, 1, 3])\n        key = tf.transpose(tf.reshape(self.w_K(key), [batchsz, -1, self.h, self.d_k]), [0, 2, 1, 3])\n        value = tf.transpose(tf.reshape(self.w_V(value), [batchsz, -1, self.h, self.d_k]), [0, 2, 1, 3])\n        x = self.attn_fn((query, key, value, mask))\n        self.attn = self.attn_fn.attn\n\n        # (B, H, T, D) -> (B, T, H, D) -> (B, T, H*D)\n        x = tf.transpose(x, [0, 2, 1, 3])\n        x = tf.reshape(x, [batchsz, -1, self.h * self.d_k])\n        return self.w_O(x)\n\n\nclass MultiHeadedRelativeAttention(tf.keras.layers.Layer):\n    """"""\n    Multi-headed relative attention from Shaw et al 2018 (https://www.aclweb.org/anthology/N18-2074.pdf)\n\n    This method follows the same approach of MultiHeadedAttention, but it computes Relative Position Representations (RPR)\n    which are used as part of the attention computations.  To facilitate this, the model has its own internal\n    embeddings lookup table, and it has an updated computation for both the attention weights and the application\n    of those weights to follow them.\n\n    """"""\n\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        rpr_k: int,\n        dropout: float = 0.1,\n        scale: bool = False,\n        d_k: Optional[int] = None,\n        name=None,\n    ):\n        """"""Constructor for multi-headed attention\n\n        :param h: The number of heads\n        :param d_model: The model hidden size\n        :param dropout (``float``): The amount of dropout to use\n        :param scale: Should we scale the dot product attention\n        :param d_k: The low-order project per head.  This is normally `d_model // num_heads` unless set explicitly\n        """"""\n        super().__init__()\n\n        if d_k is None:\n            self.d_k = d_model // num_heads\n            if d_model % num_heads != 0:\n                raise Exception(f""d_model ({d_model}) must be evenly divisible by num_heads ({num_heads})"")\n        else:\n            self.d_k = d_k\n\n        self.rpr_k = rpr_k\n        self.rpr_key = tf.keras.layers.Embedding(2 * rpr_k + 1, self.d_k)\n        self.rpr_value = tf.keras.layers.Embedding(2 * rpr_k + 1, self.d_k)\n\n        self.h = num_heads\n        self.w_Q = tf.keras.layers.Dense(units=self.d_k * self.h, name=""query_projection"")\n        self.w_K = tf.keras.layers.Dense(units=self.d_k * self.h, name=""key_projection"")\n        self.w_V = tf.keras.layers.Dense(units=self.d_k * self.h, name=""value_projection"")\n        self.w_O = tf.keras.layers.Dense(units=self.d_k * self.h, name=""output_projection"")\n        if scale:\n            self.attn_fn = SeqScaledDotProductRelativeAttention(dropout)\n        else:\n            self.attn_fn = SeqDotProductRelativeAttention(dropout)\n        self.attn = None\n\n    def make_rpr(self, seq_len: int):\n        """"""Create a matrix shifted by self.rpr_k and bounded between 0 and 2*self.rpr_k to provide 0-based indexing for embedding\n        """"""\n        seq = tf.range(seq_len)\n        window_len = 2 * self.rpr_k\n        edges = tf.reshape(seq, [1, -1]) - tf.reshape(seq, [-1, 1]) + self.rpr_k\n        edges = tf.clip_by_value(edges, 0, window_len)\n        return self.rpr_key(edges), self.rpr_value(edges)\n\n    def call(self, qkvm):\n        """"""Low-order projections of query, key and value into multiple heads, then attention application and dropout\n\n        :param query: a query for alignment. Can come from self in case of self-attn or decoder in case of E/D\n        :param key: a set of keys from encoder or self\n        :param value: a set of values from encoder or self\n        :param mask: masking (for destination) to prevent seeing what we shouldnt\n        :return: Multi-head attention output, result of attention application to sequence (B, T, d_model)\n        """"""\n        query, key, value, mask = qkvm\n        shp = get_shape_as_list(query)\n        batchsz = shp[0]\n        seq_len = shp[1]\n\n        # (B, T, H, D) -> (B, H, T, D)\n        query = tf.transpose(tf.reshape(self.w_Q(query), [batchsz, -1, self.h, self.d_k]), [0, 2, 1, 3])\n        key = tf.transpose(tf.reshape(self.w_K(key), [batchsz, -1, self.h, self.d_k]), [0, 2, 1, 3])\n        value = tf.transpose(tf.reshape(self.w_V(value), [batchsz, -1, self.h, self.d_k]), [0, 2, 1, 3])\n\n        rpr_key, rpr_value = self.make_rpr(seq_len)\n        x = self.attn_fn((query, key, value, rpr_key, rpr_value, mask))\n        self.attn = self.attn_fn.attn\n        # (B, H, T, D) -> (B, T, H, D) -> (B, T, H*D)\n        x = tf.transpose(x, [0, 2, 1, 3])\n        x = tf.reshape(x, [batchsz, -1, self.h * self.d_k])\n        return self.w_O(x)\n\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: float,\n        scale: bool = True,\n        activation_type: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[int] = None,\n        ffn_pdrop: Optional[float] = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        name: Optional[str] = None\n    ):\n        super().__init__(name=name)\n        self.layer_norms_after = layer_norms_after\n        self.d_model = d_model\n        self.d_ff = d_ff if d_ff is not None else 4 * d_model\n        if rpr_k is not None:\n            self.self_attn = MultiHeadedRelativeAttention(num_heads, d_model, rpr_k, pdrop, scale, d_k=d_k)\n        else:\n            self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale=scale, d_k=d_k)\n\n        self.ffn = FFN(d_model, activation_type, d_ff, ffn_pdrop, name=""ffn"")\n        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n        self.dropout = tf.keras.layers.Dropout(pdrop)\n\n    def call(self, inputs):\n        """"""\n        :param inputs: `(x, mask)`\n        :return: The output tensor\n        """"""\n        x, mask = inputs\n        if not self.layer_norms_after:\n            x = self.ln1(x)\n        h = self.self_attn((x, x, x, mask))\n        x = x + self.dropout(h, TRAIN_FLAG())\n        x = self.ln2(x)\n        x = x + self.dropout(self.ffn(x), TRAIN_FLAG())\n        if self.layer_norms_after:\n            x = self.ln1(x)\n        return x\n\n\nclass TransformerDecoder(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        d_model: int,\n        num_heads: int,\n        pdrop: float,\n        scale: bool = True,\n        activation_type: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[int] = None,\n        ffn_pdrop: Optional[float] = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        name: str = None\n    ):\n        super().__init__(name=name)\n        self.d_model = d_model\n        self.layer_norms_after = layer_norms_after\n        self.d_ff = d_ff if d_ff is not None else 4 * d_model\n        if rpr_k is not None:\n            self.self_attn = MultiHeadedRelativeAttention(num_heads, d_model, rpr_k, pdrop, scale, d_k=d_k, name=""self_attention"")\n            self.src_attn = MultiHeadedRelativeAttention(num_heads, d_model, rpr_k, pdrop, scale, d_k=d_k, name=""src_attention"")\n\n        else:\n            self.self_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale, d_k=d_k, name=""self_attention"")\n            self.src_attn = MultiHeadedAttention(num_heads, d_model, pdrop, scale, d_k=d_k, name=""src_attention"")\n\n        self.ffn = FFN(d_model, activation_type, d_ff, pdrop=ffn_pdrop, name=""ffn"")\n        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n        self.ln3 = tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n        self.dropout = tf.keras.layers.Dropout(pdrop)\n\n    def call(self, inputs):\n        x, memory, src_mask, tgt_mask = inputs\n        if not self.layer_norms_after:\n            x = self.ln1(x)\n        x = x + self.dropout(self.self_attn((x, x, x, tgt_mask)), TRAIN_FLAG())\n\n        x = self.ln2(x)\n        x = x + self.dropout(self.src_attn((x, memory, memory, src_mask)), TRAIN_FLAG())\n\n        x = self.ln3(x)\n        x = x + self.dropout(self.ffn(x), TRAIN_FLAG())\n        if self.layer_norms_after:\n            x = self.ln1(x)\n        return x\n\n\nclass TransformerEncoderStack(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: float,\n        scale: bool = True,\n        layers: int = 1,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        ffn_pdrop: Optional[float] = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        name=None,\n        **kwargs,\n    ):\n\n        super().__init__(name=name)\n        self.encoders = []\n        self.ln = tf.identity if layer_norms_after else tf.keras.layers.LayerNormalization(epsilon=layer_norm_eps)\n\n        if not is_sequence(rpr_k):\n            rpr_k = [rpr_k] * layers\n\n        for i in range(layers):\n            self.encoders.append(\n                TransformerEncoder(\n                    num_heads, d_model, pdrop, scale, activation, d_ff, d_k,\n                    rpr_k=rpr_k[i], ffn_pdrop=ffn_pdrop,\n                    layer_norms_after=layer_norms_after, layer_norm_eps=layer_norm_eps,\n                    name=name,\n                )\n            )\n\n    def call(self, inputs):\n        x, mask = inputs\n        for layer in self.encoders:\n            x = layer((x, mask))\n        return self.ln(x)\n\n\nclass TransformerEncoderStackWithLengths(TransformerEncoderStack):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: bool,\n        scale: bool = True,\n        layers: int = 1,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        name: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(num_heads, d_model, pdrop, scale, layers, activation, d_ff, d_k, rpr_k, layer_norms_after, layer_norm_eps, name=name)\n        self.proj = WithDropout(tf.keras.layers.Dense(d_model), pdrop)\n\n    def call(self, inputs):\n        x, lengths = inputs\n        x = self.proj(x)\n        max_seqlen = get_shape_as_list(x)[1]\n        mask = tf.expand_dims(tf.expand_dims(tf.sequence_mask(lengths, max_seqlen, dtype=tf.float32), 1), 1)\n        return super().call((x, mask))\n\n\nclass TransformerEncoderStackWithTimeMask(TransformerEncoderStack):\n    def __init__(\n        self,\n        num_heads: int,\n        d_model: int,\n        pdrop: bool,\n        scale: bool = True,\n        layers: int = 1,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        name: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(num_heads, d_model, pdrop, scale, layers, activation, d_ff, d_k, rpr_k, layer_norms_after, layer_norm_eps, name=name)\n        self.proj = WithDropout(tf.keras.layers.Dense(d_model), pdrop)\n\n    def call(self, inputs):\n        x, _ = inputs\n        x = self.proj(x)\n        max_seqlen = get_shape_as_list(x)[1]\n        mask = subsequent_mask(max_seqlen)\n        return super().call((x, mask))\n\n\nclass TransformerDecoderStack(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        d_model: int,\n        num_heads: int,\n        pdrop: float,\n        scale: bool = True,\n        layers: int = 1,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        d_k: Optional[int] = None,\n        rpr_k: Optional[Union[int, List[int]]] = None,\n        ffn_pdrop: float = 0.0,\n        layer_norms_after: bool = False,\n        layer_norm_eps: float = 1.0e-6,\n        name: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(name=name)\n        self.decoders = []\n        self.ln = tf.identity if layer_norms_after else tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        for i in range(layers):\n            self.decoders.append(\n                TransformerDecoder(d_model, num_heads, pdrop, scale, activation, d_ff, ffn_pdrop=ffn_pdrop)\n            )\n\n    def call(self, inputs):\n        x, memory, src_mask, tgt_mask = inputs\n        for layer in self.decoders:\n            x = layer((x, memory, src_mask, tgt_mask))\n        return self.ln(x)\n\n\nclass FFN(tf.keras.layers.Layer):\n    """"""\n    FFN from https://arxiv.org/abs/1706.03762\n\n    The paper does not specify any dropout in this layer, but subsequent implementations (like XLM) do use dropout.\n    """"""\n\n    def __init__(\n        self,\n        d_model: int,\n        activation: str = ""relu"",\n        d_ff: Optional[int] = None,\n        pdrop: float = 0.0,\n        name: Optional[int] = None,\n    ):\n        """"""Constructor, takes in model size (which is the external currency of each block) and the feed-forward size\n\n        :param d_model: The model size.  This is the size passed through each block\n        :param d_ff: The feed-forward internal size, which is typical 4x larger, used internally\n        :param pdrop: The probability of dropping output\n        """"""\n        super().__init__(name=name)\n        if d_ff is None:\n            d_ff = 4 * d_model\n        self.expansion = tf.keras.layers.Dense(d_ff)\n        self.squeeze = tf.keras.layers.Dense(d_model)\n        self.dropout = tf.keras.layers.Dropout(pdrop)\n        self.act = get_activation(activation)\n\n    def call(self, inputs):\n        return self.squeeze(self.dropout(self.act(self.expansion(inputs))))\n\n\nclass TaggerGreedyDecoder(tf.keras.layers.Layer):\n    def __init__(self, num_tags: int, constraint_mask: Optional[Tuple[Any, Any]] = None, name: Optional[str] = None):\n        """"""Initialize the object.\n        :param num_tags: int, The number of tags in your output (emission size)\n        :param constraint_mask: Tuple[np.ndarray, np.ndarray], Constraints on the transitions [1, N, N]\n        :param name: str, Optional name, defaults to `None`\n        """"""\n        super().__init__(name=name)\n        self.num_tags = num_tags\n        self.A = None\n        self.inv_mask = None\n        if constraint_mask is not None:\n            _, inv_mask = constraint_mask\n            self.inv_mask = inv_mask * -1e4\n\n    def build(self, input_shape):\n        self.A = self.add_weight(\n            ""transitions"",\n            shape=(self.num_tags, self.num_tags),\n            dtype=tf.float32,\n            initializer=tf.zeros_initializer(),\n            trainable=False\n        )\n        if self.inv_mask is not None:\n            self.inv_mask = self.add_weight(\n                ""inverse_constraint_mask"",\n                shape=(self.num_tags, self.num_tags),\n                dtype=tf.float32,\n                initializer=tf.constant_initializer(self.inv_mask),\n                trainable=False\n            )\n\n    @property\n    def transitions(self):\n        if self.inv_mask is not None:\n            return tf.nn.log_softmax(self.A + self.inv_mask)\n        return self.A\n\n    def neg_log_loss(self, unary, tags, lengths):\n\n        lengths = tf.cast(lengths, tf.int32)\n        max_length = tf.reduce_max(lengths)\n        tags = tags[:, :max_length]\n\n        mask = tf.sequence_mask(lengths, max_length)\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tags, logits=unary)\n        cross_entropy *= tf.cast(mask, tf.float32)\n        cross_entropy = tf.reduce_sum(cross_entropy, axis=1)\n        return tf.reduce_mean(cross_entropy, name=""loss"")\n\n    def call(self, inputs, training=False, mask=None):\n\n        unary, lengths = inputs\n\n        if self.inv_mask is not None:\n            bsz = tf.shape(unary)[0]\n            lsz = self.num_tags\n            np_gos = np.full((1, 1, lsz), -1e4, dtype=np.float32)\n            np_gos[:, :, Offsets.GO] = 0\n            gos = tf.constant(np_gos)\n            start = tf.tile(gos, [bsz, 1, 1])\n            probv = tf.concat([start, unary], axis=1)\n            viterbi, path_scores = crf_decode(probv, self.transitions, lengths + 1)\n            return tf.identity(viterbi[:, 1:], name=""best""), path_scores\n        else:\n            return tf.argmax(unary, 2, name=""best""), None\n\n\nclass CRF(tf.keras.layers.Layer):\n    def __init__(self, num_tags: int, constraint_mask: Optional[Tuple[Any, Any]] = None, name: Optional[str] = None):\n        """"""Initialize the object.\n        :param num_tags: int, The number of tags in your output (emission size)\n        :param constraint_mask: Tuple[np.ndarray, np.ndarray], Constraints on the transitions [1, N, N]\n        :param name: str, Optional name, defaults to `None`\n        """"""\n        super().__init__(name=name)\n\n        self.num_tags = num_tags\n        self.A = None\n        self.mask = None\n        self.inv_mask = None\n        if constraint_mask is not None:\n            mask, inv_mask = constraint_mask\n            self.mask = mask\n            self.inv_mask = inv_mask * -1e4\n\n    def build(self, input_shape):\n        self.A = self.add_weight(\n            ""transitions"",\n            shape=(self.num_tags, self.num_tags),\n            dtype=tf.float32,\n            initializer=tf.zeros_initializer()\n        )\n        if self.mask is not None:\n            self.mask = self.add_weight(\n                ""constraint_mask"",\n                shape=(self.num_tags, self.num_tags),\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.constant_initializer(self.mask)\n            )\n        if self.inv_mask is not None:\n            self.inv_mask = self.add_weight(\n                ""inverse_constraint_mask"",\n                shape=(self.num_tags, self.num_tags),\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.constant_initializer(self.inv_mask)\n            )\n\n    @property\n    def transitions(self):\n        if self.inv_mask is not None:\n            return (self.A * self.mask) + self.inv_mask\n        return self.A\n\n    def score_sentence(self, unary, tags, lengths):\n        """"""Score a batch of sentences.\n\n        :param unary: torch.FloatTensor: [B, T, N]\n        :param tags: torch.LongTensor: [B, T]\n        :param lengths: torch.LongTensor: [B]\n\n        :return: torch.FloatTensor: [B]\n        """"""\n        return crf_sequence_score(unary, tf.cast(tags, tf.int32), tf.cast(lengths, tf.int32), self.transitions)\n\n    def call(self, inputs, training=False):\n\n        unary, lengths = inputs\n        if training:\n            return crf_log_norm(unary, lengths, self.transitions)\n        else:\n            return self.decode(unary, lengths)\n\n    def decode(self, unary, lengths):\n        """"""Do Viterbi decode on a batch.\n\n        :param unary: torch.FloatTensor: [T, B, N] or [B, T, N]\n        :param lengths: torch.LongTensor: [B]\n\n        :return: List[torch.LongTensor]: [B] the paths\n        :return: torch.FloatTensor: [B] the path score\n        """"""\n        bsz = tf.shape(unary)[0]\n        lsz = self.num_tags\n        np_gos = np.full((1, 1, lsz), -1e4, dtype=np.float32)\n        np_gos[:, :, Offsets.GO] = 0\n        gos = tf.constant(np_gos)\n\n        start = tf.tile(gos, [bsz, 1, 1])\n        start = tf.nn.log_softmax(start, axis=-1)\n\n        probv = tf.concat([start, unary], axis=1)\n\n        viterbi, path_scores = crf_decode(probv, self.transitions, lengths + 1)\n        return tf.identity(viterbi[:, 1:], name=""best""), path_scores\n\n    def neg_log_loss(self, unary, tags, lengths):\n        """"""Neg Log Loss with a Batched CRF.\n\n        :param unary: unary outputs of length `[B, S, N]`\n        :param tags: tag truth values `[B, T]`\n        :param lengths: tensor of shape `[B]`\n\n        :return: Tensor of shape `[B]`\n        """"""\n        lengths = tf.cast(lengths, tf.int32)\n        max_length = tf.reduce_max(lengths)\n        fwd_score = self((unary, lengths), training=True)\n        tags = tags[:, :max_length]\n        gold_score = self.score_sentence(unary, tags, lengths)\n        log_likelihood = gold_score - fwd_score\n        return -tf.reduce_mean(log_likelihood)\n\n\nclass MeanPool1D(tf.keras.layers.Layer):\n    def __init__(\n        self,\n        dsz: int,\n        trainable: bool = False,\n        name: Optional[str] = None,\n        dtype: int = tf.float32,\n        batch_first: bool = True,\n        *args,\n        **kwargs,\n    ):\n        """"""This is a layers the calculates the mean pooling in a length awareway.\n\n           This was originally a wrapper around tf.keras.layers.GlobalAveragePooling1D()\n           but that had problems because the mask didn\'t work then the dimension we\n           are pooling over was variable length.\n\n           looking here https://github.com/tensorflow/tensorflow/blob/1cf0898dd4331baf93fe77205550f2c2e6c90ee5/tensorflow/python/keras/layers/pooling.py#L639\n\n           We can see that the input shape is being gotten as a list where for the\n           value of `input_shape[step_axis]` is `None` instead of getting the shape\n           via `tf.shape`. This means that when they do the reshape the\n           broadcast_shape is `[-1, None, 1]` which causes an error.\n        """"""\n        super().__init__(trainable, name, dtype)\n        self.output_dim = dsz\n        self.reduction_dim = 1 if batch_first else 0\n\n    def call(self, inputs):\n        tensor, lengths = tensor_and_lengths(inputs)\n        # Regardless of whether the input is batch first or time first the result of the\n        # sum is `[B, H]` so the lengths (which is `[B]`) should always be expanded with\n        # `-1` to `[B, -1]` so that is broadcasts.\n        return tf.reduce_sum(tensor, self.reduction_dim) / tf.cast(tf.expand_dims(lengths, -1), tf.float32)\n\n    @property\n    def requires_length(self):\n        return True\n\n\nclass TagSequenceModel(tf.keras.Model):\n    def __init__(\n        self,\n        nc: int,\n        embeddings: tf.keras.layers.Layer,\n        transducer: tf.keras.layers.Layer,\n        decoder: Optional[tf.keras.layers.Layer] = None,\n        name: str = None,\n    ):\n        super().__init__(name=name)\n        if isinstance(embeddings, dict):\n            self.embed_model = EmbeddingsStack(embeddings)\n        else:\n            assert isinstance(embeddings, EmbeddingsStack)\n            self.embed_model = embeddings\n        self.path_scores = None\n        self.transducer_model = transducer\n        self.proj_layer = TimeDistributedProjection(nc)\n        decoder_model = CRF(nc) if decoder is None else decoder\n        self.decoder_model = decoder_model\n\n    def transduce(self, inputs):\n        lengths = inputs.get(""lengths"")\n\n        embedded = self.embed_model(inputs)\n        embedded = (embedded, lengths)\n        transduced = self.proj_layer(self.transducer_model(embedded))\n        return transduced\n\n    def decode(self, transduced, lengths):\n        path, self.path_scores = self.decoder_model((transduced, lengths))\n        return path\n\n    def call(self, inputs, training=None):\n        transduced = self.transduce(inputs)\n        return self.decode(transduced, inputs.get(""lengths""))\n\n    def neg_log_loss(self, unary, tags, lengths):\n        return self.decoder_model.neg_log_loss(unary, tags, lengths)\n\n\nclass LangSequenceModel(tf.keras.Model):\n    def __init__(\n        self,\n        nc: int,\n        embeddings: tf.keras.layers.Layer,\n        transducer: tf.keras.layers.Layer,\n        decoder: Optional[tf.keras.layers.Layer] = None,\n        name: str = None,\n    ):\n        super().__init__(name=name)\n        self.embed_model = embeddings\n        self.transducer_model = transducer\n        if hasattr(transducer, ""requires_state"") and transducer.requires_state:\n            self._call = self._call_with_state\n            self.requires_state = True\n        else:\n            self._call = self._call_without_state\n            self.requires_state = False\n        self.output_layer = TimeDistributedProjection(nc)\n        self.decoder_model = decoder\n\n    def call(self, inputs):\n        return self._call(inputs)\n\n    def _call_with_state(self, inputs):\n\n        h = inputs.get(""h"")\n\n        embedded = self.embed_model(inputs)\n        transduced, hidden = self.transducer_model((embedded, h))\n        transduced = self.output_layer(transduced)\n        return transduced, hidden\n\n    def _call_without_state(self, inputs):\n        embedded = self.embed_model(inputs)\n        transduced = self.transducer_model((embedded, None))\n        transduced = self.output_layer(transduced)\n        return transduced, None\n\n\nclass EmbedPoolStackModel(tf.keras.Model):\n    def __init__(\n        self,\n        nc: int,\n        embeddings: tf.keras.layers.Layer,\n        pool_model: tf.keras.layers.Layer,\n        stack_model: Optional[tf.keras.layers.Layer] = None,\n        output_model: Optional[tf.keras.layers.Layer] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__(name=name)\n        self.embed_model = embeddings\n        self.pool_model = pool_model\n        self.stack_model = stack_model\n        self.output_layer = tf.keras.layers.Dense(nc) if output_model is None else output_model\n\n    def call(self, inputs):\n        lengths = inputs.get(""lengths"")\n        embedded = self.embed_model(inputs)\n        embedded = (embedded, lengths)\n        pooled = self.pool_model(embedded)\n        stacked = self.stack_model(pooled) if self.stack_model is not None else pooled\n        return self.output_layer(stacked)\n\n\nclass FineTuneModel(tf.keras.Model):\n    def __init__(self, nc: int, embeddings: tf.keras.layers.Layer, stack_model: Optional[tf.keras.layers.Layer] = None):\n        super().__init__()\n        self.finetuned = embeddings\n        self.stack_model = stack_model\n        self.output_layer = tf.keras.layers.Dense(nc)\n\n    def call(self, inputs):\n        base_layers = self.finetuned(inputs)\n        stacked = self.stack_model(base_layers) if self.stack_model is not None else base_layers\n        return self.output_layer(stacked)\n\n\nclass CompositeModel(tf.keras.Model):\n    def __init__(self, models):\n        super().__init__()\n        self.models = models\n        self._requires_length = any(getattr(m, ""requires_length"", False) for m in self.models)\n        # self.output_dim = sum(m.output_dim for m in self.models)\n\n    def call(self, inputs, training=None, mask=None):\n        inputs, lengths = tensor_and_lengths(inputs)\n        pooled = []\n        for m in self.models:\n            if getattr(m, ""requires_length"", False):\n                pooled.append(m((inputs, lengths)))\n            else:\n                pooled.append(m(inputs))\n        return tf.concat(pooled, -1)\n\n    @property\n    def requires_length(self):\n        return self._requires_length\n\n\ndef highway_conns(inputs, wsz_all, n):\n    """"""Produce one or more highway connection layers\n\n    :param inputs: The sub-graph input\n    :param wsz_all: The number of units\n    :param n: How many layers of gating\n    :return: graph output\n    """"""\n    x = inputs\n    for i in range(n):\n        x = Highway(wsz_all)(x)\n    return x\n\n\ndef skip_conns(inputs, wsz_all, n, activation_fn=""relu""):\n    x = inputs\n    for i in range(n):\n        x = SkipConnection(wsz_all, activation_fn)(x)\n    return x\n\n\ndef parallel_conv(input_, filtsz, dsz, motsz, activation_fn=""relu""):\n    return ParallelConv(dsz, motsz, filtsz, activation_fn)(input_)\n\n\ndef char_word_conv_embeddings(\n    char_vec, filtsz, char_dsz, nfeats, activation_fn=tf.nn.tanh, gating=skip_conns, num_gates=1\n):\n    """"""This wrapper takes in a character vector as input and performs parallel convolutions on it, followed by a\n    pooling operation and optional residual or highway connections\n\n    :param char_vec: The vector input\n    :param filtsz: A list or scalar containing filter sizes for each parallel filter\n    :param char_dsz: The character dimension size\n    :param nfeats: A list or scalar of the number of pooling units for each filter operation\n    :param activation_fn: A function for activation (`tf.nn.tanh` etc)\n    :param gating: A gating function to apply to the output\n    :param num_gates: The number of gates to apply\n    :return: The embedding output, the full number of units\n    """"""\n    if isinstance(nfeats, (list, tuple)):\n        wsz_all = np.sum(nfeats)\n    else:\n        wsz_all = len(filtsz) * nfeats\n    combine = parallel_conv(char_vec, filtsz, char_dsz, nfeats, activation_fn)\n    joined = gating(combine, wsz_all, num_gates)\n    return joined, wsz_all\n\n\ndef create_session():\n    """"""This function protects against TF allocating all the memory\n\n    Some combination of cuDNN 7.6 with CUDA 10 on TF 1.13 with RTX cards\n    allocate additional memory which isnt available since TF by default\n    hogs it all.\n\n\n    This also provides an abstraction that can be extended later to offer\n    more config params that raw `tf.compat.v1.Session()` calls dont\n\n    :return: A `tf.compat.v1.Session`\n    """"""\n    config = tf.compat.v1.ConfigProto()\n    config.gpu_options.allow_growth = True\n    return tf.compat.v1.Session(config=config)\n\n\ndef reload_checkpoint(sess: tf.compat.v1.Session, checkpoint: str, blocks_to_skip: List[str] = None):\n    """"""\n    Get the intersection of all non-output layers and declared vars in this graph and restore them\n    :param sess: A tensorflow session to restore from\n    :param checkpoint: checkpoint to read from\n    :param blocks_to_skip: which variables of the graph to skip on reload\n    :return: None\n    """"""\n    if not blocks_to_skip:\n        blocks_to_skip = [\'OptimizeLoss\', \'output/\']\n    latest = tf.train.latest_checkpoint(checkpoint)\n    if not latest:\n        latest = checkpoint\n    LOGGER.info(""Reloading %s"", latest)\n    model_vars = set([t[0] for t in tf.train.list_variables(latest)])\n    g = tf.compat.v1.get_collection_ref(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n    for block in blocks_to_skip:\n        g = [v for v in g if not v.op.name.startswith(block)]\n    g = [v for v in g if v.op.name in model_vars]\n    LOGGER.info(""Restoring %s"", g)\n    saver = tf.compat.v1.train.Saver(g)\n    saver.restore(sess, latest)\n\n\ndef tf_device_wrapper(func):\n    @wraps(func)\n    def with_device(*args, **kwargs):\n        device = kwargs.get(""device"", ""default"")\n        if device == ""cpu"" and ""sess"" not in kwargs:\n            g = tf.compat.v1.Graph()\n            sess = tf.compat.v1.Session(\n                graph=g, config=tf.compat.v1.ConfigProto(allow_soft_placement=True, device_count={""CPU"": 1, ""GPU"": 0})\n            )\n            kwargs[""sess""] = sess\n            return func(*args, **kwargs)\n        return func(*args, **kwargs)\n\n    return with_device\n\n\nclass VectorSequenceAttention(tf.keras.layers.Layer):\n    def __init__(self, hsz):\n        super().__init__()\n        self.hsz = hsz\n        self.W_c = tf.keras.layers.Dense(hsz, use_bias=False)\n\n    def call(self, qkvm):\n        query_t, keys_bth, values_bth, keys_mask = qkvm\n        # Output(t) = B x H x 1\n        # Keys = B x T x H\n        # a = B x T x 1\n        a = self._attention(query_t, keys_bth, keys_mask)\n        attended = self._update(a, query_t, values_bth)\n\n        return attended\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        pass\n\n    def _update(self, a, query_t, values_bth):\n        # a = B x T\n        # Want to apply over context, scaled by a\n        # (B x 1 x T) (B x T x H) = (B x 1 x H)\n        B, H = get_shape_as_list(a)\n        a = tf.reshape(a, [B, 1, H])\n        c_t = tf.squeeze(a @ values_bth, 1)\n\n        attended = tf.concat([c_t, query_t], -1)\n        attended = tf.nn.tanh(self.W_c(attended))\n        return attended\n\n\nclass LuongDotProductAttention(VectorSequenceAttention):\n    def __init__(self, hsz: int):\n        super().__init__(hsz)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        a = keys_bth @ tf.expand_dims(query_t, 2)\n        a = tf.squeeze(a, -1)\n        if keys_mask is not None:\n            masked_fill(a, tf.equal(keys_mask, 0), -1e9)\n        a = tf.nn.softmax(a, axis=-1)\n        return a\n\n\nclass ScaledDotProductAttention(VectorSequenceAttention):\n    def __init__(self, hsz: int):\n        super().__init__(hsz)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        a = keys_bth @ tf.expand_dims(query_t, 2)\n        a = a / math.sqrt(self.hsz)\n        a = tf.squeeze(a, -1)\n        if keys_mask is not None:\n            masked_fill(a, tf.equal(keys_mask, 0), -1e9)\n        a = tf.nn.softmax(a, axis=-1)\n        return a\n\n\nclass LuongGeneralAttention(VectorSequenceAttention):\n    def __init__(self, hsz: int):\n        super().__init__(hsz)\n        self.W_a = tf.keras.layers.Dense(self.hsz, use_bias=False)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        a = keys_bth @ tf.expand_dims(self.W_a(query_t), 2)\n        a = tf.squeeze(a, -1)\n        if keys_mask is not None:\n            masked_fill(a, tf.equal(keys_mask, 0), -1e9)\n        a = tf.nn.softmax(a, axis=-1)\n        return a\n\n\nclass BahdanauAttention(VectorSequenceAttention):\n    def __init__(self, hsz: int):\n        super().__init__(hsz)\n        self.hsz = hsz\n        self.W_a = tf.keras.layers.Dense(self.hsz, use_bias=False)\n        self.E_a = tf.keras.layers.Dense(self.hsz, use_bias=False)\n        self.v = tf.keras.layers.Dense(1, use_bias=False)\n\n    def _attention(self, query_t, keys_bth, keys_mask):\n        B, T, H = get_shape_as_list(keys_bth)\n        q = tf.reshape(self.W_a(query_t), [B, 1, H])\n        u = self.E_a(keys_bth)\n\n        z = tf.nn.tanh(q + u)\n        a = tf.squeeze(self.v(z), -1)\n\n        if keys_mask is not None:\n            masked_fill(a, tf.equal(keys_mask, 0), -1e9)\n        a = tf.nn.softmax(a, axis=-1)\n        return a\n\n    def _update(self, a, query_t, values_bth):\n        # a = B x T\n        # Want to apply over context, scaled by a\n        # (B x 1 x T) (B x T x H) = (B x 1 x H) -> (B x H)\n        # context_vector shape after sum == (batch_size, hidden_size)\n\n        B, T_k = get_shape_as_list(a)\n        a = tf.reshape(a, [B, 1, T_k])\n        c_t = tf.squeeze(a @ values_bth, 1)\n        attended = tf.concat([c_t, query_t], -1)\n        attended = self.W_c(attended)\n        return attended\n\n\ndef subsequent_mask(size: int):\n    b = tf.compat.v1.matrix_band_part(tf.ones([size, size]), -1, 0)\n    m = tf.reshape(b, [1, 1, size, size])\n    return m\n\n\ndef gnmt_length_penalty(lengths, alpha=0.8):\n    """"""Calculate a length penalty from https://arxiv.org/pdf/1609.08144.pdf\n\n    The paper states the penalty as (5 + |Y|)^a / (5 + 1)^a. This is implemented\n    as ((5 + |Y|) / 6)^a for a (very) tiny performance boost\n\n    :param lengths: `np.array`: [B, K] The lengths of the beams.\n    :param alpha: `float`: A hyperparameter. See Table 2 for a search on this\n        parameter.\n\n    :returns:\n        `torch.FloatTensor`: [B, K, 1] The penalties.\n    """"""\n    penalty = tf.constant(np.power(((5.0 + lengths) / 6.0), alpha))\n    return tf.expand_dims(penalty, -1)\n\n\ndef no_length_penalty(lengths):\n    """"""A dummy function that returns a no penalty (1).""""""\n    return tf.expand_dims(np.ones_like(lengths), -1)\n\n\ndef repeat_batch(t, K, dim=0):\n    """"""Repeat a tensor while keeping the concept of a batch.\n\n    :param t: `torch.Tensor`: The tensor to repeat.\n    :param K: `int`: The number of times to repeat the tensor.\n    :param dim: `int`: The dimension to repeat in. This should be the\n        batch dimension.\n\n    :returns: `torch.Tensor`: The repeated tensor. The new shape will be\n        batch size * K at dim, the rest of the shapes will be the same.\n\n    Example::\n\n        >>> a = tf.constant(np.arange(10).view(2, -1))\n        >>> a\n\ttensor([[0, 1, 2, 3, 4],\n\t\t[5, 6, 7, 8, 9]])\n\t>>> repeat_batch(a, 2)\n\ttensor([[0, 1, 2, 3, 4],\n\t\t[0, 1, 2, 3, 4],\n\t\t[5, 6, 7, 8, 9],\n\t\t[5, 6, 7, 8, 9]])\n    """"""\n    shape = get_shape_as_list(t)\n    tiling = [1] * (len(shape) + 1)\n    tiling[dim + 1] = K\n    tiled = tf.tile(tf.expand_dims(t, dim + 1), tiling)\n    old_bsz = shape[dim]\n    new_bsz = old_bsz * K\n    new_shape = list(shape[:dim]) + [new_bsz] + list(shape[dim + 1 :])\n    return tf.reshape(tiled, new_shape)\n\n\ndef update_lengths(lengths, eoses, idx):\n    """"""Update the length of a generated tensor based on the first EOS found.\n\n    This is useful for a decoding situation where tokens after an EOS\n    can be something other than EOS. This also makes sure that a second\n    generated EOS doesn\'t affect the lengths.\n\n    :param lengths: `torch.LongTensor`: The lengths where zero means an\n        unfinished sequence.\n    :param eoses:  `torch.ByteTensor`: A mask that has 1 for sequences that\n        generated an EOS.\n    :param idx: `int`: What value to fill the finished lengths with (normally\n        the current decoding timestep).\n\n    :returns: `torch.Tensor`: The updated lengths tensor (same shape and type).\n    """"""\n    # If a length is 0 it has never had a length set so it is eligible to have\n    # this EOS be the length.\n    updatable_lengths = lengths == 0\n    # If this length can be updated AND this token is an eos\n    lengths_mask = updatable_lengths & eoses\n    return masked_fill(lengths, lengths_mask, idx)\n\n\nclass BeamSearchBase:\n    def __init__(self, beam: int = 1, length_penalty=None, **kwargs):\n        self.length_penalty = length_penalty if length_penalty else no_length_penalty\n        self.K = beam\n\n    def init(self, encoder_outputs):\n        pass\n\n    def step(self, paths, extra):\n        pass\n\n    def update(self, beams, extra):\n        pass\n\n    def __call__(self, encoder_outputs, **kwargs):\n        """"""Perform batched Beam Search.\n\n        Note:\n            The paths and lengths generated do not include the <GO> token.\n\n        :param encoder_outputs: `namedtuple` The outputs of the encoder class.\n        :param init: `Callable(ecnoder_outputs: encoder_outputs, K: int)` -> Any: A\n            callable that is called once at the start of the search to initialize\n            things. This returns a blob that is passed to other callables.\n        :param step: `Callable(paths: torch.LongTensor, extra) -> (probs: torch.FloatTensor, extra):\n            A callable that is does a single decoding step. It returns the log\n            probabilities over the vocabulary in the last dimension. It also returns\n            any state the decoding process needs.\n        :param update: `Callable(beams: torch.LongTensor, extra) -> extra:\n            A callable that is called to edit the decoding state based on the selected\n            best beams.\n        :param length_penalty: `Callable(lengths: torch.LongTensor) -> torch.floatTensor\n            A callable that generates a penalty based on the lengths. Lengths is\n            [B, K] and the returned penalty should be [B, K, 1] (or [B, K, V] to\n            have token based penalties?)\n\n        :Keyword Arguments:\n        * *beam* -- `int`: The number of beams to use.\n        * *mxlen* -- `int`: The max number of steps to run the search for.\n\n        :returns:\n            tuple(preds: torch.LongTensor, lengths: torch.LongTensor, scores: torch.FloatTensor)\n            preds: The predicted values: [B, K, max(lengths)]\n            lengths: The length of each prediction [B, K]\n            scores: The score of each path [B, K]\n        """"""\n        mxlen = kwargs.get(""mxlen"", 100)\n        bsz = get_shape_as_list(encoder_outputs.output)[0]\n        extra = self.init(encoder_outputs)\n        paths = tf.fill((bsz, self.K, 1), Offsets.GO)\n        # This tracks the log prob of each beam. This is distinct from score which\n        # is based on the log prob and penalties.\n        log_probs = tf.zeros((bsz, self.K))\n        # Tracks the lengths of the beams, unfinished beams have a lengths of zero.\n        lengths = tf.zeros((bsz, self.K), np.int32)\n\n        for i in range(mxlen - 1):\n            probs, extra = self.step(paths, extra)\n            V = get_shape_as_list(probs)[-1]\n            probs = tf.reshape(probs, (bsz, self.K, V))  # [B, K, V]\n            if i > 0:\n                # This mask is for all beams that are done.\n                done_mask = lengths != 0  # [B, K, 1]\n                done_mask = tf.expand_dims(done_mask, -1)\n                # Can creating this mask be moved out of the loop? It never changes but we don\'t have V\n                # This mask selects the EOS token\n                eos_mask = tf.cast(\n                    tf.zeros((1, 1, V)) + tf.reshape(tf.cast(tf.range(V) == Offsets.EOS, tf.float32), (1, 1, V)),\n                    done_mask.dtype,\n                )\n                # eos_mask[:, :, Offsets.EOS] = 1\n                # This mask selects the EOS token of only the beams that are done.\n                mask = done_mask & eos_mask\n                # Put all probability mass on the EOS token for finished beams.\n                # Otherwise as the other beams get longer they will all give\n                # up and eventually select this beam and all outputs become\n                # the same.\n                probs = masked_fill(probs, done_mask, -1e8)\n                probs = masked_fill(probs, mask, 0)\n                probs = tf.expand_dims(log_probs, -1) + probs  # [B, K, V]\n                # Calculate the score of the beam based on the current length.\n                valid_lengths = masked_fill(lengths, lengths == 0, i + 1)\n                path_scores = probs / tf.cast(self.length_penalty(valid_lengths), tf.float32)\n            else:\n                # On the first step we only look at probabilities for the first beam.\n                # If we don\'t then the probs will be the same for each beam\n                # This means the same token will be selected for each beam\n                # And we won\'t get any diversity.\n                # Using only the first beam ensures K different starting points.\n                path_scores = probs[:, 0, :]\n\n            flat_scores = tf.reshape(path_scores, (bsz, -1))  # [B, K * V]\n            best_scores, best_idx = tf.math.top_k(flat_scores, self.K)\n            # Get the log_probs of the best scoring beams\n            probs = tf.reshape(probs, (bsz, -1))\n            log_probs = gather_k(flat_scores, probs, best_idx, self.K)\n            log_probs = tf.reshape(log_probs, (bsz, self.K))\n\n            best_beams = best_idx // V  # Get which beam it came from\n            best_idx = best_idx % V  # Get the index of the word regardless of which beam it is.\n\n            # Best Beam index is relative within the batch (only [0, K)).\n            # This makes the index global (e.g. best beams for the second\n            # batch example is in [K, 2*K)).\n            offsets = tf.range(bsz) * self.K\n            offset_beams = best_beams + tf.expand_dims(offsets, -1)\n            flat_beams = tf.reshape(offset_beams, [bsz * self.K])\n            # Select the paths to extend based on the best beams\n            flat_paths = tf.reshape(paths, [bsz * self.K, -1])\n            new_paths = tf.gather(flat_paths, flat_beams)\n            new_paths = tf.reshape(new_paths, [bsz, self.K, -1])\n            # Add the selected outputs to the paths\n            paths = tf.concat([new_paths, tf.expand_dims(best_idx, -1)], axis=2)\n\n            # Select the lengths to keep tracking based on the valid beams left.\n            ##\n            flat_lengths = tf.reshape(lengths, [-1])\n            lengths = tf.gather(flat_lengths, flat_beams)\n            lengths = tf.reshape(lengths, (bsz, self.K))\n            extra = self.update(flat_beams, extra)\n\n            # Updated lengths based on if we hit EOS\n            last = paths[:, :, -1]\n            eoses = last == Offsets.EOS\n            ##\n            lengths = update_lengths(lengths, eoses, i + 1)\n            if tf.reduce_sum(tf.cast(lengths != 0, np.int32)) == self.K:\n                break\n        else:\n            # This runs if the loop didn\'t break meaning one beam hit the max len\n            # Add an EOS to anything that hasn\'t hit the end. This makes the scores real.\n            probs, extra = self.step(paths, extra)\n\n            V = get_shape_as_list(probs)[-1]\n            probs = tf.reshape(probs, (bsz, self.K, V))\n            probs = probs[:, :, Offsets.EOS]  # Select the score of EOS\n            # If any of the beams are done mask out the score of this EOS (they already had an EOS)\n            probs = masked_fill(probs, (lengths != 0), 0)\n            log_probs = log_probs + probs\n            end_tokens = np.full((bsz, self.K, 1), Offsets.EOS)\n            paths = tf.concat([paths, end_tokens], axis=2)\n            lengths = update_lengths(lengths, np.ones_like(lengths) == 1, mxlen)\n            best_scores = log_probs / tf.cast(tf.squeeze(self.length_penalty(lengths), -1), tf.float32)\n\n        # Slice off the Offsets.GO token\n        paths = paths[:, :, 1:]\n        return paths, lengths, best_scores\n\n\nclass StackedLSTMCell(tf.keras.layers.AbstractRNNCell):\n    def __init__(self, num_layers: int, input_size: int, rnn_size: int, dropout: float):\n        super().__init__()\n        self.rnn_size = rnn_size\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.num_layers = num_layers\n        self.layers = []\n\n        for i in range(num_layers):\n            self.layers.append(tf.keras.layers.LSTMCell(rnn_size, use_bias=False))\n\n    @property\n    def state_size(self):\n        """"""size(s) of state(s) used by this cell.\n\n        It can be represented by an Integer, a TensorShape or a tuple of Integers\n        or TensorShapes.\n        """"""\n        raise NotImplementedError(""Abstract method"")\n\n    @property\n    def output_size(self) -> int:\n        """"""Integer or TensorShape: size of outputs produced by this cell.""""""\n        return self.rnn_size\n\n    def call(self, input, hidden):\n        h_0, c_0 = hidden\n        hs, cs = [], []\n        for i, layer in enumerate(self.layers):\n            input, (h_i, c_i) = layer(input, (h_0[i], c_0[i]))\n            if i != self.num_layers - 1:\n                input = self.dropout(input)\n            hs.append(h_i)\n            cs.append(c_i)\n\n        hs = tf.stack(hs)\n        cs = tf.stack(cs)\n\n        return input, (hs, cs)\n\n\nclass StackedGRUCell(tf.keras.layers.AbstractRNNCell):\n    def __init__(self, num_layers: int, input_size: int, rnn_size: int, dropout: float):\n        super().__init__()\n        self.dropout = tf.keras.layers.Dropout(dropout)\n        self.rnn_size = rnn_size\n        self.num_layers = num_layers\n        self.layers = []\n\n        for i in range(num_layers):\n            self.layers.append(tf.keras.layers.GRUCell(rnn_size))\n\n    def call(self, input, hidden):\n        h_0 = hidden\n        hs = []\n        for i, layer in enumerate(self.layers):\n            input, h_i = layer(input, h_0)\n            if i != self.num_layers:\n                input = self.dropout(input)\n            hs.append(h_i)\n\n        hs = tf.stack(hs)\n\n        return input, hs\n\n    @property\n    def output_size(self):\n        """"""Integer or TensorShape: size of outputs produced by this cell.""""""\n        return self.rnn_size\n\n\nif get_version(tf) < 2:\n\n    def rnn_cell(hsz: int, rnntype: str, st: bool = None):\n        """"""Produce a single RNN cell\n\n        :param hsz: The number of hidden units per LSTM\n        :param rnntype: `lstm` or `gru`\n        :param st: state is tuple? defaults to `None`\n        :return: a cell\n        """"""\n        if st is not None:\n            cell = (\n                tf.contrib.rnn.LSTMCell(hsz, state_is_tuple=st)\n                if rnntype.endswith(""lstm"")\n                else tf.contrib.rnn.GRUCell(hsz)\n            )\n        else:\n            cell = tf.contrib.rnn.LSTMCell(hsz) if rnntype.endswith(""lstm"") else tf.contrib.rnn.GRUCell(hsz)\n        return cell\n\n\nelse:\n\n    def rnn_cell(insz: int, hsz: int, rnntype: str, nlayers: int = 1, dropout: float = 0.5):\n\n        if rnntype == ""gru"":\n            rnn = StackedGRUCell(nlayers, insz, hsz, dropout)\n        else:\n            rnn = StackedLSTMCell(nlayers, insz, hsz, dropout)\n        return rnn\n'"
layers/eight_mile/tf/optz.py,0,"b'import logging\nimport tensorflow as tf\nfrom eight_mile.utils import get_version, exporter, register\nfrom eight_mile.optz import create_lr_scheduler, register_lr_scheduler, MEAD_LAYERS_LR_SCHEDULERS\nfrom eight_mile.optz import (\n    LearningRateScheduler,\n    ConstantScheduler,\n    WarmupLearningRateScheduler,\n    WarmupLinearScheduler,\n    CyclicLRScheduler,\n    PiecewiseDecayScheduler,\n    ZarembaDecayScheduler,\n    CosineDecayScheduler,\n    InverseTimeDecayScheduler,\n    ExponentialDecayScheduler,\n    CompositeLRScheduler,\n)\n\nlogger = logging.getLogger(""mead.layers"")\n\n\nclass ConstantSchedulerTensorFlow1:\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, lr, global_step):\n        return tf.identity(lr, name=""lr"")\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass WarmupLinearSchedulerTensorFlow1(WarmupLearningRateScheduler):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def __call__(self, lr, global_step):\n        return tf.identity(\n            tf.minimum(1.0, tf.cast(global_step / self.warmup_steps, dtype=tf.float32)) * lr, name=""lr""\n        )\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass CyclicLRSchedulerTensorFlow1:\n    def __init__(self, max_lr=1e-2, decay_steps=1000, **kwargs):\n        super().__init__()\n        self.max_lr = max_lr\n        self.decay_steps = decay_steps\n\n    def __call__(self, lr, global_step):\n        gs_f = tf.cast(global_step, tf.float32)\n        cycle = tf.floor(1.0 + gs_f / (2.0 * self.decay_steps))\n        x = tf.abs(gs_f / self.decay_steps - 2.0 * cycle + 1.0)\n        clr = lr + (self.max_lr - lr) * tf.maximum(0.0, 1.0 - x)\n        return tf.identity(clr, name=""lr"")\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass SGDRSchedulerTensorFlow1:\n    def __init__(self, first_decay_steps=1000, **kwargs):\n        super().__init__()\n        self.first_decay_steps = first_decay_steps\n\n    def __call__(self, lr, global_step):\n        return tf.identity(\n            tf.train.cosine_decay_restarts(lr, global_step, first_decay_steps=self.first_decay_steps), name=""lr""\n        )\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass CompositeLRSchedulerTensorFlow1:\n    def __init__(self, warm=None, rest=None, **kwargs):\n        self.warm = warm\n        self.rest = rest\n\n    def __call__(self, lr, global_step):\n        warm_tensor = self.warm(lr, global_step)\n\n        def call_warm():\n            return warm_tensor\n\n        rest_step = tf.subtract(global_step, tf.compat.v1.constant(self.warm.warmup_steps, dtype=global_step.dtype))\n        rest_tensor = self.rest(lr, rest_step)\n\n        def call_rest():\n            return rest_tensor\n\n        return tf.identity(tf.cond(global_step < self.warm.warmup_steps, call_warm, call_rest), name=""lr"")\n\n    def __str__(self):\n        return ""LRScheduler({}, {})"".format(self.warm, self.rest)\n\n\nclass PiecewiseDecaySchedulerTensorFlow1:\n    def __init__(self, boundaries=None, values=None, **kwargs):\n        super().__init__()\n        self.boundaries = boundaries\n        self.values = values\n\n    def __call__(self, lr, global_step):\n        return tf.identity(tf.compat.v1.train.piecewise_constant(global_step, self.boundaries, self.values), name=""lr"")\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass InverseTimeDecaySchedulerTensorFlow1:\n    def __init__(self, decay_steps=16000, decay_rate=0.05, staircase=False, **kwargs):\n        super().__init__()\n        self.decay_steps = decay_steps\n        self.decay_rate = decay_rate\n        self.staircase = staircase\n\n    def __call__(self, lr, global_step):\n        return tf.identity(\n            tf.train.inverse_time_decay(\n                lr, global_step, self.decay_steps, self.decay_rate, staircase=self.staircase\n            ),\n            name=""lr"",\n        )\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass ExponentialDecaySchedulerTensorFlow1:\n    def __init__(self, decay_steps=16000, decay_rate=0.5, staircase=False, **kwargs):\n        self.decay_steps = decay_steps\n        self.decay_rate = decay_rate\n        self.staircase = staircase\n\n    def __call__(self, lr, global_step):\n        return tf.identity(\n            tf.train.exponential_decay(\n                lr, global_step, self.decay_steps, self.decay_rate, staircase=self.staircase\n            ),\n            name=""lr"",\n        )\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass ZarembaDecaySchedulerTensorFlow1(PiecewiseDecaySchedulerTensorFlow1):\n    """"""Utility only, just to simplify the JSON""""""\n\n    def __init__(self, boundaries=None, decay_rate=None, **kwargs):\n        lr = float(kwargs.get(""lr"", kwargs.get(""eta"", 1.0)))\n        values = [lr / (float(decay_rate) ** i) for i in range(len(boundaries) + 1)]\n        super().__init__(boundaries=boundaries, values=values)\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass ConstantSchedulerTensorFlow2(LearningRateScheduler, tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def __call__(self, global_step):\n        return self.lr\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass WarmupLinearSchedulerTensorFlow2(\n    WarmupLearningRateScheduler, tf.keras.optimizers.schedules.LearningRateSchedule\n):\n    def __init__(self, warmup_steps=16000, **kwargs):\n        lr = float(kwargs.get(""lr"", kwargs.get(""eta"", 1.0)))\n        kwargs[""lr""] = lr\n        super().__init__(warmup_steps=warmup_steps, **kwargs)\n\n    def __call__(self, global_step):\n        return tf.minimum(1.0, global_step / float(self.warmup_steps)) * self.lr\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass CyclicLRSchedulerTensorFlow2(LearningRateScheduler, tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, max_lr=1e-2, decay_steps=1000, **kwargs):\n        lr = float(kwargs.get(""lr"", kwargs.get(""eta"", 1.0)))\n        kwargs[""lr""] = lr\n        super().__init__(**kwargs)\n        self.max_lr = max_lr\n        self.decay_steps = decay_steps\n\n    def __call__(self, global_step):\n        gs_f = tf.cast(global_step, tf.float32)\n        cycle = tf.floor(1.0 + gs_f / (2.0 * self.decay_steps))\n        x = tf.abs(gs_f / self.decay_steps - 2.0 * cycle + 1.0)\n        clr = self.lr + (self.max_lr - self.lr) * tf.maximum(0.0, 1.0 - x)\n        return tf.identity(clr, name=""lr"")\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass SGDRSchedulerTensorFlow2(LearningRateScheduler, tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, first_decay_steps=1000, **kwargs):\n        super().__init__(**kwargs)\n        self.first_decay_steps = first_decay_steps\n\n    def __call__(self, global_step):\n        return tf.identity(\n            tf.compat.v1.train.cosine_decay_restarts(\n                self.lr, global_step, first_decay_steps=self.first_decay_steps\n            ),\n            name=""lr"",\n        )\n\n    def __str__(self):\n        return type(self).__name__ + ""()""\n\n\nclass ZarembaDecaySchedulerTensorFlow2(tf.keras.optimizers.schedules.PiecewiseConstantDecay):\n    """"""Utility only, just to simplify the JSON""""""\n\n    def __init__(self, boundaries=None, decay_rate=None, **kwargs):\n        lr = float(kwargs.get(""lr"", kwargs.get(""eta"", 1.0)))\n        values = [lr / (float(decay_rate) ** i) for i in range(len(boundaries) + 1)]\n        super().__init__(boundaries, values, kwargs.get(""name""))\n\n\nclass CompositeLRSchedulerTensorFlow2(CompositeLRScheduler, tf.keras.optimizers.schedules.LearningRateSchedule):\n    pass\n\n\nclass PiecewiseDecaySchedulerTensorFlow2(tf.keras.optimizers.schedules.PiecewiseConstantDecay):\n    def __init__(self, boundaries, values, **kwargs):\n        super().__init__(boundaries, values)\n\n\nclass InverseTimeDecaySchedulerTensorFlow2(tf.keras.optimizers.schedules.InverseTimeDecay):\n    def __init__(self, decay_steps=16000, decay_rate=0.05, staircase=False, **kwargs):\n        lr = kwargs.get(""lr"", kwargs.get(""eta"", 0.01))\n        super().__init__(lr, decay_steps, decay_rate, staircase, kwargs.get(""name""))\n\n\nclass ExponentialDecaySchedulerTensorFlow2(tf.keras.optimizers.schedules.ExponentialDecay):\n    def __init__(self, decay_steps=16000, decay_rate=0.5, staircase=False, **kwargs):\n        lr = kwargs.get(""lr"", kwargs.get(""eta"", 0.01))\n        super().__init__(lr, decay_steps, decay_rate, staircase, kwargs.get(""name""))\n\n\nclass CosineDecaySchedulerTensorFlow(CosineDecayScheduler):\n    def __init__(self, decay_steps=16000, alpha=0.0, **kwargs):\n        kwargs[\'lr\'] = kwargs.get(""lr"", kwargs.get(""eta"", 0.01))\n        super().__init__(decay_steps, alpha, **kwargs)\n\n    def __call__(self, global_step):\n        global_step = tf.math.minimum(global_step, self.decay_steps)\n        cosine_decay = 0.5 * (1.0 + tf.cos(3.14159265 * global_step / self.decay_steps))\n        decayed = (1 - self.alpha) * cosine_decay + self.alpha\n        return self.lr * decayed\n\n\nif not tf.executing_eagerly():\n\n    ConstantSchedulerTensorFlow = ConstantSchedulerTensorFlow1\n    WarmupLinearSchedulerTensorFlow = WarmupLinearSchedulerTensorFlow1\n    CyclicLRSchedulerTensorFlow = CyclicLRSchedulerTensorFlow1\n    SGDRSchedulerTensorFlow = SGDRSchedulerTensorFlow1\n    CompositeLRSchedulerTensorFlow = CompositeLRSchedulerTensorFlow1\n    PiecewiseDecaySchedulerTensorFlow = PiecewiseDecaySchedulerTensorFlow1\n    ExponentialDecaySchedulerTensorFlow = ExponentialDecaySchedulerTensorFlow1\n    InverseTimeDecaySchedulerTensorFlow = InverseTimeDecaySchedulerTensorFlow1\n    ZarembaDecaySchedulerTensorFlow = ZarembaDecaySchedulerTensorFlow1\nelse:\n    ConstantSchedulerTensorFlow = ConstantSchedulerTensorFlow2\n    WarmupLinearSchedulerTensorFlow = WarmupLinearSchedulerTensorFlow2\n    CyclicLRSchedulerTensorFlow = CyclicLRSchedulerTensorFlow2\n    SGDRSchedulerTensorFlow = SGDRSchedulerTensorFlow2\n    CompositeLRSchedulerTensorFlow = CompositeLRSchedulerTensorFlow2\n    PiecewiseDecaySchedulerTensorFlow = PiecewiseDecaySchedulerTensorFlow2\n    ExponentialDecaySchedulerTensorFlow = ExponentialDecaySchedulerTensorFlow2\n    InverseTimeDecaySchedulerTensorFlow = InverseTimeDecaySchedulerTensorFlow2\n    ZarembaDecaySchedulerTensorFlow = ZarembaDecaySchedulerTensorFlow2\n\nregister(ConstantSchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""default"", ""lr_scheduler"")\nregister(WarmupLinearSchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""warmup_linear"", ""lr_scheduler"")\nregister(CyclicLRSchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""clr"", ""lr_scheduler"")\nregister(SGDRSchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""sgdr"", ""lr_scheduler"")\nregister(CompositeLRSchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""composite"", ""lr_scheduler"")\nregister(PiecewiseDecaySchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""piecewise"", ""lr_scheduler"")\nregister(ZarembaDecaySchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""zaremba"", ""lr_scheduler"")\nregister(CosineDecaySchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""cosine"", ""lr_scheduler"")\nregister(InverseTimeDecaySchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""invtime"", ""lr_scheduler"")\nregister(ExponentialDecaySchedulerTensorFlow, MEAD_LAYERS_LR_SCHEDULERS, ""exponential"", ""lr_scheduler"")\n\n\nclass AdamWOptimizer(tf.compat.v1.train.Optimizer):\n    """"""A basic Adam optimizer that includes ""correct"" L2 weight decay.\n\n    Modified from: https://github.com/google-research/bert/blob/master/optimization.py\n    This does the weight decay slightly differently from PyTorch version, putting it before the update\n    """"""\n\n    def __init__(self, learning_rate, weight_decay=0.0, beta_1=0.9, beta_2=0.999, epsilon=1e-6, name=""AdamWOptimizer""):\n        """"""Constructs a AdamWOptimizer.""""""\n        super().__init__(False, name)\n\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n\n    def _get_variable_name(self, param_name):\n        import re\n\n        """"""Get the variable name from the tensor name.""""""\n        m = re.match(""^(.*):\\\\d+$"", param_name)\n        if m is not None:\n            param_name = m.group(1)\n        return param_name\n\n    def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n        assignments = []\n        for (grad, param) in grads_and_vars:\n            if grad is None or param is None:\n                continue\n\n            param_name = self._get_variable_name(param.name)\n\n            m = tf.compat.v1.get_variable(\n                name=param_name + ""/adam_m"",\n                shape=param.shape.as_list(),\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.compat.v1.zeros_initializer(),\n            )\n            v = tf.compat.v1.get_variable(\n                name=param_name + ""/adam_v"",\n                shape=param.shape.as_list(),\n                dtype=tf.float32,\n                trainable=False,\n                initializer=tf.compat.v1.zeros_initializer(),\n            )\n\n            # Standard Adam update.\n            next_m = tf.multiply(self.beta_1, m) + tf.multiply(1.0 - self.beta_1, grad)\n            next_v = tf.multiply(self.beta_2, v) + tf.multiply(1.0 - self.beta_2, tf.square(grad))\n\n            update = next_m / (tf.sqrt(next_v) + self.epsilon)\n            update += self.weight_decay * param\n            update_with_lr = self.learning_rate * update\n            next_param = param - update_with_lr\n\n            assignments.extend([param.assign(next_param), m.assign(next_m), v.assign(next_v)])\n        return tf.group(*assignments, name=name)\n\n\nOPTIMIZER_SUMMARIES = [""learning_rate"", ""loss"", ""gradients"", ""gradient_norm"", ""global_gradient_norm""]\n\n\ndef _optimize_loss(\n    loss,\n    global_step,\n    learning_rate,\n    optimizer,\n    gradient_multipliers=None,\n    clip_gradients=None,\n    learning_rate_decay_fn=None,\n    update_ops=None,\n    variables=None,\n    name=None,\n    summaries=None,\n    colocate_gradients_with_ops=False,\n    increment_global_step=True,\n):\n    """"""Given loss and parameters for optimizer, returns a training op.\n    Various ways of passing optimizers include:\n    - by string specifying the name of the optimizer. See OPTIMIZER_CLS_NAMES\n        for full list. E.g. `optimize_loss(..., optimizer=\'Adam\')`.\n    - by function taking learning rate `Tensor` as argument and returning an\n        `Optimizer` instance. E.g. `optimize_loss(...,\n        optimizer=lambda lr: tf.train.MomentumOptimizer(lr, momentum=0.5))`.\n      Alternatively, if `learning_rate` is `None`, the function takes no\n      arguments. E.g. `optimize_loss(..., learning_rate=None,\n        optimizer=lambda: tf.train.MomentumOptimizer(0.5, momentum=0.5))`.\n    - by a subclass of `Optimizer` having a single-argument constructor\n        (the argument is the learning rate), such as AdamOptimizer or\n        AdagradOptimizer. E.g. `optimize_loss(...,\n        optimizer=tf.train.AdagradOptimizer)`.\n    - by an instance of a subclass of `Optimizer`.\n        E.g., `optimize_loss(..., optimizer=tf.train.AdagradOptimizer(0.5))`.\n    Args:\n      loss: Scalar `Tensor`.\n      global_step: Scalar int `Tensor`, step counter to update on each step\n                   unless `increment_global_step` is `False`. If not supplied,\n                   it will be fetched from the default graph (see\n                   `tf.train.get_global_step` for details). If it has\n                   not been created, no step will be incremented with each weight\n                   update. `learning_rate_decay_fn` requires `global_step`.\n      learning_rate: float or `Tensor`, magnitude of update per each training\n                     step. Can be `None`.\n      optimizer: string, class or optimizer instance, used as trainer.\n                 string should be name of optimizer, like \'SGD\',\n                   \'Adam\', \'Adagrad\'. Full list in OPTIMIZER_CLS_NAMES constant.\n                 class should be sub-class of `tf.Optimizer` that implements\n                   `compute_gradients` and `apply_gradients` functions.\n                 optimizer instance should be instantiation of `tf.Optimizer`\n                   sub-class and have `compute_gradients` and `apply_gradients`\n                   functions.\n      gradient_noise_scale: float or None, adds 0-mean normal noise scaled by this\n                            value.\n      gradient_multipliers: dict of variables or variable names to floats.\n                            If present, gradients for specified\n                            variables will be multiplied by given constant.\n      clip_gradients: float, callable or `None`. If float, is provided, a global\n        clipping is applied to prevent the norm of the gradient to exceed this\n        value. Alternatively, a callable can be provided e.g.: adaptive_clipping.\n        This callable takes a `list` of `(gradients, variables)` `tuple`s and\n        returns the same thing with the gradients modified.\n      learning_rate_decay_fn: function, takes `learning_rate` and `global_step`\n                              `Tensor`s, returns `Tensor`.\n                              Can be used to implement any learning rate decay\n                              functions.\n                              For example: `tf.train.exponential_decay`.\n                              Ignored if `learning_rate` is not supplied.\n      update_ops: list of update `Operation`s to execute at each step. If `None`,\n                  uses elements of UPDATE_OPS collection. The order of execution\n                  between `update_ops` and `loss` is non-deterministic.\n      variables: list of variables to optimize or\n                 `None` to use all trainable variables.\n      name: The name for this operation is used to scope operations and summaries.\n      summaries: List of internal quantities to visualize on tensorboard. If not\n                 set, the loss, the learning rate, and the global norm of the\n                 gradients will be reported. The complete list of possible values\n                 is in OPTIMIZER_SUMMARIES.\n      colocate_gradients_with_ops: If True, try colocating gradients with the\n                                   corresponding op.\n      increment_global_step: Whether to increment `global_step`. If your model\n        calls `optimize_loss` multiple times per training step (e.g. to optimize\n        different parts of the model), use this arg to avoid incrementing\n        `global_step` more times than necessary.\n    Returns:\n      Training op.\n    Raises:\n      ValueError: if:\n          * `loss` is an invalid type or shape.\n          * `global_step` is an invalid type or shape.\n          * `learning_rate` is an invalid type or value.\n          * `optimizer` has the wrong type.\n          * `clip_gradients` is neither float nor callable.\n          * `learning_rate` and `learning_rate_decay_fn` are supplied, but no\n            `global_step` is available.\n          * `gradients` is empty.\n    """"""\n    loss = tf.convert_to_tensor(loss)\n    if global_step is None:\n        global_step = tf.compat.v1.train.get_global_step()\n\n    with tf.compat.v1.variable_scope(name, ""OptimizeLoss"", [loss, global_step]):\n        # Update ops take UPDATE_OPS collection if not provided.\n        if update_ops is None:\n            update_ops = set(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS))\n        # Make sure update ops are ran before computing loss.\n        if update_ops:\n            with tf.compat.v1.control_dependencies(list(update_ops)):\n                loss = tf.identity(loss)\n\n        # Learning rate variable, with possible decay.\n        lr = None\n        if learning_rate is not None:\n            if isinstance(learning_rate, tf.Tensor) and learning_rate.get_shape().ndims == 0:\n                lr = learning_rate\n            elif isinstance(learning_rate, float):\n                if learning_rate < 0.0:\n                    raise ValueError(""Invalid learning_rate %s."", learning_rate)\n                lr = tf.compat.v1.get_variable(\n                    ""learning_rate"", [], trainable=False, initializer=tf.compat.v1.constant_initializer(learning_rate)\n                )\n            else:\n                raise ValueError(\n                    ""Learning rate should be 0d Tensor or float. ""\n                    ""Got %s of type %s"" % (str(learning_rate), str(type(learning_rate)))\n                )\n        if summaries is None:\n            summaries = [""loss"", ""learning_rate"", ""global_gradient_norm""]\n        else:\n            for summ in summaries:\n                if summ not in OPTIMIZER_SUMMARIES:\n                    raise ValueError(\n                        ""Summaries should be one of [%s], you provided %s."" % ("", "".join(OPTIMIZER_SUMMARIES), summ)\n                    )\n        if learning_rate is not None and learning_rate_decay_fn is not None:\n            if global_step is None:\n                raise ValueError(""global_step is required for learning_rate_decay_fn."")\n            lr = learning_rate_decay_fn(lr, global_step)\n            if ""learning_rate"" in summaries:\n                tf.compat.v1.summary.scalar(""learning_rate"", lr)\n\n        if isinstance(optimizer, type) and issubclass(optimizer, tf.compat.v1.train.Optimizer):\n            if lr is None:\n                raise ValueError(\n                    ""Learning rate is None, but should be specified if "" ""optimizer is class (%s)."" % optimizer\n                )\n            opt = optimizer(learning_rate=lr)\n        elif isinstance(optimizer, tf.compat.v1.train.Optimizer):\n            opt = optimizer\n        elif callable(optimizer):\n            if learning_rate is not None:\n                opt = optimizer(lr)\n            else:\n                opt = optimizer()\n            if not isinstance(opt, tf.compat.v1.train.Optimizer):\n                raise ValueError(\n                    ""Unrecognized optimizer: function should return "" ""subclass of Optimizer. Got %s."" % str(opt)\n                )\n        else:\n            raise ValueError(\n                ""Unrecognized optimizer: should be string, ""\n                ""subclass of Optimizer, instance of ""\n                ""subclass of Optimizer or function with one argument. ""\n                ""Got %s."" % str(optimizer)\n            )\n\n        # All trainable variables, if specific variables are not specified.\n        if variables is None:\n            variables = tf.compat.v1.trainable_variables()\n\n        # Compute gradients.\n        gradients = opt.compute_gradients(loss, variables, colocate_gradients_with_ops=colocate_gradients_with_ops)\n\n        # Multiply some gradients.\n        if gradient_multipliers is not None:\n            gradients = _multiply_gradients(gradients, gradient_multipliers)\n            if not gradients:\n                raise ValueError(\n                    ""Empty list of (gradient, var) pairs encountered. This is most ""\n                    ""likely to be caused by an improper value of gradient_multipliers.""\n                )\n\n        if ""global_gradient_norm"" in summaries or ""gradient_norm"" in summaries:\n            tf.compat.v1.summary.scalar(""global_norm/gradient_norm"", tf.linalg.global_norm(list(zip(*gradients))[0]))\n\n        # Optionally clip gradients by global norm.\n        if isinstance(clip_gradients, float):\n            gradients = _clip_gradients_by_norm(gradients, clip_gradients)\n        elif callable(clip_gradients):\n            gradients = clip_gradients(gradients)\n        elif clip_gradients is not None:\n            raise ValueError(""Unknown type %s for clip_gradients"" % type(clip_gradients))\n\n        # Add scalar summary for loss.\n        if ""loss"" in summaries:\n            tf.compat.v1.summary.scalar(""loss"", loss)\n\n        # Add histograms for variables, gradients and gradient norms.\n        for gradient, variable in gradients:\n            if isinstance(gradient, tf.IndexedSlices):\n                grad_values = gradient.values\n            else:\n                grad_values = gradient\n\n            if grad_values is not None:\n                var_name = variable.name.replace("":"", ""_"")\n                if ""gradients"" in summaries:\n                    tf.compat.v1.summary.histogram(""gradients/%s"" % var_name, grad_values)\n                if ""gradient_norm"" in summaries:\n                    tf.compat.v1.summary.scalar(""gradient_norm/%s"" % var_name, tf.linalg.global_norm([grad_values]))\n\n        if clip_gradients is not None and (""global_gradient_norm"" in summaries or ""gradient_norm"" in summaries):\n            tf.compat.v1.summary.scalar(\n                ""global_norm/clipped_gradient_norm"", tf.linalg.global_norm(list(zip(*gradients))[0])\n            )\n\n        # Create gradient updates.\n        grad_updates = opt.apply_gradients(\n            gradients, global_step=global_step if increment_global_step else None, name=""train""\n        )\n\n        # Ensure the train_tensor computes grad_updates.\n        # train_tensor = tf.compat.v1.with_dependencies([grad_updates], loss)\n        with tf.control_dependencies([grad_updates]):\n            train_tensor = tf.identity(loss)\n\n        return train_tensor\n\n\ndef _clip_gradients_by_norm(grads_and_vars, clip_gradients):\n    """"""Clips gradients by global norm.""""""\n    gradients, variables = zip(*grads_and_vars)\n    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_gradients)\n    return list(zip(clipped_gradients, variables))\n\n\ndef _multiply_gradients(grads_and_vars, gradient_multipliers):\n    """"""Multiply specified gradients.""""""\n    multiplied_grads_and_vars = []\n    for grad, var in grads_and_vars:\n        if grad is not None and (var in gradient_multipliers or var.name in gradient_multipliers):\n            key = var if var in gradient_multipliers else var.name\n            multiplier = gradient_multipliers[key]\n            if isinstance(grad, tf.IndexedSlices):\n                grad_values = grad.values * multiplier\n                grad = tf.IndexedSlices(grad_values, grad.indices, grad.dense_shape)\n            else:\n                grad *= tf.cast(multiplier, grad.dtype)\n        multiplied_grads_and_vars.append((grad, var))\n    return multiplied_grads_and_vars\n\n\ndef optimizer(loss_fn, **kwargs):\n\n    global_step = tf.compat.v1.train.get_or_create_global_step()\n    clip = kwargs.get(""clip"", None)\n    optim = kwargs.get(""optim"", ""sgd"")\n    eta = kwargs.get(""lr"", kwargs.get(""eta"", 0.01))\n    lr_scheduler = create_lr_scheduler(**kwargs)\n    colocate_gradients_with_ops = bool(kwargs.get(""colocate_gradients_with_ops"", False))\n    sgd_mom = float(kwargs.get(""mom"", 0.9))\n    if optim == ""adadelta"":\n        rho = float(kwargs.get(""rho"", 0.95))\n        eps = float(kwargs.get(""epsilon"", 1e-6))\n        logger.info(""adadelta(eta=%f, rho=%f, epsilon=%f)"", eta, rho, eps)\n        optz = lambda lr: tf.compat.v1.train.AdadeltaOptimizer(lr, rho, eps)\n    elif optim == ""adam"":\n        beta1 = float(kwargs.get(""beta1"", 0.9))\n        beta2 = float(kwargs.get(""beta2"", 0.999))\n        eps = float(kwargs.get(""epsilon"", 1e-8))\n        logger.info(""adam(eta=%f beta1=%f, beta2=%f, eps=%f)"", eta, beta1, beta2, eps)\n        optz = lambda lr: tf.compat.v1.train.AdamOptimizer(lr, beta1, beta2, eps)\n    elif optim == ""adamw"":\n        wd = float(kwargs.get(""weight_decay"", 0))\n        beta1 = float(kwargs.get(""beta1"", 0.9))\n        beta2 = float(kwargs.get(""beta2"", 0.999))\n        eps = float(kwargs.get(""epsilon"", 1e-8))\n        logger.info(""adamw(eta=%f beta1=%f, beta2=%f, eps=%f, wd=%f)"", eta, beta1, beta2, eps, wd)\n        optz = lambda lr: AdamWOptimizer(lr, wd, beta1, beta2, eps)\n    elif optim == ""rmsprop"":\n        # Get mom again with difference default\n        mom = float(kwargs.get(""mom"", 0.0))\n        logger.info(""rmsprop(eta=%f, mom=%f)"", eta, mom)\n        optz = lambda lr: tf.compat.v1.train.RMSPropOptimizer(lr, momentum=mom)\n    elif sgd_mom > 0:\n        logger.info(""sgd-mom(eta=%f, mom=%f)"", eta, sgd_mom)\n        optz = lambda lr: tf.compat.v1.train.MomentumOptimizer(lr, sgd_mom)\n    else:\n        logger.info(""sgd(eta=%f)"", eta)\n        optz = lambda lr: tf.compat.v1.train.GradientDescentOptimizer(lr)\n\n    logger.info(""clip gradients at %s"", clip)\n    return (\n        global_step,\n        _optimize_loss(\n            loss_fn,\n            global_step,\n            eta,\n            optz,\n            colocate_gradients_with_ops=colocate_gradients_with_ops,\n            clip_gradients=clip,\n            learning_rate_decay_fn=lr_scheduler,\n            increment_global_step=True,\n            variables=kwargs.get(\'variables\')\n        ),\n    )\n\n\nclass EagerOptimizer:\n    def __init__(self, loss, optimizer=None, **kwargs):\n        self.loss = loss\n        self.global_step = tf.Variable(int(kwargs.get(\'global_step\', 0)))\n        if ""lr_function"" in kwargs:\n            lr_function = kwargs[""lr_function""]\n        else:\n            if ""lr_scheduler_type"" not in kwargs:\n                kwargs[""lr_scheduler_type""] = ""default""\n            lr_function = create_lr_scheduler(**kwargs)\n        # decay_fn = None\n        # Right now this option is pointless since sparse updates dont work on GPU.  We just turn it off\n        sgd_mom = float(kwargs.get(""mom"", 0.9))\n        self.clip = kwargs.get(""clip"", 100)\n\n        if optimizer:\n            self.optimizer = optimizer\n        else:\n            optim = kwargs.get(""optim"", ""sgd"")\n            lr = kwargs.get(""lr"", kwargs.get(""eta"", 0.01))\n\n            if optim == ""adadelta"":\n                rho = float(kwargs.get(""rho"", 0.95))\n                eps = float(kwargs.get(""epsilon"", 1e-6))\n                logger.info(""adadelta(eta=%f, rho=%f, epsilon=%f)"", lr, rho, eps)\n                self.optimizer = tf.keras.optimizers.Adadelta(lr, rho, eps)\n            elif optim == ""adam"":\n                beta1 = float(kwargs.get(""beta1"", 0.9))\n                beta2 = float(kwargs.get(""beta2"", 0.999))\n                eps = float(kwargs.get(""epsilon"", 1e-8))\n                logger.info(""adam(eta=%f beta1=%f, beta2=%f, eps=%f)"", lr, beta1, beta2, eps)\n                self.optimizer = tf.keras.optimizers.Adam(lr_function, beta1, beta2, eps)\n\n            elif optim == ""adamw"":\n                import tensorflow_addons as tfa\n\n                wd = float(kwargs.get(""weight_decay"", 0))\n                beta1 = float(kwargs.get(""beta1"", 0.9))\n                beta2 = float(kwargs.get(""beta2"", 0.999))\n                eps = float(kwargs.get(""epsilon"", 1e-8))\n                logger.info(""adamw(eta=%f beta1=%f, beta2=%f, eps=%f, wd=%f)"", lr, beta1, beta2, eps, wd)\n                self.optimizer = tfa.optimizers.AdamW(\n                    weight_decay=wd, learning_rate=lr_function, beta_1=beta1, beta_2=beta2, epsilon=eps\n                )\n            elif optim == ""rmsprop"":\n                # Get mom again with difference default\n                mom = float(kwargs.get(""mom"", 0.0))\n                logger.info(""rmsprop(eta=%f, mom=%f)"", lr, mom)\n                self.optimizer = tf.keras.optimizers.RMSprop(lr_function, momentum=mom)\n            elif sgd_mom > 0:\n                logger.info(""sgd-mom(eta=%f, mom=%f)"", lr, sgd_mom)\n                self.optimizer = tf.keras.optimizers.SGD(lr_function, sgd_mom)\n            else:\n                logger.info(""sgd(eta=%f)"", lr)\n                self.optimizer = tf.keras.optimizers.SGD(lr_function)\n\n        logger.info(""clip gradients at %s"", self.clip)\n\n    def update(self, model, x, y, num_replicas=1):\n        with tf.GradientTape() as tape:\n            loss_value = self.loss(model, x, y) / num_replicas\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        grads, _ = tf.clip_by_global_norm(grads, self.clip)\n        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        self.global_step.assign_add(1)\n        return loss_value\n\n    def update_with_hidden(self, model, h, x, y):\n        with tf.GradientTape() as tape:\n            loss_value, h = self.loss(model, h, x, y)\n\n        grads = tape.gradient(loss_value, model.trainable_variables)\n        grads, _ = tf.clip_by_global_norm(grads, self.clip)\n        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        self.global_step.assign_add(1)\n        return loss_value, h\n'"
layers/eight_mile/tf/serialize.py,0,"b'import numpy as np\nfrom typing import Dict, List\nfrom eight_mile.tf.layers import TransformerEncoderStack, TransformerEncoder, MultiHeadedAttention, FFN\nfrom eight_mile.tf.embeddings import LookupTableEmbeddings, LearnedPositionalLookupTableEmbeddingsWithBias, LearnedPositionalLookupTableEmbeddings\n\nimport tensorflow as tf\n\n\ndef to_weight_array(tf_layer: tf.keras.layers.Layer, name: str) -> Dict:\n    """"""Convert a {`LayerNorm`, `tf.keras.layers.Dense`} to `weights` and `bias` arrays\n\n    :param tf_layer: A layer to get weights for\n    :param name: The name of this layer to serialize\n    :return: A Dictionary containing `weights` and `bias` keys\n    """"""\n\n    weights, bias = tf_layer.get_weights()\n    return {f""{name}/weights"": weights.T, f""{name}/bias"": bias}\n\n\ndef from_weight_array(tf_layer: tf.keras.layers.Layer, d: Dict, name: str):\n    """"""Read in {`LayerNorm`, `tf.keras.layers.Dense`} from `weights` and `bias` fields\n\n    :param tf_layer: A layer to get weights for\n    :param d: A Dict containing the arrays by key\n    :param name: The name of this layer\n    :return: None\n    """"""\n    weights = d[f""{name}/weights""]\n    bias = d[f""{name}/bias""]\n    tf_layer.set_weights([weights.T, bias])\n\n\ndef to_ffn_array(tf_ffn: FFN, name: str) -> Dict:\n    """"""Convert a `FFN` layer to a set of arrays\n\n    :param tf_ffn: An `FFN` layer for Transformers\n    :param name: The name of the layer\n    :return: A Dict containing the arrays by key\n    """"""\n    d = {}\n    d.update(to_weight_array(tf_ffn.expansion, f""{name}/expansion""))\n    d.update(to_weight_array(tf_ffn.squeeze, f""{name}/squeeze""))\n    return d\n\n\ndef from_ffn_array(tf_ffn: FFN, d: Dict, name: str):\n    """"""Restore an `FFN` layer\'s weights from a set of arrays\n\n    :param tf_ffn: An `FFN` layer for Transformers\n    :param d: A Dict containing the arrays by key\n    :param name: The name of the layer\n    :return: None\n    """"""\n    from_weight_array(tf_ffn.expansion, d, f""{name}/expansion"")\n    from_weight_array(tf_ffn.squeeze, d, f""{name}/squeeze"")\n\n\ndef to_attn_array(tf_attn: tf.keras.layers.Layer, name: str) -> Dict:\n    """"""Convert a self-attention module to a set of arrays\n\n    :param tf_attn: The self-attention layer of the transformer encoder, could be MultiHeadedAttention or\n    MultiHeadedRelativeAttention\n    :param name: The name of the layer\n    :return: A Dict containing the arrays by key\n    """"""\n    d = {}\n\n    d.update(to_weight_array(tf_attn.w_Q, f""{name}/w_Q""))\n    d.update(to_weight_array(tf_attn.w_K, f""{name}/w_K""))\n    d.update(to_weight_array(tf_attn.w_V, f""{name}/w_V""))\n    d.update(to_weight_array(tf_attn.w_O, f""{name}/w_O""))\n\n    if hasattr(tf_attn, \'rpr_key\'):\n        # Embeddings have same shape in PyTorch and TF [input_sz, output_sz]\n        rpr_key_weights = tf_attn.rpr_key.get_weights()[0]\n        rpr_value_weights = tf_attn.rpr_value.get_weights()[0]\n        d.update({f""{name}/rpr_key"": rpr_key_weights})\n        d.update({f""{name}/rpr_value"": rpr_value_weights})\n\n    return d\n\n\ndef from_attn_array(tf_attn: tf.keras.layers.Layer, d: Dict, name: str):\n    """"""Restore a self-attention module from a set of keys\n\n    :param tf_attn: A self-attention module for Transformers, could be MultiHeadedAttention or\n    MultiHeadedRelativeAttention\n    :param d: A Dict of arrays by key\n    :param name: The name of the layer\n    """"""\n    from_weight_array(tf_attn.w_Q, d, f""{name}/w_Q"")\n    from_weight_array(tf_attn.w_K, d, f""{name}/w_K"")\n    from_weight_array(tf_attn.w_V, d, f""{name}/w_V"")\n    from_weight_array(tf_attn.w_O, d, f""{name}/w_O"")\n\n    if hasattr(tf_attn, \'rpr_key\'):\n        tf_attn.rpr_key.set_weights([d[f""{name}/rpr_key""]])\n        tf_attn.rpr_value.set_weights([d[f""{name}/rpr_value""]])\n\n\ndef to_encoder_array(tf_encoder: TransformerEncoder, name: str) -> Dict:\n    """"""Convert a `TransformerEncoder` layer to an set of numpy arrays\n\n    :param tf_encoder: A `TransformerEncoder` layer\n    :param name: The layer name\n    :return: A Dict of arrays by key\n    """"""\n    d = {}\n    d.update(to_weight_array(tf_encoder.ln1, f""{name}/ln1""))\n    d.update(to_weight_array(tf_encoder.ln2, f""{name}/ln2""))\n    d.update(to_attn_array(tf_encoder.self_attn, f""{name}/attn""))\n    d.update(to_ffn_array(tf_encoder.ffn, f""{name}/ffn""))\n    return d\n\n\ndef from_encoder_array(tf_encoder: TransformerEncoder, d: Dict, name: str):\n    """"""Restore a `TransformerEncoder` layer from a set of numpy arrays\n\n    :param tf_encoder: A `TransformerEncoder` layer\n    :param d: A Dict of arrays by key\n    :param name: The layer name\n    :return: None\n    """"""\n    from_weight_array(tf_encoder.ln1, d, f""{name}/ln1"")\n    from_weight_array(tf_encoder.ln2, d, f""{name}/ln2"")\n    from_attn_array(tf_encoder.self_attn, d, f""{name}/attn"")\n    from_ffn_array(tf_encoder.ffn, d, f""{name}/ffn"")\n\n\ndef to_embed_array(tf_embed: tf.keras.layers.Layer, name: str) -> Dict:\n    """"""Convert positional embedding to a `weights` array, if it\'s learned positional embedding,\n    save the pos_weight as well.\n\n    :param tf_embed: An embedding module\n    :param name: A layer name\n    :return: A Dict containing the embedding `weights`\n    """"""\n    d = {}\n\n\n    if hasattr(tf_embed, \'pos\'):\n        pos_weights = tf.keras.backend.get_value(tf_embed.pos)\n        d.update({f""{name}/pos_weights"": pos_weights})\n\n    if hasattr(tf_embed, \'bias\'):\n        bias = tf.keras.backend.get_value(tf_embed.bias)\n        d.update({f""{name}/bias"": bias.squeeze()})\n\n    # Note: we override the Keras function forcing a single value not a list\n    # this function could break if we used get_weights() since we have broken that\n    # however, we\'d like to fix that so using the raw value feels like the lesser of 2 evils\n    weights = tf.keras.backend.get_value(tf_embed.W)\n    d.update({f""{name}/weights"": weights})\n    return d\n\n\ndef from_embed_array(tf_embed: tf.keras.layers.Layer, d: Dict, name: str, bias=None):\n    """"""Restore a simple lookup table embedding from a `weights` array\n\n    :param tf_embed: An embedding module\n    :param d: A Dict containing a `weights` array to restore\n    :param name: name of the layer\n    :return: None\n    """"""\n    weights = [d[f""{name}/weights""]]\n    if hasattr(tf_embed, \'pos\'):\n        pos_weights = d[f""{name}/pos_weights""]\n        weights = [pos_weights] + weights\n        if hasattr(tf_embed, \'bias\') and bias is not None:\n            weights = weights + [bias.reshape(1, -1)]\n\n    tf_embed.set_weights(weights)\n\n\ndef to_encoder_stack_array(\n    tf_encoder_stack: TransformerEncoderStack, name: str = ""TransformerEncoderStack""\n) -> Dict:\n    """"""Convert a `TransformerEncoderStack` to a set of weigths\n\n    :param tf_encoder_stack: A transformer encoder stack\n    :param name: A name\n    :return: A Dict containing a set of weights\n    """"""\n    d = {}\n    if isinstance(tf_encoder_stack.ln, tf.keras.layers.LayerNormalization):\n        d.update(to_weight_array(tf_encoder_stack.ln, f""{name}/ln""))\n    for i, enc_tf in enumerate(tf_encoder_stack.encoders):\n        d.update(to_encoder_array(enc_tf, f""{name}/{i}""))\n    return d\n\n\n\ndef from_encoder_stack_array(tf_encoder_stack: TransformerEncoderStack, d: Dict, name: str = ""TransformerEncoderStack""):\n    """"""Restore weights from a `TransformerEncoderStack`\n\n    :param tf_encoder_stack: A transformer encoder stack\n    :param d: A Dict containing sets of arrays\n    :param name: A name for this primitive\n    :return: None\n    """"""\n    if isinstance(tf_encoder_stack.ln, tf.keras.layers.LayerNormalization):\n        from_weight_array(tf_encoder_stack.ln, d, f""{name}/ln"")\n    for i, enc_tf in enumerate(tf_encoder_stack.encoders):\n        from_encoder_array(enc_tf, d, f""{name}/{i}"")\n\n\ndef to_tlm_array(tf_tlm: tf.keras.layers.Layer, embeddings_keys: List[str] = None, name: str = ""TLM"") -> Dict:\n    """"""Convert a Transformer LM-type module to a set of weights in a Dict\n\n    :param pytorch_tlm: A Transformer LM-type module\n    :param embeddings_keys: A key to get the embeddings from, defaults to `None` in which case, gets all keys\n    :param name: A name for this TLM\n    :return: A Dict containing all the keys to restore from Embeddings and the TransformerEncoderStack\n    """"""\n    d = {}\n    transformer = tf_tlm.transformer if hasattr(tf_tlm, \'transformer\') else tf_tlm.generator\n    d.update(to_encoder_stack_array(transformer, name=f""{name}/TransformerEncoderStack""))\n    keys_to_write = embeddings_keys if embeddings_keys else list(tf_tlm.embeddings.keys())\n\n    for embeddings_key in keys_to_write:\n        d.update(to_embed_array(tf_tlm.embeddings[embeddings_key], name=f""{name}/Embeddings/{embeddings_key}""))\n\n    if hasattr(tf_tlm.embeddings.reduction, \'ln\'):\n        d.update(to_weight_array(tf_tlm.embeddings.reduction.ln, name=f""{name}/Embeddings/reduction/ln""))\n    return d\n\n\ndef from_tlm_array(tf_tlm: tf.keras.layers.Layer, d: Dict, embeddings_keys: List[str] = None, name: str = ""TLM""):\n    """"""Restore a TLM-like model (possibly a `Model` for fine-tuning)\n\n    We just populate the `TransformerEncoderStack` and the embeddings from weights, all other values remain\n    uninitialized\n\n    :param tf_tlm: A TLM-like model\n    :param d: A Dict of weights to restore for each layer\n    :param embeddings_keys: Name of the embeddings to restore, defaults to `None` in which case restore all embeddings\n    :param name: A name for this primitive\n    :return:\n    """"""\n    transformer = tf_tlm.transformer if hasattr(tf_tlm, \'transformer\') else tf_tlm.generator\n    from_encoder_stack_array(transformer, d, name=f""{name}/TransformerEncoderStack"")\n\n    keys_to_restore = embeddings_keys if embeddings_keys else list(tf_tlm.embeddings.keys())\n    for embeddings_key in keys_to_restore:\n        # If we get this class in we have to monkey patch the embeddings so we can add the bias\n        if isinstance(tf_tlm.embeddings[embeddings_key], LearnedPositionalLookupTableEmbeddingsWithBias):\n            bias = d[f""{name}/Embeddings/tt/weights""][0]\n            from_embed_array(tf_tlm.embeddings[embeddings_key], d, f""{name}/Embeddings/{embeddings_key}"", bias)\n        else:\n            from_embed_array(tf_tlm.embeddings[embeddings_key], d, f""{name}/Embeddings/{embeddings_key}"")\n    if hasattr(tf_tlm.embeddings.reduction, \'ln\'):\n        from_weight_array(tf_tlm.embeddings.reduction.ln, d, f""{name}/Embeddings/reduction/ln"")\n\n\ndef save_tlm_npz(tf_tlm: tf.keras.layers.Layer, npz: str, embeddings_keys: List[str] = None, name: str = ""TLM"", verbose: bool = False):\n    """"""Save a TLM to an NPZ file\n\n    :param pytorch_tlm: A Transformer LM-type module\n    :param npz: A file to save\n    :param embeddings_keys: A key to get embeddings from.  Defaults to `None`, in which case, all embeddings are written\n    :param name: A name for this TLM\n    :param verbose: whether output\n    :return: None\n    """"""\n    d = to_tlm_array(tf_tlm, embeddings_keys, name)\n    if verbose:\n        print(d.keys())\n    np.savez(npz, **d)\n\n\ndef load_tlm_npz(tf_tlm: tf.keras.layers.Layer, npz: str, embeddings_key: str = \'x\', name: str = ""TLM""):\n    """"""Restore a TLM-like model (possibly a `Model` for fine-tuning\n\n    We just populate the `TransformerEncoderStack` and the embeddings from weights, all other values remain\n    uninitialized\n\n    :param tf_tlm: A TLM-like model\n    :param npz: A file to restore the weights from\n    :param embeddings_key: The name of the embeddings to restore, defaults to `x`\n    :param name: A name for this primitive\n    :return:\n    """"""\n    d = np.load(npz)\n    from_tlm_array(tf_tlm, d, embeddings_key, name)\n'"
baseline/tf/classify/training/__init__.py,0,b'from eight_mile.utils import get_version\nimport tensorflow as tf\nif not tf.executing_eagerly():\n\n    from baseline.tf.classify.training.datasets import *\n    from baseline.tf.classify.training.feed import *\nelse:\n    from baseline.tf.classify.training.eager import *\n    from baseline.tf.classify.training.distributed import *\n'
baseline/tf/classify/training/datasets.py,0,"b'import six\nimport os\nimport time\nimport logging\nimport tensorflow as tf\n\nfrom eight_mile.confusion import ConfusionMatrix\nfrom eight_mile.utils import listify\nfrom eight_mile.tf.optz import optimizer\n\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tfy import _add_ema, TRAIN_FLAG, SET_TRAIN_FLAG\nfrom baseline.tf.classify.training.utils import to_tensors\nfrom baseline.progress import create_progress_bar\nfrom baseline.train import EpochReportingTrainer, create_trainer, register_trainer, register_training_func\nfrom baseline.utils import verbose_output\nfrom baseline.model import create_model_for\nimport numpy as np\n\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\nlog = logging.getLogger(\'baseline.timing\')\n\n\n@register_training_func(\'classify\')\ndef fit_datasets(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train a classifier using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = kwargs.get(\'verbose\', {\'console\': kwargs.get(\'verbose_console\', False), \'file\': kwargs.get(\'verbose_file\', None)})\n    epochs = int(kwargs.get(\'epochs\', 20))\n    model_file = get_model_file(\'classify\', \'tf\', kwargs.get(\'basedir\'))\n\n    batchsz = kwargs[\'batchsz\']\n    lengths_key = model_params.get(\'lengths_key\')\n\n    ## First, make tf.datasets for ts, vs and es\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md\n    # effective_batch_sz = args.batchsz*args.gpus\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, lengths_key))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.repeat(epochs + 1)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, lengths_key))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.repeat(epochs + 1)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n\n    iter = tf.compat.v1.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_dataset),\n                                                     tf.compat.v1.data.get_output_shapes(train_dataset))\n\n    features, y = iter.get_next()\n    # Add features to the model params\n    model_params.update(features)\n    model_params[\'y\'] = tf.one_hot(tf.reshape(y, [-1]), len(model_params[\'labels\']))\n    # create the initialisation operations\n    train_init_op = iter.make_initializer(train_dataset)\n    valid_init_op = iter.make_initializer(valid_dataset)\n\n\n    ema = True if kwargs.get(\'ema_decay\') is not None else False\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    TRAIN_FLAG()\n    trainer = create_trainer(model_params, **kwargs)\n    last_improved = 0\n\n    for epoch in range(epochs):\n        trainer.sess.run(train_init_op)\n        trainer.train(ts, reporting_fns)\n        trainer.sess.run(valid_init_op)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, lengths_key))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.repeat(epochs + 1)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        test_init_op = iter.make_initializer(test_dataset)\n        trainer.sess.run(test_init_op)\n        trainer.test(es, reporting_fns, phase=\'Test\', verbose=verbose)\n'"
baseline/tf/classify/training/distributed.py,0,"b'import six\nimport os\nimport time\nimport logging\nimport tensorflow as tf\n\nfrom eight_mile.confusion import ConfusionMatrix\nfrom baseline.progress import create_progress_bar\nfrom eight_mile.utils import listify, get_version\nfrom eight_mile.tf.layers import get_shape_as_list\nfrom eight_mile.tf.optz import *\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tfy import SET_TRAIN_FLAG\nfrom baseline.tf.classify.training.utils import to_tensors\nfrom baseline.train import EpochReportingTrainer, register_trainer, register_training_func\nfrom baseline.utils import verbose_output\nfrom baseline.model import create_model_for\nimport numpy as np\n\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\nlog = logging.getLogger(\'baseline.timing\')\n\ndef loss(model, x, y):\n    y_ = model(x)\n    return tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n\n\n@register_trainer(task=\'classify\')\nclass ClassifyTrainerDistributedTf(EpochReportingTrainer):\n    """"""A Trainer to use if using TF2.0 in distributed mode\n    """"""\n    def __init__(self, model_params, **kwargs):\n        """"""Create a Trainer, and give it the parameters needed to instantiate the model\n\n        :param model_params: The model parameters\n        :param kwargs: See below\n\n        :Keyword Arguments:\n\n          * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n          * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n          * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n          * *optim* (`str`) -- The name of the optimizer we are using\n          * *lr* (`float`) -- The learning rate we are using\n          * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n          * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n          * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n          * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n        """"""\n        super().__init__()\n\n        self.gpus = int(kwargs.get(\'gpus\', 1))\n        if type(model_params) is dict:\n            self.model = create_model_for(\'classify\', **model_params)\n        else:\n            self.model = model_params\n\n        self.optimizer = EagerOptimizer(loss, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', six.MAXSIZE)\n        self._checkpoint = tf.train.Checkpoint(optimizer=self.optimizer.optimizer, model=self.model)\n        checkpoint_dir = \'{}-{}\'.format(""./tf-classify"", os.getpid())\n\n        self.checkpoint_manager = tf.train.CheckpointManager(self._checkpoint,\n                                                             directory=checkpoint_dir,\n                                                             max_to_keep=5)\n        devices = [\'/device:GPU:{}\'.format(i) for i in range(self.gpus)]\n        self.strategy = tf.distribute.MirroredStrategy(devices)\n\n\n    def _train(self, loader, steps=0, **kwargs):\n        """"""Train an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.  We do use a `feed_dict` for passing the `TRAIN_FLAG` in either case\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n         * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n         * *reporting_fns* (`list`) A list of reporting hooks to use\n\n        :return: Metrics\n        """"""\n        strategy = self.strategy\n        num_replicas = strategy.num_replicas_in_sync\n\n        def _replicated_train_step(inputs):\n            """"""Replicated training step.""""""\n            features, y = inputs\n            per_replica_loss = self.optimizer.update(self.model, features, y, num_replicas)\n            per_replica_batchsz = tf.cast(get_shape_as_list(y)[0], tf.float32)\n            per_replica_report_loss = per_replica_loss * per_replica_batchsz\n            return per_replica_report_loss, per_replica_batchsz\n\n        with strategy.scope():\n\n            SET_TRAIN_FLAG(True)\n            reporting_fns = kwargs.get(\'reporting_fns\', [])\n            epoch_loss = tf.Variable(0.0)\n            epoch_div = tf.Variable(0.0)\n            nstep_loss = tf.Variable(0.0)\n            nstep_div = tf.Variable(0.0)\n            self.nstep_start = time.time()\n\n            @tf.function\n            def _distributed_train_step(inputs):\n                per_replica_loss, per_replica_batchsz = strategy.experimental_run_v2(_replicated_train_step, args=(inputs,))\n                return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None), strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_batchsz, axis=None)\n\n            train_iter = iter(loader)\n            for i in range(steps):\n                step_loss, step_batchsz = _distributed_train_step(next(train_iter))\n                epoch_loss.assign_add(step_loss)\n                nstep_loss.assign_add(step_loss)\n                epoch_div.assign_add(step_batchsz)\n                nstep_div.assign_add(step_batchsz)\n                step = self.optimizer.global_step.numpy() + 1\n\n                if step % self.nsteps == 0:\n                    metrics = self.calc_metrics(nstep_loss.numpy(), nstep_div.numpy())\n                    self.report(\n                        step, metrics, self.nstep_start,\n                        \'Train\', \'STEP\', reporting_fns, self.nsteps\n                    )\n                    nstep_loss.assign(0.0)\n                    nstep_div.assign(0.0)\n                    self.nstep_start = time.time()\n\n            epoch_loss = epoch_loss.numpy()\n            epoch_div = epoch_div.numpy()\n\n            metrics = self.calc_metrics(epoch_loss, epoch_div)\n            return metrics\n\n    def _test(self, loader, steps=0, **kwargs):\n        """"""Test an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n          * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n          * *reporting_fns* (`list`) A list of reporting hooks to use\n          * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n\n        :return: Metrics\n        """"""\n\n        strategy = self.strategy\n        cm = ConfusionMatrix(self.model.labels)\n        nc = len(self.model.labels)\n\n        def _replica_test_step(inputs):\n            features, y = inputs\n            y = tf.cast(y, tf.int64)\n            per_replica_cm = tf.zeros((nc, nc), dtype=tf.int64)\n            logits = self.model(features)\n            y_ = tf.argmax(logits, axis=1, output_type=tf.int64)\n            indices = tf.stack((y, y_), axis=-1)\n            dense_shape = tf.cast(tf.shape(per_replica_cm), tf.int64)\n            sparse_ups = tf.SparseTensor(indices=indices, values=tf.ones(get_shape_as_list(indices)[0], dtype=tf.int64),\n                                         dense_shape=dense_shape)\n            per_replica_cm = tf.compat.v1.sparse_add(per_replica_cm, sparse_ups)\n            per_replica_loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n            per_replica_batchsz = tf.cast(get_shape_as_list(y)[0], tf.float32)\n            per_replica_report_loss = per_replica_loss * per_replica_batchsz\n            return per_replica_report_loss, per_replica_batchsz, per_replica_cm\n\n        @tf.function\n        def _distributed_test_step(inputs):\n            per_replica_loss, per_replica_batchsz, per_replica_cm = strategy.experimental_run_v2(_replica_test_step, args=(inputs,))\n            step_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n            step_batchsz = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_batchsz, axis=None)\n            step_cm = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_cm, axis=None)\n            return step_loss, step_batchsz, step_cm\n\n        with strategy.scope():\n\n            total_loss = tf.Variable(0.0)\n            total_norm = tf.Variable(0.0)\n            verbose = kwargs.get(""verbose"", None)\n\n            SET_TRAIN_FLAG(False)\n            test_iter = iter(loader)\n\n            for i in range(steps):\n                step_loss, step_batchsz, distributed_cm = _distributed_test_step(next(test_iter))\n                total_loss.assign_add(step_loss)\n                total_norm.assign_add(step_batchsz)\n                cm._cm += distributed_cm.numpy()\n\n            metrics = cm.get_all_metrics()\n            total_loss = total_loss.numpy()\n            total_norm = total_norm.numpy()\n            metrics[\'avg_loss\'] = total_loss / float(total_norm)\n            verbose_output(verbose, cm)\n\n            return metrics\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        self.checkpoint_manager.save()\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        print(self._checkpoint.restore(self.checkpoint_manager.latest_checkpoint))\n\n    def distribute(self, dataset):\n        return self.strategy.experimental_distribute_dataset(dataset)\n\n\n@register_training_func(\'classify\', name=\'distributed\')\ndef fit_eager_distributed(model_params, ts, vs, es=None, **kwargs):\n\n    """"""\n    Train a classifier using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = kwargs.get(\'verbose\', {\'console\': kwargs.get(\'verbose_console\', False), \'file\': kwargs.get(\'verbose_file\', None)})\n    epochs = int(kwargs.get(\'epochs\', 20))\n    model_file = get_model_file(\'classify\', \'tf\', kwargs.get(\'basedir\'))\n\n    batchsz = kwargs[\'batchsz\']\n    lengths_key = model_params.get(\'lengths_key\')\n\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, lengths_key))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=True)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, lengths_key))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=True)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n    SET_TRAIN_FLAG(True)\n    trainer = ClassifyTrainerDistributedTf(model_params, **kwargs)\n    train_dataset = trainer.distribute(train_dataset)\n    valid_dataset = trainer.distribute(valid_dataset)\n    \n    last_improved = 0\n\n    for epoch in range(epochs):\n\n        trainer.train(train_dataset, reporting_fns, steps=len(ts))\n        test_metrics = trainer.test(valid_dataset, reporting_fns, phase=\'Valid\', steps=len(vs))\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        trainer.strategy = tf.distribute.OneDeviceStrategy(\'/device:GPU:0\')\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, lengths_key))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        test_dataset = trainer.distribute(test_dataset)\n        trainer.test(test_dataset, reporting_fns, phase=\'Test\', verbose=verbose, steps=len(es))\n'"
baseline/tf/classify/training/eager.py,0,"b'import six\nimport os\nimport time\nimport logging\nimport tensorflow as tf\n\nfrom eight_mile.confusion import ConfusionMatrix\nfrom baseline.progress import create_progress_bar\nfrom eight_mile.utils import listify, get_version\nfrom eight_mile.tf.layers import get_shape_as_list\nfrom eight_mile.tf.optz import *\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tfy import SET_TRAIN_FLAG\nfrom baseline.tf.classify.training.utils import to_tensors\nfrom baseline.train import EpochReportingTrainer, register_trainer, register_training_func\nfrom baseline.utils import verbose_output\nfrom baseline.model import create_model_for\nimport numpy as np\n\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\nlog = logging.getLogger(\'baseline.timing\')\n\nTF_VERSION = get_version(tf)\nif TF_VERSION < 2:\n    tf.enable_eager_execution()\n\n\ndef loss(model, x, y):\n    y_ = model(x)\n    return tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n\n\n@register_trainer(task=\'classify\')\nclass ClassifyTrainerEagerTf(EpochReportingTrainer):\n    """"""A Trainer to use if using TF2.0\n    """"""\n    def __init__(self, model_params, **kwargs):\n        """"""Create a Trainer, and give it the parameters needed to instantiate the model\n\n        :param model_params: The model parameters\n        :param kwargs: See below\n\n        :Keyword Arguments:\n\n          * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n          * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n          * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n          * *optim* (`str`) -- The name of the optimizer we are using\n          * *lr* (`float`) -- The learning rate we are using\n          * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n          * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n          * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n          * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n        """"""\n        super().__init__()\n\n        if type(model_params) is dict:\n            self.model = create_model_for(\'classify\', **model_params)\n        else:\n            self.model = model_params\n\n        self.optimizer = EagerOptimizer(loss, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', six.MAXSIZE)\n        self._checkpoint = tf.train.Checkpoint(optimizer=self.optimizer.optimizer, model=self.model)\n        checkpoint_dir = \'{}-{}\'.format(""./tf-classify"", os.getpid())\n\n        self.checkpoint_manager = tf.train.CheckpointManager(self._checkpoint,\n                                                             directory=checkpoint_dir,\n                                                             max_to_keep=5)\n\n    def _train(self, loader, steps=0, **kwargs):\n        """"""Train an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.  We do use a `feed_dict` for passing the `TRAIN_FLAG` in either case\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n         * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n         * *reporting_fns* (`list`) A list of reporting hooks to use\n\n        :return: Metrics\n        """"""\n\n        SET_TRAIN_FLAG(True)\n        reporting_fns = kwargs.get(\'reporting_fns\', [])\n        pg = create_progress_bar(steps)\n        epoch_loss = tf.Variable(0.0)\n        epoch_div = tf.Variable(0, dtype=tf.int32)\n        nstep_loss = tf.Variable(0.0)\n        nstep_div = tf.Variable(0, dtype=tf.int32)\n        self.nstep_start = time.time()\n\n        @tf.function\n        def _train_step(inputs):\n            """"""Replicated training step.""""""\n            features, y = inputs\n            loss = self.optimizer.update(self.model, features, y)\n            batchsz = get_shape_as_list(y)[0]\n            report_loss = loss * batchsz\n            return report_loss, batchsz\n\n        for inputs in pg(loader):\n            step_report_loss, step_batchsz = _train_step(inputs)\n            epoch_loss.assign_add(step_report_loss)\n            nstep_loss.assign_add(step_report_loss)\n            epoch_div.assign_add(step_batchsz)\n            nstep_div.assign_add(step_batchsz)\n            step = self.optimizer.global_step.numpy() + 1\n\n            if step % self.nsteps == 0:\n                metrics = self.calc_metrics(nstep_loss.numpy(), nstep_div.numpy())\n                self.report(\n                    step, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                nstep_loss.assign(0.0)\n                nstep_div.assign(0)\n                self.nstep_start = time.time()\n\n        epoch_loss = epoch_loss.numpy()\n        epoch_div = epoch_div.numpy()\n        metrics = self.calc_metrics(epoch_loss, epoch_div)\n        return metrics\n\n    def _test(self, loader, steps=0, **kwargs):\n        """"""Test an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n          * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n          * *reporting_fns* (`list`) A list of reporting hooks to use\n          * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n\n        :return: Metrics\n        """"""\n\n        cm = ConfusionMatrix(self.model.labels)\n        total_loss = 0\n        total_norm = 0\n        verbose = kwargs.get(""verbose"", None)\n\n        pg = create_progress_bar(steps)\n\n        SET_TRAIN_FLAG(False)\n        for features, y in pg(loader):\n            logits = self.model(features)\n            y_ = tf.argmax(logits, axis=1, output_type=tf.int32)\n            cm.add_batch(y, y_)\n            lossv = tf.compat.v1.losses.sparse_softmax_cross_entropy(labels=y, logits=logits).numpy()\n            batchsz = int(y.shape[0])\n            assert len(y_) == batchsz\n            total_loss += lossv * batchsz\n            total_norm += batchsz\n            cm.add_batch(y, y_)\n\n        metrics = cm.get_all_metrics()\n        metrics[\'avg_loss\'] = total_loss / float(total_norm)\n        verbose_output(verbose, cm)\n\n        return metrics\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        self.checkpoint_manager.save()\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        print(self._checkpoint.restore(self.checkpoint_manager.latest_checkpoint))\n\n\n@register_training_func(\'classify\', name=\'default\')\ndef fit_eager(model_params, ts, vs, es=None, **kwargs):\n\n    """"""\n    Train a classifier using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = kwargs.get(\'verbose\', {\'console\': kwargs.get(\'verbose_console\', False), \'file\': kwargs.get(\'verbose_file\', None)})\n    epochs = int(kwargs.get(\'epochs\', 20))\n    model_file = get_model_file(\'classify\', \'tf\', kwargs.get(\'basedir\'))\n\n    batchsz = kwargs[\'batchsz\']\n    lengths_key = model_params.get(\'lengths_key\')\n\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, lengths_key))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, lengths_key))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n    SET_TRAIN_FLAG(True)\n    trainer = ClassifyTrainerEagerTf(model_params, **kwargs)\n    last_improved = 0\n\n    for epoch in range(epochs):\n\n        trainer.train(train_dataset, reporting_fns, steps=len(ts))\n        test_metrics = trainer.test(valid_dataset, reporting_fns, phase=\'Valid\', steps=len(vs))\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, lengths_key))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        trainer.test(test_dataset, reporting_fns, phase=\'Test\', verbose=verbose, steps=len(es))\n'"
baseline/tf/classify/training/feed.py,0,"b'import six\nimport os\nimport time\nimport logging\nimport tensorflow as tf\n\nfrom eight_mile.confusion import ConfusionMatrix\nfrom eight_mile.utils import listify\nfrom eight_mile.tf.optz import optimizer\n\nfrom baseline.progress import create_progress_bar\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tfy import _add_ema, TRAIN_FLAG, SET_TRAIN_FLAG\nfrom baseline.tf.classify.training.utils import to_tensors\nfrom baseline.train import EpochReportingTrainer, create_trainer, register_trainer, register_training_func\nfrom baseline.utils import verbose_output\nfrom baseline.model import create_model_for\nimport numpy as np\n\n\n@register_training_func(\'classify\', \'feed_dict\')\ndef fit(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train a classifier using TensorFlow with a `feed_dict`.  This\n    is the previous default behavior for training.  To use this, you need to pass\n    `fit_func: feed_dict` in your MEAD config\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = kwargs.get(\'verbose\', {\'console\': kwargs.get(\'verbose_console\', False), \'file\': kwargs.get(\'verbose_file\', None)})\n    epochs = int(kwargs.get(\'epochs\', 20))\n    model_file = get_model_file(\'classify\', \'tf\', kwargs.get(\'basedir\'))\n    ema = True if kwargs.get(\'ema_decay\') is not None else False\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    TRAIN_FLAG()\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n\n    for epoch in range(epochs):\n\n        trainer.train(ts, reporting_fns, dataset=False)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\', dataset=False)\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        trainer.test(es, reporting_fns, phase=\'Test\', verbose=verbose, dataset=False)\n'"
baseline/tf/classify/training/utils.py,0,"b'import six\nimport os\nimport time\nimport logging\nimport tensorflow as tf\n\nfrom eight_mile.confusion import ConfusionMatrix\nfrom eight_mile.utils import listify\nfrom eight_mile.tf.layers import reload_checkpoint\nfrom eight_mile.tf.optz import optimizer\n\nfrom baseline.progress import create_progress_bar\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tfy import _add_ema, TRAIN_FLAG, SET_TRAIN_FLAG\nfrom baseline.train import EpochReportingTrainer, create_trainer, register_trainer, register_training_func\nfrom baseline.utils import verbose_output\nfrom baseline.model import create_model_for\n\nimport numpy as np\n\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\nlog = logging.getLogger(\'baseline.timing\')\n\n\n\ndef to_tensors(ts, lengths_key):\n    """"""Convert a data feed into a tuple of `features` (`dict`) and `y` values\n\n    This method is required to produce `tf.dataset`s from the input data feed\n\n    :param ts: The data feed to convert\n    :return: A `tuple` of `features` and `y` (labels)\n    """"""\n    keys = ts[0].keys()\n    features = dict((k, []) for k in keys)\n    for sample in ts:\n        for k in features.keys():\n            # add each sample\n            for s in sample[k]:\n                features[k].append(s)\n\n    features = dict((k, np.stack(v)) for k, v in features.items())\n    features[\'lengths\'] = features[lengths_key]\n    del features[lengths_key]\n    y = features.pop(\'y\')\n    return features, y\n\n\n\ndef _report(step, metrics, start, phase, tt, reporting_fns, steps=1):\n    """"""Make a report (both metric and timing).\n\n    :param step: `int` The step number of this report (epoch or nstep number).\n    :param metrics: `dict` The metrics to report.\n    :param start: `int` The starting time of this segment.\n    :param phase: `str` The phase type. {\'Train\', \'Valid\', \'Test\'}\n    :param tt: `str` The tick type. {\'STEP\', \'EPOCH\'}\n    :param reporting_fns: `List[Callable]` The list of reporting functions to call.\n    :param steps: `int` The number of steps in this segment, used to normalize the time.\n    """"""\n    elapsed = time.time() - start\n    for reporting in reporting_fns:\n        reporting(metrics, step, phase, tt)\n    log.debug({\n        \'tick_type\': tt, \'tick\': step, \'phase\': phase,\n        \'time\': elapsed / float(steps),\n        \'step/sec\': steps / float(elapsed)\n    })\n\n\n@register_trainer(task=\'classify\', name=\'default\')\nclass ClassifyTrainerTf(EpochReportingTrainer):\n    """"""A non-eager mode Trainer for classification\n\n    The trainer can run in 2 modes: `dataset` and `feed_dict`.  When the former, the graph is assumed to\n    be connected by features attached to the input so the `feed_dict` will only be used to pass dropout information.\n\n    When the latter, we will use the baseline DataFeed to read the object into the `feed_dict`\n    """"""\n    def __init__(self, model_params, **kwargs):\n        """"""Create a Trainer, and give it the parameters needed to instantiate the model\n\n        :param model_params: The model parameters\n        :param kwargs: See below\n\n        :Keyword Arguments:\n\n          * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n          * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n          * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n          * *optim* (`str`) -- The name of the optimizer we are using\n          * *lr* (`float`) -- The learning rate we are using\n          * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n          * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n          * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n          * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n        """"""\n        super().__init__()\n        if type(model_params) is dict:\n            self.model = create_model_for(\'classify\', **model_params)\n        else:\n            self.model = model_params\n        self.sess = self.model.sess\n        self.loss = self.model.create_loss()\n        self.test_loss = self.model.create_test_loss()\n        self.global_step, train_op = optimizer(self.loss, colocate_gradients_with_ops=True, variables=self.model.trainable_variables, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', six.MAXSIZE)\n        decay = kwargs.get(\'ema_decay\', None)\n        if decay is not None:\n            self.ema = True\n            ema_op, self.ema_load, self.ema_restore = _add_ema(self.model, float(decay))\n            with tf.compat.v1.control_dependencies([ema_op]):\n                self.train_op = tf.identity(train_op)\n        else:\n            self.ema = False\n            self.train_op = train_op\n\n        tables = tf.compat.v1.tables_initializer()\n        self.model.sess.run(tables)\n        self.model.sess.run(tf.compat.v1.global_variables_initializer())\n        self.model.set_saver(tf.compat.v1.train.Saver())\n        checkpoint = kwargs.get(\'checkpoint\')\n        if checkpoint is not None:\n            skip_blocks = kwargs.get(\'blocks_to_skip\', [\'OptimizeLoss\'])\n            reload_checkpoint(self.model.sess, checkpoint, skip_blocks)\n\n    @staticmethod\n    def _get_batchsz(batch_dict):\n        return len(batch_dict[\'y\'])\n\n    def _train(self, loader, dataset=True, **kwargs):\n        """"""Train an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.  We do use a `feed_dict` for passing the `TRAIN_FLAG` in either case\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n         * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n         * *reporting_fns* (`list`) A list of reporting hooks to use\n\n        :return: Metrics\n        """"""\n        if self.ema:\n            self.sess.run(self.ema_restore)\n\n        reporting_fns = kwargs.get(\'reporting_fns\', [])\n        epoch_loss = 0\n        epoch_div = 0\n        steps = len(loader)\n        pg = create_progress_bar(steps)\n        for batch_dict in pg(loader):\n            if dataset:\n                _, step, lossv = self.sess.run([self.train_op, self.global_step, self.loss],\n                                               feed_dict={TRAIN_FLAG(): 1})\n            else:\n                feed_dict = self.model.make_input(batch_dict, True)\n                _, step, lossv = self.sess.run([self.train_op, self.global_step, self.loss], feed_dict=feed_dict)\n\n            batchsz = self._get_batchsz(batch_dict)\n            report_lossv = lossv * batchsz\n            epoch_loss += report_lossv\n            epoch_div += batchsz\n            self.nstep_agg += report_lossv\n            self.nstep_div += batchsz\n\n            if (step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = self.calc_metrics(epoch_loss, epoch_div)\n        return metrics\n\n    def _test(self, loader, **kwargs):\n        """"""Test an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n          * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n          * *reporting_fns* (`list`) A list of reporting hooks to use\n          * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n\n        :return: Metrics\n        """"""\n        if self.ema:\n            self.sess.run(self.ema_load)\n\n        use_dataset = kwargs.get(\'dataset\', True)\n\n        cm = ConfusionMatrix(self.model.labels)\n        steps = len(loader)\n        total_loss = 0\n        total_norm = 0\n        verbose = kwargs.get(""verbose"", None)\n\n        pg = create_progress_bar(steps)\n        for i, batch_dict in enumerate(pg(loader)):\n            y = batch_dict[\'y\']\n            if use_dataset:\n                guess, lossv = self.sess.run([self.model.best, self.test_loss])\n            else:\n                feed_dict = self.model.make_input(batch_dict, False)\n                guess, lossv = self.sess.run([self.model.best, self.test_loss], feed_dict=feed_dict)\n\n            batchsz = len(guess)\n            total_loss += lossv * batchsz\n            total_norm += batchsz\n            cm.add_batch(y, guess)\n\n        metrics = cm.get_all_metrics()\n        metrics[\'avg_loss\'] = total_loss / float(total_norm)\n        verbose_output(verbose, cm)\n\n        return metrics\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        checkpoint_dir = \'{}-{}\'.format(""./tf-classify"", os.getpid())\n        self.model.saver.save(self.sess,\n                              os.path.join(checkpoint_dir, \'classify\'),\n                              global_step=self.global_step,\n                              write_meta_graph=False)\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        checkpoint_dir = \'{}-{}\'.format(""./tf-classify"", os.getpid())\n        latest = tf.train.latest_checkpoint(checkpoint_dir)\n        print(\'Reloading \' + latest)\n        self.model.saver.restore(self.model.sess, latest)\n\n'"
baseline/tf/lm/training/__init__.py,0,b'from eight_mile.utils import get_version\nimport tensorflow as tf\nif not tf.executing_eagerly():\n\n    from baseline.tf.lm.training.datasets import *\n    from baseline.tf.lm.training.feed import *\nelse:\n    from baseline.tf.lm.training.eager import *\n    from baseline.tf.lm.training.distributed import *\n'
baseline/tf/lm/training/datasets.py,0,"b'import tensorflow as tf\nfrom baseline.tf.tfy import TRAIN_FLAG\nfrom eight_mile.utils import listify\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.train import create_trainer, register_training_func\nfrom baseline.tf.lm.training.utils import to_tensors, SHUF_BUF_SZ, NUM_PREFETCH\n\n\n@register_training_func(\'lm\')\ndef fit_datasets(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an language model using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n\n    model_file = get_model_file(\'lm\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'avg_loss\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    batchsz = kwargs[\'batchsz\']\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    tgt_key = model_params.get(\'tgt_key\')\n\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.repeat(epochs + 1)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.repeat(epochs + 1)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    iter = tf.compat.v1.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_dataset),\n                                                     tf.compat.v1.data.get_output_shapes(train_dataset))\n\n    features, tgt = iter.get_next()\n    # Add features to the model params\n    model_params.update(features)\n    model_params.update({\'y\': tgt})\n\n    # create the initialization operations\n    train_init_op = iter.make_initializer(train_dataset)\n    valid_init_op = iter.make_initializer(valid_dataset)\n\n    TRAIN_FLAG()\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n\n    for epoch in range(epochs):\n        trainer.sess.run(train_init_op)\n        trainer.train(ts, reporting_fns)\n        trainer.sess.run(valid_init_op)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.repeat(epochs + 1)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        test_init_op = iter.make_initializer(test_dataset)\n        trainer.sess.run(test_init_op)\n        trainer.test(es, reporting_fns, phase=\'Test\')\n\n'"
baseline/tf/lm/training/distributed.py,0,"b'import os\nimport numpy as np\nimport time\nimport tensorflow as tf\nfrom eight_mile.utils import listify, get_version\nfrom eight_mile.tf.layers import SET_TRAIN_FLAG, get_shape_as_list\nfrom eight_mile.tf.optz import EagerOptimizer\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.model import create_model_for\nfrom baseline.train import register_training_func, Trainer\nfrom baseline.tf.lm.training.utils import to_tensors, SHUF_BUF_SZ, NUM_PREFETCH\n\n\ndef loss_with_state(model, h, x, y):\n    logits, h_out = model(x, h)\n    vsz = model.embeddings[model.tgt_key].get_vsz()\n    targets = tf.reshape(y, [-1])\n    bt_x_v = tf.nn.log_softmax(tf.reshape(logits, [-1, vsz]), axis=-1)\n    one_hots = tf.one_hot(targets, vsz)\n    example_loss = -tf.reduce_sum(one_hots * bt_x_v, axis=-1)\n    loss = tf.reduce_mean(example_loss)\n    return loss, h_out\n\ndef loss_without_state(model, x, y):\n    # Model will produce a null hidden state\n    logits = model(x, None)[0]\n    vsz = model.embeddings[model.tgt_key].get_vsz()\n    targets = tf.reshape(y, [-1])\n    bt_x_v = tf.nn.log_softmax(tf.reshape(logits, [-1, vsz]), axis=-1)\n    one_hots = tf.one_hot(targets, vsz)\n    example_loss = -tf.reduce_sum(one_hots * bt_x_v, axis=-1)\n    loss = tf.reduce_mean(example_loss)\n    return loss\n\n\nclass LanguageModelTrainerDistributedTf(Trainer):\n    """"""A Trainer for LM distributed eager training\n    """"""\n    def __init__(self, model_params, **kwargs):\n        super().__init__()\n        self.gpus = int(kwargs.get(\'gpus\', 1))\n\n        if type(model_params) is dict:\n            self.model = create_model_for(\'lm\', **model_params)\n        else:\n            self.model = model_params\n\n        loss_fn = loss_with_state if self.model.requires_state else loss_without_state\n        self.optimizer = EagerOptimizer(loss_fn, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n        self._checkpoint = tf.train.Checkpoint(optimizer=self.optimizer.optimizer, model=self.model)\n        checkpoint_dir = \'{}-{}\'.format(""./tf-lm"", os.getpid())\n\n        self.checkpoint_manager = tf.train.CheckpointManager(self._checkpoint,\n                                                             directory=checkpoint_dir,\n                                                             max_to_keep=5)\n\n        devices = [\'/device:GPU:{}\'.format(i) for i in range(self.gpus)]\n        self.strategy = tf.distribute.MirroredStrategy(devices)\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        self.checkpoint_manager.save()\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        print(self._checkpoint.restore(self.checkpoint_manager.latest_checkpoint))\n\n    @staticmethod\n    def _num_toks(y):\n        return tf.reduce_prod(get_shape_as_list(y))\n\n    def train(self, ts, reporting_fns, steps=0, dataset=True):\n        """"""Train by looping over the steps\n\n        For a `tf.dataset`-backed `fit_func`, we are using the previously wired `dataset`s\n        in the model (and `dataset` is `True`).  For `feed_dict`, we convert the ts samples\n        to `feed_dict`s and hand them in one-by-one\n\n        :param ts: The training set\n        :param reporting_fns: A list of reporting hooks\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        strategy = self.strategy\n\n        def _replicated_train_step_no_state(inputs):\n\n            features, y = inputs\n            per_replica_loss = self.optimizer.update(self.model, features, y)\n            per_replica_toks = self._num_toks(y)\n            per_replica_report_loss = per_replica_loss * tf.cast(per_replica_toks, tf.float32)\n            return per_replica_report_loss, per_replica_toks\n\n        def _replicated_train_step_with_state(inputs, hidden):\n            features, y = inputs\n            per_replica_loss, new_hidden = self.optimizer.update_with_hidden(self.model, hidden, features, y)\n            per_replica_toks = self._num_toks(y)\n            per_replica_report_loss = per_replica_loss * tf.cast(per_replica_toks, tf.float32)\n            return new_hidden, per_replica_report_loss, per_replica_toks\n\n        with strategy.scope():\n            train_iter = iter(ts)\n            SET_TRAIN_FLAG(True)\n            epoch_loss = tf.Variable(0.0)\n            epoch_div = tf.Variable(0, dtype=tf.int32)\n            nstep_loss = tf.Variable(0.0)\n            nstep_div = tf.Variable(0, dtype=tf.int32)\n            self.nstep_start = time.time()\n            start = time.time()\n\n            @tf.function\n            def _distributed_train_no_state(inputs):\n                per_replica_loss, per_replica_toks = strategy.experimental_run_v2(_replicated_train_step_no_state, args=(inputs,))\n                return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None), strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_toks, axis=None)\n\n\n            @tf.function\n            def _distributed_train_with_state(inputs, hidden):\n\n                h, per_replica_loss, per_replica_toks = strategy.experimental_run_v2(_replicated_train_step_with_state, args=(inputs, hidden,))\n                step_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n                step_toks = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_toks, axis=None)\n                return h, step_loss, step_toks\n            h = None\n            for i in range(steps):\n\n                inputs = next(train_iter)\n                if self.model.requires_state:\n                    h, step_loss, step_toks = _distributed_train_with_state(inputs, h)\n                else:\n                    step_loss, step_toks = _distributed_train_no_state(inputs)\n                epoch_loss.assign_add(step_loss)\n                nstep_loss.assign_add(step_loss)\n                epoch_div.assign_add(step_toks)\n                nstep_div.assign_add(step_toks)\n                step = self.optimizer.global_step.numpy() + 1\n                if step % self.nsteps == 0:\n                    metrics = self.calc_metrics(nstep_loss.numpy(), nstep_div.numpy())\n                    self.report(\n                        step, metrics, self.nstep_start,\n                        \'Train\', \'STEP\', reporting_fns, self.nsteps\n                    )\n                    nstep_loss.assign(0.0)\n                    nstep_div.assign(0)\n                    self.nstep_start = time.time()\n\n            epoch_loss = epoch_loss.numpy()\n            epoch_div = epoch_div.numpy()\n            metrics = self.calc_metrics(epoch_loss, epoch_div)\n            self.train_epochs += 1\n            self.report(\n                self.train_epochs, metrics, start,\n                \'Train\', \'EPOCH\', reporting_fns\n            )\n            return metrics\n\n    def calc_metrics(self, agg, norm):\n        metrics = super().calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\'])\n        return metrics\n\n    def test(self, vs, reporting_fns, phase, steps=0):\n        """"""Run an epoch of testing over the dataset\n\n        If we are using a `tf.dataset`-based `fit_func`, we will just\n        cycle the number of steps and let the `dataset` yield new batches.\n\n        If we are using `feed_dict`s, we convert each batch from the `DataFeed`\n        and pass that into TF as the `feed_dict`\n\n        :param vs: A validation set\n        :param reporting_fns: Reporting hooks\n        :param phase: The phase of evaluation (`Test`, `Valid`)\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        strategy = self.strategy\n\n        def _replicated_test_step_no_state(inputs):\n            features, y = inputs\n            per_replica_loss = loss_without_state(self.model, features, y)\n            per_replica_toks = self._num_toks(y)\n            per_replica_report_loss = per_replica_loss * tf.cast(per_replica_toks, tf.float32)\n            return per_replica_report_loss, per_replica_toks\n\n        def _replicated_test_step_with_state(inputs, hidden):\n            features, y = inputs\n            per_replica_loss, new_hidden = loss_with_state(self.model, hidden, features, y)\n            per_replica_toks = self._num_toks(y)\n            per_replica_report_loss = per_replica_loss * tf.cast(per_replica_toks, tf.float32)\n            return new_hidden, per_replica_report_loss, per_replica_toks\n\n        with strategy.scope():\n            SET_TRAIN_FLAG(False)\n            test_iter = iter(vs)\n            epoch_loss = tf.Variable(0.0)\n            epoch_div = tf.Variable(0, dtype=tf.int32)\n            self.nstep_start = time.time()\n            start = time.time()\n\n            @tf.function\n            def _distributed_test_no_state(inputs):\n                per_replica_loss, per_replica_toks = strategy.experimental_run_v2(_replicated_test_step_no_state, args=(inputs,))\n                return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None), strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_toks, axis=None)\n\n            @tf.function\n            def _distributed_test_with_state(inputs, hidden):\n\n                h, per_replica_loss, per_replica_toks = strategy.experimental_run_v2(_replicated_test_step_with_state, args=(inputs, hidden,))\n                step_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n                step_toks = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_toks, axis=None)\n                return h, step_loss, step_toks\n\n            epochs = 0\n            if phase == \'Valid\':\n                self.valid_epochs += 1\n                epochs = self.valid_epochs\n\n            h = None\n            for i in range(steps):\n                inputs = next(test_iter)\n                if self.model.requires_state:\n                    h, per_replica_loss, per_replica_toks = _distributed_test_with_state(inputs, h)\n                else:\n                    per_replica_loss, per_replica_toks = _distributed_test_no_state(inputs)\n                epoch_loss.assign_add(per_replica_loss)\n                epoch_div.assign_add(per_replica_toks)\n            metrics = self.calc_metrics(epoch_loss.numpy(), epoch_div.numpy())\n            self.report(\n                epochs, metrics, start,\n                phase, \'EPOCH\', reporting_fns\n            )\n            return metrics\n\n    def distribute(self, dataset):\n        return self.strategy.experimental_distribute_dataset(dataset)\n\n\n@register_training_func(\'lm\', name=\'distributed\')\ndef fit_eager_distributed(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an language model using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n\n    model_file = get_model_file(\'lm\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'avg_loss\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    batchsz = kwargs[\'batchsz\']\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    tgt_key = model_params.get(\'tgt_key\')\n\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=True)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=True)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    trainer = LanguageModelTrainerDistributedTf(model_params, **kwargs)\n    train_dataset = trainer.distribute(train_dataset)\n    valid_dataset = trainer.distribute(valid_dataset)\n    \n    last_improved = 0\n    SET_TRAIN_FLAG(True)\n\n    for epoch in range(epochs):\n\n        trainer.train(train_dataset, reporting_fns, steps=len(ts))\n        test_metrics = trainer.test(valid_dataset, reporting_fns, phase=\'Valid\', steps=len(vs))\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        trainer.strategy = tf.distribute.OneDeviceStrategy(\'/device:GPU:0\')\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        test_dataset = trainer.distribute(test_dataset)\n        trainer.test(test_dataset, reporting_fns, phase=\'Test\', steps=len(es))\n\n'"
baseline/tf/lm/training/eager.py,0,"b'import os\nimport numpy as np\nimport time\nimport tensorflow as tf\nfrom eight_mile.utils import listify, get_version\nfrom eight_mile.tf.layers import SET_TRAIN_FLAG, get_shape_as_list\nfrom eight_mile.tf.optz import EagerOptimizer\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.model import create_model_for\nfrom baseline.train import register_training_func, Trainer\nfrom baseline.tf.lm.training.utils import to_tensors, SHUF_BUF_SZ, NUM_PREFETCH\n\n\ndef loss_with_state(model, h, x, y):\n    logits, h_out = model(x, h)\n    vsz = model.vsz\n    targets = tf.reshape(y, [-1])\n    bt_x_v = tf.nn.log_softmax(tf.reshape(logits, [-1, vsz]), axis=-1)\n    one_hots = tf.one_hot(targets, vsz)\n    example_loss = -tf.reduce_sum(one_hots * bt_x_v, axis=-1)\n    loss = tf.reduce_mean(example_loss)\n    return loss, h_out\n\n\ndef loss_without_state(model, x, y):\n    # Model will produce a null hidden state\n    logits = model(x, None)[0]\n    vsz = model.vsz\n    targets = tf.reshape(y, [-1])\n    bt_x_v = tf.nn.log_softmax(tf.reshape(logits, [-1, vsz]), axis=-1)\n    one_hots = tf.one_hot(targets, vsz)\n    example_loss = -tf.reduce_sum(one_hots * bt_x_v, axis=-1)\n    loss = tf.reduce_mean(example_loss)\n    return loss\n\n\nclass LanguageModelTrainerEagerTf(Trainer):\n    """"""A Trainer to use for eager mode\n\n    """"""\n    def __init__(self, model_params, **kwargs):\n        super().__init__()\n\n        if type(model_params) is dict:\n            self.model = create_model_for(\'lm\', **model_params)\n        else:\n            self.model = model_params\n\n        loss_fn = loss_with_state if self.model.requires_state else loss_without_state\n        self.optimizer = EagerOptimizer(loss_fn, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n        self._checkpoint = tf.train.Checkpoint(optimizer=self.optimizer.optimizer, model=self.model)\n        checkpoint_dir = \'{}-{}\'.format(""./tf-lm"", os.getpid())\n\n        self.checkpoint_manager = tf.train.CheckpointManager(self._checkpoint,\n                                                             directory=checkpoint_dir,\n                                                             max_to_keep=5)\n\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        self.checkpoint_manager.save()\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        print(self._checkpoint.restore(self.checkpoint_manager.latest_checkpoint))\n\n    @staticmethod\n    def _num_toks(y):\n        return tf.reduce_prod(get_shape_as_list(y))\n\n    def train(self, ts, reporting_fns, dataset=True):\n        """"""Train by looping over the steps\n\n        For a `tf.dataset`-backed `fit_func`, we are using the previously wired `dataset`s\n        in the model (and `dataset` is `True`).  For `feed_dict`, we convert the ts samples\n        to `feed_dict`s and hand them in one-by-one\n\n        :param ts: The training set\n        :param reporting_fns: A list of reporting hooks\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n\n        SET_TRAIN_FLAG(True)\n        epoch_loss = tf.Variable(0.0)\n        epoch_div = tf.Variable(0, dtype=tf.int32)\n        nstep_loss = tf.Variable(0.0)\n        nstep_div = tf.Variable(0, dtype=tf.int32)\n        self.nstep_start = time.time()\n        start = time.time()\n\n        def _train_step_no_state(inputs):\n            """"""Replicated training step.""""""\n\n            features, y = inputs\n            loss = self.optimizer.update(self.model, features, y)\n            toks = self._num_toks(y)\n            report_loss = loss * tf.cast(toks, tf.float32)\n            return report_loss, toks\n\n        def _train_step_with_state(inputs, hidden):\n            """"""Replicated training step.""""""\n\n            features, y = inputs\n            loss, hidden = self.optimizer.update_with_hidden(self.model, hidden, features, y)\n            toks = self._num_toks(y)\n            report_loss = loss * tf.cast(toks, tf.float32)\n            return hidden, report_loss, toks\n\n        if get_version(tf) >= 2:\n            _train_step_with_state = tf.function(_train_step_with_state)\n            _train_step_no_state = tf.function(_train_step_no_state)\n\n        h = None\n        for inputs in ts:\n            if self.model.requires_state:\n                h, step_report_loss, step_toks = _train_step_with_state(inputs, h)\n            else:\n                step_report_loss, step_toks = _train_step_no_state(inputs)\n\n            epoch_loss.assign_add(step_report_loss)\n            nstep_loss.assign_add(step_report_loss)\n            epoch_div.assign_add(step_toks)\n            nstep_div.assign_add(step_toks)\n\n            step = self.optimizer.global_step.numpy() + 1\n            if step % self.nsteps == 0:\n                metrics = self.calc_metrics(nstep_loss.numpy(), nstep_div.numpy())\n                self.report(\n                    step, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                nstep_loss.assign(0.0)\n                nstep_div.assign(0)\n                self.nstep_start = time.time()\n\n        epoch_loss = epoch_loss.numpy()\n        epoch_div = epoch_div.numpy()\n        metrics = self.calc_metrics(epoch_loss, epoch_div)\n        self.train_epochs += 1\n        self.report(\n            self.train_epochs, metrics, start,\n            \'Train\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def calc_metrics(self, agg, norm):\n        metrics = super().calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\'])\n        return metrics\n\n    def test(self, vs, reporting_fns, phase):\n        """"""Run an epoch of testing over the dataset\n\n        If we are using a `tf.dataset`-based `fit_func`, we will just\n        cycle the number of steps and let the `dataset` yield new batches.\n\n        If we are using `feed_dict`s, we convert each batch from the `DataFeed`\n        and pass that into TF as the `feed_dict`\n\n        :param vs: A validation set\n        :param reporting_fns: Reporting hooks\n        :param phase: The phase of evaluation (`Test`, `Valid`)\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        total_loss = 0.0\n        total_toks = 0\n        epochs = 0\n        if phase == \'Valid\':\n            self.valid_epochs += 1\n            epochs = self.valid_epochs\n        SET_TRAIN_FLAG(False)\n\n        start = time.time()\n        h = None\n        for features, y in vs:\n            if self.model.requires_state:\n                loss_value, h = loss_with_state(self.model, h, features, y)\n            else:\n                loss_value = loss_without_state(self.model, features, y)\n            loss_value = loss_value.numpy()\n            toks = self._num_toks(y)\n            total_loss += loss_value * tf.cast(toks, tf.float32).numpy()\n            total_toks += toks.numpy()\n\n        metrics = self.calc_metrics(total_loss, total_toks)\n        self.report(\n            epochs, metrics, start,\n            phase, \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n\n@register_training_func(\'lm\')\ndef fit_eager(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an language model using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n\n    model_file = get_model_file(\'lm\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'avg_loss\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    batchsz = kwargs[\'batchsz\']\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    tgt_key = model_params.get(\'tgt_key\')\n\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    trainer = LanguageModelTrainerEagerTf(model_params, **kwargs)\n    last_improved = 0\n    SET_TRAIN_FLAG(True)\n\n    for epoch in range(epochs):\n\n        trainer.train(train_dataset, reporting_fns)\n        test_metrics = trainer.test(valid_dataset, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        trainer.test(test_dataset, reporting_fns, phase=\'Test\')\n\n'"
baseline/tf/lm/training/feed.py,0,"b'import os\nimport time\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom eight_mile.tf.optz import optimizer\nfrom baseline.tf.tfy import TRAIN_FLAG, SET_TRAIN_FLAG\nfrom eight_mile.utils import listify\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.train import Trainer, create_trainer, register_trainer, register_training_func\nfrom baseline.model import create_model_for\nfrom collections import OrderedDict\n\n\n@register_training_func(\'lm\', \'feed_dict\')\ndef fit(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an language model using TensorFlow with a `feed_dict`.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 5\n        * *outfile* -- Model output file\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n        * *after_train_fn* (`func`) -- A callback to fire after ever epoch of training\n\n    :return: None\n    """"""\n    epochs = int(kwargs[\'epochs\']) if \'epochs\' in kwargs else 5\n    patience = int(kwargs[\'patience\']) if \'patience\' in kwargs else epochs\n\n    model_file = get_model_file(\'lm\', \'tf\', kwargs.get(\'basedir\'))\n    after_train_fn = kwargs[\'after_train_fn\'] if \'after_train_fn\' in kwargs else None\n    trainer = create_trainer(model_params, **kwargs)\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    best_metric = 1000\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'avg_loss\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    last_improved = 0\n\n    for epoch in range(epochs):\n\n        trainer.train(ts, reporting_fns, dataset=False)\n        if after_train_fn is not None:\n            after_train_fn(trainer.model)\n\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\', dataset=False)\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n    if es is not None:\n        trainer.recover_last_checkpoint()\n        trainer.test(es, reporting_fns, phase=\'Test\', dataset=False)\n\n'"
baseline/tf/lm/training/utils.py,0,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom eight_mile.tf.layers import reload_checkpoint\nfrom eight_mile.tf.optz import optimizer\nfrom baseline.tf.tfy import TRAIN_FLAG, SET_TRAIN_FLAG\nfrom baseline.train import Trainer, register_trainer\nfrom baseline.model import create_model_for\nfrom collections import OrderedDict\n\n\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\n_EVENT_FILE_GLOB_PATTERN = \'events.out.tfevents.*\'\n\n\ndef _summaries(eval_dir):\n    """"""Yields `tensorflow.Event` protos from event files in the eval dir.\n    Args:\n      eval_dir: Directory containing summary files with eval metrics.\n    Yields:\n      `tensorflow.Event` object read from the event files.\n    """"""\n    if tf.gfile.Exists(eval_dir):\n        for event_file in tf.gfile.Glob(os.path.join(eval_dir, _EVENT_FILE_GLOB_PATTERN)):\n            for event in tf.train.summary_iterator(event_file):\n                yield event\n\n\ndef read_eval_metrics(eval_dir):\n    """"""Helper to read eval metrics from eval summary files.\n    Args:\n      eval_dir: Directory containing summary files with eval metrics.\n    Returns:\n      A `dict` with global steps mapping to `dict` of metric names and values.\n    """"""\n    eval_metrics_dict = {}\n    for event in _summaries(eval_dir):\n        if not event.HasField(\'summary\'):\n            continue\n        metrics = {}\n        for value in event.summary.value:\n            if value.HasField(\'simple_value\'):\n                metrics[value.tag] = value.simple_value\n        if metrics:\n            eval_metrics_dict[event.step] = metrics\n    return OrderedDict(sorted(eval_metrics_dict.items(), key=lambda t: t[0]))\n\n\ndef to_tensors(ts):\n    """"""Convert a data feed into a tuple of `features` (`dict`) and `y` values\n\n    This method is required to produce `tf.dataset`s from the input data feed.\n    Any fields ending with `_lengths` are ignored, unless they match the\n    `src_lengths_key` or `tgt_lengths_key`, in which case, they are converted to `src_len` and `tgt_len`\n\n    :param ts: The data feed to convert\n    :param lengths_key: This is a field passed from the model params specifying source of truth of the temporal lengths\n    :return: A `tuple` of `features` and `y` (labels)\n    """"""\n    keys = ts[0].keys()\n    # This is kind of a hack\n    keys = [k for k in keys if k != \'ids\']\n\n    features = dict((k, []) for k in keys)\n\n    for sample in ts:\n        for k in features.keys():\n            for s in sample[k]:\n                features[k].append(s)\n\n    features = dict((k, np.stack(v).astype(np.int32)) for k, v in features.items())\n    tgt = features.pop(\'y\')\n    return features, tgt\n\n\n@register_trainer(task=\'lm\', name=\'default\')\nclass LanguageModelTrainerTf(Trainer):\n    """"""A Trainer to use if not using eager mode\n\n    The trainer can run in 2 modes: `dataset` and `feed_dict`.  When the former, the graph is assumed to\n    be connected by features attached to the input so the `feed_dict` will only be used to pass dropout information.\n\n    When the latter, we will use the baseline DataFeed to read the object into the `feed_dict`\n    """"""\n    def __init__(self, model_params, **kwargs):\n        super(LanguageModelTrainerTf, self).__init__()\n        if type(model_params) is dict:\n            self.model = create_model_for(\'lm\', **model_params)\n        else:\n            self.model = model_params\n        self.sess = self.model.sess\n        self.loss = self.model.create_loss()\n        self.test_loss = self.model.create_test_loss()\n        self.global_step, self.train_op = optimizer(self.loss, colocate_gradients_with_ops=True, variables=self.model.trainable_variables, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n        init = tf.compat.v1.global_variables_initializer()\n        self.model.sess.run(init)\n        saver = tf.compat.v1.train.Saver()\n        self.model.set_saver(saver)\n        checkpoint = kwargs.get(\'checkpoint\')\n        if checkpoint is not None:\n            skip_blocks = kwargs.get(\'blocks_to_skip\', [\'OptimizeLoss\'])\n            reload_checkpoint(self.model.sess, checkpoint, skip_blocks)\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        checkpoint_dir = \'{}-{}\'.format(""./tf-lm"", os.getpid())\n        self.model.saver.save(self.sess,\n                              os.path.join(checkpoint_dir, \'lm\'),\n                              global_step=self.global_step,\n                              write_meta_graph=False)\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        checkpoint_dir = \'{}-{}\'.format(""./tf-lm"", os.getpid())\n        latest = tf.train.latest_checkpoint(checkpoint_dir)\n        self.model.saver.restore(self.model.sess, latest)\n\n    @staticmethod\n    def _num_toks(batch):\n        return np.prod(batch[\'y\'].shape)\n\n    def train(self, ts, reporting_fns, dataset=True):\n        """"""Train by looping over the steps\n\n        For a `tf.dataset`-backed `fit_func`, we are using the previously wired `dataset`s\n        in the model (and `dataset` is `True`).  For `feed_dict`, we convert the ts samples\n        to `feed_dict`s and hand them in one-by-one\n\n        :param ts: The training set\n        :param reporting_fns: A list of reporting hooks\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        epoch_loss = 0.0\n        epoch_toks = 0\n\n        if self.model.requires_state:\n            state = self.model.sess.run(self.model.initial_state, self.model.make_input(ts[0], True))\n\n        fetches = {\n            ""loss"": self.loss,\n            ""train_op"": self.train_op,\n            ""global_step"": self.global_step\n        }\n\n        if self.model.requires_state:\n            fetches[""final_state""] = self.model.final_state\n\n        start = time.time()\n        self.nstep_start = start\n        for batch_dict in ts:\n\n            if dataset:\n                feed_dict = {TRAIN_FLAG(): 1}\n            else:\n                feed_dict = self.model.make_input(batch_dict, True)\n                _, global_step, lossv = self.sess.run([self.train_op, self.global_step, self.loss], feed_dict=feed_dict)\n\n            # In Keras LSTM, the order is h first, c second, its the opposite in TF 1, however I dont think it\n            # ends up mattering here\n            if self.model.requires_state:\n                for i, (s1, s2) in enumerate(self.model.initial_state):\n                    feed_dict[s1] = state[i][0]  #.c  # 0\n                    feed_dict[s2] = state[i][1]  #.h  # 1\n\n            vals = self.model.sess.run(fetches, feed_dict)\n            loss = vals[""loss""]\n\n            if self.model.requires_state:\n                state = vals[""final_state""]\n            global_step = vals[""global_step""]\n            toks = self._num_toks(batch_dict)\n            report_loss = loss * toks\n            epoch_loss += report_loss\n            epoch_toks += toks\n            self.nstep_agg += report_loss\n            self.nstep_div += toks\n\n            if (global_step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    global_step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = self.calc_metrics(epoch_loss, epoch_toks)\n        self.train_epochs += 1\n        self.report(\n            self.train_epochs, metrics, start,\n            \'Train\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def calc_metrics(self, agg, norm):\n        metrics = super(LanguageModelTrainerTf, self).calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\'])\n        return metrics\n\n    def test(self, vs, reporting_fns, phase, dataset=True):\n        """"""Run an epoch of testing over the dataset\n\n        If we are using a `tf.dataset`-based `fit_func`, we will just\n        cycle the number of steps and let the `dataset` yield new batches.\n\n        If we are using `feed_dict`s, we convert each batch from the `DataFeed`\n        and pass that into TF as the `feed_dict`\n\n        :param vs: A validation set\n        :param reporting_fns: Reporting hooks\n        :param phase: The phase of evaluation (`Test`, `Valid`)\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        total_loss = 0.0\n        total_toks = 0\n        epochs = 0\n        if phase == \'Valid\':\n            self.valid_epochs += 1\n            epochs = self.valid_epochs\n\n        if self.model.requires_state:\n            state = self.model.sess.run(self.model.initial_state, self.model.make_input(vs[0], False))\n\n        fetches = {\n            ""loss"": self.test_loss,\n        }\n\n        if self.model.requires_state:\n            fetches[""final_state""] = self.model.final_state\n\n        start = time.time()\n\n        for batch_dict in vs:\n            feed_dict = {}\n            if not dataset:\n                feed_dict = self.model.make_input(batch_dict, False)\n            # In Keras LSTM, the order is h first, c second, its the opposite in TF 1, however I dont think it\n            # ends up mattering here\n            if self.model.requires_state:\n\n                for i, (s1, s2) in enumerate(self.model.initial_state):\n                    feed_dict[s1] = state[i][0]  # .c  # 0\n                    feed_dict[s2] = state[i][1]  # .h  # 1\n\n            vals = self.model.sess.run(fetches, feed_dict)\n            loss = vals[""loss""]\n            toks = self._num_toks(batch_dict)\n            if self.model.requires_state:\n                state = vals[""final_state""]\n            total_loss += loss * toks\n            total_toks += toks\n\n        metrics = self.calc_metrics(total_loss, total_toks)\n        self.report(\n            epochs, metrics, start,\n            phase, \'EPOCH\', reporting_fns\n        )\n        return metrics\n'"
baseline/tf/seq2seq/decoders/__init__.py,0,b'import tensorflow as tf\nfrom eight_mile.utils import get_version\n\nif get_version(tf) < 2:\n    from baseline.tf.seq2seq.decoders.v1 import *\nelse:\n    from baseline.tf.seq2seq.decoders.v2 import *\n\n'
baseline/tf/seq2seq/decoders/v1.py,0,"b'from baseline.tf.tfy import *\nfrom baseline.utils import ls_props, read_json, Offsets, exporter\nfrom baseline.model import register_decoder, register_arc_policy, create_seq2seq_arc_policy\nfrom baseline.tf.embeddings import *\nfrom baseline.tf.transformer import transformer_decoder_stack, subsequent_mask\n\n\n__all__ = []\nexport = exporter(__all__)\n\n\n\n\nimport tensorflow.contrib.seq2seq as tfcontrib_seq2seq\n\n@export\nclass DecoderBase(tf.keras.layers.Layer):\n\n    def __init__(self, tgt_embedding, name=\'decoder\', **kwargs):\n        super().__init__(name=name)\n        self.tgt_embedding = tgt_embedding\n        self.beam_width = kwargs.get(\'beam\', 1)\n        self.best = None\n        self.probs = None\n        self.preds = None\n        self.mxlen = kwargs.get(\'mxlen\', 100)\n\n    def output(self, best, do_probs=True):\n        with tf.variable_scope(""Output""):\n            self.best = tf.identity(best, name=\'best\')\n            if self.beam_width > 1 or not do_probs:\n                self.probs = tf.no_op(name=\'probs\')\n            else:\n                self.probs = tf.map_fn(lambda x: tf.nn.softmax(x, name=\'probs\'), self.preds)\n\n    def call(self, inputs, **kwargs):\n        if kwargs.get(\'predict\', False):\n            return self.predict(inputs)\n        return self.decode(inputs)\n\n    def predict(self, inputs, **kwargs):\n        pass\n\n    def decode(self, inputs, **kwargs):\n        pass\n\n\n@register_decoder(name=\'transformer\')\nclass TransformerDecoder(DecoderBase):\n\n    def __init__(self, tgt_embedding, pdrop=0.1, layers=1, name=\'decode\', num_heads=4, scale=True, activation_type=\'relu\', d_ff=None, scope=\'TransformerDecoder\', **kwargs):\n        super().__init__(tgt_embedding, name=name, **kwargs)\n        # In predict mode the placeholder for the tgt embedding isn\'t created so the weights in the tgt embedding object\n        # is called `tgt/LUT/weights` because there isn\'t a placeholder called `tgt`. In decode where that placeholder\n        # exists the weights are called `tgt_1/LUT/weights`\n        #if kwargs.get(\'predict\', False):\n        #    tf.no_op(name=f""{name}/{self.tgt_embedding.name}"")\n        dsz = self.tgt_embedding.get_dsz()\n        vsz = self.tgt_embedding.get_vsz()\n        self.decoder = TransformerDecoderStack(dsz, num_heads, pdrop, scale, layers, activation_type, d_ff, name=scope)\n        self.do_weight_tying = bool(kwargs.get(\'tie_weights\', True))\n        if self.do_weight_tying:\n            self.proj = WeightTieDense(self.tgt_embedding)\n        else:\n            self.proj = tf.keras.layers.Dense(vsz, use_bias=False)\n\n    @property\n    def decoder_type(self):\n        return \'transformer\'\n\n    def predict(self, inputs, **kwargs):\n        """"""self.best is [T, B]""""""\n        encoder_outputs, src_len = inputs\n        src_enc = encoder_outputs.output\n        B = get_shape_as_list(src_enc)[0]\n\n        if hasattr(encoder_outputs, \'src_mask\'):\n            src_mask = encoder_outputs.src_mask\n        else:\n            T = get_shape_as_list(src_enc)[1]\n            src_mask = tf.sequence_mask(src_len, T, dtype=tf.float32)\n\n        def inner_loop(i, hit_eos, decoded_ids):\n\n            tgt_embed = self.tgt_embedding(decoded_ids)\n            T = get_shape_as_list(tgt_embed)[1]\n            tgt_mask = subsequent_mask(T)\n            h = self.decoder((tgt_embed, src_enc, src_mask, tgt_mask))\n            # hsz = get_shape_as_list(h)[-1]\n            # h = tf.reshape(h, [-1, hsz])\n            outputs = self.proj(h)\n\n            preds = tf.reshape(outputs, [B, T, -1])\n            next_id = tf.argmax(preds, axis=-1)[:, -1]\n            hit_eos |= tf.equal(next_id, Offsets.EOS)\n            next_id = tf.reshape(next_id, [B, 1])\n\n            decoded_ids = tf.concat([decoded_ids, next_id], axis=1)\n            return i + 1, hit_eos, decoded_ids\n\n        def is_not_finished(i, hit_eos, *_):\n            finished = i >= self.mxlen\n            finished |= tf.reduce_all(hit_eos)\n            return tf.logical_not(finished)\n\n        hit_eos = tf.fill([B], False)\n        decoded_ids = Offsets.GO * tf.ones([B, 1], dtype=tf.int64)\n        # Call the inner loop once so that the tgt weights aren\'t prefixed with `while`\n        i, hit_eos, decoded_ids = inner_loop(tf.constant(0), hit_eos, decoded_ids)\n\n        _, _, decoded_ids = tf.while_loop(is_not_finished, inner_loop, [i, hit_eos, decoded_ids],\n            shape_invariants=[\n                tf.TensorShape([]),\n                tf.TensorShape([None]),\n                tf.TensorShape([None, None])\n\n        ])\n        self.preds = tf.no_op()\n        best = tf.transpose(decoded_ids)\n        self.output(best, do_probs=False)\n        return self.best\n\n    def decode(self, inputs):\n        encoder_outputs, tgt, src_len, tgt_len = inputs\n        tgt_embed = self.tgt_embedding(tgt)\n        #if not tgt:\n        #    tgt = self.tgt_embedding.create_placeholder(self.tgt_embedding.name)\n        src_enc = encoder_outputs.output\n\n        #tgt_embed = self.tgt_embedding.encode(tgt)\n        shape = get_shape_as_list(tgt_embed)\n        B = shape[0]\n        T = shape[1]\n\n        if hasattr(encoder_outputs, \'src_mask\'):\n            src_mask = encoder_outputs.src_mask\n        else:\n            src_mask = tf.sequence_mask(src_len, T, dtype=tf.float32)\n\n        tgt_mask = subsequent_mask(T)\n        h = self.decoder((tgt_embed, src_enc, src_mask, tgt_mask))\n        outputs = self.proj(h)\n\n        self.preds = tf.transpose(tf.reshape(outputs, [B, T, -1]), [1, 0, 2])\n        best = tf.argmax(self.preds, -1)\n        self.output(best)\n        return self.best\n\n\n@export\nclass ArcPolicy(object):\n\n    def __init__(self):\n        pass\n\n    def connect(self, encoder_outputs, decoder, batch_sz):\n        pass\n\n\n@register_arc_policy(name=\'no_arc\')\nclass NoArcPolicy(ArcPolicy):\n\n    def __init__(self):\n        super().__init__()\n\n    def connect(self, encoder_outputs, decoder, batch_sz):\n        initial_state = decoder.cell.zero_state(batch_sz*decoder.beam_width, tf.float32)\n        return initial_state\n\n\nclass AbstractArcPolicy(ArcPolicy):\n\n    def __init__(self):\n        super().__init__()\n\n    def get_state(self, encoder_outputs):\n        pass\n\n    def connect(self, encoder_outputs, decoder, batch_sz):\n        final_encoder_state = self.get_state(encoder_outputs)\n        final_encoder_state = tf.contrib.seq2seq.tile_batch(final_encoder_state, multiplier=decoder.beam_width)\n\n        initial_state = decoder.cell.zero_state(batch_sz*decoder.beam_width, tf.float32)\n\n        if hasattr(initial_state, \'clone\'):\n            initial_state = initial_state.clone(cell_state=final_encoder_state)\n        else:\n            initial_state = final_encoder_state\n        return initial_state\n\n\n@register_arc_policy(name=\'default\')\nclass TransferLastHiddenPolicy(AbstractArcPolicy):\n\n    def __init__(self):\n        super().__init__()\n\n    def get_state(self, encoder_outputs):\n        return encoder_outputs.hidden\n\n\n@register_decoder(name=\'vanilla\')\nclass RNNDecoder(DecoderBase):\n\n    def __init__(self, tgt_embedding, hsz, pdrop, rnntype=\'lstm\', layers=1, vdrop=False, name=\'encoder\', scope=\'RNNDecoder\', **kwargs):\n        super().__init__(tgt_embedding, **kwargs)\n        self.hsz = hsz\n        self.pdrop = pdrop\n        self.rnntype = rnntype\n        self.layers = layers\n        self.vdrop = vdrop\n        self.arc_policy = create_seq2seq_arc_policy(**kwargs)\n        self.final_decoder_state = None\n        self.do_weight_tying = bool(kwargs.get(\'tie_weights\', False))\n        if self.do_weight_tying:\n            if self.hsz != self.tgt_embedding.get_dsz():\n                raise ValueError(""weight tying requires hsz == embedding dsz, \\\ngot {} hsz and {} dsz"".format(self.hsz, self.tgt_embedding.get_dsz()))\n\n    @property\n    def decoder_type(self):\n        return \'vanilla\'\n\n    def _create_cell(self, rnn_enc_tensor, src_len, hsz, pdrop, rnntype=\'lstm\', layers=1, vdrop=False, **kwargs):\n        return multi_rnn_cell_w_dropout(hsz, pdrop, rnntype, layers, variational=vdrop, training=TRAIN_FLAG())\n\n    def _get_tgt_weights(self):\n        Wo = tf.get_variable(""Wo"", initializer=tf.constant_initializer(self.tgt_embedding._weights,\n                                                                       dtype=tf.float32,\n                                                                       verify_shape=True),\n                             shape=[self.tgt_embedding.get_vsz(), self.tgt_embedding.get_dsz()])\n        return Wo\n\n    def predict(self, inputs, **kwargs):\n        """"""self.best is [T, B, K]""""""\n        encoder_outputs, src_len = inputs\n\n        # dynamic_decode creates a scope ""decoder"" and it pushes operations underneath.\n        # which makes it really hard to get the same objects between train and test\n        # In an ideal world, TF would just let us using tgt_embedding.encode as a function pointer\n        # This works fine for training, but then at decode time its not quite in the right place scope-wise\n        # So instead, for now, we never call .encode() and instead we create our own operator\n        # This is a huge hack where we are getting and wrapping the weight from the tgt embedding\n        # which never actually gets used inside the embeddings object to create a tensor\n        Wo = self._get_tgt_weights()\n        batch_sz = tf.shape(encoder_outputs.output)[0]\n        with tf.variable_scope(""dec"", reuse=tf.AUTO_REUSE):\n            # We just create a normal dense layer, the checkpoint will populate it with the right value\n            proj = dense_layer(self.tgt_embedding.get_vsz())\n            self.cell = self._create_cell(encoder_outputs.output, src_len, self.hsz, self.pdrop, self.rnntype, self.layers, self.vdrop, **kwargs)\n            initial_state = self.arc_policy.connect(encoder_outputs, self, batch_sz)\n            # Define a beam-search decoder\n            decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                cell=self.cell,\n                embedding=Wo,\n                start_tokens=tf.fill([batch_sz], Offsets.GO),\n                end_token=Offsets.EOS,\n                initial_state=initial_state,\n                beam_width=self.beam_width,\n                output_layer=proj,\n                length_penalty_weight=0.0\n            )\n\n            # This creates a ""decoder"" scope\n            final_outputs, final_decoder_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                decoder,\n                impute_finished=False,\n                swap_memory=True,\n                output_time_major=True,\n                maximum_iterations=self.mxlen\n            )\n            self.final_decoder_state = final_decoder_state\n            self.preds = tf.no_op()\n            best = final_outputs.predicted_ids\n            self.output(best, do_probs=False)\n            return self.best\n\n    def decode(self, inputs, **kwargs):\n        """"""self.best is [T, B]""""""\n        encoder_outputs, tgt, src_len, tgt_len = inputs\n        self.tgt_embedding.x = tgt if tgt is not None else self.tgt_embedding.create_placeholder(self.tgt_embedding.name)\n\n        # dynamic_decode creates a scope ""decoder"" and it pushes operations underneath.\n        # which makes it really hard to get the same objects between train and test\n        # In an ideal world, TF would just let us using tgt_embedding.encode as a function pointer\n        # This works fine for training, but then at decode time its not quite in the right place scope-wise\n        # So instead, for now, we never call .encode() and instead we create our own operator\n        # This is a huge hack where we are getting and wrapping the weight from the tgt embedding\n        # which never actually gets used inside the embeddings object to create a tensor\n        Wo = self._get_tgt_weights()\n        with tf.variable_scope(""dec"", reuse=tf.AUTO_REUSE):\n            tie_shape = [Wo.get_shape()[-1], Wo.get_shape()[0]]\n            self.cell = self._create_cell(encoder_outputs.output, src_len, self.hsz, self.pdrop, self.rnntype, self.layers, self.vdrop, **kwargs)\n            if self.do_weight_tying:\n                with tf.variable_scope(""Share"", custom_getter=tie_weight(Wo, tie_shape)):\n                    proj = tf.layers.Dense(self.tgt_embedding.get_vsz(), use_bias=False)\n            else:\n                proj = tf.layers.Dense(self.tgt_embedding.get_vsz(), use_bias=False)\n\n            batch_sz = tf.shape(encoder_outputs.output)[0]\n            initial_state = self.arc_policy.connect(encoder_outputs, self, batch_sz)\n\n            # Two paths depending on training or evaluating (during training)\n            # Normal expected inference path is BeamDecoder using .predict()\n            training_helper = tf.contrib.seq2seq.TrainingHelper(\n                inputs=tf.nn.embedding_lookup(Wo, self.tgt_embedding.x),\n                sequence_length=tgt_len\n            )\n            training_decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell=self.cell,\n                helper=training_helper,\n                initial_state=initial_state,\n                output_layer=proj\n            )\n            training_outputs, self.final_decoder_state, _ = tf.contrib.seq2seq.dynamic_decode(\n                training_decoder,\n                impute_finished=True,\n                swap_memory=True,\n                output_time_major=True\n            )\n\n            self.preds = training_outputs.rnn_output\n\n            # This is used to do greedy decoding during evaluation on the dev set\n            greedy_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n                Wo,\n                tf.fill([batch_sz], Offsets.GO),\n                Offsets.EOS\n            )\n            greedy_decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell=self.cell,\n                helper=greedy_helper,\n                initial_state=initial_state,\n                output_layer=proj\n            )\n            greedy_outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n                greedy_decoder,\n                impute_finished=True,\n                swap_memory=True,\n                output_time_major=True,\n                maximum_iterations=self.mxlen\n            )\n            best = greedy_outputs.sample_id\n            self.output(best)\n            return self.best\n\n\n@register_decoder(name=\'default\')\nclass RNNDecoderWithAttn(RNNDecoder):\n    def __init__(self, tgt_embedding, **kwargs):\n        self.attn_type = kwargs.get(\'attn_type\', \'bahdanau\').lower()\n        super().__init__(tgt_embedding, **kwargs)\n\n    @property\n    def decoder_type(self):\n        return \'default\'\n\n    @property\n    def attn_type(self):\n        return self._attn_type\n\n    @attn_type.setter\n    def attn_type(self, type):\n        self._attn_type = type\n\n    def _create_cell(self, rnn_enc_tensor, src_len, hsz, pdrop, rnntype=\'lstm\', layers=1, vdrop=False, **kwargs):\n        cell = multi_rnn_cell_w_dropout(hsz, pdrop, rnntype, layers, variational=vdrop, training=TRAIN_FLAG())\n        if self.beam_width > 1:\n            # Expand the encoded tensor for all beam entries\n            rnn_enc_tensor = tf.contrib.seq2seq.tile_batch(rnn_enc_tensor, multiplier=self.beam_width)\n            src_len = tf.contrib.seq2seq.tile_batch(src_len, multiplier=self.beam_width)\n        GlobalAttention = tfcontrib_seq2seq.LuongAttention if self.attn_type == \'luong\' else tfcontrib_seq2seq.BahdanauAttention\n        attn_mech = GlobalAttention(hsz, rnn_enc_tensor, src_len)\n        return tf.contrib.seq2seq.AttentionWrapper(cell, attn_mech, self.hsz, name=\'dyn_attn_cell\')\n'"
baseline/tf/seq2seq/decoders/v2.py,0,"b'from eight_mile.tf.layers import rnn_cell\nfrom baseline.tf.tfy import *\nfrom baseline.utils import ls_props, read_json, Offsets, exporter\nfrom baseline.model import register_decoder, register_arc_policy, create_seq2seq_arc_policy\nfrom baseline.tf.embeddings import *\nfrom baseline.tf.seq2seq.encoders.v2 import TransformerEncoderOutput\nfrom baseline.tf.transformer import subsequent_mask\nfrom functools import partial\n\n\n__all__ = []\nexport = exporter(__all__)\n\n\nclass ArcPolicy(tf.keras.layers.Layer):\n\n    def __init__(self):\n        super().__init__()\n\n    def call(self, inputs):\n        encoder_outputs, hsz, beam_width = inputs\n        return self.forward(encoder_outputs, hsz, beam_width)\n\n    def forward(self, encoder_outputs, hsz, beam_width=1):\n        pass\n\n\nclass AbstractArcPolicy(ArcPolicy):\n\n    def get_state(self, encoder_outputs):\n        pass\n\n    def forward(self, encoder_output, hsz, beam_width=1):\n        h_i = self.get_state(encoder_output)\n        context = encoder_output.output\n        context = repeat_batch(context, beam_width)\n        # What does the multi-RNN look like in old TF again?\n        if type(h_i) is tuple:\n            h_i = repeat_batch(h_i[0], beam_width, dim=1), repeat_batch(h_i[1], beam_width, dim=1)\n        else:\n            h_i = repeat_batch(h_i, beam_width, dim=1)\n        batch_size = get_shape_as_list(context)[0]\n        init_zeros = tf.zeros((batch_size, hsz), dtype=context.dtype)\n        return h_i, init_zeros, context\n\n\n@register_arc_policy(name=\'default\')\nclass TransferLastHiddenPolicy(AbstractArcPolicy):\n\n    def get_state(self, encoder_outputs):\n        return encoder_outputs.hidden\n\n\n@register_arc_policy(name=\'no_arc\')\nclass NoArcPolicy(AbstractArcPolicy):\n\n    def _zero_state(self, final_encoder_state):\n        num_rnns = len(final_encoder_state)\n        batchsz = get_shape_as_list(final_encoder_state)[0]\n        zstate = []\n        for i, _ in enumerate(self.rnns):\n            zstate.append((np.zeros((batchsz, num_rnns), dtype=np.float32),\n                           np.zeros((batchsz, num_rnns), dtype=np.float32)))\n\n        return zstate\n\n    def get_state(self, encoder_outputs):\n        final_encoder_state = encoder_outputs.hidden\n        return self._zero_state(final_encoder_state)\n\n\n@register_decoder(name=\'vanilla\')\nclass RNNDecoder(tf.keras.layers.Layer):\n\n    def __init__(self, tgt_embeddings, **kwargs):\n        """"""Construct an RNN decoder.  It provides the input size, the rest is up to the impl.\n\n        The default implementation provides an RNN cell, followed by a linear projection, out to a softmax\n\n        :param input_dim: The input size\n        :param kwargs:\n        :return: void\n        """"""\n        super().__init__()\n        self.hsz = kwargs[\'hsz\']\n        self.arc_policy = create_seq2seq_arc_policy(**kwargs)\n        self.tgt_embeddings = tgt_embeddings\n        rnntype = kwargs.get(\'rnntype\', \'lstm\')\n        layers = kwargs.get(\'layers\', 1)\n        feed_input = kwargs.get(\'feed_input\', True)\n        dsz = tgt_embeddings.get_dsz()\n        if feed_input:\n            self.input_i = self._feed_input\n            dsz += self.hsz\n        else:\n            self.input_i = self._basic_input\n        pdrop = kwargs.get(\'dropout\', 0.5)\n        self.decoder_rnn = rnn_cell(dsz, self.hsz, rnntype, layers, pdrop)\n        self.dropout = tf.keras.layers.Dropout(pdrop)\n        self.init_attn(**kwargs)\n\n        do_weight_tying = bool(kwargs.get(\'tie_weights\', True))\n\n        if do_weight_tying:\n            if self.hsz != self.tgt_embeddings.get_dsz():\n                raise ValueError(""weight tying requires hsz == embedding dsz, got {} hsz and {} dsz"".format(self.hsz, self.tgt_embedding.get_dsz()))\n            self.preds = WeightTieDense(self.tgt_embeddings)\n        else:\n            self.preds = tf.keras.layers.Dense(self.tgt_embeddings.get_vsz())\n\n    @staticmethod\n    def _basic_input(dst_embed_i, _):\n        """"""\n        In this function the destination embedding is passed directly to into the decoder.  The output of previous H\n        is ignored.  This is implemented using a bound method to a field in the class for speed so that this decision\n        is handled at initialization, not as a conditional in the training or inference\n\n        :param embed_i: The embedding at i\n        :param _: Ignored\n        :return: basic input\n        """"""\n        return dst_embed_i.squeeze(0)\n\n    @staticmethod\n    def _feed_input(embed_i, attn_output_i):\n        """"""\n        In this function the destination embedding is concatenated with the previous attentional output and\n        passed to the decoder. This is implemented using a bound method to a field in the class for speed\n        so that this decision is handled at initialization, not as a conditional in the training or inference\n\n        :param embed_i: The embedding at i\n        :param output_i: This is the last H state\n        :return: an input that is a concatenation of previous state and destination embedding\n        """"""\n        return tf.concat([embed_i, attn_output_i], 1)\n\n    def call(self, encoder_outputs, dst):\n        src_mask = encoder_outputs.src_mask\n        # TODO where to get beam size?\n        h_i, output_i, context_bth = self.arc_policy((encoder_outputs, self.hsz, 1))\n        output_bth, _ = self.decode_rnn(context_bth, h_i, output_i, dst, src_mask)\n        pred = self.output(output_bth)\n        return pred\n\n    def decode_rnn(self, context_bth, h_i, output_i, dst_bth, src_mask):\n        embed_out_bth = self.tgt_embeddings(dst_bth)\n\n        outputs = []\n\n        num_steps = get_shape_as_list(embed_out_bth)[1]\n        for i in range(num_steps):\n            embed_i = embed_out_bth[:, i, :]\n            # Input feeding would use previous attentional output in addition to destination embeddings\n            embed_i = self.input_i(embed_i, output_i)\n            output_i, h_i = self.decoder_rnn(embed_i, h_i)\n            output_i = self.attn(output_i, context_bth, src_mask)\n            output_i = self.dropout(output_i)\n            # Attentional outputs\n            outputs.append(output_i)\n\n        outputs_tbh = tf.stack(outputs, axis=1)\n        return outputs_tbh, h_i\n\n    def attn(self, output_t, context, src_mask=None):\n        return output_t\n\n    def init_attn(self, **kwargs):\n        pass\n\n    def output(self, x):\n        return self.preds(x)\n\n    class BeamSearch(BeamSearchBase):\n\n        def __init__(self, parent, **kwargs):\n            super().__init__(**kwargs)\n            self.parent = parent\n\n        def init(self, encoder_outputs):\n            """"""Tile batches for encoder inputs and the likes.""""""\n            src_mask = repeat_batch(encoder_outputs.src_mask, self.K)\n            h_i, dec_out, context = self.parent.arc_policy((encoder_outputs, self.parent.hsz, self.K))\n            return h_i, dec_out, context, src_mask\n\n        def step(self, paths, extra):\n            """"""Calculate the probs of the next output and update state.""""""\n            h_i, dec_out, context, src_mask = extra\n            # Our RNN decoder is now batch-first, so we need to expand the time dimension\n            last = tf.reshape(paths[:, :, -1], (-1, 1))\n            dec_out, h_i = self.parent.decode_rnn(context, h_i, dec_out, last, src_mask)\n            probs = self.parent.output(dec_out)\n            log_probs = tf.nn.log_softmax(probs, axis=-1)\n            # Collapse over time\n            dec_out = tf.squeeze(dec_out, 1)\n            return log_probs, (h_i, dec_out, context, src_mask)\n\n        def update(self, beams, extra):\n            """"""Select the correct hidden states and outputs to used based on the best performing beams.""""""\n            h_i, dec_out, context, src_mask = extra\n            h_i = tuple(tf.gather(hc, beams, axis=1) for hc in h_i)\n            dec_out = tf.gather(dec_out, beams)\n            return h_i, dec_out, context, src_mask\n\n    def beam_search(self, encoder_outputs, **kwargs):\n        alpha = kwargs.get(\'alpha\')\n        if alpha is not None:\n            kwargs[\'length_penalty\'] = partial(gnmt_length_penalty, alpha=alpha)\n        return RNNDecoder.BeamSearch(self, **kwargs)(encoder_outputs)\n\n\n@register_decoder(name=\'default\')\nclass RNNDecoderWithAttn(RNNDecoder):\n\n    def __init__(self, tgt_embeddings, **kwargs):\n        super().__init__(tgt_embeddings, **kwargs)\n\n    def init_attn(self, **kwargs):\n        attn_type = kwargs.get(\'attn_type\', \'bahdanau\').lower()\n        if attn_type == \'dot\':\n            self.attn_module = LuongDotProductAttention(self.hsz)\n        elif attn_type == \'concat\' or attn_type == \'bahdanau\':\n            self.attn_module = BahdanauAttention(self.hsz)\n        elif attn_type == \'sdp\':\n            self.attn_module = ScaledDotProductAttention(self.hsz)\n        else:\n            self.attn_module = LuongGeneralAttention(self.hsz)\n\n    def attn(self, output_t, context, src_mask=None):\n        return self.attn_module((output_t, context, context, src_mask))\n\n\n@register_decoder(name=\'transformer\')\nclass TransformerDecoderWrapper(tf.keras.layers.Layer):\n\n    def __init__(self, tgt_embeddings, dropout=0.5, layers=1, hsz=None, num_heads=4, scale=True, **kwargs):\n        super().__init__()\n        self.tgt_embeddings = tgt_embeddings\n        dsz = self.tgt_embeddings.get_dsz()\n        if hsz is None:\n            hsz = dsz\n        self.hsz = hsz\n\n        self.transformer_decoder = TransformerDecoderStack(d_model=hsz, num_heads=num_heads, pdrop=dropout, scale=scale, layers=layers)\n\n        self.proj_to_dsz = self._identity\n        self.proj_to_hsz = self._identity\n        if hsz != dsz:\n            self.proj_to_hsz = tf.keras.layers.Dense(hsz)\n            self.proj_to_dsz = tf.keras.layers.Dense(dsz)\n\n        do_weight_tying = bool(kwargs.get(\'tie_weights\', True))\n\n        if do_weight_tying:\n            if self.hsz != self.tgt_embeddings.get_dsz():\n                raise ValueError(""weight tying requires hsz == embedding dsz, got {} hsz and {} dsz"".format(self.hsz, self.tgt_embedding.get_dsz()))\n            self.preds = WeightTieDense(self.tgt_embeddings)\n        else:\n            self.preds = tf.keras.layers.Dense(self.tgt_embeddings.get_vsz())\n\n    def _identity(self, x):\n        return x\n\n    def call(self, encoder_output, dst):\n        embed_out_bth = self.tgt_embeddings(dst)\n        embed_out_bth = self.proj_to_hsz(embed_out_bth)\n        context_bth = encoder_output.output\n        T = get_shape_as_list(embed_out_bth)[1]\n        dst_mask = tf.cast(subsequent_mask(T), embed_out_bth.dtype)\n        src_mask = encoder_output.src_mask\n        output = self.transformer_decoder((embed_out_bth, context_bth, src_mask, dst_mask))\n        output = self.proj_to_dsz(output)\n        prob = self.output(output)\n        return prob\n\n    def output(self, x):\n        return self.preds(x)\n\n    class BeamSearch(BeamSearchBase):\n\n        def __init__(self, parent, **kwargs):\n            super().__init__(**kwargs)\n            self.parent = parent\n\n        def init(self, encoder_outputs):\n            """"""Tile for the batch of the encoder inputs.""""""\n            encoder_outputs = TransformerEncoderOutput(\n                repeat_batch(encoder_outputs.output, self.K),\n                repeat_batch(encoder_outputs.src_mask, self.K)\n            )\n            return encoder_outputs\n\n        def step(self, paths, extra):\n            """"""Calculate the probs for the last item based on the full path.""""""\n            B, K, T = paths.shape\n            assert K == self.K\n            return self.parent(extra, tf.reshape(paths, (B * K, T)))[:, -1], extra\n\n        def update(self, beams, extra):\n            """"""There is no state for the transformer so just pass it.""""""\n            return extra\n\n    def beam_search(self, encoder_outputs, **kwargs):\n        alpha = kwargs.get(\'alpha\')\n        if alpha is not None:\n            kwargs[\'length_penalty\'] = partial(gnmt_length_penalty, alpha=alpha)\n        return TransformerDecoderWrapper.BeamSearch(self, **kwargs)(encoder_outputs)\n\n'"
baseline/tf/seq2seq/encoders/__init__.py,0,b'import tensorflow as tf\nfrom eight_mile.utils import get_version\n\nif get_version(tf) < 2:\n    from baseline.tf.seq2seq.encoders.v1 import *\nelse:\n    from baseline.tf.seq2seq.encoders.v2 import *\n'
baseline/tf/seq2seq/encoders/v1.py,0,"b'from baseline.tf.tfy import *\nfrom baseline.tf.embeddings import *\nfrom baseline.tf.transformer import transformer_encoder_stack\nfrom baseline.utils import exporter, MAGIC_VARS\nfrom baseline.model import register_encoder\nfrom collections import namedtuple\n\n__all__ = []\nexport = exporter(__all__)\n\n\n\nRNNEncoderOutput = namedtuple(""RNNEncoderOutput"", (""output"", ""hidden"", ""src_mask""))\n\n\ndef _make_src_mask(output, lengths):\n    T = tf.shape(output)[1]\n    src_mask = tf.cast(tf.sequence_mask(lengths, T), dtype=tf.uint8)\n    return src_mask\n\n\n@export\nclass EncoderBase(tf.keras.layers.Layer):\n\n    def __init__(self, name=\'encoder\', **kwargs):\n        super().__init__(name=name)\n\n    def call(self, inputs, **kwargs):\n        pass\n\n\n@register_encoder(name=\'default\')\nclass RNNEncoder(EncoderBase):\n\n    def __init__(self, name=\'encoder\', pdrop=0.1, hsz=650, rnntype=\'blstm\', layers=1, vdrop=False, scope=\'RNNEncoder\', residual=False, create_src_mask=True, **kwargs):\n        super().__init__(name=name)\n        Encoder = BiLSTMEncoderAllLegacy if rnntype == \'blstm\' else LSTMEncoderAllLegacy\n        self.rnn = Encoder(None, hsz, layers, pdrop, vdrop, name=scope)\n        self.residual = residual\n        self.src_mask_fn = _make_src_mask if create_src_mask is True else lambda x, y: None\n\n    @property\n    def encoder_type(self):\n        return \'default\'\n\n    def call(self, inputs, **kwargs):\n        embed_in, src_len = inputs\n\n        # This comes out as a sequence T of (B, D)\n        output, hidden = self.rnn((embed_in, src_len))\n        output = output + embed_in if self.residual else output\n        return RNNEncoderOutput(output=output, hidden=hidden, src_mask=self.src_mask_fn(output, src_len))\n\n\nTransformerEncoderOutput = namedtuple(""TransformerEncoderOutput"", (""output"", ""src_mask""))\n\n\n@register_encoder(name=\'transformer\')\nclass TransformerEncoder(EncoderBase):\n\n    def __init__(self, pdrop=0.1, hsz=650, num_heads=4, layers=1, scale=True, activation_type=\'relu\', name=""encode"", d_ff=None, scope=""TransformerEncoder"", **kwargs):\n        super().__init__(name=name)\n        self.encoder = TransformerEncoderStack(num_heads, hsz, pdrop, scale, layers, activation_type, d_ff, name=scope)\n\n    @property\n    def encoder_type(self):\n        return \'transformer\'\n\n    def call(self, inputs, **kwargs):\n        embed_in, src_len = inputs\n        T = get_shape_as_list(embed_in)[1]\n        src_mask = tf.sequence_mask(src_len, T, dtype=tf.float32)\n        shp = get_shape_as_list(src_mask)\n        new_shp = [shp[0]] + [1, 1] + shp[1:]\n        src_mask = tf.reshape(src_mask, new_shp)\n        encoder_output = self.encoder((embed_in, src_mask))\n        # This comes out as a sequence T of (B, D)\n        return TransformerEncoderOutput(output=encoder_output, src_mask=src_mask)\n\n'"
baseline/tf/seq2seq/encoders/v2.py,0,"b'from baseline.tf.tfy import *\nfrom baseline.tf.embeddings import *\nfrom baseline.tf.transformer import transformer_encoder_stack\nfrom baseline.utils import exporter, MAGIC_VARS\nfrom baseline.model import register_encoder\nfrom collections import namedtuple\nfrom eight_mile.tf.layers import *\n\nRNNEncoderOutput = namedtuple(""RNNEncoderOutput"", (""output"", ""hidden"", ""src_mask""))\n\n\ndef _make_src_mask(output, lengths):\n    T = output.shape[1]\n    src_mask = tf.cast(tf.sequence_mask(lengths, T), dtype=tf.uint8)\n    return src_mask\n\n\n@register_encoder(name=\'default\')\nclass RNNEncoder(tf.keras.layers.Layer):\n\n    def __init__(self, dsz=None, hsz=None, rnntype=\'blstm\', layers=1, pdrop=0.5, residual=False, create_src_mask=True, name=\'encoder\', scope=""RNNEncoder"", **kwargs):\n        super().__init__(name=name)\n        self.residual = residual\n        hidden = hsz if hsz is not None else dsz\n        Encoder = LSTMEncoderAll if rnntype == \'lstm\' else BiLSTMEncoderAll\n        self.rnn = Encoder(dsz, hidden, layers, pdrop, name=scope)\n        self.src_mask_fn = _make_src_mask if create_src_mask is True else lambda x, y: None\n\n    def call(self, inputs):\n        btc, lengths = inputs\n        output, hidden = self.rnn((btc, lengths))\n        return RNNEncoderOutput(output=output + btc if self.residual else output,\n                                hidden=hidden,\n                                src_mask=self.src_mask_fn(output, lengths))\n\n\nTransformerEncoderOutput = namedtuple(""TransformerEncoderOutput"", (""output"", ""src_mask""))\n\n\n@register_encoder(name=\'transformer\')\nclass TransformerEncoderWrapper(tf.keras.layers.Layer):\n\n    def __init__(self, dsz, hsz=None, num_heads=4, layers=1, dropout=0.5, name=\'encoder\', scope=\'TransformerEncoder\', **kwargs):\n        super().__init__(name=name)\n        if hsz is None:\n            hsz = dsz\n        self.proj = tf.keras.layers.Dense(hsz) if hsz != dsz else self._identity\n        self.transformer = TransformerEncoderStack(num_heads=num_heads, d_model=hsz, pdrop=dropout, scale=True, layers=layers, name=scope)\n\n    def _identity(self, x):\n        return x\n\n    def call(self, inputs):\n        bth, lengths = inputs\n        T = get_shape_as_list(bth)[1]\n        src_mask = tf.sequence_mask(lengths, T, dtype=tf.float32)\n        shp = get_shape_as_list(src_mask)\n        new_shp = [shp[0]] + [1, 1] + shp[1:]\n        src_mask = tf.reshape(src_mask, new_shp)\n\n        bth = self.proj(bth)\n        output = self.transformer((bth, src_mask))\n        return TransformerEncoderOutput(output=output, src_mask=src_mask)\n'"
baseline/tf/seq2seq/training/__init__.py,0,b'from eight_mile.utils import get_version\nimport tensorflow as tf\nif not tf.executing_eagerly():\n\n    from baseline.tf.seq2seq.training.datasets import *\n    from baseline.tf.seq2seq.training.feed import *\nelse:\n    from baseline.tf.seq2seq.training.eager import *\n    from baseline.tf.seq2seq.training.distributed import *\n'
baseline/tf/seq2seq/training/datasets.py,0,"b'import tensorflow as tf\nfrom baseline.tf.tfy import TRAIN_FLAG\nfrom eight_mile.utils import listify\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.train import create_trainer, register_training_func\nfrom baseline.tf.seq2seq.training.utils import to_tensors, SHUF_BUF_SZ, NUM_PREFETCH\n\n\n@register_training_func(\'seq2seq\', \'dataset\')\ndef fit_datasets(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an encoder-decoder network using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n    model_file = get_model_file(\'seq2seq\', \'tf\', kwargs.get(\'basedir\'))\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'perplexity\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    batchsz = kwargs[\'batchsz\']\n    ## First, make tf.datasets for ts, vs and es\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md\n    # effective_batch_sz = args.batchsz*args.gpus\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    src_lengths_key = model_params.get(\'src_lengths_key\')\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, src_lengths_key))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.repeat(epochs + 1)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, src_lengths_key))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.repeat(epochs + 1)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    iter = tf.compat.v1.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_dataset),\n                                                     tf.compat.v1.data.get_output_shapes(train_dataset))\n\n    features, tgt = iter.get_next()\n    # Add features to the model params\n    model_params.update(features)\n    # This is kind of crazy, but seems to work, hardwire a graph op for `mx_tgt_len`\n    model_params.update({\'tgt\': tgt, \'mx_tgt_len\': tf.reduce_max(features[\'tgt_len\'])})\n\n    # create the initialization operations\n    train_init_op = iter.make_initializer(train_dataset)\n    valid_init_op = iter.make_initializer(valid_dataset)\n\n    TRAIN_FLAG()\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n\n    for epoch in range(epochs):\n        trainer.sess.run(train_init_op)\n        trainer.train(ts, reporting_fns)\n        trainer.sess.run(valid_init_op)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, src_lengths_key))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.repeat(epochs + 1)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        test_init_op = iter.make_initializer(test_dataset)\n\n        trainer.sess.run(test_init_op)\n        trainer.test(es, reporting_fns, phase=\'Test\')\n'"
baseline/tf/seq2seq/training/distributed.py,0,"b'import os\nimport numpy as np\nimport time\nimport tensorflow as tf\nfrom eight_mile.utils import listify\nfrom eight_mile.tf.layers import SET_TRAIN_FLAG, get_shape_as_list, autograph_options\nfrom eight_mile.tf.optz import EagerOptimizer\nfrom baseline.progress import create_progress_bar\nfrom baseline.utils import get_model_file, get_metric_cmp, convert_seq2seq_golds, convert_seq2seq_preds\nfrom eight_mile.bleu import bleu\nfrom baseline.model import create_model_for\nfrom baseline.train import register_training_func, Trainer\nfrom baseline.tf.seq2seq.training.utils import to_tensors, SHUF_BUF_SZ, NUM_PREFETCH\n\n\ndef loss(model, features, labels):\n    # Claims its T, B, H\n    logits = tf.transpose(model(features), [1, 0, 2])\n    # So ok, then transpose this too\n    labels = tf.transpose(labels, [1, 0])\n    # TxB loss mask\n    label_lengths = features[\'tgt_len\']\n    mx_seq_len = tf.reduce_max(label_lengths)-1\n    labels = labels[1:mx_seq_len + 1, :]\n    logits = logits[:mx_seq_len, :, :]\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n    loss_mask = tf.cast(tf.sequence_mask(label_lengths-1), dtype=tf.float32)\n    losses = losses * tf.transpose(loss_mask, [1, 0])\n\n    losses = tf.reduce_sum(losses)\n    losses /= tf.cast(tf.reduce_sum(label_lengths), tf.float32)\n    return losses\n\n\n\nclass Seq2SeqTrainerDistributedTf(Trainer):\n    """"""A Trainer to use for eager distributed mode\n    """"""\n    def __init__(self, model_params, **kwargs):\n        super().__init__()\n        self.gpus = int(kwargs.get(\'gpus\', 1))\n\n        if type(model_params) is dict:\n            self.model = create_model_for(\'seq2seq\', **model_params)\n        else:\n            self.model = model_params\n\n        self.tgt_rlut = kwargs[\'tgt_rlut\']\n        self.optimizer = EagerOptimizer(loss, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n        self._checkpoint = tf.train.Checkpoint(optimizer=self.optimizer.optimizer, model=self.model)\n        checkpoint_dir = \'{}-{}\'.format(""./tf-seq2seq"", os.getpid())\n\n        self.checkpoint_manager = tf.train.CheckpointManager(self._checkpoint,\n                                                             directory=checkpoint_dir,\n                                                             max_to_keep=5)\n        devices = [\'/device:GPU:{}\'.format(i) for i in range(self.gpus)]\n        self.strategy = tf.distribute.MirroredStrategy(devices)\n        self.bleu_n_grams = int(kwargs.get(""bleu_n_grams"", 4))\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        self.checkpoint_manager.save()\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        print(self._checkpoint.restore(self.checkpoint_manager.latest_checkpoint))\n\n    @staticmethod\n    def _num_toks(y):\n        return tf.prod(get_shape_as_list(y))\n\n    def _num_toks(self, lens):\n        return tf.reduce_sum(lens)\n\n    def train(self, ts, reporting_fns, steps=0):\n        """"""Train by looping over the steps\n\n        For a `tf.dataset`-backed `fit_func`, we are using the previously wired `dataset`s\n        in the model (and `dataset` is `True`).  For `feed_dict`, we convert the ts samples\n        to `feed_dict`s and hand them in one-by-one\n\n        :param ts: The training set\n        :param reporting_fns: A list of reporting hooks\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        strategy = self.strategy\n        #num_replicas = strategy.num_replicas_in_sync\n        def _replicated_train_step(inputs):\n            features, y = inputs\n            per_replica_loss = self.optimizer.update(self.model, features, y)\n            per_replica_toks = self._num_toks(features[\'tgt_len\'])\n            per_replica_report_loss = per_replica_loss * tf.cast(per_replica_toks, tf.float32)\n            return per_replica_report_loss, per_replica_toks\n\n        with strategy.scope():\n            SET_TRAIN_FLAG(True)\n            epoch_loss = tf.Variable(0.0)\n            epoch_div = tf.Variable(0, dtype=tf.int32)\n            nstep_loss = tf.Variable(0.0)\n            nstep_div = tf.Variable(0, dtype=tf.int32)\n            self.nstep_start = time.time()\n            start = time.time()\n\n            @tf.function\n            def _distributed_train_step(inputs):\n                per_replica_loss, per_replica_toks = strategy.experimental_run_v2(_replicated_train_step, args=(inputs,))\n                total_step_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n                total_toks = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_toks, axis=None)\n                return total_step_loss, total_toks\n\n            with autograph_options({""function_optimization"": False, ""layout_optimizer"": False}):\n                train_iter = iter(ts)\n                for i in range(steps):\n                    features, y = next(train_iter)\n                    step_report_loss, step_toks = _distributed_train_step((features, y))\n                    epoch_loss.assign_add(step_report_loss)\n                    nstep_loss.assign_add(step_report_loss)\n                    epoch_div.assign_add(step_toks)\n                    nstep_div.assign_add(step_toks)\n\n                    step = self.optimizer.global_step.numpy().item() + 1\n                    if step % self.nsteps == 0:\n                        metrics = self.calc_metrics(nstep_loss.numpy().item(), nstep_div.numpy().item())\n                        self.report(\n                            step, metrics, self.nstep_start,\n                            \'Train\', \'STEP\', reporting_fns, self.nsteps\n                        )\n                        nstep_loss.assign(0.0)\n                        nstep_div.assign(0)\n                        self.nstep_start = time.time()\n\n                epoch_loss = epoch_loss.numpy()\n                epoch_div = epoch_div.numpy()\n                metrics = self.calc_metrics(epoch_loss, epoch_div)\n                self.train_epochs += 1\n                self.report(\n                    self.train_epochs, metrics, start,\n                    \'Train\', \'EPOCH\', reporting_fns\n                )\n                return metrics\n\n    def calc_metrics(self, agg, norm):\n        metrics = super().calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\']).item()\n        return metrics\n\n\n    def _evaluate(self, es, reporting_fns, **kwargs):\n        """"""Run the model with beam search and report Bleu.\n\n        :param es: `tf.dataset` of input\n        :param reporting_fns: Input hooks\n        """"""\n        preds = []\n        golds = []\n        start = time.time()\n        kwargs[\'make_input\'] = False\n\n        for features, tgt in es:\n            tgt_lens = features.pop(\'tgt_len\')\n            top_preds = self.model.predict(features, **kwargs)\n            preds.extend(convert_seq2seq_preds(top_preds[:, 0, :], self.tgt_rlut))\n            golds.extend(convert_seq2seq_golds(tgt, tgt_lens, self.tgt_rlut))\n        metrics = {\'bleu\': bleu(preds, golds, self.bleu_n_grams)[0]}\n        self.report(\n            0, metrics, start, \'Test\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def test(self, vs, reporting_fns, steps=0, phase=\'Valid\', **kwargs):\n        """"""Run an epoch of testing over the dataset\n\n        If we are using a `tf.dataset`-based `fit_func`, we will just\n        cycle the number of steps and let the `dataset` yield new batches.\n\n        If we are using `feed_dict`s, we convert each batch from the `DataFeed`\n        and pass that into TF as the `feed_dict`\n\n        :param vs: A validation set\n        :param reporting_fns: Reporting hooks\n        :param phase: The phase of evaluation (`Test`, `Valid`)\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n\n\n        def _replicated_valid_step(inputs):\n            features, tgt = inputs\n            top_preds = self.model.predict(features, beam=1, make_input=False)\n            per_replica_loss = loss(self.model, features, tgt)\n            per_replica_toks = self._num_toks(features[\'tgt_len\'])\n            per_replica_report_loss = per_replica_loss * tf.cast(per_replica_toks, tf.float32)\n            return per_replica_report_loss, per_replica_toks, top_preds\n\n        if phase == \'Test\':\n            SET_TRAIN_FLAG(False)\n            return self._evaluate(vs, reporting_fns, **kwargs)\n\n        strategy = self.strategy\n        num_replicas = strategy.num_replicas_in_sync\n\n        with strategy.scope():\n\n            SET_TRAIN_FLAG(False)\n            self.valid_epochs += 1\n\n            total_loss = tf.Variable(0.0)\n            total_toks = tf.Variable(0, dtype=tf.int32)\n            preds = []\n            golds = []\n\n            start = time.time()\n\n            test_iter = iter(vs)\n\n            for i in range(steps):\n                features, tgt = next(test_iter)\n                inputs = (features, tgt)\n                per_replica_loss, per_replica_toks, _ = strategy.experimental_run_v2(_replicated_valid_step, args=(inputs,))\n                total_loss.assign_add(strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None))\n                total_toks.assign_add(strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_toks, axis=None))\n                # Not sure a good way to get top preds merged yet\n\n            metrics = self.calc_metrics(total_loss.numpy(), total_toks.numpy())\n            self.report(\n                self.valid_epochs, metrics, start,\n                phase, \'EPOCH\', reporting_fns\n            )\n            return metrics\n\n    def distribute(self, dataset):\n        return self.strategy.experimental_distribute_dataset(dataset)\n\n@register_training_func(\'seq2seq\', name=""distributed"")\ndef fit_eager_distributed(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an language model using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n\n    model_file = get_model_file(\'seq2seq\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'perplexity\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    batchsz = kwargs[\'batchsz\']\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    tgt_key = model_params.get(\'tgt_key\')\n\n    src_lengths_key = model_params.get(\'src_lengths_key\')\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, src_lengths_key, dst=True))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=True)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, src_lengths_key, dst=True))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=True)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    trainer = Seq2SeqTrainerDistributedTf(model_params, **kwargs)\n    \n    train_dataset = trainer.distribute(train_dataset)\n    valid_dataset = trainer.distribute(valid_dataset)\n    \n    last_improved = 0\n    SET_TRAIN_FLAG(True)\n\n    for epoch in range(epochs):\n\n        trainer.train(train_dataset, steps=len(ts.examples) // ts.batchsz, reporting_fns=reporting_fns)\n        test_metrics = trainer.test(valid_dataset, steps=len(vs.examples) // vs.batchsz, reporting_fns=reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, src_lengths_key, dst=True))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        trainer.test(test_dataset, steps=len(es.examples) // es.batchsz, reporting_fns=reporting_fns, phase=\'Test\')\n\n'"
baseline/tf/seq2seq/training/eager.py,0,"b'import os\nimport numpy as np\nimport time\nimport tensorflow as tf\nfrom eight_mile.utils import listify\nfrom eight_mile.tf.layers import SET_TRAIN_FLAG, get_shape_as_list, autograph_options\nfrom eight_mile.tf.optz import EagerOptimizer\nfrom baseline.progress import create_progress_bar\nfrom baseline.utils import get_model_file, get_metric_cmp, convert_seq2seq_golds, convert_seq2seq_preds\nfrom eight_mile.bleu import bleu\nfrom baseline.model import create_model_for\nfrom baseline.train import register_training_func, Trainer\nfrom baseline.tf.seq2seq.training.utils import to_tensors, SHUF_BUF_SZ, NUM_PREFETCH\n\n\ndef loss(model, features, labels):\n    # Claims its T, B, H\n    logits = tf.transpose(model(features), [1, 0, 2])\n    # So ok, then transpose this too\n    labels = tf.transpose(labels, [1, 0])\n    # TxB loss mask\n    label_lengths = features[\'tgt_len\']\n    mx_seq_len = tf.reduce_max(label_lengths)-1\n    labels = labels[1:mx_seq_len + 1, :]\n    logits = logits[:mx_seq_len, :, :]\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n    loss_mask = tf.cast(tf.sequence_mask(label_lengths-1), dtype=tf.float32)\n    losses = losses * tf.transpose(loss_mask, [1, 0])\n\n    losses = tf.reduce_sum(losses)\n    losses /= tf.cast(tf.reduce_sum(label_lengths), tf.float32)\n    return losses\n\n\n\nclass Seq2SeqTrainerEagerTf(Trainer):\n    """"""Eager mode trainer for seq2sew\n\n    The trainer can run in 2 modes: `dataset` and `feed_dict`.  When the former, the graph is assumed to\n    be connected by features attached to the input so the `feed_dict` will only be used to pass dropout information.\n\n    When the latter, we will use the baseline DataFeed to read the object into the `feed_dict`\n    """"""\n    def __init__(self, model_params, **kwargs):\n        super().__init__()\n\n        if type(model_params) is dict:\n            self.model = create_model_for(\'seq2seq\', **model_params)\n        else:\n            self.model = model_params\n\n        self.tgt_rlut = kwargs[\'tgt_rlut\']\n        self.optimizer = EagerOptimizer(loss, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n        self._checkpoint = tf.train.Checkpoint(optimizer=self.optimizer.optimizer, model=self.model)\n        checkpoint_dir = \'{}-{}\'.format(""./tf-seq2seq"", os.getpid())\n        self.bleu_n_grams = int(kwargs.get(""bleu_n_grams"", 4))\n\n        self.checkpoint_manager = tf.train.CheckpointManager(self._checkpoint,\n                                                             directory=checkpoint_dir,\n                                                             max_to_keep=5)\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        self.checkpoint_manager.save()\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        print(self._checkpoint.restore(self.checkpoint_manager.latest_checkpoint))\n\n    @staticmethod\n    def _num_toks(y):\n        return tf.prod(get_shape_as_list(y))\n\n    def _num_toks(self, lens):\n        return tf.reduce_sum(lens)\n\n    def train(self, ts, reporting_fns, dataset=True):\n        """"""Train by looping over the steps\n\n        For a `tf.dataset`-backed `fit_func`, we are using the previously wired `dataset`s\n        in the model (and `dataset` is `True`).  For `feed_dict`, we convert the ts samples\n        to `feed_dict`s and hand them in one-by-one\n\n        :param ts: The training set\n        :param reporting_fns: A list of reporting hooks\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        SET_TRAIN_FLAG(True)\n        epoch_loss = tf.Variable(0.0)\n        epoch_div = tf.Variable(0, dtype=tf.int32)\n        nstep_loss = tf.Variable(0.0)\n        nstep_div = tf.Variable(0, dtype=tf.int32)\n        self.nstep_start = time.time()\n        start = time.time()\n\n        @tf.function\n        def _train_step(features, y):\n            """"""Replicated training step.""""""\n\n            loss = self.optimizer.update(self.model, features, y)\n            toks = self._num_toks(features[\'tgt_len\'])\n            report_loss = loss * tf.cast(toks, tf.float32)\n            return report_loss, toks\n\n        with autograph_options({""function_optimization"": False, ""layout_optimizer"": False}):\n            for features, y in ts:\n                features[\'dst\'] = y[:, :-1]\n                step_report_loss, step_toks = _train_step(features, y)\n                epoch_loss.assign_add(step_report_loss)\n                nstep_loss.assign_add(step_report_loss)\n                epoch_div.assign_add(step_toks)\n                nstep_div.assign_add(step_toks)\n\n                step = self.optimizer.global_step.numpy() + 1\n                if step % self.nsteps == 0:\n                    metrics = self.calc_metrics(nstep_loss.numpy(), nstep_div.numpy())\n                    self.report(\n                        step, metrics, self.nstep_start,\n                        \'Train\', \'STEP\', reporting_fns, self.nsteps\n                    )\n                    nstep_loss.assign(0.0)\n                    nstep_div.assign(0)\n                    self.nstep_start = time.time()\n\n        epoch_loss = epoch_loss.numpy()\n        epoch_div = epoch_div.numpy()\n        metrics = self.calc_metrics(epoch_loss, epoch_div)\n        self.train_epochs += 1\n        self.report(\n            self.train_epochs, metrics, start,\n            \'Train\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def calc_metrics(self, agg, norm):\n        metrics = super().calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\'])\n        return metrics\n\n\n    def _evaluate(self, es, reporting_fns, **kwargs):\n        """"""Run the model with beam search and report Bleu.\n\n        :param es: `tf.dataset` of input\n        :param reporting_fns: Input hooks\n        """"""\n        preds = []\n        golds = []\n        start = time.time()\n\n        for features, tgt in es:\n            features[\'dst\'] = tgt[:, :-1]\n            tgt_lens = features.pop(\'tgt_len\')\n            top_preds = self.model.predict(features, make_input=False, **kwargs)\n            preds.extend(convert_seq2seq_preds(top_preds[:, 0, :], self.tgt_rlut))\n            golds.extend(convert_seq2seq_golds(tgt, tgt_lens, self.tgt_rlut))\n        metrics = {\'bleu\': bleu(preds, golds, self.bleu_n_grams)[0]}\n        self.report(\n            0, metrics, start, \'Test\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def test(self, vs, reporting_fns, phase=\'Valid\', dataset=True, **kwargs):\n        """"""Run an epoch of testing over the dataset\n\n        If we are using a `tf.dataset`-based `fit_func`, we will just\n        cycle the number of steps and let the `dataset` yield new batches.\n\n        If we are using `feed_dict`s, we convert each batch from the `DataFeed`\n        and pass that into TF as the `feed_dict`\n\n        :param vs: A validation set\n        :param reporting_fns: Reporting hooks\n        :param phase: The phase of evaluation (`Test`, `Valid`)\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        SET_TRAIN_FLAG(False)\n        if phase == \'Test\':\n            return self._evaluate(vs, reporting_fns, **kwargs)\n\n        self.valid_epochs += 1\n\n        total_loss = 0\n        total_toks = 0\n        preds = []\n        golds = []\n\n        start = time.time()\n        for features, tgt in vs:\n            features[\'dst\'] = tgt[:, :-1]\n            top_preds = self.model.predict(features, beam=1, make_input=False)\n            loss_value = loss(self.model, features, tgt).numpy()\n            toks = tf.cast(self._num_toks(features[\'tgt_len\']), tf.float32).numpy()\n            total_loss += loss_value * toks\n            total_toks += toks\n            preds.extend(convert_seq2seq_preds(top_preds[:, 0, :], self.tgt_rlut))\n            golds.extend(convert_seq2seq_golds(tgt, features[\'tgt_len\'], self.tgt_rlut))\n\n        metrics = self.calc_metrics(total_loss, total_toks)\n        metrics[\'bleu\'] = bleu(preds, golds, self.bleu_n_grams)[0]\n        self.report(\n            self.valid_epochs, metrics, start,\n            phase, \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n\n@register_training_func(\'seq2seq\')\ndef fit_eager(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an language model using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n\n    model_file = get_model_file(\'seq2seq\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'perplexity\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    batchsz = kwargs[\'batchsz\']\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    tgt_key = model_params.get(\'tgt_key\')\n\n    src_lengths_key = model_params.get(\'src_lengths_key\')\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, src_lengths_key))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, src_lengths_key))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    trainer = Seq2SeqTrainerEagerTf(model_params, **kwargs)\n    last_improved = 0\n    SET_TRAIN_FLAG(True)\n\n    for epoch in range(epochs):\n\n        trainer.train(train_dataset, reporting_fns)\n        test_metrics = trainer.test(valid_dataset, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, src_lengths_key))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        trainer.test(test_dataset, reporting_fns, phase=\'Test\')\n\n'"
baseline/tf/seq2seq/training/feed.py,0,"b'import os\nimport time\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom eight_mile.tf.optz import optimizer\nfrom baseline.tf.tfy import TRAIN_FLAG, SET_TRAIN_FLAG\nfrom eight_mile.utils import listify\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.train import Trainer, create_trainer, register_trainer, register_training_func\nfrom baseline.model import create_model_for\nfrom collections import OrderedDict\n\n#@register_training_func(\'seq2seq\', \'feed_dict\')\n@register_training_func(\'seq2seq\', \'default\')\ndef fit(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train an encoder-decoder network using TensorFlow with a `feed_dict`.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 5\n        * *outfile* -- Model output file\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n        * *after_train_fn* (`func`) -- A callback to fire after ever epoch of training\n\n    :return: None\n    """"""\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n    model_file = get_model_file(\'seq2seq\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'perplexity\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    after_train_fn = kwargs[\'after_train_fn\'] if \'after_train_fn\' in kwargs else None\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n\n    for epoch in range(epochs):\n        trainer.train(ts, reporting_fns, dataset=False)\n        if after_train_fn is not None:\n            after_train_fn(trainer.model)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\', dataset=False)\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n    if es is not None:\n        trainer.recover_last_checkpoint()\n        trainer.test(es, reporting_fns, phase=\'Test\', dataset=False)\n'"
baseline/tf/seq2seq/training/utils.py,0,"b'import os\nimport time\nimport numpy as np\nimport tensorflow as tf\nfrom eight_mile.tf.layers import create_session, reload_checkpoint\nfrom eight_mile.tf.optz import optimizer\nfrom baseline.progress import create_progress_bar\nfrom eight_mile.bleu import bleu\n\nfrom baseline.utils import (\n    convert_seq2seq_golds,\n    convert_seq2seq_preds,\n)\n\nfrom baseline.train import Trainer, register_trainer\nfrom baseline.tf.tfy import TRAIN_FLAG, SET_TRAIN_FLAG\n\nfrom baseline.model import create_model_for\n\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\n\ndef to_tensors(ts, src_lengths_key, dst=False):\n    """"""Convert a data feed into a tuple of `features` (`dict`) and `y` values\n\n    This method is required to produce `tf.dataset`s from the input data feed.\n    Any fields ending with `_lengths` are ignored, unless they match the\n    `src_lengths_key` or `tgt_lengths_key`, in which case, they are converted to `src_len` and `tgt_len`\n\n    :param ts: The data feed to convert\n    :param lengths_key: This is a field passed from the model params specifying source of truth of the temporal lengths\n    :param dst: `bool` that says if we should prepare a `dst` tensor.  This is needed in distributed mode\n    :return: A `tuple` of `features` and `y` (labels)\n    """"""\n    keys = ts[0].keys()\n    # This is kind of a hack\n    keys = [k for k in keys if \'_lengths\' not in k and k != \'ids\'] + [src_lengths_key, ""tgt_lengths""]\n\n    features = dict((k, []) for k in keys)\n    for sample in ts:\n        for k in keys:\n            for s in sample[k]:\n                features[k].append(s)\n    features[\'src_len\'] = features[src_lengths_key]\n    del features[src_lengths_key]\n    features[\'tgt_len\'] = features[\'tgt_lengths\']\n    del features[\'tgt_lengths\']\n    features = dict((k, np.stack(v).astype(np.int32)) for k, v in features.items())\n    if dst:\n        features[\'dst\'] = features[\'tgt\'][:, :-1]\n    tgt = features.pop(\'tgt\')\n\n    return features, tgt\n\n\n@register_trainer(task=\'seq2seq\', name=\'default\')\nclass Seq2SeqTrainerTf(Trainer):\n    """"""A non-eager mode Trainer for seq2seq\n\n    The trainer can run in 2 modes: `dataset` and `feed_dict`.  When the former, the graph is assumed to\n    be connected by features attached to the input so the `feed_dict` will only be used to pass dropout information.\n\n    When the latter, we will use the baseline DataFeed to read the object into the `feed_dict`\n    """"""\n    def __init__(self, model_params, **kwargs):\n        """"""Create a Trainer, and give it the parameters needed to instantiate the model\n\n        :param model_params: The model parameters\n        :param kwargs: See below\n\n        :Keyword Arguments:\n\n          * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n          * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n          * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n          * *optim* (`str`) -- The name of the optimizer we are using\n          * *lr* (`float`) -- The learning rate we are using\n          * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n          * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n          * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n          * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n          * *tgt_rlut* (`dict`) -- This is a dictionary that converts from ints back to strings, used for predictions\n          * *beam* (`int`) -- The beam size to use at prediction time, defaults to `10`\n\n        """"""\n        super().__init__()\n        if type(model_params) is dict:\n            self.model = create_model_for(\'seq2seq\', **model_params)\n        else:\n            self.model = model_params\n        self.sess = self.model.sess\n        self.loss = self.model.create_loss()\n        self.test_loss = self.model.create_test_loss()\n        self.tgt_rlut = kwargs[\'tgt_rlut\']\n        self.base_dir = kwargs[\'basedir\']\n        self.global_step, self.train_op = optimizer(self.loss, colocate_gradients_with_ops=True, variables=self.model.trainable_variables, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', 500)\n        self.beam = kwargs.get(\'beam\', 10)\n        tables = tf.compat.v1.tables_initializer()\n        self.model.sess.run(tables)\n        self.model.sess.run(tf.compat.v1.global_variables_initializer())\n        self.model.set_saver(tf.compat.v1.train.Saver())\n        self.bleu_n_grams = int(kwargs.get(""bleu_n_grams"", 4))\n\n        init = tf.compat.v1.global_variables_initializer()\n        self.model.sess.run(init)\n        checkpoint = kwargs.get(\'checkpoint\')\n        if checkpoint is not None:\n            skip_blocks = kwargs.get(\'blocks_to_skip\', [\'OptimizeLoss\'])\n            reload_checkpoint(self.model.sess, checkpoint, skip_blocks)\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        checkpoint_dir = \'{}-{}\'.format(""./tf-seq2seq"", os.getpid())\n        self.model.saver.save(self.sess,\n                              os.path.join(checkpoint_dir, \'seq2seq\'),\n                              global_step=self.global_step,\n                              write_meta_graph=False)\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        latest = os.path.join(self.base_dir, \'seq2seq-model-tf-%d\' % os.getpid())\n        # logger.info(\'Reloading %s\', latest)\n        g = tf.Graph()\n        with g.as_default():\n            SET_TRAIN_FLAG(None)\n            sess = create_session()\n            self.model = self.model.load(latest, predict=True, beam=self.beam, session=sess)\n\n    def _num_toks(self, lens):\n        return np.sum(lens)\n\n    def calc_metrics(self, agg, norm):\n        """"""Calculate metrics\n\n        :param agg: The aggregated loss\n        :param norm: The number of steps to average over\n        :return: The metrics\n        """"""\n        metrics = super().calc_metrics(agg, norm)\n        metrics[\'perplexity\'] = np.exp(metrics[\'avg_loss\'])\n        return metrics\n\n    def train(self, ts, reporting_fns, dataset=True):\n        """"""Train by looping over the steps\n\n        For a `tf.dataset`-backed `fit_func`, we are using the previously wired `dataset`s\n        in the model (and `dataset` is `True`).  For `feed_dict`, we convert the ts samples\n        to `feed_dict`s and hand them in one-by-one\n\n        :param ts: The training set\n        :param reporting_fns: A list of reporting hooks\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        epoch_loss = 0\n        epoch_toks = 0\n\n        start = time.time()\n        self.nstep_start = start\n        for batch_dict in ts:\n            if dataset:\n                _, global_step, lossv = self.sess.run([self.train_op, self.global_step, self.loss],\n                                                      feed_dict={TRAIN_FLAG(): 1})\n            else:\n                feed_dict = self.model.make_input(batch_dict, True)\n                _, global_step, lossv = self.sess.run([self.train_op, self.global_step, self.loss], feed_dict=feed_dict)\n\n            # ?? How to get this cleaner?\n            toks = self._num_toks(batch_dict[\'tgt_lengths\'])\n            report_loss = lossv * toks\n\n            epoch_loss += report_loss\n            epoch_toks += toks\n            self.nstep_agg += report_loss\n            self.nstep_div += toks\n\n            if (global_step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    global_step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = self.calc_metrics(epoch_loss, epoch_toks)\n        self.train_epochs += 1\n        self.report(\n            self.train_epochs, metrics, start,\n            \'Train\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def _evaluate(self, es, reporting_fns):\n        """"""Run the model with beam search and report Bleu.\n\n        :param es: `DataFeed` of input\n        :param reporting_fns: Input hooks\n        """"""\n        pg = create_progress_bar(len(es))\n        preds = []\n        golds = []\n        start = time.time()\n        for batch_dict in pg(es):\n            tgt = batch_dict.pop(\'tgt\')\n            tgt_lens = batch_dict.pop(\'tgt_lengths\')\n            pred = [p[0] for p in self.model.predict(batch_dict)]\n            preds.extend(convert_seq2seq_preds(pred, self.tgt_rlut))\n            golds.extend(convert_seq2seq_golds(tgt, tgt_lens, self.tgt_rlut))\n        metrics = {\'bleu\': bleu(preds, golds, self.bleu_n_grams)[0]}\n        self.report(\n            0, metrics, start, \'Test\', \'EPOCH\', reporting_fns\n        )\n        return metrics\n\n    def test(self, vs, reporting_fns, phase=\'Valid\', dataset=True):\n        """"""Run an epoch of testing over the dataset\n\n        If we are using a `tf.dataset`-based `fit_func`, we will just\n        cycle the number of steps and let the `dataset` yield new batches.\n\n        If we are using `feed_dict`s, we convert each batch from the `DataFeed`\n        and pass that into TF as the `feed_dict`\n\n        :param vs: A validation set\n        :param reporting_fns: Reporting hooks\n        :param phase: The phase of evaluation (`Test`, `Valid`)\n        :param dataset: (`bool`) Are we using `tf.dataset`s\n        :return: Metrics\n        """"""\n        if phase == \'Test\' and not dataset:\n            return self._evaluate(vs, reporting_fns)\n        self.valid_epochs += 1\n\n        total_loss = 0\n        total_toks = 0\n        preds = []\n        golds = []\n\n        start = time.time()\n        pg = create_progress_bar(len(vs))\n        for batch_dict in pg(vs):\n\n            if dataset:\n                lossv, top_preds = self.model.sess.run([self.test_loss, self.model.decoder.best])\n            else:\n                feed_dict = self.model.make_input(batch_dict)\n                lossv, top_preds = self.model.sess.run([self.test_loss, self.model.decoder.best], feed_dict=feed_dict)\n            toks = self._num_toks(batch_dict[\'tgt_lengths\'])\n            total_loss += lossv * toks\n            total_toks += toks\n\n            preds.extend(convert_seq2seq_preds(top_preds.T, self.tgt_rlut))\n            golds.extend(convert_seq2seq_golds(batch_dict[\'tgt\'], batch_dict[\'tgt_lengths\'], self.tgt_rlut))\n\n        metrics = self.calc_metrics(total_loss, total_toks)\n        metrics[\'bleu\'] = bleu(preds, golds, self.bleu_n_grams)[0]\n        self.report(\n            self.valid_epochs, metrics, start,\n            phase, \'EPOCH\', reporting_fns\n        )\n        return metrics\n'"
baseline/tf/tagger/training/__init__.py,0,b'from eight_mile.utils import get_version\nimport tensorflow as tf\nif not tf.executing_eagerly():\n\n    from baseline.tf.tagger.training.datasets import *\n    from baseline.tf.tagger.training.feed import *\nelse:\n    from baseline.tf.tagger.training.eager import *\n'
baseline/tf/tagger/training/datasets.py,0,"b'import six\nimport os\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport logging\nfrom eight_mile.utils import listify\nfrom baseline.train import create_trainer, register_training_func\nfrom baseline.tf.tfy import TRAIN_FLAG\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tfy import reload_checkpoint\nfrom baseline.tf.tagger.training.utils import to_tensors, TaggerEvaluatorTf\n\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\nlogger = logging.getLogger(\'baseline\')\n\n\n@register_training_func(\'tagger\')\ndef fit_datasets(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train a tagger using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n    conll_output = kwargs.get(\'conll_output\', None)\n    span_type = kwargs.get(\'span_type\', \'iob\')\n    txts = kwargs.get(\'txts\', None)\n    model_file = get_model_file(\'tagger\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = kwargs.get(\'verbose\', {\'console\': kwargs.get(\'verbose_console\', False), \'file\': kwargs.get(\'verbose_file\', None)})\n    epochs = int(kwargs.get(\'epochs\', 20))\n\n    batchsz = kwargs[\'batchsz\']\n    ## First, make tf.datasets for ts, vs and es\n    # https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/README.md\n    # effective_batch_sz = args.batchsz*args.gpus\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    # This is a little awkward:\n    lengths_key = model_params.get(\'lengths_key\')\n\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, lengths_key))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.repeat(epochs + 1)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, lengths_key))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.repeat(epochs + 1)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    iter = tf.compat.v1.data.Iterator.from_structure(tf.compat.v1.data.get_output_types(train_dataset),\n                                                     tf.compat.v1.data.get_output_shapes(train_dataset))\n\n    features, y = iter.get_next()\n    # Add features to the model params\n    model_params.update(features)\n    model_params[\'y\'] = y\n    # create the initialisation operations\n    train_init_op = iter.make_initializer(train_dataset)\n    valid_init_op = iter.make_initializer(valid_dataset)\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    TRAIN_FLAG()\n    trainer = create_trainer(model_params, **kwargs)\n\n    last_improved = 0\n\n    for epoch in range(epochs):\n        trainer.sess.run(train_init_op)\n        trainer.train(ts, reporting_fns)\n        trainer.sess.run(valid_init_op)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\')\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, lengths_key))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.repeat(epochs + 1)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        test_init_op = iter.make_initializer(test_dataset)\n        trainer.sess.run(test_init_op)\n        # What to do about overloading this??\n        evaluator = TaggerEvaluatorTf(trainer.model, span_type, verbose)\n        start = time.time()\n        test_metrics = evaluator.test(es, conll_output=conll_output, txts=txts)\n        duration = time.time() - start\n        for reporting in reporting_fns:\n            reporting(test_metrics, 0, \'Test\')\n        trainer.log.debug({\'phase\': \'Test\', \'time\': duration})\n'"
baseline/tf/tagger/training/eager.py,0,"b'import six\nimport os\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport logging\nfrom eight_mile.utils import listify, revlut, to_spans, write_sentence_conll, per_entity_f1, span_f1, conlleval_output\nfrom eight_mile.tf.layers import TRAIN_FLAG, SET_TRAIN_FLAG, reload_checkpoint, get_shape_as_list, autograph_options\nfrom eight_mile.tf.optz import EagerOptimizer\nfrom baseline.progress import create_progress_bar\nfrom baseline.model import create_model_for\nfrom baseline.train import register_training_func, EpochReportingTrainer\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tagger.training.utils import to_tensors\n# Number of batches to prefetch if using tf.datasets\nNUM_PREFETCH = 2\n# The shuffle buffer\nSHUF_BUF_SZ = 5000\n\nlogger = logging.getLogger(\'baseline\')\n\n\ndef loss(model, x, y):\n    unary = model.transduce(x)\n    return model.decoder.neg_log_loss(unary, y, x[\'lengths\'])\n\n\nclass TaggerEvaluatorEagerTf:\n    """"""Performs evaluation on tagger output\n    """"""\n    def __init__(self, model, span_type, verbose):\n        """"""Construct from an existing model\n\n        :param model: A model\n        :param span_type: (`str`) The span type\n        :param verbose: (`bool`) Be verbose?\n        """"""\n        self.model = model\n        self.idx2label = revlut(model.labels)\n        self.span_type = span_type\n        if verbose:\n            print(\'Setting span type {}\'.format(self.span_type))\n        self.verbose = verbose\n\n    def process_batch(self, batch, truth):\n        guess = self.model(batch)\n        sentence_lengths = batch[\'lengths\']\n\n        correct_labels = 0\n        total_labels = 0\n\n        # For fscore\n        gold_chunks = []\n        pred_chunks = []\n\n        # For each sentence\n        for b in range(len(guess)):\n            length = sentence_lengths[b]\n            sentence = guess[b][:length].numpy()\n            # truth[b] is padded, cutting at :length gives us back true length\n            gold = truth[b][:length].numpy()\n            correct_labels += np.sum(np.equal(sentence, gold))\n            total_labels += length\n\n            gold_chunks.append(set(to_spans(gold, self.idx2label, self.span_type, self.verbose)))\n            pred_chunks.append(set(to_spans(sentence, self.idx2label, self.span_type, self.verbose)))\n\n        return correct_labels, total_labels, gold_chunks, pred_chunks\n\n    def test(self, ts, steps=0, **kwargs):\n        """"""Method that evaluates on some data.  There are 2 modes this can run in, `feed_dict` and `dataset`\n\n        In `feed_dict` mode, the model cycles the test data batch-wise and feeds each batch in with a `feed_dict`.\n        In `dataset` mode, the data is still passed in to this method, but it is not passed in a `feed_dict` and is\n        mostly superfluous since the features are grafted right onto the graph.  However, we do use it for supplying\n        the ground truth, ids and text, so it is essential that the caller does not shuffle the data\n        :param ts: The test set\n        :param conll_output: (`str`) An optional file output\n        :param txts: A list of text data associated with the encoded batch\n        :param dataset: (`bool`) Is this using `tf.dataset`s\n        :return: The metrics\n        """"""\n        total_correct = total_sum = 0\n        gold_spans = []\n        pred_spans = []\n\n        pg = create_progress_bar(steps)\n        metrics = {}\n        for features, y in pg(ts):\n            correct, count, golds, guesses = self.process_batch(features, y)\n            total_correct += correct\n            total_sum += count\n            gold_spans.extend(golds)\n            pred_spans.extend(guesses)\n\n        total_acc = total_correct / float(total_sum)\n        # Only show the fscore if requested\n        metrics[\'f1\'] = span_f1(gold_spans, pred_spans)\n        metrics[\'acc\'] = total_acc\n        if self.verbose:\n            conll_metrics = per_entity_f1(gold_spans, pred_spans)\n            conll_metrics[\'acc\'] = total_acc * 100\n            conll_metrics[\'tokens\'] = total_sum\n            logger.info(conlleval_output(conll_metrics))\n\n\n        return metrics\n\n\nclass TaggerTrainerEagerTf(EpochReportingTrainer):\n    """"""A Trainer to use for eager mode training\n    """"""\n    def __init__(self, model_params, **kwargs):\n        """"""Create a Trainer, and give it the parameters needed to instantiate the model\n\n        :param model_params: The model parameters\n        :param kwargs: See below\n\n        :Keyword Arguments:\n\n          * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n          * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n          * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n          * *optim* (`str`) -- The name of the optimizer we are using\n          * *lr* (`float`) -- The learning rate we are using\n          * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n          * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n          * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n          * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n        """"""\n        super().__init__()\n        if type(model_params) is dict:\n            self.model = create_model_for(\'tagger\', **model_params)\n        else:\n            self.model = model_params\n        span_type = kwargs.get(\'span_type\', \'iob\')\n        verbose = kwargs.get(\'verbose\', False)\n        self.evaluator = TaggerEvaluatorEagerTf(self.model, span_type, verbose)\n        self.optimizer = EagerOptimizer(loss, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', six.MAXSIZE)\n        self._checkpoint = tf.train.Checkpoint(optimizer=self.optimizer.optimizer, model=self.model)\n        checkpoint_dir = \'{}-{}\'.format(""./tf-tagger"", os.getpid())\n\n        self.checkpoint_manager = tf.train.CheckpointManager(self._checkpoint,\n                                                             directory=checkpoint_dir,\n                                                             max_to_keep=5)\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        self.checkpoint_manager.save()\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        print(self._checkpoint.restore(self.checkpoint_manager.latest_checkpoint))\n\n    @staticmethod\n    def _get_batchsz(batch_dict):\n        return batch_dict[\'y\'].shape[0]\n\n    def _train(self, loader, steps=0, **kwargs):\n        """"""Train an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.  We do use a `feed_dict` for passing the `TRAIN_FLAG` in either case\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n         * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n         * *reporting_fns* (`list`) A list of reporting hooks to use\n\n        :return: Metrics\n        """"""\n        SET_TRAIN_FLAG(True)\n        reporting_fns = kwargs.get(\'reporting_fns\', [])\n        pg = create_progress_bar(steps)\n        epoch_loss = tf.Variable(0.0)\n        epoch_div = tf.Variable(0, dtype=tf.int32)\n        nstep_loss = tf.Variable(0.0)\n        nstep_div = tf.Variable(0, dtype=tf.int32)\n        self.nstep_start = time.time()\n\n        @tf.function\n        def _train_step(inputs):\n            features, y = inputs\n            loss = self.optimizer.update(self.model, features, y)\n            batchsz = get_shape_as_list(y)[0]\n            report_loss = loss * batchsz\n            return report_loss, batchsz\n\n        with autograph_options({""function_optimization"": False, ""layout_optimizer"": False}):\n            for inputs in pg(loader):\n                step_report_loss, step_batchsz = _train_step(inputs)\n                epoch_loss.assign_add(step_report_loss)\n                nstep_loss.assign_add(step_report_loss)\n                epoch_div.assign_add(step_batchsz)\n                nstep_div.assign_add(step_batchsz)\n\n                step = self.optimizer.global_step.numpy() + 1\n                if step % self.nsteps == 0:\n                    metrics = self.calc_metrics(nstep_loss.numpy(), nstep_div.numpy())\n                    self.report(\n                        step, metrics, self.nstep_start,\n                        \'Train\', \'STEP\', reporting_fns, self.nsteps\n                    )\n                    nstep_loss.assign(0.0)\n                    nstep_div.assign(0)\n                    self.nstep_start = time.time()\n\n        epoch_loss = epoch_loss.numpy()\n        epoch_div = epoch_div.numpy()\n        metrics = self.calc_metrics(epoch_loss, epoch_div)\n        return metrics\n\n    def _test(self, ts, steps=0, **kwargs):\n        """"""Test an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n          * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n          * *reporting_fns* (`list`) A list of reporting hooks to use\n          * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n\n        :return: Metrics\n        """"""\n        return self.evaluator.test(ts, steps, **kwargs)\n\n\n@register_training_func(\'tagger\')\ndef fit_eager(model_params, ts, vs, es=None, **kwargs):\n    """"""\n    Train a tagger using TensorFlow with `tf.dataset`.  This\n    is the default behavior for training.\n\n    :param model_params: The model (or parameters to create the model) to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n    conll_output = kwargs.get(\'conll_output\', None)\n    span_type = kwargs.get(\'span_type\', \'iob\')\n    txts = kwargs.get(\'txts\', None)\n    model_file = get_model_file(\'tagger\', \'tf\', kwargs.get(\'basedir\'))\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = kwargs.get(\'verbose\', {\'console\': kwargs.get(\'verbose_console\', False), \'file\': kwargs.get(\'verbose_file\', None)})\n    epochs = int(kwargs.get(\'epochs\', 20))\n\n    batchsz = kwargs[\'batchsz\']\n    test_batchsz = kwargs.get(\'test_batchsz\', batchsz)\n    lengths_key = model_params.get(\'lengths_key\')\n\n    train_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(ts, lengths_key))\n    train_dataset = train_dataset.shuffle(buffer_size=SHUF_BUF_SZ)\n    train_dataset = train_dataset.batch(batchsz, drop_remainder=False)\n    train_dataset = train_dataset.prefetch(NUM_PREFETCH)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(vs, lengths_key))\n    valid_dataset = valid_dataset.batch(batchsz, drop_remainder=False)\n    valid_dataset = valid_dataset.prefetch(NUM_PREFETCH)\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    trainer = TaggerTrainerEagerTf(model_params, **kwargs)\n\n    last_improved = 0\n\n    SET_TRAIN_FLAG(True)\n\n    for epoch in range(epochs):\n        trainer.train(train_dataset, reporting_fns, steps=len(ts))\n        test_metrics = trainer.test(valid_dataset, reporting_fns, phase=\'Valid\', steps=len(vs))\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n\n    if es is not None:\n        print(\'Reloading best checkpoint\')\n        trainer.recover_last_checkpoint()\n        test_dataset = tf.data.Dataset.from_tensor_slices(to_tensors(es, lengths_key))\n        test_dataset = test_dataset.batch(test_batchsz, drop_remainder=False)\n        test_dataset = test_dataset.prefetch(NUM_PREFETCH)\n        evaluator = TaggerEvaluatorEagerTf(trainer.model, span_type, verbose)\n        start = time.time()\n        test_metrics = evaluator.test(test_dataset, conll_output=conll_output, txts=txts, steps=len(es))\n        duration = time.time() - start\n        for reporting in reporting_fns:\n            reporting(test_metrics, 0, \'Test\')\n        trainer.log.debug({\'phase\': \'Test\', \'time\': duration})\n'"
baseline/tf/tagger/training/feed.py,0,"b'import six\nimport os\nimport time\nimport logging\nfrom eight_mile.utils import listify\n\nfrom baseline.train import create_trainer, register_training_func\nfrom baseline.tf.tfy import TRAIN_FLAG\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom baseline.tf.tfy import reload_checkpoint\nfrom baseline.tf.tagger.training.utils import TaggerEvaluatorTf\n\nlogger = logging.getLogger(\'baseline\')\n\n\n@register_training_func(\'tagger\', \'feed_dict\')\ndef fit(model_params, ts, vs, es, **kwargs):\n    """"""\n    Train a classifier using TensorFlow with a `feed_dict`.  This\n    is the previous default behavior for training.  To use this, you need to pass\n    `fit_func: feed_dict` in your MEAD config\n\n    :param model_params: The model to train\n    :param ts: A training data set\n    :param vs: A validation data set\n    :param es: A test data set, can be None\n    :param kwargs:\n        See below\n\n    :Keyword Arguments:\n        * *do_early_stopping* (``bool``) --\n          Stop after evaluation data is no longer improving.  Defaults to True\n        * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n        * *epochs* (``int``) -- how many epochs.  Default to 20\n        * *outfile* -- Model output file, defaults to classifier-model.pyth\n        * *patience* --\n           How many epochs where evaluation is no longer improving before we give up\n        * *reporting* --\n           Callbacks which may be used on reporting updates\n        * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n        * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n        * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n        * *optim* (`str`) -- The name of the optimizer we are using\n        * *lr* (`float`) -- The learning rate we are using\n        * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n        * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n        * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n        * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n    :return: None\n    """"""\n    epochs = int(kwargs.get(\'epochs\', 5))\n    patience = int(kwargs.get(\'patience\', epochs))\n    conll_output = kwargs.get(\'conll_output\', None)\n    span_type = kwargs.get(\'span_type\', \'iob\')\n    txts = kwargs.get(\'txts\', None)\n    model_file = get_model_file(\'tagger\', \'tf\', kwargs.get(\'basedir\'))\n    TRAIN_FLAG()\n\n    trainer = create_trainer(model_params, **kwargs)\n\n    do_early_stopping = bool(kwargs.get(\'do_early_stopping\', True))\n    verbose = bool(kwargs.get(\'verbose\', False))\n\n    best_metric = 0\n    if do_early_stopping:\n        early_stopping_metric = kwargs.get(\'early_stopping_metric\', \'acc\')\n        early_stopping_cmp, best_metric = get_metric_cmp(early_stopping_metric, kwargs.get(\'early_stopping_cmp\'))\n        patience = kwargs.get(\'patience\', epochs)\n        print(\'Doing early stopping on [%s] with patience [%d]\' % (early_stopping_metric, patience))\n\n    reporting_fns = listify(kwargs.get(\'reporting\', []))\n    print(\'reporting\', reporting_fns)\n\n    last_improved = 0\n    for epoch in range(epochs):\n\n        trainer.train(ts, reporting_fns, dataset=False)\n        test_metrics = trainer.test(vs, reporting_fns, phase=\'Valid\', dataset=False)\n\n        if do_early_stopping is False:\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif early_stopping_cmp(test_metrics[early_stopping_metric], best_metric):\n            last_improved = epoch\n            best_metric = test_metrics[early_stopping_metric]\n            print(\'New best %.3f\' % best_metric)\n            trainer.checkpoint()\n            trainer.model.save(model_file)\n\n        elif (epoch - last_improved) > patience:\n            print(\'Stopping due to persistent failures to improve\')\n            break\n\n    if do_early_stopping is True:\n        print(\'Best performance on %s: %.3f at epoch %d\' % (early_stopping_metric, best_metric, last_improved))\n    if es is not None:\n\n        trainer.recover_last_checkpoint()\n        # What to do about overloading this??\n        evaluator = TaggerEvaluatorTf(trainer.model, span_type, verbose)\n        start = time.time()\n        test_metrics = evaluator.test(es, conll_output=conll_output, txts=txts, dataset=False)\n        duration = time.time() - start\n        for reporting in reporting_fns:\n            reporting(test_metrics, 0, \'Test\')\n        trainer.log.debug({\'phase\': \'Test\', \'time\': duration})\n'"
baseline/tf/tagger/training/utils.py,0,"b'import six\nimport os\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport logging\nfrom eight_mile.tf.optz import optimizer\nfrom eight_mile.utils import to_spans, Offsets, revlut, span_f1, per_entity_f1, conlleval_output, write_sentence_conll\n\nfrom baseline.progress import create_progress_bar\nfrom baseline.train import EpochReportingTrainer, create_trainer, register_trainer, register_training_func\nfrom baseline.tf.tfy import TRAIN_FLAG\nfrom baseline.model import create_model_for\nfrom baseline.utils import get_model_file, get_metric_cmp\nfrom eight_mile.tf.layers import reload_checkpoint\n\nlogger = logging.getLogger(\'baseline\')\n\n\ndef to_tensors(ts, lengths_key):\n    """"""Convert a data feed into a tuple of `features` (`dict`) and `y` values\n\n    This method is required to produce `tf.dataset`s from the input data feed.\n    Any fields ending with `_lengths` are ignored, unless they match the\n    `lengths_key` name (as are `ids`)\n\n    :param ts: The data feed to convert\n    :param lengths_key: This is a field passed from the model params specifying source of truth of the temporal lengths\n    :return: A `tuple` of `features` and `y` (labels)\n    """"""\n    keys = ts[0].keys()\n    # This is kind of a hack\n    keys = [k for k in keys if \'_lengths\' not in k and k != \'ids\'] + [lengths_key]\n\n    features = dict((k, []) for k in keys)\n    for sample in ts:\n        for k in features.keys():\n            # add each sample\n            for s in sample[k]:\n                features[k].append(s)\n\n    features[\'lengths\'] = features[lengths_key]\n    del features[lengths_key]\n    features = dict((k, np.stack(v)) for k, v in features.items())\n    y = features.pop(\'y\')\n    return features, y\n\n\nclass TaggerEvaluatorTf(object):\n    """"""Performs evaluation on tagger output\n    """"""\n    def __init__(self, model, span_type, verbose):\n        """"""Construct from an existing model\n\n        :param model: A model\n        :param span_type: (`str`) The span type\n        :param verbose: (`bool`) Be verbose?\n        """"""\n        self.model = model\n        self.sess = self.model.sess\n        self.idx2label = revlut(model.labels)\n        self.span_type = span_type\n        if verbose:\n            print(\'Setting span type {}\'.format(self.span_type))\n        self.verbose = verbose\n\n    def process_batch(self, batch_dict, handle, txts, dataset=True):\n        if dataset:\n            guess = self.sess.run(self.model.best)\n        else:\n            feed_dict = self.model.make_input(batch_dict)\n            guess = self.sess.run(self.model.best, feed_dict=feed_dict)\n\n        sentence_lengths = batch_dict[self.model.lengths_key]\n\n        ids = batch_dict[\'ids\']\n        truth = batch_dict[\'y\']\n        correct_labels = 0\n        total_labels = 0\n\n        # For fscore\n        gold_chunks = []\n        pred_chunks = []\n\n        # For each sentence\n        for b in range(len(guess)):\n            length = sentence_lengths[b]\n            sentence = guess[b][:length]\n            # truth[b] is padded, cutting at :length gives us back true length\n            gold = truth[b][:length]\n\n            valid_guess = sentence[gold != Offsets.PAD]\n            valid_gold = gold[gold != Offsets.PAD]\n            valid_sentence_length = np.sum(gold != Offsets.PAD)\n            correct_labels += np.sum(np.equal(valid_guess, valid_gold))\n            total_labels += valid_sentence_length\n\n            gold_chunks.append(set(to_spans(valid_gold, self.idx2label, self.span_type, self.verbose)))\n            pred_chunks.append(set(to_spans(valid_guess, self.idx2label, self.span_type, self.verbose)))\n\n            # Should we write a file out?  If so, we have to have txts\n            if handle is not None:\n                id = ids[b]\n                txt = txts[id]\n                write_sentence_conll(handle, valid_guess, valid_gold, txt, self.idx2label)\n\n        return correct_labels, total_labels, gold_chunks, pred_chunks\n\n    def test(self, ts, conll_output=None, txts=None, dataset=True):\n        """"""Method that evaluates on some data.  There are 2 modes this can run in, `feed_dict` and `dataset`\n\n        In `feed_dict` mode, the model cycles the test data batch-wise and feeds each batch in with a `feed_dict`.\n        In `dataset` mode, the data is still passed in to this method, but it is not passed in a `feed_dict` and is\n        mostly superfluous since the features are grafted right onto the graph.  However, we do use it for supplying\n        the ground truth, ids and text, so it is essential that the caller does not shuffle the data\n        :param ts: The test set\n        :param conll_output: (`str`) An optional file output\n        :param txts: A list of text data associated with the encoded batch\n        :param dataset: (`bool`) Is this using `tf.dataset`s\n        :return: The metrics\n        """"""\n        total_correct = total_sum = 0\n        gold_spans = []\n        pred_spans = []\n\n        steps = len(ts)\n        pg = create_progress_bar(steps)\n        metrics = {}\n        # Only if they provide a file and the raw txts, we can write CONLL file\n        handle = None\n        if conll_output is not None and txts is not None:\n            handle = open(conll_output, ""w"")\n\n        try:\n            for batch_dict in pg(ts):\n                correct, count, golds, guesses = self.process_batch(batch_dict, handle, txts, dataset)\n                total_correct += correct\n                total_sum += count\n                gold_spans.extend(golds)\n                pred_spans.extend(guesses)\n\n            total_acc = total_correct / float(total_sum)\n            # Only show the fscore if requested\n            metrics[\'f1\'] = span_f1(gold_spans, pred_spans)\n            metrics[\'acc\'] = total_acc\n            if self.verbose:\n                conll_metrics = per_entity_f1(gold_spans, pred_spans)\n                conll_metrics[\'acc\'] = total_acc * 100\n                conll_metrics[\'tokens\'] = total_sum\n                logger.info(conlleval_output(conll_metrics))\n        finally:\n            if handle is not None:\n                handle.close()\n\n        return metrics\n\n\n@register_trainer(task=\'tagger\', name=\'default\')\nclass TaggerTrainerTf(EpochReportingTrainer):\n    """"""A Trainer to use if not using eager mode\n\n    The trainer can run in 2 modes: `dataset` and `feed_dict`.  When the former, the graph is assumed to\n    be connected by features attached to the input so the `feed_dict` will only be used to pass dropout information.\n\n    When the latter, we will use the baseline DataFeed to read the object into the `feed_dict`\n    """"""\n    def __init__(self, model_params, **kwargs):\n        """"""Create a Trainer, and give it the parameters needed to instantiate the model\n\n        :param model_params: The model parameters\n        :param kwargs: See below\n\n        :Keyword Arguments:\n\n          * *nsteps* (`int`) -- If we should report every n-steps, this should be passed\n          * *ema_decay* (`float`) -- If we are doing an exponential moving average, what decay to us4e\n          * *clip* (`int`) -- If we are doing gradient clipping, what value to use\n          * *optim* (`str`) -- The name of the optimizer we are using\n          * *lr* (`float`) -- The learning rate we are using\n          * *mom* (`float`) -- If we are using SGD, what value to use for momentum\n          * *beta1* (`float`) -- Adam-specific hyper-param, defaults to `0.9`\n          * *beta2* (`float`) -- Adam-specific hyper-param, defaults to `0.999`\n          * *epsilon* (`float`) -- Adam-specific hyper-param, defaults to `1e-8\n\n        """"""\n        super().__init__()\n        if type(model_params) is dict:\n            self.model = create_model_for(\'tagger\', **model_params)\n        else:\n            self.model = model_params\n        self.sess = self.model.sess\n        self.loss = self.model.create_loss()\n        span_type = kwargs.get(\'span_type\', \'iob\')\n        verbose = kwargs.get(\'verbose\', False)\n        self.evaluator = TaggerEvaluatorTf(self.model, span_type, verbose)\n        self.global_step, self.train_op = optimizer(self.loss, colocate_gradients_with_ops=True, variables=self.model.trainable_variables, **kwargs)\n        self.nsteps = kwargs.get(\'nsteps\', six.MAXSIZE)\n        tables = tf.compat.v1.tables_initializer()\n        self.model.sess.run(tables)\n        init = tf.compat.v1.global_variables_initializer()\n        self.model.sess.run(init)\n        saver = tf.compat.v1.train.Saver()\n        self.model.save_using(saver)\n        checkpoint = kwargs.get(\'checkpoint\')\n        if checkpoint is not None:\n            skip_blocks = kwargs.get(\'blocks_to_skip\', [\'OptimizeLoss\'])\n            reload_checkpoint(self.model.sess, checkpoint, skip_blocks)\n\n    def checkpoint(self):\n        """"""This method saves a checkpoint\n\n        :return: None\n        """"""\n        checkpoint_dir = \'{}-{}\'.format(""./tf-tagger"", os.getpid())\n        self.model.saver.save(self.sess, os.path.join(checkpoint_dir, \'tagger\'),\n                              global_step=self.global_step,\n                              write_meta_graph=False)\n\n    def recover_last_checkpoint(self):\n        """"""Recover the last saved checkpoint\n\n        :return: None\n        """"""\n        checkpoint_dir = \'{}-{}\'.format(""./tf-tagger"", os.getpid())\n        latest = tf.train.latest_checkpoint(checkpoint_dir)\n        print(\'Reloading \' + latest)\n        self.model.saver.restore(self.model.sess, latest)\n\n    @staticmethod\n    def _get_batchsz(batch_dict):\n        return batch_dict[\'y\'].shape[0]\n\n    def _train(self, ts, **kwargs):\n        """"""Train an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.  We do use a `feed_dict` for passing the `TRAIN_FLAG` in either case\n\n        :param ts: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n         * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n         * *reporting_fns* (`list`) A list of reporting hooks to use\n\n        :return: Metrics\n        """"""\n        use_dataset = kwargs.get(\'dataset\', True)\n        reporting_fns = kwargs.get(\'reporting_fns\', [])\n        epoch_loss = 0\n        epoch_div = 0\n        steps = len(ts)\n        pg = create_progress_bar(steps)\n        for batch_dict in pg(ts):\n            if use_dataset:\n                _, step, lossv = self.sess.run([self.train_op, self.global_step, self.loss],\n                                               feed_dict={TRAIN_FLAG(): 1})\n            else:\n                feed_dict = self.model.make_input(batch_dict, True)\n                _, step, lossv = self.sess.run([self.train_op, self.global_step, self.loss], feed_dict=feed_dict)\n\n            batchsz = self._get_batchsz(batch_dict)\n            report_loss = lossv * batchsz\n            epoch_loss += report_loss\n            epoch_div += batchsz\n            self.nstep_agg += report_loss\n            self.nstep_div += batchsz\n            if (step + 1) % self.nsteps == 0:\n                metrics = self.calc_metrics(self.nstep_agg, self.nstep_div)\n                self.report(\n                    step + 1, metrics, self.nstep_start,\n                    \'Train\', \'STEP\', reporting_fns, self.nsteps\n                )\n                self.reset_nstep()\n\n        metrics = self.calc_metrics(epoch_loss, epoch_div)\n        return metrics\n\n    def _test(self, ts, dataset=True):\n        """"""Test an epoch of data using either the input loader or using `tf.dataset`\n\n        In non-`tf.dataset` mode, we cycle the loader data feed, and pull a batch and feed it to the feed dict\n        When we use `tf.dataset`s under the hood, this function simply uses the loader to know how many steps\n        to train.\n\n        :param loader: A data feed\n        :param kwargs: See below\n\n        :Keyword Arguments:\n          * *dataset* (`bool`) Set to `True` if using `tf.dataset`s, defaults to `True`\n          * *reporting_fns* (`list`) A list of reporting hooks to use\n          * *verbose* (`dict`) A dictionary containing `console` boolean and `file` name if on\n\n        :return: Metrics\n        """"""\n        return self.evaluator.test(ts, dataset=dataset)\n'"
layers/eight_mile/calibration/metrics/__init__.py,0,"b'from eight_mile.calibration.metrics.calibration_error import expected_calibration_error, maximum_calibration_error\n'"
layers/eight_mile/calibration/metrics/calibration_error.py,0,"b'import numpy as np\n\n\n__all__ = [""expected_calibration_error"", ""maximum_calibration_error""]\n\n\ndef expected_calibration_error(\n    bin_mean_acc: np.ndarray,\n    bin_mean_conf: np.ndarray,\n    bin_counts: np.ndarray,\n) -> float:\n    """"""The difference in expectation between the confidence and the accuracy.\n\n    This is an approximation of eq (2) as described in eq (5)\n        from https://arxiv.org/abs/1706.04599\n\n    This is the absolute difference between the confidence and accuracy of each\n    bin weighted by the number of samples in each bin.\n\n    :param bin_mean_acc: `[B]` an array with the accuracy of each bin as elements\n    :param bin_meanc_conf: `[B]` an array with the mean confidence of each bin as elements\n    :param bin_counts: `[B]` an array with the number of samples in each bin\n\n    :returns: The ECE between the accuracy and confidence distributions via binning.\n    """"""\n    abs_differences = np.abs(bin_mean_acc - bin_mean_conf)\n    coeff = bin_counts / np.sum(bin_counts)\n    return np.sum(coeff * abs_differences)\n\n\ndef maximum_calibration_error(\n    bin_mean_acc: np.ndarray,\n    bin_mean_conf: np.ndarray,\n    bin_counts: np.ndarray,\n) -> float:\n    """"""The worst case deviation between confidence and accuracy.\n\n    This is an approximation of eq (4) as described in eq (5)\n        from https://arxiv.org/abs/1706.04599\n\n    This is the maximum absolute difference between the confidence and accuracy\n    of each bin. The bin_counts are used to filter out bins that have no samples\n    in them. This seems like it is very dependent on the number of bins you have\n    and should therefore be kept constant when comparing models.\n\n    :param bin_mean_acc:\n    :param bin_mean_conf:\n    :param bin_counts:\n\n    :returns: The MCE between the accuracy and confidence distributions via binning.\n    """"""\n    abs_differences = np.abs(bin_mean_acc - bin_mean_conf)\n    filtered = abs_differences[bin_counts != 0]\n    return np.max(filtered)\n'"
layers/eight_mile/calibration/plot/__init__.py,0,"b'from eight_mile.calibration.plot.confidence_histogram import confidence_histogram\nfrom eight_mile.calibration.plot.reliability_diagram import reliability_diagram, reliability_curve\n'"
layers/eight_mile/calibration/plot/confidence_histogram.py,0,"b'from typing import Optional, Tuple\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.figure import Figure\n\n\n__all__ = [""confidence_histogram""]\n\n\ndef confidence_histogram(\n    left_bins: np.array,\n    bin_counts: np.array,\n    acc: Optional[float] = None,\n    avg_conf: Optional[float] = None,\n    x_ticks: Optional[np.array] = None,\n    y_ticks: Optional[np.array] = None,\n    title: Optional[str] = ""Confidence Distribution"",\n    y_label: Optional[str] = ""% of Samples"",\n    x_label: Optional[str] = ""Confidence"",\n    conf_label: Optional[str] = ""Avg. Confidence"",\n    acc_label: Optional[str] = ""Accuracy"",\n    label_y: float = 0.4,\n    conf_spacer: float = 0.015,\n    acc_spacer: float = -0.07,\n    edge: str = ""k"",\n    color: str = ""b"",\n    fig_size: int = 3,\n    ax: Optional[Axes] = None,\n    fig: Optional[Figure] = None,\n) -> Tuple[Figure, Axes]:\n    """"""Plot a reliability graph a la https://arxiv.org/abs/1706.04599\n\n    :param left_bins: The bins that define the groups, they should specify the left\n        edge of the bin\n    :param accs: The average accuracy for examples in that bin\n    :param x_ticks: Where to show the ticks along the x axis. If not provided it\n        will display every other tick, pass an empty list to skip adding ticks\n    :param y_ticks: Where to show the ticks along the y axis. If not provided it\n        uses default matplotlib ticks, padd an empty list to skip adding ticks\n    :param title: The title for the axes, pass None to skip adding a title to the axes\n    :param y_label: A label for the y axes, pass None to skip adding a label\n    :param x_label: A label for the x axes, pass None to skip adding a label\n    :param conf_label: A label to put on the line representing the average confidence\n    :param acc_label: A label to put on the line representing the accuract\n    :param label_y: Where to start the above labels alon the y axis, 0 is the bottom, 1 is the top\n    :param conf_spacer: How far off the line to put the confidence label, use positive to move\n        right of the line and negative to move to the left\n    :param acc_spacer: How far off the line to put the accuracy label, use positive to move right\n        of the line and negative to move to the left\n    :param edge: The color to use for the edges of the output bins\n    :param color: The color to use for the main section of the output bins\n    :param fig_size: The figure size, only used if creating the figure/axis from scratch\n    :param ax: An Axes object that we can create the graph in, if None it will be created\n    :param fig: A figure that holds the axes object\n\n    :returns: `Tuple(fig, ax)` If the figure and ax are passed in they are returned, if they\n        were not passed in the created ones are returned. The don\'t use the figure at all but\n        because we want to return both in case we created them.\n    """"""\n    # If niether a figure non-axes are passed in we create them.\n    if ax is None and fig is None:\n        fig, ax = plt.subplots(1, 1, figsize=(fig_size, fig_size))\n    # If the axes was not passed in but the figure was we get the current active\n    # axes for the figure.\n    if ax is None and fig is not None:\n        ax = fig.gca()\n    # If the axes is provided but the figure we look up the figure on the axes\n    if ax is not None and fig is None:\n        fig = ax.figure\n    widths = np.diff(left_bins, append=1)\n    ax.bar(\n        left_bins,\n        bin_counts / np.sum(bin_counts),\n        align=\'edge\',\n        width=widths,\n        edgecolor=edge,\n        color=color\n    )\n    trans = ax.get_xaxis_transform()\n    if avg_conf is not None:\n        ax.axvline(avg_conf, ls=\'--\', c=\'.3\')\n        if conf_label is not None:\n            ax.text(avg_conf + conf_spacer, label_y, conf_label, rotation=90, transform=trans)\n    if acc is not None:\n        ax.axvline(acc, ls=\'--\', c=\'.3\')\n        if acc_label is not None:\n            ax.text(acc + acc_spacer, label_y, acc_label, rotation=90, transform=trans)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_aspect(""equal"", ""box"")\n    ax.set_title(title)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    if x_ticks is None:\n        x_ticks = [y for i, y in enumerate(left_bins) if i % 2 == 0] + [1.0]\n    ax.set_xticks(x_ticks)\n    if y_ticks is not None:\n        ax.set_yticks(y_ticks)\n    return fig, ax\n\n\ndef _demo():\n    small_bins = np.arange(0, 1, step=0.05)\n    counts = np.zeros_like(small_bins)\n    prev = 0\n    for i in range(len(small_bins)):\n        add = np.random.randint(10, 15 * (i + 1))\n        counts[i] = prev + add\n        prev = counts[i]\n    counts[-1] += np.random.randint(50, 1000)\n    x_ticks = np.arange(0, 1.1, step=0.2)\n    ACC = 0.72\n    CONF = 0.84\n\n    f, a = confidence_histogram(small_bins, counts, x_ticks=x_ticks, acc=ACC, avg_conf=CONF)\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    _demo()\n'"
layers/eight_mile/calibration/plot/reliability_diagram.py,0,"b'from typing import Optional, Tuple\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nfrom matplotlib.figure import Figure\n\n\n__all__ = [""reliability_diagram"", ""reliability_curve""]\n\n\ndef reliability_diagram(\n    accs: np.array,\n    confs: np.array,\n    left_bins: np.array,\n    num_classes: Optional[int] = None,\n    x_ticks: Optional[np.array] = None,\n    y_ticks: Optional[np.array] = None,\n    title: Optional[str] = ""Reliability Graph"",\n    y_label: Optional[str] = ""Accuracy"",\n    x_label: Optional[str] = ""Confidence"",\n    gap_label: Optional[str] = ""Gap"",\n    output_label: Optional[str] = ""Outputs"",\n    gap_edge: str = ""r"",\n    gap_color: str = ""r"",\n    gap_alpha: float = 0.25,\n    gap_hatch: str = ""/"",\n    output_edge: str = ""k"",\n    output_color: str = ""b"",\n    fig_size: int = 3,\n    ax: Optional[Axes] = None,\n    fig: Optional[Figure] = None,\n) -> Tuple[Figure, Axes]:\n    """"""Plot a reliability graph a la https://arxiv.org/abs/1706.04599\n\n    :param accs: The average accuracy for examples in that bin\n    :param congs: The average confidence for examples in that bin\n    :param left_bins: The bins that define the groups, they should specify the left\n        edge of the bin\n    :param x_ticks: Where to show the ticks along the x axis. If not provided it\n        will display every other tick, pass an empty list to skip adding ticks\n    :param y_ticks: Where to show the ticks along the y axis. If not provided it\n        uses default matplotlib ticks, padd an empty list to skip adding ticks\n    :param title: The title for the axes, pass None to skip adding a title to the axes\n    :param y_label: A label for the y axes, pass None to skip adding a label\n    :param x_label: A label for the x axes, pass None to skip adding a label\n    :param gap_label: A label for the gap bins, pass None to skip adding a label\n    :param output_label: A label for the output bins, pass None to skip adding a label\n    :param gap_edge: The color to use for the edges of the gap bins\n    :param gap_color: The color to use for the main section of the gap bins\n    :param gap_hatch: The hatching pattern for the gap, useful when a bin a under confident\n    :param output_edge: The color to use for the edges of the output bins\n    :param output_color: The color to use for the main section of the output bins\n    :param fig_size: The figure size, only used if creating the figure/axis from scratch\n    :param ax: An Axes object that we can create the graph in, if None it will be created\n    :param fig: A figure that holds the axes object\n\n    :returns: `Tuple(fig, ax)` If the figure and ax are passed in they are returned, if they\n        were not passed in the created ones are returned. The don\'t use the figure at all but\n        because we want to return both in case we created them.\n    """"""\n    # If neither a figure non-axes are passed in we create them.\n    if ax is None and fig is None:\n        fig, ax = plt.subplots(1, 1, figsize=(fig_size, fig_size))\n    # If the axes was not passed in but the figure was we get the current active\n    # axes for the figure.\n    if ax is None and fig is not None:\n        ax = fig.gca()\n    # If the axes is provided but the figure we look up the figure on the axes\n    if ax is not None and fig is None:\n        fig = ax.figure\n    widths = np.diff(left_bins, append=1)\n    expected = left_bins + (widths / 2)\n    expected = np.where(confs != 0, confs, expected)\n    if num_classes is not None:\n        # This is the minimum possible confidence. This is maximum entropy. If your confidence is\n        # lower than this and something else has to have a larger confidence\n        min_conf = 1.0 / num_classes\n        # These bins are lowest confidence allowed in the bin so the highest confidence is the left edge\n        # of the bin above it. So if we do a compare to the bins we will see that the first bin will always\n        # be false. But we can use a np.roll and force the last bin to be true to simulate as if we compared\n        # to the right bin\n        possible_mask = np.roll(left_bins > min_conf, shift=-1)\n        possible_mask[-1] = True\n        # Mask out the expected values for bins that we can\'t put values in. Otherwise we will see all these\n        # red bars we can\'t do anything about.\n        expected *= possible_mask\n    ax.bar(\n        left_bins,\n        accs,\n        align=\'edge\',\n        width=widths,\n        label=output_label,\n        edgecolor=output_edge,\n        color=output_color\n    )\n    ax.bar(\n        left_bins,\n        expected - accs,\n        bottom=accs,\n        align=\'edge\',\n        width=widths,\n        label=gap_label,\n        edgecolor=gap_edge,\n        color=gap_color,\n        alpha=gap_alpha,\n        hatch=gap_hatch,\n    )\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.plot(ax.get_xlim(), ax.get_ylim(), ls=""--"", c="".3"")\n    ax.set_aspect(""equal"", ""box"")\n    ax.set_title(title)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    if x_ticks is None:\n        x_ticks = [y for i, y in enumerate(left_bins) if i % 2 == 0] + [1.0]\n    ax.set_xticks(x_ticks)\n    if y_ticks is not None:\n        ax.set_yticks(y_ticks)\n    if not (gap_label is None and output_label is None):\n        ax.legend()\n    return fig, ax\n\n\ndef reliability_curve(\n    accs: np.array,\n    confs: np.array,\n    num_classes: Optional[int] = None,\n    x_ticks: Optional[np.array] = None,\n    y_ticks: Optional[np.array] = None,\n    title: Optional[str] = ""Reliability Curve"",\n    y_label: Optional[str] = ""Accuracy"",\n    x_label: Optional[str] = ""Confidence"",\n    label: Optional[str] = None,\n    color: str = ""tab:orange"",\n    line_style: str = ""o-"",\n    fig_size: int = 3,\n    ax: Optional[Axes] = None,\n    fig: Optional[Figure] = None,\n) -> Tuple[Figure, Axes]:\n    """"""Plot a reliability curve a la https://scikit-learn.org/stable/modules/calibration.html#calibration\n\n    Note:\n        When there is a bin that had no samples in it the average confidence will be zero\n        and the average accuracy will be zero too. If this is the case we skip plotting\n        that point.\n\n    :param confs: The average confidence in a bin\n    :param accs: The average accuracy for examples in that bin\n    :param x_ticks: Where to show the ticks along the x axis. If not provided it\n        will display every other tick, pass an empty list to skip adding ticks\n    :param y_ticks: Where to show the ticks along the y axis. If not provided it\n        uses default matplotlib ticks, padd an empty list to skip adding ticks\n    :param title: The title for the axes, pass None to skip adding a title to the axes\n    :param y_label: A label for the y axes, pass None to skip adding a label\n    :param x_label: A label for the x axes, pass None to skip adding a label\n    :param label: A label for the plot we are making.\n    :param color: A color for the line we are making.\n    :param line_style: The stle of line to draw.\n    :param fig_size: The figure size, only used if creating the figure/axis from scratch\n    :param ax: An Axes object that we can create the graph in, if None it will be created\n    :param fig: A figure that holds the axes object\n\n    :returns: `Tuple(fig, ax)` If the figure and ax are passed in they are returned, if they\n        were not passed in the created ones are returned. The don\'t use the figure at all but\n        because we want to return both in case we created them.\n    """"""\n    # If neither a figure non-axes are passed in we create them.\n    if ax is None and fig is None:\n        fig, ax = plt.subplots(1, 1, figsize=(fig_size, fig_size))\n    # If the axes was not passed in but the figure was we get the current active\n    # axes for the figure.\n    if ax is None and fig is not None:\n        ax = fig.gca()\n    # If the axes is provided but the figure we look up the figure on the axes\n    if ax is not None and fig is None:\n        fig = ax.figure\n    valid = (confs != 0) | (accs != 0)\n    ax.plot(\n        confs[valid], accs[valid], line_style, color=color, label=label\n    )\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.plot(ax.get_xlim(), ax.get_ylim(), ls=""--"", c="".3"")\n    ax.set_aspect(""equal"", ""box"")\n    ax.set_title(title)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    if x_ticks is None:\n        x_ticks = [y for i, y in enumerate(np.arange(0, 1 + 1e-8, 1. / len(confs))) if i % 2 == 0]\n    ax.set_xticks(x_ticks)\n    if y_ticks is not None:\n        ax.set_yticks(y_ticks)\n    if label is not None:\n        ax.legend()\n    return fig, ax\n\n\ndef _demo():\n    f, axs = plt.subplots(2, 2, figsize=(4, 4))\n    data = np.array([0, 0, .15, .25, .28, .40, .38, .42, .56, .84])\n    x = np.arange(0, 1, 1 / len(data))\n    reliability_diagram(data, x + 0.05, x, num_classes=10, ax=axs[0][0], title=""Reliability Diagram (10 classes)"")\n    data[5] = 0\n    reliability_diagram(data, x + 0.05, x, num_classes=100, ax=axs[0][1], title=""Reliability Diagram (100 classes)"")\n    conf = np.array([\n        0.07167232, 0.16942227, 0.26184   , 0.35503329, 0.44979852,\n        0.55041263, 0.64713617, 0.73971958, 0.83297067, 0.92842073\n    ])\n    acc = np.array([\n        0.        , 0.00589623, 0.00789084, 0.03013831, 0.18707902,\n        0.64843534, 0.92417131, 0.98329403, 0.98977505, 1.\n    ])\n    reliability_curve(acc, conf, label=""SVC"", ax=axs[1][0])\n    conf = np.array([\n        0.07167232, 0.16942227, 0.26184   , 0.35503329, 0.44979852,\n        0.55041263, 0.64713617, 0, 0.83297067, 0.92842073\n    ])\n    acc = np.array([\n        0.        , 0.00589623, 0.00789084, 0.03013831, 0.18707902,\n        0.64843534, 0.92417131, 0, 0.98977505, 1.\n    ])\n    reliability_curve(acc * 0.5, conf, label=""NB"", ax=axs[1][1], color=""b"")\n    reliability_curve(acc, conf, label=""SVC"", ax=axs[1][1], title=""Reliability Curve, Missing bins"")\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    _demo()\n'"
