file_path,api_count,code
lesson01-PyTorch初见/autograd_demo.py,4,"b""import  torch\nfrom    torch import autograd\n\n\nx = torch.tensor(1.)\na = torch.tensor(1., requires_grad=True)\nb = torch.tensor(2., requires_grad=True)\nc = torch.tensor(3., requires_grad=True)\n\ny = a**2 * x + b * x + c\n\nprint('before:', a.grad, b.grad, c.grad)\ngrads = autograd.grad(y, [a, b, c])\nprint('after :', grads[0], grads[1], grads[2])"""
lesson01-PyTorch初见/gpu_accelerate.py,8,"b""import \ttorch\nimport  time\nprint(torch.__version__)\nprint(torch.cuda.is_available())\n# print('hello, world.')\n\n\na = torch.randn(10000, 1000)\nb = torch.randn(1000, 2000)\n\nt0 = time.time()\nc = torch.matmul(a, b)\nt1 = time.time()\nprint(a.device, t1 - t0, c.norm(2))\n\ndevice = torch.device('cuda')\na = a.to(device)\nb = b.to(device)\n\nt0 = time.time()\nc = torch.matmul(a, b)\nt2 = time.time()\nprint(a.device, t2 - t0, c.norm(2))\n\nt0 = time.time()\nc = torch.matmul(a, b)\nt2 = time.time()\nprint(a.device, t2 - t0, c.norm(2))\n\n"""
lesson02-开发环境安装/main.py,2,"b""import  torch\n\nprint(torch.__version__)\nprint('gpu:', torch.cuda.is_available())"""
lesson04-简单回归案例实战/gd.py,0,"b'import numpy as np\n\n# y = wx + b\ndef compute_error_for_line_given_points(b, w, points):\n    totalError = 0\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        totalError += (y - (w * x + b)) ** 2\n    return totalError / float(len(points))\n\ndef step_gradient(b_current, w_current, points, learningRate):\n    b_gradient = 0\n    w_gradient = 0\n    N = float(len(points))\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        b_gradient += -(2/N) * (y - ((w_current * x) + b_current))\n        w_gradient += -(2/N) * x * (y - ((w_current * x) + b_current))\n    new_b = b_current - (learningRate * b_gradient)\n    new_m = w_current - (learningRate * w_gradient)\n    return [new_b, new_m]\n\ndef gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n    b = starting_b\n    m = starting_m\n    for i in range(num_iterations):\n        b, m = step_gradient(b, m, np.array(points), learning_rate)\n    return [b, m]\n\ndef run():\n    points = np.genfromtxt(""data.csv"", delimiter="","")\n    learning_rate = 0.0001\n    initial_b = 0 # initial y-intercept guess\n    initial_m = 0 # initial slope guess\n    num_iterations = 1000\n    print(""Starting gradient descent at b = {0}, m = {1}, error = {2}""\n          .format(initial_b, initial_m,\n                  compute_error_for_line_given_points(initial_b, initial_m, points))\n          )\n    print(""Running..."")\n    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n    print(""After {0} iterations b = {1}, m = {2}, error = {3}"".\n          format(num_iterations, b, m,\n                 compute_error_for_line_given_points(b, m, points))\n          )\n\nif __name__ == \'__main__\':\n    run()'"
lesson05-手写数字问题/mnist_train.py,3,"b""import  torch\r\nfrom    torch import nn\r\nfrom    torch.nn import functional as F\r\nfrom    torch import optim\r\n\r\nimport  torchvision\r\nfrom    matplotlib import pyplot as plt\r\n\r\nfrom    utils import plot_image, plot_curve, one_hot\r\n\r\n\r\n\r\nbatch_size = 512\r\n\r\n# step1. load dataset\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    torchvision.datasets.MNIST('mnist_data', train=True, download=True,\r\n                               transform=torchvision.transforms.Compose([\r\n                                   torchvision.transforms.ToTensor(),\r\n                                   torchvision.transforms.Normalize(\r\n                                       (0.1307,), (0.3081,))\r\n                               ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\ntest_loader = torch.utils.data.DataLoader(\r\n    torchvision.datasets.MNIST('mnist_data/', train=False, download=True,\r\n                               transform=torchvision.transforms.Compose([\r\n                                   torchvision.transforms.ToTensor(),\r\n                                   torchvision.transforms.Normalize(\r\n                                       (0.1307,), (0.3081,))\r\n                               ])),\r\n    batch_size=batch_size, shuffle=False)\r\n\r\nx, y = next(iter(train_loader))\r\nprint(x.shape, y.shape, x.min(), x.max())\r\nplot_image(x, y, 'image sample')\r\n\r\n\r\n\r\nclass Net(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n\r\n        # xw+b\r\n        self.fc1 = nn.Linear(28*28, 256)\r\n        self.fc2 = nn.Linear(256, 64)\r\n        self.fc3 = nn.Linear(64, 10)\r\n\r\n    def forward(self, x):\r\n        # x: [b, 1, 28, 28]\r\n        # h1 = relu(xw1+b1)\r\n        x = F.relu(self.fc1(x))\r\n        # h2 = relu(h1w2+b2)\r\n        x = F.relu(self.fc2(x))\r\n        # h3 = h2w3+b3\r\n        x = self.fc3(x)\r\n\r\n        return x\r\n\r\n\r\n\r\nnet = Net()\r\n# [w1, b1, w2, b2, w3, b3]\r\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\r\n\r\n\r\ntrain_loss = []\r\n\r\nfor epoch in range(3):\r\n\r\n    for batch_idx, (x, y) in enumerate(train_loader):\r\n\r\n        # x: [b, 1, 28, 28], y: [512]\r\n        # [b, 1, 28, 28] => [b, 784]\r\n        x = x.view(x.size(0), 28*28)\r\n        # => [b, 10]\r\n        out = net(x)\r\n        # [b, 10]\r\n        y_onehot = one_hot(y)\r\n        # loss = mse(out, y_onehot)\r\n        loss = F.mse_loss(out, y_onehot)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # w' = w - lr*grad\r\n        optimizer.step()\r\n\r\n        train_loss.append(loss.item())\r\n\r\n        if batch_idx % 10==0:\r\n            print(epoch, batch_idx, loss.item())\r\n\r\nplot_curve(train_loss)\r\n# we get optimal [w1, b1, w2, b2, w3, b3]\r\n\r\n\r\ntotal_correct = 0\r\nfor x,y in test_loader:\r\n    x  = x.view(x.size(0), 28*28)\r\n    out = net(x)\r\n    # out: [b, 10] => pred: [b]\r\n    pred = out.argmax(dim=1)\r\n    correct = pred.eq(y).sum().float().item()\r\n    total_correct += correct\r\n\r\ntotal_num = len(test_loader.dataset)\r\nacc = total_correct / total_num\r\nprint('test acc:', acc)\r\n\r\nx, y = next(iter(test_loader))\r\nout = net(x.view(x.size(0), 28*28))\r\npred = out.argmax(dim=1)\r\nplot_image(x, pred, 'test')\r\n\r\n\r\n\r\n\r\n\r\n"""
lesson05-手写数字问题/utils.py,2,"b'import  torch\r\nfrom    matplotlib import pyplot as plt\r\n\r\n\r\ndef plot_curve(data):\r\n    fig = plt.figure()\r\n    plt.plot(range(len(data)), data, color=\'blue\')\r\n    plt.legend([\'value\'], loc=\'upper right\')\r\n    plt.xlabel(\'step\')\r\n    plt.ylabel(\'value\')\r\n    plt.show()\r\n\r\n\r\n\r\ndef plot_image(img, label, name):\r\n\r\n    fig = plt.figure()\r\n    for i in range(6):\r\n        plt.subplot(2, 3, i + 1)\r\n        plt.tight_layout()\r\n        plt.imshow(img[i][0]*0.3081+0.1307, cmap=\'gray\', interpolation=\'none\')\r\n        plt.title(""{}: {}"".format(name, label[i].item()))\r\n        plt.xticks([])\r\n        plt.yticks([])\r\n    plt.show()\r\n\r\n\r\ndef one_hot(label, depth=10):\r\n    out = torch.zeros(label.size(0), depth)\r\n    idx = torch.LongTensor(label).view(-1, 1)\r\n    out.scatter_(dim=1, index=idx, value=1)\r\n    return out'"
lesson22-优化小实例/main.py,2,"b""import  numpy as np\r\nfrom    mpl_toolkits.mplot3d import Axes3D\r\nfrom    matplotlib import pyplot as plt\r\nimport  torch\r\n\r\n\r\n\r\ndef himmelblau(x):\r\n    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\r\n\r\n\r\nx = np.arange(-6, 6, 0.1)\r\ny = np.arange(-6, 6, 0.1)\r\nprint('x,y range:', x.shape, y.shape)\r\nX, Y = np.meshgrid(x, y)\r\nprint('X,Y maps:', X.shape, Y.shape)\r\nZ = himmelblau([X, Y])\r\n\r\nfig = plt.figure('himmelblau')\r\nax = fig.gca(projection='3d')\r\nax.plot_surface(X, Y, Z)\r\nax.view_init(60, -30)\r\nax.set_xlabel('x')\r\nax.set_ylabel('y')\r\nplt.show()\r\n\r\n\r\n# [1., 0.], [-4, 0.], [4, 0.]\r\nx = torch.tensor([-4., 0.], requires_grad=True)\r\noptimizer = torch.optim.Adam([x], lr=1e-3)\r\nfor step in range(20000):\r\n\r\n    pred = himmelblau(x)\r\n\r\n    optimizer.zero_grad()\r\n    pred.backward()\r\n    optimizer.step()\r\n\r\n    if step % 2000 == 0:\r\n        print ('step {}: x = {}, f(x) = {}'\r\n               .format(step, x.tolist(), pred.item()))\r\n"""
lesson26-LR多分类实战/main.py,14,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\nw1, b1 = torch.randn(200, 784, requires_grad=True),\\\r\n         torch.zeros(200, requires_grad=True)\r\nw2, b2 = torch.randn(200, 200, requires_grad=True),\\\r\n         torch.zeros(200, requires_grad=True)\r\nw3, b3 = torch.randn(10, 200, requires_grad=True),\\\r\n         torch.zeros(10, requires_grad=True)\r\n\r\ntorch.nn.init.kaiming_normal_(w1)\r\ntorch.nn.init.kaiming_normal_(w2)\r\ntorch.nn.init.kaiming_normal_(w3)\r\n\r\n\r\ndef forward(x):\r\n    x = x@w1.t() + b1\r\n    x = F.relu(x)\r\n    x = x@w2.t() + b2\r\n    x = F.relu(x)\r\n    x = x@w3.t() + b3\r\n    x = F.relu(x)\r\n    return x\r\n\r\n\r\n\r\noptimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss()\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n\r\n        logits = forward(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        logits = forward(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.data.max(1)[1]\r\n        correct += pred.eq(target.data).sum()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson27-MLP网络层/main.py,5,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.ReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.ReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.ReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\nnet = MLP()\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss()\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.data.max(1)[1]\r\n        correct += pred.eq(target.data).sum()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson28-激活函数与GPU加速/main.py,6,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.LeakyReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\ndevice = torch.device('cuda:0')\r\nnet = MLP().to(device)\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss().to(device)\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n        data, target = data.to(device), target.cuda()\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        data, target = data.to(device), target.cuda()\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.data.max(1)[1]\r\n        correct += pred.eq(target.data).sum()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson29-MNIST测试/main.py,6,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.LeakyReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\ndevice = torch.device('cuda:0')\r\nnet = MLP().to(device)\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss().to(device)\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n        data, target = data.to(device), target.cuda()\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        data, target = data.to(device), target.cuda()\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.argmax(dim=1)\r\n        correct += pred.eq(target).float().sum().item()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson30-Visdom可视化/main.py,6,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\nfrom visdom import Visdom\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       # transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        # transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.LeakyReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\ndevice = torch.device('cuda:0')\r\nnet = MLP().to(device)\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss().to(device)\r\n\r\nviz = Visdom()\r\n\r\nviz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\r\nviz.line([[0.0, 0.0]], [0.], win='test', opts=dict(title='test loss&acc.',\r\n                                                   legend=['loss', 'acc.']))\r\nglobal_step = 0\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n        data, target = data.to(device), target.cuda()\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        global_step += 1\r\n        viz.line([loss.item()], [global_step], win='train_loss', update='append')\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        data, target = data.to(device), target.cuda()\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.argmax(dim=1)\r\n        correct += pred.eq(target).float().sum().item()\r\n\r\n    viz.line([[test_loss, correct / len(test_loader.dataset)]],\r\n             [global_step], win='test', update='append')\r\n    viz.images(data.view(-1, 1, 28, 28), win='x')\r\n    viz.text(str(pred.detach().cpu().numpy()), win='pred',\r\n             opts=dict(title='pred'))\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson32-Train-Val-Test-交叉验证/main.py,9,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_db = datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ]))\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    train_db,\r\n    batch_size=batch_size, shuffle=True)\r\n\r\ntest_db = datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n    transforms.ToTensor(),\r\n    transforms.Normalize((0.1307,), (0.3081,))\r\n]))\r\ntest_loader = torch.utils.data.DataLoader(test_db,\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\nprint('train:', len(train_db), 'test:', len(test_db))\r\ntrain_db, val_db = torch.utils.data.random_split(train_db, [50000, 10000])\r\nprint('db1:', len(train_db), 'db2:', len(val_db))\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    train_db,\r\n    batch_size=batch_size, shuffle=True)\r\nval_loader = torch.utils.data.DataLoader(\r\n    val_db,\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.LeakyReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\ndevice = torch.device('cuda:0')\r\nnet = MLP().to(device)\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss().to(device)\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n        data, target = data.to(device), target.cuda()\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in val_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        data, target = data.to(device), target.cuda()\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.data.max(1)[1]\r\n        correct += pred.eq(target.data).sum()\r\n\r\n    test_loss /= len(val_loader.dataset)\r\n    print('\\nVAL set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(val_loader.dataset),\r\n        100. * correct / len(val_loader.dataset)))\r\n\r\n\r\n\r\ntest_loss = 0\r\ncorrect = 0\r\nfor data, target in test_loader:\r\n    data = data.view(-1, 28 * 28)\r\n    data, target = data.to(device), target.cuda()\r\n    logits = net(data)\r\n    test_loss += criteon(logits, target).item()\r\n\r\n    pred = logits.data.max(1)[1]\r\n    correct += pred.eq(target.data).sum()\r\n\r\ntest_loss /= len(test_loader.dataset)\r\nprint('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n    test_loss, correct, len(test_loader.dataset),\r\n    100. * correct / len(test_loader.dataset)))"""
lesson33-regularization/main.py,6,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\nfrom visdom import Visdom\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       # transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        # transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.LeakyReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\ndevice = torch.device('cuda:0')\r\nnet = MLP().to(device)\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.01)\r\ncriteon = nn.CrossEntropyLoss().to(device)\r\n\r\nviz = Visdom()\r\n\r\nviz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\r\nviz.line([[0.0, 0.0]], [0.], win='test', opts=dict(title='test loss&acc.',\r\n                                                   legend=['loss', 'acc.']))\r\nglobal_step = 0\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n        data, target = data.to(device), target.cuda()\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        global_step += 1\r\n        viz.line([loss.item()], [global_step], win='train_loss', update='append')\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        data, target = data.to(device), target.cuda()\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.argmax(dim=1)\r\n        correct += pred.eq(target).float().sum().item()\r\n\r\n    viz.line([[test_loss, correct / len(test_loader.dataset)]],\r\n             [global_step], win='test', update='append')\r\n    viz.images(data.view(-1, 1, 28, 28), win='x')\r\n    viz.text(str(pred.detach().cpu().numpy()), win='pred',\r\n             opts=dict(title='pred'))\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson35-Early-stopping-Dropout/main.py,6,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\nfrom visdom import Visdom\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       # transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        # transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.LeakyReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\ndevice = torch.device('cuda:0')\r\nnet = MLP().to(device)\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss().to(device)\r\n\r\nviz = Visdom()\r\n\r\nviz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\r\nviz.line([[0.0, 0.0]], [0.], win='test', opts=dict(title='test loss&acc.',\r\n                                                   legend=['loss', 'acc.']))\r\nglobal_step = 0\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n        data, target = data.to(device), target.cuda()\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        global_step += 1\r\n        viz.line([loss.item()], [global_step], win='train_loss', update='append')\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        data, target = data.to(device), target.cuda()\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.argmax(dim=1)\r\n        correct += pred.eq(target).float().sum().item()\r\n\r\n    viz.line([[test_loss, correct / len(test_loader.dataset)]],\r\n             [global_step], win='test', update='append')\r\n    viz.images(data.view(-1, 1, 28, 28), win='x')\r\n    viz.text(str(pred.detach().cpu().numpy()), win='pred',\r\n             opts=dict(title='pred'))\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson42-ResNet/resnet.py,5,"b'import  torch\r\nfrom    torch import  nn\r\nfrom    torch.nn import functional as F\r\nfrom    torch.utils.data import DataLoader\r\nfrom    torchvision import datasets\r\nfrom    torchvision import transforms\r\nfrom    torch import nn, optim\r\n\r\n# from    torchvision.models import resnet18\r\n\r\nclass ResBlk(nn.Module):\r\n    """"""\r\n    resnet block\r\n    """"""\r\n\r\n    def __init__(self, ch_in, ch_out):\r\n        """"""\r\n        :param ch_in:\r\n        :param ch_out:\r\n        """"""\r\n        super(ResBlk, self).__init__()\r\n\r\n        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1)\r\n        self.bn1 = nn.BatchNorm2d(ch_out)\r\n        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\r\n        self.bn2 = nn.BatchNorm2d(ch_out)\r\n\r\n        self.extra = nn.Sequential()\r\n        if ch_out != ch_in:\r\n            # [b, ch_in, h, w] => [b, ch_out, h, w]\r\n            self.extra = nn.Sequential(\r\n                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),\r\n                nn.BatchNorm2d(ch_out)\r\n            )\r\n\r\n\r\n    def forward(self, x):\r\n        """"""\r\n        :param x: [b, ch, h, w]\r\n        :return:\r\n        """"""\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = self.bn2(self.conv2(out))\r\n        # short cut.\r\n        # extra module: [b, ch_in, h, w] => [b, ch_out, h, w]\r\n        # element-wise add:\r\n        out = self.extra(x) + out\r\n\r\n        return out\r\n\r\n\r\n\r\n\r\nclass ResNet18(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(ResNet18, self).__init__()\r\n\r\n        self.conv1 = nn.Sequential(\r\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\r\n            nn.BatchNorm2d(16)\r\n        )\r\n        # followed 4 blocks\r\n        # [b, 64, h, w] => [b, 128, h ,w]\r\n        self.blk1 = ResBlk(16, 16)\r\n        # [b, 128, h, w] => [b, 256, h, w]\r\n        self.blk2 = ResBlk(16, 32)\r\n        # # [b, 256, h, w] => [b, 512, h, w]\r\n        # self.blk3 = ResBlk(128, 256)\r\n        # # [b, 512, h, w] => [b, 1024, h, w]\r\n        # self.blk4 = ResBlk(256, 512)\r\n\r\n        self.outlayer = nn.Linear(32*32*32, 10)\r\n\r\n    def forward(self, x):\r\n        """"""\r\n        :param x:\r\n        :return:\r\n        """"""\r\n        x = F.relu(self.conv1(x))\r\n\r\n        # [b, 64, h, w] => [b, 1024, h, w]\r\n        x = self.blk1(x)\r\n        x = self.blk2(x)\r\n        # x = self.blk3(x)\r\n        # x = self.blk4(x)\r\n\r\n        # print(x.shape)\r\n        x = x.view(x.size(0), -1)\r\n        x = self.outlayer(x)\r\n\r\n\r\n        return x\r\n\r\n\r\n\r\ndef main():\r\n    batchsz = 32\r\n\r\n    cifar_train = datasets.CIFAR10(\'cifar\', True, transform=transforms.Compose([\r\n        transforms.Resize((32, 32)),\r\n        transforms.ToTensor()\r\n    ]), download=True)\r\n    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=True)\r\n\r\n    cifar_test = datasets.CIFAR10(\'cifar\', False, transform=transforms.Compose([\r\n        transforms.Resize((32, 32)),\r\n        transforms.ToTensor()\r\n    ]), download=True)\r\n    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=True)\r\n\r\n\r\n    x, label = iter(cifar_train).next()\r\n    print(\'x:\', x.shape, \'label:\', label.shape)\r\n\r\n    device = torch.device(\'cuda\')\r\n    # model = Lenet5().to(device)\r\n    model = ResNet18().to(device)\r\n\r\n    criteon = nn.CrossEntropyLoss().to(device)\r\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\r\n    print(model)\r\n\r\n    for epoch in range(1000):\r\n\r\n        model.train()\r\n        for batchidx, (x, label) in enumerate(cifar_train):\r\n            # [b, 3, 32, 32]\r\n            # [b]\r\n            x, label = x.to(device), label.to(device)\r\n\r\n            logits = model(x)\r\n            # logits: [b, 10]\r\n            # label:  [b]\r\n            # loss: tensor scalar\r\n            loss = criteon(logits, label)\r\n\r\n            # backprop\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n\r\n\r\n        #\r\n        print(epoch, \'loss:\', loss.item())\r\n\r\n\r\n        model.eval()\r\n        with torch.no_grad():\r\n            # test\r\n            total_correct = 0\r\n            total_num = 0\r\n            for x, label in cifar_test:\r\n                # [b, 3, 32, 32]\r\n                # [b]\r\n                x, label = x.to(device), label.to(device)\r\n\r\n                # [b, 10]\r\n                logits = model(x)\r\n                # [b]\r\n                pred = logits.argmax(dim=1)\r\n                # [b] vs [b] => scalar tensor\r\n                correct = torch.eq(pred, label).float().sum().item()\r\n                total_correct += correct\r\n                total_num += x.size(0)\r\n                # print(correct)\r\n\r\n            acc = total_correct / total_num\r\n            print(epoch, \'acc:\', acc)\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()'"
lesson43-nn.Module/main.py,5,"b""import  torch\r\nfrom    torch import nn\r\nfrom    torch import optim\r\n\r\n\r\n\r\nclass MyLinear(nn.Module):\r\n\r\n    def __init__(self, inp, outp):\r\n        super(MyLinear, self).__init__()\r\n\r\n        # requires_grad = True\r\n        self.w = nn.Parameter(torch.randn(outp, inp))\r\n        self.b = nn.Parameter(torch.randn(outp))\r\n\r\n    def forward(self, x):\r\n        x = x @ self.w.t() + self.b\r\n        return x\r\n\r\n\r\nclass Flatten(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Flatten, self).__init__()\r\n\r\n    def forward(self, input):\r\n        return input.view(input.size(0), -1)\r\n\r\n\r\n\r\nclass TestNet(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(TestNet, self).__init__()\r\n\r\n        self.net = nn.Sequential(nn.Conv2d(1, 16, stride=1, padding=1),\r\n                                 nn.MaxPool2d(2, 2),\r\n                                 Flatten(),\r\n                                 nn.Linear(1*14*14, 10))\r\n\r\n    def forward(self, x):\r\n        return self.net(x)\r\n\r\n\r\nclass BasicNet(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(BasicNet, self).__init__()\r\n\r\n        self.net = nn.Linear(4, 3)\r\n\r\n    def forward(self, x):\r\n        return self.net(x)\r\n\r\n\r\n\r\nclass Net(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n\r\n        self.net = nn.Sequential(BasicNet(),\r\n                                 nn.ReLU(),\r\n                                 nn.Linear(3, 2))\r\n\r\n    def forward(self, x):\r\n        return self.net(x)\r\n\r\n\r\n\r\n\r\n\r\ndef main():\r\n    device = torch.device('cuda')\r\n    net = Net()\r\n    net.to(device)\r\n\r\n    net.train()\r\n\r\n    net.eval()\r\n\r\n    # net.load_state_dict(torch.load('ckpt.mdl'))\r\n    #\r\n    #\r\n    # torch.save(net.state_dict(), 'ckpt.mdl')\r\n\r\n    for name, t in net.named_parameters():\r\n        print('parameters:', name, t.shape)\r\n\r\n    for name, m in net.named_children():\r\n        print('children:', name, m)\r\n\r\n\r\n    for name, m in net.named_modules():\r\n        print('modules:', name, m)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()"""
lesson44-数据增强/main.py,6,"b""import  torch\r\nimport  torch.nn as nn\r\nimport  torch.nn.functional as F\r\nimport  torch.optim as optim\r\nfrom    torchvision import datasets, transforms\r\n\r\nfrom visdom import Visdom\r\n\r\nbatch_size=200\r\nlearning_rate=0.01\r\nepochs=10\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.RandomHorizontalFlip(),\r\n                       transforms.RandomVerticalFlip(),\r\n                       transforms.RandomRotation(15),\r\n                       transforms.RandomRotation([90, 180, 270]),\r\n                       transforms.Resize([32, 32]),\r\n                       transforms.RandomCrop([28, 28]),\r\n                       transforms.ToTensor(),\r\n                       # transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=batch_size, shuffle=True)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n        transforms.ToTensor(),\r\n        # transforms.Normalize((0.1307,), (0.3081,))\r\n    ])),\r\n    batch_size=batch_size, shuffle=True)\r\n\r\n\r\n\r\nclass MLP(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(MLP, self).__init__()\r\n\r\n        self.model = nn.Sequential(\r\n            nn.Linear(784, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 200),\r\n            nn.LeakyReLU(inplace=True),\r\n            nn.Linear(200, 10),\r\n            nn.LeakyReLU(inplace=True),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.model(x)\r\n\r\n        return x\r\n\r\ndevice = torch.device('cuda:0')\r\nnet = MLP().to(device)\r\noptimizer = optim.SGD(net.parameters(), lr=learning_rate)\r\ncriteon = nn.CrossEntropyLoss().to(device)\r\n\r\nviz = Visdom()\r\n\r\nviz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))\r\nviz.line([[0.0, 0.0]], [0.], win='test', opts=dict(title='test loss&acc.',\r\n                                                   legend=['loss', 'acc.']))\r\nglobal_step = 0\r\n\r\nfor epoch in range(epochs):\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data = data.view(-1, 28*28)\r\n        data, target = data.to(device), target.cuda()\r\n\r\n        logits = net(data)\r\n        loss = criteon(logits, target)\r\n\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        # print(w1.grad.norm(), w2.grad.norm())\r\n        optimizer.step()\r\n\r\n        global_step += 1\r\n        viz.line([loss.item()], [global_step], win='train_loss', update='append')\r\n\r\n        if batch_idx % 100 == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.item()))\r\n\r\n\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        data = data.view(-1, 28 * 28)\r\n        data, target = data.to(device), target.cuda()\r\n        logits = net(data)\r\n        test_loss += criteon(logits, target).item()\r\n\r\n        pred = logits.argmax(dim=1)\r\n        correct += pred.eq(target).float().sum().item()\r\n\r\n    viz.line([[test_loss, correct / len(test_loader.dataset)]],\r\n             [global_step], win='test', update='append')\r\n    viz.images(data.view(-1, 1, 28, 28), win='x')\r\n    viz.text(str(pred.detach().cpu().numpy()), win='pred',\r\n             opts=dict(title='pred'))\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n"""
lesson45-Cifar10与ResNet18实战/lenet5.py,3,"b'import  torch\nfrom    torch import nn\nfrom    torch.nn import functional as F\n\n\n\n\nclass Lenet5(nn.Module):\n    """"""\n    for cifar10 dataset.\n    """"""\n    def __init__(self):\n        super(Lenet5, self).__init__()\n\n        self.conv_unit = nn.Sequential(\n            # x: [b, 3, 32, 32] => [b, 16, ]\n            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=0),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n            #\n            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n            #\n        )\n        # flatten\n        # fc unit\n        self.fc_unit = nn.Sequential(\n            nn.Linear(32*5*5, 32),\n            nn.ReLU(),\n            # nn.Linear(120, 84),\n            # nn.ReLU(),\n            nn.Linear(32, 10)\n        )\n\n\n        # [b, 3, 32, 32]\n        tmp = torch.randn(2, 3, 32, 32)\n        out = self.conv_unit(tmp)\n        # [b, 16, 5, 5]\n        print(\'conv out:\', out.shape)\n\n        # # use Cross Entropy Loss\n        # self.criteon = nn.CrossEntropyLoss()\n\n\n\n    def forward(self, x):\n        """"""\n\n        :param x: [b, 3, 32, 32]\n        :return:\n        """"""\n        batchsz = x.size(0)\n        # [b, 3, 32, 32] => [b, 16, 5, 5]\n        x = self.conv_unit(x)\n        # [b, 16, 5, 5] => [b, 16*5*5]\n        x = x.view(batchsz, 32*5*5)\n        # [b, 16*5*5] => [b, 10]\n        logits = self.fc_unit(x)\n\n        # # [b, 10]\n        # pred = F.softmax(logits, dim=1)\n        # loss = self.criteon(logits, y)\n\n        return logits\n\ndef main():\n\n    net = Lenet5()\n\n    tmp = torch.randn(2, 3, 32, 32)\n    out = net(tmp)\n    print(\'lenet out:\', out.shape)\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
lesson45-Cifar10与ResNet18实战/main.py,4,"b""import  torch\nfrom    torch.utils.data import DataLoader\nfrom    torchvision import datasets\nfrom    torchvision import transforms\nfrom    torch import nn, optim\n\nfrom    lenet5 import Lenet5\nfrom    resnet import ResNet18\n\ndef main():\n    batchsz = 128\n\n    cifar_train = datasets.CIFAR10('cifar', True, transform=transforms.Compose([\n        transforms.Resize((32, 32)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ]), download=True)\n    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=True)\n\n    cifar_test = datasets.CIFAR10('cifar', False, transform=transforms.Compose([\n        transforms.Resize((32, 32)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ]), download=True)\n    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=True)\n\n\n    x, label = iter(cifar_train).next()\n    print('x:', x.shape, 'label:', label.shape)\n\n    device = torch.device('cuda')\n    # model = Lenet5().to(device)\n    model = ResNet18().to(device)\n\n    criteon = nn.CrossEntropyLoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    print(model)\n\n    for epoch in range(1000):\n\n        model.train()\n        for batchidx, (x, label) in enumerate(cifar_train):\n            # [b, 3, 32, 32]\n            # [b]\n            x, label = x.to(device), label.to(device)\n\n\n            logits = model(x)\n            # logits: [b, 10]\n            # label:  [b]\n            # loss: tensor scalar\n            loss = criteon(logits, label)\n\n            # backprop\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n        print(epoch, 'loss:', loss.item())\n\n\n        model.eval()\n        with torch.no_grad():\n            # test\n            total_correct = 0\n            total_num = 0\n            for x, label in cifar_test:\n                # [b, 3, 32, 32]\n                # [b]\n                x, label = x.to(device), label.to(device)\n\n                # [b, 10]\n                logits = model(x)\n                # [b]\n                pred = logits.argmax(dim=1)\n                # [b] vs [b] => scalar tensor\n                correct = torch.eq(pred, label).float().sum().item()\n                total_correct += correct\n                total_num += x.size(0)\n                # print(correct)\n\n            acc = total_correct / total_num\n            print(epoch, 'test acc:', acc)\n\n\n\nif __name__ == '__main__':\n    main()\n"""
lesson45-Cifar10与ResNet18实战/resnet.py,3,"b'import  torch\nfrom    torch import  nn\nfrom    torch.nn import functional as F\n\n\n\nclass ResBlk(nn.Module):\n    """"""\n    resnet block\n    """"""\n\n    def __init__(self, ch_in, ch_out, stride=1):\n        """"""\n\n        :param ch_in:\n        :param ch_out:\n        """"""\n        super(ResBlk, self).__init__()\n\n        # we add stride support for resbok, which is distinct from tutorials.\n        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(ch_out)\n        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(ch_out)\n\n        self.extra = nn.Sequential()\n        if ch_out != ch_in:\n            # [b, ch_in, h, w] => [b, ch_out, h, w]\n            self.extra = nn.Sequential(\n                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(ch_out)\n            )\n\n\n    def forward(self, x):\n        """"""\n\n        :param x: [b, ch, h, w]\n        :return:\n        """"""\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        # short cut.\n        # extra module: [b, ch_in, h, w] => [b, ch_out, h, w]\n        # element-wise add:\n        out = self.extra(x) + out\n        out = F.relu(out)\n        \n        return out\n\n\n\n\nclass ResNet18(nn.Module):\n\n    def __init__(self):\n        super(ResNet18, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=3, padding=0),\n            nn.BatchNorm2d(64)\n        )\n        # followed 4 blocks\n        # [b, 64, h, w] => [b, 128, h ,w]\n        self.blk1 = ResBlk(64, 128, stride=2)\n        # [b, 128, h, w] => [b, 256, h, w]\n        self.blk2 = ResBlk(128, 256, stride=2)\n        # # [b, 256, h, w] => [b, 512, h, w]\n        self.blk3 = ResBlk(256, 512, stride=2)\n        # # [b, 512, h, w] => [b, 1024, h, w]\n        self.blk4 = ResBlk(512, 512, stride=2)\n\n        self.outlayer = nn.Linear(512*1*1, 10)\n\n    def forward(self, x):\n        """"""\n\n        :param x:\n        :return:\n        """"""\n        x = F.relu(self.conv1(x))\n\n        # [b, 64, h, w] => [b, 1024, h, w]\n        x = self.blk1(x)\n        x = self.blk2(x)\n        x = self.blk3(x)\n        x = self.blk4(x)\n\n\n        # print(\'after conv:\', x.shape) #[b, 512, 2, 2]\n        # [b, 512, h, w] => [b, 512, 1, 1]\n        x = F.adaptive_avg_pool2d(x, [1, 1])\n        # print(\'after pool:\', x.shape)\n        x = x.view(x.size(0), -1)\n        x = self.outlayer(x)\n\n\n        return x\n\n\n\ndef main():\n\n    blk = ResBlk(64, 128, stride=4)\n    tmp = torch.randn(2, 64, 32, 32)\n    out = blk(tmp)\n    print(\'block:\', out.shape)\n\n    x = torch.randn(2, 3, 32, 32)\n    model = ResNet18()\n    out = model(x)\n    print(\'resnet:\', out.shape)\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
lesson46-时间序列表示/main.py,0,"b""import  torch\r\nimport  torchnlp\r\n\r\nfrom torchnlp import word_to_vector\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef main():\r\n\r\n    # vec = word_to_vector.GloVe()\r\n    vec = word_to_vector.BPEmb()\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()"""
lesson48-RNN-Layer使用/rnn.py,15,"b""import  torch\r\nfrom    torch import nn\r\nfrom    torch import optim\r\nfrom    torch.nn import functional as F\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef main():\r\n\r\n\r\n    rnn = nn.RNN(input_size=100, hidden_size=20, num_layers=1)\r\n    print(rnn)\r\n    x = torch.randn(10, 3, 100)\r\n    out, h = rnn(x, torch.zeros(1, 3, 20))\r\n    print(out.shape, h.shape)\r\n\r\n    rnn = nn.RNN(input_size=100, hidden_size=20, num_layers=4)\r\n    print(rnn)\r\n    x = torch.randn(10, 3, 100)\r\n    out, h = rnn(x, torch.zeros(4, 3, 20))\r\n    print(out.shape, h.shape)\r\n    # print(vars(rnn))\r\n\r\n    print('rnn by cell')\r\n\r\n    cell1 = nn.RNNCell(100, 20)\r\n    h1 = torch.zeros(3, 20)\r\n    for xt in x:\r\n        h1 = cell1(xt, h1)\r\n    print(h1.shape)\r\n\r\n\r\n    cell1 = nn.RNNCell(100, 30)\r\n    cell2 = nn.RNNCell(30, 20)\r\n    h1 = torch.zeros(3, 30)\r\n    h2 = torch.zeros(3, 20)\r\n    for xt in x:\r\n        h1 = cell1(xt, h1)\r\n        h2 = cell2(h1, h2)\r\n    print(h2.shape)\r\n\r\n    print('Lstm')\r\n    lstm = nn.LSTM(input_size=100, hidden_size=20, num_layers=4)\r\n    print(lstm)\r\n    x = torch.randn(10, 3, 100)\r\n    out, (h, c) = lstm(x)\r\n    print(out.shape, h.shape, c.shape)\r\n\r\n    print('one layer lstm')\r\n    cell = nn.LSTMCell(input_size=100, hidden_size=20)\r\n    h = torch.zeros(3, 20)\r\n    c = torch.zeros(3, 20)\r\n    for xt in x:\r\n        h, c = cell(xt, [h, c])\r\n    print(h.shape, c.shape)\r\n\r\n\r\n    print('two layer lstm')\r\n    cell1 = nn.LSTMCell(input_size=100, hidden_size=30)\r\n    cell2 = nn.LSTMCell(input_size=30, hidden_size=20)\r\n    h1 = torch.zeros(3, 30)\r\n    c1 = torch.zeros(3, 30)\r\n    h2 = torch.zeros(3, 20)\r\n    c2 = torch.zeros(3, 20)\r\n    for xt in x:\r\n        h1, c1 = cell1(xt, [h1, c1])\r\n        h2, c2 = cell2(h1, [h2, c2])\r\n    print(h2.shape, c2.shape)\r\n\r\n\r\n\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()"""
lesson49-时间序列预测/seris.py,8,"b'import  numpy as np\r\nimport  torch\r\nimport  torch.nn as nn\r\nimport  torch.optim as optim\r\nfrom    matplotlib import pyplot as plt\r\n\r\n\r\nnum_time_steps = 50\r\ninput_size = 1\r\nhidden_size = 16\r\noutput_size = 1\r\nlr=0.01\r\n\r\n\r\n\r\nclass Net(nn.Module):\r\n\r\n    def __init__(self, ):\r\n        super(Net, self).__init__()\r\n\r\n        self.rnn = nn.RNN(\r\n            input_size=input_size,\r\n            hidden_size=hidden_size,\r\n            num_layers=1,\r\n            batch_first=True,\r\n        )\r\n        for p in self.rnn.parameters():\r\n          nn.init.normal_(p, mean=0.0, std=0.001)\r\n\r\n        self.linear = nn.Linear(hidden_size, output_size)\r\n\r\n    def forward(self, x, hidden_prev):\r\n\r\n       out, hidden_prev = self.rnn(x, hidden_prev)\r\n       # [b, seq, h]\r\n       out = out.view(-1, hidden_size)\r\n       out = self.linear(out)\r\n       out = out.unsqueeze(dim=0)\r\n       return out, hidden_prev\r\n\r\n\r\n\r\n\r\nmodel = Net()\r\ncriterion = nn.MSELoss()\r\noptimizer = optim.Adam(model.parameters(), lr)\r\n\r\nhidden_prev = torch.zeros(1, 1, hidden_size)\r\n\r\nfor iter in range(6000):\r\n    start = np.random.randint(3, size=1)[0]\r\n    time_steps = np.linspace(start, start + 10, num_time_steps)\r\n    data = np.sin(time_steps)\r\n    data = data.reshape(num_time_steps, 1)\r\n    x = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1)\r\n    y = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)\r\n\r\n    output, hidden_prev = model(x, hidden_prev)\r\n    hidden_prev = hidden_prev.detach()\r\n\r\n    loss = criterion(output, y)\r\n    model.zero_grad()\r\n    loss.backward()\r\n    # for p in model.parameters():\r\n    #     print(p.grad.norm())\r\n    # torch.nn.utils.clip_grad_norm_(p, 10)\r\n    optimizer.step()\r\n\r\n    if iter % 100 == 0:\r\n        print(""Iteration: {} loss {}"".format(iter, loss.item()))\r\n\r\nstart = np.random.randint(3, size=1)[0]\r\ntime_steps = np.linspace(start, start + 10, num_time_steps)\r\ndata = np.sin(time_steps)\r\ndata = data.reshape(num_time_steps, 1)\r\nx = torch.tensor(data[:-1]).float().view(1, num_time_steps - 1, 1)\r\ny = torch.tensor(data[1:]).float().view(1, num_time_steps - 1, 1)\r\n\r\npredictions = []\r\ninput = x[:, 0, :]\r\nfor _ in range(x.shape[1]):\r\n  input = input.view(1, 1, 1)\r\n  (pred, hidden_prev) = model(input, hidden_prev)\r\n  input = pred\r\n  predictions.append(pred.detach().numpy().ravel()[0])\r\n\r\nx = x.data.numpy().ravel()\r\ny = y.data.numpy()\r\nplt.scatter(time_steps[:-1], x.ravel(), s=90)\r\nplt.plot(time_steps[:-1], x.ravel())\r\n\r\nplt.scatter(time_steps[1:], predictions)\r\nplt.show()'"
lesson53-情感分类实战/lstm.py,8,"b'# -*- coding: utf-8 -*-\n""""""lstm\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1GX0Rqur8T45MSYhLU9MYWAbycfLH4-Fu\n""""""\n\n!pip install torch\n!pip install torchtext\n!python -m spacy download en\n\n\n# K80 gpu for 12 hours\nimport torch\nfrom torch import nn, optim\nfrom torchtext import data, datasets\n\nprint(\'GPU:\', torch.cuda.is_available())\n\ntorch.manual_seed(123)\n\nTEXT = data.Field(tokenize=\'spacy\')\nLABEL = data.LabelField(dtype=torch.float)\ntrain_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n\nprint(\'len of train data:\', len(train_data))\nprint(\'len of test data:\', len(test_data))\n\nprint(train_data.examples[15].text)\nprint(train_data.examples[15].label)\n\n# word2vec, glove\nTEXT.build_vocab(train_data, max_size=10000, vectors=\'glove.6B.100d\')\nLABEL.build_vocab(train_data)\n\n\nbatchsz = 30\ndevice = torch.device(\'cuda\')\ntrain_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, test_data),\n    batch_size = batchsz,\n    device=device\n)\n\nclass RNN(nn.Module):\n    \n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        """"""\n        """"""\n        super(RNN, self).__init__()\n        \n        # [0-10001] => [100]\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # [100] => [256]\n        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, \n                           bidirectional=True, dropout=0.5)\n        # [256*2] => [1]\n        self.fc = nn.Linear(hidden_dim*2, 1)\n        self.dropout = nn.Dropout(0.5)\n        \n        \n    def forward(self, x):\n        """"""\n        x: [seq_len, b] vs [b, 3, 28, 28]\n        """"""\n        # [seq, b, 1] => [seq, b, 100]\n        embedding = self.dropout(self.embedding(x))\n        \n        # output: [seq, b, hid_dim*2]\n        # hidden/h: [num_layers*2, b, hid_dim]\n        # cell/c: [num_layers*2, b, hid_di]\n        output, (hidden, cell) = self.rnn(embedding)\n        \n        # [num_layers*2, b, hid_dim] => 2 of [b, hid_dim] => [b, hid_dim*2]\n        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n        \n        # [b, hid_dim*2] => [b, 1]\n        hidden = self.dropout(hidden)\n        out = self.fc(hidden)\n        \n        return out\n\nrnn = RNN(len(TEXT.vocab), 100, 256)\n\npretrained_embedding = TEXT.vocab.vectors\nprint(\'pretrained_embedding:\', pretrained_embedding.shape)\nrnn.embedding.weight.data.copy_(pretrained_embedding)\nprint(\'embedding layer inited.\')\n\noptimizer = optim.Adam(rnn.parameters(), lr=1e-3)\ncriteon = nn.BCEWithLogitsLoss().to(device)\nrnn.to(device)\n\nimport numpy as np\n\ndef binary_acc(preds, y):\n    """"""\n    get accuracy\n    """"""\n    preds = torch.round(torch.sigmoid(preds))\n    correct = torch.eq(preds, y).float()\n    acc = correct.sum() / len(correct)\n    return acc\n\ndef train(rnn, iterator, optimizer, criteon):\n    \n    avg_acc = []\n    rnn.train()\n    \n    for i, batch in enumerate(iterator):\n        \n        # [seq, b] => [b, 1] => [b]\n        pred = rnn(batch.text).squeeze(1)\n        # \n        loss = criteon(pred, batch.label)\n        acc = binary_acc(pred, batch.label).item()\n        avg_acc.append(acc)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if i%10 == 0:\n            print(i, acc)\n        \n    avg_acc = np.array(avg_acc).mean()\n    print(\'avg acc:\', avg_acc)\n    \n    \ndef eval(rnn, iterator, criteon):\n    \n    avg_acc = []\n    \n    rnn.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n\n            # [b, 1] => [b]\n            pred = rnn(batch.text).squeeze(1)\n\n            #\n            loss = criteon(pred, batch.label)\n\n            acc = binary_acc(pred, batch.label).item()\n            avg_acc.append(acc)\n        \n    avg_acc = np.array(avg_acc).mean()\n    \n    print(\'>>test:\', avg_acc)\n\nfor epoch in range(10):\n    \n    eval(rnn, test_iterator, criteon)\n    train(rnn, train_iterator, optimizer, criteon)'"
lesson55-VAE实战/ae.py,0,"b'import  torch\nfrom    torch import nn\n\n\n\n\n\nclass AE(nn.Module):\n\n\n\n    def __init__(self):\n        super(AE, self).__init__()\n\n\n        # [b, 784] => [b, 20]\n        self.encoder = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 20),\n            nn.ReLU()\n        )\n        # [b, 20] => [b, 784]\n        self.decoder = nn.Sequential(\n            nn.Linear(20, 64),\n            nn.ReLU(),\n            nn.Linear(64, 256),\n            nn.ReLU(),\n            nn.Linear(256, 784),\n            nn.Sigmoid()\n        )\n\n\n    def forward(self, x):\n        """"""\n\n        :param x: [b, 1, 28, 28]\n        :return:\n        """"""\n        batchsz = x.size(0)\n        # flatten\n        x = x.view(batchsz, 784)\n        # encoder\n        x = self.encoder(x)\n        # decoder\n        x = self.decoder(x)\n        # reshape\n        x = x.view(batchsz, 1, 28, 28)\n\n        return x, None'"
lesson55-VAE实战/main.py,3,"b""import  torch\nfrom    torch.utils.data import DataLoader\nfrom    torch import nn, optim\nfrom    torchvision import transforms, datasets\n\nfrom    ae import AE\nfrom    vae import VAE\n\nimport  visdom\n\ndef main():\n    mnist_train = datasets.MNIST('mnist', True, transform=transforms.Compose([\n        transforms.ToTensor()\n    ]), download=True)\n    mnist_train = DataLoader(mnist_train, batch_size=32, shuffle=True)\n\n\n    mnist_test = datasets.MNIST('mnist', False, transform=transforms.Compose([\n        transforms.ToTensor()\n    ]), download=True)\n    mnist_test = DataLoader(mnist_test, batch_size=32, shuffle=True)\n\n\n    x, _ = iter(mnist_train).next()\n    print('x:', x.shape)\n\n    device = torch.device('cuda')\n    # model = AE().to(device)\n    model = VAE().to(device)\n    criteon = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    print(model)\n\n    viz = visdom.Visdom()\n\n    for epoch in range(1000):\n\n\n        for batchidx, (x, _) in enumerate(mnist_train):\n            # [b, 1, 28, 28]\n            x = x.to(device)\n\n            x_hat, kld = model(x)\n            loss = criteon(x_hat, x)\n\n            if kld is not None:\n                elbo = - loss - 1.0 * kld\n                loss = - elbo\n\n            # backprop\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n        print(epoch, 'loss:', loss.item(), 'kld:', kld.item())\n\n        x, _ = iter(mnist_test).next()\n        x = x.to(device)\n        with torch.no_grad():\n            x_hat, kld = model(x)\n        viz.images(x, nrow=8, win='x', opts=dict(title='x'))\n        viz.images(x_hat, nrow=8, win='x_hat', opts=dict(title='x_hat'))\n\n\n\n\n\n\nif __name__ == '__main__':\n    main()"""
lesson55-VAE实战/vae.py,5,"b'import  torch\nfrom    torch import nn\n\n\n\n\n\nclass VAE(nn.Module):\n\n\n\n    def __init__(self):\n        super(VAE, self).__init__()\n\n\n        # [b, 784] => [b, 20]\n        # u: [b, 10]\n        # sigma: [b, 10]\n        self.encoder = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Linear(64, 20),\n            nn.ReLU()\n        )\n        # [b, 20] => [b, 784]\n        self.decoder = nn.Sequential(\n            nn.Linear(10, 64),\n            nn.ReLU(),\n            nn.Linear(64, 256),\n            nn.ReLU(),\n            nn.Linear(256, 784),\n            nn.Sigmoid()\n        )\n\n        self.criteon = nn.MSELoss()\n\n    def forward(self, x):\n        """"""\n\n        :param x: [b, 1, 28, 28]\n        :return:\n        """"""\n        batchsz = x.size(0)\n        # flatten\n        x = x.view(batchsz, 784)\n        # encoder\n        # [b, 20], including mean and sigma\n        h_ = self.encoder(x)\n        # [b, 20] => [b, 10] and [b, 10]\n        mu, sigma = h_.chunk(2, dim=1)\n        # reparametrize trick, epison~N(0, 1)\n        h = mu + sigma * torch.randn_like(sigma)\n\n        # decoder\n        x_hat = self.decoder(h)\n        # reshape\n        x_hat = x_hat.view(batchsz, 1, 28, 28)\n\n        kld = 0.5 * torch.sum(\n            torch.pow(mu, 2) +\n            torch.pow(sigma, 2) -\n            torch.log(1e-8 + torch.pow(sigma, 2)) - 1\n        ) / (batchsz*28*28)\n\n        return x_hat, kld'"
lesson57-WGAN实战/gan.py,11,"b'import  torch \nfrom    torch import nn, optim, autograd\nimport  numpy as np\nimport  visdom\nfrom    torch.nn import functional as F\nfrom    matplotlib import pyplot as plt\nimport  random\n\nh_dim = 400\nbatchsz = 512\nviz = visdom.Visdom()\n\nclass Generator(nn.Module):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(2, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, 2),\n        )\n\n    def forward(self, z):\n        output = self.net(z)\n        return output\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(2, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        output = self.net(x)\n        return output.view(-1)\n\ndef data_generator():\n\n    scale = 2.\n    centers = [\n        (1, 0),\n        (-1, 0),\n        (0, 1),\n        (0, -1),\n        (1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (1. / np.sqrt(2), -1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), -1. / np.sqrt(2))\n    ]\n    centers = [(scale * x, scale * y) for x, y in centers]\n    while True:\n        dataset = []\n        for i in range(batchsz):\n            point = np.random.randn(2) * .02\n            center = random.choice(centers)\n            point[0] += center[0]\n            point[1] += center[1]\n            dataset.append(point)\n        dataset = np.array(dataset, dtype=\'float32\')\n        dataset /= 1.414  # stdev\n        yield dataset\n\n    # for i in range(100000//25):\n    #     for x in range(-2, 3):\n    #         for y in range(-2, 3):\n    #             point = np.random.randn(2).astype(np.float32) * 0.05\n    #             point[0] += 2 * x\n    #             point[1] += 2 * y\n    #             dataset.append(point)\n    #\n    # dataset = np.array(dataset)\n    # print(\'dataset:\', dataset.shape)\n    # viz.scatter(dataset, win=\'dataset\', opts=dict(title=\'dataset\', webgl=True))\n    #\n    # while True:\n    #     np.random.shuffle(dataset)\n    #\n    #     for i in range(len(dataset)//batchsz):\n    #         yield dataset[i*batchsz : (i+1)*batchsz]\n\n\ndef generate_image(D, G, xr, epoch):\n    """"""\n    Generates and saves a plot of the true distribution, the generator, and the\n    critic.\n    """"""\n    N_POINTS = 128\n    RANGE = 3\n    plt.clf()\n\n    points = np.zeros((N_POINTS, N_POINTS, 2), dtype=\'float32\')\n    points[:, :, 0] = np.linspace(-RANGE, RANGE, N_POINTS)[:, None]\n    points[:, :, 1] = np.linspace(-RANGE, RANGE, N_POINTS)[None, :]\n    points = points.reshape((-1, 2))\n    # (16384, 2)\n    # print(\'p:\', points.shape)\n\n    # draw contour\n    with torch.no_grad():\n        points = torch.Tensor(points).cuda() # [16384, 2]\n        disc_map = D(points).cpu().numpy() # [16384]\n    x = y = np.linspace(-RANGE, RANGE, N_POINTS)\n    cs = plt.contour(x, y, disc_map.reshape((len(x), len(y))).transpose())\n    plt.clabel(cs, inline=1, fontsize=10)\n    # plt.colorbar()\n\n\n    # draw samples\n    with torch.no_grad():\n        z = torch.randn(batchsz, 2).cuda() # [b, 2]\n        samples = G(z).cpu().numpy() # [b, 2]\n    plt.scatter(xr[:, 0], xr[:, 1], c=\'orange\', marker=\'.\')\n    plt.scatter(samples[:, 0], samples[:, 1], c=\'green\', marker=\'+\')\n\n    viz.matplot(plt, win=\'contour\', opts=dict(title=\'p(x):%d\'%epoch))\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Linear):\n        # m.weight.data.normal_(0.0, 0.02)\n        nn.init.kaiming_normal_(m.weight)\n        m.bias.data.fill_(0)\n\ndef gradient_penalty(D, xr, xf):\n    """"""\n\n    :param D:\n    :param xr:\n    :param xf:\n    :return:\n    """"""\n    LAMBDA = 0.3\n\n    # only constrait for Discriminator\n    xf = xf.detach()\n    xr = xr.detach()\n\n    # [b, 1] => [b, 2]\n    alpha = torch.rand(batchsz, 1).cuda()\n    alpha = alpha.expand_as(xr)\n\n    interpolates = alpha * xr + ((1 - alpha) * xf)\n    interpolates.requires_grad_()\n\n    disc_interpolates = D(interpolates)\n\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones_like(disc_interpolates),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n\n    return gp\n\ndef main():\n\n    torch.manual_seed(23)\n    np.random.seed(23)\n\n    G = Generator().cuda()\n    D = Discriminator().cuda()\n    G.apply(weights_init)\n    D.apply(weights_init)\n\n    optim_G = optim.Adam(G.parameters(), lr=1e-3, betas=(0.5, 0.9))\n    optim_D = optim.Adam(D.parameters(), lr=1e-3, betas=(0.5, 0.9))\n\n\n    data_iter = data_generator()\n    print(\'batch:\', next(data_iter).shape)\n\n    viz.line([[0,0]], [0], win=\'loss\', opts=dict(title=\'loss\',\n                                                 legend=[\'D\', \'G\']))\n\n    for epoch in range(50000):\n\n        # 1. train discriminator for k steps\n        for _ in range(5):\n            x = next(data_iter)\n            xr = torch.from_numpy(x).cuda()\n\n            # [b]\n            predr = (D(xr))\n            # max log(lossr)\n            lossr = - (predr.mean())\n\n            # [b, 2]\n            z = torch.randn(batchsz, 2).cuda()\n            # stop gradient on G\n            # [b, 2]\n            xf = G(z).detach()\n            # [b]\n            predf = (D(xf))\n            # min predf\n            lossf = (predf.mean())\n\n            # gradient penalty\n            gp = gradient_penalty(D, xr, xf)\n\n            loss_D = lossr + lossf + gp\n            optim_D.zero_grad()\n            loss_D.backward()\n            # for p in D.parameters():\n            #     print(p.grad.norm())\n            optim_D.step()\n\n\n        # 2. train Generator\n        z = torch.randn(batchsz, 2).cuda()\n        xf = G(z)\n        predf = (D(xf))\n        # max predf\n        loss_G = - (predf.mean())\n        optim_G.zero_grad()\n        loss_G.backward()\n        optim_G.step()\n\n\n        if epoch % 100 == 0:\n            viz.line([[loss_D.item(), loss_G.item()]], [epoch], win=\'loss\', update=\'append\')\n\n            generate_image(D, G, xr, epoch)\n\n            print(loss_D.item(), loss_G.item())\n\n\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
lesson57-WGAN实战/wgan_gp.py,11,"b'import  torch\nfrom    torch import nn, optim, autograd\nimport  numpy as np\nimport  visdom\nfrom    torch.nn import functional as F\nfrom    matplotlib import pyplot as plt\nimport  random\n\nh_dim = 400\nbatchsz = 512\nviz = visdom.Visdom()\n\nclass Generator(nn.Module):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(2, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, 2),\n        )\n\n    def forward(self, z):\n        output = self.net(z)\n        return output\n\n\nclass Discriminator(nn.Module):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(2, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, h_dim),\n            nn.ReLU(True),\n            nn.Linear(h_dim, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        output = self.net(x)\n        return output.view(-1)\n\ndef data_generator():\n\n    scale = 2.\n    centers = [\n        (1, 0),\n        (-1, 0),\n        (0, 1),\n        (0, -1),\n        (1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (1. / np.sqrt(2), -1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), 1. / np.sqrt(2)),\n        (-1. / np.sqrt(2), -1. / np.sqrt(2))\n    ]\n    centers = [(scale * x, scale * y) for x, y in centers]\n    while True:\n        dataset = []\n        for i in range(batchsz):\n            point = np.random.randn(2) * .02\n            center = random.choice(centers)\n            point[0] += center[0]\n            point[1] += center[1]\n            dataset.append(point)\n        dataset = np.array(dataset, dtype=\'float32\')\n        dataset /= 1.414  # stdev\n        yield dataset\n\n    # for i in range(100000//25):\n    #     for x in range(-2, 3):\n    #         for y in range(-2, 3):\n    #             point = np.random.randn(2).astype(np.float32) * 0.05\n    #             point[0] += 2 * x\n    #             point[1] += 2 * y\n    #             dataset.append(point)\n    #\n    # dataset = np.array(dataset)\n    # print(\'dataset:\', dataset.shape)\n    # viz.scatter(dataset, win=\'dataset\', opts=dict(title=\'dataset\', webgl=True))\n    #\n    # while True:\n    #     np.random.shuffle(dataset)\n    #\n    #     for i in range(len(dataset)//batchsz):\n    #         yield dataset[i*batchsz : (i+1)*batchsz]\n\n\ndef generate_image(D, G, xr, epoch):\n    """"""\n    Generates and saves a plot of the true distribution, the generator, and the\n    critic.\n    """"""\n    N_POINTS = 128\n    RANGE = 3\n    plt.clf()\n\n    points = np.zeros((N_POINTS, N_POINTS, 2), dtype=\'float32\')\n    points[:, :, 0] = np.linspace(-RANGE, RANGE, N_POINTS)[:, None]\n    points[:, :, 1] = np.linspace(-RANGE, RANGE, N_POINTS)[None, :]\n    points = points.reshape((-1, 2))\n    # (16384, 2)\n    # print(\'p:\', points.shape)\n\n    # draw contour\n    with torch.no_grad():\n        points = torch.Tensor(points).cuda() # [16384, 2]\n        disc_map = D(points).cpu().numpy() # [16384]\n    x = y = np.linspace(-RANGE, RANGE, N_POINTS)\n    cs = plt.contour(x, y, disc_map.reshape((len(x), len(y))).transpose())\n    plt.clabel(cs, inline=1, fontsize=10)\n    # plt.colorbar()\n\n\n    # draw samples\n    with torch.no_grad():\n        z = torch.randn(batchsz, 2).cuda() # [b, 2]\n        samples = G(z).cpu().numpy() # [b, 2]\n    plt.scatter(xr[:, 0], xr[:, 1], c=\'orange\', marker=\'.\')\n    plt.scatter(samples[:, 0], samples[:, 1], c=\'green\', marker=\'+\')\n\n    viz.matplot(plt, win=\'contour\', opts=dict(title=\'p(x):%d\'%epoch))\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Linear):\n        # m.weight.data.normal_(0.0, 0.02)\n        nn.init.kaiming_normal_(m.weight)\n        m.bias.data.fill_(0)\n\ndef gradient_penalty(D, xr, xf):\n    """"""\n\n    :param D:\n    :param xr:\n    :param xf:\n    :return:\n    """"""\n    LAMBDA = 0.3\n\n    # only constrait for Discriminator\n    xf = xf.detach()\n    xr = xr.detach()\n\n    # [b, 1] => [b, 2]\n    alpha = torch.rand(batchsz, 1).cuda()\n    alpha = alpha.expand_as(xr)\n\n    interpolates = alpha * xr + ((1 - alpha) * xf)\n    interpolates.requires_grad_()\n\n    disc_interpolates = D(interpolates)\n\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones_like(disc_interpolates),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n\n    return gp\n\ndef main():\n\n    torch.manual_seed(23)\n    np.random.seed(23)\n\n    G = Generator().cuda()\n    D = Discriminator().cuda()\n    G.apply(weights_init)\n    D.apply(weights_init)\n\n    optim_G = optim.Adam(G.parameters(), lr=1e-3, betas=(0.5, 0.9))\n    optim_D = optim.Adam(D.parameters(), lr=1e-3, betas=(0.5, 0.9))\n\n\n    data_iter = data_generator()\n    print(\'batch:\', next(data_iter).shape)\n\n    viz.line([[0,0]], [0], win=\'loss\', opts=dict(title=\'loss\',\n                                                 legend=[\'D\', \'G\']))\n\n    for epoch in range(50000):\n\n        # 1. train discriminator for k steps\n        for _ in range(5):\n            x = next(data_iter)\n            xr = torch.from_numpy(x).cuda()\n\n            # [b]\n            predr = (D(xr))\n            # max log(lossr)\n            lossr = - (predr.mean())\n\n            # [b, 2]\n            z = torch.randn(batchsz, 2).cuda()\n            # stop gradient on G\n            # [b, 2]\n            xf = G(z).detach()\n            # [b]\n            predf = (D(xf))\n            # min predf\n            lossf = (predf.mean())\n\n            # gradient penalty\n            gp = gradient_penalty(D, xr, xf)\n\n            loss_D = lossr + lossf + gp\n            optim_D.zero_grad()\n            loss_D.backward()\n            # for p in D.parameters():\n            #     print(p.grad.norm())\n            optim_D.step()\n\n\n        # 2. train Generator\n        z = torch.randn(batchsz, 2).cuda()\n        xf = G(z)\n        predf = (D(xf))\n        # max predf\n        loss_G = - (predf.mean())\n        optim_G.zero_grad()\n        loss_G.backward()\n        optim_G.step()\n\n\n        if epoch % 100 == 0:\n            viz.line([[loss_D.item(), loss_G.item()]], [epoch], win=\'loss\', update=\'append\')\n\n            generate_image(D, G, xr, epoch)\n\n            print(loss_D.item(), loss_G.item())\n\n\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
lesson58-图卷积网络GCN/layers.py,4,"b'import  math\n\nimport  torch\nfrom    torch import nn\n\n\nclass GraphConvolution(nn.Module):\n    """"""\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, adj):\n        """"""\n\n        :param input:\n        :param adj:\n        :return:\n        """"""\n        support = torch.mm(input, self.weight)\n        # sparse matrix multiplication\n        output = torch.spmm(adj, support)\n        if self.bias is not None:\n            return output + self.bias\n        else:\n            return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + str(self.in_features) + \' -> \' \\\n               + str(self.out_features) + \')\'\n'"
lesson58-图卷积网络GCN/models.py,2,"b'import  torch.nn as nn\nimport  torch.nn.functional as F\nfrom    layers import GraphConvolution\n\n\nclass GCN(nn.Module):\n\n    def __init__(self, nfeat, nhid, nclass, dropout):\n        super(GCN, self).__init__()\n\n        self.gc1 = GraphConvolution(nfeat, nhid)\n        self.gc2 = GraphConvolution(nhid, nclass)\n        self.dropout = dropout\n\n    def forward(self, x, adj):\n        """"""\n\n        :param x: [2708, 1433]\n        :param adj: [2708, 2708]\n        :return:\n        """"""\n        # print(\'x:\', x.shape, \'adj:\', adj.shape)\n        # => [2708, 16]\n        x = F.relu(self.gc1(x, adj))\n        # print(\'gcn1:\', x.shape)\n        x = F.dropout(x, self.dropout, training=self.training)\n        # => [2708, 7]\n        x = self.gc2(x, adj)\n        # print(\'gcn2:\', x.shape)\n        return F.log_softmax(x, dim=1)\n'"
lesson58-图卷积网络GCN/train.py,5,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport time\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom utils import load_data, accuracy\nfrom models import GCN\n\n# Training settings\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'Disables CUDA training.\')\nparser.add_argument(\'--fastmode\', action=\'store_true\', default=False,\n                    help=\'Validate during training pass.\')\nparser.add_argument(\'--seed\', type=int, default=42, help=\'Random seed.\')\nparser.add_argument(\'--epochs\', type=int, default=200,\n                    help=\'Number of epochs to train.\')\nparser.add_argument(\'--lr\', type=float, default=0.01,\n                    help=\'Initial learning rate.\')\nparser.add_argument(\'--weight_decay\', type=float, default=5e-4,\n                    help=\'Weight decay (L2 loss on parameters).\')\nparser.add_argument(\'--hidden\', type=int, default=16,\n                    help=\'Number of hidden units.\')\nparser.add_argument(\'--dropout\', type=float, default=0.5,\n                    help=\'Dropout rate (1 - keep probability).\')\n\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n# Load data\nadj, features, labels, idx_train, idx_val, idx_test = load_data()\n\n# Model and optimizer\nmodel = GCN(nfeat=features.shape[1],\n            nhid=args.hidden,\n            nclass=labels.max().item() + 1,\n            dropout=args.dropout)\noptimizer = optim.Adam(model.parameters(),\n                       lr=args.lr, weight_decay=args.weight_decay)\n\nprint(model)\n\nif args.cuda:\n    model.cuda()\n    features = features.cuda()\n    adj = adj.cuda()\n    labels = labels.cuda()\n    idx_train = idx_train.cuda()\n    idx_val = idx_val.cuda()\n    idx_test = idx_test.cuda()\n\n\ndef train(epoch):\n\n    t = time.time()\n\n    model.train()\n    # [N, 7]\n    output = model(features, adj)\n    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n    acc_train = accuracy(output[idx_train], labels[idx_train])\n\n    optimizer.zero_grad()\n    loss_train.backward()\n    optimizer.step()\n\n    if not args.fastmode:\n        # Evaluate validation set performance separately,\n        # deactivates dropout during validation run.\n        model.eval()\n        output = model(features, adj)\n\n    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n    acc_val = accuracy(output[idx_val], labels[idx_val])\n    print(\'Epoch: {:04d}\'.format(epoch+1),\n          \'loss_train: {:.4f}\'.format(loss_train.item()),\n          \'acc_train: {:.4f}\'.format(acc_train.item()),\n          \'loss_val: {:.4f}\'.format(loss_val.item()),\n          \'acc_val: {:.4f}\'.format(acc_val.item()),\n          \'time: {:.4f}s\'.format(time.time() - t))\n\n\ndef test():\n\n    model.eval()\n\n    output = model(features, adj)\n    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n    acc_test = accuracy(output[idx_test], labels[idx_test])\n    print(""Test set results:"",\n          ""loss= {:.4f}"".format(loss_test.item()),\n          ""accuracy= {:.4f}"".format(acc_test.item()))\n\n\n# Train model\nt_total = time.time()\nfor epoch in range(args.epochs):\n    train(epoch)\nprint(""Optimization Finished!"")\nprint(""Total time elapsed: {:.4f}s"".format(time.time() - t_total))\n\n# Testing\ntest()\n'"
lesson58-图卷积网络GCN/utils.py,9,"b'import numpy as np\nimport scipy.sparse as sp\nimport torch\n\n\ndef encode_onehot(labels):\n    classes = set(labels)\n    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n                    enumerate(classes)}\n    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n                             dtype=np.int32)\n    return labels_onehot\n\n\ndef load_data(path=""./data/cora/"", dataset=""cora""):\n    """"""Load citation network dataset (cora only for now)""""""\n    print(\'Loading {} dataset...\'.format(dataset))\n\n    idx_features_labels = np.genfromtxt(""{}{}.content"".format(path, dataset),\n                                        dtype=np.dtype(str))\n    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n    labels = encode_onehot(idx_features_labels[:, -1])\n\n    # build graph\n    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n    idx_map = {j: i for i, j in enumerate(idx)}\n    edges_unordered = np.genfromtxt(""{}{}.cites"".format(path, dataset),\n                                    dtype=np.int32)\n    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n                     dtype=np.int32).reshape(edges_unordered.shape)\n    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n                        shape=(labels.shape[0], labels.shape[0]),\n                        dtype=np.float32)\n\n    # build symmetric adjacency matrix\n    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n\n    features = normalize(features)\n    adj = normalize(adj + sp.eye(adj.shape[0]))\n\n    idx_train = range(140)\n    idx_val = range(200, 500)\n    idx_test = range(500, 1500)\n\n    features = torch.FloatTensor(np.array(features.todense()))\n    labels = torch.LongTensor(np.where(labels)[1])\n    adj = sparse_mx_to_torch_sparse_tensor(adj)\n\n    idx_train = torch.LongTensor(idx_train)\n    idx_val = torch.LongTensor(idx_val)\n    idx_test = torch.LongTensor(idx_test)\n\n    return adj, features, labels, idx_train, idx_val, idx_test\n\n\ndef normalize(mx):\n    """"""Row-normalize sparse matrix""""""\n    rowsum = np.array(mx.sum(1))\n    r_inv = np.power(rowsum, -1).flatten()\n    r_inv[np.isinf(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    mx = r_mat_inv.dot(mx)\n    return mx\n\n\ndef accuracy(output, labels):\n    preds = output.max(1)[1].type_as(labels)\n    correct = preds.eq(labels).double()\n    correct = correct.sum()\n    return correct / len(labels)\n\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    """"""Convert a scipy sparse matrix to a torch sparse tensor.""""""\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)\n'"
lesson63-迁移学习-自定义数据集实战/pokemon.py,4,"b'import  torch\nimport  os, glob\nimport  random, csv\n\nfrom    torch.utils.data import Dataset, DataLoader\n\nfrom    torchvision import transforms\nfrom    PIL import Image\n\n\nclass Pokemon(Dataset):\n\n    def __init__(self, root, resize, mode):\n        super(Pokemon, self).__init__()\n\n        self.root = root\n        self.resize = resize\n\n        self.name2label = {} # ""sq..."":0\n        for name in sorted(os.listdir(os.path.join(root))):\n            if not os.path.isdir(os.path.join(root, name)):\n                continue\n\n            self.name2label[name] = len(self.name2label.keys())\n\n        # print(self.name2label)\n\n        # image, label\n        self.images, self.labels = self.load_csv(\'images.csv\')\n\n        if mode==\'train\': # 60%\n            self.images = self.images[:int(0.6*len(self.images))]\n            self.labels = self.labels[:int(0.6*len(self.labels))]\n        elif mode==\'val\': # 20% = 60%->80%\n            self.images = self.images[int(0.6*len(self.images)):int(0.8*len(self.images))]\n            self.labels = self.labels[int(0.6*len(self.labels)):int(0.8*len(self.labels))]\n        else: # 20% = 80%->100%\n            self.images = self.images[int(0.8*len(self.images)):]\n            self.labels = self.labels[int(0.8*len(self.labels)):]\n\n\n\n\n\n    def load_csv(self, filename):\n\n        if not os.path.exists(os.path.join(self.root, filename)):\n            images = []\n            for name in self.name2label.keys():\n                # \'pokemon\\\\mewtwo\\\\00001.png\n                images += glob.glob(os.path.join(self.root, name, \'*.png\'))\n                images += glob.glob(os.path.join(self.root, name, \'*.jpg\'))\n                images += glob.glob(os.path.join(self.root, name, \'*.jpeg\'))\n\n            # 1167, \'pokemon\\\\bulbasaur\\\\00000000.png\'\n            print(len(images), images)\n\n            random.shuffle(images)\n            with open(os.path.join(self.root, filename), mode=\'w\', newline=\'\') as f:\n                writer = csv.writer(f)\n                for img in images: # \'pokemon\\\\bulbasaur\\\\00000000.png\'\n                    name = img.split(os.sep)[-2]\n                    label = self.name2label[name]\n                    # \'pokemon\\\\bulbasaur\\\\00000000.png\', 0\n                    writer.writerow([img, label])\n                print(\'writen into csv file:\', filename)\n\n        # read from csv file\n        images, labels = [], []\n        with open(os.path.join(self.root, filename)) as f:\n            reader = csv.reader(f)\n            for row in reader:\n                # \'pokemon\\\\bulbasaur\\\\00000000.png\', 0\n                img, label = row\n                label = int(label)\n\n                images.append(img)\n                labels.append(label)\n\n        assert len(images) == len(labels)\n\n        return images, labels\n\n\n\n    def __len__(self):\n\n        return len(self.images)\n\n\n    def denormalize(self, x_hat):\n\n        mean = [0.485, 0.456, 0.406]\n        std = [0.229, 0.224, 0.225]\n\n        # x_hat = (x-mean)/std\n        # x = x_hat*std = mean\n        # x: [c, h, w]\n        # mean: [3] => [3, 1, 1]\n        mean = torch.tensor(mean).unsqueeze(1).unsqueeze(1)\n        std = torch.tensor(std).unsqueeze(1).unsqueeze(1)\n        # print(mean.shape, std.shape)\n        x = x_hat * std + mean\n\n        return x\n\n\n    def __getitem__(self, idx):\n        # idx~[0~len(images)]\n        # self.images, self.labels\n        # img: \'pokemon\\\\bulbasaur\\\\00000000.png\'\n        # label: 0\n        img, label = self.images[idx], self.labels[idx]\n\n        tf = transforms.Compose([\n            lambda x:Image.open(x).convert(\'RGB\'), # string path= > image data\n            transforms.Resize((int(self.resize*1.25), int(self.resize*1.25))),\n            transforms.RandomRotation(15),\n            transforms.CenterCrop(self.resize),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n\n        img = tf(img)\n        label = torch.tensor(label)\n\n\n        return img, label\n\n\n\n\n\ndef main():\n\n    import  visdom\n    import  time\n    import  torchvision\n\n    viz = visdom.Visdom()\n\n    # tf = transforms.Compose([\n    #                 transforms.Resize((64,64)),\n    #                 transforms.ToTensor(),\n    # ])\n    # db = torchvision.datasets.ImageFolder(root=\'pokemon\', transform=tf)\n    # loader = DataLoader(db, batch_size=32, shuffle=True)\n    #\n    # print(db.class_to_idx)\n    #\n    # for x,y in loader:\n    #     viz.images(x, nrow=8, win=\'batch\', opts=dict(title=\'batch\'))\n    #     viz.text(str(y.numpy()), win=\'label\', opts=dict(title=\'batch-y\'))\n    #\n    #     time.sleep(10)\n\n\n    db = Pokemon(\'pokemon\', 64, \'train\')\n\n    x,y = next(iter(db))\n    print(\'sample:\', x.shape, y.shape, y)\n\n    viz.image(db.denormalize(x), win=\'sample_x\', opts=dict(title=\'sample_x\'))\n\n    loader = DataLoader(db, batch_size=32, shuffle=True, num_workers=8)\n\n    for x,y in loader:\n        viz.images(db.denormalize(x), nrow=8, win=\'batch\', opts=dict(title=\'batch\'))\n        viz.text(str(y.numpy()), win=\'label\', opts=dict(title=\'batch-y\'))\n\n        time.sleep(10)\n\nif __name__ == \'__main__\':\n    main()'"
lesson63-迁移学习-自定义数据集实战/resnet.py,3,"b'import  torch\nfrom    torch import  nn\nfrom    torch.nn import functional as F\n\n\n\nclass ResBlk(nn.Module):\n    """"""\n    resnet block\n    """"""\n\n    def __init__(self, ch_in, ch_out, stride=1):\n        """"""\n        :param ch_in:\n        :param ch_out:\n        """"""\n        super(ResBlk, self).__init__()\n\n        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(ch_out)\n        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(ch_out)\n\n        self.extra = nn.Sequential()\n        if ch_out != ch_in:\n            # [b, ch_in, h, w] => [b, ch_out, h, w]\n            self.extra = nn.Sequential(\n                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(ch_out)\n            )\n\n\n    def forward(self, x):\n        """"""\n        :param x: [b, ch, h, w]\n        :return:\n        """"""\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        # short cut.\n        # extra module: [b, ch_in, h, w] => [b, ch_out, h, w]\n        # element-wise add:\n        out = self.extra(x) + out\n        out = F.relu(out)\n\n        return out\n\n\n\n\nclass ResNet18(nn.Module):\n\n    def __init__(self, num_class):\n        super(ResNet18, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=3, padding=0),\n            nn.BatchNorm2d(16)\n        )\n        # followed 4 blocks\n        # [b, 16, h, w] => [b, 32, h ,w]\n        self.blk1 = ResBlk(16, 32, stride=3)\n        # [b, 32, h, w] => [b, 64, h, w]\n        self.blk2 = ResBlk(32, 64, stride=3)\n        # # [b, 64, h, w] => [b, 128, h, w]\n        self.blk3 = ResBlk(64, 128, stride=2)\n        # # [b, 128, h, w] => [b, 256, h, w]\n        self.blk4 = ResBlk(128, 256, stride=2)\n\n        # [b, 256, 7, 7]\n        self.outlayer = nn.Linear(256*3*3, num_class)\n\n    def forward(self, x):\n        """"""\n        :param x:\n        :return:\n        """"""\n        x = F.relu(self.conv1(x))\n\n        # [b, 64, h, w] => [b, 1024, h, w]\n        x = self.blk1(x)\n        x = self.blk2(x)\n        x = self.blk3(x)\n        x = self.blk4(x)\n\n        # print(x.shape)\n        x = x.view(x.size(0), -1)\n        x = self.outlayer(x)\n\n\n        return x\n\n\n\ndef main():\n    blk = ResBlk(64, 128)\n    tmp = torch.randn(2, 64, 224, 224)\n    out = blk(tmp)\n    print(\'block:\', out.shape)\n\n\n    model = ResNet18(5)\n    tmp = torch.randn(2, 3, 224, 224)\n    out = model(tmp)\n    print(\'resnet:\', out.shape)\n\n    p = sum(map(lambda p:p.numel(), model.parameters()))\n    print(\'parameters size:\', p)\n\n\nif __name__ == \'__main__\':\n    main()'"
lesson63-迁移学习-自定义数据集实战/train_scratch.py,7,"b""import  torch\nfrom    torch import optim, nn\nimport  visdom\nimport  torchvision\nfrom    torch.utils.data import DataLoader\n\nfrom    pokemon import Pokemon\nfrom    resnet import ResNet18\n\n\n\nbatchsz = 32\nlr = 1e-3\nepochs = 10\n\ndevice = torch.device('cuda')\ntorch.manual_seed(1234)\n\n\ntrain_db = Pokemon('pokemon', 224, mode='train')\nval_db = Pokemon('pokemon', 224, mode='val')\ntest_db = Pokemon('pokemon', 224, mode='test')\ntrain_loader = DataLoader(train_db, batch_size=batchsz, shuffle=True,\n                          num_workers=4)\nval_loader = DataLoader(val_db, batch_size=batchsz, num_workers=2)\ntest_loader = DataLoader(test_db, batch_size=batchsz, num_workers=2)\n\n\nviz = visdom.Visdom()\n\ndef evalute(model, loader):\n    model.eval()\n    \n    correct = 0\n    total = len(loader.dataset)\n\n    for x,y in loader:\n        x,y = x.to(device), y.to(device)\n        with torch.no_grad():\n            logits = model(x)\n            pred = logits.argmax(dim=1)\n        correct += torch.eq(pred, y).sum().float().item()\n\n    return correct / total\n\ndef main():\n\n    model = ResNet18(5).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criteon = nn.CrossEntropyLoss()\n\n\n    best_acc, best_epoch = 0, 0\n    global_step = 0\n    viz.line([0], [-1], win='loss', opts=dict(title='loss'))\n    viz.line([0], [-1], win='val_acc', opts=dict(title='val_acc'))\n    for epoch in range(epochs):\n\n        for step, (x,y) in enumerate(train_loader):\n\n            # x: [b, 3, 224, 224], y: [b]\n            x, y = x.to(device), y.to(device)\n            \n            model.train()\n            logits = model(x)\n            loss = criteon(logits, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            viz.line([loss.item()], [global_step], win='loss', update='append')\n            global_step += 1\n\n        if epoch % 1 == 0:\n\n            val_acc = evalute(model, val_loader)\n            if val_acc> best_acc:\n                best_epoch = epoch\n                best_acc = val_acc\n\n                torch.save(model.state_dict(), 'best.mdl')\n\n                viz.line([val_acc], [global_step], win='val_acc', update='append')\n\n\n    print('best acc:', best_acc, 'best epoch:', best_epoch)\n\n    model.load_state_dict(torch.load('best.mdl'))\n    print('loaded from ckpt!')\n\n    test_acc = evalute(model, test_loader)\n    print('test acc:', test_acc)\n\n\n\n\n\nif __name__ == '__main__':\n    main()\n"""
lesson63-迁移学习-自定义数据集实战/train_transfer.py,8,"b""import  torch\nfrom    torch import optim, nn\nimport  visdom\nimport  torchvision\nfrom    torch.utils.data import DataLoader\n\nfrom    pokemon import Pokemon\n# from    resnet import ResNet18\nfrom    torchvision.models import resnet18\n\nfrom    utils import Flatten\n\nbatchsz = 32\nlr = 1e-3\nepochs = 10\n\ndevice = torch.device('cuda')\ntorch.manual_seed(1234)\n\n\ntrain_db = Pokemon('pokemon', 224, mode='train')\nval_db = Pokemon('pokemon', 224, mode='val')\ntest_db = Pokemon('pokemon', 224, mode='test')\ntrain_loader = DataLoader(train_db, batch_size=batchsz, shuffle=True,\n                          num_workers=4)\nval_loader = DataLoader(val_db, batch_size=batchsz, num_workers=2)\ntest_loader = DataLoader(test_db, batch_size=batchsz, num_workers=2)\n\n\nviz = visdom.Visdom()\n\ndef evalute(model, loader):\n    model.eval()\n    \n    correct = 0\n    total = len(loader.dataset)\n\n    for x,y in loader:\n        x,y = x.to(device), y.to(device)\n        with torch.no_grad():\n            logits = model(x)\n            pred = logits.argmax(dim=1)\n        correct += torch.eq(pred, y).sum().float().item()\n\n    return correct / total\n\ndef main():\n\n    # model = ResNet18(5).to(device)\n    trained_model = resnet18(pretrained=True)\n    model = nn.Sequential(*list(trained_model.children())[:-1], #[b, 512, 1, 1]\n                          Flatten(), # [b, 512, 1, 1] => [b, 512]\n                          nn.Linear(512, 5)\n                          ).to(device)\n    # x = torch.randn(2, 3, 224, 224)\n    # print(model(x).shape)\n\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criteon = nn.CrossEntropyLoss()\n\n\n    best_acc, best_epoch = 0, 0\n    global_step = 0\n    viz.line([0], [-1], win='loss', opts=dict(title='loss'))\n    viz.line([0], [-1], win='val_acc', opts=dict(title='val_acc'))\n    for epoch in range(epochs):\n\n        for step, (x,y) in enumerate(train_loader):\n\n            # x: [b, 3, 224, 224], y: [b]\n            x, y = x.to(device), y.to(device)\n\n            model.train()\n            logits = model(x)\n            loss = criteon(logits, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            viz.line([loss.item()], [global_step], win='loss', update='append')\n            global_step += 1\n\n        if epoch % 1 == 0:\n\n            val_acc = evalute(model, val_loader)\n            if val_acc> best_acc:\n                best_epoch = epoch\n                best_acc = val_acc\n\n                torch.save(model.state_dict(), 'best.mdl')\n\n                viz.line([val_acc], [global_step], win='val_acc', update='append')\n\n\n    print('best acc:', best_acc, 'best epoch:', best_epoch)\n\n    model.load_state_dict(torch.load('best.mdl'))\n    print('loaded from ckpt!')\n\n    test_acc = evalute(model, test_loader)\n    print('test acc:', test_acc)\n\n\n\n\n\nif __name__ == '__main__':\n    main()\n"""
lesson63-迁移学习-自定义数据集实战/utils.py,1,"b'from    matplotlib import pyplot as plt\nimport  torch\nfrom    torch import nn\n\nclass Flatten(nn.Module):\n\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n        return x.view(-1, shape)\n\n\ndef plot_image(img, label, name):\n\n    fig = plt.figure()\n    for i in range(6):\n        plt.subplot(2, 3, i + 1)\n        plt.tight_layout()\n        plt.imshow(img[i][0]*0.3081+0.1307, cmap=\'gray\', interpolation=\'none\')\n        plt.title(""{}: {}"".format(name, label[i].item()))\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()'"
