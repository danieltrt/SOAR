file_path,api_count,code
d2l/__init__.py,0,"b""from .base import *\nfrom .figure import *\nfrom .data import *\nfrom .model import *\nfrom .train import *\nfrom .ssd_utils import *\n__version__ = '0.1.1'\n"""
d2l/base.py,7,"b'""""""The base module contains some basic functions/classes for d2l""""""\nimport time\nimport torch\nimport numpy as np\n\n__all__ = [\'try_gpu\', \'try_all_gpus\', \'Benchmark\', \'Timer\', \'Accumulator\']\n\ndef try_gpu():\n    """"""If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.""""""\n    if torch.cuda.is_available():\n        device = torch.device(\'cuda:0\')\n    else:\n        device = torch.device(\'cpu\')\n    return device\n\ndef try_all_gpus():\n    """"""Return all available GPUs, or [torch device cpu] if there is no GPU.""""""\n    if torch.cuda.is_available():\n        devices = []\n        for i in range(16):\n            device = torch.device(\'cuda:\'+str(i))\n            devices.append(device)\n    else:\n        devices = [torch.device(\'cpu\')]\n    return devices\n\nclass Benchmark():\n    """"""Benchmark programs.""""""\n    def __init__(self, prefix=None):\n        self.prefix = prefix + \' \' if prefix else \'\'\n\n    def __enter__(self):\n        self.start = time.time()\n\n    def __exit__(self, *args):\n        print(\'%stime: %.4f sec\' % (self.prefix, time.time() - self.start))\n\nclass Timer(object):\n    """"""Record multiple running times.""""""\n    def __init__(self):\n        self.times = []\n        self.start()\n        \n    def start(self):\n        """"""Start the timer""""""\n        self.start_time = time.time()\n    \n    def stop(self):\n        """"""Stop the timer and record the time in a list""""""\n        self.times.append(time.time() - self.start_time)\n        return self.times[-1]\n        \n    def avg(self):\n        """"""Return the average time""""""\n        return sum(self.times)/len(self.times)\n    \n    def sum(self):\n        """"""Return the sum of time""""""\n        return sum(self.times)\n        \n    def cumsum(self):\n        """"""Return the accumuated times""""""\n        return np.array(self.times).cumsum().tolist()\n\nclass Accumulator(object):\n    """"""Sum a list of numbers over time""""""\n    def __init__(self, n):\n        self.data = [0.0] * n\n    def add(self, *args):\n        self.data = [a+b for a, b in zip(self.data, args)]\n    def reset(self):\n        self.data = [0] * len(self.data)\n    def __getitem__(self, i):\n        return self.data[i]\n'"
d2l/figure.py,0,"b'""""""The image module contains functions for plotting""""""\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n__all__ = [\'plt\', \'bbox_to_rect\', \'semilogy\', \'set_figsize\', \'show_bboxes\',\n           \'show_images\', \'show_trace_2d\', \'use_svg_display\', \'plot\', \'set_axes\', \'Animator\']\n\ndef bbox_to_rect(bbox, color):\n    """"""Convert bounding box to matplotlib format.""""""\n    return plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0],\n                         height=bbox[3]-bbox[1], fill=False, edgecolor=color,\n                         linewidth=2)\n\ndef semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\n             legend=None, figsize=(3.5, 2.5)):\n    """"""Plot x and log(y).""""""\n    set_figsize(figsize)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.semilogy(x_vals, y_vals)\n    if x2_vals and y2_vals:\n        plt.semilogy(x2_vals, y2_vals, linestyle=\':\')\n        plt.legend(legend)\n    plt.show()\n\n\ndef set_figsize(figsize=(3.5, 2.5)):\n    """"""Set matplotlib figure size.""""""\n    use_svg_display()\n    plt.rcParams[\'figure.figsize\'] = figsize\n\ndef _make_list(obj, default_values=None):\n    if obj is None:\n        obj = default_values\n    elif not isinstance(obj, (list, tuple)):\n        obj = [obj]\n    return obj\n\ndef show_bboxes(axes, bboxes, labels=None, colors=None):\n    """"""Show bounding boxes.""""""\n    labels = _make_list(labels)\n    colors = _make_list(colors, [\'b\', \'g\', \'r\', \'m\', \'k\'])\n    for i, bbox in enumerate(bboxes):\n        color = colors[i % len(colors)]\n        rect = bbox_to_rect(bbox.numpy(), color)\n        axes.add_patch(rect)\n        if labels and len(labels) > i:\n            text_color = \'k\' if color == \'w\' else \'w\'\n            axes.text(rect.xy[0], rect.xy[1], labels[i],\n                      va=\'center\', ha=\'center\', fontsize=9, color=text_color,\n                      bbox=dict(facecolor=color, lw=0))\n\ndef show_images(imgs, num_rows, num_cols, scale=2):\n    """"""Plot a list of images.""""""\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n    for i in range(num_rows):\n        for j in range(num_cols):\n            axes[i][j].imshow(imgs[i * num_cols + j].asnumpy())\n            axes[i][j].axes.get_xaxis().set_visible(False)\n            axes[i][j].axes.get_yaxis().set_visible(False)\n    return axes\n\ndef show_trace_2d(f, res):\n    """"""Show the trace of 2D variables during optimization.""""""\n    x1, x2 = zip(*res)\n    set_figsize()\n    plt.plot(x1, x2, \'-o\', color=\'#ff7f0e\')\n    x1 = np.arange(-5.5, 1.0, 0.1)\n    x2 = np.arange(min(-3.0, min(x2) - 1), max(1.0, max(x2) + 1), 0.1)\n    x1, x2 = np.meshgrid(x1, x2)\n    plt.contour(x1, x2, f(x1, x2), colors=\'#1f77b4\')\n    plt.xlabel(\'x1\')\n    plt.ylabel(\'x2\')\n\ndef use_svg_display():\n    """"""Use svg format to display plot in jupyter.""""""\n    display.set_matplotlib_formats(\'svg\')\n\ndef plot(X, Y=None, xlabel=None, ylabel=None, legend=[], xlim=None,\n         ylim=None, xscale=\'linear\', yscale=\'linear\', fmts=None,\n         figsize=(3.5, 2.5), axes=None):\n    """"""Plot multiple lines""""""\n    set_figsize(figsize)\n    axes = axes if axes else plt.gca()\n    #if isinstance(X, nd.NDArray): X = X.asnumpy()\n    #if isinstance(Y, nd.NDArray): Y = Y.asnumpy()\n    if not hasattr(X[0], ""__len__""): X = [X]\n    if Y is None: X, Y = [[]]*len(X), X\n    if not hasattr(Y[0], ""__len__""): Y = [Y]\n    if len(X) != len(Y): X = X * len(Y)\n    if not fmts: fmts = [\'-\']*len(X)\n    axes.cla()\n    for x, y, fmt in zip(X, Y, fmts):\n        #if isinstance(x, nd.NDArray): x = x.asnumpy()\n        #if isinstance(y, nd.NDArray): y = y.asnumpy()\n        if len(x):\n            axes.plot(x, y, fmt)\n        else:\n            axes.plot(y, fmt)\n    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    """"""A utility function to set matplotlib axes""""""\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend: axes.legend(legend)\n    axes.grid()\n\nclass Animator(object):\n    def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None,\n                 ylim=None, xscale=\'linear\', yscale=\'linear\', fmts=None,\n                 nrows=1, ncols=1, figsize=(3.5, 2.5)):\n        """"""Incrementally plot multiple lines.""""""\n        use_svg_display()\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1: self.axes = [self.axes,]\n        # use a lambda to capture arguments\n        self.config_axes = lambda : set_axes(\n            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        """"""Add multiple data points into the figure.""""""\n        if not hasattr(y, ""__len__""): y = [y]\n        n = len(y)\n        if not hasattr(x, ""__len__""): x = [x] * n\n        if not self.X: self.X = [[] for _ in range(n)]\n        if not self.Y: self.Y = [[] for _ in range(n)]\n        if not self.fmts: self.fmts = [\'-\'] * n\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n'"
d2l/model.py,8,"b'""""""The model module contains neural network building blocks""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'corr2d\', \'linreg\', \'RNNModel\' , \'Encoder\', \'Decoder\', \'EncoderDecoder\']\n\ndef corr2d(X, K):\n    """"""Compute 2D cross-correlation.""""""\n    h, w = K.shape\n    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n    return Y\n\ndef linreg(X, w, b):\n\t""""""Linear regression.""""""\n\treturn torch.mm(X,w) + b\n\nclass RNNModel(nn.Module):\n    """"""RNN model.""""""\n\n    def __init__(self, rnn_layer, num_inputs, vocab_size, **kwargs):\n        super(RNNModel, self).__init__(**kwargs)\n        self.rnn = rnn_layer\n        self.vocab_size = vocab_size\n        self.Linear = nn.Linear(num_inputs, vocab_size)\n\n    def forward(self, inputs, state):\n        """"""Forward function""""""\n        X = F.one_hot(inputs.long().transpose(0,-1), self.vocab_size)\n        X = X.to(torch.float32)\n        Y, state = self.rnn(X, state)\n        output = self.Linear(Y.reshape((-1, Y.shape[-1])))\n        return output, state\n\n    def begin_state(self, num_hiddens, device, batch_size=1, num_layers=1):\n        """"""Return the begin state""""""\n        if num_layers == 1:\n          return  torch.zeros(size=(1, batch_size, num_hiddens), dtype=torch.float32, device=device)\n        else:\n          return (torch.zeros(size=(1, batch_size, num_hiddens), dtype=torch.float32, device=device),\n                  torch.zeros(size=(1, batch_size, num_hiddens), dtype=torch.float32, device=device))\n        \nclass Residual(nn.Module):\n  \n  def __init__(self,input_channels, num_channels, use_1x1conv=False, strides=1, **kwargs):\n    super(Residual, self).__init__(**kwargs)\n    self.conv1 = nn.Conv2d(input_channels, num_channels,kernel_size=3, padding=1, stride=strides)\n    self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)\n    if use_1x1conv:\n      self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n    else:\n      self.conv3 = None\n    self.bn1 = nn.BatchNorm2d(num_channels)\n    self.bn2 = nn.BatchNorm2d(num_channels)\n    self.relu = nn.ReLU(inplace=True)\n  \n  def forward(self, X):\n    \n    Y = self.relu(self.bn1(self.conv1(X)))\n    Y = self.bn2(self.conv2(Y))\n    if self.conv3:\n      X = self.conv3(X)\n    Y += X\n    Y =self.relu(Y)\n    return Y\n\nclass Encoder(nn.Module):\n    """"""The base encoder interface for the encoder-decoder architecture.""""""\n    def __init__(self, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n    def forward(self, X, *args):\n        """"""Forward function""""""\n        raise NotImplementedError\n\nclass Decoder(nn.Module):\n    """"""The base decoder interface for the encoder-decoder archtecture.""""""\n    def __init__(self, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n    def init_state(self, enc_outputs, *args):\n        """"""Return the begin state""""""\n        raise NotImplementedError\n\n    def forward(self, X, state):\n        """"""Forward function""""""\n        raise NotImplementedError\n\nclass EncoderDecoder(nn.Module):\n    """"""The base class for the encoder-decoder architecture.""""""\n    def __init__(self, encoder, decoder, **kwargs):\n        super(EncoderDecoder, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, enc_X, dec_X, *args):\n        """"""Forward function""""""\n        enc_outputs = self.encoder(enc_X, *args)\n        dec_state = self.decoder.init_state(enc_outputs, *args)\n        return self.decoder(dec_X, dec_state)\n\n'"
d2l/ssd_utils.py,30,"b'import os\nimport errno\nfrom tqdm import tqdm\nimport torch\nimport json\nimport numpy as np\nfrom PIL import Image\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport sys\nd2l = sys.modules[__name__]\n\n# Defined in file: ./chapter_preface/preface.md\nfrom matplotlib import pyplot as plt\n\nimport d2l\nimport json\nimport time\nfrom collections import namedtuple\nimport cv2\nfrom IPython import display\n\n\n##################################### Display Functions #################################################\n\n# Defined in file: ./chapter_crashcourse/probability.md\ndef use_svg_display():\n    """"""Use the svg format to display plot in jupyter.""""""\n    display.set_matplotlib_formats(\'svg\')\n\n\n# Defined in file: ./chapter_crashcourse/probability.md\ndef set_figsize(figsize=(3.5, 2.5)):\n    """"""Change the default figure size""""""\n    use_svg_display()\n    plt.rcParams[\'figure.figsize\'] = figsize\n\n# Defined in file: ./chapter_crashcourse/naive-bayes.md\ndef show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n    """"""Plot a list of images.""""""\n    figsize = (num_cols * scale, num_rows * scale)\n    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n    axes = axes.flatten()\n    for i, (ax, img) in enumerate(zip(axes, imgs)):\n        ax.imshow(img.numpy())\n        ax.axes.get_xaxis().set_visible(False)\n        ax.axes.get_yaxis().set_visible(False)\n        if titles:\n            ax.set_title(titles[i])\n    return axes\n\ndef read_img(img_str: str, target_size: int) -> np.ndarray:\n    img = cv2.imread(img_str, cv2.IMREAD_UNCHANGED)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (target_size, target_size))\n    return img\n\n\ndef draw_boxes(img: str, boxes: list) -> np.ndarray:\n    for box in boxes:\n        cv2.rectangle(img, (int(box[0] - box[2]/2), int(box[1] - box[3]/2)),\n                      (int(box[0] + box[2]/2), int(box[1] + box[3]/2)),(0, 0, 255), 2)\n    return img\n\n\ndef draw_grid(img: str, pixel_step: int) -> np.ndarray:\n    x = pixel_step\n    y = pixel_step\n\n    while x < img.shape[1]:\n        cv2.line(img, (x, 0), (x, img.shape[0]), color=(255, 255, 255))\n        x += pixel_step\n\n    while y < img.shape[0]:\n        cv2.line(img, (0, y), (img.shape[1], y), color=(255, 255, 255))\n        y += pixel_step\n\n    return img\n\n\ndef draw_text(img: str, texts: list, locations: list) -> np.ndarray:\n    for text, loc in zip(texts, locations):\n        cv2.putText(img, text, (int(loc[0]-(loc[2]/2)-5), int(loc[1]-(loc[3]/2)-5)), cv2.FONT_HERSHEY_COMPLEX,\n                    0.3, (255, 0, 0), 1)\n    return img\n\n\n################################ Functions for making an inference on an image using trained model ###########################\n\nPredBoundingBox = namedtuple(""PredBoundingBox"", [""probability"", ""class_id"",\n                                                 ""classname"", ""bounding_box""\n                                                 ])\n\ndef invert_transformation(bb_hat, anchors):\n    """"""\n    Invert the transform from ""loc_transformation"".\n    """"""\n\n    return torch.stack([anchors[:, 0] + bb_hat[:, 0] * anchors[:, 2],\n                        anchors[:, 1] + bb_hat[:, 1] * anchors[:, 3],\n                        anchors[:, 2] * torch.exp(bb_hat[:, 2]),\n                        anchors[:, 3] * torch.exp(bb_hat[:, 3])\n                        ], dim=1)\n\ndef non_max_suppression(bounding_boxes: list, iou_threshold: float = 0.1) -> list:\n    filtered_bb = []\n\n    while len(bounding_boxes) != 0:\n        best_bb = bounding_boxes.pop(0)\n        filtered_bb.append(best_bb)\n\n        remove_items = []\n        for bb in bounding_boxes:\n            iou = jaccard(torch.tensor(best_bb.bounding_box).unsqueeze(0), \n                          torch.tensor(bb.bounding_box).unsqueeze(0))\n\n            if iou > iou_threshold:\n                remove_items.append(bb)\n        bounding_boxes = [bb for bb in bounding_boxes if bb not in remove_items]\n    return filtered_bb\n\n\ndef infer(net, epoch, background_threshold=0.9, device = ""cuda:0""):\n    \n    img = np.array(Image.open(\'../img/pikachu.jpg\').convert(\'RGB\').resize((256, 256), Image.BILINEAR))\n    X = transforms.Compose([transforms.ToTensor()])(img).to(device)\n    \n    X = X.to(device)\n    \n    # background_threshold = 0.9\n\n    net.eval()\n    anchors, class_hat, bb_hat = net(X.unsqueeze(0))\n\n    anchors = anchors.to(device)\n\n\n    bb_hat = bb_hat.reshape((1, -1, 4))\n\n    bb_hat = invert_transformation(bb_hat.squeeze(0), anchors)\n    bb_hat = bb_hat * 256.0\n\n    class_hat = class_hat.sigmoid().squeeze(0)\n\n    bb_hat = bb_hat[class_hat[:,0] < background_threshold, :]\n\n\n    bb_hat = bb_hat.detach().cpu().numpy()\n    class_hat = class_hat[class_hat[:,0] < background_threshold, :]\n\n    class_preds = class_hat[:, 1:]\n\n    prob, class_id = torch.max(class_preds,1)\n\n    prob = prob.detach().cpu().numpy()\n    class_id = class_id.detach().cpu().numpy()\n\n\n    id_cat_pikachu = dict()\n    id_cat_pikachu[0] = \'pikachu\'\n    \n    output_bb = [PredBoundingBox(probability=prob[i],\n                                 class_id=class_id[i],\n                                 classname=id_cat_pikachu[class_id[i]],\n                                 bounding_box=[bb_hat[i, 0], \n                                               bb_hat[i, 1], \n                                               bb_hat[i, 2], \n                                               bb_hat[i, 3]])\n                                 for i in range(0, len(prob))]\n\n    output_bb = sorted(output_bb, key = lambda x: x.probability, reverse=True)\n\n    filtered_bb = non_max_suppression(output_bb)\n\n    img_str = \'../img/pikachu.jpg\'\n    img = read_img(img_str, 256)\n\n    # img = (X.cpu().numpy().transpose(1,2,0)*255)\n    # img1 = np.ascontiguousarray(img, dtype=np.uint8)\n\n    img = draw_boxes(img, [bb.bounding_box for bb in filtered_bb])\n    img = draw_text(img, [bb.classname for bb in filtered_bb], [bb.bounding_box for bb in filtered_bb])\n    plt.imsave(\'ssd_outputs/img_\' + str(epoch) + \'.png\', img)\n\n\n########################################## Functions for downloading and preprocessing data ##############################################\n\n\ndef gen_bar_updater():\n    pbar = tqdm(total=None)\n\n    def bar_update(count, block_size, total_size):\n        if pbar.total is None and total_size:\n            pbar.total = total_size\n        progress_bytes = count * block_size\n        pbar.update(progress_bytes - pbar.n)\n\n    return bar_update\n\ndef calculate_md5(fpath, chunk_size=1024 * 1024):\n    md5 = hashlib.md5()\n    with open(fpath, \'rb\') as f:\n        for chunk in iter(lambda: f.read(chunk_size), b\'\'):\n            md5.update(chunk)\n    return md5.hexdigest()\n\n\ndef check_md5(fpath, md5, **kwargs):\n    return md5 == calculate_md5(fpath, **kwargs)\n\n\ndef check_integrity(fpath, md5=None):\n    if not os.path.isfile(fpath):\n        return False\n    if md5 is None:\n        return True\n    return check_md5(fpath, md5)\n\ndef makedir_exist_ok(dirpath):\n    """"""\n    Python2 support for os.makedirs(.., exist_ok=True)\n    """"""\n    try:\n        os.makedirs(dirpath)\n    except OSError as e:\n        if e.errno == errno.EEXIST:\n            pass\n        else:\n            raise\ndef download_url(url, root, filename=None, md5=None):\n    """"""Download a file from a url and place it in root.\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    """"""\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir_exist_ok(root)\n\n    # downloads file\n    if check_integrity(fpath, md5):\n        print(\'Using downloaded and verified file: \' + fpath)\n    else:\n        try:\n            print(\'Downloading \' + url + \' to \' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except (urllib.error.URLError, IOError) as e:\n            if url[:5] == \'https\':\n                url = url.replace(\'https:\', \'http:\')\n                print(\'Failed download. Trying https -> http instead.\'\n                      \' Downloading \' + url + \' to \' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )\n            else:\n                raise e\n\n# The following code is used to download pikachu dataset from mxnet aws server in the form of .rec files\n# Then this .rec file is converted into png images and json files for annotation data\n# This part requires \'mxnet\' library which can be downloaded using conda \n# using the command \'conda install mxnet\'\n# Matplotlib is also required for saving the png image files\n\n# Download Pikachu Dataset\ndef download_pikachu(data_dir):\n    root_url = (\'https://apache-mxnet.s3-accelerate.amazonaws.com/\'\n                \'gluon/dataset/pikachu/\')\n    dataset = {\'train.rec\': \'e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8\',\n               \'train.idx\': \'dcf7318b2602c06428b9988470c731621716c393\',\n               \'val.rec\': \'d6c33f799b4d058e82f2cb5bd9a976f69d72d520\'}\n    for k, v in dataset.items():\n        download_url(root_url + k, data_dir)\n\n\n# Create dataloaders in mxnet\ndef load_data_pikachu_rec_mxnet(batch_size, edge_size=256):\n    from mxnet import image\n    """"""Load the pikachu dataset""""""\n    data_dir = \'../data/pikachu\'\n    download_pikachu(data_dir)\n    train_iter = image.ImageDetIter(\n        path_imgrec=os.path.join(data_dir, \'train.rec\'),\n        path_imgidx=os.path.join(data_dir, \'train.idx\'),\n        batch_size=batch_size,\n        data_shape=(3, edge_size, edge_size),  # The shape of the output image\n        # shuffle=True,  # Read the data set in random order\n        # rand_crop=1,  # The probability of random cropping is 1\n        min_object_covered=0.95, max_attempts=200)\n    val_iter = image.ImageDetIter(\n        path_imgrec=os.path.join(data_dir, \'val.rec\'), batch_size=batch_size,\n        data_shape=(3, edge_size, edge_size), shuffle=False)\n    return train_iter, val_iter\n\n\n# Use mxnet dataloaders to convert .rec file to .png images and annotations.json\n\ndef download_and_preprocess_data(dir = \'../data/pikachu/\'):\n    \n    if os.path.exists(os.path.join(dir, \'train\')) and os.path.exists(os.path.join(dir, \'val\')):\n        return\n\n    train_iter, val_iter = load_data_pikachu_rec_mxnet(batch_size)\n    \n    os.mkdir(os.path.join(dir, \'train\'))\n    os.mkdir(os.path.join(dir, \'val\'))\n    os.mkdir(os.path.join(dir, \'train/images\'))\n    os.mkdir(os.path.join(dir, \'val/images\'))\n    \n    annotations_train = dict()\n    train_iter.reset()  # Read data from the start.\n    id = 0\n    for batch in train_iter:\n        id+=1\n        \n        X = batch.data[0].as_in_context(ctx)\n    \n        Y = batch.label[0].as_in_context(ctx)\n\n        x = X.asnumpy()\n        x = x.transpose((2,3,1,0))\n        x = x.squeeze(axis=-1)\n        plt.imsave(os.path.join(dir, \'train/images\', \'pikachu_\' + str(id) + \'.png\'), x/255.)\n        an = dict()\n        y = Y.asnumpy()\n    \n        an[\'class\'] = y[0, 0][0].tolist()\n        an[\'loc\'] = y[0,0][1:].tolist()\n        an[\'id\'] = [id]\n        an[\'image\'] = \'pikachu_\' + str(id) + \'.png\'\n        annotations_train[\'data_\' + str(id)] = an \n\n    import json\n    with open(os.path.join(dir, \'train\', \'annotations.json\'), \'w\') as outfile:\n        json.dump(annotations_train, outfile)\n    outfile.close()\n\n\n    annotations_val = dict()\n    val_iter.reset()  # Read data from the start.\n    id = 0\n    for batch in val_iter:\n        id+=1\n        \n        X = batch.data[0].as_in_context(ctx)\n    \n        Y = batch.label[0].as_in_context(ctx)\n\n        x = X.asnumpy()\n        x = x.transpose((2,3,1,0))\n        x = x.squeeze(axis=-1)\n        plt.imsave(os.path.join(dir, \'val/images\', \'pikachu_\' + str(id) + \'.png\'), x/255.)\n        an = dict()\n        y = Y.asnumpy()\n    \n        an[\'class\'] = y[0, 0][0].tolist()\n        an[\'loc\'] = y[0,0][1:].tolist()\n        an[\'id\'] = [id]\n        an[\'image\'] = \'pikachu_\' + str(id) + \'.png\'\n        annotations_val[\'data_\' + str(id)] = an \n\n    import json\n    with open(os.path.join(dir, \'val\', \'annotations.json\'), \'w\') as outfile:\n        json.dump(annotations_val, outfile)\n    outfile.close()\n\n\n################################################## PyTorch Dataloader for PIKACHU dataset ########################################################\n\nclass PIKACHU(torch.utils.data.Dataset):\n    def __init__(self, data_dir, set, transform=None, target_transform=None):\n        \n        self.image_size = (3, 256, 256)\n        self.images_dir = os.path.join(data_dir, set, \'images\')\n\n        self.set = set\n        self.transform = transforms.Compose([\n            transforms.ToTensor()])\n        self.target_transform = target_transform\n\n        annotations_file = os.path.join(data_dir, set, \'annotations.json\')\n        with open(annotations_file) as file:\n            self.annotations = json.load(file)\n\n    def __getitem__(self, index):\n        \n        annotations_i = self.annotations[\'data_\' + str(index+1)]\n        \n        image_path = os.path.join(self.images_dir, annotations_i[\'image\'])\n        img = np.array(Image.open(image_path).convert(\'RGB\').resize((self.image_size[2], self.image_size[1]), Image.BILINEAR))\n        # print(img.shape)\n        loc = np.array(annotations_i[\'loc\'])\n        \n        loc_chw = np.zeros((4,))\n        loc_chw[0] = (loc[0] + loc[2])/2\n        loc_chw[1] = (loc[1] + loc[3])/2\n        loc_chw[2] = (loc[2] - loc[0])  #width\n        loc_chw[3] = (loc[3] - loc[1])  # height\n        \n\n        label = 1 - annotations_i[\'class\']\n    \n        if self.transform is not None:\n            img = self.transform(img)       \n        return (img, loc_chw, label)\n\n    def __len__(self):\n        return len(self.annotations)\n\n###################################################### Functions for showing graph of loss function ##############################################\n\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    """"""A utility function to set matplotlib axes""""""\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend: axes.legend(legend)\n    axes.grid()\n\nclass Animator(object):\n    def __init__(self, xlabel=None, ylabel=None, legend=[], xlim=None,\n                 ylim=None, xscale=\'linear\', yscale=\'linear\', fmts=None,\n                 nrows=1, ncols=1, figsize=(3.5, 2.5)):\n        """"""Incrementally plot multiple lines.""""""\n        d2l.use_svg_display()\n        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1: self.axes = [self.axes,]\n        # use a lambda to capture arguments\n        self.config_axes = lambda : d2l.set_axes(\n            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        """"""Add multiple data points into the figure.""""""\n        if not hasattr(y, ""__len__""): y = [y]\n        n = len(y)\n        if not hasattr(x, ""__len__""): x = [x] * n\n        if not self.X: self.X = [[] for _ in range(n)]\n        if not self.Y: self.Y = [[] for _ in range(n)]\n        if not self.fmts: self.fmts = [\'-\'] * n\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\n\n################################## Functions for finding the overlap between anchors and predicted bounding boxes using IoU #######################\n\ndef center_2_hw(box: torch.Tensor) -> float:\n    """"""\n    Converting (cx, cy, w, h) to (x1, y1, x2, y2)\n    """"""\n\n    return torch.cat(\n        [box[:, 0, None] - box[:, 2, None]/2,\n         box[:, 1, None] - box[:, 3, None]/2,\n         box[:, 0, None] + box[:, 2, None]/2,\n         box[:, 1, None] + box[:, 3, None]/2\n         ], dim=1)\n\ndef intersect(box_a: torch.Tensor, box_b: torch.Tensor) -> float:\n    # Coverting (cx, cy, w, h) to (x1, y1, x2, y2) since its easier to extract min/max coordinates\n    temp_box_a, temp_box_b = center_2_hw(box_a), center_2_hw(box_b)\n    \n#     print(temp_box_a.shape)\n    \n    max_xy = torch.min(temp_box_a[:, None, 2:], temp_box_b[None, :, 2:])\n    min_xy = torch.max(temp_box_a[:, None, :2], temp_box_b[None, :, :2])\n    \n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\ndef box_area(box: torch.Tensor) -> float:\n    return box[:, 2] * box[:, 3]\n\ndef jaccard(box_a: torch.Tensor, box_b: torch.Tensor) -> float:\n#     print(box_a.shape)\n    intersection = intersect(box_a, box_b)\n    union = box_area(box_a).unsqueeze(1) + box_area(box_b).unsqueeze(0) - intersection\n    return intersection / union\n\ndef find_overlap(bb_true_i, anchors, jaccard_overlap):\n\n    jaccard_tensor = jaccard(anchors, bb_true_i)\n    _, max_overlap = torch.max(jaccard_tensor, dim=0)\n    \n    overlap_list = []    \n    for i in range(len(bb_true_i)):\n        threshold_overlap = (jaccard_tensor[:, i] > jaccard_overlap).nonzero()\n\n        if len(threshold_overlap) > 0:\n            threshold_overlap = threshold_overlap[:, 0]\n            overlap = torch.cat([max_overlap[i].view(1), threshold_overlap])\n            overlap = torch.unique(overlap)     \n        else:\n            overlap = max_overlap[i].view(1)\n        overlap_list.append(overlap)\n    return overlap_list\n\n\n#################################### Functions for saving and loading trained models #############################################\n\ndef save(model, path_to_checkpoints_dir, step, optimizer, loss):\n    \n    try:\n        os.makedirs(path_to_checkpoints_dir)\n    except:\n        pass\n\n    path_to_checkpoint = os.path.join(path_to_checkpoints_dir, f\'model-{step}_{loss}.pth\')\n    checkpoint = {\n        \'state_dict\': model.state_dict(),\n        \'step\': step,\n        \'optimizer_state_dict\': optimizer.state_dict()\n    }\n    torch.save(checkpoint, path_to_checkpoint)\n    return path_to_checkpoint\n\ndef load(model, path_to_checkpoint, optimizer):\n    checkpoint = torch.load(path_to_checkpoint)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    step = checkpoint[\'step\']\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\'optimizer_state_dict\'])\n    return step\n\n\n############################ Object Detection Related Functions ##############################\n\n\n\n\nimport itertools\nimport math\ndef MultiBoxPrior(feature_map_sizes, sizes, aspect_ratios):\n    """"""Compute default box sizes with scale and aspect transform.""""""\n    \n    sizes = [s*728 for s in sizes]\n    \n    scale = feature_map_sizes\n    steps_y = [1 / scale[0]]\n    steps_x = [1 / scale[1]]\n    \n    sizes = [s / max(scale) for s in sizes]\n    \n    num_layers = 1\n\n    boxes = []\n    for i in range(num_layers):\n        for h, w in itertools.product(range(feature_map_sizes[0]), range(feature_map_sizes[1])):\n            cx = (w + 0.5)*steps_x[i]\n            cy = (h + 0.5)*steps_y[i]\n            \n            for j in range(len(sizes)):\n\n                s = sizes[j]\n                boxes.append((cx, cy, s, s))\n\n            s = sizes[0]\n            \n            for ar in aspect_ratios:\n               \n                boxes.append((cx, cy, (s * math.sqrt(ar)), (s / math.sqrt(ar))))\n\n    return torch.Tensor(boxes) \n\ndef MultiBoxTarget(class_true, bb_true, anchors):\n    \n    class_true +=1\n    \n    class_target = torch.zeros(anchors.shape[0]).long()\n\n    overlap_list = d2l.find_overlap(bb_true, anchors, 0.5)\n    \n    overlap_coordinates = torch.zeros_like(anchors)\n    \n    for j in range(len(overlap_list)):\n        overlap = overlap_list[j]\n        class_target[overlap] = class_true[j, 0]\n        overlap_coordinates[overlap] = 1.\n        \n        \n    \n    new_anchors = torch.cat([*anchors])\n    overlap_coordinates = torch.cat([*overlap_coordinates])\n    new_anchors = new_anchors*overlap_coordinates\n    \n    return (new_anchors.unsqueeze(0), overlap_coordinates.unsqueeze(0), class_target.unsqueeze(0))\n\n\ndef MultiboxDetection(id_cat, cls_probs, anchors, nms_threshold):\n\n    id_new = dict()\n    id_new[0] = \'background\'\n    for i in (id_cat.keys()):\n        id_new[i+1] = id_cat[i]\n\n    cls_probs = cls_probs.transpose(0,1)\n\n    prob, class_id = torch.max(cls_probs,1)\n\n    prob = prob.detach().cpu().numpy()\n    class_id = class_id.detach().cpu().numpy()\n\n    output_bb = [d2l.PredBoundingBox(probability=prob[i],\n                                     class_id=class_id[i],\n                                     classname=id_new[class_id[i]],\n                                     bounding_box=[anchors[i, 0], \n                                                   anchors[i, 1], \n                                                   anchors[i, 2], \n                                                   anchors[i, 3]])\n                                     for i in range(0, len(prob))]\n\n    filtered_bb = d2l.non_max_suppression(output_bb, nms_threshold)\n    \n    out = []\n    for bb in filtered_bb:\n        out.append([bb.class_id-1, bb.probability, *bb.bounding_box])\n    out = torch.Tensor(out)\n    \n    return out\n\n############################ Functions for Multi-Scale Object Detection Jupyter Notebook ##########################\n\n# def bbox_to_rect(bbox, color):\n#     """"""Convert bounding box to matplotlib format.""""""\n#     return d2l.plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0],\n#                          height=bbox[3]-bbox[1], fill=False, edgecolor=color,\n#                          linewidth=2)\n\n# def show_bboxes(axes, bboxes, labels=None, colors=None):\n#     """"""Show bounding boxes.""""""\n#     def _make_list(obj, default_values=None):\n#         if obj is None:\n#             obj = default_values\n#         elif not isinstance(obj, (list, tuple)):\n#             obj = [obj]\n#         return obj\n#     labels = _make_list(labels)\n#     colors = _make_list(colors, [\'b\', \'g\', \'r\', \'m\', \'c\'])\n#     for i, bbox in enumerate(bboxes):\n#         color = colors[i % len(colors)]\n#         rect = bbox_to_rect(bbox.numpy(), color)\n#         axes.add_patch(rect)\n#         if labels and len(labels) > i:\n#             text_color = \'k\' if color == \'w\' else \'w\'\n#             axes.text(rect.xy[0], rect.xy[1], labels[i],\n#                       va=\'center\', ha=\'center\', fontsize=9, color=text_color,\n#                       bbox=dict(facecolor=color, lw=0))\n\ndef get_centers(h, w, fh, fw):\n    step_x = int(w/fw)\n    cx = []\n    for i in range(fw):\n        cx.append((step_x*i + step_x*(i+1))/2)\n    \n    step_y = int(h/fh)\n    cy = []\n    for j in range(fh):\n        cy.append((step_y*j + step_y*(j+1))/2)\n    cxcy = []\n    for x in cx:\n        for y in cy:\n            cxcy.append([x, y])\n    \n    return np.array(cxcy).astype(np.int16)\n\n####################################################################################################################################\n'"
d2l/train.py,29,"b'import numpy as np\nimport math\nimport time\n\nfrom .base import try_gpu, Timer, Accumulator\nfrom .figure import set_figsize, plt, Animator\nfrom .data import data_iter_consecutive, data_iter_random\nfrom .model import linreg\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\n__all__ = [\'evaluate_loss\', \'train_ch10\', \'train_2d\',\'evaluate_accuracy\', \'squared_loss\', \'grad_clipping\', \'sgd\', \'train_and_predict_rnn\', \'train_ch3\', \'train_ch5\',\'MaskedSoftmaxCELoss\',\'train_ch7\', \'translate_ch7\', \'to_onehot\' , \'predict_rnn\', \'train_and_predict_rnn_nn\', \'predict_rnn_nn\', \'grad_clipping_nn\']\n\ndef evaluate_loss(net, data_iter, loss):\n    """"""Evaluate the loss of a model on the given dataset""""""\n    metric = Accumulator(2)  # sum_loss, num_examples\n    for X, y in data_iter:\n        metric.add(loss(net(X), y).sum().detach().numpy().item(), list(y.shape)[0])\n    return metric[0] / metric[1]\n\ndef evaluate_accuracy(data_iter, net, device=torch.device(\'cpu\')):\n    """"""Evaluate accuracy of a model on the given data set.""""""\n    net.eval()  # Switch to evaluation mode for Dropout, BatchNorm etc layers.\n    acc_sum, n = torch.tensor([0], dtype=torch.float32, device=device), 0\n    for X, y in data_iter:\n        # Copy the data to device.\n        X, y = X.to(device), y.to(device)\n        with torch.no_grad():\n            y = y.long()\n            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))\n            n += y.shape[0]\n    return acc_sum.item()/n\n\ndef squared_loss(y_hat, y):\n    """"""Squared loss.""""""\n    return (y_hat - y.view(y_hat.shape)).pow(2) / 2\n\ndef grad_clipping(params, theta, device):\n    """"""Clip the gradient.""""""\n    norm = torch.tensor([0], dtype=torch.float32, device=device)\n    for param in params:\n        norm += (param.grad ** 2).sum()\n    norm = norm.sqrt().item()\n    if norm > theta:\n        for param in params:\n            param.grad.data.mul_(theta / norm)\n\ndef grad_clipping_nn(model, theta, device):\n    """"""Clip the gradient for a nn model.""""""\n    grad_clipping(model.parameters(), theta, device)\n\ndef sgd(params, lr, batch_size):\n    """"""Mini-batch stochastic gradient descent.""""""\n    for param in params:\n        param.data.sub_(lr*param.grad/batch_size)\n        param.grad.data.zero_()\n\ndef train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n                          corpus_indices, vocab, device, is_random_iter,\n                          num_epochs, num_steps, lr, clipping_theta,\n                          batch_size, prefixes):\n    """"""Train an RNN model and predict the next item in the sequence.""""""\n    if is_random_iter:\n        data_iter_fn = data_iter_random\n    else:\n        data_iter_fn = data_iter_consecutive\n    params = get_params()\n    loss =  nn.CrossEntropyLoss()\n    start = time.time()\n    for epoch in range(num_epochs):\n        if not is_random_iter:\n            # If adjacent sampling is used, the hidden state is initialized\n            # at the beginning of the epoch\n            state = init_rnn_state(batch_size, num_hiddens, device)\n        l_sum, n = 0.0, 0\n        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n        for X, Y in data_iter:\n            if is_random_iter:\n                # If random sampling is used, the hidden state is initialized\n                # before each mini-batch update\n                state = init_rnn_state(batch_size, num_hiddens, device)\n            else:\n                # Otherwise, the detach function needs to be used to separate\n                # the hidden state from the computational graph to avoid\n                # backpropagation beyond the current sample\n                for s in state:\n                    s.detach_()\n            inputs = to_onehot(X, len(vocab))\n            # outputs is num_steps terms of shape (batch_size, len(vocab))\n            (outputs, state) = rnn(inputs, state, params)\n            # After stitching it is (num_steps * batch_size, len(vocab))\n            outputs = torch.cat(outputs, dim=0)\n            # The shape of Y is (batch_size, num_steps), and then becomes\n            # a vector with a length of batch * num_steps after\n            # transposition. This gives it a one-to-one correspondence\n            # with output rows\n            y = Y.t().reshape((-1,))\n            # Average classification error via cross entropy loss\n            l = loss(outputs, y.long()).mean()\n            l.backward()\n            with torch.no_grad():\n                grad_clipping(params, clipping_theta, device)  # Clip the gradient\n                sgd(params, lr, 1)\n            # Since the error is the mean, no need to average gradients here\n            l_sum += l.item() * y.numel()\n            n += y.numel()\n        if (epoch + 1) % 50 == 0:\n            print(\'epoch %d, perplexity %f, time %.2f sec\' % (\n                epoch + 1, math.exp(l_sum / n), time.time() - start))\n            start = time.time()\n        if (epoch + 1) % 100 == 0:\n            for prefix in prefixes:\n                print(\' -\',  predict_rnn(prefix, 50, rnn, params,\n                                         init_rnn_state, num_hiddens,\n                                         vocab, device))\n\ndef train_ch3(net, train_iter, test_iter, criterion, num_epochs, batch_size, lr=None):\n    """"""Train and evaluate a model with CPU.""""""\n    optimizer = optim.SGD(net.parameters(), lr=lr)\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n        for X, y in train_iter:\n            optimizer.zero_grad()\n\n            y_hat = net(X)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n\n            y = y.type(torch.float32)\n            train_l_sum += loss.item()\n            train_acc_sum += torch.sum((torch.argmax(y_hat, dim=1).type(torch.FloatTensor) == y).detach()).float()\n            n += list(y.size())[0]\n        test_acc = evaluate_accuracy(test_iter, net)\n        print(\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f\'\\\n            % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\n\ndef train_ch5(net, train_iter, test_iter, criterion, num_epochs, batch_size, device, lr=None):\n    """"""Train and evaluate a model with CPU or GPU.""""""\n    print(\'training on\', device)\n    net.to(device)\n    optimizer = optim.SGD(net.parameters(), lr=lr)\n    for epoch in range(num_epochs):\n        net.train() # Switch to training mode\n        n, start = 0, time.time()\n        train_l_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n        train_acc_sum = torch.tensor([0.0], dtype=torch.float32, device=device)\n        for X, y in train_iter:\n            optimizer.zero_grad()\n            X, y = X.to(device), y.to(device) \n            y_hat = net(X)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n            with torch.no_grad():\n                y = y.long()\n                train_l_sum += loss.float()\n                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n                n += y.shape[0]\n\n        test_acc = evaluate_accuracy(test_iter, net, device) \n        print(\'epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec\'\\\n            % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc, time.time() - start))\n\nclass MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    def forward(self, pred, label, valid_length):\n        # the sample weights shape should be (batch_size, seq_len)\n        weights = torch.ones_like(label)\n        weights = SequenceMask(weights, valid_length).float()\n        self.reduction=\'none\'\n        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)\n        return (output*weights).mean(dim=1)\n\n\ndef train_ch7(model, data_iter, lr, num_epochs, device): \n    """"""Train an encoder-decoder model""""""\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    loss = MaskedSoftmaxCELoss()\n    tic = time.time()\n    for epoch in range(1, num_epochs+1):\n        l_sum, num_tokens_sum = 0.0, 0.0\n        for batch in data_iter:\n            optimizer.zero_grad()\n            X, X_vlen, Y, Y_vlen = [x.to(device) for x in batch]\n            Y_input, Y_label, Y_vlen = Y[:,:-1], Y[:,1:], Y_vlen-1\n            Y_hat, _ = model(X, Y_input, X_vlen, Y_vlen)\n            l = loss(Y_hat, Y_label, Y_vlen).sum()\n            l.backward()\n            with torch.no_grad():\n                grad_clipping_nn(model, 5, device)\n            num_tokens = Y_vlen.sum().item()\n            optimizer.step()\n            l_sum += l.sum().item()\n            num_tokens_sum += num_tokens\n        if epoch % 50 == 0:\n            print(""epoch {0:4d},loss {1:.3f}, time {2:.1f} sec"".format( \n                  epoch, (l_sum/num_tokens_sum), time.time()-tic))\n            tic = time.time()\n\ndef translate_ch7(model, src_sentence, src_vocab, tgt_vocab, max_len, device):\n    """"""Translate based on an encoder-decoder model with greedy search.""""""\n    src_tokens = src_vocab[src_sentence.lower().split(\' \')]\n    src_len = len(src_tokens)\n    if src_len < max_len:\n        src_tokens += [src_vocab.pad] * (max_len - src_len)\n    enc_X = torch.tensor(src_tokens, device=device)\n    enc_valid_length = torch.tensor([src_len], device=device)\n    # use expand_dim to add the batch_size dimension.\n    enc_outputs = model.encoder(enc_X.unsqueeze(dim=0), enc_valid_length)\n    dec_state = model.decoder.init_state(enc_outputs, enc_valid_length)\n    dec_X = torch.tensor([tgt_vocab.bos], device=device).unsqueeze(dim=0)\n    predict_tokens = []\n    for _ in range(max_len):\n        Y, dec_state = model.decoder(dec_X, dec_state)\n        # The token with highest score is used as the next time step input.\n        dec_X = Y.argmax(dim=2)\n        py = dec_X.squeeze(dim=0).int().item()\n        if py == tgt_vocab.eos:\n            break\n        predict_tokens.append(py)\n    return \' \'.join(tgt_vocab.to_tokens(predict_tokens))\n\n\ndef to_onehot(X,size):\n    return F.one_hot(X.long().transpose(0,-1), size)\n\ndef predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n                num_hiddens, vocab, device):\n    """"""Predict next chars with an RNN model""""""\n    state = init_rnn_state(1, num_hiddens, device)\n    output = [vocab[prefix[0]]]\n    for t in range(num_chars + len(prefix) - 1):\n        # The output of the previous time step is taken as the input of the\n        # current time step.\n        X = to_onehot(torch.tensor([output[-1]], dtype=torch.float32, device=device), len(vocab))\n        # Calculate the output and update the hidden state\n        (Y, state) = rnn(X, state, params)\n        # The input to the next time step is the character in the prefix or\n        # the current best predicted character\n        if t < len(prefix) - 1:\n            # Read off from the given sequence of characters\n            output.append(vocab[prefix[t + 1]])\n        else:\n            # This is maximum likelihood decoding. Modify this if you want\n            # use sampling, beam search or beam sampling for better sequences.\n            output.append(int(Y[0].argmax(dim=1).item()))\n    return \'\'.join([vocab.idx_to_token[i] for i in output])\n\ndef predict_rnn_nn(prefix, num_chars, batch_size, num_hiddens, num_layers, model, vocab, device):\n    """"""Predict next chars with a RNN model.""""""\n    # Use the model\'s member function to initialize the hidden state\n    state = model.begin_state(num_hiddens=num_hiddens, device=device, num_layers=num_layers)\n    output = [vocab[prefix[0]]]\n    for t in range(num_chars + len(prefix) - 1):\n        X = torch.tensor([output[-1]], dtype=torch.float32, device=device).reshape((1, 1))\n        # Forward computation does not require incoming model parameters\n        (Y, state) = model(X, state)\n        if t < len(prefix) - 1:\n            output.append(vocab[prefix[t + 1]])\n        else:\n            output.append(int(Y.argmax(dim=1).item()))\n    return \'\'.join([vocab.idx_to_token[i] for i in output])\n\ndef train_and_predict_rnn_nn(model, num_hiddens, init_gru_state, corpus_indices, vocab,\n                                device, num_epochs, num_steps, lr,\n                                clipping_theta, batch_size, prefixes, num_layers=1):\n    """"""Train a RNN model and predict the next item in the sequence.""""""\n    loss =  nn.CrossEntropyLoss()\n    optm = torch.optim.SGD(model.parameters(), lr=lr)\n    start = time.time()\n    for epoch in range(1, num_epochs+1):\n        l_sum, n = 0.0, 0\n        data_iter = data_iter_consecutive(\n            corpus_indices, batch_size, num_steps, device)\n        state = model.begin_state(batch_size=batch_size, num_hiddens=num_hiddens, device=device ,num_layers=num_layers)\n        for X, Y in data_iter:\n            for s in state:\n                s.detach()\n            X = X.to(dtype=torch.long)\n            (output, state) = model(X, state)\n            y = Y.t().reshape((-1,))\n            l = loss(output, y.long()).mean()\n            optm.zero_grad()\n            l.backward(retain_graph=True)\n            with torch.no_grad():\n                # Clip the gradient\n                grad_clipping_nn(model, clipping_theta, device)\n                # Since the error has already taken the mean, the gradient does\n                # not need to be averaged\n                optm.step()\n            l_sum += l.item() * y.numel()\n            n += y.numel()\n\n        if epoch % (num_epochs // 4) == 0:\n            print(\'epoch %d, perplexity %f, time %.2f sec\' % (\n                epoch, math.exp(l_sum / n), time.time() - start))\n            start = time.time()\n        if epoch % (num_epochs // 2) == 0:\n            for prefix in prefixes:\n                print(\' -\', predict_rnn_nn(prefix, 50, batch_size, num_hiddens, num_layers, model, vocab, device))\n\ndef train_2d(trainer):\n    """"""Optimize a 2-dim objective function with a customized trainer.""""""\n    # s1 and s2 are internal state variables and will \n    # be used later in the chapter\n    x1, x2, s1, s2 = -5, -2, 0, 0\n    results = [(x1, x2)]\n    for i in range(20):\n        x1, x2, s1, s2 = trainer(x1, x2, s1, s2)\n        results.append((x1, x2))\n    print(\'epoch %d, x1 %f, x2 %f\' % (i + 1, x1, x2))\n    return results\n\ndef train_ch10(trainer, hyperparams, data_iter, feature_dim, num_epochs=2):\n    # Initialization\n    w1 = np.random.normal(scale=0.01, size=(feature_dim, 1))\n    b1 = np.zeros(1)\n    w = Variable(torch.from_numpy(w1), requires_grad=True)\n    b = Variable(torch.from_numpy(b1), requires_grad=True)\n\n    if trainer.__name__ == \'SGD\':\n        optimizer = trainer([w, b], lr=hyperparams[\'lr\'], momentum=hyperparams[\'momentum\'])\n    elif trainer.__name__ == \'RMSprop\':\n        optimizer = trainer([w, b], lr=hyperparams[\'lr\'], alpha=hyperparams[\'gamma\'])\n\n    net, loss = lambda X: linreg(X, w, b), squared_loss\n    # Train\n    animator = Animator(xlabel=\'epoch\', ylabel=\'loss\',\n                            xlim=[0, num_epochs], ylim=[0.22, 0.35])\n    n, timer = 0, Timer()\n\n    for _ in range(num_epochs):\n        for X, y in data_iter:\n            X, y = Variable(X), Variable(y)\n            optimizer.zero_grad()\n            output = net(X)\n            l = loss(output, y).mean()\n            l.backward()\n            optimizer.step()\n            n += X.shape[0]\n            if n % 200 == 0:\n                timer.stop()\n                animator.add(n/X.shape[0]/len(data_iter),\n                             evaluate_loss(net, data_iter, loss))\n                timer.start()\n    print(\'loss: %.3f, %.3f sec/epoch\'%(animator.Y[0][-1], timer.avg()))\n    # return timer.cumsum(), animator.Y[0]'"
d2l/data/__init__.py,0,"b'""""""The data module contains functions/classes to load and (pre)process data sets""""""\n\nfrom .fashion_mnist import *\nfrom .base import *\nfrom .nmt import *\n'"
d2l/data/base.py,4,"b'import random\nimport collections\nimport numpy as np\nimport zipfile\nimport torch\nimport os\n\nclass Vocab(object): # This class is saved in d2l.\n  def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n    # sort by frequency and token\n    counter = collections.Counter(tokens)\n    token_freqs = sorted(counter.items(), key=lambda x: x[0])\n    token_freqs.sort(key=lambda x: x[1], reverse=True)\n    if use_special_tokens:\n      # padding, begin of sentence, end of sentence, unknown\n      self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n      tokens = [\'<pad>\', \'<bos>\', \'<eos>\', \'<unk>\']\n    else:\n      self.unk = 0\n      tokens = [\'<unk>\']\n    tokens += [token for token, freq in token_freqs if freq >= min_freq]\n    self.idx_to_token = []\n    self.token_to_idx = dict()\n    for token in tokens:\n      self.idx_to_token.append(token)\n      self.token_to_idx[token] = len(self.idx_to_token) - 1\n      \n  def __len__(self):\n    return len(self.idx_to_token)\n  \n  def __getitem__(self, tokens):\n    if not isinstance(tokens, (list, tuple)):\n      return self.token_to_idx.get(tokens, self.unk)\n    else:\n      return [self.__getitem__(token) for token in tokens]\n    \n  def to_tokens(self, indices):\n    if not isinstance(indices, (list, tuple)):\n      return self.idx_to_token[indices]\n    else:\n      return [self.idx_to_token[index] for index in indices]\n\n\ndef data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n    # Offset for the iterator over the data for uniform starts\n    offset = int(random.uniform(0,num_steps))\n    # Slice out data - ignore num_steps and just wrap around\n    num_indices = ((len(corpus_indices) - offset) // batch_size) * batch_size\n    indices = torch.tensor(corpus_indices[offset:(offset + num_indices)], dtype=torch.float32, device=ctx)\n    indices = indices.reshape((batch_size,-1))\n    # Need to leave one last token since targets are shifted by 1\n    num_epochs = ((num_indices // batch_size) - 1) // num_steps\n\n    for i in range(0, num_epochs * num_steps, num_steps):\n        X = indices[:,i:(i+num_steps)]\n        Y = indices[:,(i+1):(i+1+num_steps)]\n        yield X, Y\n\ndef data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n    # Offset for the iterator over the data for uniform starts\n    offset = int(random.uniform(0,num_steps))\n    corpus_indices = corpus_indices[offset:]\n    # Subtract 1 extra since we need to account for the sequence length\n    num_examples = ((len(corpus_indices) - 1) // num_steps) - 1\n    # Discard half empty batches\n    num_batches = num_examples // batch_size\n    example_indices = list(range(0, num_examples * num_steps, num_steps))\n    random.shuffle(example_indices)\n\n    # This returns a sequence of the length num_steps starting from pos\n    def _data(pos):\n        return corpus_indices[pos: pos + num_steps]\n\n    for i in range(0, batch_size * num_batches, batch_size):\n        # Batch_size indicates the random examples read each time\n        batch_indices = example_indices[i:(i+batch_size)]\n        X = [_data(j) for j in batch_indices]\n        Y = [_data(j + 1) for j in batch_indices]\n        yield torch.Tensor(X,  device=ctx), torch.Tensor(Y,  device=ctx)\n\n\ndef load_data_time_machine(num_examples=10000):\n    """"""Load the time machine data set (available in the English book).""""""\n    with open(\'../data/timemachine.txt\') as f:\n        raw_text = f.read()\n    lines = raw_text.split(\'\\n\')\n    text = \' \'.join(\' \'.join(lines).lower().split())[:num_examples]\n    vocab = Vocab(text)\n    corpus_indices = [vocab[char] for char in text]\n    return corpus_indices, vocab\n\ndef load_array(dataArray, labelArray, batch_size, is_train=True):\n    """""" Constructs a pytorch dataloader""""""\n    dataset = torch.utils.data.TensorDataset(torch.from_numpy(dataArray), torch.from_numpy(labelArray))\n    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n\ndef get_data_ch10(batch_size=10, n=1500):\n    data = np.genfromtxt(\'../data/airfoil_self_noise.dat\', delimiter=\'\\t\')\n    data = np.array((data - data.mean(axis=0)) / data.std(axis=0))\n    data_iter = load_array(data[:n, :-1], data[:n, -1],\n                               batch_size, is_train=True)\n    return data_iter, data.shape[1]-1\n\n'"
d2l/data/fashion_mnist.py,1,"b'import os\nimport sys\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom ..figure import plt, use_svg_display\n\n\ndef load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n        \'~\', \'.pytorch\', \'datasets\', \'fashion-mnist\')):\n    """"""Download the Fashion-MNIST dataset and then load into memory.""""""\n    root = os.path.expanduser(root)\n    transformer = []\n    if resize:\n        transformer += [transforms.Resize(resize)]\n    transformer += [transforms.ToTensor()]\n    transformer = transforms.Compose(transformer)\n\n    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, transform=transformer, download=True)\n    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, transform=transformer, download=True)\n    num_workers = 0 if sys.platform.startswith(\'win32\') else 4\n\n    train_iter = DataLoader(mnist_train, batch_size, shuffle=True, num_workers=num_workers)\n    test_iter = DataLoader(mnist_test, batch_size, shuffle=False, num_workers=num_workers)\n    return train_iter, test_iter\n\ndef get_fashion_mnist_labels(labels):\n    """"""Get text labels for Fashion-MNIST.""""""\n    text_labels = [\'t-shirt\', \'trouser\', \'pullover\', \'dress\', \'coat\',\n                   \'sandal\', \'shirt\', \'sneaker\', \'bag\', \'ankle boot\']\n    return [text_labels[int(i)] for i in labels]\n\n\ndef show_fashion_mnist(images, labels):\n    """"""Plot Fashion-MNIST images with labels.""""""\n    use_svg_display()\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.reshape((28, 28)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)\n'"
d2l/data/nmt.py,2,"b'#import urllib2\nimport zipfile\nimport torch\nimport requests\nfrom io import BytesIO\nfrom torch.utils import data\nfrom .base import Vocab\n\n__all__  = [\'load_data_nmt\']\n\ndef load_data_nmt(batch_size, max_len, num_examples=1000):\n    """"""Download an NMT dataset, return its vocabulary and data iterator.""""""\n    # Download and preprocess\n    def preprocess_raw(text):\n        text = text.replace(\'\\u202f\', \' \').replace(\'\\xa0\', \' \')\n        out = \'\'\n        for i, char in enumerate(text.lower()):\n            if char in (\',\', \'!\', \'.\') and text[i-1] != \' \':\n                out += \' \'\n            out += char\n        return out \n\n    url = \'http://www.manythings.org/anki/fra-eng.zip\'\n    print(""Downloading fra-eng.zip from \'{0}\'"".format(url))\n\n\n    headers={""User-Agent"": ""XY""}#dummy user agent \n    response = requests.get(url,headers=headers ,stream=True)\n    handle = BytesIO()\n\n    for chunk in response.iter_content(chunk_size=512):\n        if chunk:  # filter out keep-alive new chunks\n            handle.write(chunk)\n\n\n    with zipfile.ZipFile(handle, \'r\') as f:\n        raw_text = f.read(\'fra.txt\').decode(""utf-8"")\n\n    handle.close()\n\n    text = preprocess_raw(raw_text)\n\n    # Tokenize\n    source, target = [], []\n    for i, line in enumerate(text.split(\'\\n\')):\n        if i >= num_examples:\n            break\n        parts = line.split(\'\\t\')\n        if len(parts) == 2:\n            source.append(parts[0].split(\' \'))\n            target.append(parts[1].split(\' \'))\n\n    # Build vocab\n    def build_vocab(tokens):\n        tokens = [token for line in tokens for token in line]\n        return Vocab(tokens, min_freq=3, use_special_tokens=True)\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n\n    # Convert to index arrays\n    def pad(line, max_len, padding_token):\n        if len(line) > max_len:\n            return line[:max_len]\n        return line + [padding_token] * (max_len - len(line))\n\n    def build_array(lines, vocab, max_len, is_source):\n        lines = [vocab[line] for line in lines]\n        if not is_source:\n            lines = [[vocab.bos] + line + [vocab.eos] for line in lines]\n        array = torch.tensor([pad(line, max_len, vocab.pad) for line in lines])\n        valid_len = (array != vocab.pad).sum(1)\n        return array, valid_len\n\n    src_vocab, tgt_vocab = build_vocab(source), build_vocab(target)\n    src_array, src_valid_len = build_array(source, src_vocab, max_len, True)\n    tgt_array, tgt_valid_len = build_array(target, tgt_vocab, max_len, False)\n    train_data = data.TensorDataset(src_array, src_valid_len, tgt_array, tgt_valid_len)\n    train_iter = data.DataLoader(train_data, batch_size, shuffle=True)\n    return src_vocab, tgt_vocab, train_iter\n'"
