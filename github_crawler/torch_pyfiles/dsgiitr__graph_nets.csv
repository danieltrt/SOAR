file_path,api_count,code
ChebNet/coarsening.py,0,"b'import numpy as np\nimport scipy.sparse\nimport sklearn.metrics\n\n\ndef laplacian(W, normalized=True):\n    """"""Return graph Laplacian""""""\n\n    # Degree matrix.\n    d = W.sum(axis=0)\n\n    # Laplacian matrix.\n    if not normalized:\n        D = scipy.sparse.diags(d.A.squeeze(), 0)\n        L = D - W\n    else:\n        d += np.spacing(np.array(0, W.dtype))\n        d = 1 / np.sqrt(d)\n        D = scipy.sparse.diags(d.A.squeeze(), 0)\n        I = scipy.sparse.identity(d.size, dtype=W.dtype)\n        L = I - D * W * D\n\n    assert np.abs(L - L.T).mean() < 1e-9\n    assert type(L) is scipy.sparse.csr.csr_matrix\n    return L\n\n    \n    \ndef rescale_L(L, lmax=2):\n    """"""Rescale Laplacian eigenvalues to [-1,1]""""""\n    M, M = L.shape\n    I = scipy.sparse.identity(M, format=\'csr\', dtype=L.dtype)\n    L /= lmax * 2\n    L -= I\n    return L \n\n\ndef lmax_L(L):\n    """"""Compute largest Laplacian eigenvalue""""""\n    return scipy.sparse.linalg.eigsh(L, k=1, which=\'LM\', return_eigenvectors=False)[0]\n\n\n# graph coarsening with Heavy Edge Matching\ndef coarsen(A, levels):\n    \n    graphs, parents = HEM(A, levels)\n    perms = compute_perm(parents)\n\n    laplacians = []\n    for i,A in enumerate(graphs):\n        M, M = A.shape\n            \n        if i < levels:\n            A = perm_adjacency(A, perms[i])\n\n        A = A.tocsr()\n        A.eliminate_zeros()\n        Mnew, Mnew = A.shape\n        print(\'Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges\'.format(i, Mnew, Mnew-M, A.nnz//2))\n\n        L = laplacian(A, normalized=True)\n        laplacians.append(L)\n        \n    return laplacians, perms[0] if len(perms) > 0 else None\n\n\ndef HEM(W, levels, rid=None):\n    """"""\n    Coarsen a graph multiple times using the Heavy Edge Matching (HEM).\n\n    Input\n    W: symmetric sparse weight (adjacency) matrix\n    levels: the number of coarsened graphs\n\n    Output\n    graph[0]: original graph of size N_1\n    graph[2]: coarser graph of size N_2 < N_1\n    graph[levels]: coarsest graph of Size N_levels < ... < N_2 < N_1\n    parents[i] is a vector of size N_i with entries ranging from 1 to N_{i+1}\n        which indicate the parents in the coarser graph[i+1]\n    nd_sz{i} is a vector of size N_i that contains the size of the supernode in the graph{i}\n\n    Note\n    if ""graph"" is a list of length k, then ""parents"" will be a list of length k-1\n    """"""\n\n    N, N = W.shape\n    \n    if rid is None:\n        rid = np.random.permutation(range(N))\n        \n    ss = np.array(W.sum(axis=0)).squeeze()\n    rid = np.argsort(ss)\n        \n        \n    parents = []\n    degree = W.sum(axis=0) - W.diagonal()\n    graphs = []\n    graphs.append(W)\n\n    print(\'Heavy Edge Matching coarsening with Xavier version\')\n\n    for _ in range(levels):\n\n        weights = degree            # graclus weights\n        weights = np.array(weights).squeeze()\n\n        # PAIR THE VERTICES AND CONSTRUCT THE ROOT VECTOR\n        idx_row, idx_col, val = scipy.sparse.find(W)\n        cc = idx_row\n        rr = idx_col\n        vv = val\n\n        if not (list(cc)==list(np.sort(cc))):\n            tmp=cc\n            cc=rr\n            rr=tmp\n\n        cluster_id = HEM_one_level(cc,rr,vv,rid,weights)\n        parents.append(cluster_id)\n\n        # COMPUTE THE EDGES WEIGHTS FOR THE NEW GRAPH\n        nrr = cluster_id[rr]\n        ncc = cluster_id[cc]\n        nvv = vv\n        Nnew = cluster_id.max() + 1\n        # CSR is more appropriate: row,val pairs appear multiple times\n        W = scipy.sparse.csr_matrix((nvv,(nrr,ncc)), shape=(Nnew,Nnew))\n        W.eliminate_zeros()\n        \n        # Add new graph to the list of all coarsened graphs\n        graphs.append(W)\n        N, N = W.shape\n\n        # COMPUTE THE DEGREE (OMIT OR NOT SELF LOOPS)\n        degree = W.sum(axis=0)\n\n        # CHOOSE THE ORDER IN WHICH VERTICES WILL BE VISTED AT THE NEXT PASS\n        ss = np.array(W.sum(axis=0)).squeeze()\n        rid = np.argsort(ss)\n\n    return graphs, parents\n\n\n# Coarsen a graph given by rr,cc,vv.  rr is assumed to be ordered\ndef HEM_one_level(rr,cc,vv,rid,weights):\n\n    nnz = rr.shape[0]\n    N = rr[nnz-1] + 1\n\n    marked = np.zeros(N, np.bool)\n    rowstart = np.zeros(N, np.int32)\n    rowlength = np.zeros(N, np.int32)\n    cluster_id = np.zeros(N, np.int32)\n\n    oldval = rr[0]\n    count = 0\n    clustercount = 0\n\n    for ii in range(nnz):\n        rowlength[count] = rowlength[count] + 1\n        if rr[ii] > oldval:\n            oldval = rr[ii]\n            rowstart[count+1] = ii\n            count = count + 1\n\n    for ii in range(N):\n        tid = rid[ii]\n        if not marked[tid]:\n            wmax = 0.0\n            rs = rowstart[tid]\n            marked[tid] = True\n            bestneighbor = -1\n            for jj in range(rowlength[tid]):\n                nid = cc[rs+jj]\n                if marked[nid]:\n                    tval = 0.0\n                else:\n                    \n                    # First approach\n                    if 2==1:\n                        tval = vv[rs+jj] * (1.0/weights[tid] + 1.0/weights[nid])\n                    \n                    # Second approach\n                    if 1==1:\n                        Wij = vv[rs+jj]\n                        Wii = vv[rowstart[tid]]\n                        Wjj = vv[rowstart[nid]]\n                        di = weights[tid]\n                        dj = weights[nid]\n                        tval = (2.*Wij + Wii + Wjj) * 1./(di+dj+1e-9)\n                    \n                if tval > wmax:\n                    wmax = tval\n                    bestneighbor = nid\n\n            cluster_id[tid] = clustercount\n\n            if bestneighbor > -1:\n                cluster_id[bestneighbor] = clustercount\n                marked[bestneighbor] = True\n\n            clustercount += 1\n\n    return cluster_id\n\n\ndef compute_perm(parents):\n    """"""\n    Return a list of indices to reorder the adjacency and data matrices so\n    that the union of two neighbors from layer to layer forms a binary tree.\n    """"""\n\n    # Order of last layer is random (chosen by the clustering algorithm).\n    indices = []\n    if len(parents) > 0:\n        M_last = max(parents[-1]) + 1\n        indices.append(list(range(M_last)))\n\n    for parent in parents[::-1]:\n\n        # Fake nodes go after real ones.\n        pool_singeltons = len(parent)\n\n        indices_layer = []\n        for i in indices[-1]:\n            indices_node = list(np.where(parent == i)[0])\n            assert 0 <= len(indices_node) <= 2\n\n            # Add a node to go with a singelton.\n            if len(indices_node) is 1:\n                indices_node.append(pool_singeltons)\n                pool_singeltons += 1\n\n            # Add two nodes as children of a singelton in the parent.\n            elif len(indices_node) is 0:\n                indices_node.append(pool_singeltons+0)\n                indices_node.append(pool_singeltons+1)\n                pool_singeltons += 2\n\n            indices_layer.extend(indices_node)\n        indices.append(indices_layer)\n\n    # Sanity checks.\n    for i,indices_layer in enumerate(indices):\n        M = M_last*2**i\n        # Reduction by 2 at each layer (binary tree).\n        assert len(indices[0] == M)\n        # The new ordering does not omit an indice.\n        assert sorted(indices_layer) == list(range(M))\n\n    return indices[::-1]\n\nassert (compute_perm([np.array([4,1,1,2,2,3,0,0,3]),np.array([2,1,0,1,0])])\n        == [[3,4,0,9,1,2,5,8,6,7,10,11],[2,4,1,3,0,5],[0,1,2]])\n\n\n\ndef perm_adjacency(A, indices):\n    """"""\n    Permute adjacency matrix, i.e. exchange node ids,\n    so that binary unions form the clustering tree.\n    """"""\n    if indices is None:\n        return A\n\n    M, M = A.shape\n    Mnew = len(indices)\n    A = A.tocoo()\n\n    # Add Mnew - M isolated vertices.\n    rows = scipy.sparse.coo_matrix((Mnew-M,    M), dtype=np.float32)\n    cols = scipy.sparse.coo_matrix((Mnew, Mnew-M), dtype=np.float32)\n    A = scipy.sparse.vstack([A, rows])\n    A = scipy.sparse.hstack([A, cols])\n\n    # Permute the rows and the columns.\n    perm = np.argsort(indices)\n    A.row = np.array(perm)[A.row]\n    A.col = np.array(perm)[A.col]\n\n    assert np.abs(A - A.T).mean() < 1e-8 # 1e-9\n    assert type(A) is scipy.sparse.coo.coo_matrix\n    return A\n\n\n\ndef perm_data(x, indices):\n    """"""\n    Permute data matrix, i.e. exchange node ids,\n    so that binary unions form the clustering tree.\n    """"""\n    if indices is None:\n        return x\n\n    N, M = x.shape\n    Mnew = len(indices)\n    assert Mnew >= M\n    xnew = np.empty((N, Mnew))\n    for i,j in enumerate(indices):\n        # Existing vertex, i.e. real data.\n        if j < M:\n            xnew[:,i] = x[:,j]\n        # Fake vertex because of singeltons.\n        # They will stay 0 so that max pooling chooses the singelton.\n        # Or -infty ?\n        else:\n            xnew[:,i] = np.zeros(N)\n    return xnew\n\n'"
ChebNet/grid_graph.py,0,"b'import sklearn\nimport sklearn.metrics\nimport scipy.sparse, scipy.sparse.linalg  # scipy.spatial.distance\nimport numpy as np\n\n\ndef grid_graph(grid_side,number_edges,metric):\n    """"""Generate graph of a grid""""""\n    z = grid(grid_side)\n    dist, idx = distance_sklearn_metrics(z, k=number_edges, metric=metric)\n    A = adjacency(dist, idx)\n    print(""nb edges: "",A.nnz)\n    return A\n\n\ndef grid(m, dtype=np.float32):\n    """"""Return coordinates of grid points""""""\n    M = m**2\n    x = np.linspace(0,1,m, dtype=dtype)\n    y = np.linspace(0,1,m, dtype=dtype)\n    xx, yy = np.meshgrid(x, y)\n    z = np.empty((M,2), dtype)\n    z[:,0] = xx.reshape(M)\n    z[:,1] = yy.reshape(M)\n    return z\n\n\ndef distance_sklearn_metrics(z, k=4, metric=\'euclidean\'):\n    """"""Compute pairwise distances""""""\n    d = sklearn.metrics.pairwise.pairwise_distances(z, metric=metric, n_jobs=1)\n    # k-NN\n    idx = np.argsort(d)[:,1:k+1]\n    d.sort()\n    d = d[:,1:k+1]\n    return d, idx\n\n\ndef adjacency(dist, idx):\n    """"""Return adjacency matrix of a kNN graph""""""\n    M, k = dist.shape\n    assert M, k == idx.shape\n    assert dist.min() >= 0\n    assert dist.max() <= 1\n\n    # Pairwise distances\n    sigma2 = np.mean(dist[:,-1])**2\n    dist = np.exp(- dist**2 / sigma2)\n\n    # Weight matrix\n    I = np.arange(0, M).repeat(k)\n    J = idx.reshape(M*k)\n    V = dist.reshape(M*k)\n    W = scipy.sparse.coo_matrix((V, (I, J)), shape=(M, M))\n\n    # No self-connections\n    W.setdiag(0)\n\n    # Undirected graph\n    bigger = W.T > W\n    W = W - W.multiply(bigger) + W.T.multiply(bigger)\n\n    assert W.nnz % 2 == 0\n    assert np.abs(W - W.T).mean() < 1e-10\n    assert type(W) is scipy.sparse.csr.csr_matrix\n    return W\n\n\n    \n    \n'"
DeepWalk/DeepWalk.py,17,"b'#### Imports ####\n\nimport torch\nimport torch.nn as nn\nimport random\n\n\nadj_list = [[1,2,3], [0,2,3], [0, 1, 3], [0, 1, 2], [5, 6], [4,6], [4, 5], [1, 3]]\nsize_vertex = len(adj_list)  # number of vertices\n\n#### Hyperparameters ####\n\nw  = 3            # window size\nd  = 2            # embedding size\ny  = 200          # walks per vertex\nt  = 6            # walk length \nlr = 0.025       # learning rate\n\nv=[0,1,2,3,4,5,6,7] #labels of available vertices\n\n\n#### Random Walk ####\n\ndef RandomWalk(node,t):\n    walk = [node]        # Walk starts from this node\n    \n    for i in range(t-1):\n        node = adj_list[node][random.randint(0,len(adj_list[node])-1)]\n        walk.append(node)\n\n    return walk\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.phi  = nn.Parameter(torch.rand((size_vertex, d), requires_grad=True))    \n        self.phi2 = nn.Parameter(torch.rand((d, size_vertex), requires_grad=True))\n        \n        \n    def forward(self, one_hot):\n        hidden = torch.matmul(one_hot, self.phi)\n        out    = torch.matmul(hidden, self.phi2)\n        return out\n\nmodel = Model()\n\n\ndef skip_gram(wvi,  w):\n    for j in range(len(wvi)):\n        for k in range(max(0,j-w) , min(j+w, len(wvi))):\n            \n            #generate one hot vector\n            one_hot          = torch.zeros(size_vertex)\n            one_hot[wvi[j]]  = 1\n            \n            out              = model(one_hot)\n            loss             = torch.log(torch.sum(torch.exp(out))) - out[wvi[k]]\n            loss.backward()\n            \n            for param in model.parameters():\n                param.data.sub_(lr*param.grad)\n                param.grad.data.zero_()\n\n\nfor i in range(y):\n    random.shuffle(v)\n    for vi in v:\n        wvi=RandomWalk(vi,t)\n        skip_gram(wvi, w)\n\n\nprint(model.phi)\n\n\n#### Hierarchical Softmax ####\n\ndef func_L(w):\n    """"""\n    Parameters\n    ----------\n    w: Leaf node.\n    \n    Returns\n    -------\n    count: The length of path from the root node to the given vertex.\n    """"""\n    count=1\n    while(w!=1):\n        count+=1\n        w//=2\n\n    return count\n\n\n# func_n returns the nth node in the path from the root node to the given vertex\ndef func_n(w, j):\n    li=[w]\n    while(w!=1):\n        w = w//2\n        li.append(w)\n\n    li.reverse()\n    \n    return li[j]\n\n\ndef sigmoid(x):\n    out = 1/(1+torch.exp(-x))\n    return out\n\n\nclass HierarchicalModel(torch.nn.Module):\n    \n    def __init__(self):\n        super(HierarchicalModel, self).__init__()\n        self.phi         = nn.Parameter(torch.rand((size_vertex, d), requires_grad=True))   \n        self.prob_tensor = nn.Parameter(torch.rand((2*size_vertex, d), requires_grad=True))\n    \n    def forward(self, wi, wo):\n        one_hot     = torch.zeros(size_vertex)\n        one_hot[wi] = 1\n        w = size_vertex + wo\n        h = torch.matmul(one_hot,self.phi)\n        p = torch.tensor([1.0])\n        for j in range(1, func_L(w)-1):\n            mult = -1\n            if(func_n(w, j+1)==2*func_n(w, j)): # Left child\n                mult = 1\n        \n            p = p*sigmoid(mult*torch.matmul(self.prob_tensor[func_n(w,j)], h))\n        \n        return p\n\n\nhierarchicalModel = HierarchicalModel()\n\n\ndef HierarchicalSkipGram(wvi,  w):\n   \n    for j in range(len(wvi)):\n        for k in range(max(0,j-w) , min(j+w, len(wvi))):\n            #generate one hot vector\n       \n            prob = hierarchicalModel(wvi[j], wvi[k])\n            loss = - torch.log(prob)\n            loss.backward()\n            for param in hierarchicalModel.parameters():\n                param.data.sub_(lr*param.grad)\n                param.grad.data.zero_()\n\n\nfor i in range(y):\n    random.shuffle(v)\n    for vi in v:\n        wvi = RandomWalk(vi,t)\n        HierarchicalSkipGram(wvi, w)\n\n\n\nfor i in range(8):\n    for j in range(8):\n        print((hierarchicalModel(i,j).item()*100)//1, end=\' \')\n    print(end = \'\\n\')\n'"
GAT/GAT_PyG.py,6,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.datasets import Planetoid\nimport torch_geometric.transforms as T\n\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\n# Seed for reproducible numbers\ntorch.manual_seed(2020)\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n# Dataset used Cora\nname_data = \'Cora\'\ndataset = Planetoid(root= \'/tmp/\' + name_data, name = name_data)\ndataset.transform = T.NormalizeFeatures()\n\nprint(f""Number of Classes in {name_data}:"", dataset.num_classes)\nprint(f""Number of Node Features in {name_data}:"", dataset.num_node_features)\n\n\n# Model Definition\nclass GAT(torch.nn.Module):\n    def __init__(self):\n        super(GAT, self).__init__()\n        self.hid = 8\n        self.in_head = 8\n        self.out_head = 1\n        \n        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False, heads=self.out_head, dropout=0.6)\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        \n        x = F.dropout(x, p=0.6, training=self.training)\n        x = self.conv1(x, edge_index)\n        x = F.elu(x)\n        x = F.dropout(x, p=0.6, training=self.training)\n        x = self.conv2(x, edge_index)\n        \n        return F.log_softmax(x, dim=1)\n\n\n# Train\nmodel = GAT().to(device)\ndata = dataset[0].to(device)\n\n# Adam Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n\n# Training Loop\nmodel.train()\nfor epoch in range(1000):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    \n    if epoch%200 == 0:\n        print(loss)\n    \n    loss.backward()\n    optimizer.step()\n\n# Evaluation\nmodel.eval()\n_, pred = model(data).max(dim=1)\ncorrect = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\nacc = correct / data.test_mask.sum().item()\nprint(\'Accuracy: {:.4f}\'.format(acc))'"
GCN/GCN.py,12,"b'#### Loading Required Libraries ####\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n# get_ipython().run_line_magic(\'matplotlib\', \'notebook\')\n\nimport imageio\nfrom celluloid import Camera\nfrom IPython.display import HTML\n\nplt.rcParams[\'animation.ffmpeg_path\'] = \'/usr/local/bin/ffmpeg\'\n\n\n#### The Convolutional Layer ####\n# First we will be creating the GCNConv class, which will serve as the Layer creation class.\n# Every instance of this class will be getting Adjacency Matrix as input and will be outputing\n# \'RELU(A_hat * X * W)\', which the Net class will use.\n\nclass GCNConv(nn.Module):\n    def __init__(self, A, in_channels, out_channels):\n        super(GCNConv, self).__init__()\n        self.A_hat = A+torch.eye(A.size(0))\n        self.D     = torch.diag(torch.sum(A,1))\n        self.D     = self.D.inverse().sqrt()\n        self.A_hat = torch.mm(torch.mm(self.D, self.A_hat), self.D)\n        self.W     = nn.Parameter(torch.rand(in_channels,out_channels, requires_grad=True))\n    \n    def forward(self, X):\n        out = torch.relu(torch.mm(torch.mm(self.A_hat, X), self.W))\n        return out\n\nclass Net(torch.nn.Module):\n    def __init__(self,A, nfeat, nhid, nout):\n        super(Net, self).__init__()\n        self.conv1 = GCNConv(A,nfeat, nhid)\n        self.conv2 = GCNConv(A,nhid, nout)\n        \n    def forward(self,X):\n        H  = self.conv1(X)\n        H2 = self.conv2(H)\n        return H2\n\n\n# \'A\' is the adjacency matrix, it contains 1 at a position (i,j)\n# if there is a edge between the node i and node j.\nA=torch.Tensor([[0,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,1,0,0],\n                [1,0,1,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0],\n                [1,1,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0],\n                [1,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1],\n                [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n                [1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n                [0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n                [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n                [1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,1,1],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,0],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1],\n                [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1],\n                [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1],\n                [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,1],\n                [0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1],\n                [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,1,1],\n                [0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,1,0,1,0,1,1,0,0,0,0,0,1,1,1,0,1],\n                [0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,1,0,0,1,1,1,0,1,1,0,0,1,1,1,1,1,1,1,0]\n                ])\n\n\n# label for admin(node 1) and instructor(node 34) so only these two contain the class label(0 and 1)\n# all other are set to -1, meaning predicted value of these nodes is ignored in the loss function.\ntarget=torch.tensor([0,-1,-1,-1, -1, -1, -1, -1,-1,-1,-1,-1, -1, -1, -1, -1,-1,-1,-1,-1, -1, -1, -1, -1,-1,-1,-1,-1, -1, -1, -1, -1,-1,1])\n\n\n# X is the feature matrix.\n# Using the one-hot encoding corresponding to the index of the node.\nX=torch.eye(A.size(0))\n\n\n# Network with 10 features in the hidden layer and 2 in output layer.\nT=Net(A,X.size(0), 10, 2)\n\n\n#### Training ####\n\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\noptimizer = optim.SGD(T.parameters(), lr=0.01, momentum=0.9)\n\nloss=criterion(T(X),target)\n\n\n#### Plot animation using celluloid ####\nfig = plt.figure()\ncamera = Camera(fig)\n\nfor i in range(200):\n    optimizer.zero_grad()\n    loss=criterion(T(X), target)\n    loss.backward()\n    optimizer.step()\n    l=(T(X));\n\n    plt.scatter(l.detach().numpy()[:,0],l.detach().numpy()[:,1],c=[0, 0, 0, 0 ,0 ,0 ,0, 0, 1, 1, 0 ,0, 0, 0, 1 ,1 ,0 ,0 ,1, 0, 1, 0 ,1 ,1, 1, 1, 1 ,1 ,1, 1, 1, 1, 1, 1 ])\n    for i in range(l.shape[0]):\n        text_plot = plt.text(l[i,0], l[i,1], str(i+1))\n\n    camera.snap()\n\n    if i%20==0:\n        print(""Cross Entropy Loss: ="", loss.item())\n\nanimation = camera.animate(blit=False, interval=150)\nanimation.save(\'./train_karate_animation.mp4\', writer=\'ffmpeg\', fps=60)\nHTML(animation.to_html5_video())\n'"
GCN/GCN_PyG.py,5,"b""#### Imports ####\nfrom torch_geometric.datasets import Planetoid\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import MessagePassing\nfrom torch_geometric.utils import add_self_loops, degree\n\n\n#### Loading the Dataset ####\ndataset = Planetoid(root='/tmp/Cora', name='Cora')\n\n\n#### The Graph Convolution Layer ####\nclass GraphConvolution(MessagePassing):\n    def __init__(self, in_channels, out_channels,bias=True, **kwargs):\n        super(GraphConvolution, self).__init__(aggr='add', **kwargs)\n        self.lin = torch.nn.Linear(in_channels, out_channels,bias=bias)\n\n    def forward(self, x, edge_index):\n        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n        x = self.lin(x)\n        return self.propagate(edge_index, size=(x.size(0), x.size(0)), x=x)\n\n    def message(self, x_j, edge_index, size):\n        row, col = edge_index\n        deg = degree(row, size[0], dtype=x_j.dtype)\n        deg_inv_sqrt = deg.pow(-0.5)\n        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n        return norm.view(-1, 1) * x_j\n\n    def update(self, aggr_out):\n        return aggr_out\n\n\nclass Net(torch.nn.Module):\n    def __init__(self,nfeat, nhid, nclass, dropout):\n        super(Net, self).__init__()\n        self.conv1 = GraphConvolution(nfeat, nhid)\n        self.conv2 = GraphConvolution(nhid, nclass)\n        self.dropout=dropout\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, self.dropout, training=self.training)\n        x = self.conv2(x, edge_index)\n\n        return F.log_softmax(x, dim=1)\n\n\nnfeat=dataset.num_node_features\nnhid=16\nnclass=dataset.num_classes\ndropout=0.5\n\n\n#### Training ####\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Net(nfeat, nhid, nclass, dropout).to(device)\ndata = dataset[0].to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n\nmodel.train()\nfor epoch in range(200):\n    optimizer.zero_grad()\n    out = model(data)\n    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n    loss.backward()\n    optimizer.step()\n\n\nmodel.eval()\n_, pred = model(data).max(dim=1)\ncorrect = float (pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\nacc = correct / data.test_mask.sum().item()\nprint('Accuracy: {:.4f}'.format(acc))\n"""
GraphSAGE/GraphSAGE.py,14,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport time\nimport random\nfrom sklearn.metrics import f1_score\nfrom collections import defaultdict\n\n#from graphsage.encoders import Encoder\n#from graphsage.aggregators import MeanAggregator\n\n""""""\nSimple supervised GraphSAGE model as well as examples running the model\non the Cora and Pubmed datasets.\n""""""\n\nclass MeanAggregator(nn.Module):\n    """"""\n    Aggregates a node\'s embeddings using mean of neighbors\' embeddings\n    """"""\n    def __init__(self, features, cuda=False, gcn=False): \n        """"""\n        Initializes the aggregator for a specific graph.\n        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n        cuda -- whether to use GPU\n        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n        """"""\n\n        super(MeanAggregator, self).__init__()\n\n        self.features = features\n        self.cuda = cuda\n        self.gcn = gcn\n        \n    def forward(self, nodes, to_neighs, num_sample=10):\n        """"""\n        nodes --- list of nodes in a batch\n        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n        num_sample --- number of neighbors to sample. No sampling if None.\n        """"""\n        # Local pointers to functions (speed hack)\n        _set = set\n        if not num_sample is None:\n            _sample = random.sample\n            samp_neighs = [_set(_sample(to_neigh, \n                            num_sample,\n                            )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n        else:\n            samp_neighs = to_neighs\n\n        if self.gcn:\n            samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n        unique_nodes_list = list(set.union(*samp_neighs))\n      #  print (""\\n unl\'s size="",len(unique_nodes_list))\n        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n        mask[row_indices, column_indices] = 1\n        if self.cuda:\n            mask = mask.cuda()\n        num_neigh = mask.sum(1, keepdim=True)\n        mask = mask.div(num_neigh)\n        if self.cuda:\n            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n        else:\n            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n        to_feats = mask.mm(embed_matrix)\n        return to_feats\n\nclass Encoder(nn.Module):\n    """"""\n    Encodes a node\'s using \'convolutional\' GraphSage approach\n    """"""\n    def __init__(self, features, feature_dim, \n            embed_dim, adj_lists, aggregator,\n            num_sample=10,\n            base_model=None, gcn=False, cuda=False, \n            feature_transform=False): \n        super(Encoder, self).__init__()\n\n        self.features = features\n        self.feat_dim = feature_dim\n        self.adj_lists = adj_lists\n        self.aggregator = aggregator\n        self.num_sample = num_sample\n        if base_model != None:\n            self.base_model = base_model\n\n        self.gcn = gcn\n        self.embed_dim = embed_dim\n        self.cuda = cuda\n        self.aggregator.cuda = cuda\n        self.weight = nn.Parameter(\n                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n        init.xavier_uniform(self.weight)\n\n    def forward(self, nodes):\n        """"""\n        Generates embeddings for a batch of nodes.\n        nodes     -- list of nodes\n        """"""\n        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], \n                self.num_sample)\n        if not self.gcn:\n            if self.cuda:\n                self_feats = self.features(torch.LongTensor(nodes).cuda())\n            else:\n                self_feats = self.features(torch.LongTensor(nodes))\n            combined = torch.cat([self_feats, neigh_feats], dim=1)\n        else:\n            combined = neigh_feats\n        combined = F.relu(self.weight.mm(combined.t()))\n        return combined\n\n\nclass SupervisedGraphSage(nn.Module):\n\n    def __init__(self, num_classes, enc):\n        super(SupervisedGraphSage, self).__init__()\n        self.enc = enc\n        self.xent = nn.CrossEntropyLoss()\n\n        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n        init.xavier_uniform(self.weight)\n\n    def forward(self, nodes):\n        embeds = self.enc(nodes)\n        scores = self.weight.mm(embeds)\n        return scores.t()\n\n    def loss(self, nodes, labels):\n        scores = self.forward(nodes)\n        return self.xent(scores, labels.squeeze())\n\ndef load_cora():\n    num_nodes = 2708\n    num_feats = 1433\n    feat_data = np.zeros((num_nodes, num_feats))\n    labels = np.empty((num_nodes,1), dtype=np.int64)\n    node_map = {}\n    label_map = {}\n    with open(""../cora/cora.content"") as fp:\n        for i,line in enumerate(fp):\n            info = line.strip().split()\n            feat_data[i,:] = [float(x) for x in info[1:-1]]\n            node_map[info[0]] = i\n            if not info[-1] in label_map:\n                label_map[info[-1]] = len(label_map)\n            labels[i] = label_map[info[-1]]\n\n    adj_lists = defaultdict(set)\n    with open(""../cora/cora.cites"") as fp:\n        for i,line in enumerate(fp):\n            info = line.strip().split()\n            paper1 = node_map[info[0]]\n            paper2 = node_map[info[1]]\n            adj_lists[paper1].add(paper2)\n            adj_lists[paper2].add(paper1)\n    return feat_data, labels, adj_lists\n\ndef run_cora():\n    np.random.seed(1)\n    random.seed(1)\n    num_nodes = 2708\n    feat_data, labels, adj_lists = load_cora()\n    features = nn.Embedding(2708, 1433)\n    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n   # features.cuda()\n\n    agg1 = MeanAggregator(features, cuda=True)\n    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2,\n            base_model=enc1, gcn=True, cuda=False)\n    enc1.num_samples = 5\n    enc2.num_samples = 5\n\n    graphsage = SupervisedGraphSage(7, enc2)\n#    graphsage.cuda()\n    rand_indices = np.random.permutation(num_nodes)\n    test = rand_indices[:1000]\n    val = rand_indices[1000:1500]\n    train = list(rand_indices[1500:])\n\n    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n    times = []\n    for batch in range(100):\n        batch_nodes = train[:256]\n        random.shuffle(train)\n        start_time = time.time()\n        optimizer.zero_grad()\n        loss = graphsage.loss(batch_nodes, \n                Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n        loss.backward()\n        optimizer.step()\n        end_time = time.time()\n        times.append(end_time-start_time)\n        print (batch, loss.item())\n\n    val_output = graphsage.forward(val) \n    print (""Validation F1:"", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=""micro""))\n    print (""Average batch time:"", np.mean(times))\n\nif __name__ == ""__main__"":\n    run_cora()\n'"
