file_path,api_count,code
HandCraftedModules.py,38,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nimport numpy as np\nfrom Utils import  GaussianBlur, CircularGaussKernel\nfrom LAF import abc2A,rectifyAffineTransformationUpIsUp, sc_y_x2LAFs,sc_y_x_and_A2LAFs\nfrom Utils import generate_2dgrid, generate_2dgrid, generate_3dgrid\nfrom Utils import zero_response_at_border\n\n\nclass ScalePyramid(nn.Module):\n    def __init__(self, nLevels = 3, init_sigma = 1.6, border = 5):\n        super(ScalePyramid,self).__init__()\n        self.nLevels = nLevels;\n        self.init_sigma = init_sigma\n        self.sigmaStep =  2 ** (1. / float(self.nLevels))\n        #print 'step',self.sigmaStep\n        self.b = border\n        self.minSize = 2 * self.b + 2 + 1;\n        return\n    def forward(self,x):\n        pixelDistance = 1.0;\n        curSigma = 0.5\n        if self.init_sigma > curSigma:\n            sigma = np.sqrt(self.init_sigma**2 - curSigma**2)\n            curSigma = self.init_sigma\n            curr = GaussianBlur(sigma = sigma)(x)\n        else:\n            curr = x\n        sigmas = [[curSigma]]\n        pixel_dists = [[1.0]]\n        pyr = [[curr]]\n        j = 0\n        while True:\n            curr = pyr[-1][0]\n            for i in range(1, self.nLevels + 2):\n                sigma = curSigma * np.sqrt(self.sigmaStep*self.sigmaStep - 1.0 )\n                #print 'blur sigma', sigma\n                curr = GaussianBlur(sigma = sigma)(curr)\n                curSigma *= self.sigmaStep\n                pyr[j].append(curr)\n                sigmas[j].append(curSigma)\n                pixel_dists[j].append(pixelDistance)\n                if i == self.nLevels:\n                    nextOctaveFirstLevel = F.avg_pool2d(curr, kernel_size = 1, stride = 2, padding = 0) \n            pixelDistance = pixelDistance * 2.0\n            curSigma = self.init_sigma\n            if (nextOctaveFirstLevel[0,0,:,:].size(0)  <= self.minSize) or (nextOctaveFirstLevel[0,0,:,:].size(1) <= self.minSize):\n                break\n            pyr.append([nextOctaveFirstLevel])\n            sigmas.append([curSigma])\n            pixel_dists.append([pixelDistance])\n            j+=1\n        return pyr, sigmas, pixel_dists\n\nclass HessianResp(nn.Module):\n    def __init__(self):\n        super(HessianResp, self).__init__()\n        \n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3), bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[0.5, 0, -0.5]]]], dtype=np.float32))\n\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[0.5], [0], [-0.5]]]], dtype=np.float32))\n\n        self.gxx =  nn.Conv2d(1, 1, kernel_size=(1,3),bias = False)\n        self.gxx.weight.data = torch.from_numpy(np.array([[[[1.0, -2.0, 1.0]]]], dtype=np.float32))\n        \n        self.gyy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gyy.weight.data = torch.from_numpy(np.array([[[[1.0], [-2.0], [1.0]]]], dtype=np.float32))\n        return\n    def forward(self, x, scale):\n        gxx = self.gxx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gyy = self.gyy(F.pad(x, (0,0, 1,1), 'replicate'))\n        gxy = self.gy(F.pad(self.gx(F.pad(x, (1,1,0, 0), 'replicate')), (0,0, 1,1), 'replicate'))\n        return torch.abs(gxx * gyy - gxy * gxy) * (scale**4)\n\n\nclass AffineShapeEstimator(nn.Module):\n    def __init__(self, threshold = 0.001, patch_size = 19):\n        super(AffineShapeEstimator, self).__init__()\n        self.threshold = threshold;\n        self.PS = patch_size\n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3), bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[-1, 0, 1]]]], dtype=np.float32))\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[-1], [0], [1]]]], dtype=np.float32))\n        self.gk = torch.from_numpy(CircularGaussKernel(kernlen = self.PS, sigma = (self.PS / 2) /3.0).astype(np.float32))\n        self.gk = Variable(self.gk, requires_grad=False)\n        return\n    def invSqrt(self,a,b,c):\n        eps = 1e-12\n        mask = (b != 0).float()\n        r1 = mask * (c - a) / (2. * b + eps)\n        t1 = torch.sign(r1) / (torch.abs(r1) + torch.sqrt(1. + r1*r1));\n        r = 1.0 / torch.sqrt( 1. + t1*t1)\n        t = t1*r;\n        r = r * mask + 1.0 * (1.0 - mask);\n        t = t * mask;\n        \n        x = 1. / torch.sqrt( r*r*a - 2.0*r*t*b + t*t*c)\n        z = 1. / torch.sqrt( t*t*a + 2.0*r*t*b + r*r*c)\n        \n        d = torch.sqrt( x * z)\n        \n        x = x / d\n        z = z / d\n        \n        l1 = torch.max(x,z)\n        l2 = torch.min(x,z)\n        \n        new_a = r*r*x + t*t*z\n        new_b = -r*t*x + t*r*z\n        new_c = t*t*x + r*r *z\n\n        return new_a, new_b, new_c, l1, l2\n    def forward(self,x):\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        else:\n            self.gk = self.gk.cpu()\n        gx = self.gx(F.pad(x, (1, 1, 0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0, 0, 1, 1), 'replicate'))\n        a1 = (gx * gx * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        b1 = (gx * gy * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        c1 = (gy * gy * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        a, b, c, l1, l2 = self.invSqrt(a1,b1,c1)\n        rat1 = l1/l2\n        mask = (torch.abs(rat1) <= 6.).float().view(-1);\n        return rectifyAffineTransformationUpIsUp(abc2A(a,b,c))#, mask\nclass OrientationDetector(nn.Module):\n    def __init__(self,\n                mrSize = 3.0, patch_size = None):\n        super(OrientationDetector, self).__init__()\n        if patch_size is None:\n            patch_size = 32;\n        self.PS = patch_size;\n        self.bin_weight_kernel_size, self.bin_weight_stride = self.get_bin_weight_kernel_size_and_stride(self.PS, 1)\n        self.mrSize = mrSize;\n        self.num_ang_bins = 36\n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3),  bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[0.5, 0, -0.5]]]], dtype=np.float32))\n\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[0.5], [0], [-0.5]]]], dtype=np.float32))\n\n        self.angular_smooth =  nn.Conv1d(1, 1, kernel_size=3, padding = 1, bias = False)\n        self.angular_smooth.weight.data = torch.from_numpy(np.array([[[0.33, 0.34, 0.33]]], dtype=np.float32))\n\n        self.gk = 10. * torch.from_numpy(CircularGaussKernel(kernlen=self.PS).astype(np.float32))\n        self.gk = Variable(self.gk, requires_grad=False)\n        return\n    def get_bin_weight_kernel_size_and_stride(self, patch_size, num_spatial_bins):\n        bin_weight_stride = int(round(2.0 * np.floor(patch_size / 2) / float(num_spatial_bins + 1)))\n        bin_weight_kernel_size = int(2 * bin_weight_stride - 1);\n        return bin_weight_kernel_size, bin_weight_stride\n    def get_rotation_matrix(self, angle_in_radians):\n        angle_in_radians = angle_in_radians.view(-1, 1, 1);\n        sin_a = torch.sin(angle_in_radians)\n        cos_a = torch.cos(angle_in_radians)\n        A1_x = torch.cat([cos_a, sin_a], dim = 2)\n        A2_x = torch.cat([-sin_a, cos_a], dim = 2)\n        transform = torch.cat([A1_x,A2_x], dim = 1)\n        return transform\n\n    def forward(self, x, return_rot_matrix = False):\n        gx = self.gx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0,0, 1,1), 'replicate'))\n        mag = torch.sqrt(gx * gx + gy * gy + 1e-10)\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        mag = mag * self.gk.unsqueeze(0).unsqueeze(0).expand_as(mag)\n        ori = torch.atan2(gy,gx)\n        o_big = float(self.num_ang_bins) *(ori + 1.0 * math.pi )/ (2.0 * math.pi)\n        bo0_big =  torch.floor(o_big)\n        wo1_big = o_big - bo0_big\n        bo0_big =  bo0_big %  self.num_ang_bins\n        bo1_big = (bo0_big + 1) % self.num_ang_bins\n        wo0_big = (1.0 - wo1_big) * mag\n        wo1_big = wo1_big * mag\n        ang_bins = []\n        for i in range(0, self.num_ang_bins):\n            ang_bins.append(F.adaptive_avg_pool2d((bo0_big == i).float() * wo0_big, (1,1)))\n        ang_bins = torch.cat(ang_bins,1).view(-1,1,self.num_ang_bins)\n        ang_bins = self.angular_smooth(ang_bins)\n        values, indices = ang_bins.view(-1,self.num_ang_bins).max(1)\n        angle =  -((2. * float(np.pi) * indices.float() / float(self.num_ang_bins)) - float(math.pi))\n        if return_rot_matrix:\n            return self.get_rotation_matrix(angle)\n        return angle\n    \nclass NMS2d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS2d, self).__init__()\n        self.MP = nn.MaxPool2d(kernel_size, stride=1, return_indices=False, padding = kernel_size/2)\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n\nclass NMS3d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS3d, self).__init__()\n        self.MP = nn.MaxPool3d(kernel_size, stride=1, return_indices=False, padding = (0, kernel_size//2, kernel_size//2))\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n        \nclass NMS3dAndComposeA(nn.Module):\n    def __init__(self, w = 0, h = 0, kernel_size = 3, threshold = 0, scales = None, border = 3, mrSize = 1.0):\n        super(NMS3dAndComposeA, self).__init__()\n        self.eps = 1e-7\n        self.ks = 3\n        self.th = threshold\n        self.cube_idxs = []\n        self.border = border\n        self.mrSize = mrSize\n        self.beta = 1.0\n        self.grid_ones = Variable(torch.ones(3,3,3,3), requires_grad=False)\n        self.NMS3d = NMS3d(kernel_size, threshold)\n        if (w > 0) and (h > 0):\n            self.spatial_grid = generate_2dgrid(h, w, False).view(1, h, w,2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        else:\n            self.spatial_grid = None\n        return\n    def forward(self, low, cur, high, num_features = 0, octaveMap = None, scales = None):\n        assert low.size() == cur.size() == high.size()\n        #Filter responce map\n        self.is_cuda = low.is_cuda;\n        resp3d = torch.cat([low,cur,high], dim = 1)\n        \n        mrSize_border = int(self.mrSize);\n        if octaveMap is not None:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border) * (1. - octaveMap.float())\n        else:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border)\n        \n        num_of_nonzero_responces = (nmsed_resp > 0).float().sum().item()#data[0]\n        if (num_of_nonzero_responces <= 1):\n            return None,None,None\n        if octaveMap is not None:\n            octaveMap = (octaveMap.float() + nmsed_resp.float()).byte()\n        \n        nmsed_resp = nmsed_resp.view(-1)\n        if (num_features > 0) and (num_features < num_of_nonzero_responces):\n            nmsed_resp, idxs = torch.topk(nmsed_resp, k = num_features, dim = 0);\n        else:\n            idxs = nmsed_resp.data.nonzero().squeeze()\n            nmsed_resp = nmsed_resp[idxs]\n        #Get point coordinates grid\n        \n        if type(scales) is not list:\n            self.grid = generate_3dgrid(3,self.ks,self.ks)\n        else:\n            self.grid = generate_3dgrid(scales,self.ks,self.ks)\n        self.grid = Variable(self.grid.t().contiguous().view(3,3,3,3), requires_grad=False)\n        if self.spatial_grid is None:\n            self.spatial_grid = generate_2dgrid(low.size(2), low.size(3), False).view(1, low.size(2), low.size(3),2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        if self.is_cuda:\n            self.spatial_grid = self.spatial_grid.cuda()\n            self.grid_ones = self.grid_ones.cuda()\n            self.grid = self.grid.cuda()\n        #residual_to_patch_center\n        sc_y_x = F.conv2d(resp3d, self.grid,\n                                padding = 1) / (F.conv2d(resp3d, self.grid_ones, padding = 1) + 1e-8)\n        \n        ##maxima coords\n        sc_y_x[0,1:,:,:] = sc_y_x[0,1:,:,:] + self.spatial_grid[:,:,:,0]\n        sc_y_x = sc_y_x.view(3,-1).t()\n        sc_y_x = sc_y_x[idxs,:]\n        \n        min_size = float(min((cur.size(2)), cur.size(3)))\n        sc_y_x[:,0] = sc_y_x[:,0] / min_size\n        sc_y_x[:,1] = sc_y_x[:,1] / float(cur.size(2))\n        sc_y_x[:,2] = sc_y_x[:,2] / float(cur.size(3))\n        return nmsed_resp, sc_y_x2LAFs(sc_y_x), octaveMap\nclass NMS3dAndComposeAAff(nn.Module):\n    def __init__(self, w = 0, h = 0, kernel_size = 3, threshold = 0, scales = None, border = 3, mrSize = 1.0):\n        super(NMS3dAndComposeAAff, self).__init__()\n        self.eps = 1e-7\n        self.ks = 3\n        self.th = threshold\n        self.cube_idxs = []\n        self.border = border\n        self.mrSize = mrSize\n        self.beta = 1.0\n        self.grid_ones = Variable(torch.ones(3,3,3,3), requires_grad=False)\n        self.NMS3d = NMS3d(kernel_size, threshold)\n        if (w > 0) and (h > 0):\n            self.spatial_grid = generate_2dgrid(h, w, False).view(1, h, w,2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        else:\n            self.spatial_grid = None\n        return\n    def forward(self, low, cur, high, num_features = 0, octaveMap = None, scales = None, aff_resp = None):\n        assert low.size() == cur.size() == high.size()\n        #Filter responce map\n        self.is_cuda = low.is_cuda;\n        resp3d = torch.cat([low,cur,high], dim = 1)\n        \n        mrSize_border = int(self.mrSize);\n        if octaveMap is not None:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border) * (1. - octaveMap.float())\n        else:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border)\n        \n        num_of_nonzero_responces = (nmsed_resp > 0).float().sum().item()#data[0]\n        if (num_of_nonzero_responces <= 1):\n            return None,None,None\n        if octaveMap is not None:\n            octaveMap = (octaveMap.float() + nmsed_resp.float()).byte()\n        \n        nmsed_resp = nmsed_resp.view(-1)\n        if (num_features > 0) and (num_features < num_of_nonzero_responces):\n            nmsed_resp, idxs = torch.topk(nmsed_resp, k = num_features, dim = 0);\n        else:\n            idxs = nmsed_resp.data.nonzero().squeeze()\n            nmsed_resp = nmsed_resp[idxs]\n        #Get point coordinates grid\n        if type(scales) is not list:\n            self.grid = generate_3dgrid(3,self.ks,self.ks)\n        else:\n            self.grid = generate_3dgrid(scales,self.ks,self.ks)\n        self.grid = Variable(self.grid.t().contiguous().view(3,3,3,3), requires_grad=False)\n        if self.spatial_grid is None:\n            self.spatial_grid = generate_2dgrid(low.size(2), low.size(3), False).view(1, low.size(2), low.size(3),2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        if self.is_cuda:\n            self.spatial_grid = self.spatial_grid.cuda()\n            self.grid_ones = self.grid_ones.cuda()\n            self.grid = self.grid.cuda()\n        \n        #residual_to_patch_center\n        sc_y_x = F.conv2d(resp3d, self.grid,\n                                padding = 1) / (F.conv2d(resp3d, self.grid_ones, padding = 1) + 1e-8)\n        \n        ##maxima coords\n        sc_y_x[0,1:,:,:] = sc_y_x[0,1:,:,:] + self.spatial_grid[:,:,:,0]\n        sc_y_x = sc_y_x.view(3,-1).t()\n        sc_y_x = sc_y_x[idxs,:]\n        if aff_resp is not None:\n            A_matrices = aff_resp.view(4,-1).t()[idxs,:]        \n        min_size = float(min((cur.size(2)), cur.size(3)))\n        \n        sc_y_x[:,0] = sc_y_x[:,0] / min_size\n        sc_y_x[:,1] = sc_y_x[:,1] / float(cur.size(2))\n        sc_y_x[:,2] = sc_y_x[:,2] / float(cur.size(3))\n        return nmsed_resp, sc_y_x_and_A2LAFs(sc_y_x,A_matrices), octaveMap\n"""
HardNet.py,10,"b'import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nimport math\nimport numpy as np\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-8\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\nclass HardTFeatNet(nn.Module):\n    """"""TFeat model definition\n    """"""\n\n    def __init__(self, sm):\n        super(HardTFeatNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=7),\n            nn.Tanh(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=6),\n            nn.Tanh()\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(64, 128, kernel_size=8),\n            nn.Tanh())\n        self.SIFT = sm\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    \n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        x = self.classifier(x_features)\n        return L2Norm()(x.view(x.size(0), -1))\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self):\n        super(HardNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n        #self.features.apply(weights_init)\n\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n'"
LAF.py,84,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\nfrom scipy.spatial.distance import cdist\nfrom numpy.linalg import inv    \nfrom scipy.linalg import schur, sqrtm\nimport torch\nfrom  torch.autograd import Variable\nimport torch.nn.functional as F\n##########numpy\ndef invSqrt(a,b,c):\n    eps = 1e-12 \n    mask = (b !=  0)\n    r1 = mask * (c - a) / (2. * b + eps)\n    t1 = np.sign(r1) / (np.abs(r1) + np.sqrt(1. + r1*r1));\n    r = 1.0 / np.sqrt( 1. + t1*t1)\n    t = t1*r;\n    \n    r = r * mask + 1.0 * (1.0 - mask);\n    t = t * mask;\n    \n    x = 1. / np.sqrt( r*r*a - 2*r*t*b + t*t*c)\n    z = 1. / np.sqrt( t*t*a + 2*r*t*b + r*r*c)\n    \n    d = np.sqrt( x * z)\n    \n    x = x / d\n    z = z / d\n       \n    new_a = r*r*x + t*t*z\n    new_b = -r*t*x + t*r*z\n    new_c = t*t*x + r*r *z\n\n    return new_a, new_b, new_c\ndef LAFs2ellT(LAFs):\n    ellipses = torch.zeros((len(LAFs),5))\n    if LAFs.is_cuda:\n        ellipses = ellipses.cuda()\n    scale = torch.sqrt(LAFs[:,0,0]*LAFs[:,1,1]  - LAFs[:,0,1]*LAFs[:,1, 0] + 1e-10)#.view(-1,1,1)\n    unscaled_As = LAFs[:,0:2,0:2] / scale.view(-1,1,1).repeat(1,2,2)\n    u, W, v = bsvd2x2(unscaled_As)\n    #W = 1.0 / ((W *scale.view(-1,1,1).repeat(1,2,2))**2) \n    W[:,0,0] = 1.0 /  (scale*scale*W[:,0,0]**2 )\n    W[:,1,1] = 1.0 /  (scale*scale*W[:,1,1]**2 )\n    A = torch.bmm(torch.bmm(u,W), u.permute(0,2,1))\n    ellipses[:,0] = LAFs[:,0,2]\n    ellipses[:,1] = LAFs[:,1,2]\n    ellipses[:,2] = A[:,0,0]\n    ellipses[:,3] = A[:,0,1]\n    ellipses[:,4] = A[:,1,1]\n    return ellipses\ndef invSqrtTorch(a,b,c):\n    eps = 1e-12\n    mask = (b != 0).float()\n    r1 = mask * (c - a) / (2. * b + eps)\n    t1 = torch.sign(r1) / (torch.abs(r1) + torch.sqrt(1. + r1*r1));\n    r = 1.0 / torch.sqrt( 1. + t1*t1)\n    t = t1*r;\n    r = r * mask + 1.0 * (1.0 - mask);\n    t = t * mask;\n\n    x = 1. / torch.sqrt( r*r*a - 2.0*r*t*b + t*t*c)\n    z = 1. / torch.sqrt( t*t*a + 2.0*r*t*b + r*r*c)\n\n    d = torch.sqrt( x * z)\n\n    x = x / d\n    z = z / d\n\n    new_a = r*r*x + t*t*z\n    new_b = -r*t*x + t*r*z\n    new_c = t*t*x + r*r *z\n\n    return new_a, new_b, new_c,\n    \ndef ells2LAFsT(ells):\n    LAFs = torch.zeros((len(ells), 2,3))\n    LAFs[:,0,2] = ells[:,0]\n    LAFs[:,1,2] = ells[:,1]\n    a = ells[:,2]\n    b = ells[:,3]\n    c = ells[:,4]\n    sc = torch.sqrt(torch.sqrt(a*c - b*b + 1e-12))\n    ia,ib,ic = invSqrtTorch(a,b,c)  #because sqrtm returns ::-1, ::-1 matrix, don`t know why \n    A = torch.cat([torch.cat([(ia/sc).view(-1,1,1), (ib/sc).view(-1,1,1)], dim = 2),\n                   torch.cat([(ib/sc).view(-1,1,1), (ic/sc).view(-1,1,1)], dim = 2)], dim = 1)\n    sc = torch.sqrt(torch.abs(A[:,0,0] * A[:,1,1] - A[:,1,0] * A[:,0,1]))\n    LAFs[:,0:2,0:2] = rectifyAffineTransformationUpIsUp(A / sc.view(-1,1,1).repeat(1,2,2)) * sc.view(-1,1,1).repeat(1,2,2)\n    return LAFs\n\ndef LAFs_to_H_frames(aff_pts):\n    H3_x = torch.Tensor([0, 0, 1 ]).unsqueeze(0).unsqueeze(0).repeat(aff_pts.size(0),1,1);\n    if aff_pts.is_cuda:\n        H3_x = H3_x.cuda()\n    return torch.cat([aff_pts, H3_x], dim = 1)\n\n\ndef checkTouchBoundary(LAFs):\n    pts = torch.FloatTensor([[-1, -1, 1, 1], [-1, 1, -1, 1], [1, 1, 1, 1]]).unsqueeze(0)\n    if LAFs.is_cuda:\n        pts = pts.cuda()\n    out_pts =  torch.bmm(LAFs_to_H_frames(LAFs),pts.expand(LAFs.size(0),3,4))[:,:2,:]\n    good_points = 1 -(((out_pts > 1.0) +  (out_pts < 0.0)).sum(dim=1).sum(dim=1) > 0)\n    return good_points\n\ndef bsvd2x2(As):\n    Su = torch.bmm(As,As.permute(0,2,1))\n    phi = 0.5 * torch.atan2(Su[:,0,1] + Su[:,1,0] + 1e-12, Su[:,0,0] - Su[:,1,1] + 1e-12)\n    Cphi = torch.cos(phi)\n    Sphi = torch.sin(phi)\n    U = torch.zeros(As.size(0),2,2)\n    if As.is_cuda:\n        U = U.cuda()\n    U[:,0,0] = Cphi\n    U[:,1,1] = Cphi\n    U[:,0,1] = -Sphi\n    U[:,1,0] = Sphi\n    Sw = torch.bmm(As.permute(0,2,1),As)\n    theta = 0.5 * torch.atan2(Sw[:,0,1] + Sw[:,1,0] + 1e-12, Sw[:,0,0] - Sw[:,1,1] + 1e-12)\n    Ctheta = torch.cos(theta)\n    Stheta = torch.sin(theta)\n    W = torch.zeros(As.size(0),2,2)\n    if As.is_cuda:\n        W = W.cuda()\n    W[:,0,0] = Ctheta\n    W[:,1,1] = Ctheta\n    W[:,0,1] = -Stheta\n    W[:,1,0] = Stheta\n    SUsum = Su[:,0,0] + Su[:,1,1]\n    SUdif = torch.sqrt((Su[:,0,0] - Su[:,1,1])**2 + 4 * Su[:,0,1]*Su[:,1,0] + 1e-12)\n    if As.is_cuda:\n        SIG = torch.zeros(As.size(0),2,2).cuda()\n        SIG[:,0,0] = torch.sqrt((SUsum+SUdif)/2.0)\n        SIG[:,1,1] = torch.sqrt((SUsum-SUdif)/2.0)\n    else:\n        SIG = torch.zeros(As.size(0),2,2)\n        SIG[:,0,0] = torch.sqrt((SUsum+SUdif)/2.0)\n        SIG[:,1,1] = torch.sqrt((SUsum-SUdif)/2.0)\n    S = torch.bmm(torch.bmm(U.permute(0,2,1),As),W)\n    C = torch.sign(S)\n    C[:,0,1] = 0\n    C[:,1,0] = 0\n    V = torch.bmm(W,C)\n    return (U,SIG,V)\n\ndef getLAFelongation(LAFs):\n    u,s,v = bsvd2x2(LAFs[:,:2,:2])\n    return torch.max(s[:,0,0],s[:,1,1]) / torch.min(s[:,0,0],s[:,1,1])\n\ndef getNumCollapsed(LAFs, th = 10.0):\n    el = getLAFelongation(LAFs)\n    return (el > th).float().sum()\n\ndef Ell2LAF(ell):\n    A23 = np.zeros((2,3))\n    A23[0,2] = ell[0]\n    A23[1,2] = ell[1]\n    a = ell[2]\n    b = ell[3]\n    c = ell[4]\n    sc = np.sqrt(np.sqrt(a*c - b*b))\n    ia,ib,ic = invSqrt(a,b,c)  #because sqrtm returns ::-1, ::-1 matrix, don`t know why \n    A = np.array([[ia, ib], [ib, ic]]) / sc\n    sc = np.sqrt(A[0,0] * A[1,1] - A[1,0] * A[0,1])\n    A23[0:2,0:2] = rectifyAffineTransformationUpIsUp(A / sc) * sc\n    return A23\n\ndef rectifyAffineTransformationUpIsUp_np(A):\n    det = np.sqrt(np.abs(A[0,0]*A[1,1] - A[1,0]*A[0,1] + 1e-10))\n    b2a2 = np.sqrt(A[0,1] * A[0,1] + A[0,0] * A[0,0])\n    A_new = np.zeros((2,2))\n    A_new[0,0] = b2a2 / det\n    A_new[0,1] = 0\n    A_new[1,0] = (A[1,1]*A[0,1]+A[1,0]*A[0,0])/(b2a2*det)\n    A_new[1,1] = det / b2a2\n    return A_new\n\ndef ells2LAFs(ells):\n    LAFs = np.zeros((len(ells), 2,3))\n    for i in range(len(ells)):\n        LAFs[i,:,:] = Ell2LAF(ells[i,:])\n    return LAFs\n\ndef LAF2pts(LAF, n_pts = 50):\n    a = np.linspace(0, 2*np.pi, n_pts);\n    x = [0]\n    x.extend(list(np.sin(a)))\n    x = np.array(x).reshape(1,-1)\n    y = [0]\n    y.extend(list(np.cos(a)))\n    y = np.array(y).reshape(1,-1)\n    HLAF = np.concatenate([LAF, np.array([0,0,1]).reshape(1,3)])\n    H_pts =np.concatenate([x,y,np.ones(x.shape)])\n    H_pts_out = np.transpose(np.matmul(HLAF, H_pts))\n    H_pts_out[:,0] = H_pts_out[:,0] / H_pts_out[:, 2]\n    H_pts_out[:,1] = H_pts_out[:,1] / H_pts_out[:, 2]\n    return H_pts_out[:,0:2]\n\n\ndef convertLAFs_to_A23format(LAFs):\n    sh = LAFs.shape\n    if (len(sh) == 3) and (sh[1]  == 2) and (sh[2] == 3): # n x 2 x 3 classical [A, (x;y)] matrix\n        work_LAFs = deepcopy(LAFs)\n    elif (len(sh) == 2) and (sh[1]  == 7): #flat format, x y scale a11 a12 a21 a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] * LAFs[:,3] \n        work_LAFs[:,0,1] = LAFs[:,2] * LAFs[:,4]\n        work_LAFs[:,1,0] = LAFs[:,2] * LAFs[:,5]\n        work_LAFs[:,1,1] = LAFs[:,2] * LAFs[:,6]\n    elif (len(sh) == 2) and (sh[1]  == 6): #flat format, x y s*a11 s*a12 s*a21 s*a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] \n        work_LAFs[:,0,1] = LAFs[:,3]\n        work_LAFs[:,1,0] = LAFs[:,4]\n        work_LAFs[:,1,1] = LAFs[:,5]\n    else:\n        print ('Unknown LAF format')\n        return None\n    return work_LAFs\n\ndef LAFs2ell(in_LAFs):\n    LAFs = convertLAFs_to_A23format(in_LAFs)\n    ellipses = np.zeros((len(LAFs),5))\n    for i in range(len(LAFs)):\n        LAF = deepcopy(LAFs[i,:,:])\n        scale = np.sqrt(LAF[0,0]*LAF[1,1]  - LAF[0,1]*LAF[1, 0] + 1e-10)\n        u, W, v = np.linalg.svd(LAF[0:2,0:2] / scale, full_matrices=True)\n        W[0] = 1. / (W[0]*W[0]*scale*scale)\n        W[1] = 1. / (W[1]*W[1]*scale*scale)\n        A =  np.matmul(np.matmul(u, np.diag(W)), u.transpose())\n        ellipses[i,0] = LAF[0,2]\n        ellipses[i,1] = LAF[1,2]\n        ellipses[i,2] = A[0,0]\n        ellipses[i,3] = A[0,1]\n        ellipses[i,4] = A[1,1]\n    return ellipses\n\ndef visualize_LAFs(img, LAFs, color = 'r', show = False, save_to = None):\n    work_LAFs = convertLAFs_to_A23format(LAFs)\n    plt.figure()\n    plt.imshow(255 - img)\n    for i in range(len(work_LAFs)):\n        ell = LAF2pts(work_LAFs[i,:,:])\n        plt.plot( ell[:,0], ell[:,1], color)\n    if show:\n        plt.show()\n    if save_to is not None:\n        plt.savefig(save_to)\n    return \n\n####pytorch\n\ndef get_normalized_affine_shape(tilt, angle_in_radians):\n    assert tilt.size(0) == angle_in_radians.size(0)\n    num = tilt.size(0)\n    tilt_A = Variable(torch.eye(2).view(1,2,2).repeat(num,1,1))\n    if tilt.is_cuda:\n        tilt_A = tilt_A.cuda()\n    tilt_A[:,0,0] = tilt.view(-1);\n    rotmat = get_rotation_matrix(angle_in_radians)\n    out_A = rectifyAffineTransformationUpIsUp(torch.bmm(rotmat, torch.bmm(tilt_A, rotmat)))\n    #re_scale = (1.0/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    #re_scale = (0.5 + 0.5/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    return out_A# * re_scale.view(-1,1,1).expand(num,2,2)\n\ndef get_rotation_matrix(angle_in_radians):\n    angle_in_radians = angle_in_radians.view(-1, 1, 1);\n    sin_a = torch.sin(angle_in_radians)\n    cos_a = torch.cos(angle_in_radians)\n    A1_x = torch.cat([cos_a, sin_a], dim = 2)\n    A2_x = torch.cat([-sin_a, cos_a], dim = 2)\n    transform = torch.cat([A1_x,A2_x], dim = 1)\n    return transform\n    \ndef rectifyAffineTransformationUpIsUp(A):\n    det = torch.sqrt(torch.abs(A[:,0,0]*A[:,1,1] - A[:,1,0]*A[:,0,1] + 1e-10))\n    b2a2 = torch.sqrt(A[:,0,1] * A[:,0,1] + A[:,0,0] * A[:,0,0])\n    A1_ell = torch.cat([(b2a2 / det).contiguous().view(-1,1,1), 0 * det.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([((A[:,1,1]*A[:,0,1]+A[:,1,0]*A[:,0,0])/(b2a2*det)).contiguous().view(-1,1,1),\n                        (det / b2a2).contiguous().view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\ndef rectifyAffineTransformationUpIsUpFullyConv(A):#A is (n,4,h,w) tensor\n    det = torch.sqrt(torch.abs(A[:,0:1,:,:]*A[:,3:4,:,:] - A[:,1:2,:,:]*A[:,2:3,:,:] + 1e-10))\n    b2a2 = torch.sqrt(A[:,1:2,:,:] * A[:,1:2,:,:] + A[:,0:1,:,:] * A[:,0:1,:,:])\n    return torch.cat([(b2a2 / det).contiguous(),0 * det.contiguous(),\n                      (A[:,3:4,:,:]*A[:,1:2,:,:]+A[:,2:3,:,:]*A[:,0:1,:,:])/(b2a2*det),(det / b2a2).contiguous()], dim = 1)\n\ndef abc2A(a,b,c, normalize = False):\n    A1_ell = torch.cat([a.view(-1,1,1), b.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([b.view(-1,1,1), c.view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\n\n\ndef angles2A(angles):\n    cos_a = torch.cos(angles).view(-1, 1, 1)\n    sin_a = torch.sin(angles).view(-1, 1, 1)\n    A1_ang = torch.cat([cos_a, sin_a], dim = 2)\n    A2_ang = torch.cat([-sin_a, cos_a], dim = 2)\n    return  torch.cat([A1_ang, A2_ang], dim = 1)\n\ndef generate_patch_grid_from_normalized_LAFs(LAFs, w, h, PS):\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3) * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    grid = F.affine_grid(LAFs * Variable(coef.expand(num_lafs,2,3)), torch.Size((num_lafs,1,PS,PS)))\n    grid[:,:,:,0] = 2.0 * grid[:,:,:,0] / float(w)  - 1.0\n    grid[:,:,:,1] = 2.0 * grid[:,:,:,1] / float(h)  - 1.0     \n    return grid\n\ndef batched_grid_apply(img, grid, batch_size = 32):\n    n_patches = len(grid)\n    if n_patches > batch_size:\n        bs = batch_size\n        n_batches = int(n_patches / bs + 1)\n        for batch_idx in range(n_batches):\n            st = batch_idx * bs\n            if batch_idx == n_batches - 1:\n                if (batch_idx + 1) * bs > n_patches:\n                    end = n_patches\n                else:\n                    end = (batch_idx + 1) * bs\n            else:\n                end = (batch_idx + 1) * bs\n            if st >= end:\n                continue\n            if batch_idx == 0:\n                if img.size(0) != grid.size(0):\n                    first_batch_out = F.grid_sample(img.expand(end - st, img.size(1), img.size(2), img.size(3)), grid[st:end, :,:,:])# kwargs)\n                else:\n                    first_batch_out = F.grid_sample(img[st:end], grid[st:end, :,:,:])# kwargs)\n                out_size = torch.Size([n_patches] + list(first_batch_out.size()[1:]))\n                out = torch.zeros(out_size);\n                if img.is_cuda:\n                    out = out.cuda()\n                out[st:end] = first_batch_out\n            else:\n                if img.size(0) != grid.size(0):\n                    out[st:end,:,:] = F.grid_sample(img.expand(end - st, img.size(1), img.size(2), img.size(3)), grid[st:end, :,:,:])\n                else:\n                    out[st:end,:,:] = F.grid_sample(img[st:end], grid[st:end, :,:,:])\n        return out\n    else:\n        if img.size(0) != grid.size(0):\n            return F.grid_sample(img.expand(grid.size(0), img.size(1), img.size(2), img.size(3)), grid)\n        else:\n            return F.grid_sample(img, grid)\n\ndef extract_patches(img, LAFs, PS = 32, bs = 32):\n    w = img.size(3)\n    h = img.size(2)\n    ch = img.size(1)\n    grid = generate_patch_grid_from_normalized_LAFs(LAFs, float(w),float(h), PS)\n    if bs is None:\n        return torch.nn.functional.grid_sample(img.expand(grid.size(0), ch, h, w),  grid)  \n    else:\n        return batched_grid_apply(img, grid, bs)\ndef get_pyramid_inverted_index_for_LAFs(LAFs, PS, sigmas):\n    return\n\ndef extract_patches_from_pyramid_with_inv_index(scale_pyramid, pyr_inv_idxs, LAFs, PS = 19):\n    patches = torch.zeros(LAFs.size(0),scale_pyramid[0][0].size(1), PS, PS)\n    if LAFs.is_cuda:\n        patches = patches.cuda()\n    patches = Variable(patches)\n    if pyr_inv_idxs is not None:\n        for i in range(len(scale_pyramid)):\n            for j in range(len(scale_pyramid[i])):\n                cur_lvl_idxs = pyr_inv_idxs[i][j]\n                if cur_lvl_idxs is None:\n                    continue\n                cur_lvl_idxs = cur_lvl_idxs.view(-1)\n                #print i,j,cur_lvl_idxs.shape\n                patches[cur_lvl_idxs,:,:,:] = extract_patches(scale_pyramid[i][j], LAFs[cur_lvl_idxs, :,:], PS, 32 )\n    return patches\n\ndef get_inverted_pyr_index(scale_pyr, pyr_idxs, level_idxs):\n    pyr_inv_idxs = []\n    ### Precompute octave inverted indexes\n    for i in range(len(scale_pyr)):\n        pyr_inv_idxs.append([])\n        cur_idxs = pyr_idxs == i #torch.nonzero((pyr_idxs == i).data)\n        for j in range(0, len(scale_pyr[i])):\n            cur_lvl_idxs = torch.nonzero(((level_idxs == j) * cur_idxs).data)\n            if cur_lvl_idxs.size(0) == 0:\n                pyr_inv_idxs[i].append(None)\n            else:\n                pyr_inv_idxs[i].append(cur_lvl_idxs.squeeze())\n    return pyr_inv_idxs\n\n\ndef denormalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef normalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  / min_size\n    coef[0,0,2] = 1.0 / w\n    coef[0,1,2] = 1.0 / h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef sc_y_x2LAFs(sc_y_x):\n    base_LAF = torch.eye(2).float().unsqueeze(0).expand(sc_y_x.size(0),2,2)\n    if sc_y_x.is_cuda:\n        base_LAF = base_LAF.cuda()\n    base_A = Variable(base_LAF, requires_grad=False)\n    A = sc_y_x[:,:1].unsqueeze(1).expand_as(base_A) * base_A\n    LAFs  = torch.cat([A,\n                       torch.cat([sc_y_x[:,2:].unsqueeze(-1),\n                                    sc_y_x[:,1:2].unsqueeze(-1)], dim=1)], dim = 2)\n        \n    return LAFs\ndef sc_y_x_and_A2LAFs(sc_y_x, A_flat):\n    base_A = A_flat.view(-1,2,2)\n    A = sc_y_x[:,:1].unsqueeze(1).expand_as(base_A) * base_A\n    LAFs  = torch.cat([A,\n                       torch.cat([sc_y_x[:,2:].unsqueeze(-1),\n                                    sc_y_x[:,1:2].unsqueeze(-1)], dim=1)], dim = 2)\n        \n    return LAFs\ndef get_LAFs_scales(LAFs):\n    return torch.sqrt(torch.abs(LAFs[:,0,0] *LAFs[:,1,1] - LAFs[:,0,1] * LAFs[:,1,0]) + 1e-12)\n\ndef get_pyramid_and_level_index_for_LAFs(dLAFs,  sigmas, pix_dists, PS):\n    scales = get_LAFs_scales(dLAFs);\n    needed_sigmas = scales / PS;\n    sigmas_full_list = []\n    level_idxs_full = []\n    oct_idxs_full = []\n    for oct_idx in range(len(sigmas)):\n        sigmas_full_list = sigmas_full_list + list(np.array(sigmas[oct_idx])*np.array(pix_dists[oct_idx]))\n        oct_idxs_full = oct_idxs_full + [oct_idx]*len(sigmas[oct_idx])\n        level_idxs_full = level_idxs_full + list(range(0,len(sigmas[oct_idx])))\n    oct_idxs_full = torch.LongTensor(oct_idxs_full)\n    level_idxs_full = torch.LongTensor(level_idxs_full)\n    \n    closest_imgs = cdist(np.array(sigmas_full_list).reshape(-1,1), needed_sigmas.data.cpu().numpy().reshape(-1,1)).argmin(axis = 0)\n    closest_imgs = torch.from_numpy(closest_imgs)\n    if dLAFs.is_cuda:\n        closest_imgs = closest_imgs.cuda()\n        oct_idxs_full = oct_idxs_full.cuda()\n        level_idxs_full = level_idxs_full.cuda()\n    return  Variable(oct_idxs_full[closest_imgs]), Variable(level_idxs_full[closest_imgs])\n"""
Losses.py,53,"b'import torch\nimport torch.nn as nn\nimport sys\n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1).unsqueeze(-1)\n    d2_sq = torch.sum(positive * positive, dim=1).unsqueeze(-1)\n\n    eps = 1e-6\n    return torch.sqrt((d1_sq.repeat(1, positive.size(0)) + torch.t(d2_sq.repeat(1, anchor.size(0)))\n                      - 2.0 * torch.bmm(anchor.unsqueeze(0), torch.t(positive).unsqueeze(0)).squeeze(0))+eps)\n\ndef distance_vectors_pairwise(anchor, positive, negative = None):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    a_sq = torch.sum(anchor * anchor, dim=1)\n    p_sq = torch.sum(positive * positive, dim=1)\n\n    eps = 1e-8\n    d_a_p = torch.sqrt(a_sq + p_sq - 2*torch.sum(anchor * positive, dim = 1) + eps)\n    if negative is not None:\n        n_sq = torch.sum(negative * negative, dim=1)\n        d_a_n = torch.sqrt(a_sq + n_sq - 2*torch.sum(anchor * negative, dim = 1) + eps)\n        d_p_n = torch.sqrt(p_sq + n_sq - 2*torch.sum(positive * negative, dim = 1) + eps)\n        return d_a_p, d_a_n, d_p_n\n    return d_a_p\n\n\ndef loss_random_sampling(anchor, positive, negative, anchor_swap = False, margin = 1.0, loss_type = ""triplet_margin""):\n    """"""Loss with random sampling (no hard in batch).\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.size() == negative.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    (pos, d_a_n, d_p_n) = distance_vectors_pairwise(anchor, positive, negative)\n    if anchor_swap:\n       min_neg = torch.min(d_a_n, d_p_n)\n    else:\n       min_neg = d_a_n\n\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\ndef loss_HardNegC(anchor, positive, margin = 1.0):\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix_detach = distance_matrix_vector(anchor, positive.detach()) + eps\n    pos1 = distance_vectors_pairwise(anchor,positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix_detach.size(1))).cuda()\n    # steps to filter out same patches that occur in distance matrix as negatives\n    dist_without_min_on_diag = dist_matrix_detach + eye*10\n    mask = (dist_without_min_on_diag.ge(0.008).float()-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag + mask\n    min_neg = torch.min(dist_without_min_on_diag,1)[0]\n    loss = torch.clamp(margin + pos1 - min_neg, min=0.0)\n    loss = 0.5 * torch.mean(loss)\n    dist_matrix_detach2 = distance_matrix_vector(anchor.detach(), positive) + eps\n    # steps to filter out same patches that occur in distance matrix as negatives\n    dist_without_min_on_diag2 = dist_matrix_detach2 + eye*10\n    mask2 = (dist_without_min_on_diag2.ge(0.008).float()-1)*-1\n    mask2 = mask2.type_as(dist_without_min_on_diag2)*10\n    dist_without_min_on_diag2 = dist_without_min_on_diag2 + mask2\n    min_neg2 = torch.min(dist_without_min_on_diag2,0)[0]\n    loss += 0.5 * torch.clamp(margin + pos1 - min_neg2, min=0.0).mean()\n    return loss\n\ndef loss_L2Net(anchor, positive, anchor_swap = False,  margin = 1.0, loss_type = ""triplet_margin""):\n    """"""L2Net losses: using whole batch as negatives, not only hardest.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008)-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    \n    if loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos1);\n        exp_den = torch.sum(torch.exp(2.0 - dist_matrix),1) + eps;\n        loss = -torch.log( exp_pos / exp_den )\n        if anchor_swap:\n            exp_den1 = torch.sum(torch.exp(2.0 - dist_matrix),0) + eps;\n            loss += -torch.log( exp_pos / exp_den1 )\n    else: \n        print (\'Only softmax loss works with L2Net sampling\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_HardNet(anchor, positive, anchor_swap = False, anchor_ave = False,\\\n        margin = 1.0, batch_reduce = \'min\', loss_type = ""triplet_margin""):\n    """"""HardNet margin loss - calculates loss based on distance matrix based on positive distance and closest negative distance.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive) +eps\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008).float()-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    if batch_reduce == \'min\':\n        min_neg = torch.min(dist_without_min_on_diag,1)[0]\n        if anchor_swap:\n            min_neg2 = torch.min(dist_without_min_on_diag,0)[0]\n            min_neg = torch.min(min_neg,min_neg2)\n        if False:\n            dist_matrix_a = distance_matrix_vector(anchor, anchor)+ eps\n            dist_matrix_p = distance_matrix_vector(positive,positive)+eps\n            dist_without_min_on_diag_a = dist_matrix_a+eye*10\n            dist_without_min_on_diag_p = dist_matrix_p+eye*10\n            min_neg_a = torch.min(dist_without_min_on_diag_a,1)[0]\n            min_neg_p = torch.t(torch.min(dist_without_min_on_diag_p,0)[0])\n            min_neg_3 = torch.min(min_neg_p,min_neg_a)\n            min_neg = torch.min(min_neg,min_neg_3)\n            print (min_neg_a)\n            print (min_neg_p)\n            print (min_neg_3)\n            print (min_neg)\n        min_neg = min_neg\n        pos = pos1\n    elif batch_reduce == \'average\':\n        pos = pos1.repeat(anchor.size(0)).view(-1,1).squeeze(0)\n        min_neg = dist_without_min_on_diag.view(-1,1)\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).contiguous().view(-1,1)\n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = min_neg.squeeze(0)\n    elif batch_reduce == \'random\':\n        idxs = torch.autograd.Variable(torch.randperm(anchor.size()[0]).long()).cuda()\n        min_neg = dist_without_min_on_diag.gather(1,idxs.view(-1,1))\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).gather(1,idxs.view(-1,1)) \n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = torch.t(min_neg).squeeze(0)\n        pos = pos1\n    else: \n        print (\'Unknown batch reduce mode. Try min, average or random\')\n        sys.exit(1)\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n'"
OnePassSIR.py,17,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nimport math\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom copy import deepcopy\nfrom Utils import GaussianBlur, batch_eig2x2, line_prepender, batched_forward\nfrom LAF import LAFs2ell,abc2A, angles2A, generate_patch_grid_from_normalized_LAFs, extract_patches, get_inverted_pyr_index, denormalizeLAFs, extract_patches_from_pyramid_with_inv_index, rectifyAffineTransformationUpIsUp\nfrom LAF import get_pyramid_and_level_index_for_LAFs, normalizeLAFs, checkTouchBoundary\nfrom HandCraftedModules import HessianResp, AffineShapeEstimator, OrientationDetector, ScalePyramid, NMS3dAndComposeA,NMS3dAndComposeAAff\nimport time\n\nclass OnePassSIR(nn.Module):\n    def __init__(self, \n                 border = 16,\n                 num_features = 500,\n                 patch_size = 32,\n                 mrSize = 3.0,\n                 nlevels = 3,\n                 th = None,#16.0/ 3.0,\n                 num_Baum_iters = 0,\n                 init_sigma = 1.6,\n                 RespNet = None, OriNet = None, AffNet = None):\n        super(OnePassSIR, self).__init__()\n        self.mrSize = mrSize\n        self.PS = patch_size\n        self.b = border;\n        self.num = num_features\n        self.th = th;\n        if th is not None:\n            self.num = -1\n        else:\n            self.th = 0\n        self.nlevels = nlevels\n        self.num_Baum_iters = num_Baum_iters\n        self.init_sigma = init_sigma\n        if RespNet is not None:\n            self.RespNet = RespNet\n        else:\n            self.RespNet = HessianResp()\n        if OriNet is not None:\n            self.OriNet = OriNet\n        else:\n            self.OriNet= OrientationDetector(patch_size = 19);\n        if AffNet is not None:\n            self.AffNet = AffNet\n        else:\n            self.AffNet = AffineShapeEstimator(patch_size = 19)\n        self.ScalePyrGen = ScalePyramid(nLevels = self.nlevels, init_sigma = self.init_sigma, border = self.b)\n        return\n    \n    def multiScaleDetectorAff(self,x, num_features = 0):\n        t = time.time()\n        self.scale_pyr, self.sigmas, self.pix_dists = self.ScalePyrGen(x)\n        ### Detect keypoints in scale space\n        aff_matrices = []\n        top_responces = []\n        pyr_idxs = []\n        level_idxs = []\n        det_t = 0\n        nmst = 0\n        for oct_idx in range(len(self.sigmas)):\n            #print oct_idx\n            octave = self.scale_pyr[oct_idx]\n            sigmas_oct = self.sigmas[oct_idx]\n            pix_dists_oct = self.pix_dists[oct_idx]\n            low = None\n            cur = None\n            high = None\n            octaveMap = (self.scale_pyr[oct_idx][0] * 0).byte()\n            nms_f = NMS3dAndComposeAAff(w = octave[0].size(3),\n                                     h =  octave[0].size(2),\n                                     border = self.b, mrSize = self.mrSize)\n            #oct_aff_map =  F.upsample(self.AffNet(octave[0]), (octave[0].size(2), octave[0].size(3)),mode='bilinear')\n            oct_aff_map =  self.AffNet(octave[0])\n            for level_idx in range(1, len(octave)-1):\n                if cur is None:\n                    low = torch.clamp(self.RespNet(octave[level_idx - 1], (sigmas_oct[level_idx - 1 ])) - self.th, min = 0)\n                else:\n                    low = cur\n                if high is None:\n                    cur =  torch.clamp(self.RespNet(octave[level_idx ], (sigmas_oct[level_idx ])) - self.th, min = 0)\n                else:\n                    cur = high\n                high = torch.clamp(self.RespNet(octave[level_idx + 1], (sigmas_oct[level_idx + 1 ])) - self.th, min = 0)\n                top_resp, aff_matrix, octaveMap_current  = nms_f(low, cur, high,\n                                                                 num_features = num_features,\n                                                                 octaveMap = octaveMap,\n                                                                 scales = sigmas_oct[level_idx - 1:level_idx + 2],\n                                                                 aff_resp = oct_aff_map)\n                if top_resp is None:\n                    continue\n                octaveMap = octaveMap_current\n                not_touch_boundary_idx = checkTouchBoundary(torch.cat([aff_matrix[:,:2,:2] *3.0, aff_matrix[:,:,2:]], dim =2))\n                aff_matrices.append(aff_matrix[not_touch_boundary_idx.byte()]), top_responces.append(top_resp[not_touch_boundary_idx.byte()])\n                pyr_id = Variable(oct_idx * torch.ones(aff_matrices[-1].size(0)))\n                lev_id = Variable((level_idx - 1) * torch.ones(aff_matrices[-1].size(0))) #prevBlur\n                if x.is_cuda:\n                    pyr_id = pyr_id.cuda()\n                    lev_id = lev_id.cuda()\n                pyr_idxs.append(pyr_id)\n                level_idxs.append(lev_id)\n        all_responses = torch.cat(top_responces, dim = 0)\n        aff_m_scales = torch.cat(aff_matrices,dim = 0)\n        pyr_idxs_scales = torch.cat(pyr_idxs,dim = 0)\n        level_idxs_scale = torch.cat(level_idxs, dim = 0)\n        if (num_features > 0) and (num_features < all_responses.size(0)):\n            all_responses, idxs = torch.topk(all_responses, k = num_features);\n            LAFs = torch.index_select(aff_m_scales, 0, idxs)\n            final_pyr_idxs = pyr_idxs_scales[idxs]\n            final_level_idxs = level_idxs_scale[idxs]\n        else:\n            return all_responses, aff_m_scales, pyr_idxs_scales , level_idxs_scale\n        return all_responses, LAFs, final_pyr_idxs, final_level_idxs,\n\n    def getOrientation(self, LAFs, final_pyr_idxs, final_level_idxs):\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)\n        max_iters = 1\n        ### Detect orientation\n        for i in range(max_iters):\n            angles = self.OriNet(patches_small)\n            if len(angles.size()) > 2:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles), LAFs[:,:,2:]], dim = 2)\n            else:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles2A(angles).view(-1,2,2)), LAFs[:,:,2:]], dim = 2)\n            if i != max_iters-1:\n                patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)        \n        return LAFs\n    def extract_patches_from_pyr(self, dLAFs, PS = 41):\n        pyr_idxs, level_idxs = get_pyramid_and_level_index_for_LAFs(dLAFs, self.sigmas, self.pix_dists, PS)\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, pyr_idxs, level_idxs)\n        patches = extract_patches_from_pyramid_with_inv_index(self.scale_pyr,\n                                                      pyr_inv_idxs,\n                                                      normalizeLAFs(dLAFs, self.scale_pyr[0][0].size(3), self.scale_pyr[0][0].size(2)), \n                                                      PS = PS)\n        return patches\n    def forward(self,x, do_ori = True):\n        ### Detection\n        t = time.time()\n        num_features_prefilter = self.num\n        responses, LAFs, final_pyr_idxs, final_level_idxs = self.multiScaleDetectorAff(x,num_features_prefilter)\n        print time.time() - t, 'detection multiscale'\n        t = time.time()\n        LAFs[:,0:2,0:2] =   self.mrSize * LAFs[:,:,0:2]\n        if do_ori:\n            LAFs = self.getOrientation(LAFs, final_pyr_idxs, final_level_idxs)\n        #pyr_inv_idxs = get_inverted_pyr_index(scale_pyr, final_pyr_idxs, final_level_idxs)\n        #patches = extract_patches_from_pyramid_with_inv_index(scale_pyr, pyr_inv_idxs, LAFs, PS = self.PS)\n        #patches = extract_patches(x, LAFs, PS = self.PS)\n        #print time.time() - t, len(LAFs), ' patches extraction'\n        return denormalizeLAFs(LAFs, x.size(3), x.size(2)), responses\n"""
ReprojectionStuff.py,66,"b'import torch\nfrom torch.autograd import Variable\nfrom torch.autograd import Variable as V\nimport numpy as np\nfrom LAF import rectifyAffineTransformationUpIsUp, LAFs_to_H_frames\nfrom Utils import zeros_like\n\n\ndef linH(H, x, y):\n    assert x.size(0) == y.size(0)\n    A = torch.zeros(x.size(0),2,2)\n    if x.is_cuda:\n        A = A.cuda()\n    den = x * H[2,0] + y * H[2,1] + H[2,2]\n    num1_densq = (x*H[0,0] + y*H[0,1] + H[0,2]) / (den*den)\n    num2_densq = (x*H[1,0] + y*H[1,1] + H[1,2]) / (den*den)\n    A[:,0,0] = H[0,0]/den - num1_densq * H[2,0]\n    A[:,0,1] = H[0,1]/den - num1_densq * H[2,1]\n    A[:,1,0] = H[1,0]/den - num2_densq * H[2,0]\n    A[:,1,1] = H[1,1]/den - num2_densq * H[2,1]\n    return A\n\ndef reprojectLAFs(LAFs1, H1to2, return_LHFs = False):\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    #LHF1_in_2 = torch.zeros(LHF1.size(0), ,3,3)\n    #if LHF1.is_cuda:\n    #    LHF1_in_2 = LHF1_in_2.cuda()\n    #LHF1_in_2 = Variable(LHF1_in_2)\n    #LHF1_in_2[:,:,2] = torch.bmm(H1to2.expand(LHF1.size(0),3,3), LHF1[:,:,2:])\n    #LHF1_in_2[:,:,2] = LHF1_in_2[:,:,2] / LHF1_in_2[:,2:,2].expand(LHF1_in_2.size(0), 3)\n    #As  = linH(H1to2, LAFs1[:,0,2], LAFs1[:,1,2])\n    #LHF1_in_2[:,0:2,0:2] = torch.bmm(As, LHF1[:,0:2,0:2])\n    xy1 = torch.bmm(H1to2.expand(LHF1.size(0),3,3), LHF1[:,:,2:])\n    xy1 = xy1 / xy1[:,2:,:].expand(xy1.size(0), 3, 1)\n    As  = linH(H1to2, LAFs1[:,0,2], LAFs1[:,1,2])\n    AF = torch.bmm(As, LHF1[:,0:2,0:2])\n    \n    if return_LHFs:\n        return LAFs_to_H_frames(torch.cat([AF, xy1[:,:2,:]], dim = 2))\n    return torch.cat([AF, xy1[:,:2,:]], dim = 2)\n\ndef Px2GridA(w, h):\n    A = torch.eye(3)\n    A[0,0] = 2.0  / float(w)\n    A[1,1] = 2.0  / float(h)\n    A[0,2] = -1\n    A[1,2] = -1\n    return A\ndef Grid2PxA(w, h):\n    A = torch.eye(3)\n    A[0,0] = float(w) / 2.0\n    A[0,2] = float(w) / 2.0\n    A[1,1] = float(h) / 2.0\n    A[1,2] = float(h) / 2.0\n    return A\n\ndef affineAug(img, max_add = 0.5):\n    img_s = img.squeeze()\n    h,w = img_s.size()\n    ### Generate A\n    A = torch.eye(3)\n    rand_add = max_add *(torch.rand(3,3) - 0.5) * 2.0\n    ##No perspective change\n    rand_add[2,0:2] = 0\n    rand_add[2,2] = 0;\n    A  = A + rand_add\n    denormA = Grid2PxA(w,h)\n    normA = Px2GridA(w, h)\n    if img.is_cuda:\n        A = A.cuda()\n        denormA = denormA.cuda()\n        normA = normA.cuda()\n    grid = torch.nn.functional.affine_grid(A[0:2,:].unsqueeze(0), torch.Size((1,1,h,w)))\n    H_Orig2New = torch.mm(torch.mm(denormA, torch.inverse(A)), normA)\n    new_img = torch.nn.functional.grid_sample(img_s.float().unsqueeze(0).unsqueeze(0),  grid)  \n    return new_img, H_Orig2New, \n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1)\n    d2_sq = torch.sum(positive * positive, dim=1)\n    eps = 1e-12\n    return torch.sqrt(torch.abs((d1_sq.expand(positive.size(0), anchor.size(0)) +\n                       torch.t(d2_sq.expand(anchor.size(0), positive.size(0)))\n                      - 2.0 * torch.bmm(positive.unsqueeze(0), torch.t(anchor).unsqueeze(0)).squeeze(0))+eps))\n\ndef ratio_matrix_vector(a, p):\n    eps = 1e-12\n    return a.expand(p.size(0), a.size(0)) / (torch.t(p.expand(a.size(0), p.size(0))) + eps)\n\n   \ndef inverseLHFs(LHFs):\n    LHF1_inv =torch.zeros(LHFs.size())\n    if LHFs.is_cuda:\n        LHF1_inv = LHF1_inv.cuda()\n    for i in range(LHF1_inv.size(0)):\n        LHF1_inv[i,:,:] = LHFs[i,:,:].inverse()\n    return LHF1_inv\n\n    \ndef reproject_to_canonical_Frob_batched(LHF1_inv, LHF2, batch_size = 2, skip_center = False):\n    out = torch.zeros((LHF1_inv.size(0), LHF2.size(0)))\n    eye1 = torch.eye(3)\n    if LHF1_inv.is_cuda:\n        out = out.cuda()\n        eye1 = eye1.cuda()\n    len1 = LHF1_inv.size(0)\n    len2 = LHF2.size(0)\n    n_batches = int(np.floor(len1 / batch_size) + 1);\n    for b_idx in range(n_batches):\n        #print b_idx\n        start = b_idx * batch_size;\n        fin = min((b_idx+1) * batch_size, len1)\n        current_bs = fin - start\n        if current_bs == 0:\n            break\n        should_be_eyes = torch.bmm(LHF1_inv[start:fin, :, :].unsqueeze(0).expand(len2,current_bs, 3, 3).contiguous().view(-1,3,3),\n                                   LHF2.unsqueeze(1).expand(len2,current_bs, 3,3).contiguous().view(-1,3,3))\n        if skip_center:\n            out[start:fin, :] = torch.sum(((should_be_eyes - eye1.unsqueeze(0).expand_as(should_be_eyes))**2)[:,:2,:2] , dim=1).sum(dim = 1).view(current_bs, len2)\n        else:\n            out[start:fin, :] = torch.sum((should_be_eyes - eye1.unsqueeze(0).expand_as(should_be_eyes))**2 , dim=1).sum(dim = 1).view(current_bs, len2)\n    return out\n\ndef get_GT_correspondence_indexes(LAFs1, LAFs2, H1to2, dist_threshold = 4):    \n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    just_centers1 = LAFs1[:,:,2];\n    just_centers2_repr_to_1 = LHF2_in_1_pre[:,0:2,2];\n    \n    dist  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1)\n    min_dist, idxs_in_2 = torch.min(dist,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    mask =  min_dist <= dist_threshold\n    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n\ndef get_GT_correspondence_indexes_Fro(LAFs1,LAFs2, H1to2, dist_threshold = 4,\n                                      skip_center_in_Fro = False):\n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1_inv = inverseLHFs(LAFs_to_H_frames(LAFs1))\n    frob_norm_dist = reproject_to_canonical_Frob_batched(LHF1_inv, LHF2_in_1_pre, batch_size = 2, skip_center = skip_center_in_Fro)\n    min_dist, idxs_in_2 = torch.min(frob_norm_dist,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    #print min_dist.min(), min_dist.max(), min_dist.mean()\n    mask =  min_dist <= dist_threshold\n    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n\ndef get_GT_correspondence_indexes_Fro_and_center(LAFs1,LAFs2, H1to2, \n                                                 dist_threshold = 4, \n                                                 center_dist_th = 2.0,\n                                                 scale_diff_coef = 0.3,\n                                                 skip_center_in_Fro = False,\n                                                 do_up_is_up = False,\n                                                 return_LAF2_in_1 = False,\n                                                 inv_to_eye = True):\n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    if do_up_is_up:\n        sc2 = torch.sqrt(torch.abs(LHF2_in_1_pre[:,0,0] * LHF2_in_1_pre[:,1,1] - LHF2_in_1_pre[:,1,0] * LHF2_in_1_pre[:,0,1])).unsqueeze(-1).unsqueeze(-1).expand(LHF2_in_1_pre.size(0), 2,2)\n        LHF2_in_1 = torch.zeros(LHF2_in_1_pre.size())\n        if LHF2_in_1_pre.is_cuda:\n            LHF2_in_1 = LHF2_in_1.cuda()\n        LHF2_in_1[:, :2,:2] = rectifyAffineTransformationUpIsUp(LHF2_in_1_pre[:, :2,:2]/sc2) * sc2\n        LHF2_in_1[:,:, 2] = LHF2_in_1_pre[:,:,2]\n        sc1 = torch.sqrt(torch.abs(LAFs1[:,0,0] * LAFs1[:,1,1] - LAFs1[:,1,0] * LAFs1[:,0,1])).unsqueeze(-1).unsqueeze(-1).expand(LAFs1.size(0), 2,2)\n        LHF1 = LAFs_to_H_frames(torch.cat([rectifyAffineTransformationUpIsUp(LAFs1[:, :2,:2]/sc1) * sc1, LAFs1[:,:,2:]], dim = 2 ))\n    else:\n        LHF2_in_1 = LHF2_in_1_pre\n        LHF1 = LAFs_to_H_frames(LAFs1)\n    if inv_to_eye:\n        LHF1_inv = inverseLHFs(LHF1)\n        frob_norm_dist = reproject_to_canonical_Frob_batched(LHF1_inv, LHF2_in_1, batch_size = 2, skip_center = skip_center_in_Fro)\n    else:\n        if not skip_center_in_Fro:\n            frob_norm_dist = distance_matrix_vector(LHF2_in_1.view(LHF2_in_1.size(0), -1), LHF1.view(LHF1.size(0),-1))\n        else:\n            frob_norm_dist = distance_matrix_vector(LHF2_in_1[:,0:2, 0:2].contiguous().view(LHF2_in_1.size(0), -1), LHF1[:,0:2,0:2].contiguous().view(LHF1.size(0),-1))\n    #### Center replated\n    just_centers1 = LAFs1[:,:,2];\n    just_centers2_repr_to_1 = LHF2_in_1[:,0:2,2];\n    if scale_diff_coef > 0:\n        scales1 = torch.sqrt(torch.abs(LAFs1[:,0,0] * LAFs1[:,1,1] - LAFs1[:,1,0] * LAFs1[:,0,1]))\n        scales2 = torch.sqrt(torch.abs(LHF2_in_1[:,0,0] * LHF2_in_1[:,1,1] - LHF2_in_1[:,1,0] * LHF2_in_1[:,0,1]))\n        scale_matrix = ratio_matrix_vector(scales2, scales1)\n        scale_dist_mask = (torch.abs(1.0 - scale_matrix) <= scale_diff_coef) \n    center_dist_mask  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1) >= center_dist_th\n    frob_norm_dist_masked = (1.0 - scale_dist_mask.float() + center_dist_mask.float()) * 1000. + frob_norm_dist;\n    \n    min_dist, idxs_in_2 = torch.min(frob_norm_dist_masked,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    #min_dist, idxs_in_2 = torch.min(dist,1)\n    #print min_dist.min(), min_dist.max(), min_dist.mean()\n    mask =  (min_dist <= dist_threshold )\n    \n    if return_LAF2_in_1:\n        return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask], LHF2_in_1[:,0:2,:]\n    else:\n        return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\ndef get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log):\n    xy1 = LHF1[:,0:2,2];\n    xy2in1 = LHF2_in_1[:,0:2,2];\n    center_dist_matrix =  distance_matrix_vector(xy2in1, xy1)\n    scales1 = torch.sqrt(torch.abs(LHF1[:,0,0] * LHF1[:,1,1] - LHF1[:,1,0] * LHF1[:,0,1]));\n    scales2 = torch.sqrt(torch.abs(LHF2_in_1[:,0,0] * LHF2_in_1[:,1,1] - LHF2_in_1[:,1,0] * LHF2_in_1[:,0,1]));\n    scale_matrix = torch.abs(torch.log(ratio_matrix_vector(scales2, scales1)))\n    mask_matrix = 1000.0*(scale_matrix  > scale_log).float() * (center_dist_matrix > xy_th).float() + center_dist_matrix + scale_matrix\n\n    d2_to_1, nn_idxs_in_2 = torch.min(mask_matrix,1)\n    d1_to_2, nn_idxs_in_1 = torch.min(mask_matrix,0)\n\n    flat_idxs_1 = torch.arange(0, nn_idxs_in_2.size(0));\n    if LHF1.is_cuda:\n        flat_idxs_1 = flat_idxs_1.cuda()\n    mask = d2_to_1 <= 100.0;\n\n    final_mask = (flat_idxs_1 == nn_idxs_in_1[nn_idxs_in_2].float()).float() * mask.float()\n    idxs_in1 = flat_idxs_1[final_mask.long()].nonzero().squeeze()\n    idxs_in_2_final = nn_idxs_in_2[idxs_in1];\n    #torch.arange(0, nn_idxs_in_2.size(0))#[mask2.data]\n    return idxs_in1, idxs_in_2_final\ndef get_LHFScale(LHF):\n    return torch.sqrt(torch.abs(LHF[:,0,0] * LHF[:,1,1] - LHF[:,1,0] * LHF[:,0,1]));\ndef LAFMagic(LAFs1, LAFs2, H1to2, xy_th  = 5.0, scale_log = 0.4, t = 1.0, sc = 1.0, aff = 1.0):\n    LHF2_in_1 = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    idxs_in1, idxs_in_2 = get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log)\n    if len(idxs_in1) == 0:\n        print(\'Warning, no correspondences found\')\n        return None\n    LHF1_good = LHF1[idxs_in1,:,:]\n    LHF2_good = LHF2_in_1[idxs_in_2,:,:]\n    scales1 = get_LHFScale(LHF1_good);\n    scales2 = get_LHFScale(LHF2_good);\n    max_scale = torch.max(scales1,scales2);\n    min_scale = torch.min(scales1, scales2);\n    mean_scale = 0.5 * (max_scale + min_scale)\n    eps = 1e-12;\n    if t != 0:\n        dist_loss = torch.sqrt(torch.sum((LHF1_good[:,0:2,2] - LHF2_good[:,0:2,2])**2, dim = 1) + eps) / V(mean_scale.data);\n    else:\n        dist_loss = 0\n    if sc != 0 :\n        scale_loss = torch.log1p( (max_scale-min_scale)/(mean_scale))\n    else:\n        scale_loss = 0\n    if aff != 0:\n        A1 = LHF1_good[:,:2,:2] / scales1.view(-1,1,1).expand(scales1.size(0),2,2);\n        A2 = LHF2_good[:,:2,:2] / scales2.view(-1,1,1).expand(scales2.size(0),2,2);\n        shape_loss = ((A1 - A2)**2).mean(dim = 1).mean(dim = 1);\n    else:\n        shape_loss = 0;\n    loss = t * dist_loss + sc * scale_loss + aff *shape_loss;\n    #print dist_loss, scale_loss, shape_loss\n    return loss, idxs_in1, idxs_in_2, LHF2_in_1[:,0:2,:]\ndef LAFMagicFro(LAFs1, LAFs2, H1to2, xy_th  = 5.0, scale_log = 0.4):\n    LHF2_in_1 = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    idxs_in1, idxs_in_2 = get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log)\n    if len(idxs_in1) == 0:\n        print(\'Warning, no correspondences found\')\n        return None\n    LHF1_good = LHF1[idxs_in1,:,:]\n    LHF2_good = LHF2_in_1[idxs_in_2,:,:]\n    scales1 = get_LHFScale(LHF1_good);\n    scales2 = get_LHFScale(LHF2_good);\n    max_scale = torch.max(scales1,scales2);\n    min_scale = torch.min(scales1, scales2);\n    mean_scale = 0.5 * (max_scale + min_scale)\n    eps = 1e-12;\n    dist_loss = (torch.sqrt((LHF1_good.view(-1,9) - LHF2_good.view(-1,9))**2 + eps) / V(mean_scale.data).view(-1,1).expand(LHF1_good.size(0),9)).mean(dim=1); \n    loss = dist_loss;\n    #print dist_loss, scale_loss, shape_loss\n    return loss, idxs_in1, idxs_in_2, LHF2_in_1[:,0:2,:]\ndef pr_l(x):\n    return x.mean().data.cpu().numpy()[0]\ndef add_1(A):\n    add = torch.eye(2).unsqueeze(0).expand(A.size(0),2,2)\n    add = torch.cat([add, torch.zeros(A.size(0),2,1)], dim = 2)\n    if A.is_cuda:\n        add = add.cuda()\n    return add\ndef identity_loss(A):\n    return torch.clamp(torch.sqrt((A - add_1(A))**2 + 1e-15).view(-1,6).mean(dim = 1) - 0.3*0, min = 0.0, max = 100.0).mean()\n\n\n\n'"
SparseImgRepresenter.py,27,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nimport math\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom copy import deepcopy\nfrom Utils import GaussianBlur, batch_eig2x2, line_prepender, batched_forward\nfrom LAF import LAFs2ell,abc2A, angles2A, generate_patch_grid_from_normalized_LAFs, extract_patches, get_inverted_pyr_index, denormalizeLAFs, extract_patches_from_pyramid_with_inv_index, rectifyAffineTransformationUpIsUp\nfrom LAF import get_pyramid_and_level_index_for_LAFs, normalizeLAFs, checkTouchBoundary\nfrom HandCraftedModules import HessianResp, AffineShapeEstimator, OrientationDetector, ScalePyramid, NMS3dAndComposeA\nimport time\n\nclass ScaleSpaceAffinePatchExtractor(nn.Module):\n    def __init__(self, \n                 border = 16,\n                 num_features = 500,\n                 patch_size = 32,\n                 mrSize = 3.0,\n                 nlevels = 3,\n                 num_Baum_iters = 0,\n                 init_sigma = 1.6,\n                 th = None,\n                 RespNet = None, OriNet = None, AffNet = None):\n        super(ScaleSpaceAffinePatchExtractor, self).__init__()\n        self.mrSize = mrSize\n        self.PS = patch_size\n        self.b = border;\n        self.num = num_features\n        self.nlevels = nlevels\n        self.num_Baum_iters = num_Baum_iters\n        self.init_sigma = init_sigma\n        self.th = th;\n        if th is not None:\n            self.num = -1\n        else:\n            self.th = 0\n        if RespNet is not None:\n            self.RespNet = RespNet\n        else:\n            self.RespNet = HessianResp()\n        if OriNet is not None:\n            self.OriNet = OriNet\n        else:\n            self.OriNet= OrientationDetector(patch_size = 19);\n        if AffNet is not None:\n            self.AffNet = AffNet\n        else:\n            self.AffNet = AffineShapeEstimator(patch_size = 19)\n        self.ScalePyrGen = ScalePyramid(nLevels = self.nlevels, init_sigma = self.init_sigma, border = self.b)\n        return\n    \n    def multiScaleDetector(self,x, num_features = 0):\n        t = time.time()\n        self.scale_pyr, self.sigmas, self.pix_dists = self.ScalePyrGen(x)\n        ### Detect keypoints in scale space\n        aff_matrices = []\n        top_responces = []\n        pyr_idxs = []\n        level_idxs = []\n        det_t = 0\n        nmst = 0\n        for oct_idx in range(len(self.sigmas)):\n            #print oct_idx\n            octave = self.scale_pyr[oct_idx]\n            sigmas_oct = self.sigmas[oct_idx]\n            pix_dists_oct = self.pix_dists[oct_idx]\n            low = None\n            cur = None\n            high = None\n            octaveMap = (self.scale_pyr[oct_idx][0] * 0).byte()\n            nms_f = NMS3dAndComposeA(w = octave[0].size(3),\n                                     h =  octave[0].size(2),\n                                     border = self.b, mrSize = self.mrSize)\n            for level_idx in range(1, len(octave)-1):\n                if cur is None:\n                    low = torch.clamp(self.RespNet(octave[level_idx - 1], (sigmas_oct[level_idx - 1 ])) - self.th, min = 0)\n                else:\n                    low = cur\n                if high is None:\n                    cur =  torch.clamp(self.RespNet(octave[level_idx ], (sigmas_oct[level_idx ])) - self.th, min = 0)\n                else:\n                    cur = high\n                high = torch.clamp(self.RespNet(octave[level_idx + 1], (sigmas_oct[level_idx + 1 ])) - self.th, min = 0)\n                top_resp, aff_matrix, octaveMap_current  = nms_f(low, cur, high,\n                                                                 num_features = num_features,\n                                                                 octaveMap = octaveMap,\n                                                                 scales = sigmas_oct[level_idx - 1:level_idx + 2])\n                if top_resp is None:\n                    continue\n                octaveMap = octaveMap_current\n                aff_matrices.append(aff_matrix), top_responces.append(top_resp)\n                pyr_id = Variable(oct_idx * torch.ones(aff_matrix.size(0)))\n                lev_id = Variable((level_idx - 1) * torch.ones(aff_matrix.size(0))) #prevBlur\n                if x.is_cuda:\n                    pyr_id = pyr_id.cuda()\n                    lev_id = lev_id.cuda()\n                pyr_idxs.append(pyr_id)\n                level_idxs.append(lev_id)\n        all_responses = torch.cat(top_responces, dim = 0)\n        aff_m_scales = torch.cat(aff_matrices,dim = 0)\n        pyr_idxs_scales = torch.cat(pyr_idxs,dim = 0)\n        level_idxs_scale = torch.cat(level_idxs, dim = 0)\n        if (num_features > 0) and (num_features < all_responses.size(0)):\n            all_responses, idxs = torch.topk(all_responses, k = num_features);\n            LAFs = torch.index_select(aff_m_scales, 0, idxs)\n            final_pyr_idxs = pyr_idxs_scales[idxs]\n            final_level_idxs = level_idxs_scale[idxs]\n        else:\n            return all_responses, aff_m_scales, pyr_idxs_scales , level_idxs_scale\n        return all_responses, LAFs, final_pyr_idxs, final_level_idxs,\n    \n    def getAffineShape(self, final_resp, LAFs, final_pyr_idxs, final_level_idxs, num_features = 0):\n        pe_time = 0\n        affnet_time = 0\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        t = time.time()\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.AffNet.PS)\n        pe_time+=time.time() - t\n        t = time.time()\n        base_A = torch.eye(2).unsqueeze(0).expand(final_pyr_idxs.size(0),2,2)\n        if final_resp.is_cuda:\n            base_A = base_A.cuda()\n        base_A = Variable(base_A)\n        is_good = None\n        n_patches = patches_small.size(0)\n        for i in range(self.num_Baum_iters):\n            t = time.time()\n            A = batched_forward(self.AffNet, patches_small, 256)\n            is_good_current = 1\n            affnet_time += time.time() - t\n            if is_good is None:\n                is_good = is_good_current\n            else:\n                is_good = is_good * is_good_current\n            base_A = torch.bmm(A, base_A); \n            new_LAFs = torch.cat([torch.bmm(base_A,LAFs[:,:,0:2]), LAFs[:,:,2:] ], dim =2)\n            #print torch.sqrt(new_LAFs[0,0,0]*new_LAFs[0,1,1] - new_LAFs[0,1,0] *new_LAFs[0,0,1]) * scale_pyr[0][0].size(2)\n            if i != self.num_Baum_iters - 1:\n                pe_time+=time.time() - t\n                t = time.time()\n                patches_small =  extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, new_LAFs, PS = self.AffNet.PS)\n                pe_time+= time.time() - t\n                l1,l2 = batch_eig2x2(A)      \n                ratio1 =  torch.abs(l1 / (l2 + 1e-8))\n                converged_mask = (ratio1 <= 1.2) * (ratio1 >= (0.8)) \n        l1,l2 = batch_eig2x2(base_A)\n        ratio = torch.abs(l1 / (l2 + 1e-8))\n        idxs_mask =  ((ratio < 6.0) * (ratio > (1./6.))) * checkTouchBoundary(new_LAFs)\n        num_survived = idxs_mask.float().sum()\n        if (num_features > 0) and (num_survived.data.item() > num_features):\n            final_resp =  final_resp * idxs_mask.float() #zero bad points\n            final_resp, idxs = torch.topk(final_resp, k = num_features);\n        else:\n            idxs = Variable(torch.nonzero(idxs_mask.data).view(-1).long())\n            final_resp = final_resp[idxs]\n        final_pyr_idxs = final_pyr_idxs[idxs]\n        final_level_idxs = final_level_idxs[idxs]\n        base_A = torch.index_select(base_A, 0, idxs)\n        LAFs = torch.index_select(LAFs, 0, idxs)\n        new_LAFs = torch.cat([torch.bmm(base_A, LAFs[:,:,0:2]),\n                               LAFs[:,:,2:]], dim =2)\n        print ('affnet_time',affnet_time)\n        print ('pe_time', pe_time)\n        return final_resp, new_LAFs, final_pyr_idxs, final_level_idxs  \n    \n    def getOrientation(self, LAFs, final_pyr_idxs, final_level_idxs):\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)\n        max_iters = 1\n        ### Detect orientation\n        for i in range(max_iters):\n            angles = self.OriNet(patches_small)\n            if len(angles.size()) > 2:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles), LAFs[:,:,2:]], dim = 2)\n            else:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles2A(angles).view(-1,2,2)), LAFs[:,:,2:]], dim = 2)\n            if i != max_iters:\n                patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)        \n        return LAFs\n    def extract_patches_from_pyr(self, dLAFs, PS = 41):\n        pyr_idxs, level_idxs = get_pyramid_and_level_index_for_LAFs(dLAFs, self.sigmas, self.pix_dists, PS)\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, pyr_idxs, level_idxs)\n        patches = extract_patches_from_pyramid_with_inv_index(self.scale_pyr,\n                                                      pyr_inv_idxs,\n                                                      normalizeLAFs(dLAFs, self.scale_pyr[0][0].size(3), self.scale_pyr[0][0].size(2)), \n                                                      PS = PS)\n        return patches\n    def forward(self,x, do_ori = False):\n        ### Detection\n        t = time.time()\n        num_features_prefilter = self.num\n        if self.num_Baum_iters > 0:\n            num_features_prefilter = int(1.5 * self.num);\n        responses, LAFs, final_pyr_idxs, final_level_idxs = self.multiScaleDetector(x,num_features_prefilter)\n        print (time.time() - t, 'detection multiscale')\n        t = time.time()\n        LAFs[:,0:2,0:2] =   self.mrSize * LAFs[:,:,0:2]\n        if self.num_Baum_iters > 0:\n            responses, LAFs, final_pyr_idxs, final_level_idxs  = self.getAffineShape(responses, LAFs, final_pyr_idxs, final_level_idxs, self.num)\n        print (time.time() - t, 'affine shape iters')\n        t = time.time()\n        if do_ori:\n            LAFs = self.getOrientation(LAFs, final_pyr_idxs, final_level_idxs)\n            #pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        #patches = extract_patches_from_pyramid_with_inv_index(scale_pyr, pyr_inv_idxs, LAFs, PS = self.PS)\n        #patches = extract_patches(x, LAFs, PS = self.PS)\n        #print time.time() - t, len(LAFs), ' patches extraction'\n        return denormalizeLAFs(LAFs, x.size(3), x.size(2)), responses\n"""
Utils.py,25,"b'import torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport cv2\nimport numpy as np\n\n# resize image to size 32x32\ncv2_scale = lambda x: cv2.resize(x, dsize=(32, 32),\n                                 interpolation=cv2.INTER_LINEAR)\n# reshape image\nnp_reshape32 = lambda x: np.reshape(x, (32, 32, 1))\nnp_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n\ndef zeros_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.zeros(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.zeros(y)\n\ndef ones_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.ones(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.ones(y)\n    \n\ndef batched_forward(model, data, batch_size, **kwargs):\n    n_patches = len(data)\n    if n_patches > batch_size:\n        bs = batch_size\n        n_batches = int(n_patches / bs + 1)\n        for batch_idx in range(n_batches):\n            st = batch_idx * bs\n            if batch_idx == n_batches - 1:\n                if (batch_idx + 1) * bs > n_patches:\n                    end = n_patches\n                else:\n                    end = (batch_idx + 1) * bs\n            else:\n                end = (batch_idx + 1) * bs\n            if st >= end:\n                continue\n            if batch_idx == 0:\n                first_batch_out = model(data[st:end], kwargs)\n                out_size = torch.Size([n_patches] + list(first_batch_out.size()[1:]))\n                #out_size[0] = n_patches\n                out = torch.zeros(out_size);\n                if data.is_cuda:\n                    out = out.cuda()\n                out = Variable(out)\n                out[st:end] = first_batch_out\n            else:\n                out[st:end,:,:] = model(data[st:end], kwargs)\n        return out\n    else:\n        return model(data, kwargs)\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n\ndef CircularGaussKernel(kernlen=None, circ_zeros = False, sigma = None, norm = True):\n    assert ((kernlen is not None) or sigma is not None)\n    if kernlen is None:\n        kernlen = int(2.0 * 3.0 * sigma + 1.0)\n        if (kernlen % 2 == 0):\n            kernlen = kernlen + 1;\n        halfSize = kernlen / 2;\n    halfSize = kernlen / 2;\n    r2 = float(halfSize*halfSize)\n    if sigma is None:\n        sigma2 = 0.9 * r2;\n        sigma = np.sqrt(sigma2)\n    else:\n        sigma2 = 2.0 * sigma * sigma    \n    x = np.linspace(-halfSize,halfSize,kernlen)\n    xv, yv = np.meshgrid(x, x, sparse=False, indexing=\'xy\')\n    distsq = (xv)**2 + (yv)**2\n    kernel = np.exp(-( distsq/ (sigma2)))\n    if circ_zeros:\n        kernel *= (distsq <= r2).astype(np.float32)\n    if norm:\n        kernel /= np.sum(kernel)\n    return kernel\n\ndef generate_2dgrid(h,w, centered = True):\n    if centered:\n        x = torch.linspace(-w/2+1, w/2, w)\n        y = torch.linspace(-h/2+1, h/2, h)\n    else:\n        x = torch.linspace(0, w-1, w)\n        y = torch.linspace(0, h-1, h)\n    grid2d = torch.stack([y.repeat(w,1).t().contiguous().view(-1), x.repeat(h)],1)\n    return grid2d\n\ndef generate_3dgrid(d, h, w, centered = True):\n    if type(d) is not list:\n        if centered:\n            z = torch.linspace(-d/2+1, d/2, d)\n        else:\n            z = torch.linspace(0, d-1, d)\n        dl = d\n    else:\n        z = torch.FloatTensor(d)\n        dl = len(d)\n    grid2d = generate_2dgrid(h,w, centered = centered)\n    grid3d = torch.cat([z.repeat(w*h,1).t().contiguous().view(-1,1), grid2d.repeat(dl,1)],dim = 1)\n    return grid3d\n\ndef zero_response_at_border(x, b):\n    if (b < x.size(3)) and (b < x.size(2)):\n        x[:, :,  0:b, :] =  0\n        x[:, :,  x.size(2) - b: , :] =  0\n        x[:, :, :,  0:b] =  0\n        x[:, :, :,   x.size(3) - b: ] =  0\n    else:\n        return x * 0\n    return x\n\nclass GaussianBlur(nn.Module):\n    def __init__(self, sigma=1.6):\n        super(GaussianBlur, self).__init__()\n        weight = self.calculate_weights(sigma)\n        self.register_buffer(\'buf\', weight)\n        return\n    def calculate_weights(self,  sigma):\n        kernel = CircularGaussKernel(sigma = sigma, circ_zeros = False)\n        h,w = kernel.shape\n        halfSize = float(h) / 2.;\n        self.pad = int(np.floor(halfSize))\n        return torch.from_numpy(kernel.astype(np.float32)).view(1,1,h,w);\n    def forward(self, x):\n        w = Variable(self.buf)\n        if x.is_cuda:\n            w = w.cuda()\n        return F.conv2d(F.pad(x, (self.pad,self.pad,self.pad,self.pad), \'replicate\'), w, padding = 0)\n\ndef batch_eig2x2(A):\n    trace = A[:,0,0] + A[:,1,1]\n    delta1 = (trace*trace - 4 * ( A[:,0,0]*  A[:,1,1] -  A[:,1,0]* A[:,0,1]))\n    mask = delta1 > 0\n    delta = torch.sqrt(torch.abs(delta1))\n    l1 = mask.float() * (trace + delta) / 2.0 +  1000.  * (1.0 - mask.float())\n    l2 = mask.float() * (trace - delta) / 2.0 +  0.0001  * (1.0 - mask.float())\n    return l1,l2\n\ndef line_prepender(filename, line):\n    with open(filename, \'r+\') as f:\n        content = f.read()\n        f.seek(0, 0)\n        f.write(line.rstrip(\'\\r\\n\') + \'\\n\' + content)\n    return\n'"
architectures.py,81,"b""from __future__ import division, print_function\nimport os\nimport errno\nimport numpy as np\nimport sys\nfrom copy import deepcopy\nimport math\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom Utils import L2Norm, generate_2dgrid\nfrom Utils import str2bool\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A, extract_patches,normalizeLAFs,  get_rotation_matrix\nfrom LAF import get_LAFs_scales, get_normalized_affine_shape\nfrom LAF import rectifyAffineTransformationUpIsUp,rectifyAffineTransformationUpIsUpFullyConv\n\nclass LocalNorm2d(nn.Module):\n    def __init__(self, kernel_size = 33):\n        super(LocalNorm2d, self).__init__()\n        self.ks = kernel_size\n        self.pool = nn.AvgPool2d(kernel_size = self.ks, stride = 1,  padding = 0)\n        self.eps = 1e-10\n        return\n    def forward(self,x):\n        pd = int(self.ks/2)\n        mean = self.pool(F.pad(x, (pd,pd,pd,pd), 'reflect'))\n        return torch.clamp((x - mean) / (torch.sqrt(torch.abs(self.pool(F.pad(x*x,  (pd,pd,pd,pd), 'reflect')) - mean*mean )) + self.eps), min = -6.0, max = 6.0)\n\nclass OriNetFast(nn.Module):\n    def __init__(self, PS = 16):\n        super(OriNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 2, kernel_size=int(PS/4), stride=1,padding=1, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/4)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.9)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_rot_matrix = True):\n        xy = self.features(self.input_norm(input)).view(-1,2) \n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(angle)\n        return angle\n\nclass GHH(nn.Module):\n    def __init__(self, n_in, n_out, s = 4, m = 4):\n        super(GHH, self).__init__()\n        self.n_out = n_out\n        self.s = s\n        self.m = m\n        self.conv = nn.Linear(n_in, n_out * s * m)\n        d = torch.arange(0, s)\n        self.deltas = -1.0 * (d % 2 != 0).float()  + 1.0 * (d % 2 == 0).float()\n        self.deltas = Variable(self.deltas)\n        return\n    def forward(self,x):\n        x_feats = self.conv(x.view(x.size(0),-1)).view(x.size(0), self.n_out, self.s, self.m);\n        max_feats = x_feats.max(dim = 3)[0];\n        if x.is_cuda:\n            self.deltas = self.deltas.cuda()\n        else:\n            self.deltas = self.deltas.cpu()\n        out =  (max_feats * self.deltas.view(1,1,-1).expand_as(max_feats)).sum(dim = 2)\n        return out\n\nclass YiNet(nn.Module):\n    def __init__(self, PS = 28):\n        super(YiNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 10, kernel_size=5, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding = 1),\n            nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=2, padding = 2),\n            nn.Conv2d(20, 50, kernel_size=3, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.AdaptiveMaxPool2d(1),\n            GHH(50, 100),\n            GHH(100, 2)\n        )\n        self.input_mean = 0.427117081207483\n        self.input_std = 0.21888339179665006;\n        self.PS = PS\n        return\n    def import_weights(self, dir_name):\n        self.features[0].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_W.npy'))).float()\n        self.features[0].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_b.npy'))).float().view(-1)\n        self.features[3].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_W.npy'))).float()\n        self.features[3].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_b.npy'))).float().view(-1)\n        self.features[6].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_W.npy'))).float()\n        self.features[6].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_b.npy'))).float().view(-1)\n        self.features[9].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_W.npy'))).float().view(50, 1600).contiguous().t().contiguous()\n        self.features[9].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_b.npy'))).float().view(1600)\n        self.features[10].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_W.npy'))).float().view(100, 32).contiguous().t().contiguous()\n        self.features[10].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_b.npy'))).float().view(32)\n        self.input_mean = float(np.load(os.path.join(dir_name, 'input_mean.npy')))\n        self.input_std = float(np.load(os.path.join(dir_name, 'input_std.npy')))\n        return\n    def input_norm1(self,x):\n        return (x - self.input_mean) / self.input_std\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def forward(self, input, return_rot_matrix = False):\n        xy = self.features(self.input_norm(input))\n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(-angle)\n        return angle\nclass AffNetFast4(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0,0,1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n        return rectifyAffineTransformationUpIsUp(xy).contiguous()\n\n    \nclass AffNetFast(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,3)\n        a1 = torch.cat([1.0 + xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), 1.0 + xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        return rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n\nclass AffNetFast52RotUp(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52RotUp, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0, 1, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, rectifyAffineTransformationUpIsUp(torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1)).contiguous())\n\nclass AffNetFast52Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Tanh()\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0, 0.8, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFast5Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast5Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0, 1, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        rot = get_rotation_matrix(torch.atan2(x[:,3], x[:,4]+1e-8))\n        if input.is_cuda:\n            return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), torch.zeros(x.size(0),1,1).cuda()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n        else:\n            return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), torch.zeros(x.size(0),1,1)], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFast4Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Tanh()\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0,0,0.8])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        return self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n\nclass AffNetFast4RotNosc(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4RotNosc, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0,0,1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        A = self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n        scale =  torch.sqrt(torch.abs(A[:,0,0]*A[:,1,1] - A[:,1,0]*A[:,0,1] + 1e-10))\n        return A / (scale.view(-1,1,1).repeat(1,2,2) + 1e-8)\n\nclass AffNetFastScale(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFastScale, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,4)\n        a1 = torch.cat([1.0 + xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), 1.0 + xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        scale = torch.exp(xy[:,3].contiguous().view(-1,1,1).repeat(1,2,2))\n        return scale * rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n\nclass AffNetFast2Par(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast2Par, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0, 0, 1 ])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,3)\n        angle = torch.atan2(x[:,1], x[:,2]+1e-8);\n        rot = get_rotation_matrix(angle)\n        tilt = torch.exp(1.8 * F.tanh(x[:,0]))\n        tilt_matrix = torch.eye(2).unsqueeze(0).repeat(input.size(0),1,1)\n        if x.is_cuda:\n            tilt_matrix = tilt_matrix.cuda()\n        tilt_matrix[:,0,0] = torch.sqrt(tilt)\n        tilt_matrix[:,1,1] = 1.0 / torch.sqrt(tilt)\n        return rectifyAffineTransformationUpIsUp(torch.bmm(rot, tilt_matrix)).contiguous()\n\nclass AffNetFastFullConv(nn.Module):\n    def __init__(self, PS = 32, stride = 2):\n        super(AffNetFastFullConv, self).__init__()\n        self.lrn = LocalNorm2d(33)\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=stride, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=stride, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding = 0, bias = True),\n        )\n        self.stride = stride\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        norm_inp  = self.lrn(input)\n        ff = self.features(F.pad(norm_inp, (14,14,14,14), 'reflect'))\n        xy = F.tanh(F.upsample(ff, (input.size(2), input.size(3)),mode='bilinear'))\n        a0bc = torch.cat([1.0 + xy[:,0:1,:,:].contiguous(), 0*xy[:,1:2,:,:].contiguous(),\n                          xy[:,1:2,:,:].contiguous(),  1.0 + xy[:,2:,:,:].contiguous()], dim = 1).contiguous()\n        return rectifyAffineTransformationUpIsUpFullyConv(a0bc).contiguous()\n    \nclass AffNetFast52RotL(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52RotL, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0, 0.8, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFastBias(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFastBias, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8, 0, 0.8 ])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,3)\n        a1 = torch.cat([xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        return rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n"""
augmentation.py,17,"b'import numpy as np\nfrom PIL import Image\n\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport math\nimport torch.utils.data as data\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom LAF import get_rotation_matrix,get_normalized_affine_shape \n\ndef get_random_rotation_LAFs(patches, angle_mag = math.pi):\n    rot_LAFs = Variable(torch.FloatTensor([[0.5, 0, 0.5],[0, 0.5, 0.5]]).unsqueeze(0).repeat(patches.size(0),1,1));\n    phi  = (Variable(2.0 * torch.rand(patches.size(0)) - 1.0) ).view(-1,1,1)\n    if patches.is_cuda:\n        rot_LAFs = rot_LAFs.cuda()\n        phi = phi.cuda()\n    rotmat = get_rotation_matrix(angle_mag * phi)\n    inv_rotmat = get_rotation_matrix(-angle_mag * phi)\n    rot_LAFs[:,0:2,0:2]  = torch.bmm(rotmat, rot_LAFs[:,0:2,0:2]);\n    return rot_LAFs, inv_rotmat\n\ndef get_random_shifts_LAFs(patches, w_mag, h_mag = 3):\n    shift_w =  (torch.IntTensor(patches.size(0)).random_(2*w_mag) - w_mag / 2).float() / 2.0\n    shift_h =  (torch.IntTensor(patches.size(0)).random_(2*w_mag) - w_mag / 2).float() / 2.0\n    if patches.is_cuda:\n        shift_h = shift_h.cuda()\n        shift_w = shift_w.cuda()\n    shift_h = Variable(shift_h)\n    shift_w = Variable(shift_w)\n    return shift_w, shift_h\n\ndef get_random_norm_affine_LAFs(patches, max_tilt = 1.0):\n    assert max_tilt > 0\n    aff_LAFs = Variable(torch.FloatTensor([[0.5, 0, 0.5],[0, 0.5, 0.5]]).unsqueeze(0).repeat(patches.size(0),1,1));\n    tilt = Variable( 1/max_tilt + (max_tilt - 1./max_tilt)* torch.rand(patches.size(0), 1, 1));\n    phi  = math.pi * (Variable(2.0 * torch.rand(patches.size(0)) - 1.0) ).view(-1,1,1)\n    if patches.is_cuda:\n        tilt = tilt.cuda()\n        phi = phi.cuda()\n        aff_LAFs = aff_LAFs.cuda()\n    TA = get_normalized_affine_shape(tilt, phi)\n    #inv_TA = Variable(torch.zeros(patches.size(0),2,2));\n    #if patches.is_cuda:\n    #    inv_TA = inv_TA.cuda()\n    #for i in range(len(inv_TA)):\n    #    inv_TA[i,:,:] = TA[i,:,:].inverse();\n    aff_LAFs[:,0:2,0:2]  = torch.bmm(TA, aff_LAFs[:,0:2,0:2]);\n    return aff_LAFs, None#inv_TA;\n\n'"
dataset.py,28,"b'# Training settings\nimport os\nimport errno\nimport numpy as np\nfrom PIL import Image\nimport torchvision.datasets as dset\n\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport math\nimport torch.utils.data as data\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nimport random\nimport cv2\nimport copy\nfrom Utils import str2bool\n\ndef find_files(_data_dir, _image_ext):\n    """"""Return a list with the file names of the images containing the patches\n    """"""\n    files = []\n    # find those files with the specified extension\n    for file_dir in os.listdir(_data_dir):\n        if file_dir.endswith(_image_ext):\n            files.append(os.path.join(_data_dir, file_dir))\n    return sorted(files)  # sort files in ascend order to keep relations\ndef np2torch(npr):\n    if len(npr.shape) == 4:\n        return torch.from_numpy(np.rollaxis(npr, 3, 1))\n    elif len(npr.shape) == 3:\n        torch.from_numpy(np.rollaxis(npr, 2, 0))\n    else:\n        return torch.from_numpy(npr)\ndef read_patch_file(fname, patch_w = 65, patch_h = 65, start_patch_idx = 0):\n    img = Image.open(fname).convert(\'RGB\')\n    width, height = img.size\n    #print (img.size, patch_w, patch_h)\n    assert ((height % patch_h == 0) and (width % patch_w == 0))\n    patch_idxs = []\n    patches = []\n    current_patch_idx = start_patch_idx\n    for y in range(0, height, patch_h):\n        patch_idxs.append([])\n        curr_patches = []\n        for x in range(0, width, patch_w):\n            patch = np.array(img.crop((x, y, x + patch_w, y + patch_h))).mean(axis = 2, keepdims = True)\n            #print(patch.astype(np.float32).std(), patch.mean())\n            if (patch.mean() != 0) and (patch.astype(np.float32).std() > 1e-2):\n                curr_patches.append(patch.astype(np.uint8))\n                patch_idxs[-1].append(current_patch_idx)\n                current_patch_idx+=1\n        if len(curr_patches) > 1:\n            patches = patches + curr_patches\n        else:\n            for i in range(len(curr_patches)):\n                current_patch_idx -=1\n            patch_idxs = patch_idxs[:-1] \n    return np2torch(np.array(patches)), patch_idxs, patch_idxs[-1][-1]\n\ndef read_image_dir(dir_name, ext, patch_w, patch_h, good_fnames):\n    fnames = find_files(dir_name, ext)\n    patches = []\n    idxs = []\n    current_max_idx = 0\n    for f in fnames:\n        if f.split(\'/\')[-1].replace(\'.png\', \'\') not in good_fnames:\n            continue\n        try:\n            torch_patches, p_idxs_list, max_idx = read_patch_file(f, patch_w, patch_h, current_max_idx)\n        except:\n            continue\n        current_max_idx = max_idx + 1\n        #if patches is None:\n        #    patches = torch_patches\n        #    idxs = p_idxs_list\n        #else:\n        patches.append(torch_patches)\n        idxs = idxs + p_idxs_list\n        print (f, len(idxs))\n    print( \'torch.cat\')\n    patches = torch.cat(patches, dim = 0)\n    print (\'done\')\n    return patches, idxs\n\n\nclass HPatchesDM(data.Dataset):\n    image_ext = \'png\'\n    def __init__(self, root, name, train=True, transform=None,\n                 download=True, pw = 65, ph = 65,\n                 n_pairs = 1000, batch_size = 128, split_name = \'b\'):\n        self.root = os.path.expanduser(root)\n        self.name = name\n        self.n_pairs = n_pairs\n        self.split_name = split_name\n        self.batch_size = batch_size\n        self.train = train\n        self.data_dir = os.path.join(self.root, name)\n        if self.train:\n            self.data_file = os.path.join(self.root, \'{}.pt\'.format(self.name  + \'_train\' ))\n        else:\n            self.data_file = os.path.join(self.root, \'{}.pt\'.format(self.name  + \'_test\' ))            \n        self.transform = transform\n        self.patch_h = ph\n        self.patch_w = pw\n        self.batch_size = batch_size\n        if download:\n            self.download()\n\n        if not self._check_datafile_exists():\n            raise RuntimeError(\'Dataset not found.\' +\n                               \' You can use download=True to download it\')\n\n        # load the serialized data\n        self.patches, self.idxs = torch.load(self.data_file)\n        print(\'Generating {} triplets\'.format(self.n_pairs))\n        self.pairs = self.generate_pairs(self.idxs, self.n_pairs)\n        return\n    def generate_pairs(self, labels, n_pairs):\n        pairs = []\n        n_classes = len(labels)\n        # add only unique indices in batch\n        already_idxs = set()\n        for x in tqdm(range(n_pairs)):\n            if len(already_idxs) >= self.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes)\n            while len(labels[c1]) < 3:\n                c1 = np.random.randint(0, n_classes)\n            already_idxs.add(c1)\n            if len(labels[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(labels[c1]))\n                while (self.patches[labels[c1][n1],:,:,:].float().std() < 1e-2):\n                    n1 = np.random.randint(0, len(labels[c1]))\n                n2 = np.random.randint(0, len(labels[c1]))\n                while (self.patches[labels[c1][n2],:,:,:].float().std() < 1e-2):\n                    n2 = np.random.randint(0, len(labels[c1]))\n            pairs.append([labels[c1][n1], labels[c1][n2]])\n        return torch.LongTensor(np.array(pairs))\n    def __getitem__(self, index):\n        def transform_pair(i1,i2):\n            if self.transform is not None:\n                return self.transform(i1.cpu().numpy()), self.transform(i2.cpu().numpy())\n            else:\n                return i1,i2\n        t = self.pairs[index]\n        a, p = self.patches[t[0],:,:,:], self.patches[t[1],:,:,:]\n        a1,p1 = transform_pair(a,p)\n        return (a1,p1)\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def _check_datafile_exists(self):\n        return os.path.exists(self.data_file)\n\n    def _check_downloaded(self):\n        return os.path.exists(self.data_dir)\n\n    def download(self):\n        if self._check_datafile_exists():\n            print(\'# Found cached data {}\'.format(self.data_file))\n            return\n        # process and save as torch files\n        print(\'# Caching data {}\'.format(self.data_file))\n        import json\n        from pprint import pprint\n        #print self.urls[\'splits\']\n        with open(os.path.join(self.root, \'splits.json\')) as splits_file:    \n            data = json.load(splits_file)\n        if self.train:\n            self.img_fnames = data[self.split_name][\'train\']\n        else:\n            self.img_fnames = data[self.split_name][\'test\']\n        dataset = read_image_dir(self.data_dir, self.image_ext, self.patch_w, self.patch_h, self.img_fnames)\n        print(\'saving...\')\n        with open(self.data_file, \'wb\') as f:\n            torch.save(dataset, f)\n        return\nclass TotalDatasetsLoader(data.Dataset):\n    def __init__(self, datasets_path, train = True, transform = None, batch_size = None, n_triplets = 5000000, fliprot = False, *arg, **kw):\n        super(TotalDatasetsLoader, self).__init__()\n        datasets_path = [os.path.join(datasets_path, dataset) for dataset in os.listdir(datasets_path)]\n        start = True\n        for dataset_p in datasets_path:\n            d = torch.load(dataset_p)\n            if start:\n                data = d[0]\n                labels = d[1]\n                start = False\n            else:\n                data = torch.cat([data, d[0]])\n                labels = torch.cat([labels, d[1]+ torch.max(labels) + 1])\n        #datasets = [torch.load(dataset) for dataset in datasets_path]\n        #data, labels = datasets[0][0], datasets[0][1]\n        #\n        #for i in range(1,len(datasets)):\n        #    data = torch.cat([data,datasets[i][0]])\n        #    labels = torch.cat([labels, datasets[i][1]+torch.max(labels)+1])\n        #\n        #del datasets\n        self.data, self.labels = data, labels\n        self.transform = transform\n        self.train = train\n        self.n_triplets = n_triplets\n        self.batch_size = batch_size\n        self.fliprot = fliprot\n        if self.train:\n                print(\'Generating {} triplets\'.format(self.n_triplets))\n                self.pairs = self.generate_pairs(self.labels, self.n_triplets, self.batch_size)\n\n\n    def generate_pairs(self, labels, num_triplets, batch_size):\n            def create_indices(_labels):\n                inds = dict()\n                for idx, ind in enumerate(_labels):\n                    if ind not in inds:\n                        inds[ind] = []\n                    inds[ind].append(idx)\n                return inds\n            triplets = []\n            indices = create_indices(labels.numpy())\n            unique_labels = np.unique(labels.numpy())\n            n_classes = unique_labels.shape[0]\n            # add only unique indices in batch\n            already_idxs = set()\n            for x in tqdm(range(num_triplets)):\n                if len(already_idxs) >= batch_size:\n                    already_idxs = set()\n                c1 = unique_labels[np.random.randint(0, n_classes)]\n                while c1 in already_idxs:\n                    c1 = unique_labels[np.random.randint(0, n_classes)]\n                already_idxs.add(c1)\n                try:\n                    y = indices[c1]\n                except:\n                    print indices.keys()\n                    sys.exit(0)\n                if len(indices[c1]) == 2:  # hack to speed up process\n                    n1, n2 = 0, 1\n                else:\n                    n1 = np.random.randint(0, len(indices[c1]))\n                    n2 = np.random.randint(0, len(indices[c1]))\n                    while n1 == n2:\n                        n2 = np.random.randint(0, len(indices[c1]))\n                triplets.append([indices[c1][n1], indices[c1][n2]])\n            return torch.LongTensor(np.array(triplets))\n\n    def __getitem__(self, index):\n            def transform_img(img):\n                if self.transform is not None:\n                    img = (img.numpy())/255.0\n                    img = self.transform(img)\n                return img\n\n            t = self.pairs[index]\n            a, p = self.data[t[0]], self.data[t[1]]\n            img_a = transform_img(a)\n            img_p = transform_img(p)\n\n            # transform images if required\n            if self.fliprot:\n                do_flip = random.random() > 0.5\n                do_rot = random.random() > 0.5\n\n                if do_rot:\n                    img_a = img_a.permute(0,2,1)\n                    img_p = img_p.permute(0,2,1)\n\n                if do_flip:\n                    img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                    img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n            return img_a, img_p\n\n    def __len__(self):\n            if self.train:\n                return self.pairs.size(0)\n\nclass TripletPhotoTour(dset.PhotoTour):\n    """"""From the PhotoTour Dataset it generates triplet samples\n    note: a triplet is composed by a pair of matching images and one of\n    different class.\n    """"""\n    urls = {\n        \'notredame_harris\': [\n            \'http://matthewalunbrown.com/patchdata/notredame_harris.zip\',\n            \'notredame_harris.zip\',\n            \'69f8c90f78e171349abdf0307afefe4d\'\n        ],\n        \'yosemite_harris\': [\n            \'http://matthewalunbrown.com/patchdata/yosemite_harris.zip\',\n            \'yosemite_harris.zip\',\n            \'a73253d1c6fbd3ba2613c45065c00d46\'\n        ],\n        \'liberty_harris\': [\n            \'http://matthewalunbrown.com/patchdata/liberty_harris.zip\',\n            \'liberty_harris.zip\',\n            \'c731fcfb3abb4091110d0ae8c7ba182c\'\n        ],\n        \'notredame\': [\n            \'http://icvl.ee.ic.ac.uk/vbalnt/notredame.zip\',\n            \'notredame.zip\',\n            \'509eda8535847b8c0a90bbb210c83484\'\n        ],\n        \'yosemite\': [\n            \'http://icvl.ee.ic.ac.uk/vbalnt/yosemite.zip\',\n            \'yosemite.zip\',\n            \'533b2e8eb7ede31be40abc317b2fd4f0\'\n        ],\n        \'liberty\': [\n            \'http://icvl.ee.ic.ac.uk/vbalnt/liberty.zip\',\n            \'liberty.zip\',\n            \'fdd9152f138ea5ef2091746689176414\'\n        ],\n    }\n    mean = {\'notredame\': 0.4854, \'yosemite\': 0.4844, \'liberty\': 0.4437, \'notredame_harris\': 0.4854, \'yosemite_harris\': 0.4844, \'liberty_harris\': 0.4437}\n    std = {\'notredame\': 0.1864, \'yosemite\': 0.1818, \'liberty\': 0.2019, \'notredame_harris\': 0.1864, \'yosemite_harris\': 0.1818, \'liberty_harris\': 0.2019}\n    lens = {\'notredame\': 468159, \'yosemite\': 633587, \'liberty\': 450092, \'liberty_harris\': 379587, \'yosemite_harris\': 450912 , \'notredame_harris\': 325295}\n    def __init__(self, train=True, transform=None, batch_size = None, n_triplets = 5000, load_random_triplets = False,  *arg, **kw):\n        super(TripletPhotoTour, self).__init__(*arg, **kw)\n        self.transform = transform\n        self.out_triplets = load_random_triplets\n        self.train = train\n        self.n_triplets = 1000\n        self.batch_size = batch_size\n\n        if self.train:\n            print(\'Generating {} triplets\'.format(self.n_triplets))\n            self.pairs = self.generate_pairs(self.labels, self.n_triplets)\n    def generate_pairs(self,labels, num_triplets):\n        def create_indices(_labels):\n            inds = dict()\n            for idx, ind in enumerate(_labels):\n                if ind not in inds:\n                    inds[ind] = []\n                inds[ind].append(idx)\n            return inds\n        triplets = []\n        indices = create_indices(labels.numpy())\n        unique_labels = np.unique(labels.numpy())\n        n_classes = unique_labels.shape[0]\n        # add only unique indices in batch\n        already_idxs = set()\n        for x in tqdm(range(num_triplets)):\n            if len(already_idxs) >= self.batch_size:\n                already_idxs = set()\n            c1 = np.random.randint(0, n_classes - 1)\n            while c1 in already_idxs:\n                c1 = np.random.randint(0, n_classes - 1)\n            already_idxs.add(c1)\n            c2 = np.random.randint(0, n_classes - 1)\n            while c1 == c2:\n                c2 = np.random.randint(0, n_classes - 1)\n            if len(indices[c1]) == 2:  # hack to speed up process\n                n1, n2 = 0, 1\n            else:\n                n1 = np.random.randint(0, len(indices[c1]) - 1)\n                n2 = np.random.randint(0, len(indices[c1]) - 1)\n                while n1 == n2:\n                    n2 = np.random.randint(0, len(indices[c1]) - 1)\n            n3 = np.random.randint(0, len(indices[c2]) - 1)\n            triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3]])\n        return torch.LongTensor(np.array(triplets))\n    def __getitem__(self, index):\n        def transform_img(img):\n            if self.transform is not None:\n                img = self.transform(img.numpy())\n            return img\n\n        if not self.train:\n            m = self.matches[index]\n            img1 = transform_img(self.data[m[0]])\n            img2 = transform_img(self.data[m[1]])\n            return img1, img2, m[2]\n\n        t = self.pairs[index]\n        a, p, n = self.data[t[0]], self.data[t[1]], self.data[t[2]]\n        img_a = transform_img(a)\n        img_p = transform_img(p)\n        img_n = None\n        if self.out_triplets:\n            img_n = transform_img(n)\n        # transform images if required\n        if True:#args.fliprot:\n            do_flip = random.random() > 0.5\n            do_rot = random.random() > 0.5\n            if do_rot:\n                img_a = img_a.permute(0,2,1)\n                img_p = img_p.permute(0,2,1)\n                if self.out_triplets:\n                    img_n = img_n.permute(0,2,1)\n            if do_flip:\n                img_a = torch.from_numpy(deepcopy(img_a.numpy()[:,:,::-1]))\n                img_p = torch.from_numpy(deepcopy(img_p.numpy()[:,:,::-1]))\n                if self.out_triplets:\n                    img_n = torch.from_numpy(deepcopy(img_n.numpy()[:,:,::-1]))\n        if self.out_triplets:\n            return (img_a, img_p, img_n)\n        else:\n            return (img_a, img_p)\n\n    def __len__(self):\n        if self.train:\n            return self.pairs.size(0)\n        else:\n            return self.matches.size(0)\n\n'"
gen_ds.py,13,"b""\nimport os\nimport errno\nimport numpy as np\nfrom PIL import Image\nimport torchvision.datasets as dset\n\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport math\nimport torch.utils.data as data\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nimport random\nimport cv2\nimport copy\nfrom Utils import str2bool\n\nfrom dataset import  TripletPhotoTour\nroot='dataset'\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='notredame',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\n\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='yosemite',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\n\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='liberty',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='notredame_harris',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='yosemite_harris',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\ntrain_loader = torch.utils.data.DataLoader(\n        TripletPhotoTour(train=True,\n                         batch_size=128,\n                         root=root,\n                         name='liberty_harris',\n                         download=True,\n                         transform=None),\n                         batch_size=128,\n                         shuffle=False)\n\n"""
pytorch_sift.py,15,"b""import torch\nimport math\nimport torch.nn.init\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torch.nn.functional as F\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.abs(torch.sum(x * x, dim = 1)) + self.eps)\n        x= x / norm.unsqueeze(1).expand_as(x)\n        return x\n\ndef getPoolingKernel(kernel_size = 25):\n    step = 1. / float(np.floor( kernel_size / 2.));\n    x_coef = np.arange(step/2., 1. ,step)\n    xc2 = np.hstack([x_coef,[1], x_coef[::-1]])\n    kernel = np.outer(xc2.T,xc2)\n    kernel = np.maximum(0,kernel)\n    return kernel\ndef get_bin_weight_kernel_size_and_stride(patch_size, num_spatial_bins):\n    bin_weight_stride = int(round(2.0 * math.floor(patch_size / 2) / float(num_spatial_bins + 1)))\n    bin_weight_kernel_size = int(2 * bin_weight_stride - 1);\n    return bin_weight_kernel_size, bin_weight_stride\nclass SIFTNet(nn.Module):\n    def CircularGaussKernel(self,kernlen=21):\n        halfSize = kernlen / 2;\n        r2 = float(halfSize*halfSize);\n        sigma2 = 0.9 * r2;\n        disq = 0;\n        kernel = np.zeros((kernlen,kernlen))\n        for y in range(kernlen):\n            for x in range(kernlen):\n                disq = (y - halfSize)*(y - halfSize) +  (x - halfSize)*(x - halfSize);\n                if disq < r2:\n                    kernel[y,x] = math.exp(-disq / sigma2)\n                else:\n                    kernel[y,x] = 0.\n        return kernel\n    def __init__(self, patch_size = 65, num_ang_bins = 8, num_spatial_bins = 4, clipval = 0.2):\n        super(SIFTNet, self).__init__()\n        gk = torch.from_numpy(self.CircularGaussKernel(kernlen=patch_size).astype(np.float32))\n        self.bin_weight_kernel_size, self.bin_weight_stride = get_bin_weight_kernel_size_and_stride(patch_size, num_spatial_bins)\n        self.gk = Variable(gk)\n        self.num_ang_bins = num_ang_bins\n        self.num_spatial_bins = num_spatial_bins\n        self.clipval = clipval\n        self.gx =  nn.Sequential(nn.Conv2d(1, 1, kernel_size=(1,3),  bias = False))\n        for l in self.gx:\n            if isinstance(l, nn.Conv2d):\n                l.weight.data = torch.from_numpy(np.array([[[[-1, 0, 1]]]], dtype=np.float32))\n        self.gy =  nn.Sequential(nn.Conv2d(1, 1, kernel_size=(3,1),  bias = False))\n        for l in self.gy:\n            if isinstance(l, nn.Conv2d):\n                l.weight.data = torch.from_numpy(np.array([[[[-1], [0], [1]]]], dtype=np.float32))\n        self.pk = nn.Sequential(nn.Conv2d(1, 1, kernel_size=(self.bin_weight_kernel_size, self.bin_weight_kernel_size),\n                            stride = (self.bin_weight_stride, self.bin_weight_stride),\n                            bias = False))\n        for l in self.pk:\n            if isinstance(l, nn.Conv2d):\n                nw = getPoolingKernel(kernel_size = self.bin_weight_kernel_size)\n                new_weights = np.array(nw.reshape((1, 1, self.bin_weight_kernel_size, self.bin_weight_kernel_size)))\n                l.weight.data = torch.from_numpy(new_weights.astype(np.float32))\n    def forward(self, x):\n        gx = self.gx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0,0, 1,1), 'replicate'))\n        mag = torch.sqrt(gx **2 + gy **2 + 1e-10)\n        ori = torch.atan2(gy,gx + 1e-8)\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        else:\n            self.gk = self.gk.cpu()\n        mag  = mag * self.gk.expand_as(mag)\n        o_big = (ori +2.0 * math.pi )/ (2.0 * math.pi) * float(self.num_ang_bins)\n        bo0_big =  torch.floor(o_big)\n        wo1_big = o_big - bo0_big\n        bo0_big =  bo0_big %  self.num_ang_bins\n        bo1_big = (bo0_big + 1) % self.num_ang_bins\n        wo0_big = (1.0 - wo1_big) * mag\n        wo1_big = wo1_big * mag\n        ang_bins = []\n        for i in range(0, self.num_ang_bins):\n            ang_bins.append(self.pk((bo0_big == i).float() * wo0_big + (bo1_big == i).float() * wo1_big))\n        ang_bins = torch.cat(ang_bins,1)\n        ang_bins = ang_bins.view(ang_bins.size(0), -1)\n        ang_bins = L2Norm()(ang_bins)\n        ang_bins = torch.clamp(ang_bins, 0.,float(self.clipval))\n        ang_bins = L2Norm()(ang_bins)\n        return ang_bins\n"""
train_AffNet_test_on_graffity.py,37,"b'#from __future__ import division, print_function\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport seaborn as sns\nimport os\nimport errno\nimport numpy as np\nfrom PIL import Image\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport math\nimport torch.utils.data as data\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.datasets as dset\nimport gc\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nimport random\nimport cv2\nimport copy\nfrom Utils import L2Norm, cv2_scale\n#from Utils import np_reshape64 as np_reshape\nnp_reshape = lambda x: np.reshape(x, (64, 64, 1))\nfrom Utils import str2bool\nfrom dataset import HPatchesDM,TripletPhotoTour, TotalDatasetsLoader\ncv2_scale40 = lambda x: cv2.resize(x, dsize=(40, 40),\n                                 interpolation=cv2.INTER_LINEAR)\nfrom augmentation import get_random_norm_affine_LAFs,get_random_rotation_LAFs, get_random_shifts_LAFs\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A, extract_patches,normalizeLAFs\nfrom pytorch_sift import SIFTNet\nfrom HardNet import HardNet, L2Norm, HardTFeatNet\nfrom Losses import loss_HardNegC, loss_HardNet\nfrom SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A,visualize_LAFs\nfrom Losses import distance_matrix_vector\nfrom ReprojectionStuff import get_GT_correspondence_indexes\n\nPS = 32\ntilt_schedule = {\'0\': 3.0, \'1\': 4.0, \'3\': 4.5, \'5\': 4.8, \'6\': 5.2, \'8\':  5.8 }\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch AffNet\')\n\nparser.add_argument(\'--dataroot\', type=str,\n                    default=\'datasets/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--log-dir\', default=\'./logs\',\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--num-workers\', default= 8,\n                    help=\'Number of workers to be created\')\nparser.add_argument(\'--pin-memory\',type=bool, default= True,\n                    help=\'\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'E\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--batch-size\', type=int, default=128, metavar=\'BS\',\n                    help=\'input batch size for training (default: 128)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1024, metavar=\'BST\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--n-pairs\', type=int, default=500000, metavar=\'N\',\n                    help=\'how many pairs will generate from the dataset\')\nparser.add_argument(\'--n-test-pairs\', type=int, default=50000, metavar=\'N\',\n                    help=\'how many pairs will generate from the test dataset\')\nparser.add_argument(\'--lr\', type=float, default=0.005, metavar=\'LR\',\n                    help=\'learning rate (default: 0.005)\')\nparser.add_argument(\'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n# Device options\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--expname\', default=\'\', type=str,\n                    help=\'experiment name\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'S\',\n                    help=\'random seed (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'LI\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--descriptor\', type=str,\n                    default=\'pixels\',\n                    help=\'which descriptor distance is minimized. Variants: pixels, SIFT, HardNet\')\nparser.add_argument(\'--loss\', type=str,\n                    default=\'HardNet\',\n                    help=\'Variants: HardNet, HardNegC, PosDist\')\nparser.add_argument(\'--arch\', type=str,\n                    default=\'AffNetFast\',\n                    help=\'Variants: AffNetFast, AffNetFast4, AffNetFast4Rot\')\n\n\nargs = parser.parse_args()\n\n\n# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n# order to prevent any memory allocation on unused GPUs\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\nif args.cuda:\n    cudnn.benchmark = True\n    torch.cuda.manual_seed_all(args.seed)\n\n# create loggin directory\nif not os.path.exists(args.log_dir):\n    os.makedirs(args.log_dir)\n\n# set random seeds\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\nrandom.seed(args.seed)\n\nif args.descriptor == \'SIFT\':\n    descriptor = SIFTNet(patch_size=PS)\n    if not args.no_cuda:\n        descriptor = descriptor.cuda()\nelif args.descriptor == \'HardNet\':\n    descriptor = HardNet()\n    if not args.no_cuda:\n        descriptor = descriptor.cuda()\n    model_weights = \'HardNet++.pth\'\n    hncheckpoint = torch.load(model_weights)\n    descriptor.load_state_dict(hncheckpoint[\'state_dict\'])\n    descriptor.train()\nelif args.descriptor == \'TFeat\':\n    descriptor = HardTFeatNet(sm=SIFTNet(patch_size = 32))\n    if not args.no_cuda:\n        descriptor = descriptor.cuda()\n    model_weights = \'HardTFeat.pth\'\n    hncheckpoint = torch.load(model_weights)\n    descriptor.load_state_dict(hncheckpoint[\'state_dict\'])\n    descriptor.train()\nelse:\n    descriptor = lambda x: L2Norm()(x.view(x.size(0),-1) - x.view(x.size(0),-1).mean(dim=1, keepdim=True).expand(x.size(0),x.size(1)*x.size(2)*x.size(3)).detach())\n\nsuffix = args.expname +""_"" + args.arch + \'_6Brown_\' +  args.descriptor + \'_\' + str(args.lr) + \'_\' + str(args.n_pairs) + ""_"" + str(args.loss) \n##########################################3\ndef create_loaders():\n\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n    transform = transforms.Compose([\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor()\n            ])\n\n    train_loader = torch.utils.data.DataLoader(\n            TotalDatasetsLoader(datasets_path = args.dataroot, train=True,\n                             n_triplets = args.n_pairs,\n                             fliprot=True,\n                             batch_size=args.batch_size,\n                             download=True,\n                             transform=transform),\n                             batch_size=args.batch_size,\n                             shuffle=False, **kwargs)\n    return train_loader, None\n\ndef extract_and_crop_patches_by_predicted_transform(patches, trans, crop_size = 32):\n    assert patches.size(0) == trans.size(0)\n    st = int((patches.size(2) - crop_size) / 2)\n    fin = st + crop_size\n    rot_LAFs = Variable(torch.FloatTensor([[0.5, 0, 0.5],[0, 0.5, 0.5]]).unsqueeze(0).repeat(patches.size(0),1,1));\n    if patches.is_cuda:\n        rot_LAFs = rot_LAFs.cuda()\n        trans = trans.cuda()\n    rot_LAFs1  = torch.cat([torch.bmm(trans, rot_LAFs[:,0:2,0:2]), rot_LAFs[:,0:2,2:]], dim = 2);\n    return extract_patches(patches,  rot_LAFs1, PS = patches.size(2))[:,:, st:fin, st:fin].contiguous()\n    \ndef extract_random_LAF(data, max_rot = math.pi, max_tilt = 1.0, crop_size = 32):\n    st = int((data.size(2) - crop_size)/2)\n    fin = st + crop_size\n    if type(max_rot) is float:\n        rot_LAFs, inv_rotmat = get_random_rotation_LAFs(data, max_rot)\n    else:\n        rot_LAFs = max_rot\n        inv_rotmat = None\n    aff_LAFs, inv_TA = get_random_norm_affine_LAFs(data, max_tilt);\n    aff_LAFs[:,0:2,0:2] = torch.bmm(rot_LAFs[:,0:2,0:2],aff_LAFs[:,0:2,0:2])\n    data_aff = extract_patches(data,  aff_LAFs, PS = data.size(2))\n    data_affcrop = data_aff[:,:, st:fin, st:fin].contiguous()\n    return data_affcrop, data_aff, rot_LAFs,inv_rotmat,inv_TA \ndef train(train_loader, model, optimizer, epoch):\n    # switch to train mode\n    model.train()\n    pbar = tqdm(enumerate(train_loader))\n    for batch_idx, data in pbar:\n        data_a, data_p = data\n        if args.cuda:\n            data_a, data_p  = data_a.float().cuda(), data_p.float().cuda()\n            data_a, data_p = Variable(data_a), Variable(data_p)\n        st = int((data_p.size(2) - model.PS)/2)\n        fin = st + model.PS\n        ep1 = epoch\n        while str(ep1) not in tilt_schedule.keys():\n            ep1 -=1\n            if ep1 < 0:\n                break\n        max_tilt = tilt_schedule[str(ep1)]\n        data_a_aff_crop, data_a_aff, rot_LAFs_a, inv_rotmat_a, inv_TA_a = extract_random_LAF(data_a, math.pi, max_tilt, model.PS)\n        if \'Rot\' not in args.arch:\n            data_p_aff_crop, data_p_aff, rot_LAFs_p, inv_rotmat_p, inv_TA_p = extract_random_LAF(data_p, rot_LAFs_a, max_tilt, model.PS)\n        else:\n            data_p_aff_crop, data_p_aff, rot_LAFs_p, inv_rotmat_p, inv_TA_p = extract_random_LAF(data_p, math.pi, max_tilt, model.PS)\n        if inv_rotmat_p is None:\n            inv_rotmat_p = inv_rotmat_a\n        out_a_aff, out_p_aff = model(data_a_aff_crop,True), model(data_p_aff_crop,True)\n        #out_a_aff_back = torch.bmm(torch.bmm(out_a_aff, inv_TA_a),  inv_rotmat_a)\n        #out_p_aff_back = torch.bmm(torch.bmm(out_p_aff, inv_TA_p),  inv_rotmat_p)\n        ###### Get descriptors\n        out_patches_a_crop = extract_and_crop_patches_by_predicted_transform(data_a_aff, out_a_aff, crop_size = model.PS)\n        out_patches_p_crop = extract_and_crop_patches_by_predicted_transform(data_p_aff, out_p_aff, crop_size = model.PS)\n        desc_a = descriptor(out_patches_a_crop)\n        desc_p = descriptor(out_patches_p_crop)\n        descr_dist =  torch.sqrt(((desc_a - desc_p)**2).view(data_a.size(0),-1).sum(dim=1) + 1e-6).mean()\n        #geom_dist = torch.sqrt(((out_a_aff_back - out_p_aff_back)**2 ).view(-1,4).sum(dim=1) + 1e-8).mean()\n        if args.loss == \'HardNet\':\n            loss = loss_HardNet(desc_a,desc_p); \n        elif args.loss == \'HardNegC\':\n            loss = loss_HardNegC(desc_a,desc_p); \n        #elif args.loss == \'Geom\':\n        #    loss = geom_dist; \n        elif args.loss == \'PosDist\':\n            loss = descr_dist; \n        else:\n            print(\'Unknown loss function\')\n            sys.exit(1)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        adjust_learning_rate(optimizer)\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(\n                \'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f},{:.4f}\'.format(\n                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader),\n                    float(loss.detach().cpu().numpy()), float(descr_dist.detach().cpu().numpy())))\n    torch.save({\'epoch\': epoch + 1, \'state_dict\': model.state_dict()},\n               \'{}/checkpoint_{}.pth\'.format(LOG_DIR,epoch))\ndef load_grayscale_var(fname):\n    img = Image.open(fname).convert(\'RGB\')\n    img = np.mean(np.array(img), axis = 2)\n    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n    if args.cuda:\n        var_image_reshape = var_image_reshape.cuda()\n    return var_image_reshape\ndef get_geometry_and_descriptors(img, det, desc, do_ori = True):\n    with torch.no_grad():\n        LAFs, resp = det(img,do_ori = do_ori)\n        patches = det.extract_patches_from_pyr(LAFs, PS = 32)\n        descriptors = desc(patches)\n    return LAFs, descriptors\n\ndef test(model,epoch):\n    torch.cuda.empty_cache()\n    # switch to evaluate mode\n    model.eval()\n    detector = ScaleSpaceAffinePatchExtractor( mrSize = 5.192, num_features = 3000,\n                                          border = 5, num_Baum_iters = 1, \n                                          AffNet = model)\n    descriptor = HardNet()\n    model_weights = \'HardNet++.pth\'\n    hncheckpoint = torch.load(model_weights)\n    descriptor.load_state_dict(hncheckpoint[\'state_dict\'])\n    descriptor.eval()\n    if args.cuda:\n        detector = detector.cuda()\n        descriptor = descriptor.cuda()\n    input_img_fname1 = \'test-graf/img1.png\'#sys.argv[1]\n    input_img_fname2 = \'test-graf/img6.png\'#sys.argv[1]\n    H_fname = \'test-graf/H1to6p\'#sys.argv[1]\n    output_img_fname = \'graf_match.png\'#sys.argv[3]\n    img1 = load_grayscale_var(input_img_fname1)\n    img2 = load_grayscale_var(input_img_fname2)\n    H = np.loadtxt(H_fname)    \n    H1to2 = Variable(torch.from_numpy(H).float())\n    SNN_threshold = 0.8\n    with torch.no_grad():\n        LAFs1, descriptors1 = get_geometry_and_descriptors(img1, detector, descriptor)\n        torch.cuda.empty_cache()\n        LAFs2, descriptors2 = get_geometry_and_descriptors(img2, detector, descriptor)\n        visualize_LAFs(img1.detach().cpu().numpy().squeeze(), LAFs1.detach().cpu().numpy().squeeze(), \'b\', show = False, save_to = LOG_DIR + ""/detections1_"" + str(epoch) + \'.png\')\n        visualize_LAFs(img2.detach().cpu().numpy().squeeze(), LAFs2.detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/detection2_"" + str(epoch) + \'.png\')\n        dist_matrix = distance_matrix_vector(descriptors1, descriptors2)\n        min_dist, idxs_in_2 = torch.min(dist_matrix,1)\n        dist_matrix[:,idxs_in_2] = 100000;# mask out nearest neighbour to find second nearest\n        min_2nd_dist, idxs_2nd_in_2 = torch.min(dist_matrix,1)\n        mask = (min_dist / (min_2nd_dist + 1e-8)) <= SNN_threshold\n        tent_matches_in_1 = indxs_in1 = torch.autograd.Variable(torch.arange(0, idxs_in_2.size(0)), requires_grad = False).cuda()[mask]\n        tent_matches_in_2 = idxs_in_2[mask]\n        tent_matches_in_1 = tent_matches_in_1.long()\n        tent_matches_in_2 = tent_matches_in_2.long()\n        LAF1s_tent = LAFs1[tent_matches_in_1,:,:]\n        LAF2s_tent = LAFs2[tent_matches_in_2,:,:]\n        min_dist, plain_indxs_in1, idxs_in_2 = get_GT_correspondence_indexes(LAF1s_tent, LAF2s_tent,H1to2.cuda(), dist_threshold = 6) \n        plain_indxs_in1 = plain_indxs_in1.long()\n        inl_ratio = float(plain_indxs_in1.size(0)) / float(tent_matches_in_1.size(0))\n        print \'Test epoch\', str(epoch) \n        print \'Test on graf1-6,\', tent_matches_in_1.size(0), \'tentatives\', plain_indxs_in1.size(0), \'true matches\', str(inl_ratio)[:5], \' inl.ratio\'\n        visualize_LAFs(img1.detach().cpu().numpy().squeeze(), LAF1s_tent[plain_indxs_in1.long(),:,:].detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/inliers1_"" + str(epoch) + \'.png\')\n        visualize_LAFs(img2.detach().cpu().numpy().squeeze(), LAF2s_tent[plain_indxs_in1.long(),:,:].detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/inliers2_"" + str(epoch) + \'.png\')\n    ################\n        print (\'Now native ori\')\n        #if \'Rot\' not in args.arch:\n        #    return\n        del LAFs1, descriptors1, LAFs2, descriptors2, dist_matrix,tent_matches_in_2,plain_indxs_in1, tent_matches_in_1,idxs_in_2,mask,min_2nd_dist,idxs_2nd_in_2,min_dist,LAF1s_tent,LAF2s_tent\n        torch.cuda.empty_cache()\n        gc.collect()\n        LAFs1, descriptors1 = get_geometry_and_descriptors(img1, detector, descriptor, False)\n        LAFs2, descriptors2 = get_geometry_and_descriptors(img2, detector, descriptor, False)\n        visualize_LAFs(img1.detach().cpu().numpy().squeeze(), LAFs1.detach().cpu().numpy().squeeze(), \'b\', show = False, save_to = LOG_DIR + ""/ori_detections1_"" + str(epoch) + \'.png\')\n        visualize_LAFs(img2.detach().cpu().numpy().squeeze(), LAFs2.detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/ori_detection2_"" + str(epoch) + \'.png\')\n        dist_matrix = distance_matrix_vector(descriptors1, descriptors2)\n        min_dist, idxs_in_2 = torch.min(dist_matrix,1)\n        dist_matrix[:,idxs_in_2] = 100000;# mask out nearest neighbour to find second nearest\n        min_2nd_dist, idxs_2nd_in_2 = torch.min(dist_matrix,1)\n        mask = (min_dist / (min_2nd_dist + 1e-8)) <= SNN_threshold\n        tent_matches_in_1 = indxs_in1 = torch.autograd.Variable(torch.arange(0, idxs_in_2.size(0)), requires_grad = False).cuda()[mask]\n        tent_matches_in_2 = idxs_in_2[mask]\n        tent_matches_in_1 = tent_matches_in_1.long()\n        tent_matches_in_2 = tent_matches_in_2.long()\n        LAF1s_tent = LAFs1[tent_matches_in_1,:,:]\n        LAF2s_tent = LAFs2[tent_matches_in_2,:,:]\n        min_dist, plain_indxs_in1, idxs_in_2 = get_GT_correspondence_indexes(LAF1s_tent, LAF2s_tent,H1to2.cuda(), dist_threshold = 6) \n        plain_indxs_in1 = plain_indxs_in1.long()\n        inl_ratio = float(plain_indxs_in1.size(0)) / float(tent_matches_in_1.size(0))\n        print \'Test epoch\', str(epoch) \n        print \'Test on ori graf1-6,\', tent_matches_in_1.size(0), \'tentatives\', plain_indxs_in1.size(0), \'true matches\', str(inl_ratio)[:5], \' inl.ratio\'\n        visualize_LAFs(img1.detach().cpu().numpy().squeeze(), LAF1s_tent[plain_indxs_in1.long(),:,:].detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/ori_inliers1_"" + str(epoch) + \'.png\')\n        visualize_LAFs(img2.detach().cpu().numpy().squeeze(), LAF2s_tent[idxs_in_2.long(),:,:].detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/ori_inliers2_"" + str(epoch) + \'.png\')\n    return\n\ndef adjust_learning_rate(optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0.\n        else:\n            group[\'step\'] += 1.\n        group[\'lr\'] = args.lr * (\n        1.0 - float(group[\'step\']) * float(args.batch_size) / (args.n_pairs * float(args.epochs)))\n    return\n\ndef create_optimizer(model, new_lr):\n    optimizer = optim.SGD(model.parameters(), lr=new_lr,\n                          momentum=0.9, dampening=0.9,\n                          weight_decay=args.wd)\n    return optimizer\n\ndef main(train_loader, test_loader, model):\n    # print the experiment configuration\n    print(\'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n    if args.cuda:\n        model.cuda()\n    optimizer1 = create_optimizer(model, args.lr)\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint {}\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            checkpoint = torch.load(args.resume)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(\'=> no checkpoint found at {}\'.format(args.resume))\n    start = args.start_epoch\n    end = start + args.epochs\n    test(model, -1)\n    for epoch in range(start, end):\n        # iterate over test loaders and test results\n        train(train_loader, model, optimizer1, epoch)\n        test(model, epoch)\n    return 0\nif __name__ == \'__main__\':\n    LOG_DIR = args.log_dir\n    LOG_DIR = os.path.join(args.log_dir,suffix)\n    if not os.path.isdir(LOG_DIR):\n        os.makedirs(LOG_DIR)\n    from architectures import AffNetFast, AffNetFastScale, AffNetFast4, AffNetFast4RotNosc, AffNetFast52RotUp,AffNetFast52Rot,AffNetFast5Rot, AffNetFast4Rot, AffNetFast4Rot\n    from architectures import AffNetFast2Par,AffNetFastBias\n    if args.arch == \'AffNetFast\':\n        model = AffNetFast(PS=PS)\n    elif args.arch == \'AffNetFastBias\':\n        model = AffNetFastBias(PS=PS)\n    elif args.arch == \'AffNetFastScale\':\n        model = AffNetFastScale(PS=PS)\n    elif args.arch == \'AffNetFast2Par\':\n        model = AffNetFast2Par(PS=PS)\n    elif args.arch == \'AffNetFast4\':\n        model = AffNetFast4(PS=PS)\n    elif args.arch == \'AffNetFast4Rot\':\n        model = AffNetFast4Rot(PS=PS)\n    elif args.arch == \'AffNetFast52\':\n        model = AffNetFast52Rot(PS=PS)\n    elif args.arch == \'AffNetFast52Rot\':\n        model = AffNetFast52Rot(PS=PS)\n    elif args.arch == \'AffNetFast52RotUp\':\n        model = AffNetFast52RotUp(PS=PS)\n    elif args.arch == \'AffNetFast5Rot\':\n        model = AffNetFast5Rot(PS=PS)\n    elif args.arch == \'AffNetFast4RotNosc\':\n        model = AffNetFast4RotNosc(PS=PS)\n    else:\n        print (args.arch, \'is incorrect architecture\')\n        sys.exit(1)\n    train_loader, test_loader = create_loaders()\n    main(train_loader, test_loader, model)\n'"
train_OriNet_test_on_graffity.py,37,"b'#from __future__ import division, print_function\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport os\nimport errno\nimport numpy as np\nfrom PIL import Image\nimport sys\nfrom copy import deepcopy\nimport argparse\nimport math\nimport torch.utils.data as data\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.datasets as dset\nimport gc\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nimport random\nimport cv2\nimport copy\nfrom Utils import L2Norm, cv2_scale\n#from Utils import np_reshape64 as np_reshape\nnp_reshape = lambda x: np.reshape(x, (64, 64, 1))\nfrom Utils import str2bool\nfrom dataset import HPatchesDM,TripletPhotoTour, TotalDatasetsLoader\ncv2_scale40 = lambda x: cv2.resize(x, dsize=(40, 40),\n                                 interpolation=cv2.INTER_LINEAR)\nfrom augmentation import get_random_norm_affine_LAFs,get_random_rotation_LAFs, get_random_shifts_LAFs\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A, extract_patches,normalizeLAFs\nfrom pytorch_sift import SIFTNet\nfrom HardNet import HardNet, L2Norm\nfrom Losses import loss_HardNetDetach, loss_HardNet\nfrom SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A,visualize_LAFs\nimport seaborn as sns\nfrom Losses import distance_matrix_vector\nfrom ReprojectionStuff import get_GT_correspondence_indexes\n\nPS = 32\ntilt_schedule = {\'0\': 3.0, \'1\': 4.0, \'3\': 4.5, \'5\': 4.8, \'6\': 5.2, \'8\':  5.8 }\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch AffNet\')\n\nparser.add_argument(\'--dataroot\', type=str,\n                    default=\'datasets/\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--log-dir\', default=\'./logs\',\n                    help=\'folder to output model checkpoints\')\nparser.add_argument(\'--num-workers\', default= 8,\n                    help=\'Number of workers to be created\')\nparser.add_argument(\'--pin-memory\',type=bool, default= True,\n                    help=\'\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'E\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--batch-size\', type=int, default=128, metavar=\'BS\',\n                    help=\'input batch size for training (default: 128)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1024, metavar=\'BST\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--n-pairs\', type=int, default=500000, metavar=\'N\',\n                    help=\'how many pairs will generate from the dataset\')\nparser.add_argument(\'--n-test-pairs\', type=int, default=50000, metavar=\'N\',\n                    help=\'how many pairs will generate from the test dataset\')\nparser.add_argument(\'--lr\', type=float, default=0.005, metavar=\'LR\',\n                    help=\'learning rate (default: 0.005)\')\nparser.add_argument(\'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n# Device options\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--gpu-id\', default=\'0\', type=str,\n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--expname\', default=\'\', type=str,\n                    help=\'experiment name\')\nparser.add_argument(\'--seed\', type=int, default=0, metavar=\'S\',\n                    help=\'random seed (default: 0)\')\nparser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'LI\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--descriptor\', type=str,\n                    default=\'pixels\',\n                    help=\'which descriptor distance is minimized. Variants: pixels, SIFT, HardNet\')\nparser.add_argument(\'--loss\', type=str,\n                    default=\'HardNet\',\n                    help=\'Variants: HardNet, HardNetDetach, PosDist, Geom\')\nparser.add_argument(\'--arch\', type=str,\n                    default=\'AffNetFast\',\n                    help=\'Variants: AffNetFast, AffNetSlow, AffNetFast4, AffNetFast4Rot\')\n\n\nargs = parser.parse_args()\n\n\n# set the device to use by setting CUDA_VISIBLE_DEVICES env variable in\n# order to prevent any memory allocation on unused GPUs\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\nif args.cuda:\n    cudnn.benchmark = True\n    torch.cuda.manual_seed_all(args.seed)\n\n# create loggin directory\nif not os.path.exists(args.log_dir):\n    os.makedirs(args.log_dir)\n\n# set random seeds\ntorch.manual_seed(args.seed)\nnp.random.seed(args.seed)\nrandom.seed(args.seed)\nclass HardTFeatNet(nn.Module):\n    """"""TFeat model definition\n    """"""\n\n    def __init__(self, sm):\n        super(HardTFeatNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=7),\n            nn.Tanh(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=6),\n            nn.Tanh()\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(64, 128, kernel_size=8),\n            nn.Tanh())\n        self.SIFT = sm\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    \n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        x = self.classifier(x_features)\n        return x.view(x.size(0), -1)\n\nif args.descriptor == \'SIFT\':\n    descriptor = SIFTNet(patch_size=PS)\n    if not args.no_cuda:\n        descriptor = descriptor.cuda()\nelif args.descriptor == \'HardNet\':\n    descriptor = HardNet()\n    if not args.no_cuda:\n        descriptor = descriptor.cuda()\n    model_weights = \'HardNet++.pth\'\n    hncheckpoint = torch.load(model_weights)\n    descriptor.load_state_dict(hncheckpoint[\'state_dict\'])\n    descriptor.train()\nelif args.descriptor == \'TFeat\':\n    descriptor = HardTFeatNet(sm=SIFTNet(patch_size = 32))\n    if not args.no_cuda:\n        descriptor = descriptor.cuda()\n    model_weights = \'HardTFeat.pth\'\n    hncheckpoint = torch.load(model_weights)\n    descriptor.load_state_dict(hncheckpoint[\'state_dict\'])\n    descriptor.train()\nelse:\n    descriptor = lambda x: L2Norm()(x.view(x.size(0),-1) - x.view(x.size(0),-1).mean(dim=1, keepdim=True).expand(x.size(0),x.size(1)*x.size(2)*x.size(3)).detach())\n\nsuffix = args.expname +\'_OriNet_6Brown_\' +  args.descriptor + \'_\' + str(args.lr) + \'_\' + str(args.n_pairs) + ""_"" + str(args.loss) \n##########################################3\ndef create_loaders():\n\n    kwargs = {\'num_workers\': args.num_workers, \'pin_memory\': args.pin_memory} if args.cuda else {}\n    transform = transforms.Compose([\n            transforms.Lambda(np_reshape),\n            transforms.ToTensor()\n            ])\n\n    train_loader = torch.utils.data.DataLoader(\n            TotalDatasetsLoader(datasets_path = args.dataroot, train=True,\n                             n_triplets = args.n_pairs,\n                             fliprot=True,\n                             batch_size=args.batch_size,\n                             download=True,\n                             transform=transform),\n                             batch_size=args.batch_size,\n                             shuffle=False, **kwargs)\n\n    #test_loader = torch.utils.data.DataLoader(\n    #        HPatchesDM(\'dataset/HP_HessianPatches/\',\'\', train=False,\n    #                         n_pairs = args.n_test_pairs,\n    #                         batch_size=args.test_batch_size,\n    #                         download=True,\n    #                         transform=transforms.Compose([])),\n    #                         batch_size=args.test_batch_size,\n    #                         shuffle=False, **kwargs)\n    return train_loader, None\n\ndef extract_and_crop_patches_by_predicted_transform(patches, trans, crop_size = 32):\n    assert patches.size(0) == trans.size(0)\n    st = int((patches.size(2) - crop_size) / 2)\n    fin = st + crop_size\n    rot_LAFs = Variable(torch.FloatTensor([[0.5, 0, 0.5],[0, 0.5, 0.5]]).unsqueeze(0).repeat(patches.size(0),1,1));\n    if patches.is_cuda:\n        rot_LAFs = rot_LAFs.cuda()\n        trans = trans.cuda()\n    rot_LAFs1  = torch.cat([torch.bmm(trans, rot_LAFs[:,0:2,0:2]), rot_LAFs[:,0:2,2:]], dim = 2);\n    return extract_patches(patches,  rot_LAFs1, PS = patches.size(2))[:,:, st:fin, st:fin].contiguous()\n    \ndef extract_random_LAF(data, max_rot = math.pi, max_tilt = 1.0, crop_size = 32):\n    st = int((data.size(2) - crop_size)/2)\n    fin = st + crop_size\n    if type(max_rot) is float:\n        rot_LAFs, inv_rotmat = get_random_rotation_LAFs(data, max_rot)\n    else:\n        rot_LAFs = max_rot\n        inv_rotmat = None\n    aff_LAFs, inv_TA = get_random_norm_affine_LAFs(data, max_tilt);\n    aff_LAFs[:,0:2,0:2] = torch.bmm(rot_LAFs[:,0:2,0:2],aff_LAFs[:,0:2,0:2])\n    data_aff = extract_patches(data,  aff_LAFs, PS = data.size(2))\n    data_affcrop = data_aff[:,:, st:fin, st:fin].contiguous()\n    return data_affcrop, data_aff, rot_LAFs,inv_rotmat,inv_TA \ndef train(train_loader, model, optimizer, epoch):\n    # switch to train mode\n    model.train()\n    pbar = tqdm(enumerate(train_loader))\n    for batch_idx, data in pbar:\n        data_a, data_p = data\n        if args.cuda:\n            data_a, data_p  = data_a.float().cuda(), data_p.float().cuda()\n            data_a, data_p = Variable(data_a), Variable(data_p)\n        rot_LAFs, inv_rotmat = get_random_rotation_LAFs(data_a, math.pi)\n        scale = Variable( 0.9 + 0.3* torch.rand(data_a.size(0), 1, 1));\n        if args.cuda:\n            scale = scale.cuda()\n        rot_LAFs[:,0:2,0:2] = rot_LAFs[:,0:2,0:2] * scale.expand(data_a.size(0),2,2)\n        shift_w, shift_h = get_random_shifts_LAFs(data_a, 2, 2)\n        rot_LAFs[:,0,2] = rot_LAFs[:,0,2] + shift_w / float(data_a.size(3))\n        rot_LAFs[:,1,2] = rot_LAFs[:,1,2] + shift_h / float(data_a.size(2))\n        data_a_rot = extract_patches(data_a,  rot_LAFs, PS = data_a.size(2))\n        st = int((data_p.size(2) - model.PS)/2)\n        fin = st + model.PS\n\n        data_p_crop = data_p[:,:, st:fin, st:fin].contiguous()\n        data_a_rot_crop = data_a_rot[:,:, st:fin, st:fin].contiguous()\n        out_a_rot, out_p, out_a = model(data_a_rot_crop,True), model(data_p_crop,True), model(data_a[:,:, st:fin, st:fin].contiguous(), True)\n        out_p_rotatad = torch.bmm(inv_rotmat, out_p)\n\n        ######Apply rot and get sifts\n        out_patches_a_crop = extract_and_crop_patches_by_predicted_transform(data_a_rot, out_a_rot, crop_size = model.PS)\n        out_patches_p_crop = extract_and_crop_patches_by_predicted_transform(data_p, out_p, crop_size = model.PS)\n\n        desc_a = descriptor(out_patches_a_crop)\n        desc_p = descriptor(out_patches_p_crop)\n        descr_dist =  torch.sqrt(((desc_a - desc_p)**2).view(data_a.size(0),-1).sum(dim=1) + 1e-6).mean()\n        geom_dist = torch.sqrt(((out_a_rot - out_p_rotatad)**2 ).view(-1,4).sum(dim=1)[0] + 1e-8).mean()\n        if args.loss == \'HardNet\':\n            loss = loss_HardNet(desc_a,desc_p); \n        elif args.loss == \'HardNetDetach\':\n            loss = loss_HardNetDetach(desc_a,desc_p); \n        elif args.loss == \'Geom\':\n            loss = geom_dist; \n        elif args.loss == \'PosDist\':\n            loss = descr_dist; \n        else:\n            print(\'Unknown loss function\')\n            sys.exit(1)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        adjust_learning_rate(optimizer)\n        if batch_idx % args.log_interval == 0:\n            pbar.set_description(\n                \'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}, {:.4f},{:.4f}\'.format(\n                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader),\n                    float(loss.detach().cpu().numpy()), float(geom_dist.detach().cpu().numpy()), float(descr_dist.detach().cpu().numpy())))\n    torch.save({\'epoch\': epoch + 1, \'state_dict\': model.state_dict()},\n               \'{}/checkpoint_{}.pth\'.format(LOG_DIR,epoch))\ndef load_grayscale_var(fname):\n    img = Image.open(fname).convert(\'RGB\')\n    img = np.mean(np.array(img), axis = 2)\n    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n    if args.cuda:\n        var_image_reshape = var_image_reshape.cuda()\n    return var_image_reshape\ndef get_geometry_and_descriptors(img, det, desc, do_ori = True):\n    with torch.no_grad():\n        LAFs, resp = det(img,do_ori = do_ori)\n        patches = det.extract_patches_from_pyr(LAFs, PS = 32)\n        descriptors = desc(patches)\n    return LAFs, descriptors\n\ndef test(model,epoch):\n    torch.cuda.empty_cache()\n    # switch to evaluate mode\n    model.eval()\n    from architectures import AffNetFast\n    affnet = AffNetFast()\n    model_weights = \'pretrained/AffNet.pth\'\n    hncheckpoint = torch.load(model_weights)\n    affnet.load_state_dict(hncheckpoint[\'state_dict\'])\n    affnet.eval()\n    detector = ScaleSpaceAffinePatchExtractor( mrSize = 5.192, num_features = 3000,\n                                          border = 5, num_Baum_iters = 1, \n                                          AffNet = affnet, OriNet = model)\n    descriptor = HardNet()\n    model_weights = \'HardNet++.pth\'\n    hncheckpoint = torch.load(model_weights)\n    descriptor.load_state_dict(hncheckpoint[\'state_dict\'])\n    descriptor.eval()\n    if args.cuda:\n        detector = detector.cuda()\n        descriptor = descriptor.cuda()\n    input_img_fname1 = \'test-graf/img1.png\'#sys.argv[1]\n    input_img_fname2 = \'test-graf/img6.png\'#sys.argv[1]\n    H_fname = \'test-graf/H1to6p\'#sys.argv[1]\n    output_img_fname = \'graf_match.png\'#sys.argv[3]\n    img1 = load_grayscale_var(input_img_fname1)\n    img2 = load_grayscale_var(input_img_fname2)\n    H = np.loadtxt(H_fname)    \n    H1to2 = Variable(torch.from_numpy(H).float())\n    SNN_threshold = 0.8\n    with torch.no_grad():\n        LAFs1, descriptors1 = get_geometry_and_descriptors(img1, detector, descriptor)\n        torch.cuda.empty_cache()\n        LAFs2, descriptors2 = get_geometry_and_descriptors(img2, detector, descriptor)\n        visualize_LAFs(img1.detach().cpu().numpy().squeeze(), LAFs1.detach().cpu().numpy().squeeze(), \'b\', show = False, save_to = LOG_DIR + ""/detections1_"" + str(epoch) + \'.png\')\n        visualize_LAFs(img2.detach().cpu().numpy().squeeze(), LAFs2.detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/detection2_"" + str(epoch) + \'.png\')\n        dist_matrix = distance_matrix_vector(descriptors1, descriptors2)\n        min_dist, idxs_in_2 = torch.min(dist_matrix,1)\n        dist_matrix[:,idxs_in_2] = 100000;# mask out nearest neighbour to find second nearest\n        min_2nd_dist, idxs_2nd_in_2 = torch.min(dist_matrix,1)\n        mask = (min_dist / (min_2nd_dist + 1e-8)) <= SNN_threshold\n        tent_matches_in_1 = indxs_in1 = torch.autograd.Variable(torch.arange(0, idxs_in_2.size(0)), requires_grad = False).cuda()[mask]\n        tent_matches_in_2 = idxs_in_2[mask]\n        tent_matches_in_1 = tent_matches_in_1.long()\n        tent_matches_in_2 = tent_matches_in_2.long()\n        LAF1s_tent = LAFs1[tent_matches_in_1,:,:]\n        LAF2s_tent = LAFs2[tent_matches_in_2,:,:]\n        min_dist, plain_indxs_in1, idxs_in_2 = get_GT_correspondence_indexes(LAF1s_tent, LAF2s_tent,H1to2.cuda(), dist_threshold = 6) \n        plain_indxs_in1 = plain_indxs_in1.long()\n        inl_ratio = float(plain_indxs_in1.size(0)) / float(tent_matches_in_1.size(0))\n        print \'Test epoch\', str(epoch) \n        print \'Test on graf1-6,\', tent_matches_in_1.size(0), \'tentatives\', plain_indxs_in1.size(0), \'true matches\', str(inl_ratio)[:5], \' inl.ratio\'\n        visualize_LAFs(img1.detach().cpu().numpy().squeeze(), LAF1s_tent[plain_indxs_in1.long(),:,:].detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/inliers1_"" + str(epoch) + \'.png\')\n        visualize_LAFs(img2.detach().cpu().numpy().squeeze(), LAF2s_tent[idxs_in_2.long(),:,:].detach().cpu().numpy().squeeze(), \'g\', show = False, save_to = LOG_DIR + ""/inliers2_"" + str(epoch) + \'.png\')\n    return\n\ndef adjust_learning_rate(optimizer):\n    """"""Updates the learning rate given the learning rate decay.\n    The routine has been implemented according to the original Lua SGD optimizer\n    """"""\n    for group in optimizer.param_groups:\n        if \'step\' not in group:\n            group[\'step\'] = 0.\n        else:\n            group[\'step\'] += 1.\n        group[\'lr\'] = args.lr * (\n        1.0 - float(group[\'step\']) * float(args.batch_size) / (args.n_pairs * float(args.epochs)))\n    return\n\ndef create_optimizer(model, new_lr):\n    optimizer = optim.SGD(model.parameters(), lr=new_lr,\n                          momentum=0.9, dampening=0.9,\n                          weight_decay=args.wd)\n    return optimizer\n\ndef main(train_loader, test_loader, model):\n    # print the experiment configuration\n    print(\'\\nparsed options:\\n{}\\n\'.format(vars(args)))\n    if args.cuda:\n        model.cuda()\n    optimizer1 = create_optimizer(model, args.lr)\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(\'=> loading checkpoint {}\'.format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            checkpoint = torch.load(args.resume)\n            model.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(\'=> no checkpoint found at {}\'.format(args.resume))\n    start = args.start_epoch\n    end = start + args.epochs\n    test(model, -1)\n    for epoch in range(start, end):\n        # iterate over test loaders and test results\n        train(train_loader, model, optimizer1, epoch)\n        test(model, epoch)\n    return 0\nif __name__ == \'__main__\':\n    LOG_DIR = args.log_dir\n    LOG_DIR = os.path.join(args.log_dir,suffix)\n    if not os.path.isdir(LOG_DIR):\n        os.makedirs(LOG_DIR)\n    from architectures import OriNetFast\n    model = OriNetFast(PS=32)\n    train_loader, test_loader = create_loaders()\n    main(train_loader, test_loader, model)\n\n'"
examples/direct_shape_optimization/HandCraftedModules.py,38,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nimport numpy as np\nfrom Utils import  GaussianBlur, CircularGaussKernel\nfrom LAF import abc2A,rectifyAffineTransformationUpIsUp, sc_y_x2LAFs,sc_y_x_and_A2LAFs\nfrom Utils import generate_2dgrid, generate_2dgrid, generate_3dgrid\nfrom Utils import zero_response_at_border\n\n\nclass ScalePyramid(nn.Module):\n    def __init__(self, nLevels = 3, init_sigma = 1.6, border = 5):\n        super(ScalePyramid,self).__init__()\n        self.nLevels = nLevels;\n        self.init_sigma = init_sigma\n        self.sigmaStep =  2 ** (1. / float(self.nLevels))\n        #print 'step',self.sigmaStep\n        self.b = border\n        self.minSize = 2 * self.b + 2 + 1;\n        return\n    def forward(self,x):\n        pixelDistance = 1.0;\n        curSigma = 0.5\n        if self.init_sigma > curSigma:\n            sigma = np.sqrt(self.init_sigma**2 - curSigma**2)\n            curSigma = self.init_sigma\n            curr = GaussianBlur(sigma = sigma)(x)\n        else:\n            curr = x\n        sigmas = [[curSigma]]\n        pixel_dists = [[1.0]]\n        pyr = [[curr]]\n        j = 0\n        while True:\n            curr = pyr[-1][0]\n            for i in range(1, self.nLevels + 2):\n                sigma = curSigma * np.sqrt(self.sigmaStep*self.sigmaStep - 1.0 )\n                #print 'blur sigma', sigma\n                curr = GaussianBlur(sigma = sigma)(curr)\n                curSigma *= self.sigmaStep\n                pyr[j].append(curr)\n                sigmas[j].append(curSigma)\n                pixel_dists[j].append(pixelDistance)\n                if i == self.nLevels:\n                    nextOctaveFirstLevel = F.avg_pool2d(curr, kernel_size = 1, stride = 2, padding = 0) \n            pixelDistance = pixelDistance * 2.0\n            curSigma = self.init_sigma\n            if (nextOctaveFirstLevel[0,0,:,:].size(0)  <= self.minSize) or (nextOctaveFirstLevel[0,0,:,:].size(1) <= self.minSize):\n                break\n            pyr.append([nextOctaveFirstLevel])\n            sigmas.append([curSigma])\n            pixel_dists.append([pixelDistance])\n            j+=1\n        return pyr, sigmas, pixel_dists\n\nclass HessianResp(nn.Module):\n    def __init__(self):\n        super(HessianResp, self).__init__()\n        \n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3), bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[0.5, 0, -0.5]]]], dtype=np.float32))\n\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[0.5], [0], [-0.5]]]], dtype=np.float32))\n\n        self.gxx =  nn.Conv2d(1, 1, kernel_size=(1,3),bias = False)\n        self.gxx.weight.data = torch.from_numpy(np.array([[[[1.0, -2.0, 1.0]]]], dtype=np.float32))\n        \n        self.gyy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gyy.weight.data = torch.from_numpy(np.array([[[[1.0], [-2.0], [1.0]]]], dtype=np.float32))\n        return\n    def forward(self, x, scale):\n        gxx = self.gxx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gyy = self.gyy(F.pad(x, (0,0, 1,1), 'replicate'))\n        gxy = self.gy(F.pad(self.gx(F.pad(x, (1,1,0, 0), 'replicate')), (0,0, 1,1), 'replicate'))\n        return torch.abs(gxx * gyy - gxy * gxy) * (scale**4)\n\n\nclass AffineShapeEstimator(nn.Module):\n    def __init__(self, threshold = 0.001, patch_size = 19):\n        super(AffineShapeEstimator, self).__init__()\n        self.threshold = threshold;\n        self.PS = patch_size\n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3), bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[-1, 0, 1]]]], dtype=np.float32))\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[-1], [0], [1]]]], dtype=np.float32))\n        self.gk = torch.from_numpy(CircularGaussKernel(kernlen = self.PS, sigma = (self.PS / 2) /3.0).astype(np.float32))\n        self.gk = Variable(self.gk, requires_grad=False)\n        return\n    def invSqrt(self,a,b,c):\n        eps = 1e-12\n        mask = (b != 0).float()\n        r1 = mask * (c - a) / (2. * b + eps)\n        t1 = torch.sign(r1) / (torch.abs(r1) + torch.sqrt(1. + r1*r1));\n        r = 1.0 / torch.sqrt( 1. + t1*t1)\n        t = t1*r;\n        r = r * mask + 1.0 * (1.0 - mask);\n        t = t * mask;\n        \n        x = 1. / torch.sqrt( r*r*a - 2.0*r*t*b + t*t*c)\n        z = 1. / torch.sqrt( t*t*a + 2.0*r*t*b + r*r*c)\n        \n        d = torch.sqrt( x * z)\n        \n        x = x / d\n        z = z / d\n        \n        l1 = torch.max(x,z)\n        l2 = torch.min(x,z)\n        \n        new_a = r*r*x + t*t*z\n        new_b = -r*t*x + t*r*z\n        new_c = t*t*x + r*r *z\n\n        return new_a, new_b, new_c, l1, l2\n    def forward(self,x):\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        else:\n            self.gk = self.gk.cpu()\n        gx = self.gx(F.pad(x, (1, 1, 0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0, 0, 1, 1), 'replicate'))\n        a1 = (gx * gx * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        b1 = (gx * gy * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        c1 = (gy * gy * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        a, b, c, l1, l2 = self.invSqrt(a1,b1,c1)\n        rat1 = l1/l2\n        mask = (torch.abs(rat1) <= 6.).float().view(-1);\n        return rectifyAffineTransformationUpIsUp(abc2A(a,b,c))#, mask\nclass OrientationDetector(nn.Module):\n    def __init__(self,\n                mrSize = 3.0, patch_size = None):\n        super(OrientationDetector, self).__init__()\n        if patch_size is None:\n            patch_size = 32;\n        self.PS = patch_size;\n        self.bin_weight_kernel_size, self.bin_weight_stride = self.get_bin_weight_kernel_size_and_stride(self.PS, 1)\n        self.mrSize = mrSize;\n        self.num_ang_bins = 36\n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3),  bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[0.5, 0, -0.5]]]], dtype=np.float32))\n\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[0.5], [0], [-0.5]]]], dtype=np.float32))\n\n        self.angular_smooth =  nn.Conv1d(1, 1, kernel_size=3, padding = 1, bias = False)\n        self.angular_smooth.weight.data = torch.from_numpy(np.array([[[0.33, 0.34, 0.33]]], dtype=np.float32))\n\n        self.gk = 10. * torch.from_numpy(CircularGaussKernel(kernlen=self.PS).astype(np.float32))\n        self.gk = Variable(self.gk, requires_grad=False)\n        return\n    def get_bin_weight_kernel_size_and_stride(self, patch_size, num_spatial_bins):\n        bin_weight_stride = int(round(2.0 * np.floor(patch_size / 2) / float(num_spatial_bins + 1)))\n        bin_weight_kernel_size = int(2 * bin_weight_stride - 1);\n        return bin_weight_kernel_size, bin_weight_stride\n    def get_rotation_matrix(self, angle_in_radians):\n        angle_in_radians = angle_in_radians.view(-1, 1, 1);\n        sin_a = torch.sin(angle_in_radians)\n        cos_a = torch.cos(angle_in_radians)\n        A1_x = torch.cat([cos_a, sin_a], dim = 2)\n        A2_x = torch.cat([-sin_a, cos_a], dim = 2)\n        transform = torch.cat([A1_x,A2_x], dim = 1)\n        return transform\n\n    def forward(self, x, return_rot_matrix = False):\n        gx = self.gx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0,0, 1,1), 'replicate'))\n        mag = torch.sqrt(gx * gx + gy * gy + 1e-10)\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        mag = mag * self.gk.unsqueeze(0).unsqueeze(0).expand_as(mag)\n        ori = torch.atan2(gy,gx)\n        o_big = float(self.num_ang_bins) *(ori + 1.0 * math.pi )/ (2.0 * math.pi)\n        bo0_big =  torch.floor(o_big)\n        wo1_big = o_big - bo0_big\n        bo0_big =  bo0_big %  self.num_ang_bins\n        bo1_big = (bo0_big + 1) % self.num_ang_bins\n        wo0_big = (1.0 - wo1_big) * mag\n        wo1_big = wo1_big * mag\n        ang_bins = []\n        for i in range(0, self.num_ang_bins):\n            ang_bins.append(F.adaptive_avg_pool2d((bo0_big == i).float() * wo0_big, (1,1)))\n        ang_bins = torch.cat(ang_bins,1).view(-1,1,self.num_ang_bins)\n        ang_bins = self.angular_smooth(ang_bins)\n        values, indices = ang_bins.view(-1,self.num_ang_bins).max(1)\n        angle =  -((2. * float(np.pi) * indices.float() / float(self.num_ang_bins)) - float(math.pi))\n        if return_rot_matrix:\n            return self.get_rotation_matrix(angle)\n        return angle\n    \nclass NMS2d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS2d, self).__init__()\n        self.MP = nn.MaxPool2d(kernel_size, stride=1, return_indices=False, padding = kernel_size/2)\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n\nclass NMS3d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS3d, self).__init__()\n        self.MP = nn.MaxPool3d(kernel_size, stride=1, return_indices=False, padding = (0, kernel_size//2, kernel_size//2))\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n        \nclass NMS3dAndComposeA(nn.Module):\n    def __init__(self, w = 0, h = 0, kernel_size = 3, threshold = 0, scales = None, border = 3, mrSize = 1.0):\n        super(NMS3dAndComposeA, self).__init__()\n        self.eps = 1e-7\n        self.ks = 3\n        self.th = threshold\n        self.cube_idxs = []\n        self.border = border\n        self.mrSize = mrSize\n        self.beta = 1.0\n        self.grid_ones = Variable(torch.ones(3,3,3,3), requires_grad=False)\n        self.NMS3d = NMS3d(kernel_size, threshold)\n        if (w > 0) and (h > 0):\n            self.spatial_grid = generate_2dgrid(h, w, False).view(1, h, w,2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        else:\n            self.spatial_grid = None\n        return\n    def forward(self, low, cur, high, num_features = 0, octaveMap = None, scales = None):\n        assert low.size() == cur.size() == high.size()\n        #Filter responce map\n        self.is_cuda = low.is_cuda;\n        resp3d = torch.cat([low,cur,high], dim = 1)\n        \n        mrSize_border = int(self.mrSize);\n        if octaveMap is not None:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border) * (1. - octaveMap.float())\n        else:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border)\n        \n        num_of_nonzero_responces = (nmsed_resp > 0).float().sum().item()#data[0]\n        if (num_of_nonzero_responces <= 1):\n            return None,None,None\n        if octaveMap is not None:\n            octaveMap = (octaveMap.float() + nmsed_resp.float()).byte()\n        \n        nmsed_resp = nmsed_resp.view(-1)\n        if (num_features > 0) and (num_features < num_of_nonzero_responces):\n            nmsed_resp, idxs = torch.topk(nmsed_resp, k = num_features, dim = 0);\n        else:\n            idxs = nmsed_resp.data.nonzero().squeeze()\n            nmsed_resp = nmsed_resp[idxs]\n        #Get point coordinates grid\n        \n        if type(scales) is not list:\n            self.grid = generate_3dgrid(3,self.ks,self.ks)\n        else:\n            self.grid = generate_3dgrid(scales,self.ks,self.ks)\n        self.grid = Variable(self.grid.t().contiguous().view(3,3,3,3), requires_grad=False)\n        if self.spatial_grid is None:\n            self.spatial_grid = generate_2dgrid(low.size(2), low.size(3), False).view(1, low.size(2), low.size(3),2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        if self.is_cuda:\n            self.spatial_grid = self.spatial_grid.cuda()\n            self.grid_ones = self.grid_ones.cuda()\n            self.grid = self.grid.cuda()\n        #residual_to_patch_center\n        sc_y_x = F.conv2d(resp3d, self.grid,\n                                padding = 1) / (F.conv2d(resp3d, self.grid_ones, padding = 1) + 1e-8)\n        \n        ##maxima coords\n        sc_y_x[0,1:,:,:] = sc_y_x[0,1:,:,:] + self.spatial_grid[:,:,:,0]\n        sc_y_x = sc_y_x.view(3,-1).t()\n        sc_y_x = sc_y_x[idxs,:]\n        \n        min_size = float(min((cur.size(2)), cur.size(3)))\n        sc_y_x[:,0] = sc_y_x[:,0] / min_size\n        sc_y_x[:,1] = sc_y_x[:,1] / float(cur.size(2))\n        sc_y_x[:,2] = sc_y_x[:,2] / float(cur.size(3))\n        return nmsed_resp, sc_y_x2LAFs(sc_y_x), octaveMap\nclass NMS3dAndComposeAAff(nn.Module):\n    def __init__(self, w = 0, h = 0, kernel_size = 3, threshold = 0, scales = None, border = 3, mrSize = 1.0):\n        super(NMS3dAndComposeAAff, self).__init__()\n        self.eps = 1e-7\n        self.ks = 3\n        self.th = threshold\n        self.cube_idxs = []\n        self.border = border\n        self.mrSize = mrSize\n        self.beta = 1.0\n        self.grid_ones = Variable(torch.ones(3,3,3,3), requires_grad=False)\n        self.NMS3d = NMS3d(kernel_size, threshold)\n        if (w > 0) and (h > 0):\n            self.spatial_grid = generate_2dgrid(h, w, False).view(1, h, w,2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        else:\n            self.spatial_grid = None\n        return\n    def forward(self, low, cur, high, num_features = 0, octaveMap = None, scales = None, aff_resp = None):\n        assert low.size() == cur.size() == high.size()\n        #Filter responce map\n        self.is_cuda = low.is_cuda;\n        resp3d = torch.cat([low,cur,high], dim = 1)\n        \n        mrSize_border = int(self.mrSize);\n        if octaveMap is not None:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border) * (1. - octaveMap.float())\n        else:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border)\n        \n        num_of_nonzero_responces = (nmsed_resp > 0).float().sum().item()#data[0]\n        if (num_of_nonzero_responces <= 1):\n            return None,None,None\n        if octaveMap is not None:\n            octaveMap = (octaveMap.float() + nmsed_resp.float()).byte()\n        \n        nmsed_resp = nmsed_resp.view(-1)\n        if (num_features > 0) and (num_features < num_of_nonzero_responces):\n            nmsed_resp, idxs = torch.topk(nmsed_resp, k = num_features, dim = 0);\n        else:\n            idxs = nmsed_resp.data.nonzero().squeeze()\n            nmsed_resp = nmsed_resp[idxs]\n        #Get point coordinates grid\n        if type(scales) is not list:\n            self.grid = generate_3dgrid(3,self.ks,self.ks)\n        else:\n            self.grid = generate_3dgrid(scales,self.ks,self.ks)\n        self.grid = Variable(self.grid.t().contiguous().view(3,3,3,3), requires_grad=False)\n        if self.spatial_grid is None:\n            self.spatial_grid = generate_2dgrid(low.size(2), low.size(3), False).view(1, low.size(2), low.size(3),2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        if self.is_cuda:\n            self.spatial_grid = self.spatial_grid.cuda()\n            self.grid_ones = self.grid_ones.cuda()\n            self.grid = self.grid.cuda()\n        \n        #residual_to_patch_center\n        sc_y_x = F.conv2d(resp3d, self.grid,\n                                padding = 1) / (F.conv2d(resp3d, self.grid_ones, padding = 1) + 1e-8)\n        \n        ##maxima coords\n        sc_y_x[0,1:,:,:] = sc_y_x[0,1:,:,:] + self.spatial_grid[:,:,:,0]\n        sc_y_x = sc_y_x.view(3,-1).t()\n        sc_y_x = sc_y_x[idxs,:]\n        if aff_resp is not None:\n            A_matrices = aff_resp.view(4,-1).t()[idxs,:]        \n        min_size = float(min((cur.size(2)), cur.size(3)))\n        \n        sc_y_x[:,0] = sc_y_x[:,0] / min_size\n        sc_y_x[:,1] = sc_y_x[:,1] / float(cur.size(2))\n        sc_y_x[:,2] = sc_y_x[:,2] / float(cur.size(3))\n        return nmsed_resp, sc_y_x_and_A2LAFs(sc_y_x,A_matrices), octaveMap\n"""
examples/direct_shape_optimization/HardNet.py,10,"b'import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nimport math\nimport numpy as np\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-8\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\nclass HardTFeatNet(nn.Module):\n    """"""TFeat model definition\n    """"""\n\n    def __init__(self, sm):\n        super(HardTFeatNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=7),\n            nn.Tanh(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=6),\n            nn.Tanh()\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(64, 128, kernel_size=8),\n            nn.Tanh())\n        self.SIFT = sm\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    \n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        x = self.classifier(x_features)\n        return L2Norm()(x.view(x.size(0), -1))\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self):\n        super(HardNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n        #self.features.apply(weights_init)\n\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n'"
examples/direct_shape_optimization/LAF.py,84,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\nfrom scipy.spatial.distance import cdist\nfrom numpy.linalg import inv    \nfrom scipy.linalg import schur, sqrtm\nimport torch\nfrom  torch.autograd import Variable\nimport torch.nn.functional as F\n##########numpy\ndef invSqrt(a,b,c):\n    eps = 1e-12 \n    mask = (b !=  0)\n    r1 = mask * (c - a) / (2. * b + eps)\n    t1 = np.sign(r1) / (np.abs(r1) + np.sqrt(1. + r1*r1));\n    r = 1.0 / np.sqrt( 1. + t1*t1)\n    t = t1*r;\n    \n    r = r * mask + 1.0 * (1.0 - mask);\n    t = t * mask;\n    \n    x = 1. / np.sqrt( r*r*a - 2*r*t*b + t*t*c)\n    z = 1. / np.sqrt( t*t*a + 2*r*t*b + r*r*c)\n    \n    d = np.sqrt( x * z)\n    \n    x = x / d\n    z = z / d\n       \n    new_a = r*r*x + t*t*z\n    new_b = -r*t*x + t*r*z\n    new_c = t*t*x + r*r *z\n\n    return new_a, new_b, new_c\ndef LAFs2ellT(LAFs):\n    ellipses = torch.zeros((len(LAFs),5))\n    if LAFs.is_cuda:\n        ellipses = ellipses.cuda()\n    scale = torch.sqrt(LAFs[:,0,0]*LAFs[:,1,1]  - LAFs[:,0,1]*LAFs[:,1, 0] + 1e-10)#.view(-1,1,1)\n    unscaled_As = LAFs[:,0:2,0:2] / scale.view(-1,1,1).repeat(1,2,2)\n    u, W, v = bsvd2x2(unscaled_As)\n    #W = 1.0 / ((W *scale.view(-1,1,1).repeat(1,2,2))**2) \n    W[:,0,0] = 1.0 /  (scale*scale*W[:,0,0]**2 )\n    W[:,1,1] = 1.0 /  (scale*scale*W[:,1,1]**2 )\n    A = torch.bmm(torch.bmm(u,W), u.permute(0,2,1))\n    ellipses[:,0] = LAFs[:,0,2]\n    ellipses[:,1] = LAFs[:,1,2]\n    ellipses[:,2] = A[:,0,0]\n    ellipses[:,3] = A[:,0,1]\n    ellipses[:,4] = A[:,1,1]\n    return ellipses\ndef invSqrtTorch(a,b,c):\n    eps = 1e-12\n    mask = (b != 0).float()\n    r1 = mask * (c - a) / (2. * b + eps)\n    t1 = torch.sign(r1) / (torch.abs(r1) + torch.sqrt(1. + r1*r1));\n    r = 1.0 / torch.sqrt( 1. + t1*t1)\n    t = t1*r;\n    r = r * mask + 1.0 * (1.0 - mask);\n    t = t * mask;\n\n    x = 1. / torch.sqrt( r*r*a - 2.0*r*t*b + t*t*c)\n    z = 1. / torch.sqrt( t*t*a + 2.0*r*t*b + r*r*c)\n\n    d = torch.sqrt( x * z)\n\n    x = x / d\n    z = z / d\n\n    new_a = r*r*x + t*t*z\n    new_b = -r*t*x + t*r*z\n    new_c = t*t*x + r*r *z\n\n    return new_a, new_b, new_c,\n    \ndef ells2LAFsT(ells):\n    LAFs = torch.zeros((len(ells), 2,3))\n    LAFs[:,0,2] = ells[:,0]\n    LAFs[:,1,2] = ells[:,1]\n    a = ells[:,2]\n    b = ells[:,3]\n    c = ells[:,4]\n    sc = torch.sqrt(torch.sqrt(a*c - b*b + 1e-12))\n    ia,ib,ic = invSqrtTorch(a,b,c)  #because sqrtm returns ::-1, ::-1 matrix, don`t know why \n    A = torch.cat([torch.cat([(ia/sc).view(-1,1,1), (ib/sc).view(-1,1,1)], dim = 2),\n                   torch.cat([(ib/sc).view(-1,1,1), (ic/sc).view(-1,1,1)], dim = 2)], dim = 1)\n    sc = torch.sqrt(torch.abs(A[:,0,0] * A[:,1,1] - A[:,1,0] * A[:,0,1]))\n    LAFs[:,0:2,0:2] = rectifyAffineTransformationUpIsUp(A / sc.view(-1,1,1).repeat(1,2,2)) * sc.view(-1,1,1).repeat(1,2,2)\n    return LAFs\n\ndef LAFs_to_H_frames(aff_pts):\n    H3_x = torch.Tensor([0, 0, 1 ]).unsqueeze(0).unsqueeze(0).repeat(aff_pts.size(0),1,1);\n    if aff_pts.is_cuda:\n        H3_x = H3_x.cuda()\n    return torch.cat([aff_pts, H3_x], dim = 1)\n\n\ndef checkTouchBoundary(LAFs):\n    pts = torch.FloatTensor([[-1, -1, 1, 1], [-1, 1, -1, 1], [1, 1, 1, 1]]).unsqueeze(0)\n    if LAFs.is_cuda:\n        pts = pts.cuda()\n    out_pts =  torch.bmm(LAFs_to_H_frames(LAFs),pts.expand(LAFs.size(0),3,4))[:,:2,:]\n    good_points = 1 -(((out_pts > 1.0) +  (out_pts < 0.0)).sum(dim=1).sum(dim=1) > 0)\n    return good_points\n\ndef bsvd2x2(As):\n    Su = torch.bmm(As,As.permute(0,2,1))\n    phi = 0.5 * torch.atan2(Su[:,0,1] + Su[:,1,0] + 1e-12, Su[:,0,0] - Su[:,1,1] + 1e-12)\n    Cphi = torch.cos(phi)\n    Sphi = torch.sin(phi)\n    U = torch.zeros(As.size(0),2,2)\n    if As.is_cuda:\n        U = U.cuda()\n    U[:,0,0] = Cphi\n    U[:,1,1] = Cphi\n    U[:,0,1] = -Sphi\n    U[:,1,0] = Sphi\n    Sw = torch.bmm(As.permute(0,2,1),As)\n    theta = 0.5 * torch.atan2(Sw[:,0,1] + Sw[:,1,0] + 1e-12, Sw[:,0,0] - Sw[:,1,1] + 1e-12)\n    Ctheta = torch.cos(theta)\n    Stheta = torch.sin(theta)\n    W = torch.zeros(As.size(0),2,2)\n    if As.is_cuda:\n        W = W.cuda()\n    W[:,0,0] = Ctheta\n    W[:,1,1] = Ctheta\n    W[:,0,1] = -Stheta\n    W[:,1,0] = Stheta\n    SUsum = Su[:,0,0] + Su[:,1,1]\n    SUdif = torch.sqrt((Su[:,0,0] - Su[:,1,1])**2 + 4 * Su[:,0,1]*Su[:,1,0] + 1e-12)\n    if As.is_cuda:\n        SIG = torch.zeros(As.size(0),2,2).cuda()\n        SIG[:,0,0] = torch.sqrt((SUsum+SUdif)/2.0)\n        SIG[:,1,1] = torch.sqrt((SUsum-SUdif)/2.0)\n    else:\n        SIG = torch.zeros(As.size(0),2,2)\n        SIG[:,0,0] = torch.sqrt((SUsum+SUdif)/2.0)\n        SIG[:,1,1] = torch.sqrt((SUsum-SUdif)/2.0)\n    S = torch.bmm(torch.bmm(U.permute(0,2,1),As),W)\n    C = torch.sign(S)\n    C[:,0,1] = 0\n    C[:,1,0] = 0\n    V = torch.bmm(W,C)\n    return (U,SIG,V)\n\ndef getLAFelongation(LAFs):\n    u,s,v = bsvd2x2(LAFs[:,:2,:2])\n    return torch.max(s[:,0,0],s[:,1,1]) / torch.min(s[:,0,0],s[:,1,1])\n\ndef getNumCollapsed(LAFs, th = 10.0):\n    el = getLAFelongation(LAFs)\n    return (el > th).float().sum()\n\ndef Ell2LAF(ell):\n    A23 = np.zeros((2,3))\n    A23[0,2] = ell[0]\n    A23[1,2] = ell[1]\n    a = ell[2]\n    b = ell[3]\n    c = ell[4]\n    sc = np.sqrt(np.sqrt(a*c - b*b))\n    ia,ib,ic = invSqrt(a,b,c)  #because sqrtm returns ::-1, ::-1 matrix, don`t know why \n    A = np.array([[ia, ib], [ib, ic]]) / sc\n    sc = np.sqrt(A[0,0] * A[1,1] - A[1,0] * A[0,1])\n    A23[0:2,0:2] = rectifyAffineTransformationUpIsUp(A / sc) * sc\n    return A23\n\ndef rectifyAffineTransformationUpIsUp_np(A):\n    det = np.sqrt(np.abs(A[0,0]*A[1,1] - A[1,0]*A[0,1] + 1e-10))\n    b2a2 = np.sqrt(A[0,1] * A[0,1] + A[0,0] * A[0,0])\n    A_new = np.zeros((2,2))\n    A_new[0,0] = b2a2 / det\n    A_new[0,1] = 0\n    A_new[1,0] = (A[1,1]*A[0,1]+A[1,0]*A[0,0])/(b2a2*det)\n    A_new[1,1] = det / b2a2\n    return A_new\n\ndef ells2LAFs(ells):\n    LAFs = np.zeros((len(ells), 2,3))\n    for i in range(len(ells)):\n        LAFs[i,:,:] = Ell2LAF(ells[i,:])\n    return LAFs\n\ndef LAF2pts(LAF, n_pts = 50):\n    a = np.linspace(0, 2*np.pi, n_pts);\n    x = [0]\n    x.extend(list(np.sin(a)))\n    x = np.array(x).reshape(1,-1)\n    y = [0]\n    y.extend(list(np.cos(a)))\n    y = np.array(y).reshape(1,-1)\n    HLAF = np.concatenate([LAF, np.array([0,0,1]).reshape(1,3)])\n    H_pts =np.concatenate([x,y,np.ones(x.shape)])\n    H_pts_out = np.transpose(np.matmul(HLAF, H_pts))\n    H_pts_out[:,0] = H_pts_out[:,0] / H_pts_out[:, 2]\n    H_pts_out[:,1] = H_pts_out[:,1] / H_pts_out[:, 2]\n    return H_pts_out[:,0:2]\n\n\ndef convertLAFs_to_A23format(LAFs):\n    sh = LAFs.shape\n    if (len(sh) == 3) and (sh[1]  == 2) and (sh[2] == 3): # n x 2 x 3 classical [A, (x;y)] matrix\n        work_LAFs = deepcopy(LAFs)\n    elif (len(sh) == 2) and (sh[1]  == 7): #flat format, x y scale a11 a12 a21 a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] * LAFs[:,3] \n        work_LAFs[:,0,1] = LAFs[:,2] * LAFs[:,4]\n        work_LAFs[:,1,0] = LAFs[:,2] * LAFs[:,5]\n        work_LAFs[:,1,1] = LAFs[:,2] * LAFs[:,6]\n    elif (len(sh) == 2) and (sh[1]  == 6): #flat format, x y s*a11 s*a12 s*a21 s*a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] \n        work_LAFs[:,0,1] = LAFs[:,3]\n        work_LAFs[:,1,0] = LAFs[:,4]\n        work_LAFs[:,1,1] = LAFs[:,5]\n    else:\n        print ('Unknown LAF format')\n        return None\n    return work_LAFs\n\ndef LAFs2ell(in_LAFs):\n    LAFs = convertLAFs_to_A23format(in_LAFs)\n    ellipses = np.zeros((len(LAFs),5))\n    for i in range(len(LAFs)):\n        LAF = deepcopy(LAFs[i,:,:])\n        scale = np.sqrt(LAF[0,0]*LAF[1,1]  - LAF[0,1]*LAF[1, 0] + 1e-10)\n        u, W, v = np.linalg.svd(LAF[0:2,0:2] / scale, full_matrices=True)\n        W[0] = 1. / (W[0]*W[0]*scale*scale)\n        W[1] = 1. / (W[1]*W[1]*scale*scale)\n        A =  np.matmul(np.matmul(u, np.diag(W)), u.transpose())\n        ellipses[i,0] = LAF[0,2]\n        ellipses[i,1] = LAF[1,2]\n        ellipses[i,2] = A[0,0]\n        ellipses[i,3] = A[0,1]\n        ellipses[i,4] = A[1,1]\n    return ellipses\n\ndef visualize_LAFs(img, LAFs, color = 'r', show = False, save_to = None):\n    work_LAFs = convertLAFs_to_A23format(LAFs)\n    plt.figure()\n    plt.imshow(255 - img)\n    for i in range(len(work_LAFs)):\n        ell = LAF2pts(work_LAFs[i,:,:])\n        plt.plot( ell[:,0], ell[:,1], color)\n    if show:\n        plt.show()\n    if save_to is not None:\n        plt.savefig(save_to)\n    return \n\n####pytorch\n\ndef get_normalized_affine_shape(tilt, angle_in_radians):\n    assert tilt.size(0) == angle_in_radians.size(0)\n    num = tilt.size(0)\n    tilt_A = Variable(torch.eye(2).view(1,2,2).repeat(num,1,1))\n    if tilt.is_cuda:\n        tilt_A = tilt_A.cuda()\n    tilt_A[:,0,0] = tilt.view(-1);\n    rotmat = get_rotation_matrix(angle_in_radians)\n    out_A = rectifyAffineTransformationUpIsUp(torch.bmm(rotmat, torch.bmm(tilt_A, rotmat)))\n    #re_scale = (1.0/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    #re_scale = (0.5 + 0.5/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    return out_A# * re_scale.view(-1,1,1).expand(num,2,2)\n\ndef get_rotation_matrix(angle_in_radians):\n    angle_in_radians = angle_in_radians.view(-1, 1, 1);\n    sin_a = torch.sin(angle_in_radians)\n    cos_a = torch.cos(angle_in_radians)\n    A1_x = torch.cat([cos_a, sin_a], dim = 2)\n    A2_x = torch.cat([-sin_a, cos_a], dim = 2)\n    transform = torch.cat([A1_x,A2_x], dim = 1)\n    return transform\n    \ndef rectifyAffineTransformationUpIsUp(A):\n    det = torch.sqrt(torch.abs(A[:,0,0]*A[:,1,1] - A[:,1,0]*A[:,0,1] + 1e-10))\n    b2a2 = torch.sqrt(A[:,0,1] * A[:,0,1] + A[:,0,0] * A[:,0,0])\n    A1_ell = torch.cat([(b2a2 / det).contiguous().view(-1,1,1), 0 * det.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([((A[:,1,1]*A[:,0,1]+A[:,1,0]*A[:,0,0])/(b2a2*det)).contiguous().view(-1,1,1),\n                        (det / b2a2).contiguous().view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\ndef rectifyAffineTransformationUpIsUpFullyConv(A):#A is (n,4,h,w) tensor\n    det = torch.sqrt(torch.abs(A[:,0:1,:,:]*A[:,3:4,:,:] - A[:,1:2,:,:]*A[:,2:3,:,:] + 1e-10))\n    b2a2 = torch.sqrt(A[:,1:2,:,:] * A[:,1:2,:,:] + A[:,0:1,:,:] * A[:,0:1,:,:])\n    return torch.cat([(b2a2 / det).contiguous(),0 * det.contiguous(),\n                      (A[:,3:4,:,:]*A[:,1:2,:,:]+A[:,2:3,:,:]*A[:,0:1,:,:])/(b2a2*det),(det / b2a2).contiguous()], dim = 1)\n\ndef abc2A(a,b,c, normalize = False):\n    A1_ell = torch.cat([a.view(-1,1,1), b.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([b.view(-1,1,1), c.view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\n\n\ndef angles2A(angles):\n    cos_a = torch.cos(angles).view(-1, 1, 1)\n    sin_a = torch.sin(angles).view(-1, 1, 1)\n    A1_ang = torch.cat([cos_a, sin_a], dim = 2)\n    A2_ang = torch.cat([-sin_a, cos_a], dim = 2)\n    return  torch.cat([A1_ang, A2_ang], dim = 1)\n\ndef generate_patch_grid_from_normalized_LAFs(LAFs, w, h, PS):\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3) * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    grid = F.affine_grid(LAFs * Variable(coef.expand(num_lafs,2,3)), torch.Size((num_lafs,1,PS,PS)))\n    grid[:,:,:,0] = 2.0 * grid[:,:,:,0] / float(w)  - 1.0\n    grid[:,:,:,1] = 2.0 * grid[:,:,:,1] / float(h)  - 1.0     \n    return grid\n\ndef batched_grid_apply(img, grid, batch_size = 32):\n    n_patches = len(grid)\n    if n_patches > batch_size:\n        bs = batch_size\n        n_batches = int(n_patches / bs + 1)\n        for batch_idx in range(n_batches):\n            st = batch_idx * bs\n            if batch_idx == n_batches - 1:\n                if (batch_idx + 1) * bs > n_patches:\n                    end = n_patches\n                else:\n                    end = (batch_idx + 1) * bs\n            else:\n                end = (batch_idx + 1) * bs\n            if st >= end:\n                continue\n            if batch_idx == 0:\n                if img.size(0) != grid.size(0):\n                    first_batch_out = F.grid_sample(img.expand(end - st, img.size(1), img.size(2), img.size(3)), grid[st:end, :,:,:])# kwargs)\n                else:\n                    first_batch_out = F.grid_sample(img[st:end], grid[st:end, :,:,:])# kwargs)\n                out_size = torch.Size([n_patches] + list(first_batch_out.size()[1:]))\n                out = torch.zeros(out_size);\n                if img.is_cuda:\n                    out = out.cuda()\n                out[st:end] = first_batch_out\n            else:\n                if img.size(0) != grid.size(0):\n                    out[st:end,:,:] = F.grid_sample(img.expand(end - st, img.size(1), img.size(2), img.size(3)), grid[st:end, :,:,:])\n                else:\n                    out[st:end,:,:] = F.grid_sample(img[st:end], grid[st:end, :,:,:])\n        return out\n    else:\n        if img.size(0) != grid.size(0):\n            return F.grid_sample(img.expand(grid.size(0), img.size(1), img.size(2), img.size(3)), grid)\n        else:\n            return F.grid_sample(img, grid)\n\ndef extract_patches(img, LAFs, PS = 32, bs = 32):\n    w = img.size(3)\n    h = img.size(2)\n    ch = img.size(1)\n    grid = generate_patch_grid_from_normalized_LAFs(LAFs, float(w),float(h), PS)\n    if bs is None:\n        return torch.nn.functional.grid_sample(img.expand(grid.size(0), ch, h, w),  grid)  \n    else:\n        return batched_grid_apply(img, grid, bs)\ndef get_pyramid_inverted_index_for_LAFs(LAFs, PS, sigmas):\n    return\n\ndef extract_patches_from_pyramid_with_inv_index(scale_pyramid, pyr_inv_idxs, LAFs, PS = 19):\n    patches = torch.zeros(LAFs.size(0),scale_pyramid[0][0].size(1), PS, PS)\n    if LAFs.is_cuda:\n        patches = patches.cuda()\n    patches = Variable(patches)\n    if pyr_inv_idxs is not None:\n        for i in range(len(scale_pyramid)):\n            for j in range(len(scale_pyramid[i])):\n                cur_lvl_idxs = pyr_inv_idxs[i][j]\n                if cur_lvl_idxs is None:\n                    continue\n                cur_lvl_idxs = cur_lvl_idxs.view(-1)\n                #print i,j,cur_lvl_idxs.shape\n                patches[cur_lvl_idxs,:,:,:] = extract_patches(scale_pyramid[i][j], LAFs[cur_lvl_idxs, :,:], PS, 32 )\n    return patches\n\ndef get_inverted_pyr_index(scale_pyr, pyr_idxs, level_idxs):\n    pyr_inv_idxs = []\n    ### Precompute octave inverted indexes\n    for i in range(len(scale_pyr)):\n        pyr_inv_idxs.append([])\n        cur_idxs = pyr_idxs == i #torch.nonzero((pyr_idxs == i).data)\n        for j in range(0, len(scale_pyr[i])):\n            cur_lvl_idxs = torch.nonzero(((level_idxs == j) * cur_idxs).data)\n            if cur_lvl_idxs.size(0) == 0:\n                pyr_inv_idxs[i].append(None)\n            else:\n                pyr_inv_idxs[i].append(cur_lvl_idxs.squeeze())\n    return pyr_inv_idxs\n\n\ndef denormalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef normalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  / min_size\n    coef[0,0,2] = 1.0 / w\n    coef[0,1,2] = 1.0 / h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef sc_y_x2LAFs(sc_y_x):\n    base_LAF = torch.eye(2).float().unsqueeze(0).expand(sc_y_x.size(0),2,2)\n    if sc_y_x.is_cuda:\n        base_LAF = base_LAF.cuda()\n    base_A = Variable(base_LAF, requires_grad=False)\n    A = sc_y_x[:,:1].unsqueeze(1).expand_as(base_A) * base_A\n    LAFs  = torch.cat([A,\n                       torch.cat([sc_y_x[:,2:].unsqueeze(-1),\n                                    sc_y_x[:,1:2].unsqueeze(-1)], dim=1)], dim = 2)\n        \n    return LAFs\ndef sc_y_x_and_A2LAFs(sc_y_x, A_flat):\n    base_A = A_flat.view(-1,2,2)\n    A = sc_y_x[:,:1].unsqueeze(1).expand_as(base_A) * base_A\n    LAFs  = torch.cat([A,\n                       torch.cat([sc_y_x[:,2:].unsqueeze(-1),\n                                    sc_y_x[:,1:2].unsqueeze(-1)], dim=1)], dim = 2)\n        \n    return LAFs\ndef get_LAFs_scales(LAFs):\n    return torch.sqrt(torch.abs(LAFs[:,0,0] *LAFs[:,1,1] - LAFs[:,0,1] * LAFs[:,1,0]) + 1e-12)\n\ndef get_pyramid_and_level_index_for_LAFs(dLAFs,  sigmas, pix_dists, PS):\n    scales = get_LAFs_scales(dLAFs);\n    needed_sigmas = scales / PS;\n    sigmas_full_list = []\n    level_idxs_full = []\n    oct_idxs_full = []\n    for oct_idx in range(len(sigmas)):\n        sigmas_full_list = sigmas_full_list + list(np.array(sigmas[oct_idx])*np.array(pix_dists[oct_idx]))\n        oct_idxs_full = oct_idxs_full + [oct_idx]*len(sigmas[oct_idx])\n        level_idxs_full = level_idxs_full + list(range(0,len(sigmas[oct_idx])))\n    oct_idxs_full = torch.LongTensor(oct_idxs_full)\n    level_idxs_full = torch.LongTensor(level_idxs_full)\n    \n    closest_imgs = cdist(np.array(sigmas_full_list).reshape(-1,1), needed_sigmas.data.cpu().numpy().reshape(-1,1)).argmin(axis = 0)\n    closest_imgs = torch.from_numpy(closest_imgs)\n    if dLAFs.is_cuda:\n        closest_imgs = closest_imgs.cuda()\n        oct_idxs_full = oct_idxs_full.cuda()\n        level_idxs_full = level_idxs_full.cuda()\n    return  Variable(oct_idxs_full[closest_imgs]), Variable(level_idxs_full[closest_imgs])\n"""
examples/direct_shape_optimization/Losses.py,51,"b'import torch\nimport torch.nn as nn\nimport sys\n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1).unsqueeze(-1)\n    d2_sq = torch.sum(positive * positive, dim=1).unsqueeze(-1)\n\n    eps = 1e-6\n    return torch.sqrt((d1_sq.repeat(1, positive.size(0)) + torch.t(d2_sq.repeat(1, anchor.size(0)))\n                      - 2.0 * torch.bmm(anchor.unsqueeze(0), torch.t(positive).unsqueeze(0)).squeeze(0))+eps)\n\ndef distance_vectors_pairwise(anchor, positive, negative = None):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    a_sq = torch.sum(anchor * anchor, dim=1)\n    p_sq = torch.sum(positive * positive, dim=1)\n\n    eps = 1e-8\n    d_a_p = torch.sqrt(a_sq + p_sq - 2*torch.sum(anchor * positive, dim = 1) + eps)\n    if negative is not None:\n        n_sq = torch.sum(negative * negative, dim=1)\n        d_a_n = torch.sqrt(a_sq + n_sq - 2*torch.sum(anchor * negative, dim = 1) + eps)\n        d_p_n = torch.sqrt(p_sq + n_sq - 2*torch.sum(positive * negative, dim = 1) + eps)\n        return d_a_p, d_a_n, d_p_n\n    return d_a_p\n\n\ndef loss_random_sampling(anchor, positive, negative, anchor_swap = False, margin = 1.0, loss_type = ""triplet_margin""):\n    """"""Loss with random sampling (no hard in batch).\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.size() == negative.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    (pos, d_a_n, d_p_n) = distance_vectors_pairwise(anchor, positive, negative)\n    if anchor_swap:\n       min_neg = torch.min(d_a_n, d_p_n)\n    else:\n       min_neg = d_a_n\n\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_L2Net(anchor, positive, anchor_swap = False,  margin = 1.0, loss_type = ""triplet_margin""):\n    """"""L2Net losses: using whole batch as negatives, not only hardest.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008)-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    \n    if loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos1);\n        exp_den = torch.sum(torch.exp(2.0 - dist_matrix),1) + eps;\n        loss = -torch.log( exp_pos / exp_den )\n        if anchor_swap:\n            exp_den1 = torch.sum(torch.exp(2.0 - dist_matrix),0) + eps;\n            loss += -torch.log( exp_pos / exp_den1 )\n    else: \n        print (\'Only softmax loss works with L2Net sampling\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_HardNet(anchor, positive, anchor_swap = False, anchor_ave = False,\\\n        margin = 1.0, batch_reduce = \'min\', loss_type = ""triplet_margin""):\n    """"""HardNet margin loss - calculates loss based on distance matrix based on positive distance and closest negative distance.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive) +eps\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008).float()-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    if batch_reduce == \'min\':\n        min_neg = torch.min(dist_without_min_on_diag,1)[0]\n        if anchor_swap:\n            min_neg2 = torch.min(dist_without_min_on_diag,0)[0]\n            min_neg = torch.min(min_neg,min_neg2)\n        if False:\n            dist_matrix_a = distance_matrix_vector(anchor, anchor)+ eps\n            dist_matrix_p = distance_matrix_vector(positive,positive)+eps\n            dist_without_min_on_diag_a = dist_matrix_a+eye*10\n            dist_without_min_on_diag_p = dist_matrix_p+eye*10\n            min_neg_a = torch.min(dist_without_min_on_diag_a,1)[0]\n            min_neg_p = torch.t(torch.min(dist_without_min_on_diag_p,0)[0])\n            min_neg_3 = torch.min(min_neg_p,min_neg_a)\n            min_neg = torch.min(min_neg,min_neg_3)\n            print (min_neg_a)\n            print (min_neg_p)\n            print (min_neg_3)\n            print (min_neg)\n        min_neg = min_neg\n        pos = pos1\n    elif batch_reduce == \'average\':\n        pos = pos1.repeat(anchor.size(0)).view(-1,1).squeeze(0)\n        min_neg = dist_without_min_on_diag.view(-1,1)\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).contiguous().view(-1,1)\n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = min_neg.squeeze(0)\n    elif batch_reduce == \'random\':\n        idxs = torch.autograd.Variable(torch.randperm(anchor.size()[0]).long()).cuda()\n        min_neg = dist_without_min_on_diag.gather(1,idxs.view(-1,1))\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).gather(1,idxs.view(-1,1)) \n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = torch.t(min_neg).squeeze(0)\n        pos = pos1\n    else: \n        print (\'Unknown batch reduce mode. Try min, average or random\')\n        sys.exit(1)\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\n\ndef global_orthogonal_regularization(anchor, negative):\n\n    neg_dis = torch.sum(torch.mul(anchor,negative),1)\n    dim = anchor.size(1)\n    gor = torch.pow(torch.mean(neg_dis),2) + torch.clamp(torch.mean(torch.pow(neg_dis,2))-1.0/dim, min=0.0)\n    \n    return gor\n\ndef get_snn(anchor, positive):\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-12\n    dist_matrix_detach = distance_matrix_vector(anchor, positive) + eps\n    pos1 = distance_vectors_pairwise(anchor,positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix_detach.size(1))).cuda()\n    # steps to filter out same patches that occur in distance matrix as negatives\n    dist_without_min_on_diag = dist_matrix_detach + eye*10\n    mask = (dist_without_min_on_diag.ge(0.08).float()-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag + mask\n    min_neg = torch.min(dist_without_min_on_diag,1)[0]\n    return pos1/(min_neg + 1e-8)'"
examples/direct_shape_optimization/ReprojectionStuff.py,66,"b'import torch\nfrom torch.autograd import Variable\nfrom torch.autograd import Variable as V\nimport numpy as np\nfrom LAF import rectifyAffineTransformationUpIsUp, LAFs_to_H_frames\nfrom Utils import zeros_like\n\n\ndef linH(H, x, y):\n    assert x.size(0) == y.size(0)\n    A = torch.zeros(x.size(0),2,2)\n    if x.is_cuda:\n        A = A.cuda()\n    den = x * H[2,0] + y * H[2,1] + H[2,2]\n    num1_densq = (x*H[0,0] + y*H[0,1] + H[0,2]) / (den*den)\n    num2_densq = (x*H[1,0] + y*H[1,1] + H[1,2]) / (den*den)\n    A[:,0,0] = H[0,0]/den - num1_densq * H[2,0]\n    A[:,0,1] = H[0,1]/den - num1_densq * H[2,1]\n    A[:,1,0] = H[1,0]/den - num2_densq * H[2,0]\n    A[:,1,1] = H[1,1]/den - num2_densq * H[2,1]\n    return A\n\ndef reprojectLAFs(LAFs1, H1to2, return_LHFs = False):\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    #LHF1_in_2 = torch.zeros(LHF1.size(0), ,3,3)\n    #if LHF1.is_cuda:\n    #    LHF1_in_2 = LHF1_in_2.cuda()\n    #LHF1_in_2 = Variable(LHF1_in_2)\n    #LHF1_in_2[:,:,2] = torch.bmm(H1to2.expand(LHF1.size(0),3,3), LHF1[:,:,2:])\n    #LHF1_in_2[:,:,2] = LHF1_in_2[:,:,2] / LHF1_in_2[:,2:,2].expand(LHF1_in_2.size(0), 3)\n    #As  = linH(H1to2, LAFs1[:,0,2], LAFs1[:,1,2])\n    #LHF1_in_2[:,0:2,0:2] = torch.bmm(As, LHF1[:,0:2,0:2])\n    xy1 = torch.bmm(H1to2.expand(LHF1.size(0),3,3), LHF1[:,:,2:])\n    xy1 = xy1 / xy1[:,2:,:].expand(xy1.size(0), 3, 1)\n    As  = linH(H1to2, LAFs1[:,0,2], LAFs1[:,1,2])\n    AF = torch.bmm(As, LHF1[:,0:2,0:2])\n    \n    if return_LHFs:\n        return LAFs_to_H_frames(torch.cat([AF, xy1[:,:2,:]], dim = 2))\n    return torch.cat([AF, xy1[:,:2,:]], dim = 2)\n\ndef Px2GridA(w, h):\n    A = torch.eye(3)\n    A[0,0] = 2.0  / float(w)\n    A[1,1] = 2.0  / float(h)\n    A[0,2] = -1\n    A[1,2] = -1\n    return A\ndef Grid2PxA(w, h):\n    A = torch.eye(3)\n    A[0,0] = float(w) / 2.0\n    A[0,2] = float(w) / 2.0\n    A[1,1] = float(h) / 2.0\n    A[1,2] = float(h) / 2.0\n    return A\n\ndef affineAug(img, max_add = 0.5):\n    img_s = img.squeeze()\n    h,w = img_s.size()\n    ### Generate A\n    A = torch.eye(3)\n    rand_add = max_add *(torch.rand(3,3) - 0.5) * 2.0\n    ##No perspective change\n    rand_add[2,0:2] = 0\n    rand_add[2,2] = 0;\n    A  = A + rand_add\n    denormA = Grid2PxA(w,h)\n    normA = Px2GridA(w, h)\n    if img.is_cuda:\n        A = A.cuda()\n        denormA = denormA.cuda()\n        normA = normA.cuda()\n    grid = torch.nn.functional.affine_grid(A[0:2,:].unsqueeze(0), torch.Size((1,1,h,w)))\n    H_Orig2New = torch.mm(torch.mm(denormA, torch.inverse(A)), normA)\n    new_img = torch.nn.functional.grid_sample(img_s.float().unsqueeze(0).unsqueeze(0),  grid)  \n    return new_img, H_Orig2New, \n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1)\n    d2_sq = torch.sum(positive * positive, dim=1)\n    eps = 1e-12\n    return torch.sqrt(torch.abs((d1_sq.expand(positive.size(0), anchor.size(0)) +\n                       torch.t(d2_sq.expand(anchor.size(0), positive.size(0)))\n                      - 2.0 * torch.bmm(positive.unsqueeze(0), torch.t(anchor).unsqueeze(0)).squeeze(0))+eps))\n\ndef ratio_matrix_vector(a, p):\n    eps = 1e-12\n    return a.expand(p.size(0), a.size(0)) / (torch.t(p.expand(a.size(0), p.size(0))) + eps)\n\n   \ndef inverseLHFs(LHFs):\n    LHF1_inv =torch.zeros(LHFs.size())\n    if LHFs.is_cuda:\n        LHF1_inv = LHF1_inv.cuda()\n    for i in range(LHF1_inv.size(0)):\n        LHF1_inv[i,:,:] = LHFs[i,:,:].inverse()\n    return LHF1_inv\n\n    \ndef reproject_to_canonical_Frob_batched(LHF1_inv, LHF2, batch_size = 2, skip_center = False):\n    out = torch.zeros((LHF1_inv.size(0), LHF2.size(0)))\n    eye1 = torch.eye(3)\n    if LHF1_inv.is_cuda:\n        out = out.cuda()\n        eye1 = eye1.cuda()\n    len1 = LHF1_inv.size(0)\n    len2 = LHF2.size(0)\n    n_batches = int(np.floor(len1 / batch_size) + 1);\n    for b_idx in range(n_batches):\n        #print b_idx\n        start = b_idx * batch_size;\n        fin = min((b_idx+1) * batch_size, len1)\n        current_bs = fin - start\n        if current_bs == 0:\n            break\n        should_be_eyes = torch.bmm(LHF1_inv[start:fin, :, :].unsqueeze(0).expand(len2,current_bs, 3, 3).contiguous().view(-1,3,3),\n                                   LHF2.unsqueeze(1).expand(len2,current_bs, 3,3).contiguous().view(-1,3,3))\n        if skip_center:\n            out[start:fin, :] = torch.sum(((should_be_eyes - eye1.unsqueeze(0).expand_as(should_be_eyes))**2)[:,:2,:2] , dim=1).sum(dim = 1).view(current_bs, len2)\n        else:\n            out[start:fin, :] = torch.sum((should_be_eyes - eye1.unsqueeze(0).expand_as(should_be_eyes))**2 , dim=1).sum(dim = 1).view(current_bs, len2)\n    return out\n\ndef get_GT_correspondence_indexes(LAFs1, LAFs2, H1to2, dist_threshold = 4):    \n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    just_centers1 = LAFs1[:,:,2];\n    just_centers2_repr_to_1 = LHF2_in_1_pre[:,0:2,2];\n    \n    dist  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1)\n    min_dist, idxs_in_2 = torch.min(dist,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    mask =  min_dist <= dist_threshold\n    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n\ndef get_GT_correspondence_indexes_Fro(LAFs1,LAFs2, H1to2, dist_threshold = 4,\n                                      skip_center_in_Fro = False):\n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1_inv = inverseLHFs(LAFs_to_H_frames(LAFs1))\n    frob_norm_dist = reproject_to_canonical_Frob_batched(LHF1_inv, LHF2_in_1_pre, batch_size = 2, skip_center = skip_center_in_Fro)\n    min_dist, idxs_in_2 = torch.min(frob_norm_dist,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    #print min_dist.min(), min_dist.max(), min_dist.mean()\n    mask =  min_dist <= dist_threshold\n    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n\ndef get_GT_correspondence_indexes_Fro_and_center(LAFs1,LAFs2, H1to2, \n                                                 dist_threshold = 4, \n                                                 center_dist_th = 2.0,\n                                                 scale_diff_coef = 0.3,\n                                                 skip_center_in_Fro = False,\n                                                 do_up_is_up = False,\n                                                 return_LAF2_in_1 = False,\n                                                 inv_to_eye = True):\n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    if do_up_is_up:\n        sc2 = torch.sqrt(torch.abs(LHF2_in_1_pre[:,0,0] * LHF2_in_1_pre[:,1,1] - LHF2_in_1_pre[:,1,0] * LHF2_in_1_pre[:,0,1])).unsqueeze(-1).unsqueeze(-1).expand(LHF2_in_1_pre.size(0), 2,2)\n        LHF2_in_1 = torch.zeros(LHF2_in_1_pre.size())\n        if LHF2_in_1_pre.is_cuda:\n            LHF2_in_1 = LHF2_in_1.cuda()\n        LHF2_in_1[:, :2,:2] = rectifyAffineTransformationUpIsUp(LHF2_in_1_pre[:, :2,:2]/sc2) * sc2\n        LHF2_in_1[:,:, 2] = LHF2_in_1_pre[:,:,2]\n        sc1 = torch.sqrt(torch.abs(LAFs1[:,0,0] * LAFs1[:,1,1] - LAFs1[:,1,0] * LAFs1[:,0,1])).unsqueeze(-1).unsqueeze(-1).expand(LAFs1.size(0), 2,2)\n        LHF1 = LAFs_to_H_frames(torch.cat([rectifyAffineTransformationUpIsUp(LAFs1[:, :2,:2]/sc1) * sc1, LAFs1[:,:,2:]], dim = 2 ))\n    else:\n        LHF2_in_1 = LHF2_in_1_pre\n        LHF1 = LAFs_to_H_frames(LAFs1)\n    if inv_to_eye:\n        LHF1_inv = inverseLHFs(LHF1)\n        frob_norm_dist = reproject_to_canonical_Frob_batched(LHF1_inv, LHF2_in_1, batch_size = 2, skip_center = skip_center_in_Fro)\n    else:\n        if not skip_center_in_Fro:\n            frob_norm_dist = distance_matrix_vector(LHF2_in_1.view(LHF2_in_1.size(0), -1), LHF1.view(LHF1.size(0),-1))\n        else:\n            frob_norm_dist = distance_matrix_vector(LHF2_in_1[:,0:2, 0:2].contiguous().view(LHF2_in_1.size(0), -1), LHF1[:,0:2,0:2].contiguous().view(LHF1.size(0),-1))\n    #### Center replated\n    just_centers1 = LAFs1[:,:,2];\n    just_centers2_repr_to_1 = LHF2_in_1[:,0:2,2];\n    if scale_diff_coef > 0:\n        scales1 = torch.sqrt(torch.abs(LAFs1[:,0,0] * LAFs1[:,1,1] - LAFs1[:,1,0] * LAFs1[:,0,1]))\n        scales2 = torch.sqrt(torch.abs(LHF2_in_1[:,0,0] * LHF2_in_1[:,1,1] - LHF2_in_1[:,1,0] * LHF2_in_1[:,0,1]))\n        scale_matrix = ratio_matrix_vector(scales2, scales1)\n        scale_dist_mask = (torch.abs(1.0 - scale_matrix) <= scale_diff_coef) \n    center_dist_mask  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1) >= center_dist_th\n    frob_norm_dist_masked = (1.0 - scale_dist_mask.float() + center_dist_mask.float()) * 1000. + frob_norm_dist;\n    \n    min_dist, idxs_in_2 = torch.min(frob_norm_dist_masked,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    #min_dist, idxs_in_2 = torch.min(dist,1)\n    #print min_dist.min(), min_dist.max(), min_dist.mean()\n    mask =  (min_dist <= dist_threshold )\n    \n    if return_LAF2_in_1:\n        return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask], LHF2_in_1[:,0:2,:]\n    else:\n        return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\ndef get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log):\n    xy1 = LHF1[:,0:2,2];\n    xy2in1 = LHF2_in_1[:,0:2,2];\n    center_dist_matrix =  distance_matrix_vector(xy2in1, xy1)\n    scales1 = torch.sqrt(torch.abs(LHF1[:,0,0] * LHF1[:,1,1] - LHF1[:,1,0] * LHF1[:,0,1]));\n    scales2 = torch.sqrt(torch.abs(LHF2_in_1[:,0,0] * LHF2_in_1[:,1,1] - LHF2_in_1[:,1,0] * LHF2_in_1[:,0,1]));\n    scale_matrix = torch.abs(torch.log(ratio_matrix_vector(scales2, scales1)))\n    mask_matrix = 1000.0*(scale_matrix  > scale_log).float() * (center_dist_matrix > xy_th).float() + center_dist_matrix + scale_matrix\n\n    d2_to_1, nn_idxs_in_2 = torch.min(mask_matrix,1)\n    d1_to_2, nn_idxs_in_1 = torch.min(mask_matrix,0)\n\n    flat_idxs_1 = torch.arange(0, nn_idxs_in_2.size(0));\n    if LHF1.is_cuda:\n        flat_idxs_1 = flat_idxs_1.cuda()\n    mask = d2_to_1 <= 100.0;\n\n    final_mask = (flat_idxs_1 == nn_idxs_in_1[nn_idxs_in_2].float()).float() * mask.float()\n    idxs_in1 = flat_idxs_1[final_mask.long()].nonzero().squeeze()\n    idxs_in_2_final = nn_idxs_in_2[idxs_in1];\n    #torch.arange(0, nn_idxs_in_2.size(0))#[mask2.data]\n    return idxs_in1, idxs_in_2_final\ndef get_LHFScale(LHF):\n    return torch.sqrt(torch.abs(LHF[:,0,0] * LHF[:,1,1] - LHF[:,1,0] * LHF[:,0,1]));\ndef LAFMagic(LAFs1, LAFs2, H1to2, xy_th  = 5.0, scale_log = 0.4, t = 1.0, sc = 1.0, aff = 1.0):\n    LHF2_in_1 = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    idxs_in1, idxs_in_2 = get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log)\n    if len(idxs_in1) == 0:\n        print(\'Warning, no correspondences found\')\n        return None\n    LHF1_good = LHF1[idxs_in1,:,:]\n    LHF2_good = LHF2_in_1[idxs_in_2,:,:]\n    scales1 = get_LHFScale(LHF1_good);\n    scales2 = get_LHFScale(LHF2_good);\n    max_scale = torch.max(scales1,scales2);\n    min_scale = torch.min(scales1, scales2);\n    mean_scale = 0.5 * (max_scale + min_scale)\n    eps = 1e-12;\n    if t != 0:\n        dist_loss = torch.sqrt(torch.sum((LHF1_good[:,0:2,2] - LHF2_good[:,0:2,2])**2, dim = 1) + eps) / V(mean_scale.data);\n    else:\n        dist_loss = 0\n    if sc != 0 :\n        scale_loss = torch.log1p( (max_scale-min_scale)/(mean_scale))\n    else:\n        scale_loss = 0\n    if aff != 0:\n        A1 = LHF1_good[:,:2,:2] / scales1.view(-1,1,1).expand(scales1.size(0),2,2);\n        A2 = LHF2_good[:,:2,:2] / scales2.view(-1,1,1).expand(scales2.size(0),2,2);\n        shape_loss = ((A1 - A2)**2).mean(dim = 1).mean(dim = 1);\n    else:\n        shape_loss = 0;\n    loss = t * dist_loss + sc * scale_loss + aff *shape_loss;\n    #print dist_loss, scale_loss, shape_loss\n    return loss, idxs_in1, idxs_in_2, LHF2_in_1[:,0:2,:]\ndef LAFMagicFro(LAFs1, LAFs2, H1to2, xy_th  = 5.0, scale_log = 0.4):\n    LHF2_in_1 = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    idxs_in1, idxs_in_2 = get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log)\n    if len(idxs_in1) == 0:\n        print(\'Warning, no correspondences found\')\n        return None\n    LHF1_good = LHF1[idxs_in1,:,:]\n    LHF2_good = LHF2_in_1[idxs_in_2,:,:]\n    scales1 = get_LHFScale(LHF1_good);\n    scales2 = get_LHFScale(LHF2_good);\n    max_scale = torch.max(scales1,scales2);\n    min_scale = torch.min(scales1, scales2);\n    mean_scale = 0.5 * (max_scale + min_scale)\n    eps = 1e-12;\n    dist_loss = (torch.sqrt((LHF1_good.view(-1,9) - LHF2_good.view(-1,9))**2 + eps) / V(mean_scale.data).view(-1,1).expand(LHF1_good.size(0),9)).mean(dim=1); \n    loss = dist_loss;\n    #print dist_loss, scale_loss, shape_loss\n    return loss, idxs_in1, idxs_in_2, LHF2_in_1[:,0:2,:]\ndef pr_l(x):\n    return x.mean().data.cpu().numpy()[0]\ndef add_1(A):\n    add = torch.eye(2).unsqueeze(0).expand(A.size(0),2,2)\n    add = torch.cat([add, torch.zeros(A.size(0),2,1)], dim = 2)\n    if A.is_cuda:\n        add = add.cuda()\n    return add\ndef identity_loss(A):\n    return torch.clamp(torch.sqrt((A - add_1(A))**2 + 1e-15).view(-1,6).mean(dim = 1) - 0.3*0, min = 0.0, max = 100.0).mean()\n\n\n\n'"
examples/direct_shape_optimization/SparseImgRepresenter.py,27,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nimport math\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom copy import deepcopy\nfrom Utils import GaussianBlur, batch_eig2x2, line_prepender, batched_forward\nfrom LAF import LAFs2ell,abc2A, angles2A, generate_patch_grid_from_normalized_LAFs, extract_patches, get_inverted_pyr_index, denormalizeLAFs, extract_patches_from_pyramid_with_inv_index, rectifyAffineTransformationUpIsUp\nfrom LAF import get_pyramid_and_level_index_for_LAFs, normalizeLAFs, checkTouchBoundary\nfrom HandCraftedModules import HessianResp, AffineShapeEstimator, OrientationDetector, ScalePyramid, NMS3dAndComposeA\nimport time\n\nclass ScaleSpaceAffinePatchExtractor(nn.Module):\n    def __init__(self, \n                 border = 16,\n                 num_features = 500,\n                 patch_size = 32,\n                 mrSize = 3.0,\n                 nlevels = 3,\n                 num_Baum_iters = 0,\n                 init_sigma = 1.6,\n                 th = None,\n                 RespNet = None, OriNet = None, AffNet = None):\n        super(ScaleSpaceAffinePatchExtractor, self).__init__()\n        self.mrSize = mrSize\n        self.PS = patch_size\n        self.b = border;\n        self.num = num_features\n        self.nlevels = nlevels\n        self.num_Baum_iters = num_Baum_iters\n        self.init_sigma = init_sigma\n        self.th = th;\n        if th is not None:\n            self.num = -1\n        else:\n            self.th = 0\n        if RespNet is not None:\n            self.RespNet = RespNet\n        else:\n            self.RespNet = HessianResp()\n        if OriNet is not None:\n            self.OriNet = OriNet\n        else:\n            self.OriNet= OrientationDetector(patch_size = 19);\n        if AffNet is not None:\n            self.AffNet = AffNet\n        else:\n            self.AffNet = AffineShapeEstimator(patch_size = 19)\n        self.ScalePyrGen = ScalePyramid(nLevels = self.nlevels, init_sigma = self.init_sigma, border = self.b)\n        return\n    \n    def multiScaleDetector(self,x, num_features = 0):\n        t = time.time()\n        self.scale_pyr, self.sigmas, self.pix_dists = self.ScalePyrGen(x)\n        ### Detect keypoints in scale space\n        aff_matrices = []\n        top_responces = []\n        pyr_idxs = []\n        level_idxs = []\n        det_t = 0\n        nmst = 0\n        for oct_idx in range(len(self.sigmas)):\n            #print oct_idx\n            octave = self.scale_pyr[oct_idx]\n            sigmas_oct = self.sigmas[oct_idx]\n            pix_dists_oct = self.pix_dists[oct_idx]\n            low = None\n            cur = None\n            high = None\n            octaveMap = (self.scale_pyr[oct_idx][0] * 0).byte()\n            nms_f = NMS3dAndComposeA(w = octave[0].size(3),\n                                     h =  octave[0].size(2),\n                                     border = self.b, mrSize = self.mrSize)\n            for level_idx in range(1, len(octave)-1):\n                if cur is None:\n                    low = torch.clamp(self.RespNet(octave[level_idx - 1], (sigmas_oct[level_idx - 1 ])) - self.th, min = 0)\n                else:\n                    low = cur\n                if high is None:\n                    cur =  torch.clamp(self.RespNet(octave[level_idx ], (sigmas_oct[level_idx ])) - self.th, min = 0)\n                else:\n                    cur = high\n                high = torch.clamp(self.RespNet(octave[level_idx + 1], (sigmas_oct[level_idx + 1 ])) - self.th, min = 0)\n                top_resp, aff_matrix, octaveMap_current  = nms_f(low, cur, high,\n                                                                 num_features = num_features,\n                                                                 octaveMap = octaveMap,\n                                                                 scales = sigmas_oct[level_idx - 1:level_idx + 2])\n                if top_resp is None:\n                    continue\n                octaveMap = octaveMap_current\n                aff_matrices.append(aff_matrix), top_responces.append(top_resp)\n                pyr_id = Variable(oct_idx * torch.ones(aff_matrix.size(0)))\n                lev_id = Variable((level_idx - 1) * torch.ones(aff_matrix.size(0))) #prevBlur\n                if x.is_cuda:\n                    pyr_id = pyr_id.cuda()\n                    lev_id = lev_id.cuda()\n                pyr_idxs.append(pyr_id)\n                level_idxs.append(lev_id)\n        all_responses = torch.cat(top_responces, dim = 0)\n        aff_m_scales = torch.cat(aff_matrices,dim = 0)\n        pyr_idxs_scales = torch.cat(pyr_idxs,dim = 0)\n        level_idxs_scale = torch.cat(level_idxs, dim = 0)\n        if (num_features > 0) and (num_features < all_responses.size(0)):\n            all_responses, idxs = torch.topk(all_responses, k = num_features);\n            LAFs = torch.index_select(aff_m_scales, 0, idxs)\n            final_pyr_idxs = pyr_idxs_scales[idxs]\n            final_level_idxs = level_idxs_scale[idxs]\n        else:\n            return all_responses, aff_m_scales, pyr_idxs_scales , level_idxs_scale\n        return all_responses, LAFs, final_pyr_idxs, final_level_idxs,\n    \n    def getAffineShape(self, final_resp, LAFs, final_pyr_idxs, final_level_idxs, num_features = 0):\n        pe_time = 0\n        affnet_time = 0\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        t = time.time()\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.AffNet.PS)\n        pe_time+=time.time() - t\n        t = time.time()\n        base_A = torch.eye(2).unsqueeze(0).expand(final_pyr_idxs.size(0),2,2)\n        if final_resp.is_cuda:\n            base_A = base_A.cuda()\n        base_A = Variable(base_A)\n        is_good = None\n        n_patches = patches_small.size(0)\n        for i in range(self.num_Baum_iters):\n            t = time.time()\n            A = batched_forward(self.AffNet, patches_small, 256)\n            is_good_current = 1\n            affnet_time += time.time() - t\n            if is_good is None:\n                is_good = is_good_current\n            else:\n                is_good = is_good * is_good_current\n            base_A = torch.bmm(A, base_A); \n            new_LAFs = torch.cat([torch.bmm(base_A,LAFs[:,:,0:2]), LAFs[:,:,2:] ], dim =2)\n            #print torch.sqrt(new_LAFs[0,0,0]*new_LAFs[0,1,1] - new_LAFs[0,1,0] *new_LAFs[0,0,1]) * scale_pyr[0][0].size(2)\n            if i != self.num_Baum_iters - 1:\n                pe_time+=time.time() - t\n                t = time.time()\n                patches_small =  extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, new_LAFs, PS = self.AffNet.PS)\n                pe_time+= time.time() - t\n                l1,l2 = batch_eig2x2(A)      \n                ratio1 =  torch.abs(l1 / (l2 + 1e-8))\n                converged_mask = (ratio1 <= 1.2) * (ratio1 >= (0.8)) \n        l1,l2 = batch_eig2x2(base_A)\n        ratio = torch.abs(l1 / (l2 + 1e-8))\n        idxs_mask =  ((ratio < 6.0) * (ratio > (1./6.))) * checkTouchBoundary(new_LAFs)\n        num_survived = idxs_mask.float().sum()\n        if (num_features > 0) and (num_survived.data.item() > num_features):\n            final_resp =  final_resp * idxs_mask.float() #zero bad points\n            final_resp, idxs = torch.topk(final_resp, k = num_features);\n        else:\n            idxs = Variable(torch.nonzero(idxs_mask.data).view(-1).long())\n            final_resp = final_resp[idxs]\n        final_pyr_idxs = final_pyr_idxs[idxs]\n        final_level_idxs = final_level_idxs[idxs]\n        base_A = torch.index_select(base_A, 0, idxs)\n        LAFs = torch.index_select(LAFs, 0, idxs)\n        new_LAFs = torch.cat([torch.bmm(base_A, LAFs[:,:,0:2]),\n                               LAFs[:,:,2:]], dim =2)\n        print ('affnet_time',affnet_time)\n        print ('pe_time', pe_time)\n        return final_resp, new_LAFs, final_pyr_idxs, final_level_idxs  \n    \n    def getOrientation(self, LAFs, final_pyr_idxs, final_level_idxs):\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)\n        max_iters = 1\n        ### Detect orientation\n        for i in range(max_iters):\n            angles = self.OriNet(patches_small)\n            if len(angles.size()) > 2:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles), LAFs[:,:,2:]], dim = 2)\n            else:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles2A(angles).view(-1,2,2)), LAFs[:,:,2:]], dim = 2)\n            if i != max_iters:\n                patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)        \n        return LAFs\n    def extract_patches_from_pyr(self, dLAFs, PS = 41):\n        pyr_idxs, level_idxs = get_pyramid_and_level_index_for_LAFs(dLAFs, self.sigmas, self.pix_dists, PS)\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, pyr_idxs, level_idxs)\n        patches = extract_patches_from_pyramid_with_inv_index(self.scale_pyr,\n                                                      pyr_inv_idxs,\n                                                      normalizeLAFs(dLAFs, self.scale_pyr[0][0].size(3), self.scale_pyr[0][0].size(2)), \n                                                      PS = PS)\n        return patches\n    def forward(self,x, do_ori = False):\n        ### Detection\n        t = time.time()\n        num_features_prefilter = self.num\n        if self.num_Baum_iters > 0:\n            num_features_prefilter = int(1.5 * self.num);\n        responses, LAFs, final_pyr_idxs, final_level_idxs = self.multiScaleDetector(x,num_features_prefilter)\n        print (time.time() - t, 'detection multiscale')\n        t = time.time()\n        LAFs[:,0:2,0:2] =   self.mrSize * LAFs[:,:,0:2]\n        if self.num_Baum_iters > 0:\n            responses, LAFs, final_pyr_idxs, final_level_idxs  = self.getAffineShape(responses, LAFs, final_pyr_idxs, final_level_idxs, self.num)\n        print (time.time() - t, 'affine shape iters')\n        t = time.time()\n        if do_ori:\n            LAFs = self.getOrientation(LAFs, final_pyr_idxs, final_level_idxs)\n            #pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        #patches = extract_patches_from_pyramid_with_inv_index(scale_pyr, pyr_inv_idxs, LAFs, PS = self.PS)\n        #patches = extract_patches(x, LAFs, PS = self.PS)\n        #print time.time() - t, len(LAFs), ' patches extraction'\n        return denormalizeLAFs(LAFs, x.size(3), x.size(2)), responses\n"""
examples/direct_shape_optimization/Utils.py,25,"b'import torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport cv2\nimport numpy as np\n\n# resize image to size 32x32\ncv2_scale = lambda x: cv2.resize(x, dsize=(32, 32),\n                                 interpolation=cv2.INTER_LINEAR)\n# reshape image\nnp_reshape = lambda x: np.reshape(x, (32, 32, 1))\n\ndef zeros_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.zeros(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.zeros(y)\n\ndef ones_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.ones(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.ones(y)\n    \n\ndef batched_forward(model, data, batch_size, **kwargs):\n    n_patches = len(data)\n    if n_patches > batch_size:\n        bs = batch_size\n        n_batches = n_patches / bs + 1\n        for batch_idx in range(n_batches):\n            st = batch_idx * bs\n            if batch_idx == n_batches - 1:\n                if (batch_idx + 1) * bs > n_patches:\n                    end = n_patches\n                else:\n                    end = (batch_idx + 1) * bs\n            else:\n                end = (batch_idx + 1) * bs\n            if st >= end:\n                continue\n            if batch_idx == 0:\n                try:\n                    first_batch_out = model(data[st:end])#, kwargs)\n                except:\n                    first_batch_out = model(data[st:end])# kwargs)\n                out_size = torch.Size([n_patches] + list(first_batch_out.size()[1:]))\n                #out_size[0] = n_patches\n                out = torch.zeros(out_size);\n                if data.is_cuda:\n                    out = out.cuda()\n                out = Variable(out)\n                out[st:end] = first_batch_out\n            else:\n                try:\n                    out[st:end] = model(data[st:end])#, kwargs)\n                except:\n                    out[st:end] = model(data[st:end])#, kwargs)\n        return out\n    else:\n        return model(data)#, kwargs)\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n\ndef CircularGaussKernel(kernlen=None, circ_zeros = False, sigma = None, norm = True):\n    assert ((kernlen is not None) or sigma is not None)\n    if kernlen is None:\n        kernlen = int(2.0 * 3.0 * sigma + 1.0)\n        if (kernlen % 2 == 0):\n            kernlen = kernlen + 1;\n        halfSize = kernlen / 2;\n    halfSize = kernlen / 2;\n    r2 = float(halfSize*halfSize)\n    if sigma is None:\n        sigma2 = 0.9 * r2;\n        sigma = np.sqrt(sigma2)\n    else:\n        sigma2 = 2.0 * sigma * sigma    \n    x = np.linspace(-halfSize,halfSize,kernlen)\n    xv, yv = np.meshgrid(x, x, sparse=False, indexing=\'xy\')\n    distsq = (xv)**2 + (yv)**2\n    kernel = np.exp(-( distsq/ (sigma2)))\n    if circ_zeros:\n        kernel *= (distsq <= r2).astype(np.float32)\n    if norm:\n        kernel /= np.sum(kernel)\n    return kernel\n\ndef generate_2dgrid(h,w, centered = True):\n    if centered:\n        x = torch.linspace(-w/2+1, w/2, w)\n        y = torch.linspace(-h/2+1, h/2, h)\n    else:\n        x = torch.linspace(0, w-1, w)\n        y = torch.linspace(0, h-1, h)\n    grid2d = torch.stack([y.repeat(w,1).t().contiguous().view(-1), x.repeat(h)],1)\n    return grid2d\n\ndef generate_3dgrid(d, h, w, centered = True):\n    if type(d) is not list:\n        if centered:\n            z = torch.linspace(-d/2+1, d/2, d)\n        else:\n            z = torch.linspace(0, d-1, d)\n        dl = d\n    else:\n        z = torch.FloatTensor(d)\n        dl = len(d)\n    grid2d = generate_2dgrid(h,w, centered = centered)\n    grid3d = torch.cat([z.repeat(w*h,1).t().contiguous().view(-1,1), grid2d.repeat(dl,1)],dim = 1)\n    return grid3d\n\ndef zero_response_at_border(x, b):\n    if (b < x.size(3)) and (b < x.size(2)):\n        x[:, :,  0:b, :] =  0\n        x[:, :,  x.size(2) - b: , :] =  0\n        x[:, :, :,  0:b] =  0\n        x[:, :, :,   x.size(3) - b: ] =  0\n    else:\n        return x * 0\n    return x\n\nclass GaussianBlur(nn.Module):\n    def __init__(self, sigma=1.6):\n        super(GaussianBlur, self).__init__()\n        weight = self.calculate_weights(sigma)\n        self.register_buffer(\'buf\', weight)\n        return\n    def calculate_weights(self,  sigma):\n        kernel = CircularGaussKernel(sigma = sigma, circ_zeros = False)\n        h,w = kernel.shape\n        halfSize = float(h) / 2.;\n        self.pad = int(np.floor(halfSize))\n        return torch.from_numpy(kernel.astype(np.float32)).view(1,1,h,w);\n    def forward(self, x):\n        w = Variable(self.buf)\n        if x.is_cuda:\n            w = w.cuda()\n        return F.conv2d(F.pad(x, (self.pad,self.pad,self.pad,self.pad), \'replicate\'), w, padding = 0)\n\ndef batch_eig2x2(A):\n    trace = A[:,0,0] + A[:,1,1]\n    delta1 = (trace*trace - 4 * ( A[:,0,0]*  A[:,1,1] -  A[:,1,0]* A[:,0,1]))\n    mask = delta1 > 0\n    delta = torch.sqrt(torch.abs(delta1))\n    l1 = mask.float() * (trace + delta) / 2.0 +  1000.  * (1.0 - mask.float())\n    l2 = mask.float() * (trace - delta) / 2.0 +  0.0001  * (1.0 - mask.float())\n    return l1,l2\n\ndef line_prepender(filename, line):\n    with open(filename, \'r+\') as f:\n        content = f.read()\n        f.seek(0, 0)\n        f.write(line.rstrip(\'\\r\\n\') + \'\\n\' + content)\n    return\n'"
examples/direct_shape_optimization/architectures.py,81,"b""from __future__ import division, print_function\nimport os\nimport errno\nimport numpy as np\nimport sys\nfrom copy import deepcopy\nimport math\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom Utils import L2Norm, generate_2dgrid\nfrom Utils import str2bool\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A, extract_patches,normalizeLAFs,  get_rotation_matrix\nfrom LAF import get_LAFs_scales, get_normalized_affine_shape\nfrom LAF import rectifyAffineTransformationUpIsUp,rectifyAffineTransformationUpIsUpFullyConv\n\nclass LocalNorm2d(nn.Module):\n    def __init__(self, kernel_size = 33):\n        super(LocalNorm2d, self).__init__()\n        self.ks = kernel_size\n        self.pool = nn.AvgPool2d(kernel_size = self.ks, stride = 1,  padding = 0)\n        self.eps = 1e-10\n        return\n    def forward(self,x):\n        pd = int(self.ks/2)\n        mean = self.pool(F.pad(x, (pd,pd,pd,pd), 'reflect'))\n        return torch.clamp((x - mean) / (torch.sqrt(torch.abs(self.pool(F.pad(x*x,  (pd,pd,pd,pd), 'reflect')) - mean*mean )) + self.eps), min = -6.0, max = 6.0)\n\nclass OriNetFast(nn.Module):\n    def __init__(self, PS = 16):\n        super(OriNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 2, kernel_size=int(PS/4), stride=1,padding=1, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/4)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.9)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_rot_matrix = True):\n        xy = self.features(self.input_norm(input)).view(-1,2) \n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(angle)\n        return angle\n\nclass GHH(nn.Module):\n    def __init__(self, n_in, n_out, s = 4, m = 4):\n        super(GHH, self).__init__()\n        self.n_out = n_out\n        self.s = s\n        self.m = m\n        self.conv = nn.Linear(n_in, n_out * s * m)\n        d = torch.arange(0, s)\n        self.deltas = -1.0 * (d % 2 != 0).float()  + 1.0 * (d % 2 == 0).float()\n        self.deltas = Variable(self.deltas)\n        return\n    def forward(self,x):\n        x_feats = self.conv(x.view(x.size(0),-1)).view(x.size(0), self.n_out, self.s, self.m);\n        max_feats = x_feats.max(dim = 3)[0];\n        if x.is_cuda:\n            self.deltas = self.deltas.cuda()\n        else:\n            self.deltas = self.deltas.cpu()\n        out =  (max_feats * self.deltas.view(1,1,-1).expand_as(max_feats)).sum(dim = 2)\n        return out\n\nclass YiNet(nn.Module):\n    def __init__(self, PS = 28):\n        super(YiNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 10, kernel_size=5, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding = 1),\n            nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=2, padding = 2),\n            nn.Conv2d(20, 50, kernel_size=3, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.AdaptiveMaxPool2d(1),\n            GHH(50, 100),\n            GHH(100, 2)\n        )\n        self.input_mean = 0.427117081207483\n        self.input_std = 0.21888339179665006;\n        self.PS = PS\n        return\n    def import_weights(self, dir_name):\n        self.features[0].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_W.npy'))).float()\n        self.features[0].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_b.npy'))).float().view(-1)\n        self.features[3].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_W.npy'))).float()\n        self.features[3].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_b.npy'))).float().view(-1)\n        self.features[6].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_W.npy'))).float()\n        self.features[6].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_b.npy'))).float().view(-1)\n        self.features[9].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_W.npy'))).float().view(50, 1600).contiguous().t().contiguous()\n        self.features[9].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_b.npy'))).float().view(1600)\n        self.features[10].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_W.npy'))).float().view(100, 32).contiguous().t().contiguous()\n        self.features[10].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_b.npy'))).float().view(32)\n        self.input_mean = float(np.load(os.path.join(dir_name, 'input_mean.npy')))\n        self.input_std = float(np.load(os.path.join(dir_name, 'input_std.npy')))\n        return\n    def input_norm1(self,x):\n        return (x - self.input_mean) / self.input_std\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def forward(self, input, return_rot_matrix = False):\n        xy = self.features(self.input_norm(input))\n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(-angle)\n        return angle\nclass AffNetFast4(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0,0,1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n        return rectifyAffineTransformationUpIsUp(xy).contiguous()\n\n    \nclass AffNetFast(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,3)\n        a1 = torch.cat([1.0 + xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), 1.0 + xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        return rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n\nclass AffNetFast52RotUp(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52RotUp, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0, 1, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, rectifyAffineTransformationUpIsUp(torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1)).contiguous())\n\nclass AffNetFast52Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Tanh()\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0, 0.8, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFast5Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast5Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0, 1, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        rot = get_rotation_matrix(torch.atan2(x[:,3], x[:,4]+1e-8))\n        if input.is_cuda:\n            return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), torch.zeros(x.size(0),1,1).cuda()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n        else:\n            return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), torch.zeros(x.size(0),1,1)], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFast4Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Tanh()\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0,0,0.8])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        return self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n\nclass AffNetFast4RotNosc(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4RotNosc, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0,0,1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        A = self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n        scale =  torch.sqrt(torch.abs(A[:,0,0]*A[:,1,1] - A[:,1,0]*A[:,0,1] + 1e-10))\n        return A / (scale.view(-1,1,1).repeat(1,2,2) + 1e-8)\n\nclass AffNetFastScale(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFastScale, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,4)\n        a1 = torch.cat([1.0 + xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), 1.0 + xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        scale = torch.exp(xy[:,3].contiguous().view(-1,1,1).repeat(1,2,2))\n        return scale * rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n\nclass AffNetFast2Par(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast2Par, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0, 0, 1 ])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,3)\n        angle = torch.atan2(x[:,1], x[:,2]+1e-8);\n        rot = get_rotation_matrix(angle)\n        tilt = torch.exp(1.8 * F.tanh(x[:,0]))\n        tilt_matrix = torch.eye(2).unsqueeze(0).repeat(input.size(0),1,1)\n        if x.is_cuda:\n            tilt_matrix = tilt_matrix.cuda()\n        tilt_matrix[:,0,0] = torch.sqrt(tilt)\n        tilt_matrix[:,1,1] = 1.0 / torch.sqrt(tilt)\n        return rectifyAffineTransformationUpIsUp(torch.bmm(rot, tilt_matrix)).contiguous()\n\nclass AffNetFastFullConv(nn.Module):\n    def __init__(self, PS = 32, stride = 2):\n        super(AffNetFastFullConv, self).__init__()\n        self.lrn = LocalNorm2d(33)\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=stride, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=stride, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding = 0, bias = True),\n        )\n        self.stride = stride\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        norm_inp  = self.lrn(input)\n        ff = self.features(F.pad(norm_inp, (14,14,14,14), 'reflect'))\n        xy = F.tanh(F.upsample(ff, (input.size(2), input.size(3)),mode='bilinear'))\n        a0bc = torch.cat([1.0 + xy[:,0:1,:,:].contiguous(), 0*xy[:,1:2,:,:].contiguous(),\n                          xy[:,1:2,:,:].contiguous(),  1.0 + xy[:,2:,:,:].contiguous()], dim = 1).contiguous()\n        return rectifyAffineTransformationUpIsUpFullyConv(a0bc).contiguous()\n    \nclass AffNetFast52RotL(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52RotL, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0, 0.8, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFastBias(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFastBias, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8, 0, 0.8 ])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,3)\n        a1 = torch.cat([xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        return rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n"""
examples/direct_shape_optimization/hesaffBaum.py,6,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport os\nimport time\n\nfrom PIL import Image\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport math\nimport torch.nn.functional as F\n\nfrom copy import deepcopy\n\nfrom SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\nfrom LAF import denormalizeLAFs, LAFs2ellT, abc2A\nfrom Utils import line_prepender\nfrom architectures import AffNetFast\nfrom HandCraftedModules import AffineShapeEstimator\nUSE_CUDA = False\ntry:\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\n    nfeats = int(sys.argv[3])\nexcept:\n    print ""Wrong input format. Try python hesaffBaum.py imgs/cat.png cat.txt 2000""\n    sys.exit(1)\n\nimg = Image.open(input_img_fname).convert(\'RGB\')\nimg = np.mean(np.array(img), axis = 2)\n\nvar_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\nvar_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n\nHA = ScaleSpaceAffinePatchExtractor( mrSize = 5.192, num_features = nfeats, border = 5, num_Baum_iters = 16, AffNet = AffineShapeEstimator(patch_size=19))\nif USE_CUDA:\n    HA = HA.cuda()\n    var_image_reshape = var_image_reshape.cuda()\n\nLAFs, resp = HA(var_image_reshape)\nells  = LAFs2ellT(LAFs.cpu()).cpu().numpy()\n\nnp.savetxt(output_fname, ells, delimiter=\' \', fmt=\'%10.10f\')\nline_prepender(output_fname, str(len(ells)))\nline_prepender(output_fname, \'1.0\')\n'"
examples/direct_shape_optimization/optimization_script.py,31,"b'import torch\nimport torch.nn\nimport numpy as np\nimport seaborn as sns\nfrom Losses import distance_matrix_vector,distance_vectors_pairwise, get_snn\nfrom ReprojectionStuff import get_GT_correspondence_indexes\nfrom SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\n#%matplotlib inline\nimport seaborn as sns\n#plt.imshow(255 - img1.cpu().data.numpy()[0,0,:,:])\nfrom LAF import visualize_LAFs\nimport sys\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport matplotlib.animation as animation\nimport os\nfrom Utils import batched_forward\nfrom PIL import Image\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A\nfrom HardNet import HardNet\nfrom LAF import extract_patches, normalizeLAFs,convertLAFs_to_A23format,LAF2pts \nfrom LAF import LAFs2ell,abc2A, angles2A, generate_patch_grid_from_normalized_LAFs\nfrom LAF import extract_patches, get_inverted_pyr_index,get_pyramid_and_level_index_for_LAFs\nfrom HandCraftedModules import ScalePyramid\nfrom LAF import denormalizeLAFs, extract_patches_from_pyramid_with_inv_index, rectifyAffineTransformationUpIsUp\nimport math\nfrom copy import deepcopy\nimport gc\n\ndef loss_HardNegC(anchor, positive, margin = 1.0):\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix_detach = distance_matrix_vector(anchor, positive.detach()) + eps\n    pos1 = distance_vectors_pairwise(anchor,positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix_detach.size(1))).cuda()\n    # steps to filter out same patches that occur in distance matrix as negatives\n    dist_without_min_on_diag = dist_matrix_detach + eye*10\n    mask = (dist_without_min_on_diag.ge(0.08).float()-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag + mask\n    min_neg = torch.min(dist_without_min_on_diag,1)[0]\n    #loss = 0.5 * torch.mean(loss)\n    dist_matrix_detach2 = distance_matrix_vector(anchor.detach(), positive) + eps\n    # steps to filter out same patches that occur in distance matrix as negatives\n    dist_without_min_on_diag2 = dist_matrix_detach2 + eye*10\n    mask2 = (dist_without_min_on_diag2.ge(0.08).float()-1)*-1\n    mask2 = mask2.type_as(dist_without_min_on_diag2)*10\n    dist_without_min_on_diag2 = dist_without_min_on_diag2 + mask2\n    min_neg2 = torch.min(dist_without_min_on_diag2,0)[0]\n    #min_neg_all = torch.min(min_neg.view(-1),min_neg2.view(-1))\n    #print min_neg_all.shape\n    loss = torch.clamp(margin + pos1 - 0.5*(min_neg2 + min_neg), min=0.0).mean()\n    return loss\ndef get_input_param_optimizer(x, lr):\n    input_param = nn.Parameter(x.data)\n    optimizer = optim.Adam([input_param], lr=lr)\n    return input_param, optimizer\n\ndef visualize_LAFs_on_fig(ax, LAFs, color = \'r\'):\n    work_LAFs = convertLAFs_to_A23format(LAFs)\n    lines = {}\n    for i in range(len(work_LAFs)):\n        ell = LAF2pts(work_LAFs[i,:,:])\n        lines[str(i)], = ax.plot( ell[:,0], ell[:,1], color)\n    return lines, ax\ndef getScale(A):\n    return torch.sqrt(torch.abs(A[:,0,0] * A[:,1,1] - A[:,0,1]*A[:,1,0]) + 1e-12)\ndef FrobNorm(a1,a2, norm_scale = True):\n    if norm_scale:\n        sc1 = getScale(a1)\n        sc2 = getScale(a2)\n        min_sc = 0.5 * (sc1 + sc2)\n        a1 = a1 / min_sc.view(-1,1,1).expand_as(a1)\n        a2 = a2 / min_sc.view(-1,1,1).expand_as(a2)\n    return torch.sqrt(((a1 - a2)**2).view(a1.size(0),-1).sum(dim=1)+1e-12).mean()\nclass LAFDiscrOptimDetach():\n    def __init__(self, descriptors_list, names_list, loss_fn = loss_HardNegC, cuda = False, lr = 1e-0,\n                loss_name = \'HardNegC\'):\n        assert len(descriptors_list) == len(names_list)\n        self.desc_list = descriptors_list\n        self.names_list = names_list\n        self.colors = [\'r\', \'g\', \'b\', \'y\', \'m\', \'k\', \'c\']\n        self.cuda = cuda\n        self.out_lafs1 = {}\n        self.out_lafs2 = {}\n        self.loss_func = loss_fn\n        self.loss_name = loss_name;\n        self.save_dir = self.loss_name + \'_saves\'\n        self.out_loss_mean = {}\n        self.cent_diff = {}\n        self.shape_diff = {}\n        self.snn = {}\n        self.lr = lr;\n        for dn in names_list:\n            self.out_lafs1[dn] = []\n            self.out_lafs2[dn] = []      \n            self.out_loss_mean[dn] =[]\n            self.cent_diff[dn] = []\n            self.shape_diff[dn] =[]\n            self.snn[dn] =[]\n        self.ScalePyrGen = ScalePyramid(nLevels = 1, init_sigma = 1.6, border = 32)\n        return\n    def extract_patches_from_pyr(self, dLAFs, scale_pyr, sigmas, pix_dists, PS = 32):\n        pyr_idxs, level_idxs = get_pyramid_and_level_index_for_LAFs(dLAFs, sigmas, pix_dists, PS)\n        pyr_inv_idxs = get_inverted_pyr_index(scale_pyr, pyr_idxs, level_idxs)\n        patches = extract_patches_from_pyramid_with_inv_index(scale_pyr,\n                                                      pyr_inv_idxs,\n                                                      normalizeLAFs(dLAFs, scale_pyr[0][0].size(3),\n                                                                    scale_pyr[0][0].size(2)), \n                                                      PS = PS)\n        return patches\n    def optimize(self, laf1, laf2, img1, img2, n_iters = 10):\n        if self.cuda:\n            img1,img2 = img1.cuda(),img2.cuda()\n        w1,h1 = img1.size(3),img1.size(2)\n        w2,h2 = img2.size(3),img2.size(2)\n        self.scale_pyr1, self.sigmas1, self.pix_dists1 = self.ScalePyrGen(img1)\n        self.scale_pyr2, self.sigmas2, self.pix_dists2 = self.ScalePyrGen(img2)\n        \n        for d_idx in range(len(self.names_list)):\n            D = self.desc_list[d_idx]\n            try:\n                if self.cuda:\n                    D = D.cuda()\n            except:\n                pass\n            N = self.names_list[d_idx]\n            print N\n            l1 = deepcopy(Variable(deepcopy(laf1)))\n            l2 = deepcopy(Variable(deepcopy(laf2)))\n            if self.cuda:\n                l1 = l1.cuda()\n                l2 = l2.cuda()\n            l1o, opt1 = get_input_param_optimizer(l1[:,:2,:2], self.lr)\n            l2o, opt2 = get_input_param_optimizer(l2[:,:2,:2], self.lr)\n            self.out_lafs1[N].append(deepcopy(torch.cat([l1o.data, l1.data[:,:,2:]], dim = 2).cpu()))\n            self.out_lafs2[N].append(deepcopy(torch.cat([l2o.data, l2.data[:,:,2:]], dim = 2).cpu()))\n            self.shape_diff[N].append(deepcopy(FrobNorm(deepcopy(l1o.detach()),deepcopy(l2o.detach()))))\n            for it in range(n_iters):\n                #p1 = extract_patches(img1, normalizeLAFs(torch.cat([l1o, l1[:,:,2:]],dim = 2), w1, h1));\n                p1 = self.extract_patches_from_pyr(torch.cat([l1o, l1[:,:,2:]],dim = 2),\n                                                   self.scale_pyr1,\n                                                   self.sigmas1,\n                                                   self.pix_dists1, PS = 32)\n                desc1 = batched_forward(D,p1, 32)\n                p2 = self.extract_patches_from_pyr(torch.cat([l2o, l2[:,:,2:]],dim = 2),\n                                                   self.scale_pyr2,\n                                                   self.sigmas2,\n                                                   self.pix_dists2, PS = 32)\n                #p2 = extract_patches(img2, normalizeLAFs(torch.cat([l2o, l2[:,:,2:]],dim = 2), w2, h2));\n                desc2 = batched_forward(D,p2, 32)\n                loss = self.loss_func(desc1,desc2)\n                if it % 10 == 0:\n                    print loss.data.cpu().numpy()\n                opt1.zero_grad()\n                opt2.zero_grad()\n                loss.backward() \n                opt1.step()\n                opt2.step()\n                self.out_lafs1[N].append(deepcopy(torch.cat([l1o.data, l1.data[:,:,2:]], dim = 2).cpu()))\n                self.out_lafs2[N].append(deepcopy(torch.cat([l2o.data, l2.data[:,:,2:]], dim = 2).cpu()))\n                self.out_loss_mean[N].append(deepcopy(loss.data.mean()))\n                self.cent_diff[N].append(0)#deepcopy(torch.sqrt(((l1o.data[:,:,2] - l2o.data[:,:,2])**2).sum(dim=1)+1e-12).mean()))\n                self.shape_diff[N].append(deepcopy(FrobNorm(deepcopy(l1o.detach()),deepcopy(l2o.detach()))))\n                with torch.no_grad():\n                    self.snn[N].append(deepcopy(get_snn(desc1,desc2).cpu().view(1,-1)))\n            del l1,l2,opt1,opt2\n            gc.collect()\n            torch.cuda.empty_cache()\n        self.img1 = img1.data.cpu().numpy()\n        self.img2 = img2.data.cpu().numpy()\n        return\n    def save_data(self, fname):\n        data_dict = {\'LAFs1\':self.out_lafs1, \'LAFs2\':self.out_lafs2, \'loss_mean\': self.out_loss_mean,\n                    \'cent_diff\': self.cent_diff, \'shape_diff\': self.shape_diff, \'snn\': self.snn}\n        if not os.path.isdir(self.save_dir):\n            os.makedirs(self.save_dir)\n        with open(os.path.join(self.save_dir, fname), \'wb\') as ff:\n            pickle.dump(data_dict, ff)\n        return\n    def load_data(self, fname):\n        with open(os.path.join(self.save_dir, fname), \'rb\') as ff:\n            data_dict = pickle.load(ff)\n        self.out_lafs1 = data_dict[\'LAFs1\']\n        self.out_lafs2 = data_dict[\'LAFs2\']\n        self.out_loss_mean = data_dict[\'loss_mean\']\n        self.cent_diff = data_dict[\'cent_diff\']\n        self.shape_diff = data_dict[\'shape_diff\']\n        self.snn = data_dict[\'snn\']\n        return    \n    def savemp4_per_img(self, img, lafs_dict, fname):\n        fig, ax = plt.subplots()\n        fig.set_tight_layout(True)\n        ax.imshow(255 - img.squeeze())\n        lines_dict = {}\n        for d_idx in range(len(self.names_list)):\n            N = self.names_list[d_idx];\n            C = self.colors[d_idx];\n            num_frames = len(lafs_dict[N])\n            lines_dict[N], ax =  visualize_LAFs_on_fig(ax, lafs_dict[N][0], C)\n        ax.legend(self.names_list)\n        def visualize_LAFs_on_fig_mod(laf_dict, line_dict, dname, i):\n            work_LAFs = convertLAFs_to_A23format(laf_dict[dname][i])\n            ells = []\n            for jj in range(len(work_LAFs)):\n                ell = LAF2pts(work_LAFs[jj,:,:])\n                line_dict[dname][str(i)].set_data(ell[:,0], ell[:,1])\n            return line_dict[dname],ax\n        def update_LAF(i):\n            for d_idx in range(len(self.names_list)):\n                N = self.names_list[d_idx];\n                C = self.colors[d_idx];\n                lines_dict[N],ax =  visualize_LAFs_on_fig_mod(lafs_dict, lines_dict, N, i)\n            return lines_dict[N], ax \n        ax.legend(self.names_list)\n        anim = FuncAnimation(fig, update_LAF, frames=np.arange(0, num_frames), interval=75)\n        import matplotlib.animation as animation\n        Writer = animation.writers[\'ffmpeg\']\n        writer = Writer(fps=24, metadata=dict(artist=\'Me\'), bitrate=1800)\n        anim.save(fname, dpi=96, writer=writer)#\'imagemagick\')        \n        return\n    def savemp4_per_desc(self, fname):\n        if not os.path.isdir(self.loss_name):\n            os.makedirs(self.loss_name)\n        img = self.img1\n        for d_idx in range(len(self.names_list)):\n            fig, ax = plt.subplots()\n            fig.set_tight_layout(True)\n            ax.imshow(255 - img.squeeze())\n            lines_dict = {}\n            N = self.names_list[d_idx];\n            C = self.colors[d_idx];\n            lafs_dict = {""1"": self.out_lafs1[N], ""2"": self.out_lafs2[N]}\n            num_frames = len(lafs_dict[""1""])\n            lines_dict[\'1\'], ax =  visualize_LAFs_on_fig(ax, lafs_dict[""1""][0], \'r\')\n            lines_dict[\'2\'], ax =  visualize_LAFs_on_fig(ax, lafs_dict[""2""][0], \'b\')\n            ax.legend([\'img1\', \'img2\'])\n            def visualize_LAFs_on_fig_mod(laf_dict, line_dict, dname, i):\n                work_LAFs = convertLAFs_to_A23format(laf_dict[dname][i])\n                for jj in range(len(work_LAFs)):\n                    ell = LAF2pts(work_LAFs[jj,:,:])\n                    line_dict[dname][str(jj)].set_data(ell[:,0], ell[:,1])\n                return line_dict[dname],ax\n            def update_LAF(i):\n                lines_dict[""1""],ax =  visualize_LAFs_on_fig_mod(lafs_dict, lines_dict, ""1"", i)\n                lines_dict[""2""],ax =  visualize_LAFs_on_fig_mod(lafs_dict, lines_dict, ""2"", i)\n                return lines_dict[""1""], ax \n            anim = FuncAnimation(fig, update_LAF, frames=np.arange(0, num_frames), interval=75)\n            Writer = animation.writers[\'ffmpeg\']\n            writer = Writer(fps=24, metadata=dict(artist=\'Me\'), bitrate=900)\n            anim.save(os.path.join(self.loss_name, N + ""_"" +  fname), dpi=72, writer=writer)\n        return\ndef load_grayscale_var(fname):\n    img = Image.open(fname).convert(\'RGB\')\n    img = np.mean(np.array(img), axis = 2)\n    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n    if False:\n        var_image_reshape = var_image_reshape.cuda()\n    return var_image_reshape\n\nfrom HardNet import HardNet, HardTFeatNet, L2Norm\nfrom pytorch_sift import SIFTNet\nimport math\n\nd1 =  lambda x: L2Norm()(x.view(x.size(0),-1) - x.view(x.size(0),-1).mean(dim=1, keepdim=True).expand(x.size(0),x.size(1)*x.size(2)*x.size(3)).detach())\nd2 = HardNet()\nmodel_weights = \'../../HardNet++.pth\'\nhncheckpoint = torch.load(model_weights)\nd2.load_state_dict(hncheckpoint[\'state_dict\'])\nd2.eval()\nd3 = SIFTNet(patch_size = 32);\n\n\nmodel_weights = \'HardTFeat.pth\'\nd4 = HardTFeatNet(sm=SIFTNet(patch_size = 32))\ncheckpoint = torch.load(model_weights)\nd4.load_state_dict(checkpoint[\'state_dict\'])\nd4 = nn.Sequential(d4,L2Norm())\n\ndesc_list = [d1,d2,d3,d4]\ndesc_names = [\'Pixels\', \'HardNet\', \'SIFT\', \'TFeat\']\nUSE_CUDA = False\ndetector = ScaleSpaceAffinePatchExtractor( mrSize = 5.12, num_features = 200,\n                                              border = 32, num_Baum_iters = 0)\ndescriptor = HardNet()\nmodel_weights = \'../../HardNet++.pth\'\nhncheckpoint = torch.load(model_weights)\ndescriptor.load_state_dict(hncheckpoint[\'state_dict\'])\ndescriptor.eval()\nif USE_CUDA:\n    detector = detector.cuda()\n    descriptor = descriptor.cuda()\ndef get_geometry(img, det):\n    with torch.no_grad():\n        LAFs, resp = det(img)\n    return LAFs#, descriptors\n\n#visualize_LAFs(img1.cpu().numpy().squeeze(), L3.cpu().numpy().squeeze(), \'g\')\n#LOP.optimize(L1, L2, img1, img2, n_iters = 200);\n#LOP.savemp4_per_desc(\'test.mp4\') \nfrom Losses import loss_HardNet\ndef posdist_loss(a,p):\n    return distance_vectors_pairwise(a,p).mean()\nloss_names = [\'PosDist\', \'HardNet\', \'HardNegC\']\nloss_fns = [posdist_loss, loss_HardNet, loss_HardNegC]\nloss_names = [\'HardNegC\']\nloss_fns = [loss_HardNegC]\n\nimport cPickle as pickle\ndn = \'/home/old-ufo/Dropbox/HP/\'\nfor dir1 in sorted(os.listdir(dn)):\n    if \'i_cast\' not in dir1 and \'i_in\' not in dir1:\n        continue\n    print dir1\n    img1 = load_grayscale_var(os.path.join(dn,dir1) + \'/1.ppm\')\n    img2 = load_grayscale_var(os.path.join(dn,dir1) + \'/5.ppm\')\n    H_fname = os.path.join(dn,dir1) + \'/H_1_5\'\n    H = np.loadtxt(H_fname)\n    H1to2 = Variable(torch.from_numpy(H).float())\n    with torch.no_grad():\n        LAFs1 = get_geometry(img1, detector)\n        LAFs2 = deepcopy(LAFs1)\n        LAFs2[:,1,0] = 0.3 * LAFs2[:,0,0]\n        #LAFs2 = get_geometry(img2, detector)\n        min_dist, plain_indxs_in1, idxs_in_2 = get_GT_correspondence_indexes(LAFs1, LAFs2, \n                                                                         H1to2, dist_threshold = 1)\n        LAFs1 = LAFs1[plain_indxs_in1.long(),:,:]\n        LAFs2 = LAFs2[idxs_in_2.long(),:,:]\n    for loss_idx in range(len(loss_names)):\n        ln = loss_names[loss_idx]\n        print ln\n        lf = loss_fns[loss_idx]\n        LOP = LAFDiscrOptimDetach(desc_list, desc_names, cuda = True, lr = 2.0, loss_fn = lf, loss_name = ln)\n        try:\n            LOP.load_data(dir1 + \'.pickle\');\n        except:\n            LOP.optimize(deepcopy(LAFs1.data), deepcopy(LAFs2.data), img1, img2, n_iters = 100);\n            LOP.save_data(dir1 + \'.pickle\');\n    print \'opt done\'\n    #LOP.savemp4_per_desc(dir1 + \'.mp4\')  \n'"
examples/direct_shape_optimization/pytorch_sift.py,15,"b""import torch\nimport math\nimport torch.nn.init\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torch.nn.functional as F\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.abs(torch.sum(x * x, dim = 1)) + self.eps)\n        x= x / norm.unsqueeze(1).expand_as(x)\n        return x\n\ndef getPoolingKernel(kernel_size = 25):\n    step = 1. / float(np.floor( kernel_size / 2.));\n    x_coef = np.arange(step/2., 1. ,step)\n    xc2 = np.hstack([x_coef,[1], x_coef[::-1]])\n    kernel = np.outer(xc2.T,xc2)\n    kernel = np.maximum(0,kernel)\n    return kernel\ndef get_bin_weight_kernel_size_and_stride(patch_size, num_spatial_bins):\n    bin_weight_stride = int(round(2.0 * math.floor(patch_size / 2) / float(num_spatial_bins + 1)))\n    bin_weight_kernel_size = int(2 * bin_weight_stride - 1);\n    return bin_weight_kernel_size, bin_weight_stride\nclass SIFTNet(nn.Module):\n    def CircularGaussKernel(self,kernlen=21):\n        halfSize = kernlen / 2;\n        r2 = float(halfSize*halfSize);\n        sigma2 = 0.9 * r2;\n        disq = 0;\n        kernel = np.zeros((kernlen,kernlen))\n        for y in range(kernlen):\n            for x in range(kernlen):\n                disq = (y - halfSize)*(y - halfSize) +  (x - halfSize)*(x - halfSize);\n                if disq < r2:\n                    kernel[y,x] = math.exp(-disq / sigma2)\n                else:\n                    kernel[y,x] = 0.\n        return kernel\n    def __init__(self, patch_size = 65, num_ang_bins = 8, num_spatial_bins = 4, clipval = 0.2):\n        super(SIFTNet, self).__init__()\n        gk = torch.from_numpy(self.CircularGaussKernel(kernlen=patch_size).astype(np.float32))\n        self.bin_weight_kernel_size, self.bin_weight_stride = get_bin_weight_kernel_size_and_stride(patch_size, num_spatial_bins)\n        self.gk = Variable(gk)\n        self.num_ang_bins = num_ang_bins\n        self.num_spatial_bins = num_spatial_bins\n        self.clipval = clipval\n        self.gx =  nn.Sequential(nn.Conv2d(1, 1, kernel_size=(1,3),  bias = False))\n        for l in self.gx:\n            if isinstance(l, nn.Conv2d):\n                l.weight.data = torch.from_numpy(np.array([[[[-1, 0, 1]]]], dtype=np.float32))\n        self.gy =  nn.Sequential(nn.Conv2d(1, 1, kernel_size=(3,1),  bias = False))\n        for l in self.gy:\n            if isinstance(l, nn.Conv2d):\n                l.weight.data = torch.from_numpy(np.array([[[[-1], [0], [1]]]], dtype=np.float32))\n        self.pk = nn.Sequential(nn.Conv2d(1, 1, kernel_size=(self.bin_weight_kernel_size, self.bin_weight_kernel_size),\n                            stride = (self.bin_weight_stride, self.bin_weight_stride),\n                            bias = False))\n        for l in self.pk:\n            if isinstance(l, nn.Conv2d):\n                nw = getPoolingKernel(kernel_size = self.bin_weight_kernel_size)\n                new_weights = np.array(nw.reshape((1, 1, self.bin_weight_kernel_size, self.bin_weight_kernel_size)))\n                l.weight.data = torch.from_numpy(new_weights.astype(np.float32))\n    def forward(self, x):\n        gx = self.gx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0,0, 1,1), 'replicate'))\n        mag = torch.sqrt(gx **2 + gy **2 + 1e-10)\n        ori = torch.atan2(gy,gx + 1e-8)\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        else:\n            self.gk = self.gk.cpu()\n        mag  = mag * self.gk.expand_as(mag)\n        o_big = (ori +2.0 * math.pi )/ (2.0 * math.pi) * float(self.num_ang_bins)\n        bo0_big =  torch.floor(o_big)\n        wo1_big = o_big - bo0_big\n        bo0_big =  bo0_big %  self.num_ang_bins\n        bo1_big = (bo0_big + 1) % self.num_ang_bins\n        wo0_big = (1.0 - wo1_big) * mag\n        wo1_big = wo1_big * mag\n        ang_bins = []\n        for i in range(0, self.num_ang_bins):\n            ang_bins.append(self.pk((bo0_big == i).float() * wo0_big + (bo1_big == i).float() * wo1_big))\n        ang_bins = torch.cat(ang_bins,1)\n        ang_bins = ang_bins.view(ang_bins.size(0), -1)\n        ang_bins = L2Norm()(ang_bins)\n        ang_bins = torch.clamp(ang_bins, 0.,float(self.clipval))\n        ang_bins = L2Norm()(ang_bins)\n        return ang_bins\n"""
examples/hesaffnet/HandCraftedModules.py,38,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nimport numpy as np\nfrom Utils import  GaussianBlur, CircularGaussKernel\nfrom LAF import abc2A,rectifyAffineTransformationUpIsUp, sc_y_x2LAFs,sc_y_x_and_A2LAFs\nfrom Utils import generate_2dgrid, generate_2dgrid, generate_3dgrid\nfrom Utils import zero_response_at_border\n\n\nclass ScalePyramid(nn.Module):\n    def __init__(self, nLevels = 3, init_sigma = 1.6, border = 5):\n        super(ScalePyramid,self).__init__()\n        self.nLevels = nLevels;\n        self.init_sigma = init_sigma\n        self.sigmaStep =  2 ** (1. / float(self.nLevels))\n        #print 'step',self.sigmaStep\n        self.b = border\n        self.minSize = 2 * self.b + 2 + 1;\n        return\n    def forward(self,x):\n        pixelDistance = 1.0;\n        curSigma = 0.5\n        if self.init_sigma > curSigma:\n            sigma = np.sqrt(self.init_sigma**2 - curSigma**2)\n            curSigma = self.init_sigma\n            curr = GaussianBlur(sigma = sigma)(x)\n        else:\n            curr = x\n        sigmas = [[curSigma]]\n        pixel_dists = [[1.0]]\n        pyr = [[curr]]\n        j = 0\n        while True:\n            curr = pyr[-1][0]\n            for i in range(1, self.nLevels + 2):\n                sigma = curSigma * np.sqrt(self.sigmaStep*self.sigmaStep - 1.0 )\n                #print 'blur sigma', sigma\n                curr = GaussianBlur(sigma = sigma)(curr)\n                curSigma *= self.sigmaStep\n                pyr[j].append(curr)\n                sigmas[j].append(curSigma)\n                pixel_dists[j].append(pixelDistance)\n                if i == self.nLevels:\n                    nextOctaveFirstLevel = F.avg_pool2d(curr, kernel_size = 1, stride = 2, padding = 0) \n            pixelDistance = pixelDistance * 2.0\n            curSigma = self.init_sigma\n            if (nextOctaveFirstLevel[0,0,:,:].size(0)  <= self.minSize) or (nextOctaveFirstLevel[0,0,:,:].size(1) <= self.minSize):\n                break\n            pyr.append([nextOctaveFirstLevel])\n            sigmas.append([curSigma])\n            pixel_dists.append([pixelDistance])\n            j+=1\n        return pyr, sigmas, pixel_dists\n\nclass HessianResp(nn.Module):\n    def __init__(self):\n        super(HessianResp, self).__init__()\n        \n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3), bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[0.5, 0, -0.5]]]], dtype=np.float32))\n\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[0.5], [0], [-0.5]]]], dtype=np.float32))\n\n        self.gxx =  nn.Conv2d(1, 1, kernel_size=(1,3),bias = False)\n        self.gxx.weight.data = torch.from_numpy(np.array([[[[1.0, -2.0, 1.0]]]], dtype=np.float32))\n        \n        self.gyy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gyy.weight.data = torch.from_numpy(np.array([[[[1.0], [-2.0], [1.0]]]], dtype=np.float32))\n        return\n    def forward(self, x, scale):\n        gxx = self.gxx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gyy = self.gyy(F.pad(x, (0,0, 1,1), 'replicate'))\n        gxy = self.gy(F.pad(self.gx(F.pad(x, (1,1,0, 0), 'replicate')), (0,0, 1,1), 'replicate'))\n        return torch.abs(gxx * gyy - gxy * gxy) * (scale**4)\n\n\nclass AffineShapeEstimator(nn.Module):\n    def __init__(self, threshold = 0.001, patch_size = 19):\n        super(AffineShapeEstimator, self).__init__()\n        self.threshold = threshold;\n        self.PS = patch_size\n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3), bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[-1, 0, 1]]]], dtype=np.float32))\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[-1], [0], [1]]]], dtype=np.float32))\n        self.gk = torch.from_numpy(CircularGaussKernel(kernlen = self.PS, sigma = (self.PS / 2) /3.0).astype(np.float32))\n        self.gk = Variable(self.gk, requires_grad=False)\n        return\n    def invSqrt(self,a,b,c):\n        eps = 1e-12\n        mask = (b != 0).float()\n        r1 = mask * (c - a) / (2. * b + eps)\n        t1 = torch.sign(r1) / (torch.abs(r1) + torch.sqrt(1. + r1*r1));\n        r = 1.0 / torch.sqrt( 1. + t1*t1)\n        t = t1*r;\n        r = r * mask + 1.0 * (1.0 - mask);\n        t = t * mask;\n        \n        x = 1. / torch.sqrt( r*r*a - 2.0*r*t*b + t*t*c)\n        z = 1. / torch.sqrt( t*t*a + 2.0*r*t*b + r*r*c)\n        \n        d = torch.sqrt( x * z)\n        \n        x = x / d\n        z = z / d\n        \n        l1 = torch.max(x,z)\n        l2 = torch.min(x,z)\n        \n        new_a = r*r*x + t*t*z\n        new_b = -r*t*x + t*r*z\n        new_c = t*t*x + r*r *z\n\n        return new_a, new_b, new_c, l1, l2\n    def forward(self,x):\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        else:\n            self.gk = self.gk.cpu()\n        gx = self.gx(F.pad(x, (1, 1, 0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0, 0, 1, 1), 'replicate'))\n        a1 = (gx * gx * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        b1 = (gx * gy * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        c1 = (gy * gy * self.gk.unsqueeze(0).unsqueeze(0).expand_as(gx)).view(x.size(0),-1).mean(dim=1)\n        a, b, c, l1, l2 = self.invSqrt(a1,b1,c1)\n        rat1 = l1/l2\n        mask = (torch.abs(rat1) <= 6.).float().view(-1);\n        return rectifyAffineTransformationUpIsUp(abc2A(a,b,c))#, mask\nclass OrientationDetector(nn.Module):\n    def __init__(self,\n                mrSize = 3.0, patch_size = None):\n        super(OrientationDetector, self).__init__()\n        if patch_size is None:\n            patch_size = 32;\n        self.PS = patch_size;\n        self.bin_weight_kernel_size, self.bin_weight_stride = self.get_bin_weight_kernel_size_and_stride(self.PS, 1)\n        self.mrSize = mrSize;\n        self.num_ang_bins = 36\n        self.gx =  nn.Conv2d(1, 1, kernel_size=(1,3),  bias = False)\n        self.gx.weight.data = torch.from_numpy(np.array([[[[0.5, 0, -0.5]]]], dtype=np.float32))\n\n        self.gy =  nn.Conv2d(1, 1, kernel_size=(3,1), bias = False)\n        self.gy.weight.data = torch.from_numpy(np.array([[[[0.5], [0], [-0.5]]]], dtype=np.float32))\n\n        self.angular_smooth =  nn.Conv1d(1, 1, kernel_size=3, padding = 1, bias = False)\n        self.angular_smooth.weight.data = torch.from_numpy(np.array([[[0.33, 0.34, 0.33]]], dtype=np.float32))\n\n        self.gk = 10. * torch.from_numpy(CircularGaussKernel(kernlen=self.PS).astype(np.float32))\n        self.gk = Variable(self.gk, requires_grad=False)\n        return\n    def get_bin_weight_kernel_size_and_stride(self, patch_size, num_spatial_bins):\n        bin_weight_stride = int(round(2.0 * np.floor(patch_size / 2) / float(num_spatial_bins + 1)))\n        bin_weight_kernel_size = int(2 * bin_weight_stride - 1);\n        return bin_weight_kernel_size, bin_weight_stride\n    def get_rotation_matrix(self, angle_in_radians):\n        angle_in_radians = angle_in_radians.view(-1, 1, 1);\n        sin_a = torch.sin(angle_in_radians)\n        cos_a = torch.cos(angle_in_radians)\n        A1_x = torch.cat([cos_a, sin_a], dim = 2)\n        A2_x = torch.cat([-sin_a, cos_a], dim = 2)\n        transform = torch.cat([A1_x,A2_x], dim = 1)\n        return transform\n\n    def forward(self, x, return_rot_matrix = False):\n        gx = self.gx(F.pad(x, (1,1,0, 0), 'replicate'))\n        gy = self.gy(F.pad(x, (0,0, 1,1), 'replicate'))\n        mag = torch.sqrt(gx * gx + gy * gy + 1e-10)\n        if x.is_cuda:\n            self.gk = self.gk.cuda()\n        mag = mag * self.gk.unsqueeze(0).unsqueeze(0).expand_as(mag)\n        ori = torch.atan2(gy,gx)\n        o_big = float(self.num_ang_bins) *(ori + 1.0 * math.pi )/ (2.0 * math.pi)\n        bo0_big =  torch.floor(o_big)\n        wo1_big = o_big - bo0_big\n        bo0_big =  bo0_big %  self.num_ang_bins\n        bo1_big = (bo0_big + 1) % self.num_ang_bins\n        wo0_big = (1.0 - wo1_big) * mag\n        wo1_big = wo1_big * mag\n        ang_bins = []\n        for i in range(0, self.num_ang_bins):\n            ang_bins.append(F.adaptive_avg_pool2d((bo0_big == i).float() * wo0_big, (1,1)))\n        ang_bins = torch.cat(ang_bins,1).view(-1,1,self.num_ang_bins)\n        ang_bins = self.angular_smooth(ang_bins)\n        values, indices = ang_bins.view(-1,self.num_ang_bins).max(1)\n        angle =  -((2. * float(np.pi) * indices.float() / float(self.num_ang_bins)) - float(math.pi))\n        if return_rot_matrix:\n            return self.get_rotation_matrix(angle)\n        return angle\n    \nclass NMS2d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS2d, self).__init__()\n        self.MP = nn.MaxPool2d(kernel_size, stride=1, return_indices=False, padding = kernel_size/2)\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n\nclass NMS3d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS3d, self).__init__()\n        self.MP = nn.MaxPool3d(kernel_size, stride=1, return_indices=False, padding = (0, kernel_size//2, kernel_size//2))\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n        \nclass NMS3dAndComposeA(nn.Module):\n    def __init__(self, w = 0, h = 0, kernel_size = 3, threshold = 0, scales = None, border = 3, mrSize = 1.0):\n        super(NMS3dAndComposeA, self).__init__()\n        self.eps = 1e-7\n        self.ks = 3\n        self.th = threshold\n        self.cube_idxs = []\n        self.border = border\n        self.mrSize = mrSize\n        self.beta = 1.0\n        self.grid_ones = Variable(torch.ones(3,3,3,3), requires_grad=False)\n        self.NMS3d = NMS3d(kernel_size, threshold)\n        if (w > 0) and (h > 0):\n            self.spatial_grid = generate_2dgrid(h, w, False).view(1, h, w,2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        else:\n            self.spatial_grid = None\n        return\n    def forward(self, low, cur, high, num_features = 0, octaveMap = None, scales = None):\n        assert low.size() == cur.size() == high.size()\n        #Filter responce map\n        self.is_cuda = low.is_cuda;\n        resp3d = torch.cat([low,cur,high], dim = 1)\n        \n        mrSize_border = int(self.mrSize);\n        if octaveMap is not None:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border) * (1. - octaveMap.float())\n        else:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border)\n        \n        num_of_nonzero_responces = (nmsed_resp > 0).float().sum().item()#data[0]\n        if (num_of_nonzero_responces <= 1):\n            return None,None,None\n        if octaveMap is not None:\n            octaveMap = (octaveMap.float() + nmsed_resp.float()).byte()\n        \n        nmsed_resp = nmsed_resp.view(-1)\n        if (num_features > 0) and (num_features < num_of_nonzero_responces):\n            nmsed_resp, idxs = torch.topk(nmsed_resp, k = num_features, dim = 0);\n        else:\n            idxs = nmsed_resp.data.nonzero().squeeze()\n            nmsed_resp = nmsed_resp[idxs]\n        #Get point coordinates grid\n        \n        if type(scales) is not list:\n            self.grid = generate_3dgrid(3,self.ks,self.ks)\n        else:\n            self.grid = generate_3dgrid(scales,self.ks,self.ks)\n        self.grid = Variable(self.grid.t().contiguous().view(3,3,3,3), requires_grad=False)\n        if self.spatial_grid is None:\n            self.spatial_grid = generate_2dgrid(low.size(2), low.size(3), False).view(1, low.size(2), low.size(3),2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        if self.is_cuda:\n            self.spatial_grid = self.spatial_grid.cuda()\n            self.grid_ones = self.grid_ones.cuda()\n            self.grid = self.grid.cuda()\n        #residual_to_patch_center\n        sc_y_x = F.conv2d(resp3d, self.grid,\n                                padding = 1) / (F.conv2d(resp3d, self.grid_ones, padding = 1) + 1e-8)\n        \n        ##maxima coords\n        sc_y_x[0,1:,:,:] = sc_y_x[0,1:,:,:] + self.spatial_grid[:,:,:,0]\n        sc_y_x = sc_y_x.view(3,-1).t()\n        sc_y_x = sc_y_x[idxs,:]\n        \n        min_size = float(min((cur.size(2)), cur.size(3)))\n        sc_y_x[:,0] = sc_y_x[:,0] / min_size\n        sc_y_x[:,1] = sc_y_x[:,1] / float(cur.size(2))\n        sc_y_x[:,2] = sc_y_x[:,2] / float(cur.size(3))\n        return nmsed_resp, sc_y_x2LAFs(sc_y_x), octaveMap\nclass NMS3dAndComposeAAff(nn.Module):\n    def __init__(self, w = 0, h = 0, kernel_size = 3, threshold = 0, scales = None, border = 3, mrSize = 1.0):\n        super(NMS3dAndComposeAAff, self).__init__()\n        self.eps = 1e-7\n        self.ks = 3\n        self.th = threshold\n        self.cube_idxs = []\n        self.border = border\n        self.mrSize = mrSize\n        self.beta = 1.0\n        self.grid_ones = Variable(torch.ones(3,3,3,3), requires_grad=False)\n        self.NMS3d = NMS3d(kernel_size, threshold)\n        if (w > 0) and (h > 0):\n            self.spatial_grid = generate_2dgrid(h, w, False).view(1, h, w,2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        else:\n            self.spatial_grid = None\n        return\n    def forward(self, low, cur, high, num_features = 0, octaveMap = None, scales = None, aff_resp = None):\n        assert low.size() == cur.size() == high.size()\n        #Filter responce map\n        self.is_cuda = low.is_cuda;\n        resp3d = torch.cat([low,cur,high], dim = 1)\n        \n        mrSize_border = int(self.mrSize);\n        if octaveMap is not None:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border) * (1. - octaveMap.float())\n        else:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border)\n        \n        num_of_nonzero_responces = (nmsed_resp > 0).float().sum().item()#data[0]\n        if (num_of_nonzero_responces <= 1):\n            return None,None,None\n        if octaveMap is not None:\n            octaveMap = (octaveMap.float() + nmsed_resp.float()).byte()\n        \n        nmsed_resp = nmsed_resp.view(-1)\n        if (num_features > 0) and (num_features < num_of_nonzero_responces):\n            nmsed_resp, idxs = torch.topk(nmsed_resp, k = num_features, dim = 0);\n        else:\n            idxs = nmsed_resp.data.nonzero().squeeze()\n            nmsed_resp = nmsed_resp[idxs]\n        #Get point coordinates grid\n        if type(scales) is not list:\n            self.grid = generate_3dgrid(3,self.ks,self.ks)\n        else:\n            self.grid = generate_3dgrid(scales,self.ks,self.ks)\n        self.grid = Variable(self.grid.t().contiguous().view(3,3,3,3), requires_grad=False)\n        if self.spatial_grid is None:\n            self.spatial_grid = generate_2dgrid(low.size(2), low.size(3), False).view(1, low.size(2), low.size(3),2).permute(3,1, 2, 0)\n            self.spatial_grid = Variable(self.spatial_grid)\n        if self.is_cuda:\n            self.spatial_grid = self.spatial_grid.cuda()\n            self.grid_ones = self.grid_ones.cuda()\n            self.grid = self.grid.cuda()\n        \n        #residual_to_patch_center\n        sc_y_x = F.conv2d(resp3d, self.grid,\n                                padding = 1) / (F.conv2d(resp3d, self.grid_ones, padding = 1) + 1e-8)\n        \n        ##maxima coords\n        sc_y_x[0,1:,:,:] = sc_y_x[0,1:,:,:] + self.spatial_grid[:,:,:,0]\n        sc_y_x = sc_y_x.view(3,-1).t()\n        sc_y_x = sc_y_x[idxs,:]\n        if aff_resp is not None:\n            A_matrices = aff_resp.view(4,-1).t()[idxs,:]        \n        min_size = float(min((cur.size(2)), cur.size(3)))\n        \n        sc_y_x[:,0] = sc_y_x[:,0] / min_size\n        sc_y_x[:,1] = sc_y_x[:,1] / float(cur.size(2))\n        sc_y_x[:,2] = sc_y_x[:,2] / float(cur.size(3))\n        return nmsed_resp, sc_y_x_and_A2LAFs(sc_y_x,A_matrices), octaveMap\n"""
examples/hesaffnet/HardNet.py,10,"b'import sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nimport math\nimport numpy as np\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-8\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\nclass HardNetNarELU(nn.Module):\n    """"""TFeat model definition\n    """"""\n\n    def __init__(self,sm):\n        super(HardNetNarELU, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n            nn.ELU(),\n            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n            nn.ELU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n            nn.ELU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.ELU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2,padding=1),\n            nn.ELU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ELU()\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Conv2d(64, 128, kernel_size=8),\n            nn.BatchNorm2d(128, affine=False))\n        self.SIFT = sm\n        return\n\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        #print(sp)\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(input)#self.input_norm(input))\n        #x = self.classifier[1](x_features)\n        x = nn.AdaptiveAvgPool2d(1)(x_features).view(x_features.size(0), -1)\n        return x\n        #return L2Norm()(x)\n\n\nclass HardNet(nn.Module):\n    """"""HardNet model definition\n    """"""\n    def __init__(self):\n        super(HardNet, self).__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n            nn.BatchNorm2d(128, affine=False),\n        )\n        #self.features.apply(weights_init)\n\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n\n    def forward(self, input):\n        x_features = self.features(self.input_norm(input))\n        x = x_features.view(x_features.size(0), -1)\n        return L2Norm()(x)\n'"
examples/hesaffnet/LAF.py,84,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\nfrom scipy.spatial.distance import cdist\nfrom numpy.linalg import inv    \nfrom scipy.linalg import schur, sqrtm\nimport torch\nfrom  torch.autograd import Variable\nimport torch.nn.functional as F\n##########numpy\ndef invSqrt(a,b,c):\n    eps = 1e-12 \n    mask = (b !=  0)\n    r1 = mask * (c - a) / (2. * b + eps)\n    t1 = np.sign(r1) / (np.abs(r1) + np.sqrt(1. + r1*r1));\n    r = 1.0 / np.sqrt( 1. + t1*t1)\n    t = t1*r;\n    \n    r = r * mask + 1.0 * (1.0 - mask);\n    t = t * mask;\n    \n    x = 1. / np.sqrt( r*r*a - 2*r*t*b + t*t*c)\n    z = 1. / np.sqrt( t*t*a + 2*r*t*b + r*r*c)\n    \n    d = np.sqrt( x * z)\n    \n    x = x / d\n    z = z / d\n       \n    new_a = r*r*x + t*t*z\n    new_b = -r*t*x + t*r*z\n    new_c = t*t*x + r*r *z\n\n    return new_a, new_b, new_c\ndef LAFs2ellT(LAFs):\n    ellipses = torch.zeros((len(LAFs),5))\n    if LAFs.is_cuda:\n        ellipses = ellipses.cuda()\n    scale = torch.sqrt(LAFs[:,0,0]*LAFs[:,1,1]  - LAFs[:,0,1]*LAFs[:,1, 0] + 1e-10)#.view(-1,1,1)\n    unscaled_As = LAFs[:,0:2,0:2] / scale.view(-1,1,1).repeat(1,2,2)\n    u, W, v = bsvd2x2(unscaled_As)\n    #W = 1.0 / ((W *scale.view(-1,1,1).repeat(1,2,2))**2) \n    W[:,0,0] = 1.0 /  (scale*scale*W[:,0,0]**2 )\n    W[:,1,1] = 1.0 /  (scale*scale*W[:,1,1]**2 )\n    A = torch.bmm(torch.bmm(u,W), u.permute(0,2,1))\n    ellipses[:,0] = LAFs[:,0,2]\n    ellipses[:,1] = LAFs[:,1,2]\n    ellipses[:,2] = A[:,0,0]\n    ellipses[:,3] = A[:,0,1]\n    ellipses[:,4] = A[:,1,1]\n    return ellipses\ndef invSqrtTorch(a,b,c):\n    eps = 1e-12\n    mask = (b != 0).float()\n    r1 = mask * (c - a) / (2. * b + eps)\n    t1 = torch.sign(r1) / (torch.abs(r1) + torch.sqrt(1. + r1*r1));\n    r = 1.0 / torch.sqrt( 1. + t1*t1)\n    t = t1*r;\n    r = r * mask + 1.0 * (1.0 - mask);\n    t = t * mask;\n\n    x = 1. / torch.sqrt( r*r*a - 2.0*r*t*b + t*t*c)\n    z = 1. / torch.sqrt( t*t*a + 2.0*r*t*b + r*r*c)\n\n    d = torch.sqrt( x * z)\n\n    x = x / d\n    z = z / d\n\n    new_a = r*r*x + t*t*z\n    new_b = -r*t*x + t*r*z\n    new_c = t*t*x + r*r *z\n\n    return new_a, new_b, new_c,\n    \ndef ells2LAFsT(ells):\n    LAFs = torch.zeros((len(ells), 2,3))\n    LAFs[:,0,2] = ells[:,0]\n    LAFs[:,1,2] = ells[:,1]\n    a = ells[:,2]\n    b = ells[:,3]\n    c = ells[:,4]\n    sc = torch.sqrt(torch.sqrt(a*c - b*b + 1e-12))\n    ia,ib,ic = invSqrtTorch(a,b,c)  #because sqrtm returns ::-1, ::-1 matrix, don`t know why \n    A = torch.cat([torch.cat([(ia/sc).view(-1,1,1), (ib/sc).view(-1,1,1)], dim = 2),\n                   torch.cat([(ib/sc).view(-1,1,1), (ic/sc).view(-1,1,1)], dim = 2)], dim = 1)\n    sc = torch.sqrt(torch.abs(A[:,0,0] * A[:,1,1] - A[:,1,0] * A[:,0,1]))\n    LAFs[:,0:2,0:2] = rectifyAffineTransformationUpIsUp(A / sc.view(-1,1,1).repeat(1,2,2)) * sc.view(-1,1,1).repeat(1,2,2)\n    return LAFs\n\ndef LAFs_to_H_frames(aff_pts):\n    H3_x = torch.Tensor([0, 0, 1 ]).unsqueeze(0).unsqueeze(0).repeat(aff_pts.size(0),1,1);\n    if aff_pts.is_cuda:\n        H3_x = H3_x.cuda()\n    return torch.cat([aff_pts, H3_x], dim = 1)\n\n\ndef checkTouchBoundary(LAFs):\n    pts = torch.FloatTensor([[-1, -1, 1, 1], [-1, 1, -1, 1], [1, 1, 1, 1]]).unsqueeze(0)\n    if LAFs.is_cuda:\n        pts = pts.cuda()\n    out_pts =  torch.bmm(LAFs_to_H_frames(LAFs),pts.expand(LAFs.size(0),3,4))[:,:2,:]\n    good_points = 1 -(((out_pts > 1.0) +  (out_pts < 0.0)).sum(dim=1).sum(dim=1) > 0)\n    return good_points\n\ndef bsvd2x2(As):\n    Su = torch.bmm(As,As.permute(0,2,1))\n    phi = 0.5 * torch.atan2(Su[:,0,1] + Su[:,1,0] + 1e-12, Su[:,0,0] - Su[:,1,1] + 1e-12)\n    Cphi = torch.cos(phi)\n    Sphi = torch.sin(phi)\n    U = torch.zeros(As.size(0),2,2)\n    if As.is_cuda:\n        U = U.cuda()\n    U[:,0,0] = Cphi\n    U[:,1,1] = Cphi\n    U[:,0,1] = -Sphi\n    U[:,1,0] = Sphi\n    Sw = torch.bmm(As.permute(0,2,1),As)\n    theta = 0.5 * torch.atan2(Sw[:,0,1] + Sw[:,1,0] + 1e-12, Sw[:,0,0] - Sw[:,1,1] + 1e-12)\n    Ctheta = torch.cos(theta)\n    Stheta = torch.sin(theta)\n    W = torch.zeros(As.size(0),2,2)\n    if As.is_cuda:\n        W = W.cuda()\n    W[:,0,0] = Ctheta\n    W[:,1,1] = Ctheta\n    W[:,0,1] = -Stheta\n    W[:,1,0] = Stheta\n    SUsum = Su[:,0,0] + Su[:,1,1]\n    SUdif = torch.sqrt((Su[:,0,0] - Su[:,1,1])**2 + 4 * Su[:,0,1]*Su[:,1,0] + 1e-12)\n    if As.is_cuda:\n        SIG = torch.zeros(As.size(0),2,2).cuda()\n        SIG[:,0,0] = torch.sqrt((SUsum+SUdif)/2.0)\n        SIG[:,1,1] = torch.sqrt((SUsum-SUdif)/2.0)\n    else:\n        SIG = torch.zeros(As.size(0),2,2)\n        SIG[:,0,0] = torch.sqrt((SUsum+SUdif)/2.0)\n        SIG[:,1,1] = torch.sqrt((SUsum-SUdif)/2.0)\n    S = torch.bmm(torch.bmm(U.permute(0,2,1),As),W)\n    C = torch.sign(S)\n    C[:,0,1] = 0\n    C[:,1,0] = 0\n    V = torch.bmm(W,C)\n    return (U,SIG,V)\n\ndef getLAFelongation(LAFs):\n    u,s,v = bsvd2x2(LAFs[:,:2,:2])\n    return torch.max(s[:,0,0],s[:,1,1]) / torch.min(s[:,0,0],s[:,1,1])\n\ndef getNumCollapsed(LAFs, th = 10.0):\n    el = getLAFelongation(LAFs)\n    return (el > th).float().sum()\n\ndef Ell2LAF(ell):\n    A23 = np.zeros((2,3))\n    A23[0,2] = ell[0]\n    A23[1,2] = ell[1]\n    a = ell[2]\n    b = ell[3]\n    c = ell[4]\n    sc = np.sqrt(np.sqrt(a*c - b*b))\n    ia,ib,ic = invSqrt(a,b,c)  #because sqrtm returns ::-1, ::-1 matrix, don`t know why \n    A = np.array([[ia, ib], [ib, ic]]) / sc\n    sc = np.sqrt(A[0,0] * A[1,1] - A[1,0] * A[0,1])\n    A23[0:2,0:2] = rectifyAffineTransformationUpIsUp(A / sc) * sc\n    return A23\n\ndef rectifyAffineTransformationUpIsUp_np(A):\n    det = np.sqrt(np.abs(A[0,0]*A[1,1] - A[1,0]*A[0,1] + 1e-10))\n    b2a2 = np.sqrt(A[0,1] * A[0,1] + A[0,0] * A[0,0])\n    A_new = np.zeros((2,2))\n    A_new[0,0] = b2a2 / det\n    A_new[0,1] = 0\n    A_new[1,0] = (A[1,1]*A[0,1]+A[1,0]*A[0,0])/(b2a2*det)\n    A_new[1,1] = det / b2a2\n    return A_new\n\ndef ells2LAFs(ells):\n    LAFs = np.zeros((len(ells), 2,3))\n    for i in range(len(ells)):\n        LAFs[i,:,:] = Ell2LAF(ells[i,:])\n    return LAFs\n\ndef LAF2pts(LAF, n_pts = 50):\n    a = np.linspace(0, 2*np.pi, n_pts);\n    x = [0]\n    x.extend(list(np.sin(a)))\n    x = np.array(x).reshape(1,-1)\n    y = [0]\n    y.extend(list(np.cos(a)))\n    y = np.array(y).reshape(1,-1)\n    HLAF = np.concatenate([LAF, np.array([0,0,1]).reshape(1,3)])\n    H_pts =np.concatenate([x,y,np.ones(x.shape)])\n    H_pts_out = np.transpose(np.matmul(HLAF, H_pts))\n    H_pts_out[:,0] = H_pts_out[:,0] / H_pts_out[:, 2]\n    H_pts_out[:,1] = H_pts_out[:,1] / H_pts_out[:, 2]\n    return H_pts_out[:,0:2]\n\n\ndef convertLAFs_to_A23format(LAFs):\n    sh = LAFs.shape\n    if (len(sh) == 3) and (sh[1]  == 2) and (sh[2] == 3): # n x 2 x 3 classical [A, (x;y)] matrix\n        work_LAFs = deepcopy(LAFs)\n    elif (len(sh) == 2) and (sh[1]  == 7): #flat format, x y scale a11 a12 a21 a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] * LAFs[:,3] \n        work_LAFs[:,0,1] = LAFs[:,2] * LAFs[:,4]\n        work_LAFs[:,1,0] = LAFs[:,2] * LAFs[:,5]\n        work_LAFs[:,1,1] = LAFs[:,2] * LAFs[:,6]\n    elif (len(sh) == 2) and (sh[1]  == 6): #flat format, x y s*a11 s*a12 s*a21 s*a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] \n        work_LAFs[:,0,1] = LAFs[:,3]\n        work_LAFs[:,1,0] = LAFs[:,4]\n        work_LAFs[:,1,1] = LAFs[:,5]\n    else:\n        print ('Unknown LAF format')\n        return None\n    return work_LAFs\n\ndef LAFs2ell(in_LAFs):\n    LAFs = convertLAFs_to_A23format(in_LAFs)\n    ellipses = np.zeros((len(LAFs),5))\n    for i in range(len(LAFs)):\n        LAF = deepcopy(LAFs[i,:,:])\n        scale = np.sqrt(LAF[0,0]*LAF[1,1]  - LAF[0,1]*LAF[1, 0] + 1e-10)\n        u, W, v = np.linalg.svd(LAF[0:2,0:2] / scale, full_matrices=True)\n        W[0] = 1. / (W[0]*W[0]*scale*scale)\n        W[1] = 1. / (W[1]*W[1]*scale*scale)\n        A =  np.matmul(np.matmul(u, np.diag(W)), u.transpose())\n        ellipses[i,0] = LAF[0,2]\n        ellipses[i,1] = LAF[1,2]\n        ellipses[i,2] = A[0,0]\n        ellipses[i,3] = A[0,1]\n        ellipses[i,4] = A[1,1]\n    return ellipses\n\ndef visualize_LAFs(img, LAFs, color = 'r', show = False, save_to = None):\n    work_LAFs = convertLAFs_to_A23format(LAFs)\n    plt.figure()\n    plt.imshow(255 - img)\n    for i in range(len(work_LAFs)):\n        ell = LAF2pts(work_LAFs[i,:,:])\n        plt.plot( ell[:,0], ell[:,1], color)\n    if show:\n        plt.show()\n    if save_to is not None:\n        plt.savefig(save_to)\n    return \n\n####pytorch\n\ndef get_normalized_affine_shape(tilt, angle_in_radians):\n    assert tilt.size(0) == angle_in_radians.size(0)\n    num = tilt.size(0)\n    tilt_A = Variable(torch.eye(2).view(1,2,2).repeat(num,1,1))\n    if tilt.is_cuda:\n        tilt_A = tilt_A.cuda()\n    tilt_A[:,0,0] = tilt.view(-1);\n    rotmat = get_rotation_matrix(angle_in_radians)\n    out_A = rectifyAffineTransformationUpIsUp(torch.bmm(rotmat, torch.bmm(tilt_A, rotmat)))\n    #re_scale = (1.0/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    #re_scale = (0.5 + 0.5/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    return out_A# * re_scale.view(-1,1,1).expand(num,2,2)\n\ndef get_rotation_matrix(angle_in_radians):\n    angle_in_radians = angle_in_radians.view(-1, 1, 1);\n    sin_a = torch.sin(angle_in_radians)\n    cos_a = torch.cos(angle_in_radians)\n    A1_x = torch.cat([cos_a, sin_a], dim = 2)\n    A2_x = torch.cat([-sin_a, cos_a], dim = 2)\n    transform = torch.cat([A1_x,A2_x], dim = 1)\n    return transform\n    \ndef rectifyAffineTransformationUpIsUp(A):\n    det = torch.sqrt(torch.abs(A[:,0,0]*A[:,1,1] - A[:,1,0]*A[:,0,1] + 1e-10))\n    b2a2 = torch.sqrt(A[:,0,1] * A[:,0,1] + A[:,0,0] * A[:,0,0])\n    A1_ell = torch.cat([(b2a2 / det).contiguous().view(-1,1,1), 0 * det.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([((A[:,1,1]*A[:,0,1]+A[:,1,0]*A[:,0,0])/(b2a2*det)).contiguous().view(-1,1,1),\n                        (det / b2a2).contiguous().view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\ndef rectifyAffineTransformationUpIsUpFullyConv(A):#A is (n,4,h,w) tensor\n    det = torch.sqrt(torch.abs(A[:,0:1,:,:]*A[:,3:4,:,:] - A[:,1:2,:,:]*A[:,2:3,:,:] + 1e-10))\n    b2a2 = torch.sqrt(A[:,1:2,:,:] * A[:,1:2,:,:] + A[:,0:1,:,:] * A[:,0:1,:,:])\n    return torch.cat([(b2a2 / det).contiguous(),0 * det.contiguous(),\n                      (A[:,3:4,:,:]*A[:,1:2,:,:]+A[:,2:3,:,:]*A[:,0:1,:,:])/(b2a2*det),(det / b2a2).contiguous()], dim = 1)\n\ndef abc2A(a,b,c, normalize = False):\n    A1_ell = torch.cat([a.view(-1,1,1), b.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([b.view(-1,1,1), c.view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\n\n\ndef angles2A(angles):\n    cos_a = torch.cos(angles).view(-1, 1, 1)\n    sin_a = torch.sin(angles).view(-1, 1, 1)\n    A1_ang = torch.cat([cos_a, sin_a], dim = 2)\n    A2_ang = torch.cat([-sin_a, cos_a], dim = 2)\n    return  torch.cat([A1_ang, A2_ang], dim = 1)\n\ndef generate_patch_grid_from_normalized_LAFs(LAFs, w, h, PS):\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3) * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    grid = F.affine_grid(LAFs * Variable(coef.expand(num_lafs,2,3)), torch.Size((num_lafs,1,PS,PS)))\n    grid[:,:,:,0] = 2.0 * grid[:,:,:,0] / float(w)  - 1.0\n    grid[:,:,:,1] = 2.0 * grid[:,:,:,1] / float(h)  - 1.0     \n    return grid\n\ndef batched_grid_apply(img, grid, batch_size = 32):\n    n_patches = len(grid)\n    if n_patches > batch_size:\n        bs = batch_size\n        n_batches = int(n_patches / bs + 1)\n        for batch_idx in range(n_batches):\n            st = batch_idx * bs\n            if batch_idx == n_batches - 1:\n                if (batch_idx + 1) * bs > n_patches:\n                    end = n_patches\n                else:\n                    end = (batch_idx + 1) * bs\n            else:\n                end = (batch_idx + 1) * bs\n            if st >= end:\n                continue\n            if batch_idx == 0:\n                if img.size(0) != grid.size(0):\n                    first_batch_out = F.grid_sample(img.expand(end - st, img.size(1), img.size(2), img.size(3)), grid[st:end, :,:,:])# kwargs)\n                else:\n                    first_batch_out = F.grid_sample(img[st:end], grid[st:end, :,:,:])# kwargs)\n                out_size = torch.Size([n_patches] + list(first_batch_out.size()[1:]))\n                out = torch.zeros(out_size);\n                if img.is_cuda:\n                    out = out.cuda()\n                out[st:end] = first_batch_out\n            else:\n                if img.size(0) != grid.size(0):\n                    out[st:end,:,:] = F.grid_sample(img.expand(end - st, img.size(1), img.size(2), img.size(3)), grid[st:end, :,:,:])\n                else:\n                    out[st:end,:,:] = F.grid_sample(img[st:end], grid[st:end, :,:,:])\n        return out\n    else:\n        if img.size(0) != grid.size(0):\n            return F.grid_sample(img.expand(grid.size(0), img.size(1), img.size(2), img.size(3)), grid)\n        else:\n            return F.grid_sample(img, grid)\n\ndef extract_patches(img, LAFs, PS = 32, bs = 32):\n    w = img.size(3)\n    h = img.size(2)\n    ch = img.size(1)\n    grid = generate_patch_grid_from_normalized_LAFs(LAFs, float(w),float(h), PS)\n    if bs is None:\n        return torch.nn.functional.grid_sample(img.expand(grid.size(0), ch, h, w),  grid)  \n    else:\n        return batched_grid_apply(img, grid, bs)\ndef get_pyramid_inverted_index_for_LAFs(LAFs, PS, sigmas):\n    return\n\ndef extract_patches_from_pyramid_with_inv_index(scale_pyramid, pyr_inv_idxs, LAFs, PS = 19):\n    patches = torch.zeros(LAFs.size(0),scale_pyramid[0][0].size(1), PS, PS)\n    if LAFs.is_cuda:\n        patches = patches.cuda()\n    patches = Variable(patches)\n    if pyr_inv_idxs is not None:\n        for i in range(len(scale_pyramid)):\n            for j in range(len(scale_pyramid[i])):\n                cur_lvl_idxs = pyr_inv_idxs[i][j]\n                if cur_lvl_idxs is None:\n                    continue\n                cur_lvl_idxs = cur_lvl_idxs.view(-1)\n                #print i,j,cur_lvl_idxs.shape\n                patches[cur_lvl_idxs,:,:,:] = extract_patches(scale_pyramid[i][j], LAFs[cur_lvl_idxs, :,:], PS, 32 )\n    return patches\n\ndef get_inverted_pyr_index(scale_pyr, pyr_idxs, level_idxs):\n    pyr_inv_idxs = []\n    ### Precompute octave inverted indexes\n    for i in range(len(scale_pyr)):\n        pyr_inv_idxs.append([])\n        cur_idxs = pyr_idxs == i #torch.nonzero((pyr_idxs == i).data)\n        for j in range(0, len(scale_pyr[i])):\n            cur_lvl_idxs = torch.nonzero(((level_idxs == j) * cur_idxs).data)\n            if cur_lvl_idxs.size(0) == 0:\n                pyr_inv_idxs[i].append(None)\n            else:\n                pyr_inv_idxs[i].append(cur_lvl_idxs.squeeze())\n    return pyr_inv_idxs\n\n\ndef denormalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef normalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  / min_size\n    coef[0,0,2] = 1.0 / w\n    coef[0,1,2] = 1.0 / h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef sc_y_x2LAFs(sc_y_x):\n    base_LAF = torch.eye(2).float().unsqueeze(0).expand(sc_y_x.size(0),2,2)\n    if sc_y_x.is_cuda:\n        base_LAF = base_LAF.cuda()\n    base_A = Variable(base_LAF, requires_grad=False)\n    A = sc_y_x[:,:1].unsqueeze(1).expand_as(base_A) * base_A\n    LAFs  = torch.cat([A,\n                       torch.cat([sc_y_x[:,2:].unsqueeze(-1),\n                                    sc_y_x[:,1:2].unsqueeze(-1)], dim=1)], dim = 2)\n        \n    return LAFs\ndef sc_y_x_and_A2LAFs(sc_y_x, A_flat):\n    base_A = A_flat.view(-1,2,2)\n    A = sc_y_x[:,:1].unsqueeze(1).expand_as(base_A) * base_A\n    LAFs  = torch.cat([A,\n                       torch.cat([sc_y_x[:,2:].unsqueeze(-1),\n                                    sc_y_x[:,1:2].unsqueeze(-1)], dim=1)], dim = 2)\n        \n    return LAFs\ndef get_LAFs_scales(LAFs):\n    return torch.sqrt(torch.abs(LAFs[:,0,0] *LAFs[:,1,1] - LAFs[:,0,1] * LAFs[:,1,0]) + 1e-12)\n\ndef get_pyramid_and_level_index_for_LAFs(dLAFs,  sigmas, pix_dists, PS):\n    scales = get_LAFs_scales(dLAFs);\n    needed_sigmas = scales / PS;\n    sigmas_full_list = []\n    level_idxs_full = []\n    oct_idxs_full = []\n    for oct_idx in range(len(sigmas)):\n        sigmas_full_list = sigmas_full_list + list(np.array(sigmas[oct_idx])*np.array(pix_dists[oct_idx]))\n        oct_idxs_full = oct_idxs_full + [oct_idx]*len(sigmas[oct_idx])\n        level_idxs_full = level_idxs_full + list(range(0,len(sigmas[oct_idx])))\n    oct_idxs_full = torch.LongTensor(oct_idxs_full)\n    level_idxs_full = torch.LongTensor(level_idxs_full)\n    \n    closest_imgs = cdist(np.array(sigmas_full_list).reshape(-1,1), needed_sigmas.data.cpu().numpy().reshape(-1,1)).argmin(axis = 0)\n    closest_imgs = torch.from_numpy(closest_imgs)\n    if dLAFs.is_cuda:\n        closest_imgs = closest_imgs.cuda()\n        oct_idxs_full = oct_idxs_full.cuda()\n        level_idxs_full = level_idxs_full.cuda()\n    return  Variable(oct_idxs_full[closest_imgs]), Variable(level_idxs_full[closest_imgs])\n"""
examples/hesaffnet/Losses.py,49,"b'import torch\nimport torch.nn as nn\nimport sys\n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1).unsqueeze(-1)\n    d2_sq = torch.sum(positive * positive, dim=1).unsqueeze(-1)\n\n    eps = 1e-6\n    return torch.sqrt((d1_sq.repeat(1, positive.size(0)) + torch.t(d2_sq.repeat(1, anchor.size(0)))\n                      - 2.0 * torch.bmm(anchor.unsqueeze(0), torch.t(positive).unsqueeze(0)).squeeze(0))+eps)\n\ndef distance_vectors_pairwise(anchor, positive, negative):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    a_sq = torch.sum(anchor * anchor, dim=1)\n    p_sq = torch.sum(positive * positive, dim=1)\n    n_sq = torch.sum(negative * negative, dim=1)\n\n    eps = 1e-8\n    d_a_p = torch.sqrt(a_sq + p_sq - 2*torch.sum(anchor * positive, dim = 1) + eps)\n    d_a_n = torch.sqrt(a_sq + n_sq - 2*torch.sum(anchor * negative, dim = 1) + eps)\n    d_p_n = torch.sqrt(p_sq + n_sq - 2*torch.sum(positive * negative, dim = 1) + eps)\n    return d_a_p, d_a_n, d_p_n\n\ndef loss_random_sampling(anchor, positive, negative, anchor_swap = False, margin = 1.0, loss_type = ""triplet_margin""):\n    """"""Loss with random sampling (no hard in batch).\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.size() == negative.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    (pos, d_a_n, d_p_n) = distance_vectors_pairwise(anchor, positive, negative)\n    if anchor_swap:\n       min_neg = torch.min(d_a_n, d_p_n)\n    else:\n       min_neg = d_a_n\n\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_L2Net(anchor, positive, anchor_swap = False,  margin = 1.0, loss_type = ""triplet_margin""):\n    """"""L2Net losses: using whole batch as negatives, not only hardest.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008)-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    \n    if loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos1);\n        exp_den = torch.sum(torch.exp(2.0 - dist_matrix),1) + eps;\n        loss = -torch.log( exp_pos / exp_den )\n        if anchor_swap:\n            exp_den1 = torch.sum(torch.exp(2.0 - dist_matrix),0) + eps;\n            loss += -torch.log( exp_pos / exp_den1 )\n    else: \n        print (\'Only softmax loss works with L2Net sampling\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_HardNet(anchor, positive, anchor_swap = False, anchor_ave = False,\\\n        margin = 1.0, batch_reduce = \'min\', loss_type = ""triplet_margin""):\n    """"""HardNet margin loss - calculates loss based on distance matrix based on positive distance and closest negative distance.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive) +eps\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008).float()-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    if batch_reduce == \'min\':\n        min_neg = torch.min(dist_without_min_on_diag,1)[0]\n        if anchor_swap:\n            min_neg2 = torch.min(dist_without_min_on_diag,0)[0]\n            min_neg = torch.min(min_neg,min_neg2)\n        if False:\n            dist_matrix_a = distance_matrix_vector(anchor, anchor)+ eps\n            dist_matrix_p = distance_matrix_vector(positive,positive)+eps\n            dist_without_min_on_diag_a = dist_matrix_a+eye*10\n            dist_without_min_on_diag_p = dist_matrix_p+eye*10\n            min_neg_a = torch.min(dist_without_min_on_diag_a,1)[0]\n            min_neg_p = torch.t(torch.min(dist_without_min_on_diag_p,0)[0])\n            min_neg_3 = torch.min(min_neg_p,min_neg_a)\n            min_neg = torch.min(min_neg,min_neg_3)\n            print (min_neg_a)\n            print (min_neg_p)\n            print (min_neg_3)\n            print (min_neg)\n        min_neg = min_neg\n        pos = pos1\n    elif batch_reduce == \'average\':\n        pos = pos1.repeat(anchor.size(0)).view(-1,1).squeeze(0)\n        min_neg = dist_without_min_on_diag.view(-1,1)\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).contiguous().view(-1,1)\n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = min_neg.squeeze(0)\n    elif batch_reduce == \'random\':\n        idxs = torch.autograd.Variable(torch.randperm(anchor.size()[0]).long()).cuda()\n        min_neg = dist_without_min_on_diag.gather(1,idxs.view(-1,1))\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).gather(1,idxs.view(-1,1)) \n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = torch.t(min_neg).squeeze(0)\n        pos = pos1\n    else: \n        print (\'Unknown batch reduce mode. Try min, average or random\')\n        sys.exit(1)\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\n\ndef global_orthogonal_regularization(anchor, negative):\n\n    neg_dis = torch.sum(torch.mul(anchor,negative),1)\n    dim = anchor.size(1)\n    gor = torch.pow(torch.mean(neg_dis),2) + torch.clamp(torch.mean(torch.pow(neg_dis,2))-1.0/dim, min=0.0)\n    \n    return gor\n\n'"
examples/hesaffnet/NMS.py,5,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom Utils import CircularGaussKernel, generate_2dgrid, generate_2dgrid, generate_3dgrid, zero_response_at_border\nfrom LAF import sc_y_x2LAFs\n \nclass NMS2d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS2d, self).__init__()\n        self.MP = nn.MaxPool2d(kernel_size, stride=1, return_indices=False, padding = kernel_size/2)\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n\nclass NMS3d(nn.Module):\n    def __init__(self, kernel_size = 3, threshold = 0):\n        super(NMS3d, self).__init__()\n        self.MP = nn.MaxPool3d(kernel_size, stride=1, return_indices=False, padding = (0, kernel_size/2, kernel_size/2))\n        self.eps = 1e-5\n        self.th = threshold\n        return\n    def forward(self, x):\n        #local_maxima = self.MP(x)\n        if self.th > self.eps:\n            return  x * (x > self.th).float() * ((x + self.eps - self.MP(x)) > 0).float()\n        else:\n            return ((x - self.MP(x) + self.eps) > 0).float() * x\n        \n\nclass NMS3dAndComposeA(nn.Module):\n    def __init__(self,kernel_size = 3, threshold = 0, scales = None, border = 3, mrSize = 1.0):\n        super(NMS3dAndComposeA, self).__init__()\n        self.eps = 1e-7\n        self.ks = 3\n        if type(scales) is not list:\n            self.grid = generate_3dgrid(3,self.ks,self.ks)\n        else:\n            self.grid = generate_3dgrid(scales,self.ks,self.ks)\n        self.grid = Variable(self.grid.t().contiguous().view(3,3,3,3), requires_grad=False)\n        self.th = threshold\n        self.cube_idxs = []\n        self.border = border\n        self.mrSize = mrSize\n        self.beta = 1.0\n        self.grid_ones = Variable(torch.ones(3,3,3,3), requires_grad=False)\n        self.NMS3d = NMS3d(kernel_size, threshold)\n        return\n    def forward(self, low, cur, high, octaveMap = None, num_features = 0):\n        assert low.size() == cur.size() == high.size()\n        \n        #Filter responce map\n        self.is_cuda = low.is_cuda;\n        resp3d = torch.cat([low,cur,high], dim = 1)\n        \n        mrSize_border = int(self.mrSize);\n        if octaveMap is not None:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border) * (1. - octaveMap.float())\n        else:\n            nmsed_resp = zero_response_at_border(self.NMS3d(resp3d.unsqueeze(1)).squeeze(1)[:,1:2,:,:], mrSize_border)\n        \n        num_of_nonzero_responces = (nmsed_resp > 0).sum().data[0]\n        if (num_of_nonzero_responces == 0):\n            return None,None,None\n        if octaveMap is not None:\n            octaveMap = (octaveMap.float() + nmsed_resp.float()).byte()\n        \n        nmsed_resp = nmsed_resp.view(-1)\n        if (num_features > 0) and (num_features < num_of_nonzero_responces):\n            nmsed_resp, idxs = torch.topk(nmsed_resp, k = num_features);\n        else:\n            idxs = nmsed_resp.data.nonzero().squeeze()\n            nmsed_resp = nmsed_resp[idxs]\n        \n        #Get point coordinates\n        \n        spatial_grid = Variable(generate_2dgrid(low.size(2), low.size(3), False)).view(1,low.size(2), low.size(3),2)\n        spatial_grid = spatial_grid.permute(3,1, 2, 0)\n        if self.is_cuda:\n            spatial_grid = spatial_grid.cuda()\n            self.grid = self.grid.cuda()\n            self.grid_ones = self.grid_ones.cuda()\n        #residual_to_patch_center\n        sc_y_x = F.conv2d(resp3d, self.grid,\n                                padding = 1) / (F.conv2d(resp3d, self.grid_ones, padding = 1) + 1e-8)\n        \n        ##maxima coords\n        sc_y_x[0,1:,:,:] = sc_y_x[0,1:,:,:] + spatial_grid[:,:,:,0]\n        sc_y_x = sc_y_x.view(3,-1).t()\n        sc_y_x = sc_y_x[idxs,:]\n        \n        min_size = float(min((cur.size(2)), cur.size(3)))\n        sc_y_x[:,0] = sc_y_x[:,0] / min_size\n        sc_y_x[:,1] = sc_y_x[:,1] / float(cur.size(2))\n        sc_y_x[:,2] = sc_y_x[:,2] / float(cur.size(3))\n        \n        return nmsed_resp, sc_y_x2LAFs(sc_y_x), octaveMap'"
examples/hesaffnet/OnePassSIR.py,17,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nimport math\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom copy import deepcopy\nfrom Utils import GaussianBlur, batch_eig2x2, line_prepender, batched_forward\nfrom LAF import LAFs2ell,abc2A, angles2A, generate_patch_grid_from_normalized_LAFs, extract_patches, get_inverted_pyr_index, denormalizeLAFs, extract_patches_from_pyramid_with_inv_index, rectifyAffineTransformationUpIsUp\nfrom LAF import get_pyramid_and_level_index_for_LAFs, normalizeLAFs, checkTouchBoundary\nfrom HandCraftedModules import HessianResp, AffineShapeEstimator, OrientationDetector, ScalePyramid, NMS3dAndComposeA,NMS3dAndComposeAAff\nimport time\n\nclass OnePassSIR(nn.Module):\n    def __init__(self, \n                 border = 16,\n                 num_features = 500,\n                 patch_size = 32,\n                 mrSize = 3.0,\n                 nlevels = 3,\n                 th = None,#16.0/ 3.0,\n                 num_Baum_iters = 0,\n                 init_sigma = 1.6,\n                 RespNet = None, OriNet = None, AffNet = None):\n        super(OnePassSIR, self).__init__()\n        self.mrSize = mrSize\n        self.PS = patch_size\n        self.b = border;\n        self.num = num_features\n        self.th = th;\n        if th is not None:\n            self.num = -1\n        else:\n            self.th = 0\n        self.nlevels = nlevels\n        self.num_Baum_iters = num_Baum_iters\n        self.init_sigma = init_sigma\n        if RespNet is not None:\n            self.RespNet = RespNet\n        else:\n            self.RespNet = HessianResp()\n        if OriNet is not None:\n            self.OriNet = OriNet\n        else:\n            self.OriNet= OrientationDetector(patch_size = 19);\n        if AffNet is not None:\n            self.AffNet = AffNet\n        else:\n            self.AffNet = AffineShapeEstimator(patch_size = 19)\n        self.ScalePyrGen = ScalePyramid(nLevels = self.nlevels, init_sigma = self.init_sigma, border = self.b)\n        return\n    \n    def multiScaleDetectorAff(self,x, num_features = 0):\n        t = time.time()\n        self.scale_pyr, self.sigmas, self.pix_dists = self.ScalePyrGen(x)\n        ### Detect keypoints in scale space\n        aff_matrices = []\n        top_responces = []\n        pyr_idxs = []\n        level_idxs = []\n        det_t = 0\n        nmst = 0\n        for oct_idx in range(len(self.sigmas)):\n            #print oct_idx\n            octave = self.scale_pyr[oct_idx]\n            sigmas_oct = self.sigmas[oct_idx]\n            pix_dists_oct = self.pix_dists[oct_idx]\n            low = None\n            cur = None\n            high = None\n            octaveMap = (self.scale_pyr[oct_idx][0] * 0).byte()\n            nms_f = NMS3dAndComposeAAff(w = octave[0].size(3),\n                                     h =  octave[0].size(2),\n                                     border = self.b, mrSize = self.mrSize)\n            #oct_aff_map =  F.upsample(self.AffNet(octave[0]), (octave[0].size(2), octave[0].size(3)),mode='bilinear')\n            oct_aff_map =  self.AffNet(octave[0])\n            for level_idx in range(1, len(octave)-1):\n                if cur is None:\n                    low = torch.clamp(self.RespNet(octave[level_idx - 1], (sigmas_oct[level_idx - 1 ])) - self.th, min = 0)\n                else:\n                    low = cur\n                if high is None:\n                    cur =  torch.clamp(self.RespNet(octave[level_idx ], (sigmas_oct[level_idx ])) - self.th, min = 0)\n                else:\n                    cur = high\n                high = torch.clamp(self.RespNet(octave[level_idx + 1], (sigmas_oct[level_idx + 1 ])) - self.th, min = 0)\n                top_resp, aff_matrix, octaveMap_current  = nms_f(low, cur, high,\n                                                                 num_features = num_features,\n                                                                 octaveMap = octaveMap,\n                                                                 scales = sigmas_oct[level_idx - 1:level_idx + 2],\n                                                                 aff_resp = oct_aff_map)\n                if top_resp is None:\n                    continue\n                octaveMap = octaveMap_current\n                not_touch_boundary_idx = checkTouchBoundary(torch.cat([aff_matrix[:,:2,:2] *3.0, aff_matrix[:,:,2:]], dim =2))\n                aff_matrices.append(aff_matrix[not_touch_boundary_idx.byte()]), top_responces.append(top_resp[not_touch_boundary_idx.byte()])\n                pyr_id = Variable(oct_idx * torch.ones(aff_matrices[-1].size(0)))\n                lev_id = Variable((level_idx - 1) * torch.ones(aff_matrices[-1].size(0))) #prevBlur\n                if x.is_cuda:\n                    pyr_id = pyr_id.cuda()\n                    lev_id = lev_id.cuda()\n                pyr_idxs.append(pyr_id)\n                level_idxs.append(lev_id)\n        all_responses = torch.cat(top_responces, dim = 0)\n        aff_m_scales = torch.cat(aff_matrices,dim = 0)\n        pyr_idxs_scales = torch.cat(pyr_idxs,dim = 0)\n        level_idxs_scale = torch.cat(level_idxs, dim = 0)\n        if (num_features > 0) and (num_features < all_responses.size(0)):\n            all_responses, idxs = torch.topk(all_responses, k = num_features);\n            LAFs = torch.index_select(aff_m_scales, 0, idxs)\n            final_pyr_idxs = pyr_idxs_scales[idxs]\n            final_level_idxs = level_idxs_scale[idxs]\n        else:\n            return all_responses, aff_m_scales, pyr_idxs_scales , level_idxs_scale\n        return all_responses, LAFs, final_pyr_idxs, final_level_idxs,\n\n    def getOrientation(self, LAFs, final_pyr_idxs, final_level_idxs):\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)\n        max_iters = 1\n        ### Detect orientation\n        for i in range(max_iters):\n            angles = self.OriNet(patches_small)\n            if len(angles.size()) > 2:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles), LAFs[:,:,2:]], dim = 2)\n            else:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles2A(angles).view(-1,2,2)), LAFs[:,:,2:]], dim = 2)\n            if i != max_iters-1:\n                patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)        \n        return LAFs\n    def extract_patches_from_pyr(self, dLAFs, PS = 41):\n        pyr_idxs, level_idxs = get_pyramid_and_level_index_for_LAFs(dLAFs, self.sigmas, self.pix_dists, PS)\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, pyr_idxs, level_idxs)\n        patches = extract_patches_from_pyramid_with_inv_index(self.scale_pyr,\n                                                      pyr_inv_idxs,\n                                                      normalizeLAFs(dLAFs, self.scale_pyr[0][0].size(3), self.scale_pyr[0][0].size(2)), \n                                                      PS = PS)\n        return patches\n    def forward(self,x, do_ori = True):\n        ### Detection\n        t = time.time()\n        num_features_prefilter = self.num\n        responses, LAFs, final_pyr_idxs, final_level_idxs = self.multiScaleDetectorAff(x,num_features_prefilter)\n        print time.time() - t, 'detection multiscale'\n        t = time.time()\n        LAFs[:,0:2,0:2] =   self.mrSize * LAFs[:,:,0:2]\n        if do_ori:\n            LAFs = self.getOrientation(LAFs, final_pyr_idxs, final_level_idxs)\n        #pyr_inv_idxs = get_inverted_pyr_index(scale_pyr, final_pyr_idxs, final_level_idxs)\n        #patches = extract_patches_from_pyramid_with_inv_index(scale_pyr, pyr_inv_idxs, LAFs, PS = self.PS)\n        #patches = extract_patches(x, LAFs, PS = self.PS)\n        #print time.time() - t, len(LAFs), ' patches extraction'\n        return denormalizeLAFs(LAFs, x.size(3), x.size(2)), responses\n"""
examples/hesaffnet/ReprojectionStuff.py,66,"b'import torch\nfrom torch.autograd import Variable\nfrom torch.autograd import Variable as V\nimport numpy as np\nfrom LAF import rectifyAffineTransformationUpIsUp, LAFs_to_H_frames\nfrom Utils import zeros_like\n\n\ndef linH(H, x, y):\n    assert x.size(0) == y.size(0)\n    A = torch.zeros(x.size(0),2,2)\n    if x.is_cuda:\n        A = A.cuda()\n    den = x * H[2,0] + y * H[2,1] + H[2,2]\n    num1_densq = (x*H[0,0] + y*H[0,1] + H[0,2]) / (den*den)\n    num2_densq = (x*H[1,0] + y*H[1,1] + H[1,2]) / (den*den)\n    A[:,0,0] = H[0,0]/den - num1_densq * H[2,0]\n    A[:,0,1] = H[0,1]/den - num1_densq * H[2,1]\n    A[:,1,0] = H[1,0]/den - num2_densq * H[2,0]\n    A[:,1,1] = H[1,1]/den - num2_densq * H[2,1]\n    return A\n\ndef reprojectLAFs(LAFs1, H1to2, return_LHFs = False):\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    #LHF1_in_2 = torch.zeros(LHF1.size(0), ,3,3)\n    #if LHF1.is_cuda:\n    #    LHF1_in_2 = LHF1_in_2.cuda()\n    #LHF1_in_2 = Variable(LHF1_in_2)\n    #LHF1_in_2[:,:,2] = torch.bmm(H1to2.expand(LHF1.size(0),3,3), LHF1[:,:,2:])\n    #LHF1_in_2[:,:,2] = LHF1_in_2[:,:,2] / LHF1_in_2[:,2:,2].expand(LHF1_in_2.size(0), 3)\n    #As  = linH(H1to2, LAFs1[:,0,2], LAFs1[:,1,2])\n    #LHF1_in_2[:,0:2,0:2] = torch.bmm(As, LHF1[:,0:2,0:2])\n    xy1 = torch.bmm(H1to2.expand(LHF1.size(0),3,3), LHF1[:,:,2:])\n    xy1 = xy1 / xy1[:,2:,:].expand(xy1.size(0), 3, 1)\n    As  = linH(H1to2, LAFs1[:,0,2], LAFs1[:,1,2])\n    AF = torch.bmm(As, LHF1[:,0:2,0:2])\n    \n    if return_LHFs:\n        return LAFs_to_H_frames(torch.cat([AF, xy1[:,:2,:]], dim = 2))\n    return torch.cat([AF, xy1[:,:2,:]], dim = 2)\n\ndef Px2GridA(w, h):\n    A = torch.eye(3)\n    A[0,0] = 2.0  / float(w)\n    A[1,1] = 2.0  / float(h)\n    A[0,2] = -1\n    A[1,2] = -1\n    return A\ndef Grid2PxA(w, h):\n    A = torch.eye(3)\n    A[0,0] = float(w) / 2.0\n    A[0,2] = float(w) / 2.0\n    A[1,1] = float(h) / 2.0\n    A[1,2] = float(h) / 2.0\n    return A\n\ndef affineAug(img, max_add = 0.5):\n    img_s = img.squeeze()\n    h,w = img_s.size()\n    ### Generate A\n    A = torch.eye(3)\n    rand_add = max_add *(torch.rand(3,3) - 0.5) * 2.0\n    ##No perspective change\n    rand_add[2,0:2] = 0\n    rand_add[2,2] = 0;\n    A  = A + rand_add\n    denormA = Grid2PxA(w,h)\n    normA = Px2GridA(w, h)\n    if img.is_cuda:\n        A = A.cuda()\n        denormA = denormA.cuda()\n        normA = normA.cuda()\n    grid = torch.nn.functional.affine_grid(A[0:2,:].unsqueeze(0), torch.Size((1,1,h,w)))\n    H_Orig2New = torch.mm(torch.mm(denormA, torch.inverse(A)), normA)\n    new_img = torch.nn.functional.grid_sample(img_s.float().unsqueeze(0).unsqueeze(0),  grid)  \n    return new_img, H_Orig2New, \n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1)\n    d2_sq = torch.sum(positive * positive, dim=1)\n    eps = 1e-12\n    return torch.sqrt(torch.abs((d1_sq.expand(positive.size(0), anchor.size(0)) +\n                       torch.t(d2_sq.expand(anchor.size(0), positive.size(0)))\n                      - 2.0 * torch.bmm(positive.unsqueeze(0), torch.t(anchor).unsqueeze(0)).squeeze(0))+eps))\n\ndef ratio_matrix_vector(a, p):\n    eps = 1e-12\n    return a.expand(p.size(0), a.size(0)) / (torch.t(p.expand(a.size(0), p.size(0))) + eps)\n\n   \ndef inverseLHFs(LHFs):\n    LHF1_inv =torch.zeros(LHFs.size())\n    if LHFs.is_cuda:\n        LHF1_inv = LHF1_inv.cuda()\n    for i in range(LHF1_inv.size(0)):\n        LHF1_inv[i,:,:] = LHFs[i,:,:].inverse()\n    return LHF1_inv\n\n    \ndef reproject_to_canonical_Frob_batched(LHF1_inv, LHF2, batch_size = 2, skip_center = False):\n    out = torch.zeros((LHF1_inv.size(0), LHF2.size(0)))\n    eye1 = torch.eye(3)\n    if LHF1_inv.is_cuda:\n        out = out.cuda()\n        eye1 = eye1.cuda()\n    len1 = LHF1_inv.size(0)\n    len2 = LHF2.size(0)\n    n_batches = int(np.floor(len1 / batch_size) + 1);\n    for b_idx in range(n_batches):\n        #print b_idx\n        start = b_idx * batch_size;\n        fin = min((b_idx+1) * batch_size, len1)\n        current_bs = fin - start\n        if current_bs == 0:\n            break\n        should_be_eyes = torch.bmm(LHF1_inv[start:fin, :, :].unsqueeze(0).expand(len2,current_bs, 3, 3).contiguous().view(-1,3,3),\n                                   LHF2.unsqueeze(1).expand(len2,current_bs, 3,3).contiguous().view(-1,3,3))\n        if skip_center:\n            out[start:fin, :] = torch.sum(((should_be_eyes - eye1.unsqueeze(0).expand_as(should_be_eyes))**2)[:,:2,:2] , dim=1).sum(dim = 1).view(current_bs, len2)\n        else:\n            out[start:fin, :] = torch.sum((should_be_eyes - eye1.unsqueeze(0).expand_as(should_be_eyes))**2 , dim=1).sum(dim = 1).view(current_bs, len2)\n    return out\n\ndef get_GT_correspondence_indexes(LAFs1, LAFs2, H1to2, dist_threshold = 4):    \n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    just_centers1 = LAFs1[:,:,2];\n    just_centers2_repr_to_1 = LHF2_in_1_pre[:,0:2,2];\n    \n    dist  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1)\n    min_dist, idxs_in_2 = torch.min(dist,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    mask =  min_dist <= dist_threshold\n    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n\ndef get_GT_correspondence_indexes_Fro(LAFs1,LAFs2, H1to2, dist_threshold = 4,\n                                      skip_center_in_Fro = False):\n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1_inv = inverseLHFs(LAFs_to_H_frames(LAFs1))\n    frob_norm_dist = reproject_to_canonical_Frob_batched(LHF1_inv, LHF2_in_1_pre, batch_size = 2, skip_center = skip_center_in_Fro)\n    min_dist, idxs_in_2 = torch.min(frob_norm_dist,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    #print min_dist.min(), min_dist.max(), min_dist.mean()\n    mask =  min_dist <= dist_threshold\n    return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\n\ndef get_GT_correspondence_indexes_Fro_and_center(LAFs1,LAFs2, H1to2, \n                                                 dist_threshold = 4, \n                                                 center_dist_th = 2.0,\n                                                 scale_diff_coef = 0.3,\n                                                 skip_center_in_Fro = False,\n                                                 do_up_is_up = False,\n                                                 return_LAF2_in_1 = False,\n                                                 inv_to_eye = True):\n    LHF2_in_1_pre = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    if do_up_is_up:\n        sc2 = torch.sqrt(torch.abs(LHF2_in_1_pre[:,0,0] * LHF2_in_1_pre[:,1,1] - LHF2_in_1_pre[:,1,0] * LHF2_in_1_pre[:,0,1])).unsqueeze(-1).unsqueeze(-1).expand(LHF2_in_1_pre.size(0), 2,2)\n        LHF2_in_1 = torch.zeros(LHF2_in_1_pre.size())\n        if LHF2_in_1_pre.is_cuda:\n            LHF2_in_1 = LHF2_in_1.cuda()\n        LHF2_in_1[:, :2,:2] = rectifyAffineTransformationUpIsUp(LHF2_in_1_pre[:, :2,:2]/sc2) * sc2\n        LHF2_in_1[:,:, 2] = LHF2_in_1_pre[:,:,2]\n        sc1 = torch.sqrt(torch.abs(LAFs1[:,0,0] * LAFs1[:,1,1] - LAFs1[:,1,0] * LAFs1[:,0,1])).unsqueeze(-1).unsqueeze(-1).expand(LAFs1.size(0), 2,2)\n        LHF1 = LAFs_to_H_frames(torch.cat([rectifyAffineTransformationUpIsUp(LAFs1[:, :2,:2]/sc1) * sc1, LAFs1[:,:,2:]], dim = 2 ))\n    else:\n        LHF2_in_1 = LHF2_in_1_pre\n        LHF1 = LAFs_to_H_frames(LAFs1)\n    if inv_to_eye:\n        LHF1_inv = inverseLHFs(LHF1)\n        frob_norm_dist = reproject_to_canonical_Frob_batched(LHF1_inv, LHF2_in_1, batch_size = 2, skip_center = skip_center_in_Fro)\n    else:\n        if not skip_center_in_Fro:\n            frob_norm_dist = distance_matrix_vector(LHF2_in_1.view(LHF2_in_1.size(0), -1), LHF1.view(LHF1.size(0),-1))\n        else:\n            frob_norm_dist = distance_matrix_vector(LHF2_in_1[:,0:2, 0:2].contiguous().view(LHF2_in_1.size(0), -1), LHF1[:,0:2,0:2].contiguous().view(LHF1.size(0),-1))\n    #### Center replated\n    just_centers1 = LAFs1[:,:,2];\n    just_centers2_repr_to_1 = LHF2_in_1[:,0:2,2];\n    if scale_diff_coef > 0:\n        scales1 = torch.sqrt(torch.abs(LAFs1[:,0,0] * LAFs1[:,1,1] - LAFs1[:,1,0] * LAFs1[:,0,1]))\n        scales2 = torch.sqrt(torch.abs(LHF2_in_1[:,0,0] * LHF2_in_1[:,1,1] - LHF2_in_1[:,1,0] * LHF2_in_1[:,0,1]))\n        scale_matrix = ratio_matrix_vector(scales2, scales1)\n        scale_dist_mask = (torch.abs(1.0 - scale_matrix) <= scale_diff_coef) \n    center_dist_mask  = distance_matrix_vector(just_centers2_repr_to_1, just_centers1) >= center_dist_th\n    frob_norm_dist_masked = (1.0 - scale_dist_mask.float() + center_dist_mask.float()) * 1000. + frob_norm_dist;\n    \n    min_dist, idxs_in_2 = torch.min(frob_norm_dist_masked,1)\n    plain_indxs_in1 = torch.arange(0, idxs_in_2.size(0))\n    if LAFs1.is_cuda:\n        plain_indxs_in1 = plain_indxs_in1.cuda()\n    #min_dist, idxs_in_2 = torch.min(dist,1)\n    #print min_dist.min(), min_dist.max(), min_dist.mean()\n    mask =  (min_dist <= dist_threshold )\n    \n    if return_LAF2_in_1:\n        return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask], LHF2_in_1[:,0:2,:]\n    else:\n        return min_dist[mask], plain_indxs_in1[mask], idxs_in_2[mask]\ndef get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log):\n    xy1 = LHF1[:,0:2,2];\n    xy2in1 = LHF2_in_1[:,0:2,2];\n    center_dist_matrix =  distance_matrix_vector(xy2in1, xy1)\n    scales1 = torch.sqrt(torch.abs(LHF1[:,0,0] * LHF1[:,1,1] - LHF1[:,1,0] * LHF1[:,0,1]));\n    scales2 = torch.sqrt(torch.abs(LHF2_in_1[:,0,0] * LHF2_in_1[:,1,1] - LHF2_in_1[:,1,0] * LHF2_in_1[:,0,1]));\n    scale_matrix = torch.abs(torch.log(ratio_matrix_vector(scales2, scales1)))\n    mask_matrix = 1000.0*(scale_matrix  > scale_log).float() * (center_dist_matrix > xy_th).float() + center_dist_matrix + scale_matrix\n\n    d2_to_1, nn_idxs_in_2 = torch.min(mask_matrix,1)\n    d1_to_2, nn_idxs_in_1 = torch.min(mask_matrix,0)\n\n    flat_idxs_1 = torch.arange(0, nn_idxs_in_2.size(0));\n    if LHF1.is_cuda:\n        flat_idxs_1 = flat_idxs_1.cuda()\n    mask = d2_to_1 <= 100.0;\n\n    final_mask = (flat_idxs_1 == nn_idxs_in_1[nn_idxs_in_2].float()).float() * mask.float()\n    idxs_in1 = flat_idxs_1[final_mask.long()].nonzero().squeeze()\n    idxs_in_2_final = nn_idxs_in_2[idxs_in1];\n    #torch.arange(0, nn_idxs_in_2.size(0))#[mask2.data]\n    return idxs_in1, idxs_in_2_final\ndef get_LHFScale(LHF):\n    return torch.sqrt(torch.abs(LHF[:,0,0] * LHF[:,1,1] - LHF[:,1,0] * LHF[:,0,1]));\ndef LAFMagic(LAFs1, LAFs2, H1to2, xy_th  = 5.0, scale_log = 0.4, t = 1.0, sc = 1.0, aff = 1.0):\n    LHF2_in_1 = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    idxs_in1, idxs_in_2 = get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log)\n    if len(idxs_in1) == 0:\n        print(\'Warning, no correspondences found\')\n        return None\n    LHF1_good = LHF1[idxs_in1,:,:]\n    LHF2_good = LHF2_in_1[idxs_in_2,:,:]\n    scales1 = get_LHFScale(LHF1_good);\n    scales2 = get_LHFScale(LHF2_good);\n    max_scale = torch.max(scales1,scales2);\n    min_scale = torch.min(scales1, scales2);\n    mean_scale = 0.5 * (max_scale + min_scale)\n    eps = 1e-12;\n    if t != 0:\n        dist_loss = torch.sqrt(torch.sum((LHF1_good[:,0:2,2] - LHF2_good[:,0:2,2])**2, dim = 1) + eps) / V(mean_scale.data);\n    else:\n        dist_loss = 0\n    if sc != 0 :\n        scale_loss = torch.log1p( (max_scale-min_scale)/(mean_scale))\n    else:\n        scale_loss = 0\n    if aff != 0:\n        A1 = LHF1_good[:,:2,:2] / scales1.view(-1,1,1).expand(scales1.size(0),2,2);\n        A2 = LHF2_good[:,:2,:2] / scales2.view(-1,1,1).expand(scales2.size(0),2,2);\n        shape_loss = ((A1 - A2)**2).mean(dim = 1).mean(dim = 1);\n    else:\n        shape_loss = 0;\n    loss = t * dist_loss + sc * scale_loss + aff *shape_loss;\n    #print dist_loss, scale_loss, shape_loss\n    return loss, idxs_in1, idxs_in_2, LHF2_in_1[:,0:2,:]\ndef LAFMagicFro(LAFs1, LAFs2, H1to2, xy_th  = 5.0, scale_log = 0.4):\n    LHF2_in_1 = reprojectLAFs(LAFs2, torch.inverse(H1to2), True)\n    LHF1 = LAFs_to_H_frames(LAFs1)\n    idxs_in1, idxs_in_2 = get_closest_correspondences_idxs(LHF1, LHF2_in_1, xy_th, scale_log)\n    if len(idxs_in1) == 0:\n        print(\'Warning, no correspondences found\')\n        return None\n    LHF1_good = LHF1[idxs_in1,:,:]\n    LHF2_good = LHF2_in_1[idxs_in_2,:,:]\n    scales1 = get_LHFScale(LHF1_good);\n    scales2 = get_LHFScale(LHF2_good);\n    max_scale = torch.max(scales1,scales2);\n    min_scale = torch.min(scales1, scales2);\n    mean_scale = 0.5 * (max_scale + min_scale)\n    eps = 1e-12;\n    dist_loss = (torch.sqrt((LHF1_good.view(-1,9) - LHF2_good.view(-1,9))**2 + eps) / V(mean_scale.data).view(-1,1).expand(LHF1_good.size(0),9)).mean(dim=1); \n    loss = dist_loss;\n    #print dist_loss, scale_loss, shape_loss\n    return loss, idxs_in1, idxs_in_2, LHF2_in_1[:,0:2,:]\ndef pr_l(x):\n    return x.mean().data.cpu().numpy()[0]\ndef add_1(A):\n    add = torch.eye(2).unsqueeze(0).expand(A.size(0),2,2)\n    add = torch.cat([add, torch.zeros(A.size(0),2,1)], dim = 2)\n    if A.is_cuda:\n        add = add.cuda()\n    return add\ndef identity_loss(A):\n    return torch.clamp(torch.sqrt((A - add_1(A))**2 + 1e-15).view(-1,6).mean(dim = 1) - 0.3*0, min = 0.0, max = 100.0).mean()\n\n\n\n'"
examples/hesaffnet/SparseImgRepresenter.py,27,"b""import torch\nimport torch.nn as nn\nimport numpy as np\nimport math\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom copy import deepcopy\nfrom Utils import GaussianBlur, batch_eig2x2, line_prepender, batched_forward\nfrom LAF import LAFs2ell,abc2A, angles2A, generate_patch_grid_from_normalized_LAFs, extract_patches, get_inverted_pyr_index, denormalizeLAFs, extract_patches_from_pyramid_with_inv_index, rectifyAffineTransformationUpIsUp\nfrom LAF import get_pyramid_and_level_index_for_LAFs, normalizeLAFs, checkTouchBoundary\nfrom HandCraftedModules import HessianResp, AffineShapeEstimator, OrientationDetector, ScalePyramid, NMS3dAndComposeA\nimport time\n\nclass ScaleSpaceAffinePatchExtractor(nn.Module):\n    def __init__(self, \n                 border = 16,\n                 num_features = 500,\n                 patch_size = 32,\n                 mrSize = 3.0,\n                 nlevels = 3,\n                 num_Baum_iters = 0,\n                 init_sigma = 1.6,\n                 th = None,\n                 RespNet = None, OriNet = None, AffNet = None):\n        super(ScaleSpaceAffinePatchExtractor, self).__init__()\n        self.mrSize = mrSize\n        self.PS = patch_size\n        self.b = border;\n        self.num = num_features\n        self.nlevels = nlevels\n        self.num_Baum_iters = num_Baum_iters\n        self.init_sigma = init_sigma\n        self.th = th;\n        if th is not None:\n            self.num = -1\n        else:\n            self.th = 0\n        if RespNet is not None:\n            self.RespNet = RespNet\n        else:\n            self.RespNet = HessianResp()\n        if OriNet is not None:\n            self.OriNet = OriNet\n        else:\n            self.OriNet= OrientationDetector(patch_size = 19);\n        if AffNet is not None:\n            self.AffNet = AffNet\n        else:\n            self.AffNet = AffineShapeEstimator(patch_size = 19)\n        self.ScalePyrGen = ScalePyramid(nLevels = self.nlevels, init_sigma = self.init_sigma, border = self.b)\n        return\n    \n    def multiScaleDetector(self,x, num_features = 0):\n        t = time.time()\n        self.scale_pyr, self.sigmas, self.pix_dists = self.ScalePyrGen(x)\n        ### Detect keypoints in scale space\n        aff_matrices = []\n        top_responces = []\n        pyr_idxs = []\n        level_idxs = []\n        det_t = 0\n        nmst = 0\n        for oct_idx in range(len(self.sigmas)):\n            #print oct_idx\n            octave = self.scale_pyr[oct_idx]\n            sigmas_oct = self.sigmas[oct_idx]\n            pix_dists_oct = self.pix_dists[oct_idx]\n            low = None\n            cur = None\n            high = None\n            octaveMap = (self.scale_pyr[oct_idx][0] * 0).byte()\n            nms_f = NMS3dAndComposeA(w = octave[0].size(3),\n                                     h =  octave[0].size(2),\n                                     border = self.b, mrSize = self.mrSize)\n            for level_idx in range(1, len(octave)-1):\n                if cur is None:\n                    low = torch.clamp(self.RespNet(octave[level_idx - 1], (sigmas_oct[level_idx - 1 ])) - self.th, min = 0)\n                else:\n                    low = cur\n                if high is None:\n                    cur =  torch.clamp(self.RespNet(octave[level_idx ], (sigmas_oct[level_idx ])) - self.th, min = 0)\n                else:\n                    cur = high\n                high = torch.clamp(self.RespNet(octave[level_idx + 1], (sigmas_oct[level_idx + 1 ])) - self.th, min = 0)\n                top_resp, aff_matrix, octaveMap_current  = nms_f(low, cur, high,\n                                                                 num_features = num_features,\n                                                                 octaveMap = octaveMap,\n                                                                 scales = sigmas_oct[level_idx - 1:level_idx + 2])\n                if top_resp is None:\n                    continue\n                octaveMap = octaveMap_current\n                aff_matrices.append(aff_matrix), top_responces.append(top_resp)\n                pyr_id = Variable(oct_idx * torch.ones(aff_matrix.size(0)))\n                lev_id = Variable((level_idx - 1) * torch.ones(aff_matrix.size(0))) #prevBlur\n                if x.is_cuda:\n                    pyr_id = pyr_id.cuda()\n                    lev_id = lev_id.cuda()\n                pyr_idxs.append(pyr_id)\n                level_idxs.append(lev_id)\n        all_responses = torch.cat(top_responces, dim = 0)\n        aff_m_scales = torch.cat(aff_matrices,dim = 0)\n        pyr_idxs_scales = torch.cat(pyr_idxs,dim = 0)\n        level_idxs_scale = torch.cat(level_idxs, dim = 0)\n        if (num_features > 0) and (num_features < all_responses.size(0)):\n            all_responses, idxs = torch.topk(all_responses, k = num_features);\n            LAFs = torch.index_select(aff_m_scales, 0, idxs)\n            final_pyr_idxs = pyr_idxs_scales[idxs]\n            final_level_idxs = level_idxs_scale[idxs]\n        else:\n            return all_responses, aff_m_scales, pyr_idxs_scales , level_idxs_scale\n        return all_responses, LAFs, final_pyr_idxs, final_level_idxs,\n    \n    def getAffineShape(self, final_resp, LAFs, final_pyr_idxs, final_level_idxs, num_features = 0):\n        pe_time = 0\n        affnet_time = 0\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        t = time.time()\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.AffNet.PS)\n        pe_time+=time.time() - t\n        t = time.time()\n        base_A = torch.eye(2).unsqueeze(0).expand(final_pyr_idxs.size(0),2,2)\n        if final_resp.is_cuda:\n            base_A = base_A.cuda()\n        base_A = Variable(base_A)\n        is_good = None\n        n_patches = patches_small.size(0)\n        for i in range(self.num_Baum_iters):\n            t = time.time()\n            A = batched_forward(self.AffNet, patches_small, 256)\n            is_good_current = 1\n            affnet_time += time.time() - t\n            if is_good is None:\n                is_good = is_good_current\n            else:\n                is_good = is_good * is_good_current\n            base_A = torch.bmm(A, base_A); \n            new_LAFs = torch.cat([torch.bmm(base_A,LAFs[:,:,0:2]), LAFs[:,:,2:] ], dim =2)\n            #print torch.sqrt(new_LAFs[0,0,0]*new_LAFs[0,1,1] - new_LAFs[0,1,0] *new_LAFs[0,0,1]) * scale_pyr[0][0].size(2)\n            if i != self.num_Baum_iters - 1:\n                pe_time+=time.time() - t\n                t = time.time()\n                patches_small =  extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, new_LAFs, PS = self.AffNet.PS)\n                pe_time+= time.time() - t\n                l1,l2 = batch_eig2x2(A)      \n                ratio1 =  torch.abs(l1 / (l2 + 1e-8))\n                converged_mask = (ratio1 <= 1.2) * (ratio1 >= (0.8)) \n        l1,l2 = batch_eig2x2(base_A)\n        ratio = torch.abs(l1 / (l2 + 1e-8))\n        idxs_mask =  ((ratio < 6.0) * (ratio > (1./6.))) * checkTouchBoundary(new_LAFs)\n        num_survived = idxs_mask.float().sum()\n        if (num_features > 0) and (num_survived.data.item() > num_features):\n            final_resp =  final_resp * idxs_mask.float() #zero bad points\n            final_resp, idxs = torch.topk(final_resp, k = num_features);\n        else:\n            idxs = Variable(torch.nonzero(idxs_mask.data).view(-1).long())\n            final_resp = final_resp[idxs]\n        final_pyr_idxs = final_pyr_idxs[idxs]\n        final_level_idxs = final_level_idxs[idxs]\n        base_A = torch.index_select(base_A, 0, idxs)\n        LAFs = torch.index_select(LAFs, 0, idxs)\n        new_LAFs = torch.cat([torch.bmm(base_A, LAFs[:,:,0:2]),\n                               LAFs[:,:,2:]], dim =2)\n        print ('affnet_time',affnet_time)\n        print ('pe_time', pe_time)\n        return final_resp, new_LAFs, final_pyr_idxs, final_level_idxs  \n    \n    def getOrientation(self, LAFs, final_pyr_idxs, final_level_idxs):\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)\n        max_iters = 1\n        ### Detect orientation\n        for i in range(max_iters):\n            angles = self.OriNet(patches_small)\n            if len(angles.size()) > 2:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles), LAFs[:,:,2:]], dim = 2)\n            else:\n                LAFs = torch.cat([torch.bmm( LAFs[:,:,:2], angles2A(angles).view(-1,2,2)), LAFs[:,:,2:]], dim = 2)\n            if i != max_iters:\n                patches_small = extract_patches_from_pyramid_with_inv_index(self.scale_pyr, pyr_inv_idxs, LAFs, PS = self.OriNet.PS)        \n        return LAFs\n    def extract_patches_from_pyr(self, dLAFs, PS = 41):\n        pyr_idxs, level_idxs = get_pyramid_and_level_index_for_LAFs(dLAFs, self.sigmas, self.pix_dists, PS)\n        pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, pyr_idxs, level_idxs)\n        patches = extract_patches_from_pyramid_with_inv_index(self.scale_pyr,\n                                                      pyr_inv_idxs,\n                                                      normalizeLAFs(dLAFs, self.scale_pyr[0][0].size(3), self.scale_pyr[0][0].size(2)), \n                                                      PS = PS)\n        return patches\n    def forward(self,x, do_ori = False):\n        ### Detection\n        t = time.time()\n        num_features_prefilter = self.num\n        if self.num_Baum_iters > 0:\n            num_features_prefilter = int(1.5 * self.num);\n        responses, LAFs, final_pyr_idxs, final_level_idxs = self.multiScaleDetector(x,num_features_prefilter)\n        print (time.time() - t, 'detection multiscale')\n        t = time.time()\n        LAFs[:,0:2,0:2] =   self.mrSize * LAFs[:,:,0:2]\n        if self.num_Baum_iters > 0:\n            responses, LAFs, final_pyr_idxs, final_level_idxs  = self.getAffineShape(responses, LAFs, final_pyr_idxs, final_level_idxs, self.num)\n        print (time.time() - t, 'affine shape iters')\n        t = time.time()\n        if do_ori:\n            LAFs = self.getOrientation(LAFs, final_pyr_idxs, final_level_idxs)\n            #pyr_inv_idxs = get_inverted_pyr_index(self.scale_pyr, final_pyr_idxs, final_level_idxs)\n        #patches = extract_patches_from_pyramid_with_inv_index(scale_pyr, pyr_inv_idxs, LAFs, PS = self.PS)\n        #patches = extract_patches(x, LAFs, PS = self.PS)\n        #print time.time() - t, len(LAFs), ' patches extraction'\n        return denormalizeLAFs(LAFs, x.size(3), x.size(2)), responses\n"""
examples/hesaffnet/Utils.py,25,"b'import torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport cv2\nimport numpy as np\n\n# resize image to size 32x32\ncv2_scale = lambda x: cv2.resize(x, dsize=(32, 32),\n                                 interpolation=cv2.INTER_LINEAR)\n# reshape image\nnp_reshape32 = lambda x: np.reshape(x, (32, 32, 1))\nnp_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n\ndef zeros_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.zeros(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.zeros(y)\n\ndef ones_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.ones(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.ones(y)\n    \n\ndef batched_forward(model, data, batch_size, **kwargs):\n    n_patches = len(data)\n    if n_patches > batch_size:\n        bs = batch_size\n        n_batches = int(n_patches / bs + 1)\n        for batch_idx in range(n_batches):\n            st = batch_idx * bs\n            if batch_idx == n_batches - 1:\n                if (batch_idx + 1) * bs > n_patches:\n                    end = n_patches\n                else:\n                    end = (batch_idx + 1) * bs\n            else:\n                end = (batch_idx + 1) * bs\n            if st >= end:\n                continue\n            if batch_idx == 0:\n                first_batch_out = model(data[st:end], kwargs)\n                out_size = torch.Size([n_patches] + list(first_batch_out.size()[1:]))\n                #out_size[0] = n_patches\n                out = torch.zeros(out_size);\n                if data.is_cuda:\n                    out = out.cuda()\n                out = Variable(out)\n                out[st:end] = first_batch_out\n            else:\n                out[st:end,:,:] = model(data[st:end], kwargs)\n        return out\n    else:\n        return model(data, kwargs)\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n\ndef CircularGaussKernel(kernlen=None, circ_zeros = False, sigma = None, norm = True):\n    assert ((kernlen is not None) or sigma is not None)\n    if kernlen is None:\n        kernlen = int(2.0 * 3.0 * sigma + 1.0)\n        if (kernlen % 2 == 0):\n            kernlen = kernlen + 1;\n        halfSize = kernlen / 2;\n    halfSize = kernlen / 2;\n    r2 = float(halfSize*halfSize)\n    if sigma is None:\n        sigma2 = 0.9 * r2;\n        sigma = np.sqrt(sigma2)\n    else:\n        sigma2 = 2.0 * sigma * sigma    \n    x = np.linspace(-halfSize,halfSize,kernlen)\n    xv, yv = np.meshgrid(x, x, sparse=False, indexing=\'xy\')\n    distsq = (xv)**2 + (yv)**2\n    kernel = np.exp(-( distsq/ (sigma2)))\n    if circ_zeros:\n        kernel *= (distsq <= r2).astype(np.float32)\n    if norm:\n        kernel /= np.sum(kernel)\n    return kernel\n\ndef generate_2dgrid(h,w, centered = True):\n    if centered:\n        x = torch.linspace(-w/2+1, w/2, w)\n        y = torch.linspace(-h/2+1, h/2, h)\n    else:\n        x = torch.linspace(0, w-1, w)\n        y = torch.linspace(0, h-1, h)\n    grid2d = torch.stack([y.repeat(w,1).t().contiguous().view(-1), x.repeat(h)],1)\n    return grid2d\n\ndef generate_3dgrid(d, h, w, centered = True):\n    if type(d) is not list:\n        if centered:\n            z = torch.linspace(-d/2+1, d/2, d)\n        else:\n            z = torch.linspace(0, d-1, d)\n        dl = d\n    else:\n        z = torch.FloatTensor(d)\n        dl = len(d)\n    grid2d = generate_2dgrid(h,w, centered = centered)\n    grid3d = torch.cat([z.repeat(w*h,1).t().contiguous().view(-1,1), grid2d.repeat(dl,1)],dim = 1)\n    return grid3d\n\ndef zero_response_at_border(x, b):\n    if (b < x.size(3)) and (b < x.size(2)):\n        x[:, :,  0:b, :] =  0\n        x[:, :,  x.size(2) - b: , :] =  0\n        x[:, :, :,  0:b] =  0\n        x[:, :, :,   x.size(3) - b: ] =  0\n    else:\n        return x * 0\n    return x\n\nclass GaussianBlur(nn.Module):\n    def __init__(self, sigma=1.6):\n        super(GaussianBlur, self).__init__()\n        weight = self.calculate_weights(sigma)\n        self.register_buffer(\'buf\', weight)\n        return\n    def calculate_weights(self,  sigma):\n        kernel = CircularGaussKernel(sigma = sigma, circ_zeros = False)\n        h,w = kernel.shape\n        halfSize = float(h) / 2.;\n        self.pad = int(np.floor(halfSize))\n        return torch.from_numpy(kernel.astype(np.float32)).view(1,1,h,w);\n    def forward(self, x):\n        w = Variable(self.buf)\n        if x.is_cuda:\n            w = w.cuda()\n        return F.conv2d(F.pad(x, (self.pad,self.pad,self.pad,self.pad), \'replicate\'), w, padding = 0)\n\ndef batch_eig2x2(A):\n    trace = A[:,0,0] + A[:,1,1]\n    delta1 = (trace*trace - 4 * ( A[:,0,0]*  A[:,1,1] -  A[:,1,0]* A[:,0,1]))\n    mask = delta1 > 0\n    delta = torch.sqrt(torch.abs(delta1))\n    l1 = mask.float() * (trace + delta) / 2.0 +  1000.  * (1.0 - mask.float())\n    l2 = mask.float() * (trace - delta) / 2.0 +  0.0001  * (1.0 - mask.float())\n    return l1,l2\n\ndef line_prepender(filename, line):\n    with open(filename, \'r+\') as f:\n        content = f.read()\n        f.seek(0, 0)\n        f.write(line.rstrip(\'\\r\\n\') + \'\\n\' + content)\n    return\n'"
examples/hesaffnet/architectures.py,81,"b""from __future__ import division, print_function\nimport os\nimport errno\nimport numpy as np\nimport sys\nfrom copy import deepcopy\nimport math\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom Utils import L2Norm, generate_2dgrid\nfrom Utils import str2bool\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A, extract_patches,normalizeLAFs,  get_rotation_matrix\nfrom LAF import get_LAFs_scales, get_normalized_affine_shape\nfrom LAF import rectifyAffineTransformationUpIsUp,rectifyAffineTransformationUpIsUpFullyConv\n\nclass LocalNorm2d(nn.Module):\n    def __init__(self, kernel_size = 33):\n        super(LocalNorm2d, self).__init__()\n        self.ks = kernel_size\n        self.pool = nn.AvgPool2d(kernel_size = self.ks, stride = 1,  padding = 0)\n        self.eps = 1e-10\n        return\n    def forward(self,x):\n        pd = int(self.ks/2)\n        mean = self.pool(F.pad(x, (pd,pd,pd,pd), 'reflect'))\n        return torch.clamp((x - mean) / (torch.sqrt(torch.abs(self.pool(F.pad(x*x,  (pd,pd,pd,pd), 'reflect')) - mean*mean )) + self.eps), min = -6.0, max = 6.0)\n\nclass OriNetFast(nn.Module):\n    def __init__(self, PS = 16):\n        super(OriNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 2, kernel_size=int(PS/4), stride=1,padding=1, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/4)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.9)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_rot_matrix = True):\n        xy = self.features(self.input_norm(input)).view(-1,2) \n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(angle)\n        return angle\n\nclass GHH(nn.Module):\n    def __init__(self, n_in, n_out, s = 4, m = 4):\n        super(GHH, self).__init__()\n        self.n_out = n_out\n        self.s = s\n        self.m = m\n        self.conv = nn.Linear(n_in, n_out * s * m)\n        d = torch.arange(0, s)\n        self.deltas = -1.0 * (d % 2 != 0).float()  + 1.0 * (d % 2 == 0).float()\n        self.deltas = Variable(self.deltas)\n        return\n    def forward(self,x):\n        x_feats = self.conv(x.view(x.size(0),-1)).view(x.size(0), self.n_out, self.s, self.m);\n        max_feats = x_feats.max(dim = 3)[0];\n        if x.is_cuda:\n            self.deltas = self.deltas.cuda()\n        else:\n            self.deltas = self.deltas.cpu()\n        out =  (max_feats * self.deltas.view(1,1,-1).expand_as(max_feats)).sum(dim = 2)\n        return out\n\nclass YiNet(nn.Module):\n    def __init__(self, PS = 28):\n        super(YiNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 10, kernel_size=5, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding = 1),\n            nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=2, padding = 2),\n            nn.Conv2d(20, 50, kernel_size=3, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.AdaptiveMaxPool2d(1),\n            GHH(50, 100),\n            GHH(100, 2)\n        )\n        self.input_mean = 0.427117081207483\n        self.input_std = 0.21888339179665006;\n        self.PS = PS\n        return\n    def import_weights(self, dir_name):\n        self.features[0].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_W.npy'))).float()\n        self.features[0].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_b.npy'))).float().view(-1)\n        self.features[3].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_W.npy'))).float()\n        self.features[3].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_b.npy'))).float().view(-1)\n        self.features[6].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_W.npy'))).float()\n        self.features[6].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_b.npy'))).float().view(-1)\n        self.features[9].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_W.npy'))).float().view(50, 1600).contiguous().t().contiguous()\n        self.features[9].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_b.npy'))).float().view(1600)\n        self.features[10].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_W.npy'))).float().view(100, 32).contiguous().t().contiguous()\n        self.features[10].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_b.npy'))).float().view(32)\n        self.input_mean = float(np.load(os.path.join(dir_name, 'input_mean.npy')))\n        self.input_std = float(np.load(os.path.join(dir_name, 'input_std.npy')))\n        return\n    def input_norm1(self,x):\n        return (x - self.input_mean) / self.input_std\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def forward(self, input, return_rot_matrix = False):\n        xy = self.features(self.input_norm(input))\n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(-angle)\n        return angle\nclass AffNetFast4(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0,0,1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n        return rectifyAffineTransformationUpIsUp(xy).contiguous()\n\n    \nclass AffNetFast(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,3)\n        a1 = torch.cat([1.0 + xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), 1.0 + xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        return rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n\nclass AffNetFast52RotUp(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52RotUp, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0, 1, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, rectifyAffineTransformationUpIsUp(torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1)).contiguous())\n\nclass AffNetFast52Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Tanh()\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0, 0.8, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFast5Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast5Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0, 1, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        rot = get_rotation_matrix(torch.atan2(x[:,3], x[:,4]+1e-8))\n        if input.is_cuda:\n            return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), torch.zeros(x.size(0),1,1).cuda()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n        else:\n            return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), torch.zeros(x.size(0),1,1)], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFast4Rot(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4Rot, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Tanh()\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0,0,0.8])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        return self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n\nclass AffNetFast4RotNosc(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast4RotNosc, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([1,0,0,1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        A = self.features(self.input_norm(input)).view(-1,2,2).contiguous()\n        scale =  torch.sqrt(torch.abs(A[:,0,0]*A[:,1,1] - A[:,1,0]*A[:,0,1] + 1e-10))\n        return A / (scale.view(-1,1,1).repeat(1,2,2) + 1e-8)\n\nclass AffNetFastScale(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFastScale, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 4, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,4)\n        a1 = torch.cat([1.0 + xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), 1.0 + xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        scale = torch.exp(xy[:,3].contiguous().view(-1,1,1).repeat(1,2,2))\n        return scale * rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n\nclass AffNetFast2Par(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast2Par, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0, 0, 1 ])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,3)\n        angle = torch.atan2(x[:,1], x[:,2]+1e-8);\n        rot = get_rotation_matrix(angle)\n        tilt = torch.exp(1.8 * F.tanh(x[:,0]))\n        tilt_matrix = torch.eye(2).unsqueeze(0).repeat(input.size(0),1,1)\n        if x.is_cuda:\n            tilt_matrix = tilt_matrix.cuda()\n        tilt_matrix[:,0,0] = torch.sqrt(tilt)\n        tilt_matrix[:,1,1] = 1.0 / torch.sqrt(tilt)\n        return rectifyAffineTransformationUpIsUp(torch.bmm(rot, tilt_matrix)).contiguous()\n\nclass AffNetFastFullConv(nn.Module):\n    def __init__(self, PS = 32, stride = 2):\n        super(AffNetFastFullConv, self).__init__()\n        self.lrn = LocalNorm2d(33)\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=stride, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=stride, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding = 0, bias = True),\n        )\n        self.stride = stride\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        norm_inp  = self.lrn(input)\n        ff = self.features(F.pad(norm_inp, (14,14,14,14), 'reflect'))\n        xy = F.tanh(F.upsample(ff, (input.size(2), input.size(3)),mode='bilinear'))\n        a0bc = torch.cat([1.0 + xy[:,0:1,:,:].contiguous(), 0*xy[:,1:2,:,:].contiguous(),\n                          xy[:,1:2,:,:].contiguous(),  1.0 + xy[:,2:,:,:].contiguous()], dim = 1).contiguous()\n        return rectifyAffineTransformationUpIsUpFullyConv(a0bc).contiguous()\n    \nclass AffNetFast52RotL(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast52RotL, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 5, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.AdaptiveAvgPool2d(1),\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8,0, 0.8, 0, 1])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        x  = self.features(self.input_norm(input)).view(-1,5)\n        angle = torch.atan2(x[:,3], x[:,4]+1e-8);\n        rot = get_rotation_matrix(angle)\n        return torch.bmm(rot, torch.cat([torch.cat([x[:,0:1].view(-1,1,1), x[:,1:2].view(x.size(0),1,1).contiguous()], dim = 2), x[:,1:3].view(-1,1,2).contiguous()], dim = 1))\n\nclass AffNetFastBias(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFastBias, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1).detach()\n        sp = torch.std(flat, dim=1).detach() + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                if m.weight.data.shape[-1] == 8: #last layer:\n                    nn.init.orthogonal(m.weight.data, gain=1.0)\n                    print ('last layer init bias')\n                    m.bias.data = torch.FloatTensor([0.8, 0, 0.8 ])\n                else:\n                    nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,3)\n        a1 = torch.cat([xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        return rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n"""
examples/hesaffnet/extract_geomOriTh.py,12,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport os\nimport time\n\nfrom PIL import Image\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport math\nimport torch.nn.functional as F\n\nfrom copy import deepcopy\nfrom HardNet import HardNet\nfrom OnePassSIR import OnePassSIR\nfrom LAF import denormalizeLAFs, LAFs2ell,LAFs2ellT, abc2A\nfrom Utils import line_prepender\nfrom architectures import AffNetFastFullAff, OriNetFast\nfrom time import time\nUSE_CUDA = True\n\ntry:\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\n    th = 28.41#float(sys.argv[3])\nexcept:\n    print ""Wrong input format. Try python hesaffnet.py imgs/cat.png cat.txt 5.3333""\n    sys.exit(1)\n\ndef get_geometry_and_descriptors(img, det,desc):\n    with torch.no_grad():\n        tt = time()\n        LAFs, resp = det(img)\n        print(\'det time = \', time() - tt)\n        tt = time()\n        patches = det.extract_patches_from_pyr(LAFs, PS = 32)\n        print(\'extract time = \', time() - tt)\n        tt = time()\n        descriptors = desc(patches)\n        print(\'desc time = \', time() - tt)\n    return LAFs, descriptors\ndef load_grayscale_var(fname):\n    img = Image.open(fname).convert(\'RGB\')\n    img = np.mean(np.array(img), axis = 2)\n    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n    if USE_CUDA:\n        var_image_reshape = var_image_reshape.cuda()\n    return var_image_reshape\n\n\nimg = load_grayscale_var(input_img_fname)\nAffNetPix = AffNetFastFullAff(PS = 32)\nweightd_fname = \'/home/old-ufo/dev/affnet-priv/pretrained/AffNet.pth\'\ncheckpoint = torch.load(weightd_fname)\nAffNetPix.load_state_dict(checkpoint[\'state_dict\'])\nAffNetPix.eval()\nONet = OriNetFast(PS =32)\no_fname = \'/home/old-ufo/dev/affnet-priv/examples/hesaffnet/OriNet.pth\'\ncheckpoint = torch.load(o_fname)\nONet.load_state_dict(checkpoint[\'state_dict\'])\nONet.eval()\ndescriptor = HardNet()\nmodel_weights = \'../../HardNet++.pth\'\nhncheckpoint = torch.load(model_weights)\ndescriptor.load_state_dict(hncheckpoint[\'state_dict\'])\ndescriptor.eval()\n\nHA = OnePassSIR( mrSize = 5.192, num_features = -1, th = th, border = 15, num_Baum_iters = 1, AffNet = AffNetPix, OriNet = ONet)\nimport scipy.io as sio\nif USE_CUDA:\n    HA = HA.cuda()\n    descriptor = descriptor.cuda()\nwith torch.no_grad():\n    t = time()\n    LAFs,descs = get_geometry_and_descriptors(img, HA,descriptor)\n    #lt = time()\n    #ells = LAFs2ellT(LAFs.cpu()).cpu().numpy()\n    #print (\'LAFs2ell time\', time() - lt)\n#print (\'Total time\', time() - t)\n#sio.savemat(\'descs.mat\', {\'descs\': descs.cpu().numpy()})\n#sio.savemat(\'geoms.mat\', {\'LAFs\': LAFs.cpu().numpy()})\nnp.save(\'lafs1.npy\',LAFs.cpu().numpy())\n#np.savetxt(output_fname, ells, delimiter=\' \', fmt=\'%10.10f\')\n#line_prepender(output_fname, str(len(ells)))\n#line_prepender(output_fname, \'1.0\')\n#torch.save(descs, \'dd.t7\')\n'"
examples/hesaffnet/extract_geom_and_desc_upisup.py,10,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport os\nimport time\n\nfrom PIL import Image\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport math\nimport torch.nn.functional as F\n\nfrom copy import deepcopy\nfrom HardNet import HardNet\nfrom OnePassSIR import OnePassSIR\nfrom LAF import denormalizeLAFs, LAFs2ell,LAFs2ellT, abc2A\nfrom Utils import line_prepender\nfrom architectures import AffNetFastFullAff\nfrom time import time\nUSE_CUDA = True\n\ntry:\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\n    nfeats = int(sys.argv[3])\nexcept:\n    print ""Wrong input format. Try python hesaffnet.py imgs/cat.png cat.txt 2000""\n    sys.exit(1)\n\ndef get_geometry_and_descriptors(img, det, desc):\n    with torch.no_grad():\n        tt = time()\n        LAFs, resp = det(img)\n        print(\'det time = \', time() - tt)\n        tt = time()\n        patches = det.extract_patches_from_pyr(LAFs, PS = 32)\n        print(\'extract time = \', time() - tt)\n        tt = time()\n        descriptors = desc(patches)\n        print(\'desc time = \', time() - tt)\n    return LAFs, descriptors\ndef load_grayscale_var(fname):\n    img = Image.open(fname).convert(\'RGB\')\n    img = np.mean(np.array(img), axis = 2)\n    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n    if USE_CUDA:\n        var_image_reshape = var_image_reshape.cuda()\n    return var_image_reshape\n\n\nimg = load_grayscale_var(input_img_fname)\nAffNetPix = AffNetFastFullAff(PS = 32)\nweightd_fname = \'../../pretrained/AffNet.pth\'\ncheckpoint = torch.load(weightd_fname)\nAffNetPix.load_state_dict(checkpoint[\'state_dict\'])\nAffNetPix.eval()\nHA = OnePassSIR( mrSize = 5.192, num_features = nfeats, border = 15, num_Baum_iters = 1, AffNet = AffNetPix)\ndescriptor = HardNet()\nmodel_weights = \'../../HardNet++.pth\'\nhncheckpoint = torch.load(model_weights)\ndescriptor.load_state_dict(hncheckpoint[\'state_dict\'])\ndescriptor.eval()\nif USE_CUDA:\n    HA = HA.cuda()\n    descriptor = descriptor.cuda()\nwith torch.no_grad():\n    t = time()\n    LAFs, descriptors = get_geometry_and_descriptors(img, HA, descriptor)\n    lt = time()\n    ells = LAFs2ellT(LAFs.cpu()).cpu().numpy()\n    print (\'LAFs2ell time\', time() - lt)\nprint (\'Total time\', time() - t)\nnp.savetxt(output_fname, ells, delimiter=\' \', fmt=\'%10.10f\')\nline_prepender(output_fname, str(len(ells)))\nline_prepender(output_fname, \'1.0\')\n'"
examples/hesaffnet/extract_geom_and_desc_upisupTh.py,10,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport os\nimport time\n\nfrom PIL import Image\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport math\nimport torch.nn.functional as F\n\nfrom copy import deepcopy\nfrom HardNet import HardNet\nfrom OnePassSIR import OnePassSIR\nfrom LAF import denormalizeLAFs, LAFs2ell,LAFs2ellT, abc2A\nfrom Utils import line_prepender\nfrom architectures import AffNetFastFullAff\nfrom time import time\nUSE_CUDA = True\n\ntry:\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\n    th = float(sys.argv[3])\nexcept:\n    print ""Wrong input format. Try python hesaffnet.py imgs/cat.png cat.txt 5.3333""\n    sys.exit(1)\n\ndef get_geometry_and_descriptors(img, det, desc):\n    with torch.no_grad():\n        tt = time()\n        LAFs, resp = det(img)\n        print(\'det time = \', time() - tt)\n        tt = time()\n        patches = det.extract_patches_from_pyr(LAFs, PS = 32)\n        print(\'extract time = \', time() - tt)\n        tt = time()\n        descriptors = desc(patches)\n        print(\'desc time = \', time() - tt)\n    return LAFs, descriptors\ndef load_grayscale_var(fname):\n    img = Image.open(fname).convert(\'RGB\')\n    img = np.mean(np.array(img), axis = 2)\n    var_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\n    var_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n    if USE_CUDA:\n        var_image_reshape = var_image_reshape.cuda()\n    return var_image_reshape\n\n\nimg = load_grayscale_var(input_img_fname)\nAffNetPix = AffNetFastFullAff(PS = 32)\nweightd_fname = \'../../pretrained/AffNet.pth\'\ncheckpoint = torch.load(weightd_fname)\nAffNetPix.load_state_dict(checkpoint[\'state_dict\'])\nAffNetPix.eval()\nHA = OnePassSIR( mrSize = 5.192, num_features = -1, th = th, border = 15, num_Baum_iters = 1, AffNet = AffNetPix)\ndescriptor = HardNet()\nmodel_weights = \'../../HardNet++.pth\'\nhncheckpoint = torch.load(model_weights)\ndescriptor.load_state_dict(hncheckpoint[\'state_dict\'])\ndescriptor.eval()\nif USE_CUDA:\n    HA = HA.cuda()\n    descriptor = descriptor.cuda()\nwith torch.no_grad():\n    t = time()\n    LAFs, descriptors = get_geometry_and_descriptors(img, HA, descriptor)\n    lt = time()\n    ells = LAFs2ellT(LAFs.cpu()).cpu().numpy()\n    print (\'LAFs2ell time\', time() - lt)\nprint (\'Total time\', time() - t)\nnp.savetxt(output_fname, ells, delimiter=\' \', fmt=\'%10.10f\')\nline_prepender(output_fname, str(len(ells)))\nline_prepender(output_fname, \'1.0\')\n'"
examples/hesaffnet/hesaffBaum.py,6,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport os\nimport time\n\nfrom PIL import Image\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport math\nimport torch.nn.functional as F\n\nfrom copy import deepcopy\n\nfrom SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\nfrom LAF import denormalizeLAFs, LAFs2ellT, abc2A\nfrom Utils import line_prepender\nfrom architectures import AffNetFast\nfrom HandCraftedModules import AffineShapeEstimator\nUSE_CUDA = False\ntry:\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\n    nfeats = int(sys.argv[3])\nexcept:\n    print ""Wrong input format. Try python hesaffBaum.py imgs/cat.png cat.txt 2000""\n    sys.exit(1)\n\nimg = Image.open(input_img_fname).convert(\'RGB\')\nimg = np.mean(np.array(img), axis = 2)\n\nvar_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\nvar_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n\nHA = ScaleSpaceAffinePatchExtractor( mrSize = 5.192, num_features = nfeats, border = 5, num_Baum_iters = 16, AffNet = AffineShapeEstimator(patch_size=19))\nif USE_CUDA:\n    HA = HA.cuda()\n    var_image_reshape = var_image_reshape.cuda()\n\nLAFs, resp = HA(var_image_reshape)\nells  = LAFs2ellT(LAFs.cpu()).cpu().numpy()\n\nnp.savetxt(output_fname, ells, delimiter=\' \', fmt=\'%10.10f\')\nline_prepender(output_fname, str(len(ells)))\nline_prepender(output_fname, \'1.0\')\n'"
examples/hesaffnet/hesaffnet.py,8,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nimport os\nimport time\n\nfrom PIL import Image\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport math\nimport torch.nn.functional as F\n\nfrom copy import deepcopy\n\nfrom SparseImgRepresenter import ScaleSpaceAffinePatchExtractor\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A\nfrom Utils import line_prepender\nfrom architectures import AffNetFast\nUSE_CUDA = False\nth = 28.41 # default threshold for HessianAffine \nth = -1\ntry:\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\n    nfeats = int(sys.argv[3])\nexcept:\n    print (""Wrong input format. Try python hesaffnet.py imgs/cat.png cat.txt 2000"")\n    sys.exit(1)\n\nimg = Image.open(input_img_fname).convert(\'RGB\')\nimg = np.mean(np.array(img), axis = 2)\n\nvar_image = torch.autograd.Variable(torch.from_numpy(img.astype(np.float32)), volatile = True)\nvar_image_reshape = var_image.view(1, 1, var_image.size(0),var_image.size(1))\n\n\nAffNetPix = AffNetFast(PS = 32)\nweightd_fname = \'../../pretrained/AffNet.pth\'\n\ncheckpoint = torch.load(weightd_fname)\nAffNetPix.load_state_dict(checkpoint[\'state_dict\'])\n\nAffNetPix.eval()\n    \nHA = ScaleSpaceAffinePatchExtractor( mrSize = 5.192, num_features = nfeats, border = 5, num_Baum_iters = 1, th = th,  AffNet = AffNetPix)\nif USE_CUDA:\n    HA = HA.cuda()\n    var_image_reshape = var_image_reshape.cuda()\nwith torch.no_grad():\n    LAFs, resp = HA(var_image_reshape)\nells = LAFs2ell(LAFs.data.cpu().numpy())\n\nnp.savetxt(output_fname, ells, delimiter=\' \', fmt=\'%10.10f\')\nline_prepender(output_fname, str(len(ells)))\nline_prepender(output_fname, \'1.0\')\n'"
examples/just_shape/LAF.py,38,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\nfrom scipy.spatial.distance import cdist\nfrom numpy.linalg import inv    \nfrom scipy.linalg import schur, sqrtm\nimport torch\nfrom  torch.autograd import Variable\n\n##########numpy\ndef invSqrt(a,b,c):\n    eps = 1e-12 \n    mask = (b !=  0)\n    r1 = mask * (c - a) / (2. * b + eps)\n    t1 = np.sign(r1) / (np.abs(r1) + np.sqrt(1. + r1*r1));\n    r = 1.0 / np.sqrt( 1. + t1*t1)\n    t = t1*r;\n    \n    r = r * mask + 1.0 * (1.0 - mask);\n    t = t * mask;\n    \n    x = 1. / np.sqrt( r*r*a - 2*r*t*b + t*t*c)\n    z = 1. / np.sqrt( t*t*a + 2*r*t*b + r*r*c)\n    \n    d = np.sqrt( x * z)\n    \n    x = x / d\n    z = z / d\n       \n    new_a = r*r*x + t*t*z\n    new_b = -r*t*x + t*r*z\n    new_c = t*t*x + r*r *z\n\n    return new_a, new_b, new_c\n\ndef Ell2LAF(ell):\n    A23 = np.zeros((2,3))\n    A23[0,2] = ell[0]\n    A23[1,2] = ell[1]\n    a = ell[2]\n    b = ell[3]\n    c = ell[4]\n    sc = np.sqrt(np.sqrt(a*c - b*b))\n    ia,ib,ic = invSqrt(a,b,c)  #because sqrtm returns ::-1, ::-1 matrix, don`t know why \n    A = np.array([[ia, ib], [ib, ic]]) / sc\n    sc = np.sqrt(A[0,0] * A[1,1] - A[1,0] * A[0,1])\n    A23[0:2,0:2] = rectifyAffineTransformationUpIsUp(A / sc) * sc\n    return A23\n\ndef rectifyAffineTransformationUpIsUp_np(A):\n    det = np.sqrt(np.abs(A[0,0]*A[1,1] - A[1,0]*A[0,1] + 1e-10))\n    b2a2 = np.sqrt(A[0,1] * A[0,1] + A[0,0] * A[0,0])\n    A_new = np.zeros((2,2))\n    A_new[0,0] = b2a2 / det\n    A_new[0,1] = 0\n    A_new[1,0] = (A[1,1]*A[0,1]+A[1,0]*A[0,0])/(b2a2*det)\n    A_new[1,1] = det / b2a2\n    return A_new\n\ndef ells2LAFs(ells):\n    LAFs = np.zeros((len(ells), 2,3))\n    for i in range(len(ells)):\n        LAFs[i,:,:] = Ell2LAF(ells[i,:])\n    return LAFs\n\ndef LAF2pts(LAF, n_pts = 50):\n    a = np.linspace(0, 2*np.pi, n_pts);\n    x = [0]\n    x.extend(list(np.sin(a)))\n    x = np.array(x).reshape(1,-1)\n    y = [0]\n    y.extend(list(np.cos(a)))\n    y = np.array(y).reshape(1,-1)\n    HLAF = np.concatenate([LAF, np.array([0,0,1]).reshape(1,3)])\n    H_pts =np.concatenate([x,y,np.ones(x.shape)])\n    H_pts_out = np.transpose(np.matmul(HLAF, H_pts))\n    H_pts_out[:,0] = H_pts_out[:,0] / H_pts_out[:, 2]\n    H_pts_out[:,1] = H_pts_out[:,1] / H_pts_out[:, 2]\n    return H_pts_out[:,0:2]\n\n\ndef convertLAFs_to_A23format(LAFs):\n    sh = LAFs.shape\n    if (len(sh) == 3) and (sh[1]  == 2) and (sh[2] == 3): # n x 2 x 3 classical [A, (x;y)] matrix\n        work_LAFs = deepcopy(LAFs)\n    elif (len(sh) == 2) and (sh[1]  == 7): #flat format, x y scale a11 a12 a21 a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] * LAFs[:,3] \n        work_LAFs[:,0,1] = LAFs[:,2] * LAFs[:,4]\n        work_LAFs[:,1,0] = LAFs[:,2] * LAFs[:,5]\n        work_LAFs[:,1,1] = LAFs[:,2] * LAFs[:,6]\n    elif (len(sh) == 2) and (sh[1]  == 6): #flat format, x y s*a11 s*a12 s*a21 s*a22\n        work_LAFs = np.zeros((sh[0], 2,3))\n        work_LAFs[:,0,2] = LAFs[:,0]\n        work_LAFs[:,1,2] = LAFs[:,1]\n        work_LAFs[:,0,0] = LAFs[:,2] \n        work_LAFs[:,0,1] = LAFs[:,3]\n        work_LAFs[:,1,0] = LAFs[:,4]\n        work_LAFs[:,1,1] = LAFs[:,5]\n    else:\n        print 'Unknown LAF format'\n        return None\n    return work_LAFs\n\ndef LAFs2ell(in_LAFs):\n    LAFs = convertLAFs_to_A23format(in_LAFs)\n    ellipses = np.zeros((len(LAFs),5))\n    for i in range(len(LAFs)):\n        LAF = deepcopy(LAFs[i,:,:])\n        scale = np.sqrt(LAF[0,0]*LAF[1,1]  - LAF[0,1]*LAF[1, 0] + 1e-10)\n        u, W, v = np.linalg.svd(LAF[0:2,0:2] / scale, full_matrices=True)\n        W[0] = 1. / (W[0]*W[0]*scale*scale)\n        W[1] = 1. / (W[1]*W[1]*scale*scale)\n        A =  np.matmul(np.matmul(u, np.diag(W)), u.transpose())\n        ellipses[i,0] = LAF[0,2]\n        ellipses[i,1] = LAF[1,2]\n        ellipses[i,2] = A[0,0]\n        ellipses[i,3] = A[0,1]\n        ellipses[i,4] = A[1,1]\n    return ellipses\n\ndef visualize_LAFs(img, LAFs):\n    work_LAFs = convertLAFs_to_A23format(LAFs)\n    plt.figure()\n    plt.imshow(255 - img)\n    for i in range(len(work_LAFs)):\n        ell = LAF2pts(work_LAFs[i,:,:])\n        plt.plot( ell[:,0], ell[:,1], 'r')\n    plt.show()\n    return \n\n####pytorch\n\ndef get_normalized_affine_shape(tilt, angle_in_radians):\n    assert tilt.size(0) == angle_in_radians.size(0)\n    num = tilt.size(0)\n    tilt_A = Variable(torch.eye(2).view(1,2,2).repeat(num,1,1))\n    if tilt.is_cuda:\n        tilt_A = tilt_A.cuda()\n    tilt_A[:,0,0] = tilt;\n    rotmat = get_rotation_matrix(angle_in_radians)\n    out_A = rectifyAffineTransformationUpIsUp(torch.bmm(rotmat, torch.bmm(tilt_A, rotmat)))\n    #re_scale = (1.0/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    #re_scale = (0.5 + 0.5/torch.sqrt((out_A **2).sum(dim=1).max(dim=1)[0])) #It is heuristic to for keeping scale change small\n    return out_A# * re_scale.view(-1,1,1).expand(num,2,2)\n\ndef get_rotation_matrix(angle_in_radians):\n    angle_in_radians = angle_in_radians.view(-1, 1, 1);\n    sin_a = torch.sin(angle_in_radians)\n    cos_a = torch.cos(angle_in_radians)\n    A1_x = torch.cat([cos_a, sin_a], dim = 2)\n    A2_x = torch.cat([-sin_a, cos_a], dim = 2)\n    transform = torch.cat([A1_x,A2_x], dim = 1)\n    return transform\n    \ndef rectifyAffineTransformationUpIsUp(A):\n    det = torch.sqrt(torch.abs(A[:,0,0]*A[:,1,1] - A[:,1,0]*A[:,0,1] + 1e-10))\n    b2a2 = torch.sqrt(A[:,0,1] * A[:,0,1] + A[:,0,0] * A[:,0,0])\n    A1_ell = torch.cat([(b2a2 / det).contiguous().view(-1,1,1), 0 * det.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([((A[:,1,1]*A[:,0,1]+A[:,1,0]*A[:,0,0])/(b2a2*det)).contiguous().view(-1,1,1),\n                        (det / b2a2).contiguous().view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\n\n\ndef abc2A(a,b,c, normalize = False):\n    A1_ell = torch.cat([a.view(-1,1,1), b.view(-1,1,1)], dim = 2)\n    A2_ell = torch.cat([b.view(-1,1,1), c.view(-1,1,1)], dim = 2)\n    return torch.cat([A1_ell, A2_ell], dim = 1)\n\n\n\ndef angles2A(angles):\n    cos_a = torch.cos(angles).view(-1, 1, 1)\n    sin_a = torch.sin(angles).view(-1, 1, 1)\n    A1_ang = torch.cat([cos_a, sin_a], dim = 2)\n    A2_ang = torch.cat([-sin_a, cos_a], dim = 2)\n    return  torch.cat([A1_ang, A2_ang], dim = 1)\n\ndef generate_patch_grid_from_normalized_LAFs(LAFs, w, h, PS):\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3) * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    grid = torch.nn.functional.affine_grid(LAFs * Variable(coef.expand(num_lafs,2,3)), torch.Size((num_lafs,1,PS,PS)))\n    grid[:,:,:,0] = 2.0 * grid[:,:,:,0] / float(w)  - 1.0\n    grid[:,:,:,1] = 2.0 * grid[:,:,:,1] / float(h)  - 1.0     \n    return grid\n    \ndef extract_patches(img, LAFs, PS = 32):\n    w = img.size(3)\n    h = img.size(2)\n    ch = img.size(1)\n    grid = generate_patch_grid_from_normalized_LAFs(LAFs, float(w),float(h), PS)\n    return torch.nn.functional.grid_sample(img.expand(grid.size(0), ch, h, w),  grid)  \n\ndef get_pyramid_inverted_index_for_LAFs(LAFs, PS, sigmas):\n    return\n\ndef extract_patches_from_pyramid_with_inv_index(scale_pyramid, pyr_inv_idxs, LAFs, PS = 19):\n    patches = torch.zeros(LAFs.size(0),scale_pyramid[0][0].size(1), PS, PS)\n    if LAFs.is_cuda:\n        patches = patches.cuda()\n    patches = Variable(patches)\n    if pyr_inv_idxs is not None:\n        for i in range(len(scale_pyramid)):\n            for j in range(len(scale_pyramid[i])):\n                cur_lvl_idxs = pyr_inv_idxs[i][j]\n                if cur_lvl_idxs is None:\n                    continue\n                cur_lvl_idxs = cur_lvl_idxs.view(-1)\n                #print i,j,cur_lvl_idxs.shape\n                patches[cur_lvl_idxs,:,:,:] = extract_patches(scale_pyramid[i][j], LAFs[cur_lvl_idxs, :,:], PS )\n    return patches\n\ndef get_inverted_pyr_index(scale_pyr, pyr_idxs, level_idxs):\n    pyr_inv_idxs = []\n    ### Precompute octave inverted indexes\n    for i in range(len(scale_pyr)):\n        pyr_inv_idxs.append([])\n        cur_idxs = pyr_idxs == i #torch.nonzero((pyr_idxs == i).data)\n        for j in range(0, len(scale_pyr[i])):\n            cur_lvl_idxs = torch.nonzero(((level_idxs == j) * cur_idxs).data)\n            if len(cur_lvl_idxs.size()) == 0:\n                pyr_inv_idxs[i].append(None)\n            else:\n                pyr_inv_idxs[i].append(cur_lvl_idxs.squeeze())\n    return pyr_inv_idxs\n\n\ndef denormalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  * min_size\n    coef[0,0,2] = w\n    coef[0,1,2] = h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef normalizeLAFs(LAFs, w, h):\n    w = float(w)\n    h = float(h)\n    num_lafs = LAFs.size(0)\n    min_size = min(h,w)\n    coef = torch.ones(1,2,3).float()  / min_size\n    coef[0,0,2] = 1.0 / w\n    coef[0,1,2] = 1.0 / h\n    if LAFs.is_cuda:\n        coef = coef.cuda()\n    return Variable(coef.expand(num_lafs,2,3)) * LAFs\n\ndef sc_y_x2LAFs(sc_y_x):\n    base_LAF = torch.eye(2).float().unsqueeze(0).expand(sc_y_x.size(0),2,2)\n    if sc_y_x.is_cuda:\n        base_LAF = base_LAF.cuda()\n    base_A = Variable(base_LAF, requires_grad=False)\n    A = sc_y_x[:,:1].unsqueeze(1).expand_as(base_A) * base_A\n    LAFs  = torch.cat([A,\n                       torch.cat([sc_y_x[:,2:].unsqueeze(-1),\n                                    sc_y_x[:,1:2].unsqueeze(-1)], dim=1)], dim = 2)\n        \n    return LAFs\ndef get_LAFs_scales(LAFs):\n    return torch.sqrt(torch.abs(LAFs[:,0,0] *LAFs[:,1,1] - LAFs[:,0,1] * LAFs[:,1,0]) + 1e-12)\n\ndef get_pyramid_and_level_index_for_LAFs(dLAFs,  sigmas, pix_dists, PS):\n    scales = get_LAFs_scales(dLAFs);\n    needed_sigmas = scales / PS;\n    sigmas_full_list = []\n    level_idxs_full = []\n    oct_idxs_full = []\n    for oct_idx in range(len(sigmas)):\n        sigmas_full_list = sigmas_full_list + list(np.array(sigmas[oct_idx])*np.array(pix_dists[oct_idx]))\n        oct_idxs_full = oct_idxs_full + [oct_idx]*len(sigmas[oct_idx])\n        level_idxs_full = level_idxs_full + range(0,len(sigmas[oct_idx]))\n    oct_idxs_full = torch.LongTensor(oct_idxs_full)\n    level_idxs_full = torch.LongTensor(level_idxs_full)\n    \n    closest_imgs = cdist(np.array(sigmas_full_list).reshape(-1,1), needed_sigmas.data.cpu().numpy().reshape(-1,1)).argmin(axis = 0)\n    closest_imgs = torch.from_numpy(closest_imgs)\n    if dLAFs.is_cuda:\n        closest_imgs = closest_imgs.cuda()\n        oct_idxs_full = oct_idxs_full.cuda()\n        level_idxs_full = level_idxs_full.cuda()\n    return  Variable(oct_idxs_full[closest_imgs]), Variable(level_idxs_full[closest_imgs])\n"""
examples/just_shape/Utils.py,25,"b'import torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport cv2\nimport numpy as np\n\n# resize image to size 32x32\ncv2_scale = lambda x: cv2.resize(x, dsize=(32, 32),\n                                 interpolation=cv2.INTER_LINEAR)\n# reshape image\nnp_reshape32 = lambda x: np.reshape(x, (32, 32, 1))\nnp_reshape64 = lambda x: np.reshape(x, (64, 64, 1))\n\ndef zeros_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.zeros(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.zeros(y)\n\ndef ones_like(x):\n    assert x.__class__.__name__.find(\'Variable\') != -1 or x.__class__.__name__.find(\'Tensor\') != -1, ""Object is neither a Tensor nor a Variable""\n    y = torch.ones(x.size())\n    if x.is_cuda:\n       y = y.cuda()\n    if x.__class__.__name__ == \'Variable\':\n        return torch.autograd.Variable(y, requires_grad=x.requires_grad)\n    elif x.__class__.__name__.find(\'Tensor\') != -1:\n        return torch.ones(y)\n    \n\ndef batched_forward(model, data, batch_size, **kwargs):\n    n_patches = len(data)\n    if n_patches > batch_size:\n        bs = batch_size\n        n_batches = n_patches / bs + 1\n        for batch_idx in range(n_batches):\n            st = batch_idx * bs\n            if batch_idx == n_batches - 1:\n                if (batch_idx + 1) * bs > n_patches:\n                    end = n_patches\n                else:\n                    end = (batch_idx + 1) * bs\n            else:\n                end = (batch_idx + 1) * bs\n            if st >= end:\n                continue\n            if batch_idx == 0:\n                first_batch_out = model(data[st:end], kwargs)\n                out_size = torch.Size([n_patches] + list(first_batch_out.size()[1:]))\n                #out_size[0] = n_patches\n                out = torch.zeros(out_size);\n                if data.is_cuda:\n                    out = out.cuda()\n                out = Variable(out)\n                out[st:end] = first_batch_out\n            else:\n                out[st:end,:,:] = model(data[st:end], kwargs)\n        return out\n    else:\n        return model(data, kwargs)\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n\ndef CircularGaussKernel(kernlen=None, circ_zeros = False, sigma = None, norm = True):\n    assert ((kernlen is not None) or sigma is not None)\n    if kernlen is None:\n        kernlen = int(2.0 * 3.0 * sigma + 1.0)\n        if (kernlen % 2 == 0):\n            kernlen = kernlen + 1;\n        halfSize = kernlen / 2;\n    halfSize = kernlen / 2;\n    r2 = float(halfSize*halfSize)\n    if sigma is None:\n        sigma2 = 0.9 * r2;\n        sigma = np.sqrt(sigma2)\n    else:\n        sigma2 = 2.0 * sigma * sigma    \n    x = np.linspace(-halfSize,halfSize,kernlen)\n    xv, yv = np.meshgrid(x, x, sparse=False, indexing=\'xy\')\n    distsq = (xv)**2 + (yv)**2\n    kernel = np.exp(-( distsq/ (sigma2)))\n    if circ_zeros:\n        kernel *= (distsq <= r2).astype(np.float32)\n    if norm:\n        kernel /= np.sum(kernel)\n    return kernel\n\ndef generate_2dgrid(h,w, centered = True):\n    if centered:\n        x = torch.linspace(-w/2+1, w/2, w)\n        y = torch.linspace(-h/2+1, h/2, h)\n    else:\n        x = torch.linspace(0, w-1, w)\n        y = torch.linspace(0, h-1, h)\n    grid2d = torch.stack([y.repeat(w,1).t().contiguous().view(-1), x.repeat(h)],1)\n    return grid2d\n\ndef generate_3dgrid(d, h, w, centered = True):\n    if type(d) is not list:\n        if centered:\n            z = torch.linspace(-d/2+1, d/2, d)\n        else:\n            z = torch.linspace(0, d-1, d)\n        dl = d\n    else:\n        z = torch.FloatTensor(d)\n        dl = len(d)\n    grid2d = generate_2dgrid(h,w, centered = centered)\n    grid3d = torch.cat([z.repeat(w*h,1).t().contiguous().view(-1,1), grid2d.repeat(dl,1)],dim = 1)\n    return grid3d\n\ndef zero_response_at_border(x, b):\n    if (b < x.size(3)) and (b < x.size(2)):\n        x[:, :,  0:b, :] =  0\n        x[:, :,  x.size(2) - b: , :] =  0\n        x[:, :, :,  0:b] =  0\n        x[:, :, :,   x.size(3) - b: ] =  0\n    else:\n        return x * 0\n    return x\n\nclass GaussianBlur(nn.Module):\n    def __init__(self, sigma=1.6):\n        super(GaussianBlur, self).__init__()\n        weight = self.calculate_weights(sigma)\n        self.register_buffer(\'buf\', weight)\n        return\n    def calculate_weights(self,  sigma):\n        kernel = CircularGaussKernel(sigma = sigma, circ_zeros = False)\n        h,w = kernel.shape\n        halfSize = float(h) / 2.;\n        self.pad = int(np.floor(halfSize))\n        return torch.from_numpy(kernel.astype(np.float32)).view(1,1,h,w);\n    def forward(self, x):\n        w = Variable(self.buf)\n        if x.is_cuda:\n            w = w.cuda()\n        return F.conv2d(F.pad(x, (self.pad,self.pad,self.pad,self.pad), \'replicate\'), w, padding = 0)\n\ndef batch_eig2x2(A):\n    trace = A[:,0,0] + A[:,1,1]\n    delta1 = (trace*trace - 4 * ( A[:,0,0]*  A[:,1,1] -  A[:,1,0]* A[:,0,1]))\n    mask = delta1 > 0\n    delta = torch.sqrt(torch.abs(delta1))\n    l1 = mask.float() * (trace + delta) / 2.0 +  1000.  * (1.0 - mask.float())\n    l2 = mask.float() * (trace - delta) / 2.0 +  0.0001  * (1.0 - mask.float())\n    return l1,l2\n\ndef line_prepender(filename, line):\n    with open(filename, \'r+\') as f:\n        content = f.read()\n        f.seek(0, 0)\n        f.write(line.rstrip(\'\\r\\n\') + \'\\n\' + content)\n    return\n'"
examples/just_shape/architectures.py,27,"b""from __future__ import division, print_function\nimport os\nimport errno\nimport numpy as np\nimport sys\nfrom copy import deepcopy\nimport math\nimport torch\nimport torch.nn.init\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom Utils import L2Norm, generate_2dgrid\nfrom Utils import str2bool\nfrom LAF import denormalizeLAFs, LAFs2ell, abc2A, extract_patches,normalizeLAFs,  get_rotation_matrix\nfrom LAF import get_LAFs_scales, get_normalized_affine_shape\nfrom LAF import rectifyAffineTransformationUpIsUp\n\nclass OriNetFast(nn.Module):\n    def __init__(self, PS = 16):\n        super(OriNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 2, kernel_size=int(PS/4), stride=1,padding=1, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/4)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.9)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_rot_matrix = True):\n        xy = self.features(self.input_norm(input)).view(-1,2) \n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(angle)\n        return angle\n\nclass GHH(nn.Module):\n    def __init__(self, n_in, n_out, s = 4, m = 4):\n        super(GHH, self).__init__()\n        self.n_out = n_out\n        self.s = s\n        self.m = m\n        self.conv = nn.Linear(n_in, n_out * s * m)\n        d = torch.arange(0, s)\n        self.deltas = -1.0 * (d % 2 != 0).float()  + 1.0 * (d % 2 == 0).float()\n        self.deltas = Variable(self.deltas)\n        return\n    def forward(self,x):\n        x_feats = self.conv(x.view(x.size(0),-1)).view(x.size(0), self.n_out, self.s, self.m);\n        max_feats = x_feats.max(dim = 3)[0];\n        if x.is_cuda:\n            self.deltas = self.deltas.cuda()\n        else:\n            self.deltas = self.deltas.cpu()\n        out =  (max_feats * self.deltas.view(1,1,-1).expand_as(max_feats)).sum(dim = 2)\n        return out\n\nclass YiNet(nn.Module):\n    def __init__(self, PS = 28):\n        super(YiNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 10, kernel_size=5, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding = 1),\n            nn.Conv2d(10, 20, kernel_size=5, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=4, stride=2, padding = 2),\n            nn.Conv2d(20, 50, kernel_size=3, stride=1, padding=0, bias = True),\n            nn.ReLU(),\n            nn.AdaptiveMaxPool2d(1),\n            GHH(50, 100),\n            GHH(100, 2)\n        )\n        self.input_mean = 0.427117081207483\n        self.input_std = 0.21888339179665006;\n        self.PS = PS\n        return\n    def import_weights(self, dir_name):\n        self.features[0].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_W.npy'))).float()\n        self.features[0].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer0_b.npy'))).float().view(-1)\n        self.features[3].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_W.npy'))).float()\n        self.features[3].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer1_b.npy'))).float().view(-1)\n        self.features[6].weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_W.npy'))).float()\n        self.features[6].bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer2_b.npy'))).float().view(-1)\n        self.features[9].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_W.npy'))).float().view(50, 1600).contiguous().t().contiguous()#.view(1600, 50, 1, 1).contiguous()\n        self.features[9].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer3_b.npy'))).float().view(1600)\n        self.features[10].conv.weight.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_W.npy'))).float().view(100, 32).contiguous().t().contiguous()#.view(32, 100, 1, 1).contiguous()\n        self.features[10].conv.bias.data = torch.from_numpy(np.load(os.path.join(dir_name, 'layer4_b.npy'))).float().view(32)\n        self.input_mean = float(np.load(os.path.join(dir_name, 'input_mean.npy')))\n        self.input_std = float(np.load(os.path.join(dir_name, 'input_std.npy')))\n        return\n    def input_norm1(self,x):\n        return (x - self.input_mean) / self.input_std\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def forward(self, input, return_rot_matrix = False):\n        xy = self.features(self.input_norm(input))\n        angle = torch.atan2(xy[:,0] + 1e-8, xy[:,1]+1e-8);\n        if return_rot_matrix:\n            return get_rotation_matrix(-angle)\n        return angle\n    \nclass AffNetFast(nn.Module):\n    def __init__(self, PS = 32):\n        super(AffNetFast, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(16, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(32, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias = False),\n            nn.BatchNorm2d(64, affine=False),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 3, kernel_size=8, stride=1, padding=0, bias = True),\n            nn.Tanh(),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.PS = PS\n        self.features.apply(self.weights_init)\n        self.halfPS = int(PS/2)\n        return\n    def input_norm(self,x):\n        flat = x.view(x.size(0), -1)\n        mp = torch.mean(flat, dim=1)\n        sp = torch.std(flat, dim=1) + 1e-7\n        return (x - mp.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n    def weights_init(self,m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.orthogonal(m.weight.data, gain=0.8)\n            try:\n                nn.init.constant(m.bias.data, 0.01)\n            except:\n                pass\n        return\n    def forward(self, input, return_A_matrix = False):\n        xy = self.features(self.input_norm(input)).view(-1,3)\n        a1 = torch.cat([1.0 + xy[:,0].contiguous().view(-1,1,1), 0 * xy[:,0].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        a2 = torch.cat([xy[:,1].contiguous().view(-1,1,1), 1.0 + xy[:,2].contiguous().view(-1,1,1)], dim = 2).contiguous()\n        return rectifyAffineTransformationUpIsUp(torch.cat([a1,a2], dim = 1).contiguous())\n\n"""
examples/just_shape/detect_affine_shape.py,6,"b'#!/usr/bin/python2 -utt\n# -*- coding: utf-8 -*-\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport time\nimport os\nimport cv2\nimport math\nimport numpy as np\nfrom architectures import AffNetFast \nPS = 32\nUSE_CUDA = False\n\n\nmodel = AffNetFast(PS = PS)\nweightd_fname = \'../../pretrained/AffNet.pth\'\n\ncheckpoint = torch.load(weightd_fname)\nmodel.load_state_dict(checkpoint[\'state_dict\'])\n\nmodel.eval()\nif USE_CUDA:\n    model.cuda()\n\ntry:\n    input_img_fname = sys.argv[1]\n    output_fname = sys.argv[2]\nexcept:\n    print ""Wrong input format. Try ./detect_affine_shape.py imgs/ref.png out.txt""\n    sys.exit(1)\n\nimage = cv2.imread(input_img_fname,0)\nh,w = image.shape\n\nn_patches =  h/w\n\ndescriptors_for_net = np.zeros((n_patches, 4))\n\npatches = np.ndarray((n_patches, 1, PS, PS), dtype=np.float32)\nfor i in range(n_patches):\n    patch =  image[i*(w): (i+1)*(w), 0:w]\n    patches[i,0,:,:] = cv2.resize(patch,(PS,PS)) / 255.\nbs = 128;\nouts = []\nn_batches = n_patches / bs + 1\nt = time.time()\nfor batch_idx in range(n_batches):\n    if batch_idx == n_batches - 1:\n        if (batch_idx + 1) * bs > n_patches:\n            end = n_patches\n        else:\n            end = (batch_idx + 1) * bs\n    else:\n        end = (batch_idx + 1) * bs\n    if batch_idx * bs >= end:\n        continue\n    data_a = patches[batch_idx * bs: end, :, :, :].astype(np.float32)\n    data_a = torch.from_numpy(data_a)\n    if USE_CUDA:\n        data_a = data_a.cuda()\n    data_a = Variable(data_a, volatile=True)\n    # compute output\n    out_a = model(data_a)\n    descriptors_for_net[batch_idx * bs: end,:] = out_a.data.cpu().numpy().reshape(-1, 4)\net  = time.time() - t\nnp.savetxt(output_fname,  descriptors_for_net, delimiter=\' \', fmt=\'%10.5f\')    \n'"
examples/toy_example_figure1/Losses.py,43,"b'import torch\nimport torch.nn as nn\nimport sys\nfrom Utils import L2Norm\n\ndef distance_matrix_vector(anchor, positive):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    d1_sq = torch.sum(anchor * anchor, dim=1).unsqueeze(-1)\n    d2_sq = torch.sum(positive * positive, dim=1).unsqueeze(-1)\n\n    eps = 1e-6\n    return torch.sqrt((d1_sq.repeat(1, anchor.size(0)) + torch.t(d2_sq.repeat(1, positive.size(0)))\n                      - 2.0 * torch.bmm(anchor.unsqueeze(0), torch.t(positive).unsqueeze(0)).squeeze(0))+eps)\n\ndef distance_vectors_pairwise(anchor, positive, negative = None):\n    """"""Given batch of anchor descriptors and positive descriptors calculate distance matrix""""""\n\n    a_sq = torch.sum(anchor * anchor, dim=1)\n    p_sq = torch.sum(positive * positive, dim=1)\n\n    eps = 1e-8\n    d_a_p = torch.sqrt(a_sq + p_sq - 2*torch.sum(anchor * positive, dim = 1) + eps)\n    if negative is not None:\n        n_sq = torch.sum(negative * negative, dim=1)\n        d_a_n = torch.sqrt(a_sq + n_sq - 2*torch.sum(anchor * negative, dim = 1) + eps)\n        d_p_n = torch.sqrt(p_sq + n_sq - 2*torch.sum(positive * negative, dim = 1) + eps)\n        return d_a_p, d_a_n, d_p_n\n    return d_a_p\n\ndef loss_random_sampling(anchor, positive, negative, anchor_swap = False, margin = 1.0, loss_type = ""triplet_margin""):\n    """"""Loss with random sampling (no hard in batch).\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.size() == negative.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    (pos, d_a_n, d_p_n) = distance_vectors_pairwise(anchor, positive, negative)\n    if anchor_swap:\n       min_neg = torch.min(d_a_n, d_p_n)\n    else:\n       min_neg = d_a_n\n\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_L2Net(anchor, positive, anchor_swap = False,  margin = 1.0, loss_type = ""triplet_margin""):\n    """"""L2Net losses: using whole batch as negatives, not only hardest.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive)\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008)-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    \n    if loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos1);\n        exp_den = torch.sum(torch.exp(2.0 - dist_matrix),1) + eps;\n        loss = -torch.log( exp_pos / exp_den )\n        if anchor_swap:\n            exp_den1 = torch.sum(torch.exp(2.0 - dist_matrix),0) + eps;\n            loss += -torch.log( exp_pos / exp_den1 )\n    else: \n        print (\'Only softmax loss works with L2Net sampling\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n\ndef loss_HardNet(anchor, positive, anchor_swap = True, anchor_ave = False,\\\n        margin = 1.0, batch_reduce = \'min\', loss_type = ""triplet_margin""):\n    """"""HardNet margin loss - calculates loss based on distance matrix based on positive distance and closest negative distance.\n    """"""\n\n    assert anchor.size() == positive.size(), ""Input sizes between positive and negative must be equal.""\n    assert anchor.dim() == 2, ""Inputd must be a 2D matrix.""\n    eps = 1e-8\n    dist_matrix = distance_matrix_vector(anchor, positive) +eps\n    eye = torch.autograd.Variable(torch.eye(dist_matrix.size(1))).cuda()\n\n    # steps to filter out same patches that occur in distance matrix as negatives\n    pos1 = torch.diag(dist_matrix)\n    dist_without_min_on_diag = dist_matrix+eye*10\n    mask = (dist_without_min_on_diag.ge(0.008).float()-1)*-1\n    mask = mask.type_as(dist_without_min_on_diag)*10\n    dist_without_min_on_diag = dist_without_min_on_diag+mask\n    if batch_reduce == \'min\':\n        min_neg = torch.min(dist_without_min_on_diag,1)[0]\n        if anchor_swap:\n            min_neg2 = torch.min(dist_without_min_on_diag,0)[0]\n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = min_neg\n        pos = pos1\n    elif batch_reduce == \'average\':\n        pos = pos1.repeat(anchor.size(0)).view(-1,1).squeeze(0)\n        min_neg = dist_without_min_on_diag.view(-1,1)\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).contiguous().view(-1,1)\n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = min_neg.squeeze(0)\n    elif batch_reduce == \'random\':\n        idxs = torch.autograd.Variable(torch.randperm(anchor.size()[0]).long()).cuda()\n        min_neg = dist_without_min_on_diag.gather(1,idxs.view(-1,1))\n        if anchor_swap:\n            min_neg2 = torch.t(dist_without_min_on_diag).gather(1,idxs.view(-1,1)) \n            min_neg = torch.min(min_neg,min_neg2)\n        min_neg = torch.t(min_neg).squeeze(0)\n        pos = pos1\n    else: \n        print (\'Unknown batch reduce mode. Try min, average or random\')\n        sys.exit(1)\n    if loss_type == ""triplet_margin"":\n        loss = torch.clamp(margin + pos - min_neg, min=0.0)\n    elif loss_type == \'softmax\':\n        exp_pos = torch.exp(2.0 - pos);\n        exp_den = exp_pos + torch.exp(2.0 - min_neg) + eps;\n        loss = - torch.log( exp_pos / exp_den )\n    elif loss_type == \'contrastive\':\n        loss = torch.clamp(margin - min_neg, min=0.0) + pos;\n    else: \n        print (\'Unknown loss type. Try triplet_margin, softmax or contrastive\')\n        sys.exit(1)\n    loss = torch.mean(loss)\n    return loss\n'"
examples/toy_example_figure1/Utils.py,4,"b""import torch\nimport torch.nn.init\nimport torch.nn as nn\nimport cv2\nimport numpy as np\n\n# resize image to size 32x32\ncv2_scale36 = lambda x: cv2.resize(x, dsize=(36, 36),\n                                 interpolation=cv2.INTER_LINEAR)\ncv2_scale = lambda x: cv2.resize(x, dsize=(32, 32),\n                                 interpolation=cv2.INTER_LINEAR)\n# reshape image\nnp_reshape = lambda x: np.reshape(x, (32, 32, 1))\n\nclass L2Norm(nn.Module):\n    def __init__(self):\n        super(L2Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sqrt(torch.sum(x * x, dim = 1) + self.eps)\n        x= x / norm.unsqueeze(-1).expand_as(x)\n        return x\n\nclass L1Norm(nn.Module):\n    def __init__(self):\n        super(L1Norm,self).__init__()\n        self.eps = 1e-10\n    def forward(self, x):\n        norm = torch.sum(torch.abs(x), dim = 1) + self.eps\n        x= x / norm.expand_as(x)\n        return x\n\n\ndef str2bool(v):\n    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n        return False\n"""
