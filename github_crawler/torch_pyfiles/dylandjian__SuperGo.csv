file_path,api_count,code
const.py,2,"b'import torch\nimport multiprocessing\n\n##### CONFIG\n\n## CUDA variable from Torch\nCUDA = torch.cuda.is_available()\n## Dtype of the tensors depending on CUDA\nDEVICE = torch.device(""cuda"") if CUDA else torch.device(""cpu"")\n## Number of self-play parallel games\nPARALLEL_SELF_PLAY = 2\n## Number of evaluation parallel games \nPARALLEL_EVAL = 3\n## MCTS parallel\nMCTS_PARALLEL = 4\n\n\n##### GLOBAL\n\n## Size of the Go board\nGOBAN_SIZE = 9\n## Number of move to end a game\nMOVE_LIMIT = GOBAN_SIZE ** 2 * 2.5\n## Maximum ratio that can be replaced in the rotation buffer\nMAX_REPLACEMENT = 0.4\n## Number of last states to keep\nHISTORY = 7\n## Learning rate\nLR = 0.01\n## Number of MCTS simulation\nMCTS_SIM = 64\n## Exploration constant\nC_PUCT = 0.2\n## L2 Regularization\nL2_REG = 0.0001\n## Momentum\nMOMENTUM = 0.9\n## Activate MCTS\nMCTS_FLAG = True\n## Epsilon for Dirichlet noise\nEPS = 0.25\n## Alpha for Dirichlet noise\nALPHA = 0.03\n## Batch size for evaluation during MCTS\nBATCH_SIZE_EVAL = 2\n## Number of self-play before training\nSELF_PLAY_MATCH = PARALLEL_SELF_PLAY\n## Number of moves before changing temperature to stop\n## exploration\nTEMPERATURE_MOVE = 5 \n\n\n##### TRAINING\n\n## Number of moves to consider when creating the batch\nMOVES = 2000\n## Number of mini-batch before evaluation during training\nBATCH_SIZE = 64\n## Number of channels of the output feature maps\nOUTPLANES_MAP = 10\n## Shape of the input state\nINPLANES = (HISTORY + 1) * 2 + 1\n## Probabilities for all moves + pass\nOUTPLANES = (GOBAN_SIZE ** 2) + 1\n## Number of residual blocks\nBLOCKS = 10\n## Number of training step before evaluating\nTRAIN_STEPS = 6 * BATCH_SIZE\n## Optimizer\nADAM = False\n## Learning rate annealing factor\nLR_DECAY = 0.1\n## Learning rate annnealing interval\nLR_DECAY_ITE = 100 * TRAIN_STEPS\n## Print the loss\nLOSS_TICK = BATCH_SIZE // 4\n## Refresh the dataset\nREFRESH_TICK = BATCH_SIZE\n\n\n##### EVALUATION\n\n## Number of matches against its old version to evaluate\n## the newly trained network\nEVAL_MATCHS = 20\n## Threshold to keep the new neural net\nEVAL_THRESH = 0.55\n'"
human.py,0,"b'#!/home/dylan/.virtualenvs/superGo/bin/python\n\nimport click\nfrom lib.play import play, Game\nfrom lib.utils import load_player\nfrom lib.gtp import Engine\n\n\n@click.command()\n@click.option(""--folder"", default=-1)\n@click.option(""--ite"", default=-1)\n@click.option(""--gtp/--no-gtp"", default=False)\ndef main(folder, ite, gtp):\n    player, _ = load_player(folder, ite)\n    if not isinstance(player, str):\n        game = Game(player, 0)\n        engine = Engine(game, board_size=game.goban_size)\n        while True:\n            print(engine.send(input()))\n    elif not gtp:\n        print(player)\n    else:\n        print(""\xc2\xaf\\_(\xe3\x83\x84)_/\xc2\xaf"")\n\n\nif __name__ == ""__main__"":\n    main()'"
main.py,0,"b'import multiprocessing\nimport time\nimport signal\nimport click\nimport os\nfrom lib.train import train\nfrom lib.play import play, self_play\nfrom lib.process import MyPool\n\n\n@click.command()\n@click.option(""--folder"", default=-1)\n@click.option(""--version"", default=False)\ndef main(folder, version):\n\n    ## Start method for PyTorch\n    multiprocessing.set_start_method(\'spawn\')\n\n    ## Create folder name if not provided\n    if folder == -1:\n        current_time = str(int(time.time()))\n    else:\n        current_time = str(folder)\n\n    ## Catch SIGNINT\n    original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n    pool = MyPool(2)\n    signal.signal(signal.SIGINT, original_sigint_handler)\n\n    try:\n        self_play_proc = pool.apply_async(self_play, args=(current_time, version,))\n        train_proc = pool.apply_async(train, args=(current_time, version,))\n\n        ## Comment one line or the other to get the stack trace\n        ## Must add a loooooong timer otherwise signals are not caught\n        # self_play_proc.get(60000000)\n        train_proc.get(60000000)\n\n    except KeyboardInterrupt:\n        pool.terminate()\n    else:\n        pool.close()\n        pool.join()\n\nif __name__ == ""__main__"":\n    main()\n\n\n'"
purge.py,0,"b'from pymongo import MongoClient\nimport click\n\n\n\n@click.command()\n@click.option(""--folder"", default=False)\ndef main(folder):\n    if not folder:\n        print(""[PURGE] You need to specify the table/folder name"")\n    else:\n        ## Init the client\n        client = MongoClient()\n        collection = client.superGo[str(folder)]\n\n        bulk = collection.initialize_unordered_bulk_op()\n\n        ## Remove 90% of the games\n        total_games = collection.find().count()\n        new_games = int(total_games * 0.90)\n        collection.remove({""id"": {""$lte"": new_games}})\n\n        ## Update the ids\n        games = collection.find()\n        new_id = 0\n        for game in games:\n            bulk.find({\'id\': game[\'id\']}).update({\'$set\': {""id"": new_id}})\n            new_id += 1\n        bulk.execute()\n\n        \n\nif __name__ == ""__main__"":\n    main()'"
viewer.py,0,"b'#!/home/dylan/.virtualenvs/superGo/bin/python\n\nimport numpy as np\nimport re\nimport pickle\nimport sys\nimport click\nfrom lib.gtp import format_success, parse_message\nfrom pymongo import MongoClient\n\n\ndef game_to_gtp(game, game_id, collection_name, color):\n    """""" Take a game from the database and convert it to send GTP instructions """"""\n\n    board_size = int(np.sqrt(len(game[0][0][1]) - 1))\n    moves = np.array(game[0])[:,3]\n    move_count = 0 if color == 0 else 1\n    game_winner = game[1]\n\n    ## Wait for input\n    while True:\n        try:\n            message_id, command, arguments = parse_message(input())\n        except EOFError:\n            break\n\n        if ""genmove"" in command:\n            if move_count < moves.shape[0]:\n                move = moves[move_count]\n                if move == board_size ** 2:\n                    print(format_success(None, response=""pass""))\n                else:\n                    print(format_success(None, response=""{}{}"".format(""ABCDEFGHJKLMNOPQRSTYVWYZ""\\\n                            [int(move % board_size)], int(board_size - move // board_size))))\n                move_count += 2\n            else:\n                print(\'?name    %s    ???\\n\\n\' % (command))\n        elif ""name"" in command:\n            print(format_success(None, response=""folder {}, game id: {}, winner: {}""\\\n                                    .format(collection_name, game_id, game_winner)))\n        elif ""play"" in command:\n            print(format_success(message_id, """"))\n        else:\n            print(\'?name    %s    ???\\n\\n\' % (command))\n\n\n@click.command()\n@click.option(""--folder"", default=-1)\n@click.option(""--game_id"", default=-1)\n@click.option(""--color"", default=1)\ndef main(folder, game_id, color):\n    ## Init Mongo\n    client = MongoClient()\n    db = client.superGo\n\n    ## Get latest bot version\n    if folder == -1:\n        collection = list(db.collection_names())\n        collection.sort()\n        collection = collection[-1]\n    else:\n        collection = str(folder)\n\n    if collection:\n        game_collection = db[collection]\n\n        ## Get the latest game\n        if game_id == -1:\n            last_game = game_collection.find().sort(\'_id\', -1).limit(1)\n        else:\n            last_game = game_collection.find({""id"": game_id})\n        if last_game.count() == 0:\n            print(""Wrong game_id or the database superGo doesnt have any collections"")\n        else:\n            for game in last_game:\n                final_game = pickle.loads(game[\'game\'])\n                game_to_gtp(final_game, game[\'id\'], collection, color)\n                break\n\nif __name__ == ""__main__"":\n    main()\n'"
lib/__init__.py,0,b''
lib/dataset.py,1,"b'import numpy as np\nimport timeit\nfrom torch.utils.data import Dataset, DataLoader\nfrom const import *\nfrom . import utils\n\n\nclass SelfPlayDataset(Dataset):\n    """"""\n    Self-play dataset containing state, probabilities\n    and the winner of the game.\n    """"""\n\n    def __init__(self):\n        """""" Instanciate a dataset """"""\n\n        self.states = np.zeros((MOVES, (HISTORY + 1) * 2 + 1, GOBAN_SIZE, GOBAN_SIZE))\n        self.plays = np.zeros((MOVES, GOBAN_SIZE ** 2 + 1))\n        self.winners = np.zeros(MOVES)\n        self.current_len = 0\n\n\n    def __len__(self):\n        return self.current_len\n\n\n    def __getitem__(self, idx):\n        states = utils.sample_rotation(self.states[idx]) \n        return utils.formate_state(states, self.plays[idx], self.winners[idx])\n\n\n    def update(self, game):\n        """""" Rotate the circular buffer to add new games at end """"""\n\n        dataset = np.array(game[0])\n        number_moves = dataset.shape[0]\n        self.current_len = min(self.current_len + number_moves, MOVES)\n        \n        self.states = np.roll(self.states, number_moves, axis=0)\n        self.states[:number_moves] = np.vstack(dataset[:,0])\n\n        self.plays = np.roll(self.plays, number_moves, axis=0)\n        self.plays[:number_moves] = np.vstack(dataset[:,1])\n\n        ## Replace the player color with the end game result\n        players = dataset[:,2]\n        players[np.where(players - 1 != game[1])] = -1\n        players[np.where(players != -1)] = 1\n\n        self.winners = np.roll(self.winners, number_moves, axis=0)\n        self.winners[:number_moves] = players\n\n        return number_moves\n'"
lib/evaluate.py,0,"b'import timeit\nfrom .play import play\nfrom const import *\n\n\ndef evaluate(player, new_player):\n    """""" Used to evaluate the best network against \n        the newly trained model """""" \n\n    print(""[EVALUATION] Starting to evaluate trained model !"")\n    start_time = timeit.default_timer()\n\n    ## Play the matches and get the results\n    results = play(player, opponent=new_player)\n    final_time = timeit.default_timer() - start_time\n    print(""[EVALUATION] Total duration: %.3f seconds, average duration:""\n            "" %.3f seconds"" % ((final_time, final_time / EVAL_MATCHS)))\n\n    ## Count the number of wins for each players\n    black_wins = 0\n    white_wins = 0\n    for result in results:\n        if result[0] == 1:\n            white_wins += 1\n        elif result[0] == 0:\n            black_wins += 1\n        else:\n            print(""[EVALUATION] Error during evaluation"")\n    \n    print(""[EVALUATION] black wins: %d vs %d for white""\\\n             % (black_wins, white_wins))\n    \n    ## Check if the trained player (black) is better than\n    ## the current best player depending on the threshold\n    if black_wins >= EVAL_THRESH * len(results):\n        return True\n    return False\n'"
lib/game.py,0,"b'import numpy as np\nimport pickle\nfrom const import *\nfrom models.mcts import MCTS\nfrom .go import GoEnv as Board\nfrom .utils import _prepare_state\n\n\nclass Game:\n    """""" A single process that is used to play a game between 2 agents """"""\n\n    def __init__(self, player, id, color=""black"", mcts_flag=MCTS_FLAG, goban_size=GOBAN_SIZE, opponent=False):\n        self.goban_size = goban_size\n        self.id = id + 1\n        self.human_pass = False\n        self.board = self._create_board(color)\n        self.player_color = 2 if color == ""black"" else 1\n        self.mcts = mcts_flag\n        if mcts_flag:\n            self.mcts = MCTS()\n        self.player = player\n        self.opponent = opponent\n\n\n    def _create_board(self, color):\n        """"""\n        Create a board with a goban_size and the color is\n        for the starting player\n        """"""\n    \n        board = Board(color, self.goban_size)\n        board.reset()\n        return board\n    \n\n    def _swap_color(self):\n        if self.player_color == 1:\n            self.player_color = 2\n        else:\n            self.player_color = 1\n\n    \n    def _get_move(self, board, probas):\n        """""" Select a move without MCTS """"""\n\n        player_move = None\n        legal_moves = board.get_legal_moves()\n\n        while player_move not in legal_moves and len(legal_moves) > 0:\n            player_move = np.random.choice(probas.shape[0], p=probas)\n            if player_move not in legal_moves:\n                old_proba = probas[player_move]\n                probas = probas + (old_proba / (probas.shape[0] - 1))\n                probas[player_move] = 0\n\n        return player_move\n\n\n    def _play(self, state, player, other_pass, competitive=False):\n        """""" Choose a move depending on MCTS or not """"""\n\n        if self.mcts:\n            if player.passed is True or other_pass:\n                action_scores = np.zeros((self.goban_size ** 2 + 1,))\n                action_scores[-1] = 1\n                action = self.goban_size ** 2\n            else:\n                action_scores, action = self.mcts.search(self.board, player,\\\n                                             competitive=competitive)\n\n            if action == self.goban_size ** 2:\n                player.passed = True\n            \n        else:\n            feature_maps = player.extractor(state)\n            _, probas = player.predict(state)\n            probas = probas[0].cpu().data.numpy()\n            if player.passed is True:\n                action = self.goban_size ** 2\n            else:\n                action = self._get_move(self.board, probas)\n\n            if action == self.goban_size ** 2:\n                player.passed = True\n\n            action_scores = np.zeros((self.goban_size ** 2 + 1),)\n            action_scores[action] = 1\n\n        state, reward, done = self.board.step(action)\n        return state, reward, done, action_scores, action\n\n\n    def __call__(self):\n        """"""\n        Make a game between the player and the opponent and return all the states\n        and the associated move. Also returns the winner in order to create the\n        training dataset\n        """"""\n\n        done = False\n        state = self.board.reset()\n        dataset = []\n        moves = 0\n        comp = False\n\n        while not done:\n\n            ## Prevent game from cycling\n            if moves > MOVE_LIMIT:\n                reward = self.board.get_winner()\n                if self.opponent:\n                    print(""[EVALUATION] Match %d done in eval after max move, winner %s""\n                        % (self.id, ""black"" if reward == 0 else ""white""))\n                    return pickle.dumps([reward])\n                return pickle.dumps((dataset, reward)) \n            \n            ## Adaptative temperature to stop exploration\n            if moves > TEMPERATURE_MOVE:\n                comp = True\n\n            ## For evaluation\n            if self.opponent:\n                state, reward, done, _, action = self._play(_prepare_state(state), \\\n                                                self.player, self.opponent.passed, competitive=True)\n                state, reward, done, _, action = self._play(_prepare_state(state), \\\n                                                self.opponent, self.player.passed, competitive=True)\n                moves += 2\n\n            ## For self-play\n            else:\n                state = _prepare_state(state)\n                new_state, reward, done, probas, action = self._play(state, self.player, \\\n                                                            False, competitive=comp)\n                self._swap_color()\n                dataset.append((state.cpu().data.numpy(), probas, \\\n                                self.player_color, action))\n                state = new_state \n                moves += 1\n            \n        ## Pickle the result because multiprocessing\n        if self.opponent:\n            print(""[EVALUATION] Match %d done in eval after %d moves, winner %s"" % (self.id,\n                        moves, ""black"" if reward == 0 else ""white""))\n            return pickle.dumps([reward])\n\n        return pickle.dumps((dataset, reward))\n\n    \n    def solo_play(self, move=None):\n        """""" Used to play against a human or for GTP, cant be called\n        in a multiprocess scenario """"""\n\n        ## Agent plays the first move of the game\n        if move is None:\n            state = _prepare_state(self.board.state)\n            state, reward, done, probas, move = self._play(state, self.player, self.human_pass, competitive=True)\n            self._swap_color()\n            return move\n        ## Otherwise just play a move and answer it\n        else:\n            state, reward, done = self.board.step(move)\n            if move != self.board.board_size ** 2:\n                self.mcts.advance(move)\n            else:\n                self.human_pass = True\n            self._swap_color()\n            return True\n    \n\n    def reset(self):\n        state = self.board.reset()\n'"
lib/go.py,0,"b'import pachi_py\nfrom copy import deepcopy\nimport numpy as np\nimport sys\nimport six\nfrom const import HISTORY, GOBAN_SIZE\n\n\ndef _pass_action(board_size):\n    return board_size ** 2\n\n\ndef _resign_action(board_size):\n    return board_size ** 2 + 1\n\n\ndef _coord_to_action(board, c):\n    """""" Converts Pachi coordinates to actions """"""\n\n    if c == pachi_py.PASS_COORD:\n        return _pass_action(board.size)\n    if c == pachi_py.RESIGN_COORD:\n        return _resign_action(board.size)\n\n    i, j = board.coord_to_ij(c)\n    return i*board.size + j\n\n\ndef _action_to_coord(board, a):\n    """""" Converts actions to Pachi coordinates """"""\n\n    if a == _pass_action(board.size):\n        return pachi_py.PASS_COORD\n    if a == _resign_action(board.size):\n        return pachi_py.RESIGN_COORD\n\n    return board.ij_to_coord(a // board.size, a % board.size)\n\n\ndef _format_state(history, player_color, board_size):\n    """""" \n    Format the encoded board into the state that is the input\n    of the feature model, defined in the AlphaGo Zero paper \n    BLACK = 1\n    WHITE = 2\n    """"""\n\n    state_history = np.concatenate((history[0], history[1]), axis=0)\n    to_play = np.full((1, board_size, board_size), player_color - 1)\n    final_state = np.concatenate((state_history, to_play), axis=0)\n    return final_state\n    \n\n\nclass GoEnv():\n\n    def __init__(self, player_color, board_size):\n        self.board_size = board_size\n        self.history = [np.zeros((HISTORY + 1, board_size, board_size)),\n                        np.zeros((HISTORY + 1, board_size, board_size))]\n\n        colormap = {\n            \'black\': pachi_py.BLACK,\n            \'white\': pachi_py.WHITE,\n        }\n        self.player_color = colormap[player_color]\n\n        self.komi = self._get_komi(board_size)\n        self.state = _format_state(self.history,\n                        self.player_color, self.board_size)\n        self.done = True\n\n\n    def _get_komi(self, board_size):\n        """""" Initialize a komi depending on the size of the board """"""\n\n        if 14 <= board_size <= 19:\n            return 7.5\n        elif 9 <= board_size <= 13:\n            return 5.5\n        return 0\n    \n\n    def get_legal_moves(self):\n        """""" Get all the legal moves and transform their coords into 1d """"""\n\n        legal_moves = self.board.get_legal_coords(self.player_color, filter_suicides=True)\n        final_moves = []\n\n        for pachi_move in legal_moves:\n            move = _coord_to_action(self.board, pachi_move)\n            if move != self.board_size ** 2 or self.test_move(move):\n                final_moves.append(move)\n        \n        if len(final_moves) == 0:\n            return [self.board_size ** 2]\n\n        return final_moves\n\n\n    def _act(self, action, history):\n        """""" Executes an action for the current player """"""\n\n        self.board = self.board.play(_action_to_coord(self.board, action), self.player_color)\n        board = self.board.encode()\n        color = self.player_color - 1\n        history[color] = np.roll(history[color], 1, axis=0)\n        history[color][0] = np.array(board[color])\n        self.player_color = pachi_py.stone_other(self.player_color)\n\n\n    def test_move(self, action):\n        """"""\n        Test if a specific valid action should be played,\n        depending on the current score. This is used to stop\n        the agent from passing if it makes him loose\n        """"""\n\n        board_clone = self.board.clone()\n        current_score = board_clone.fast_score  + self.komi\n\n        board_clone = board_clone.play(_action_to_coord(board_clone, action), self.player_color)\n        new_score = board_clone.fast_score + self.komi\n\n        if self.player_color - 1 == 0 and new_score >= current_score \\\n           or self.player_color - 1 == 1 and new_score <= current_score:\n           return False\n        return True\n\n\n    def reset(self):\n        """""" Reset the board """"""\n\n        self.board = pachi_py.CreateBoard(self.board_size)\n        opponent_resigned = False\n        self.done = self.board.is_terminal or opponent_resigned\n        return _format_state(self.history, self.player_color, self.board_size)\n\n\n    def render(self):\n        """""" Print the board for human reading """"""\n\n        outfile = sys.stdout\n        outfile.write(\'To play: {}\\n{}\\n\'.format(six.u(\n                        pachi_py.color_to_str(self.player_color)),\n                        self.board.__repr__().decode()))\n        return outfile\n\n\n    def get_winner(self):\n        """""" Get the winner, using the Tromp Taylor scoring + the komi """"""\n\n        score = self.board.fast_score + self.komi\n        white_wins = self.board.fast_score > 0\n        black_wins = self.board.fast_score < 0\n        reward = 1 if white_wins else 0\n\n        return reward\n    \n\n    def step(self, action):\n        """""" Perfoms an action and choose the winner if the 2 player\n            have passed """"""\n\n        if not self.done:\n            try:\n                self._act(action, self.history)\n            except pachi_py.IllegalMove:\n                six.reraise(*sys.exc_info())\n\n        # Reward: if nonterminal, then the reward is -1\n        if not self.board.is_terminal:\n            return _format_state(self.history, self.player_color, self.board_size), \\\n                    -1, False\n\n        assert self.board.is_terminal\n        self.done = True\n        reward = self.get_winner()\n        return _format_state(self.history, self.player_color, self.board_size), reward, True\n\n\n    def __deepcopy__(self, memo):\n        """""" Used to overwrite the deepcopy implicit method since\n            the board cannot be deepcopied """"""\n\n        cls = self.__class__\n        result = cls.__new__(cls)\n        memo[id(self)] = result\n        for k, v in self.__dict__.items():\n            if k == ""board"":\n                setattr(result, k, self.board.clone())\n            else:\n                setattr(result, k, deepcopy(v, memo))\n        return result\n'"
lib/gtp.py,0,"b'import re\nimport string\n\n\ndef pre_engine(s):\n    """""" Clean the message sent to the engine """"""\n\n    s = re.sub(""[^\\t\\n -~]"", """", s)\n    s = s.split(""#"")[0]\n    s = s.replace(""\\t"", "" "")\n    return s\n\n\ndef gtp_boolean(b):\n    return ""true"" if b else ""false""\n\n\ndef gtp_list(l):\n    return ""\\n"".join(l)\n\n\ndef gtp_color(color):\n    return { BLACK: ""B"", WHITE: ""W"" }[color]\n\n\ndef coord_to_gtp(coord, board_size):\n    """""" From 1d coord (0 for position 0,0 on the board) to A1 """"""\n\n    if coord == board_size ** 2:\n        return ""pass""\n    return ""{}{}"".format(""ABCDEFGHJKLMNOPQRSTYVWYZ""[int(coord % board_size)],\\\n                int(board_size - coord // board_size))\n\n\ndef gtp_to_coord(gtp_coord, board_size):\n    """""" From something like A1 to 1d coord like 0 (for 0,0 in 1d) """"""\n\n    ## 97 to convert back to the position in the alphabet, then - 1 for index\n    coord = gtp_coord.split()[1]\n    if coord == ""pass"":\n        return board_size ** 2\n\n    x = ""ABCDEFGHJKLMNOPQRSTYVWYZ"".index(coord[0]) + 1\n    y = board_size - int(coord[1])\n    final_coord = y * board_size + x - 1\n    return final_coord \n\n\ndef gtp_move(color, vertex):\n    return "" "".join([gtp_color(color), gtp_vertex(vertex)])\n\n\ndef parse_message(message):\n    """""" Parse the command sent to the agent """"""\n\n    message = pre_engine(message).strip()\n    first, rest = (message.split("" "", 1) + [None])[:2]\n    if first.isdigit():\n        message_id = int(first)\n        if rest is not None:\n            command, arguments = (rest.split("" "", 1) + [None])[:2]\n        else:\n            command, arguments = None, None\n    else:\n        message_id = None\n        command, arguments = first, rest\n\n    return message_id, command, arguments\n\n\nWHITE = 1\nBLACK = -1\nEMPTY = 0\n\nPASS = (0, 0)\nRESIGN = ""resign""\n\n\ndef parse_color(color):\n    if color.lower() in [""b"", ""black""]:\n        return BLACK\n    elif color.lower() in [""w"", ""white""]:\n        return WHITE\n    else:\n        return False\n\n\nMIN_BOARD_SIZE = 7\nMAX_BOARD_SIZE = 19\n\n\ndef format_success(message_id, response=None):\n    if response is None:\n        response = """"\n    else:\n        response = "" {}"".format(response)\n    if message_id:\n        return ""={}{}\\n\\n"".format(message_id, response)\n    else:\n        return ""={}\\n\\n"".format(response)\n\n\ndef format_error(message_id, response):\n    if response:\n        response = "" {}"".format(response)\n    if message_id:\n        return ""?{}{}\\n\\n"".format(message_id, response)\n    else:\n        return ""?{}\\n\\n"".format(response)\n\n\nclass Engine:\n\n    def __init__(self, game, komi=7.5, board_size=19, version=""0.2"", name=""AlphaGo""):\n        self.board_size = board_size\n        self.komi = komi\n        self._game = game\n        self._name = name\n        self._version = version\n        self.disconnect = False\n        self.known_commands = [\n            field[4:] for field in dir(self) if field.startswith(""cmd_"")]\n\n    def send(self, message):\n        message_id, command, arguments = parse_message(message)\n        if command in self.known_commands:\n            return format_success(\n                message_id, getattr(self, ""cmd_"" + command)(arguments))\n        else:\n            return format_error(message_id, ""unknown command"")\n\n    def vertex_in_range(self, vertex):\n        if vertex == PASS:\n            return True\n        if 1 <= vertex[0] <= self.size and 1 <= vertex[1] <= self.size:\n            return True\n        else:\n            return False\n    \n    # commands\n\n    def cmd_protocol_version(self, arguments):\n        return 2\n\n    def cmd_name(self, arguments):\n        return self._name\n\n    def cmd_version(self, arguments):\n        return self._version\n\n    def cmd_known_command(self, arguments):\n        return gtp_boolean(arguments in self.known_commands)\n\n    def cmd_list_commands(self, arguments):\n        return gtp_list(self.known_commands)\n\n    def cmd_quit(self, arguments):\n        self.disconnect = True\n\n    def cmd_boardsize(self, arguments):\n        if arguments.isdigit():\n            size = int(arguments)\n            if MIN_BOARD_SIZE <= size <= MAX_BOARD_SIZE:\n                self.size = size\n            else:\n                raise ValueError(""unacceptable size"")\n        else:\n            raise ValueError(""non digit size"")\n\n    def cmd_clear_board(self, arguments):\n        self._game.reset()\n\n    def cmd_komi(self, arguments):\n        try:\n            komi = float(arguments)\n            self.komi = komi\n        except ValueError:\n            raise ValueError(""syntax error"")\n\n    def cmd_play(self, arguments):\n        move = gtp_to_coord(arguments, self.board_size)\n        if self._game.solo_play(move):\n            return """"\n        else:\n            raise ValueError(""illegal move"")\n\n    def cmd_genmove(self, arguments):\n        c = parse_color(arguments)\n        if c:\n            move = self._game.solo_play()\n            return coord_to_gtp(move, self.board_size)\n        else:\n            raise ValueError(""unknown player: {}"".format(arguments))\n\n'"
lib/play.py,0,"b'import pickle\nimport time\nimport timeit\nfrom copy import deepcopy\nfrom const import *\nfrom pymongo import MongoClient\nfrom .utils import get_player, load_player, _prepare_state\nfrom .process import GameManager, create_matches\n\n\n\ndef self_play(current_time, loaded_version):\n    """"""\n    Used to create a learning dataset for the value and policy network.\n    Play against itself and backtrack the winner to maximize winner moves\n    probabilities\n    """"""\n\n    ## Init database connection\n    client = MongoClient()\n    collection = client.superGo[current_time]\n\n    game_id = 0\n    current_version = 1\n    player = False\n\n    while True:\n\n        ## Load the player when restarting traning\n        if loaded_version:\n            new_player, checkpoint = load_player(current_time, \n                                                loaded_version)\n            game_id = collection.find().count()\n            current_version = checkpoint[\'version\'] + 1\n            loaded_version = False\n        else:\n            new_player, checkpoint = get_player(current_time, current_version)\n            if new_player:\n                current_version = checkpoint[\'version\'] + 1\n\n        ## Waiting for the first player to be saved\n        print(""[PLAY] Current improvement level: %d"" % current_version)\n        if current_version == 1 and not player and not new_player:\n            print(""[PLAY] Waiting for first player"")\n            time.sleep(5)\n            continue\n\n        if new_player:\n            player = new_player\n            print(""[PLAY] New player !"")\n\n        ## Create the self-play match queue of processes\n        queue, results = create_matches(player , \\\n                    cores=PARALLEL_SELF_PLAY, match_number=SELF_PLAY_MATCH) \n        print(""[PLAY] Starting to fetch fresh games"")\n        start_time = timeit.default_timer()\n\n        try:\n            queue.join()\n\n            ## Collect the results and push them in the database\n            for _ in range(SELF_PLAY_MATCH):\n                result = results.get()\n                if result:\n                    collection.insert({\n                        ""game"": result,\n                        ""id"": game_id\n                    })\n                    game_id += 1\n            final_time = timeit.default_timer() - start_time\n            print(""[PLAY] Done fetching in %.3f seconds, average duration:""\n                "" %.3f seconds"" % ((final_time, final_time / SELF_PLAY_MATCH)))\n        finally:\n            queue.close()\n            results.close()\n\n\ndef play(player, opponent):\n    """""" Game between two players, for evaluation """"""\n\n    ## Create the evaluation match queue of processes\n    queue, results = create_matches(deepcopy(player), opponent=deepcopy(opponent), \\\n                cores=PARALLEL_EVAL, match_number=EVAL_MATCHS) \n    try:\n        queue.join()\n        \n        ## Gather the results and push them into a result queue\n        ## that will be sent back to the evaluation process\n        print(""[EVALUATION] Starting to fetch fresh games"")\n        final_result = []\n        for idx in range(EVAL_MATCHS):\n            result = results.get()\n            if result:\n                final_result.append(pickle.loads(result))\n        print(""[EVALUATION] Done fetching"")\n    finally:\n        queue.close()\n        results.close()\n    return final_result\n'"
lib/process.py,0,"b'import multiprocessing\nimport multiprocessing.pool\nfrom .game import Game\n\n\nclass NoDaemonProcess(multiprocessing.Process):\n    # make \'daemon\' attribute always return False\n    def _get_daemon(self):\n        return False\n    def _set_daemon(self, value):\n        pass\n    daemon = property(_get_daemon, _set_daemon)\n\n\n\nclass MyPool(multiprocessing.pool.Pool):\n    Process = NoDaemonProcess\n\n\n\nclass GameManager(multiprocessing.Process):\n    """"""\n    Used to manage a Queue of process. In charge of the interaction\n    between the processes.\n    """"""\n\n    def __init__(self, game_queue, result_queue):\n        multiprocessing.Process.__init__(self)\n        self.game_queue = game_queue\n        self.result_queue = result_queue\n\n\n    def run(self):\n        """""" Execute a task from the game_queue """"""\n\n        process_name = self.name\n        while True:\n            try:\n                next_task = self.game_queue.get(600000)\n\n                ## End the processes that are done\n                if next_task is None:\n                    self.game_queue.task_done()\n                    break\n\n                answer = next_task()\n                self.game_queue.task_done()\n                self.result_queue.put(answer)\n            except Exception as e:\n                print(""Game has thrown an error"")\n\n\n\ndef create_matches(player, opponent=None, cores=1, match_number=10):\n    """""" Create the process queue """"""\n\n    queue = multiprocessing.JoinableQueue()\n    results = multiprocessing.Queue()\n    game_results = []\n\n    game_managers = [\n        GameManager(queue, results)\n        for _ in range(cores)\n    ]\n\n    for game_manager in game_managers:\n        game_manager.start()\n\n    for game_id in range(match_number):\n        queue.put(Game(player, game_id, opponent=opponent))\n    \n    for _ in range(cores):\n        queue.put(None)\n    \n    return queue, results\n\n\n'"
lib/train.py,11,"b'import torch\nimport signal\nimport torch.nn as nn \nimport numpy as np\nimport pickle\nimport time\nimport torch.nn.functional as F\nimport multiprocessing\nimport multiprocessing.pool\nfrom lib.process import MyPool\nfrom lib.dataset import SelfPlayDataset\nfrom lib.evaluate import evaluate\nfrom lib.utils import load_player\nfrom copy import deepcopy\nfrom pymongo import MongoClient\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom const import *\nfrom models.agent import Player\n\n\n\nclass AlphaLoss(torch.nn.Module):\n    """"""\n    Custom loss as defined in the paper :\n    (z - v) ** 2 --> MSE Loss\n    (-pi * logp) --> Cross Entropy Loss\n    z : self_play_winner\n    v : winner\n    pi : self_play_probas\n    p : probas\n    \n    The loss is then averaged over the entire batch\n    """"""\n\n    def __init__(self):\n        super(AlphaLoss, self).__init__()\n\n    def forward(self, winner, self_play_winner, probas, self_play_probas):\n        value_error = (self_play_winner - winner) ** 2\n        policy_error = torch.sum((-self_play_probas * (1e-6 + probas).log()), 1)\n        total_error = (value_error.view(-1) + policy_error).mean()\n        return total_error\n\n\ndef fetch_new_games(collection, dataset, last_id, loaded_version=None):\n    """""" Update the dataset with new games from the databse """"""\n\n    ## Fetch new games in reverse order so we add the newest games first\n    new_games = collection.find({""id"": {""$gt"": last_id}}).sort(\'_id\', -1)\n    added_moves = 0\n    added_games = 0\n    print(""[TRAIN] Fetching: %d new games from the db""% (new_games.count()))\n\n    for game in new_games:\n        number_moves = dataset.update(pickle.loads(game[\'game\']))\n        added_moves += number_moves\n        added_games += 1\n\n        ## You cant replace more than 40% of the dataset at a time\n        if added_moves >= MOVES * MAX_REPLACEMENT and not loaded_version:\n            break\n    \n    print(""[TRAIN] Last id: %d, added games: %d, added moves: %d""\\\n                    % (last_id, added_games, added_moves))\n    return last_id + added_games\n\n\ndef train_epoch(player, optimizer, example, criterion):\n    """""" Used to train the 3 models over a single batch """"""\n\n    optimizer.zero_grad()\n    winner, probas = player.predict(example[\'state\'])\n\n    loss = criterion(winner, example[\'winner\'], \\\n                     probas, example[\'move\'])\n    loss.backward()\n    optimizer.step()\n\n    return float(loss)\n\n\ndef update_lr(lr, optimizer, total_ite, lr_decay=LR_DECAY, lr_decay_ite=LR_DECAY_ITE):\n    """""" Decay learning rate by a factor of lr_decay every lr_decay_ite iteration """"""\n\n    if total_ite % lr_decay_ite != 0 or lr <= 0.0001:\n        return lr, optimizer\n    \n    print(""[TRAIN] Decaying the learning rate !"")\n    lr = lr * lr_decay\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n    return lr, optimizer\n\n\ndef create_state(current_version, lr, total_ite, optimizer):\n    """""" Create a checkpoint to be saved """"""\n\n    state = {\n        \'version\': current_version,\n        \'lr\': lr,\n        \'total_ite\': total_ite,\n        \'optimizer\': optimizer.state_dict()\n    }\n    return state\n\n\ndef collate_fn(example):\n    """""" Custom way of collating example in dataloader """"""\n\n    state = []\n    probas = []\n    winner = []\n\n    for ex in example:\n        state.extend(ex[0])\n        probas.extend(ex[1])\n        winner.extend(ex[2])\n\n    state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n    probas = torch.tensor(probas, dtype=torch.float, device=DEVICE)\n    winner = torch.tensor(winner, dtype=torch.float, device=DEVICE)\n    return state, probas, winner\n\n\ndef create_optimizer(player, lr, param=None):\n    """""" Create or load a saved optimizer """"""\n\n    joint_params = list(player.extractor.parameters()) + \\\n                list(player.policy_net.parameters()) +\\\n                list(player.value_net.parameters())\n\n    if ADAM:\n        opt = torch.optim.Adam(joint_params, lr=lr, weight_decay=L2_REG)\n    else:\n        opt = torch.optim.SGD(joint_params, lr=lr, \\\n                        weight_decay=L2_REG, momentum=MOMENTUM)\n    \n    if param:\n        opt.load_state_dict(param)\n    \n    return opt\n\n\ndef train(current_time, loaded_version):\n    """""" Train the models using the data generated by the self-play """"""\n\n    last_id = 0\n    total_ite = 1\n    lr = LR\n    version = 1\n    pool = False \n    criterion = AlphaLoss()\n    dataset = SelfPlayDataset()\n    \n    ## Database connection\n    client = MongoClient()\n    collection = client.superGo[current_time]\n\n    ## First player either from disk or fresh\n    if loaded_version:\n        player, checkpoint = load_player(current_time, loaded_version) \n        optimizer = create_optimizer(player, lr, param=checkpoint[\'optimizer\'])\n        total_ite = checkpoint[\'total_ite\']\n        lr = checkpoint[\'lr\']\n        version = checkpoint[\'version\']\n        last_id = collection.find().count() - (MOVES // MOVE_LIMIT) * 2 \n    else:\n        player = Player()\n        optimizer = create_optimizer(player, lr)\n        state = create_state(version, lr, total_ite, optimizer)\n        player.save_models(state, current_time)\n    best_player = deepcopy(player)\n\n    ## Callback after the evaluation is done, must be a closure\n    def new_agent(result):\n        if result:\n            nonlocal version, pending_player, current_time, \\\n                    lr, total_ite, best_player \n            version += 1\n            state = create_state(version, lr, total_ite, optimizer)\n            best_player = pending_player\n            pending_player.save_models(state, current_time)\n            print(""[EVALUATION] New best player saved !"")\n        else:\n            nonlocal last_id\n            ## Force a new fetch in case the player didnt improve\n            last_id = fetch_new_games(collection, dataset, last_id)\n\n    ## Wait before the circular before is full\n    while len(dataset) < MOVES:\n        last_id = fetch_new_games(collection, dataset, last_id, loaded_version=loaded_version)\n        time.sleep(5)\n\n    print(""[TRAIN] Circular buffer full !"")\n    print(""[TRAIN] Starting to train !"")\n    dataloader = DataLoader(dataset, collate_fn=collate_fn, \\\n                batch_size=BATCH_SIZE, shuffle=True)\n\n    while True:\n        batch_loss = []\n        for batch_idx, (state, move, winner) in enumerate(dataloader):\n            running_loss = []\n            lr, optimizer = update_lr(lr, optimizer, total_ite)\n    \n            ## Evaluate a copy of the current network asynchronously\n            if total_ite % TRAIN_STEPS == 0:\n                pending_player = deepcopy(player)\n                last_id = fetch_new_games(collection, dataset, last_id)\n\n                ## Wait in case an evaluation is still going on\n                if pool:\n                    print(""[EVALUATION] Waiting for eval to end before re-eval"")\n                    pool.close()\n                    pool.join()\n                pool = MyPool(1)\n                try:\n                    pool.apply_async(evaluate, args=(pending_player, best_player), \\\n                            callback=new_agent)\n                except Exception as e:\n                    client.close()\n                    pool.terminate()\n            \n            example = {\n                \'state\': state,\n                \'winner\': winner,\n                \'move\' : move\n            }\n            loss = train_epoch(player, optimizer, example, criterion)\n            running_loss.append(loss)\n\n            ## Print running loss\n            if total_ite % LOSS_TICK == 0:\n                print(""[TRAIN] current iteration: %d, averaged loss: %.3f""\\\n                        % (total_ite, np.mean(running_loss)))\n                batch_loss.append(np.mean(running_loss))\n                running_loss = []\n            \n            ## Fetch new games\n            if total_ite % REFRESH_TICK == 0:\n                last_id = fetch_new_games(collection, dataset, last_id)\n            \n            total_ite += 1\n    \n        if len(batch_loss) > 0:\n            print(""[TRAIN] Average backward pass loss : %.3f, current lr: %f"" % (np.mean(batch_loss), lr))\n    \n'"
lib/utils.py,2,"b'import os\nimport numpy as np\nimport random\nimport torch\nfrom models.agent import Player\nfrom const import *\n\n\ndef _prepare_state(state):\n    """"""\n    Transform the numpy state into a PyTorch tensor with cuda\n    if available\n    """"""\n\n    x = torch.from_numpy(np.array([state]))\n    x = torch.tensor(x, dtype=torch.float, device=DEVICE)\n    return x\n\n\ndef get_version(folder_path, version):\n    """""" Either get the last versionration of \n        the specific folder or verify it version exists """"""\n\n    if int(version) == -1:\n        files = os.listdir(folder_path)\n        if len(files) > 0:\n            all_version = list(map(lambda x: int(x.split(\'-\')[0]), files))\n            all_version.sort()\n            file_version = all_version[-1]\n        else:\n            return False\n    else:\n        test_file = ""{}-extractor.pth.tar"".format(version)\n        if not os.path.isfile(os.path.join(folder_path, test_file)):\n            return False\n        file_version = version\n    return file_version\n\n\ndef load_player(folder, version):\n    """""" Load a player given a folder and a version """"""\n\n    path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \\\n                   \'..\', \'saved_models\')\n    if folder == -1:\n        folders = os.listdir(path)\n        folders.sort()\n        if len(folders) > 0:\n            folder = folders[-1]\n        else:\n            return False, False\n    elif not os.path.isdir(os.path.join(path, str(folder))):\n        return False, False\n\n    folder_path = os.path.join(path, str(folder))\n    last_version = get_version(folder_path, version)\n    if not last_version:\n        return False, False\n\n    return get_player(folder, int(last_version))\n\n\ndef get_player(current_time, version):\n    """""" Load the models of a specific player """"""\n\n    path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \\\n                            \'..\', \'saved_models\', str(current_time))\n    try:\n        mod = os.listdir(path)\n        models = list(filter(lambda model: (model.split(\'-\')[0] == str(version)), mod))\n        models.sort()\n        if len(models) == 0:\n            return False, version\n    except FileNotFoundError:\n        return False, version\n    \n    player = Player()\n    checkpoint = player.load_models(path, models)\n    return player, checkpoint\n\n\ndef sample_rotation(state, num=8):\n    """""" Apply a certain number of random transformation to the input state """"""\n\n    ## Create the dihedral group of a square with all the operations needed\n    ## in order to get the specific transformation and randomize their order\n    dh_group = [(None, None), ((np.rot90, 1), None), ((np.rot90, 2), None),\n                ((np.rot90, 3), None), (np.fliplr, None), (np.flipud, None),\n                (np.flipud,  (np.rot90, 1)), (np.fliplr, (np.rot90, 1))]\n    random.shuffle(dh_group)\n\n    states = []\n    boards = (HISTORY + 1) * 2 ## Number of planes to rotate\n\n    for idx in range(num):\n        new_state = np.zeros((boards + 1, GOBAN_SIZE, GOBAN_SIZE,))\n        new_state[:boards] = state[:boards]\n\n        ## Apply the transformations in the tuple defining how to get\n        ## the desired dihedral rotation / transformation\n        for grp in dh_group[idx]:\n            for i in range(boards):\n                if isinstance(grp, tuple):\n                    new_state[i] = grp[0](new_state[i], k=grp[1])\n                elif grp is not None:\n                    new_state[i] = grp(new_state[i])\n\n        new_state[boards] = state[boards]\n        states.append(new_state)\n    \n    if len(states) == 1:\n        return np.array(states[0])\n    return np.array(states)\n\n\ndef formate_state(state, probas, winner):\n    """""" Repeat the probas and the winner to make every example identical after\n        the dihedral rotation has been applied """"""\n\n    probas = np.reshape(probas, (1, probas.shape[0]))\n    probas = np.repeat(probas, 8, axis=0)\n    winner = np.full((8, 1), winner)\n    return state, probas, winner\n'"
models/__init__.py,0,b''
models/agent.py,2,"b'import os\nfrom .feature import Extractor\nfrom .value import ValueNet\nfrom .policy import PolicyNet\nfrom const import *\n\n\nclass Player:\n    def __init__(self):\n        """""" Create an agent and initialize the networks """"""\n\n        self.extractor = Extractor(INPLANES, OUTPLANES_MAP).to(DEVICE)\n        self.value_net = ValueNet(OUTPLANES_MAP, OUTPLANES).to(DEVICE)\n        self.policy_net = PolicyNet(OUTPLANES_MAP, OUTPLANES).to(DEVICE)    \n        self.passed = False\n    \n\n    def predict(self, state):\n        """""" Predict the probabilities and the winner from a given state """"""\n\n        feature_maps = self.extractor(state)\n        winner = self.value_net(feature_maps)\n        probas = self.policy_net(feature_maps)\n        return winner, probas\n\n\n    def save_models(self, state, current_time):\n        """""" Save the models """"""\n\n        for model in [""extractor"", ""policy_net"", ""value_net""]:\n            self._save_checkpoint(getattr(self, model), model,\\\n                                state, current_time)\n\n\n    def _save_checkpoint(self, model, filename, state, current_time):\n        """""" Save a checkpoint of the models """"""\n\n        dir_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \\\n                            \'..\', \'saved_models\', current_time)\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n\n        filename = os.path.join(dir_path, ""{}-{}.pth.tar"".format(state[\'version\'], filename))\n        state[\'model\'] = model.state_dict()\n        torch.save(state, filename)\n\n\n    def load_models(self, path, models):\n        """""" Load an already saved model """"""\n\n        names = [""extractor"", ""policy_net"", ""value_net""]\n        for i in range(0, len(models)):\n            checkpoint = torch.load(os.path.join(path, models[i]))\n            model = getattr(self, names[i])\n            model.load_state_dict(checkpoint[\'model\'])\n            return checkpoint\n\n'"
models/feature.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom const import BLOCKS\n\n\nclass BasicBlock(nn.Module):\n    """"""\n    Basic residual block with 2 convolutions and a skip connection before the last\n    ReLU activation.\n    """""" \n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out))\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += residual\n        out = F.relu(out)\n\n        return out\n\n\nclass Extractor(nn.Module):\n    """"""\n    This network is used as a feature extractor, takes as input the \'state\' defined in\n    the AlphaGo Zero paper\n    - The state of the past n turns of the board (7 in the paper) for each player.\n      This means that the first n matrices of the input state will be 1 and 0, where 1\n      is a stone. \n      This is done to take into consideration Go rules (repetitions are forbidden) and\n      give a sense of time\n\n    - The color of the stone that is next to play. This could have been a single bit, but\n      for implementation purposes, it is actually expended to the whole matrix size.\n      If it is black turn, then the last matrix of the input state will be a NxN matrix\n      full of 1, where N is the size of the board, 19 in the case of AlphaGo.\n      This is done to take into consideration the komi.\n\n    The ouput is a series of feature maps that retains the meaningful informations\n    contained in the input state in order to make a good prediction on both which is more\n    likely to win the game from the current state, and also which move is the best one to\n    make. \n    """"""\n\n    def __init__(self, inplanes, outplanes):\n        super(Extractor, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, outplanes, stride=1, kernel_size=3,\n                        padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(outplanes)\n\n        for block in range(BLOCKS):\n            setattr(self, ""res{}"".format(block), \\\n                BasicBlock(outplanes, outplanes))\n    \n\n    def forward(self, x):\n        """"""\n        x : tensor representing the state\n        feature_maps : result of the residual layers forward pass\n        """"""\n\n        x = F.relu(self.bn1(self.conv1(x)))\n        for block in range(BLOCKS - 1):\n            x = getattr(self, ""res{}"".format(block))(x)\n        \n        feature_maps = getattr(self, ""res{}"".format(BLOCKS - 1))(x)\n        return feature_maps\n\n\n\n\n\n\n\n\n'"
models/mcts.py,2,"b'import numpy as np\nimport torch\nimport threading\nimport time\nimport random\nfrom collections import OrderedDict\nfrom numba import jit\nfrom copy import deepcopy\nfrom const import *\nfrom lib.utils import _prepare_state, sample_rotation\n\n\n@jit\ndef _opt_select(nodes, c_puct=C_PUCT):\n    """""" Optimized version of the selection based of the PUCT formula """"""\n\n    total_count = 0\n    for i in range(nodes.shape[0]):\n        total_count += nodes[i][1]\n    \n    action_scores = np.zeros(nodes.shape[0])\n    for i in range(nodes.shape[0]):\n        action_scores[i] = nodes[i][0] + c_puct * nodes[i][2] * \\\n                (np.sqrt(total_count) / (1 + nodes[i][1]))\n    \n    equals = np.where(action_scores == np.max(action_scores))[0]\n    if equals.shape[0] > 0:\n        return np.random.choice(equals)\n    return equals[0]\n\n\ndef dirichlet_noise(probas):\n    """""" Add Dirichlet noise in the root node """"""\n\n    dim = (probas.shape[0],)\n    new_probas = (1 - EPS) * probas + \\\n                    EPS * np.random.dirichlet(np.full(dim, ALPHA))\n    return new_probas\n\n\nclass Node:\n\n    def __init__(self, parent=None, proba=None, move=None):\n        """"""\n        p : probability of reaching that node, given by the policy net\n        n : number of time this node has been visited during simulations\n        w : total action value, given by the value network\n        q : mean action value (w / n)\n        """"""\n\n        self.p = proba\n        self.n = 0\n        self.w = 0\n        self.q = 0\n        self.childrens = []\n        self.parent = parent\n        self.move = move\n    \n\n    def update(self, v):\n        """""" Update the node statistics after a playout """"""\n\n        self.w = self.w + v\n        self.q = self.w / self.n if self.n > 0 else 0\n\n\n    def is_leaf(self):\n        """""" Check whether node is a leaf or not """"""\n\n        return len(self.childrens) == 0\n\n\n    def expand(self, probas):\n        """""" Create a child node for every non-zero move probability """"""\n\n        self.childrens = [Node(parent=self, move=idx, proba=probas[idx]) \\\n                    for idx in range(probas.shape[0]) if probas[idx] > 0]\n\n\n\nclass EvaluatorThread(threading.Thread):\n    def __init__(self, player, eval_queue, result_queue, condition_search, condition_eval):\n        """""" Used to be able to batch evaluate positions during tree search """"""\n\n        threading.Thread.__init__(self)\n        self.eval_queue = eval_queue\n        self.result_queue = result_queue\n        self.player = player\n        self.condition_search = condition_search\n        self.condition_eval = condition_eval\n\n\n    def run(self):\n        for sim in range(MCTS_SIM // MCTS_PARALLEL):\n\n            ## Wait for the eval_queue to be filled by new positions to evaluate\n            self.condition_search.acquire()\n            while len(self.eval_queue) < MCTS_PARALLEL:\n                self.condition_search.wait()\n            self.condition_search.release()\n\n            self.condition_eval.acquire()\n            while len(self.result_queue) < MCTS_PARALLEL:\n                keys = list(self.eval_queue.keys())\n                max_len = BATCH_SIZE_EVAL if len(keys) > BATCH_SIZE_EVAL else len(keys)\n\n                ## Predict the feature_maps, policy and value\n                states = torch.tensor(np.array(list(self.eval_queue.values()))[0:max_len],\n                            dtype=torch.float, device=DEVICE)\n                v, probas = self.player.predict(states)\n\n                ## Replace the state with the result in the eval_queue\n                ## and notify all the threads that the result are available\n                for idx, i in zip(keys, range(max_len)):\n                    del self.eval_queue[idx]\n                    self.result_queue[idx] = (probas[i].cpu().data.numpy(), v[i])\n\n                self.condition_eval.notifyAll()\n            self.condition_eval.release()\n\n\n\nclass SearchThread(threading.Thread):\n\n    def __init__(self, mcts, game, eval_queue, result_queue, thread_id, lock, condition_search, condition_eval):\n        """""" Run a single simulation """"""\n\n        threading.Thread.__init__(self)\n        self.eval_queue = eval_queue\n        self.result_queue = result_queue\n        self.mcts = mcts\n        self.game = game\n        self.lock = lock\n        self.thread_id = thread_id\n        self.condition_eval = condition_eval\n        self.condition_search = condition_search\n    \n\n    def run(self):\n        game = deepcopy(self.game)\n        state = game.state\n        current_node = self.mcts.root\n        done = False\n\n        ## Traverse the tree until leaf\n        while not current_node.is_leaf() and not done:\n            ## Select the action that maximizes the PUCT algorithm\n            current_node = current_node.childrens[_opt_select( \\\n                    np.array([[node.q, node.n, node.p] \\\n                    for node in current_node.childrens]))]\n\n            ## Virtual loss since multithreading\n            self.lock.acquire()\n            current_node.n += 1\n            self.lock.release()\n\n            state, _, done = game.step(current_node.move)\n\n        if not done:\n\n            ## Add current leaf state with random dihedral transformation\n            ## to the evaluation queue\n            self.condition_search.acquire()\n            self.eval_queue[self.thread_id] = sample_rotation(state, num=1)\n            self.condition_search.notify()\n            self.condition_search.release()\n\n            ## Wait for the evaluator to be done\n            self.condition_eval.acquire()\n            while self.thread_id not in self.result_queue.keys():\n                self.condition_eval.wait()\n\n            ## Copy the result to avoid GPU memory leak\n            result = self.result_queue.pop(self.thread_id)\n            probas = np.array(result[0])\n            v = float(result[1])\n            self.condition_eval.release()\n\n            ## Add noise in the root node\n            if not current_node.parent:\n                probas = dirichlet_noise(probas)\n            \n            ## Modify probability vector depending on valid moves\n            ## and normalize after that\n            valid_moves = game.get_legal_moves()\n            illegal_moves = np.setdiff1d(np.arange(game.board_size ** 2 + 1),\n                                        np.array(valid_moves))\n            probas[illegal_moves] = 0\n            total = np.sum(probas)\n            probas /= total\n\n            ## Create the child nodes for the current leaf\n            self.lock.acquire()\n            current_node.expand(probas)\n\n            ## Backpropagate the result of the simulation\n            while current_node.parent:\n                current_node.update(v)\n                current_node = current_node.parent\n            self.lock.release()\n\n\n\nclass MCTS:\n    def __init__(self):\n        self.root = Node()\n\n\n    def _draw_move(self, action_scores, competitive=False):\n        """"""\n        Find the best move, either deterministically for competitive play\n        or stochiasticly according to some temperature constant\n        """"""\n\n        if competitive:\n            moves = np.where(action_scores == np.max(action_scores))[0]\n            move = np.random.choice(moves)\n            total = np.sum(action_scores)\n            probas = action_scores / total\n\n        else:\n            total = np.sum(action_scores)\n            probas = action_scores / total\n            move = np.random.choice(action_scores.shape[0], p=probas)\n\n        return move, probas\n\n\n    def advance(self, move):\n        """""" Manually advance in the tree, used for GTP """"""\n\n        for idx in range(len(self.root.childrens)):\n            if self.root.childrens[idx].move == move:\n                final_idx = idx\n                break\n        self.root = self.root.childrens[final_idx]\n\n\n    def search(self, current_game, player, competitive=False):\n        """"""\n        Search the best moves through the game tree with\n        the policy and value network to update node statistics\n        """"""\n\n        ## Locking for thread synchronization\n        condition_eval = threading.Condition()\n        condition_search = threading.Condition()\n        lock = threading.Lock()\n\n        ## Single thread for the evaluator (for now)\n        eval_queue = OrderedDict()\n        result_queue = {}\n        evaluator = EvaluatorThread(player, eval_queue, result_queue, condition_search, condition_eval)\n        evaluator.start()\n\n        threads = []\n        ## Do exactly the required number of simulation per thread\n        for sim in range(MCTS_SIM // MCTS_PARALLEL):\n            for idx in range(MCTS_PARALLEL):\n                threads.append(SearchThread(self, current_game, eval_queue, result_queue, idx, \n                                        lock, condition_search, condition_eval))\n                threads[-1].start()\n            for thread in threads:\n                thread.join()\n        evaluator.join()\n\n        ## Create the visit count vector\n        action_scores = np.zeros((current_game.board_size ** 2 + 1,))\n        for node in self.root.childrens:\n            action_scores[node.move] = node.n\n\n        ## Pick the best move\n        final_move, final_probas = self._draw_move(action_scores, competitive=competitive)\n\n        ## Advance the root to keep the statistics of the childrens\n        for idx in range(len(self.root.childrens)):\n            if self.root.childrens[idx].move == final_move:\n                break\n        self.root = self.root.childrens[idx]\n\n        return final_probas, final_move\n\n\n'"
models/policy.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n\nclass PolicyNet(nn.Module):\n    """"""\n    This network is used in order to predict which move has the best potential to lead to a win\n    given the same \'state\' described in the Feature Extractor model.\n    """"""\n\n    def __init__(self, inplanes, outplanes):\n        super(PolicyNet, self).__init__()\n        self.outplanes = outplanes\n        self.conv = nn.Conv2d(inplanes, 1, kernel_size=1)\n        self.bn = nn.BatchNorm2d(1)\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n        self.fc = nn.Linear(outplanes - 1, outplanes)\n        \n\n    def forward(self, x):\n        """"""\n        x : feature maps extracted from the state\n        probas : a NxN + 1 matrix where N is the board size\n                 Each value in this matrix represent the likelihood\n                 of winning by playing this intersection\n        """"""\n \n        x = F.relu(self.bn(self.conv(x)))\n        x = x.view(-1, self.outplanes - 1)\n        x = self.fc(x)\n        probas = self.logsoftmax(x).exp()\n\n        return probas'"
models/value.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\n\nclass ValueNet(nn.Module):\n\n    """"""\n    This network is used to predict which player is more likely to win given the input \'state\'\n    described in the Feature Extractor model.\n    The output is a continuous variable, between -1 and 1. \n    """"""\n\n    def __init__(self, inplanes, outplanes):\n        super(ValueNet, self).__init__()\n        self.outplanes = outplanes\n        self.conv = nn.Conv2d(inplanes, 1, kernel_size=1)\n        self.bn = nn.BatchNorm2d(1)\n        self.fc1 = nn.Linear(outplanes - 1, 256)\n        self.fc2 = nn.Linear(256, 1)\n        \n\n    def forward(self, x):\n        """"""\n        x : feature maps extracted from the state\n        winning : probability of the current agent winning the game\n                  considering the actual state of the board\n        """"""\n \n        x = F.relu(self.bn(self.conv(x)))\n        x = x.view(-1, self.outplanes - 1)\n        x = F.relu(self.fc1(x))\n        winning = F.tanh(self.fc2(x))\n        return winning'"
