file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nimport os\nimport shutil\nimport sys\nfrom setuptools import setup, find_packages\n\nVERSION = \'0.0.1\'\n\nlong_description = """"""\nFast CPU/CUDA Scattering implementation\n\nCuPy/PyTorch CUDA and NumPy/PyTorch CUDA implementation\n""""""\n\nsetup_info = dict(\n    # Metadata\n    name=\'scatwave\',\n    version=VERSION,\n    author=\'Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko\',\n    author_email=\'edouard.oyallon@ens.fr, eugene.belilovsky@inria.fr, sergey.zagoruyko@enpc.fr\',\n    url=\'https://github.com/edouardoyallon/pyscatwave\',\n    description=\'Fast CPU/CUDA Scattering implementation\',\n    long_description=long_description,\n    license=\'BSD\',\n\n    # Package info\n    packages=find_packages(exclude=(\'test\',)),\n\n    zip_safe=True,\n\n    install_requires=[\n        \'torch\',\n        \'six\'\n    ]\n)\n\nsetup(**setup_info)\n'"
examples/mnist.py,15,"b'from tqdm import tqdm\nimport math\nimport torch\nimport torch.optim\nimport torchnet as tnt\nfrom torchvision.datasets.mnist import MNIST\nfrom torchnet.engine import Engine\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom scatwave.scattering import Scattering\n\n\ndef get_iterator(mode):\n    ds = MNIST(root=\'./\', download=True, train=mode)\n    data = getattr(ds, \'train_data\' if mode else \'test_data\')\n    labels = getattr(ds, \'train_labels\' if mode else \'test_labels\')\n    tds = tnt.dataset.TensorDataset([data, labels])\n    return tds.parallel(batch_size=128, num_workers=4, shuffle=mode)\n\n\ndef conv_init(ni, no, k):\n    return torch.Tensor(no, ni, k, k).normal_(0, 2/math.sqrt(ni*k*k))\n\n\ndef linear_init(ni, no):\n    return torch.Tensor(no, ni).normal_(0, 2/math.sqrt(ni))\n\n\ndef f(o, params, stats, mode):\n    o = F.batch_norm(o, running_mean=stats[\'bn.running_mean\'],\n                     running_var=stats[\'bn.running_var\'],\n                     weight=params[\'bn.weight\'],\n                     bias=params[\'bn.bias\'], training=mode)\n    o = F.conv2d(o, params[\'conv1.weight\'], params[\'conv1.bias\'])\n    o = F.relu(o)\n    o = o.view(o.size(0), -1)\n    o = F.linear(o, params[\'linear2.weight\'], params[\'linear2.bias\'])\n    o = F.relu(o)\n    o = F.linear(o, params[\'linear3.weight\'], params[\'linear3.bias\'])\n    return o\n\n\ndef main():\n    """"""Train a simple Hybrid Scattering + CNN model on MNIST.\n\n    Scattering features are normalized by batch normalization.\n    The model achieves 99.6% testing accuracy after 10 epochs.\n    """"""\n    meter_loss = tnt.meter.AverageValueMeter()\n    classerr = tnt.meter.ClassErrorMeter(accuracy=True)\n\n    scat = Scattering(M=28, N=28, J=2).cuda()\n    K = 81\n\n    params = {\n        \'conv1.weight\':     conv_init(K, 64, 1),\n        \'conv1.bias\':       torch.zeros(64),\n        \'bn.weight\':        torch.Tensor(K).uniform_(),\n        \'bn.bias\':          torch.zeros(K),\n        \'linear2.weight\':   linear_init(64*7*7, 512),\n        \'linear2.bias\':     torch.zeros(512),\n        \'linear3.weight\':   linear_init(512, 10),\n        \'linear3.bias\':     torch.zeros(10),\n    }\n\n    stats = {\'bn.running_mean\': torch.zeros(K).cuda(),\n             \'bn.running_var\': torch.ones(K).cuda()}\n\n    for k, v in list(params.items()):\n        params[k] = Variable(v.cuda(), requires_grad=True)\n\n    def h(sample):\n        x = scat(sample[0].float().cuda().unsqueeze(1) / 255.0).squeeze(1)\n        inputs = Variable(x)\n        targets = Variable(torch.LongTensor(sample[1]).cuda())\n        o = f(inputs, params, stats, sample[2])\n        return F.cross_entropy(o, targets), o\n\n    def on_sample(state):\n        state[\'sample\'].append(state[\'train\'])\n\n    def on_forward(state):\n        classerr.add(state[\'output\'].data,\n                     torch.LongTensor(state[\'sample\'][1]))\n        meter_loss.add(state[\'loss\'].item())\n\n    def on_start_epoch(state):\n        classerr.reset()\n        state[\'iterator\'] = tqdm(state[\'iterator\'])\n\n    def on_end_epoch(state):\n        print(\'Training accuracy:\', classerr.value())\n\n    def on_end(state):\n        print(\'Training\' if state[\'train\'] else \'Testing\', \'accuracy\')\n        print(classerr.value())\n        classerr.reset()\n\n    optimizer = torch.optim.SGD(list(params.values()), lr=0.01, momentum=0.9,\n                                weight_decay=0.0005)\n\n    engine = Engine()\n    engine.hooks[\'on_sample\'] = on_sample\n    engine.hooks[\'on_forward\'] = on_forward\n    engine.hooks[\'on_start_epoch\'] = on_start_epoch\n    engine.hooks[\'on_end_epoch\'] = on_end_epoch\n    engine.hooks[\'on_end\'] = on_end\n    print(\'Training:\')\n    engine.train(h, get_iterator(True), 10, optimizer)\n    print(\'Testing:\')\n    engine.test(h, get_iterator(False))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/reconstruction_exp.py,8,"b'\n""""""\nAuthors: Gabriel Huang\n""""""\n\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom tqdm import tqdm\n\nfrom scatwave.differentiable import scattering, cast, prepare_padding_size\nfrom scatwave.filters_bank import filters_bank\n\n\ndef reshape_scattering(scat):\n    \'\'\'\n    Reshape scattering for visualization purposes\n    \'\'\'\n    assert type(scat) == np.ndarray and len(scat.shape) == 3\n    n = int(np.ceil(np.sqrt(scat.shape[0])))\n\n    filler = np.zeros((n*n-scat.shape[0], scat.shape[1], scat.shape[2]))\n    padded = np.concatenate((scat, filler), axis=0)\n    padded = padded.reshape((n, n, scat.shape[1], scat.shape[2]))\n    padded = np.moveaxis(padded, 1, 2)\n    padded = np.reshape(padded, (n * scat.shape[1], n * scat.shape[2]))\n    return padded\n\n\nclass ScatteringTranscoder(object):\n    def __init__(self, M, N, J, cache=\'.cache\'):\n        \'\'\'\n        M : int\n            height to resize\n\n        N : int\n            width to resize\n\n        J : int\n            scale\n\n        cache : str or False\n            filename of cache for filter banks\n        \'\'\'\n        assert M == N, \'for now must have M=N\'\n\n        self.M = M\n        self.N = N\n        self.J = J\n        self.cache = cache\n        self.M_padded, self.N_padded = prepare_padding_size(M, N, J)\n\n        # Filter banks\n        self.cache_file = \'{}-Mpad-{}-Npad-{}-J-{}\'.format(\n            self.cache, self.M_padded, self.N_padded, self.J)\n        filters = filters_bank(\n            self.M_padded, self.N_padded, self.J, cache=self.cache_file)\n        self.Psi = filters[\'psi\']\n        self.Phi = [filters[\'phi\'][j] for j in range(self.J)]\n        self.Psi, self.Phi = cast(self.Psi, self.Phi, torch.cuda.FloatTensor)\n\n        # image preprocessor\n        self.preprocessor = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Scale(M),\n                transforms.CenterCrop(M),\n                transforms.ToTensor(),\n            ])\n\n    def process_img(self, img):\n        inputs = Variable(self.preprocessor(img).unsqueeze(0).cuda())\n        return inputs\n\n    def deprocess_img(self, img):\n        img = img.data.cpu().numpy()\n        img = np.moveaxis(img, 1, -1)  # move channel to end\n        return img\n\n    def encode_processed(self, processed_imgs):\n        \'\'\'\n        imgs : torch.Tensor\n            shape (batch, channel. height, width)\n        \'\'\'\n        # processed_imgs = torch.Tensor(processed_imgs).cuda()\n        assert (len(processed_imgs.size()) == 4\n                and processed_imgs.size()[1] in (1, 3)),\\\n            \'image shape must be (batch, channel, height, width)\'\n\n        S_imgs = scattering(processed_imgs, self.Psi, self.Phi, self.J)\n        return S_imgs\n\n    def decode(self, S_imgs, step=0.5, iterations=100):\n        \'\'\'\n        S_imgs : torch.Tensor\n            shape (batch, channel, scat_channel, scat_height, scat_width)\n        \'\'\'\n        assert len(S_imgs.size()) == 5 and S_imgs.size()[1] in (1, 3),\\\n            \'tensor shape must be (batch, channel, scat_channel, scat_height, scat_width)\'\n\n        noise = torch.Tensor(S_imgs.size()[0], S_imgs.size()[1], self.M, self.N).normal_(std=0.1).cuda()\n        reconstructed = Variable(noise, requires_grad=True)\n        abs_losses = []\n        rel_losses = []\n\n        optimizer = torch.optim.Adam([reconstructed], step)\n        iterator = tqdm(xrange(iterations))\n\n        try:\n            for i in iterator:\n\n                optimizer.zero_grad()\n                S_reconstructed = scattering(reconstructed, self.Psi, self.Phi, self.J)\n                loss = torch.abs(S_imgs - S_reconstructed).mean()\n                rel_loss = (loss / S_imgs.abs().mean()).data[0]\n                abs_losses.append(loss.data[0])\n                rel_losses.append(rel_loss)\n                loss.backward()\n                optimizer.step()\n\n                iterator.set_description(\'relative {:.4} absolute {:.4}\'.format(rel_loss, loss.data[0]))\n\n                \'\'\'\n                if i % 5 == 0 or i == iterations-1:\n                    I = (reconstructed.data.cpu()[0].permute(1,2,0).squeeze().numpy()).clip(0,1)\n                    I = Image.fromarray(np.uint8(I*255.0),\'YCbCr\').convert(\'RGB\')\n\n                    # Save file\n                    reconstructed_file = \'{}.reconstructed.jpg\'.format(base)\n                    print \'Saving to {}\'.format(reconstructed_file)\n                    misc.imsave(reconstructed_file, I)\n\n                    print(\'iteration%i\'%(i))\n                \'\'\'\n        except KeyboardInterrupt:\n            print \'Interrupted\'\n        return reconstructed, {\'abs_losses\': abs_losses, \'rel_losses\': rel_losses}\n'"
scatwave/FFT.py,7,"b'from __future__ import print_function\nimport torch\nfrom torch.autograd import Function\nfrom collections import defaultdict, namedtuple\nfrom skcuda import cublas, cufft\nimport numpy as np\n\n\nclass FFTcache(object):\n\n    def __init__(self):\n        self.fft_cache=defaultdict(lambda: None)\n\n    def buildCache(self, input, type):\n        k = input.ndimension() - 3\n        n = np.asarray([input.size(k), input.size(k+1)], np.int32)\n        batch = input.nelement() // (2*input.size(k) * input.size(k + 1))\n        idist = input.size(k) * input.size(k + 1)\n        istride = 1\n        ostride = istride\n        odist = idist\n        rank = 2\n        print(rank, n.ctypes.data, n.ctypes.data, istride, idist, n.ctypes.data, ostride, odist, type, batch)\n        plan = cufft.cufftPlanMany(rank, n.ctypes.data, n.ctypes.data, istride, idist, n.ctypes.data, ostride, odist, type, batch)\n        self.fft_cache[(input.size(),type,input.get_device())] = plan\n\n    def __del__(self):\n        for keys in self.fft_cache:\n            try:\n                cufft.cufftDestroy(self.fft_cache[keys])\n            except:\n                pass\n\n    def c2c(self, input, inverse=False):\n        assert input.is_contiguous()\n        output = input.new(input.size())\n        flag = cufft.CUFFT_INVERSE if inverse else cufft.CUFFT_FORWARD\n        ffttype = cufft.CUFFT_C2C if isinstance(input, torch.cuda.FloatTensor) else cufft.CUFFT_Z2Z\n        if (self.fft_cache[(input.size(), ffttype, input.get_device())] is None):\n            self.buildCache(input, ffttype)\n        cufft.cufftExecC2C(self.fft_cache[(input.size(), ffttype, input.get_device())],\n                           input.data_ptr(), output.data_ptr(), flag)\n        return output\n\n    def c2r(self, input):\n        output = input.new(input.size()[:-1])\n        if(self.fft_cache[(input.size(), cufft.CUFFT_C2R, input.get_device())] is None):\n            self.buildCache(input, cufft.CUFFT_C2R)\n        cufft.cufftExecC2R(self.fft_cache[(input.size(), cufft.CUFFT_C2R, input.get_device())],\n                           input.data_ptr(), output.data_ptr())\n        return output\n\n    def r2c(self, input):\n        output = input.new(input.size() + torch.Size((2,)))\n        if(self.fft_cache[(input.size(), cufft.CUFFT_R2C, input.get_device())] is None):\n            self.buildCache(input, cufft.CUFFT_R2C)\n        cufft.cufftExecR2C(self.fft_cache[(input.size(), cufft.CUFFT_R2C, input.get_device())],\n                           input.data_ptr(), output.data_ptr())\n        return output\n\n\nfft = FFTcache()\n\nclass FFT_C2C(torch.autograd.Function):\n\n    def forward(self, input):\n        return fft.c2c(input, inverse=False)\n\n    def backward(self, grad_output):\n        return fft.c2c(grad_output, inverse=True)\n\n\nclass FFT_iC2C(torch.autograd.Function):\n\n    def forward(self, input):\n        return fft.c2c(input, inverse=True)\n\n    def backward(self, grad_output):\n        return fft.c2c(grad_output, inverse=False)\n\nclass FFT_C2R(torch.autograd.Function):\n\n    def forward(self, input):\n        return fft.c2r(input)\n\n    def backward(self, grad_output):\n        return fft.r2c(grad_output)\n\n\nclass FFT_R2C(torch.autograd.Function):\n\n    def forward(self, input):\n        return fft.r2c(input)\n\n    def backward(self, grad_output):\n        return fft.c2r(grad_output)\n\n\ndef fft_c2c(input):\n    return FFT_C2C()(input)\n\ndef ifft_c2c(input):\n    return FFT_iC2C()(input)\n\ndef ifft_c2r(input):\n    # return FFT_C2R()(input)\n    return FFT_iC2C()(input).select(input.dim()-1, 0)\n\ndef fft_r2c(input):\n    return FFT_R2C()(input)\n'"
scatwave/__init__.py,0,"b'""""""\nAuthors: Eugene Belilovsky, Edouard Oyallon and Sergey Zagoruyko\nAll rights reserved, 2017.\n""""""\n__all__ = [\'Scattering\']\n\n\nfrom .scattering import Scattering\nfrom . import utils\n'"
scatwave/differentiable.py,5,"b""from __future__ import print_function\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom .FFT import fft_c2c, ifft_c2r, ifft_c2c\n\n\ndef prepare_padding_size(M, N, J):\n    M_padded = ((M + 2 ** (J))//2**J+1)*2**J\n    N_padded = ((N + 2 ** (J))//2**J+1)*2**J\n\n    return M_padded, N_padded\n\n\ndef cast(Psi, Phi, _type):\n    for key, item in enumerate(Psi):\n        for key2, item2 in Psi[key].items():\n            if torch.is_tensor(item2):\n                Psi[key][key2] = Variable(item2.type(_type))\n    Phi = [Variable(v.type(_type)) for v in Phi]\n    return Psi, Phi\n\n\ndef pad(input, J):\n    out_ = F.pad(input, (2**J,) * 4, mode='reflect').unsqueeze(input.dim())\n    return torch.cat([out_, Variable(input.data.new(out_.size()).zero_())], 4)\n\n\ndef unpad(input):\n    return input[..., 1:-1, 1:-1]\n\n\ndef cdgmm(A, B):\n    C = Variable(A.data.new(A.size()))\n\n    A_r = A[..., 0].contiguous().view(-1, A.size(-2)*A.size(-3))\n    A_i = A[..., 1].contiguous().view(-1, A.size(-2)*A.size(-3))\n\n    B_r = B[...,0].contiguous().view(B.size(-2)*B.size(-3)).unsqueeze(0).expand_as(A_i)\n    B_i = B[..., 1].contiguous().view(B.size(-2)*B.size(-3)).unsqueeze(0).expand_as(A_r)\n\n    C[..., 0] = (A_r * B_r - A_i * B_i).view(A.shape[:-1])\n    C[..., 1] = (A_r * B_i + A_i * B_r).view(A.shape[:-1])\n    return C\n\n\ndef periodize(input, k):\n    return input.view(input.size(0), input.size(1),\n                      k, input.size(2) // k,\n                      k, input.size(3) // k,\n                      2).mean(4).squeeze(4).mean(2).squeeze(2)\n\n\ndef modulus(input):\n    norm = input.norm(p=2, dim=-1, keepdim=True)\n    return torch.cat([norm, Variable(norm.data.new(norm.size()).zero_())], -1)\n\n\ndef scattering(input, psi, phi, J):\n    M, N = input.size(-2), input.size(-1)\n    M_padded, N_padded = prepare_padding_size(M, N, J)\n    S = Variable(input.data.new(input.size(0),\n                                input.size(1),\n                                1 + 8*J + 8*8*J*(J - 1) // 2,\n                                M_padded//(2**J)-2,\n                                N_padded//(2**J)-2))\n    U_r = pad(input, J)\n    U_0_c = fft_c2c(U_r)\n\n    # First low pass filter\n    U_1_c = periodize(cdgmm(U_0_c, phi[0]), k=2**J)\n\n    U_J_r = ifft_c2r(U_1_c)\n\n    n = 0\n    S[..., n, :, :] = unpad(U_J_r)\n    n = n + 1\n\n    for n1 in range(len(psi)):\n        j1 = psi[n1]['j']\n        U_1_c = cdgmm(U_0_c, psi[n1][0])\n        if(j1 > 0):\n            U_1_c = periodize(U_1_c, k=2 ** j1)\n        U_1_c = fft_c2c(modulus(ifft_c2c(U_1_c)))\n\n        # Second low pass filter\n        U_2_c = periodize(cdgmm(U_1_c, phi[j1]), k=2**(J-j1))\n        U_J_r = ifft_c2r(U_2_c)\n        S[..., n, :, :] = unpad(U_J_r)\n        n = n + 1\n\n        for n2 in range(len(psi)):\n            j2 = psi[n2]['j']\n            if(j1 < j2):\n                U_2_c = periodize(cdgmm(U_1_c, psi[n2][j1]), k=2 ** (j2-j1))\n                U_2_c = fft_c2c(modulus(ifft_c2c(U_2_c)))\n\n                # Third low pass filter\n                U_2_c = periodize(cdgmm(U_2_c, phi[j2]), k=2 ** (J-j2))\n                U_J_r = ifft_c2r(U_2_c)\n\n                S[..., n, :, :] = unpad(U_J_r)\n                n = n + 1\n    return S\n\n\n\n"""
scatwave/filters_bank.py,2,"b'""""""\nAuthors: Eugene Belilovsky, Edouard Oyallon and Sergey Zagoruyko\nAll rights reserved, 2017.\n""""""\n\n__all__ = [\'filters_bank\']\n\nimport torch\nimport numpy as np\nimport scipy.fftpack as fft\n\n\n\n\ndef filters_bank(M, N, J, L=8):\n    filters = {}\n    filters[\'psi\'] = []\n\n\n    offset_unpad = 0\n    for j in range(J):\n        for theta in range(L):\n            psi = {}\n            psi[\'j\'] = j\n            psi[\'theta\'] = theta\n            psi_signal = morlet_2d(M, N, 0.8 * 2**j, (int(L-L/2-1)-theta) * np.pi / L, 3.0 / 4.0 * np.pi /2**j,offset=offset_unpad) # The 5 is here just to match the LUA implementation :)\n            psi_signal_fourier = fft.fft2(psi_signal)\n            for res in range(j + 1):\n                psi_signal_fourier_res = crop_freq(psi_signal_fourier, res)\n                psi[res]=torch.FloatTensor(np.stack((np.real(psi_signal_fourier_res), np.imag(psi_signal_fourier_res)), axis=2))\n                # Normalization to avoid doing it with the FFT!\n                psi[res].div_(M*N// 2**(2*j))\n            filters[\'psi\'].append(psi)\n\n    filters[\'phi\'] = {}\n    phi_signal = gabor_2d(M, N, 0.8 * 2**(J-1), 0, 0, offset=offset_unpad)\n    phi_signal_fourier = fft.fft2(phi_signal)\n    filters[\'phi\'][\'j\'] = J\n    for res in range(J):\n        phi_signal_fourier_res = crop_freq(phi_signal_fourier, res)\n        filters[\'phi\'][res]=torch.FloatTensor(np.stack((np.real(phi_signal_fourier_res), np.imag(phi_signal_fourier_res)), axis=2))\n        filters[\'phi\'][res].div_(M*N // 2 ** (2 * J))\n\n    return filters\n\n\ndef crop_freq(x, res):\n    M = x.shape[0]\n    N = x.shape[1]\n\n    crop = np.zeros((M // 2 ** res, N // 2 ** res), np.complex64)\n\n    mask = np.ones(x.shape, np.float32)\n    len_x = int(M * (1 - 2 ** (-res)))\n    start_x = int(M * 2 ** (-res - 1))\n    len_y = int(N * (1 - 2 ** (-res)))\n    start_y = int(N * 2 ** (-res - 1))\n    mask[start_x:start_x + len_x,:] = 0\n    mask[:, start_y:start_y + len_y] = 0\n    x = np.multiply(x,mask)\n\n    for k in range(int(M / 2 ** res)):\n        for l in range(int(N / 2 ** res)):\n            for i in range(int(2 ** res)):\n                for j in range(int(2 ** res)):\n                    crop[k, l] += x[k + i * int(M / 2 ** res), l + j * int(N / 2 ** res)]\n\n    return crop\n\n\ndef morlet_2d(M, N, sigma, theta, xi, slant=0.5, offset=0, fft_shift=None):\n    """""" This function generated a morlet""""""\n    wv = gabor_2d(M, N, sigma, theta, xi, slant, offset, fft_shift)\n    wv_modulus = gabor_2d(M, N, sigma, theta, 0, slant, offset, fft_shift)\n    K = np.sum(wv) / np.sum(wv_modulus)\n\n    mor = wv - K * wv_modulus\n    return mor\n\n\ndef gabor_2d(M, N, sigma, theta, xi, slant=1.0, offset=0, fft_shift=None):\n    gab = np.zeros((M, N), np.complex64)\n    R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]], np.float32)\n    R_inv = np.array([[np.cos(theta), np.sin(theta)], [-np.sin(theta), np.cos(theta)]], np.float32)\n    D = np.array([[1, 0], [0, slant * slant]])\n    curv = np.dot(R, np.dot(D, R_inv)) / ( 2 * sigma * sigma)\n\n    for ex in [-2, -1, 0, 1, 2]:\n        for ey in [-2, -1, 0, 1, 2]:\n            [xx, yy] = np.mgrid[offset + ex * M:offset + M + ex * M, offset + ey * N:offset + N + ey * N]\n            arg = -(curv[0, 0] * np.multiply(xx, xx) + (curv[0, 1] + curv[1, 0]) * np.multiply(xx, yy) + curv[\n                1, 1] * np.multiply(yy, yy)) + 1.j * (xx * xi * np.cos(theta) + yy * xi * np.sin(theta))\n            gab = gab + np.exp(arg)\n\n    norm_factor = (2 * 3.1415 * sigma * sigma / slant)\n    gab = gab / norm_factor\n\n    if (fft_shift):\n        gab = np.fft.fftshift(gab, axes=(0, 1))\n    return gab\n'"
scatwave/scattering.py,7,"b'""""""\nAuthors: Eugene Belilovsky, Edouard Oyallon and Sergey Zagoruyko\nAll rights reserved, 2017.\n""""""\n\n__all__ = [\'Scattering\']\n\nimport warnings\nimport torch\nfrom .utils import cdgmm, Modulus, Periodize, Fft\nfrom .filters_bank import filters_bank\nfrom torch.nn import ReflectionPad2d as pad_function\n\nclass Scattering(object):\n    """"""Scattering module.\n\n    Runs scattering on an input image in NCHW format\n\n    Input args:\n        M, N: input image size\n        J: number of layers\n        pre_pad: if set to True, module expect pre-padded images\n        jit: compile kernels on the fly for speed\n    """"""\n    def __init__(self, M, N, J, pre_pad=False, jit=True):\n        super(Scattering, self).__init__()\n        self.M, self.N, self.J = M, N, J\n        self.pre_pad = pre_pad\n        self.jit = jit\n        self.fft = Fft()\n        self.modulus = Modulus(jit=jit)\n        self.periodize = Periodize(jit=jit)\n\n        self._prepare_padding_size([1, 1, M, N])\n\n        self.padding_module = pad_function(2**J)\n\n        # Create the filters\n        filters = filters_bank(self.M_padded, self.N_padded, J)\n\n        self.Psi = filters[\'psi\']\n        self.Phi = [filters[\'phi\'][j] for j in range(J)]\n\n    def _type(self, _type):\n        for key, item in enumerate(self.Psi):\n            for key2, item2 in self.Psi[key].items():\n                if torch.is_tensor(item2):\n                    self.Psi[key][key2] = item2.type(_type)\n        self.Phi = [v.type(_type) for v in self.Phi]\n        self.padding_module.type(str(_type).split(\'\\\'\')[1])\n        return self\n\n    def cuda(self):\n        return self._type(torch.cuda.FloatTensor)\n\n    def cpu(self):\n        return self._type(torch.FloatTensor)\n\n    def _prepare_padding_size(self, s):\n        M = s[-2]\n        N = s[-1]\n\n        self.M_padded = ((M + 2 ** (self.J))//2**self.J+1)*2**self.J\n        self.N_padded = ((N + 2 ** (self.J))//2**self.J+1)*2**self.J\n\n        if self.pre_pad:\n            warnings.warn(\'Make sure you padded the input before to feed it!\', RuntimeWarning, stacklevel=2)\n\n        s[-2] = self.M_padded\n        s[-1] = self.N_padded\n        self.padded_size_batch = torch.Size([a for a in s])\n\n    # This function copies and view the real to complex\n    def _pad(self, input):\n        if(self.pre_pad):\n            output = input.new(input.size(0), input.size(1), input.size(2), input.size(3), 2).fill_(0)\n            output.narrow(output.ndimension()-1, 0, 1).copy_(input)\n        else:\n            out_ = self.padding_module(input)\n            output = input.new(out_.size(0), out_.size(1), out_.size(2), out_.size(3), 2).fill_(0)\n            output.narrow(4, 0, 1).copy_(out_.unsqueeze(4))\n        return output\n\n    def _unpad(self, in_):\n        return in_[..., 1:-1, 1:-1]\n\n    def forward(self, input):\n        if not torch.is_tensor(input):\n            raise(TypeError(\'The input should be a torch.cuda.FloatTensor, a torch.FloatTensor or a torch.DoubleTensor\'))\n\n        if (not input.is_contiguous()):\n            raise (RuntimeError(\'Tensor must be contiguous!\'))\n\n        if((input.size(-1)!=self.N or input.size(-2)!=self.M) and not self.pre_pad):\n            raise (RuntimeError(\'Tensor must be of spatial size (%i,%i)!\'%(self.M,self.N)))\n\n        if ((input.size(-1) != self.N_padded or input.size(-2) != self.M_padded) and self.pre_pad):\n            raise (RuntimeError(\'Padded tensor must be of spatial size (%i,%i)!\' % (self.M_padded, self.N_padded)))\n\n        if (input.dim() != 4):\n            raise (RuntimeError(\'Input tensor must be 4D\'))\n\n        J = self.J\n        phi = self.Phi\n        psi = self.Psi\n        n = 0\n\n        fft = self.fft\n        periodize = self.periodize\n        modulus = self.modulus\n        pad = self._pad\n        unpad = self._unpad\n\n        S = input.new(input.size(0),\n                      input.size(1),\n                      1 + 8*J + 8*8*J*(J - 1) // 2,\n                      self.M_padded//(2**J)-2,\n                      self.N_padded//(2**J)-2)\n        U_r = pad(input)\n        U_0_c = fft(U_r, \'C2C\')  # We trick here with U_r and U_2_c\n\n        # First low pass filter\n        U_1_c = periodize(cdgmm(U_0_c, phi[0], jit=self.jit), k=2**J)\n\n        U_J_r = fft(U_1_c, \'C2R\')\n\n        S[..., n, :, :].copy_(unpad(U_J_r))\n        n = n + 1\n\n        for n1 in range(len(psi)):\n            j1 = psi[n1][\'j\']\n            U_1_c = cdgmm(U_0_c, psi[n1][0], jit=self.jit)\n            if(j1 > 0):\n                U_1_c = periodize(U_1_c, k=2 ** j1)\n            fft(U_1_c, \'C2C\', inverse=True, inplace=True)\n            U_1_c = fft(modulus(U_1_c), \'C2C\')\n\n            # Second low pass filter\n            U_2_c = periodize(cdgmm(U_1_c, phi[j1], jit=self.jit), k=2**(J-j1))\n            U_J_r = fft(U_2_c, \'C2R\')\n            S[..., n, :, :].copy_(unpad(U_J_r))\n            n = n + 1\n\n            for n2 in range(len(psi)):\n                j2 = psi[n2][\'j\']\n                if(j1 < j2):\n                    U_2_c = periodize(cdgmm(U_1_c, psi[n2][j1], jit=self.jit), k=2 ** (j2-j1))\n                    fft(U_2_c, \'C2C\', inverse=True, inplace=True)\n                    U_2_c = fft(modulus(U_2_c), \'C2C\')\n\n                    # Third low pass filter\n                    U_2_c = periodize(cdgmm(U_2_c, phi[j2], jit=self.jit), k=2 ** (J-j2))\n                    U_J_r = fft(U_2_c, \'C2R\')\n\n                    S[..., n, :, :].copy_(unpad(U_J_r))\n                    n = n + 1\n\n        return S\n\n    def __call__(self, input):\n        return self.forward(input)\n'"
scatwave/utils.py,17,"b'""""""\nAuthors: Eugene Belilovsky, Edouard Oyallon and Sergey Zagoruyko\nAll rights reserved, 2017.\n""""""\nfrom collections import defaultdict, namedtuple\n\nimport torch\nfrom skcuda import cublas, cufft\nfrom pynvrtc.compiler import Program\nimport numpy as np\nfrom cupy.cuda.function import Module\nfrom cupy.cuda import device\nfrom string import Template\n\n\nStream = namedtuple(\'Stream\', [\'ptr\'])\n\n\ndef getDtype(t):\n    if isinstance(t, torch.cuda.FloatTensor):\n        return \'float\'\n    elif isinstance(t, torch.cuda.DoubleTensor):\n        return \'double\'\n\n\ndef get_compute_arch(t):\n    return \'compute_%s\' % device.Device().compute_capability\n\n\ndef iscomplex(input):\n    return input.size(-1) == 2\n\n\nclass Periodize(object):\n    """"""This class builds a wrapper to the periodiziation kernels and cache them.\n        """"""\n    def __init__(self, jit=True):\n        self.periodize_cache = defaultdict(lambda: None)\n        self.block = (32, 32, 1)\n        self.jit = jit\n\n    def GET_BLOCKS(self, N, threads):\n        return (N + threads - 1) // threads\n\n    def __call__(self, input, k):\n        out = input.new(input.size(0), input.size(1), input.size(2) // k, input.size(3) // k, 2)\n\n        if not self.jit or isinstance(input, (torch.FloatTensor, torch.DoubleTensor)):\n            y = input.view(input.size(0), input.size(1),\n                           input.size(2)//out.size(2), out.size(2),\n                           input.size(3)//out.size(3), out.size(3),\n                           2)\n\n            out = y.mean(4).squeeze(4).mean(2).squeeze(2)\n            return out\n\n        if not iscomplex(input):\n            raise (TypeError(\'The input and outputs should be complex\'))\n\n        input = input.contiguous()\n\n        if (self.periodize_cache[(input.size(), out.size(), input.get_device())] is None):\n            kernel = \'\'\'\n            #define NW ${W} / ${k}\n            #define NH ${H} / ${k}\n            extern ""C""\n            __global__ void periodize(const ${Dtype}2 *input, ${Dtype}2 *output)\n            {\n              int tx = blockIdx.x * blockDim.x + threadIdx.x;\n              int ty = blockIdx.y * blockDim.y + threadIdx.y;\n              int tz = blockIdx.z * blockDim.z + threadIdx.z;\n              if(tx >= NW || ty >= NH || tz >= ${B})\n                return;\n              input += tz * ${H} * ${W} + ty * ${W} + tx;\n              ${Dtype}2 res = make_${Dtype}2(0.f, 0.f);\n              for (int j=0; j<${k}; ++j)\n                for (int i=0; i<${k}; ++i)\n                {\n                  const ${Dtype}2 &c = input[j * NH * ${W} + i * NW];\n                  res.x += c.x;\n                  res.y += c.y;\n                }\n              res.x /= ${k} * ${k};\n              res.y /= ${k} * ${k};\n              output[tz * NH * NW + ty * NW + tx] = res;\n            }\n            \'\'\'\n            B = input.nelement() // (2*input.size(-2) * input.size(-3))\n            W = input.size(-2)\n            H = input.size(-3)\n            k = input.size(-2) // out.size(-2)\n            kernel = Template(kernel).substitute(B=B, H=H, W=W, k=k, Dtype=getDtype(input))\n            name = str(input.get_device())+\'-\'+str(B)+\'-\'+str(k)+\'-\'+str(H)+\'-\'+str(W)+\'-periodize.cu\'\n            print(name)\n            prog = Program(kernel, name)\n            ptx = prog.compile([\'-arch=\'+get_compute_arch(input)])\n            module = Module()\n            module.load(bytes(ptx.encode()))\n            self.periodize_cache[(input.size(), out.size(), input.get_device())] = module\n        grid = (self.GET_BLOCKS(out.size(-3), self.block[0]),\n                self.GET_BLOCKS(out.size(-2), self.block[1]),\n                self.GET_BLOCKS(out.nelement() // (2*out.size(-2) * out.size(-3)), self.block[2]))\n        periodize = self.periodize_cache[(input.size(), out.size(), input.get_device())].get_function(\'periodize\')\n        periodize(grid=grid, block=self.block, args=[input.data_ptr(), out.data_ptr()],\n                  stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n        return out\n\n\nclass Modulus(object):\n    """"""This class builds a wrapper to the moduli kernels and cache them.\n        """"""\n    def __init__(self, jit=True):\n        self.modulus_cache = defaultdict(lambda: None)\n        self.CUDA_NUM_THREADS = 1024\n        self.jit = jit\n\n    def GET_BLOCKS(self, N):\n        return (N + self.CUDA_NUM_THREADS - 1) // self.CUDA_NUM_THREADS\n\n    def __call__(self, input):\n        if not self.jit or not isinstance(input, torch.cuda.FloatTensor):\n            norm = input.norm(2, input.dim() - 1)\n            return torch.stack([norm, norm.new(norm.size()).zero_()], -1)\n\n        out = input.new(input.size())\n        input = input.contiguous()\n\n        if not iscomplex(input):\n            raise TypeError(\'The input and outputs should be complex\')\n\n        if (self.modulus_cache[input.get_device()] is None):\n            kernel = """"""\n            extern ""C""\n            __global__ void abs_complex_value(const float * x, float2 * z, int n)\n            {\n                int i = blockIdx.x * blockDim.x + threadIdx.x;\n            if (i >= n)\n                return;\n            z[i] = make_float2(normf(2, x + 2*i), 0);\n\n            }\n            """"""\n            print(\'modulus.cu\')\n            prog = Program(kernel, \'modulus.cu\')\n            ptx = prog.compile([\'-arch=\'+get_compute_arch(input)])\n            module = Module()\n            module.load(bytes(ptx.encode()))\n            self.modulus_cache[input.get_device()] = module\n        fabs = self.modulus_cache[input.get_device()].get_function(\'abs_complex_value\')\n        fabs(grid=(self.GET_BLOCKS(int(out.nelement())//2), 1, 1),\n             block=(self.CUDA_NUM_THREADS, 1, 1),\n             args=[input.data_ptr(), out.data_ptr(), out.numel() // 2],\n             stream=Stream(ptr=torch.cuda.current_stream().cuda_stream))\n        return out\n\n\nclass Fft(object):\n    """"""This class builds a wrapper to the FFTs kernels and cache them.\n\n    As a try, the library will purely work with complex data. The FFTS are UNORMALIZED.\n        """"""\n\n    def __init__(self):\n        self.fft_cache = defaultdict(lambda: None)\n\n    def buildCache(self, input, type):\n        k = input.ndimension() - 3\n        n = np.asarray([input.size(k), input.size(k+1)], np.int32)\n        batch = input.nelement() // (2*input.size(k) * input.size(k + 1))\n        idist = input.size(k) * input.size(k + 1)\n        istride = 1\n        ostride = istride\n        odist = idist\n        rank = 2\n        plan = cufft.cufftPlanMany(rank, n.ctypes.data, n.ctypes.data, istride,\n                                   idist, n.ctypes.data, ostride, odist, type, batch)\n        self.fft_cache[(input.size(), type, input.get_device())] = plan\n\n    def __del__(self):\n        for keys in self.fft_cache:\n            try:\n                cufft.cufftDestroy(self.fft_cache[keys])\n            except:\n                pass\n\n    def __call__(self, input, direction=\'C2C\', inplace=False, inverse=False):\n        if direction == \'C2R\':\n            inverse = True\n\n        if not isinstance(input, torch.cuda.FloatTensor):\n            if not isinstance(input, (torch.FloatTensor, torch.DoubleTensor)):\n                raise(TypeError(\'The input should be a torch.cuda.FloatTensor, \\\n                                torch.FloatTensor or a torch.DoubleTensor\'))\n            else:\n                input_np = input[..., 0].numpy() + 1.0j * input[..., 1].numpy()\n                f = lambda x: np.stack((np.real(x), np.imag(x)), axis=len(x.shape))\n                out_type = input.numpy().dtype\n\n                if direction == \'C2R\':\n                    out = np.real(np.fft.ifft2(input_np)).astype(out_type)*input.size(-2)*input.size(-3)\n                    return torch.from_numpy(out)\n\n                if inplace:\n                    if inverse:\n                        out = f(np.fft.ifft2(input_np)).astype(out_type)*input.size(-2)*input.size(-3)\n                    else:\n                        out = f(np.fft.fft2(input_np)).astype(out_type)\n                    input.copy_(torch.from_numpy(out))\n                    return\n                else:\n                    if inverse:\n                        out = f(np.fft.ifft2(input_np)).astype(out_type)*input.size(-2)*input.size(-3)\n                    else:\n                        out = f(np.fft.fft2(input_np)).astype(out_type)\n                    return torch.from_numpy(out)\n\n        if not iscomplex(input):\n            raise(TypeError(\'The input should be complex (e.g. last dimension is 2)\'))\n\n        if (not input.is_contiguous()):\n            raise (RuntimeError(\'Tensors must be contiguous!\'))\n\n        if direction == \'C2R\':\n            output = input.new(input.size()[:-1])\n            if(self.fft_cache[(input.size(), cufft.CUFFT_C2R, input.get_device())] is None):\n                self.buildCache(input, cufft.CUFFT_C2R)\n            cufft.cufftExecC2R(self.fft_cache[(input.size(), cufft.CUFFT_C2R, input.get_device())],\n                               input.data_ptr(), output.data_ptr())\n            return output\n        elif direction == \'C2C\':\n            output = input.new(input.size()) if not inplace else input\n            flag = cufft.CUFFT_INVERSE if inverse else cufft.CUFFT_FORWARD\n            if (self.fft_cache[(input.size(), cufft.CUFFT_C2C, input.get_device())] is None):\n                self.buildCache(input, cufft.CUFFT_C2C)\n            cufft.cufftExecC2C(self.fft_cache[(input.size(), cufft.CUFFT_C2C, input.get_device())],\n                               input.data_ptr(), output.data_ptr(), flag)\n            return output\n\n\ndef cdgmm(A, B, jit=True, inplace=False):\n    """"""This function uses the C-wrapper to use cuBLAS.\n        """"""\n    A, B = A.contiguous(), B.contiguous()\n\n    if A.size()[-3:] != B.size():\n        raise RuntimeError(\'The filters are not compatible for multiplication!\')\n\n    if not iscomplex(A) or not iscomplex(B):\n        raise TypeError(\'The input, filter and output should be complex\')\n\n    if B.ndimension() != 3:\n        raise RuntimeError(\'The filters must be simply a complex array!\')\n\n    if type(A) is not type(B):\n        raise RuntimeError(\'A and B should be same type!\')\n\n    if not jit or isinstance(A, (torch.FloatTensor, torch.DoubleTensor)):\n        C = A.new(A.size())\n\n        A_r = A[..., 0]\n        A_i = A[..., 1]\n\n        B_r = B[..., 0].unsqueeze(0)\n        B_i = B[..., 1].unsqueeze(0)\n\n        C[..., 0].copy_(A_r * B_r - A_i * B_i)\n        C[..., 1].copy_(A_r * B_i + A_i * B_r)\n\n        # faster if B is actually real\n        #B[...,1] = B[...,0]\n        #C = A * B.unsqueeze(0).expand_as(A)\n        return C if not inplace else A.copy_(C)\n    else:\n        C = A.new(A.size()) if not inplace else A\n        m, n = B.nelement() // 2, A.nelement() // B.nelement()\n        lda = m\n        ldc = m\n        incx = 1\n        handle = torch.cuda.current_blas_handle()\n        stream = torch.cuda.current_stream()._as_parameter_\n        cublas.cublasSetStream(handle, stream)\n        cublas.cublasCdgmm(handle, \'l\', m, n, A.data_ptr(), lda, B.data_ptr(), incx, C.data_ptr(), ldc)\n        return C\n'"
test/test_scattering.py,12,"b'"""""" This script will test the submodules used by the scattering module""""""\n\nimport torch\nimport unittest\nimport torch_testing as tt\nfrom scatwave.scattering import Scattering\nfrom scatwave import utils as sl\n\nclass TestScattering(unittest.TestCase):\n    def testFFTCentralFreq(self):\n        # Checked the 0 frequency\n        for gpu in [True, False]:\n            x = torch.FloatTensor(10, 10, 2).fill_(0)\n            x.narrow(2, 0, 1).fill_(1)\n            if gpu:\n                x = x.cuda()\n\n            a = x.sum()\n            fft = sl.Fft()\n            fft(x, inplace=True)\n            b = x[0,0,0]\n            tt.assert_almost_equal(a.cpu(), b.cpu(), decimal=6)\n\n    def testFFTCentralFreqBatch(self):\n        # Same for batches\n        for gpu in [True, False]:\n            x = torch.FloatTensor(4,10,10,2).fill_(0)\n            x.narrow(3,0,1).fill_(1)\n            if gpu:\n                x = x.cuda()\n\n            a = x.sum()\n            fft = sl.Fft()\n            fft(x, inplace=True)\n            c = x[:,0,0,0].sum()\n            tt.assert_equal(a.cpu(), c.cpu())\n\n    def testFFTUnormalized(self):\n        # Check for a random tensor:\n        x = torch.FloatTensor(25, 17, 3, 2).bernoulli_(0.5)\n        for gpu in [True, False]:\n\n            if gpu:\n                x=x.cuda()\n            else:\n                x=x.cpu()\n            x.narrow(3,1,1).fill_(0)\n\n            fft=sl.Fft()\n            y = fft(x)\n            z = fft(y, direction=\'C2R\')\n\n            z /= 17*3 # FFTs are unnormalized\n\n\n            tt.assert_allclose(x.select(3,0).cpu(), z.cpu(), atol=1e-6)\n\n\n\n\n    # Checkked the modulus\n    def testModulus(self):\n        for jit in [True, False]:\n            modulus = sl.Modulus(jit=jit)\n            x = torch.cuda.FloatTensor(100,10,4,2).copy_(torch.rand(100,10,4,2))\n            y = modulus(x)\n            u = torch.squeeze(torch.sqrt(torch.sum(x * x, 3)))\n            v = y[..., 0]\n\n            tt.assert_allclose(u.cpu(), v.cpu(), atol=1e-6)\n\n\n    def testPeriodization(self):\n        for jit in [True, False]:\n            x = torch.rand(100, 1, 128, 128, 2).cuda().double()\n            y = torch.zeros(100, 1, 8, 8, 2).cuda().double()\n\n            for i in range(8):\n                for j in range(8):\n                    for m in range(16):\n                        for n in range(16):\n                            y[...,i,j,:] += x[...,i+m*8,j+n*8,:]\n\n            y = y / (16*16)\n\n            periodize = sl.Periodize(jit=jit)\n\n            z = periodize(x, k=16)\n            tt.assert_allclose(y.cpu(), z.cpu(), atol=1e-8)\n\n            z = periodize(x.cpu(), k=16)\n            tt.assert_allclose(y.cpu(), z, atol=1e-8)\n\n\n    # Check the CUBLAS routines\n    def testCublas(self):\n        for jit in [True, False]:\n            x = torch.rand(100,128,128,2).cuda()\n            filter = torch.rand(128,128,2).cuda()\n            filter[..., 1] = 0\n            y = torch.ones(100,128,128,2).cuda()\n            z = torch.Tensor(100,128,128,2).cuda()\n\n            for i in range(100):\n                y[i,:,:,0]=x[i,:,:,0] * filter[:,:,0]-x[i,:,:,1] * filter[:,:,1]\n                y[i, :, :, 1] = x[i, :, :, 1] * filter[:, :, 0] + x[i, :, :, 0] *filter[:, :, 1]\n            z = sl.cdgmm(x, filter, jit=jit)\n\n            tt.assert_allclose(y.cpu(), z.cpu(), atol=1e-6)\n\n    def testScattering(self):\n        data = torch.load(\'test/test_data.pt\')\n        x = data[\'x\']\n        S = data[\'S\']\n        scat = Scattering(128, 128, 4, pre_pad=False,jit=True)\n        scat.cuda()\n        x = x.cuda()\n        S = S.cuda()\n        tt.assert_allclose(S.cpu(), scat(x).cpu(), atol=1e-6)\n\n        scat = Scattering(128, 128, 4, pre_pad=False, jit=False)\n        Sg = []\n        Sc = []\n        for gpu in [True, False]:\n            if gpu:\n                x = x.cuda()\n                scat.cuda()\n                Sg = scat(x)\n            else:\n                x = x.cpu()\n                scat.cpu()\n                Sc = scat(x)\n        """"""there are huge round off errors with fftw, numpy fft, cufft...\n        and the kernels of periodization. We do not wish to play with that as it is meaningless.""""""\n        tt.assert_allclose(Sg.cpu(), Sc.cpu(), atol=1e-1)\n\n\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
