file_path,api_count,code
generate.py,3,"b'#!/usr/bin/env python3\n\n""""""\nMain evaluation script\n""""""\n\nimport os\nimport os.path as osp\nimport logging\nimport time\nimport pickle\nimport json\nimport random\nimport numpy as np\nimport gc\nfrom nmt.params import parse_eval_params, set_env\n\n\ndef generate(params):\n    jobname = params[\'modelname\'] + \'_eval\'\n    set_env(jobname, params[\'gpu_id\'])\n    import torch\n    from nmt.loader import ReadData\n    import nmt.models.setup as ms\n    from nmt.models.evaluate import sample_model\n    # Data loading:\n    logger = logging.getLogger(jobname)\n    logger.info(\'Reading data ...\')\n    if params[\'read_length\']:\n        logger.warn(\'Max sequence length for loader: %d\', params[\'read_length\'])\n        params[\'data\'][\'max_src_length\'] = params[\'read_length\']\n        params[\'data\'][\'max_trg_length\'] = params[\'read_length\']\n        evaldir = \'%s/evaluations/%s_%d\' % (params[\'modelname\'], params[\'split\'], params[\'read_length\'])\n    else:\n        evaldir = \'%s/evaluations/%s\' % (params[\'modelname\'], params[\'split\'])\n\n    if not osp.exists(evaldir):\n        os.makedirs(evaldir)\n\n    src_loader, trg_loader = ReadData(params[\'data\'], jobname)\n    src_vocab_size = src_loader.get_vocab_size()\n    trg_vocab_size = trg_loader.get_vocab_size()\n    trg_specials = {\'EOS\': trg_loader.eos,\n                    \'BOS\': trg_loader.bos,\n                    \'PAD\': trg_loader.pad\n                    }\n    # reproducibility:\n    seed = params[\'optim\'][\'seed\']\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n    model = ms.build_model(jobname, params, src_vocab_size,\n                           trg_vocab_size, trg_specials)\n\n\n    # load model\'s weights\n    if not params[\'last\']:\n        flag = \'-best\'\n    else:\n        flag = """"\n    logger.info(\'Loading model with flag: %s\', flag)\n    saved_state = torch.load(osp.join(params[\'modelname\'], \'model%s.pth\' % flag))\n    model.load_state_dict(\n        torch.load(osp.join(\n            params[\'modelname\'], \'model%s.pth\' % flag)\n        )\n    )\n    model = model.cuda()\n    model.eval()\n    eval_kwargs = {\'split\': params[\'split\'],\n                   \'batch_size\': params[\'batch_size\'],\n                   \'beam_size\': params[\'beam_size\'],\n                   # \'beam_size\': 1,\n                   \'max_length_a\': params[\'max_length_a\'],\n                   \'max_length_b\': params[\'max_length_b\'],\n                   \'verbose\': params[\'verbose\'],\n                   \'block_ngram_repeat\': params[\'block_ngram_repeat\'],\n                   \'stepwise_penalty\': params[\'stepwise_penalty\'],\n                   ""normalize_length"": params[\'norm\'],\n                   ""lenpen"": params[\'length_penalty\'],\n                   ""lenpen_mode"": params[\'length_penalty_mode\'],\n                   \'remove_bpe\': True,\n                   }\n    params[\'output\'] = \'%s/bw%d_n%d_a%d_b%d_pl%d_%s\' % (evaldir,\n                                                        eval_kwargs[\'beam_size\'],\n                                                        params[\'batch_size\'],\n                                                        params[\'max_length_a\']*10,\n                                                        params[\'max_length_b\'],\n                                                        params[\'length_penalty\']*10,\n                                                        params[\'length_penalty_mode\'])\n    if params[\'norm\']:\n        params[\'output\'] += ""_norm""\n\n    if params[\'last\']:\n        params[\'output\'] = params[\'output\'] + \'_last\'\n\n    print(\'Evaluation params:\', eval_kwargs)\n    preds, bleu = sample_model(jobname,\n                               model, src_loader, trg_loader,\n                               eval_kwargs)\n    logger.warn(\'Bleu (split=%s, beam=%d) : %.3f\' % (eval_kwargs[\'split\'],\n                                                     eval_kwargs[\'beam_size\'],\n                                                     bleu))\n    with open(params[\'output\'] + \'.res\', \'w\') as f:\n        f.write(str(bleu))\n    json.dump(preds, open(params[\'output\'] +\n                          \'.json\', \'w\',\n                          encoding=\'utf8\'),\n              ensure_ascii=False)\n\n\nif __name__ == ""__main__"":\n    params = parse_eval_params()\n    generate(params)\n\n'"
preprocess.py,0,"b'#!/usr/bin/env python3\n\n""""""\nMain per-processing script\n""""""\n\nimport os.path as osp\nimport argparse\nimport h5py\nimport numpy as np\nfrom nmt.utils import pdump\n\n\ndef build_vocab(sentences, max_words, vocab_file):\n    """"""\n    Build vocabulary\n    """"""\n    # count up the number of words\n    counts = {}\n    # lets look at the distribution of lengths as well\n    sent_lengths = {}\n    for txt in sentences:\n        nw = len(txt)\n        sent_lengths[nw] = sent_lengths.get(nw, 0) + 1\n        for w in txt:\n            counts[w] = counts.get(w, 0) + 1\n    cw = sorted([(count, w) for w, count in counts.items()], reverse=True)\n    print(\'top words and their counts:\')\n    print(\'\\n\'.join(map(str, cw[:20])))\n\n    # print some stats\n    total_words = sum(counts.values())\n    print(\'total words:\', total_words)\n    vocab = [w for (c, w) in cw[:max_words]]\n    bad_words = [w for (c, w) in cw[max_words:]]\n\n    bad_count = sum(counts[w] for w in bad_words)\n    print(\'number of bad words: %d/%d = %.2f%%\' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n    print(\'number of words in vocab would be %d\' % (len(vocab), ))\n    print(\'number of UNKs: %d/%d = %.2f%%\' % (bad_count, total_words, bad_count*100.0/total_words))\n    max_len = max(sent_lengths.keys())\n    print(\'max length sentence in raw data: \', max_len)\n    # print(\'sentence length distribution (count, number of words):\')\n    # sum_len = sum(sent_lengths.values())\n    # for i in range(max_len+1):\n        # print(\'%2d: %10d   %f%%\' % (i, sent_lengths.get(i, 0), sent_lengths.get(i, 0)*100.0/sum_len))\n\n    # additional special UNK token we will use below to map infrequent words to\n    print(\'inserting the special UNK token\')\n    vocab.insert(0, ""<BOS>"")\n    vocab.insert(0, ""<EOS>"")\n    vocab.insert(0, ""<UNK>"")\n    vocab.insert(0, ""<PAD>"")\n    # writing a vocab file:\n    with open(vocab_file, \'w\') as fv:\n        for word in vocab:\n            fv.write(word+\'\\n\')\n    # Dump the statistics for later use:\n    pdump({""counts"": counts,\n           ""vocab"": vocab,\n           ""bad words"": bad_words,\n           ""lengths"": sent_lengths},\n          vocab_file + "".stats"")\n\n    return vocab\n\n\ndef encode_sentences(sentences, params, wtoi):\n    """"""\n    encode all sentences into one large array, which will be 1-indexed.\n    No special tokens are added, except from the <pad> after the effective length\n    """"""\n    max_length = params.max_length\n    lengths = []\n    m = len(sentences)\n    IL = np.zeros((m, max_length), dtype=\'uint32\')  # <PAD> token is 0\n    M = np.zeros((m, max_length), dtype=\'uint32\')\n    print(\'...Encoding \', end="""")\n    for i, sent in enumerate(sentences):\n        lengths.append(len(sent))\n        for k, w in enumerate(sent):\n            if k < max_length:\n                IL[i, k] = wtoi[w] if w in wtoi else wtoi[\'<UNK>\']\n                M[i, k] = int(w in wtoi)\n        # bar.update(i)\n        if not i % 10000:\n            print(""."", end="""")\n\n    print(""\\n"")\n    assert np.all(np.array(lengths) > 0), \'error: some line has no words\'\n    return IL, M, lengths\n\n\ndef main_trg(params, train_order, val_order, test_order, vocab=None):\n    """"""\n    Main preprocessing\n    """"""\n    max_length = params.max_length\n    train_trg = \'data/%s/train.%s\' % (params.data_dir, params.trg)\n    val_trg = \'data/%s/valid.%s\' % (params.data_dir, params.trg)\n    test_trg = \'data/%s/test.%s\' % (params.data_dir, params.trg)\n    with open(train_trg, \'r\') as f:\n        sentences = f.readlines()\n        sentences = [sent.strip().split()[:max_length] for sent in sentences]\n        if train_order is not None:\n            sentences = [sentences[k] for k in train_order]\n    print(""Read %d lines from %s"" % (len(sentences), train_trg))\n\n    if vocab is None:\n        vocab_file = ""data/%s/vocab.%s"" % (params.data_dir, params.trg)\n        if osp.exists(vocab_file):\n            print(\'...Reading vocabulary file (%s)\' % vocab_file)\n            vocab = []\n            for line in open(vocab_file, \'r\'):\n                vocab.append(line.strip())\n            if \'<BOS>\' not in vocab:\n                print(\'Inserting BOS\')\n                vocab.insert(0, ""<BOS>"")\n            if \'<EOS>\' not in vocab:\n                print(\'Inserting EOS\')\n                vocab.insert(0, ""<EOS>"")\n            if \'<UNK>\' not in vocab:\n                print(\'Inserting UNK\')\n                vocab.insert(0, ""<UNK>"")\n            if \'<PAD>\' not in vocab:\n                print(\'Inserting PAD\')\n                vocab.insert(0, ""<PAD>"")\n        else:\n            print(\'...Creating vocabulary of the %d frequent tokens\'\n              % params.max_words_trg)\n            vocab = build_vocab(sentences, params.max_words_trg,\n                                vocab_file)\n    print(\'...Vocabulary size:\', len(vocab))\n    itow = {i: w for i, w in enumerate(vocab)}\n    wtoi = {w: i for i, w in enumerate(vocab)}\n\n    # encode captions in large arrays, ready to ship to hdf5 file\n    IL_train, Mask_train, Lengths_train = encode_sentences(sentences, params, wtoi)\n\n    with open(val_trg, \'r\') as f:\n        sentences = f.readlines()\n        sentences = [sent.strip().split()[:max_length] for sent in sentences]\n        if val_order is not None:\n            sentences = [sentences[k] for k in val_order]\n\n    print(""Read %d lines from %s"" % (len(sentences), val_trg))\n    IL_val, Mask_val, Lengths_val = encode_sentences(sentences, params, wtoi)\n\n    with open(test_trg, \'r\') as f:\n        sentences = f.readlines()\n        sentences = [sent.strip().split()[:max_length] for sent in sentences]\n        if test_order is not None:\n            sentences = [sentences[k] for k in test_order]\n\n    print(""Read %d lines from %s"" % (len(sentences), test_trg))\n    IL_test, Mask_test, Lengths_test = encode_sentences(sentences, params, wtoi)\n\n    # create output h5 file\n    f = h5py.File(\'data/%s/%s.h5\' % (params.data_dir, params.trg), ""w"")\n    f.create_dataset(""labels_train"", dtype=\'uint32\', data=IL_train)\n    f.create_dataset(""lengths_train"", dtype=\'uint32\', data=Lengths_train)\n\n    f.create_dataset(""labels_val"", dtype=\'uint32\', data=IL_val)\n    f.create_dataset(""lengths_val"", dtype=\'uint32\', data=Lengths_val)\n\n    f.create_dataset(""labels_test"", dtype=\'uint32\', data=IL_test)\n    f.create_dataset(""lengths_test"", dtype=\'uint32\', data=Lengths_test)\n\n    print(\'Wrote h5file for the target langauge\')\n    pdump({\'itow\': itow, \'params\': params},\n          \'data/%s/%s.infos\' % (params.data_dir, params.trg))\n\n\ndef main_src(params):\n    """"""\n    Main preprocessing\n    """"""\n    max_length = params.max_length\n    batch_size = params.batch_size # 32\n    train_src = \'data/%s/train.%s\' % (params.data_dir, params.src)\n    val_src = \'data/%s/valid.%s\' % (params.data_dir, params.src)\n    test_src = \'data/%s/test.%s\' % (params.data_dir, params.src)\n    with open(train_src, \'r\') as f:\n        sentences = f.readlines()\n        sentences = [sent.strip().split()[:max_length] for sent in sentences]\n    \n    print(""Read %d lines from %s"" % (len(sentences), train_src))\n    if params.sort:\n        print(\'...Sorting by length\')\n        train_order = sorted(range(len(sentences)),\n                             key=lambda k: len(sentences[k]),\n                             reverse=True)\n        sentences = [sentences[k] for k in train_order]\n    elif params.shuffle_sort:\n        print(\'...Batching by length\')\n        train_order = sorted(range(len(sentences)),\n                             key=lambda k: len(sentences[k]),\n                             reverse=True)\n        batched_order = np.array_split(train_order,\n                                       len(train_order)//batch_size)\n        print(\'...Shuffling\')\n        np.random.shuffle(batched_order)\n        train_order = np.concatenate(batched_order)\n        sentences = [sentences[k] for k in train_order]\n    else:\n        train_order = None\n    \n    vocab_file = ""data/%s/vocab.%s"" % (params.data_dir, params.src)\n    if osp.exists(vocab_file):\n        print(\'...Reading vocabulary file (%s)\' % vocab_file)\n        vocab = []\n        for line in open(vocab_file, \'r\'):\n            vocab.append(line.strip())\n        if \'<BOS>\' not in vocab:\n            print(\'Inserting BOS\')\n            vocab.insert(0, ""<BOS>"")\n        if \'<EOS>\' not in vocab:\n            print(\'Inserting EOS\')\n            vocab.insert(0, ""<EOS>"")\n        if \'<UNK>\' not in vocab:\n            vocab.insert(0, ""<UNK>"")\n        if \'<PAD>\' not in vocab:\n            vocab.insert(0, ""<PAD>"")\n    else:\n        print(\'...Creating vocabulary of the %d frequent tokens\'\n              % params.max_words_src)\n        vocab = build_vocab(sentences, params.max_words_src,\n                            vocab_file)\n    print(\'...Vocabulary size:\', len(vocab))\n    itow = {i: w for i, w in enumerate(vocab)}\n    wtoi = {w: i for i, w in enumerate(vocab)}\n\n    # encode captions in large arrays, ready to ship to hdf5 file\n    IL_train_src, _, Lengths_train = encode_sentences(sentences, params, wtoi)\n\n    with open(val_src, \'r\') as f:\n        sentences = f.readlines()\n        sentences = [sent.strip().split()[:max_length] for sent in sentences]\n    print(""Read %d lines from %s"" % (len(sentences), val_src))\n    if params.shuffle_sort_eval:\n        print(\'...Batching by length\')\n        val_order = sorted(range(len(sentences)),\n                           key=lambda k: len(sentences[k]),\n                           reverse=True)\n        batched_order = np.array_split(val_order,\n                                       len(val_order)//batch_size)\n        np.random.shuffle(batched_order)\n        val_order = np.concatenate(batched_order)\n        sentences = [sentences[k] for k in val_order]\n    else:\n        val_order = None\n    IL_val_src, _, Lengths_val = encode_sentences(sentences, params, wtoi)\n\n    with open(test_src, \'r\') as f:\n        sentences = f.readlines()\n        sentences = [sent.strip().split()[:max_length] for sent in sentences]\n    print(""Read %d lines from %s"" % (len(sentences), test_src))\n    if params.shuffle_sort_eval:\n        print(\'...Batching by length\')\n        test_order = sorted(range(len(sentences)),\n                            key=lambda k: len(sentences[k]),\n                            reverse=True)\n        batched_order = np.array_split(test_order,\n                                       len(test_order)//batch_size)\n        np.random.shuffle(batched_order)\n        test_order = np.concatenate(batched_order)\n        sentences = [sentences[k] for k in test_order]\n    else:\n        test_order = None\n\n    IL_test_src, _, Lengths_test = encode_sentences(sentences, params, wtoi)\n\n    # HDF5 encoding\n    f = h5py.File(\'data/%s/%s.h5\' % (params.data_dir, params.src), ""w"")\n    f.create_dataset(""labels_train"", dtype=\'uint32\', data=IL_train_src)\n    f.create_dataset(""lengths_train"", dtype=\'uint32\', data=Lengths_train)\n    f.create_dataset(""labels_val"", dtype=\'uint32\', data=IL_val_src)\n    f.create_dataset(""lengths_val"", dtype=\'uint32\', data=Lengths_val)\n    f.create_dataset(""labels_test"", dtype=\'uint32\', data=IL_test_src)\n    f.create_dataset(""lengths_test"", dtype=\'uint32\', data=Lengths_test)\n\n    print(\'Wrote h5 file for the source langauge\')\n    pdump({\'itow\': itow, \'params\': params},\n          \'data/%s/%s.infos\' % (params.data_dir, params.src))\n    return train_order, val_order, test_order, vocab\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-d\', \'--data_dir\', type=str, default=\'WMT14\')\n    parser.add_argument(\'--src\', type=str, default=\'en\')\n    parser.add_argument(\'--trg\', type=str, default=\'fr\')\n    parser.add_argument(\'--max_words_src\', default=30000, type=int,\n                        help=""Max words in the source vocabulary"")\n    parser.add_argument(\'--max_words_trg\', default=30000, type=int,\n                        help=""Max words in the target vocabulary"")\n    parser.add_argument(\'--max_length\', default=50, type=int,\n                        help=\'max length of a sentence\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=32, type=int,\n                        help=\'batch size to sort by length\')\n    parser.add_argument(\'--sort\', action=\'store_true\',\n                        help=\'sort the training set by source sequence length\')\n    parser.add_argument(\'--share_vocab\', action=\'store_true\',\n                        help=\'share the source and target vocab\')\n    parser.add_argument(\'--shuffle_sort\', action=\'store_true\',\n                        help=\'sort the training set by source sequence length\')\n    parser.add_argument(\'--shuffle_sort_eval\', action=\'store_true\',\n                        help=\'sort the training set by source sequence length\')\n    params = parser.parse_args()\n    # Default settings for IWSLT DE-EN & WMT EN-DE:\n    if params.data_dir == \'iwslt\':\n        params.src = ""de""\n        params.trg = ""en""\n        params.max_words_src = 14000\n        params.max_words_trg = 14000\n        params.shuffle_sort = True\n        params.shuffle_sort_eval = True\n        params.max_length = 200\n        params.batch_size = 32\n\n    if params.data_dir == \'envi\':\n        params.src = ""en""\n        params.trg = ""vi""\n        params.max_words_src = 10000\n        params.max_words_trg = 10000\n        params.shuffle_sort = True\n        params.shuffle_sort_eval = True\n        params.max_length = 200\n        params.batch_size = 32\n\n    if params.data_dir == \'envi_word\':\n        params.src = ""en""\n        params.trg = ""vi""\n        params.max_words_src = 17700\n        params.max_words_trg = 17000\n        params.shuffle_sort = True\n        params.shuffle_sort_eval = True\n        params.max_length = 120\n        params.batch_size = 32\n\n    if params.data_dir == \'wmt_en_de\':\n        params.src = ""en""\n        params.trg = ""de""\n        params.max_words_src = 32800\n        params.max_words_trg = 32800\n        params.shuffle_sort = True\n        params.shuffle_sort_eval = True\n        params.max_length = 200\n        params.batch_size = 64\n\n    print(\'Source language: \', params.src)\n    train_order, val_order, test_order, vocab = main_src(params)\n    print(\'\\nTarget language: \', params.trg)\n    if params.share_vocab:\n        main_trg(params, train_order, val_order, test_order, vocab)\n    else:\n        main_trg(params, train_order, val_order, test_order)\n'"
train.py,4,"b'#!/usr/bin/env python3\n\n""""""\nMain training script\n""""""\n\nimport time\nimport logging\nfrom nmt.params import parse_params, set_env\n\n\ndef train(params):\n    """"""\n    Train NMT model\n    """"""\n    jobname = params[\'modelname\']\n    ngp = set_env(jobname, params[\'gpu_id\'])\n    import torch\n    devices = {}\n    for i in range(ngp):\n        devices[i] = torch.cuda.get_device_name(i)\n\n    from nmt.loader import ReadData\n    import nmt.models.setup as ms\n    from nmt.trainer import Trainer\n\n    logger = logging.getLogger(jobname)\n    # Data loading:\n    src_loader, trg_loader = ReadData(params[\'data\'], params[\'modelname\'])\n    src_vocab_size = src_loader.get_vocab_size()\n    trg_vocab_size = trg_loader.get_vocab_size()\n    trg_specials = {\'EOS\': trg_loader.eos,\n                    \'BOS\': trg_loader.bos,\n                    \'UNK\': trg_loader.unk,\n                    \'PAD\': trg_loader.pad,\n                   }\n    model = ms.build_model(jobname, params, src_vocab_size,\n                           trg_vocab_size, trg_specials)\n    logger.info(\'num. model params: %d\', sum(p.data.numel()\n                                             for p in model.parameters()))\n\n    criterion = ms.define_loss(jobname, params[\'loss\'], trg_loader)\n    trainer = Trainer(jobname, params, model, criterion)\n    trainer.set_devices(devices)\n\n    # Recover last checkpoint\n    iters = trainer.load_checkpoint()\n    src_loader.iterators = iters.get(\'src_iterators\', src_loader.iterators)\n    trg_loader.iterators = iters.get(\'trg_iterators\', trg_loader.iterators)\n\n    if trainer.lr_patient:\n        trainer.update_params()\n    while True:\n        # update parameters: lr, ...\n        if not trainer.lr_patient:\n            trainer.update_params()\n        torch.cuda.synchronize()\n        avg_loss = torch.zeros(1).cuda()\n        avg_ml_loss = torch.zeros(1).cuda()\n        total_ntokens = 0\n        total_nseqs = 0\n        start = time.time()\n        # Default num_batches=1\n        for _ in range(params[\'optim\'][\'num_batches\']):\n            data_src, order = src_loader.get_src_batch(\'train\')\n            data_trg = trg_loader.get_trg_batch(\'train\', order)\n            losses, batch_size, ntokens = trainer.step(data_src, data_trg)\n            avg_loss += ntokens * losses[\'final\']\n            avg_ml_loss += ntokens * losses[\'ml\']\n            total_nseqs += batch_size\n            total_ntokens += ntokens\n\n        avg_loss /= total_ntokens\n        avg_ml_loss /= total_ntokens\n\n        trainer.backward_step(avg_loss, avg_ml_loss,\n                              total_ntokens, total_nseqs,\n                              start, data_src[\'bounds\'][\'wrapped\'])\n        trainer.increment_time(time.time()-start)\n        # Evaluate on validation set then save\n        if trainer.evaluate:\n            trainer.validate(src_loader, trg_loader)\n        if trainer.done:\n            logger.info(\'Max epochs reached!\')\n            break\n\n\nif __name__ == ""__main__"":\n    params = parse_params()\n    train(params)\n'"
nmt/_trackers.py,0,"b""TRACKERS = {\n    'train/loss': [],\n    'train/ml_loss': [],\n    'val/loss': [],\n    'val/ml_loss': [],\n    'val/perf/bleu': [],\n    'optim/lr': [],\n    'optim/grad_norm': [],\n    'optim/scheduled_sampling': [],\n    'optim/ntokens': [],\n    'optim/batch_size': [],\n    'iteration': 0,\n    'epoch': 1,\n    'batch_offset': 0,\n    'update': set(),\n    'devices': [],\n    'time': []\n}\n"""
nmt/optimizer.py,1,"b'from math import sqrt, pi, cos, ceil\nfrom torch import optim\nfrom torch.optim import lr_scheduler\n\n\ndef LRScheduler(opt, optimizer, last_epoch=-1):\n    ref = opt[\'schedule\']\n    if ref == ""early-stopping"":\n        if opt[\'criterion\'] == ""loss"":\n            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                       patience=opt[\'patience\'],\n                                                       factor=opt[\'decay_rate\'],\n                                                       verbose=True,\n                                                       threshold=0.01,\n                                                       min_lr=1e-5)\n        elif opt[\'criterion\'] == ""perf"":\n            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                       mode=""max"",\n                                                       patience=opt[\'patience\'],\n                                                       factor=opt[\'decay_rate\'],\n                                                       verbose=True,\n                                                       threshold=0.05)\n\n    elif ref == ""cosine-ep"":\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n                                                   T_max=opt[\'max_epochs\'],\n                                                   eta_min=opt.get(\'min_lr\', 0))\n\n    elif ref == ""cosine"":\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n                                                   T_max=opt[\'max_updates\'],\n                                                   eta_min=opt.get(\'min_lr\', 0))\n    elif ref == ""shifted-cosine"":\n        scheduler = ShiftedCosine(optimizer,\n                                  T_max=opt[\'max_updates\'],\n                                  cycles=opt[\'cycles\'])\n\n    elif ref == ""plateau-cosine"":\n        scheduler = PlateauCosine(optimizer,\n                                  T1=opt[\'T1\'],\n                                  T2=opt[\'T2\'],\n                                  eta1=opt[\'eta1\'],\n                                  eta2=opt[\'eta2\'])\n\n    elif ref == ""step"":\n        scheduler = lr_scheduler.StepLR(optimizer,\n                                        step_size=opt[\'decay_every\'],\n                                        gamma=opt[\'decay_rate\'],\n                                        last_epoch=last_epoch)\n        # self.lr_scheduler = lr_scheduler.LambdaLR(self.optimizer.optimizer, self.anneal)\n    elif ref == ""step-iter"":\n        scheduler = lr_scheduler.StepLR(optimizer,\n                                        step_size=opt[\'decay_every\'],\n                                        gamma=opt[\'decay_rate\'],\n                                        last_epoch=last_epoch)\n\n    elif ref == ""inverse-square"":\n        scheduler = InverseSquareRoot(optimizer,\n                                      warmup=opt[\'warmup\'],\n                                      last_epoch=last_epoch)\n\n    elif ref == \'multi-step\':\n        milestones = list(opt[\'milestones\'].split(\',\'))\n        scheduler = lr_scheduler.MultiStepLR(optimizer,\n                                             milestones,\n                                             gamma=opt[\'decay_rate\'],\n                                             last_epoch=last_epoch)\n    else:\n        raise ValueError(\'Unknown scheduler % s\' % ref)\n    scheduler.mode = ref\n    return scheduler\n\n\nclass NAG(optim.Optimizer):\n    def __init__(self, params,\n                 lr=.25, momentum=0,\n                 weight_decay=0):\n        defaults = dict(lr=lr, lr_old=lr,\n                        momentum=momentum,\n                        weight_decay=weight_decay)\n        super(NAG, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        source: fairseq/optim/nag.py\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\'weight_decay\']\n            momentum = group[\'momentum\']\n            lr = group[\'lr\']\n            lr_old = group.get(\'lr_old\', lr)\n            lr_correct = lr / lr_old\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad.data\n                param_state = self.state[p]\n                if \'momentum_buffer\' not in param_state:\n                    param_state[\'momentum_buffer\'] = d_p.clone().zero_()\n\n                buf = param_state[\'momentum_buffer\']\n\n                if weight_decay != 0:\n                    p.data.mul_(1 - lr * weight_decay)\n                p.data.add_(momentum * momentum * lr_correct, buf)\n                p.data.add_(-(1 + momentum) * lr, d_p)\n\n                buf.mul_(momentum * lr_correct).add_(-lr, d_p)\n\n            group[\'lr_old\'] = lr\n\n        return loss\n\nclass Optimizer(object):\n    """"""\n    Wrapper for the optimizer (fairseq style)\n    """"""\n    def __init__(self, opt, model):\n        super().__init__()\n        #  rmsprop | sgd | sgdmom | adagrad | adam\n        ref = opt[\'solver\'].lower()\n        lr = opt[\'LR\'][\'base\']\n        if isinstance(model, list):\n            params = [{\'params\': m.parameters(),\n                       \'lr\': lr}\n                      for m in model]\n        else:\n            params = [{\'params\': model.parameters(), \'lr\': lr}]\n\n        if ref == \'adam\':\n            optimizer = optim.Adam(params,\n                                   lr=lr,\n                                   betas=(opt[\'alpha\'], opt[\'beta\']),\n                                   weight_decay=opt[\'weight_decay\'],\n                                   eps=float(opt[\'epsilon\']),\n                                   amsgrad=bool(opt.get(\'amsgrad\', 0)))\n        elif ref == \'sgd\':\n            optimizer = optim.SGD(params,\n                                  lr=lr,\n                                  momentum=opt.get(\'momentum\', 0),\n                                  dampening=opt.get(\'dampening\', 0),\n                                  weight_decay=opt[\'weight_decay\'],\n                                  nesterov=bool(opt.get(\'nesterov\', 0)))\n\n        elif ref.lower() == \'rmsprop\':\n            optimizer = optim.RMSprop(params,\n                                      lr=lr,\n                                      alpha=opt[\'alpha\'],\n                                      eps=opt[\'epsilon\'],\n                                      weight_decay=opt[\'weight_decay\'],\n                                      momentum=opt.get(\'momentum\', 0),\n                                      centered=False)\n        elif ref.lower() == \'adagrad\':\n            optimizer = optim.Adagrad(params,\n                                      lr=lr,\n                                      lr_decay=opt.get(\'lr_decay\', 0),\n                                      weight_decay=opt[\'weight_decay\'],\n                                      initial_accumulator_value=0)\n        elif ref.lower() == \'nag\':\n            optimizer = NAG(params,\n                            lr=lr,\n                            momentum=opt[\'momentum\'],\n                            weight_decay=opt[\'weight_decay\']\n                            )\n\n        else:\n            raise ValueError(\'Unknown optimizer % s\' % ref)\n\n        self.optimizer = optimizer\n\n    def get_lr(self):\n        """"""Return the current learning rate.""""""\n        return self.optimizer.param_groups[0][\'lr\']\n\n    def set_lr(self, lr):\n        """"""Set the learning rate.""""""\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] = lr\n\n    def state_dict(self):\n        """"""Return the optimizer\'s state dict.""""""\n        return self.optimizer.state_dict()\n\n    def load(self, state_dict):\n        """"""Load an optimizer state dict. """"""\n        self.optimizer.load_state_dict(state_dict)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.""""""\n        return self.optimizer.step(closure)\n\n    def zero_grad(self):\n        """"""Clears the gradients of all optimized parameters.""""""\n        return self.optimizer.zero_grad()\n\n\n    def require_grad(self):\n        """"""Set requires_grad true for all params""""""\n        for p in self.optimizer.param_groups:\n            if isinstance(p, dict):\n                for pp in p[\'params\']:\n                    pp.requires_grad = True\n\n        \nclass InverseSquareRoot(lr_scheduler._LRScheduler):\n    """"""\n    Follow the schedule of Vaswani et al. 2017\n    """"""\n\n    def __init__(self, optimizer,\n                 warmup=4000, last_epoch=-1):\n        self.warmup = warmup\n        super(InverseSquareRoot, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        it = self.last_epoch + 1\n        scale_factor = min(1 / sqrt(it), it / self.warmup ** 1.5)\n        return [base_lr * scale_factor\n                for base_lr in self.base_lrs]\n\n\nclass ShiftedCosine(lr_scheduler._LRScheduler):\n\n    """"""\n    Similar to cosine\n    """"""\n\n    def __init__(self, optimizer, cycles, T_max, last_epoch=-1):\n        self.T_max = T_max\n        self.cycle_duration = ceil(T_max / cycles)\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        scale_factor = 1/2 * (cos(pi * (self.last_epoch % self.cycle_duration) / self.cycle_duration) + 1)\n        return [base_lr * scale_factor\n                for base_lr in self.base_lrs]\n\n\nclass PlateauCosine(lr_scheduler._LRScheduler):\n\n    """"""\n    Steep decrease then ~ plateau\n    [self.eta_min + (base_lr - self.eta_min) *\n                    (1 + math.cos(math.pi * self.last_epoch / self.T_max)) / 2\n                                    for base_lr in self.base_lrs]\n    """"""\n\n    def __init__(self, optimizer, T1, T2, eta1, eta2, last_epoch=-1):\n        self.T1 = int(T1)\n        self.T2 = int(T2)\n        self.eta1 = float(eta1)\n        self.eta2 = float(eta2)\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch > self.T1:\n            # use a wider period\n            return [self.eta2 + (self.eta1 - self.eta2) *\n                    (1 + cos(pi * self.last_epoch / self.T2)) / 2\n                    for base_lr in self.base_lrs]\n\n        # very steep\n        return [self.eta1 + (base_lr - self.eta1) *\n                (1 + cos(pi * self.last_epoch / self.T1)) / 2\n                for base_lr in self.base_lrs]\n\n\n\n     \n'"
nmt/trainer.py,14,"b'# -*- coding: utf-8 -*-\n""""""\nTrainer class\n""""""\n\nimport sys\nimport os.path as osp\nimport logging\nimport json\nimport time\nimport numpy as np\n\nimport torch\nfrom tensorboardX import SummaryWriter\nfrom nmt.utils import pload, pdump, set_seed\nfrom nmt.models.evaluate import evaluate_model, evaluate_val_loss\nfrom .optimizer import Optimizer,  LRScheduler\nfrom ._trackers import TRACKERS\n\n\nclass Trainer(object):\n    """"""\n    Training a model with a given criterion\n    """"""\n\n    def __init__(self, jobname, params, model, criterion):\n\n        if not torch.cuda.is_available():\n            raise NotImplementedError(\'Training on CPU is not supported\')\n\n        self.params = params\n        self.jobname = jobname\n\n        self.logger = logging.getLogger(jobname)\n        # reproducibility:\n        set_seed(params[\'optim\'][\'seed\'])\n\n        self.clip_norm = params[\'optim\'][\'grad_clip\']\n        self.num_batches = params[\'optim\'][\'num_batches\']\n\n        # Move to GPU\n        self.model = model.cuda()\n        self.criterion = criterion.cuda()\n\n        # Initialize optimizer and LR scheduler\n        self.optimizer = Optimizer(params[\'optim\'], model)\n        self.lr_patient = params[\'optim\'][\'LR\'][\'schedule\'] == ""early-stopping""\n        if self.lr_patient:\n            self.lr_patient = params[\'optim\'][\'LR\'][\'criterion\']\n            self.logger.info(\'updating the lr wrt %s\', self.lr_patient)\n        self.lr_scheduler = LRScheduler(params[\'optim\'][\'LR\'],\n                                        self.optimizer.optimizer,\n                                        )\n\n        self.tb_writer = SummaryWriter(params[\'eventname\'])\n        self.log_every = params[\'track\'][\'log_every\']\n        self.checkpoint = params[\'track\'][\'checkpoint\']\n        self.evaluate = False\n        self.done = False\n        self.trackers = TRACKERS\n        self.iteration = 0\n        self.epoch = 0\n        self.batch_offset = 0\n        # Dump  the model params:\n        json.dump(params, open(\'%s/params.json\' % params[\'modelname\'], \'w\'))\n\n    def update_params(self, val_loss=None):\n        """"""\n        Update dynamic params:\n        lr, scheduled_sampling probability and tok/seq\'s alpha\n        """"""\n        epoch = self.epoch\n        iteration = self.iteration\n        if not self.lr_patient:\n            if self.lr_scheduler.mode in [""step-iter"", ""inverse-square"",\n                                          ""cosine"", \'shifted-cosine\',\n                                          \'plateau-cosine\']:\n                self.lr_scheduler.step(iteration)\n            else:\n                self.lr_scheduler.step(epoch - 1)\n        self.track(\'optim/lr\', self.optimizer.get_lr())\n\n    def step(self, data_src, data_trg, ntokens=0):\n        """"""\n        A signle forward step\n        """"""\n        # Clear the grads\n        self.optimizer.zero_grad()\n        batch_size = data_src[\'labels\'].size(0)\n        # evaluate the loss\n        decoder_logit = self.model(data_src, data_trg)\n        losses, stats = self.criterion(decoder_logit, data_trg[\'out_labels\'])\n        if not ntokens:\n            ntokens = torch.sum(data_src[\'lengths\'] *\n                                data_trg[\'lengths\']).data.item()\n\n        return losses, batch_size, ntokens\n\n    def backward_step(self, loss, ml_loss, ntokens, nseqs, start, wrapped):\n        """"""\n        A single backward step\n        """"""\n        loss.backward()\n        if self.clip_norm > 0:\n            grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n                                                       self.clip_norm)\n        self.track(\'optim/grad_norm\', grad_norm)\n        self.track(\'optim/ntokens\', ntokens)\n        self.track(\'optim/batch_size\', nseqs)\n\n        self.optimizer.step()\n        # torch.cuda.empty_cache()  # FIXME\n        if np.isnan(loss.data.item()):\n            sys.exit(\'Loss is nan\')\n        torch.cuda.synchronize()\n        self.iteration += 1\n        if wrapped:\n            self.epoch += 1\n        # Log\n        if (self.iteration % self.log_every == 0):\n            self.track(\'train/loss\', loss.data.item())\n            self.track(\'train/ml_loss\', ml_loss.data.item())\n            self.to_stderr(nseqs, ntokens, time.time()-start)\n            self.tensorboard()\n\n        self.evaluate = (self.iteration % self.checkpoint == 0)\n        self.done = (self.epoch > self.params[\'optim\'][\'max_epochs\'])\n\n    def validate(self, src_loader=None, trg_loader=None):\n        """"""\n        Evaluate on the dev set\n        """"""\n        params = self.params\n        self.log(\'Evaluating the model on the validation set..\')\n        self.model.eval()\n        if params.get(\'eval_bleu\', 1):\n            _, val_ml_loss, val_loss, bleu = evaluate_model(params[\'modelname\'],\n                                                            self,\n                                                            src_loader,\n                                                            trg_loader,\n                                                            params[\'track\'])\n            self.log(\'BLEU: %.5f \' % bleu)\n            self.track(\'val/perf/bleu\', bleu)\n            save_best = (self.trackers[\'val/perf/bleu\'][-1] ==\n                         max(self.trackers[\'val/perf/bleu\']))\n            save_every = 0\n\n        else:\n            val_ml_loss, val_loss = evaluate_val_loss(params[\'modelname\'],\n                                                      self,\n                                                      src_loader,\n                                                      trg_loader,\n                                                      params[\'track\'])\n            save_every = 1\n            save_best = 0\n\n        self.track(\'val/loss\', val_loss)\n        self.track(\'val/ml_loss\', val_ml_loss)\n        self.tensorboard()\n        # Save model if still improving on the dev set\n        self.save_model(src_loader, trg_loader, save_best, save_every)\n        self.model.train()\n        if self.lr_patient == ""loss"":\n            self.log(\'Updating the learning rate - LOSS\')\n            self.lr_scheduler.step(val_loss)\n            self.track(\'optim/lr\', self.optimizer.get_lr())\n        elif self.lr_patient == ""perf"":\n            assert not save_every\n            self.log(\'Updating the learning rate - PERF\')\n            self.lr_scheduler.step(bleu)\n            self.track(\'optim/lr\', self.optimizer.get_lr())\n\n    def save_model(self, src_loader, trg_loader, save_best, save_every):\n        """"""\n        checkoint model, optimizer and history\n        """"""\n        params = self.params\n        modelname = params[\'modelname\']\n        checkpoint_path = osp.join(modelname, \'model.pth\')\n        torch.save(self.model.state_dict(), checkpoint_path)\n        self.log(""model saved to {}"".format(checkpoint_path))\n        optimizer_path = osp.join(modelname, \'optimizer.pth\')\n        torch.save(self.optimizer.state_dict(), optimizer_path)\n        self.log(""optimizer saved to {}"".format(optimizer_path))\n        self.trackers[\'src_iterators\'] = src_loader.iterators\n        self.trackers[\'trg_iterators\'] = trg_loader.iterators\n        self.trackers[\'iteration\'] = self.iteration\n        self.trackers[\'epoch\'] = self.epoch\n        pdump(self.trackers, osp.join(modelname, \'trackers.pkl\'))\n\n        if save_best:\n            checkpoint_path = osp.join(modelname, \'model-best.pth\')\n            torch.save(self.model.state_dict(), checkpoint_path)\n            self.log(""model saved to {}"".format(checkpoint_path))\n            optimizer_path = osp.join(modelname, \'optimizer-best.pth\')\n            torch.save(self.optimizer.state_dict(), optimizer_path)\n            self.log(""optimizer saved to {}"".format(optimizer_path))\n            pdump(self.trackers, osp.join(modelname, \'trackers-best.pkl\'))\n\n        if save_every:\n            checkpoint_path = osp.join(modelname, \'model-%d.pth\' % self.iteration)\n            torch.save(self.model.state_dict(), checkpoint_path)\n            self.log(""model saved to {}"".format(checkpoint_path))\n\n    def load_checkpoint(self):\n        """"""\n        Load last saved params:\n        for use with oar\'s idempotent jobs\n        """"""\n        params = self.params\n        modelname = params[\'modelname\']\n        iterators_state = {}\n        history = {}\n        if osp.exists(osp.join(modelname, \'model.pth\')):\n            self.warn(\'Picking up where we left\')\n            # load model\'s weights\n            saved_state = torch.load(osp.join(modelname, \'model.pth\'))\n            saved = list(saved_state)\n            required_state = self.model.state_dict()\n            required = list(required_state)\n            del required_state\n            if ""module"" in required[0] and ""module"" not in saved[0]:\n                for k in saved:\n                    kbis = ""module.%s"" % k\n                    saved_state[kbis] = saved_state[k]\n                    del saved_state[k]\n\n            for k in saved:\n                if ""increment"" in k:\n                    del saved_state[k]\n                if ""transiton"" in k:\n                    kk = k.replace(""transiton"", ""transition"")\n                    saved_state[kk] = saved_state[k]\n                    del saved_state[k]\n            self.model.load_state_dict(saved_state)\n            # load the optimizer\'s last state:\n            self.optimizer.load(\n                torch.load(osp.join(modelname, \'optimizer.pth\')\n                           ))\n            history = pload(osp.join(modelname, \'trackers.pkl\'))\n            iterators_state = {\'src_iterators\': history[\'src_iterators\'],\n                               \'trg_iterators\': history[\'trg_iterators\']}\n\n        elif params[\'start_from\']:\n            start_from = params[\'start_from\']\n            # Start from a pre-trained model:\n            self.warn(\'Starting from %s\' % start_from)\n            if params[\'start_from_best\']:\n                flag = \'-best\'\n                self.warn(\'Starting from the best saved model\')\n            else:\n                flag = \'\'\n            # load model\'s weights\n            self.model.load_state_dict(\n                    torch.load(osp.join(start_from, \'model%s.pth\' % flag))\n                    )\n            # load the optimizer\'s last state:\n            if not params[\'optim\'][\'reset\']:\n                self.optimizer.load(\n                    torch.load(osp.join(start_from, \'optimizer%s.pth\' % flag)\n                               ))\n            history = pload(osp.join(start_from, \'trackers%s.pkl\' % flag))\n        self.trackers.update(history)\n        self.epoch = self.trackers[\'epoch\']\n        self.iteration = self.trackers[\'iteration\']\n        return iterators_state\n\n    def log(self, message):\n        self.logger.info(message)\n\n    def warn(self, message):\n        self.logger.warning(message)\n\n    def debug(self, message):\n        self.logger.debug(message)\n\n    def set_devices(self, devices):\n        self.trackers[\'devices\'].append(devices)\n        self.trackers[\'time\'].append(0)\n\n    def increment_time(self, t):\n        self.trackers[\'time\'][-1] += t\n\n    def track(self, k, v):\n        """"""\n        Track key metrics\n        """"""\n        if k not in self.trackers:\n            raise ValueError(\'Tracking unknown entity %s\' % k)\n        if isinstance(self.trackers[k], list):\n            self.trackers[k].append(v)\n        else:\n            self.trackers[k] = v\n        self.trackers[\'update\'].add(k)\n\n    def tensorboard(self):\n        """"""\n        Write tensorboard events\n        """"""\n        for k in self.trackers[\'update\']:\n            self.tb_writer.add_scalar(k, self.trackers[k][-1], self.iteration)\n        self.tb_writer.file_writer.flush()\n        self.trackers[\'update\'] = set()\n\n    def to_stderr(self, batch_size, ntokens, timing):\n        """"""\n        Log to stderr\n        """"""\n        self.log(\'| epoch {:2d} \'\n                 \'| iteration {:5d} \'\n                 \'| lr {:02.2e} \'\n                 \'| seq {:3d} \'\n                 \'| sXt {:5d} \'\n                 \'| ms/batch {:6.3f} \'\n                 \'| total time {:6.2f} s\'\n                 \'| loss {:6.3f} \'\n                 \'| ml {:6.3f}\'\n                 .format(self.epoch,\n                         self.iteration,\n                         self.optimizer.get_lr(),\n                         batch_size,\n                         ntokens,\n                         timing * 1000,\n                         sum(self.trackers[\'time\']),\n                         self.trackers[\'train/loss\'][-1],\n                         self.trackers[\'train/ml_loss\'][-1]))\n\n'"
nmt/loader/__init__.py,0,"b'from .dataloader import textDataLoader\n\n\ndef ReadData(params, jobname):\n    ddir = params[\'dir\']\n    src = params[\'src\']\n    trg = params[\'trg\']\n    dataparams = {\'h5\': ""%s/%s.%s"" % (ddir, src, ""h5""),\n                  \'infos\': ""%s/%s.%s"" % (ddir, src, ""infos""),\n                  \'batch_size\': params[\'batch_size\'],\n                  \'max_length\': params[\'max_src_length\']\n                 }\n    src_loader = textDataLoader(dataparams, jobname=jobname)\n\n    dataparams = {\'h5\': ""%s/%s.%s"" % (ddir, trg, ""h5""),\n                  \'infos\': ""%s/%s.%s"" % (ddir, trg, ""infos""),\n                  \'batch_size\': params[\'batch_size\'],\n                  \'max_length\': params[\'max_trg_length\']\n                 }\n\n    trg_loader = textDataLoader(dataparams, jobname=jobname)\n    return src_loader, trg_loader\n\n\ndef ReadRawData(params, h5path, jobname):\n    ddir = params[\'dir\']\n    src = params[\'src\']\n    trg = params[\'trg\']\n    dataparams = {\'h5\': ""%s.%s.h5"" % (h5path, src),\n                  \'infos\': ""%s/%s.%s"" % (ddir, src, ""infos""),\n                  \'batch_size\': params[\'batch_size\'],\n                  \'max_length\': params[\'max_src_length\'],\n                  ""raw"": True\n                 }\n    src_loader = textDataLoader(dataparams, jobname=jobname)\n\n    dataparams = {\'h5\': ""%s.%s.h5"" % (h5path, trg),\n                  \'infos\': ""%s/%s.%s"" % (ddir, trg, ""infos""),\n                  \'batch_size\': params[\'batch_size\'],\n                  \'max_length\': params[\'max_trg_length\'],\n                  ""raw"" : True\n                 }\n\n    trg_loader = textDataLoader(dataparams, jobname=jobname)\n    return src_loader, trg_loader\n\n\n\n'"
nmt/loader/dataloader.py,5,"b'import logging\nimport json\nimport h5py\nimport numpy as np\nimport torch\nfrom nmt.utils import pload\n\nclass textDataLoader(object):\n    """"""\n    Text data iterator class\n    """"""\n    def __init__(self, params, jobname):\n        self.logger = logging.getLogger(jobname)\n        infos = pload(params[\'infos\'])\n        self.ix_to_word = infos[\'itow\']\n        self.vocab_size = len(self.ix_to_word)\n        self.ref = params[""h5""]\n        self.logger.info(\'Loading h5 file: %s\' % params[\'h5\'])\n        self.logger.info(\'...Vocab size is %d \' % self.vocab_size)\n        self.h5_file = h5py.File(params[\'h5\'])\n        raw = params.get(\'raw\', False)\n        print(\'Raw:\', raw)\n        if not raw:\n            self.max_indices = {\n                \'train\': len(self.h5_file[""labels_train""]),\n                \'val\': len(self.h5_file[""labels_val""]),\n                \'test\': len(self.h5_file[""labels_test""])\n                }\n            self.logger.info(\'...Train:  %d | Dev: %d | Test: %d\',\n                             self.max_indices[\'train\'],\n                             self.max_indices[\'val\'],\n                             self.max_indices[\'test\'])\n            self.iterators = {\'train\': 0, \'val\': 0, \'test\': 0}\n        else:\n            print(\'Missing the usual splits\')\n            print(\'h5 keys:\', list(self.h5_file))\n            self.max_indices = {\n                \'full\': len(self.h5_file[""labels_full""]),\n                }\n            self.logger.info(\'...Full:  %d\',\n                             self.max_indices[\'full\'])\n            self.iterators = {\'full\': 0}\n\n        self.batch_size = params[\'batch_size\']\n        self.seq_length = params[\'max_length\']\n        self.logger.warning(\'...Reading sequences up to %d\', self.seq_length)\n        word_to_ix = {w: ix for ix, w in self.ix_to_word.items()}\n        self.pad = word_to_ix[\'<PAD>\']\n        self.unk = word_to_ix[\'<UNK>\']\n        try:\n            self.eos = word_to_ix[\'<EOS>\']\n            self.bos = word_to_ix[\'<BOS>\']\n        except:\n            self.eos = self.pad\n            self.bos = self.pad\n\n    def get_vocab_size(self):\n        return self.vocab_size\n\n    def get_vocab(self):\n        return self.ix_to_word\n\n    def get_seq_length(self):\n        return self.seq_length\n\n    def get_src_batch(self, split, batch_size=None):\n        batch_size = batch_size or self.batch_size\n        label_batch = np.zeros([batch_size, self.seq_length], dtype=\'int\')\n        len_batch = []\n        pointer = \'labels_%s\' % split\n        len_pointer = \'lengths_%s\' % split\n        max_index = self.max_indices[split]\n        wrapped = False\n        for i in range(batch_size):\n            ri = self.iterators[split]\n            ri_next = ri + 1\n            if ri_next >= max_index:\n                ri_next = 0\n                wrapped = True\n            self.iterators[split] = ri_next\n            label_batch[i] = self.h5_file[pointer][ri, :self.seq_length]\n            len_batch.append(min(self.h5_file[len_pointer][ri],\n                                 self.seq_length))\n\n        order = sorted(range(batch_size), key=lambda k: -len_batch[k])\n\n        data = {}\n        data[\'labels\'] = torch.from_numpy(\n            label_batch[order, :max(len_batch)]\n        ).cuda()\n\n        data[\'lengths\'] = torch.from_numpy(\n            np.array([len_batch[k] for k in order]).astype(int)\n        ).cuda()\n\n        data[\'bounds\'] = {\'it_pos_now\': self.iterators[split],\n                          \'it_max\': max_index, \'wrapped\': wrapped}\n        return data, order\n\n    def get_trg_batch(self, split, order, batch_size=None):\n        batch_size = batch_size or self.batch_size\n        in_label_batch = np.zeros([batch_size, self.seq_length + 1], dtype=\'int\')\n        out_label_batch = np.zeros([batch_size, self.seq_length + 1], dtype=\'int\')\n        len_batch = []\n        pointer = \'labels_%s\' % split\n        len_pointer = \'lengths_%s\' % split\n        max_index = self.max_indices[split]\n        wrapped = False\n        for i in range(batch_size):\n            ri = self.iterators[split]\n            ri_next = ri + 1\n            if ri_next >= max_index:\n                ri_next = 0\n                wrapped = True\n            self.iterators[split] = ri_next\n            # add <bos>\n            in_label_batch[i, 0] = self.bos\n            in_label_batch[i, 1:] = self.h5_file[pointer][ri, :self.seq_length]\n            # add <eos>\n            ll = min(self.seq_length, self.h5_file[len_pointer][ri])\n            len_batch.append(ll + 1)\n            out_label_batch[i] = np.insert(in_label_batch[i, 1:], ll, self.eos)\n\n        data = {}\n        data[\'labels\'] = torch.from_numpy(in_label_batch[order, :max(len_batch)]).cuda()\n        data[\'out_labels\'] = torch.from_numpy(out_label_batch[order, :max(len_batch)]).cuda()\n        data[\'lengths\'] = torch.from_numpy(\n            np.array([len_batch[k] for k in order]).astype(int)\n        ).cuda()\n\n        data[\'bounds\'] = {\'it_pos_now\': self.iterators[split],\n                          \'it_max\': max_index, \'wrapped\': wrapped}\n        return data\n\n    def reset_iterator(self, split):\n        self.iterators[split] = 0\n\n'"
nmt/loss/__init__.py,0,"b'from .cross_entropy import MLCriterion, SmoothMLCriterion\n'"
nmt/loss/cross_entropy.py,13,"b'# -*- coding: utf-8 -*-\n""""""\nRegular  & label-smoothed cross-entropy losses\n""""""\n\nimport logging\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nmt.utils import to_contiguous\n\n\nclass SmoothMLCriterion(nn.Module):\n    """"""\n    Label smoothed cross entropy loss\n    """"""\n    def __init__(self, job_name, params):\n        super().__init__()\n        self.logger = logging.getLogger(job_name)\n        self.th_mask = params.get(\'mask_threshold\', 1)  # both pad and unk\n        self.normalize_batch = bool(params.get(\'normalize_batch\', 1))\n        self.eps = params.get(\'label_smoothing\', 0.1)\n        self.version = \'label smoothed ml\'\n\n    def log(self):\n        self.logger.info(\'Label smoothed ML loss with eps=%.2e\' % self.eps)\n\n    def forward(self, logp, target):\n        """"""\n        logp : the decoder logits (N, seq_length, V)\n        target : the ground truth labels (N, seq_length)\n        """"""\n        mask = target.gt(self.th_mask).float()\n        output, ml_loss = get_smooth_ml_loss(logp, target, mask,\n                                             norm=self.normalize_batch,\n                                             eps=self.eps)\n        return {""final"": output, ""ml"": ml_loss}, {}\n\n\n\n\nclass MLCriterion(nn.Module):\n    """"""\n    The default cross entropy loss\n    """"""\n    def __init__(self, job_name, params):\n        super().__init__()\n        self.logger = logging.getLogger(job_name)\n        self.th_mask = params.get(\'mask_threshold\', 1)  # both pad and unk\n        self.normalize = params.get(\'normalize\', \'ntokens\')\n        self.version = \'ml\'\n\n    def log(self):\n        self.logger.info(\'Default ML loss\')\n\n    def forward(self, logp, target):\n        """"""\n        logp : the decoder logits (N, seq_length, V)\n        target : the ground truth labels (N, seq_length)\n        """"""\n        output = self.get_ml_loss(logp, target)\n        return {""final"": output, ""ml"": output}, {}\n\n    def get_ml_loss(self, logp, target):\n        """"""\n        Compute the usual ML loss\n        """"""\n        # print(\'logp:\', logp.size(), ""target:"", target.size())\n        batch_size = logp.size(0)\n        seq_length = logp.size(1)\n        vocab = logp.size(2)\n        target = target[:, :seq_length]\n        logp = to_contiguous(logp).view(-1, logp.size(2))\n        target = to_contiguous(target).view(-1, 1)\n        mask = target.gt(self.th_mask)\n        ml_output = - logp.gather(1, target)[mask]\n        ml_output = torch.sum(ml_output)\n\n        if self.normalize == \'ntokens\':\n            # print(\'initial ml:\', ml_output.data.item())\n            norm = torch.sum(mask)\n            ml_output /= norm.float()\n            # print(\'norm ml:\', ml_output.data.item(), \'// %d\' % norm.data.item())\n        elif self.normalize == \'seqlen\':\n            # print(\'initial ml:\', ml_output.data.item())\n            norm = seq_length\n            ml_output /= norm\n            # print(\'norm ml:\', ml_output.data.item(), \'// %d\' % norm)\n        elif self.normalize == \'batch\':\n            # print(\'initial ml:\', ml_output.data.item())\n            norm = batch_size\n            ml_output /= norm\n            # print(\'norm ml:\', ml_output.data.item(), \'// %d\' % norm)\n\n        else:\n            raise ValueError(\'Unknown normalizing scheme\')\n        return ml_output\n\n\n\nclass MLCriterionNLL(nn.Module):\n    """"""\n    The defaul cross entropy loss with the option\n    of scaling the sentence loss\n    """"""\n    def __init__(self, job_name, params, pad_token):\n        super().__init__()\n        self.logger = logging.getLogger(job_name)\n        self.pad_token = pad_token\n        self.normalize_batch = params[\'normalize_batch\']\n        self.penalize_confidence = params[\'penalize_confidence\']\n        self.sentence_avg = False\n        self.version = \'ml\'\n\n    def log(self):\n        self.logger.info(\'Default ML loss\')\n\n    def forward(self, logp, target, ntokens):\n        """"""\n        logp : the decoder logits (N, seq_length, V)\n        target : the ground truth labels (N, seq_length)\n        """"""\n        logp = logp.view(-1, logp.size(-1))\n        loss = F.nll_loss(logp, target.view(-1),\n                          size_average=False,\n                          ignore_index=self.pad_token,\n                          reduce=True)\n\n        print(\'loss pre norm:\', loss.data.item())\n        sample_size = target.size(0) \\\n                if self.sentence_avg else ntokens\n        print(\'sample size:\', sample_size)\n        output = loss / sample_size\n        print(\'returning:\', output.data.item())\n        return {""final"": output, ""ml"": output}, {}\n\n    def track(self, logp, target, mask, add_dirac=False):\n        """"""\n        logp : the decoder logits (N, seq_length, V)\n        target : the ground truth labels (N, seq_length)\n        mask : the ground truth mask to ignore UNK tokens (N, seq_length)\n        """"""\n        # truncate to the same size\n        N = logp.size(0)\n        seq_length = logp.size(1)\n        target = target[:, :seq_length].data.cpu().numpy()\n        logp = torch.exp(logp).data.cpu().numpy()\n        target_d = np.zeros_like(logp)\n        rows = np.arange(N).reshape(-1, 1).repeat(seq_length, axis=1)\n        cols = np.arange(seq_length).reshape(1, -1).repeat(N, axis=0)\n        target_d[rows, cols, target] = 1\n        return logp, target_d\n\ndef get_ml_loss(logp, target, mask, scores=None,\n                norm=True, penalize=0):\n    """"""\n    Compute the usual ML loss\n    """"""\n    # print(\'logp:\', logp.size(), ""target:"", target.size())\n    seq_length = logp.size(1)\n    target = target[:, :seq_length]\n    mask = mask[:, :seq_length]\n    binary_mask = mask\n    if scores is not None:\n        # row_scores = scores.unsqueeze(1).repeat(1, seq_length)\n        row_scores = scores.repeat(1, seq_length)\n        mask = torch.mul(mask, row_scores)\n    logp = to_contiguous(logp).view(-1, logp.size(2))\n    target = to_contiguous(target).view(-1, 1)\n    mask = to_contiguous(mask).view(-1, 1)\n    if penalize:\n        logp = logp.gather(1, target)\n        neg_entropy = torch.sum(torch.exp(logp) * logp)\n        ml_output = torch.sum(-logp * mask) + penalize * neg_entropy\n    else:\n        ml_output = - logp.gather(1, target) * mask\n        ml_output = torch.sum(ml_output)\n\n    if norm:\n        ml_output /= torch.sum(binary_mask)\n    return ml_output\n\ndef get_smooth_ml_loss(logp, target, mask,\n                       norm=True, eps=0):\n    """"""\n    Cross entropy with label smoothing\n    """"""\n    # print(\'logp:\', logp.size(), ""target:"", target.size())\n    seq_length = logp.size(1)\n    target = target[:, :seq_length]\n    mask = mask[:, :seq_length]\n    binary_mask = mask\n    logp = to_contiguous(logp).view(-1, logp.size(2))\n    target = to_contiguous(target).view(-1, 1)\n    mask = to_contiguous(mask).view(-1, 1)\n    ml_output = - logp.gather(1, target) * mask\n    ml_output = torch.sum(ml_output)\n    smooth_loss = -logp.sum(dim=1, keepdim=True) * mask\n    smooth_loss = smooth_loss.sum() / logp.size(1)\n    if norm:\n        ml_output /= torch.sum(binary_mask)\n        smooth_loss /= torch.sum(binary_mask)\n    output = (1 - eps) * ml_output + eps * smooth_loss\n    return output, ml_output\n'"
nmt/models/__init__.py,0,b'\n'
nmt/models/aggregator.py,1,"b'import torch.nn as nn\nfrom .pooling import *\n\n\nclass Aggregator(nn.Module):\n    def __init__(self, input_channls, force_output_channels=None, params={}):\n        nn.Module.__init__(self)\n        mode = params.get(""mode"", ""max"")\n        mapping = params.get(\'mapping\', \'linear\')\n        num_fc = params.get(\'num_fc\', 1)\n        self.output_channels = input_channls\n        if mode == \'mean\':\n            self.project = average_code\n        elif mode == \'max\':\n            self.project = max_code\n        elif mode == \'truncated-max\':\n            self.project = truncated_max\n        elif mode == \'truncated-mean\':\n            self.project = truncated_mean\n        elif mode == ""max-attention"":\n            self.project = MaxAttention(params, input_channls)\n            self.output_channels *= (2 - (params[\'first_aggregator\'] == ""skip""))\n        else:\n            raise ValueError(\'Unknown mode %s\' % mode)\n        self.add_lin = 0\n        print(\'Aggregator:\')\n        if force_output_channels is not None:\n            self.add_lin = 1\n            # Map the final output to the requested dimension\n            # for when tying the embeddings with the final projection layer\n            assert self.output_channels > force_output_channels, ""Avoid decompressing the channels"" #FIXME\n            print(self.output_channels, end=\'\')\n            if num_fc == 1:\n                lin = nn.Linear(self.output_channels, force_output_channels)\n                print("">"", force_output_channels)\n            elif num_fc == 2:\n                # IDEA: ~ https://arxiv.org/pdf/1808.10681.pdf Beyond weight-tying.\n                interm = (self.output_channels + force_output_channels ) // 2\n                lin = nn.Sequential(\n                        nn.Linear(self.output_channels, interm),\n                        nn.ReLU(inplace=True),\n                        nn.Linear(interm, force_output_channels)\n                        )\n                print("">"", interm, "">"", force_output_channels)\n            else:\n                raise ValueError(\'Not yet implemented\')\n\n            if mapping == ""linear"" :\n                self.lin = lin \n            elif mapping == ""tanh"":\n                self.lin = nn.Sequential(\n                        lin,\n                        nn.Tanh()\n                        )\n            elif mapping == ""relu"":\n                self.lin = nn.Sequential(\n                        lin,\n                        nn.ReLU(inplace=True)\n                        )\n            self.output_channels = force_output_channels\n            \n\n    def forward(self, tensor, src_lengths, track=False, *args):\n        if not track:\n            proj = self.project(tensor, src_lengths, track, *args)\n            proj = proj.permute(0, 2, 1)\n            if self.add_lin:\n                return self.lin(proj)\n            else:\n                return proj\n        else:\n            proj, attn = self.project(tensor, src_lengths, track, *args)\n            proj = proj.permute(0, 2, 1)\n            if self.add_lin:\n                proj = self.lin(proj)\n            return proj, attn\n\n\n\n\n'"
nmt/models/beam_onmt.py,2,"b'from __future__ import division\nimport torch\n\n\nclass Beam(object):\n    """"""\n    Class for managing the internals of the beam search process.\n    Takes care of beams, back pointers, and scores.\n\n    Args:\n       size (int): beam size\n       pad, bos, eos (int): indices of padding, beginning, and ending.\n       n_best (int): nbest size to use\n       cuda (bool): use gpu\n       global_scorer (:obj:`GlobalScorer`)\n    """"""\n\n    def __init__(self, size, pad, bos, eos,\n                 n_best=1, cuda=False,\n                 global_scorer=None,\n                 min_length=0,\n                 stepwise_penalty=False,\n                 block_ngram_repeat=0,\n                 exclusion_tokens=set()):\n\n        self.size = size\n        self.tt = torch.cuda if cuda else torch\n        # The score for each translation on the beam.\n        self.scores = self.tt.FloatTensor(size).zero_()\n        self.all_scores = []\n\n        # The backpointers at each time-step.\n        self.prev_ks = []\n\n        # The outputs at each time-step.\n        self.next_ys = [self.tt.LongTensor(size)\n                        .fill_(pad)]\n        self.next_ys[0][0] = bos\n\n        # Has EOS topped the beam yet.\n        self._eos = eos\n        self.eos_top = False\n\n        # The attentions (matrix) for each time.\n        self.attn = []\n\n        # Time and k pair for finished.\n        self.finished = []\n        self.n_best = n_best\n\n        # Information for global scoring.\n        self.global_scorer = global_scorer\n        self.global_state = {}\n\n        # Minimum prediction length\n        self.min_length = min_length\n\n        # Apply Penalty at every step\n        self.stepwise_penalty = stepwise_penalty\n        self.block_ngram_repeat = block_ngram_repeat\n        self.exclusion_tokens = exclusion_tokens\n\n    def get_current_state(self):\n        ""Get the outputs for the current timestep.""\n        return self.next_ys[-1]\n\n    def get_current_origin(self):\n        ""Get the backpointers for the current timestep.""\n        return self.prev_ks[-1]\n\n    def advance(self, word_probs, attn_out):\n        """"""\n        Given prob over words for every last beam `wordLk` and attention\n        `attn_out`: Compute and update the beam search.\n\n        Parameters:\n\n        * `word_probs`- probs of advancing from the last step (K x words)\n        * `attn_out`- attention at the last step\n\n        Returns: True if beam search is complete.\n        """"""\n        num_words = word_probs.size(1)\n        if self.stepwise_penalty:\n            self.global_scorer.update_score(self, attn_out)\n        # force the output to be longer than self.min_length\n        cur_len = len(self.next_ys)\n        if cur_len < self.min_length:\n            for k in range(len(word_probs)):\n                word_probs[k][self._eos] = -1e20\n        # Sum the previous scores.\n        if len(self.prev_ks) > 0:\n            beam_scores = word_probs + \\\n                self.scores.unsqueeze(1).expand_as(word_probs)\n            # Don\'t let EOS have children.\n            for i in range(self.next_ys[-1].size(0)):\n                if self.next_ys[-1][i] == self._eos:\n                    beam_scores[i] = -1e20\n\n            # Block ngram repeats\n            if self.block_ngram_repeat > 0:\n                ngrams = []\n                le = len(self.next_ys)\n                for j in range(self.next_ys[-1].size(0)):\n                    hyp, _ = self.get_hyp(le - 1, j)\n                    ngrams = set()\n                    fail = False\n                    gram = []\n                    for i in range(le - 1):\n                        # Last n tokens, n = block_ngram_repeat\n                        gram = (gram + [hyp[i]])[-self.block_ngram_repeat:]\n                        # Skip the blocking if it is in the exclusion list\n                        if set(gram) & self.exclusion_tokens:\n                            continue\n                        if tuple(gram) in ngrams:\n                            fail = True\n                        ngrams.add(tuple(gram))\n                    if fail:\n                        beam_scores[j] = -10e20\n        else:\n            beam_scores = word_probs[0]\n        flat_beam_scores = beam_scores.view(-1)\n        best_scores, best_scores_id = flat_beam_scores.topk(self.size, 0,\n                                                            True, True)\n\n        self.all_scores.append(self.scores)\n        self.scores = best_scores\n\n        # best_scores_id is flattened beam x word array, so calculate which\n        # word and beam each score came from\n        prev_k = best_scores_id / num_words\n        self.prev_ks.append(prev_k)\n        self.next_ys.append((best_scores_id - prev_k * num_words))\n        self.attn.append(attn_out.index_select(0, prev_k))\n        self.global_scorer.update_global_state(self)\n        for i in range(self.next_ys[-1].size(0)):\n            if self.next_ys[-1][i] == self._eos:\n                global_scores = self.global_scorer.score(self, self.scores)\n                s = global_scores[i]\n                self.finished.append((s, len(self.next_ys) - 1, i))\n\n        # End condition is when top-of-beam is EOS and no global score.\n        if self.next_ys[-1][0] == self._eos:\n            self.all_scores.append(self.scores)\n            self.eos_top = True\n        return self.done()\n\n    def done(self):\n        return self.eos_top and len(self.finished) >= self.n_best\n\n    def sort_finished(self, minimum=None):\n        if minimum is not None:\n            i = 0\n            # Add from beam until we have minimum outputs.\n            while len(self.finished) < minimum:\n                global_scores = self.global_scorer.score(self, self.scores)\n                s = global_scores[i]\n                self.finished.append((s, len(self.next_ys) - 1, i))\n                i += 1\n\n        # print(\'sorting finished:\', self.finished)\n        self.finished.sort(key=lambda a: -a[0])\n        scores = [sc for sc, _, _ in self.finished]\n        ks = [(t, k) for _, t, k in self.finished]\n        return scores, ks\n\n    def get_hyp(self, timestep, k):\n        """"""\n        Walk back to construct the full hypothesis.\n        """"""\n        hyp, attn = [], []\n        for j in range(len(self.prev_ks[:timestep]) - 1, -1, -1):\n            hyp.append(self.next_ys[j + 1][k])\n            attn.append(self.attn[j][k])\n            k = self.prev_ks[j][k]\n        return hyp[::-1], torch.stack(attn[::-1])\n\n\n\n'"
nmt/models/beam_search.py,2,"b'""""""Beam search implementation in PyTorch.""""""\n#         hyp1#-hyp1---hyp1 -hyp1\n#                 \\             /\n#         hyp2 \\-hyp2 /-hyp2#hyp2\n#                               /      \\\n#         hyp3#-hyp3---hyp3 -hyp3\n#         ========================\n#\n# Takes care of beams, back pointers, and scores.\n# Code borrowed from PyTorch OpenNMT example\n# https://github.com/pytorch/examples/blob/master/OpenNMT/onmt/Beam.py\n\nimport torch\n_BOS = 3\n_EOS = 2\n_UNK = 1\n_PAD = 0\n\n\nclass Beam(object):\n    """"""Ordered beam of candidate outputs.""""""\n\n    def __init__(self, size, opt, cuda=True):\n        """"""Initialize params.""""""\n        self.size = size\n        self.done = False\n        self.pad = opt.get(\'PAD\', _PAD)\n        self.bos = opt.get(\'BOS\', _BOS)\n        self.eos = opt.get(\'EOS\', _EOS)\n        self.norm_len = opt.get(\'normalize_length\', 0)\n        self.tt = torch.cuda if cuda else torch\n\n        # The score for each translation on the beam.\n        self.scores = self.tt.FloatTensor(size).zero_()\n\n        # The backpointers at each time-step.\n        self.prevKs = []\n\n        # The outputs at each time-step.\n        self.nextYs = [self.tt.LongTensor(size).fill_(self.bos)]\n        # self.nextYs[0][0] = self.bos\n\n        # The attentions (matrix) for each time.\n        self.attn = []\n\n    # Get the outputs for the current timestep.\n    def get_current_state(self):\n        """"""Get state of beam.""""""\n        return self.nextYs[-1]\n\n    # Get the backpointers for the current timestep.\n    def get_current_origin(self):\n        """"""Get the backpointer to the beam at this step.""""""\n        return self.prevKs[-1]\n\n    #  Given prob over words for every last beam `wordLk` and attention\n    #   `attnOut`: Compute and update the beam search.\n    #\n    # Parameters:\n    #\n    #     * `wordLk`- probs of advancing from the last step (K x words)\n    #     * `attnOut`- attention at the last step\n    #\n    # Returns: True if beam search is complete.\n\n    def advance(self, workd_lk, t):\n        """"""Advance the beam.""""""\n        # print(\'word_lk:\', workd_lk.size())\n        # print(\'t=\', t)\n        num_words = workd_lk.size(1)\n\n        # print(\'initial scores:\', self.scores, ""Prev:"", len(self.prevKs))\n        # Sum the previous scores.\n        if len(self.prevKs) > 0:\n            # print(\'Prev[0]:\', self.prevKs)\n            if self.norm_len:\n                beam_lk = (workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk) * t)/(t+1)\n            else:\n                beam_lk = workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk)\n            # print(\'beam scores:\', beam_lk) # beam_size * V\n        else:\n            beam_lk = workd_lk[0]\n            # print(\'beam scores:\', beam_lk)\n        flat_beam_lk = beam_lk.view(-1)\n        bestScores, bestScoresId = flat_beam_lk.topk(self.size, 0, True, True)\n        self.scores = bestScores\n        # bestScoresId is flattened beam x word array, so calculate which\n        # word and beam each score came from\n        prev_k = bestScoresId / num_words\n        self.prevKs.append(prev_k)\n        self.nextYs.append(bestScoresId - prev_k * num_words)\n\n        # End condition is when top-of-beam is EOS.\n        # print(self.nextYs[-1])\n        if self.nextYs[-1][0] == self.eos:\n            self.done = True\n\n        return self.done\n\n    def sort_best(self):\n        """"""Sort the beam.""""""\n        # print(\'sorting:\', self.scores)\n        return torch.sort(self.scores, 0, True)\n\n    # Get the score of the best in the beam.\n    def get_best(self):\n        """"""Get the most likely candidate.""""""\n        scores, ids = self.sort_best()\n        return scores[1], ids[1]\n\n    # Walk back to construct the full hypothesis.\n    #\n    # Parameters.\n    #\n    #     * `k` - the position in the beam to construct.\n    #\n    # Returns.\n    #\n    #     1. The hypothesis\n    #     2. The attention at each time step.\n    def get_hyp(self, k):\n        """"""Get hypotheses.""""""\n        hyp = []\n        # print(len(self.prevKs), len(self.nextYs), len(self.attn))\n        for j in range(len(self.prevKs) - 1, -1, -1):\n            hyp.append(self.nextYs[j + 1][k])\n            k = self.prevKs[j][k]\n\n        return hyp[::-1]\n'"
nmt/models/cond_decoder.py,12,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom .beam_search import Beam\nfrom .beam_onmt import Beam as Beam_ONMT\nfrom .lstm import LSTMAttention, LSTMAttentionV2, LSTM\n\n\nclass CondDecoder(nn.Module):\n    def __init__(self, params, enc_params, vocab_size, special_tokens):\n        nn.Module.__init__(self)\n        self.input_dim = params[\'input_dim\']\n        self.size = params[\'cell_dim\']\n        self.nlayers = 1\n        self.vocab_size = vocab_size\n        self.pad_token = 0\n        self.eos_token = special_tokens[\'EOS\']\n        self.bos_token = special_tokens[\'BOS\']\n        self.embedding = nn.Embedding(\n            self.vocab_size,\n            self.input_dim,\n            self.pad_token,\n            scale_grad_by_freq=bool(params[\'scale_grad_by_freq\'])\n        )\n        self.input_dropout = nn.Dropout(params[\'input_dropout\'])\n        # print(\'Setting attention!\', params[\'attention_mode\'])\n        if params[""attention_mode""] == ""none"":\n            self.cell = LSTM(self.input_dim,\n                             self.size,\n                             self.nlayers,\n                             batch_first=True,\n                             )\n        else:\n            if params[\'state_update\'] == 1:\n                self.cell = LSTMAttention(params, enc_params)\n            elif params[\'state_update\'] == 2:\n                self.cell = LSTMAttentionV2(params, enc_params)\n\n        self.prediction_dropout = nn.Dropout(params[\'prediction_dropout\'])\n        self.prediction = nn.Linear(self.size,\n                                    self.vocab_size)\n\n    def init_weights(self):\n        """"""Initialize weights.""""""\n        initdev = 0.01\n        self.embedding.weight.data.normal_(0.0, initdev)\n        self.prediction.bias.data.fill_(0)\n\n    def forward_(self, source, data):\n        labels = data[\'labels\']\n        emb = self.input_dropout(self.embedding(labels))\n        h, (_, _), _ = self.cell(\n            emb,\n            source[\'state\'],\n            source[\'ctx\'],\n            source[\'emb\']\n        )\n\n        # print(\'decoder hidden stats:\', h.size())\n        return h\n\n    def forward(self, source, data):\n        labels = data[\'labels\']\n        emb = self.input_dropout(self.embedding(labels))\n        h, (_, _), _ = self.cell(\n            emb,\n            source[\'state\'],\n            source[\'ctx\'],\n            source[\'emb\']\n        )\n\n        h_reshape = h.contiguous().view(\n            h.size()[0] * h.size()[1],\n            h.size()[2]\n        )\n        logits = F.log_softmax(\n            self.prediction(\n                self.prediction_dropout(\n                    h_reshape)),\n            dim=1)\n        logits = logits.view(\n            h.size(0),\n            h.size(1),\n            logits.size(1)\n        )\n        return logits\n\n    def sample_beam_new(self, source, scorer=None, kwargs={}):\n        beam_size = kwargs.get(\'beam_size\', 3)\n        state = source[\'state\']\n        source[""emb""] = source[""emb""].repeat(beam_size, 1, 1)\n        source[""ctx""] = source[""ctx""].repeat(beam_size, 1, 1)\n        batch_size = state[0].size(0)\n        dec_states = [state[0].repeat(beam_size, 1),\n                      state[1].repeat(beam_size, 1)]\n\n        # remap special tokens:\n        beam_args = {}\n        for k in [\'EOS\', \'BOS\', \'PAD\']:\n            beam_args[k.lower()] = kwargs[k]\n\n        beam = [Beam(beam_size, kwargs) for k in range(batch_size)]\n        beam = [Beam_ONMT(beam_size, **beam_args,\n                          cuda=True,\n                          global_scorer=scorer,\n                          block_ngram_repeat=kwargs[\'block_ngram_repeat\'],\n                          exclusion_tokens=set([self.eos_token,\n                                                self.bos_token]),\n                          stepwise_penalty=kwargs[\'stepwise_penalty\'])\n                for k in range(batch_size)]\n\n        batch_idx = list(range(batch_size))\n        remaining_sents = batch_size\n        max_length = kwargs.get(\'max_length\', 50)\n        for t in range(max_length):\n            input = torch.stack([b.get_current_state()\n                                 for b in beam if not b.done()]\n                                ).t().contiguous().view(1, -1)\n\n            emb = self.embedding(input.transpose(1, 0))\n            h, dec_states, attn = self.cell(emb,\n                                            dec_states,\n                                            source[\'ctx\'],\n                                            source[\'emb\'])\n            h_reshape = h.contiguous().view(\n                h.size()[0] * h.size()[1],\n                h.size()[2]\n            )\n            out = F.log_softmax(\n                self.prediction(h_reshape),\n                dim=1)\n\n            word_lk = out.view(beam_size,\n                               remaining_sents,\n                               -1).transpose(0, 1).contiguous()\n            active = []\n            for b in range(batch_size):\n                if beam[b].done():\n                    continue\n\n                idx = batch_idx[b]\n                if not beam[b].advance(word_lk.data[idx], attn):  #FIXME\n                    active += [b]\n\n                for dec_state in dec_states:  # iterate over h, c\n                    # layers x beam * sent x dim\n                    dec_size = dec_state.size()\n                    sent_states = dec_state.view(\n                        beam_size, remaining_sents, dec_size[-1]\n                    )[:, idx, :]\n                    sent_states.data.copy_(\n                        sent_states.data.index_select(\n                            0,\n                            beam[b].get_current_origin()\n                        )\n                    )\n            if not active:\n                break\n\n            # in this section, the sentences that are still active are\n            # compacted so that the decoder is not run on completed sentences\n            active_idx = torch.cuda.LongTensor([batch_idx[k] for k in active])\n            batch_idx = {beam: idx for idx, beam in enumerate(active)}\n\n            def update_active(t):\n                # select only the remaining active sentences\n                view = t.data.contiguous().view(\n                    -1, remaining_sents,\n                    # self.model.decoder.hidden_size\n                    t.size(-1)\n                )\n                new_size = list(t.size())\n                new_size[-2] = new_size[-2] * len(active_idx) \\\n                    // remaining_sents\n                result = view.index_select(1, active_idx).view(*new_size)\n                return result\n\n            dec_states = (\n                update_active(dec_states[0]),\n                update_active(dec_states[1])\n            )\n            source[\'ctx\'] = update_active(source[\'ctx\'].t()).t()\n            source[\'emb\'] = update_active(source[\'emb\'].t()).t()\n\n            remaining_sents = len(active)\n\n        # Wrap up\n        allHyp, allScores = [], []\n        n_best = 1\n        for b in range(batch_size):\n            scores, ks = beam[b].sort_finished(n_best)\n            # print(\'scores:\', scores)\n            # print(\'ks:\', ks)\n            allScores += [scores[:n_best]]\n            # hyps = list(zip(*[beam[b].get_hyp(k) for k in ks[:n_best]]))\n            hyps, _ = beam[b].get_hyp(*ks[0])\n            allHyp += [hyps]\n        return allHyp, allScores\n\n    def sample_beam(self, source, scorer=None, kwargs={}):\n        beam_size = kwargs.get(\'beam_size\', 3)\n        state = source[\'state\']\n        source[""emb""] = source[""emb""].repeat(beam_size, 1, 1)\n        source[""ctx""] = source[""ctx""].repeat(beam_size, 1, 1)\n        batch_size = state[0].size(0)\n        dec_states = [state[0].repeat(beam_size, 1),\n                      state[1].repeat(beam_size, 1)]\n\n        beam = [Beam(beam_size, kwargs) for k in range(batch_size)]\n        batch_idx = list(range(batch_size))\n        remaining_sents = batch_size\n        max_length = kwargs.get(\'max_length\', 50)\n        for t in range(max_length):\n            input = torch.stack([b.get_current_state()\n                                 for b in beam if not b.done]\n                                ).t().contiguous().view(1, -1)\n\n            emb = self.embedding(input.transpose(1, 0))\n            h, dec_states, _ = self.cell(emb,\n                                         dec_states,\n                                         source[\'ctx\'],\n                                         source[\'emb\'])\n            h_reshape = h.contiguous().view(\n                h.size()[0] * h.size()[1],\n                h.size()[2]\n            )\n            out = F.log_softmax(\n                self.prediction(h_reshape),\n                dim=1)\n\n            word_lk = out.view(beam_size,\n                               remaining_sents,\n                               -1).transpose(0, 1).contiguous()\n            active = []\n            for b in range(batch_size):\n                if beam[b].done:\n                    continue\n\n                idx = batch_idx[b]\n                if not beam[b].advance(word_lk.data[idx], t):\n                    active += [b]\n\n                for dec_state in dec_states:  # iterate over h, c\n                    # layers x beam * sent x dim\n                    dec_size = dec_state.size()\n                    sent_states = dec_state.view(\n                        beam_size, remaining_sents, dec_size[-1]\n                    )[:, idx, :]\n                    sent_states.data.copy_(\n                        sent_states.data.index_select(\n                            0,\n                            beam[b].get_current_origin()\n                        )\n                    )\n            if not active:\n                break\n\n            # in this section, the sentences that are still active are\n            # compacted so that the decoder is not run on completed sentences\n            active_idx = torch.cuda.LongTensor([batch_idx[k] for k in active])\n            batch_idx = {beam: idx for idx, beam in enumerate(active)}\n\n            def update_active(t):\n                # select only the remaining active sentences\n                view = t.data.contiguous().view(\n                    -1, remaining_sents,\n                    # self.model.decoder.hidden_size\n                    t.size(-1)\n                )\n                new_size = list(t.size())\n                new_size[-2] = new_size[-2] * len(active_idx) \\\n                    // remaining_sents\n                result = view.index_select(1, active_idx).view(*new_size)\n                return result\n\n            dec_states = (\n                update_active(dec_states[0]),\n                update_active(dec_states[1])\n            )\n            source[\'ctx\'] = update_active(source[\'ctx\'].t()).t()\n            source[\'emb\'] = update_active(source[\'emb\'].t()).t()\n\n            remaining_sents = len(active)\n\n        # Wrap up\n        allHyp, allScores = [], []\n        n_best = 1\n        for b in range(batch_size):\n            scores, ks = beam[b].sort_best()\n            allScores += [scores[:n_best]]\n            # hyps = list(zip(*[beam[b].get_hyp(k) for k in ks[:n_best]]))\n            hyps = beam[b].get_hyp(ks[0])\n            allHyp += [hyps]\n        return allHyp, allScores\n\n    def sample(self, source, scorer=None, kwargs={}):\n        beam_size = kwargs.get(\'beam_size\', 1)\n        if beam_size > 1:\n            return self.sample_beam(source, scorer, kwargs)\n            # return self.sample_beam_new(source, scorer, kwargs)\n\n        state = source[\'state\']\n        batch_size = state[0].size(0)\n        max_length = kwargs.get(\'max_length\', 50)\n        seq = []\n        scores = None\n        for t in range(max_length):\n            if t == 0:\n                input = torch.LongTensor([[self.bos_token]\n                                          for i in range(batch_size)\n                                          ]).cuda()\n            emb = self.embedding(input)\n            h, state, _ = self.cell(\n                emb,\n                state,\n                source[\'ctx\'],\n                source[\'emb\']\n            )\n            h_reshape = h.contiguous().view(\n                h.size()[0] * h.size()[1],\n                h.size()[2]\n            )\n            logits = F.log_softmax(\n                self.prediction(h_reshape),\n                dim=1)[:, 1:]  # remove the proba of padding\n            np_logits = logits.data.cpu().numpy()\n            decoder_argmax = 1 + np_logits.argmax(axis=-1)\n            if t:\n                scores += np_logits[:, decoder_argmax - 1]\n            else:\n                scores = np_logits[:, decoder_argmax - 1]\n            next_preds = torch.from_numpy(decoder_argmax).view(-1, 1).cuda()\n            seq.append(next_preds)\n            input = next_preds\n            if t >= 2:\n                # stop when all finished\n                unfinished = torch.add(\n                        torch.mul((input ==\n                                   self.eos_token\n                                   ).type_as(logits), -1), 1)\n                if unfinished.sum().data[0] == 0:\n                    break\n\n        seq = torch.cat(seq, 1).data.cpu().numpy()\n        return seq, scores\n'"
nmt/models/conv1d.py,1,"b'import torch as t\nimport torch.nn as nn\n\n\nclass MaskedConv1d(nn.Conv1d):\n    """"""\n    Masked (autoregressive) conv1d\n    """"""\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=3, padding=1, dilation=1,\n                 groups=1, bias=False):\n        # pad = (dilation * (kernel_size - 1)) // 2\n        super(MaskedConv1d, self).__init__(in_channels, out_channels,\n                                           kernel_size,\n                                           padding=padding,\n                                           groups=groups,\n                                           dilation=dilation,\n                                           bias=bias)\n        self.register_buffer(\'mask\', self.weight.data.clone())\n        self.incremental_state = t.zeros(1, 1, 1)\n        _, _, kH = self.weight.size()\n        self.mask.fill_(1)\n        if kH > 1:\n            self.mask[:, :, kH // 2 + 1:] = 0\n\n    def forward(self, x):\n        self.weight.data *= self.mask\n        return super(MaskedConv1d, self).forward(x)\n\n    def update(self, x):\n        k = self.kernel_size // 2 + 1\n        buffer = self.incremental_state\n        if buffer.size(-1) < k:\n            output = self.forward(x)\n            self.incremental_state = x.clone()\n        else:\n            # shift the buffer and add the recent input:\n            buffer[:, :, :-1] = buffer[:, :, 1:].clone()\n            buffer[:, :, -1:] = x[:, :, -1:]\n            output = self.forward(buffer)\n            self.incremental_state = buffer.clone()\n        return output\n\n\n\n'"
nmt/models/conv2d.py,3,"b'import torch\nimport torch.nn as nn\nimport time\n\n\nclass AsymmetricMaskedConv2d(nn.Conv2d):\n    """"""\n    Masked (autoregressive) conv2d kx1 kernel\n    FIXME: particular case of the MaskedConv2d\n    """"""\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=3, dilation=1,\n                 groups=1, bias=False):\n        pad = (dilation * (kernel_size - 1)) // 2\n        super().__init__(in_channels, out_channels,\n                         (kernel_size, 1),\n                         padding=(pad, 0),\n                         groups=groups,\n                         dilation=dilation,\n                         bias=bias)\n        self.register_buffer(\'mask\', self.weight.data.clone())\n        _, _, kH, kW = self.weight.size()\n        self.mask.fill_(1)\n        if kH > 1:\n            self.mask[:, :, kH // 2 + 1:, :] = 0\n        self.incremental_state = torch.zeros(1, 1, 1, 1)\n\n    def forward(self, x, *args):\n        self.weight.data *= self.mask\n        return super().forward(x)\n\n    def update(self, x):\n        k = self.weight.size(2) // 2 + 1\n        buffer = self.incremental_state\n        if buffer.size(2) < k:\n            output = self.forward(x)\n            self.incremental_state = x.clone()\n        else:\n            # shift the buffer and add the recent input:\n            buffer[:, :, :-1, :] = buffer[:, :, 1:, :].clone()\n            buffer[:, :, -1:, :] = x[:, :, -1:, :]\n            output = self.forward(buffer)\n            self.incremental_state = buffer.clone()\n        return output\n\n\nclass MaskedConv2d(nn.Conv2d):\n    """"""\n    Masked (autoregressive) conv2d\n    """"""\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=3, dilation=1,\n                 groups=1, bias=False):\n        pad = (dilation * (kernel_size - 1)) // 2\n        super(MaskedConv2d, self).__init__(in_channels, out_channels,\n                                           kernel_size,\n                                           padding=pad,\n                                           groups=groups,\n                                           dilation=dilation,\n                                           bias=bias)\n        self.register_buffer(\'mask\', self.weight.data.clone())\n        _, _, kH, kW = self.weight.size()\n        self.mask.fill_(1)\n        if kH > 1:\n            self.mask[:, :, kH // 2 + 1:, :] = 0\n        self.incremental_state = torch.zeros(1, 1, 1, 1)\n\n    def forward(self, x, *args):\n        self.weight.data *= self.mask\n        return super(MaskedConv2d, self).forward(x)\n\n    def update(self, x):\n        k = self.weight.size(2) // 2 + 1\n        buffer = self.incremental_state\n        if buffer.size(2) < k:\n            output = self.forward(x)\n            self.incremental_state = x.clone()\n        else:\n            # shift the buffer and add the recent input:\n            buffer[:, :, :-1, :] = buffer[:, :, 1:, :].clone()\n            buffer[:, :, -1:, :] = x[:, :, -1:, :]\n            output = self.forward(buffer)\n            self.incremental_state = buffer.clone()\n        return output\n\n\nclass GatedConv2d(MaskedConv2d):\n    """"""\n    Gated version of the masked conv2d\n    """"""\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=3, dilation=1,\n                 bias=False, groups=1):\n        super(GatedConv2d, self).__init__(in_channels,\n                                          2*out_channels,\n                                          kernel_size,\n                                          dilation=dilation,\n                                          bias=bias)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = super(GatedConv2d, self).forward(x)\n        mask, out = x.chunk(2, dim=1)\n        mask = self.sigmoid(mask)\n        return out * mask\n\n\n\n'"
nmt/models/dense_modules.py,10,"b'""""""\nVariants of dense layers\n""""""\n\nfrom math import sqrt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .conv2d import MaskedConv2d, GatedConv2d, AsymmetricMaskedConv2d\n\n\ndef _setup_conv_dilated(num_input_features, kernel_size, params, first=False):\n    """"""\n    Common setup of convolutional layers in a dense layer\n    """"""\n    bn_size = params.get(\'bn_size\', 4)\n    growth_rate = params.get(\'growth_rate\', 32)\n    bias = params.get(\'bias\', 0)\n    drop_rate = params.get(\'conv_dropout\', 0.)\n    init_weights = params.get(\'init_weights\', 0)\n    weight_norm = params.get(\'weight_norm\', 0)\n    gated = params.get(\'gated\', 0)\n    dilation = params.get(\'dilation\', 2)\n    print(\'Dilation: \', dilation)\n\n    CV = GatedConv2d if gated else MaskedConv2d\n    interm_features = bn_size * growth_rate\n    conv1 = nn.Conv2d(\n        num_input_features,\n        interm_features,\n        kernel_size=1,\n        bias=bias)\n    conv2 = CV(\n        interm_features,\n        interm_features,\n        kernel_size=kernel_size,\n        bias=bias)\n\n    conv3 = CV(\n        interm_features,\n        growth_rate,\n        kernel_size=kernel_size,\n        bias=bias,\n        dilation=dilation)\n\n    if init_weights == ""manual"":\n        if not first:\n            # proceeded by dropout and relu\n            cst = 2 * (1 - drop_rate) \n        else:\n            cst = 1\n        # n_l = num_input_features \n        std1 = sqrt(cst / num_input_features)\n        conv1.weight.data.normal_(0, std1)\n        # n_l = num_input_features * k * [(k-1)/2]\n        # only relu\n        std2 = sqrt(2 / (interm_featires * kernel_size *\n                         (kernel_size - 1) // 2))\n        conv2.weight.data.normal_(0, std2)\n        conv3.weight.data.normal_(0, std2)\n\n        if bias:\n            conv1.bias.data.zero_()\n            conv2.bias.data.zero_()\n            conv3.bias.data.zero_()\n\n    elif init_weights == ""kaiming"":\n        nn.init.kaiming_normal_(conv1.weight, mode=""fan_out"", nonlinearity=\'relu\')\n        nn.init.kaiming_normal_(conv2.weight, mode=""fan_out"", nonlinearity=\'relu\')\n        nn.init.kaiming_normal_(conv3.weight, mode=""fan_out"", nonlinearity=\'relu\')\n\n    if weight_norm:\n        conv1 = nn.utils.weight_norm(conv1, dim=0) # dim = None ?\n        conv2 = nn.utils.weight_norm(conv2, dim=0)\n        conv3 = nn.utils.weight_norm(conv3, dim=0)\n\n    return conv1, conv2, conv3\n\n\ndef _setup_conv(num_input_features, kernel_size, params, first=False):\n    """"""\n    Common setup of convolutional layers in a dense layer\n    """"""\n    bn_size = params.get(\'bn_size\', 4)\n    growth_rate = params.get(\'growth_rate\', 32)\n    bias = params.get(\'bias\', 0)\n    drop_rate = params.get(\'conv_dropout\', 0.)\n    init_weights = params.get(\'init_weights\', 0)\n    weight_norm = params.get(\'weight_norm\', 0)\n    gated = params.get(\'gated\', 0)\n    depthwise = params.get(\'depthwise\', 0)\n\n    CV = GatedConv2d if gated else MaskedConv2d\n    interm_features = bn_size * growth_rate\n    conv1 = nn.Conv2d(\n        num_input_features,\n        interm_features,\n        kernel_size=1,\n        bias=bias)\n    gp = growth_rate if depthwise else 1\n    conv2 = CV(\n        interm_features,\n        growth_rate,\n        kernel_size=kernel_size,\n        bias=bias,\n        groups=gp)\n\n    if init_weights == ""manual"":\n        # Init weights so that var(in) = var(out)\n        if not first:\n            # proceeded by dropout and relu\n            cst = 2 * (1 - drop_rate) \n        else:\n            cst = 1\n        # n_l = num_input_features \n        std1 = sqrt(cst / num_input_features)\n        conv1.weight.data.normal_(0, std1)\n        # n_l = num_input_features * k * [(k-1)/2]\n        # only relu\n        std2 = sqrt(2 / (bn_size * growth_rate * kernel_size *\n                                   (kernel_size - 1) // 2))\n        conv2.weight.data.normal_(0, std2)\n        if bias:\n            conv1.bias.data.zero_()\n            conv2.bias.data.zero_()\n\n    elif init_weights == ""kaiming"":\n        #  Use pytorch\'s kaiming_normal_\n        nn.init.kaiming_normal_(conv1.weight, mode=""fan_out"", nonlinearity=\'relu\')\n        nn.init.kaiming_normal_(conv2.weight, mode=""fan_out"", nonlinearity=\'relu\')\n\n    if weight_norm:\n        conv1 = nn.utils.weight_norm(conv1, dim=0) # dim = None ?\n        conv2 = nn.utils.weight_norm(conv2, dim=0)\n\n    return conv1, conv2\n\n\nclass _MainDenseLayer(nn.Module):\n    """"""\n    Main dense layer declined in 2 variants\n    """"""\n    def __init__(self,\n                 num_input_features,\n                 kernel_size,\n                 params\n                ):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.bn_size = params.get(\'bn_size\', 4)\n        self.growth_rate = params.get(\'growth_rate\', 32)\n        self.drop_rate = params.get(\'conv_dropout\', 0.)\n        \n    def forward(self, x):\n        new_features = self.seq(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(\n                new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n    def reset_buffers(self):\n        for layer in list(self.seq.children()):\n            if isinstance(layer, MaskedConv2d):\n                layer.incremental_state = torch.zeros(1, 1, 1, 1)\n        # self.conv2.incremental_state = torch.zeros(1, 1, 1, 1)\n\n    def update(self, x):\n        maxh = self.kernel_size // 2 + 1\n        if x.size(2) > maxh:\n            x = x[:, :, -maxh:, :].contiguous()\n        res = x\n        for layer in list(self.seq.children()):\n            if isinstance(layer, MaskedConv2d):\n                x = layer.update(x)\n            else:\n                x = layer(x)\n        return torch.cat([res, x], 1)\n\n    def track(self, x):\n        new_features = self.seq(x)\n        return x, new_features\n\n\nclass DenseLayer(_MainDenseLayer):\n    """"""\n    BN > ReLU > Conv(1) > BN > ReLU > Conv(k)\n    """"""\n    def __init__(self,\n                 num_input_features,\n                 kernel_size,\n                 params,\n                 first=False\n                ):\n        super().__init__(num_input_features, kernel_size, params)\n        conv1, conv2 = _setup_conv(num_input_features, kernel_size, params)\n        self.seq = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            conv1,\n            nn.BatchNorm2d(self.bn_size * self.growth_rate),\n            nn.ReLU(inplace=True),\n            conv2\n            )\n\n\nclass DenseLayer_midDP(_MainDenseLayer):\n    """"""\n    BN > ReLU > Conv(1) > Dropout > BN > ReLU > Conv(k)\n    """"""\n    def __init__(self,\n                 num_input_features,\n                 kernel_size,\n                 params,\n                 first=False\n                ):\n        super().__init__(num_input_features, kernel_size, params)\n        conv1, conv2 = _setup_conv(num_input_features, kernel_size, params)\n        self.seq = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            conv1,\n            nn.Dropout(p=self.drop_rate, inplace=True),\n            nn.BatchNorm2d(self.bn_size * self.growth_rate),\n            nn.ReLU(inplace=True),\n            conv2\n            )\n\n\nclass DenseLayer_noBN(_MainDenseLayer):\n    """"""\n    ReLU > Conv(1) > ReLU > Conv(k)\n    #TODO: check activ\' var\n    """"""\n    def __init__(self,\n                 num_input_features,\n                 kernel_size,\n                 params,\n                 first=False\n                ):\n        super().__init__(num_input_features, kernel_size, params)\n        conv1, conv2 = _setup_conv(num_input_features, kernel_size, params, first=first)\n        self.seq = nn.Sequential(\n            nn.ReLU(inplace=True),\n            conv1,\n            nn.ReLU(inplace=True),\n            conv2\n            )\n\n\nclass DenseLayer_Dil(_MainDenseLayer):\n    """"""\n    BN > ReLU > Conv(1)\n    > BN > ReLU > Conv(k)\n    > BN > ReLU > Conv(k, dilated)\n\n    """"""\n    def __init__(self,\n                 num_input_features,\n                 kernel_size,\n                 params,\n                 first=False\n                ):\n        super().__init__(num_input_features, kernel_size, params)\n        conv1, conv2, conv3 = _setup_conv_dilated(num_input_features,\n                                                  kernel_size,\n                                                  params)\n        self.seq = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            conv1,\n            nn.BatchNorm2d(self.bn_size * self.growth_rate),\n            nn.ReLU(inplace=True),\n            conv2,\n            nn.BatchNorm2d(self.bn_size * self.growth_rate),\n            nn.ReLU(inplace=True),\n            conv3\n            )\n\n\n\nclass DenseLayer_Asym(nn.Module):\n    """"""\n    Dense layer with asymmetric convolution ie decompose a 3x3 conv into\n    a 3x1 1D conv followed by a 1x3 1D conv.\n    As suggested in: \n    Efficient Dense Modules of Asymmetric Convolution for\n    Real-Time Semantic Segmentation\n    https://arxiv.org/abs/1809.06323\n    """"""\n    def __init__(self,\n                 num_input_features,\n                 kernel_size,\n                 params,\n                 first=False\n                ):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.drop_rate = params.get(\'conv_dropout\', 0.)\n        bias = params.get(\'bias\', 0)\n        bn_size = params.get(\'bn_size\', 4)\n        growth_rate = params.get(\'growth_rate\', 32)\n        dim1 = bn_size * growth_rate\n        dim2 = bn_size // 2 * growth_rate\n\n        conv1 = nn.Conv2d(\n            num_input_features,\n            dim1,\n            kernel_size=1,\n            bias=False)\n\n        pad = (kernel_size - 1) // 2\n        conv2s = nn.Conv2d(\n            dim1,\n            dim2,\n            kernel_size=(1, kernel_size),\n            padding=(0, pad),\n            bias=False)\n\n        conv2t = AsymmetricMaskedConv2d(\n            dim2,\n            growth_rate,\n            kernel_size=kernel_size,\n            bias=False)\n        \n        self.seq = nn.Sequential(\n            nn.BatchNorm2d(num_input_features),\n            nn.ReLU(inplace=True),\n            conv1,\n            nn.BatchNorm2d(dim1),\n            nn.ReLU(inplace=True),\n            conv2s,\n            conv2t\n            )\n\n    def forward(self, x):\n        new_features = self.seq(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(\n                new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n    def reset_buffers(self):\n        for layer in list(self.seq.children()):\n            if isinstance(layer, AsymmetricMaskedConv2d):\n                layer.incremental_state = torch.zeros(1, 1, 1, 1)\n        # self.conv2.incremental_state = torch.zeros(1, 1, 1, 1)\n\n    def update(self, x):\n        maxh = self.kernel_size // 2 + 1\n        if x.size(2) > maxh:\n            x = x[:, :, -maxh:, :].contiguous()\n        res = x\n        for layer in list(self.seq.children()):\n            if isinstance(layer, AsymmetricMaskedConv2d):\n                x = layer.update(x)\n            else:\n                x = layer(x)\n        return torch.cat([res, x], 1)\n\n    def track(self, x):\n        new_features = self.seq(x)\n        return x, new_features\n\n\n'"
nmt/models/densenet.py,2,"b'""""""\nDenseNet architecture\n""""""\n\nfrom math import sqrt\nimport torch\nimport torch.nn as nn\nfrom .dense_modules import *\nfrom .transitions import Transition, Transition2\n\n\nclass DenseBlock(nn.Sequential):\n    def __init__(self, num_layers,\n                 num_input_features,\n                 kernels,\n                 params):\n        super(DenseBlock, self).__init__()\n        layer_type = params.get(\'layer_type\', 1)\n        growth_rate = params.get(\'growth_rate\', 32)\n        if layer_type == ""regular"":\n            LayerModule = DenseLayer\n        elif layer_type == ""mid-dropout"":  # Works fine, basically another dropout\n            LayerModule = DenseLayer_midDP\n        elif layer_type == ""nobn"":  # W/o BN works fine if weights initialized ""correctly""\n            LayerModule = DenseLayer_noBN\n        elif layer_type == ""asym"":\n            LayerModule = DenseLayer_Asym\n        elif layer_type == ""dilated"": # 3 conv in each layer, the 3rd being dilated\n            LayerModule = DenseLayer_Dil\n        else:\n            raise ValueError(\'Unknown type: %d\' % layer_type)\n        print(\'Dense channels:\', num_input_features, end=\'\')\n        for i in range(num_layers):\n            print("">"", num_input_features + (i + 1) * growth_rate, end=\'\')\n            layer = LayerModule(\n                num_input_features + i * growth_rate,\n                kernels[i],\n                params,\n                first=i==0,\n                )\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n        \n    def update(self, x):\n        for layer in list(self.children()):\n            x = layer.update(x)\n        return x\n\n    def reset_buffers(self):\n        for layer in list(self.children()):\n            layer.reset_buffers()\n\n    def track(self, x):\n        activations = []\n        for layer in list(self.children()):\n            # layer is a DenseLayer\n            x, newf = layer.track(x)\n            activations.append(newf.data.cpu().numpy())\n            x = torch.cat([x, newf], 1)\n        return x, activations\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, num_init_features, params):\n        super(DenseNet, self).__init__()\n        block_layers = params.get(\'num_layers\', (24))\n        block_kernels = params[\'kernels\']\n        growth_rate = params.get(\'growth_rate\', 32)\n        divide_channels = params.get(\'divide_channels\', 2)\n        init_weights = params.get(\'init_weights\', 0)\n        normalize_channels = params.get(\'normalize_channels\', 0)\n        transition_type = params.get(\'transition_type\', 1)\n        skip_last_trans = params.get(\'skip_last_trans\', 0)\n\n        if transition_type == 1:\n            TransitionLayer = Transition\n        elif transition_type == 2:\n            TransitionLayer = Transition2\n\n        self.features = nn.Sequential()\n        num_features = num_init_features\n        # start by normalizing the input channels #FIXME\n        if normalize_channels:\n            self.features.add_module(\'initial_norm\',\n                                     nn.GroupNorm(1, num_features))\n\n        # start by reducing the input channels\n        if divide_channels > 1:\n            # In net2: trans = TransitionLayer\n            trans = nn.Conv2d(num_features, num_features // divide_channels, 1)\n            if init_weights == ""manual"":\n                std = sqrt(1 / num_features)\n                trans.weight.data.normal_(0, std)\n            self.features.add_module(\'initial_transition\', trans)\n            num_features = num_features // divide_channels\n        # Each denseblock\n        for i, (num_layers, kernels) in enumerate(zip(block_layers,\n                                                      block_kernels)):\n            block = DenseBlock(num_layers, num_features,\n                                kernels, params)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            # In net2: Only between blocks\n            if not i == len(block_layers) - 1 or not skip_last_trans:\n                trans = TransitionLayer(\n                    num_input_features=num_features,\n                    num_output_features=num_features // 2,\n                    init_weights=init_weights)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n                print(""> (trans) "", num_features, end=\'\')\n        print()\n        self.output_channels = num_features\n        # Final batch norm\n        self.features.add_module(\'norm_last\', nn.BatchNorm2d(num_features))\n        self.features.add_module(\'relu_last\', nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        return self.features(x.contiguous())\n\n    def update(self, x):\n        x = x.contiguous()\n        for layer in list(self.features.children()):\n            if isinstance(layer, DenseBlock):\n                x = layer.update(x)\n            else:\n                x = layer(x)\n        return x\n\n    def reset_buffers(self):\n        for layer in list(self.features.children()):\n            if isinstance(layer, DenseBlock):\n                layer.reset_buffers()\n\n    def track(self, x):\n        activations = []\n        x = x.contiguous()\n        for layer in list(self.features.children()):\n            if isinstance(layer, DenseBlock):\n                x, actv = layer.track(x)\n                activations.append(actv)\n            else:\n                x = layer(x)\n        return x, activations\n'"
nmt/models/efficient_densenet.py,7,"b'# Adapted from:\n# https://github.com/gpleiss/efficient_densenet_pytorch/blob/master/models/densenet.py\n\nimport math\nfrom math import sqrt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom collections import OrderedDict\nfrom .conv2d import MaskedConv2d, GatedConv2d\nfrom .transitions import Transition\n\n\ndef _bn_function_factory(norm, relu, conv):\n    def bn_function(*inputs):\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = conv(relu(norm(concated_features)))\n        return bottleneck_output\n\n    return bn_function\n\n\nclass _DenseLayer(nn.Module):\n    def __init__(self,\n                 num_input_features,\n                 growth_rate,\n                 kernel_size=3,\n                 bn_size=4,\n                 drop_rate=0,\n                 gated=False,\n                 bias=False,\n                 init_weights=0,\n                 weight_norm=False,\n                 efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.kernel_size = kernel_size\n        self.drop_rate = drop_rate\n        self.efficient = efficient\n        if gated:\n            CV = GatedConv2d\n        else:\n            CV = MaskedConv2d\n        conv1 = nn.Conv2d(\n            num_input_features,\n            bn_size * growth_rate,\n            kernel_size=1,\n            bias=bias)\n        conv2 = CV(\n            bn_size * growth_rate,\n            growth_rate,\n            kernel_size=kernel_size,\n            bias=bias)\n        if init_weights == ""manual"":\n            std1 = sqrt(2 / num_input_features)\n            conv1.weight.data.normal_(0, std1)\n            std2 = sqrt(\n                2 * (1 - drop_rate) / (bn_size * growth_rate * kernel_size *\n                                       (kernel_size - 1) // 2))\n            conv2.weight.data.normal_(0, std2)\n            if bias:\n                conv1.bias.data.zero_()\n                conv2.bias.data.zero_()\n        elif init_weights == ""kaiming"":\n            nn.init.kaiming_normal_(conv1.weight, mode=""fan_out"", nonlinearity=\'relu\')\n            nn.init.kaiming_normal_(conv2.weight, mode=""fan_out"", nonlinearity=\'relu\')\n        if weight_norm:\n            conv1 = nn.utils.weight_norm(conv1, dim=0)\n            conv2 = nn.utils.weight_norm(conv2, dim=0)\n\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', conv1)\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', conv2)\n        \n    def forward(self, *prev_features):\n        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n        if self.efficient and any(prev_feature.requires_grad\n                                  for prev_feature in prev_features):\n            # Does not compute intermediate values, but recompute them in the backward pass:\n            # tradeoff btw memory & computation\n            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n        else:\n            bottleneck_output = bn_function(*prev_features)\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate,\n                                     training=self.training)\n        return new_features\n\n    def reset_buffers(self):\n        self.conv2.incremental_state = torch.zeros(1, 1, 1, 1)\n\n    def update(self, x):\n        maxh = self.kernel_size // 2 + 1\n        if x.size(2) > maxh:\n            x = x[:, :, -maxh:, :].contiguous()\n        res = x\n        x = self.conv1(self.relu1(self.norm1(x)))\n        x = self.conv2.update(self.relu2(self.norm2(x)))\n        return torch.cat([res, x], 1)\n\n        \nclass _DenseBlock(nn.Module):\n    def __init__(self, num_layers, num_input_features,\n                 kernels, bn_size,\n                 growth_rate, drop_rate, gated,\n                 bias, init_weights,\n                 weight_norm,\n                 efficient=False):\n        super(_DenseBlock, self).__init__()\n        print(\'Dense channels:\', num_input_features, end=\'\')\n        for i in range(num_layers):\n            print("">"", num_input_features + (i+1) * growth_rate, end=\'\')\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate,\n                kernels[i],\n                bn_size,\n                drop_rate,\n                gated=gated,\n                bias=bias,\n                init_weights=init_weights,\n                weight_norm=weight_norm,\n                efficient=efficient,\n            )\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for name, layer in self.named_children():\n            new_features = layer(*features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n    def update(self, x):\n        for layer in list(self.children()):\n            x = layer.update(x)\n        return x\n\n    def reset_buffers(self):\n        for layer in list(self.children()):\n            layer.reset_buffers()\n\n\n\nclass Efficient_DenseNet(nn.Module):\n    """""" \n    efficient (bool):\n    set to True to use checkpointing. Much more memory efficient, but slower.\n    """"""\n    def __init__(self, num_init_features, params):\n        super(Efficient_DenseNet, self).__init__()\n        growth_rate = params.get(\'growth_rate\', 32)\n        block_layers = params.get(\'num_layers\', (6, 12, 24, 16))\n        # kernel_size = params.get(\'kernel\', 3)\n        block_kernels = params[\'kernels\']\n        bn_size = params.get(\'bn_size\', 4)\n        drop_rate = params.get(\'conv_dropout\', 0)\n        gated = params.get(\'gated\', 0)\n        bias = bool(params.get(\'bias\', 1))\n        init_weights = params.get(\'init_weights\', 0)\n        weight_norm = params.get(\'weight_norm\', 0)\n        divide_channels = params.get(\'divide_channels\', 2)\n        efficient = params.get(\'efficient\', 0)\n\n        self.features = nn.Sequential()\n        num_features = num_init_features\n        # start by reducig the input channels\n        if divide_channels > 1:\n            trans = nn.Conv2d(num_features, num_features // divide_channels, 1)\n            if init_weights == ""manual"":\n                std = sqrt(1 / num_features)\n                trans.weight.data.normal_(0, std)\n            self.features.add_module(\'initial_transition\', trans)\n            num_features = num_features // divide_channels\n\n        # Each denseblock\n        for i, (num_layers, kernels) in enumerate(zip(block_layers,\n                                                      block_kernels)):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                kernels=kernels,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                gated=gated,\n                bias=bias,\n                init_weights=init_weights,\n                weight_norm=weight_norm,\n                efficient=efficient\n            )\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            trans = Transition(\n                num_input_features=num_features,\n                num_output_features=num_features // 2,\n                init_weights=init_weights)\n            self.features.add_module(\'transition%d\' % (i + 1), trans)\n            num_features = num_features // 2\n            print(""> (trans) "", num_features)\n\n        self.output_channels = num_features\n        # Final batch norm\n        self.features.add_module(\'norm_final\', nn.BatchNorm2d(num_features))\n        self.features.add_module(\'relu_last\', nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        return  self.features(x.contiguous())\n\n    def update(self, x):\n        x = x.contiguous()\n        for layer in list(self.features.children()):\n            if isinstance(layer, _DenseBlock):\n                x = layer.update(x)\n            else:\n                x = layer(x)\n        return x\n\n    def reset_buffers(self):\n        for layer in list(self.features.children()):\n            if isinstance(layer, _DenseBlock):\n                layer.reset_buffers()\n\n\n'"
nmt/models/embedding.py,12,"b'from math import sqrt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .conv1d import MaskedConv1d\n\n\ndef read_list(param):\n    """""" Parse list of integers """"""\n    return [int(p) for p in str(param).split(\',\')]\n\n\ndef make_positions(tensor, padding_idx, left_pad):\n    len = tensor.size(1)\n    max_pos = padding_idx + 1 + len\n    out = torch.arange(padding_idx + 1, max_pos).long().cuda()\n    mask = tensor.ne(padding_idx)\n    positions = out[:len].expand_as(tensor)\n    final = tensor.clone().masked_scatter_(mask, positions[mask])\n    if left_pad:\n        zero_left = torch.zeros(tensor.size(0), 1).type_as(final)\n        final = torch.cat([\n            torch.zeros(tensor.size(0), 1).type_as(final),\n            final[:, :-1]\n        ], dim=1)\n\n    return final\n\n\nclass PosEmbedding(nn.Embedding):\n    # CHECK\n    def __init__(self, max_length, position_dim, pad_left=False):\n        super(PosEmbedding, self).__init__(max_length, position_dim, 0)\n        self.pad_left = pad_left\n\n    def forward(self, labels):\n        positions = make_positions(labels, self.padding_idx, self.pad_left)\n        return super().forward(positions)\n\n    def map(self, inputs):\n        return super(PosEmbedding, self).forward(inputs)\n\n\nclass Embedding(nn.Module):\n    def __init__(self, params,\n                 vocab_size, padding_idx,\n                 pad_left=False):\n\n        nn.Module.__init__(self)\n        self.dimension = params[\'input_dim\']\n        self.encode_length = params[\'encode_length\']\n        self.encode_position = params[\'encode_position\']\n        self.dropout = params[\'input_dropout\']\n        self.init_std = params.get(\'init_std\', .01)\n        self.zero_pad = params.get(\'zero_pad\', 0)\n        self.padding_idx = padding_idx\n        # self.normalize = params.get(\'normalize\', 0)\n        self.label_embedding = nn.Embedding(\n            vocab_size,\n            self.dimension,\n            padding_idx,\n            scale_grad_by_freq=False\n        )\n\n        if self.encode_position:\n            self.pos_embedding = PosEmbedding(params[\'max_length\'],\n                                              self.dimension,\n                                              pad_left=pad_left)\n\n        if self.encode_length:\n            self.dimension += self.encode_length\n            # Encode Ts and Tt in the embeddings:\n            self.length_embedding = nn.Embedding(params[\'max_length\'],\n                                                 self.encode_length)\n\n    def init_weights(self):\n        std = self.init_std\n        self.label_embedding.weight.data.normal_(0, std)\n        # fill padding with zero (default in pytorch if not reinitializing)\n        if self.zero_pad:\n            self.label_embedding.weight.data[self.padding_idx].fill_(0)\n        if self.encode_position:\n            self.pos_embedding.weight.data.normal_(0, std)\n        if self.encode_length:\n            self.length_embedding.weight.data.normal_(0, std)\n        # if self.normalize:\n            # W = self.label_embedding.weight\n            # norms = torch.norm(W, p=2, dim=1, keepdim=True).expand_as(W)\n            # self.label_embedding.weight = W.div(norms)\n\n    def forward(self, data):\n        labels = data[""labels""]\n        emb = self.label_embedding(labels)\n        if self.encode_position:\n            pos = self.pos_embedding(labels)\n            # print(\'lab emb:\', emb, emb.dtype, emb.device)\n            # print(\'pos emb:\', pos, pos.dtype, pos.device)\n            emb = sqrt(0.5) * (emb + pos)\n        if self.encode_length:\n            lens = self.length_embedding(data[\'lengths\']).unsqueeze(\n                1\n            ).repeat(1, emb.size(1), 1)\n            emb = torch.cat((emb, lens), dim=2)\n        if self.dropout:\n            emb = F.dropout(emb,\n                            p=self.dropout,\n                            training=self.training)\n        return emb\n\n    def single_token(self, tok, position, length=None):\n        emb = self.label_embedding(tok)\n        if self.encode_position:\n            position = torch.ones((tok.size(0), 1)).type_as(tok) * position\n            pos = self.pos_embedding.map(position)\n            emb += pos\n        if self.encode_length:\n            lens = self.length_embedding(length).unsqueeze(\n                1\n            ).repeat(1, emb.size(1), 1)\n            emb = torch.cat((emb, lens), dim=2)\n        if self.dropout:\n            emb = F.dropout(emb,\n                            p=self.dropout,\n                            training=self.training)\n        return emb\n\n    def reset_buffers(self):\n        pass\n\n\nclass ConvEmbedding(nn.Module):\n    """"""\n    A 1d convolutional network on top of the lookup embeddings.\n    TODO : \xc3\xa0 la ELMO, learnable combination of the layers activations\n    TODO : add skip connections\n    """"""\n    def __init__(self, params,\n                 vocab_size, padding_idx,\n                 is_target=False):\n        nn.Module.__init__(self)\n        self.dimension = params[\'input_dim\']\n        self.encode_length = params[\'encode_length\']\n        self.encode_position = params[\'encode_position\']\n        self.dropout = params[\'input_dropout\']\n        self.init_std = params.get(\'init_std\', .01)\n        self.nlayers = params[\'num_layers\']\n        kernels = read_list(params[\'kernels\'])\n        out_channels = read_list(params[\'channels\'])\n        assert len(out_channels) == self.nlayers, ""Number of channels should match the depth""\n        assert len(kernels) == self.nlayers, ""Number of kernel sizes should match the depth""\n        out_channels.insert(0, self.dimension)\n        print(\'channels:\', out_channels, ""kernels:"", kernels)\n        self.kernel_size = max(kernels)  \n        #FIXME often times the same kernel size! \n        #TODO if not, buffer size different for each conv\n        self.label_embedding = nn.Embedding(\n            vocab_size,\n            self.dimension,\n            padding_idx,\n            scale_grad_by_freq=False\n        )\n        self.conv = nn.Sequential()\n        if is_target:\n            conv = MaskedConv1d\n            self.incremental_state = None\n        else:\n            conv = nn.Conv1d\n\n        for l in range(self.nlayers):\n            kernel = kernels[l]\n            pad = (kernel - 1) // 2\n            self.conv.add_module(""conv%d"" % l,\n                                 conv(out_channels[l],\n                                      out_channels[l+1],\n                                      kernel,\n                                      padding=pad,\n                                      bias=False))\n            print(\'%d > %d\' % (out_channels[l], out_channels[l+1]))\n        self.dimension = out_channels[-1]\n\n    def init_weights(self):\n        self.label_embedding.weight.data.normal_(0, self.init_std)\n\n    def forward(self, data):\n        labels = data[""labels""]\n        emb = self.label_embedding(labels)\n        emb = emb.permute(0, 2, 1)\n        emb = self.conv(emb)\n        if self.dropout:\n            emb = F.dropout(emb,\n                            p=self.dropout,\n                            training=self.training)\n        emb = emb.permute(0, 2, 1)\n        return emb\n\n    def single_token(self, labels, position=0):  # FIXME change to update\n        if self.incremental_state is not None:\n            if self.incremental_state.size(1) >= self.kernel_size:\n                buffer = self.incremental_state\n                # shift the buffer and add the recent input:\n                buffer[:, :-1] = buffer[:, 1:].clone()\n                buffer[:, -1:] = labels[:, -1:]\n                labels = buffer\n            else:\n                buffer = self.incremental_state\n                # shift the buffer and add the recent input:\n                buffer = torch.cat((buffer, labels), dim=1)\n                labels = buffer\n\n        self.incremental_state = labels\n        emb = self.label_embedding(labels)\n        emb = emb.permute(0, 2, 1)\n        for cvl in list(self.conv.children()):\n            emb = cvl(emb)\n        emb = emb.permute(0, 2, 1)\n        return emb\n\n    def reset_buffers(self):\n        self.incremental_state = None\n        for clv in list(self.conv.children()):\n            clv.incremental_state = torch.zeros(1, 1, 1)\n\n\n'"
nmt/models/encoder.py,8,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\n\n""""""\nAn (bi-)LSTM/GRU encoder\n""""""\nclass Encoder(nn.Module):\n    def __init__(self, params, vocab_size):\n        nn.Module.__init__(self)\n        # input\n        self.input_dim = params[\'input_dim\']\n        self.vocab_size = vocab_size\n        self.pad_token = 0\n        # cell\n        self.bidirectional = params[\'bidirectional\']\n        self.nd = 2 if self.bidirectional else 1\n        self.cell_type = params[\'cell_type\'].upper()\n        self.nlayers = params[\'num_layers\']\n        self.size = params[\'cell_dim\']\n        self.parallel = params[\'parallel\']\n        # if bidirectional split\n        self.hidden_dim = self.size // self.nd\n\n        # layers\n        self.embedding = nn.Embedding(\n            self.vocab_size,\n            self.input_dim,\n            self.pad_token,\n            scale_grad_by_freq=bool(params[\'scale_grad_by_freq\'])\n        )\n\n        self.input_dropout = nn.Dropout(params[\'input_dropout\'])\n        if params[\'cell_dropout\'] and self.nlayers == 1:\n            # dropout effective only if nlyaers > 1\n            params[\'cell_dropout\'] = 0\n        self.cell = getattr(nn,\n                            self.cell_type)(\n                                self.input_dim,\n                                self.hidden_dim,\n                                self.nlayers,\n                                bidirectional=self.bidirectional,\n                                batch_first=True,\n                                dropout=params[\'cell_dropout\']\n                            )\n\n\n    def init_weights(self):\n        """"""Initialize weights.""""""\n        initdev = 0.01\n        self.embedding.weight.data.normal_(0.0, initdev)\n        self.embedding.weight.data.normal_(0.0, initdev)\n        # FIXME add the option of initializing with loaded weights and freeze\n\n    def init_state(self, batch_size):\n        """"""Get cell states and hidden states.""""""\n        h0 = torch.zeros(\n            self.nlayers * self.nd,\n            batch_size,\n            self.hidden_dim\n        )\n        if self.cell_type == \'GRU\':\n            return h0.cuda()\n\n        c0 = torch.zeros(\n            self.nlayers * self.nd,\n            batch_size,\n            self.hidden_dim\n        )\n        return h0.cuda(), c0.cuda()\n\n\n    def forward(self, data):\n        labels = data[\'labels\']\n        lengths = data[\'lengths\']\n        batch_size = labels.size(0)\n        emb = self.input_dropout(self.embedding(labels))\n        _emb = emb  # to pass in case needed for attenion scores\n        pack_emb = pack_padded_sequence(emb,\n                                        lengths,\n                                        batch_first=True)\n\n        state = self.init_state(batch_size)\n        ctx, state = self.cell(pack_emb, state)\n        # unpack\n        ctx, _ = pad_packed_sequence(ctx,\n                                     batch_first=True)\n        if self.bidirectional:\n            if self.cell_type == ""LSTM"":\n                h_t = torch.cat((state[0][-1],\n                                 state[0][-2]), 1)\n                c_t = torch.cat((state[1][-1],\n                                 state[1][-2]), 1)\n                final_state = [h_t, c_t]\n\n            elif self.cell_type == ""GRU"":\n                h_t = torch.cat((state[-1],\n                                 state[-2]), 1)\n                final_state = [h_t]\n\n        else:\n            if self.cell_type == ""LSTM"":\n                h_t = state[0][-1]\n                c_t = state[1][-1]\n                final_state = [h_t, c_t]\n\n            elif self.cell_type == ""GRU"":\n                h_t = state[0][-1]\n                final_state = [h_t]\n        return {""emb"": _emb, ""ctx"": ctx, ""state"": final_state}\n\n'"
nmt/models/evaluate.py,1,"b'# -*- coding: utf-8 -*-\n\n""""""Evaluation utils.""""""\nimport logging\nfrom collections import Counter\nimport math\nimport time\nimport numpy as np\nimport torch\nfrom nmt.utils import decode_sequence\nimport nmt.utils.logging as lg\nfrom nmt.models.gnmt import GNMTGlobalScorer\nfrom nmt.utils import get_scores\n\n\ndef corpus_bleu(hypotheses, references, smoothing=False, order=4, **kwargs):\n    """"""\n    Computes the BLEU score at the corpus-level between a\n    list of translation hypotheses and references.\n    With the default settings, this computes the exact same\n    score as `multi-bleu.perl`.\n\n    All corpus-based evaluation functions should follow this interface.\n\n    :param hypotheses: list of strings\n    :param references: list of strings\n    :param smoothing: apply +1 smoothing\n    :param order: count n-grams up to this value of n.\n                  `multi-bleu.perl` uses a value of 4.\n    :param kwargs: additional (unused) parameters\n    :return: score (float), and summary containing additional information (str)\n    """"""\n    total = np.zeros((order,))\n    correct = np.zeros((order,))\n\n    hyp_length = 0\n    ref_length = 0\n\n    for hyp, ref in zip(hypotheses, references):\n        hyp = hyp.split()\n        ref = ref.split()\n\n        hyp_length += len(hyp)\n        ref_length += len(ref)\n\n        for i in range(order):\n            hyp_ngrams = Counter(zip(*[hyp[j:] for j in range(i + 1)]))\n            ref_ngrams = Counter(zip(*[ref[j:] for j in range(i + 1)]))\n\n            total[i] += sum(hyp_ngrams.values())\n            correct[i] += sum(min(count, ref_ngrams[bigram])\n                              for bigram, count in hyp_ngrams.items())\n\n    if smoothing:\n        total += 1\n        correct += 1\n\n    def divide(x, y):\n        with np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n            z = np.true_divide(x, y)\n            z[~ np.isfinite(z)] = 0\n        return z\n\n    scores = divide(correct, total)\n\n    score = math.exp(\n        sum(math.log(score) if score > 0 else float(\'-inf\') for score in scores) / order\n    )\n\n    bp = min(1, math.exp(1 - ref_length / hyp_length)) if hyp_length > 0 else 0.0\n    bleu = 100 * bp * score\n\n    return bleu, \'penalty={:.3f} ratio={:.3f}\'.format(bp, hyp_length / ref_length)\n\ndef score_split(job_name, model, src_loader, trg_loader, eval_kwargs):\n    """"""Return P(trg | src) for every pair""""""\n    scores = []\n    batch_size = eval_kwargs.get(\'batch_size\', 1)\n    max_samples = eval_kwargs.get(\'max_samples\', -1)\n    split = eval_kwargs.get(\'split\', \'val\')\n    verbose = eval_kwargs.get(\'verbose\', 0)\n    eval_kwargs[\'BOS\'] = trg_loader.bos\n    eval_kwargs[\'EOS\'] = trg_loader.eos\n    eval_kwargs[\'PAD\'] = trg_loader.pad\n    eval_kwargs[\'UNK\'] = trg_loader.unk\n    logger = logging.getLogger(job_name)\n\n    # Switch to evaluation mode\n    model.eval()\n    src_loader.reset_iterator(split)\n    trg_loader.reset_iterator(split)\n    n = 0\n    start = time.time()\n    while True:\n        # get batch\n        batch_size = 1\n        data_src, order = src_loader.get_src_batch(split, batch_size)\n        data_trg = trg_loader.get_trg_batch(split, order, batch_size)\n        n += batch_size\n        sc = get_scores(model(data_src, data_trg), data_trg[\'out_labels\'])\n        scores.append(sc.data.item())\n        print(\'score:\', scores[-1])\n        if max_samples == -1:\n            ix1 = data_src[\'bounds\'][\'it_max\']\n        else:\n            ix1 = max_samples\n        if data_src[\'bounds\'][\'wrapped\']:\n            break\n        if n >= ix1:\n            break\n    logger.warn(\'Evaluated %d samples in %.2f s\', n, time.time()-start)\n    return scores\n\n\ndef evaluate_val_loss(job_name, trainer, src_loader, trg_loader, eval_kwargs):\n    """"""Evaluate model.""""""\n    ground_truths = []\n    batch_size = eval_kwargs.get(\'batch_size\', 1)\n    max_samples = eval_kwargs.get(\'max_samples\', -1)\n    split = eval_kwargs.get(\'split\', \'val\')\n    verbose = eval_kwargs.get(\'verbose\', 0)\n    eval_kwargs[\'BOS\'] = trg_loader.bos\n    eval_kwargs[\'EOS\'] = trg_loader.eos\n    eval_kwargs[\'PAD\'] = trg_loader.pad\n    eval_kwargs[\'UNK\'] = trg_loader.unk\n    logger = logging.getLogger(job_name)\n\n    # Switch to evaluation mode\n    model = trainer.model\n    crit = trainer.criterion\n    model.eval()\n    src_loader.reset_iterator(split)\n    trg_loader.reset_iterator(split)\n    n = 0\n    loss_sum = 0\n    ml_loss_sum = 0\n    loss_evals = 0\n    start = time.time()\n    while True:\n        # get batch\n        data_src, order = src_loader.get_src_batch(split, batch_size)\n        data_trg = trg_loader.get_trg_batch(split, order, batch_size)\n        n += batch_size\n        if model.version == \'seq2seq\':\n            source = model.encoder(data_src)\n            source = model.map(source)\n            if trainer.criterion.version == ""seq"":\n                losses, stats = crit(model, source, data_trg)\n            else:  # ML & Token-level\n                # init and forward decoder combined\n                decoder_logit = model.decoder(source, data_trg)\n                losses, stats = crit(decoder_logit, data_trg[\'out_labels\'])\n        else:\n            losses, stats = crit(model(data_src, data_trg), data_trg[\'out_labels\'])\n\n        loss_sum += losses[\'final\'].data.item()\n        ml_loss_sum += losses[\'ml\'].data.item()\n        loss_evals = loss_evals + 1\n        if max_samples == -1:\n            ix1 = data_src[\'bounds\'][\'it_max\']\n        else:\n            ix1 = max_samples\n        if data_src[\'bounds\'][\'wrapped\']:\n            break\n        if n >= ix1:\n            break\n    logger.warn(\'Evaluated %d samples in %.2f s\', n, time.time()-start)\n    return ml_loss_sum / loss_evals, loss_sum / loss_evals\n\n\ndef evaluate_model(job_name, trainer, src_loader, trg_loader, eval_kwargs):\n    """"""Evaluate model.""""""\n    preds = []\n    ground_truths = []\n    batch_size = eval_kwargs.get(\'batch_size\', 1)\n    max_samples = eval_kwargs.get(\'max_samples\', -1)\n    split = eval_kwargs.get(\'split\', \'val\')\n    verbose = eval_kwargs.get(\'verbose\', 0)\n    eval_kwargs[\'BOS\'] = trg_loader.bos\n    eval_kwargs[\'EOS\'] = trg_loader.eos\n    eval_kwargs[\'PAD\'] = trg_loader.pad\n    eval_kwargs[\'UNK\'] = trg_loader.unk\n    logger = logging.getLogger(job_name)\n\n    # Make sure to be in evaluation mode\n    model = trainer.model\n    crit = trainer.criterion\n    model.eval()\n    src_loader.reset_iterator(split)\n    trg_loader.reset_iterator(split)\n    n = 0\n    loss_sum = 0\n    ml_loss_sum = 0\n    loss_evals = 0\n    start = time.time()\n    while True:\n        # get batch\n        data_src, order = src_loader.get_src_batch(split, batch_size)\n        data_trg = trg_loader.get_trg_batch(split, order, batch_size)\n        n += batch_size\n        if model.version == \'seq2seq\':\n            source = model.encoder(data_src)\n            source = model.map(source)\n            if trainer.criterion.version == ""seq"":\n                losses, stats = crit(model, source, data_trg)\n            else:  # ML & Token-level\n                # init and forward decoder combined\n                decoder_logit = model.decoder(source, data_trg)\n                losses, stats = crit(decoder_logit, data_trg[\'out_labels\'])\n            batch_preds, _ = model.sample(source, eval_kwargs)\n        else:\n            losses, stats = crit(model(data_src, data_trg), data_trg[\'out_labels\'])\n            batch_preds, _ = model.sample(data_src, eval_kwargs)\n\n        loss_sum += losses[\'final\'].data.item()\n        ml_loss_sum += losses[\'ml\'].data.item()\n        loss_evals = loss_evals + 1\n        # Initialize target with <BOS> for every sentence Index = 2\n        if isinstance(batch_preds, list):\n            # wiht beam size unpadded preds\n            sent_preds = [decode_sequence(trg_loader.get_vocab(),\n                                          np.array(pred).reshape(1, -1),\n                                          eos=trg_loader.eos,\n                                          bos=trg_loader.bos)[0]\n                          for pred in batch_preds]\n        else:\n            # decode\n            sent_preds = decode_sequence(trg_loader.get_vocab(), batch_preds,\n                                         eos=trg_loader.eos,\n                                         bos=trg_loader.bos)\n        # Do the same for gold sentences\n        sent_source = decode_sequence(src_loader.get_vocab(),\n                                      data_src[\'labels\'],\n                                      eos=src_loader.eos,\n                                      bos=src_loader.bos)\n        sent_gold = decode_sequence(trg_loader.get_vocab(),\n                                    data_trg[\'out_labels\'],\n                                    eos=trg_loader.eos,\n                                    bos=trg_loader.bos)\n        if not verbose:\n            verb = not (n % 1000)\n        else:\n            verb = verbose\n        for (sl, l, gl) in zip(sent_source, sent_preds, sent_gold):\n            preds.append(l)\n            ground_truths.append(gl)\n            if verb:\n                lg.print_sampled(sl, gl, l)\n        if max_samples == -1:\n            ix1 = data_src[\'bounds\'][\'it_max\']\n        else:\n            ix1 = max_samples\n        if data_src[\'bounds\'][\'wrapped\']:\n            break\n        if n >= ix1:\n            break\n    logger.warn(\'Evaluated %d samples in %.2f s\', len(preds), time.time()-start)\n    bleu_moses, _ = corpus_bleu(preds, ground_truths)\n    return preds, ml_loss_sum / loss_evals, loss_sum / loss_evals, bleu_moses\n\n\ndef sample_model(job_name, model, src_loader, trg_loader, eval_kwargs):\n    """"""Evaluate model.""""""\n    preds = []\n    ground_truths = []\n    batch_size = eval_kwargs.get(\'batch_size\', 1)\n    split = eval_kwargs.get(\'split\', \'val\')\n    verbose = eval_kwargs.get(\'verbose\', 0)\n    eval_kwargs[\'BOS\'] = trg_loader.bos\n    eval_kwargs[\'EOS\'] = trg_loader.eos\n    eval_kwargs[\'PAD\'] = trg_loader.pad\n    eval_kwargs[\'UNK\'] = trg_loader.unk\n    remove_bpe = eval_kwargs.get(\'remove_bpe\', True)\n    logger = logging.getLogger(job_name)\n    model.eval()\n    src_loader.reset_iterator(split)\n    trg_loader.reset_iterator(split)\n    n = 0\n    start = time.time()\n    lenpen_mode = eval_kwargs.get(\'lenpen_mode\', \'wu\')\n    scorer = GNMTGlobalScorer(eval_kwargs[\'lenpen\'], 0, \'none\', lenpen_mode)\n\n    while True:\n        # get batch\n        data_src, order = src_loader.get_src_batch(split, batch_size)\n        data_trg = trg_loader.get_trg_batch(split, order, batch_size)\n        n += batch_size\n        if model.version == \'seq2seq\':\n            source = model.encoder(data_src)\n            source = model.map(source)\n            batch_preds, _ = model.decoder.sample(source, scorer, eval_kwargs)\n        else:\n            batch_preds, _ = model.sample(data_src, scorer, eval_kwargs)\n\n        torch.cuda.empty_cache()  # FIXME choose an optimal freq\n        # Initialize target with <BOS> for every sentence Index = 2\n        if isinstance(batch_preds, list):\n            # wiht beam size unpadded preds\n            sent_preds = [decode_sequence(trg_loader.get_vocab(),\n                                          np.array(pred).reshape(1, -1),\n                                          eos=trg_loader.eos,\n                                          bos=trg_loader.bos,\n                                          remove_bpe=remove_bpe)[0]\n                          for pred in batch_preds]\n        else:\n            # decode\n            sent_preds = decode_sequence(trg_loader.get_vocab(), batch_preds,\n                                         eos=trg_loader.eos,\n                                         bos=trg_loader.bos,\n                                         remove_bpe=remove_bpe)\n        # Do the same for gold sentences\n        sent_source = decode_sequence(src_loader.get_vocab(),\n                                      data_src[\'labels\'],\n                                      eos=src_loader.eos,\n                                      bos=src_loader.bos,\n                                      remove_bpe=remove_bpe)\n        sent_gold = decode_sequence(trg_loader.get_vocab(),\n                                    data_trg[\'out_labels\'],\n                                    eos=trg_loader.eos,\n                                    bos=trg_loader.bos,\n                                    remove_bpe=remove_bpe)\n        if not verbose:\n            verb = not (n % 1000)\n        else:\n            verb = verbose\n        for (sl, l, gl) in zip(sent_source, sent_preds, sent_gold):\n            preds.append(l)\n            ground_truths.append(gl)\n            if verb:\n                lg.print_sampled(sl, gl, l)\n        ix1 = data_src[\'bounds\'][\'it_max\']\n        # ix1 = 20\n        if data_src[\'bounds\'][\'wrapped\']:\n            break\n        if n >= ix1:\n            break\n        del sent_source, sent_preds, sent_gold, batch_preds\n    logger.warn(\'Sampled %d sentences in %.2f s\', len(preds), time.time() - start)\n    bleu_moses, _ = corpus_bleu(preds, ground_truths)\n    return preds, bleu_moses\n\n\ndef track_model(job_name, model, src_loader, trg_loader, eval_kwargs):\n    """"""Evaluate model.""""""\n    source = []\n    preds = []\n    ground_truths = []\n    batched_alphas = []\n    batched_aligns = []\n    batched_activ_aligns = []\n    batched_activs = []\n    batched_embed_activs = []\n    batch_size = eval_kwargs.get(\'batch_size\', 1)\n    assert batch_size == 1, ""Batch size must be 1""\n    split = eval_kwargs.get(\'split\', \'val\')\n    verbose = eval_kwargs.get(\'verbose\', 0)\n    max_samples = eval_kwargs.get(\'max_samples\', -1)\n    eval_kwargs[\'BOS\'] = trg_loader.bos\n    eval_kwargs[\'EOS\'] = trg_loader.eos\n    eval_kwargs[\'PAD\'] = trg_loader.pad\n    eval_kwargs[\'UNK\'] = trg_loader.unk\n    print(\'src_loader ref:\', src_loader.ref)\n    remove_bpe = \'BPE\' in src_loader.ref\n    print(\'Removing bpe:\', remove_bpe)\n    logger = logging.getLogger(job_name)\n    # Make sure to be in evaluation mode\n    model.eval()\n    offset = eval_kwargs.get(\'offset\', 0)\n    print(\'Starting from \', offset)\n    src_loader.iterators[split] = offset\n    trg_loader.iterators[split] = offset\n    # src_loader.reset_iterator(split)\n    # trg_loader.reset_iterator(split)\n    n = 0\n    while True:\n        # get batch\n        data_src, order = src_loader.get_src_batch(split, batch_size)\n        data_trg = trg_loader.get_trg_batch(split, order, batch_size)\n        n += batch_size\n        if model.version == \'seq2seq\':\n            source = model.encoder(data_src)\n            source = model.map(source)\n            batch_preds, _ = model.decoder.sample(source, eval_kwargs)\n        else:\n            # track returns seq, alphas, aligns, activ_aligns, activs, embed_activs, clean_cstr\n            batch_preds, alphas, aligns, activ_aligns, activs, embed_activs, C = model.track(data_src, eval_kwargs)\n            batched_alphas.append(alphas)\n            batched_aligns.append(aligns)\n            batched_activ_aligns.append(activ_aligns)\n            batched_activs.append(activs)\n            batched_embed_activs.append(embed_activs)\n\n        # Initialize target with <BOS> for every sentence Index = 2\n        if isinstance(batch_preds, list):\n            # wiht beam size unpadded preds\n            sent_preds = [decode_sequence(trg_loader.get_vocab(),\n                                          np.array(pred).reshape(1, -1),\n                                          eos=trg_loader.eos,\n                                          bos=trg_loader.bos,\n                                          remove_bpe=False)[0]\n                          for pred in batch_preds]\n        else:\n            # decode\n            sent_preds = decode_sequence(trg_loader.get_vocab(), batch_preds,\n                                         eos=trg_loader.eos,\n                                         bos=trg_loader.bos,\n                                         remove_bpe=False)\n        # Do the same for gold sentences\n        sent_source = decode_sequence(src_loader.get_vocab(),\n                                      data_src[\'labels\'].data.cpu().numpy(),\n                                      eos=src_loader.eos,\n                                      bos=src_loader.bos,\n                                      remove_bpe=False)\n        source.append(sent_source)\n        sent_gold = decode_sequence(trg_loader.get_vocab(),\n                                    data_trg[\'out_labels\'].data.cpu().numpy(),\n                                    eos=trg_loader.eos,\n                                    bos=trg_loader.bos,\n                                    remove_bpe=False)\n        if not verbose:\n            verb = not (n % 300)\n        else:\n            verb = verbose\n        for (sl, l, gl) in zip(sent_source, sent_preds, sent_gold):\n            preds.append(l)\n            ground_truths.append(gl)\n            if verb:\n                lg.print_sampled(sl, gl, l)\n        if max_samples == -1:\n            ix1 = data_src[\'bounds\'][\'it_max\']\n        else:\n            ix1 = max_samples\n\n        if data_src[\'bounds\'][\'wrapped\']:\n            break\n        if n >= ix1:\n            logger.warn(\'Evaluated the required samples (%s)\' % n)\n            break\n    print(\'Sampled %d sentences\' % len(preds))\n    bleu_moses, _ = corpus_bleu(preds, ground_truths)\n\n    return {\'source\': source,\n            \'preds\': preds,\n            \'alpha\': batched_alphas,\n            \'align\': batched_aligns,\n            \'activ_align\': batched_activ_aligns,\n            \'activ\': batched_activs,\n            \'embed_activ\': batched_embed_activs,\n            \'channels_cst\': C,\n            ""bleu"": bleu_moses,\n            }\n\n\n\n    \n'"
nmt/models/gnmt.py,1,"b'""""""\nAdapted from OpenNMT\n""""""\n\nfrom __future__ import division\nimport torch\nfrom  .penalties_onmt import PenaltyBuilder\n\n\nclass GNMTGlobalScorer(object):\n    """"""\n    NMT re-ranking score from\n    ""Google\'s Neural Machine Translation System"" :cite:`wu2016google`\n\n    Args:\n       alpha (float): length parameter\n       beta (float):  coverage parameter\n    """"""\n\n    def __init__(self, alpha, beta, cov_penalty, length_penalty):\n        self.alpha = alpha\n        self.beta = beta\n        penalty_builder = PenaltyBuilder(cov_penalty,\n                                         length_penalty)\n        # Term will be subtracted from probability\n        self.cov_penalty = penalty_builder.coverage_penalty()\n        # Probability will be divided by this\n        self.length_penalty = penalty_builder.length_penalty()\n\n    def score(self, beam, logprobs):\n        """"""\n        Rescores a prediction based on penalty functions\n        """"""\n        normalized_probs = self.length_penalty(beam,\n                                               logprobs,\n                                               self.alpha)\n        if not beam.stepwise_penalty:\n            penalty = self.cov_penalty(beam,\n                                       beam.global_state[""coverage""],\n                                       self.beta)\n            normalized_probs -= penalty\n\n        return normalized_probs\n\n    def update_score(self, beam, attn):\n        """"""\n        Function to update scores of a Beam that is not finished\n        """"""\n        if ""prev_penalty"" in beam.global_state.keys():\n            beam.scores.add_(beam.global_state[""prev_penalty""])\n            penalty = self.cov_penalty(beam,\n                                       beam.global_state[""coverage""] + attn,\n                                       self.beta)\n            beam.scores.sub_(penalty)\n\n    def update_global_state(self, beam):\n        ""Keeps the coverage vector as sum of attentions""\n        if len(beam.prev_ks) == 1:\n            beam.global_state[""prev_penalty""] = beam.scores.clone().fill_(0.0)\n            beam.global_state[""coverage""] = beam.attn[-1]\n            self.cov_total = beam.attn[-1].sum(1)\n        else:\n            # print(\'adding: min(\', beam.attn[-1], beam.global_state[\'coverage\'],\n                  # \'summed in dim 1\')\n            self.cov_total += torch.min(beam.attn[-1],\n                                        beam.global_state[\'coverage\']).sum(1)\n            beam.global_state[""coverage""] = beam.global_state[""coverage""] \\\n                .index_select(0, beam.prev_ks[-1]).add(beam.attn[-1])\n\n            prev_penalty = self.cov_penalty(beam,\n                                            beam.global_state[""coverage""],\n                                            self.beta)\n            beam.global_state[""prev_penalty""] = prev_penalty\n'"
nmt/models/log_efficient_densenet.py,5,"b'# Adapted from:\n# https://github.com/gpleiss/efficient_densenet_pytorch/blob/master/models/densenet.py\n\nfrom math import sqrt, log2, floor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom .dense_modules import _setup_conv\nfrom .transitions import Transition\n\n\ndef is_power2(num):\n    """""" True iff integer is a power of 2""""""\n    return ((num & (num - 1)) == 0) and num != 0\n\n\ndef _bn_function_factory(norm, relu, conv, index, mode=1):\n    # for index i, select {i-[2^k] for k in 0,.. log(i)}\n    if mode == 1:\n        connexions = [index - 2**k for k in range(1+floor(log2(index)))]\n    elif mode == 2:\n        # Make sure the first input is always in\n        connexions = [index - 2**k for k in range(1+floor(log2(index)))]\n        if 0 not in connexions:\n            connexions.append(0)\n\n    # print(\'index:\', index, "">> connected to:"", connexions)\n    def bn_function(*inputs):\n        concatenated_features = torch.cat([inputs[c] for c in connexions], 1)\n        # print(\'concat feature:\', concatenated_features.size())\n        bottleneck_output = conv(relu(norm(concatenated_features)))\n        return bottleneck_output\n\n    return bn_function\n\n\nclass _DenseLayer(nn.Module):\n    def __init__(self,\n                 num_input_features,\n                 kernel_size,\n                 params,\n                 index):\n        super().__init__()\n        self.kernel_size = kernel_size\n        self.bn_size = params.get(\'bn_size\', 4)\n        self.growth_rate = params.get(\'growth_rate\', 32)\n        self.drop_rate = params.get(\'conv_dropout\', 0.)\n        self.index = index\n        self.mode = params.get(\'log_mode\', 1)\n        conv1, conv2 = _setup_conv(num_input_features, kernel_size, params)\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', conv1)\n        self.add_module(\'norm2\', nn.BatchNorm2d(self.bn_size * self.growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', conv2)\n        \n    def forward(self, *prev_features):\n        bn_function = _bn_function_factory(\n            self.norm1,\n            self.relu1,\n            self.conv1,\n            self.index,\n            self.mode\n            )\n        if any(prev_feature.requires_grad for prev_feature in prev_features):\n            # Does not compute intermediate values\n            # but recompute them in the backward pass\n            # tradeoff btw memory & computation\n            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n        else:\n            bottleneck_output = bn_function(*prev_features)\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        # new_features has g channels\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate,\n                                     training=self.training)\n        return new_features\n\n\nclass _DenseBlock(nn.Module):\n    def __init__(self, num_layers, num_input_features,\n                 kernels, params):\n        super(_DenseBlock, self).__init__()\n        growth_rate = params.get(\'growth_rate\', 32)\n        log_mode = params.get(\'log_mode\', 1)\n        print(\'Dense channels:\', num_input_features, end=\'\')\n        for i in range(num_layers):\n            index = i + 1\n            numc = floor(log2(index)) + 1\n            if log_mode == 1:\n                if is_power2(index):\n                    numf = num_input_features + (numc - 1) * growth_rate\n                else:\n                    numf = numc * growth_rate\n            elif log_mode == 2:\n                if is_power2(index):\n                    numf = num_input_features + (numc - 1) * growth_rate\n                else:\n                    numf = numc * growth_rate + num_input_features\n\n            print(""> (%d)"" % numc, numf, end=\'\')\n            layer = _DenseLayer(\n                numf,\n                kernels[i],\n                params,\n                i+1,\n            )\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for layer in self.children():\n            new_features = layer(*features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n\nclass Log_Efficient_DenseNet(nn.Module):\n    """""" \n    set to True to use checkpointing. Much more memory efficient, but slower.\n    log connections inside a block:\n    x_i = f_i(concat({x_{i-[2^k]}, i < [log(i)]}))\n    Implementation of Log-Densenet V1 described in:\n    ``LOG-DENSENET: HOW TO SPARSIFY A DENSENET``\n    arxiv: https://arxiv.org/pdf/1711.00002.pdf\n    """"""\n    def __init__(self, num_init_features, params):\n        super(Log_Efficient_DenseNet, self).__init__()\n        growth_rate = params.get(\'growth_rate\', 32)\n        block_layers = params.get(\'num_layers\', (6, 12, 24, 16))\n        block_kernels = params[\'kernels\']\n        init_weights = params.get(\'init_weights\', 0)\n        divide_channels = params.get(\'divide_channels\', 2)\n        skip_last_trans = params.get(\'skip_last_trans\', 0)\n        self.features = nn.Sequential()\n        num_features = num_init_features\n        # start by reducig the input channels\n        if divide_channels > 1:\n            trans = nn.Conv2d(num_features, num_features // divide_channels, 1)\n            if init_weights == ""manual"":\n                std = sqrt(1 / num_features)\n                trans.weight.data.normal_(0, std)\n            self.features.add_module(\'initial_transition\', trans)\n            num_features = num_features // divide_channels\n\n        # Each denseblock\n        for i, (num_layers, kernels) in enumerate(zip(block_layers,\n                                                      block_kernels)):\n            block = _DenseBlock(num_layers,\n                                num_features,\n                                kernels,\n                                params\n                               )\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features += num_layers * growth_rate\n            if not i == len(block_layers) - 1 or not skip_last_trans:\n                trans = Transition(\n                    num_input_features=num_features,\n                    num_output_features=num_features // 2,\n                    init_weights=init_weights)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n                print(""> (trans) "", num_features, end=\'\')\n        print()\n        self.output_channels = num_features\n        # Final batch norm\n        self.features.add_module(\'norm_final\', nn.BatchNorm2d(num_features))\n        self.features.add_module(\'relu_last\', nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        return  self.features(x.contiguous())\n'"
nmt/models/lstm.py,22,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nclass AllamanisConvAttention(nn.Module):\n    """"""\n    Convolutional attention\n    @inproceedings{allamanis2016convolutional,\n          title={A Convolutional Attention Network for\n                 Extreme Summarization of Source Code},\n          author={Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},\n          booktitle={International Conference on Machine Learning (ICML)},\n          year={2016}\n      }\n    """"""\n\n    def __init__(self, params, enc_params):\n        super(AllamanisConvAttention, self).__init__()\n        src_emb_dim = enc_params[\'input_dim\']\n        dims = params[\'attention_channels\'].split(\',\')\n        dim1, dim2 = [int(d) for d in dims]\n        print(\'Out channels dims:\', dim1, dim2)\n        widths = params[\'attention_windows\'].split(\',\')\n        w1, w2, w3 = [int(w) for w in widths]\n        print(\'Moving windows sizes:\', w1, w2, w3)\n        trg_dim = params[\'cell_dim\']\n        self.normalize = params[\'normalize_attention\']\n        # padding to maintaing the same length\n        self.conv1 = nn.Conv1d(src_emb_dim, dim1, w1, padding=(w1-1)//2)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv1d(dim1, trg_dim, w2, padding=(w2-1)//2)\n        self.conv3 = nn.Conv1d(trg_dim, 1, w3, padding=(w3-1)//2)\n        self.sm = nn.Softmax(dim=2)\n        self.linear_out = nn.Linear(trg_dim + src_emb_dim, trg_dim, bias=False)\n        self.tanh = nn.Tanh()\n\n    def score(self, input, context, src_emb):\n        """"""\n        input: batch x trg_dim\n        context & src_emb : batch x Tx x src_dim (resp. src_emb_dim)\n        return the alphas for comuting the weighted context\n        """"""\n        src_emb = src_emb.transpose(1, 2)\n        L1 = self.relu(self.conv1(src_emb))\n        L2 = self.conv2(L1)\n        # columnwise multiplication\n        L2 = L2 * input.unsqueeze(2).repeat(1, 1, L2.size(2))\n        # L2 normalization:\n        if self.normalize:\n            norm = L2.norm(p=2, dim=1, keepdim=True)  # check if 2 is the right dim\n            L2 = L2.div(norm)\n            if len((norm == 0).nonzero()):\n                print(\'Zero norm!!\')\n        attn = self.conv3(L2)\n        attn_sm = self.sm(attn)\n        return attn_sm\n\n    def forward(self, input, context, src_emb):\n        """"""\n        Score the context (resp src embedding)\n        and return a new context as a combination of either\n        the source embeddings or the hidden source codes\n        """"""\n        attn_sm = self.score(input, context, src_emb)\n        weighted_context = torch.bmm(attn_sm, src_emb).squeeze(1)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n        return h_tilde, attn_sm\n\n\nclass AllamanisConvAttentionBis(AllamanisConvAttention):\n    """"""\n    Similar to AllamanisConvAttention with the only difference at computing\n    the weighted context which takes the encoder\'s hidden states\n    instead of the source word embeddings\n    """"""\n\n    def __init__(self, params, enc_params):\n        super(AllamanisConvAttentionBis, self).__init__(params)\n        trg_dim = params[\'cell_dim\']\n        src_dim = enc_params[\'cell_dim\']\n        self.linear_out = nn.Linear(trg_dim + src_dim, trg_dim, bias=False)\n\n\n    def forward(self, input, context, src_emb):\n        attn_sm = self.score(input, context, src_emb)\n        attn_reshape = attn_sm.transpose(1, 2)\n        weighted_context = torch.bmm(context.transpose(1, 2),\n                                     attn_reshape).squeeze(2)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n        return h_tilde, attn_sm\n\n\nclass ConvAttentionHid(nn.Module):\n    """"""\n    Convolutional attention\n    All around similar to Allamanis attention while never\n    using the source word embeddings\n    """"""\n\n    def __init__(self, params, enc_params):\n        super(ConvAttentionHid, self).__init__()\n        src_dim = enc_params[\'cell_dim\']\n        dims = params[\'attention_channels\'].split(\',\')\n        dim1, dim2 = [int(d) for d in dims]\n        print(\'Out channels dims:\', dim1, dim2)\n        widths = params[\'attention_windows\'].split(\',\')\n        w1, w2, w3 = [int(w) for w in widths]\n        print(\'Moving windows sizes:\', w1, w2, w3)\n        trg_dim = params[\'cell_dim\']\n        self.normalize = params[\'normalize_attention\']\n        # padding to maintaing the same length\n        self.conv1 = nn.Conv1d(src_dim, dim1, w1, padding=(w1-1)//2)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv1d(dim1, trg_dim, w2, padding=(w2-1)//2)\n        self.conv3 = nn.Conv1d(trg_dim, 1, w3, padding=(w3-1)//2)\n        self.sm = nn.Softmax(dim=2)\n        self.linear_out = nn.Linear(trg_dim + src_dim, trg_dim, bias=False)\n        self.tanh = nn.Tanh()\n\n    def forward(self, input, context, src_emb):\n        """"""Propogate input through the network.\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        context = context.transpose(1, 2)\n        L1 = self.relu(self.conv1(context))\n        L2 = self.conv2(L1)\n        # columnwise dot product\n        L2 = L2 * input.unsqueeze(2).repeat(1, 1, L2.size(2))\n        # L2 normalization:\n        if self.normalize:\n            norm = L2.norm(p=2, dim=2, keepdim=True)\n            L2 = L2.div(norm)\n            if len((norm == 0).nonzero()):\n                print(\'Zero norm!!\')\n        attn = self.conv3(L2)\n        attn_sm = self.sm(attn)\n        attn_reshape = attn_sm.transpose(1, 2)\n        weighted_context = torch.bmm(context,\n                                     attn_reshape).squeeze(2)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n        return h_tilde, attn\n\n\nclass ConvAttentionHidCat(nn.Module):\n    """"""\n    Convolutional attention\n    Use the encoder hidden states all around, Jakob\'s idea\n    """"""\n\n    def __init__(self, params, enc_params):\n        super(ConvAttentionHidCat, self).__init__()\n        src_dim = enc_params[\'cell_dim\']\n        trg_dim = params[\'cell_dim\']\n        self.normalize = params[\'normalize_attention\']\n        widths = params[\'attention_windows\'].split(\',\')\n        self.num_conv_layers = len(widths)\n        dims = params[\'attention_channels\'].split(\',\')\n        assert len(dims) == self.num_conv_layers - 1\n        if self.num_conv_layers == 3:\n            w1, w2, w3 = [int(w) for w in widths]\n            print(\'Moving windows sizes:\', w1, w2, w3)\n            dim1, dim2 = [int(d) for d in dims]\n            print(\'Out channels dims:\', dim1, dim2)\n        elif self.num_conv_layers == 4:\n            w1, w2, w3, w4 = [int(w) for w in widths]\n            print(\'Moving windows sizes:\', w1, w2, w3, w4)\n            dim1, dim2, dim3 = [int(d) for d in dims]\n            print(\'Out channels dims:\', dim1, dim2, dim3)\n        else:\n            raise ValueError(\'Number of layers is either 3 or 4, still working on a general form\')\n        # padding to maintaing the same length\n        self.conv1 = nn.Conv1d(src_dim + trg_dim, dim1, w1, padding=(w1-1)//2)\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv1d(dim1, dim2, w2, padding=(w2-1)//2)\n        if self.num_conv_layers == 3:\n            self.conv3 = nn.Conv1d(dim2, 1, w3, padding=(w3-1)//2)\n        elif self.num_conv_layers == 4:\n            self.conv3 = nn.Conv1d(dim2, dim3, w3, padding=(w3-1)//2)\n            self.conv4 = nn.Conv1d(dim3, 1, w4, padding=(w4-1)//2)\n\n        self.sm = nn.Softmax(dim=2)\n        self.linear_out = nn.Linear(trg_dim + src_dim, trg_dim, bias=False)\n        self.tanh = nn.Tanh()\n\n    def forward(self, input, context, src_emb):\n        """"""Propogate input through the network.\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        context = context.transpose(1, 2)\n        input_cat = torch.cat((context,\n                               input.unsqueeze(2).repeat(1,\n                                                         1,\n                                                         context.size(2))),\n                              1)\n        L1 = self.relu(self.conv1(input_cat))\n        L2 = self.conv2(L1)\n        # L2 normalization:\n        if self.normalize:\n            norm = L2.norm(p=2, dim=2, keepdim=True)\n            L2 = L2.div(norm)\n            if len((norm == 0).nonzero()):\n                print(\'Zero norm!!\')\n        # print(\'L2 normalized:\', L2.size())\n        if self.num_conv_layers == 3:\n            attn = self.conv3(L2)\n        else:\n            attn = self.conv4(self.conv3(L2))\n        attn_sm = self.sm(attn)\n        attn_reshape = attn_sm.transpose(1, 2)\n        weighted_context = torch.bmm(context,\n                                     attn_reshape).squeeze(2)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n        return h_tilde, attn\n\n\nclass LocalDotAttention(nn.Module):\n    """"""\n    Soft Dot/ local-predictive attention\n\n    Ref: http://www.aclweb.org/anthology/D15-1166\n    Effective approaches to attention based NMT (Luong et al. EMNLP 15)\n    """"""\n\n    def __init__(self, params):\n        super(LocalDotAttention, self).__init__()\n        dim = params[\'cell_dim\']\n        dropout = params[\'attention_dropout\']\n        self.window = 4  # D\n        self.sigma = self.window / 2\n        self.linear_in = nn.Linear(dim, dim, bias=False)\n        self.linear_out = nn.Linear(dim * 2, dim, bias=False)\n        self.linear_predict_1 = nn.Linear(dim, dim//2, bias=False)\n        self.linear_predict_2 = nn.Linear(dim//2, 1, bias=False)\n\n        self.tanh = nn.Tanh()\n        self.sm = nn.Softmax(dim=1)\n        self.dropout = nn.Dropout(dropout)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input, context, src_emb=None):\n        """"""Propogate input through the network.\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        Tx = context.size(1)\n        # predict the alignement position:\n        pt = self.tanh(self.linear_predict_1(input))\n        print(\'pt:\', pt.size())\n        pt = self.linear_predict_2(pt)\n        print(\'pt size:\', pt.size())\n        pt = Tx * self.sigmoid(pt)\n        bl, bh = (pt-self.window).int(), (pt+self.window).int()\n        indices = torch.cat([torch.arange(i.item(), j.item()).unsqueeze(0)\n                             for i, j in zip(bl, bh)],\n                            dim=0).long().cuda()\n        print(\'indices:\', indices.size())\n        # Get attention\n        target = self.linear_in(input).unsqueeze(2)  # batch x dim x 1\n        print(\'type(context):\', type(context))\n        context_window = context.gather(0, indices)\n        print(\'context window:\', context_window.size())\n        attn = torch.bmm(context_window, target).squeeze(2)  # batch x sourceL\n        attn = self.sm(self.dropout(attn))\n        attn3 = attn.view(attn.size(0), 1, attn.size(1))  # batch x 1 x sourceL\n        weighted_context = torch.bmm(attn3, context_window).squeeze(1)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n\n        return h_tilde, attn\n\n\nclass SoftDotAttention(nn.Module):\n    """"""Soft Dot Attention.\n\n    Ref: http://www.aclweb.org/anthology/D15-1166\n    Effective approaches to attention based NMT (Luong et al. EMNLP 15)\n    Adapted from PyTorch OPEN NMT.\n    """"""\n\n    def __init__(self, params):\n        super(SoftDotAttention, self).__init__()\n        dim = params[\'cell_dim\']\n        self.linear_in = nn.Linear(dim, dim, bias=False)\n        self.sm = nn.Softmax(dim=1)\n        self.linear_out = nn.Linear(dim * 2, dim, bias=False)\n        self.tanh = nn.Tanh()\n        self.dropout = nn.Dropout(params[\'attention_dropout\'])\n\n    def forward(self, input, context, src_emb=None):\n        """"""Propogate input through the network.\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        target = self.linear_in(input).unsqueeze(2)  # batch x dim x 1\n        # Get attention\n        attn = torch.bmm(context, target).squeeze(2)  # batch x sourceL\n        attn = self.sm(self.dropout(attn))\n        attn3 = attn.view(attn.size(0), 1, attn.size(1))  # batch x 1 x sourceL\n\n        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n\n        return h_tilde, attn\n\n\nclass LSTMAttention(nn.Module):\n    """"""\n    A long short-term memory (LSTM) cell with attention.\n    Use SoftDotAttention\n    """"""\n\n    def __init__(self, params, enc_params):\n        super(LSTMAttention, self).__init__()\n        # Params:\n        self.mode = params[\'attention_mode\']\n        self.input_size = params[\'input_dim\']\n        self.hidden_size = params[\'cell_dim\']\n        self.input_weights = nn.Linear(self.input_size,\n                                       4 * self.hidden_size)\n        self.hidden_weights = nn.Linear(self.hidden_size,\n                                        4 * self.hidden_size)\n\n        if self.mode == ""dot"":\n            self.attention_layer = SoftDotAttention(params)\n        elif self.mode == ""local-dot"":\n            self.attention_layer = LocalDotAttention(params)\n        elif self.mode == ""allamanis"":  # conv\n            self.attention_layer = AllamanisConvAttention(params, enc_params)\n        elif self.mode == ""allamanis-v2"":  # conv2\n            self.attention_layer = AllamanisConvAttentionBis(params, enc_params)\n        elif self.mode == ""conv-hid"":   # conv3\n            self.attention_layer = ConvAttentionHid(params, enc_params)\n        elif self.mode == ""conv-hid-cat"":  # conv4\n            self.attention_layer = ConvAttentionHidCat(params, enc_params)\n        else:\n            raise ValueError(\'Unkown attention mode %s\' % self.mode)\n\n\n    def forward(self, input, hidden, ctx, src_emb):\n        """"""Propogate input through the network.""""""\n        # print(\'input:\', input.size())\n        # print(\'hidden:\', hidden[0].size(), hidden[1].size())\n        # print(\'ctx:\', ctx.size())\n\n        def recurrence(input, hidden):\n            """"""Recurrence helper.""""""\n            # print(\'hidden\', hidden[0].size(), hidden[1].size())\n            hx, cx = hidden  # n_b x hidden_dim #FIXME Assuming LSTM\n            gates = self.input_weights(input) + \\\n                self.hidden_weights(hx)\n            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n            ingate = F.sigmoid(ingate)\n            forgetgate = F.sigmoid(forgetgate)\n            cellgate = F.tanh(cellgate)\n            outgate = F.sigmoid(outgate)\n\n            cy = (forgetgate * cx) + (ingate * cellgate)\n            hy = outgate * F.tanh(cy)  # n_b x hidden_dim\n            h_tilde, attn = self.attention_layer(hy, ctx, src_emb)\n            return (h_tilde, cy), attn\n\n        input = input.transpose(0, 1)\n        output = []\n        attention = []\n        steps = list(range(input.size(0)))\n        for i in steps:\n            hidden, attn = recurrence(input[i], hidden)\n            if isinstance(hidden, tuple):\n                h = hidden[0]\n            else:\n                h = hidden\n            output.append(h)\n            attention.append(attn)\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n        output = output.transpose(0, 1)\n        attention = torch.cat(attention, 0)\n        return output, hidden, attention\n\n\nclass LSTMAttentionV2(LSTMAttention):\n    """"""\n    A long short-term memory (LSTM) cell with attention.\n    Use SoftDotAttention\n    """"""\n\n    def __init__(self, params, enc_params):\n        super(LSTMAttentionV2, self).__init__(params, enc_params)\n\n    def forward(self, input, hidden, ctx, src_emb):\n        """"""Propogate input through the network.""""""\n        def recurrence(input, hidden):\n            """"""Recurrence helper.""""""\n            hx, cx = hidden  # n_b x hidden_dim #FIXME Assuming LSTM\n            gates = self.input_weights(input) + \\\n                self.hidden_weights(hx)\n            ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n            ingate = F.sigmoid(ingate)\n            forgetgate = F.sigmoid(forgetgate)\n            cellgate = F.tanh(cellgate)\n            outgate = F.sigmoid(outgate)\n\n            cy = (forgetgate * cx) + (ingate * cellgate)\n            hy = outgate * F.tanh(cy)  # n_b x hidden_dim\n            h_tilde, _ = self.attention_layer(hy, ctx, src_emb)\n            return h_tilde, (hy, cy)\n\n        input = input.transpose(0, 1)\n        output = []\n        steps = list(range(input.size(0)))\n        for i in steps:\n            htilde, hidden = recurrence(input[i], hidden)\n            output.append(htilde)\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n        output = output.transpose(0, 1)\n        return output, hidden\n\nclass LSTM(nn.LSTM):\n    def __init__(self, *args, **kwargs):\n        super(LSTM, self).__init__(*args, **kwargs)\n\n    def forward(self, input, hidden, ctx, src_emb):\n        if hidden[0].size(0) != 1:\n            # print(\'unsqueezing the state\')\n            hidden = [h.unsqueeze(0) for h in hidden]\n        # print(\'input:\', input.size(), \'hidden:\', hidden[0].size())\n        output, hdec = super(LSTM, self).forward(input, hidden)\n        # print(\'out & hid:\', output.size(), hdec[0].size())\n        return output, hdec\n\n'"
nmt/models/modules.py,15,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n""""""\nTODO Test position encoding cat after modif and check sum\n""""""\n\n\nclass positional_encoding(nn.Module):\n    def __init__(self, num_units, zeros_pad=True, scale=True):\n        \'\'\'Sinusoidal Positional_Encoding.\n        Args:\n          num_units: Output dimensionality\n          zero_pad: Boolean. If True, all the values\n                    of the first row (id = 0) should be constant zero\n          scale: Boolean. If True, the output will be multiplied\n                 by sqrt num_units(check details from paper)\n        \'\'\'\n        super(positional_encoding, self).__init__()\n        self.num_units = num_units\n        self.zeros_pad = zeros_pad\n        self.scale = scale\n\n    def forward(self, inputs):\n        # inputs: A 2d Tensor with shape of (N, T).\n        N, T = inputs.size()[0: 2]\n        # First part of the PE function: sin and cos argument\n        position_ind = torch.unsqueeze(torch.arange(0, T), 0).repeat(N, 1).long()\n        position_enc = torch.Tensor([\n            [pos / np.power(10000, 2. * i / self.num_units)\n             for i in range(self.num_units)]\n            for pos in range(T)])\n        # Second part, apply the cosine to even columns and sin to odds.\n        position_enc[:, 0::2] = torch.sin(position_enc[:, 0::2])  # dim 2i\n        position_enc[:, 1::2] = torch.cos(position_enc[:, 1::2])  # dim 2i+1\n        lookup_table = position_enc\n        if self.zeros_pad:\n            lookup_table = torch.cat((torch.zeros(1, self.num_units),\n                                     lookup_table[1:, :]), 0)\n            padding_idx = 0\n        else:\n            padding_idx = -1\n        outputs = F.embedding(\n            position_ind,\n            lookup_table,\n            padding_idx,\n            None, 2, False, False\n        )\n        if self.scale:\n            outputs = outputs * self.num_units ** 0.5\n        return outputs\n\n\nclass ChannelsNormalization(nn.Module):\n    def __init__(self, n_channels, eps=1e-3):\n        super(ChannelsNormalization, self).__init__()\n        self.eps = eps\n        self.a_2 = nn.Parameter(torch.ones(1, n_channels, 1, 1), requires_grad=True)\n        self.b_2 = nn.Parameter(torch.zeros(1, n_channels, 1, 1), requires_grad=True)\n\n    def forward(self, z):\n        mu = torch.mean(z, keepdim=True, dim=1)\n        sigma = torch.std(z, keepdim=True, dim=1)\n        # print(\'z:\', z.size(), ""mu, sigma:"", mu.size(), sigma.size())\n        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n        ln_out = ln_out * self.a_2.expand_as(ln_out) + self.b_2.expand_as(ln_out)\n        return ln_out\n\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, d_hid, eps=1e-3):\n        super(LayerNormalization, self).__init__()\n        self.eps = eps\n        self.a_2 = nn.Parameter(torch.ones(d_hid), requires_grad=True)\n        self.b_2 = nn.Parameter(torch.zeros(d_hid), requires_grad=True)\n\n    def forward(self, z):\n        if z.size(1) == 1:\n            return z\n        mu = torch.mean(z, keepdim=True, dim=-1)\n        sigma = torch.std(z, keepdim=True, dim=-1)\n        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n        ln_out = ln_out * self.a_2.expand_as(ln_out) + self.b_2.expand_as(ln_out)\n        return ln_out\n'"
nmt/models/norm.py,3,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor as T\n\n\nclass LayerNorm(nn.Module):\n    """"""\n    Layer Normalization based on Ba & al.:\n    \'Layer Normalization\'\n    https://arxiv.org/pdf/1607.06450.pdf\n    """"""\n\n    def __init__(self, input_size, learnable=True, epsilon=1e-6):\n        super(LayerNorm, self).__init__()\n        self.input_size = input_size\n        self.learnable = learnable\n        self.alpha = T(1, input_size).fill_(0)\n        self.beta = T(1, input_size).fill_(0)\n        self.epsilon = epsilon\n        self.alpha = nn.Parameter(self.alpha)\n        self.beta = nn.Parameter(self.beta)\n        self.init_weights()\n\n    def init_weights(self):\n        std = 1.0 / math.sqrt(self.input_size)\n        for w in self.parameters():\n            w.data.uniform_(-std, std)\n\n    def forward(self, x):\n        size = x.size()\n        x = x.view(x.size(0), -1)\n        x = (x - torch.mean(x, 1).expand_as(x)) / torch.sqrt(torch.var(x, 1).expand_as(x) + self.epsilon)\n        if self.learnable:\n            x =  self.alpha.expand_as(x) * x + self.beta.expand_as(x)\n        return x.view(size)\n'"
nmt/models/penalties_onmt.py,2,"b'""""""\nSource: \n    https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/penalties.py\n""""""\nfrom __future__ import division\nimport torch\n\n\nclass PenaltyBuilder(object):\n    """"""\n    Returns the Length and Coverage Penalty function for Beam Search.\n\n    Args:\n        length_pen (str): option name of length pen\n        cov_pen (str): option name of cov pen\n    """"""\n\n    def __init__(self, cov_pen, length_pen):\n        self.length_pen = length_pen\n        self.cov_pen = cov_pen\n\n    def coverage_penalty(self):\n        if self.cov_pen == ""wu"":\n            return self.coverage_wu\n        elif self.cov_pen == ""summary"":\n            return self.coverage_summary\n        else:\n            # print(\'skipping the coverage penalty\')\n            return self.coverage_none\n\n    def length_penalty(self):\n        if self.length_pen == ""wu"":\n            return self.length_wu\n        elif self.length_pen == ""avg"":\n            return self.length_average\n        else:\n            return self.length_none\n\n    """"""\n    Below are all the different penalty terms implemented so far\n    """"""\n\n    def coverage_wu(self, beam, cov, beta=0.):\n        """"""\n        NMT coverage re-ranking score from\n        ""Google\'s Neural Machine Translation System"" :cite:`wu2016google`.\n        """"""\n        penalty = -torch.min(cov, cov.clone().fill_(1.0)).log().sum(1)\n        return beta * penalty\n\n    def coverage_summary(self, beam, cov, beta=0.):\n        """"""\n        Our summary penalty.\n        """"""\n        penalty = torch.max(cov, cov.clone().fill_(1.0)).sum(1)\n        penalty -= cov.size(1)\n        return beta * penalty\n\n    def coverage_none(self, beam, cov, beta=0.):\n        """"""\n        returns zero as penalty\n        """"""\n        return beam.scores.clone().fill_(0.0)\n\n    def length_wu(self, beam, logprobs, alpha=0.):\n        """"""\n        NMT length re-ranking score from\n        ""Google\'s Neural Machine Translation System"" :cite:`wu2016google`.\n        """"""\n\n        modifier = (((5 + len(beam.next_ys)) ** alpha) /\n                    ((5 + 1) ** alpha))\n        return (logprobs / modifier)\n\n    def length_average(self, beam, logprobs, alpha=0.):\n        """"""\n        Returns the average probability of tokens in a sequence.\n        """"""\n        return logprobs / len(beam.next_ys)\n\n    def length_none(self, beam, logprobs, alpha=0., beta=0.):\n        """"""\n        Returns unmodified scores.\n        """"""\n        return logprobs\n'"
nmt/models/pervasive.py,40,"b'""""""\nPervasive attention\n""""""\nimport math\nimport itertools\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .densenet import DenseNet\nfrom .efficient_densenet import Efficient_DenseNet\nfrom .log_efficient_densenet import Log_Efficient_DenseNet\n\nfrom .aggregator import Aggregator\nfrom .embedding import Embedding, ConvEmbedding\nfrom .beam_search import Beam\n\n\ndef _expand(tensor, dim, reps):\n    # Expand 4D tensor in the source or the target dimension\n    if dim == 1:\n        return tensor.repeat(1, reps, 1, 1)\n        # return tensor.expand(-1, reps, -1, -1)\n    if dim == 2:\n        return tensor.repeat(1, 1, reps, 1)\n        # return tensor.expand(-1, -1, reps, -1)\n    else:\n        raise NotImplementedError\n\n\nclass Pervasive(nn.Module):\n    def __init__(self, jobname, params, src_vocab_size, trg_vocab_size,\n                 special_tokens):\n        nn.Module.__init__(self)\n        self.logger = logging.getLogger(jobname)\n        self.version = \'conv\'\n        self.params = params\n        self.merge_mode  = params[\'network\'].get(\'merge_mode\', \'concat\')\n        self.src_vocab_size = src_vocab_size\n        self.trg_vocab_size = trg_vocab_size\n        self.padding_idx = special_tokens[\'PAD\']\n        self.mask_version = params.get(\'mask_version\', -1)\n        # assert self.padding_idx == 0, ""Padding token should be 0""\n        self.bos_token = special_tokens[\'BOS\']\n        self.eos_token = special_tokens[\'EOS\']\n        self.kernel_size = max(list(itertools.chain.from_iterable(\n            params[\'network\'][\'kernels\']\n            )))\n        if params[\'encoder\'][\'type\'] == ""none"":\n            self.src_embedding = Embedding(\n                params[\'encoder\'],\n                src_vocab_size,\n                padding_idx=self.padding_idx\n                )\n        elif params[\'encoder\'][\'type\'] == ""conv"":\n            self.src_embedding = ConvEmbedding(\n                params[\'encoder\'],\n                src_vocab_size,\n                padding_idx=self.padding_idx\n                )\n\n        self.trg_embedding = Embedding(\n            params[\'decoder\'],\n            trg_vocab_size,\n            padding_idx=self.padding_idx,\n            pad_left=True\n            )\n\n        if self.merge_mode == \'concat\':\n            self.input_channels = self.src_embedding.dimension + \\\n                                  self.trg_embedding.dimension\n        elif self.merge_mode == ""product"":\n            self.input_channels = self.src_embedding.dimension \n        elif self.merge_mode == ""bilinear"":\n            bilinear_dim = params[\'network\'].get(\'bilinear_dimension\', 128)\n            self.input_channels = bilinear_dim\n            std = params[\'encoder\'].get(\'init_std\', 0.01)\n            self.bw = nn.Parameter(std * torch.randn(bilinear_dim))\n        elif self.merge_mode == ""multi-sim"":\n            self.sim_dim = params[\'network\'].get(\'similarity_dimension\', 128)\n            self.input_channels = self.sim_dim\n            std = params[\'encoder\'].get(\'init_std\', 0.01)\n            self.bw = nn.Parameter(std * torch.randn(self.sim_dim,\n                                                     self.trg_embedding.dimension,\n                                                     self.src_embedding.dimension))\n\n        elif self.merge_mode == ""multi-sim2"":\n            self.sim_dim = params[\'network\'].get(\'similarity_dimension\', 128)\n            self.input_channels = self.sim_dim\n            std = params[\'encoder\'].get(\'init_std\', 0.01)\n            self.bw = nn.Parameter(torch.empty(self.sim_dim,\n                                               self.trg_embedding.dimension,\n                                               self.src_embedding.dimension))\n\n            nn.init.orthogonal_(self.bw)\n        else:\n            raise ValueError(\'Unknown merging mode\')\n\n\n        self.logger.info(\'Model input channels: %d\', self.input_channels)\n        self.logger.info(""Selected network: %s"", params[\'network\'][\'type\'])\n\n\n        if params[\'network\'][\'divide_channels\'] > 1:\n            self.logger.warning(\'Reducing the input channels by %d\',\n                                params[\'network\'][\'divide_channels\'])\n\n        if params[""network""][\'type\'] == ""densenet"":\n            self.net = DenseNet(self.input_channels, params[\'network\'])\n            self.network_output_channels = self.net.output_channels\n\n        elif params[""network""][\'type\'] == ""efficient-densenet"":\n            self.net = Efficient_DenseNet(self.input_channels, params[\'network\'])\n            self.network_output_channels = self.net.output_channels\n\n        elif params[""network""][\'type\'] == ""log-densenet"":\n            self.net = Log_Efficient_DenseNet(self.input_channels, params[\'network\'])\n            self.network_output_channels = self.net.output_channels\n        else:\n            raise ValueError(\n                \'Unknown architecture %s\' % params[\'network\'][\'type\'])\n\n        self.tie_target_weights = params[\'decoder\'][\'tie_target_weights\']\n        self.copy_source_weights = params[\'decoder\'][\'copy_source_weights\']\n\n        if self.tie_target_weights:\n            self.logger.warning(\'Tying the decoder weights\')\n            last_dim = params[\'decoder\'][\'input_dim\']\n        else:\n            last_dim = None\n\n        self.aggregator = Aggregator(self.network_output_channels,\n                                     last_dim,\n                                     params[\'aggregator\'])\n        self.final_output_channels = self.aggregator.output_channels  # d_h\n\n        self.prediction_dropout = nn.Dropout(\n            params[\'decoder\'][\'prediction_dropout\'])\n        self.logger.info(\'Output channels: %d\', self.final_output_channels)\n        self.prediction = nn.Linear(self.final_output_channels,\n                                    self.trg_vocab_size)\n        if self.copy_source_weights:\n            self.trg_embedding.label_embedding.weight = self.src_embedding.label_embedding.weight\n        if self.tie_target_weights:\n            self.prediction.weight = self.trg_embedding.label_embedding.weight\n\n    def init_weights(self):\n        """"""\n        Called after setup.buil_model to intialize the weights\n        """"""\n        if self.params[\'network\'][\'init_weights\'] == ""kaiming"":\n            nn.init.kaiming_normal_(self.prediction.weight)\n\n        self.src_embedding.init_weights()\n        self.trg_embedding.init_weights()\n        self.prediction.bias.data.fill_(0)\n    \n    def merge(self, src_emb, trg_emb):\n        """"""\n        Merge source and target embeddings\n        *_emb : N, T_t, T_s, d\n        """"""\n        N, Tt, Ts, _ = src_emb.size()\n        if self.merge_mode == \'concat\':\n            # 2d grid:\n            return torch.cat((src_emb, trg_emb), dim=3)\n        elif self.merge_mode == \'product\':\n            return src_emb * trg_emb\n        elif self.merge_mode == \'bilinear\':\n            # self.bw : d\n            # for every target position\n            X = []\n            for t in range(Tt):\n                # trg_emb[:, t, :] (N, 1, d_t)\n                e = trg_emb[:, t:t+1, 0, :]\n                w = self.bw.expand(N, -1).unsqueeze(-1)\n                # print(\'e:\', e.size())\n                # print(\'bw:\', w.size())\n                x = torch.bmm(w, e).transpose(1, 2)\n                # print(\'x:\', x.size())\n                # x  (N, d_t, d) & src_emb (N, T_s, d_s = d_t) => (N, 1, T_s, d)\n                x = torch.bmm(src_emb[:,0], x).unsqueeze(1)\n                # print(\'appending:\', x.size())\n                X.append(x)\n            return torch.cat(X, dim=1)\n\n        elif self.merge_mode == ""multi-sim"":\n            # self.bw d, d_t, ds\n            X = []\n            for k in range(self.sim_dim):\n                w = self.bw[k].expand(N, -1, -1)\n                # print(\'w:\', w.size())\n                # print(trg_emb[:,:,0].size())\n                # print(src_emb[:,0].size())\n                x = torch.bmm(torch.bmm(trg_emb[:,:,0], w), src_emb[:,0].transpose(1,2)).unsqueeze(-1)\n                # print(\'x:\', x.size())\n                X.append(x)\n            return torch.cat(X, dim=-1)\n\n        elif self.merge_mode == ""multi-sim2"":\n            # self.bw d, d_t, ds\n            X = []\n            for n in range(N):\n                x = torch.bmm(torch.bmm(trg_emb[n:n+1,:,0].expand(self.sim_dim, -1, -1), self.bw),\n                              src_emb[n:n+1,0].expand(self.sim_dim, -1, -1).transpose(1,2)\n                             ).unsqueeze(0)\n                X.append(x)\n            return torch.cat(X, dim=0).permute(0, 2, 3, 1)\n\n        else:\n            raise ValueError(\'Unknown merging mode\')\n\n    # @profile\n    def forward(self, data_src, data_trg):\n        src_emb = self.src_embedding(data_src)\n        trg_emb = self.trg_embedding(data_trg)\n        Ts = src_emb.size(1)  # source sequence length\n        Tt = trg_emb.size(1)  # target sequence length\n        # 2d grid:\n        src_emb = _expand(src_emb.unsqueeze(1), 1, Tt)\n        trg_emb = _expand(trg_emb.unsqueeze(2), 2, Ts)\n        X = self.merge(src_emb, trg_emb)\n        # del src_emb, trg_emb\n        X = self._forward(X, data_src[\'lengths\'])\n        logits = F.log_softmax(\n            self.prediction(self.prediction_dropout(X)), dim=2)\n        return logits\n\n    # @profile\n    def _forward(self, X, src_lengths=None, track=False):\n        X = X.permute(0, 3, 1, 2)\n        X = self.net(X)\n        if track:\n            X, attn = self.aggregator(X, src_lengths, track=True)\n            return X, attn\n        X = self.aggregator(X, src_lengths, track=track)\n        return X\n\n    def update(self, X, src_lengths=None, track=False):\n        X = X.permute(0, 3, 1, 2)\n        X = self.net.update(X)\n        attn = None\n        if track:\n            X, attn = self.aggregator(X, src_lengths, track=track)\n        else:\n            X = self.aggregator(X, src_lengths, track=track)\n        return X, attn\n\n    def track_update(self, data_src, kwargs={}):\n        """"""\n        Sample and return tracked activations\n        Using update where past activations are discarded\n        """"""\n        batch_size = data_src[\'labels\'].size(0)\n        src_emb = self.src_embedding(data_src)\n        Ts = src_emb.size(1)  # source sequence length\n        max_length = int(\n            kwargs.get(\'max_length_a\', 0) * Ts +\n            kwargs.get(\'max_length_b\', 50)\n            )\n\n        trg_labels = torch.LongTensor(\n            [[self.bos_token] for i in range(batch_size)]\n            ).cuda()\n        trg_emb = self.trg_embedding.single_token(trg_labels, 0)\n        # 2d grid:\n        src_emb = src_emb.unsqueeze(1)  # Tt=1\n        src_emb_ = src_emb\n        seq = []\n        alphas = []\n        aligns = []\n        activ_aligns = []\n        activs = []\n        trg_emb = _expand(trg_emb.unsqueeze(2), 2, Ts)\n        for t in range(max_length):\n            X = self.merge(src_emb, trg_emb)\n            Y, attn = self.update(X, data_src[""lengths""], track=True)\n            # align, activ_distrib, activ = attn\n            if attn[0] is not None:\n                alphas.append(attn[0])\n            aligns.append(attn[1])\n            activ_aligns.append(attn[2])\n            activs.append(attn[3][0])\n            proj = self.prediction_dropout(Y[:, -1, :])\n            logits = F.log_softmax(self.prediction(proj), dim=1)\n            if self.padding_idx:\n                logits[:, self.padding_idx] = -math.inf\n                npargmax = logits.data.cpu().numpy().argmax(axis=-1)\n            else:\n                logits = logits[:, 1:]  # remove pad\n                npargmax = 1 + logits.data.cpu().numpy().argmax(axis=-1)\n            next_preds = torch.from_numpy(npargmax).view(-1, 1).cuda()\n            seq.append(next_preds)\n            trg_emb_t = self.trg_embedding.single_token(next_preds,\n                                                        t).unsqueeze(2)\n            trg_emb_t = _expand(trg_emb_t, 2, Ts)\n            max_h = self.kernel_size // 2 + 1  \n            # keep only what\'s needed\n            if trg_emb.size(1) > max_h:\n                trg_emb = trg_emb[:, -max_h:, :, :]\n            trg_emb = torch.cat((trg_emb, trg_emb_t), dim=1)\n            src_emb = _expand(src_emb_, 1, trg_emb.size(1))\n            if t >= 1:\n                # stop when all finished\n                unfinished = torch.add(\n                    torch.mul((next_preds == self.eos_token).type_as(logits),\n                              -1), 1)\n                if unfinished.sum().data.item() == 0:\n                    break\n        seq = torch.cat(seq, 1)\n        self.net.reset_buffers()\n        self.trg_embedding.reset_buffers()\n        return seq, alphas, aligns, activ_aligns, activs\n\n\n    def track(self, data_src, kwargs={}):\n        """"""\n        Sample and return tracked activations\n        """"""\n        batch_size = data_src[\'labels\'].size(0)\n        src_emb = self.src_embedding(data_src)\n        Ts = src_emb.size(1)  # source sequence length\n        max_length = int(\n            kwargs.get(\'max_length_a\', 0) * Ts +\n            kwargs.get(\'max_length_b\', 50)\n            )\n        trg_labels = torch.LongTensor(\n            [[self.bos_token] for i in range(batch_size)]\n            ).cuda()\n        trg_emb = self.trg_embedding.single_token(trg_labels, 0)\n        # 2d grid:\n        src_emb = src_emb.unsqueeze(1)  # Tt=1\n        src_emb_ = src_emb\n        seq = []\n        alphas = []\n        aligns = []\n        activ_aligns = []\n        activs = []\n        trg_emb = _expand(trg_emb.unsqueeze(2), 2, Ts)\n        for t in range(max_length):\n            X = self.merge(src_emb, trg_emb)\n            Y, attn = self._forward(X, data_src[""lengths""], track=True)\n            if attn[0] is not None:\n                alphas.append(attn[0])\n            aligns.append(attn[1])\n            activ_aligns.append(attn[2])\n            activs.append(attn[3][0])\n            proj = self.prediction_dropout(Y[:, -1, :])\n            logits = F.log_softmax(self.prediction(proj), dim=1)\n            if self.padding_idx:\n                logits[:, self.padding_idx] = -math.inf\n                npargmax = logits.data.cpu().numpy().argmax(axis=-1)\n            else:\n                logits = logits[:, 1:]  # remove pad\n                npargmax = 1 + logits.data.cpu().numpy().argmax(axis=-1)\n            next_preds = torch.from_numpy(npargmax).view(-1, 1).cuda()\n            seq.append(next_preds)\n            trg_emb_t = self.trg_embedding.single_token(next_preds,\n                                                        t).unsqueeze(2)\n            trg_emb_t = _expand(trg_emb_t, 2, Ts)\n            trg_emb = torch.cat((trg_emb, trg_emb_t), dim=1)\n            src_emb = _expand(src_emb_, 1, trg_emb.size(1))\n            if t >= 1:\n                # stop when all finished\n                unfinished = torch.add(\n                    torch.mul((next_preds == self.eos_token).type_as(logits),\n                              -1), 1)\n                if unfinished.sum().data.item() == 0:\n                    break\n        seq = torch.cat(seq, 1)\n        self.trg_embedding.reset_buffers()\n        return seq, alphas, aligns, activ_aligns, activs\n\n    def sample_update(self, data_src, scorer, kwargs={}):\n        """"""\n        Sample in evaluation mode\n        Using update where past activations are discarded\n        """"""\n        beam_size = kwargs.get(\'beam_size\', 1)\n        if beam_size > 1:\n            # Without update\n            return self.sample_beam(data_src, kwargs)\n        batch_size = data_src[\'labels\'].size(0)\n        src_emb = self.src_embedding(data_src)\n        Ts = src_emb.size(1)  # source sequence length\n        max_length = int(\n            kwargs.get(\'max_length_a\', 0) * Ts +\n            kwargs.get(\'max_length_b\', 50)\n            )\n        trg_labels = torch.LongTensor(\n            [[self.bos_token] for i in range(batch_size)]\n            ).cuda()\n        trg_emb = self.trg_embedding.single_token(trg_labels, 0)\n        # 2d grid:\n        src_emb = src_emb.unsqueeze(1)  # Tt=1\n        src_emb_ = src_emb\n        seq = []\n        trg_emb = _expand(trg_emb.unsqueeze(2), 2, Ts)\n        for t in range(max_length):\n            X = self.merge(src_emb, trg_emb)\n            Y, _ = self.update(X, data_src[""lengths""])\n            proj = self.prediction_dropout(Y[:, -1, :])\n            logits = F.log_softmax(self.prediction(proj), dim=1)\n            if self.padding_idx:\n                logits[:, self.padding_idx] = -math.inf\n                npargmax = logits.data.cpu().numpy().argmax(axis=-1)\n            else:\n                logits = logits[:, 1:]  # remove pad\n                npargmax = 1 + logits.data.cpu().numpy().argmax(axis=-1)\n            next_preds = torch.from_numpy(npargmax).view(-1, 1).cuda()\n            seq.append(next_preds)\n            trg_emb_t = self.trg_embedding.single_token(next_preds,\n                                                        t).unsqueeze(2)\n            trg_emb_t = _expand(trg_emb_t, 2, Ts)\n            max_h = self.kernel_size // 2 + 1 \n            # keep only what\'s needed\n            if trg_emb.size(1) > max_h:\n                trg_emb = trg_emb[:, -max_h:, :, :]\n            trg_emb = torch.cat((trg_emb, trg_emb_t), dim=1)\n            src_emb = _expand(src_emb_, 1, trg_emb.size(1))\n            if t >= 1:\n                # stop when all finished\n                unfinished = torch.add(\n                    torch.mul((next_preds == self.eos_token).type_as(logits),\n                              -1), 1)\n                if unfinished.sum().data.item() == 0:\n                    break\n        seq = torch.cat(seq, 1)\n        self.net.reset_buffers()\n        self.trg_embedding.reset_buffers()\n        return seq, None\n\n    def sample(self, data_src, scorer, kwargs={}):\n        """"""\n        Sample in evaluation mode\n        """"""\n        beam_size = kwargs.get(\'beam_size\', 1)\n        if beam_size > 1:\n            return self.sample_beam(data_src, kwargs)\n        batch_size = data_src[\'labels\'].size(0)\n        src_emb = self.src_embedding(data_src)\n        Ts = src_emb.size(1)  # source sequence length\n        max_length = int(\n            kwargs.get(\'max_length_a\', 0) * Ts +\n            kwargs.get(\'max_length_b\', 50)\n            )\n        trg_labels = torch.LongTensor(\n            [[self.bos_token] for i in range(batch_size)]\n            ).cuda()\n        trg_emb = self.trg_embedding.single_token(trg_labels, 0)\n        # 2d grid:\n        src_emb = src_emb.unsqueeze(1)  # Tt=1\n        src_emb_ = src_emb\n        seq = []\n        trg_emb = _expand(trg_emb.unsqueeze(2), 2, Ts)\n        for t in range(max_length):\n            X = self.merge(src_emb, trg_emb)\n            Y = self._forward(X, data_src[""lengths""])\n            proj = self.prediction_dropout(Y[:, -1, :])\n            logits = F.log_softmax(self.prediction(proj), dim=1)\n            if self.padding_idx:\n                logits[:, self.padding_idx] = -math.inf\n                npargmax = logits.data.cpu().numpy().argmax(axis=-1)\n            else:\n                logits = logits[:, 1:]  # remove pad\n                npargmax = 1 + logits.data.cpu().numpy().argmax(axis=-1)\n            next_preds = torch.from_numpy(npargmax).view(-1, 1).cuda()\n            seq.append(next_preds)\n            trg_emb_t = self.trg_embedding.single_token(next_preds,\n                                                        t).unsqueeze(2)\n            trg_emb_t = _expand(trg_emb_t, 2, Ts)\n            trg_emb = torch.cat((trg_emb, trg_emb_t), dim=1)\n            src_emb = _expand(src_emb_, 1, trg_emb.size(1))\n            if t >= 1:\n                # stop when all finished\n                unfinished = torch.add(\n                    torch.mul((next_preds == self.eos_token).type_as(logits),\n                              -1), 1)\n                if unfinished.sum().data.item() == 0:\n                    break\n        seq = torch.cat(seq, 1)\n        return seq, None\n\n    def sample_beam(self, data_src, kwargs={}):\n        beam_size = kwargs[\'beam_size\']\n        src_labels = data_src[\'labels\']\n        src_lengths = data_src[\'lengths\']\n        batch_size = src_labels.size(0)\n        beam = [Beam(beam_size, kwargs) for k in range(batch_size)]\n        batch_idx = list(range(batch_size))\n        remaining_sents = batch_size\n        Ts = src_labels.size(1)  # source sequence length\n        max_length = int(\n            kwargs.get(\'max_length_a\', 0) * Ts +\n            kwargs.get(\'max_length_b\', 50)\n            )\n        src_labels = src_labels.repeat(beam_size, 1)\n        src_lengths = src_lengths.repeat(beam_size, 1)\n        for t in range(max_length):\n            # Source:\n            src_emb = self.src_embedding({\n                \'labels\': src_labels,\n                \'lengths\': None\n            }).unsqueeze(1).repeat(1, t + 1, 1, 1)\n            trg_labels_t = torch.stack([\n                b.get_current_state() for b in beam if not b.done\n            ]).t().contiguous().view(-1, 1)\n            if t:\n                # append to the previous tokens\n                trg_labels = torch.cat((trg_labels, trg_labels_t), dim=1)\n            else:\n                trg_labels = trg_labels_t\n\n            trg_emb = self.trg_embedding({\n                \'labels\': trg_labels,\n                \'lengths\': None\n            }).unsqueeze(2).repeat(1, 1, Ts, 1)\n            # X: N, Tt, Ts, Ds+Dt\n            X = self.merge(src_emb, trg_emb)\n            Y = self._forward(X, src_lengths)\n            proj = self.prediction_dropout(Y[:, -1, :])\n            logits = F.log_softmax(self.prediction(proj), dim=1)\n            word_lk = logits.view(beam_size,\n                                  remaining_sents,\n                                  -1).transpose(0, 1).contiguous()\n            active = []\n            for b in range(batch_size):\n                if beam[b].done:\n                    continue\n                idx = batch_idx[b]\n                if not beam[b].advance(word_lk.data[idx], t):\n                    active += [b]\n                trg_labels_prev = trg_labels.view(beam_size,\n                                                  remaining_sents,\n                                                  t + 1)\n                trg_labels = trg_labels_prev[\n                    beam[b].get_current_origin()].view(-1, t + 1)\n            if not active:\n                break\n            # in this section, the sentences that are still active are\n            # compacted so that the decoder is not run on completed sentences\n            active_idx = torch.cuda.LongTensor([batch_idx[k] for k in active])\n            batch_idx = {beam: idx for idx, beam in enumerate(active)}\n\n            def update_active(t):\n                # select only the remaining active sentences\n                view = t.data.contiguous().view(beam_size,\n                                                remaining_sents,\n                                                *t.size()[1:])\n                new_size = list(view.size())\n                new_size[1] = new_size[1] * len(active_idx) \\\n                    // remaining_sents\n                result = view.index_select(1, active_idx).view(*new_size)\n                return result.view(-1, result.size(-1))\n\n            src_labels = update_active(src_labels)\n            src_lengths = update_active(src_lengths)\n            trg_labels = update_active(trg_labels)\n            remaining_sents = len(active)\n\n        # Wrap up\n        allHyp, allScores = [], []\n        n_best = 1\n        for b in range(batch_size):\n            scores, ks = beam[b].sort_best()\n            allScores += [scores[:n_best]]\n            hyps = beam[b].get_hyp(ks[0])\n            allHyp += [hyps]\n        return allHyp, allScores\n\n\nclass Pervasive_Parallel(nn.DataParallel):\n    """"""\n    Wrapper for parallel training\n    """"""\n    def __init__(self, jobname, params, src_vocab_size, trg_vocab_size,\n                 special_tokens):\n        model = Pervasive(jobname, params, src_vocab_size, trg_vocab_size,\n                          special_tokens)\n        nn.DataParallel.__init__(self, model)\n        self.logger = logging.getLogger(jobname)\n        self.version = \'conv\'\n        self.params = params\n        self.src_vocab_size = src_vocab_size\n        self.trg_vocab_size = trg_vocab_size\n        self.pad_token = special_tokens[\'PAD\']\n        # assert self.pad_token == 0, ""Padding token should be 0""\n        self.bos_token = special_tokens[\'BOS\']\n        self.eos_token = special_tokens[\'EOS\']\n        self.kernel_size = max(list(itertools.chain.from_iterable(\n            params[\'network\'][\'kernels\']\n            )))\n\n    def init_weights(self):\n        self.module.init_weights()\n\n    def _forward(self, X, src_lengths=None):\n        return self.module._forward(self, X, src_lengths)\n\n    def update(self, X, src_lengths=None):\n        return self.module.update(X, src_lengths)\n\n    def sample(self, data_src, scorer=None, kwargs={}):\n        return self.module.sample(data_src, scorer, kwargs)\n'"
nmt/models/pooling.py,11,"b'import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n__all__ = [\'truncated_max\', ""truncated_mean"",\n           \'average_code\', \'max_code\',\n           ""MaxAttention"",\n          ]\n\n\ndef truncated_max(tensor, src_lengths, track=False, *args):\n    """"""\n    Max-pooling up to effective legth\n    """"""\n    # input size: N, d, Tt, Ts\n    # src_lengths : N,\n    Pool = []\n    Attention = []\n    for n in range(tensor.size(0)):\n        X = tensor[n]\n        xpool, attn = X[:, :, :src_lengths[n]].max(dim=2)\n        if track:\n            targets = torch.arange(src_lengths[n])\n            align = targets.apply_(lambda k: sum(attn[:, -1] == k))\n            align = align/align.sum()\n            Attention.append(align.unsqueeze(0))\n        Pool.append(xpool.unsqueeze(0))\n    result = torch.cat(Pool, dim=0)\n    if track:\n        return result, torch.cat(Attention, dim=0).cuda()\n    return result\n\n\ndef truncated_mean(tensor, src_lengths, *args):\n    """"""\n    Average-pooling up to effective legth\n    """"""\n    # input size: N, d, Tt, Ts\n    # src_lengths : N,\n    # n=1\n    # print(\'tensor:\', tensor.size(), \'lengths:\', src_lengths)\n    Pool = []\n    Attention = []\n    for n in range(tensor.size(0)):\n        X = tensor[n]\n        xpool = X[:, :, :src_lengths[n]].mean(dim=2)\n        xpool *=  math.sqrt(src_lengths[n])\n        Pool.append(xpool.unsqueeze(0))\n    result = torch.cat(Pool, dim=0)\n    return result\n\n\ndef average_code(tensor, *args):\n    return tensor.mean(dim=3)\n\n\ndef max_code(tensor, src_lengths=None, track=False):\n    # input size: N, d, Tt, Ts\n    # src_lengths : N, 1\n    if track:\n        batch_size, nchannels, _, max_len = tensor.size()\n        xpool, attn = tensor.max(dim=3)\n        targets = torch.arange(max_len).type_as(attn)\n        align = []\n        activ_distrib = []\n        activ = []\n        for n in range(batch_size):\n            # distribution of the argmax indices\n            align.append(np.array([\n                torch.sum(attn[n, :, -1] == k, dim=-1).data.item() / nchannels\n                for k in targets\n            ]))\n            # weighted distribution of the argmax indices\n            activ_distrib.append(np.array([\n                torch.sum((attn[n, :, -1] == k).float() * xpool[n, :, -1], dim=-1).data.item()\n                for k in targets\n            ]))\n            # return the sparse tensor (0 if not pooled, value otherwise)\n            activ.append(np.array([\n                ((attn[n, :, -1] == k).float() * xpool[n, :, -1]).data.cpu().numpy()\n                for k in targets\n            ]))\n\n        align = np.array(align)\n        activ = np.array(activ)\n        activ_distrib = np.array(activ_distrib)\n        return xpool, (None, align, activ_distrib, activ)\n    else:\n        return tensor.max(dim=3)[0]\n\n\nclass MaxAttention(nn.Module):\n    def __init__(self, params, in_channels):\n        super(MaxAttention, self).__init__()\n        self.in_channels = in_channels\n        self.attend = nn.Linear(in_channels, 1)\n        self.dropout = params[\'attention_dropout\']\n        self.scale_ctx = params.get(\'scale_ctx\', 1)\n        if params[\'nonlin\'] == ""tanh"":\n            self.nonlin = F.tanh\n        elif params[\'nonlin\'] == ""relu"":\n            self.nonlin = F.relu\n        else:\n            self.nonlin = lambda x: x\n        if params[\'first_aggregator\'] == ""max"":\n            self.max = max_code\n        elif params[\'first_aggregator\'] == ""truncated-max"":\n            self.max = truncated_max\n        elif params[\'first_aggregator\'] == ""skip"":\n            self.max = None\n        else:\n            raise ValueError(\'Unknown mode for first aggregator \', params[\'first_aggregator\'])\n\n    def forward(self, X, src_lengths, track=False, *args):\n        if track:\n            N, d, Tt, Ts = X.size()\n            Xatt = X.permute(0, 2, 3, 1)\n            alphas = self.nonlin(self.attend(Xatt))\n            alphas = F.softmax(alphas, dim=2)\n            # print(\'alpha:\', alphas.size(), alphas)\n            # alphas : N, Tt, Ts , 1\n            context = alphas.expand_as(Xatt) * Xatt\n            # Mean over Ts >>> N, Tt, d\n            context = context.mean(dim=2).permute(0, 2, 1)\n            if self.scale_ctx:\n                context = math.sqrt(Ts) * context\n            # Projection N, Tt, d\n            if self.max is not None:\n                Xpool, tracking = self.max(X,\n                                           src_lengths,\n                                           track=True)\n                feat = torch.cat((Xpool, context), dim=1)\n                return feat, (alphas[0, -1, :, 0].data.cpu().numpy(), *tracking[1:])\n            else:\n                return context\n        else:\n            N, d, Tt, Ts = X.size()\n            Xatt = X.permute(0, 2, 3, 1)\n            alphas = self.nonlin(self.attend(Xatt))\n            alphas = F.softmax(alphas, dim=2)\n            # alphas : N, Tt, Ts , 1\n            context = alphas.expand_as(Xatt) * Xatt\n            # Mean over Ts >>> N, Tt, d\n            context = context.mean(dim=2).permute(0, 2, 1)\n            if self.scale_ctx:\n                context = math.sqrt(Ts) * context\n            # Projection N, Tt, d\n            if self.max is not None:\n                Xpool = self.max(X, src_lengths)\n                return torch.cat((Xpool, context), dim=1)\n            else:\n                return context\n\n'"
nmt/models/seq2seq.py,1,"b'""""""\nSequence to Sequence with attention\nParent model.\n""""""\nimport torch.nn as nn\nfrom .encoder import Encoder\nfrom .cond_decoder import CondDecoder\nimport logging\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, jobname, params, src_vocab_size, trg_vocab_size, trg_specials):\n        """"""Initialize model.""""""\n        nn.Module.__init__(self)\n        self.logger = logging.getLogger(jobname)\n        self.version = ""seq2seq""\n        self.params = params\n        self.encoder = Encoder(params[\'encoder\'], src_vocab_size)\n        self.decoder = CondDecoder(params[\'decoder\'], params[\'encoder\'],\n                                   trg_vocab_size, trg_specials)\n        self.mapper_dropout = nn.Dropout(params[\'mapper\'][\'dropout\'])\n        self.mapper = nn.Linear(self.encoder.size,\n                                self.decoder.size)\n\n    def init_weights(self):\n        """"""Initialize weights.""""""\n        self.encoder.init_weights()\n        self.decoder.init_weights()\n        self.mapper.bias.data.fill_(0)\n\n    def map(self, source):\n        """""" map the source code to the decoder cell size """"""\n        # map hT^(enc) to h0^(dec)\n        source[\'state\'][0] = nn.Tanh()(self.mapper_dropout(\n            self.mapper(source[\'state\'][0])\n        ))\n        return source\n\n    def forward(self, data_src, data_trg):\n        source = self.encoder(data_src)\n        source = self.map(source)\n        logits = self.decoder(source, data_trg)\n        return logits\n\n    def sample(self, source, kwargs={}):\n        """"""\n        Sample given source with keys:\n            state - ctx - emb\n        """"""\n        return self.decoder.sample(source, kwargs)\n\n'"
nmt/models/setup.py,0,"b'""""""\nSetup the model and the loss criterion\n""""""\nimport nmt.loss as loss\nfrom .seq2seq import Seq2Seq\nfrom .pervasive import Pervasive, Pervasive_Parallel\n\n\n\ndef build_model(jobname, params, src_vocab_size, trg_vocab_size, trg_specials):\n    ref = params[\'model\']\n    if ref == ""seq2seq-attention"":\n        model = Seq2Seq(jobname, params,\n                        src_vocab_size,\n                        trg_vocab_size,\n                        trg_specials)\n    elif ref == ""pervasive"":\n        model = Pervasive(jobname, params, src_vocab_size,\n                          trg_vocab_size, trg_specials)\n    elif ref == ""pervasive-parallel"":\n        model = Pervasive_Parallel(jobname, params, src_vocab_size,\n                                   trg_vocab_size, trg_specials)\n\n    else:\n        raise ValueError(\'Unknown model %s\' % ref)\n\n    model.init_weights()\n    return model\n\n\ndef define_loss(jobname, params, trg_dict):\n    """"""\n    Define training criterion\n    """"""\n    ver = params[\'version\'].lower()\n    if ver == \'ml\':\n        crit = loss.MLCriterion(jobname, params)\n    elif ver == \'smooth_ml\':\n        crit = loss.SmoothMLCriterion(jobname, params)\n    else:\n        raise ValueError(\'unknown loss mode %s\' % ver)\n    crit.log()\n    return crit\n\n\n'"
nmt/models/transitions.py,1,"b'from math import sqrt\nimport torch.nn as nn\n\n\nclass Transition(nn.Sequential):\n    """"""\n    Transiton btw dense blocks:\n    BN > ReLU > Conv(k=1) to reduce the number of channels\n    """"""\n    def __init__(self, num_input_features, num_output_features, init_weights=0):\n        super(Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        conv = nn.Conv2d(\n                num_input_features,\n                num_output_features,\n                kernel_size=1,\n                bias=False)\n        if init_weights == ""manual"":\n            std = sqrt(2 / num_input_features)\n            conv.weight.data.normal_(0, std)\n        self.add_module(\'conv\', conv)\n\n    def forward(self, x, *args):\n        return super(Transition, self).forward(x)\n\n\nclass Transition2(nn.Sequential):\n    """"""\n    Transiton btw dense blocks:\n    ReLU > Conv(k=1) to reduce the number of channels\n\n    """"""\n    def __init__(self, num_input_features, num_output_features):\n        super(Transition2, self).__init__()\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\n            \'conv\',\n            nn.Conv2d(\n                num_input_features,\n                num_output_features,\n                kernel_size=1,\n                bias=False))\n        \n    def forward(self, x, *args):\n        return super(Transition2, self).forward(x)\n\n'"
nmt/params/__init__.py,0,"b'from .parse import parse_params, parse_eval_params\nfrom .gpu import set_env\n'"
nmt/params/gpu.py,0,"b'import subprocess\nimport os\nimport logging\n\n\ndef set_env(jobname, manual_gpu_id):\n    """"""\n    Setup Cuda visible devices\n    """"""\n    logger = logging.getLogger(jobname)\n    # setup gpu\n    try:\n        gpu_id = subprocess.check_output(\n            \'gpu_getIDs.sh\', shell=True).decode(\'UTF-8\')\n        gpu_ids = gpu_id.split()\n        num_gpus = 1\n        if len(gpu_ids) > 1:\n            gpu_id = "","".join(gpu_ids)\n            num_gpus = len(gpu_ids)\n        else:\n            gpu_id = str(gpu_ids[0])\n        logger.warning(\'Assigned GPUs: %s\', gpu_id)\n        assert type(gpu_id) == str\n    except:\n        gpu_id = manual_gpu_id\n        num_gpus = len(gpu_id.split(\',\'))\n        logger.warning(\'Selected GPUs: %s\', gpu_id)\n\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = gpu_id\n    return num_gpus\n'"
nmt/params/parse.py,0,"b'""""""\nRead config files and pass dict of parameters\n""""""\nimport sys\nimport os\nimport os.path as osp\nimport argparse\nimport yaml\nfrom .utils import update, parse_densenet_params, create_logger\n\n\ndef parse_eval_params():\n    """"""\n    Parse parametres from config file for evaluation\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-c"", ""--config"", type=str,\n                        help=""configuration file"")\n    # Command line arguments\n    parser.add_argument(""-v"", ""--verbose"", type=int,\n                        default=1, help=""code verbosity"")\n    parser.add_argument(""-g"", ""--gpu_id"", type=str,\n                        default=\'0\', help=""gpu id"")\n    parser.add_argument(""-b"", ""--beam_size"", type=int,\n                        default=5, help=""beam size for decoding"")\n    parser.add_argument(""-o"", ""--offset"", type=int,\n                        default=0, help=""starting index used to visualize a specific batch"")\n    parser.add_argument(""--read_length"", type=int,\n                        default=0, help=""max length for loading"")\n    parser.add_argument(""--max_length_a"",  type=float,\n                        default=0, help=""Decode up to a*source_lenght + b : (a)"")\n    parser.add_argument(""--max_length_b"", type=int,\n                        default=80, help=""Decode up to a*source_lenght + b : (b)"")\n    parser.add_argument(""-n"", ""--batch_size"", type=int,\n                        default=1, help=""batch size for decoding"")\n    parser.add_argument(""-l"", ""--last"", action=""store_true"",\n                        help=""evaluate with the last checkpoint instead of the best one"")\n    parser.add_argument(""--norm"", action=""store_true"", help=""Normalize scores by length"")\n    parser.add_argument(""-m"", ""--max_samples"", type=int,\n                        default=100, help=""Decode up to max_samples sequences"")\n    parser.add_argument(""--block_ngram_repeat"", type=int,\n                        default=0, help=""GNMT parameter"")\n    parser.add_argument(""--length_penalty"", ""-p"", type=float,\n                        default=0, help=""length penalty for GNMT"")\n    parser.add_argument(""--length_penalty_mode"", type=str,\n                        default=""wu"", help=""length penalty mode, either wu or avg for GNMTscorer"")\n\n    parser.add_argument(""--stepwise_penalty"", action=""store_true"")\n    parser.add_argument(""-s"", ""--split"", type=str,\n                        default=""test"", help=""Split to evaluate"")\n\n    args = parser.parse_args()\n    default_config_path = ""config/default.yaml""\n    if args.config:\n        config = yaml.load(open(args.config))\n        default_config_path = config.get(\'default_config\', default_config_path)\n\n    default_config = yaml.load(open(default_config_path))\n    default_config = update(default_config, config)\n    parser.set_defaults(**default_config)\n    args = parser.parse_args()\n    # Mkdir the model save directory\n    args.eventname = \'events/\' + args.modelname\n    args.modelname = \'save/\' + args.modelname\n    args = vars(args)\n    args = parse_densenet_params(args)\n    # Make sure the dirs exist:\n    if not osp.exists(args[\'modelname\']):\n        sys.exit(\'Missing direcetory %s\' % args[\'modelname\'])\n    # Create the logger\n    logger = create_logger(args[\'modelname\'] + ""_eval"",\n                           \'%s/eval.log\' % args[\'modelname\'])\n    return args\n\n\ndef parse_params():\n    """"""\n    Parse parametres from config file for training\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-c"", ""--config"",\n                        help=""Specify config file"", metavar=""FILE"")\n    parser.add_argument(""-v"", ""--verbose"", type=int,\n                        default=1, help=""code verbosity"")\n    parser.add_argument(""-g"", ""--gpu_id"", type=str,\n                        default=\'0\', help=""gpu id"")\n    args = parser.parse_args()\n    default_config_path = ""config/default.yaml""\n    if args.config:\n        config = yaml.load(open(args.config))\n        default_config_path = config.get(\'default_config\', default_config_path)\n\n    default_config = yaml.load(open(default_config_path))\n    default_config = update(default_config, config)\n    parser.set_defaults(**default_config)\n    args = parser.parse_args()\n    args.eventname = \'events/\' + args.modelname\n    args.modelname = \'save/\' + args.modelname\n    # Make sure the dirs exist:\n    if not osp.exists(args.eventname):\n        os.makedirs(args.eventname)\n    if not osp.exists(args.modelname):\n        os.makedirs(args.modelname)\n    # Create the logger\n    logger = create_logger(args.modelname, \'%s/train.log\' % args.modelname)\n    args = vars(args)\n    args = parse_densenet_params(args)\n    return args\n\n'"
nmt/params/utils.py,0,"b'""""""\nMiscellaneous\n""""""\nimport os\nimport os.path as osp\nimport collections\nimport itertools\nimport logging\n\n\ndef read_list(param):\n    """""" Parse list of integers """"""\n    return [int(p) for p in str(param).split(\',\')]\n\n\ndef update(d, u):\n    """"""update dict of dicts""""""\n    for k, v in u.items():\n        if isinstance(v, collections.Mapping):\n            d[k] = update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n\n\ndef parse_densenet_params(args):\n    if ""network"" in args:\n        if ""num_layers"" in args[\'network\']:\n            num_layers = read_list(args[\'network\'][\'num_layers\'])\n            if ""kernels"" in args[\'network\']:\n                kernels = read_list(args[\'network\'][\'kernels\'])\n                if len(kernels) == 1 :\n                    args[\'network\'][\'kernels\'] = [kernels * n for n in num_layers]\n                else:\n                    assert len(kernels) == sum(num_layers), ""The number of kernel sizes must match that of the network layers""\n                    batched_kernels = []\n                    left = 0 \n                    for nn in num_layers:\n                        batched_kernels.append(kernels[left:left+nn])\n                        left += nn\n                    args[""network""][""kernels""] = batched_kernels\n            args[\'network\'][\'num_layers\'] = num_layers\n    # print(args[\'network\'][\'num_layers\'])\n    # print(args[\'network\'][\'kernels\'])\n    return args\n\n\nclass ColorStreamHandler(logging.StreamHandler):\n    """"""Logging with colors""""""\n    DEFAULT = \'\\x1b[0m\'\n    RED = \'\\x1b[31m\'\n    GREEN = \'\\x1b[32m\'\n    YELLOW = \'\\x1b[33m\'\n    CYAN = \'\\x1b[36m\'\n\n    CRITICAL = RED\n    ERROR = RED\n    WARNING = YELLOW\n    INFO = GREEN\n    DEBUG = CYAN\n\n    @classmethod\n    def _get_color(cls, level):\n        if level >= logging.CRITICAL:\n            return cls.CRITICAL\n        if level >= logging.ERROR:\n            return cls.ERROR\n        if level >= logging.WARNING:\n            return cls.WARNING\n        if level >= logging.INFO:\n            return cls.INFO\n        if level >= logging.DEBUG:\n            return cls.DEBUG\n        return cls.DEFAULT\n\n    def __init__(self, stream=None):\n        logging.StreamHandler.__init__(self, stream)\n\n    def format(self, record):\n        text = logging.StreamHandler.format(self, record)\n        color = self._get_color(record.levelno)\n        return color + text + self.DEFAULT\n\n\ndef create_logger(job_name, log_file=None, debug=True):\n    """"""\n    Initialize global logger\n    log_file: log to this file, besides console output\n    return: created logger\n    """"""\n    logging.basicConfig(level=5,\n                        format=\'%(asctime)s %(name)-12s %(levelname)-8s %(message)s\',\n                        datefmt=\'%m-%d %H:%M\')\n    logging.root.handlers = []\n    if debug:\n        chosen_level = 5\n    else:\n        chosen_level = logging.INFO\n    logger = logging.getLogger(job_name)\n    formatter = logging.Formatter(fmt=\'%(asctime)s %(message)s\',\n                                  datefmt=\'%m/%d %H:%M\')\n    if log_file is not None:\n        log_dir = osp.dirname(log_file)\n        if log_dir:\n            if not osp.exists(log_dir):\n                os.makedirs(log_dir)\n        # cerate file handler\n        fh = logging.FileHandler(log_file)\n        fh.setLevel(chosen_level)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n    # Colored stream handler\n    sh = ColorStreamHandler()\n    sh.setLevel(chosen_level)\n    sh.setFormatter(formatter)\n    logger.addHandler(sh)\n    return logger\n'"
nmt/utils/__init__.py,0,"b'from .utils import pload, pdump\nfrom .utils import to_contiguous, decode_sequence, set_seed, get_scores\n'"
nmt/utils/logging.py,0,"b'# -*- coding: utf-8 -*-\n\n_ENDC = \'\\033[0m\'\nGREEN = \'\\x1b[32m\'\nYELLOW = \'\\x1b[33m\'\n\n\ndef print_sampled(source, gt, pred, score=None):\n    """"""\n    Print translated sequences\n    """"""\n    source = "" "".join(source.split()).encode(\'utf-8\')\n    gt = "" "".join(gt.split()).encode(\'utf-8\')\n    pred = "" "".join(pred.split()).encode(\'utf-8\')\n    print(""SRC: "", source, GREEN, \'\\nTRG:\', gt, YELLOW, ""\\nHYP: "", pred, _ENDC)\n    return pred\n\n\n\n'"
nmt/utils/utils.py,5,"b'# -*- coding: utf-8 -*-\n""""""\nUtilities\n""""""\n\nimport random\nimport pickle\nimport numpy as np\nimport torch\n\n\ndef pload(path):\n    """"""\n    Pickle load\n    """"""\n    return pickle.load(open(path, \'rb\'),\n                       encoding=\'iso-8859-1\')\n\n\ndef pdump(obj, path):\n    """"""\n    Picke dump\n    """"""\n    pickle.dump(obj, open(path, \'wb\'),\n                protocol=pickle.HIGHEST_PROTOCOL)\n\n\ndef set_seed(seed):\n    """"""\n    Set seed for reproducibility\n    """"""\n    torch.backends.cudnn.deterministic = True\n    torch.cuda.manual_seed_all(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n\ndef to_contiguous(tensor):\n    """"""\n    Return a contiguous tensor\n    Especially after: narrow() , view() , expand() or transpose()\n    """"""\n    if tensor.is_contiguous():\n        return tensor\n    return tensor.contiguous()\n\n\ndef decode_sequence(ix_to_word, seq, eos, bos, remove_bpe=0):\n    """"""\n    Decode sequence into natural language\n    Input: seq, N*D numpy array, with elements in 0 .. vocab_size.\n    """"""\n    N, D = seq.shape\n    out = []\n    for i in range(N):\n        txt = []\n        for j in range(D):\n            ix = seq[i, j].item()\n            if ix > 0 and not ix == eos:\n                if ix == bos:\n                    continue\n                else:\n                    txt.append(ix_to_word[ix])\n            else:\n                break\n        sent = "" "".join(txt)\n        if remove_bpe:\n            sent = sent.replace(\'@@ \', \'\')\n        out.append(sent)\n    return out\n\n\ndef get_scores(logp, target):\n    """"""\n    Return scores per sentence\n    """"""\n    batch_size = logp.size(0)  # assume 1\n    seq_length = logp.size(1)\n    mask = target.gt(1).float()\n\n    target = target[:, :seq_length]\n    mask = mask[:, :seq_length]\n    logp = to_contiguous(logp).view(-1, logp.size(2))\n    target = to_contiguous(target).view(-1, 1)\n    mask = to_contiguous(mask).view(-1, 1)\n    ml_output = - logp.gather(1, target) * mask\n    ml_output = torch.sum(ml_output) / torch.sum(mask)\n    score = - ml_output\n    # normalize\n    # ml_output /= torch.sum(mask)\n    return score\n\n\n'"
