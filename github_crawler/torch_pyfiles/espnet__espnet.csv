file_path,api_count,code
setup.py,1,"b'#!/usr/bin/env python3\nfrom distutils.version import LooseVersion\nimport os\nfrom setuptools import find_packages\nfrom setuptools import setup\n\n\nrequirements = {\n    ""install"": [\n        ""setuptools>=38.5.1"",\n        ""configargparse>=1.2.1"",\n        ""typeguard>=2.7.0"",\n        ""dataclasses"",  # For Python<3.7\n        ""humanfriendly"",\n        ""scipy>=1.4.1"",\n        ""matplotlib>=3.1.0"",\n        ""pillow>=6.1.0"",\n        ""editdistance==0.5.2"",\n        # DNN related\n        # \'torch==1.0.1\',  # Installation from anaconda is recommended for PyTorch\n        ""chainer==6.0.0"",\n        # \'cupy==6.0.0\',  # Do not install cupy as default\n        ""tensorboard>=1.14"",  # For pytorch>=1.1.0\n        ""tensorboardX>=1.8"",  # For pytorch<1.1.0\n        # Signal processing related\n        ""librosa>=0.7.0"",\n        ""resampy"",\n        ""pysptk>=0.1.17"",\n        # Natural language processing related\n        # FIXME(kamo): Sentencepiece 0.1.90 breaks backwardcompatibility?\n        ""sentencepiece<0.1.90,>=0.1.82"",\n        ""nltk>=3.4.5"",\n        # File IO related\n        ""PyYAML>=5.1.2"",\n        ""soundfile>=0.10.2"",\n        ""h5py==2.9.0"",\n        ""kaldiio>=2.15.0"",\n        # TTS related\n        ""inflect>=1.0.0"",\n        ""unidecode>=1.0.22"",\n        ""jaconv"",\n        ""g2p_en"",\n        ""nnmnkwii"",\n        ""espnet_tts_frontend"",\n        # ASR frontend related\n        ""museval>=0.2.1"",\n        ""pystoi>=0.2.2"",\n        ""nara_wpe>=0.0.5"",\n        ""torch_complex"",\n        ""pytorch_wpe"",\n    ],\n    ""setup"": [""numpy"", ""pytest-runner""],\n    ""test"": [\n        ""pytest>=3.3.0"",\n        ""pytest-pythonpath>=0.7.3"",\n        ""pytest-cov>=2.7.1"",\n        ""hacking>=1.1.0"",\n        ""mock>=2.0.0"",\n        ""pycodestyle"",\n        ""jsondiff>=1.2.0"",\n        ""flake8>=3.7.8"",\n        ""flake8-docstrings>=1.3.1"",\n        ""black"",\n    ],\n    ""doc"": [\n        ""Sphinx==2.1.2"",\n        ""sphinx-rtd-theme>=0.2.4"",\n        ""sphinx-argparse>=0.2.5"",\n        ""commonmark==0.8.1"",\n        ""recommonmark>=0.4.0"",\n        ""travis-sphinx>=2.0.1"",\n        ""nbsphinx>=0.4.2"",\n        ""sphinx-markdown-tables>=0.0.12"",\n    ],\n}\ntry:\n    import torch\n\n    if LooseVersion(torch.__version__) >= LooseVersion(""1.1.0""):\n        requirements[""install""].append(""torch_optimizer"")\n    del torch\nexcept ImportError:\n    pass\n\ninstall_requires = requirements[""install""]\nsetup_requires = requirements[""setup""]\ntests_require = requirements[""test""]\nextras_require = {\n    k: v for k, v in requirements.items() if k not in [""install"", ""setup""]\n}\n\ndirname = os.path.dirname(__file__)\nsetup(\n    name=""espnet"",\n    version=""0.7.0"",\n    url=""http://github.com/espnet/espnet"",\n    author=""Shinji Watanabe"",\n    author_email=""shinjiw@ieee.org"",\n    description=""ESPnet: end-to-end speech processing toolkit"",\n    long_description=open(os.path.join(dirname, ""README.md""), encoding=""utf-8"").read(),\n    long_description_content_type=""text/markdown"",\n    license=""Apache Software License"",\n    packages=find_packages(include=[""espnet*""]),\n    # #448: ""scripts"" is inconvenient for developping because they are copied\n    # scripts=get_all_scripts(\'espnet/bin\'),\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    extras_require=extras_require,\n    python_requires="">=3.6.0"",\n    classifiers=[\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: 3.8"",\n        ""Development Status :: 5 - Production/Stable"",\n        ""Intended Audience :: Science/Research"",\n        ""Operating System :: POSIX :: Linux"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Topic :: Software Development :: Libraries :: Python Modules"",\n    ],\n)\n'"
doc/argparse2rst.py,0,"b'#!/usr/bin/env python3\nimport importlib.machinery as imm\nimport logging\nimport pathlib\nimport re\n\nimport configargparse\n\n\nclass ModuleInfo:\n    def __init__(self, path):\n        self.path = pathlib.Path(path)\n        name = str(self.path.parent / self.path.stem)\n        name = name.replace(""/"", ""."")\n        self.name = re.sub(r""^[\\.]+"", """", name)\n        self.module = imm.SourceFileLoader(self.name, path).load_module()\n        if not hasattr(self.module, ""get_parser""):\n            raise ValueError(f""{path} does not have get_parser()"")\n\n\ndef get_parser():\n    parser = configargparse.ArgumentParser(\n        description=\'generate RST from argparse options\',\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'src\', type=str, nargs=\'+\',\n                        help=\'source python files that contain get_parser() func\')\n    return parser\n\n\n# parser\nargs = get_parser().parse_args()\n\n\nmodinfo = []\n\nfor p in args.src:\n    if ""__init__.py"" in p:\n        continue\n    modinfo.append(ModuleInfo(p))\n\n\n# print refs\nfor m in modinfo:\n    logging.info(f""processing: {m.path.name}"")\n    d = m.module.get_parser().description\n    assert d is not None\n    print(f""- :ref:`{m.path.name}`: {d}"")\n\nprint()\n\n# print argparse\nfor m in modinfo:\n    cmd = m.path.name\n    sep = ""~"" * len(cmd)\n    print(f""""""\n\n.. _{cmd}:\n\n{cmd}\n{sep}\n\n.. argparse::\n   :module: {m.name}\n   :func: get_parser\n   :prog: {cmd}\n\n"""""")\n'"
doc/conf.py,0,"b'# -*- coding: utf-8 -*-\n# flake8: noqa\n#\n# ESPnet documentation build configuration file, created by\n# sphinx-quickstart on Thu Dec  7 15:46:00 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'../espnet/nets\'))\nsys.path.insert(0, os.path.abspath(\'../utils\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""nbsphinx"",\n    ""sphinx.ext.autodoc"",\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.todo"",\n    ""sphinxarg.ext"",\n    ""sphinx_markdown_tables"",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = \'.rst\'\nsource_suffix = [\'.rst\', \'.md\']\n\n# enable to markdown\nfrom recommonmark.parser import CommonMarkParser\n\nsource_parsers = {\n    \'.md\': CommonMarkParser,\n}\n\n# AutoStructify setting ref: https://qiita.com/pashango2/items/d1b379b699af85b529ce\nfrom recommonmark.transform import AutoStructify\n\ngithub_doc_root = \'https://github.com/rtfd/recommonmark/tree/master/doc/\'\n\n\ndef setup(app):\n    app.add_config_value(\'recommonmark_config\', {\n        \'url_resolver\': lambda url: github_doc_root + url,\n        \'auto_toc_tree_section\': \'Contents\',\n    }, True)\n    app.add_transform(AutoStructify)\n\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'ESPnet\'\ncopyright = u\'2017, Shinji Watanabe\'\nauthor = u\'Shinji Watanabe\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nimport espnet\nversion = espnet.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = espnet.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\n    \'_build\', \'Thumbs.db\', \'.DS_Store\', ""README.md"",\n    # NOTE: becuase these genearate files are directly included\n    # from the other files, we should exclude these files manually.\n    ""_gen/modules.rst"",\n    ""_gen/utils_sh.rst"",\n    ""_gen/utils_py.rst"",\n    ""_gen/espnet_bin.rst"",\n    ""_gen/espnet-bin.rst""\n]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n\n# html_theme = \'nature\'\nimport sphinx_rtd_theme\n\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    \'**\': [\n        \'relations.html\',  # needs \'show_related\': True theme option to display\n        \'searchbox.html\',\n    ]\n}\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'ESPnetdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'ESPnet.tex\', u\'ESPnet Documentation\',\n     u\'Shinji Watanabe\', \'manual\'),\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'espnet\', u\'ESPnet Documentation\',\n     [author], 1)\n]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'ESPnet\', u\'ESPnet Documentation\',\n     author, \'ESPnet\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\nautoclass_content = \'both\'\n\n# NOTE(kan-bayashi): Do not update outputs in notebook automatically.\nnbsphinx_execute = \'never\'\n'"
doc/module2rst.py,0,"b'#!/usr/bin/env python3\nfrom glob import glob\nimport importlib\nimport os\n\nimport configargparse\n\n\n# parser\nparser = configargparse.ArgumentParser(\n    description=\'generate RST files from <root> module recursively into <dst>/_gen\',\n    config_file_parser_class=configargparse.YAMLConfigFileParser,\n    formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--root\', nargs=\'+\',\n                    help=\'root module to generate docs recursively\')\nparser.add_argument(\'--dst\', type=str,\n                    help=\'destination path to generate RSTs\')\nparser.add_argument(\'--exclude\', nargs=\'*\', default=[],\n                    help=\'exclude module name\')\nargs = parser.parse_args()\nprint(args)\n\n\ndef to_module(path_name):\n    ret = path_name.replace("".py"", """").replace(""/"", ""."")\n    if ret.endswith("".""):\n        return ret[:-1]\n    return ret\n\n\ndef gen_rst(module_path, f):\n    name = to_module(module_path)\n    module = importlib.import_module(name)\n    title = name + "" package""\n    sep = ""="" * len(title)\n    doc = module.__doc__\n    if doc is None:\n        doc = """"\n    f.write(f""""""\n{title}\n{sep}\n{doc}\n\n"""""")\n\n    for cpath in glob(module_path + ""/**/*.py"", recursive=True):\n        print(cpath)\n        if not os.path.exists(cpath):\n            continue\n        if ""__pycache__"" in cpath:\n            continue\n        cname = to_module(cpath)\n        csep = ""-"" * len(cname)\n        f.write(f""""""\n.. _{cname}:\n\n{cname}\n{csep}\n\n.. automodule:: {cname}\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n"""""")\n    f.flush()\n\n\nmodules_rst = """"""\n.. toctree::\n   :maxdepth: 1\n   :caption: Package Reference:\n\n""""""\ngendir = args.dst + ""/_gen""\nos.makedirs(gendir, exist_ok=True)\nfor root in args.root:\n    for p in glob(root + ""/**"", recursive=False):\n        if p in args.exclude:\n            continue\n        if ""__pycache__"" in p:\n            continue\n        if ""__init__"" in p:\n            continue\n        fname = to_module(p) + "".rst""\n        dst = f""{gendir}/{fname}""\n        modules_rst += f""   ./_gen/{fname}\\n""\n        print(f""[INFO] generating {dst}"")\n        with open(dst, ""w"") as f:\n            gen_rst(p, f)\n\n\nwith open(gendir + ""/modules.rst"", ""w"") as f:\n    f.write(modules_rst)\n'"
espnet/__init__.py,0,"b'""""""Initialize main package.""""""\n\nimport pkg_resources\n\ntry:\n    __version__ = pkg_resources.get_distribution(""espnet"").version\nexcept Exception:\n    __version__ = ""(Not installed from setup.py)""\ndel pkg_resources\n'"
espnet2/__init__.py,0,b''
test/__init__.py,0,b''
test/test_asr_init.py,5,"b'# coding: utf-8\n\nimport argparse\nimport importlib\nimport json\nimport numpy as np\nimport os\nimport pytest\nimport tempfile\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\ndef make_arg(**kwargs):\n    train_defaults = dict(\n        elayers=1,\n        subsample=""1_2_2_1_1"",\n        etype=""vggblstm"",\n        eunits=16,\n        eprojs=8,\n        dtype=""lstm"",\n        dlayers=1,\n        dunits=16,\n        atype=""location"",\n        aheads=2,\n        awin=5,\n        aconv_chans=4,\n        aconv_filts=10,\n        mtlalpha=0.5,\n        lsm_type="""",\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        adim=16,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        nbest=5,\n        beam_size=2,\n        penalty=0.5,\n        maxlenratio=1.0,\n        minlenratio=0.0,\n        ctc_weight=0.2,\n        lm_weight=0.0,\n        rnnlm=None,\n        verbose=2,\n        char_list=[""a"", ""e"", ""i"", ""o"", ""u""],\n        outdir=None,\n        ctc_type=""warpctc"",\n        report_cer=False,\n        report_wer=False,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        replace_sos=False,\n        tgt_lang=False,\n        enc_init=None,\n        enc_init_mods=""enc."",\n        dec_init=None,\n        dec_init_mods=""dec.,att."",\n        model_module=""espnet.nets.pytorch_backend.e2e_asr:E2E"",\n    )\n    train_defaults.update(kwargs)\n\n    return argparse.Namespace(**train_defaults)\n\n\ndef get_default_scope_inputs():\n    idim = 40\n    odim = 5\n    ilens = [20, 15]\n    olens = [4, 3]\n\n    return idim, odim, ilens, olens\n\n\ndef pytorch_prepare_inputs(idim, odim, ilens, olens, is_cuda=False):\n    np.random.seed(1)\n\n    xs = [np.random.randn(ilen, idim).astype(np.float32) for ilen in ilens]\n    ys = [np.random.randint(1, odim, olen).astype(np.int32) for olen in olens]\n    ilens = np.array([x.shape[0] for x in xs], dtype=np.int32)\n\n    xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0)\n    ys_pad = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n    ilens = torch.from_numpy(ilens).long()\n\n    if is_cuda:\n        xs_pad = xs_pad.cuda()\n        ys_pad = ys_pad.cuda()\n        ilens = ilens.cuda()\n\n    return xs_pad, ilens, ys_pad\n\n\n@pytest.mark.parametrize(\n    ""enc_init, enc_mods, dec_init, dec_mods, mtlalpha"",\n    [\n        (None, ""enc."", None, ""dec., att."", 0.0),\n        (None, ""enc."", None, ""dec., att."", 0.5),\n        (None, ""enc."", None, ""dec., att."", 1.0),\n        (True, ""enc."", None, ""dec., att."", 0.5),\n        (None, ""enc."", True, ""dec., att."", 0.0),\n        (None, ""enc."", True, ""dec., att."", 0.5),\n        (None, ""enc."", True, ""dec., att."", 1.0),\n        (True, ""enc."", True, ""dec., att."", 0.0),\n        (True, ""enc."", True, ""dec., att."", 0.5),\n        (True, ""enc."", True, ""dec., att."", 1.0),\n        (True, ""test"", None, ""dec., att."", 0.0),\n        (True, ""test"", None, ""dec., att."", 0.5),\n        (True, ""test"", None, ""dec., att."", 1.0),\n        (None, ""enc."", True, ""test"", 0.0),\n        (None, ""enc."", True, ""test"", 0.5),\n        (None, ""enc."", True, ""test"", 1.0),\n        (True, ""enc.enc.0"", None, ""dec., att."", 0.0),\n        (True, ""enc.enc.0"", None, ""dec., att."", 0.5),\n        (True, ""enc.enc.0"", None, ""dec., att."", 1.0),\n        (None, ""enc."", True, ""dec.embed."", 0.0),\n        (None, ""enc."", True, ""dec.embed."", 0.5),\n        (None, ""enc."", True, ""dec.embed."", 1.0),\n        (True, ""enc.enc.0, enc.enc.1"", True, ""dec., att."", 0.0),\n        (True, ""enc.enc.0"", True, ""dec.embed.,dec.decoder.1"", 0.5),\n        (True, ""enc.enc.0, enc.enc.1"", True, ""dec.embed.,dec.decoder.1"", 1.0),\n    ],\n)\ndef test_pytorch_trainable_transferable_and_decodable(\n    enc_init, enc_mods, dec_init, dec_mods, mtlalpha\n):\n    idim, odim, ilens, olens = get_default_scope_inputs()\n    args = make_arg()\n\n    module = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr"")\n    model = module.E2E(idim, odim, args)\n\n    batch = pytorch_prepare_inputs(idim, odim, ilens, olens)\n\n    loss = model(*batch)\n    loss.backward()\n\n    with torch.no_grad():\n        in_data = np.random.randn(20, idim)\n        model.recognize(in_data, args, args.char_list)\n\n    if not os.path.exists("".pytest_cache""):\n        os.makedirs("".pytest_cache"")\n    utils = importlib.import_module(""espnet.asr.asr_utils"")\n\n    tmppath = tempfile.mktemp()\n    utils.torch_save(tmppath, model)\n\n    if enc_init is not None:\n        enc_init = tmppath\n    if dec_init is not None:\n        dec_init = tmppath\n\n    # create dummy model.json for saved model to go through\n    # get_model_conf(...) called in load_trained_modules method.\n    model_conf = os.path.dirname(tmppath) + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        f.write(\n            json.dumps(\n                (40, 5, vars(args)), indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n\n    args = make_arg(\n        enc_init=enc_init,\n        enc_init_mods=enc_mods,\n        dec_init=dec_init,\n        dec_init_mods=dec_mods,\n        mtlalpha=mtlalpha,\n    )\n    transfer = importlib.import_module(""espnet.asr.pytorch_backend.asr_init"")\n    model = transfer.load_trained_modules(40, 5, args)\n\n    loss = model(*batch)\n    loss.backward()\n\n    with torch.no_grad():\n        in_data = np.random.randn(20, idim)\n        model.recognize(in_data, args, args.char_list)\n'"
test/test_asr_interface.py,0,"b'import pytest\n\nfrom espnet.nets.asr_interface import dynamic_import_asr\n\n\n@pytest.mark.parametrize(\n    ""name, backend"",\n    [(nn, backend) for nn in (""transformer"", ""rnn"") for backend in (""pytorch"",)],\n)\ndef test_asr_build(name, backend):\n    model = dynamic_import_asr(name, backend).build(\n        10, 10, mtlalpha=0.123, adim=4, eunits=3, dunits=3, elayers=2, dlayers=2\n    )\n    assert model.mtlalpha == 0.123\n'"
test/test_batch_beam_search.py,16,"b'from argparse import Namespace\n\nimport numpy\nimport pytest\nimport torch\n\nfrom espnet.nets.batch_beam_search import BatchBeamSearch\nfrom espnet.nets.batch_beam_search import BeamSearch\nfrom espnet.nets.beam_search import Hypothesis\nfrom espnet.nets.lm_interface import dynamic_import_lm\nfrom espnet.nets.scorers.length_bonus import LengthBonus\n\nfrom test.test_beam_search import prepare\nfrom test.test_beam_search import transformer_args\n\n\ndef test_batchfy_hyp():\n    vocab_size = 5\n    eos = -1\n    # simplest beam search\n    beam = BatchBeamSearch(\n        beam_size=3,\n        vocab_size=vocab_size,\n        weights={""a"": 0.5, ""b"": 0.5},\n        scorers={""a"": LengthBonus(vocab_size), ""b"": LengthBonus(vocab_size)},\n        pre_beam_score_key=""a"",\n        sos=eos,\n        eos=eos,\n    )\n    hs = [\n        Hypothesis(\n            yseq=torch.tensor([0, 1, 2]),\n            score=torch.tensor(0.15),\n            scores={""a"": torch.tensor(0.1), ""b"": torch.tensor(0.2)},\n            states={""a"": 1, ""b"": 2},\n        ),\n        Hypothesis(\n            yseq=torch.tensor([0, 1]),\n            score=torch.tensor(0.1),\n            scores={""a"": torch.tensor(0.0), ""b"": torch.tensor(0.2)},\n            states={""a"": 3, ""b"": 4},\n        ),\n    ]\n    bs = beam.batchfy(hs)\n    assert torch.all(bs.yseq == torch.tensor([[0, 1, 2], [0, 1, eos]]))\n    assert torch.all(bs.score == torch.tensor([0.15, 0.1]))\n    assert torch.all(bs.scores[""a""] == torch.tensor([0.1, 0.0]))\n    assert torch.all(bs.scores[""b""] == torch.tensor([0.2, 0.2]))\n    assert bs.states[""a""] == [1, 3]\n    assert bs.states[""b""] == [2, 4]\n\n    us = beam.unbatchfy(bs)\n    for i in range(len(hs)):\n        assert us[i].yseq.tolist() == hs[i].yseq.tolist()\n        assert us[i].score == hs[i].score\n        assert us[i].scores == hs[i].scores\n        assert us[i].states == hs[i].states\n\n\nlstm_lm = Namespace(type=""lstm"", layer=1, unit=2, dropout_rate=0.0)\ngru_lm = Namespace(type=""gru"", layer=1, unit=2, dropout_rate=0.0)\ntransformer_lm = Namespace(\n    layer=1, unit=2, att_unit=2, embed_unit=2, head=1, pos_enc=""none"", dropout_rate=0.0\n)\n\n\n@pytest.mark.parametrize(\n    ""model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, bonus, device, dtype"",\n    [\n        (nn, args, ctc, lm_nn, lm_args, lm, bonus, device, dtype)\n        for device in (""cpu"", ""cuda"")\n        # ((""rnn"", rnn_args),)\n        for nn, args in ((""transformer"", transformer_args),)\n        for ctc in (0.0,)  # 0.5, 1.0)\n        for lm_nn, lm_args in (\n            (""default"", lstm_lm),\n            (""default"", gru_lm),\n            (""transformer"", transformer_lm),\n        )\n        for lm in (0.0, 0.5)\n        for bonus in (0.0, 0.1)\n        for dtype in (""float32"", ""float64"")  # TODO(karita): float16\n    ],\n)\ndef test_batch_beam_search_equal(\n    model_class, args, ctc_weight, lm_nn, lm_args, lm_weight, bonus, device, dtype\n):\n    if device == ""cuda"" and not torch.cuda.is_available():\n        pytest.skip(""no cuda device is available"")\n    if device == ""cpu"" and dtype == ""float16"":\n        pytest.skip(""cpu float16 implementation is not available in pytorch yet"")\n\n    # seed setting\n    torch.manual_seed(123)\n    torch.backends.cudnn.deterministic = True\n    # https://github.com/pytorch/pytorch/issues/6351\n    torch.backends.cudnn.benchmark = False\n\n    dtype = getattr(torch, dtype)\n    model, x, ilens, y, data, train_args = prepare(\n        model_class, args, mtlalpha=ctc_weight\n    )\n    model.eval()\n    char_list = train_args.char_list\n    lm = dynamic_import_lm(lm_nn, backend=""pytorch"")(len(char_list), lm_args)\n    lm.eval()\n\n    # test previous beam search\n    args = Namespace(\n        beam_size=3,\n        penalty=bonus,\n        ctc_weight=ctc_weight,\n        maxlenratio=0,\n        lm_weight=lm_weight,\n        minlenratio=0,\n        nbest=5,\n    )\n\n    # new beam search\n    scorers = model.scorers()\n    if lm_weight != 0:\n        scorers[""lm""] = lm\n    scorers[""length_bonus""] = LengthBonus(len(char_list))\n    weights = dict(\n        decoder=1.0 - ctc_weight,\n        ctc=ctc_weight,\n        lm=args.lm_weight,\n        length_bonus=args.penalty,\n    )\n    model.to(device, dtype=dtype)\n    model.eval()\n    with torch.no_grad():\n        enc = model.encode(x[0, : ilens[0]].to(device, dtype=dtype))\n\n    legacy_beam = BeamSearch(\n        beam_size=args.beam_size,\n        vocab_size=len(char_list),\n        weights=weights,\n        scorers=scorers,\n        token_list=train_args.char_list,\n        sos=model.sos,\n        eos=model.eos,\n        pre_beam_score_key=None if ctc_weight == 1.0 else ""decoder"",\n    )\n    legacy_beam.to(device, dtype=dtype)\n    legacy_beam.eval()\n\n    beam = BatchBeamSearch(\n        beam_size=args.beam_size,\n        vocab_size=len(char_list),\n        weights=weights,\n        scorers=scorers,\n        token_list=train_args.char_list,\n        sos=model.sos,\n        eos=model.eos,\n    )\n    beam.to(device, dtype=dtype)\n    beam.eval()\n    with torch.no_grad():\n        legacy_nbest_bs = legacy_beam(\n            x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio\n        )\n        nbest_bs = beam(\n            x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio\n        )\n\n    for i, (expected, actual) in enumerate(zip(legacy_nbest_bs, nbest_bs)):\n        assert expected.yseq.tolist() == actual.yseq.tolist()\n        numpy.testing.assert_allclose(\n            expected.score.cpu(), actual.score.cpu(), rtol=1e-6\n        )\n'"
test/test_beam_search.py,11,"b'from argparse import Namespace\n\nimport numpy\nimport pytest\nimport torch\n\nfrom espnet.nets.asr_interface import dynamic_import_asr\nfrom espnet.nets.beam_search import BeamSearch\nfrom espnet.nets.lm_interface import dynamic_import_lm\nfrom espnet.nets.scorers.length_bonus import LengthBonus\n\nrnn_args = Namespace(\n    elayers=1,\n    subsample=""1_2_2_1_1"",\n    etype=""vggblstm"",\n    eunits=16,\n    eprojs=8,\n    dtype=""lstm"",\n    dlayers=1,\n    dunits=16,\n    atype=""location"",\n    aheads=2,\n    awin=5,\n    aconv_chans=4,\n    aconv_filts=10,\n    lsm_type="""",\n    lsm_weight=0.0,\n    sampling_probability=0.0,\n    adim=16,\n    dropout_rate=0.0,\n    dropout_rate_decoder=0.0,\n    nbest=5,\n    beam_size=2,\n    penalty=0.5,\n    maxlenratio=1.0,\n    minlenratio=0.0,\n    ctc_weight=0.2,\n    lm_weight=0.0,\n    rnnlm=None,\n    streaming_min_blank_dur=10,\n    streaming_onset_margin=2,\n    streaming_offset_margin=2,\n    verbose=2,\n    outdir=None,\n    ctc_type=""warpctc"",\n    report_cer=False,\n    report_wer=False,\n    sym_space=""<space>"",\n    sym_blank=""<blank>"",\n    sortagrad=0,\n    grad_noise=False,\n    context_residual=False,\n    use_frontend=False,\n    replace_sos=False,\n    tgt_lang=False,\n)\n\ntransformer_args = Namespace(\n    adim=16,\n    aheads=2,\n    dropout_rate=0.0,\n    transformer_attn_dropout_rate=None,\n    elayers=2,\n    eunits=16,\n    dlayers=2,\n    dunits=16,\n    sym_space=""<space>"",\n    sym_blank=""<blank>"",\n    transformer_init=""pytorch"",\n    transformer_input_layer=""conv2d"",\n    transformer_length_normalized_loss=True,\n    report_cer=False,\n    report_wer=False,\n    ctc_type=""warpctc"",\n    lsm_weight=0.001,\n)\n\n\n# from test.test_e2e_asr_transformer import prepare\ndef prepare(E2E, args, mtlalpha=0.0):\n    args.mtlalpha = mtlalpha\n    args.char_list = [""a"", ""e"", ""i"", ""o"", ""u""]\n    idim = 40\n    odim = 5\n    model = dynamic_import_asr(E2E, ""pytorch"")(idim, odim, args)\n    batchsize = 5\n    x = torch.randn(batchsize, 40, idim)\n    ilens = [40, 30, 20, 15, 10]\n    n_token = odim - 1\n    # avoid 0 for eps in ctc\n    y = (torch.rand(batchsize, 10) * n_token % (n_token - 1)).long() + 1\n    olens = [3, 9, 10, 2, 3]\n    for i in range(batchsize):\n        x[i, ilens[i] :] = -1\n        y[i, olens[i] :] = -1\n\n    data = []\n    for i in range(batchsize):\n        data.append(\n            (\n                ""utt%d"" % i,\n                {\n                    ""input"": [{""shape"": [ilens[i], idim]}],\n                    ""output"": [{""shape"": [olens[i]]}],\n                },\n            )\n        )\n    return model, x, torch.tensor(ilens), y, data, args\n\n\n@pytest.mark.parametrize(\n    ""model_class, args, ctc_weight, lm_weight, bonus, device, dtype"",\n    [\n        (nn, args, ctc, lm, bonus, device, dtype)\n        for device in (""cpu"", ""cuda"")\n        for nn, args in ((""transformer"", transformer_args), (""rnn"", rnn_args))\n        for ctc in (0.0, 0.5, 1.0)\n        for lm in (0.0, 0.5)\n        for bonus in (0.0, 0.1)\n        for dtype in (""float16"", ""float32"", ""float64"")\n    ],\n)\ndef test_beam_search_equal(\n    model_class, args, ctc_weight, lm_weight, bonus, device, dtype\n):\n    if device == ""cuda"" and not torch.cuda.is_available():\n        pytest.skip(""no cuda device is available"")\n    if device == ""cpu"" and dtype == ""float16"":\n        pytest.skip(""cpu float16 implementation is not available in pytorch yet"")\n\n    # seed setting\n    torch.manual_seed(123)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = (\n        False  # https://github.com/pytorch/pytorch/issues/6351\n    )\n\n    dtype = getattr(torch, dtype)\n    model, x, ilens, y, data, train_args = prepare(\n        model_class, args, mtlalpha=ctc_weight\n    )\n    model.eval()\n    char_list = train_args.char_list\n    lm_args = Namespace(type=""lstm"", layer=1, unit=2, embed_unit=2, dropout_rate=0.0)\n    lm = dynamic_import_lm(""default"", backend=""pytorch"")(len(char_list), lm_args)\n    lm.eval()\n\n    # test previous beam search\n    args = Namespace(\n        beam_size=3,\n        penalty=bonus,\n        ctc_weight=ctc_weight,\n        maxlenratio=0,\n        lm_weight=lm_weight,\n        minlenratio=0,\n        nbest=5,\n    )\n\n    feat = x[0, : ilens[0]].numpy()\n    # legacy beam search\n    with torch.no_grad():\n        nbest = model.recognize(feat, args, char_list, lm.model)\n\n    # new beam search\n    scorers = model.scorers()\n    if lm_weight != 0:\n        scorers[""lm""] = lm\n    scorers[""length_bonus""] = LengthBonus(len(char_list))\n    weights = dict(\n        decoder=1.0 - ctc_weight,\n        ctc=ctc_weight,\n        lm=args.lm_weight,\n        length_bonus=args.penalty,\n    )\n    model.to(device, dtype=dtype)\n    model.eval()\n    beam = BeamSearch(\n        beam_size=args.beam_size,\n        vocab_size=len(char_list),\n        weights=weights,\n        scorers=scorers,\n        token_list=train_args.char_list,\n        sos=model.sos,\n        eos=model.eos,\n        pre_beam_score_key=None if ctc_weight == 1.0 else ""decoder"",\n    )\n    beam.to(device, dtype=dtype)\n    beam.eval()\n    with torch.no_grad():\n        enc = model.encode(torch.as_tensor(feat).to(device, dtype=dtype))\n        nbest_bs = beam(\n            x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio\n        )\n    if dtype == torch.float16:\n        # skip because results are different. just checking it is decodable\n        return\n\n    for i, (expected, actual) in enumerate(zip(nbest, nbest_bs)):\n        actual = actual.asdict()\n        assert expected[""yseq""] == actual[""yseq""]\n        numpy.testing.assert_allclose(expected[""score""], actual[""score""], rtol=1e-6)\n'"
test/test_cli.py,0,"b'import numpy as np\nimport pytest\n\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import assert_scipy_wav_style\nfrom espnet.utils.cli_writers import file_writer_helper\n\n\n@pytest.mark.parametrize(""filetype"", [""mat"", ""hdf5"", ""sound.hdf5"", ""sound""])\ndef test_KaldiReader(tmpdir, filetype):\n    ark = str(tmpdir.join(""a.foo""))\n    scp = str(tmpdir.join(""a.scp""))\n    fs = 16000\n\n    with file_writer_helper(\n        wspecifier=f""ark,scp:{ark},{scp}"",\n        filetype=filetype,\n        write_num_frames=""ark,t:out.txt"",\n        compress=False,\n        compression_method=2,\n        pcm_format=""wav"",\n    ) as writer:\n\n        if ""sound"" in filetype:\n            aaa = np.random.randint(-10, 10, 100, dtype=np.int16)\n            bbb = np.random.randint(-10, 10, 50, dtype=np.int16)\n        else:\n            aaa = np.random.randn(10, 10)\n            bbb = np.random.randn(13, 5)\n        if ""sound"" in filetype:\n            writer[""aaa""] = fs, aaa\n            writer[""bbb""] = fs, bbb\n        else:\n            writer[""aaa""] = aaa\n            writer[""bbb""] = bbb\n        valid = {""aaa"": aaa, ""bbb"": bbb}\n\n    # 1. Test ark read\n    if filetype != ""sound"":\n        for key, value in file_reader_helper(\n            f""ark:{ark}"", filetype=filetype, return_shape=False\n        ):\n            if ""sound"" in filetype:\n                assert_scipy_wav_style(value)\n                value = value[1]\n            np.testing.assert_array_equal(value, valid[key])\n    # 2. Test scp read\n    for key, value in file_reader_helper(\n        f""scp:{scp}"", filetype=filetype, return_shape=False\n    ):\n        if ""sound"" in filetype:\n            assert_scipy_wav_style(value)\n            value = value[1]\n        np.testing.assert_array_equal(value, valid[key])\n\n    # 3. Test ark shape read\n    if filetype != ""sound"":\n        for key, value in file_reader_helper(\n            f""ark:{ark}"", filetype=filetype, return_shape=True\n        ):\n            if ""sound"" in filetype:\n                value = value[1]\n            np.testing.assert_array_equal(value, valid[key].shape)\n    # 4. Test scp shape read\n    for key, value in file_reader_helper(\n        f""scp:{scp}"", filetype=filetype, return_shape=True\n    ):\n        if ""sound"" in filetype:\n            value = value[1]\n        np.testing.assert_array_equal(value, valid[key].shape)\n'"
test/test_e2e_asr.py,13,"b'# coding: utf-8\n\n# Copyright 2017 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport argparse\nimport importlib\nimport os\nimport tempfile\n\nimport chainer\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.utils.training.batchfy import make_batchset\nfrom test.utils_test import make_dummy_json\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        elayers=1,\n        subsample=""1_2_2_1_1"",\n        etype=""vggblstm"",\n        eunits=16,\n        eprojs=8,\n        dtype=""lstm"",\n        dlayers=1,\n        dunits=16,\n        atype=""location"",\n        aheads=2,\n        awin=5,\n        aconv_chans=4,\n        aconv_filts=10,\n        mtlalpha=0.5,\n        lsm_type="""",\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        adim=16,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        nbest=5,\n        beam_size=2,\n        penalty=0.5,\n        maxlenratio=1.0,\n        minlenratio=0.0,\n        ctc_weight=0.2,\n        ctc_window_margin=0,\n        lm_weight=0.0,\n        rnnlm=None,\n        streaming_min_blank_dur=10,\n        streaming_onset_margin=2,\n        streaming_offset_margin=2,\n        verbose=2,\n        char_list=[u""\xe3\x81\x82"", u""\xe3\x81\x84"", u""\xe3\x81\x86"", u""\xe3\x81\x88"", u""\xe3\x81\x8a""],\n        outdir=None,\n        ctc_type=""warpctc"",\n        report_cer=False,\n        report_wer=False,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        sortagrad=0,\n        grad_noise=False,\n        context_residual=False,\n        use_frontend=False,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef prepare_inputs(mode, ilens=[20, 15], olens=[4, 3], is_cuda=False):\n    np.random.seed(1)\n    assert len(ilens) == len(olens)\n    xs = [np.random.randn(ilen, 40).astype(np.float32) for ilen in ilens]\n    ys = [np.random.randint(1, 5, olen).astype(np.int32) for olen in olens]\n    ilens = np.array([x.shape[0] for x in xs], dtype=np.int32)\n\n    if mode == ""chainer"":\n        if is_cuda:\n            xp = importlib.import_module(""cupy"")\n            xs = [chainer.Variable(xp.array(x)) for x in xs]\n            ys = [chainer.Variable(xp.array(y)) for y in ys]\n            ilens = xp.array(ilens)\n        else:\n            xs = [chainer.Variable(x) for x in xs]\n            ys = [chainer.Variable(y) for y in ys]\n        return xs, ilens, ys\n\n    elif mode == ""pytorch"":\n        ilens = torch.from_numpy(ilens).long()\n        xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0)\n        ys_pad = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n        if is_cuda:\n            xs_pad = xs_pad.cuda()\n            ilens = ilens.cuda()\n            ys_pad = ys_pad.cuda()\n\n        return xs_pad, ilens, ys_pad\n    else:\n        raise ValueError(""Invalid mode"")\n\n\ndef convert_batch(batch, backend=""pytorch"", is_cuda=False, idim=40, odim=5):\n    ilens = np.array([x[1][""input""][0][""shape""][0] for x in batch])\n    olens = np.array([x[1][""output""][0][""shape""][0] for x in batch])\n    xs = [np.random.randn(ilen, idim).astype(np.float32) for ilen in ilens]\n    ys = [np.random.randint(1, odim, olen).astype(np.int32) for olen in olens]\n    is_pytorch = backend == ""pytorch""\n    if is_pytorch:\n        xs = pad_list([torch.from_numpy(x).float() for x in xs], 0)\n        ilens = torch.from_numpy(ilens).long()\n        ys = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n\n        if is_cuda:\n            xs = xs.cuda()\n            ilens = ilens.cuda()\n            ys = ys.cuda()\n    else:\n        if is_cuda:\n            xp = importlib.import_module(""cupy"")\n            xs = [chainer.Variable(xp.array(x)) for x in xs]\n            ys = [chainer.Variable(xp.array(y)) for y in ys]\n            ilens = xp.array(ilens)\n        else:\n            xs = [chainer.Variable(x) for x in xs]\n            ys = [chainer.Variable(y) for y in ys]\n\n    return xs, ilens, ys\n\n\n@pytest.mark.parametrize(\n    ""module, model_dict"",\n    [\n        (""espnet.nets.chainer_backend.e2e_asr"", {}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""elayers"": 2, ""dlayers"": 2}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vggblstmp""}),\n        (\n            ""espnet.nets.chainer_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""noatt""},\n        ),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vggblstmp"", ""atype"": ""dot""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""grup""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""lstmp""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""bgrup""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""blstmp""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""bgru""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""blstm""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vgggru""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vggbgrup""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vgglstm""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vgglstmp""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vggbgru""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vggbgrup""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""etype"": ""vggblstmp"", ""dtype"": ""gru""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""mtlalpha"": 0.0}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""mtlalpha"": 1.0}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""sampling_probability"": 0.5}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""ctc_type"": ""builtin""}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""ctc_weight"": 0.0}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""ctc_weight"": 1.0}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""report_cer"": True}),\n        (""espnet.nets.chainer_backend.e2e_asr"", {""report_wer"": True}),\n        (\n            ""espnet.nets.chainer_backend.e2e_asr"",\n            {""report_cer"": True, ""report_wer"": True},\n        ),\n        (\n            ""espnet.nets.chainer_backend.e2e_asr"",\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 0.0},\n        ),\n        (\n            ""espnet.nets.chainer_backend.e2e_asr"",\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 1.0},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""elayers"": 2, ""dlayers"": 2}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""grup""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""lstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""bgrup""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""blstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""bgru""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""blstm""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vgggru""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vgggrup""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vgglstm""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vgglstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vggbgru""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vggbgrup""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vggblstmp"", ""dtype"": ""gru""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""noatt""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vggblstmp"", ""atype"": ""add""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""etype"": ""vggblstmp"", ""atype"": ""dot""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""coverage""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""coverage_location""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""location2d""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""location_recurrent""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_dot""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_add""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_loc""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_multi_res_loc""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""mtlalpha"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""mtlalpha"": 1.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""sampling_probability"": 0.5}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""ctc_type"": ""builtin""}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""ctc_weight"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""ctc_weight"": 1.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""context_residual"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""grad_noise"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""report_cer"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr"", {""report_wer"": True}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""report_cer"": True, ""report_wer"": True},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 0.0},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 1.0},\n        ),\n    ],\n)\ndef test_model_trainable_and_decodable(module, model_dict):\n    args = make_arg(**model_dict)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"")\n    else:\n        batch = prepare_inputs(""chainer"")\n\n    m = importlib.import_module(module)\n    model = m.E2E(40, 5, args)\n    loss = model(*batch)\n    if isinstance(loss, tuple):\n        # chainer return several values as tuple\n        loss[0].backward()  # trainable\n    else:\n        loss.backward()  # trainable\n\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(10, 40)\n        model.recognize(in_data, args, args.char_list)  # decodable\n        if ""pytorch"" in module:\n            batch_in_data = [np.random.randn(10, 40), np.random.randn(5, 40)]\n            model.recognize_batch(\n                batch_in_data, args, args.char_list\n            )  # batch decodable\n\n\ndef test_window_streaming_e2e_encoder_and_ctc_with_offline_attention():\n    m = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr"")\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    n = importlib.import_module(""espnet.nets.pytorch_backend.streaming.window"")\n    asr = n.WindowStreamingE2E(model, args)\n\n    in_data = np.random.randn(100, 40)\n    for i in range(10):\n        asr.accept_input(in_data)\n\n    asr.decode_with_attention_offline()\n\n\ndef test_segment_streaming_e2e():\n    m = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr"")\n    args = make_arg()\n    args.etype = ""vgglstm""  # uni-directional\n    args.batchsize = 0\n    model = m.E2E(40, 5, args)\n    n = importlib.import_module(""espnet.nets.pytorch_backend.streaming.segment"")\n    asr = n.SegmentStreamingE2E(model, args)\n\n    in_data = np.random.randn(100, 40)\n    r = np.prod(model.subsample)\n    for i in range(0, 100, r):\n        asr.accept_input(in_data[i : i + r])\n\n    args.batchsize = 1\n    for i in range(0, 100, r):\n        asr.accept_input(in_data[i : i + r])\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_gradient_noise_injection(module):\n    args = make_arg(grad_noise=True)\n    args_org = make_arg()\n    dummy_json = make_dummy_json(2, [10, 20], [10, 20], idim=20, odim=5)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_asr as m\n    else:\n        import espnet.nets.chainer_backend.e2e_asr as m\n    batchset = make_batchset(dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True)\n    model = m.E2E(20, 5, args)\n    model_org = m.E2E(20, 5, args_org)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        loss_org = model_org(*convert_batch(batch, module, idim=20, odim=5))\n        loss.backward()\n        grad = [param.grad for param in model.parameters()][10]\n        loss_org.backward()\n        grad_org = [param.grad for param in model_org.parameters()][10]\n        assert grad[0] != grad_org[0]\n\n\n@pytest.mark.parametrize(""module"", [""pytorch"", ""chainer""])\ndef test_sortagrad_trainable(module):\n    args = make_arg(sortagrad=1)\n    dummy_json = make_dummy_json(4, [10, 20], [10, 20], idim=20, odim=5)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_asr as m\n    else:\n        import espnet.nets.chainer_backend.e2e_asr as m\n    batchset = make_batchset(dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True)\n    model = m.E2E(20, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(50, 20)\n        model.recognize(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch"", ""chainer""])\ndef test_sortagrad_trainable_with_batch_bins(module):\n    args = make_arg(sortagrad=1)\n    idim = 20\n    odim = 5\n    dummy_json = make_dummy_json(4, [10, 20], [10, 20], idim=idim, odim=odim)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_asr as m\n    else:\n        import espnet.nets.chainer_backend.e2e_asr as m\n    batch_elems = 2000\n    batchset = make_batchset(dummy_json, batch_bins=batch_elems, shortest_first=True)\n    for batch in batchset:\n        n = 0\n        for uttid, info in batch:\n            ilen = int(info[""input""][0][""shape""][0])\n            olen = int(info[""output""][0][""shape""][0])\n            n += ilen * idim + olen * odim\n        assert olen < batch_elems\n\n    model = m.E2E(20, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(100, 20)\n        model.recognize(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch"", ""chainer""])\ndef test_sortagrad_trainable_with_batch_frames(module):\n    args = make_arg(sortagrad=1)\n    idim = 20\n    odim = 5\n    dummy_json = make_dummy_json(4, [10, 20], [10, 20], idim=idim, odim=odim)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_asr as m\n    else:\n        import espnet.nets.chainer_backend.e2e_asr as m\n    batch_frames_in = 50\n    batch_frames_out = 50\n    batchset = make_batchset(\n        dummy_json,\n        batch_frames_in=batch_frames_in,\n        batch_frames_out=batch_frames_out,\n        shortest_first=True,\n    )\n    for batch in batchset:\n        i = 0\n        o = 0\n        for uttid, info in batch:\n            i += int(info[""input""][0][""shape""][0])\n            o += int(info[""output""][0][""shape""][0])\n        assert i <= batch_frames_in\n        assert o <= batch_frames_out\n\n    model = m.E2E(20, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(100, 20)\n        model.recognize(in_data, args, args.char_list)\n\n\ndef init_torch_weight_const(m, val):\n    for p in m.parameters():\n        if p.dim() > 1:\n            p.data.fill_(val)\n\n\ndef init_chainer_weight_const(m, val):\n    for p in m.params():\n        if p.data.ndim > 1:\n            p.data[:] = val\n\n\ndef test_chainer_ctc_type():\n    ch = importlib.import_module(""espnet.nets.chainer_backend.e2e_asr"")\n    np.random.seed(0)\n    batch = prepare_inputs(""chainer"")\n\n    def _propagate(ctc_type):\n        args = make_arg(ctc_type=ctc_type)\n        np.random.seed(0)\n        model = ch.E2E(40, 5, args)\n        _, ch_ctc, _, _ = model(*batch)\n        ch_ctc.backward()\n        W_grad = model.ctc.ctc_lo.W.grad\n        b_grad = model.ctc.ctc_lo.b.grad\n        return ch_ctc.data, W_grad, b_grad\n\n    ref_loss, ref_W_grad, ref_b_grad = _propagate(""builtin"")\n    loss, W_grad, b_grad = _propagate(""warpctc"")\n    np.testing.assert_allclose(ref_loss, loss, rtol=1e-5)\n    np.testing.assert_allclose(ref_W_grad, W_grad)\n    np.testing.assert_allclose(ref_b_grad, b_grad)\n\n\n@pytest.mark.parametrize(""etype"", [""blstmp"", ""vggblstmp""])\ndef test_loss_and_ctc_grad(etype):\n    ch = importlib.import_module(""espnet.nets.chainer_backend.e2e_asr"")\n    th = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr"")\n    args = make_arg(etype=etype)\n    ch_model = ch.E2E(40, 5, args)\n    ch_model.cleargrads()\n    th_model = th.E2E(40, 5, args)\n\n    const = 1e-4\n    init_torch_weight_const(th_model, const)\n    init_chainer_weight_const(ch_model, const)\n\n    ch_batch = prepare_inputs(""chainer"")\n    th_batch = prepare_inputs(""pytorch"")\n\n    _, ch_ctc, ch_att, ch_acc = ch_model(*ch_batch)\n    th_model(*th_batch)\n    th_ctc, th_att = th_model.loss_ctc, th_model.loss_att\n\n    # test masking\n    ch_ench = ch_model.att.pre_compute_enc_h.data\n    th_ench = th_model.att[0].pre_compute_enc_h.detach().numpy()\n    np.testing.assert_equal(ch_ench == 0.0, th_ench == 0.0)\n\n    # test loss with constant weights (1.0) and bias (0.0) except for foget-bias (1.0)\n    np.testing.assert_allclose(ch_ctc.data, th_ctc.detach().numpy())\n    np.testing.assert_allclose(ch_att.data, th_att.detach().numpy())\n\n    # test ctc grads\n    ch_ctc.backward()\n    th_ctc.backward()\n    np.testing.assert_allclose(\n        ch_model.ctc.ctc_lo.W.grad,\n        th_model.ctc.ctc_lo.weight.grad.data.numpy(),\n        1e-7,\n        1e-8,\n    )\n    np.testing.assert_allclose(\n        ch_model.ctc.ctc_lo.b.grad,\n        th_model.ctc.ctc_lo.bias.grad.data.numpy(),\n        1e-5,\n        1e-6,\n    )\n\n    # test cross-entropy grads\n    ch_model.cleargrads()\n    th_model.zero_grad()\n\n    _, ch_ctc, ch_att, ch_acc = ch_model(*ch_batch)\n    th_model(*th_batch)\n    th_ctc, th_att = th_model.loss_ctc, th_model.loss_att\n    ch_att.backward()\n    th_att.backward()\n    np.testing.assert_allclose(\n        ch_model.dec.output.W.grad,\n        th_model.dec.output.weight.grad.data.numpy(),\n        1e-7,\n        1e-8,\n    )\n    np.testing.assert_allclose(\n        ch_model.dec.output.b.grad,\n        th_model.dec.output.bias.grad.data.numpy(),\n        1e-5,\n        1e-6,\n    )\n\n\n@pytest.mark.parametrize(""etype"", [""blstmp"", ""vggblstmp""])\ndef test_mtl_loss(etype):\n    ch = importlib.import_module(""espnet.nets.chainer_backend.e2e_asr"")\n    th = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr"")\n    args = make_arg(etype=etype)\n    ch_model = ch.E2E(40, 5, args)\n    th_model = th.E2E(40, 5, args)\n\n    const = 1e-4\n    init_torch_weight_const(th_model, const)\n    init_chainer_weight_const(ch_model, const)\n\n    ch_batch = prepare_inputs(""chainer"")\n    th_batch = prepare_inputs(""pytorch"")\n\n    _, ch_ctc, ch_att, ch_acc = ch_model(*ch_batch)\n    th_model(*th_batch)\n    th_ctc, th_att = th_model.loss_ctc, th_model.loss_att\n\n    # test masking\n    ch_ench = ch_model.att.pre_compute_enc_h.data\n    th_ench = th_model.att[0].pre_compute_enc_h.detach().numpy()\n    np.testing.assert_equal(ch_ench == 0.0, th_ench == 0.0)\n\n    # test loss with constant weights (1.0) and bias (0.0) except for foget-bias (1.0)\n    np.testing.assert_allclose(ch_ctc.data, th_ctc.detach().numpy())\n    np.testing.assert_allclose(ch_att.data, th_att.detach().numpy())\n\n    # test grads in mtl mode\n    ch_loss = ch_ctc * 0.5 + ch_att * 0.5\n    th_loss = th_ctc * 0.5 + th_att * 0.5\n    ch_model.cleargrads()\n    th_model.zero_grad()\n    ch_loss.backward()\n    th_loss.backward()\n    np.testing.assert_allclose(\n        ch_model.ctc.ctc_lo.W.grad,\n        th_model.ctc.ctc_lo.weight.grad.data.numpy(),\n        1e-7,\n        1e-8,\n    )\n    np.testing.assert_allclose(\n        ch_model.ctc.ctc_lo.b.grad,\n        th_model.ctc.ctc_lo.bias.grad.data.numpy(),\n        1e-5,\n        1e-6,\n    )\n    np.testing.assert_allclose(\n        ch_model.dec.output.W.grad,\n        th_model.dec.output.weight.grad.data.numpy(),\n        1e-7,\n        1e-8,\n    )\n    np.testing.assert_allclose(\n        ch_model.dec.output.b.grad,\n        th_model.dec.output.bias.grad.data.numpy(),\n        1e-5,\n        1e-6,\n    )\n\n\n@pytest.mark.parametrize(""etype"", [""blstmp"", ""vggblstmp""])\ndef test_zero_length_target(etype):\n    ch = importlib.import_module(""espnet.nets.chainer_backend.e2e_asr"")\n    th = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr"")\n    args = make_arg(etype=etype)\n    ch_model = ch.E2E(40, 5, args)\n    ch_model.cleargrads()\n    th_model = th.E2E(40, 5, args)\n\n    ch_batch = prepare_inputs(""chainer"", olens=[4, 0])\n    th_batch = prepare_inputs(""pytorch"", olens=[4, 0])\n\n    ch_model(*ch_batch)\n    th_model(*th_batch)\n\n    # NOTE: We ignore all zero length case because chainer also fails.\n    # Have a nice data-prep!\n    # out_data = """"\n    # data = [\n    #     (""aaa"", dict(feat=np.random.randn(200, 40).astype(np.float32), tokenid="""")),\n    #     (""bbb"", dict(feat=np.random.randn(100, 40).astype(np.float32), tokenid="""")),\n    #     (""cc"", dict(feat=np.random.randn(100, 40).astype(np.float32), tokenid=""""))\n    # ]\n    # ch_ctc, ch_att, ch_acc = ch_model(data)\n    # th_ctc, th_att, th_acc = th_model(data)\n\n\n@pytest.mark.parametrize(\n    ""module, atype"",\n    [\n        (""espnet.nets.chainer_backend.e2e_asr"", ""noatt""),\n        (""espnet.nets.chainer_backend.e2e_asr"", ""dot""),\n        (""espnet.nets.chainer_backend.e2e_asr"", ""location""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""noatt""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""dot""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""add""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""location""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""coverage""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""coverage_location""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""location2d""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""location_recurrent""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""multi_head_dot""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""multi_head_add""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""multi_head_loc""),\n        (""espnet.nets.pytorch_backend.e2e_asr"", ""multi_head_multi_res_loc""),\n    ],\n)\ndef test_calculate_all_attentions(module, atype):\n    m = importlib.import_module(module)\n    args = make_arg(atype=atype)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"")\n    else:\n        batch = prepare_inputs(""chainer"")\n    model = m.E2E(40, 5, args)\n    with chainer.no_backprop_mode():\n        if ""pytorch"" in module:\n            att_ws = model.calculate_all_attentions(*batch)[0]\n        else:\n            att_ws = model.calculate_all_attentions(*batch)\n        print(att_ws.shape)\n\n\ndef test_chainer_save_and_load():\n    m = importlib.import_module(""espnet.nets.chainer_backend.e2e_asr"")\n    utils = importlib.import_module(""espnet.asr.asr_utils"")\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    # initialize randomly\n    for p in model.params():\n        p.data = np.random.randn(*p.data.shape)\n    tmppath = tempfile.mktemp()\n    chainer.serializers.save_npz(tmppath, model)\n    p_saved = [p.data for p in model.params()]\n    # set constant value\n    for p in model.params():\n        p.data = np.zeros_like(p.data)\n    utils.chainer_load(tmppath, model)\n    for p1, p2 in zip(p_saved, model.params()):\n        np.testing.assert_array_equal(p1, p2.data)\n    if os.path.exists(tmppath):\n        os.remove(tmppath)\n\n\ndef test_torch_save_and_load():\n    m = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr"")\n    utils = importlib.import_module(""espnet.asr.asr_utils"")\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    # initialize randomly\n    for p in model.parameters():\n        p.data.uniform_()\n    if not os.path.exists("".pytest_cache""):\n        os.makedirs("".pytest_cache"")\n    tmppath = tempfile.mktemp()\n    utils.torch_save(tmppath, model)\n    p_saved = [p.data.numpy() for p in model.parameters()]\n    # set constant value\n    for p in model.parameters():\n        p.data.zero_()\n    utils.torch_load(tmppath, model)\n    for p1, p2 in zip(p_saved, model.parameters()):\n        np.testing.assert_array_equal(p1, p2.data.numpy())\n    if os.path.exists(tmppath):\n        os.remove(tmppath)\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available() and not chainer.cuda.available, reason=""gpu required""\n)\n@pytest.mark.parametrize(\n    ""module"",\n    [""espnet.nets.chainer_backend.e2e_asr"", ""espnet.nets.pytorch_backend.e2e_asr""],\n)\ndef test_gpu_trainable(module):\n    m = importlib.import_module(module)\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"", is_cuda=True)\n        model.cuda()\n    else:\n        batch = prepare_inputs(""chainer"", is_cuda=True)\n        model.to_gpu()\n    loss = model(*batch)\n    if isinstance(loss, tuple):\n        # chainer return several values as tuple\n        loss[0].backward()  # trainable\n    else:\n        loss.backward()  # trainable\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(\n    ""module"",\n    [""espnet.nets.chainer_backend.e2e_asr"", ""espnet.nets.pytorch_backend.e2e_asr""],\n)\ndef test_multi_gpu_trainable(module):\n    m = importlib.import_module(module)\n    ngpu = 2\n    device_ids = list(range(ngpu))\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    if ""pytorch"" in module:\n        model = torch.nn.DataParallel(model, device_ids)\n        batch = prepare_inputs(""pytorch"", is_cuda=True)\n        model.cuda()\n        loss = 1.0 / ngpu * model(*batch)\n        loss.backward(loss.new_ones(ngpu))  # trainable\n    else:\n        import copy\n        import cupy\n\n        losses = []\n        for device in device_ids:\n            with cupy.cuda.Device(device):\n                batch = prepare_inputs(""chainer"", is_cuda=True)\n                _model = copy.deepcopy(\n                    model\n                )  # Transcribed from training.updaters.ParallelUpdater\n                _model.to_gpu()\n                loss = 1.0 / ngpu * _model(*batch)[0]\n                losses.append(loss)\n\n        for loss in losses:\n            loss.backward()  # trainable\n'"
test/test_e2e_asr_mulenc.py,13,"b'# coding: utf-8\n\n# Copyright 2019 Ruizhi Li\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport argparse\nimport importlib\nimport os\nimport tempfile\n\nimport chainer\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.utils.training.batchfy import make_batchset\nfrom test.utils_test import make_dummy_json\n\n\ndef make_arg(num_encs, **kwargs):\n    defaults = dict(\n        num_encs=num_encs,\n        elayers=[1 for _ in range(num_encs)],\n        subsample=[""1_2_2_1_1"" for _ in range(num_encs)],\n        etype=[""vggblstmp"" for _ in range(num_encs)],\n        eunits=[16 for _ in range(num_encs)],\n        eprojs=8,\n        dtype=""lstm"",\n        dlayers=1,\n        dunits=16,\n        atype=[""location"" for _ in range(num_encs)],\n        aheads=[2 for _ in range(num_encs)],\n        awin=[5 for _ in range(num_encs)],\n        aconv_chans=[4 for _ in range(num_encs)],\n        aconv_filts=[10 for _ in range(num_encs)],\n        han_type=""multi_head_add"",\n        han_heads=2,\n        han_win=5,\n        han_conv_chans=4,\n        han_conv_filts=10,\n        han_dim=16,\n        mtlalpha=0.5,\n        lsm_type="""",\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        adim=[16 for _ in range(num_encs)],\n        dropout_rate=[0.0 for _ in range(num_encs)],\n        dropout_rate_decoder=0.0,\n        nbest=5,\n        beam_size=2,\n        penalty=0.5,\n        maxlenratio=1.0,\n        minlenratio=0.0,\n        ctc_weight=0.2,\n        ctc_window_margin=0,\n        lm_weight=0.0,\n        rnnlm=None,\n        streaming_min_blank_dur=10,\n        streaming_onset_margin=2,\n        streaming_offset_margin=2,\n        verbose=2,\n        char_list=[u""\xe3\x81\x82"", u""\xe3\x81\x84"", u""\xe3\x81\x86"", u""\xe3\x81\x88"", u""\xe3\x81\x8a""],\n        outdir=None,\n        ctc_type=""warpctc"",\n        report_cer=False,\n        report_wer=False,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        sortagrad=0,\n        grad_noise=False,\n        context_residual=False,\n        use_frontend=False,\n        share_ctc=False,\n        weights_ctc_train=[0.5 for _ in range(num_encs)],\n        weights_ctc_dec=[0.5 for _ in range(num_encs)],\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef prepare_inputs(mode, num_encs=2, is_cuda=False):\n    ilens_list = [[20, 15] for _ in range(num_encs)]\n    olens = [4, 3]\n    np.random.seed(1)\n    assert len(ilens_list[0]) == len(ilens_list[1]) == len(olens)\n    xs_list = [\n        [np.random.randn(ilen, 40).astype(np.float32) for ilen in ilens]\n        for ilens in ilens_list\n    ]\n    ys = [np.random.randint(1, 5, olen).astype(np.int32) for olen in olens]\n    ilens_list = [np.array([x.shape[0] for x in xs], dtype=np.int32) for xs in xs_list]\n\n    if mode == ""pytorch"":\n        ilens_list = [torch.from_numpy(ilens).long() for ilens in ilens_list]\n        xs_pad_list = [\n            pad_list([torch.from_numpy(x).float() for x in xs], 0) for xs in xs_list\n        ]\n        ys_pad = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n        if is_cuda:\n            xs_pad_list = [xs_pad.cuda() for xs_pad in xs_pad_list]\n            ilens_list = [ilens.cuda() for ilens in ilens_list]\n            ys_pad = ys_pad.cuda()\n\n        return xs_pad_list, ilens_list, ys_pad\n    else:\n        raise ValueError(""Invalid mode"")\n\n\ndef convert_batch(\n    batch, backend=""pytorch"", is_cuda=False, idim=40, odim=5, num_inputs=2\n):\n    ilens_list = [\n        np.array([x[1][""input""][idx][""shape""][0] for x in batch])\n        for idx in range(num_inputs)\n    ]\n    olens = np.array([x[1][""output""][0][""shape""][0] for x in batch])\n    xs_list = [\n        [np.random.randn(ilen, idim).astype(np.float32) for ilen in ilens_list[idx]]\n        for idx in range(num_inputs)\n    ]\n    ys = [np.random.randint(1, odim, olen).astype(np.int32) for olen in olens]\n    is_pytorch = backend == ""pytorch""\n    if is_pytorch:\n        xs_list = [\n            pad_list([torch.from_numpy(x).float() for x in xs_list[idx]], 0)\n            for idx in range(num_inputs)\n        ]\n        ilens_list = [\n            torch.from_numpy(ilens_list[idx]).long() for idx in range(num_inputs)\n        ]\n        ys = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n\n        if is_cuda:\n            xs_list = [xs_list[idx].cuda() for idx in range(num_inputs)]\n            ilens_list = [ilens_list[idx].cuda() for idx in range(num_inputs)]\n            ys = ys.cuda()\n\n    return xs_list, ilens_list, ys\n\n\n@pytest.mark.parametrize(\n    ""module, num_encs, model_dict"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""elayers"": [2, 3], ""dlayers"": 2},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""etype"": [""grup"", ""grup""]}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""lstmp"", ""lstmp""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""bgrup"", ""bgrup""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""blstmp"", ""blstmp""]},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""etype"": [""bgru"", ""bgru""]}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""blstm"", ""blstm""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""vgggru"", ""vgggru""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""vgggrup"", ""vgggrup""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""vgglstm"", ""vgglstm""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""vgglstmp"", ""vgglstmp""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""vggbgru"", ""vggbgru""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""vggbgrup"", ""vggbgrup""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""vggblstm"", ""vggblstm""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""etype"": [""blstmp"", ""vggblstmp""]},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""dtype"": ""gru""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""noatt"", ""noatt""]},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""atype"": [""add"", ""add""]}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""atype"": [""dot"", ""dot""]}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""coverage"", ""coverage""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""coverage_location"", ""coverage_location""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""location2d"", ""location2d""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""location_recurrent"", ""location_recurrent""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""multi_head_dot"", ""multi_head_dot""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""multi_head_add"", ""multi_head_add""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""multi_head_loc"", ""multi_head_loc""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""atype"": [""multi_head_multi_res_loc"", ""multi_head_multi_res_loc""]},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""han_type"": ""noatt""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""han_type"": ""add""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""han_type"": ""dot""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""han_type"": ""coverage""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""han_type"": ""coverage_location""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""han_type"": ""location2d""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""han_type"": ""location_recurrent""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""han_type"": ""multi_head_dot""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""han_type"": ""multi_head_add""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""han_type"": ""multi_head_loc""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""han_type"": ""multi_head_multi_res_loc""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""mtlalpha"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""mtlalpha"": 1.0}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""sampling_probability"": 0.5},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""ctc_type"": ""builtin""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""ctc_weight"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""ctc_weight"": 1.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""context_residual"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""grad_noise"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""report_cer"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""report_wer"": True}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""report_cer"": True, ""report_wer"": True},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 0.0},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            2,\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 1.0},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, {""share_ctc"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""elayers"": [2, 3, 4], ""dlayers"": 2},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""grup"", ""grup"", ""grup""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""lstmp"", ""lstmp"", ""lstmp""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""bgrup"", ""bgrup"", ""bgrup""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""blstmp"", ""blstmp"", ""blstmp""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""bgru"", ""bgru"", ""bgru""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""blstm"", ""blstm"", ""blstm""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""vgggru"", ""vgggru"", ""vgggru""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""vgggrup"", ""vgggrup"", ""vgggrup""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""vgglstm"", ""vgglstm"", ""vgglstm""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""vgglstmp"", ""vgglstmp"", ""vgglstmp""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""vggbgru"", ""vggbgru"", ""vggbgru""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""vggbgrup"", ""vggbgrup"", ""vggbgrup""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""vggblstm"", ""vggblstm"", ""vggblstm""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""etype"": [""blstmp"", ""vggblstmp"", ""vggblstmp""]},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""dtype"": ""gru""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""noatt"", ""noatt"", ""noatt""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""add"", ""add"", ""add""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""dot"", ""dot"", ""dot""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""coverage"", ""coverage"", ""coverage""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""coverage_location"", ""coverage_location"", ""coverage_location""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""location2d"", ""location2d"", ""location2d""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {\n                ""atype"": [\n                    ""location_recurrent"",\n                    ""location_recurrent"",\n                    ""location_recurrent"",\n                ]\n            },\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""multi_head_dot"", ""multi_head_dot"", ""multi_head_dot""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""multi_head_add"", ""multi_head_add"", ""multi_head_add""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""atype"": [""multi_head_loc"", ""multi_head_loc"", ""multi_head_loc""]},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {\n                ""atype"": [\n                    ""multi_head_multi_res_loc"",\n                    ""multi_head_multi_res_loc"",\n                    ""multi_head_multi_res_loc"",\n                ]\n            },\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""han_type"": ""noatt""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""han_type"": ""add""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""han_type"": ""dot""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""han_type"": ""coverage""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""han_type"": ""coverage_location""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""han_type"": ""location2d""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""han_type"": ""location_recurrent""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""han_type"": ""multi_head_dot""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""han_type"": ""multi_head_add""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""han_type"": ""multi_head_loc""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""han_type"": ""multi_head_multi_res_loc""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""mtlalpha"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""mtlalpha"": 1.0}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""sampling_probability"": 0.5},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""ctc_type"": ""builtin""}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""ctc_weight"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""ctc_weight"": 1.0}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""context_residual"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""grad_noise"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""report_cer"": True}),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""report_wer"": True}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""report_cer"": True, ""report_wer"": True},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 0.0},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr_mulenc"",\n            3,\n            {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 1.0},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, {""share_ctc"": True}),\n    ],\n)\ndef test_model_trainable_and_decodable(module, num_encs, model_dict):\n    args = make_arg(num_encs=num_encs, **model_dict)\n    batch = prepare_inputs(""pytorch"", num_encs)\n\n    # test trainable\n    m = importlib.import_module(module)\n    model = m.E2E([40 for _ in range(num_encs)], 5, args)\n    loss = model(*batch)\n    loss.backward()  # trainable\n\n    # test attention plot\n    dummy_json = make_dummy_json(\n        num_encs, [10, 20], [10, 20], idim=40, odim=5, num_inputs=num_encs\n    )\n    batchset = make_batchset(dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True)\n    att_ws = model.calculate_all_attentions(\n        *convert_batch(batchset[0], ""pytorch"", idim=40, odim=5, num_inputs=num_encs)\n    )\n    from espnet.asr.asr_utils import PlotAttentionReport\n\n    tmpdir = tempfile.mkdtemp()\n    plot = PlotAttentionReport(\n        model.calculate_all_attentions, batchset[0], tmpdir, None, None, None\n    )\n    for i in range(num_encs):\n        # att-encoder\n        att_w = plot.get_attention_weight(0, att_ws[i][0])\n        plot._plot_and_save_attention(att_w, ""{}/att{}.png"".format(tmpdir, i))\n    # han\n    att_w = plot.get_attention_weight(0, att_ws[num_encs][0])\n    plot._plot_and_save_attention(att_w, ""{}/han.png"".format(tmpdir), han_mode=True)\n\n    # test decodable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = [np.random.randn(10, 40) for _ in range(num_encs)]\n        model.recognize(in_data, args, args.char_list)  # decodable\n        if ""pytorch"" in module:\n            batch_in_data = [\n                [np.random.randn(10, 40), np.random.randn(5, 40)]\n                for _ in range(num_encs)\n            ]\n            model.recognize_batch(\n                batch_in_data, args, args.char_list\n            )  # batch decodable\n\n\n@pytest.mark.parametrize(""module, num_encs"", [(""pytorch"", 2), (""pytorch"", 3)])\ndef test_gradient_noise_injection(module, num_encs):\n    args = make_arg(num_encs=num_encs, grad_noise=True)\n    args_org = make_arg(num_encs=num_encs)\n    dummy_json = make_dummy_json(\n        num_encs, [10, 20], [10, 20], idim=20, odim=5, num_inputs=num_encs\n    )\n    import espnet.nets.pytorch_backend.e2e_asr_mulenc as m\n\n    batchset = make_batchset(dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True)\n    model = m.E2E([20 for _ in range(num_encs)], 5, args)\n    model_org = m.E2E([20 for _ in range(num_encs)], 5, args_org)\n    for batch in batchset:\n        loss = model(\n            *convert_batch(batch, module, idim=20, odim=5, num_inputs=num_encs)\n        )\n        loss_org = model_org(\n            *convert_batch(batch, module, idim=20, odim=5, num_inputs=num_encs)\n        )\n        loss.backward()\n        grad = [param.grad for param in model.parameters()][10]\n        loss_org.backward()\n        grad_org = [param.grad for param in model_org.parameters()][10]\n        assert grad[0] != grad_org[0]\n\n\n@pytest.mark.parametrize(""module, num_encs"", [(""pytorch"", 2), (""pytorch"", 3)])\ndef test_sortagrad_trainable(module, num_encs):\n    args = make_arg(num_encs=num_encs, sortagrad=1)\n    dummy_json = make_dummy_json(\n        6, [10, 20], [10, 20], idim=20, odim=5, num_inputs=num_encs\n    )\n    import espnet.nets.pytorch_backend.e2e_asr_mulenc as m\n\n    batchset = make_batchset(dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True)\n    model = m.E2E([20 for _ in range(num_encs)], 5, args)\n    num_utts = 0\n    for batch in batchset:\n        num_utts += len(batch)\n        loss = model(\n            *convert_batch(batch, module, idim=20, odim=5, num_inputs=num_encs)\n        )\n        loss.backward()  # trainable\n    assert num_utts == 6\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = [np.random.randn(50, 20) for _ in range(num_encs)]\n        model.recognize(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module, num_encs"", [(""pytorch"", 2), (""pytorch"", 3)])\ndef test_sortagrad_trainable_with_batch_bins(module, num_encs):\n    args = make_arg(num_encs=num_encs, sortagrad=1)\n    idim = 20\n    odim = 5\n    dummy_json = make_dummy_json(\n        4, [10, 20], [10, 20], idim=idim, odim=odim, num_inputs=num_encs\n    )\n    import espnet.nets.pytorch_backend.e2e_asr_mulenc as m\n\n    batch_elems = 2000\n    batchset = make_batchset(dummy_json, batch_bins=batch_elems, shortest_first=True)\n    for batch in batchset:\n        n = 0\n        for uttid, info in batch:\n            ilen = int(info[""input""][0][""shape""][0])  # based on the first input\n            olen = int(info[""output""][0][""shape""][0])\n            n += ilen * idim + olen * odim\n        assert olen < batch_elems\n\n    model = m.E2E([20 for _ in range(num_encs)], 5, args)\n    for batch in batchset:\n        loss = model(\n            *convert_batch(batch, module, idim=20, odim=5, num_inputs=num_encs)\n        )\n        loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = [np.random.randn(100, 20) for _ in range(num_encs)]\n        model.recognize(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module, num_encs"", [(""pytorch"", 2), (""pytorch"", 3)])\ndef test_sortagrad_trainable_with_batch_frames(module, num_encs):\n    args = make_arg(num_encs=num_encs, sortagrad=1)\n    idim = 20\n    odim = 5\n    dummy_json = make_dummy_json(\n        4, [10, 20], [10, 20], idim=idim, odim=odim, num_inputs=num_encs\n    )\n    import espnet.nets.pytorch_backend.e2e_asr_mulenc as m\n\n    batch_frames_in = 50\n    batch_frames_out = 50\n    batchset = make_batchset(\n        dummy_json,\n        batch_frames_in=batch_frames_in,\n        batch_frames_out=batch_frames_out,\n        shortest_first=True,\n    )\n    for batch in batchset:\n        i = 0\n        o = 0\n        for uttid, info in batch:\n            i += int(info[""input""][0][""shape""][0])  # based on the first input\n            o += int(info[""output""][0][""shape""][0])\n        assert i <= batch_frames_in\n        assert o <= batch_frames_out\n\n    model = m.E2E([20 for _ in range(num_encs)], 5, args)\n    for batch in batchset:\n        loss = model(\n            *convert_batch(batch, module, idim=20, odim=5, num_inputs=num_encs)\n        )\n        loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = [np.random.randn(100, 20) for _ in range(num_encs)]\n        model.recognize(in_data, args, args.char_list)\n\n\ndef init_torch_weight_const(m, val):\n    for p in m.parameters():\n        if p.dim() > 1:\n            p.data.fill_(val)\n\n\n@pytest.mark.parametrize(\n    ""module, num_encs, atype"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""noatt""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""dot""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""add""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""location""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""coverage""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""coverage_location""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""location2d""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""location_recurrent""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""multi_head_dot""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""multi_head_add""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""multi_head_loc""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2, ""multi_head_multi_res_loc""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""noatt""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""dot""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""add""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""location""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""coverage""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""coverage_location""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""location2d""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""location_recurrent""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""multi_head_dot""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""multi_head_add""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""multi_head_loc""),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3, ""multi_head_multi_res_loc""),\n    ],\n)\ndef test_calculate_all_attentions(module, num_encs, atype):\n    m = importlib.import_module(module)\n    args = make_arg(\n        num_encs=num_encs, atype=[atype for _ in range(num_encs)], han_type=atype\n    )\n    batch = prepare_inputs(""pytorch"", num_encs)\n    model = m.E2E([40 for _ in range(num_encs)], 5, args)\n    with chainer.no_backprop_mode():\n        att_ws = model.calculate_all_attentions(*batch)\n        for i in range(num_encs):\n            print(att_ws[i][0].shape)  # att\n        print(att_ws[num_encs][0].shape)  # han\n\n\n@pytest.mark.parametrize(""num_encs"", [2, 3])\ndef test_torch_save_and_load(num_encs):\n    m = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr_mulenc"")\n    utils = importlib.import_module(""espnet.asr.asr_utils"")\n    args = make_arg(num_encs=num_encs)\n    model = m.E2E([40 for _ in range(num_encs)], 5, args)\n    # initialize randomly\n    for p in model.parameters():\n        p.data.uniform_()\n    if not os.path.exists("".pytest_cache""):\n        os.makedirs("".pytest_cache"")\n    tmppath = tempfile.mktemp()\n    utils.torch_save(tmppath, model)\n    p_saved = [p.data.numpy() for p in model.parameters()]\n    # set constant value\n    for p in model.parameters():\n        p.data.zero_()\n    utils.torch_load(tmppath, model)\n    for p1, p2 in zip(p_saved, model.parameters()):\n        np.testing.assert_array_equal(p1, p2.data.numpy())\n    if os.path.exists(tmppath):\n        os.remove(tmppath)\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available() and not chainer.cuda.available, reason=""gpu required""\n)\n@pytest.mark.parametrize(\n    ""module, num_encs"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3),\n    ],\n)\ndef test_gpu_trainable(module, num_encs):\n    m = importlib.import_module(module)\n    args = make_arg(num_encs=num_encs)\n    model = m.E2E([40 for _ in range(num_encs)], 5, args)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"", num_encs, is_cuda=True)\n        model.cuda()\n    loss = model(*batch)\n    loss.backward()  # trainable\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(\n    ""module, num_encs"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 2),\n        (""espnet.nets.pytorch_backend.e2e_asr_mulenc"", 3),\n    ],\n)\ndef test_multi_gpu_trainable(module, num_encs):\n    m = importlib.import_module(module)\n    ngpu = 2\n    device_ids = list(range(ngpu))\n    args = make_arg(num_encs=num_encs)\n    model = m.E2E([40 for _ in range(num_encs)], 5, args)\n    if ""pytorch"" in module:\n        model = torch.nn.DataParallel(model, device_ids)\n        batch = prepare_inputs(""pytorch"", num_encs, is_cuda=True)\n        model.cuda()\n        loss = 1.0 / ngpu * model(*batch)\n        loss.backward(loss.new_ones(ngpu))  # trainable\n'"
test/test_e2e_asr_sa_transducer.py,16,"b'# coding: utf-8\n\nimport argparse\nimport importlib\nimport logging\nimport numpy\nimport pytest\nimport torch\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n)\n\n\ndef make_train_args(**kwargs):\n    train_defaults = dict(\n        transformer_init=""pytorch"",\n        transformer_input_layer=""conv2d"",\n        transformer_dec_input_layer=""embed"",\n        etype=""transformer"",\n        elayers=2,\n        eunits=16,\n        dtype=""transformer"",\n        dlayers=2,\n        dunits=16,\n        adim=16,\n        aheads=2,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        transformer_attn_dropout_rate_encoder=0.0,\n        transformer_attn_dropout_rate_decoder=0.0,\n        joint_dim=8,\n        mtlalpha=1.0,\n        trans_type=""warp-transducer"",\n        rnnt_mode=""rnnt_mode"",\n        char_list=[""a"", ""e"", ""i"", ""o"", ""u""],\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        report_cer=False,\n        report_wer=False,\n        score_norm_transducer=True,\n        beam_size=1,\n        nbest=1,\n        verbose=2,\n        outdir=None,\n        rnnlm=None,\n    )\n    train_defaults.update(kwargs)\n\n    return argparse.Namespace(**train_defaults)\n\n\ndef make_recog_args(**kwargs):\n    recog_defaults = dict(\n        batchsize=0,\n        beam_size=1,\n        nbest=1,\n        verbose=2,\n        score_norm_transducer=True,\n        rnnlm=None,\n    )\n    recog_defaults.update(kwargs)\n\n    return argparse.Namespace(**recog_defaults)\n\n\ndef get_default_scope_inputs():\n    bs = 5\n    idim = 40\n    odim = 5\n\n    ilens = [40, 30, 20, 15, 10]\n    olens = [3, 9, 10, 2, 3]\n\n    return bs, idim, odim, ilens, olens\n\n\ndef test_sequential():\n    from espnet.nets.pytorch_backend.transformer.repeat import MultiSequential\n\n    class Masked(torch.nn.Module):\n        def forward(self, x, m):\n            return x, m\n\n    f = MultiSequential(Masked(), Masked())\n    x = torch.randn(2, 3)\n    m = torch.randn(2, 3) > 0\n    assert len(f(x, m)) == 2\n\n    if torch.cuda.is_available():\n        f = torch.nn.DataParallel(f)\n        f.cuda()\n        assert len(f(x.cuda(), m.cuda())) == 2\n\n\ndef subsequent_mask(size):\n    # http://nlp.seas.harvard.edu/2018/04/03/attention.html\n    ""Mask out subsequent positions.""\n    attn_shape = (1, size, size)\n    subsequent_mask = numpy.triu(numpy.ones(attn_shape), k=1).astype(""uint8"")\n\n    return torch.from_numpy(subsequent_mask) == 0\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_mask(module):\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.transformer.mask"".format(module)\n    )\n    m = T.subsequent_mask(3)\n    assert (m.unsqueeze(0) == subsequent_mask(3)).all()\n\n\ndef prepare(backend, args):\n    bs, idim, odim, ilens, olens = get_default_scope_inputs()\n    n_token = odim - 1\n\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_asr_transducer"".format(backend)\n    )\n    model = T.E2E(idim, odim, args)\n\n    x = torch.randn(bs, 40, idim)\n    y = (torch.rand(bs, 10) * n_token % n_token).long()\n\n    for i in range(bs):\n        x[i, ilens[i] :] = -1\n        y[i, olens[i] :] = model.ignore_id\n\n    data = []\n    for i in range(bs):\n        data.append(\n            (\n                ""utt%d"" % i,\n                {\n                    ""input"": [{""shape"": [ilens[i], idim]}],\n                    ""output"": [{""shape"": [olens[i]]}],\n                },\n            )\n        )\n\n    return model, x, torch.tensor(ilens), y, data\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_sa_transducer_mask(module):\n    from espnet.nets.pytorch_backend.nets_utils import make_pad_mask\n    from espnet.nets.pytorch_backend.transducer.utils import prepare_loss_inputs\n    from espnet.nets.pytorch_backend.transformer.mask import target_mask\n\n    train_args = make_train_args()\n    model, x, ilens, y, data = prepare(module, train_args)\n\n    # dummy mask\n    x_mask = (~make_pad_mask(ilens.tolist())).to(x.device).unsqueeze(-2)\n\n    _, target, _, _ = prepare_loss_inputs(y, x_mask)\n    y_mask = target_mask(target, model.blank_id)\n\n    y = model.decoder.embed(target.type(torch.long))\n    y[0, 3:] = float(""nan"")\n\n    a = model.decoder.decoders[0].self_attn\n    a(y, y, y, y_mask)\n    assert not numpy.isnan(a.attn[0, :, :3, :3].detach().numpy()).any()\n\n\n@pytest.mark.parametrize(\n    ""train_dic, recog_dic"",\n    [\n        ({}, {}),\n        ({}, {""beam_size"": 4}),\n        ({}, {""beam_size"": 4, ""nbest"": 4}),\n        ({}, {""beam_size"": 5, ""score_norm_transducer"": False}),\n        ({""num_save_attention"": 1}, {}),\n        ({""dropout_rate_encoder"": 0.1, ""dropout_rate_decoder"": 0.1}, {}),\n        ({""eunits"": 16, ""elayers"": 2, ""joint_dim"": 2}, {}),\n        (\n            {""adim"": 16, ""aheads"": 2, ""transformer_attn_dropout_rate_encoder"": 0.2},\n            {""beam_size"": 3},\n        ),\n        (\n            {\n                ""transformer_attn_dropout_rate_encoder"": 0.2,\n                ""transformer_attn_dropout_rate_decoder"": 0.3,\n            },\n            {},\n        ),\n        (\n            {""dlayers"": 2, ""dunits"": 16, ""joint_dim"": 3},\n            {""score_norm_transducer"": False},\n        ),\n        ({""transformer_input_layer"": ""vgg2l""}, {}),\n        (\n            {\n                ""transformer_input_layer"": ""vgg2l"",\n                ""eunits"": 8,\n                ""adim"": 4,\n                ""joint_dim"": 2,\n            },\n            {},\n        ),\n        ({""report_cer"": True, ""beam_size"": 1}, {}),\n        ({""report_wer"": True, ""beam_size"": 1}, {}),\n        ({""report_cer"": True, ""beam_size"": 2}, {}),\n        ({""report_wer"": True, ""beam_size"": 2}, {}),\n        ({""report_cer"": True, ""report_wer"": True, ""beam_size"": 1}, {}),\n        ({""report_wer"": True, ""report_wer"": True, ""score_norm_transducer"": False}, {}),\n    ],\n)\ndef test_sa_transducer_trainable_and_decodable(train_dic, recog_dic):\n    from espnet.nets.pytorch_backend.transformer import plot\n\n    train_args = make_train_args(**train_dic)\n    recog_args = make_recog_args(**recog_dic)\n\n    model, x, ilens, y, data = prepare(""pytorch"", train_args)\n\n    optim = torch.optim.Adam(model.parameters(), 0.01)\n    loss = model(x, ilens, y)\n\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n\n    attn_dict = model.calculate_all_attentions(x[0:1], ilens[0:1], y[0:1])\n    plot.plot_multi_head_attention(data, attn_dict, ""/tmp/espnet-test"")\n\n    with torch.no_grad():\n        nbest = model.recognize(x[0, : ilens[0]].numpy(), recog_args)\n\n        print(y[0])\n        print(nbest[0][""yseq""][1:-1])\n\n\ndef test_sa_transducer_parallel():\n    if not torch.cuda.is_available():\n        return\n\n    train_args = make_train_args()\n\n    model, x, ilens, y, data = prepare(""pytorch"", train_args)\n    model = torch.nn.DataParallel(model).cuda()\n\n    logging.debug(ilens)\n\n    optim = torch.optim.Adam(model.parameters(), 0.02)\n\n    for i in range(10):\n        loss = model(x, torch.as_tensor(ilens), y)\n\n        optim.zero_grad()\n        loss.mean().backward()\n        optim.step()\n\n        print(loss)\n'"
test/test_e2e_asr_transducer.py,7,"b'# coding: utf-8\n\nimport argparse\nimport importlib\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\ndef get_default_train_args(**kwargs):\n    train_defaults = dict(\n        etype=""vggblstmp"",\n        elayers=1,\n        subsample=""1_2_2_1_1"",\n        eunits=8,\n        eprojs=8,\n        dtype=""lstm"",\n        dlayers=1,\n        dunits=8,\n        dec_embed_dim=8,\n        atype=""location"",\n        adim=8,\n        aheads=2,\n        awin=5,\n        aconv_chans=4,\n        aconv_filts=10,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        dropout_rate_embed_decoder=0.0,\n        joint_dim=8,\n        mtlalpha=1.0,\n        rnnt_mode=""rnnt"",\n        use_frontend=False,\n        trans_type=""warp-transducer"",\n        char_list=[""a"", ""b"", ""c"", ""d""],\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        report_cer=False,\n        report_wer=False,\n        score_norm_transducer=True,\n        beam_size=1,\n        nbest=1,\n        verbose=2,\n        outdir=None,\n        rnnlm=None,\n    )\n    train_defaults.update(kwargs)\n\n    return argparse.Namespace(**train_defaults)\n\n\ndef get_default_recog_args(**kwargs):\n    recog_defaults = dict(\n        batchsize=0,\n        beam_size=2,\n        nbest=1,\n        verbose=2,\n        score_norm_transducer=True,\n        rnnlm=None,\n    )\n    recog_defaults.update(kwargs)\n\n    return argparse.Namespace(**recog_defaults)\n\n\ndef get_default_scope_inputs():\n    idim = 40\n    odim = 4\n    ilens = [20, 15]\n    olens = [4, 3]\n\n    return idim, odim, ilens, olens\n\n\ndef prepare_inputs(backend, idim, odim, ilens, olens, is_cuda=False):\n    np.random.seed(1)\n\n    xs = [np.random.randn(ilen, idim).astype(np.float32) for ilen in ilens]\n    ys = [np.random.randint(1, odim, olen).astype(np.int32) for olen in olens]\n    ilens = np.array([x.shape[0] for x in xs], dtype=np.int32)\n\n    if backend == ""pytorch"":\n        xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0)\n        ys_pad = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n        ilens = torch.from_numpy(ilens).long()\n\n        if is_cuda:\n            xs_pad = xs_pad.cuda()\n            ys_pad = ys_pad.cuda()\n            ilens = ilens.cuda()\n\n        return xs_pad, ilens, ys_pad\n    else:\n        raise ValueError(""Invalid backend"")\n\n\n@pytest.mark.parametrize(\n    ""train_dic, recog_dic"",\n    [\n        ({}, {""beam_size"": 1}),\n        ({""rnnt_mode"": ""rnnt-att""}, {""beam_size"": 1}),\n        ({}, {""beam_size"": 8}),\n        ({""rnnt_mode"": ""rnnt-att""}, {""beam_size"": 8}),\n        ({}, {}),\n        ({""rnnt_mode"": ""rnnt-att""}, {}),\n        ({""etype"": ""gru""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""gru""}, {}),\n        ({""etype"": ""blstm""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""blstm""}, {}),\n        ({""etype"": ""vgggru""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""vgggru""}, {}),\n        ({""etype"": ""vggbru""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""vggbgru""}, {}),\n        ({""etype"": ""vgggrup""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""vgggrup""}, {}),\n        ({""etype"": ""blstm"", ""elayers"": 2}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""blstm"", ""elayers"": 2}, {}),\n        ({""etype"": ""blstm"", ""eunits"": 16}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""blstm"", ""eunits"": 16}, {}),\n        ({""etype"": ""blstm"", ""eprojs"": 16}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""etype"": ""blstm"", ""eprojs"": 16}, {}),\n        ({""dtype"": ""gru""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""dtype"": ""gru""}, {}),\n        ({""dtype"": ""bgrup""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""dtype"": ""bgrup""}, {}),\n        ({""dtype"": ""gru"", ""dlayers"": 2}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""dtype"": ""gru"", ""dlayers"": 2}, {}),\n        ({""dtype"": ""lstm"", ""dlayers"": 3}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""dtype"": ""lstm"", ""dlayers"": 3}, {}),\n        ({""dtype"": ""gru"", ""dunits"": 16}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""dtype"": ""gru"", ""dunits"": 16}, {}),\n        ({""dtype"": ""lstm"", ""dlayers"": 2, ""dunits"": 16}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""dtype"": ""lstm"", ""dlayers"": 3, ""dunits"": 16}, {}),\n        ({""joint-dim"": 16}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""joint-dim"": 16}, {}),\n        ({""dtype"": ""lstm"", ""dlayers"": 2, ""dunits"": 16, ""joint-dim"": 4}, {}),\n        (\n            {\n                ""rnnt_mode"": ""rnnt-att"",\n                ""dtype"": ""lstm"",\n                ""dlayers"": 3,\n                ""dunits"": 16,\n                ""joint-dim"": 4,\n            },\n            {},\n        ),\n        ({""dec-embed-dim"": 16}, {}),\n        ({""dec-embed-dim"": 16, ""dropout-rate-embed-decoder"": 0.1}, {}),\n        ({""dunits"": 16}, {""beam_size"": 1}),\n        ({""rnnt_mode"": ""rnnt-att"", ""dunits"": 2}, {""beam_size"": 1}),\n        ({""dropout-rate-decoder"": 0.2}, {}),\n        ({""rnnt-mode"": ""rnnt-att"", ""dropout-rate-decoder"": 0.2}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""noatt""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""dot""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""coverage""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""coverage""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""coverage_location""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""location2d""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""location_recurrent""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""multi_head_dot""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""multi_head_add""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""multi_head_loc""}, {}),\n        ({""rnnt_mode"": ""rnnt-att"", ""atype"": ""multi_head_multi_res_loc""}, {}),\n        ({}, {""score_norm_transducer"": False}),\n        ({""rnnt_mode"": ""rnnt-att""}, {""score_norm_transducer"": False}),\n        ({}, {""nbest"": 2}),\n        ({""rnnt_mode"": ""rnnt-att""}, {""nbest"": 2}),\n        ({""beam_size"": 1, ""report_cer"": True, ""report_wer"": True}, {}),\n        (\n            {\n                ""rnnt_mode"": ""rnnt-att"",\n                ""beam_size"": 1,\n                ""report_cer"": True,\n                ""report_wer"": True,\n            },\n            {},\n        ),\n        ({""beam_size"": 1, ""report_cer"": True, ""report_wer"": False}, {}),\n        (\n            {\n                ""rnnt_mode"": ""rnnt-att"",\n                ""beam_size"": 1,\n                ""report_cer"": True,\n                ""report_wer"": False,\n            },\n            {},\n        ),\n        ({""beam_size"": 1, ""report_cer"": False, ""report_wer"": True}, {}),\n        (\n            {\n                ""rnnt_mode"": ""rnnt-att"",\n                ""beam_size"": 1,\n                ""report_cer"": False,\n                ""report_wer"": True,\n            },\n            {},\n        ),\n    ],\n)\ndef test_pytorch_transducer_trainable_and_decodable(\n    train_dic, recog_dic, backend=""pytorch""\n):\n    idim, odim, ilens, olens = get_default_scope_inputs()\n    train_args = get_default_train_args(**train_dic)\n\n    module = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_asr_transducer"".format(backend)\n    )\n    model = module.E2E(idim, odim, train_args)\n\n    batch = prepare_inputs(backend, idim, odim, ilens, olens)\n\n    loss = model(*batch)\n    loss.backward()\n\n    with torch.no_grad():\n        in_data = np.random.randn(20, idim)\n        recog_args = get_default_recog_args(**recog_dic)\n        model.recognize(in_data, recog_args, train_args.char_list)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""gpu required"")\n@pytest.mark.parametrize(""backend"", [""pytorch""])\ndef test_pytorch_transducer_gpu_trainable(backend):\n    idim, odim, ilens, olens = get_default_scope_inputs()\n    train_args = get_default_train_args()\n\n    module = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_asr_transducer"".format(backend)\n    )\n    model = module.E2E(idim, odim, train_args)\n    model.cuda()\n\n    batch = prepare_inputs(backend, idim, odim, ilens, olens, is_cuda=True)\n\n    loss = model(*batch)\n    loss.backward()\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(""backend"", [""pytorch""])\ndef test_pytorch_multi_gpu_trainable(backend):\n    idim, odim, ilens, olens = get_default_scope_inputs()\n    train_args = get_default_train_args()\n\n    ngpu = 2\n    device_ids = list(range(ngpu))\n\n    module = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_asr_transducer"".format(backend)\n    )\n    model = module.E2E(idim, odim, train_args)\n    model = torch.nn.DataParallel(model, device_ids)\n    model.cuda()\n\n    batch = prepare_inputs(backend, idim, odim, ilens, olens, is_cuda=True)\n\n    loss = 1.0 / ngpu * model(*batch)\n    loss.backward(loss.new_ones(ngpu))\n\n\n@pytest.mark.parametrize(\n    ""atype"",\n    [\n        ""noatt"",\n        ""dot"",\n        ""location"",\n        ""noatt"",\n        ""add"",\n        ""coverage"",\n        ""coverage_location"",\n        ""location2d"",\n        ""location_recurrent"",\n        ""multi_head_dot"",\n        ""multi_head_add"",\n        ""multi_head_loc"",\n        ""multi_head_multi_res_loc"",\n    ],\n)\ndef test_pytorch_calculate_all_attentions(atype, backend=""pytorch""):\n    idim, odim, ilens, olens = get_default_scope_inputs()\n    train_args = get_default_train_args(rnnt_mode=""rnnt-att"", atype=atype)\n\n    module = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_asr_transducer"".format(backend)\n    )\n    model = module.E2E(idim, odim, train_args)\n\n    batch = prepare_inputs(backend, idim, odim, ilens, olens, is_cuda=False)\n\n    att_ws = model.calculate_all_attentions(*batch)[0]\n    print(att_ws.shape)\n'"
test/test_e2e_asr_transformer.py,22,"b'import argparse\nimport chainer\nimport importlib\nimport logging\nimport numpy\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import rename_state_dict\n\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n)\n\n\ndef test_sequential():\n    class Masked(torch.nn.Module):\n        def forward(self, x, m):\n            return x, m\n\n    from espnet.nets.pytorch_backend.transformer.repeat import MultiSequential\n\n    f = MultiSequential(Masked(), Masked())\n    x = torch.randn(2, 3)\n    m = torch.randn(2, 3) > 0\n    assert len(f(x, m)) == 2\n    if torch.cuda.is_available():\n        f = torch.nn.DataParallel(f)\n        f.cuda()\n        assert len(f(x.cuda(), m.cuda())) == 2\n\n\ndef subsequent_mask(size, backend=""pytorch""):\n    # http://nlp.seas.harvard.edu/2018/04/03/attention.html\n    ""Mask out subsequent positions.""\n    attn_shape = (1, size, size)\n    subsequent_mask = numpy.triu(numpy.ones(attn_shape), k=1).astype(""uint8"")\n    if backend == ""pytorch"":\n        return torch.from_numpy(subsequent_mask) == 0\n    else:\n        return subsequent_mask == 0\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_mask(module):\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_asr_transformer"".format(module)\n    )\n    m = T.subsequent_mask(3)\n    print(m)\n    print(subsequent_mask(3))\n    assert (m.unsqueeze(0) == subsequent_mask(3)).all()\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        adim=16,\n        aheads=2,\n        dropout_rate=0.0,\n        transformer_attn_dropout_rate=None,\n        elayers=2,\n        eunits=16,\n        dlayers=2,\n        dunits=16,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        transformer_init=""pytorch"",\n        transformer_input_layer=""conv2d"",\n        transformer_length_normalized_loss=True,\n        report_cer=False,\n        report_wer=False,\n        mtlalpha=0.0,\n        lsm_weight=0.001,\n        char_list=[""<blank>"", ""a"", ""e"", ""i"", ""o"", ""u""],\n        ctc_type=""warpctc"",\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef prepare(backend, args):\n    idim = 40\n    odim = 5\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_asr_transformer"".format(backend)\n    )\n\n    model = T.E2E(idim, odim, args)\n    batchsize = 5\n    if backend == ""pytorch"":\n        x = torch.randn(batchsize, 40, idim)\n    else:\n        x = numpy.random.randn(batchsize, 40, idim).astype(numpy.float32)\n    ilens = [40, 30, 20, 15, 10]\n    n_token = odim - 1\n    if backend == ""pytorch"":\n        y = (torch.rand(batchsize, 10) * n_token % n_token).long()\n    else:\n        y = (numpy.random.rand(batchsize, 10) * n_token % n_token).astype(numpy.int32)\n    olens = [3, 9, 10, 2, 3]\n    for i in range(batchsize):\n        x[i, ilens[i] :] = -1\n        y[i, olens[i] :] = model.ignore_id\n\n    data = []\n    for i in range(batchsize):\n        data.append(\n            (\n                ""utt%d"" % i,\n                {\n                    ""input"": [{""shape"": [ilens[i], idim]}],\n                    ""output"": [{""shape"": [olens[i]]}],\n                },\n            )\n        )\n    if backend == ""pytorch"":\n        return model, x, torch.tensor(ilens), y, data\n    else:\n        return model, x, ilens, y, data\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_transformer_mask(module):\n    args = make_arg()\n    model, x, ilens, y, data = prepare(module, args)\n    from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n    from espnet.nets.pytorch_backend.transformer.mask import target_mask\n\n    yi, yo = add_sos_eos(y, model.sos, model.eos, model.ignore_id)\n    y_mask = target_mask(yi, model.ignore_id)\n    y = model.decoder.embed(yi)\n    y[0, 3:] = float(""nan"")\n    a = model.decoder.decoders[0].self_attn\n    a(y, y, y, y_mask)\n    assert not numpy.isnan(a.attn[0, :, :3, :3].detach().numpy()).any()\n\n\n@pytest.mark.parametrize(\n    ""module, model_dict"",\n    [\n        (""pytorch"", {}),\n        (""pytorch"", {""report_cer"": True}),\n        (""pytorch"", {""report_wer"": True}),\n        (""pytorch"", {""report_cer"": True, ""report_wer"": True}),\n        (""pytorch"", {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 0.0}),\n        (""pytorch"", {""report_cer"": True, ""report_wer"": True, ""mtlalpha"": 1.0}),\n        (""chainer"", {}),\n    ],\n)\ndef test_transformer_trainable_and_decodable(module, model_dict):\n    args = make_arg(**model_dict)\n    model, x, ilens, y, data = prepare(module, args)\n\n    # test beam search\n    recog_args = argparse.Namespace(\n        beam_size=1,\n        penalty=0.0,\n        ctc_weight=0.0,\n        maxlenratio=1.0,\n        lm_weight=0,\n        minlenratio=0,\n        nbest=1,\n    )\n    if module == ""pytorch"":\n        # test trainable\n        optim = torch.optim.Adam(model.parameters(), 0.01)\n        loss = model(x, ilens, y)\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n        # test attention plot\n        attn_dict = model.calculate_all_attentions(x[0:1], ilens[0:1], y[0:1])\n        from espnet.nets.pytorch_backend.transformer import plot\n\n        plot.plot_multi_head_attention(data, attn_dict, ""/tmp/espnet-test"")\n\n        # test decodable\n        with torch.no_grad():\n            nbest = model.recognize(x[0, : ilens[0]].numpy(), recog_args)\n            print(y[0])\n            print(nbest[0][""yseq""][1:-1])\n    else:\n        # test trainable\n        optim = chainer.optimizers.Adam(0.01)\n        optim.setup(model)\n        loss, loss_ctc, loss_att, acc = model(x, ilens, y)\n        model.cleargrads()\n        loss.backward()\n        optim.update()\n\n        # test attention plot\n        attn_dict = model.calculate_all_attentions(x[0:1], ilens[0:1], y[0:1])\n        from espnet.nets.pytorch_backend.transformer import plot\n\n        plot.plot_multi_head_attention(data, attn_dict, ""/tmp/espnet-test"")\n\n        # test decodable\n        with chainer.no_backprop_mode():\n            nbest = model.recognize(x[0, : ilens[0]], recog_args)\n            print(y[0])\n            print(nbest[0][""yseq""][1:-1])\n\n\ndef prepare_copy_task(d_model, d_ff=64, n=1):\n    T = importlib.import_module(""espnet.nets.pytorch_backend.e2e_asr_transformer"")\n    idim = 11\n    odim = idim\n\n    if d_model:\n        args = argparse.Namespace(\n            adim=d_model,\n            aheads=2,\n            dropout_rate=0.1,\n            elayers=n,\n            eunits=d_ff,\n            dlayers=n,\n            dunits=d_ff,\n            transformer_init=""xavier_uniform"",\n            transformer_input_layer=""embed"",\n            lsm_weight=0.01,\n            transformer_attn_dropout_rate=None,\n            transformer_length_normalized_loss=True,\n            mtlalpha=0.0,\n        )\n        model = T.E2E(idim, odim, args)\n    else:\n        model = None\n\n    x = torch.randint(1, idim - 1, size=(30, 5)).long()\n    ilens = torch.full((x.size(0),), x.size(1)).long()\n    data = []\n    for i in range(x.size(0)):\n        data.append(\n            (\n                ""utt%d"" % i,\n                {\n                    ""input"": [{""shape"": [ilens[i], idim]}],\n                    ""output"": [{""shape"": [ilens[i], idim]}],\n                },\n            )\n        )\n    return model, x, ilens, x, data\n\n\ndef run_transformer_copy():\n    # copy task defined in http://nlp.seas.harvard.edu/2018/04/03/attention.html#results\n    d_model = 32\n    model, x, ilens, y, data = prepare_copy_task(d_model)\n    model.train()\n    if torch.cuda.is_available():\n        model.cuda()\n    optim = torch.optim.Adam(model.parameters(), 0.01)\n    max_acc = 0\n    for i in range(1000):\n        _, x, ilens, y, data = prepare_copy_task(None)\n        if torch.cuda.is_available():\n            x = x.cuda()\n            y = y.cuda()\n        loss = model(x, ilens, y)\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        acc = model.acc\n        print(i, loss.item(), acc)\n        max_acc = max(acc, max_acc)\n        # attn_dict = model.calculate_all_attentions(x, ilens, y)\n        # T.plot_multi_head_attention(\n        #    data, attn_dict, ""/tmp/espnet-test"", ""iter%d.png"" % i\n        # )\n    assert max_acc > 0.9\n\n    model.cpu()\n    model.eval()\n    # test beam search\n    recog_args = argparse.Namespace(\n        beam_size=1, penalty=0.0, ctc_weight=0.0, maxlenratio=0, minlenratio=0, nbest=1\n    )\n    if torch.cuda.is_available():\n        x = x.cpu()\n        y = y.cpu()\n\n    with torch.no_grad():\n        print(""===== greedy decoding ====="")\n        for i in range(10):\n            nbest = model.recognize(x[i, : ilens[i]].numpy(), recog_args)\n            print(""gold:"", y[i].tolist())\n            print(""pred:"", nbest[0][""yseq""][1:-1])\n        print(""===== beam search decoding ====="")\n        recog_args.beam_size = 4\n        recog_args.nbest = 4\n        for i in range(10):\n            nbest = model.recognize(x[i, : ilens[i]].numpy(), recog_args)\n            print(""gold:"", y[i].tolist())\n            print(""pred:"", [n[""yseq""][1:-1] for n in nbest])\n    # # test attention plot\n    # attn_dict = model.calculate_all_attentions(x[:3], ilens[:3], y[:3])\n    # T.plot_multi_head_attention(data, attn_dict, ""/tmp/espnet-test"")\n    # assert(False)\n\n\ndef test_transformer_parallel():\n    if not torch.cuda.is_available():\n        return\n\n    args = make_arg()\n    model, x, ilens, y, data = prepare(""pytorch"", args)\n    model = torch.nn.DataParallel(model).cuda()\n    logging.debug(ilens)\n    # test acc is almost 100%\n    optim = torch.optim.Adam(model.parameters(), 0.02)\n    max_acc = 0.0\n    for i in range(40):\n        loss = model(x, torch.as_tensor(ilens), y)\n        optim.zero_grad()\n        acc = float(model.module.acc)\n        max_acc = max(acc, max_acc)\n        loss.mean().backward()\n        optim.step()\n        print(loss, acc)\n        # attn_dict = model.calculate_all_attentions(x, ilens, y)\n        # T.plot_multi_head_attention(\n        #    data, attn_dict, ""/tmp/espnet-test"", ""iter%d.png"" % i\n        # )\n    assert max_acc > 0.8\n\n\n# https://github.com/espnet/espnet/issues/1750\ndef test_v0_3_transformer_input_compatibility():\n    args = make_arg()\n    model, x, ilens, y, data = prepare(""pytorch"", args)\n    # these old names are used in v.0.3.x\n    state_dict = model.state_dict()\n    prefix = ""encoder.""\n    rename_state_dict(prefix + ""embed."", prefix + ""input_layer."", state_dict)\n    rename_state_dict(prefix + ""after_norm."", prefix + ""norm."", state_dict)\n    prefix = ""decoder.""\n    rename_state_dict(prefix + ""after_norm."", prefix + ""output_norm."", state_dict)\n    model.load_state_dict(state_dict)\n\n\nif __name__ == ""__main__"":\n    run_transformer_copy()\n'"
test/test_e2e_compatibility.py,1,"b'#!/usr/bin/env python3\n# coding: utf-8\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\n\nimport importlib\nimport os\nfrom os.path import join\nimport re\nimport shutil\nimport subprocess\nimport tempfile\n\nimport chainer\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.asr.asr_utils import chainer_load\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import torch_load\n\n\ndef download_zip_from_google_drive(download_dir, file_id):\n    # directory check\n    os.makedirs(download_dir, exist_ok=True)\n    tmpzip = join(download_dir, ""tmp.zip"")\n\n    # download zip file from google drive via wget\n    cmd = [\n        ""wget"",\n        ""https://drive.google.com/uc?export=download&id=%s"" % file_id,\n        ""-O"",\n        tmpzip,\n    ]\n    subprocess.run(cmd, check=True)\n\n    try:\n        # unzip downloaded files\n        cmd = [""unzip"", tmpzip, ""-d"", download_dir]\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError:\n        # sometimes, wget from google drive is failed due to virus check confirmation\n        # to avoid it, we need to do some tricky processings\n        # see\n        # https://stackoverflow.com/questions/20665881/direct-download-from-google-drive-using-google-drive-api\n        out = subprocess.check_output(\n            ""curl -c /tmp/cookies ""\n            \'""https://drive.google.com/uc?export=download&id=%s""\' % file_id,\n            shell=True,\n        )\n        out = out.decode(""utf-8"")\n        dllink = ""https://drive.google.com{}"".format(\n            re.findall(r\'<a id=""uc-download-link"" [^>]* href=""([^""]*)"">\', out)[\n                0\n            ].replace(""&amp;"", ""&"")\n        )\n        subprocess.call(\n            f\'curl -L -b /tmp/cookies ""{dllink}"" > {tmpzip}\', shell=True\n        )  # NOQA\n        cmd = [""unzip"", tmpzip, ""-d"", download_dir]\n        subprocess.run(cmd, check=True)\n\n    # get model file path\n    cmd = [""find"", download_dir, ""-name"", ""model.*.best""]\n    cmd_state = subprocess.run(cmd, stdout=subprocess.PIPE, check=True)\n\n    return cmd_state.stdout.decode(""utf-8"").split(""\\n"")[0]\n\n\n# TODO(kan-bayashi): make it to be compatible with python2\n# file id in google drive can be obtain from sharing link\n# ref: https://qiita.com/namakemono/items/c963e75e0af3f7eed732\n@pytest.mark.skipif(True, reason=""Skip due to unstable download"")\n@pytest.mark.parametrize(\n    ""module, download_info"",\n    [\n        (\n            ""espnet.nets.pytorch_backend.e2e_asr"",\n            (""v.0.3.0 egs/an4/asr1 pytorch"", ""1zF88bRNbJhw9hNBq3NrDg8vnGGibREmg""),\n        ),\n        (\n            ""espnet.nets.chainer_backend.e2e_asr"",\n            (""v.0.3.0 egs/an4/asr1 chainer"", ""1m2SZLNxvur3q13T6Zrx6rEVfqEifgPsx""),\n        ),\n    ],\n)\ndef test_downloaded_asr_model_decodable(module, download_info):\n    # download model\n    print(download_info[0])\n    tmpdir = tempfile.mkdtemp(prefix=""tmp_"", dir=""."")\n    model_path = download_zip_from_google_drive(tmpdir, download_info[1])\n\n    # load trained model parameters\n    m = importlib.import_module(module)\n    idim, odim, train_args = get_model_conf(model_path)\n    model = m.E2E(idim, odim, train_args)\n    if ""chainer"" in module:\n        chainer_load(model_path, model)\n    else:\n        torch_load(model_path, model)\n\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(128, idim)\n        model.recognize(in_data, train_args, train_args.char_list)  # decodable\n\n    # remove\n    if os.path.exists(tmpdir):\n        shutil.rmtree(tmpdir)\n'"
test/test_e2e_mt.py,13,"b'# coding: utf-8\n\n# Copyright 2019 Hirofumi Inaguma\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport argparse\nimport importlib\nimport os\nimport tempfile\n\nimport chainer\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.utils.training.batchfy import make_batchset\nfrom test.utils_test import make_dummy_json_mt\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        elayers=1,\n        subsample=""2_2"",\n        etype=""blstm"",\n        eunits=16,\n        eprojs=16,\n        dtype=""lstm"",\n        dlayers=1,\n        dunits=16,\n        atype=""add"",\n        aheads=2,\n        mtlalpha=0.5,\n        lsm_type="""",\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        adim=16,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        nbest=5,\n        beam_size=3,\n        penalty=0.5,\n        maxlenratio=1.0,\n        minlenratio=0.0,\n        ctc_weight=0.0,  # dummy\n        ctc_window_margin=0,  # dummy\n        verbose=2,\n        char_list=[u""\xe3\x81\x82"", u""\xe3\x81\x84"", u""\xe3\x81\x86"", u""\xe3\x81\x88"", u""\xe3\x81\x8a""],\n        outdir=None,\n        report_bleu=False,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        sortagrad=0,\n        context_residual=False,\n        tie_src_tgt_embedding=False,\n        tie_classifier=False,\n        multilingual=False,\n        replace_sos=False,\n        tgt_lang=False,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef prepare_inputs(mode, ilens=[20, 10], olens=[4, 3], is_cuda=False):\n    np.random.seed(1)\n    assert len(ilens) == len(olens)\n    xs = [np.random.randint(0, 5, ilen).astype(np.int32) for ilen in ilens]\n    ys = [np.random.randint(0, 5, olen).astype(np.int32) for olen in olens]\n    ilens = np.array([x.shape[0] for x in xs], dtype=np.int32)\n\n    if mode == ""chainer"":\n        raise NotImplementedError\n\n    elif mode == ""pytorch"":\n        ilens = torch.from_numpy(ilens).long()\n        xs_pad = pad_list([torch.from_numpy(x).long() for x in xs], 0)\n        ys_pad = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n        if is_cuda:\n            xs_pad = xs_pad.cuda()\n            ilens = ilens.cuda()\n            ys_pad = ys_pad.cuda()\n\n        return xs_pad, ilens, ys_pad\n    else:\n        raise ValueError(""Invalid mode"")\n\n\ndef convert_batch(batch, backend=""pytorch"", is_cuda=False, idim=5, odim=5):\n    ilens = np.array([x[1][""output""][1][""shape""][0] for x in batch])\n    olens = np.array([x[1][""output""][0][""shape""][0] for x in batch])\n    xs = [np.random.randint(0, idim, ilen).astype(np.int32) for ilen in ilens]\n    ys = [np.random.randint(0, odim, olen).astype(np.int32) for olen in olens]\n    is_pytorch = backend == ""pytorch""\n    if is_pytorch:\n        xs = pad_list([torch.from_numpy(x).long() for x in xs], 0)\n        ilens = torch.from_numpy(ilens).long()\n        ys = pad_list([torch.from_numpy(y).long() for y in ys], -1)\n\n        if is_cuda:\n            xs = xs.cuda()\n            ilens = ilens.cuda()\n            ys = ys.cuda()\n    else:\n        raise NotImplementedError\n\n    return xs, ilens, ys\n\n\n@pytest.mark.parametrize(\n    ""module, model_dict"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_mt"", {}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""atype"": ""noatt""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""atype"": ""dot""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""atype"": ""coverage""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""atype"": ""multi_head_dot""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""atype"": ""multi_head_add""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""etype"": ""grup""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""etype"": ""lstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""etype"": ""bgrup""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""etype"": ""blstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""etype"": ""bgru""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""etype"": ""blstm""}),\n        (""espnet.nets.pytorch_backend.e2e_mt"", {""context_residual"": True}),\n    ],\n)\ndef test_model_trainable_and_decodable(module, model_dict):\n    args = make_arg(**model_dict)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"")\n    else:\n        raise NotImplementedError\n\n    m = importlib.import_module(module)\n    model = m.E2E(6, 5, args)\n    loss = model(*batch)\n    if isinstance(loss, tuple):\n        # chainer return several values as tuple\n        loss[0].backward()  # trainable\n    else:\n        loss.backward()  # trainable\n\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randint(0, 5, (1, 10))\n        model.translate(in_data, args, args.char_list)  # decodable\n        if ""pytorch"" in module:\n            batch_in_data = np.random.randint(0, 5, (2, 10))\n            model.translate_batch(\n                batch_in_data, args, args.char_list\n            )  # batch decodable\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_sortagrad_trainable(module):\n    args = make_arg(sortagrad=1)\n    dummy_json = make_dummy_json_mt(4, [10, 20], [10, 20], idim=6, odim=5)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_mt as m\n    else:\n        import espnet.nets.chainer_backend.e2e_mt as m\n    batchset = make_batchset(\n        dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True, mt=True, iaxis=1, oaxis=0\n    )\n    model = m.E2E(6, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=6, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randint(0, 5, (1, 100))\n        model.translate(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_sortagrad_trainable_with_batch_bins(module):\n    args = make_arg(sortagrad=1)\n    idim = 6\n    odim = 5\n    dummy_json = make_dummy_json_mt(4, [10, 20], [10, 20], idim=idim, odim=odim)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_mt as m\n    else:\n        raise NotImplementedError\n    batch_elems = 2000\n    batchset = make_batchset(\n        dummy_json,\n        batch_bins=batch_elems,\n        shortest_first=True,\n        mt=True,\n        iaxis=1,\n        oaxis=0,\n    )\n    for batch in batchset:\n        n = 0\n        for uttid, info in batch:\n            ilen = int(info[""output""][1][""shape""][0])\n            olen = int(info[""output""][0][""shape""][0])\n            n += ilen * idim + olen * odim\n        assert olen < batch_elems\n\n    model = m.E2E(6, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=6, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randint(0, 5, (1, 100))\n        model.translate(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_sortagrad_trainable_with_batch_frames(module):\n    args = make_arg(sortagrad=1)\n    idim = 6\n    odim = 5\n    dummy_json = make_dummy_json_mt(4, [10, 20], [10, 20], idim=idim, odim=odim)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_mt as m\n    else:\n        raise NotImplementedError\n    batch_frames_in = 20\n    batch_frames_out = 20\n    batchset = make_batchset(\n        dummy_json,\n        batch_frames_in=batch_frames_in,\n        batch_frames_out=batch_frames_out,\n        shortest_first=True,\n        mt=True,\n        iaxis=1,\n        oaxis=0,\n    )\n    for batch in batchset:\n        i = 0\n        o = 0\n        for uttid, info in batch:\n            i += int(info[""output""][1][""shape""][0])\n            o += int(info[""output""][0][""shape""][0])\n        assert i <= batch_frames_in\n        assert o <= batch_frames_out\n\n    model = m.E2E(6, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=6, odim=5))\n        loss.backward()\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randint(0, 5, (1, 100))\n        model.translate(in_data, args, args.char_list)\n\n\ndef init_torch_weight_const(m, val):\n    for p in m.parameters():\n        if p.dim() > 1:\n            p.data.fill_(val)\n\n\n@pytest.mark.parametrize(""etype"", [""blstm""])\ndef test_loss(etype):\n    # ch = importlib.import_module(\'espnet.nets.chainer_backend.e2e_mt\')\n    th = importlib.import_module(""espnet.nets.pytorch_backend.e2e_mt"")\n    args = make_arg(etype=etype)\n    th_model = th.E2E(6, 5, args)\n\n    const = 1e-4\n    init_torch_weight_const(th_model, const)\n\n    th_batch = prepare_inputs(""pytorch"")\n\n    th_model(*th_batch)\n    th_att = th_model.loss\n\n    th_model.zero_grad()\n\n    th_model(*th_batch)\n    th_att = th_model.loss\n    th_att.backward()\n\n\n@pytest.mark.parametrize(""etype"", [""blstm""])\ndef test_zero_length_target(etype):\n    th = importlib.import_module(""espnet.nets.pytorch_backend.e2e_mt"")\n    args = make_arg(etype=etype)\n    th_model = th.E2E(6, 5, args)\n\n    th_batch = prepare_inputs(""pytorch"", olens=[4, 0])\n\n    th_model(*th_batch)\n\n    # NOTE: We ignore all zero length case because chainer also fails.\n    # Have a nice data-prep!\n    # out_data = """"\n    # data = [\n    #     (""aaa"",\n    #      dict(feat=np.random.randint(0, 5, (1, 200)).astype(np.float32), tokenid="""")),\n    #     (""bbb"",\n    #      dict(feat=np.random.randint(0, 5, (1, 100)).astype(np.float32), tokenid="""")),\n    #     (""cc"",\n    #      dict(feat=np.random.randint(0, 5, (1, 100)).astype(np.float32), tokenid=""""))\n    # ]\n    # th_ctc, th_att, th_acc = th_model(data)\n\n\n@pytest.mark.parametrize(\n    ""module, atype"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_mt"", ""noatt""),\n        (""espnet.nets.pytorch_backend.e2e_mt"", ""dot""),\n        (""espnet.nets.pytorch_backend.e2e_mt"", ""add""),\n        (""espnet.nets.pytorch_backend.e2e_mt"", ""coverage""),\n        (""espnet.nets.pytorch_backend.e2e_mt"", ""multi_head_dot""),\n        (""espnet.nets.pytorch_backend.e2e_mt"", ""multi_head_add""),\n    ],\n)\ndef test_calculate_all_attentions(module, atype):\n    m = importlib.import_module(module)\n    args = make_arg(atype=atype)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"")\n    else:\n        raise NotImplementedError\n    model = m.E2E(6, 5, args)\n    with chainer.no_backprop_mode():\n        if ""pytorch"" in module:\n            att_ws = model.calculate_all_attentions(*batch)[0]\n        else:\n            raise NotImplementedError\n        print(att_ws.shape)\n\n\ndef test_torch_save_and_load():\n    m = importlib.import_module(""espnet.nets.pytorch_backend.e2e_mt"")\n    utils = importlib.import_module(""espnet.asr.asr_utils"")\n    args = make_arg()\n    model = m.E2E(6, 5, args)\n    # initialize randomly\n    for p in model.parameters():\n        p.data.uniform_()\n    if not os.path.exists("".pytest_cache""):\n        os.makedirs("".pytest_cache"")\n    tmppath = tempfile.mktemp()\n    utils.torch_save(tmppath, model)\n    p_saved = [p.data.numpy() for p in model.parameters()]\n    # set constant value\n    for p in model.parameters():\n        p.data.zero_()\n    utils.torch_load(tmppath, model)\n    for p1, p2 in zip(p_saved, model.parameters()):\n        np.testing.assert_array_equal(p1, p2.data.numpy())\n    if os.path.exists(tmppath):\n        os.remove(tmppath)\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available() and not chainer.cuda.available, reason=""gpu required""\n)\n@pytest.mark.parametrize(""module"", [""espnet.nets.pytorch_backend.e2e_mt""])\ndef test_gpu_trainable(module):\n    m = importlib.import_module(module)\n    args = make_arg()\n    model = m.E2E(6, 5, args)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"", is_cuda=True)\n        model.cuda()\n    else:\n        raise NotImplementedError\n    loss = model(*batch)\n    if isinstance(loss, tuple):\n        # chainer return several values as tuple\n        loss[0].backward()  # trainable\n    else:\n        loss.backward()  # trainable\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(""module"", [""espnet.nets.pytorch_backend.e2e_mt""])\ndef test_multi_gpu_trainable(module):\n    m = importlib.import_module(module)\n    ngpu = 2\n    device_ids = list(range(ngpu))\n    args = make_arg()\n    model = m.E2E(6, 5, args)\n    if ""pytorch"" in module:\n        model = torch.nn.DataParallel(model, device_ids)\n        batch = prepare_inputs(""pytorch"", is_cuda=True)\n        model.cuda()\n        loss = 1.0 / ngpu * model(*batch)\n        loss.backward(loss.new_ones(ngpu))  # trainable\n    else:\n        raise NotImplementedError\n'"
test/test_e2e_mt_transformer.py,5,"b'# coding: utf-8\n\n# Copyright 2019 Hirofumi Inaguma\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport importlib\nimport logging\nimport numpy\nimport pytest\nimport torch\n\nfrom test.test_e2e_asr_transformer import run_transformer_copy\nfrom test.test_e2e_asr_transformer import subsequent_mask\n\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_mask(module):\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_mt_transformer"".format(module)\n    )\n    m = T.subsequent_mask(3)\n    print(m)\n    print(subsequent_mask(3))\n    assert (m.unsqueeze(0) == subsequent_mask(3)).all()\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        adim=16,\n        aheads=2,\n        dropout_rate=0.0,\n        transformer_attn_dropout_rate=None,\n        elayers=2,\n        eunits=16,\n        dlayers=2,\n        dunits=16,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        transformer_init=""pytorch"",\n        transformer_input_layer=""conv2d"",\n        transformer_length_normalized_loss=True,\n        report_bleu=False,\n        lsm_weight=0.001,\n        char_list=[""<blank>"", ""a"", ""e"", ""i"", ""o"", ""u""],\n        tie_src_tgt_embedding=False,\n        tie_classifier=False,\n        multilingual=False,\n        replace_sos=False,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef prepare(backend, args):\n    idim = 5\n    odim = 5\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_mt_transformer"".format(backend)\n    )\n\n    model = T.E2E(idim, odim, args)\n    batchsize = 5\n    n_token = odim - 1\n    if backend == ""pytorch"":\n        y_src = (torch.randn(batchsize, 10) * n_token % n_token).long() + 1\n        y_tgt = (torch.randn(batchsize, 11) * n_token % n_token).long() + 1\n        # NOTE: + 1 to avoid to assign idx:0\n    else:\n        y_src = numpy.random.randn(batchsize, 10, idim).astype(numpy.int64) + 1\n        y_tgt = numpy.random.randn(batchsize, 11, idim).astype(numpy.int64) + 1\n    ilens = [3, 9, 10, 2, 3]\n    olens = [4, 10, 11, 3, 4]\n    for i in range(batchsize):\n        y_src[i, ilens[i] :] = model.pad\n        y_tgt[i, olens[i] :] = model.ignore_id\n\n    data = []\n    for i in range(batchsize):\n        data.append(\n            (\n                ""utt%d"" % i,\n                {""input"": [{""shape"": [ilens[i]]}], ""output"": [{""shape"": [olens[i]]}]},\n            )\n        )\n    if backend == ""pytorch"":\n        return model, y_src, torch.tensor(ilens), y_tgt, data\n    else:\n        return model, y_src, ilens, y_tgt, data\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_transformer_mask(module):\n    args = make_arg()\n    model, y_src, ilens, y_tgt, data = prepare(module, args)\n    from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n    from espnet.nets.pytorch_backend.transformer.mask import target_mask\n\n    yi, yo = add_sos_eos(y_tgt, model.sos, model.eos, model.ignore_id)\n    y_mask = target_mask(yi, model.ignore_id)\n    y_tgt = model.decoder.embed(yi)\n    y_tgt[0, 3:] = float(""nan"")\n    a = model.decoder.decoders[0].self_attn\n    a(y_tgt, y_tgt, y_tgt, y_mask)\n    assert not numpy.isnan(a.attn[0, :, :3, :3].detach().numpy()).any()\n\n\n@pytest.mark.parametrize(\n    ""module, model_dict"",\n    [\n        (""pytorch"", {}),\n        (""pytorch"", {""report_bleu"": True}),\n        (""pytorch"", {""tie_src_tgt_embedding"": True}),\n        (""pytorch"", {""tie_classifier"": True}),\n        (""pytorch"", {""tie_src_tgt_embedding"": True, ""tie_classifier"": True}),\n    ],\n)\ndef test_transformer_trainable_and_decodable(module, model_dict):\n    args = make_arg(**model_dict)\n    model, y_src, ilens, y_tgt, data = prepare(module, args)\n\n    # test beam search\n    trans_args = argparse.Namespace(\n        beam_size=1,\n        penalty=0.0,\n        ctc_weight=0.0,\n        maxlenratio=1.0,\n        lm_weight=0,\n        minlenratio=0,\n        nbest=1,\n        tgt_lang=False,\n    )\n    if module == ""pytorch"":\n        # test trainable\n        optim = torch.optim.Adam(model.parameters(), 0.01)\n        loss = model(y_src, ilens, y_tgt)\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n        # test attention plot\n        attn_dict = model.calculate_all_attentions(y_src[0:1], ilens[0:1], y_tgt[0:1])\n        from espnet.nets.pytorch_backend.transformer import plot\n\n        plot.plot_multi_head_attention(data, attn_dict, ""/tmp/espnet-test"")\n\n        # test decodable\n        with torch.no_grad():\n            nbest = model.translate(\n                [y_src[0, : ilens[0]].numpy()], trans_args, args.char_list\n            )\n            print(y_tgt[0])\n            print(nbest[0][""yseq""][1:-1])\n    else:\n        raise NotImplementedError\n\n\nif __name__ == ""__main__"":\n    run_transformer_copy()\n'"
test/test_e2e_st.py,15,"b'# coding: utf-8\n\n# Copyright 2019 Hirofumi Inaguma\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport argparse\nimport importlib\nimport os\nimport tempfile\n\nimport chainer\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.utils.training.batchfy import make_batchset\nfrom test.utils_test import make_dummy_json_st\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        elayers=1,\n        subsample=""1_2_2_1_1"",\n        etype=""vggblstm"",\n        eunits=16,\n        eprojs=8,\n        dtype=""lstm"",\n        dlayers=1,\n        dunits=16,\n        atype=""add"",\n        aheads=2,\n        awin=5,\n        aconv_chans=4,\n        aconv_filts=10,\n        mtlalpha=0.0,\n        lsm_type="""",\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        adim=16,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        nbest=5,\n        beam_size=2,\n        penalty=0.5,\n        maxlenratio=1.0,\n        minlenratio=0.0,\n        ctc_weight=0.0,\n        ctc_window_margin=0,  # dummy\n        lm_weight=0.0,\n        rnnlm=None,\n        streaming_min_blank_dur=10,\n        streaming_onset_margin=2,\n        streaming_offset_margin=2,\n        verbose=2,\n        char_list=[u""\xe3\x81\x82"", u""\xe3\x81\x84"", u""\xe3\x81\x86"", u""\xe3\x81\x88"", u""\xe3\x81\x8a""],\n        outdir=None,\n        ctc_type=""warpctc"",\n        report_bleu=False,\n        report_cer=False,\n        report_wer=False,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        sortagrad=0,\n        grad_noise=False,\n        context_residual=False,\n        multilingual=False,\n        replace_sos=False,\n        tgt_lang=False,\n        asr_weight=0.0,\n        mt_weight=0.0,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef prepare_inputs(\n    mode, ilens=[20, 15], olens_tgt=[4, 3], olens_src=[3, 2], is_cuda=False\n):\n    np.random.seed(1)\n    assert len(ilens) == len(olens_tgt)\n    xs = [np.random.randn(ilen, 40).astype(np.float32) for ilen in ilens]\n    ys_tgt = [np.random.randint(1, 5, olen).astype(np.int32) for olen in olens_tgt]\n    ys_src = [np.random.randint(1, 5, olen).astype(np.int32) for olen in olens_src]\n    ilens = np.array([x.shape[0] for x in xs], dtype=np.int32)\n\n    if mode == ""chainer"":\n        raise NotImplementedError\n\n    elif mode == ""pytorch"":\n        ilens = torch.from_numpy(ilens).long()\n        xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0)\n        ys_pad_tgt = pad_list([torch.from_numpy(y).long() for y in ys_tgt], -1)\n        ys_pad_src = pad_list([torch.from_numpy(y).long() for y in ys_src], -1)\n        if is_cuda:\n            xs_pad = xs_pad.cuda()\n            ilens = ilens.cuda()\n            ys_pad_tgt = ys_pad_tgt.cuda()\n            ys_pad_src = ys_pad_src.cuda()\n\n        return xs_pad, ilens, ys_pad_tgt, ys_pad_src\n    else:\n        raise ValueError(""Invalid mode"")\n\n\ndef convert_batch(batch, backend=""pytorch"", is_cuda=False, idim=40, odim=5):\n    ilens = np.array([x[1][""input""][0][""shape""][0] for x in batch])\n    olens_tgt = np.array([x[1][""output""][0][""shape""][0] for x in batch])\n    olens_src = np.array([x[1][""output""][1][""shape""][0] for x in batch])\n    xs = [np.random.randn(ilen, idim).astype(np.float32) for ilen in ilens]\n    ys_tgt = [np.random.randint(1, odim, olen).astype(np.int32) for olen in olens_tgt]\n    ys_src = [np.random.randint(1, odim, olen).astype(np.int32) for olen in olens_src]\n    is_pytorch = backend == ""pytorch""\n    if is_pytorch:\n        xs = pad_list([torch.from_numpy(x).float() for x in xs], 0)\n        ilens = torch.from_numpy(ilens).long()\n        ys_tgt = pad_list([torch.from_numpy(y).long() for y in ys_tgt], -1)\n        ys_src = pad_list([torch.from_numpy(y).long() for y in ys_src], -1)\n\n        if is_cuda:\n            xs = xs.cuda()\n            ilens = ilens.cuda()\n            ys_tgt = ys_tgt.cuda()\n            ys_src = ys_src.cuda()\n    else:\n        raise NotImplementedError\n\n    return xs, ilens, ys_tgt, ys_src\n\n\n@pytest.mark.parametrize(\n    ""module, model_dict"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_st"", {}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""elayers"": 2, ""dlayers"": 2}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""grup""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""lstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""bgrup""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""blstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""bgru""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""blstm""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vgggru""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vgggrup""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vgglstm""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vgglstmp""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vggbgru""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vggbgrup""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vggblstmp"", ""dtype"": ""gru""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""noatt""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vggblstmp"", ""atype"": ""add""}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""etype"": ""vggblstmp"", ""atype"": ""dot""}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""coverage""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""coverage_location""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""location2d""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""location_recurrent""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_dot""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_add""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_loc""},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""etype"": ""vggblstmp"", ""atype"": ""multi_head_multi_res_loc""},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""asr_weight"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""asr_weight"": 0.2}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""mt_weight"": 0.0}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""mt_weight"": 0.2}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""asr_weight"": 0.2, ""mtlalpha"": 0.0, ""mt_weight"": 0.2},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""asr_weight"": 0.2, ""mtlalpha"": 0.5, ""mt_weight"": 0.2},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""asr_weight"": 0.2, ""mtlalpha"": 1.0, ""mt_weight"": 0.2},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""sampling_probability"": 0.5}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""context_residual"": True}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""grad_noise"": True}),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""report_cer"": True, ""asr_weight"": 0.0}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_cer"": True, ""asr_weight"": 0.5, ""mtlalpha"": 0.0},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_cer"": True, ""asr_weight"": 0.5, ""mtlalpha"": 0.5},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_cer"": True, ""asr_weight"": 0.5, ""mtlalpha"": 1.0},\n        ),\n        (""espnet.nets.pytorch_backend.e2e_st"", {""report_wer"": True, ""asr_weight"": 0.0}),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_wer"": True, ""asr_weight"": 0.5, ""mtlalpha"": 0.0},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_wer"": True, ""asr_weight"": 0.5, ""mtlalpha"": 0.5},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_wer"": True, ""asr_weight"": 0.5, ""mtlalpha"": 1.0},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_cer"": True, ""report_wer"": True},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {""report_cer"": True, ""report_wer"": True, ""asr_weight"": 0.0},\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {\n                ""report_cer"": True,\n                ""report_wer"": True,\n                ""asr_weight"": 0.5,\n                ""mtlalpha"": 0.0,\n            },\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {\n                ""report_cer"": True,\n                ""report_wer"": True,\n                ""asr_weight"": 0.5,\n                ""mtlalpha"": 0.5,\n            },\n        ),\n        (\n            ""espnet.nets.pytorch_backend.e2e_st"",\n            {\n                ""report_cer"": True,\n                ""report_wer"": True,\n                ""asr_weight"": 0.5,\n                ""mtlalpha"": 1.0,\n            },\n        ),\n    ],\n)\ndef test_model_trainable_and_decodable(module, model_dict):\n    args = make_arg(**model_dict)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"")\n    else:\n        raise NotImplementedError\n\n    m = importlib.import_module(module)\n    model = m.E2E(40, 5, args)\n    loss = model(*batch)\n    if isinstance(loss, tuple):\n        # chainer return several values as tuple\n        loss[0].backward()  # trainable\n    else:\n        loss.backward()  # trainable\n\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(10, 40)\n        model.translate(in_data, args, args.char_list)  # decodable\n        if ""pytorch"" in module:\n            batch_in_data = [np.random.randn(10, 40), np.random.randn(5, 40)]\n            model.translate_batch(\n                batch_in_data, args, args.char_list\n            )  # batch decodable\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_gradient_noise_injection(module):\n    args = make_arg(grad_noise=True)\n    args_org = make_arg()\n    dummy_json = make_dummy_json_st(2, [10, 20], [10, 20], [10, 20], idim=20, odim=5)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_st as m\n    else:\n        raise NotImplementedError\n    batchset = make_batchset(dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True)\n    model = m.E2E(20, 5, args)\n    model_org = m.E2E(20, 5, args_org)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        loss_org = model_org(*convert_batch(batch, module, idim=20, odim=5))\n        loss.backward()\n        grad = [param.grad for param in model.parameters()][10]\n        loss_org.backward()\n        grad_org = [param.grad for param in model_org.parameters()][10]\n        assert grad[0] != grad_org[0]\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_sortagrad_trainable(module):\n    args = make_arg(sortagrad=1)\n    dummy_json = make_dummy_json_st(4, [10, 20], [10, 20], [10, 20], idim=20, odim=5)\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_st as m\n    else:\n        raise NotImplementedError\n    batchset = make_batchset(dummy_json, 2, 2 ** 10, 2 ** 10, shortest_first=True)\n    model = m.E2E(20, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(50, 20)\n        model.translate(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_sortagrad_trainable_with_batch_bins(module):\n    args = make_arg(sortagrad=1)\n    idim = 20\n    odim = 5\n    dummy_json = make_dummy_json_st(\n        4, [10, 20], [10, 20], [10, 20], idim=idim, odim=odim\n    )\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_st as m\n    else:\n        raise NotImplementedError\n    batch_elems = 2000\n    batchset = make_batchset(dummy_json, batch_bins=batch_elems, shortest_first=True)\n    for batch in batchset:\n        n = 0\n        for uttid, info in batch:\n            ilen = int(info[""input""][0][""shape""][0])\n            olen = int(info[""output""][0][""shape""][0])\n            n += ilen * idim + olen * odim\n        assert olen < batch_elems\n\n    model = m.E2E(20, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(100, 20)\n        model.translate(in_data, args, args.char_list)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_sortagrad_trainable_with_batch_frames(module):\n    args = make_arg(sortagrad=1)\n    idim = 20\n    odim = 5\n    dummy_json = make_dummy_json_st(\n        4, [10, 20], [10, 20], [10, 20], idim=idim, odim=odim\n    )\n    if module == ""pytorch"":\n        import espnet.nets.pytorch_backend.e2e_st as m\n    else:\n        raise NotImplementedError\n    batch_frames_in = 50\n    batch_frames_out = 50\n    batchset = make_batchset(\n        dummy_json,\n        batch_frames_in=batch_frames_in,\n        batch_frames_out=batch_frames_out,\n        shortest_first=True,\n    )\n    for batch in batchset:\n        i = 0\n        o = 0\n        for uttid, info in batch:\n            i += int(info[""input""][0][""shape""][0])\n            o += int(info[""output""][0][""shape""][0])\n        assert i <= batch_frames_in\n        assert o <= batch_frames_out\n\n    model = m.E2E(20, 5, args)\n    for batch in batchset:\n        loss = model(*convert_batch(batch, module, idim=20, odim=5))\n        if isinstance(loss, tuple):\n            # chainer return several values as tuple\n            loss[0].backward()  # trainable\n        else:\n            loss.backward()  # trainable\n    with torch.no_grad(), chainer.no_backprop_mode():\n        in_data = np.random.randn(100, 20)\n        model.translate(in_data, args, args.char_list)\n\n\ndef init_torch_weight_const(m, val):\n    for p in m.parameters():\n        if p.dim() > 1:\n            p.data.fill_(val)\n\n\ndef init_chainer_weight_const(m, val):\n    for p in m.params():\n        if p.data.ndim > 1:\n            p.data[:] = val\n\n\n@pytest.mark.parametrize(""etype"", [""blstmp"", ""vggblstmp""])\ndef test_mtl_loss(etype):\n    th = importlib.import_module(""espnet.nets.pytorch_backend.e2e_st"")\n    args = make_arg(etype=etype)\n    th_model = th.E2E(40, 5, args)\n\n    const = 1e-4\n    init_torch_weight_const(th_model, const)\n\n    th_batch = prepare_inputs(""pytorch"")\n\n    th_model(*th_batch)\n    th_asr, th_st = th_model.loss_asr, th_model.loss_st\n\n    # test grads in mtl mode\n    th_loss = th_asr * 0.5 + th_st * 0.5\n    th_model.zero_grad()\n    th_loss.backward()\n\n\n@pytest.mark.parametrize(""etype"", [""blstmp"", ""vggblstmp""])\ndef test_zero_length_target(etype):\n    th = importlib.import_module(""espnet.nets.pytorch_backend.e2e_st"")\n    args = make_arg(etype=etype)\n    th_model = th.E2E(40, 5, args)\n\n    th_batch = prepare_inputs(""pytorch"", olens_tgt=[4, 0], olens_src=[3, 0])\n\n    th_model(*th_batch)\n\n    # NOTE: We ignore all zero length case because chainer also fails.\n    # Have a nice data-prep!\n    # out_data = """"\n    # data = [\n    #     (""aaa"", dict(feat=np.random.randn(200, 40).astype(np.float32), tokenid="""")),\n    #     (""bbb"", dict(feat=np.random.randn(100, 40).astype(np.float32), tokenid="""")),\n    #     (""cc"", dict(feat=np.random.randn(100, 40).astype(np.float32), tokenid=""""))\n    # ]\n    # th_asr, th_st, th_acc = th_model(data)\n\n\n@pytest.mark.parametrize(\n    ""module, atype"",\n    [\n        (""espnet.nets.pytorch_backend.e2e_st"", ""noatt""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""dot""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""add""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""location""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""coverage""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""coverage_location""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""location2d""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""location_recurrent""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""multi_head_dot""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""multi_head_add""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""multi_head_loc""),\n        (""espnet.nets.pytorch_backend.e2e_st"", ""multi_head_multi_res_loc""),\n    ],\n)\ndef test_calculate_all_attentions(module, atype):\n    m = importlib.import_module(module)\n    args = make_arg(atype=atype)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"")\n    else:\n        raise NotImplementedError\n    model = m.E2E(40, 5, args)\n    with chainer.no_backprop_mode():\n        if ""pytorch"" in module:\n            att_ws = model.calculate_all_attentions(*batch)[0]\n        else:\n            raise NotImplementedError\n        print(att_ws.shape)\n\n\ndef test_torch_save_and_load():\n    m = importlib.import_module(""espnet.nets.pytorch_backend.e2e_st"")\n    utils = importlib.import_module(""espnet.asr.asr_utils"")\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    # initialize randomly\n    for p in model.parameters():\n        p.data.uniform_()\n    if not os.path.exists("".pytest_cache""):\n        os.makedirs("".pytest_cache"")\n    tmppath = tempfile.mktemp()\n    utils.torch_save(tmppath, model)\n    p_saved = [p.data.numpy() for p in model.parameters()]\n    # set constant value\n    for p in model.parameters():\n        p.data.zero_()\n    utils.torch_load(tmppath, model)\n    for p1, p2 in zip(p_saved, model.parameters()):\n        np.testing.assert_array_equal(p1, p2.data.numpy())\n    if os.path.exists(tmppath):\n        os.remove(tmppath)\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available() and not chainer.cuda.available, reason=""gpu required""\n)\n@pytest.mark.parametrize(""module"", [""espnet.nets.pytorch_backend.e2e_st""])\ndef test_gpu_trainable(module):\n    m = importlib.import_module(module)\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    if ""pytorch"" in module:\n        batch = prepare_inputs(""pytorch"", is_cuda=True)\n        model.cuda()\n    else:\n        raise NotImplementedError\n    loss = model(*batch)\n    if isinstance(loss, tuple):\n        # chainer return several values as tuple\n        loss[0].backward()  # trainable\n    else:\n        loss.backward()  # trainable\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(""module"", [""espnet.nets.pytorch_backend.e2e_st""])\ndef test_multi_gpu_trainable(module):\n    m = importlib.import_module(module)\n    ngpu = 2\n    device_ids = list(range(ngpu))\n    args = make_arg()\n    model = m.E2E(40, 5, args)\n    if ""pytorch"" in module:\n        model = torch.nn.DataParallel(model, device_ids)\n        batch = prepare_inputs(""pytorch"", is_cuda=True)\n        model.cuda()\n        loss = 1.0 / ngpu * model(*batch)\n        loss.backward(loss.new_ones(ngpu))  # trainable\n    else:\n        raise NotImplementedError\n'"
test/test_e2e_st_transformer.py,6,"b'# coding: utf-8\n\n# Copyright 2019 Hirofumi Inaguma\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport importlib\nimport logging\nimport numpy\nimport pytest\nimport torch\n\nfrom test.test_e2e_asr_transformer import run_transformer_copy\nfrom test.test_e2e_asr_transformer import subsequent_mask\n\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n)\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_mask(module):\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_st_transformer"".format(module)\n    )\n    m = T.subsequent_mask(3)\n    print(m)\n    print(subsequent_mask(3))\n    assert (m.unsqueeze(0) == subsequent_mask(3)).all()\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        adim=16,\n        aheads=2,\n        dropout_rate=0.0,\n        transformer_attn_dropout_rate=None,\n        elayers=2,\n        eunits=16,\n        dlayers=2,\n        dunits=16,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        transformer_init=""pytorch"",\n        transformer_input_layer=""conv2d"",\n        transformer_length_normalized_loss=True,\n        report_bleu=False,\n        report_cer=False,\n        report_wer=False,\n        mtlalpha=0.0,  # for CTC-ASR\n        lsm_weight=0.001,\n        char_list=[""<blank>"", ""a"", ""e"", ""i"", ""o"", ""u""],\n        ctc_type=""warpctc"",\n        asr_weight=0.0,\n        mt_weight=0.0,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef prepare(backend, args):\n    idim = 40\n    odim = 5\n    T = importlib.import_module(\n        ""espnet.nets.{}_backend.e2e_st_transformer"".format(backend)\n    )\n\n    model = T.E2E(idim, odim, args)\n    batchsize = 5\n    if backend == ""pytorch"":\n        x = torch.randn(batchsize, 40, idim)\n    else:\n        x = numpy.random.randn(batchsize, 40, idim).astype(numpy.float32)\n    ilens = [40, 30, 20, 15, 10]\n    n_token = odim - 1\n    if backend == ""pytorch"":\n        y_src = (torch.rand(batchsize, 10) * n_token % n_token).long()\n        y_tgt = (torch.rand(batchsize, 11) * n_token % n_token).long()\n    else:\n        y_src = (numpy.random.rand(batchsize, 10) * n_token % n_token).astype(\n            numpy.int32\n        )\n        y_tgt = (numpy.random.rand(batchsize, 11) * n_token % n_token).astype(\n            numpy.int32\n        )\n    olens = [3, 9, 10, 2, 3]\n    for i in range(batchsize):\n        x[i, ilens[i] :] = -1\n        y_tgt[i, olens[i] :] = model.ignore_id\n        y_src[i, olens[i] :] = model.ignore_id\n\n    data = []\n    for i in range(batchsize):\n        data.append(\n            (\n                ""utt%d"" % i,\n                {\n                    ""input"": [{""shape"": [ilens[i], idim]}],\n                    ""output"": [{""shape"": [olens[i]]}],\n                },\n            )\n        )\n    if backend == ""pytorch"":\n        return model, x, torch.tensor(ilens), y_tgt, y_src, data\n    else:\n        return model, x, ilens, y_tgt, y_src, data\n\n\n@pytest.mark.parametrize(""module"", [""pytorch""])\ndef test_transformer_mask(module):\n    args = make_arg()\n    model, x, ilens, y_tgt, y_src, data = prepare(module, args)\n    from espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\n    from espnet.nets.pytorch_backend.transformer.mask import target_mask\n\n    yi, yo = add_sos_eos(y_tgt, model.sos, model.eos, model.ignore_id)\n    y_mask = target_mask(yi, model.ignore_id)\n    y_tgt = model.decoder.embed(yi)\n    y_tgt[0, 3:] = float(""nan"")\n    a = model.decoder.decoders[0].self_attn\n    a(y_tgt, y_tgt, y_tgt, y_mask)\n    assert not numpy.isnan(a.attn[0, :, :3, :3].detach().numpy()).any()\n\n\n@pytest.mark.parametrize(\n    ""module, model_dict"",\n    [\n        (""pytorch"", {""asr_weight"": 0.0, ""mt_weight"": 0.0}),  # pure E2E-ST\n        (\n            ""pytorch"",\n            {""asr_weight"": 0.1, ""mtlalpha"": 0.0, ""mt_weight"": 0.0},\n        ),  # MTL w/ attention ASR\n        (\n            ""pytorch"",\n            {""asr_weight"": 0.1, ""mtlalpha"": 0.0, ""mt_weight"": 0.1},\n        ),  # MTL w/ attention ASR + MT\n        (\n            ""pytorch"",\n            {""asr_weight"": 0.1, ""mtlalpha"": 1.0, ""mt_weight"": 0.0},\n        ),  # MTL w/ CTC ASR\n        (""pytorch"", {""asr_weight"": 0.1, ""mtlalpha"": 1.0, ""ctc_type"": ""builtin""}),\n        (""pytorch"", {""asr_weight"": 0.1, ""mtlalpha"": 1.0, ""report_cer"": True}),\n        (""pytorch"", {""asr_weight"": 0.1, ""mtlalpha"": 1.0, ""report_wer"": True}),\n        (\n            ""pytorch"",\n            {\n                ""asr_weight"": 0.1,\n                ""mtlalpha"": 1.0,\n                ""report_cer"": True,\n                ""report_wer"": True,\n            },\n        ),\n        (\n            ""pytorch"",\n            {""asr_weight"": 0.1, ""mtlalpha"": 1.0, ""mt_weight"": 0.1},\n        ),  # MTL w/ CTC ASR + MT\n        (\n            ""pytorch"",\n            {""asr_weight"": 0.1, ""mtlalpha"": 0.5, ""mt_weight"": 0.0},\n        ),  # MTL w/ attention ASR + CTC ASR\n        (\n            ""pytorch"",\n            {""asr_weight"": 0.1, ""mtlalpha"": 0.5, ""mt_weight"": 0.1},\n        ),  # MTL w/ attention ASR + CTC ASR + MT\n    ],\n)\ndef test_transformer_trainable_and_decodable(module, model_dict):\n    args = make_arg(**model_dict)\n    model, x, ilens, y_tgt, y_src, data = prepare(module, args)\n\n    # test beam search\n    trans_args = argparse.Namespace(\n        beam_size=1,\n        penalty=0.0,\n        ctc_weight=0.0,\n        maxlenratio=1.0,\n        lm_weight=0,\n        minlenratio=0,\n        nbest=1,\n        tgt_lang=False,\n    )\n    if module == ""pytorch"":\n        # test trainable\n        optim = torch.optim.Adam(model.parameters(), 0.01)\n        loss = model(x, ilens, y_tgt, y_src)\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n        # test attention plot\n        attn_dict = model.calculate_all_attentions(\n            x[0:1], ilens[0:1], y_tgt[0:1], y_src[0:1]\n        )\n        from espnet.nets.pytorch_backend.transformer import plot\n\n        plot.plot_multi_head_attention(data, attn_dict, ""/tmp/espnet-test"")\n\n        # test decodable\n        with torch.no_grad():\n            nbest = model.translate(\n                x[0, : ilens[0]].numpy(), trans_args, args.char_list\n            )\n            print(y_tgt[0])\n            print(nbest[0][""yseq""][1:-1])\n    else:\n        raise NotImplementedError\n\n\nif __name__ == ""__main__"":\n    run_transformer_copy()\n'"
test/test_e2e_tts_fastspeech.py,23,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport json\nimport os\nimport shutil\nimport tempfile\n\nfrom argparse import Namespace\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.e2e_tts_fastspeech import FeedForwardTransformer\nfrom espnet.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2\nfrom espnet.nets.pytorch_backend.e2e_tts_transformer import Transformer\nfrom espnet.nets.pytorch_backend.fastspeech.duration_calculator import (\n    DurationCalculator,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.fastspeech.length_regulator import LengthRegulator\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\ndef prepare_inputs(\n    idim, odim, ilens, olens, spk_embed_dim=None, device=torch.device(""cpu"")\n):\n    xs = [np.random.randint(0, idim, lg) for lg in ilens]\n    ys = [np.random.randn(lg, odim) for lg in olens]\n    ilens = torch.LongTensor(ilens).to(device)\n    olens = torch.LongTensor(olens).to(device)\n    xs = pad_list([torch.from_numpy(x).long() for x in xs], 0).to(device)\n    ys = pad_list([torch.from_numpy(y).float() for y in ys], 0).to(device)\n    labels = ys.new_zeros(ys.size(0), ys.size(1))\n    for i, lg in enumerate(olens):\n        labels[i, lg - 1 :] = 1\n    batch = {\n        ""xs"": xs,\n        ""ilens"": ilens,\n        ""ys"": ys,\n        ""labels"": labels,\n        ""olens"": olens,\n    }\n\n    if spk_embed_dim is not None:\n        batch[""spembs""] = torch.FloatTensor(\n            np.random.randn(len(ilens), spk_embed_dim)\n        ).to(device)\n\n    return batch\n\n\ndef make_taco2_args(**kwargs):\n    defaults = dict(\n        model_module=""espnet.nets.pytorch_backend.e2e_tts_tacotron2:Tacotron2"",\n        use_speaker_embedding=False,\n        spk_embed_dim=None,\n        embed_dim=32,\n        elayers=1,\n        eunits=32,\n        econv_layers=2,\n        econv_filts=5,\n        econv_chans=32,\n        dlayers=2,\n        dunits=32,\n        prenet_layers=2,\n        prenet_units=32,\n        postnet_layers=2,\n        postnet_filts=5,\n        postnet_chans=32,\n        output_activation=None,\n        atype=""location"",\n        adim=32,\n        aconv_chans=16,\n        aconv_filts=5,\n        cumulate_att_w=True,\n        use_batch_norm=True,\n        use_concate=True,\n        use_residual=False,\n        dropout_rate=0.5,\n        zoneout_rate=0.1,\n        reduction_factor=1,\n        threshold=0.5,\n        maxlenratio=5.0,\n        minlenratio=0.0,\n        use_cbhg=False,\n        spc_dim=None,\n        cbhg_conv_bank_layers=4,\n        cbhg_conv_bank_chans=32,\n        cbhg_conv_proj_filts=3,\n        cbhg_conv_proj_chans=32,\n        cbhg_highway_layers=4,\n        cbhg_highway_units=32,\n        cbhg_gru_units=32,\n        use_masking=True,\n        use_weighted_masking=False,\n        bce_pos_weight=1.0,\n        use_guided_attn_loss=False,\n        guided_attn_loss_sigma=0.4,\n        guided_attn_loss_lambda=1.0,\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_transformer_args(**kwargs):\n    defaults = dict(\n        model_module=""espnet.nets.pytorch_backend.e2e_tts_transformer:Transformer"",\n        embed_dim=0,\n        spk_embed_dim=None,\n        eprenet_conv_layers=0,\n        eprenet_conv_filts=0,\n        eprenet_conv_chans=0,\n        dprenet_layers=2,\n        dprenet_units=64,\n        adim=32,\n        aheads=4,\n        elayers=2,\n        eunits=32,\n        dlayers=2,\n        dunits=32,\n        positionwise_layer_type=""linear"",\n        positionwise_conv_kernel_size=1,\n        postnet_layers=2,\n        postnet_filts=5,\n        postnet_chans=32,\n        eprenet_dropout_rate=0.1,\n        dprenet_dropout_rate=0.5,\n        postnet_dropout_rate=0.1,\n        transformer_enc_dropout_rate=0.1,\n        transformer_enc_positional_dropout_rate=0.1,\n        transformer_enc_attn_dropout_rate=0.0,\n        transformer_dec_dropout_rate=0.1,\n        transformer_dec_positional_dropout_rate=0.1,\n        transformer_dec_attn_dropout_rate=0.3,\n        transformer_enc_dec_attn_dropout_rate=0.0,\n        spk_embed_integration_type=""add"",\n        use_masking=True,\n        use_weighted_masking=False,\n        bce_pos_weight=1.0,\n        use_batch_norm=True,\n        use_scaled_pos_enc=True,\n        encoder_normalize_before=True,\n        decoder_normalize_before=True,\n        encoder_concat_after=False,\n        decoder_concat_after=False,\n        transformer_init=""pytorch"",\n        initial_encoder_alpha=1.0,\n        initial_decoder_alpha=1.0,\n        reduction_factor=1,\n        loss_type=""L1"",\n        use_guided_attn_loss=False,\n        num_heads_applied_guided_attn=2,\n        num_layers_applied_guided_attn=2,\n        guided_attn_loss_sigma=0.4,\n        modules_applied_guided_attn=[""encoder"", ""decoder"", ""encoder-decoder""],\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_feedforward_transformer_args(**kwargs):\n    defaults = dict(\n        spk_embed_dim=None,\n        adim=32,\n        aheads=4,\n        elayers=2,\n        eunits=32,\n        dlayers=2,\n        dunits=32,\n        duration_predictor_layers=2,\n        duration_predictor_chans=64,\n        duration_predictor_kernel_size=3,\n        duration_predictor_dropout_rate=0.1,\n        positionwise_layer_type=""linear"",\n        positionwise_conv_kernel_size=1,\n        postnet_layers=0,\n        postnet_filts=5,\n        postnet_chans=32,\n        transformer_enc_dropout_rate=0.1,\n        transformer_enc_positional_dropout_rate=0.1,\n        transformer_enc_attn_dropout_rate=0.0,\n        transformer_dec_dropout_rate=0.1,\n        transformer_dec_positional_dropout_rate=0.1,\n        transformer_dec_attn_dropout_rate=0.3,\n        transformer_enc_dec_attn_dropout_rate=0.0,\n        spk_embed_integration_type=""add"",\n        use_masking=True,\n        use_weighted_masking=False,\n        use_scaled_pos_enc=True,\n        encoder_normalize_before=True,\n        decoder_normalize_before=True,\n        encoder_concat_after=False,\n        decoder_concat_after=False,\n        transformer_init=""pytorch"",\n        initial_encoder_alpha=1.0,\n        initial_decoder_alpha=1.0,\n        transfer_encoder_from_teacher=False,\n        transferred_encoder_module=""all"",\n        reduction_factor=1,\n        teacher_model=None,\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\n@pytest.mark.parametrize(\n    ""teacher_type, model_dict"",\n    [\n        (""transformer"", {}),\n        (""transformer"", {""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""add""}),\n        (""transformer"", {""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""concat""}),\n        (""transformer"", {""use_masking"": False}),\n        (""transformer"", {""use_scaled_pos_enc"": False}),\n        (\n            ""transformer"",\n            {""positionwise_layer_type"": ""conv1d"", ""positionwise_conv_kernel_size"": 3},\n        ),\n        (\n            ""transformer"",\n            {\n                ""positionwise_layer_type"": ""conv1d-linear"",\n                ""positionwise_conv_kernel_size"": 3,\n            },\n        ),\n        (""transformer"", {""encoder_normalize_before"": False}),\n        (""transformer"", {""decoder_normalize_before"": False}),\n        (\n            ""transformer"",\n            {""encoder_normalize_before"": False, ""decoder_normalize_before"": False},\n        ),\n        (""transformer"", {""encoder_concat_after"": True}),\n        (""transformer"", {""decoder_concat_after"": True}),\n        (""transformer"", {""encoder_concat_after"": True, ""decoder_concat_after"": True}),\n        (""transformer"", {""transfer_encoder_from_teacher"": True}),\n        (\n            ""transformer"",\n            {\n                ""transfer_encoder_from_teacher"": True,\n                ""transferred_encoder_module"": ""embed"",\n            },\n        ),\n        (""transformer"", {""use_masking"": False}),\n        (""transformer"", {""use_masking"": False, ""use_weighted_masking"": True}),\n        (""transformer"", {""postnet_layers"": 2}),\n        (""transformer"", {""reduction_factor"": 2}),\n        (""transformer"", {""reduction_factor"": 3}),\n        (""transformer"", {""reduction_factor"": 4}),\n        (""transformer"", {""reduction_factor"": 5}),\n        (""tacotron2"", {}),\n        (""tacotron2"", {""spk_embed_dim"": 16}),\n        (""tacotron2"", {""reduction_factor"": 2}),\n        (""tacotron2"", {""reduction_factor"": 3}),\n        (""tacotron2"", {""reduction_factor"": 4}),\n        (""tacotron2"", {""reduction_factor"": 5}),\n    ],\n)\ndef test_fastspeech_trainable_and_decodable(teacher_type, model_dict):\n    # make args\n    idim, odim = 10, 25\n    model_args = make_feedforward_transformer_args(**model_dict)\n\n    # setup batch\n    ilens = [10, 5]\n    olens = [20, 15]\n    batch = prepare_inputs(idim, odim, ilens, olens, model_args[""spk_embed_dim""])\n\n    # define teacher model and save it\n    if teacher_type == ""transformer"":\n        teacher_model_args = make_transformer_args(**model_dict)\n        teacher_model = Transformer(idim, odim, Namespace(**teacher_model_args))\n    elif teacher_type == ""tacotron2"":\n        teacher_model_args = make_taco2_args(**model_dict)\n        teacher_model = Tacotron2(idim, odim, Namespace(**teacher_model_args))\n    else:\n        raise ValueError()\n    tmpdir = tempfile.mkdtemp(prefix=""tmp_"", dir=""/tmp"")\n    torch.save(teacher_model.state_dict(), tmpdir + ""/model.dummy.best"")\n    with open(tmpdir + ""/model.json"", ""wb"") as f:\n        f.write(\n            json.dumps(\n                (idim, odim, teacher_model_args),\n                indent=4,\n                ensure_ascii=False,\n                sort_keys=True,\n            ).encode(""utf_8"")\n        )\n\n    # define model\n    model_args[""teacher_model""] = tmpdir + ""/model.dummy.best""\n    model = FeedForwardTransformer(idim, odim, Namespace(**model_args))\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # decodable\n    model.eval()\n    with torch.no_grad():\n        if model_args[""spk_embed_dim""] is None:\n            spemb = None\n        else:\n            spemb = batch[""spembs""][0]\n        model.inference(batch[""xs""][0][: batch[""ilens""][0]], None, spemb=spemb)\n        model.calculate_all_attentions(**batch)\n\n    # remove tmpdir\n    if os.path.exists(tmpdir):\n        shutil.rmtree(tmpdir)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""gpu required"")\n@pytest.mark.parametrize(\n    ""teacher_type, model_dict"",\n    [\n        (""transformer"", {}),\n        (""transformer"", {""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""add""}),\n        (""transformer"", {""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""concat""}),\n        (""transformer"", {""use_masking"": False}),\n        (""transformer"", {""use_masking"": False, ""use_weighted_masking"": True}),\n        (""transformer"", {""use_scaled_pos_enc"": False}),\n        (""transformer"", {""encoder_normalize_before"": False}),\n        (""transformer"", {""decoder_normalize_before"": False}),\n        (\n            ""transformer"",\n            {""encoder_normalize_before"": False, ""decoder_normalize_before"": False},\n        ),\n        (""transformer"", {""encoder_concat_after"": True}),\n        (""transformer"", {""decoder_concat_after"": True}),\n        (""transformer"", {""encoder_concat_after"": True, ""decoder_concat_after"": True}),\n        (""transformer"", {""transfer_encoder_from_teacher"": True}),\n        (\n            ""transformer"",\n            {\n                ""transfer_encoder_from_teacher"": True,\n                ""transferred_encoder_module"": ""embed"",\n            },\n        ),\n        (""tacotron2"", {}),\n        (""tacotron2"", {""spk_embed_dim"": 16}),\n    ],\n)\ndef test_fastspeech_gpu_trainable_and_decodable(teacher_type, model_dict):\n    # make args\n    idim, odim = 10, 25\n    model_args = make_feedforward_transformer_args(**model_dict)\n\n    # setup batch\n    ilens = [10, 5]\n    olens = [20, 15]\n    device = torch.device(""cuda"")\n    batch = prepare_inputs(\n        idim, odim, ilens, olens, model_args[""spk_embed_dim""], device=device\n    )\n\n    # define teacher model and save it\n    if teacher_type == ""transformer"":\n        teacher_model_args = make_transformer_args(**model_dict)\n        teacher_model = Transformer(idim, odim, Namespace(**teacher_model_args))\n    elif teacher_type == ""tacotron2"":\n        teacher_model_args = make_taco2_args(**model_dict)\n        teacher_model = Tacotron2(idim, odim, Namespace(**teacher_model_args))\n    else:\n        raise ValueError()\n    tmpdir = tempfile.mkdtemp(prefix=""tmp_"", dir=""/tmp"")\n    torch.save(teacher_model.state_dict(), tmpdir + ""/model.dummy.best"")\n    with open(tmpdir + ""/model.json"", ""wb"") as f:\n        f.write(\n            json.dumps(\n                (idim, odim, teacher_model_args),\n                indent=4,\n                ensure_ascii=False,\n                sort_keys=True,\n            ).encode(""utf_8"")\n        )\n\n    # define model\n    model_args[""teacher_model""] = tmpdir + ""/model.dummy.best""\n    model = FeedForwardTransformer(idim, odim, Namespace(**model_args))\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # decodable\n    model.eval()\n    with torch.no_grad():\n        if model_args[""spk_embed_dim""] is None:\n            spemb = None\n        else:\n            spemb = batch[""spembs""][0]\n        model.inference(batch[""xs""][0][: batch[""ilens""][0]], None, spemb=spemb)\n        model.calculate_all_attentions(**batch)\n\n    # remove tmpdir\n    if os.path.exists(tmpdir):\n        shutil.rmtree(tmpdir)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(\n    ""teacher_type, model_dict"",\n    [\n        (""transformer"", {}),\n        (""transformer"", {""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""add""}),\n        (""transformer"", {""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""concat""}),\n        (""transformer"", {""use_masking"": False}),\n        (""transformer"", {""use_masking"": False, ""use_weighted_masking"": True}),\n        (""transformer"", {""use_scaled_pos_enc"": False}),\n        (""transformer"", {""encoder_normalize_before"": False}),\n        (""transformer"", {""decoder_normalize_before"": False}),\n        (\n            ""transformer"",\n            {""encoder_normalize_before"": False, ""decoder_normalize_before"": False},\n        ),\n        (""transformer"", {""encoder_concat_after"": True}),\n        (""transformer"", {""decoder_concat_after"": True}),\n        (""transformer"", {""encoder_concat_after"": True, ""decoder_concat_after"": True}),\n        (""transformer"", {""transfer_encoder_from_teacher"": True}),\n        (\n            ""transformer"",\n            {\n                ""transfer_encoder_from_teacher"": True,\n                ""transferred_encoder_module"": ""embed"",\n            },\n        ),\n        (""tacotron2"", {}),\n        (""tacotron2"", {""spk_embed_dim"": 16}),\n    ],\n)\ndef test_fastspeech_multi_gpu_trainable(teacher_type, model_dict):\n    # make args\n    idim, odim = 10, 25\n    model_args = make_feedforward_transformer_args(**model_dict)\n\n    # setup batch\n    ilens = [10, 5]\n    olens = [20, 15]\n    device = torch.device(""cuda"")\n    batch = prepare_inputs(\n        idim, odim, ilens, olens, model_args[""spk_embed_dim""], device=device\n    )\n\n    # define teacher model and save it\n    if teacher_type == ""transformer"":\n        teacher_model_args = make_transformer_args(**model_dict)\n        teacher_model = Transformer(idim, odim, Namespace(**teacher_model_args))\n    elif teacher_type == ""tacotron2"":\n        teacher_model_args = make_taco2_args(**model_dict)\n        teacher_model = Tacotron2(idim, odim, Namespace(**teacher_model_args))\n    else:\n        raise ValueError()\n    tmpdir = tempfile.mkdtemp(prefix=""tmp_"", dir=""/tmp"")\n    torch.save(teacher_model.state_dict(), tmpdir + ""/model.dummy.best"")\n    with open(tmpdir + ""/model.json"", ""wb"") as f:\n        f.write(\n            json.dumps(\n                (idim, odim, teacher_model_args),\n                indent=4,\n                ensure_ascii=False,\n                sort_keys=True,\n            ).encode(""utf_8"")\n        )\n\n    # define model\n    ngpu = 2\n    device_ids = list(range(ngpu))\n    model_args[""teacher_model""] = tmpdir + ""/model.dummy.best""\n    model = FeedForwardTransformer(idim, odim, Namespace(**model_args))\n    model = torch.nn.DataParallel(model, device_ids)\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # remove tmpdir\n    if os.path.exists(tmpdir):\n        shutil.rmtree(tmpdir)\n\n\n@pytest.mark.parametrize(\n    ""model_dict"",\n    [\n        ({""transfer_encoder_from_teacher"": True}),\n        (\n            {\n                ""transfer_encoder_from_teacher"": True,\n                ""transferred_encoder_module"": ""embed"",\n            }\n        ),\n        ({""transfer_encoder_from_teacher"": True, ""use_scaled_pos_enc"": False}),\n        ({""transfer_encoder_from_teacher"": True, ""encoder_normalize_before"": False}),\n        ({""transfer_encoder_from_teacher"": True, ""decoder_normalize_before"": False}),\n        (\n            {\n                ""transfer_encoder_from_teacher"": True,\n                ""encoder_normalize_before"": False,\n                ""decoder_normalize_before"": False,\n            }\n        ),\n        ({""transfer_encoder_from_teacher"": True, ""encoder_concat_after"": True}),\n        ({""transfer_encoder_from_teacher"": True, ""decoder_concat_after"": True}),\n        (\n            {\n                ""transfer_encoder_from_teacher"": True,\n                ""encoder_concat_after"": True,\n                ""decoder_concat_after"": True,\n            }\n        ),\n    ],\n)\ndef test_initialization(model_dict):\n    # make args\n    idim, odim = 10, 25\n    teacher_model_args = make_transformer_args(**model_dict)\n    model_args = make_feedforward_transformer_args(**model_dict)\n\n    # define teacher model and save it\n    teacher_model = Transformer(idim, odim, Namespace(**teacher_model_args))\n    tmpdir = tempfile.mkdtemp(prefix=""tmp_"", dir=""/tmp"")\n    torch.save(teacher_model.state_dict(), tmpdir + ""/model.dummy.best"")\n    with open(tmpdir + ""/model.json"", ""wb"") as f:\n        f.write(\n            json.dumps(\n                (idim, odim, teacher_model_args),\n                indent=4,\n                ensure_ascii=False,\n                sort_keys=True,\n            ).encode(""utf_8"")\n        )\n\n    # define model\n    model_args[""teacher_model""] = tmpdir + ""/model.dummy.best""\n    model = FeedForwardTransformer(idim, odim, Namespace(**model_args))\n\n    # check initialization\n    if model_args[""transferred_encoder_module""] == ""all"":\n        for p1, p2 in zip(\n            model.encoder.parameters(), model.teacher.encoder.parameters()\n        ):\n            np.testing.assert_array_equal(p1.data.cpu().numpy(), p2.data.cpu().numpy())\n    else:\n        np.testing.assert_array_equal(\n            model.encoder.embed[0].weight.data.cpu().numpy(),\n            model.teacher.encoder.embed[0].weight.data.cpu().numpy(),\n        )\n\n    # remove tmpdir\n    if os.path.exists(tmpdir):\n        shutil.rmtree(tmpdir)\n\n\ndef test_length_regulator():\n    # prepare inputs\n    idim = 5\n    ilens = [10, 5, 3]\n    xs = pad_list([torch.randn((ilen, idim)) for ilen in ilens], 0.0)\n    ds = pad_list([torch.arange(ilen) for ilen in ilens], 0)\n\n    # test with non-zero durations\n    length_regulator = LengthRegulator()\n    xs_expand = length_regulator(xs, ds, ilens)\n    assert int(xs_expand.shape[1]) == int(ds.sum(dim=-1).max())\n\n    # test with duration including zero\n    ds[:, 2] = 0\n    xs_expand = length_regulator(xs, ds, ilens)\n    assert int(xs_expand.shape[1]) == int(ds.sum(dim=-1).max())\n\n\ndef test_duration_calculator():\n    # define duration calculator\n    idim, odim = 10, 25\n    teacher_model_args = make_transformer_args()\n    teacher = Transformer(idim, odim, Namespace(**teacher_model_args))\n    duration_calculator = DurationCalculator(teacher)\n\n    # setup batch\n    ilens = [10, 5, 3]\n    olens = [20, 15, 10]\n    batch = prepare_inputs(idim, odim, ilens, olens)\n\n    # calculate durations\n    ds = duration_calculator(batch[""xs""], batch[""ilens""], batch[""ys""], batch[""olens""])\n    np.testing.assert_array_equal(\n        ds.sum(dim=-1).cpu().numpy(), batch[""olens""].cpu().numpy()\n    )\n\n\n@pytest.mark.parametrize(\n    ""alpha"", [(1.0), (0.5), (2.0)],\n)\ndef test_fastspeech_inference(alpha):\n    # make args\n    idim, odim = 10, 25\n    model_args = make_feedforward_transformer_args()\n\n    # setup batch\n    ilens = [10, 5]\n    olens = [20, 15]\n    batch = prepare_inputs(idim, odim, ilens, olens, model_args[""spk_embed_dim""])\n\n    # define model\n    model = FeedForwardTransformer(idim, odim, Namespace(**model_args))\n\n    # test inference\n    inference_args = Namespace(**{""fastspeech_alpha"": alpha})\n    model.eval()\n    with torch.no_grad():\n        if model_args[""spk_embed_dim""] is None:\n            spemb = None\n        else:\n            spemb = batch[""spembs""][0]\n        model.inference(\n            batch[""xs""][0][: batch[""ilens""][0]], inference_args, spemb=spemb,\n        )\n'"
test/test_e2e_tts_tacotron2.py,17,"b'#!/usr/bin/env python3\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom argparse import Namespace\n\nfrom espnet.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\ndef make_taco2_args(**kwargs):\n    defaults = dict(\n        use_speaker_embedding=False,\n        spk_embed_dim=None,\n        embed_dim=32,\n        elayers=1,\n        eunits=32,\n        econv_layers=2,\n        econv_filts=5,\n        econv_chans=32,\n        dlayers=2,\n        dunits=32,\n        prenet_layers=2,\n        prenet_units=32,\n        postnet_layers=2,\n        postnet_filts=5,\n        postnet_chans=32,\n        output_activation=None,\n        atype=""location"",\n        adim=32,\n        aconv_chans=16,\n        aconv_filts=5,\n        cumulate_att_w=True,\n        use_batch_norm=True,\n        use_concate=True,\n        use_residual=False,\n        dropout_rate=0.5,\n        zoneout_rate=0.1,\n        reduction_factor=1,\n        threshold=0.5,\n        maxlenratio=5.0,\n        minlenratio=0.0,\n        use_cbhg=False,\n        spc_dim=None,\n        cbhg_conv_bank_layers=4,\n        cbhg_conv_bank_chans=32,\n        cbhg_conv_proj_filts=3,\n        cbhg_conv_proj_chans=32,\n        cbhg_highway_layers=4,\n        cbhg_highway_units=32,\n        cbhg_gru_units=32,\n        use_masking=True,\n        use_weighted_masking=False,\n        bce_pos_weight=1.0,\n        use_guided_attn_loss=False,\n        guided_attn_loss_sigma=0.4,\n        guided_attn_loss_lambda=1.0,\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_inference_args(**kwargs):\n    defaults = dict(\n        threshold=0.5,\n        maxlenratio=5.0,\n        minlenratio=0.0,\n        use_att_constraint=False,\n        backward_window=1,\n        forward_window=3,\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef prepare_inputs(\n    bs,\n    idim,\n    odim,\n    maxin_len,\n    maxout_len,\n    spk_embed_dim=None,\n    spc_dim=None,\n    device=torch.device(""cpu""),\n):\n    ilens = np.sort(np.random.randint(1, maxin_len, bs))[::-1].tolist()\n    olens = np.sort(np.random.randint(3, maxout_len, bs))[::-1].tolist()\n    xs = [np.random.randint(0, idim, lg) for lg in ilens]\n    ys = [np.random.randn(lg, odim) for lg in olens]\n    ilens = torch.LongTensor(ilens).to(device)\n    olens = torch.LongTensor(olens).to(device)\n    xs = pad_list([torch.from_numpy(x).long() for x in xs], 0).to(device)\n    ys = pad_list([torch.from_numpy(y).float() for y in ys], 0).to(device)\n    labels = ys.new_zeros(ys.size(0), ys.size(1))\n    for i, lg in enumerate(olens):\n        labels[i, lg - 1 :] = 1\n\n    batch = {\n        ""xs"": xs,\n        ""ilens"": ilens,\n        ""ys"": ys,\n        ""labels"": labels,\n        ""olens"": olens,\n    }\n\n    if spk_embed_dim is not None:\n        spembs = torch.from_numpy(np.random.randn(bs, spk_embed_dim)).float().to(device)\n        batch[""spembs""] = spembs\n    if spc_dim is not None:\n        spcs = [np.random.randn(lg, spc_dim) for lg in olens]\n        spcs = pad_list([torch.from_numpy(spc).float() for spc in spcs], 0).to(device)\n        batch[""extras""] = spcs\n\n    return batch\n\n\n@pytest.mark.parametrize(\n    ""model_dict, inference_dict"",\n    [\n        ({}, {}),\n        ({""use_masking"": False}, {}),\n        ({""bce_pos_weight"": 10.0}, {}),\n        ({""atype"": ""forward""}, {}),\n        ({""atype"": ""forward_ta""}, {}),\n        ({""prenet_layers"": 0}, {}),\n        ({""postnet_layers"": 0}, {}),\n        ({""prenet_layers"": 0, ""postnet_layers"": 0}, {}),\n        ({""output_activation"": ""relu""}, {}),\n        ({""cumulate_att_w"": False}, {}),\n        ({""use_batch_norm"": False}, {}),\n        ({""use_concate"": False}, {}),\n        ({""use_residual"": True}, {}),\n        ({""dropout_rate"": 0.0}, {}),\n        ({""zoneout_rate"": 0.0}, {}),\n        ({""reduction_factor"": 2}, {}),\n        ({""reduction_factor"": 3}, {}),\n        ({""use_speaker_embedding"": True}, {}),\n        ({""use_masking"": False}, {}),\n        ({""use_masking"": False, ""use_weighted_masking"": True}, {}),\n        ({""use_cbhg"": True}, {}),\n        ({""reduction_factor"": 3, ""use_cbhg"": True}, {}),\n        ({""use_guided_attn_loss"": True}, {}),\n        ({""reduction_factor"": 3, ""use_guided_attn_loss"": True}, {}),\n        ({}, {""use_att_constraint"": True}),\n        ({""atype"": ""forward""}, {""use_att_constraint"": True}),\n        ({""atype"": ""forward_ta""}, {""use_att_constraint"": True}),\n    ],\n)\ndef test_tacotron2_trainable_and_decodable(model_dict, inference_dict):\n    # make args\n    model_args = make_taco2_args(**model_dict)\n    inference_args = make_inference_args(**inference_dict)\n\n    # setup batch\n    bs = 2\n    maxin_len = 10\n    maxout_len = 10\n    idim = 5\n    odim = 10\n    if model_args[""use_cbhg""]:\n        model_args[""spc_dim""] = 129\n    if model_args[""use_speaker_embedding""]:\n        model_args[""spk_embed_dim""] = 128\n    batch = prepare_inputs(\n        bs,\n        idim,\n        odim,\n        maxin_len,\n        maxout_len,\n        model_args[""spk_embed_dim""],\n        model_args[""spc_dim""],\n    )\n\n    # define model\n    model = Tacotron2(idim, odim, Namespace(**model_args))\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # decodable\n    model.eval()\n    with torch.no_grad():\n        spemb = None if model_args[""spk_embed_dim""] is None else batch[""spembs""][0]\n        model.inference(\n            batch[""xs""][0][: batch[""ilens""][0]], Namespace(**inference_args), spemb\n        )\n        model.calculate_all_attentions(**batch)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""gpu required"")\n@pytest.mark.parametrize(\n    ""model_dict, inference_dict"",\n    [\n        ({}, {}),\n        ({""atype"": ""forward""}, {}),\n        ({""atype"": ""forward_ta""}, {}),\n        ({""use_speaker_embedding"": True, ""spk_embed_dim"": 128}, {}),\n        ({""use_cbhg"": True, ""spc_dim"": 128}, {}),\n        ({""reduction_factor"": 3}, {}),\n        ({""use_guided_attn_loss"": True}, {}),\n        ({""use_masking"": False}, {}),\n        ({""use_masking"": False, ""use_weighted_masking"": True}, {}),\n        ({}, {""use_att_constraint"": True}),\n        ({""atype"": ""forward""}, {""use_att_constraint"": True}),\n        ({""atype"": ""forward_ta""}, {""use_att_constraint"": True}),\n    ],\n)\ndef test_tacotron2_gpu_trainable_and_decodable(model_dict, inference_dict):\n    bs = 2\n    maxin_len = 10\n    maxout_len = 10\n    idim = 5\n    odim = 10\n    device = torch.device(""cuda"")\n    model_args = make_taco2_args(**model_dict)\n    inference_args = make_inference_args(**inference_dict)\n    batch = prepare_inputs(\n        bs,\n        idim,\n        odim,\n        maxin_len,\n        maxout_len,\n        model_args[""spk_embed_dim""],\n        model_args[""spc_dim""],\n        device=device,\n    )\n\n    # define model\n    model = Tacotron2(idim, odim, Namespace(**model_args))\n    optimizer = torch.optim.Adam(model.parameters())\n    model.to(device)\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # decodable\n    model.eval()\n    with torch.no_grad():\n        spemb = None if model_args[""spk_embed_dim""] is None else batch[""spembs""][0]\n        model.inference(\n            batch[""xs""][0][: batch[""ilens""][0]], Namespace(**inference_args), spemb\n        )\n        model.calculate_all_attentions(**batch)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(\n    ""model_dict"",\n    [\n        ({}),\n        ({""atype"": ""forward""}),\n        ({""atype"": ""forward_ta""}),\n        ({""use_speaker_embedding"": True, ""spk_embed_dim"": 128}),\n        ({""use_cbhg"": True, ""spc_dim"": 128}),\n        ({""reduction_factor"": 3}),\n        ({""use_guided_attn_loss"": True}),\n        ({""use_masking"": False}),\n        ({""use_masking"": False, ""use_weighted_masking"": True}),\n    ],\n)\ndef test_tacotron2_multi_gpu_trainable(model_dict):\n    ngpu = 2\n    device_ids = list(range(ngpu))\n    device = torch.device(""cuda"")\n    bs = 10\n    maxin_len = 10\n    maxout_len = 10\n    idim = 5\n    odim = 10\n    model_args = make_taco2_args(**model_dict)\n    batch = prepare_inputs(\n        bs,\n        idim,\n        odim,\n        maxin_len,\n        maxout_len,\n        model_args[""spk_embed_dim""],\n        model_args[""spc_dim""],\n        device=device,\n    )\n\n    # define model\n    model = Tacotron2(idim, odim, Namespace(**model_args))\n    model = torch.nn.DataParallel(model, device_ids)\n    optimizer = torch.optim.Adam(model.parameters())\n    model.to(device)\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n'"
test/test_e2e_tts_transformer.py,22,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom argparse import Namespace\n\nfrom espnet.nets.pytorch_backend.e2e_tts_transformer import subsequent_mask\nfrom espnet.nets.pytorch_backend.e2e_tts_transformer import Transformer\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\ndef make_transformer_args(**kwargs):\n    defaults = dict(\n        embed_dim=32,\n        spk_embed_dim=None,\n        eprenet_conv_layers=2,\n        eprenet_conv_filts=5,\n        eprenet_conv_chans=32,\n        dprenet_layers=2,\n        dprenet_units=32,\n        adim=32,\n        aheads=4,\n        elayers=2,\n        eunits=32,\n        dlayers=2,\n        dunits=32,\n        postnet_layers=2,\n        postnet_filts=5,\n        postnet_chans=32,\n        eprenet_dropout_rate=0.1,\n        dprenet_dropout_rate=0.5,\n        postnet_dropout_rate=0.1,\n        transformer_enc_dropout_rate=0.1,\n        transformer_enc_positional_dropout_rate=0.1,\n        transformer_enc_attn_dropout_rate=0.0,\n        transformer_dec_dropout_rate=0.1,\n        transformer_dec_positional_dropout_rate=0.1,\n        transformer_dec_attn_dropout_rate=0.3,\n        transformer_enc_dec_attn_dropout_rate=0.0,\n        spk_embed_integration_type=""add"",\n        use_masking=True,\n        use_weighted_masking=False,\n        bce_pos_weight=1.0,\n        use_batch_norm=True,\n        use_scaled_pos_enc=True,\n        encoder_normalize_before=True,\n        decoder_normalize_before=True,\n        encoder_concat_after=False,\n        decoder_concat_after=False,\n        transformer_init=""pytorch"",\n        initial_encoder_alpha=1.0,\n        initial_decoder_alpha=1.0,\n        reduction_factor=1,\n        loss_type=""L1"",\n        use_guided_attn_loss=False,\n        num_heads_applied_guided_attn=2,\n        num_layers_applied_guided_attn=2,\n        guided_attn_loss_sigma=0.4,\n        guided_attn_loss_lambda=1.0,\n        modules_applied_guided_attn=[""encoder"", ""decoder"", ""encoder-decoder""],\n    )\n    defaults.update(kwargs)\n    return defaults\n\n\ndef make_inference_args(**kwargs):\n    defaults = dict(threshold=0.5, maxlenratio=5.0, minlenratio=0.0)\n    defaults.update(kwargs)\n    return defaults\n\n\ndef prepare_inputs(\n    idim, odim, ilens, olens, spk_embed_dim=None, device=torch.device(""cpu"")\n):\n    xs = [np.random.randint(0, idim, lg) for lg in ilens]\n    ys = [np.random.randn(lg, odim) for lg in olens]\n    ilens = torch.LongTensor(ilens).to(device)\n    olens = torch.LongTensor(olens).to(device)\n    xs = pad_list([torch.from_numpy(x).long() for x in xs], 0).to(device)\n    ys = pad_list([torch.from_numpy(y).float() for y in ys], 0).to(device)\n    labels = ys.new_zeros(ys.size(0), ys.size(1))\n    for i, l in enumerate(olens):\n        labels[i, l - 1 :] = 1\n    batch = {\n        ""xs"": xs,\n        ""ilens"": ilens,\n        ""ys"": ys,\n        ""labels"": labels,\n        ""olens"": olens,\n    }\n\n    if spk_embed_dim is not None:\n        batch[""spembs""] = torch.FloatTensor(\n            np.random.randn(len(ilens), spk_embed_dim)\n        ).to(device)\n\n    return batch\n\n\n@pytest.mark.parametrize(\n    ""model_dict"",\n    [\n        ({}),\n        ({""use_masking"": False}),\n        ({""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""concat""}),\n        ({""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""add""}),\n        ({""use_scaled_pos_enc"": False}),\n        ({""bce_pos_weight"": 10.0}),\n        ({""reduction_factor"": 2}),\n        ({""reduction_factor"": 3}),\n        ({""encoder_normalize_before"": False}),\n        ({""decoder_normalize_before"": False}),\n        ({""encoder_normalize_before"": False, ""decoder_normalize_before"": False}),\n        ({""encoder_concat_after"": True}),\n        ({""decoder_concat_after"": True}),\n        ({""encoder_concat_after"": True, ""decoder_concat_after"": True}),\n        ({""loss_type"": ""L1""}),\n        ({""loss_type"": ""L2""}),\n        ({""loss_type"": ""L1+L2""}),\n        ({""use_masking"": False}),\n        ({""use_masking"": False, ""use_weighted_masking"": True}),\n        ({""use_guided_attn_loss"": True}),\n        ({""use_guided_attn_loss"": True, ""reduction_factor"": 3}),\n        (\n            {\n                ""use_guided_attn_loss"": True,\n                ""modules_applied_guided_attn"": [""encoder-decoder""],\n            }\n        ),\n        (\n            {\n                ""use_guided_attn_loss"": True,\n                ""modules_applied_guided_attn"": [""encoder"", ""decoder""],\n            }\n        ),\n        ({""use_guided_attn_loss"": True, ""num_heads_applied_guided_attn"": -1}),\n        ({""use_guided_attn_loss"": True, ""num_layers_applied_guided_attn"": -1}),\n        (\n            {\n                ""use_guided_attn_loss"": True,\n                ""modules_applied_guided_attn"": [""encoder""],\n                ""elayers"": 2,\n                ""dlayers"": 3,\n            }\n        ),\n    ],\n)\ndef test_transformer_trainable_and_decodable(model_dict):\n    # make args\n    model_args = make_transformer_args(**model_dict)\n    inference_args = make_inference_args()\n\n    # setup batch\n    idim = 5\n    odim = 10\n    ilens = [10, 5]\n    olens = [20, 15]\n    batch = prepare_inputs(idim, odim, ilens, olens, model_args[""spk_embed_dim""])\n\n    # define model\n    model = Transformer(idim, odim, Namespace(**model_args))\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # check gradient of ScaledPositionalEncoding\n    if model.use_scaled_pos_enc:\n        assert model.encoder.embed[1].alpha.grad is not None\n        assert model.decoder.embed[1].alpha.grad is not None\n\n    # decodable\n    model.eval()\n    with torch.no_grad():\n        if model_args[""spk_embed_dim""] is None:\n            spemb = None\n        else:\n            spemb = batch[""spembs""][0]\n        model.inference(\n            batch[""xs""][0][: batch[""ilens""][0]],\n            Namespace(**inference_args),\n            spemb=spemb,\n        )\n        model.calculate_all_attentions(**batch)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""gpu required"")\n@pytest.mark.parametrize(\n    ""model_dict"",\n    [\n        ({}),\n        ({""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""concat""}),\n        ({""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""add""}),\n        ({""use_masking"": False}),\n        ({""use_scaled_pos_enc"": False}),\n        ({""bce_pos_weight"": 10.0}),\n        ({""encoder_normalize_before"": False}),\n        ({""decoder_normalize_before"": False}),\n        ({""encoder_normalize_before"": False, ""decoder_normalize_before"": False}),\n        ({""decoder_concat_after"": True}),\n        ({""encoder_concat_after"": True, ""decoder_concat_after"": True}),\n        ({""use_masking"": False}),\n        ({""use_masking"": False, ""use_weighted_masking"": True}),\n    ],\n)\ndef test_transformer_gpu_trainable_and_decodable(model_dict):\n    # make args\n    model_args = make_transformer_args(**model_dict)\n    inference_args = make_inference_args()\n\n    idim = 5\n    odim = 10\n    ilens = [10, 5]\n    olens = [20, 15]\n    device = torch.device(""cuda"")\n    batch = prepare_inputs(\n        idim, odim, ilens, olens, model_args[""spk_embed_dim""], device=device\n    )\n\n    # define model\n    model = Transformer(idim, odim, Namespace(**model_args))\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # check gradient of ScaledPositionalEncoding\n    if model.use_scaled_pos_enc:\n        assert model.encoder.embed[1].alpha.grad is not None\n        assert model.decoder.embed[1].alpha.grad is not None\n\n    # decodable\n    model.eval()\n    with torch.no_grad():\n        if model_args[""spk_embed_dim""] is None:\n            spemb = None\n        else:\n            spemb = batch[""spembs""][0]\n        model.inference(\n            batch[""xs""][0][: batch[""ilens""][0]],\n            Namespace(**inference_args),\n            spemb=spemb,\n        )\n        model.calculate_all_attentions(**batch)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason=""multi gpu required"")\n@pytest.mark.parametrize(\n    ""model_dict"",\n    [\n        ({}),\n        ({""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""concat""}),\n        ({""spk_embed_dim"": 16, ""spk_embed_integration_type"": ""add""}),\n        ({""use_masking"": False}),\n        ({""use_scaled_pos_enc"": False}),\n        ({""bce_pos_weight"": 10.0}),\n        ({""encoder_normalize_before"": False}),\n        ({""decoder_normalize_before"": False}),\n        ({""encoder_normalize_before"": False, ""decoder_normalize_before"": False}),\n        ({""decoder_concat_after"": True}),\n        ({""encoder_concat_after"": True, ""decoder_concat_after"": True}),\n        ({""use_masking"": False}),\n        ({""use_masking"": False, ""use_weighted_masking"": True}),\n    ],\n)\ndef test_transformer_multi_gpu_trainable(model_dict):\n    # make args\n    model_args = make_transformer_args(**model_dict)\n\n    # setup batch\n    idim = 5\n    odim = 10\n    ilens = [10, 5]\n    olens = [20, 15]\n    device = torch.device(""cuda"")\n    batch = prepare_inputs(\n        idim, odim, ilens, olens, model_args[""spk_embed_dim""], device=device\n    )\n\n    # define model\n    ngpu = 2\n    device_ids = list(range(ngpu))\n    model = Transformer(idim, odim, Namespace(**model_args))\n    model = torch.nn.DataParallel(model, device_ids)\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters())\n\n    # trainable\n    loss = model(**batch).mean()\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # check gradient of ScaledPositionalEncoding\n    if model.module.use_scaled_pos_enc:\n        assert model.module.encoder.embed[1].alpha.grad is not None\n        assert model.module.decoder.embed[1].alpha.grad is not None\n\n\n@pytest.mark.parametrize(""model_dict"", [({})])\ndef test_attention_masking(model_dict):\n    # make args\n    model_args = make_transformer_args(**model_dict)\n\n    # setup batch\n    idim = 5\n    odim = 10\n    ilens = [10, 5]\n    olens = [20, 15]\n    batch = prepare_inputs(idim, odim, ilens, olens)\n\n    # define model\n    model = Transformer(idim, odim, Namespace(**model_args))\n\n    # test encoder self-attention\n    xs = model.encoder.embed(batch[""xs""])\n    xs[1, ilens[1] :] = float(""nan"")\n    x_masks = model._source_mask(batch[""ilens""])\n    a = model.encoder.encoders[0].self_attn\n    a(xs, xs, xs, x_masks)\n    aws = a.attn.detach().numpy()\n    for aw, ilen in zip(aws, batch[""ilens""]):\n        assert not np.isnan(aw[:, :ilen, :ilen]).any()\n        np.testing.assert_almost_equal(\n            aw[:, :ilen, :ilen].sum(), float(aw.shape[0] * ilen), decimal=4\n        )\n        assert aw[:, ilen:, ilen:].sum() == 0.0\n\n    # test encoder-decoder attention\n    ys = model.decoder.embed(batch[""ys""])\n    ys[1, olens[1] :] = float(""nan"")\n    xy_masks = x_masks\n    a = model.decoder.decoders[0].src_attn\n    a(ys, xs, xs, xy_masks)\n    aws = a.attn.detach().numpy()\n    for aw, ilen, olen in zip(aws, batch[""ilens""], batch[""olens""]):\n        assert not np.isnan(aw[:, :olen, :ilen]).any()\n        np.testing.assert_almost_equal(\n            aw[:, :olen, :ilen].sum(), float(aw.shape[0] * olen), decimal=4\n        )\n        assert aw[:, olen:, ilen:].sum() == 0.0\n\n    # test decoder self-attention\n    y_masks = model._target_mask(batch[""olens""])\n    a = model.decoder.decoders[0].self_attn\n    a(ys, ys, ys, y_masks)\n    aws = a.attn.detach().numpy()\n    for aw, olen in zip(aws, batch[""olens""]):\n        assert not np.isnan(aw[:, :olen, :olen]).any()\n        np.testing.assert_almost_equal(\n            aw[:, :olen, :olen].sum(), float(aw.shape[0] * olen), decimal=4\n        )\n        assert aw[:, olen:, olen:].sum() == 0.0\n\n\n@pytest.mark.parametrize(\n    ""model_dict"",\n    [\n        ({}),\n        ({""reduction_factor"": 3}),\n        ({""reduction_factor"": 4}),\n        ({""decoder_normalize_before"": False}),\n        ({""encoder_normalize_before"": False, ""decoder_normalize_before"": False}),\n        ({""decoder_concat_after"": True}),\n        ({""encoder_concat_after"": True, ""decoder_concat_after"": True}),\n    ],\n)\ndef test_forward_and_inference_are_equal(model_dict):\n    # make args\n    model_args = make_transformer_args(dprenet_dropout_rate=0.0, **model_dict)\n\n    # setup batch\n    idim = 5\n    odim = 10\n    ilens = [10]\n    olens = [20]\n    batch = prepare_inputs(idim, odim, ilens, olens)\n    xs = batch[""xs""]\n    ilens = batch[""ilens""]\n    ys = batch[""ys""]\n    olens = batch[""olens""]\n\n    # define model\n    model = Transformer(idim, odim, Namespace(**model_args))\n    model.eval()\n\n    # TODO(kan-bayashi): update following ugly part\n    with torch.no_grad():\n        # --------- forward calculation ---------\n        x_masks = model._source_mask(ilens)\n        hs_fp, h_masks = model.encoder(xs, x_masks)\n        if model.reduction_factor > 1:\n            ys_in = ys[:, model.reduction_factor - 1 :: model.reduction_factor]\n            olens_in = olens.new([olen // model.reduction_factor for olen in olens])\n        else:\n            ys_in, olens_in = ys, olens\n        ys_in = model._add_first_frame_and_remove_last_frame(ys_in)\n        y_masks = model._target_mask(olens_in)\n        zs, _ = model.decoder(ys_in, y_masks, hs_fp, h_masks)\n        before_outs = model.feat_out(zs).view(zs.size(0), -1, model.odim)\n        logits = model.prob_out(zs).view(zs.size(0), -1)\n        after_outs = before_outs + model.postnet(before_outs.transpose(1, 2)).transpose(\n            1, 2\n        )\n        # --------- forward calculation ---------\n\n        # --------- inference calculation ---------\n        hs_ir, _ = model.encoder(xs, None)\n        maxlen = ys_in.shape[1]\n        minlen = ys_in.shape[1]\n        idx = 0\n        # this is the inferene calculation but we use groundtruth to check the behavior\n        ys_in_ = ys_in[0, idx].view(1, 1, model.odim)\n        np.testing.assert_array_equal(\n            ys_in_.new_zeros(1, 1, model.odim).detach().cpu().numpy(),\n            ys_in_.detach().cpu().numpy(),\n        )\n        outs, probs = [], []\n        while True:\n            idx += 1\n            y_masks = subsequent_mask(idx).unsqueeze(0)\n            z = model.decoder.forward_one_step(ys_in_, y_masks, hs_ir)[\n                0\n            ]  # (B, idx, adim)\n            outs += [model.feat_out(z).view(1, -1, model.odim)]  # [(1, r, odim), ...]\n            probs += [torch.sigmoid(model.prob_out(z))[0]]  # [(r), ...]\n            if idx >= maxlen:\n                if idx < minlen:\n                    continue\n                outs = torch.cat(outs, dim=1).transpose(\n                    1, 2\n                )  # (1, L, odim) -> (1, odim, L)\n                if model.postnet is not None:\n                    outs = outs + model.postnet(outs)  # (1, odim, L)\n                outs = outs.transpose(2, 1).squeeze(0)  # (L, odim)\n                probs = torch.cat(probs, dim=0)\n                break\n            ys_in_ = torch.cat(\n                (ys_in_, ys_in[0, idx].view(1, 1, model.odim)), dim=1\n            )  # (1, idx + 1, odim)\n        # --------- inference calculation ---------\n\n        # check both are equal\n        np.testing.assert_array_almost_equal(\n            hs_fp.detach().cpu().numpy(), hs_ir.detach().cpu().numpy(),\n        )\n        np.testing.assert_array_almost_equal(\n            after_outs.squeeze(0).detach().cpu().numpy(), outs.detach().cpu().numpy(),\n        )\n        np.testing.assert_array_almost_equal(\n            torch.sigmoid(logits.squeeze(0)).detach().cpu().numpy(),\n            probs.detach().cpu().numpy(),\n        )\n'"
test/test_initialization.py,1,"b'# coding: utf-8\n\n# Copyright 2017 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\nimport argparse\n\nimport numpy\nimport os\nimport pytest\nimport random\n\nargs = argparse.Namespace(\n    elayers=4,\n    subsample=""1_2_2_1_1"",\n    etype=""vggblstmp"",\n    eunits=320,\n    eprojs=320,\n    dtype=""lstm"",\n    dlayers=2,\n    dunits=300,\n    atype=""location"",\n    aconv_chans=10,\n    aconv_filts=100,\n    mtlalpha=0.5,\n    lsm_type="""",\n    lsm_weight=0.0,\n    sampling_probability=0.0,\n    adim=320,\n    dropout_rate=0.0,\n    dropout_rate_decoder=0.0,\n    beam_size=3,\n    penalty=0.5,\n    maxlenratio=1.0,\n    minlenratio=0.0,\n    ctc_weight=0.2,\n    verbose=True,\n    char_list=[u""\xe3\x81\x82"", u""\xe3\x81\x84"", u""\xe3\x81\x86"", u""\xe3\x81\x88"", u""\xe3\x81\x8a""],\n    outdir=None,\n    seed=1,\n    ctc_type=""warpctc"",\n    report_cer=False,\n    report_wer=False,\n    sym_space=""<space>"",\n    sym_blank=""<blank>"",\n    context_residual=False,\n    use_frontend=False,\n    replace_sos=False,\n    tgt_lang=False,\n)\n\n\ndef test_lecun_init_torch():\n    torch = pytest.importorskip(""torch"")\n    nseed = args.seed\n    random.seed(nseed)\n    torch.manual_seed(nseed)\n    numpy.random.seed(nseed)\n    os.environ[""CHAINER_SEED""] = str(nseed)\n    import espnet.nets.pytorch_backend.e2e_asr as m\n\n    model = m.E2E(40, 5, args)\n    b = model.ctc.ctc_lo.bias.data.numpy()\n    assert numpy.all(b == 0.0)\n    w = model.ctc.ctc_lo.weight.data.numpy()\n    numpy.testing.assert_allclose(w.mean(), 0.0, 1e-2, 1e-2)\n    numpy.testing.assert_allclose(w.var(), 1.0 / w.shape[1], 1e-2, 1e-2)\n\n    for name, p in model.named_parameters():\n        print(name)\n        data = p.data.numpy()\n        if ""embed"" in name:\n            numpy.testing.assert_allclose(data.mean(), 0.0, 5e-2, 5e-2)\n            numpy.testing.assert_allclose(data.var(), 1.0, 5e-2, 5e-2)\n        elif ""dec.decoder.0.bias_ih"" in name:\n            assert data.sum() == data.size // 4\n        elif ""dec.decoder.1.bias_ih"" in name:\n            assert data.sum() == data.size // 4\n        elif data.ndim == 1:\n            assert numpy.all(data == 0.0)\n        else:\n            numpy.testing.assert_allclose(data.mean(), 0.0, 5e-2, 5e-2)\n            numpy.testing.assert_allclose(\n                data.var(), 1.0 / numpy.prod(data.shape[1:]), 5e-2, 5e-2\n            )\n\n\ndef test_lecun_init_chainer():\n    nseed = args.seed\n    random.seed(nseed)\n    numpy.random.seed(nseed)\n    os.environ[""CHAINER_SEED""] = str(nseed)\n    import espnet.nets.chainer_backend.e2e_asr as m\n\n    model = m.E2E(40, 5, args)\n    b = model.ctc.ctc_lo.b.data\n    assert numpy.all(b == 0.0)\n    w = model.ctc.ctc_lo.W.data\n    numpy.testing.assert_allclose(w.mean(), 0.0, 1e-2, 1e-2)\n    numpy.testing.assert_allclose(w.var(), 1.0 / w.shape[1], 1e-2, 1e-2)\n\n    for name, p in model.namedparams():\n        print(name)\n        data = p.data\n        if ""rnn0/upward/b"" in name:\n            assert data.sum() == data.size // 4\n        elif ""rnn1/upward/b"" in name:\n            assert data.sum() == data.size // 4\n        elif ""embed"" in name:\n            numpy.testing.assert_allclose(data.mean(), 0.0, 5e-2, 5e-2)\n            numpy.testing.assert_allclose(data.var(), 1.0, 5e-2, 5e-2)\n        elif data.ndim == 1:\n            assert numpy.all(data == 0.0)\n        else:\n            numpy.testing.assert_allclose(data.mean(), 0.0, 5e-2, 5e-2)\n            numpy.testing.assert_allclose(\n                data.var(), 1.0 / numpy.prod(data.shape[1:]), 5e-2, 5e-2\n            )\n'"
test/test_io_voxforge.py,0,"b'# coding: utf-8\n\n# Copyright 2017 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\nimport os\n\nimport numpy\nimport pytest\n\n\n# TODO(karita): use much smaller corpus like AN4 and download if it does not exists\ndef test_voxforge_feats():\n    import kaldiio\n\n    pytest.importorskip(""kaldi_io"")\n    import kaldi_io\n\n    train_scp = ""scp:egs/voxforge/asr1/data/tr_it/feats.scp""\n    if not os.path.exists(train_scp):\n        pytest.skip(""voxforge scp has not been created"")\n\n    r1 = kaldiio.load_scp(train_scp).items()\n    r2 = kaldi_io.RandomAccessBaseFloatMatrixReader(train_scp)\n\n    for k, v1 in r1:\n        k = str(k)\n        print(k)\n        v2 = r2[k]\n        assert v1.shape == v2.shape\n        numpy.testing.assert_allclose(v1, v2, atol=1e-5)\n'"
test/test_lm.py,15,"b'import chainer\nimport numpy\nimport pytest\nimport torch\n\nimport espnet.lm.chainer_backend.lm as lm_chainer\nfrom espnet.nets.beam_search import beam_search\nfrom espnet.nets.lm_interface import dynamic_import_lm\nimport espnet.nets.pytorch_backend.lm.default as lm_pytorch\nfrom espnet.nets.scorers.length_bonus import LengthBonus\n\nfrom test.test_beam_search import prepare\nfrom test.test_beam_search import rnn_args\n\n\ndef transfer_lstm(ch_lstm, th_lstm):\n    ch_lstm.upward.W.data[:] = 1\n    th_lstm.weight_ih.data[:] = torch.from_numpy(ch_lstm.upward.W.data)\n    ch_lstm.upward.b.data[:] = 1\n    th_lstm.bias_hh.data[:] = torch.from_numpy(ch_lstm.upward.b.data)\n    # NOTE: only lateral weight can directly transfer\n    # rest of the weights and biases have quite different placements\n    th_lstm.weight_hh.data[:] = torch.from_numpy(ch_lstm.lateral.W.data)\n    th_lstm.bias_ih.data.zero_()\n\n\ndef transfer_lm(ch_rnnlm, th_rnnlm):\n    assert isinstance(ch_rnnlm, lm_chainer.RNNLM)\n    assert isinstance(th_rnnlm, lm_pytorch.RNNLM)\n    th_rnnlm.embed.weight.data = torch.from_numpy(ch_rnnlm.embed.W.data)\n    if th_rnnlm.typ == ""lstm"":\n        for n in range(ch_rnnlm.n_layers):\n            transfer_lstm(ch_rnnlm.rnn[n], th_rnnlm.rnn[n])\n    else:\n        assert False\n    th_rnnlm.lo.weight.data = torch.from_numpy(ch_rnnlm.lo.W.data)\n    th_rnnlm.lo.bias.data = torch.from_numpy(ch_rnnlm.lo.b.data)\n\n\ndef test_lm():\n    n_vocab = 3\n    n_layers = 2\n    n_units = 2\n    batchsize = 5\n    for typ in [""lstm""]:  # TODO(anyone) gru\n        rnnlm_ch = lm_chainer.ClassifierWithState(\n            lm_chainer.RNNLM(n_vocab, n_layers, n_units, typ=typ)\n        )\n        rnnlm_th = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(n_vocab, n_layers, n_units, typ=typ)\n        )\n        transfer_lm(rnnlm_ch.predictor, rnnlm_th.predictor)\n\n        # test prediction equality\n        x = torch.from_numpy(numpy.random.randint(n_vocab, size=batchsize)).long()\n        with torch.no_grad(), chainer.no_backprop_mode(), chainer.using_config(\n            ""train"", False\n        ):\n            rnnlm_th.predictor.eval()\n            state_th, y_th = rnnlm_th.predictor(None, x.long())\n            state_ch, y_ch = rnnlm_ch.predictor(None, x.data.numpy())\n            for k in state_ch.keys():\n                for n in range(len(state_th[k])):\n                    print(k, n)\n                    print(state_th[k][n].data.numpy())\n                    print(state_ch[k][n].data)\n                    numpy.testing.assert_allclose(\n                        state_th[k][n].data.numpy(), state_ch[k][n].data, 1e-5\n                    )\n            numpy.testing.assert_allclose(y_th.data.numpy(), y_ch.data, 1e-5)\n\n\n@pytest.mark.parametrize(\n    ""lm_name, lm_args, device, dtype"",\n    [\n        (nn, args, device, dtype)\n        for nn, args in (\n            (""default"", dict(type=""lstm"", layer=2, unit=2, dropout_rate=0.5)),\n            (""default"", dict(type=""gru"", layer=2, unit=2, dropout_rate=0.5)),\n            (""seq_rnn"", dict(type=""lstm"", layer=2, unit=2, dropout_rate=0.5)),\n            (""seq_rnn"", dict(type=""gru"", layer=2, unit=2, dropout_rate=0.5)),\n            (\n                ""transformer"",\n                dict(\n                    layer=2, unit=2, att_unit=2, head=2, dropout_rate=0.5, embed_unit=3\n                ),\n            ),\n            (\n                ""transformer"",\n                dict(\n                    layer=2,\n                    unit=2,\n                    att_unit=2,\n                    head=2,\n                    dropout_rate=0.5,\n                    pos_enc=""none"",\n                    embed_unit=3,\n                ),\n            ),\n        )\n        for device in (""cpu"", ""cuda"")\n        for dtype in (""float16"", ""float32"", ""float64"")\n    ],\n)\ndef test_lm_trainable_and_decodable(lm_name, lm_args, device, dtype):\n    if device == ""cuda"" and not torch.cuda.is_available():\n        pytest.skip(""no cuda device is available"")\n    if device == ""cpu"" and dtype == ""float16"":\n        pytest.skip(""cpu float16 implementation is not available in pytorch yet"")\n\n    dtype = getattr(torch, dtype)\n    model, x, ilens, y, data, train_args = prepare(""rnn"", rnn_args)\n    char_list = train_args.char_list\n    n_vocab = len(char_list)\n    lm = dynamic_import_lm(lm_name, backend=""pytorch"").build(n_vocab, **lm_args)\n    lm.to(device=device, dtype=dtype)\n\n    # test trainable\n    a = torch.randint(1, n_vocab, (3, 2), device=device)\n    b = torch.randint(1, n_vocab, (3, 2), device=device)\n    loss, logp, count = lm(a, b)\n    loss.backward()\n    for p in lm.parameters():\n        assert p.grad is not None\n\n    # test decodable\n    model.to(device=device, dtype=dtype).eval()\n    lm.eval()\n\n    scorers = model.scorers()\n    scorers[""lm""] = lm\n    scorers[""length_bonus""] = LengthBonus(len(char_list))\n    weights = dict(decoder=1.0, lm=1.0, length_bonus=1.0)\n    with torch.no_grad():\n        feat = x[0, : ilens[0]].to(device=device, dtype=dtype)\n        enc = model.encode(feat)\n        beam_size = 3\n        result = beam_search(\n            x=enc,\n            sos=model.sos,\n            eos=model.eos,\n            beam_size=beam_size,\n            vocab_size=len(train_args.char_list),\n            weights=weights,\n            scorers=scorers,\n            token_list=train_args.char_list,\n        )\n    assert len(result) >= beam_size\n'"
test/test_loss.py,14,"b'# coding: utf-8\n\n# Copyright 2017 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom distutils.version import LooseVersion\n\nimport chainer.functions as F\nimport numpy\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.e2e_asr import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import th_accuracy\n\n\n@pytest.mark.parametrize(""use_warpctc"", [True, False])\n@pytest.mark.parametrize(\n    ""in_length,out_length"", [([11, 17, 15], [4, 2, 3]), ([4], [1])]\n)\ndef test_ctc_loss(in_length, out_length, use_warpctc):\n    pytest.importorskip(""torch"")\n    if use_warpctc:\n        pytest.importorskip(""warpctc_pytorch"")\n        import warpctc_pytorch\n\n        torch_ctcloss = warpctc_pytorch.CTCLoss(size_average=True)\n    else:\n        if LooseVersion(torch.__version__) < LooseVersion(""1.0""):\n            pytest.skip(""pytorch < 1.0 doesn\'t support CTCLoss"")\n        _ctcloss_sum = torch.nn.CTCLoss(reduction=""sum"")\n\n        def torch_ctcloss(th_pred, th_target, th_ilen, th_olen):\n            th_pred = th_pred.log_softmax(2)\n            loss = _ctcloss_sum(th_pred, th_target, th_ilen, th_olen)\n            # Batch-size average\n            loss = loss / th_pred.size(1)\n            return loss\n\n    n_out = 7\n    input_length = numpy.array(in_length, dtype=numpy.int32)\n    label_length = numpy.array(out_length, dtype=numpy.int32)\n    np_pred = [\n        numpy.random.rand(il, n_out).astype(numpy.float32) for il in input_length\n    ]\n    np_target = [\n        numpy.random.randint(0, n_out, size=ol, dtype=numpy.int32)\n        for ol in label_length\n    ]\n\n    # NOTE: np_pred[i] seems to be transposed and used axis=-1 in e2e_asr.py\n    ch_pred = F.separate(F.pad_sequence(np_pred), axis=-2)\n    ch_target = F.pad_sequence(np_target, padding=-1)\n    ch_loss = F.connectionist_temporal_classification(\n        ch_pred, ch_target, 0, input_length, label_length\n    ).data\n\n    th_pred = pad_list([torch.from_numpy(x) for x in np_pred], 0.0).transpose(0, 1)\n    th_target = torch.from_numpy(numpy.concatenate(np_target))\n    th_ilen = torch.from_numpy(input_length)\n    th_olen = torch.from_numpy(label_length)\n    th_loss = torch_ctcloss(th_pred, th_target, th_ilen, th_olen).numpy()\n    numpy.testing.assert_allclose(th_loss, ch_loss, 0.05)\n\n\ndef test_attn_loss():\n    n_out = 7\n    _eos = n_out - 1\n    n_batch = 3\n    label_length = numpy.array([4, 2, 3], dtype=numpy.int32)\n    np_pred = numpy.random.rand(n_batch, max(label_length) + 1, n_out).astype(\n        numpy.float32\n    )\n    # NOTE: 0 is only used for CTC, never appeared in attn target\n    np_target = [\n        numpy.random.randint(1, n_out - 1, size=ol, dtype=numpy.int32)\n        for ol in label_length\n    ]\n\n    eos = numpy.array([_eos], ""i"")\n    ys_out = [F.concat([y, eos], axis=0) for y in np_target]\n\n    # padding for ys with -1\n    # pys: utt x olen\n    # NOTE: -1 is default ignore index for chainer\n    pad_ys_out = F.pad_sequence(ys_out, padding=-1)\n    y_all = F.reshape(np_pred, (n_batch * (max(label_length) + 1), n_out))\n    ch_loss = F.softmax_cross_entropy(y_all, F.concat(pad_ys_out, axis=0))\n\n    # NOTE: this index 0 is only for CTC not attn. so it can be ignored\n    # unfortunately, torch cross_entropy does not accept out-of-bound ids\n    th_ignore = 0\n    th_pred = torch.from_numpy(y_all.data)\n    th_target = pad_list([torch.from_numpy(t.data).long() for t in ys_out], th_ignore)\n    if LooseVersion(torch.__version__) < LooseVersion(""1.0""):\n        reduction_str = ""elementwise_mean""\n    else:\n        reduction_str = ""mean""\n    th_loss = torch.nn.functional.cross_entropy(\n        th_pred, th_target.view(-1), ignore_index=th_ignore, reduction=reduction_str\n    )\n    print(ch_loss)\n    print(th_loss)\n\n    # NOTE: warpctc_pytorch.CTCLoss does not normalize itself by batch-size\n    # while chainer\'s default setting does\n    loss_data = float(th_loss)\n    numpy.testing.assert_allclose(loss_data, ch_loss.data, 0.05)\n\n\ndef test_train_acc():\n    n_out = 7\n    _eos = n_out - 1\n    n_batch = 3\n    label_length = numpy.array([4, 2, 3], dtype=numpy.int32)\n    np_pred = numpy.random.rand(n_batch, max(label_length) + 1, n_out).astype(\n        numpy.float32\n    )\n    # NOTE: 0 is only used for CTC, never appeared in attn target\n    np_target = [\n        numpy.random.randint(1, n_out - 1, size=ol, dtype=numpy.int32)\n        for ol in label_length\n    ]\n\n    eos = numpy.array([_eos], ""i"")\n    ys_out = [F.concat([y, eos], axis=0) for y in np_target]\n\n    # padding for ys with -1\n    # pys: utt x olen\n    # NOTE: -1 is default ignore index for chainer\n    pad_ys_out = F.pad_sequence(ys_out, padding=-1)\n    y_all = F.reshape(np_pred, (n_batch * (max(label_length) + 1), n_out))\n    ch_acc = F.accuracy(y_all, F.concat(pad_ys_out, axis=0), ignore_label=-1)\n\n    # NOTE: this index 0 is only for CTC not attn. so it can be ignored\n    # unfortunately, torch cross_entropy does not accept out-of-bound ids\n    th_ignore = 0\n    th_pred = torch.from_numpy(y_all.data)\n    th_ys = [torch.from_numpy(numpy.append(t, eos)).long() for t in np_target]\n    th_target = pad_list(th_ys, th_ignore)\n    th_acc = th_accuracy(th_pred, th_target, th_ignore)\n\n    numpy.testing.assert_allclose(ch_acc.data, th_acc)\n'"
test/test_multi_spkrs.py,14,"b'# coding: utf-8\n\n# Copyright 2018 Hiroshi Seki\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport importlib\nimport numpy\nimport re\nimport torch\n\nimport pytest\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        aconv_chans=10,\n        aconv_filts=100,\n        adim=320,\n        aheads=4,\n        apply_uttmvn=False,\n        atype=""location"",\n        awin=5,\n        badim=320,\n        batch_bins=0,\n        batch_count=""auto"",\n        batch_frames_in=0,\n        batch_frames_inout=0,\n        batch_frames_out=0,\n        batch_size=10,\n        bdropout_rate=0.0,\n        beam_size=3,\n        blayers=2,\n        bnmask=3,\n        bprojs=300,\n        btype=""blstmp"",\n        bunits=300,\n        char_list=[""a"", ""i"", ""u"", ""e"", ""o""],\n        context_residual=False,\n        ctc_type=""warpctc"",\n        ctc_weight=0.2,\n        dlayers=1,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        dtype=""lstm"",\n        dunits=300,\n        elayers_sd=1,\n        elayers=2,\n        etype=""vggblstmp"",\n        eprojs=100,\n        eunits=100,\n        fbank_fmax=None,\n        fbank_fmin=0.0,\n        fbank_fs=16000,\n        mtlalpha=0.5,\n        lsm_type="""",\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        nbest=5,\n        maxlenratio=1.0,\n        minlenratio=0.0,\n        n_mels=80,\n        num_spkrs=1,\n        outdir=None,\n        penalty=0.5,\n        ref_channel=0,\n        replace_sos=False,\n        spa=False,\n        stats_file=None,\n        subsample=""1_2_2_1_1"",\n        tgt_lang=False,\n        use_beamformer=False,\n        use_dnn_mask_for_wpe=False,\n        use_frontend=False,\n        use_wpe=False,\n        uttmvn_norm_means=False,\n        uttmvn_norm_vars=False,\n        verbose=2,\n        wdropout_rate=0.0,\n        weight_decay=0.0,\n        wlayers=2,\n        wpe_delay=3,\n        wpe_taps=5,\n        wprojs=300,\n        wtype=""blstmp"",\n        wunits=300,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef init_torch_weight_const(m, val):\n    for p in m.parameters():\n        p.data.fill_(val)\n\n\ndef init_chainer_weight_const(m, val):\n    for p in m.params():\n        p.data[:] = val\n\n\n@pytest.mark.parametrize(\n    (""etype"", ""dtype"", ""num_spkrs"", ""spa"", ""m_str"", ""text_idx1""),\n    [\n        (""vggblstmp"", ""lstm"", 2, True, ""espnet.nets.pytorch_backend.e2e_asr_mix"", 0),\n        (""vggbgrup"", ""gru"", 2, True, ""espnet.nets.pytorch_backend.e2e_asr_mix"", 1),\n    ],\n)\ndef test_recognition_results_multi_outputs(\n    etype, dtype, num_spkrs, spa, m_str, text_idx1\n):\n    const = 1e-4\n    numpy.random.seed(1)\n\n    # ctc_weight: 0.5 (hybrid CTC/attention), cannot be 0.0 (attention) or 1.0 (CTC)\n    for text_idx2, ctc_weight in enumerate([0.5]):\n        args = make_arg(\n            etype=etype, ctc_weight=ctc_weight, num_spkrs=num_spkrs, spa=spa\n        )\n        m = importlib.import_module(m_str)\n        model = m.E2E(40, 5, args)\n\n        if ""pytorch"" in m_str:\n            init_torch_weight_const(model, const)\n        else:\n            init_chainer_weight_const(model, const)\n\n        data = [\n            (\n                ""aaa"",\n                dict(\n                    feat=numpy.random.randn(100, 40).astype(numpy.float32),\n                    token=["""", """"],\n                ),\n            )\n        ]\n\n        in_data = data[0][1][""feat""]\n        nbest_hyps = model.recognize(in_data, args, args.char_list)\n\n        for i in range(num_spkrs):\n            y_hat = nbest_hyps[i][0][""yseq""][1:]\n            seq_hat = [args.char_list[int(idx)] for idx in y_hat]\n            seq_hat_text = """".join(seq_hat).replace(""<space>"", "" "")\n\n            assert re.match(r""[aiueo]+"", seq_hat_text)\n\n\n@pytest.mark.parametrize(\n    (""etype"", ""dtype"", ""num_spkrs"", ""m_str"", ""data_idx""),\n    [(""vggblstmp"", ""lstm"", 2, ""espnet.nets.pytorch_backend.e2e_asr_mix"", 0)],\n)\ndef test_pit_process(etype, dtype, num_spkrs, m_str, data_idx):\n    bs = 10\n    m = importlib.import_module(m_str)\n\n    losses_2 = torch.ones([bs, 4], dtype=torch.float32)\n    for i in range(bs):\n        losses_2[i][i % 4] = 0\n    true_losses_2 = torch.ones(bs, dtype=torch.float32) / 2\n    perm_choices_2 = [[0, 1], [1, 0], [1, 0], [0, 1]]\n    true_perm_2 = []\n    for i in range(bs):\n        true_perm_2.append(perm_choices_2[i % 4])\n    true_perm_2 = torch.tensor(true_perm_2).long()\n\n    losses = [losses_2]\n    true_losses = [torch.mean(true_losses_2)]\n    true_perm = [true_perm_2]\n\n    args = make_arg(etype=etype, num_spkrs=num_spkrs)\n    model = m.E2E(40, 5, args)\n    min_loss, min_perm = model.pit.pit_process(losses[data_idx])\n\n    assert min_loss == true_losses[data_idx]\n    assert torch.equal(min_perm, true_perm[data_idx])\n\n\n@pytest.mark.parametrize(\n    (""use_frontend"", ""use_beamformer"", ""bnmask"", ""num_spkrs"", ""m_str""),\n    [(True, True, 3, 2, ""espnet.nets.pytorch_backend.e2e_asr_mix"")],\n)\ndef test_dnn_beamformer(use_frontend, use_beamformer, bnmask, num_spkrs, m_str):\n    bs = 4\n    m = importlib.import_module(m_str)\n    const = 1e-4\n    numpy.random.seed(1)\n\n    args = make_arg(\n        use_frontend=use_frontend,\n        use_beamformer=use_beamformer,\n        bnmask=bnmask,\n        num_spkrs=num_spkrs,\n    )\n    model = m.E2E(257, 5, args)\n    beamformer = model.frontend.beamformer\n    mask_estimator = beamformer.mask\n\n    if ""pytorch"" in m_str:\n        init_torch_weight_const(model, const)\n    else:\n        init_chainer_weight_const(model, const)\n\n    # STFT feature\n    feat_real = torch.from_numpy(numpy.random.uniform(size=(bs, 100, 2, 257))).float()\n    feat_imag = torch.from_numpy(numpy.random.uniform(size=(bs, 100, 2, 257))).float()\n    feat = m.to_torch_tensor({""real"": feat_real, ""imag"": feat_imag})\n    ilens = torch.tensor([100] * bs).long()\n\n    # dnn_beamformer\n    enhanced, ilens, mask_speeches = beamformer(feat, ilens)\n    assert (bnmask - 1) == len(mask_speeches)\n    assert (bnmask - 1) == len(enhanced)\n\n    # beamforming by hand\n    feat = feat.permute(0, 3, 2, 1)\n    masks, _ = mask_estimator(feat, ilens)\n    mask_speech1, mask_speech2, mask_noise = masks\n\n    b = importlib.import_module(""espnet.nets.pytorch_backend.frontends.beamformer"")\n\n    psd_speech1 = b.get_power_spectral_density_matrix(feat, mask_speech1)\n    psd_speech2 = b.get_power_spectral_density_matrix(feat, mask_speech2)\n    psd_noise = b.get_power_spectral_density_matrix(feat, mask_noise)\n\n    u1 = torch.zeros(*(feat.size()[:-3] + (feat.size(-2),)), device=feat.device)\n    u1[..., args.ref_channel].fill_(1)\n    u2 = torch.zeros(*(feat.size()[:-3] + (feat.size(-2),)), device=feat.device)\n    u2[..., args.ref_channel].fill_(1)\n\n    ws1 = b.get_mvdr_vector(psd_speech1, psd_speech2 + psd_noise, u1)\n    ws2 = b.get_mvdr_vector(psd_speech2, psd_speech1 + psd_noise, u2)\n\n    enhanced1 = b.apply_beamforming_vector(ws1, feat).transpose(-1, -2)\n    enhanced2 = b.apply_beamforming_vector(ws2, feat).transpose(-1, -2)\n\n    assert torch.equal(enhanced1.real, enhanced[0].real)\n    assert torch.equal(enhanced2.real, enhanced[1].real)\n    assert torch.equal(enhanced1.imag, enhanced[0].imag)\n    assert torch.equal(enhanced2.imag, enhanced[1].imag)\n'"
test/test_optimizer.py,7,"b'# coding: utf-8\n\n# Copyright 2017 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport chainer\nimport numpy\nimport pytest\nimport torch\n\nfrom espnet.optimizer.factory import dynamic_import_optimizer\nfrom espnet.optimizer.pytorch import OPTIMIZER_FACTORY_DICT\n\n\nclass ChModel(chainer.Chain):\n    def __init__(self):\n        super(ChModel, self).__init__()\n        with self.init_scope():\n            self.a = chainer.links.Linear(3, 1)\n\n    def __call__(self, x):\n        return chainer.functions.sum(self.a(x))\n\n\nclass ThModel(torch.nn.Module):\n    def __init__(self):\n        super(ThModel, self).__init__()\n        self.a = torch.nn.Linear(3, 1)\n\n    def forward(self, x):\n        return self.a(x).sum()\n\n\n@pytest.mark.parametrize(""name"", OPTIMIZER_FACTORY_DICT.keys())\ndef test_optimizer_backend_compatible(name):\n    torch.set_grad_enabled(True)\n    # model construction\n    ch_model = ChModel()\n    th_model = ThModel()\n\n    # copy params\n    th_model.a.weight.data = torch.from_numpy(numpy.copy(ch_model.a.W.data))\n    th_model.a.bias.data = torch.from_numpy(numpy.copy(ch_model.a.b.data))\n\n    # optimizer setup\n    th_opt = dynamic_import_optimizer(name, ""pytorch"").build(th_model.parameters())\n    ch_opt = dynamic_import_optimizer(name, ""chainer"").build(ch_model)\n\n    # forward\n    ch_model.cleargrads()\n    data = numpy.random.randn(2, 3).astype(numpy.float32)\n    ch_loss = ch_model(data)\n    th_loss = th_model(torch.from_numpy(data))\n    chainer.functions.sum(ch_loss).backward()\n    th_loss.backward()\n    numpy.testing.assert_allclose(ch_loss.data, th_loss.item(), rtol=1e-6)\n    ch_opt.update()\n    th_opt.step()\n    numpy.testing.assert_allclose(\n        ch_model.a.W.data, th_model.a.weight.data.numpy(), rtol=1e-6\n    )\n    numpy.testing.assert_allclose(\n        ch_model.a.b.data, th_model.a.bias.data.numpy(), rtol=1e-6\n    )\n\n\ndef test_pytorch_optimizer_factory():\n    model = torch.nn.Linear(2, 1)\n    opt_class = dynamic_import_optimizer(""adam"", ""pytorch"")\n    optimizer = opt_class.build(model.parameters(), lr=0.9)\n    for g in optimizer.param_groups:\n        assert g[""lr""] == 0.9\n\n    opt_class = dynamic_import_optimizer(""sgd"", ""pytorch"")\n    optimizer = opt_class.build(model.parameters(), lr=0.9)\n    for g in optimizer.param_groups:\n        assert g[""lr""] == 0.9\n\n    opt_class = dynamic_import_optimizer(""adadelta"", ""pytorch"")\n    optimizer = opt_class.build(model.parameters(), rho=0.9)\n    for g in optimizer.param_groups:\n        assert g[""rho""] == 0.9\n\n\ndef test_chainer_optimizer_factory():\n    model = chainer.links.Linear(2, 1)\n    opt_class = dynamic_import_optimizer(""adam"", ""chainer"")\n    optimizer = opt_class.build(model, lr=0.9)\n    assert optimizer.alpha == 0.9\n\n    opt_class = dynamic_import_optimizer(""sgd"", ""chainer"")\n    optimizer = opt_class.build(model, lr=0.9)\n    assert optimizer.lr == 0.9\n\n    opt_class = dynamic_import_optimizer(""adadelta"", ""chainer"")\n    optimizer = opt_class.build(model, rho=0.9)\n    assert optimizer.rho == 0.9\n'"
test/test_positional_encoding.py,29,"b'import pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.embedding import ScaledPositionalEncoding\n\n\n@pytest.mark.parametrize(\n    ""dtype, device"",\n    [(dt, dv) for dt in (""float32"", ""float64"") for dv in (""cpu"", ""cuda"")],\n)\ndef test_pe_extendable(dtype, device):\n    if device == ""cuda"" and not torch.cuda.is_available():\n        pytest.skip(""no cuda device is available"")\n    dtype = getattr(torch, dtype)\n    dim = 2\n    pe = PositionalEncoding(dim, 0.0, 3).to(dtype=dtype, device=device)\n    x = torch.rand(2, 3, dim, dtype=dtype, device=device)\n    y = pe(x)\n    init_cache = pe.pe\n\n    # test not extended from init\n    x = torch.rand(2, 3, dim, dtype=dtype, device=device)\n    y = pe(x)\n    assert pe.pe is init_cache\n\n    x = torch.rand(2, 5, dim, dtype=dtype, device=device)\n    y = pe(x)\n\n    sd = pe.state_dict()\n    assert len(sd) == 0, ""PositionalEncoding should save nothing""\n    pe2 = PositionalEncoding(dim, 0.0, 3).to(dtype=dtype, device=device)\n    pe2.load_state_dict(sd)\n    y2 = pe2(x)\n    assert torch.allclose(y, y2)\n\n\n@pytest.mark.parametrize(\n    ""dtype, device"",\n    [(dt, dv) for dt in (""float32"", ""float64"") for dv in (""cpu"", ""cuda"")],\n)\ndef test_scaled_pe_extendable(dtype, device):\n    if device == ""cuda"" and not torch.cuda.is_available():\n        pytest.skip(""no cuda device is available"")\n    dtype = getattr(torch, dtype)\n    dim = 2\n    pe = ScaledPositionalEncoding(dim, 0.0, 3).to(dtype=dtype, device=device)\n    x = torch.rand(2, 3, dim, dtype=dtype, device=device)\n    y = pe(x)\n    init_cache = pe.pe\n\n    # test not extended from init\n    x = torch.rand(2, 3, dim, dtype=dtype, device=device)\n    y = pe(x)\n    assert pe.pe is init_cache\n\n    x = torch.rand(2, 5, dim, dtype=dtype, device=device)\n    y = pe(x)\n\n    sd = pe.state_dict()\n    assert sd == {""alpha"": pe.alpha}, ""ScaledPositionalEncoding should save only alpha""\n    pe2 = ScaledPositionalEncoding(dim, 0.0, 3).to(dtype=dtype, device=device)\n    pe2.load_state_dict(sd)\n    y2 = pe2(x)\n    assert torch.allclose(y, y2)\n\n\nclass LegacyPositionalEncoding(torch.nn.Module):\n    """"""Positional encoding module until v.0.5.2.""""""\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        import math\n\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.max_len = max_len\n        self.xscale = math.sqrt(d_model)\n        self.register_buffer(""pe"", pe)\n\n    def forward(self, x):\n        x = x * self.xscale + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass LegacyScaledPositionalEncoding(LegacyPositionalEncoding):\n    """"""Positional encoding module until v.0.5.2.""""""\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        super().__init__(d_model=d_model, dropout_rate=dropout_rate, max_len=max_len)\n        self.alpha = torch.nn.Parameter(torch.tensor(1.0))\n\n    def forward(self, x):\n        x = x + self.alpha * self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\ndef test_compatibility():\n    """"""Regression test for #1121""""""\n    x = torch.rand(2, 3, 4)\n\n    legacy_net = torch.nn.Sequential(\n        LegacyPositionalEncoding(4, 0.0), torch.nn.Linear(4, 2)\n    )\n\n    latest_net = torch.nn.Sequential(PositionalEncoding(4, 0.0), torch.nn.Linear(4, 2))\n\n    latest_net.load_state_dict(legacy_net.state_dict())\n    legacy = legacy_net(x)\n    latest = latest_net(x)\n    assert torch.allclose(legacy, latest)\n\n    legacy_net = torch.nn.Sequential(\n        LegacyScaledPositionalEncoding(4, 0.0), torch.nn.Linear(4, 2)\n    )\n\n    latest_net = torch.nn.Sequential(\n        ScaledPositionalEncoding(4, 0.0), torch.nn.Linear(4, 2)\n    )\n\n    latest_net.load_state_dict(legacy_net.state_dict())\n    legacy = legacy_net(x)\n    latest = latest_net(x)\n    assert torch.allclose(legacy, latest)\n'"
test/test_recog.py,11,"b'# coding: utf-8\n\n# Copyright 2018 Hiroshi Seki\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nfrom distutils.version import LooseVersion\nimport importlib\n\nimport numpy\nimport pytest\nimport torch\n\nimport espnet.lm.chainer_backend.lm as lm_chainer\nimport espnet.lm.pytorch_backend.extlm as extlm_pytorch\nimport espnet.nets.pytorch_backend.lm.default as lm_pytorch\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(""1.2.0"")\n\n\ndef make_arg(**kwargs):\n    defaults = dict(\n        elayers=4,\n        subsample=""1_2_2_1_1"",\n        etype=""blstmp"",\n        eunits=100,\n        eprojs=100,\n        dtype=""lstm"",\n        dlayers=1,\n        dunits=300,\n        atype=""location"",\n        aconv_chans=10,\n        aconv_filts=100,\n        mtlalpha=0.5,\n        lsm_type="""",\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        adim=320,\n        dropout_rate=0.0,\n        dropout_rate_decoder=0.0,\n        nbest=5,\n        beam_size=3,\n        penalty=0.5,\n        maxlenratio=1.0,\n        minlenratio=0.0,\n        ctc_weight=0.2,\n        ctc_window_margin=0,\n        verbose=2,\n        char_list=[""a"", ""i"", ""u"", ""e"", ""o""],\n        word_list=[""<blank>"", ""<unk>"", ""ai"", ""iu"", ""ue"", ""eo"", ""oa"", ""<eos>""],\n        outdir=None,\n        ctc_type=""warpctc"",\n        report_cer=False,\n        report_wer=False,\n        sym_space=""<space>"",\n        sym_blank=""<blank>"",\n        context_residual=False,\n        use_frontend=False,\n        replace_sos=False,\n        tgt_lang=False,\n    )\n    defaults.update(kwargs)\n    return argparse.Namespace(**defaults)\n\n\ndef init_torch_weight_const(m, val):\n    for p in m.parameters():\n        p.data.fill_(val)\n\n\ndef init_torch_weight_random(m, rand_range):\n    for name, p in m.named_parameters():\n        p.data.uniform_(rand_range[0], rand_range[1])\n        # set small bias for <blank> output\n        if ""wordlm.lo.bias"" in name or ""dec.output.bias"" in name:\n            p.data[0] = -10.0\n\n\ndef init_chainer_weight_const(m, val):\n    for p in m.params():\n        p.data[:] = val\n\n\n@pytest.mark.skipif(is_torch_1_2_plus, reason=""pytestskip"")\n@pytest.mark.parametrize(\n    (""etype"", ""dtype"", ""m_str"", ""text_idx1""),\n    [\n        (""blstmp"", ""lstm"", ""espnet.nets.chainer_backend.e2e_asr"", 0),\n        (""blstmp"", ""lstm"", ""espnet.nets.pytorch_backend.e2e_asr"", 1),\n        (""vggblstmp"", ""lstm"", ""espnet.nets.chainer_backend.e2e_asr"", 2),\n        (""vggblstmp"", ""lstm"", ""espnet.nets.pytorch_backend.e2e_asr"", 3),\n        (""bgrup"", ""gru"", ""espnet.nets.chainer_backend.e2e_asr"", 4),\n        (""bgrup"", ""gru"", ""espnet.nets.pytorch_backend.e2e_asr"", 5),\n        (""vggbgrup"", ""gru"", ""espnet.nets.chainer_backend.e2e_asr"", 6),\n        (""vggbgrup"", ""gru"", ""espnet.nets.pytorch_backend.e2e_asr"", 7),\n    ],\n)\ndef test_recognition_results(etype, dtype, m_str, text_idx1):\n    const = 1e-4\n    numpy.random.seed(1)\n    seq_true_texts = [\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""iuiuiuiuiuiuiuo"", ""ieieieieieieieieo""],\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""iuiuiuiuiuiuiuo"", ""ieieieieieieieieo""],\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""iuiuiuiuiuiuiuo"", ""ieieieieieieieieo""],\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""iuiuiuiuiuiuiuo"", ""ieieieieieieieieo""],\n    ]\n\n    # ctc_weight: 0.0 (attention), 0.5 (hybrid CTC/attention), 1.0 (CTC)\n    for text_idx2, ctc_weight in enumerate([0.0, 0.5, 1.0]):\n        seq_true_text = seq_true_texts[text_idx1][text_idx2]\n\n        args = make_arg(etype=etype, ctc_weight=ctc_weight)\n        m = importlib.import_module(m_str)\n        model = m.E2E(40, 5, args)\n\n        if ""pytorch"" in m_str:\n            init_torch_weight_const(model, const)\n        else:\n            init_chainer_weight_const(model, const)\n\n        data = [\n            (\n                ""aaa"",\n                dict(\n                    feat=numpy.random.randn(100, 40).astype(numpy.float32),\n                    token=seq_true_text,\n                ),\n            )\n        ]\n\n        in_data = data[0][1][""feat""]\n        nbest_hyps = model.recognize(in_data, args, args.char_list)\n        y_hat = nbest_hyps[0][""yseq""][1:]\n        seq_hat = [args.char_list[int(idx)] for idx in y_hat]\n        seq_hat_text = """".join(seq_hat).replace(""<space>"", "" "")\n        seq_true_text = data[0][1][""token""]\n\n        assert seq_hat_text == seq_true_text\n\n\n@pytest.mark.skipif(is_torch_1_2_plus, reason=""pytestskip"")\n@pytest.mark.parametrize(\n    (""etype"", ""dtype"", ""m_str"", ""text_idx1""),\n    [\n        (""blstmp"", ""lstm"", ""espnet.nets.chainer_backend.e2e_asr"", 0),\n        (""blstmp"", ""lstm"", ""espnet.nets.pytorch_backend.e2e_asr"", 1),\n        (""vggblstmp"", ""lstm"", ""espnet.nets.chainer_backend.e2e_asr"", 2),\n        (""vggblstmp"", ""lstm"", ""espnet.nets.pytorch_backend.e2e_asr"", 3),\n        (""bgrup"", ""gru"", ""espnet.nets.chainer_backend.e2e_asr"", 4),\n        (""bgrup"", ""gru"", ""espnet.nets.pytorch_backend.e2e_asr"", 5),\n        (""vggbgrup"", ""gru"", ""espnet.nets.chainer_backend.e2e_asr"", 6),\n        (""vggbgrup"", ""gru"", ""espnet.nets.pytorch_backend.e2e_asr"", 7),\n    ],\n)\ndef test_recognition_results_with_lm(etype, dtype, m_str, text_idx1):\n    const = 1e-4\n    numpy.random.seed(1)\n    seq_true_texts = [\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""o"", ""ieieieieieieieieo""],\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""o"", ""ieieieieieieieieo""],\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""o"", ""ieieieieieieieieo""],\n        [""o"", ""iuiuiuiuiuiuiuiuo"", ""iuiuiuiuiuiuiuiuo""],\n        [""o"", ""o"", ""ieieieieieieieieo""],\n    ]\n\n    # ctc_weight: 0.0 (attention), 0.5 (hybrid CTC/attention), 1.0 (CTC)\n    for text_idx2, ctc_weight in enumerate([0.0, 0.5, 1.0]):\n        seq_true_text = seq_true_texts[text_idx1][text_idx2]\n\n        args = make_arg(\n            etype=etype, rnnlm=""dummy"", ctc_weight=ctc_weight, lm_weight=0.3\n        )\n        m = importlib.import_module(m_str)\n        model = m.E2E(40, 5, args)\n\n        if ""pytorch"" in m_str:\n            rnnlm = lm_pytorch.ClassifierWithState(\n                lm_pytorch.RNNLM(len(args.char_list), 2, 10)\n            )\n            init_torch_weight_const(model, const)\n            init_torch_weight_const(rnnlm, const)\n        else:\n            rnnlm = lm_chainer.ClassifierWithState(\n                lm_chainer.RNNLM(len(args.char_list), 2, 10)\n            )\n            init_chainer_weight_const(model, const)\n            init_chainer_weight_const(rnnlm, const)\n\n        data = [\n            (\n                ""aaa"",\n                dict(\n                    feat=numpy.random.randn(100, 40).astype(numpy.float32),\n                    token=seq_true_text,\n                ),\n            )\n        ]\n\n        in_data = data[0][1][""feat""]\n        nbest_hyps = model.recognize(in_data, args, args.char_list, rnnlm)\n        y_hat = nbest_hyps[0][""yseq""][1:]\n        seq_hat = [args.char_list[int(idx)] for idx in y_hat]\n        seq_hat_text = """".join(seq_hat).replace(""<space>"", "" "")\n        seq_true_text = data[0][1][""token""]\n\n        assert seq_hat_text == seq_true_text\n\n\n@pytest.mark.parametrize(\n    (""etype"", ""dtype"", ""m_str""),\n    [\n        (""blstmp"", ""lstm"", ""espnet.nets.chainer_backend.e2e_asr""),\n        (""blstmp"", ""lstm"", ""espnet.nets.pytorch_backend.e2e_asr""),\n        (""vggblstmp"", ""lstm"", ""espnet.nets.chainer_backend.e2e_asr""),\n        (""vggblstmp"", ""lstm"", ""espnet.nets.pytorch_backend.e2e_asr""),\n        (""bgrup"", ""gru"", ""espnet.nets.chainer_backend.e2e_asr""),\n        (""bgrup"", ""gru"", ""espnet.nets.pytorch_backend.e2e_asr""),\n        (""vggbgrup"", ""gru"", ""espnet.nets.chainer_backend.e2e_asr""),\n        (""vggbgrup"", ""gru"", ""espnet.nets.pytorch_backend.e2e_asr""),\n    ],\n)\ndef test_batch_beam_search(etype, dtype, m_str):\n    numpy.random.seed(1)\n\n    # ctc_weight: 0.0 (attention), 0.5 (hybrid CTC/attention), 1.0 (CTC)\n    for ctc_weight in [0.0, 0.5, 1.0]:\n        args = make_arg(\n            etype=etype, rnnlm=""dummy"", ctc_weight=ctc_weight, lm_weight=0.3\n        )\n        m = importlib.import_module(m_str)\n        model = m.E2E(40, 5, args)\n\n        if ""pytorch"" in m_str:\n            torch.manual_seed(1)\n            rnnlm = lm_pytorch.ClassifierWithState(\n                lm_pytorch.RNNLM(len(args.char_list), 2, 10)\n            )\n            init_torch_weight_random(model, (-0.1, 0.1))\n            init_torch_weight_random(rnnlm, (-0.1, 0.1))\n            model.eval()\n            rnnlm.eval()\n        else:\n            # chainer module\n            continue\n\n        data = [(""aaa"", dict(feat=numpy.random.randn(100, 40).astype(numpy.float32)))]\n        in_data = data[0][1][""feat""]\n\n        for lm_weight in [0.0, 0.3]:\n            if lm_weight == 0.0:\n                s_nbest_hyps = model.recognize(in_data, args, args.char_list)\n                b_nbest_hyps = model.recognize_batch([in_data], args, args.char_list)\n            else:\n                s_nbest_hyps = model.recognize(in_data, args, args.char_list, rnnlm)\n                b_nbest_hyps = model.recognize_batch(\n                    [in_data], args, args.char_list, rnnlm\n                )\n\n            assert s_nbest_hyps[0][""yseq""] == b_nbest_hyps[0][0][""yseq""]\n\n        if ctc_weight > 0.0:\n            args.ctc_window_margin = 40\n            s_nbest_hyps = model.recognize(in_data, args, args.char_list, rnnlm)\n            b_nbest_hyps = model.recognize_batch([in_data], args, args.char_list, rnnlm)\n            assert s_nbest_hyps[0][""yseq""] == b_nbest_hyps[0][0][""yseq""]\n\n        # Test word LM in batch decoding\n        if ""pytorch"" in m_str:\n            rand_range = (-0.01, 0.01)\n            torch.manual_seed(1)\n            char_list = [""<blank>"", ""<space>""] + args.char_list + [""<eos>""]\n            args = make_arg(\n                etype=etype,\n                rnnlm=""dummy"",\n                ctc_weight=ctc_weight,\n                ctc_window_margin=40,\n                lm_weight=0.3,\n                beam_size=5,\n            )\n            m = importlib.import_module(m_str)\n            model = m.E2E(40, len(char_list), args)\n\n            char_dict = {x: i for i, x in enumerate(char_list)}\n            word_dict = {x: i for i, x in enumerate(args.word_list)}\n\n            word_rnnlm = lm_pytorch.ClassifierWithState(\n                lm_pytorch.RNNLM(len(args.word_list), 2, 10)\n            )\n            rnnlm = lm_pytorch.ClassifierWithState(\n                extlm_pytorch.LookAheadWordLM(\n                    word_rnnlm.predictor, word_dict, char_dict\n                )\n            )\n            init_torch_weight_random(model, rand_range)\n            init_torch_weight_random(rnnlm, rand_range)\n            model.eval()\n            rnnlm.eval()\n            s_nbest_hyps = model.recognize(in_data, args, char_list, rnnlm)\n            b_nbest_hyps = model.recognize_batch([in_data], args, char_list, rnnlm)\n            assert s_nbest_hyps[0][""yseq""] == b_nbest_hyps[0][0][""yseq""]\n'"
test/test_scheduler.py,2,"b'from espnet.scheduler.chainer import ChainerScheduler\nfrom espnet.scheduler.pytorch import PyTorchScheduler\nfrom espnet.scheduler import scheduler\n\nimport chainer\nimport numpy\nimport pytest\nimport torch\n\n\n@pytest.mark.parametrize(""name"", scheduler.SCHEDULER_DICT.keys())\ndef test_scheduler(name):\n    s = scheduler.dynamic_import_scheduler(name).build(""lr"")\n    assert s.key == ""lr""\n    assert isinstance(s.scale(0), float)\n    assert isinstance(s.scale(1000), float)\n\n\ndef test_pytorch_scheduler():\n    warmup = 30000\n    s = scheduler.NoamScheduler.build(""lr"", warmup=warmup)\n    net = torch.nn.Linear(2, 1)\n    o = torch.optim.SGD(net.parameters(), lr=1.0)\n    so = PyTorchScheduler([s], o)\n    so.step(0)\n    for g in o.param_groups:\n        assert g[""lr""] == s.scale(0)\n\n    so.step(warmup)\n    for g in o.param_groups:\n        numpy.testing.assert_allclose(g[""lr""], 1.0, rtol=1e-4)\n\n\ndef test_chainer_scheduler():\n    warmup = 30000\n    s = scheduler.NoamScheduler.build(""lr"", warmup=warmup)\n    net = chainer.links.Linear(2, 1)\n    o = chainer.optimizers.SGD(lr=1.0)\n    o.setup(net)\n    so = ChainerScheduler([s], o)\n    so.step(0)\n    assert o.lr == s.scale(0)\n\n    so.step(warmup)\n    numpy.testing.assert_allclose(o.lr, 1.0, rtol=1e-4)\n'"
test/test_sentencepiece.py,0,"b'import os\n\nimport sentencepiece as spm\n\n\nroot = os.path.dirname(os.path.abspath(__file__))\n\n\ndef test_spm_compatibility():\n    """"""""test python API with legacy C++ tool outputs\n\n    NOTE: hard-coded strings are generated by spm v0.1.82\n    """"""\n    testfile = root + ""/tedlium2.txt""\n    nbpe = 100\n    bpemode = ""unigram""\n    bpemodel = ""test_spm""\n\n    # test train\n    spm.SentencePieceTrainer.Train(\n        f""--input={testfile} --vocab_size={nbpe} --model_type={bpemode} \\\n          --model_prefix={bpemodel} --input_sentence_size=100000000 \\\n          --character_coverage=1.0 --bos_id=-1 --eos_id=-1 \\\n          --unk_id=0 --user_defined_symbols=[laughter],[noise],[vocalized-noise]""\n    )\n    with open(f""{bpemodel}.vocab"", ""r"") as fa, open(\n        root + ""/tedlium2.vocab"", ""r""\n    ) as fb:\n        for a, b in zip(fa, fb):\n            assert a == b\n\n    # test encode and decode\n    sp = spm.SentencePieceProcessor()\n    sp.Load(f""{bpemodel}.model"")\n    txt = ""test sentencepiece.[noise]""\n    actual = sp.EncodeAsPieces(txt)\n    expect = ""\xe2\x96\x81 te s t \xe2\x96\x81 s en t en c e p ie c e . [noise]"".split()\n    assert actual == expect\n    assert sp.DecodePieces(actual) == txt\n'"
test/test_tensorboard.py,0,"b'from collections import defaultdict\n\nimport chainer\nimport numpy\n\nfrom espnet.utils.training.evaluator import BaseEvaluator\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\n\n\nclass DummyWriter:\n    def __init__(self):\n        self.data = defaultdict(dict)\n\n    def add_scalar(self, k, v, n):\n        self.data[k][n] = v\n\n\ndef test_tensorboard_evaluator():\n    # setup model\n    model = chainer.links.Classifier(chainer.links.Linear(3, 2))\n    optimizer = chainer.optimizers.Adam()\n    optimizer.setup(model)\n\n    # setup data\n    data_size = 6\n    xs = numpy.random.randn(data_size, 3).astype(numpy.float32)\n    ys = (numpy.random.randn(data_size) > 1).astype(numpy.int32)\n    data = chainer.datasets.TupleDataset(xs, ys)\n    batch_size = 2\n    epoch = 10\n\n    # test runnable without tensorboard logger\n    trainer = chainer.training.Trainer(\n        chainer.training.StandardUpdater(\n            chainer.iterators.SerialIterator(data, batch_size), optimizer\n        ),\n        (epoch, ""epoch""),\n    )\n    trainer.extend(\n        BaseEvaluator(\n            chainer.iterators.SerialIterator(data, batch_size, repeat=False), model\n        )\n    )\n    trainer.run()\n\n    # test runnable with tensorboard logger\n    for log_interval in [1, 3]:\n        trainer = chainer.training.Trainer(\n            chainer.training.StandardUpdater(\n                chainer.iterators.SerialIterator(data, batch_size), optimizer\n            ),\n            (epoch, ""epoch""),\n        )\n        trainer.extend(\n            BaseEvaluator(\n                chainer.iterators.SerialIterator(data, batch_size, repeat=False), model\n            )\n        )\n        writer = DummyWriter()\n        trainer.extend(TensorboardLogger(writer), trigger=(log_interval, ""iteration""))\n        trainer.run()\n\n        # test the number of log entries\n        assert TensorboardLogger.default_name in trainer._extensions\n        assert (\n            len(writer.data[""main/loss""]) == trainer.updater.iteration // log_interval\n        )\n        assert len(writer.data[""validation/main/loss""]) == epoch\n'"
test/test_torch.py,5,"b'# coding: utf-8\n\n# Copyright 2017 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\nimport pytest\n\npytest.importorskip(""torch"")\nimport torch  # NOQA\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list  # NOQA\n\n\ndef test_pad_list():\n    xs = [[1, 2, 3], [1, 2], [1, 2, 3, 4]]\n    xs = list(map(lambda x: torch.LongTensor(x), xs))\n    xpad = pad_list(xs, -1)\n\n    es = [[1, 2, 3, -1], [1, 2, -1, -1], [1, 2, 3, 4]]\n    assert xpad.data.tolist() == es\n\n\ndef test_bmm_attention():\n    b, t, h = 3, 2, 5\n    enc_h = torch.randn(b, t, h)\n    w = torch.randn(b, t)\n    naive = torch.sum(enc_h * w.view(b, t, 1), dim=1)\n    # (b, 1, t) x (b, t, h) -> (b, 1, h)\n    fast = torch.matmul(w.unsqueeze(1), enc_h).squeeze(1)\n    import numpy\n\n    numpy.testing.assert_allclose(naive.numpy(), fast.numpy(), 1e-6, 1e-6)\n'"
test/test_train_dtype.py,6,"b'import pytest\nimport torch\n\nfrom espnet.nets.asr_interface import dynamic_import_asr\n\n\n@pytest.mark.parametrize(\n    ""dtype, device, model, conf"",\n    [\n        (dtype, device, nn, conf)\n        for nn, conf in [\n            (\n                ""transformer"",\n                dict(adim=4, eunits=3, dunits=3, elayers=2, dlayers=2, mtlalpha=0.0),\n            ),\n            (\n                ""transformer"",\n                dict(\n                    adim=4,\n                    eunits=3,\n                    dunits=3,\n                    elayers=2,\n                    dlayers=2,\n                    mtlalpha=0.5,\n                    ctc_type=""builtin"",\n                ),\n            ),\n            (\n                ""transformer"",\n                dict(\n                    adim=4,\n                    eunits=3,\n                    dunits=3,\n                    elayers=2,\n                    dlayers=2,\n                    mtlalpha=0.5,\n                    ctc_type=""warpctc"",\n                ),\n            ),\n            (\n                ""rnn"",\n                dict(adim=4, eunits=3, dunits=3, elayers=2, dlayers=2, mtlalpha=0.0),\n            ),\n            (\n                ""rnn"",\n                dict(\n                    adim=4,\n                    eunits=3,\n                    dunits=3,\n                    elayers=2,\n                    dlayers=2,\n                    mtlalpha=0.5,\n                    ctc_type=""builtin"",\n                ),\n            ),\n            (\n                ""rnn"",\n                dict(\n                    adim=4,\n                    eunits=3,\n                    dunits=3,\n                    elayers=2,\n                    dlayers=2,\n                    mtlalpha=0.5,\n                    ctc_type=""warpctc"",\n                ),\n            ),\n        ]\n        for dtype in (""float16"", ""float32"", ""float64"")\n        for device in (""cpu"", ""cuda"")\n    ],\n)\ndef test_train_pytorch_dtype(dtype, device, model, conf):\n    if device == ""cuda"" and not torch.cuda.is_available():\n        pytest.skip(""no cuda device is available"")\n    if device == ""cpu"" and dtype == ""float16"":\n        pytest.skip(""cpu float16 implementation is not available in pytorch yet"")\n\n    idim = 10\n    odim = 10\n    model = dynamic_import_asr(model, ""pytorch"").build(idim, odim, **conf)\n    dtype = getattr(torch, dtype)\n    device = torch.device(device)\n    model.to(dtype=dtype, device=device)\n\n    x = torch.rand(2, 10, idim, dtype=dtype, device=device)\n    ilens = torch.tensor([10, 7], device=device)\n    y = torch.randint(1, odim, (2, 3), device=device)\n    opt = torch.optim.Adam(model.parameters())\n    loss = model(x, ilens, y)\n    assert loss.dtype == dtype\n    model.zero_grad()\n    loss.backward()\n    assert any(p.grad is not None for p in model.parameters())\n    opt.step()\n'"
test/test_transform.py,0,"b'import kaldiio\nimport numpy as np\n\nfrom espnet.transform.add_deltas import add_deltas\nfrom espnet.transform.cmvn import CMVN\nfrom espnet.transform.functional import FuncTrans\nfrom espnet.transform.spectrogram import logmelspectrogram\nfrom espnet.transform.transformation import Transformation\n\n\ndef test_preprocessing(tmpdir):\n    cmvn_ark = str(tmpdir.join(""cmvn.ark""))\n    kwargs = {\n        ""process"": [\n            {""type"": ""fbank"", ""n_mels"": 80, ""fs"": 16000, ""n_fft"": 1024, ""n_shift"": 512},\n            {""type"": ""cmvn"", ""stats"": cmvn_ark, ""norm_vars"": True},\n            {""type"": ""delta"", ""window"": 2, ""order"": 2},\n        ],\n        ""mode"": ""sequential"",\n    }\n\n    # Creates cmvn_ark\n    samples = np.random.randn(100, 80)\n    stats = np.empty((2, 81), dtype=np.float32)\n    stats[0, :80] = samples.sum(axis=0)\n    stats[1, :80] = (samples ** 2).sum(axis=0)\n    stats[0, -1] = 100.0\n    stats[1, -1] = 0.0\n    kaldiio.save_mat(cmvn_ark, stats)\n\n    bs = 1\n    xs = [np.random.randn(1000).astype(np.float32) for _ in range(bs)]\n    preprocessing = Transformation(kwargs)\n    processed_xs = preprocessing(xs)\n\n    for idx, x in enumerate(xs):\n        opt = dict(kwargs[""process""][0])\n        opt.pop(""type"")\n        x = logmelspectrogram(x, **opt)\n\n        opt = dict(kwargs[""process""][1])\n        opt.pop(""type"")\n        x = CMVN(**opt)(x)\n\n        opt = dict(kwargs[""process""][2])\n        opt.pop(""type"")\n        x = add_deltas(x, **opt)\n\n        np.testing.assert_allclose(processed_xs[idx], x)\n\n\ndef test_optional_args():\n    kwargs = {\n        ""process"": [\n            {\n                ""type"": ""channel_selector"",\n                ""train_channel"": 0,\n                ""eval_channel"": 1,\n                ""axis"": 0,\n            }\n        ],\n        ""mode"": ""sequential"",\n    }\n    preprocessing = Transformation(kwargs)\n    assert preprocessing(np.array([100, 200]), train=True) == 100\n    assert preprocessing(np.array([100, 200]), train=False) == 200\n\n\ndef test_func_trans():\n    def foo_bar(x, a=1, b=2):\n        """"""Foo bar\n\n        :param x: input\n        :param int a: default 1\n        :param int b: default 2\n        """"""\n        return x + a - b\n\n    class FooBar(FuncTrans):\n        _func = foo_bar\n        __doc__ = foo_bar.__doc__\n\n    assert FooBar(a=2)(0) == 0\n    try:\n        FooBar(d=1)\n    except TypeError as e:\n        raised = True\n        assert str(e) == ""foo_bar() got an unexpected keyword argument \'d\'""\n    assert raised\n    assert str(FooBar(a=100)) == ""FooBar(a=100, b=2)""\n\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    FooBar.add_arguments(parser)\n    # NOTE: index 0 is help\n    assert parser._actions[1].option_strings == [""--foo-bar-a""]\n    assert parser._actions[1].default == 1\n    assert parser._actions[1].type == int\n    assert parser._actions[2].option_strings == [""--foo-bar-b""]\n    assert parser._actions[2].default == 2\n    assert parser._actions[2].type == int\n'"
test/test_transformer_decode.py,10,"b'import numpy\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.transformer.decoder import Decoder\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\n\n\nRTOL = 1e-4\n\n\n@pytest.mark.parametrize(""normalize_before"", [True, False])\ndef test_decoder_cache(normalize_before):\n    adim = 4\n    odim = 5\n    decoder = Decoder(\n        odim=odim,\n        attention_dim=adim,\n        linear_units=3,\n        num_blocks=2,\n        normalize_before=normalize_before,\n        dropout_rate=0.0,\n    )\n    dlayer = decoder.decoders[0]\n    memory = torch.randn(2, 5, adim)\n\n    x = torch.randn(2, 5, adim) * 100\n    mask = subsequent_mask(x.shape[1]).unsqueeze(0)\n    prev_mask = mask[:, :-1, :-1]\n    decoder.eval()\n    with torch.no_grad():\n        # layer-level test\n        y = dlayer(x, mask, memory, None)[0]\n        cache = dlayer(x[:, :-1], prev_mask, memory, None)[0]\n        y_fast = dlayer(x, mask, memory, None, cache=cache)[0]\n        numpy.testing.assert_allclose(y.numpy(), y_fast.numpy(), rtol=RTOL)\n\n        # decoder-level test\n        x = torch.randint(0, odim, x.shape[:2])\n        y, _ = decoder.forward_one_step(x, mask, memory)\n        y_, cache = decoder.forward_one_step(\n            x[:, :-1], prev_mask, memory, cache=decoder.init_state(None)\n        )\n        y_fast, _ = decoder.forward_one_step(x, mask, memory, cache=cache)\n        numpy.testing.assert_allclose(y.numpy(), y_fast.numpy(), rtol=RTOL)\n\n\n@pytest.mark.parametrize(""normalize_before"", [True, False])\ndef test_encoder_cache(normalize_before):\n    adim = 4\n    idim = 5\n    encoder = Encoder(\n        idim=idim,\n        attention_dim=adim,\n        linear_units=3,\n        num_blocks=2,\n        normalize_before=normalize_before,\n        dropout_rate=0.0,\n        input_layer=""embed"",\n    )\n    elayer = encoder.encoders[0]\n    x = torch.randn(2, 5, adim)\n    mask = subsequent_mask(x.shape[1]).unsqueeze(0)\n    prev_mask = mask[:, :-1, :-1]\n    encoder.eval()\n    with torch.no_grad():\n        # layer-level test\n        y = elayer(x, mask, None)[0]\n        cache = elayer(x[:, :-1], prev_mask, None)[0]\n        y_fast = elayer(x, mask, cache=cache)[0]\n        numpy.testing.assert_allclose(y.numpy(), y_fast.numpy(), rtol=RTOL)\n\n        # encoder-level test\n        x = torch.randint(0, idim, x.shape[:2])\n        y = encoder.forward_one_step(x, mask)[0]\n        y_, _, cache = encoder.forward_one_step(x[:, :-1], prev_mask)\n        y_fast, _, _ = encoder.forward_one_step(x, mask, cache=cache)\n        numpy.testing.assert_allclose(y.numpy(), y_fast.numpy(), rtol=RTOL)\n\n\nif __name__ == ""__main__"":\n    # benchmark with synth dataset\n    from time import time\n\n    import matplotlib.pyplot as plt\n\n    adim = 4\n    odim = 5\n    model = ""decoder""\n    if model == ""decoder"":\n        decoder = Decoder(\n            odim=odim,\n            attention_dim=adim,\n            linear_units=3,\n            num_blocks=2,\n            dropout_rate=0.0,\n        )\n        decoder.eval()\n    else:\n        encoder = Encoder(\n            idim=odim,\n            attention_dim=adim,\n            linear_units=3,\n            num_blocks=2,\n            dropout_rate=0.0,\n            input_layer=""embed"",\n        )\n        encoder.eval()\n\n    xlen = 100\n    xs = torch.randint(0, odim, (1, xlen))\n    memory = torch.randn(2, 500, adim)\n    mask = subsequent_mask(xlen).unsqueeze(0)\n\n    result = {""cached"": [], ""baseline"": []}\n    n_avg = 10\n    for key, value in result.items():\n        cache = None\n        print(key)\n        for i in range(xlen):\n            x = xs[:, : i + 1]\n            m = mask[:, : i + 1, : i + 1]\n            start = time()\n            for _ in range(n_avg):\n                with torch.no_grad():\n                    if key == ""baseline"":\n                        cache = None\n                    if model == ""decoder"":\n                        y, new_cache = decoder.forward_one_step(\n                            x, m, memory, cache=cache\n                        )\n                    else:\n                        y, _, new_cache = encoder.forward_one_step(x, m, cache=cache)\n            if key == ""cached"":\n                cache = new_cache\n            dur = (time() - start) / n_avg\n            value.append(dur)\n        plt.plot(range(xlen), value, label=key)\n    plt.xlabel(""hypothesis length"")\n    plt.ylabel(""average time [sec]"")\n    plt.grid()\n    plt.legend()\n    plt.savefig(f""benchmark_{model}.png"")\n'"
test/test_utils.py,0,"b'#!/usr/bin/env python3\nimport h5py\nimport kaldiio\nimport numpy as np\nimport pytest\n\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.io_utils import SoundHDF5File\nfrom espnet.utils.training.batchfy import make_batchset\nfrom test.utils_test import make_dummy_json\n\n\n@pytest.mark.parametrize(""swap_io"", [True, False])\ndef test_make_batchset(swap_io):\n    dummy_json = make_dummy_json(128, [128, 512], [16, 128])\n    # check w/o adaptive batch size\n    batchset = make_batchset(\n        dummy_json, 24, 2 ** 10, 2 ** 10, min_batch_size=1, swap_io=swap_io\n    )\n    assert sum([len(batch) >= 1 for batch in batchset]) == len(batchset)\n    print([len(batch) for batch in batchset])\n    batchset = make_batchset(\n        dummy_json, 24, 2 ** 10, 2 ** 10, min_batch_size=10, swap_io=swap_io\n    )\n    assert sum([len(batch) >= 10 for batch in batchset]) == len(batchset)\n    print([len(batch) for batch in batchset])\n\n    # check w/ adaptive batch size\n    batchset = make_batchset(\n        dummy_json, 24, 256, 64, min_batch_size=10, swap_io=swap_io\n    )\n    assert sum([len(batch) >= 10 for batch in batchset]) == len(batchset)\n    print([len(batch) for batch in batchset])\n    batchset = make_batchset(\n        dummy_json, 24, 256, 64, min_batch_size=10, swap_io=swap_io\n    )\n    assert sum([len(batch) >= 10 for batch in batchset]) == len(batchset)\n\n\n@pytest.mark.parametrize(""swap_io"", [True, False])\ndef test_sortagrad(swap_io):\n    dummy_json = make_dummy_json(128, [1, 700], [1, 700])\n    if swap_io:\n        batchset = make_batchset(\n            dummy_json,\n            16,\n            2 ** 10,\n            2 ** 10,\n            batch_sort_key=""input"",\n            shortest_first=True,\n            swap_io=True,\n        )\n        key = ""output""\n    else:\n        batchset = make_batchset(dummy_json, 16, 2 ** 10, 2 ** 10, shortest_first=True)\n        key = ""input""\n    prev_start_ilen = batchset[0][0][1][key][0][""shape""][0]\n    for batch in batchset:\n        cur_start_ilen = batch[0][1][key][0][""shape""][0]\n        assert cur_start_ilen >= prev_start_ilen\n        prev_ilen = cur_start_ilen\n        for sample in batch:\n            cur_ilen = sample[1][key][0][""shape""][0]\n            assert cur_ilen <= prev_ilen\n            prev_ilen = cur_ilen\n        prev_start_ilen = cur_start_ilen\n\n\ndef test_load_inputs_and_targets_legacy_format(tmpdir):\n    # batch = [(""F01_050C0101_PED_REAL"",\n    #          {""input"": [{""feat"": ""some/path.ark:123""}],\n    #           ""output"": [{""tokenid"": ""1 2 3 4""}],\n    ark = str(tmpdir.join(""test.ark""))\n    scp = str(tmpdir.join(""test.scp""))\n\n    desire_xs = []\n    desire_ys = []\n    with kaldiio.WriteHelper(""ark,scp:{},{}"".format(ark, scp)) as f:\n        for i in range(10):\n            x = np.random.random((100, 100)).astype(np.float32)\n            uttid = ""uttid{}"".format(i)\n            f[uttid] = x\n            desire_xs.append(x)\n            desire_ys.append(np.array([1, 2, 3, 4]))\n\n    batch = []\n    with open(scp, ""r"") as f:\n        for line in f:\n            uttid, path = line.strip().split()\n            batch.append(\n                (\n                    uttid,\n                    {\n                        ""input"": [{""feat"": path, ""name"": ""input1""}],\n                        ""output"": [{""tokenid"": ""1 2 3 4"", ""name"": ""target1""}],\n                    },\n                )\n            )\n\n    load_inputs_and_targets = LoadInputsAndTargets()\n    xs, ys = load_inputs_and_targets(batch)\n    for x, xd in zip(xs, desire_xs):\n        np.testing.assert_array_equal(x, xd)\n    for y, yd in zip(ys, desire_ys):\n        np.testing.assert_array_equal(y, yd)\n\n\ndef test_load_inputs_and_targets_legacy_format_multi_inputs(tmpdir):\n    # batch = [(""F01_050C0101_PED_REAL"",\n    #          {""input"": [{""feat"": ""some/path1.ark:123"",\n    #                      ""name"": ""input1""}\n    #                     {""feat"": ""some/path2.ark:123""\n    #                      ""name"": ""input2""}],\n    #           ""output"": [{""tokenid"": ""1 2 3 4""}],\n    ark_1 = str(tmpdir.join(""test_1.ark""))\n    scp_1 = str(tmpdir.join(""test_1.scp""))\n\n    ark_2 = str(tmpdir.join(""test_2.ark""))\n    scp_2 = str(tmpdir.join(""test_2.scp""))\n\n    desire_xs_1 = []\n    desire_xs_2 = []\n    desire_ys = []\n    with kaldiio.WriteHelper(""ark,scp:{},{}"".format(ark_1, scp_1)) as f:\n        for i in range(10):\n            x = np.random.random((100, 100)).astype(np.float32)\n            uttid = ""uttid{}"".format(i)\n            f[uttid] = x\n            desire_xs_1.append(x)\n            desire_ys.append(np.array([1, 2, 3, 4]))\n\n    with kaldiio.WriteHelper(""ark,scp:{},{}"".format(ark_2, scp_2)) as f:\n        for i in range(10):\n            x = np.random.random((100, 100)).astype(np.float32)\n            uttid = ""uttid{}"".format(i)\n            f[uttid] = x\n            desire_xs_2.append(x)\n            desire_ys.append(np.array([1, 2, 3, 4]))\n\n    batch = []\n    with open(scp_1, ""r"") as f:\n        lines_1 = f.readlines()\n    with open(scp_2, ""r"") as f:\n        lines_2 = f.readlines()\n\n    for line_1, line_2 in zip(lines_1, lines_2):\n        uttid, path_1 = line_1.strip().split()\n        uttid, path_2 = line_2.strip().split()\n        batch.append(\n            (\n                uttid,\n                {\n                    ""input"": [\n                        {""feat"": path_1, ""name"": ""input1""},\n                        {""feat"": path_2, ""name"": ""input2""},\n                    ],\n                    ""output"": [{""tokenid"": ""1 2 3 4"", ""name"": ""target1""}],\n                },\n            )\n        )\n\n    load_inputs_and_targets = LoadInputsAndTargets()\n    xs_1, xs_2, ys = load_inputs_and_targets(batch)\n    for x, xd in zip(xs_1, desire_xs_1):\n        np.testing.assert_array_equal(x, xd)\n    for x, xd in zip(xs_2, desire_xs_2):\n        np.testing.assert_array_equal(x, xd)\n    for y, yd in zip(ys, desire_ys):\n        np.testing.assert_array_equal(y, yd)\n\n\ndef test_load_inputs_and_targets_new_format(tmpdir):\n    # batch = [(""F01_050C0101_PED_REAL"",\n    #           {""input"": [{""feat"": ""some/path.h5"",\n    #                       ""filetype"": ""hdf5""}],\n    #           ""output"": [{""tokenid"": ""1 2 3 4""}],\n\n    p = tmpdir.join(""test.h5"")\n\n    desire_xs = []\n    desire_ys = []\n    batch = []\n    with h5py.File(str(p), ""w"") as f:\n        # batch: List[Tuple[str, Dict[str, List[Dict[str, Any]]]]]\n        for i in range(10):\n            x = np.random.random((100, 100)).astype(np.float32)\n            uttid = ""uttid{}"".format(i)\n            f[uttid] = x\n            batch.append(\n                (\n                    uttid,\n                    {\n                        ""input"": [\n                            {\n                                ""feat"": str(p) + "":"" + uttid,\n                                ""filetype"": ""hdf5"",\n                                ""name"": ""input1"",\n                            }\n                        ],\n                        ""output"": [{""tokenid"": ""1 2 3 4"", ""name"": ""target1""}],\n                    },\n                )\n            )\n            desire_xs.append(x)\n            desire_ys.append(np.array([1, 2, 3, 4]))\n\n    load_inputs_and_targets = LoadInputsAndTargets()\n    xs, ys = load_inputs_and_targets(batch)\n    for x, xd in zip(xs, desire_xs):\n        np.testing.assert_array_equal(x, xd)\n    for y, yd in zip(ys, desire_ys):\n        np.testing.assert_array_equal(y, yd)\n\n\n@pytest.mark.parametrize(""fmt"", [""flac"", ""wav""])\ndef test_sound_hdf5_file(tmpdir, fmt):\n    valid = {\n        ""a"": np.random.randint(-100, 100, 25, dtype=np.int16),\n        ""b"": np.random.randint(-1000, 1000, 100, dtype=np.int16),\n    }\n\n    # Note: Specify the file format by extension\n    p = tmpdir.join(""test.{}.h5"".format(fmt)).strpath\n    f = SoundHDF5File(p, ""a"")\n\n    for k, v in valid.items():\n        f[k] = (v, 8000)\n\n    for k, v in valid.items():\n        t, r = f[k]\n        assert r == 8000\n        np.testing.assert_array_equal(t, v)\n\n\n@pytest.mark.parametrize(""typ"", [""ctc"", ""wer"", ""cer"", ""all""])\ndef test_error_calculator(tmpdir, typ):\n    from espnet.nets.e2e_asr_common import ErrorCalculator\n\n    space = ""<space>""\n    blank = ""<blank>""\n    char_list = [blank, space, ""a"", ""e"", ""i"", ""o"", ""u""]\n    ys_pad = [np.random.randint(0, len(char_list), x) for x in range(120, 150, 5)]\n    ys_hat = [np.random.randint(0, len(char_list), x) for x in range(120, 150, 5)]\n    if typ == ""ctc"":\n        cer, wer = False, False\n    elif typ == ""wer"":\n        cer, wer = False, True\n    elif typ == ""cer"":\n        cer, wer = True, False\n    else:\n        cer, wer = True, True\n\n    ec = ErrorCalculator(char_list, space, blank, cer, wer)\n\n    if typ == ""ctc"":\n        cer_ctc_val = ec(ys_pad, ys_hat, is_ctc=True)\n        _cer, _wer = ec(ys_pad, ys_hat)\n        assert cer_ctc_val is not None\n        assert _cer is None\n        assert _wer is None\n    elif typ == ""wer"":\n        _cer, _wer = ec(ys_pad, ys_hat)\n        assert _cer is None\n        assert _wer is not None\n    elif typ == ""cer"":\n        _cer, _wer = ec(ys_pad, ys_hat)\n        assert _cer is not None\n        assert _wer is None\n    else:\n        cer_ctc_val = ec(ys_pad, ys_hat, is_ctc=True)\n        _cer, _wer = ec(ys_pad, ys_hat)\n        assert cer_ctc_val is not None\n        assert _cer is not None\n        assert _wer is not None\n\n\ndef test_error_calculator_nospace(tmpdir):\n    from espnet.nets.e2e_asr_common import ErrorCalculator\n\n    space = ""<space>""\n    blank = ""<blank>""\n    char_list = [blank, ""a"", ""e"", ""i"", ""o"", ""u""]\n    ys_pad = [np.random.randint(0, len(char_list), x) for x in range(120, 150, 5)]\n    ys_hat = [np.random.randint(0, len(char_list), x) for x in range(120, 150, 5)]\n    cer, wer = True, True\n\n    ec = ErrorCalculator(char_list, space, blank, cer, wer)\n\n    cer_ctc_val = ec(ys_pad, ys_hat, is_ctc=True)\n    _cer, _wer = ec(ys_pad, ys_hat)\n    assert cer_ctc_val is not None\n    assert _cer is not None\n    assert _wer is not None\n'"
test/utils_test.py,0,"b'import numpy as np\n\n\ndef make_dummy_json(\n    n_utts=10,\n    ilen_range=(100, 300),\n    olen_range=(10, 300),\n    idim=83,\n    odim=52,\n    num_inputs=1,\n):\n    ilens = np.random.randint(ilen_range[0], ilen_range[1], n_utts)\n    olens = np.random.randint(olen_range[0], olen_range[1], n_utts)\n    dummy_json = {}\n    for idx in range(n_utts):\n        input = []\n        for input_idx in range(num_inputs):\n            input += [{""shape"": [ilens[idx], idim]}]\n        output = [{""shape"": [olens[idx], odim]}]\n        dummy_json[""utt_%d"" % idx] = {""input"": input, ""output"": output}\n    return dummy_json\n\n\ndef make_dummy_json_st(\n    n_utts=10,\n    ilen_range=(100, 300),\n    olen_range=(10, 300),\n    olen_asr_range=(10, 300),\n    idim=83,\n    odim=52,\n):\n    ilens = np.random.randint(ilen_range[0], ilen_range[1], n_utts)\n    olens = np.random.randint(olen_range[0], olen_range[1], n_utts)\n    olens_asr = np.random.randint(olen_asr_range[0], olen_asr_range[1], n_utts)\n    dummy_json = {}\n    for idx in range(n_utts):\n        input = [{""shape"": [ilens[idx], idim]}]\n        output = [{""shape"": [olens[idx], odim]}, {""shape"": [olens_asr[idx], odim]}]\n        dummy_json[""utt_%d"" % idx] = {""input"": input, ""output"": output}\n    return dummy_json\n\n\ndef make_dummy_json_mt(\n    n_utts=10, ilen_range=(100, 300), olen_range=(10, 300), idim=83, odim=52\n):\n    ilens = np.random.randint(ilen_range[0], ilen_range[1], n_utts)\n    olens = np.random.randint(olen_range[0], olen_range[1], n_utts)\n    dummy_json = {}\n    for idx in range(n_utts):\n        output = [{""shape"": [olens[idx], odim]}, {""shape"": [ilens[idx], idim]}]\n        dummy_json[""utt_%d"" % idx] = {""output"": output}\n    return dummy_json\n'"
tools/check_install.py,12,"b'#!/usr/bin/env python3\n\n""""""Script to check whether the installation is done correctly.""""""\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport importlib\nimport logging\nimport sys\n\nfrom distutils.version import LooseVersion\n\n\n# NOTE: add the libraries which are not included in setup.py\nMANUALLY_INSTALLED_LIBRARIES = [\n    (""espnet"", None),\n    (""kaldiio"", None),\n    (""matplotlib"", None),\n    (""chainer"", (""6.0.0"")),\n    (""chainer_ctc"", None),\n    (""warprnnt_pytorch"", (""0.1"")),\n]\n\n# NOTE: list all torch versions which are compatible with espnet\nCOMPATIBLE_TORCH_VERSIONS = (\n    ""0.4.1"",\n    ""1.0.0"",\n    ""1.0.1"",\n    ""1.0.1.post2"",\n    ""1.1.0"",\n    ""1.2.0"",\n    ""1.3.0"",\n    ""1.3.1"",\n    ""1.4.0"",\n    ""1.5.0"",\n)\n\n\ndef main(args):\n    """"""Check the installation.""""""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--no-cuda"",\n        action=""store_true"",\n        default=False,\n        help=""Disable cuda-related tests"",\n    )\n    args = parser.parse_args(args)\n\n    logging.basicConfig(level=logging.INFO, format=""%(levelname)s: %(message)s"")\n    logging.info(f""python version = {sys.version}"")\n\n    library_list = []\n\n    if not args.no_cuda:\n        library_list.append((""cupy"", (""6.0.0"")))\n\n    # check torch installation at first\n    try:\n        import torch\n\n        logging.info(f""pytorch version = {torch.__version__}"")\n        if torch.__version__ not in COMPATIBLE_TORCH_VERSIONS:\n            logging.warning(f""{torch.__version__} is not tested. please be careful."")\n    except ImportError:\n        logging.warning(""torch is not installed."")\n        logging.warning(""please try to setup again and then re-run this script."")\n        sys.exit(1)\n\n    # warpctc can be installed only for pytorch < 1.2\n    if LooseVersion(torch.__version__) < LooseVersion(""1.2.0""):\n        library_list.append((""warpctc_pytorch"", (""0.1.1"", ""0.1.3"")))\n\n    library_list.extend(MANUALLY_INSTALLED_LIBRARIES)\n\n    # check library availableness\n    logging.info(""library availableness check start."")\n    logging.info(""# libraries to be checked = %d"" % len(library_list))\n    is_correct_installed_list = []\n    for idx, (name, version) in enumerate(library_list):\n        try:\n            importlib.import_module(name)\n            logging.info(""--> %s is installed."" % name)\n            is_correct_installed_list.append(True)\n        except ImportError:\n            logging.warning(""--> %s is not installed."" % name)\n            is_correct_installed_list.append(False)\n    logging.info(""library availableness check done."")\n    logging.info(\n        ""%d / %d libraries are correctly installed.""\n        % (sum(is_correct_installed_list), len(library_list))\n    )\n\n    if len(library_list) != sum(is_correct_installed_list):\n        logging.warning(""please try to setup again and then re-run this script."")\n        sys.exit(1)\n\n    # check library version\n    num_version_specified = sum(\n        [True if v is not None else False for n, v in library_list]\n    )\n    logging.info(""library version check start."")\n    logging.info(""# libraries to be checked = %d"" % num_version_specified)\n    is_correct_version_list = []\n    for idx, (name, version) in enumerate(library_list):\n        if version is not None:\n            # Note: temp. fix for warprnnt_pytorch\n            # not found version with importlib\n            if name == ""warprnnt_pytorch"":\n                import pkg_resources\n\n                vers = pkg_resources.get_distribution(name).version\n            else:\n                vers = importlib.import_module(name).__version__\n            if vers is not None:\n                is_correct = vers in version\n                if is_correct:\n                    logging.info(""--> %s version is matched (%s)."" % (name, vers))\n                    is_correct_version_list.append(True)\n                else:\n                    logging.warning(\n                        ""--> %s version is incorrect (%s is not in %s).""\n                        % (name, vers, str(version))\n                    )\n                    is_correct_version_list.append(False)\n            else:\n                logging.info(\n                    ""--> %s has no version info, but version is specified."" % name\n                )\n                logging.info(""--> maybe it is better to reinstall the latest version."")\n                is_correct_version_list.append(False)\n    logging.info(""library version check done."")\n    logging.info(\n        ""%d / %d libraries are correct version.""\n        % (sum(is_correct_version_list), num_version_specified)\n    )\n\n    if sum(is_correct_version_list) != num_version_specified:\n        logging.info(""please try to setup again and then re-run this script."")\n        sys.exit(1)\n\n    # check cuda availableness\n    if args.no_cuda:\n        logging.info(""cuda availableness check skipped."")\n    else:\n        logging.info(""cuda availableness check start."")\n        import chainer\n        import torch\n\n        try:\n            assert torch.cuda.is_available()\n            logging.info(""--> cuda is available in torch."")\n        except AssertionError:\n            logging.warning(""--> it seems that cuda is not available in torch."")\n        try:\n            assert torch.backends.cudnn.is_available()\n            logging.info(""--> cudnn is available in torch."")\n        except AssertionError:\n            logging.warning(""--> it seems that cudnn is not available in torch."")\n        try:\n            assert chainer.backends.cuda.available\n            logging.info(""--> cuda is available in chainer."")\n        except AssertionError:\n            logging.warning(""--> it seems that cuda is not available in chainer."")\n        try:\n            assert chainer.backends.cuda.cudnn_enabled\n            logging.info(""--> cudnn is available in chainer."")\n        except AssertionError:\n            logging.warning(""--> it seems that cudnn is not available in chainer."")\n        try:\n            from cupy.cuda import nccl  # NOQA\n\n            logging.info(""--> nccl is installed."")\n        except ImportError:\n            logging.warning(\n                ""--> it seems that nccl is not installed. multi-gpu is not enabled.""\n            )\n            logging.warning(\n                ""--> if you want to use multi-gpu, please install it and then re-setup.""\n            )\n        try:\n            assert torch.cuda.device_count() > 1\n            logging.info(\n                f""--> multi-gpu is available (#gpus={torch.cuda.device_count()}).""\n            )\n        except AssertionError:\n            logging.warning(""--> it seems that only single gpu is available."")\n            logging.warning(""--> maybe your machine has only one gpu."")\n        logging.info(""cuda availableness check done."")\n\n    logging.info(""installation check is done."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
utils/addjson.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport sys\n\nfrom distutils.util import strtobool\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""add multiple json values to an input or output value"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""jsons"", type=str, nargs=""+"", help=""json files"")\n    parser.add_argument(\n        ""-i"",\n        ""--is-input"",\n        default=True,\n        type=strtobool,\n        help=""If true, add to input. If false, add to output"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    # make intersection set for utterance keys\n    js = []\n    intersec_ks = []\n    for x in args.jsons:\n        with codecs.open(x, ""r"", encoding=""utf-8"") as f:\n            j = json.load(f)\n        ks = j[""utts""].keys()\n        logging.info(x + "": has "" + str(len(ks)) + "" utterances"")\n        if len(intersec_ks) > 0:\n            intersec_ks = intersec_ks.intersection(set(ks))\n            if len(intersec_ks) == 0:\n                logging.warning(""Empty intersection"")\n                break\n        else:\n            intersec_ks = set(ks)\n        js.append(j)\n    logging.info(""new json has "" + str(len(intersec_ks)) + "" utterances"")\n\n    # updated original dict to keep intersection\n    intersec_org_dic = dict()\n    for k in intersec_ks:\n        v = js[0][""utts""][k]\n        intersec_org_dic[k] = v\n\n    intersec_add_dic = dict()\n    for k in intersec_ks:\n        v = js[1][""utts""][k]\n        for j in js[2:]:\n            v.update(j[""utts""][k])\n        intersec_add_dic[k] = v\n\n    new_dic = dict()\n    for key_id in intersec_org_dic:\n        orgdic = intersec_org_dic[key_id]\n        adddic = intersec_add_dic[key_id]\n\n        if ""utt2spk"" not in orgdic:\n            orgdic[""utt2spk""] = """"\n        # NOTE: for machine translation\n\n        # add as input\n        if args.is_input:\n            # original input\n            input_list = orgdic[""input""]\n            # additional input\n            in_add_dic = {}\n            if ""idim"" in adddic and ""ilen"" in adddic:\n                in_add_dic[""shape""] = [int(adddic[""ilen""]), int(adddic[""idim""])]\n            elif ""idim"" in adddic:\n                in_add_dic[""shape""] = [int(adddic[""idim""])]\n            # add all other key value\n            for key, value in adddic.items():\n                if key in [""idim"", ""ilen""]:\n                    continue\n                in_add_dic[key] = value\n            # add name\n            in_add_dic[""name""] = ""input%d"" % (len(input_list) + 1)\n\n            input_list.append(in_add_dic)\n            new_dic[key_id] = {\n                ""input"": input_list,\n                ""output"": orgdic[""output""],\n                ""utt2spk"": orgdic[""utt2spk""],\n            }\n        # add as output\n        else:\n            # original output\n            output_list = orgdic[""output""]\n            # additional output\n            out_add_dic = {}\n            # add shape\n            if ""odim"" in adddic and ""olen"" in adddic:\n                out_add_dic[""shape""] = [int(adddic[""olen""]), int(adddic[""odim""])]\n            elif ""odim"" in adddic:\n                out_add_dic[""shape""] = [int(adddic[""odim""])]\n            # add all other key value\n            for key, value in adddic.items():\n                if key in [""odim"", ""olen""]:\n                    continue\n                out_add_dic[key] = value\n            # add name\n            out_add_dic[""name""] = ""target%d"" % (len(output_list) + 1)\n\n            output_list.append(out_add_dic)\n            new_dic[key_id] = {\n                ""input"": orgdic[""input""],\n                ""output"": output_list,\n                ""utt2spk"": orgdic[""utt2spk""],\n            }\n            if ""lang"" in orgdic.keys():\n                new_dic[key_id][""lang""] = orgdic[""lang""]\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(\n        {""utts"": new_dic},\n        indent=4,\n        ensure_ascii=False,\n        sort_keys=True,\n        separators=("","", "": ""),\n    )\n    sys.stdout = codecs.getwriter(""utf-8"")(\n        sys.stdout if is_python2 else sys.stdout.buffer\n    )\n    print(jsonstring)\n'"
utils/apply-cmvn.py,0,"b'#!/usr/bin/env python3\nimport argparse\nfrom distutils.util import strtobool\nimport logging\n\nimport kaldiio\nimport numpy\n\nfrom espnet.transform.cmvn import CMVN\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_utils import is_scipy_wav_style\nfrom espnet.utils.cli_writers import file_writer_helper\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""apply mean-variance normalization to files"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--in-filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--stats-filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""npy""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--out-filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5""],\n        help=""Specify the file format for the wspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n\n    parser.add_argument(\n        ""--norm-means"",\n        type=strtobool,\n        default=True,\n        help=""Do variance normalization or not."",\n    )\n    parser.add_argument(\n        ""--norm-vars"",\n        type=strtobool,\n        default=False,\n        help=""Do variance normalization or not."",\n    )\n    parser.add_argument(\n        ""--reverse"", type=strtobool, default=False, help=""Do reverse mode or not""\n    )\n    parser.add_argument(\n        ""--spk2utt"",\n        type=str,\n        help=""A text file of speaker to utterance-list map. ""\n        ""(Don\'t give rspecifier format, such as ""\n        \'""ark:spk2utt"")\',\n    )\n    parser.add_argument(\n        ""--utt2spk"",\n        type=str,\n        help=""A text file of utterance to speaker map. ""\n        ""(Don\'t give rspecifier format, such as ""\n        \'""ark:utt2spk"")\',\n    )\n    parser.add_argument(\n        ""--write-num-frames"", type=str, help=""Specify wspecifer for utt2num_frames""\n    )\n    parser.add_argument(\n        ""--compress"", type=strtobool, default=False, help=""Save in compressed format""\n    )\n    parser.add_argument(\n        ""--compression-method"",\n        type=int,\n        default=2,\n        help=""Specify the method(if mat) or "" ""gzip-level(if hdf5)"",\n    )\n    parser.add_argument(\n        ""stats_rspecifier_or_rxfilename"",\n        help=""Input stats. e.g. ark:stats.ark or stats.mat"",\n    )\n    parser.add_argument(\n        ""rspecifier"", type=str, help=""Read specifier id. e.g. ark:some.ark""\n    )\n    parser.add_argument(\n        ""wspecifier"", type=str, help=""Write specifier id. e.g. ark:some.ark""\n    )\n    return parser\n\n\ndef main():\n    args = get_parser().parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    if "":"" in args.stats_rspecifier_or_rxfilename:\n        is_rspcifier = True\n        if args.stats_filetype == ""npy"":\n            stats_filetype = ""hdf5""\n        else:\n            stats_filetype = args.stats_filetype\n\n        stats_dict = dict(\n            file_reader_helper(args.stats_rspecifier_or_rxfilename, stats_filetype)\n        )\n    else:\n        is_rspcifier = False\n        if args.stats_filetype == ""mat"":\n            stats = kaldiio.load_mat(args.stats_rspecifier_or_rxfilename)\n        else:\n            stats = numpy.load(args.stats_rspecifier_or_rxfilename)\n        stats_dict = {None: stats}\n\n    cmvn = CMVN(\n        stats=stats_dict,\n        norm_means=args.norm_means,\n        norm_vars=args.norm_vars,\n        utt2spk=args.utt2spk,\n        spk2utt=args.spk2utt,\n        reverse=args.reverse,\n    )\n\n    with file_writer_helper(\n        args.wspecifier,\n        filetype=args.out_filetype,\n        write_num_frames=args.write_num_frames,\n        compress=args.compress,\n        compression_method=args.compression_method,\n    ) as writer:\n        for utt, mat in file_reader_helper(args.rspecifier, args.in_filetype):\n            if is_scipy_wav_style(mat):\n                # If data is sound file, then got as Tuple[int, ndarray]\n                rate, mat = mat\n            mat = cmvn(mat, utt if is_rspcifier else None)\n            writer[utt] = mat\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/average_checkpoints.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport json\nimport os\n\nimport numpy as np\n\n\ndef main():\n    if args.log is not None:\n        with open(args.log) as f:\n            logs = json.load(f)\n        val_scores = []\n        for log in logs:\n            if ""validation/main/acc"" in log.keys():\n                val_scores += [[log[""epoch""], log[""validation/main/acc""]]]\n            elif ""val_perplexity"" in log.keys():\n                val_scores += [[log[""epoch""], 1 / log[""val_perplexity""]]]\n            elif ""validation/main/loss"" in log.keys():\n                val_scores += [[log[""epoch""], -log[""validation/main/loss""]]]\n        if len(val_scores) == 0:\n            raise ValueError(\n                ""`validation/main/acc` or `val_perplexity` is not found in log.""\n            )\n        val_scores = np.array(val_scores)\n        sort_idx = np.argsort(val_scores[:, -1])\n        sorted_val_scores = val_scores[sort_idx][::-1]\n        print(""best val scores = "" + str(sorted_val_scores[: args.num, 1]))\n        print(\n            ""selected epochs = ""\n            + str(sorted_val_scores[: args.num, 0].astype(np.int64))\n        )\n        last = [\n            os.path.dirname(args.snapshots[0]) + ""/snapshot.ep.%d"" % (int(epoch))\n            for epoch in sorted_val_scores[: args.num, 0]\n        ]\n    else:\n        last = sorted(args.snapshots, key=os.path.getmtime)\n        last = last[-args.num :]\n    print(""average over"", last)\n    avg = None\n\n    if args.backend == ""pytorch"":\n        import torch\n\n        # sum\n        for path in last:\n            states = torch.load(path, map_location=torch.device(""cpu""))[""model""]\n            if avg is None:\n                avg = states\n            else:\n                for k in avg.keys():\n                    avg[k] += states[k]\n\n        # average\n        for k in avg.keys():\n            if avg[k] is not None:\n                avg[k] /= args.num\n\n        torch.save(avg, args.out)\n\n    elif args.backend == ""chainer"":\n        # sum\n        for path in last:\n            states = np.load(path)\n            if avg is None:\n                keys = [x.split(""main/"")[1] for x in states if ""model"" in x]\n                avg = dict()\n                for k in keys:\n                    avg[k] = states[""updater/model:main/{}"".format(k)]\n            else:\n                for k in keys:\n                    avg[k] += states[""updater/model:main/{}"".format(k)]\n        # average\n        for k in keys:\n            if avg[k] is not None:\n                avg[k] /= args.num\n        np.savez_compressed(args.out, **avg)\n        os.rename(""{}.npz"".format(args.out), args.out)  # numpy save with .npz extension\n    else:\n        raise ValueError(""Incorrect type of backend"")\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=""average models from snapshot"")\n    parser.add_argument(""--snapshots"", required=True, type=str, nargs=""+"")\n    parser.add_argument(""--out"", required=True, type=str)\n    parser.add_argument(""--num"", default=10, type=int)\n    parser.add_argument(""--backend"", default=""chainer"", type=str)\n    parser.add_argument(""--log"", default=None, type=str, nargs=""?"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    args = get_parser().parse_args()\n    main()\n'"
utils/change_yaml.py,0,"b'#!/usr/bin/env python3\nimport argparse\nfrom pathlib import Path\n\nimport yaml\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""change specified attributes of a YAML file"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    egroup = parser.add_mutually_exclusive_group()\n    parser.add_argument(""inyaml"", nargs=""?"")\n    egroup.add_argument(""-o"", ""--outyaml"")\n    egroup.add_argument(""--outdir"")\n    parser.add_argument(\n        ""-a"",\n        ""--arg"",\n        action=""append"",\n        default=[],\n        help=""e.g -a a.b.c=4 -> {\'a\': {\'b\': {\'c\': 4}}}"",\n    )\n    parser.add_argument(\n        ""-d"",\n        ""--delete"",\n        action=""append"",\n        default=[],\n        help=\'e.g -d a -> ""a"" is removed from the input yaml\',\n    )\n    return parser\n\n\ndef main():\n    args = get_parser().parse_args()\n\n    if args.inyaml is None:\n        indict = {}\n    else:\n        with open(args.inyaml, ""r"") as f:\n            indict = yaml.load(f, Loader=yaml.Loader)\n        if indict is None:\n            indict = {}\n\n    if args.outyaml is None:\n        # Auto naming from arguments\n        eles = []\n        if args.inyaml is not None:\n            p = Path(args.inyaml)\n            if args.outdir is None:\n                outdir = p.parent\n            else:\n                outdir = Path(args.outdir)\n            eles.append(str(outdir / p.stem))\n\n        table = str.maketrans(""{}[]()"", ""%%__--"", "" |&;#*?~\\""\'\\\\"")\n        for arg in args.delete:\n            value = arg.translate(table)\n            eles.append(""del-"" + value)\n        for arg in args.arg:\n            if ""="" not in arg:\n                raise RuntimeError(f\'""{arg}"" does\\\'t include ""=""\')\n            key, value = arg.split(""="")\n            key = key.translate(table)\n            value = value.translate(table)\n            eles.append(key + value)\n\n        outyaml = ""_"".join(eles)\n        if outyaml == """":\n            outyaml = ""config""\n        outyaml += "".yaml""\n        if args.inyaml == outyaml:\n            p = Path(args.outyaml)\n            outyaml = p.parent / (p.stem + "".2"" + p.suffix)\n\n        outyaml = Path(outyaml)\n    else:\n        outyaml = Path(args.outyaml)\n\n    for arg in args.delete + args.arg:\n        if ""="" in arg:\n            key, value = arg.split(""="")\n            if not value.strip() == """":\n                value = yaml.load(value, Loader=yaml.Loader)\n        else:\n            key = arg\n            value = None\n\n        keys = key.split(""."")\n        d = indict\n        for idx, k in enumerate(keys):\n            if idx == len(keys) - 1:\n                if isinstance(d, (tuple, list)):\n                    k = int(k)\n                    if k >= len(d):\n                        d += type(d)(None for _ in range(k - len(d) + 1))\n                if value is not None:\n                    d[k] = value\n                else:\n                    del d[k]\n            else:\n                if isinstance(d, (tuple, list)):\n                    k = int(k)\n                    if k >= len(d):\n                        d += type(d)(None for _ in range(k - len(d) + 1))\n                elif isinstance(d, dict):\n                    if k not in d:\n                        d[k] = {}\n                if not isinstance(d[k], (dict, tuple, list)):\n                    d[k] = {}\n                d = d[k]\n\n    outyaml.parent.mkdir(parents=True, exist_ok=True)\n    with outyaml.open(""w"") as f:\n        yaml.dump(indict, f, Dumper=yaml.Dumper, indent=4, sort_keys=False)\n    print(outyaml)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/compute-cmvn-stats.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport logging\n\nimport kaldiio\nimport numpy as np\n\nfrom espnet.transform.transformation import Transformation\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_utils import is_scipy_wav_style\nfrom espnet.utils.cli_writers import file_writer_helper\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""Compute cepstral mean and ""\n        ""variance normalization statistics""\n        ""If wspecifier provided: per-utterance by default, ""\n        ""or per-speaker if""\n        ""spk2utt option provided; if wxfilename: global"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--spk2utt"",\n        type=str,\n        help=""A text file of speaker to utterance-list map. ""\n        ""(Don\'t give rspecifier format, such as ""\n        \'""ark:utt2spk"")\',\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--in-filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--out-filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""npy""],\n        help=""Specify the file format for the wspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""rspecifier"", type=str, help=""Read specifier for feats. e.g. ark:some.ark""\n    )\n    parser.add_argument(\n        ""wspecifier_or_wxfilename"", type=str, help=""Write specifier. e.g. ark:some.ark""\n    )\n    return parser\n\n\ndef main():\n    args = get_parser().parse_args()\n\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    is_wspecifier = "":"" in args.wspecifier_or_wxfilename\n\n    if is_wspecifier:\n        if args.spk2utt is not None:\n            logging.info(""Performing as speaker CMVN mode"")\n            utt2spk_dict = {}\n            with open(args.spk2utt) as f:\n                for line in f:\n                    spk, utts = line.rstrip().split(None, 1)\n                    for utt in utts.split():\n                        utt2spk_dict[utt] = spk\n\n            def utt2spk(x):\n                return utt2spk_dict[x]\n\n        else:\n            logging.info(""Performing as utterance CMVN mode"")\n\n            def utt2spk(x):\n                return x\n\n        if args.out_filetype == ""npy"":\n            logging.warning(\n                ""--out-filetype npy is allowed only for ""\n                ""Global CMVN mode, changing to hdf5""\n            )\n            args.out_filetype = ""hdf5""\n\n    else:\n        logging.info(""Performing as global CMVN mode"")\n        if args.spk2utt is not None:\n            logging.warning(""spk2utt is not used for global CMVN mode"")\n\n        def utt2spk(x):\n            return None\n\n        if args.out_filetype == ""hdf5"":\n            logging.warning(\n                ""--out-filetype hdf5 is not allowed for ""\n                ""Global CMVN mode, changing to npy""\n            )\n            args.out_filetype = ""npy""\n\n    if args.preprocess_conf is not None:\n        preprocessing = Transformation(args.preprocess_conf)\n        logging.info(""Apply preprocessing: {}"".format(preprocessing))\n    else:\n        preprocessing = None\n\n    # Calculate stats for each speaker\n    counts = {}\n    sum_feats = {}\n    square_sum_feats = {}\n\n    idx = 0\n    for idx, (utt, matrix) in enumerate(\n        file_reader_helper(args.rspecifier, args.in_filetype), 1\n    ):\n        if is_scipy_wav_style(matrix):\n            # If data is sound file, then got as Tuple[int, ndarray]\n            rate, matrix = matrix\n        if preprocessing is not None:\n            matrix = preprocessing(matrix, uttid_list=utt)\n\n        spk = utt2spk(utt)\n\n        # Init at the first seen of the spk\n        if spk not in counts:\n            counts[spk] = 0\n            feat_shape = matrix.shape[1:]\n            # Accumulate in double precision\n            sum_feats[spk] = np.zeros(feat_shape, dtype=np.float64)\n            square_sum_feats[spk] = np.zeros(feat_shape, dtype=np.float64)\n\n        counts[spk] += matrix.shape[0]\n        sum_feats[spk] += matrix.sum(axis=0)\n        square_sum_feats[spk] += (matrix ** 2).sum(axis=0)\n    logging.info(""Processed {} utterances"".format(idx))\n    assert idx > 0, idx\n\n    cmvn_stats = {}\n    for spk in counts:\n        feat_shape = sum_feats[spk].shape\n        cmvn_shape = (2, feat_shape[0] + 1) + feat_shape[1:]\n        _cmvn_stats = np.empty(cmvn_shape, dtype=np.float64)\n        _cmvn_stats[0, :-1] = sum_feats[spk]\n        _cmvn_stats[1, :-1] = square_sum_feats[spk]\n\n        _cmvn_stats[0, -1] = counts[spk]\n        _cmvn_stats[1, -1] = 0.0\n\n        # You can get the mean and std as following,\n        # >>> N = _cmvn_stats[0, -1]\n        # >>> mean = _cmvn_stats[0, :-1] / N\n        # >>> std = np.sqrt(_cmvn_stats[1, :-1] / N - mean ** 2)\n\n        cmvn_stats[spk] = _cmvn_stats\n\n    # Per utterance or speaker CMVN\n    if is_wspecifier:\n        with file_writer_helper(\n            args.wspecifier_or_wxfilename, filetype=args.out_filetype\n        ) as writer:\n            for spk, mat in cmvn_stats.items():\n                writer[spk] = mat\n\n    # Global CMVN\n    else:\n        matrix = cmvn_stats[None]\n        if args.out_filetype == ""npy"":\n            np.save(args.wspecifier_or_wxfilename, matrix)\n        elif args.out_filetype == ""mat"":\n            # Kaldi supports only matrix or vector\n            kaldiio.save_mat(args.wspecifier_or_wxfilename, matrix)\n        else:\n            raise RuntimeError(\n                ""Not supporting: --out-filetype {}"".format(args.out_filetype)\n            )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/compute-fbank-feats.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nfrom distutils.util import strtobool\nimport logging\n\nimport kaldiio\nimport numpy\n\nfrom espnet.transform.spectrogram import logmelspectrogram\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_writers import file_writer_helper\nfrom espnet2.utils.types import int_or_none\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""compute FBANK feature from WAV"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--fs"", type=int, help=""Sampling frequency"")\n    parser.add_argument(\n        ""--fmax"", type=int_or_none, default=None, nargs=""?"", help=""Maximum frequency""\n    )\n    parser.add_argument(\n        ""--fmin"", type=int_or_none, default=None, nargs=""?"", help=""Minimum frequency""\n    )\n    parser.add_argument(""--n_mels"", type=int, default=80, help=""Number of mel basis"")\n    parser.add_argument(""--n_fft"", type=int, default=1024, help=""FFT length in point"")\n    parser.add_argument(\n        ""--n_shift"", type=int, default=512, help=""Shift length in point""\n    )\n    parser.add_argument(\n        ""--win_length"",\n        type=int_or_none,\n        default=None,\n        nargs=""?"",\n        help=""Analisys window length in point"",\n    )\n    parser.add_argument(\n        ""--window"",\n        type=str,\n        default=""hann"",\n        choices=[""hann"", ""hamming""],\n        help=""Type of window"",\n    )\n    parser.add_argument(\n        ""--write-num-frames"", type=str, help=""Specify wspecifer for utt2num_frames""\n    )\n    parser.add_argument(\n        ""--filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5""],\n        help=""Specify the file format for output. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--compress"", type=strtobool, default=False, help=""Save in compressed format""\n    )\n    parser.add_argument(\n        ""--compression-method"",\n        type=int,\n        default=2,\n        help=""Specify the method(if mat) or "" ""gzip-level(if hdf5)"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--normalize"",\n        choices=[1, 16, 24, 32],\n        type=int,\n        default=None,\n        help=""Give the bit depth of the PCM, ""\n        ""then normalizes data to scale in [-1,1]"",\n    )\n    parser.add_argument(""rspecifier"", type=str, help=""WAV scp file"")\n    parser.add_argument(\n        ""--segments"",\n        type=str,\n        help=""segments-file format: each line is either""\n        ""<segment-id> <recording-id> <start-time> <end-time>""\n        ""e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5"",\n    )\n    parser.add_argument(""wspecifier"", type=str, help=""Write specifier"")\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    with kaldiio.ReadHelper(\n        args.rspecifier, segments=args.segments\n    ) as reader, file_writer_helper(\n        args.wspecifier,\n        filetype=args.filetype,\n        write_num_frames=args.write_num_frames,\n        compress=args.compress,\n        compression_method=args.compression_method,\n    ) as writer:\n        for utt_id, (rate, array) in reader:\n            assert rate == args.fs\n            array = array.astype(numpy.float32)\n            if args.normalize is not None and args.normalize != 1:\n                array = array / (1 << (args.normalize - 1))\n\n            lmspc = logmelspectrogram(\n                x=array,\n                fs=args.fs,\n                n_mels=args.n_mels,\n                n_fft=args.n_fft,\n                n_shift=args.n_shift,\n                win_length=args.win_length,\n                window=args.window,\n                fmin=args.fmin,\n                fmax=args.fmax,\n            )\n            writer[utt_id] = lmspc\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/compute-stft-feats.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nfrom distutils.util import strtobool\nimport logging\n\nimport kaldiio\nimport numpy\n\nfrom espnet.transform.spectrogram import spectrogram\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_writers import file_writer_helper\nfrom espnet2.utils.types import int_or_none\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""compute STFT feature from WAV"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--n_fft"", type=int, default=1024, help=""FFT length in point"")\n    parser.add_argument(\n        ""--n_shift"", type=int, default=512, help=""Shift length in point""\n    )\n    parser.add_argument(\n        ""--win_length"",\n        type=int_or_none,\n        default=None,\n        nargs=""?"",\n        help=""Analisys window length in point"",\n    )\n    parser.add_argument(\n        ""--window"",\n        type=str,\n        default=""hann"",\n        choices=[""hann"", ""hamming""],\n        help=""Type of window"",\n    )\n    parser.add_argument(\n        ""--write-num-frames"", type=str, help=""Specify wspecifer for utt2num_frames""\n    )\n    parser.add_argument(\n        ""--filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5""],\n        help=""Specify the file format. "" \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--compress"", type=strtobool, default=False, help=""Save in compressed format""\n    )\n    parser.add_argument(\n        ""--compression-method"",\n        type=int,\n        default=2,\n        help=""Specify the method(if mat) or "" ""gzip-level(if hdf5)"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--normalize"",\n        choices=[1, 16, 24, 32],\n        type=int,\n        default=None,\n        help=""Give the bit depth of the PCM, ""\n        ""then normalizes data to scale in [-1,1]"",\n    )\n    parser.add_argument(""rspecifier"", type=str, help=""WAV scp file"")\n    parser.add_argument(\n        ""--segments"",\n        type=str,\n        help=""segments-file format: each line is either""\n        ""<segment-id> <recording-id> <start-time> <end-time>""\n        ""e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5"",\n    )\n    parser.add_argument(""wspecifier"", type=str, help=""Write specifier"")\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    with kaldiio.ReadHelper(\n        args.rspecifier, segments=args.segments\n    ) as reader, file_writer_helper(\n        args.wspecifier,\n        filetype=args.filetype,\n        write_num_frames=args.write_num_frames,\n        compress=args.compress,\n        compression_method=args.compression_method,\n    ) as writer:\n        for utt_id, (_, array) in reader:\n            array = array.astype(numpy.float32)\n            if args.normalize is not None and args.normalize != 1:\n                array = array / (1 << (args.normalize - 1))\n            spc = spectrogram(\n                x=array,\n                n_fft=args.n_fft,\n                n_shift=args.n_shift,\n                win_length=args.win_length,\n                window=args.window,\n            )\n            writer[utt_id] = spc\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/concatjson.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""concatenate json files"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""jsons"", type=str, nargs=""+"", help=""json files"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    args = get_parser().parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    # make intersection set for utterance keys\n    js = {}\n    for x in args.jsons:\n        with codecs.open(x, encoding=""utf-8"") as f:\n            j = json.load(f)\n        ks = j[""utts""].keys()\n        logging.debug(x + "": has "" + str(len(ks)) + "" utterances"")\n        js.update(j[""utts""])\n    logging.info(""new json has "" + str(len(js.keys())) + "" utterances"")\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(\n        {""utts"": js},\n        indent=4,\n        sort_keys=True,\n        ensure_ascii=False,\n        separators=("","", "": ""),\n    )\n    sys.stdout = codecs.getwriter(""utf-8"")(\n        sys.stdout if is_python2 else sys.stdout.buffer\n    )\n    print(jsonstring)\n'"
utils/convert_fbank_to_wav.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\nimport os\n\nfrom distutils.version import LooseVersion\n\nimport librosa\nimport numpy as np\nfrom scipy.io.wavfile import write\n\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\nEPS = 1e-10\n\n\ndef logmelspc_to_linearspc(lmspc, fs, n_mels, n_fft, fmin=None, fmax=None):\n    """"""Convert log Mel filterbank to linear spectrogram.\n\n    Args:\n        lmspc (ndarray): Log Mel filterbank (T, n_mels).\n        fs (int): Sampling frequency.\n        n_mels (int): Number of mel basis.\n        n_fft (int): Number of FFT points.\n        f_min (int, optional): Minimum frequency to analyze.\n        f_max (int, optional): Maximum frequency to analyze.\n\n    Returns:\n        ndarray: Linear spectrogram (T, n_fft // 2 + 1).\n\n    """"""\n    assert lmspc.shape[1] == n_mels\n    fmin = 0 if fmin is None else fmin\n    fmax = fs / 2 if fmax is None else fmax\n    mspc = np.power(10.0, lmspc)\n    mel_basis = librosa.filters.mel(fs, n_fft, n_mels, fmin, fmax)\n    inv_mel_basis = np.linalg.pinv(mel_basis)\n    spc = np.maximum(EPS, np.dot(inv_mel_basis, mspc.T).T)\n\n    return spc\n\n\ndef griffin_lim(spc, n_fft, n_shift, win_length, window=""hann"", n_iters=100):\n    """"""Convert linear spectrogram into waveform using Griffin-Lim.\n\n    Args:\n        spc (ndarray): Linear spectrogram (T, n_fft // 2 + 1).\n        n_fft (int): Number of FFT points.\n        n_shift (int): Shift size in points.\n        win_length (int): Window length in points.\n        window (str, optional): Window function type.\n        n_iters (int, optionl): Number of iterations of Griffin-Lim Algorithm.\n\n    Returns:\n        ndarray: Reconstructed waveform (N,).\n\n    """"""\n    # assert the size of input linear spectrogram\n    assert spc.shape[1] == n_fft // 2 + 1\n\n    if LooseVersion(librosa.__version__) >= LooseVersion(""0.7.0""):\n        # use librosa\'s fast Grriffin-Lim algorithm\n        spc = np.abs(spc.T)\n        y = librosa.griffinlim(\n            S=spc,\n            n_iter=n_iters,\n            hop_length=n_shift,\n            win_length=win_length,\n            window=window,\n            center=True if spc.shape[1] > 1 else False,\n        )\n    else:\n        # use slower version of Grriffin-Lim algorithm\n        logging.warning(\n            ""librosa version is old. use slow version of Grriffin-Lim algorithm.""\n            ""if you want to use fast Griffin-Lim, please update librosa via ""\n            ""`source ./path.sh && pip install librosa==0.7.0`.""\n        )\n        cspc = np.abs(spc).astype(np.complex).T\n        angles = np.exp(2j * np.pi * np.random.rand(*cspc.shape))\n        y = librosa.istft(cspc * angles, n_shift, win_length, window=window)\n        for i in range(n_iters):\n            angles = np.exp(\n                1j\n                * np.angle(librosa.stft(y, n_fft, n_shift, win_length, window=window))\n            )\n            y = librosa.istft(cspc * angles, n_shift, win_length, window=window)\n\n    return y\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert FBANK to WAV using Griffin-Lim algorithm"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--fs"", type=int, default=22050, help=""Sampling frequency"")\n    parser.add_argument(\n        ""--fmax"", type=int, default=None, nargs=""?"", help=""Maximum frequency""\n    )\n    parser.add_argument(\n        ""--fmin"", type=int, default=None, nargs=""?"", help=""Minimum frequency""\n    )\n    parser.add_argument(""--n_fft"", type=int, default=1024, help=""FFT length in point"")\n    parser.add_argument(\n        ""--n_shift"", type=int, default=512, help=""Shift length in point""\n    )\n    parser.add_argument(\n        ""--win_length"",\n        type=int,\n        default=None,\n        nargs=""?"",\n        help=""Analisys window length in point"",\n    )\n    parser.add_argument(\n        ""--n_mels"", type=int, default=None, nargs=""?"", help=""Number of mel basis""\n    )\n    parser.add_argument(\n        ""--window"",\n        type=str,\n        default=""hann"",\n        choices=[""hann"", ""hamming""],\n        help=""Type of window"",\n    )\n    parser.add_argument(\n        ""--iters"", type=int, default=100, help=""Number of iterations in Grriffin Lim""\n    )\n    parser.add_argument(\n        ""--filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(""rspecifier"", type=str, help=""Input feature"")\n    parser.add_argument(""outdir"", type=str, help=""Output directory"")\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # logging info\n    logging.basicConfig(\n        level=logging.INFO,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n    logging.info(get_commandline_args())\n\n    # check directory\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    for idx, (utt_id, lmspc) in enumerate(\n        file_reader_helper(args.rspecifier, args.filetype), 1\n    ):\n        if args.n_mels is not None:\n            spc = logmelspc_to_linearspc(\n                lmspc,\n                fs=args.fs,\n                n_mels=args.n_mels,\n                n_fft=args.n_fft,\n                fmin=args.fmin,\n                fmax=args.fmax,\n            )\n        else:\n            spc = lmspc\n        y = griffin_lim(\n            spc,\n            n_fft=args.n_fft,\n            n_shift=args.n_shift,\n            win_length=args.win_length,\n            window=args.window,\n            n_iters=args.iters,\n        )\n        logging.info(""(%d) %s"" % (idx, utt_id))\n        write(\n            args.outdir + ""/%s.wav"" % utt_id,\n            args.fs,\n            (y * np.iinfo(np.int16).max).astype(np.int16),\n        )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/copy-feats.py,0,"b'#!/usr/bin/env python3\nimport argparse\nfrom distutils.util import strtobool\nimport logging\n\nfrom espnet.transform.transformation import Transformation\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_utils import is_scipy_wav_style\nfrom espnet.utils.cli_writers import file_writer_helper\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""copy feature with preprocessing"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--in-filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--out-filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for the wspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--write-num-frames"", type=str, help=""Specify wspecifer for utt2num_frames""\n    )\n    parser.add_argument(\n        ""--compress"", type=strtobool, default=False, help=""Save in compressed format""\n    )\n    parser.add_argument(\n        ""--compression-method"",\n        type=int,\n        default=2,\n        help=""Specify the method(if mat) or "" ""gzip-level(if hdf5)"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""rspecifier"", type=str, help=""Read specifier for feats. e.g. ark:some.ark""\n    )\n    parser.add_argument(\n        ""wspecifier"", type=str, help=""Write specifier. e.g. ark:some.ark""\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    if args.preprocess_conf is not None:\n        preprocessing = Transformation(args.preprocess_conf)\n        logging.info(""Apply preprocessing: {}"".format(preprocessing))\n    else:\n        preprocessing = None\n\n    with file_writer_helper(\n        args.wspecifier,\n        filetype=args.out_filetype,\n        write_num_frames=args.write_num_frames,\n        compress=args.compress,\n        compression_method=args.compression_method,\n    ) as writer:\n        for utt, mat in file_reader_helper(args.rspecifier, args.in_filetype):\n            if is_scipy_wav_style(mat):\n                # If data is sound file, then got as Tuple[int, ndarray]\n                rate, mat = mat\n\n            if preprocessing is not None:\n                mat = preprocessing(mat, uttid_list=utt)\n\n            # shape = (Time, Channel)\n            if args.out_filetype in [""sound.hdf5"", ""sound""]:\n                # Write Tuple[int, numpy.ndarray] (scipy style)\n                writer[utt] = (rate, mat)\n            else:\n                writer[utt] = mat\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/dump-pcm.py,0,"b'#!/usr/bin/env python3\nimport argparse\nfrom distutils.util import strtobool\nimport logging\n\nimport kaldiio\nimport numpy\n\nfrom espnet.transform.transformation import Transformation\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_writers import file_writer_helper\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""dump PCM files from a WAV scp file"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--write-num-frames"", type=str, help=""Specify wspecifer for utt2num_frames""\n    )\n    parser.add_argument(\n        ""--filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for output. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--format"",\n        type=str,\n        default=None,\n        help=""The file format for output pcm. ""\n        ""This option is only valid ""\n        \'when ""--filetype"" is ""sound.hdf5"" or ""sound""\',\n    )\n    parser.add_argument(\n        ""--compress"", type=strtobool, default=False, help=""Save in compressed format""\n    )\n    parser.add_argument(\n        ""--compression-method"",\n        type=int,\n        default=2,\n        help=""Specify the method(if mat) or "" ""gzip-level(if hdf5)"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--normalize"",\n        choices=[1, 16, 24, 32],\n        type=int,\n        default=None,\n        help=""Give the bit depth of the PCM, ""\n        ""then normalizes data to scale in [-1,1]"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""--keep-length"",\n        type=strtobool,\n        default=True,\n        help=""Truncating or zero padding if the output length ""\n        ""is changed from the input by preprocessing"",\n    )\n    parser.add_argument(""rspecifier"", type=str, help=""WAV scp file"")\n    parser.add_argument(\n        ""--segments"",\n        type=str,\n        help=""segments-file format: each line is either""\n        ""<segment-id> <recording-id> <start-time> <end-time>""\n        ""e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5"",\n    )\n    parser.add_argument(""wspecifier"", type=str, help=""Write specifier"")\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    if args.preprocess_conf is not None:\n        preprocessing = Transformation(args.preprocess_conf)\n        logging.info(""Apply preprocessing: {}"".format(preprocessing))\n    else:\n        preprocessing = None\n\n    with file_writer_helper(\n        args.wspecifier,\n        filetype=args.filetype,\n        write_num_frames=args.write_num_frames,\n        compress=args.compress,\n        compression_method=args.compression_method,\n        pcm_format=args.format,\n    ) as writer:\n        for utt_id, (rate, array) in kaldiio.ReadHelper(args.rspecifier, args.segments):\n            if args.filetype == ""mat"":\n                # Kaldi-matrix doesn\'t support integer\n                array = array.astype(numpy.float32)\n\n            if array.ndim == 1:\n                # (Time) -> (Time, Channel)\n                array = array[:, None]\n\n            if args.normalize is not None and args.normalize != 1:\n                array = array.astype(numpy.float32)\n                array = array / (1 << (args.normalize - 1))\n\n            if preprocessing is not None:\n                orgtype = array.dtype\n                out = preprocessing(array, uttid_list=utt_id)\n                out = out.astype(orgtype)\n\n                if args.keep_length:\n                    if len(out) > len(array):\n                        out = numpy.pad(\n                            out,\n                            [(0, len(out) - len(array))]\n                            + [(0, 0) for _ in range(out.ndim - 1)],\n                            mode=""constant"",\n                        )\n                    elif len(out) < len(array):\n                        # The length can be changed by stft, for example.\n                        out = out[: len(out)]\n\n                array = out\n\n            # shape = (Time, Channel)\n            if args.filetype in [""sound.hdf5"", ""sound""]:\n                # Write Tuple[int, numpy.ndarray] (scipy style)\n                writer[utt_id] = (rate, array)\n            else:\n                writer[utt_id] = array\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/eval-source-separation.py,0,"b'#!/usr/bin/env python3\nimport argparse\nfrom collections import OrderedDict\nfrom distutils.util import strtobool\nimport itertools\nimport logging\nimport os\nfrom pathlib import Path\nimport shutil\nimport subprocess\nimport sys\nfrom tempfile import TemporaryDirectory\nimport warnings\n\nimport museval\nimport numpy as np\nfrom pystoi.stoi import stoi\nimport soundfile\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef eval_STOI(ref, y, fs, extended=False, compute_permutation=True):\n    """"""Calculate STOI\n\n    Reference:\n        A short-time objective intelligibility measure\n            for time-frequency weighted noisy speech\n        https://ieeexplore.ieee.org/document/5495701\n\n    Note(kamo):\n        STOI is defined on the signal at 10kHz\n        and the input at the other sampling rate will be resampled.\n        Thus, the result differs depending on the implementation of resampling.\n        Especially, pystoi cannot reproduce matlab\'s resampling now.\n\n    :param ref (np.ndarray): Reference (Nsrc, Nframe, Nmic)\n    :param y (np.ndarray): Enhanced (Nsrc, Nframe, Nmic)\n    :param fs (int): Sample frequency\n    :param extended (bool): stoi or estoi\n    :param compute_permutation (bool):\n    :return: value, perm\n    :rtype: Tuple[Tuple[float, ...], Tuple[int, ...]]\n    """"""\n    if ref.shape != y.shape:\n        raise ValueError(\n            ""ref and y should have the same shape: {} != {}"".format(ref.shape, y.shape)\n        )\n    if ref.ndim != 3:\n        raise ValueError(""Input must have 3 dims: {}"".format_map(ref.ndim))\n    n_src = ref.shape[0]\n    n_mic = ref.shape[2]\n\n    if compute_permutation:\n        index_list = list(itertools.permutations(range(n_src)))\n    else:\n        index_list = [list(range(n_src))]\n\n    values = [\n        [\n            sum(stoi(ref[i, :, ch], y[j, :, ch], fs, extended) for ch in range(n_mic))\n            / n_mic\n            for i, j in enumerate(indices)\n        ]\n        for indices in index_list\n    ]\n\n    best_pairs = sorted(\n        [(v, i) for v, i in zip(values, index_list)], key=lambda x: sum(x[0])\n    )[-1]\n    value, perm = best_pairs\n    return tuple(value), tuple(perm)\n\n\ndef eval_PESQ(ref, enh, fs, compute_permutation: bool = True, wideband: bool = True):\n    """"""Evaluate PESQ\n\n    PESQ program can be downloaded from here:\n        http://www.itu.int/rec/dologin_pub.asp?lang=e&id=T-REC-P.862-200511-I!Amd2!SOFT-ZST-E&type=items\n\n    Reference:\n        Perceptual evaluation of speech quality (PESQ)-a new method\n            for speech quality assessment of telephone networks and codecs\n        https://ieeexplore.ieee.org/document/941023\n\n    :param x (np.ndarray): Reference (Nsrc, Nframe, Nmic)\n    :param y (np.ndarray): Enhanced (Nsrc, Nframe, Nmic)\n    :param fs (int): Sample frequency\n    :param compute_permutation (bool):\n    """"""\n    if shutil.which(""PESQ"") is None:\n        raise RuntimeError(""PESQ: command not found: Please install"")\n    if fs not in (8000, 16000):\n        raise ValueError(""Sample frequency must be 8000 or 16000: {}"".format(fs))\n    if ref.shape != enh.shape:\n        raise ValueError(\n            ""ref and enh should have the same shape: {} != {}"".format(\n                ref.shape, enh.shape\n            )\n        )\n    if ref.ndim != 3:\n        raise ValueError(""Input must have 3 dims: {}"".format_map(ref.ndim))\n\n    n_src = ref.shape[0]\n    n_mic = ref.shape[2]\n    with TemporaryDirectory() as d:\n        # Dumping wav files temporary\n        ref_files = []\n        enh_files = []\n        for isrc in range(n_src):\n            refs = []  # [Nsrc, Nmic]\n            enhs = []  # [Nsrc, Nmic]\n            for imic in range(n_mic):\n                wv = str(os.path.join(d, ""ref.{}.{}.wav"".format(isrc, imic)))\n                soundfile.write(wv, ref[isrc, :, imic].astype(np.int16), fs)\n                refs.append(wv)\n\n                wv = str(os.path.join(d, ""enh.{}.{}.wav"".format(isrc, imic)))\n                soundfile.write(wv, enh[isrc, :, imic].astype(np.int16), fs)\n                enhs.append(wv)\n            ref_files.append(refs)\n            enh_files.append(enhs)\n\n        if compute_permutation:\n            index_list = list(itertools.permutations(range(n_src)))\n        else:\n            index_list = [list(range(n_src))]\n\n        values = []\n        for indices in index_list:\n            values2 = []\n            for i, j in enumerate(indices):\n                lis = []\n                for imic in range(n_mic):\n                    # PESQ +<8000|16000> <ref.wav> <enh.wav> [smos] [cond]\n                    if wideband:\n                        commands = [\n                            ""PESQ"",\n                            ""+{}"".format(fs),\n                            ""+wb"",\n                            ref_files[i][imic],\n                            enh_files[j][imic],\n                        ]\n                    else:\n                        commands = [\n                            ""PESQ"",\n                            ""+{}"".format(fs),\n                            ref_files[i][imic],\n                            enh_files[j][imic],\n                        ]\n                    with subprocess.Popen(\n                        commands, stdout=subprocess.DEVNULL, cwd=d\n                    ) as p:\n                        _, _ = p.communicate()\n\n                    # e.g.\n                    # REFERENCE\t DEGRADED\t PESQMOS\t MOSLQO\t SAMPLE_FREQ\t MODE\n                    # /tmp/t/ref.0.wav\t /tmp/t/enh.0.wav\t -1.000\t 4.644\t 16000\twb\n                    result_txt = Path(d) / ""pesq_results.txt""\n                    if result_txt.exists():\n                        with result_txt.open(""r"") as f:\n                            lis.append(float(f.readlines()[1].split()[3]))\n                    else:\n                        # Sometimes PESQ is failed. I don\'t know why.\n                        warnings.warn(""Processing error is found."")\n                        lis.append(1.0)\n                    # Averaging over n_mic\n                # Averaging over n_mic\n                values2.append(sum(lis) / len(lis))\n            values.append(values2)\n    best_pairs = sorted(\n        [(v, i) for v, i in zip(values, index_list)], key=lambda x: sum(x[0])\n    )[-1]\n    value, perm = best_pairs\n    return tuple(value), tuple(perm)\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""Evaluate enhanced speech. ""\n        ""e.g. {c} --ref ref.scp --enh enh.scp --outdir outputdir""\n        ""or {c} --ref ref.scp ref2.scp --enh enh.scp enh2.scp ""\n        ""--outdir outputdir"".format(c=sys.argv[0]),\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--ref"",\n        dest=""reffiles"",\n        nargs=""+"",\n        type=str,\n        required=True,\n        help=""WAV file lists for reference"",\n    )\n    parser.add_argument(\n        ""--enh"",\n        dest=""enhfiles"",\n        nargs=""+"",\n        type=str,\n        required=True,\n        help=""WAV files lists for enhanced"",\n    )\n    parser.add_argument(""--outdir"", type=str, required=True)\n    parser.add_argument(\n        ""--keylist"",\n        type=str,\n        help=""Specify the target samples. By default, ""\n        ""using all keys in the first reference file"",\n    )\n    parser.add_argument(\n        ""--evaltypes"",\n        type=str,\n        nargs=""+"",\n        choices=[""SDR"", ""STOI"", ""ESTOI"", ""PESQ""],\n        default=[""SDR"", ""STOI"", ""ESTOI"", ""PESQ""],\n    )\n    parser.add_argument(\n        ""--permutation"",\n        type=strtobool,\n        default=True,\n        help=""Compute all permutations or "" ""use the pair of input order"",\n    )\n\n    # About BSS Eval v4:\n    # The 2018 Signal Separation Evaluation Campaign\n    # https://arxiv.org/abs/1804.06267\n    parser.add_argument(\n        ""--bss-eval-images"",\n        type=strtobool,\n        default=True,\n        help=""Use bss_eval_images or bss_eval_sources. ""\n        ""For more detail, see museval source codes."",\n    )\n    parser.add_argument(\n        ""--bss-eval-version"",\n        type=str,\n        default=""v3"",\n        choices=[""v3"", ""v4""],\n        help=""Specify bss-eval-version: v3 or v4"",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n    if len(args.reffiles) != len(args.enhfiles):\n        raise RuntimeError(\n            ""The number of ref files are different ""\n            ""from the enh files: {} != {}"".format(\n                len(args.reffiles), len(args.enhfiles)\n            )\n        )\n    if len(args.enhfiles) == 1:\n        args.permutation = False\n\n    # Read text files and created a mapping of key2filepath\n    reffiles_dict = OrderedDict()  # Dict[str, Dict[str, str]]\n    for ref in args.reffiles:\n        d = OrderedDict()\n        with open(ref, ""r"") as f:\n            for line in f:\n                key, path = line.split(None, 1)\n                d[key] = path.rstrip()\n        reffiles_dict[ref] = d\n\n    enhfiles_dict = OrderedDict()  # Dict[str, Dict[str, str]]\n    for enh in args.enhfiles:\n        d = OrderedDict()\n        with open(enh, ""r"") as f:\n            for line in f:\n                key, path = line.split(None, 1)\n                d[key] = path.rstrip()\n        enhfiles_dict[enh] = d\n\n    if args.keylist is not None:\n        with open(args.keylist, ""r"") as f:\n            keylist = [line.rstrip().split()[0] for line in f]\n    else:\n        keylist = list(reffiles_dict.values())[0]\n\n    if len(keylist) == 0:\n        raise RuntimeError(""No keys are found"")\n\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    evaltypes = []\n    for evaltype in args.evaltypes:\n        if evaltype == ""SDR"":\n            evaltypes += [""SDR"", ""ISR"", ""SIR"", ""SAR""]\n        else:\n            evaltypes.append(evaltype)\n\n    # Open files in write mode\n    writers = {k: open(os.path.join(args.outdir, k), ""w"") for k in evaltypes}\n\n    for key in keylist:\n        # 1. Load ref files\n        rate_prev = None\n\n        ref_signals = []\n        for listname, d in reffiles_dict.items():\n            if key not in d:\n                raise RuntimeError(""{} doesn\'t exist in {}"".format(key, listname))\n            filepath = d[key]\n            signal, rate = soundfile.read(filepath, dtype=np.int16)\n            if signal.ndim == 1:\n                # (Nframe) -> (Nframe, 1)\n                signal = signal[:, None]\n            ref_signals.append(signal)\n            if rate_prev is not None and rate != rate_prev:\n                raise RuntimeError(""Sampling rates mismatch"")\n            rate_prev = rate\n\n        # 2. Load enh files\n        enh_signals = []\n        for listname, d in enhfiles_dict.items():\n            if key not in d:\n                raise RuntimeError(""{} doesn\'t exist in {}"".format(key, listname))\n            filepath = d[key]\n            signal, rate = soundfile.read(filepath, dtype=np.int16)\n            if signal.ndim == 1:\n                # (Nframe) -> (Nframe, 1)\n                signal = signal[:, None]\n            enh_signals.append(signal)\n            if rate_prev is not None and rate != rate_prev:\n                raise RuntimeError(""Sampling rates mismatch"")\n            rate_prev = rate\n\n        for signal in ref_signals + enh_signals:\n            if signal.shape[1] != ref_signals[0].shape[1]:\n                raise RuntimeError(""The number of channels mismatch"")\n\n        # 3. Zero padding to adjust the length to the maximum length in inputs\n        ml = max(len(s) for s in ref_signals + enh_signals)\n        ref_signals = [\n            np.pad(s, [(0, ml - len(s)), (0, 0)], mode=""constant"") if len(s) < ml else s\n            for s in ref_signals\n        ]\n\n        enh_signals = [\n            np.pad(s, [(0, ml - len(s)), (0, 0)], mode=""constant"") if len(s) < ml else s\n            for s in enh_signals\n        ]\n\n        # ref_signals, enh_signals: (Nsrc, Nframe, Nmic)\n        ref_signals = np.stack(ref_signals, axis=0)\n        enh_signals = np.stack(enh_signals, axis=0)\n\n        # 4. Evaluates\n        for evaltype in args.evaltypes:\n            if evaltype == ""SDR"":\n                (sdr, isr, sir, sar, perm) = museval.metrics.bss_eval(\n                    ref_signals,\n                    enh_signals,\n                    window=np.inf,\n                    hop=np.inf,\n                    compute_permutation=args.permutation,\n                    filters_len=512,\n                    framewise_filters=args.bss_eval_version == ""v3"",\n                    bsseval_sources_version=not args.bss_eval_images,\n                )\n\n                # sdr: (Nsrc, Nframe)\n                writers[""SDR""].write(\n                    ""{} {}\\n"".format(key, "" "".join(map(str, sdr[:, 0])))\n                )\n                writers[""ISR""].write(\n                    ""{} {}\\n"".format(key, "" "".join(map(str, isr[:, 0])))\n                )\n                writers[""SIR""].write(\n                    ""{} {}\\n"".format(key, "" "".join(map(str, sir[:, 0])))\n                )\n                writers[""SAR""].write(\n                    ""{} {}\\n"".format(key, "" "".join(map(str, sar[:, 0])))\n                )\n\n            elif evaltype == ""STOI"":\n                stoi, perm = eval_STOI(\n                    ref_signals,\n                    enh_signals,\n                    rate,\n                    extended=False,\n                    compute_permutation=args.permutation,\n                )\n                writers[""STOI""].write(""{} {}\\n"".format(key, "" "".join(map(str, stoi))))\n\n            elif evaltype == ""ESTOI"":\n                estoi, perm = eval_STOI(\n                    ref_signals,\n                    enh_signals,\n                    rate,\n                    extended=True,\n                    compute_permutation=args.permutation,\n                )\n                writers[""ESTOI""].write(""{} {}\\n"".format(key, "" "".join(map(str, estoi))))\n\n            elif evaltype == ""PESQ"":\n                pesq, perm = eval_PESQ(\n                    ref_signals, enh_signals, rate, compute_permutation=args.permutation\n                )\n                writers[""PESQ""].write(""{} {}\\n"".format(key, "" "".join(map(str, pesq))))\n            else:\n                # Cannot reach\n                raise RuntimeError\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/eval_perm_free_error.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Johns Hopkins University (Xuankai Chang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\nimport argparse\nimport codecs\nimport json\nimport logging\nimport re\nimport six\nimport sys\n\nimport numpy as np\n\n\ndef permutationDFS(source, start, res):\n    # get permutations with DFS\n    # return order in [[1, 2], [2, 1]] or\n    # [[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 2, 1], [3, 1, 2]]\n    if start == len(source) - 1:  # reach final state\n        res.append(source.tolist())\n    for i in range(start, len(source)):\n        # swap values at position start and i\n        source[start], source[i] = source[i], source[start]\n        permutationDFS(source, start + 1, res)\n        # reverse the swap\n        source[start], source[i] = source[i], source[start]\n\n\n# pre-set the permutation scheme (ref_idx, hyp_idx)\ndef permutation_schemes(num_spkrs):\n    src = [x for x in range(1, num_spkrs + 1)]\n    perms = []\n\n    # get all permutations of [1, ..., num_spkrs]\n    # [[r1h1, r2h2], [r1h2, r2h1]]\n    # [[r1h1, r2h2, r3h3], [r1h1, r2h3, r3h2], [r1h2, r2h1, r3h3],\n    #  [r1h2, r2h3, r3h2], [r1h3, r2h2, r3h1], [r1h3, r2h1, r3h2]]]\n    # ...\n    permutationDFS(np.array(src), 0, perms)\n\n    keys = []\n    for perm in perms:\n        keys.append([""r%dh%d"" % (i, j) for i, j in enumerate(perm, 1)])\n\n    return sum(keys, []), keys\n\n\ndef convert_score(keys, dic):\n    ret = {}\n    pat = re.compile(r""\\d+"")\n    for k in keys:\n        score = dic[k][""Scores""]\n        score = list(map(int, pat.findall(score)))  # [c,s,d,i]\n        assert len(score) == 4\n        ret[k] = score\n    return ret\n\n\ndef get_utt_permutation(old_dic, num_spkrs=2):\n    perm, keys = permutation_schemes(num_spkrs)\n    new_dic = {}\n\n    for id in old_dic.keys():\n        # compute error rate for each utt\n        in_dic = old_dic[id]\n        score = convert_score(perm, in_dic)\n        perm_score = []\n        for ks in keys:\n            tmp_score = [0, 0, 0, 0]\n            for k in ks:\n                tmp_score = [tmp_score[i] + score[k][i] for i in range(4)]\n            perm_score.append(tmp_score)\n\n        error_rate = [\n            sum(s[1:4]) / float(sum(s[0:3])) for s in perm_score\n        ]  # (s+d+i) / (c+s+d)\n\n        min_idx, min_v = min(enumerate(error_rate), key=lambda x: x[1])\n        dic = {}\n        for k in keys[min_idx]:\n            dic[k] = in_dic[k]\n        dic[""Scores""] = ""(#C #S #D #I) "" + "" "".join(map(str, perm_score[min_idx]))\n        new_dic[id] = dic\n\n    return new_dic\n\n\ndef get_results(result_file, result_key):\n    re_id = r""^id: ""\n    re_strings = {\n        ""Speaker"": r""^Speaker sentences"",\n        ""Scores"": r""^Scores: "",\n        ""REF"": r""^REF: "",\n        ""HYP"": r""^HYP: "",\n    }\n    re_id = re.compile(re_id)\n    re_patterns = {}\n    for p in re_strings.keys():\n        re_patterns[p] = re.compile(re_strings[p])\n\n    results = {}\n    tmp_id = None\n    tmp_ret = {}\n\n    with open(result_file, ""r"") as f:\n        line = f.readline()\n        while line:\n            x = line.rstrip()\n            x_split = x.split()\n\n            if re_id.match(x):\n                if tmp_id:\n                    results[tmp_id] = {result_key: tmp_ret}\n                    tmp_ret = {}\n                tmp_id = x_split[1]\n            for p in re_patterns.keys():\n                if re_patterns[p].match(x):\n                    tmp_ret[p] = "" "".join(x_split[1:])\n            line = f.readline()\n\n    if tmp_ret != {}:\n        results[tmp_id] = {result_key: tmp_ret}\n\n    return {""utts"": results}\n\n\ndef merge_results(results):\n    rslt_lst = []\n\n    # make intersection set for utterance keys\n    intersec_keys = []\n    for x in results.keys():\n        j = results[x]\n\n        ks = j[""utts""].keys()\n        logging.info(x + "": has "" + str(len(ks)) + "" utterances"")\n\n        if len(intersec_keys) > 0:\n            intersec_keys = intersec_keys.intersection(set(ks))\n        else:\n            intersec_keys = set(ks)\n        rslt_lst.append(j)\n\n    logging.info(\n        ""After merge, the result has "" + str(len(intersec_keys)) + "" utterances""\n    )\n\n    # merging results\n    dic = dict()\n    for k in intersec_keys:\n        v = rslt_lst[0][""utts""][k]\n        for j in rslt_lst[1:]:\n            v.update(j[""utts""][k])\n        dic[k] = v\n\n    return dic\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=""evaluate permutation-free error"")\n    parser.add_argument(\n        ""--num-spkrs"", type=int, default=2, help=""number of mixed speakers.""\n    )\n    parser.add_argument(\n        ""results"",\n        type=str,\n        nargs=""+"",\n        help=""the scores between references and hypotheses, ""\n        ""in ascending order of references (1st) and hypotheses (2nd), ""\n        ""e.g. [r1h1, r1h2, r2h1, r2h2] in 2-speaker-mix case."",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    if len(args.results) != args.num_spkrs ** 2:\n        parser.print_help()\n        sys.exit(1)\n\n    # Read results from files\n    results = {}\n    for r in six.moves.range(1, args.num_spkrs + 1):\n        for h in six.moves.range(1, args.num_spkrs + 1):\n            idx = (r - 1) * args.num_spkrs + h - 1\n            key = ""r{}h{}"".format(r, h)\n\n            result = get_results(args.results[idx], key)\n            results[key] = result\n\n    # Merge the results of every permutation\n    results = merge_results(results)\n\n    # Get the final results with best permutation\n    new_results = get_utt_permutation(results, args.num_spkrs)\n\n    # Get WER/CER\n    pat = re.compile(r""\\d+"")\n    score = np.zeros((len(new_results.keys()), 4))\n    for idx, key in enumerate(new_results.keys()):\n        # [c, s, d, i]\n        tmp_score = list(map(int, pat.findall(new_results[key][""Scores""])))\n        score[idx] = tmp_score\n    return score, new_results\n\n\nif __name__ == ""__main__"":\n    sys.stdout = codecs.getwriter(""utf-8"")(sys.stdout.buffer)\n\n    scores, new_results = main()\n    score_sum = np.sum(scores, axis=0, dtype=int)\n\n    # Print results\n    print(sys.argv)\n    print(""Total Scores: (#C #S #D #I) "" + "" "".join(map(str, list(score_sum))))\n    print(\n        ""Error Rate:   {:0.2f}"".format(\n            100 * sum(score_sum[1:4]) / float(sum(score_sum[0:3]))\n        )\n    )\n    print(""Total Utts: "", str(scores.shape[0]))\n\n    print(\n        json.dumps(\n            {""utts"": new_results},\n            indent=4,\n            ensure_ascii=False,\n            sort_keys=True,\n            separators=("","", "": ""),\n        )\n    )\n'"
utils/feat-to-shape.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport logging\nimport sys\n\nfrom espnet.transform.transformation import Transformation\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_utils import is_scipy_wav_style\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert feature to its shape"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""rspecifier"", type=str, help=""Read specifier for feats. e.g. ark:some.ark""\n    )\n    parser.add_argument(\n        ""out"",\n        nargs=""?"",\n        type=argparse.FileType(""w""),\n        default=sys.stdout,\n        help=""The output filename. "" ""If omitted, then output to sys.stdout"",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    if args.preprocess_conf is not None:\n        preprocessing = Transformation(args.preprocess_conf)\n        logging.info(""Apply preprocessing: {}"".format(preprocessing))\n    else:\n        preprocessing = None\n\n    # There are no necessary for matrix without preprocessing,\n    # so change to file_reader_helper to return shape.\n    # This make sense only with filetype=""hdf5"".\n    for utt, mat in file_reader_helper(\n        args.rspecifier, args.filetype, return_shape=preprocessing is None\n    ):\n        if preprocessing is not None:\n            if is_scipy_wav_style(mat):\n                # If data is sound file, then got as Tuple[int, ndarray]\n                rate, mat = mat\n            mat = preprocessing(mat, uttid_list=utt)\n            shape_str = "","".join(map(str, mat.shape))\n        else:\n            if len(mat) == 2 and isinstance(mat[1], tuple):\n                # If data is sound file, Tuple[int, Tuple[int, ...]]\n                rate, mat = mat\n            shape_str = "","".join(map(str, mat))\n        args.out.write(""{} {}\\n"".format(utt, shape_str))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/feats2npy.py,0,"b'#!/usr/bin/env python\n#  coding: utf-8\n\nimport argparse\nfrom kaldiio import ReadHelper\nimport numpy as np\nimport os\nfrom os.path import join\nimport sys\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""Convet kaldi-style features to numpy arrays"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""scp_file"", type=str, help=""scp file"")\n    parser.add_argument(""out_dir"", type=str, help=""output directory"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    args = get_parser().parse_args(sys.argv[1:])\n    os.makedirs(args.out_dir, exist_ok=True)\n    with ReadHelper(f""scp:{args.scp_file}"") as f:\n        for utt_id, arr in f:\n            out_path = join(args.out_dir, f""{utt_id}-feats.npy"")\n            np.save(out_path, arr, allow_pickle=False)\n    sys.exit(0)\n'"
utils/filt.py,0,"b'#!/usr/bin/env python3\n\n# Apache 2.0\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport sys\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""filter words in a text file"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--exclude"",\n        ""-v"",\n        dest=""exclude"",\n        action=""store_true"",\n        help=""exclude filter words"",\n    )\n    parser.add_argument(""filt"", type=str, help=""filter list"")\n    parser.add_argument(""infile"", type=str, help=""input file"")\n    return parser\n\n\ndef main(args):\n    args = get_parser().parse_args(args)\n    filter_file(args.infile, args.filt, args.exclude)\n\n\ndef filter_file(infile, filt, exclude):\n    vocab = set()\n    with codecs.open(filt, ""r"", encoding=""utf-8"") as vocabfile:\n        for line in vocabfile:\n            vocab.add(line.strip())\n\n    sys.stdout = codecs.getwriter(""utf-8"")(\n        sys.stdout if is_python2 else sys.stdout.buffer\n    )\n    with codecs.open(infile, ""r"", encoding=""utf-8"") as textfile:\n        for line in textfile:\n            if exclude:\n                print(\n                    "" "".join(\n                        map(\n                            lambda word: word if word not in vocab else """",\n                            line.strip().split(),\n                        )\n                    )\n                )\n            else:\n                print(\n                    "" "".join(\n                        map(\n                            lambda word: word if word in vocab else ""<UNK>"",\n                            line.strip().split(),\n                        )\n                    )\n                )\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
utils/generate_wav_from_fbank.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""This code is based on https://github.com/kan-bayashi/PytorchWaveNetVocoder.""""""\n\n# Copyright 2019 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\nimport os\nimport time\n\nimport h5py\nimport numpy as np\nimport pysptk\nimport torch\n\nfrom scipy.io.wavfile import write\nfrom sklearn.preprocessing import StandardScaler\n\nfrom espnet.nets.pytorch_backend.wavenet import decode_mu_law\nfrom espnet.nets.pytorch_backend.wavenet import encode_mu_law\nfrom espnet.nets.pytorch_backend.wavenet import WaveNet\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\nclass TimeInvariantMLSAFilter(object):\n    """"""Time invariant MLSA filter.\n\n    This module is used to perform noise shaping described in\n    `An investigation of noise shaping with perceptual\n     weighting for WaveNet-based speech generation`_.\n\n    Args:\n        coef (ndaaray): MLSA filter coefficient (D,).\n        alpha (float): All pass constant value.\n        n_shift (int): Shift length in points.\n\n    .. _`An investigation of noise shaping with perceptual\n        weighting for WaveNet-based speech generation`:\n        https://ieeexplore.ieee.org/abstract/document/8461332\n\n    """"""\n\n    def __init__(self, coef, alpha, n_shift):\n        self.coef = coef\n        self.n_shift = n_shift\n        self.mlsa_filter = pysptk.synthesis.Synthesizer(\n            pysptk.synthesis.MLSADF(order=coef.shape[0] - 1, alpha=alpha),\n            hopsize=n_shift,\n        )\n\n    def __call__(self, y):\n        """"""Apply time invariant MLSA filter.\n\n        Args:\n            y (ndarray): Waveform signal normalized from -1 to 1 (N,).\n\n        Returns:\n            y (ndarray): Filtered waveform signal normalized from -1 to 1 (N,).\n\n        """"""\n        # check shape and type\n        assert len(y.shape) == 1\n        y = np.float64(y)\n\n        # get frame number and then replicate mlsa coef\n        num_frames = int(len(y) / self.n_shift) + 1\n        coef = np.tile(self.coef, [num_frames, 1])\n\n        return self.mlsa_filter.synthesis(y, coef)\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""generate wav from FBANK using wavenet vocoder"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--fs"", type=int, default=22050, help=""Sampling frequency"")\n    parser.add_argument(""--n_fft"", type=int, default=1024, help=""FFT length in point"")\n    parser.add_argument(\n        ""--n_shift"", type=int, default=256, help=""Shift length in point""\n    )\n    parser.add_argument(""--model"", type=str, default=None, help=""WaveNet model"")\n    parser.add_argument(\n        ""--filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(""rspecifier"", type=str, help=""Input feature e.g. scp:feat.scp"")\n    parser.add_argument(""outdir"", type=str, help=""Output directory"")\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # logging info\n    logging.basicConfig(\n        level=logging.INFO,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n    logging.info(get_commandline_args())\n\n    # check directory\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n\n    # load model config\n    model_dir = os.path.dirname(args.model)\n    train_args = torch.load(os.path.join(model_dir, ""model.conf""))\n\n    # load statistics\n    scaler = StandardScaler()\n    with h5py.File(os.path.join(model_dir, ""stats.h5"")) as f:\n        scaler.mean_ = f[""/melspc/mean""][()]\n        scaler.scale_ = f[""/melspc/scale""][()]\n        # TODO(kan-bayashi): include following info as default\n        coef = f[""/mlsa/coef""][()]\n        alpha = f[""/mlsa/alpha""][()]\n\n    # define MLSA filter for noise shaping\n    mlsa_filter = TimeInvariantMLSAFilter(coef=coef, alpha=alpha, n_shift=args.n_shift,)\n\n    # define model and laod parameters\n    device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n    model = WaveNet(\n        n_quantize=train_args.n_quantize,\n        n_aux=train_args.n_aux,\n        n_resch=train_args.n_resch,\n        n_skipch=train_args.n_skipch,\n        dilation_depth=train_args.dilation_depth,\n        dilation_repeat=train_args.dilation_repeat,\n        kernel_size=train_args.kernel_size,\n        upsampling_factor=train_args.upsampling_factor,\n    )\n    model.load_state_dict(torch.load(args.model, map_location=""cpu"")[""model""])\n    model.eval()\n    model.to(device)\n\n    for idx, (utt_id, lmspc) in enumerate(\n        file_reader_helper(args.rspecifier, args.filetype), 1\n    ):\n        logging.info(""(%d) %s"" % (idx, utt_id))\n\n        # perform preprocesing\n        x = encode_mu_law(\n            np.zeros((1)), mu=train_args.n_quantize\n        )  # quatize initial seed waveform\n        h = scaler.transform(lmspc)  # normalize features\n\n        # convert to tensor\n        x = torch.tensor(x, dtype=torch.long, device=device)  # (1,)\n        h = torch.tensor(h, dtype=torch.float, device=device)  # (T, n_aux)\n\n        # get length of waveform\n        n_samples = (h.shape[0] - 1) * args.n_shift + args.n_fft\n\n        # generate\n        start_time = time.time()\n        with torch.no_grad():\n            y = model.generate(x, h, n_samples, interval=100)\n        logging.info(\n            ""generation speed = %s (sec / sample)""\n            % ((time.time() - start_time) / (len(y) - 1))\n        )\n        y = decode_mu_law(y, mu=train_args.n_quantize)\n\n        # apply mlsa filter for noise shaping\n        y = mlsa_filter(y)\n\n        # save as .wav file\n        write(\n            os.path.join(args.outdir, ""%s.wav"" % utt_id),\n            args.fs,\n            (y * np.iinfo(np.int16).max).astype(np.int16),\n        )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/get_yaml.py,0,"b'#!/usr/bin/env python3\nimport argparse\n\nimport yaml\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""get a specified attribute from a YAML file"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""inyaml"")\n    parser.add_argument(\n        ""attr"", help=\'foo.bar will access yaml.load(inyaml)[""foo""][""bar""]\'\n    )\n    return parser\n\n\ndef main():\n    args = get_parser().parse_args()\n    with open(args.inyaml, ""r"") as f:\n        indict = yaml.load(f, Loader=yaml.Loader)\n\n    try:\n        for attr in args.attr.split("".""):\n            if attr.isdigit():\n                attr = int(attr)\n            indict = indict[attr]\n        print(indict)\n    except KeyError:\n        # print nothing\n        # sys.exit(1)\n        pass\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/json2sctm.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport subprocess\nimport sys\n\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=""convert json to sctm"")\n    parser.add_argument(""json"", type=str, default=None, nargs=""?"", help=""input trn"")\n    parser.add_argument(""dict"", type=str, help=""dict"")\n    parser.add_argument(\n        ""--num-spkrs"", type=int, default=1, nargs=""?"", help=""number of speakers""\n    )\n    parser.add_argument(""--refs"", type=str, nargs=""*"", help=""ref for all speakers"")\n    parser.add_argument(""--hyps"", type=str, nargs=""*"", help=""hyp for all outputs"")\n    parser.add_argument(""--orig-stm"", type=str, nargs=""?"", help=""orig stm"")\n    parser.add_argument(""--stm"", type=str, default=None, nargs=""+"", help=""output stm"")\n    parser.add_argument(""--ctm"", type=str, default=None, nargs=""+"", help=""output ctm"")\n    parser.add_argument(\n        ""--bpe"", type=str, default=None, nargs=""?"", help=""BPE model if applicable""\n    )\n    return parser\n\n\ndef main(args):\n    from utils import json2trn\n    from utils import trn2ctm\n    from utils import trn2stm\n\n    parser = get_parser()\n    args = parser.parse_args(args)\n    if args.refs is None:\n        refs = [""ref_tmp.trn""]\n        del_ref = True\n    else:\n        refs = args.refs\n        del_ref = False\n    if args.hyps is None:\n        hyps = [""hyp_tmp.trn""]\n        del_hyp = True\n    else:\n        hyps = args.hyps\n        del_hyp = False\n    json2trn.convert(args.json, args.dict, refs, hyps, args.num_spkrs)\n    for trn in refs + hyps:\n        # We don\'t remove non-lang-syms because kaldi already removes them when scoring\n        call_args = [""sed"", ""-i.bak2"", ""-r"", ""s/<blank> //g"", trn]\n        subprocess.check_call(call_args)\n        if args.bpe is not None:\n            with open(wrd_name(trn), ""w"") as out:\n                with open(trn, ""r"") as spm_in:\n                    sed_args = [""sed"", ""-e"", ""s/\xe2\x96\x81/ /g""]\n                    sed = subprocess.Popen(sed_args, stdout=out, stdin=subprocess.PIPE)\n                    spm_args = [\n                        ""spm_decode"",\n                        ""--model="" + args.bpe,\n                        ""--input_format=piece"",\n                    ]\n                    subprocess.Popen(spm_args, stdin=spm_in)\n                    sed.communicate()\n        else:\n            call_args = [\n                ""sed"",\n                ""-e"",\n                ""s/ //g"",\n                ""-e"",\n                ""s/(/ (/"",\n                ""-e"",\n                ""s/<space>/ /g"",\n                trn,\n            ]\n            with open(wrd_name(trn), ""w"") as out:\n                sed = subprocess.Popen(call_args, stdout=out)\n                sed.communicate()\n    for trn, stm in zip(refs, args.stm):\n        trn2stm.convert(wrd_name(trn), stm, args.orig_stm)\n    if del_ref:\n        os.remove(refs[0])\n        os.remove(refs[0] + "".bak2"")\n        os.remove(wrd_name(refs[0]))\n\n    for trn, ctm in zip(hyps, args.ctm):\n        trn2ctm.convert(wrd_name(trn), ctm)\n    if del_hyp:\n        os.remove(hyps[0])\n        os.remove(hyps[0] + "".bak2"")\n        os.remove(wrd_name(hyps[0]))\n\n\ndef wrd_name(trn):\n    split = trn.split(""."")\n    return ""."".join(split[:-1]) + "".wrd."" + split[-1]\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
utils/json2text.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport logging\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert ASR recognized json to text"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""json"", type=str, help=""json files"")\n    parser.add_argument(""dict"", type=str, help=""dict"")\n    parser.add_argument(""ref"", type=str, help=""ref"")\n    parser.add_argument(""hyp"", type=str, help=""hyp"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    args = get_parser().parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    logging.info(""reading %s"", args.json)\n    with codecs.open(args.json, ""r"", encoding=""utf-8"") as f:\n        j = json.load(f)\n\n    logging.info(""reading %s"", args.dict)\n    with codecs.open(args.dict, ""r"", encoding=""utf-8"") as f:\n        dictionary = f.readlines()\n    char_list = [entry.split("" "")[0] for entry in dictionary]\n    char_list.insert(0, ""<blank>"")\n    char_list.append(""<eos>"")\n    # print([x.encode(\'utf-8\') for x in char_list])\n\n    logging.info(""writing hyp trn to %s"", args.hyp)\n    logging.info(""writing ref trn to %s"", args.ref)\n    h = codecs.open(args.hyp, ""w"", encoding=""utf-8"")\n    r = codecs.open(args.ref, ""w"", encoding=""utf-8"")\n\n    for x in j[""utts""]:\n        seq = [\n            char_list[int(i)] for i in j[""utts""][x][""output""][0][""rec_tokenid""].split()\n        ]\n        h.write(x + "" "" + "" "".join(seq).replace(""<eos>"", """") + ""\\n"")\n\n        if ""tokenid"" in j[""utts""][x][""output""][0].keys():\n            seq = [\n                char_list[int(i)] for i in j[""utts""][x][""output""][0][""tokenid""].split()\n            ]\n            r.write(x + "" "" + "" "".join(seq).replace(""<eos>"", """") + ""\\n"")\n'"
utils/json2trn.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#           2018 Xuankai Chang (Shanghai Jiao Tong University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert a json to a transcription file with a token dictionary"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""json"", type=str, help=""json files"")\n    parser.add_argument(""dict"", type=str, help=""dict"")\n    parser.add_argument(""--num-spkrs"", type=int, default=1, help=""number of speakers"")\n    parser.add_argument(""--refs"", type=str, nargs=""+"", help=""ref for all speakers"")\n    parser.add_argument(""--hyps"", type=str, nargs=""+"", help=""hyp for all outputs"")\n    return parser\n\n\ndef main(args):\n    args = get_parser().parse_args(args)\n    convert(args.json, args.dict, args.refs, args.hyps, args.num_spkrs)\n\n\ndef convert(jsonf, dic, refs, hyps, num_spkrs=1):\n    n_ref = len(refs)\n    n_hyp = len(hyps)\n    assert n_ref == n_hyp\n    assert n_ref == num_spkrs\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    logging.info(""reading %s"", jsonf)\n    with codecs.open(jsonf, ""r"", encoding=""utf-8"") as f:\n        j = json.load(f)\n\n    logging.info(""reading %s"", dic)\n    with codecs.open(dic, ""r"", encoding=""utf-8"") as f:\n        dictionary = f.readlines()\n    char_list = [entry.split("" "")[0] for entry in dictionary]\n    char_list.insert(0, ""<blank>"")\n    char_list.append(""<eos>"")\n\n    for ns in range(num_spkrs):\n        hyp_file = codecs.open(hyps[ns], ""w"", encoding=""utf-8"")\n        ref_file = codecs.open(refs[ns], ""w"", encoding=""utf-8"")\n\n        for x in j[""utts""]:\n            # recognition hypothesis\n            if num_spkrs == 1:\n                seq = [\n                    char_list[int(i)]\n                    for i in j[""utts""][x][""output""][0][""rec_tokenid""].split()\n                ]\n            else:\n                seq = [\n                    char_list[int(i)]\n                    for i in j[""utts""][x][""output""][ns][0][""rec_tokenid""].split()\n                ]\n            # In the recognition hypothesis,\n            # the <eos> symbol is usually attached in the last part of the sentence\n            # and it is removed below.\n            hyp_file.write("" "".join(seq).replace(""<eos>"", """")),\n            hyp_file.write(\n                "" ("" + j[""utts""][x][""utt2spk""].replace(""-"", ""_"") + ""-"" + x + "")\\n""\n            )\n\n            # reference\n            if num_spkrs == 1:\n                seq = j[""utts""][x][""output""][0][""token""]\n            else:\n                seq = j[""utts""][x][""output""][ns][0][""token""]\n            # Unlike the recognition hypothesis,\n            # the reference is directly generated from a token without dictionary\n            # to avoid to include <unk> symbols in the reference to make scoring normal.\n            # The detailed discussion can be found at\n            # https://github.com/espnet/espnet/issues/993\n            ref_file.write(\n                seq + "" ("" + j[""utts""][x][""utt2spk""].replace(""-"", ""_"") + ""-"" + x + "")\\n""\n            )\n\n        hyp_file.close()\n        ref_file.close()\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
utils/json2trn_mt.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n# NOTE: this is made for machine translation\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert json to machine translation transcription"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""json"", type=str, help=""json files"")\n    parser.add_argument(""dict"", type=str, help=""dict for target language"")\n    parser.add_argument(""--refs"", type=str, nargs=""+"", help=""ref for all speakers"")\n    parser.add_argument(""--hyps"", type=str, nargs=""+"", help=""hyp for all outputs"")\n    parser.add_argument(""--srcs"", type=str, nargs=""+"", help=""src for all outputs"")\n    parser.add_argument(\n        ""--dict-src"",\n        type=str,\n        help=""dict for source language"",\n        default=False,\n        nargs=""?"",\n    )\n    return parser\n\n\ndef main(args):\n    parser = get_parser()\n    args = parser.parse_args(args)\n    convert(args.json, args.dict, args.refs, args.hyps, args.srcs, args.dict_src)\n\n\ndef convert(jsonf, dic, refs, hyps, srcs, dic_src):\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    logging.info(""reading %s"", jsonf)\n    with codecs.open(jsonf, ""r"", encoding=""utf-8"") as f:\n        j = json.load(f)\n\n    # target dictionary\n    logging.info(""reading %s"", dic)\n    with codecs.open(dic, ""r"", encoding=""utf-8"") as f:\n        dictionary = f.readlines()\n    char_list_tgt = [entry.split("" "")[0] for entry in dictionary]\n    char_list_tgt.insert(0, ""<blank>"")\n    char_list_tgt.append(""<eos>"")\n\n    # source dictionary\n    logging.info(""reading %s"", dic_src)\n    if dic_src:\n        with codecs.open(dic_src, ""r"", encoding=""utf-8"") as f:\n            dictionary = f.readlines()\n        char_list_src = [entry.split("" "")[0] for entry in dictionary]\n        char_list_src.insert(0, ""<blank>"")\n        char_list_src.append(""<eos>"")\n\n    if hyps:\n        hyp_file = codecs.open(hyps[0], ""w"", encoding=""utf-8"")\n    ref_file = codecs.open(refs[0], ""w"", encoding=""utf-8"")\n    if srcs:\n        src_file = codecs.open(srcs[0], ""w"", encoding=""utf-8"")\n\n    for x in j[""utts""]:\n        # hyps\n        if hyps:\n            seq = [\n                char_list_tgt[int(i)]\n                for i in j[""utts""][x][""output""][0][""rec_tokenid""].split()\n            ]\n            hyp_file.write("" "".join(seq).replace(""<eos>"", """")),\n            hyp_file.write(\n                "" ("" + j[""utts""][x][""utt2spk""].replace(""-"", ""_"") + ""-"" + x + "")\\n""\n            )\n\n        # ref\n        seq = [\n            char_list_tgt[int(i)] for i in j[""utts""][x][""output""][0][""tokenid""].split()\n        ]\n        ref_file.write("" "".join(seq).replace(""<eos>"", """")),\n        ref_file.write(\n            "" ("" + j[""utts""][x][""utt2spk""].replace(""-"", ""_"") + ""-"" + x + "")\\n""\n        )\n\n        # src\n        if ""tokenid_src"" in j[""utts""][x][""output""][0].keys():\n            if dic_src:\n                seq = [\n                    char_list_src[int(i)]\n                    for i in j[""utts""][x][""output""][0][""tokenid_src""].split()\n                ]\n            else:\n                seq = [\n                    char_list_tgt[int(i)]\n                    for i in j[""utts""][x][""output""][0][""tokenid_src""].split()\n                ]\n            src_file.write("" "".join(seq).replace(""<eos>"", """")),\n            src_file.write(\n                "" ("" + j[""utts""][x][""utt2spk""].replace(""-"", ""_"") + ""-"" + x + "")\\n""\n            )\n\n    if hyps:\n        hyp_file.close()\n    ref_file.close()\n    if srcs:\n        src_file.close()\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
utils/json2trn_wo_dict.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Okayama University (Katsuki Inoue)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert a json to a transcription file with a token dictionary"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""json"", type=str, help=""json files"")\n    parser.add_argument(""--num-spkrs"", type=int, default=1, help=""number of speakers"")\n    parser.add_argument(""--refs"", type=str, nargs=""+"", help=""ref for all speakers"")\n    parser.add_argument(""--hyps"", type=str, nargs=""+"", help=""hyp for all outputs"")\n    return parser\n\n\ndef main(args):\n    args = get_parser().parse_args(args)\n    convert(args.json, args.refs, args.hyps, args.num_spkrs)\n\n\ndef convert(jsonf, refs, hyps, num_spkrs=1):\n    n_ref = len(refs)\n    n_hyp = len(hyps)\n    assert n_ref == n_hyp\n    assert n_ref == num_spkrs\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    logging.info(""reading %s"", jsonf)\n    with codecs.open(jsonf, ""r"", encoding=""utf-8"") as f:\n        j = json.load(f)\n\n    for ns in range(num_spkrs):\n        hyp_file = codecs.open(hyps[ns], ""w"", encoding=""utf-8"")\n        ref_file = codecs.open(refs[ns], ""w"", encoding=""utf-8"")\n\n        for x in j[""utts""]:\n            # recognition hypothesis\n            if num_spkrs == 1:\n                seq = j[""utts""][x][""output""][0][""rec_text""].replace(""<eos>"", """")\n            else:\n                seq = j[""utts""][x][""output""][ns][0][""rec_text""].replace(""<eos>"", """")\n            # In the recognition hypothesis,\n            # the <eos> symbol is usually attached in the last part of the sentence\n            # and it is removed below.\n            hyp_file.write(seq)\n            hyp_file.write("" ("" + x.replace(""-"", ""_"") + "")\\n"")\n\n            # reference\n            if num_spkrs == 1:\n                seq = j[""utts""][x][""output""][0][""text""]\n            else:\n                seq = j[""utts""][x][""output""][ns][0][""text""]\n            # Unlike the recognition hypothesis,\n            # the reference is directly generated from a token without dictionary\n            # to avoid to include <unk> symbols in the reference to make scoring normal.\n            # The detailed discussion can be found at\n            # https://github.com/espnet/espnet/issues/993\n            ref_file.write(seq + "" ("" + x.replace(""-"", ""_"") + "")\\n"")\n\n        hyp_file.close()\n        ref_file.close()\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
utils/merge_scp2json.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nfrom distutils.util import strtobool\nfrom io import open\nimport json\nimport logging\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\n# Special types:\ndef shape(x):\n    """"""Change str to List[int]\n\n    >>> shape(\'3,5\')\n    [3, 5]\n    >>> shape(\' [3, 5] \')\n    [3, 5]\n\n    """"""\n\n    # x: \' [3, 5] \' -> \'3, 5\'\n    x = x.strip()\n    if x[0] == ""["":\n        x = x[1:]\n    if x[-1] == ""]"":\n        x = x[:-1]\n\n    return list(map(int, x.split("","")))\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""Given each file paths with such format as ""\n        ""<key>:<file>:<type>. type> can be omitted and the default ""\n        \'is ""str"". e.g. {} \'\n        ""--input-scps feat:data/feats.scp shape:data/utt2feat_shape:shape ""\n        ""--input-scps feat:data/feats2.scp shape:data/utt2feat2_shape:shape ""\n        ""--output-scps text:data/text shape:data/utt2text_shape:shape ""\n        ""--scps utt2spk:data/utt2spk"".format(sys.argv[0]),\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--input-scps"",\n        type=str,\n        nargs=""*"",\n        action=""append"",\n        default=[],\n        help=""Json files for the inputs"",\n    )\n    parser.add_argument(\n        ""--output-scps"",\n        type=str,\n        nargs=""*"",\n        action=""append"",\n        default=[],\n        help=""Json files for the outputs"",\n    )\n    parser.add_argument(\n        ""--scps"",\n        type=str,\n        nargs=""+"",\n        default=[],\n        help=""The json files except for the input and outputs"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=1, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--allow-one-column"",\n        type=strtobool,\n        default=False,\n        help=""Allow one column in input scp files. ""\n        ""In this case, the value will be empty string."",\n    )\n    parser.add_argument(\n        ""--out"",\n        ""-O"",\n        type=str,\n        help=""The output filename. "" ""If omitted, then output to sys.stdout"",\n    )\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_parser()\n    args = parser.parse_args()\n    args.scps = [args.scps]\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    # List[List[Tuple[str, str, Callable[[str], Any], str, str]]]\n    input_infos = []\n    output_infos = []\n    infos = []\n    for lis_list, key_scps_list in [\n        (input_infos, args.input_scps),\n        (output_infos, args.output_scps),\n        (infos, args.scps),\n    ]:\n        for key_scps in key_scps_list:\n            lis = []\n            for key_scp in key_scps:\n                sps = key_scp.split("":"")\n                if len(sps) == 2:\n                    key, scp = sps\n                    type_func = None\n                    type_func_str = ""none""\n                elif len(sps) == 3:\n                    key, scp, type_func_str = sps\n                    fail = False\n\n                    try:\n                        # type_func: Callable[[str], Any]\n                        # e.g. type_func_str = ""int"" -> type_func = int\n                        type_func = eval(type_func_str)\n                    except Exception:\n                        raise RuntimeError(""Unknown type: {}"".format(type_func_str))\n\n                    if not callable(type_func):\n                        raise RuntimeError(""Unknown type: {}"".format(type_func_str))\n\n                else:\n                    raise RuntimeError(\n                        ""Format <key>:<filepath> ""\n                        ""or <key>:<filepath>:<type>  ""\n                        ""e.g. feat:data/feat.scp ""\n                        ""or shape:data/feat.scp:shape: {}"".format(key_scp)\n                    )\n\n                for item in lis:\n                    if key == item[0]:\n                        raise RuntimeError(\n                            \'The key ""{}"" is duplicated: {} {}\'.format(\n                                key, item[3], key_scp\n                            )\n                        )\n\n                lis.append((key, scp, type_func, key_scp, type_func_str))\n            lis_list.append(lis)\n\n    # Open  scp files\n    input_fscps = [\n        [open(i[1], ""r"", encoding=""utf-8"") for i in il] for il in input_infos\n    ]\n    output_fscps = [\n        [open(i[1], ""r"", encoding=""utf-8"") for i in il] for il in output_infos\n    ]\n    fscps = [[open(i[1], ""r"", encoding=""utf-8"") for i in il] for il in infos]\n\n    # Note(kamo): What is done here?\n    # The final goal is creating a JSON file such as.\n    # {\n    #     ""utts"": {\n    #         ""sample_id1"": {(omitted)},\n    #         ""sample_id2"": {(omitted)},\n    #          ....\n    #     }\n    # }\n    #\n    # To reduce memory usage, reading the input text files for each lines\n    # and writing JSON elements per samples.\n    if args.out is None:\n        out = sys.stdout\n    else:\n        out = open(args.out, ""w"", encoding=""utf-8"")\n    out.write(\'{\\n    ""utts"": {\\n\')\n    nutt = 0\n    while True:\n        nutt += 1\n        # List[List[str]]\n        input_lines = [[f.readline() for f in fl] for fl in input_fscps]\n        output_lines = [[f.readline() for f in fl] for fl in output_fscps]\n        lines = [[f.readline() for f in fl] for fl in fscps]\n\n        # Get the first line\n        concat = sum(input_lines + output_lines + lines, [])\n        if len(concat) == 0:\n            break\n        first = concat[0]\n\n        # Sanity check: Must be sorted by the first column and have same keys\n        count = 0\n        for ls_list in (input_lines, output_lines, lines):\n            for ls in ls_list:\n                for line in ls:\n                    if line == """" or first == """":\n                        if line != first:\n                            concat = sum(input_infos + output_infos + infos, [])\n                            raise RuntimeError(\n                                ""The number of lines mismatch ""\n                                \'between: ""{}"" and ""{}""\'.format(\n                                    concat[0][1], concat[count][1]\n                                )\n                            )\n\n                    elif line.split()[0] != first.split()[0]:\n                        concat = sum(input_infos + output_infos + infos, [])\n                        raise RuntimeError(\n                            ""The keys are mismatch at {}th line ""\n                            \'between ""{}"" and ""{}"":\\n>>> {}\\n>>> {}\'.format(\n                                nutt,\n                                concat[0][1],\n                                concat[count][1],\n                                first.rstrip(),\n                                line.rstrip(),\n                            )\n                        )\n                    count += 1\n\n        # The end of file\n        if first == """":\n            if nutt != 1:\n                out.write(""\\n"")\n            break\n        if nutt != 1:\n            out.write("",\\n"")\n\n        entry = {}\n        for inout, _lines, _infos in [\n            (""input"", input_lines, input_infos),\n            (""output"", output_lines, output_infos),\n            (""other"", lines, infos),\n        ]:\n\n            lis = []\n            for idx, (line_list, info_list) in enumerate(zip(_lines, _infos), 1):\n                if inout == ""input"":\n                    d = {""name"": ""input{}"".format(idx)}\n                elif inout == ""output"":\n                    d = {""name"": ""target{}"".format(idx)}\n                else:\n                    d = {}\n\n                # info_list: List[Tuple[str, str, Callable]]\n                # line_list: List[str]\n                for line, info in zip(line_list, info_list):\n                    sps = line.split(None, 1)\n                    if len(sps) < 2:\n                        if not args.allow_one_column:\n                            raise RuntimeError(\n                                ""Format error {}th line in {}: ""\n                                \' Expecting ""<key> <value>"":\\n>>> {}\'.format(\n                                    nutt, info[1], line\n                                )\n                            )\n                        uttid = sps[0]\n                        value = """"\n                    else:\n                        uttid, value = sps\n\n                    key = info[0]\n                    type_func = info[2]\n                    value = value.rstrip()\n\n                    if type_func is not None:\n                        try:\n                            # type_func: Callable[[str], Any]\n                            value = type_func(value)\n                        except Exception:\n                            logging.error(\n                                \'""{}"" is an invalid function \'\n                                ""for the {} th line in {}: \\n>>> {}"".format(\n                                    info[4], nutt, info[1], line\n                                )\n                            )\n                            raise\n\n                    d[key] = value\n                lis.append(d)\n\n            if inout != ""other"":\n                entry[inout] = lis\n            else:\n                # If key == \'other\'. only has the first item\n                entry.update(lis[0])\n\n        entry = json.dumps(\n            entry, indent=4, ensure_ascii=False, sort_keys=True, separators=("","", "": "")\n        )\n        # Add indent\n        indent = ""    "" * 2\n        entry = (""\\n"" + indent).join(entry.split(""\\n""))\n\n        uttid = first.split()[0]\n        out.write(\'        ""{}"": {}\'.format(uttid, entry))\n\n    out.write(""    }\\n}\\n"")\n\n    logging.info(""{} entries in {}"".format(nutt, out.name))\n'"
utils/mergejson.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport os\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""merge json files"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--input-jsons"",\n        type=str,\n        nargs=""+"",\n        action=""append"",\n        default=[],\n        help=""Json files for the inputs"",\n    )\n    parser.add_argument(\n        ""--output-jsons"",\n        type=str,\n        nargs=""+"",\n        action=""append"",\n        default=[],\n        help=""Json files for the outputs"",\n    )\n    parser.add_argument(\n        ""--jsons"",\n        type=str,\n        nargs=""+"",\n        action=""append"",\n        default=[],\n        help=""The json files except for the input and outputs"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(""-O"", dest=""output"", type=str, help=""Output json file"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    js_dict = {}  # Dict[str, List[List[Dict[str, Dict[str, dict]]]]]\n    # make intersection set for utterance keys\n    intersec_ks = None  # Set[str]\n    for jtype, jsons_list in [\n        (""input"", args.input_jsons),\n        (""output"", args.output_jsons),\n        (""other"", args.jsons),\n    ]:\n        js_dict[jtype] = []\n        for jsons in jsons_list:\n            js = []\n            for x in jsons:\n                if os.path.isfile(x):\n                    with codecs.open(x, encoding=""utf-8"") as f:\n                        j = json.load(f)\n                    ks = list(j[""utts""].keys())\n                    logging.info(x + "": has "" + str(len(ks)) + "" utterances"")\n                    if intersec_ks is not None:\n                        intersec_ks = intersec_ks.intersection(set(ks))\n                        if len(intersec_ks) == 0:\n                            logging.warning(""No intersection"")\n                            break\n                    else:\n                        intersec_ks = set(ks)\n                    js.append(j)\n            js_dict[jtype].append(js)\n    logging.info(""new json has "" + str(len(intersec_ks)) + "" utterances"")\n\n    new_dic = {}\n    for k in intersec_ks:\n        new_dic[k] = {""input"": [], ""output"": []}\n        for jtype in [""input"", ""output"", ""other""]:\n            for idx, js in enumerate(js_dict[jtype], 1):\n                # Merge dicts from jsons into a dict\n                dic = {k2: v for j in js for k2, v in j[""utts""][k].items()}\n\n                if jtype == ""other"":\n                    new_dic[k].update(dic)\n                else:\n                    _dic = {}\n\n                    # FIXME(kamo): ad-hoc way to change str to List[int]\n                    if jtype == ""input"":\n                        _dic[""name""] = ""input{}"".format(idx)\n                        if ""ilen"" in dic and ""idim"" in dic:\n                            _dic[""shape""] = (int(dic[""ilen""]), int(dic[""idim""]))\n                        elif ""ilen"" in dic:\n                            _dic[""shape""] = (int(dic[""ilen""]),)\n                        elif ""idim"" in dic:\n                            _dic[""shape""] = (int(dic[""idim""]),)\n\n                    elif jtype == ""output"":\n                        _dic[""name""] = ""target{}"".format(idx)\n                        if ""olen"" in dic and ""odim"" in dic:\n                            _dic[""shape""] = (int(dic[""olen""]), int(dic[""odim""]))\n                        elif ""ilen"" in dic:\n                            _dic[""shape""] = (int(dic[""olen""]),)\n                        elif ""idim"" in dic:\n                            _dic[""shape""] = (int(dic[""odim""]),)\n                    if ""shape"" in dic:\n                        # shape: ""80,1000"" -> [80, 1000]\n                        _dic[""shape""] = list(map(int, dic[""shape""].split("","")))\n\n                    for k2, v in dic.items():\n                        if k2 not in [""ilen"", ""idim"", ""olen"", ""odim"", ""shape""]:\n                            _dic[k2] = v\n                    new_dic[k][jtype].append(_dic)\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    if args.output is not None:\n        sys.stdout = codecs.open(args.output, ""w"", encoding=""utf-8"")\n    else:\n        sys.stdout = codecs.getwriter(""utf-8"")(\n            sys.stdout if is_python2 else sys.stdout.buffer\n        )\n    print(\n        json.dumps(\n            {""utts"": new_dic},\n            indent=4,\n            ensure_ascii=False,\n            sort_keys=True,\n            separators=("","", "": ""),\n        )\n    )\n'"
utils/mix-mono-wav-scp.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport io\nimport sys\n\nPY2 = sys.version_info[0] == 2\n\nif PY2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        description=""Mixing wav.scp files into a multi-channel wav.scp "" ""using sox."",\n    )\n    parser.add_argument(""scp"", type=str, nargs=""+"", help=""Give wav.scp"")\n    parser.add_argument(\n        ""out"",\n        nargs=""?"",\n        type=argparse.FileType(""w""),\n        default=sys.stdout,\n        help=""The output filename. "" ""If omitted, then output to sys.stdout"",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    fscps = [io.open(scp, ""r"", encoding=""utf-8"") for scp in args.scp]\n    for linenum, lines in enumerate(zip_longest(*fscps)):\n        keys = []\n        wavs = []\n\n        for line, scp in zip(lines, args.scp):\n            if line is None:\n                raise RuntimeError(""Numbers of line mismatch"")\n\n            sps = line.split("" "", 1)\n            if len(sps) != 2:\n                raise RuntimeError(\n                    \'Invalid line is found: {}, line {}: ""{}"" \'.format(\n                        scp, linenum, line\n                    )\n                )\n            key, wav = sps\n            keys.append(key)\n            wavs.append(wav.strip())\n\n        if not all(k == keys[0] for k in keys):\n            raise RuntimeError(\n                ""The ids mismatch. Hint; the input files must be ""\n                ""sorted and must have same ids: {}"".format(keys)\n            )\n\n        args.out.write(\n            ""{} sox -M {} -c {} -t wav - |\\n"".format(\n                keys[0], "" "".join(""{}"".format(w) for w in wavs), len(fscps)\n            )\n        )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/result2json.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#           2018 Xuankai Chang (Shanghai Jiao Tong University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport re\nimport sys\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert sclite\'s result.txt file to json""\n    )\n    parser.add_argument(""--key"", ""-k"", type=str, help=""key"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_parser()\n    args = parser.parse_args()\n\n    key = re.findall(r""r\\d+h\\d+"", args.key)[0]\n\n    re_id = r""^id: ""\n    re_strings = {\n        ""Speaker"": r""^Speaker sentences"",\n        ""Scores"": r""^Scores: "",\n        ""REF"": r""^REF: "",\n        ""HYP"": r""^HYP: "",\n    }\n    re_id = re.compile(re_id)\n    re_patterns = {}\n    for p in re_strings.keys():\n        re_patterns[p] = re.compile(re_strings[p])\n\n    ret = {}\n    tmp_id = None\n    tmp_ret = {}\n\n    sys.stdin = codecs.getreader(""utf-8"")(sys.stdin if is_python2 else sys.stdin.buffer)\n    sys.stdout = codecs.getwriter(""utf-8"")(\n        sys.stdout if is_python2 else sys.stdout.buffer\n    )\n    line = sys.stdin.readline()\n    while line:\n        x = line.rstrip()\n        x_split = x.split()\n\n        if re_id.match(x):\n            if tmp_id:\n                ret[tmp_id] = {key: tmp_ret}\n                tmp_ret = {}\n            tmp_id = x_split[1]\n        for p in re_patterns.keys():\n            if re_patterns[p].match(x):\n                tmp_ret[p] = "" "".join(x_split[1:])\n        line = sys.stdin.readline()\n\n    if tmp_ret != {}:\n        ret[tmp_id] = {key: tmp_ret}\n\n    all_l = {""utts"": ret}\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(\n        all_l, indent=4, ensure_ascii=False, sort_keys=True, separators=("","", "": "")\n    )\n    print(jsonstring)\n'"
utils/scp2json.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport sys\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert scp to json"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--key"", ""-k"", type=str, help=""key"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_parser()\n    args = parser.parse_args()\n\n    new_line = {}\n    sys.stdin = codecs.getreader(""utf-8"")(sys.stdin if is_python2 else sys.stdin.buffer)\n    sys.stdout = codecs.getwriter(""utf-8"")(\n        sys.stdout if is_python2 else sys.stdout.buffer\n    )\n    line = sys.stdin.readline()\n    while line:\n        x = line.rstrip().split()\n        v = {args.key: "" "".join(x[1:])}\n        new_line[x[0]] = v\n        line = sys.stdin.readline()\n\n    all_l = {""utts"": new_line}\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(\n        all_l, indent=4, ensure_ascii=False, sort_keys=True, separators=("","", "": "")\n    )\n    print(jsonstring)\n'"
utils/splitjson.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport os\nimport sys\n\nimport numpy as np\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""split a json file for parallel processing"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""json"", type=str, help=""json file"")\n    parser.add_argument(\n        ""--parts"", ""-p"", type=int, help=""Number of subparts to be prepared"", default=0\n    )\n    return parser\n\n\nif __name__ == ""__main__"":\n    args = get_parser().parse_args()\n\n    # logging info\n    logging.basicConfig(\n        level=logging.INFO,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n    logging.info(get_commandline_args())\n\n    # check directory\n    filename = os.path.basename(args.json).split(""."")[0]\n    dirname = os.path.dirname(args.json)\n    dirname = ""{}/split{}utt"".format(dirname, args.parts)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    # load json and split keys\n    j = json.load(codecs.open(args.json, ""r"", encoding=""utf-8""))\n    utt_ids = sorted(list(j[""utts""].keys()))\n    logging.info(""number of utterances = %d"" % len(utt_ids))\n    if len(utt_ids) < args.parts:\n        logging.error(""#utterances < #splits. Use smaller split number."")\n        sys.exit(1)\n    utt_id_lists = np.array_split(utt_ids, args.parts)\n    utt_id_lists = [utt_id_list.tolist() for utt_id_list in utt_id_lists]\n\n    for i, utt_id_list in enumerate(utt_id_lists):\n        new_dic = dict()\n        for utt_id in utt_id_list:\n            new_dic[utt_id] = j[""utts""][utt_id]\n        jsonstring = json.dumps(\n            {""utts"": new_dic},\n            indent=4,\n            ensure_ascii=False,\n            sort_keys=True,\n            separators=("","", "": ""),\n        )\n        fl = ""{}/{}.{}.json"".format(dirname, filename, i + 1)\n        sys.stdout = codecs.open(fl, ""w+"", encoding=""utf-8"")\n        print(jsonstring)\n        sys.stdout.close()\n'"
utils/text2token.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport re\nimport sys\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef exist_or_not(i, match_pos):\n    start_pos = None\n    end_pos = None\n    for pos in match_pos:\n        if pos[0] <= i < pos[1]:\n            start_pos = pos[0]\n            end_pos = pos[1]\n            break\n\n    return start_pos, end_pos\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert raw text to tokenized text"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--nchar"",\n        ""-n"",\n        default=1,\n        type=int,\n        help=""number of characters to split, i.e., \\\n                        aabb -> a a b b with -n 1 and aa bb with -n 2"",\n    )\n    parser.add_argument(\n        ""--skip-ncols"", ""-s"", default=0, type=int, help=""skip first n columns""\n    )\n    parser.add_argument(""--space"", default=""<space>"", type=str, help=""space symbol"")\n    parser.add_argument(\n        ""--non-lang-syms"",\n        ""-l"",\n        default=None,\n        type=str,\n        help=""list of non-linguistic symobles, e.g., <NOISE> etc."",\n    )\n    parser.add_argument(""text"", type=str, default=False, nargs=""?"", help=""input text"")\n    parser.add_argument(\n        ""--trans_type"",\n        ""-t"",\n        type=str,\n        default=""char"",\n        choices=[""char"", ""phn""],\n        help=""""""Transcript type. char/phn. e.g., for TIMIT FADG0_SI1279 -\n                        If trans_type is char,\n                        read from SI1279.WRD file -> ""bricks are an alternative""\n                        Else if trans_type is phn,\n                        read from SI1279.PHN file -> ""sil b r ih sil k s aa r er n aa l\n                        sil t er n ih sil t ih v sil"" """""",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    rs = []\n    if args.non_lang_syms is not None:\n        with codecs.open(args.non_lang_syms, ""r"", encoding=""utf-8"") as f:\n            nls = [x.rstrip() for x in f.readlines()]\n            rs = [re.compile(re.escape(x)) for x in nls]\n\n    if args.text:\n        f = codecs.open(args.text, encoding=""utf-8"")\n    else:\n        f = codecs.getreader(""utf-8"")(sys.stdin if is_python2 else sys.stdin.buffer)\n\n    sys.stdout = codecs.getwriter(""utf-8"")(\n        sys.stdout if is_python2 else sys.stdout.buffer\n    )\n    line = f.readline()\n    n = args.nchar\n    while line:\n        x = line.split()\n        print("" "".join(x[: args.skip_ncols]), end="" "")\n        a = "" "".join(x[args.skip_ncols :])\n\n        # get all matched positions\n        match_pos = []\n        for r in rs:\n            i = 0\n            while i >= 0:\n                m = r.search(a, i)\n                if m:\n                    match_pos.append([m.start(), m.end()])\n                    i = m.end()\n                else:\n                    break\n\n        if args.trans_type == ""phn"":\n            a = a.split("" "")\n        else:\n            if len(match_pos) > 0:\n                chars = []\n                i = 0\n                while i < len(a):\n                    start_pos, end_pos = exist_or_not(i, match_pos)\n                    if start_pos is not None:\n                        chars.append(a[start_pos:end_pos])\n                        i = end_pos\n                    else:\n                        chars.append(a[i])\n                        i += 1\n                a = chars\n\n            a = [a[j : j + n] for j in range(0, len(a), n)]\n\n        a_flat = []\n        for z in a:\n            a_flat.append("""".join(z))\n\n        a_chars = [z.replace("" "", args.space) for z in a_flat]\n        if args.trans_type == ""phn"":\n            a_chars = [z.replace(""sil"", args.space) for z in a_chars]\n        print("" "".join(a_chars))\n        line = f.readline()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/text2vocabulary.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Mitsubishi Electric Research Laboratories (Takaaki Hori)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport logging\nimport six\nimport sys\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""create a vocabulary file from text files"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--output"", ""-o"", default="""", type=str, help=""output a vocabulary file""\n    )\n    parser.add_argument(""--cutoff"", ""-c"", default=0, type=int, help=""cut-off frequency"")\n    parser.add_argument(\n        ""--vocabsize"", ""-s"", default=20000, type=int, help=""vocabulary size""\n    )\n    parser.add_argument(""text_files"", nargs=""*"", help=""input text files"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # count the word occurrences\n    counts = {}\n    exclude = [""<sos>"", ""<eos>"", ""<unk>""]\n    if len(args.text_files) == 0:\n        args.text_files.append(""-"")\n    for fn in args.text_files:\n        fd = (\n            codecs.open(fn, ""r"", encoding=""utf-8"")\n            if fn != ""-""\n            else codecs.getreader(""utf-8"")(\n                sys.stdin if is_python2 else sys.stdin.buffer\n            )\n        )\n        for ln in fd.readlines():\n            for tok in ln.split():\n                if tok not in exclude:\n                    if tok not in counts:\n                        counts[tok] = 1\n                    else:\n                        counts[tok] += 1\n        if fn != ""-"":\n            fd.close()\n\n    # limit the vocabulary size\n    total_count = sum(counts.values())\n    invocab_count = 0\n    vocabulary = []\n    for w, c in sorted(counts.items(), key=lambda x: -x[1]):\n        if c <= args.cutoff:\n            break\n        if len(vocabulary) >= args.vocabsize:\n            break\n        vocabulary.append(w)\n        invocab_count += c\n\n    logging.warning(\n        ""OOV rate = %.2f %%"" % (float(total_count - invocab_count) / total_count * 100)\n    )\n    # write the vocabulary\n    fd = (\n        codecs.open(args.output, ""w"", encoding=""utf-8"")\n        if args.output\n        else codecs.getwriter(""utf-8"")(sys.stdout if is_python2 else sys.stdout.buffer)\n    )\n    six.print_(""<unk> 1"", file=fd)\n    for n, w in enumerate(sorted(vocabulary)):\n        six.print_(""%s %d"" % (w, n + 2), file=fd)\n    if args.output:\n        fd.close()\n'"
utils/trim_silence.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport argparse\nimport codecs\nimport logging\nimport os\n\nimport kaldiio\nimport librosa\nimport matplotlib.pyplot as plt\nimport numpy\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef _time_to_str(time_idx):\n    time_idx = time_idx * 10 ** 4\n    return ""%06d"" % time_idx\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""Trim slience with simple power thresholding ""\n        ""and make segments file."",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--fs"", type=int, help=""Sampling frequency"")\n    parser.add_argument(\n        ""--threshold"", type=float, default=60, help=""Threshold in decibels""\n    )\n    parser.add_argument(\n        ""--win_length"", type=int, default=1024, help=""Analisys window length in point""\n    )\n    parser.add_argument(\n        ""--shift_length"", type=int, default=256, help=""Shift length in point""\n    )\n    parser.add_argument(\n        ""--min_silence"", type=float, default=0.01, help=""minimum silence length""\n    )\n    parser.add_argument(\n        ""--figdir"", type=str, default=""figs"", help=""Directory to save figures""\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--normalize"",\n        choices=[1, 16, 24, 32],\n        type=int,\n        default=None,\n        help=""Give the bit depth of the PCM, ""\n        ""then normalizes data to scale in [-1,1]"",\n    )\n    parser.add_argument(""rspecifier"", type=str, help=""WAV scp file"")\n    parser.add_argument(""wspecifier"", type=str, help=""Segments file"")\n\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # set logger\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    if not os.path.exists(args.figdir):\n        os.makedirs(args.figdir)\n\n    with kaldiio.ReadHelper(args.rspecifier) as reader, codecs.open(\n        args.wspecifier, ""w"", encoding=""utf-8""\n    ) as f:\n        for utt_id, (rate, array) in reader:\n            assert rate == args.fs\n            array = array.astype(numpy.float32)\n            if args.normalize is not None and args.normalize != 1:\n                array = array / (1 << (args.normalize - 1))\n            array_trim, idx = librosa.effects.trim(\n                y=array,\n                top_db=args.threshold,\n                frame_length=args.win_length,\n                hop_length=args.shift_length,\n            )\n            start, end = idx / args.fs\n\n            # save figure\n            plt.subplot(2, 1, 1)\n            plt.plot(array)\n            plt.title(""Original"")\n            plt.subplot(2, 1, 2)\n            plt.plot(array_trim)\n            plt.title(""Trim"")\n            plt.tight_layout()\n            plt.savefig(args.figdir + ""/"" + utt_id + "".png"")\n            plt.close()\n\n            # added minimum silence part\n            start = max(0.0, start - args.min_silence)\n            end = min(len(array) / args.fs, end + args.min_silence)\n\n            # write to segments file\n            segment = ""%s %s %f %f\\n"" % (utt_id, utt_id, start, end)\n            f.write(segment)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
utils/trn2ctm.py,0,"b'#!/usr/bin/python\n\nimport argparse\nimport codecs\nimport math\nimport re\nimport sys\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=""convert trn to ctm"")\n    parser.add_argument(""trn"", type=str, default=None, nargs=""?"", help=""input trn"")\n    parser.add_argument(""ctm"", type=str, default=None, nargs=""?"", help=""output ctm"")\n    return parser\n\n\ndef main(args):\n    args = get_parser().parse_args(args)\n    convert(args.trn, args.ctm)\n\n\ndef convert(trn=None, ctm=None):\n    if trn is not None:\n        with codecs.open(trn, ""r"", encoding=""utf-8"") as trn:\n            content = trn.readlines()\n    else:\n        trn = codecs.getreader(""utf-8"")(sys.stdin if is_python2 else sys.stdin.buffer)\n        content = trn.readlines()\n    split_content = []\n    for i, line in enumerate(content):\n        idx = line.rindex(""("")\n        split = [line[:idx].strip().upper(), line[idx + 1 :].strip()[:-1]]\n        while ""(("" in split[0]:\n            split[0] = split[0].replace(""(("", ""("")\n        while ""  "" in split[0]:\n            split[0] = split[0].replace(""  "", "" "")\n        segm_info = re.split(""[-_]"", split[1])\n        segm_info = [s.strip() for s in segm_info]\n        col1 = segm_info[0] + ""_"" + segm_info[1]\n        col2 = segm_info[2]\n        start_time_int = int(segm_info[6])\n        end_time_int = int(segm_info[7])\n        diff_int = end_time_int - start_time_int\n        word_split = split[0].split("" "")\n        word_split = list(\n            filter(lambda x: len(x) > 0 and any([c != "" "" for c in x]), word_split)\n        )\n        if len(word_split) > 0:\n            step_int = int(math.floor(float(diff_int) / len(word_split)))\n            step = str(step_int)\n            for j, word in enumerate(word_split):\n                start_time = str(int(start_time_int + step_int * j))\n                col3 = (\n                    (start_time[:-2] if len(start_time) > 2 else ""0"")\n                    + "".""\n                    + (start_time[-2:] if len(start_time) > 1 else ""00"")\n                )\n                if j == len(word_split) - 1:\n                    diff = str(int(end_time_int - int(start_time)))\n                else:\n                    diff = step\n                col4 = (diff[:-2] if len(diff) > 2 else ""0"") + ""."" + diff[-2:]\n                segm_info = [col1, col2, col3, col4]\n                split_content.append("" "".join(segm_info) + ""  "" + word)\n    if ctm is not None:\n        sys.stdout = codecs.open(ctm, ""w"", encoding=""utf-8"")\n    else:\n        sys.stdout = codecs.getwriter(""utf-8"")(\n            sys.stdout if is_python2 else sys.stdout.buffer\n        )\n    for c_line in split_content:\n        print(c_line)\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
utils/trn2stm.py,0,"b'#!/usr/bin/python\n\nimport argparse\nimport codecs\nimport re\nimport sys\n\nis_python2 = sys.version_info[0] == 2\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(description=""convert trn to stm"")\n    parser.add_argument(\n        ""--orig-stm"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""Original stm file to add additional information to the generated one"",\n    )\n    parser.add_argument(""trn"", type=str, default=None, nargs=""?"", help=""input trn"")\n    parser.add_argument(""stm"", type=str, default=None, nargs=""?"", help=""output stm"")\n    return parser\n\n\ndef main(args):\n    args = get_parser().parse_args(args)\n    convert(args.trn, args.stm, args.orig_stm)\n\n\ndef convert(trn=None, stm=None, orig_stm=None):\n    if orig_stm is not None:\n        with codecs.open(orig_stm, ""r"", encoding=""utf-8"") as orig_stm:\n            orig_content = orig_stm.readlines()\n            has_orig = True\n            header = []\n            content = []\n            for line in orig_content:\n                (header if line.startswith("";;"") else content).append(line.strip())\n            del orig_content\n            content = [x.split("" "") for x in content]\n            mapping = {}\n            for x in content:\n                mapping[x[2]] = x[5]\n            del content\n    else:\n        has_orig = False\n        header = None\n        mapping = None\n\n    if trn is not None:\n        with codecs.open(trn, ""r"", encoding=""utf-8"") as trn:\n            content = trn.readlines()\n    else:\n        trn = codecs.getreader(""utf-8"")(sys.stdin if is_python2 else sys.stdin.buffer)\n        content = trn.readlines()\n\n    for i, line in enumerate(content):\n        idx = line.rindex(""("")\n        split = [line[:idx].strip().upper() + "" "", line[idx + 1 :].strip()[:-1]]\n        while ""(("" in split[0]:\n            split[0] = split[0].replace(""(("", ""("")\n        while ""  "" in split[0]:\n            split[0] = split[0].replace(""  "", "" "")\n        segm_info = re.split(""[-_]"", split[1])\n        segm_info = [s.strip() for s in segm_info]\n        col1 = segm_info[0] + ""_"" + segm_info[1]\n        col2 = segm_info[2]\n        col3 = segm_info[3] + ""_"" + segm_info[4] + ""_"" + segm_info[5]\n        start_time = str(int(segm_info[6]))\n        end_time = str(int(segm_info[7]))\n        col4 = (\n            (start_time[:-2] if len(start_time) > 2 else ""0"")\n            + "".""\n            + (start_time[-2:] if len(start_time) > 1 else ""00"")\n        )\n        col5 = (\n            (end_time[:-2] if len(end_time) > 2 else ""0"")\n            + "".""\n            + (end_time[-2:] if len(end_time) > 1 else ""00"")\n        )\n        col6 = mapping[col3] if has_orig else """"\n        segm_info = [col1, col2, col3, col4, col5, col6]\n        content[i] = "" "".join(segm_info) + ""  "" + split[0]\n    if stm is not None:\n        sys.stdout = codecs.open(stm, ""w"", encoding=""utf-8"")\n    else:\n        sys.stdout = codecs.getwriter(""utf-8"")(\n            sys.stdout if is_python2 else sys.stdout.buffer\n        )\n    if has_orig:\n        for h_line in header:\n            print(h_line)\n    for c_line in content:\n        print(c_line)\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/asr/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/asr/asr_mix_utils.py,1,"b'#!/usr/bin/env python3\n\n""""""\nThis script is used to provide utility functions designed for multi-speaker ASR.\n\nCopyright 2017 Johns Hopkins University (Shinji Watanabe)\n Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nMost functions can be directly used as in asr_utils.py:\n    CompareValueTrigger, restore_snapshot, adadelta_eps_decay, chainer_load,\n    torch_snapshot, torch_save, torch_resume, AttributeDict, get_model_conf.\n\n""""""\n\nimport copy\nimport logging\nimport os\n\nfrom chainer.training import extension\n\nimport matplotlib\n\nfrom espnet.asr.asr_utils import parse_hypothesis\n\n\nmatplotlib.use(""Agg"")\n\n\n# * -------------------- chainer extension related -------------------- *\nclass PlotAttentionReport(extension.Extension):\n    """"""Plot attention reporter.\n\n    Args:\n        att_vis_fn (espnet.nets.*_backend.e2e_asr.calculate_all_attentions):\n            Function of attention visualization.\n        data (list[tuple(str, dict[str, dict[str, Any]])]): List json utt key items.\n        outdir (str): Directory to save figures.\n        converter (espnet.asr.*_backend.asr.CustomConverter):\n            CustomConverter object. Function to convert data.\n        device (torch.device): The destination device to send tensor.\n        reverse (bool): If True, input and output length are reversed.\n\n    """"""\n\n    def __init__(self, att_vis_fn, data, outdir, converter, device, reverse=False):\n        """"""Initialize PlotAttentionReport.""""""\n        self.att_vis_fn = att_vis_fn\n        self.data = copy.deepcopy(data)\n        self.outdir = outdir\n        self.converter = converter\n        self.device = device\n        self.reverse = reverse\n        if not os.path.exists(self.outdir):\n            os.makedirs(self.outdir)\n\n    def __call__(self, trainer):\n        """"""Plot and save imaged matrix of att_ws.""""""\n        att_ws_sd = self.get_attention_weights()\n        for ns, att_ws in enumerate(att_ws_sd):\n            for idx, att_w in enumerate(att_ws):\n                filename = ""%s/%s.ep.{.updater.epoch}.output%d.png"" % (\n                    self.outdir,\n                    self.data[idx][0],\n                    ns + 1,\n                )\n                att_w = self.get_attention_weight(idx, att_w, ns)\n                self._plot_and_save_attention(att_w, filename.format(trainer))\n\n    def log_attentions(self, logger, step):\n        """"""Add image files of attention matrix to tensorboard.""""""\n        att_ws_sd = self.get_attention_weights()\n        for ns, att_ws in enumerate(att_ws_sd):\n            for idx, att_w in enumerate(att_ws):\n                att_w = self.get_attention_weight(idx, att_w, ns)\n                plot = self.draw_attention_plot(att_w)\n                logger.add_figure(""%s"" % (self.data[idx][0]), plot.gcf(), step)\n                plot.clf()\n\n    def get_attention_weights(self):\n        """"""Return attention weights.\n\n        Returns:\n            arr_ws_sd (numpy.ndarray): attention weights. It\'s shape would be\n                differ from bachend.dtype=float\n                * pytorch-> 1) multi-head case => (B, H, Lmax, Tmax). 2)\n                  other case => (B, Lmax, Tmax).\n                * chainer-> attention weights (B, Lmax, Tmax).\n\n        """"""\n        batch = self.converter([self.converter.transform(self.data)], self.device)\n        att_ws_sd = self.att_vis_fn(*batch)\n        return att_ws_sd\n\n    def get_attention_weight(self, idx, att_w, spkr_idx):\n        """"""Transform attention weight in regard to self.reverse.""""""\n        if self.reverse:\n            dec_len = int(self.data[idx][1][""input""][0][""shape""][0])\n            enc_len = int(self.data[idx][1][""output""][spkr_idx][""shape""][0])\n        else:\n            dec_len = int(self.data[idx][1][""output""][spkr_idx][""shape""][0])\n            enc_len = int(self.data[idx][1][""input""][0][""shape""][0])\n        if len(att_w.shape) == 3:\n            att_w = att_w[:, :dec_len, :enc_len]\n        else:\n            att_w = att_w[:dec_len, :enc_len]\n        return att_w\n\n    def draw_attention_plot(self, att_w):\n        """"""Visualize attention weights matrix.\n\n        Args:\n            att_w(Tensor): Attention weight matrix.\n\n        Returns:\n            matplotlib.pyplot: pyplot object with attention matrix image.\n\n        """"""\n        import matplotlib.pyplot as plt\n\n        if len(att_w.shape) == 3:\n            for h, aw in enumerate(att_w, 1):\n                plt.subplot(1, len(att_w), h)\n                plt.imshow(aw, aspect=""auto"")\n                plt.xlabel(""Encoder Index"")\n                plt.ylabel(""Decoder Index"")\n        else:\n            plt.imshow(att_w, aspect=""auto"")\n            plt.xlabel(""Encoder Index"")\n            plt.ylabel(""Decoder Index"")\n        plt.tight_layout()\n        return plt\n\n    def _plot_and_save_attention(self, att_w, filename):\n        plt = self.draw_attention_plot(att_w)\n        plt.savefig(filename)\n        plt.close()\n\n\ndef add_results_to_json(js, nbest_hyps_sd, char_list):\n    """"""Add N-best results to json.\n\n    Args:\n        js (dict[str, Any]): Groundtruth utterance dict.\n        nbest_hyps_sd (list[dict[str, Any]]):\n            List of hypothesis for multi_speakers (# Utts x # Spkrs).\n        char_list (list[str]): List of characters.\n\n    Returns:\n        dict[str, Any]: N-best results added utterance dict.\n\n    """"""\n    # copy old json info\n    new_js = dict()\n    new_js[""utt2spk""] = js[""utt2spk""]\n    num_spkrs = len(nbest_hyps_sd)\n    new_js[""output""] = []\n\n    for ns in range(num_spkrs):\n        tmp_js = []\n        nbest_hyps = nbest_hyps_sd[ns]\n\n        for n, hyp in enumerate(nbest_hyps, 1):\n            # parse hypothesis\n            rec_text, rec_token, rec_tokenid, score = parse_hypothesis(hyp, char_list)\n\n            # copy ground-truth\n            out_dic = dict(js[""output""][ns].items())\n\n            # update name\n            out_dic[""name""] += ""[%d]"" % n\n\n            # add recognition results\n            out_dic[""rec_text""] = rec_text\n            out_dic[""rec_token""] = rec_token\n            out_dic[""rec_tokenid""] = rec_tokenid\n            out_dic[""score""] = score\n\n            # add to list of N-best result dicts\n            tmp_js.append(out_dic)\n\n            # show 1-best result\n            if n == 1:\n                logging.info(""groundtruth: %s"" % out_dic[""text""])\n                logging.info(""prediction : %s"" % out_dic[""rec_text""])\n\n        new_js[""output""].append(tmp_js)\n    return new_js\n'"
espnet/asr/asr_utils.py,13,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport copy\nimport json\nimport logging\n\n# matplotlib related\nimport os\nimport shutil\nimport tempfile\n\n# chainer related\nimport chainer\n\nfrom chainer import training\nfrom chainer.training import extension\n\nfrom chainer.serializers.npz import DictionarySerializer\nfrom chainer.serializers.npz import NpzDeserializer\n\n# io related\nimport matplotlib\nimport numpy as np\nimport torch\n\nmatplotlib.use(""Agg"")\n\n\n# * -------------------- training iterator related -------------------- *\n\n\nclass CompareValueTrigger(object):\n    """"""Trigger invoked when key value getting bigger or lower than before.\n\n    Args:\n        key (str) : Key of value.\n        compare_fn ((float, float) -> bool) : Function to compare the values.\n        trigger (tuple(int, str)) : Trigger that decide the comparison interval.\n\n    """"""\n\n    def __init__(self, key, compare_fn, trigger=(1, ""epoch"")):\n        self._key = key\n        self._best_value = None\n        self._interval_trigger = training.util.get_trigger(trigger)\n        self._init_summary()\n        self._compare_fn = compare_fn\n\n    def __call__(self, trainer):\n        """"""Get value related to the key and compare with current value.""""""\n        observation = trainer.observation\n        summary = self._summary\n        key = self._key\n        if key in observation:\n            summary.add({key: observation[key]})\n\n        if not self._interval_trigger(trainer):\n            return False\n\n        stats = summary.compute_mean()\n        value = float(stats[key])  # copy to CPU\n        self._init_summary()\n\n        if self._best_value is None:\n            # initialize best value\n            self._best_value = value\n            return False\n        elif self._compare_fn(self._best_value, value):\n            return True\n        else:\n            self._best_value = value\n            return False\n\n    def _init_summary(self):\n        self._summary = chainer.reporter.DictSummary()\n\n\nclass PlotAttentionReport(extension.Extension):\n    """"""Plot attention reporter.\n\n    Args:\n        att_vis_fn (espnet.nets.*_backend.e2e_asr.E2E.calculate_all_attentions):\n            Function of attention visualization.\n        data (list[tuple(str, dict[str, list[Any]])]): List json utt key items.\n        outdir (str): Directory to save figures.\n        converter (espnet.asr.*_backend.asr.CustomConverter): Function to convert data.\n        device (int | torch.device): Device.\n        reverse (bool): If True, input and output length are reversed.\n        ikey (str): Key to access input (for ASR ikey=""input"", for MT ikey=""output"".)\n        iaxis (int): Dimension to access input (for ASR iaxis=0, for MT iaxis=1.)\n        okey (str): Key to access output (for ASR okey=""input"", MT okay=""output"".)\n        oaxis (int): Dimension to access output (for ASR oaxis=0, for MT oaxis=0.)\n\n    """"""\n\n    def __init__(\n        self,\n        att_vis_fn,\n        data,\n        outdir,\n        converter,\n        transform,\n        device,\n        reverse=False,\n        ikey=""input"",\n        iaxis=0,\n        okey=""output"",\n        oaxis=0,\n    ):\n        self.att_vis_fn = att_vis_fn\n        self.data = copy.deepcopy(data)\n        self.outdir = outdir\n        self.converter = converter\n        self.transform = transform\n        self.device = device\n        self.reverse = reverse\n        self.ikey = ikey\n        self.iaxis = iaxis\n        self.okey = okey\n        self.oaxis = oaxis\n        if not os.path.exists(self.outdir):\n            os.makedirs(self.outdir)\n\n    def __call__(self, trainer):\n        """"""Plot and save image file of att_ws matrix.""""""\n        att_ws = self.get_attention_weights()\n        if isinstance(att_ws, list):  # multi-encoder case\n            num_encs = len(att_ws) - 1\n            # atts\n            for i in range(num_encs):\n                for idx, att_w in enumerate(att_ws[i]):\n                    filename = ""%s/%s.ep.{.updater.epoch}.att%d.png"" % (\n                        self.outdir,\n                        self.data[idx][0],\n                        i + 1,\n                    )\n                    att_w = self.get_attention_weight(idx, att_w)\n                    np_filename = ""%s/%s.ep.{.updater.epoch}.att%d.npy"" % (\n                        self.outdir,\n                        self.data[idx][0],\n                        i + 1,\n                    )\n                    np.save(np_filename.format(trainer), att_w)\n                    self._plot_and_save_attention(att_w, filename.format(trainer))\n            # han\n            for idx, att_w in enumerate(att_ws[num_encs]):\n                filename = ""%s/%s.ep.{.updater.epoch}.han.png"" % (\n                    self.outdir,\n                    self.data[idx][0],\n                )\n                att_w = self.get_attention_weight(idx, att_w)\n                np_filename = ""%s/%s.ep.{.updater.epoch}.han.npy"" % (\n                    self.outdir,\n                    self.data[idx][0],\n                )\n                np.save(np_filename.format(trainer), att_w)\n                self._plot_and_save_attention(\n                    att_w, filename.format(trainer), han_mode=True\n                )\n        else:\n            for idx, att_w in enumerate(att_ws):\n                filename = ""%s/%s.ep.{.updater.epoch}.png"" % (\n                    self.outdir,\n                    self.data[idx][0],\n                )\n                att_w = self.get_attention_weight(idx, att_w)\n                np_filename = ""%s/%s.ep.{.updater.epoch}.npy"" % (\n                    self.outdir,\n                    self.data[idx][0],\n                )\n                np.save(np_filename.format(trainer), att_w)\n                self._plot_and_save_attention(att_w, filename.format(trainer))\n\n    def log_attentions(self, logger, step):\n        """"""Add image files of att_ws matrix to the tensorboard.""""""\n        att_ws = self.get_attention_weights()\n        if isinstance(att_ws, list):  # multi-encoder case\n            num_encs = len(att_ws) - 1\n            # atts\n            for i in range(num_encs):\n                for idx, att_w in enumerate(att_ws[i]):\n                    att_w = self.get_attention_weight(idx, att_w)\n                    plot = self.draw_attention_plot(att_w)\n                    logger.add_figure(\n                        ""%s_att%d"" % (self.data[idx][0], i + 1), plot.gcf(), step\n                    )\n                    plot.clf()\n            # han\n            for idx, att_w in enumerate(att_ws[num_encs]):\n                att_w = self.get_attention_weight(idx, att_w)\n                plot = self.draw_han_plot(att_w)\n                logger.add_figure(""%s_han"" % (self.data[idx][0]), plot.gcf(), step)\n                plot.clf()\n        else:\n            for idx, att_w in enumerate(att_ws):\n                att_w = self.get_attention_weight(idx, att_w)\n                plot = self.draw_attention_plot(att_w)\n                logger.add_figure(""%s"" % (self.data[idx][0]), plot.gcf(), step)\n                plot.clf()\n\n    def get_attention_weights(self):\n        """"""Return attention weights.\n\n        Returns:\n            numpy.ndarray: attention weights.float. Its shape would be\n                differ from backend.\n                * pytorch-> 1) multi-head case => (B, H, Lmax, Tmax), 2)\n                  other case => (B, Lmax, Tmax).\n                * chainer-> (B, Lmax, Tmax)\n\n        """"""\n        batch = self.converter([self.transform(self.data)], self.device)\n        if isinstance(batch, tuple):\n            att_ws = self.att_vis_fn(*batch)\n        else:\n            att_ws = self.att_vis_fn(**batch)\n        return att_ws\n\n    def get_attention_weight(self, idx, att_w):\n        """"""Transform attention matrix with regard to self.reverse.""""""\n        if self.reverse:\n            dec_len = int(self.data[idx][1][self.ikey][self.iaxis][""shape""][0])\n            enc_len = int(self.data[idx][1][self.okey][self.oaxis][""shape""][0])\n        else:\n            dec_len = int(self.data[idx][1][self.okey][self.oaxis][""shape""][0])\n            enc_len = int(self.data[idx][1][self.ikey][self.iaxis][""shape""][0])\n        if len(att_w.shape) == 3:\n            att_w = att_w[:, :dec_len, :enc_len]\n        else:\n            att_w = att_w[:dec_len, :enc_len]\n        return att_w\n\n    def draw_attention_plot(self, att_w):\n        """"""Plot the att_w matrix.\n\n        Returns:\n            matplotlib.pyplot: pyplot object with attention matrix image.\n\n        """"""\n        import matplotlib.pyplot as plt\n\n        att_w = att_w.astype(np.float32)\n        if len(att_w.shape) == 3:\n            for h, aw in enumerate(att_w, 1):\n                plt.subplot(1, len(att_w), h)\n                plt.imshow(aw, aspect=""auto"")\n                plt.xlabel(""Encoder Index"")\n                plt.ylabel(""Decoder Index"")\n        else:\n            plt.imshow(att_w, aspect=""auto"")\n            plt.xlabel(""Encoder Index"")\n            plt.ylabel(""Decoder Index"")\n        plt.tight_layout()\n        return plt\n\n    def draw_han_plot(self, att_w):\n        """"""Plot the att_w matrix for hierarchical attention.\n\n        Returns:\n            matplotlib.pyplot: pyplot object with attention matrix image.\n\n        """"""\n        import matplotlib.pyplot as plt\n\n        if len(att_w.shape) == 3:\n            for h, aw in enumerate(att_w, 1):\n                legends = []\n                plt.subplot(1, len(att_w), h)\n                for i in range(aw.shape[1]):\n                    plt.plot(aw[:, i])\n                    legends.append(""Att{}"".format(i))\n                plt.ylim([0, 1.0])\n                plt.xlim([0, aw.shape[0]])\n                plt.grid(True)\n                plt.ylabel(""Attention Weight"")\n                plt.xlabel(""Decoder Index"")\n                plt.legend(legends)\n        else:\n            legends = []\n            for i in range(att_w.shape[1]):\n                plt.plot(att_w[:, i])\n                legends.append(""Att{}"".format(i))\n            plt.ylim([0, 1.0])\n            plt.xlim([0, att_w.shape[0]])\n            plt.grid(True)\n            plt.ylabel(""Attention Weight"")\n            plt.xlabel(""Decoder Index"")\n            plt.legend(legends)\n        plt.tight_layout()\n        return plt\n\n    def _plot_and_save_attention(self, att_w, filename, han_mode=False):\n        if han_mode:\n            plt = self.draw_han_plot(att_w)\n        else:\n            plt = self.draw_attention_plot(att_w)\n        plt.savefig(filename)\n        plt.close()\n\n\ndef restore_snapshot(model, snapshot, load_fn=chainer.serializers.load_npz):\n    """"""Extension to restore snapshot.\n\n    Returns:\n        An extension function.\n\n    """"""\n\n    @training.make_extension(trigger=(1, ""epoch""))\n    def restore_snapshot(trainer):\n        _restore_snapshot(model, snapshot, load_fn)\n\n    return restore_snapshot\n\n\ndef _restore_snapshot(model, snapshot, load_fn=chainer.serializers.load_npz):\n    load_fn(snapshot, model)\n    logging.info(""restored from "" + str(snapshot))\n\n\ndef adadelta_eps_decay(eps_decay):\n    """"""Extension to perform adadelta eps decay.\n\n    Args:\n        eps_decay (float): Decay rate of eps.\n\n    Returns:\n        An extension function.\n\n    """"""\n\n    @training.make_extension(trigger=(1, ""epoch""))\n    def adadelta_eps_decay(trainer):\n        _adadelta_eps_decay(trainer, eps_decay)\n\n    return adadelta_eps_decay\n\n\ndef _adadelta_eps_decay(trainer, eps_decay):\n    optimizer = trainer.updater.get_optimizer(""main"")\n    # for chainer\n    if hasattr(optimizer, ""eps""):\n        current_eps = optimizer.eps\n        setattr(optimizer, ""eps"", current_eps * eps_decay)\n        logging.info(""adadelta eps decayed to "" + str(optimizer.eps))\n    # pytorch\n    else:\n        for p in optimizer.param_groups:\n            p[""eps""] *= eps_decay\n            logging.info(""adadelta eps decayed to "" + str(p[""eps""]))\n\n\ndef adam_lr_decay(eps_decay):\n    """"""Extension to perform adam lr decay.\n\n    Args:\n        eps_decay (float): Decay rate of lr.\n\n    Returns:\n        An extension function.\n\n    """"""\n\n    @training.make_extension(trigger=(1, ""epoch""))\n    def adam_lr_decay(trainer):\n        _adam_lr_decay(trainer, eps_decay)\n\n    return adam_lr_decay\n\n\ndef _adam_lr_decay(trainer, eps_decay):\n    optimizer = trainer.updater.get_optimizer(""main"")\n    # for chainer\n    if hasattr(optimizer, ""lr""):\n        current_lr = optimizer.lr\n        setattr(optimizer, ""lr"", current_lr * eps_decay)\n        logging.info(""adam lr decayed to "" + str(optimizer.lr))\n    # pytorch\n    else:\n        for p in optimizer.param_groups:\n            p[""lr""] *= eps_decay\n            logging.info(""adam lr decayed to "" + str(p[""lr""]))\n\n\ndef torch_snapshot(savefun=torch.save, filename=""snapshot.ep.{.updater.epoch}""):\n    """"""Extension to take snapshot of the trainer for pytorch.\n\n    Returns:\n        An extension function.\n\n    """"""\n\n    @extension.make_extension(trigger=(1, ""epoch""), priority=-100)\n    def torch_snapshot(trainer):\n        _torch_snapshot_object(trainer, trainer, filename.format(trainer), savefun)\n\n    return torch_snapshot\n\n\ndef _torch_snapshot_object(trainer, target, filename, savefun):\n    # make snapshot_dict dictionary\n    s = DictionarySerializer()\n    s.save(trainer)\n    if hasattr(trainer.updater.model, ""model""):\n        # (for TTS)\n        if hasattr(trainer.updater.model.model, ""module""):\n            model_state_dict = trainer.updater.model.model.module.state_dict()\n        else:\n            model_state_dict = trainer.updater.model.model.state_dict()\n    else:\n        # (for ASR)\n        if hasattr(trainer.updater.model, ""module""):\n            model_state_dict = trainer.updater.model.module.state_dict()\n        else:\n            model_state_dict = trainer.updater.model.state_dict()\n    snapshot_dict = {\n        ""trainer"": s.target,\n        ""model"": model_state_dict,\n        ""optimizer"": trainer.updater.get_optimizer(""main"").state_dict(),\n    }\n\n    # save snapshot dictionary\n    fn = filename.format(trainer)\n    prefix = ""tmp"" + fn\n    tmpdir = tempfile.mkdtemp(prefix=prefix, dir=trainer.out)\n    tmppath = os.path.join(tmpdir, fn)\n    try:\n        savefun(snapshot_dict, tmppath)\n        shutil.move(tmppath, os.path.join(trainer.out, fn))\n    finally:\n        shutil.rmtree(tmpdir)\n\n\ndef add_gradient_noise(model, iteration, duration=100, eta=1.0, scale_factor=0.55):\n    """"""Adds noise from a standard normal distribution to the gradients.\n\n    The standard deviation (`sigma`) is controlled by the three hyper-parameters below.\n    `sigma` goes to zero (no noise) with more iterations.\n\n    Args:\n        model (torch.nn.model): Model.\n        iteration (int): Number of iterations.\n        duration (int) {100, 1000}:\n            Number of durations to control the interval of the `sigma` change.\n        eta (float) {0.01, 0.3, 1.0}: The magnitude of `sigma`.\n        scale_factor (float) {0.55}: The scale of `sigma`.\n    """"""\n    interval = (iteration // duration) + 1\n    sigma = eta / interval ** scale_factor\n    for param in model.parameters():\n        if param.grad is not None:\n            _shape = param.grad.size()\n            noise = sigma * torch.randn(_shape).to(param.device)\n            param.grad += noise\n\n\n# * -------------------- general -------------------- *\ndef get_model_conf(model_path, conf_path=None):\n    """"""Get model config information by reading a model config file (model.json).\n\n    Args:\n        model_path (str): Model path.\n        conf_path (str): Optional model config path.\n\n    Returns:\n        list[int, int, dict[str, Any]]: Config information loaded from json file.\n\n    """"""\n    if conf_path is None:\n        model_conf = os.path.dirname(model_path) + ""/model.json""\n    else:\n        model_conf = conf_path\n    with open(model_conf, ""rb"") as f:\n        logging.info(""reading a config file from "" + model_conf)\n        confs = json.load(f)\n    if isinstance(confs, dict):\n        # for lm\n        args = confs\n        return argparse.Namespace(**args)\n    else:\n        # for asr, tts, mt\n        idim, odim, args = confs\n        return idim, odim, argparse.Namespace(**args)\n\n\ndef chainer_load(path, model):\n    """"""Load chainer model parameters.\n\n    Args:\n        path (str): Model path or snapshot file path to be loaded.\n        model (chainer.Chain): Chainer model.\n\n    """"""\n    if ""snapshot"" in os.path.basename(path):\n        chainer.serializers.load_npz(path, model, path=""updater/model:main/"")\n    else:\n        chainer.serializers.load_npz(path, model)\n\n\ndef torch_save(path, model):\n    """"""Save torch model states.\n\n    Args:\n        path (str): Model path to be saved.\n        model (torch.nn.Module): Torch model.\n\n    """"""\n    if hasattr(model, ""module""):\n        torch.save(model.module.state_dict(), path)\n    else:\n        torch.save(model.state_dict(), path)\n\n\ndef snapshot_object(target, filename):\n    """"""Returns a trainer extension to take snapshots of a given object.\n\n    Args:\n        target (model): Object to serialize.\n        filename (str): Name of the file into which the object is serialized.It can\n            be a format string, where the trainer object is passed to\n            the :meth: `str.format` method. For example,\n            ``\'snapshot_{.updater.iteration}\'`` is converted to\n            ``\'snapshot_10000\'`` at the 10,000th iteration.\n\n    Returns:\n        An extension function.\n\n    """"""\n\n    @extension.make_extension(trigger=(1, ""epoch""), priority=-100)\n    def snapshot_object(trainer):\n        torch_save(os.path.join(trainer.out, filename.format(trainer)), target)\n\n    return snapshot_object\n\n\ndef torch_load(path, model):\n    """"""Load torch model states.\n\n    Args:\n        path (str): Model path or snapshot file path to be loaded.\n        model (torch.nn.Module): Torch model.\n\n    """"""\n    if ""snapshot"" in os.path.basename(path):\n        model_state_dict = torch.load(path, map_location=lambda storage, loc: storage)[\n            ""model""\n        ]\n    else:\n        model_state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n\n    if hasattr(model, ""module""):\n        model.module.load_state_dict(model_state_dict)\n    else:\n        model.load_state_dict(model_state_dict)\n\n    del model_state_dict\n\n\ndef torch_resume(snapshot_path, trainer):\n    """"""Resume from snapshot for pytorch.\n\n    Args:\n        snapshot_path (str): Snapshot file path.\n        trainer (chainer.training.Trainer): Chainer\'s trainer instance.\n\n    """"""\n    # load snapshot\n    snapshot_dict = torch.load(snapshot_path, map_location=lambda storage, loc: storage)\n\n    # restore trainer states\n    d = NpzDeserializer(snapshot_dict[""trainer""])\n    d.load(trainer)\n\n    # restore model states\n    if hasattr(trainer.updater.model, ""model""):\n        # (for TTS model)\n        if hasattr(trainer.updater.model.model, ""module""):\n            trainer.updater.model.model.module.load_state_dict(snapshot_dict[""model""])\n        else:\n            trainer.updater.model.model.load_state_dict(snapshot_dict[""model""])\n    else:\n        # (for ASR model)\n        if hasattr(trainer.updater.model, ""module""):\n            trainer.updater.model.module.load_state_dict(snapshot_dict[""model""])\n        else:\n            trainer.updater.model.load_state_dict(snapshot_dict[""model""])\n\n    # retore optimizer states\n    trainer.updater.get_optimizer(""main"").load_state_dict(snapshot_dict[""optimizer""])\n\n    # delete opened snapshot\n    del snapshot_dict\n\n\n# * ------------------ recognition related ------------------ *\ndef parse_hypothesis(hyp, char_list):\n    """"""Parse hypothesis.\n\n    Args:\n        hyp (list[dict[str, Any]]): Recognition hypothesis.\n        char_list (list[str]): List of characters.\n\n    Returns:\n        tuple(str, str, str, float)\n\n    """"""\n    # remove sos and get results\n    tokenid_as_list = list(map(int, hyp[""yseq""][1:]))\n    token_as_list = [char_list[idx] for idx in tokenid_as_list]\n    score = float(hyp[""score""])\n\n    # convert to string\n    tokenid = "" "".join([str(idx) for idx in tokenid_as_list])\n    token = "" "".join(token_as_list)\n    text = """".join(token_as_list).replace(""<space>"", "" "")\n\n    return text, token, tokenid, score\n\n\ndef add_results_to_json(js, nbest_hyps, char_list):\n    """"""Add N-best results to json.\n\n    Args:\n        js (dict[str, Any]): Groundtruth utterance dict.\n        nbest_hyps_sd (list[dict[str, Any]]):\n            List of hypothesis for multi_speakers: nutts x nspkrs.\n        char_list (list[str]): List of characters.\n\n    Returns:\n        dict[str, Any]: N-best results added utterance dict.\n\n    """"""\n    # copy old json info\n    new_js = dict()\n    new_js[""utt2spk""] = js[""utt2spk""]\n    new_js[""output""] = []\n\n    for n, hyp in enumerate(nbest_hyps, 1):\n        # parse hypothesis\n        rec_text, rec_token, rec_tokenid, score = parse_hypothesis(hyp, char_list)\n\n        # copy ground-truth\n        if len(js[""output""]) > 0:\n            out_dic = dict(js[""output""][0].items())\n        else:\n            # for no reference case (e.g., speech translation)\n            out_dic = {""name"": """"}\n\n        # update name\n        out_dic[""name""] += ""[%d]"" % n\n\n        # add recognition results\n        out_dic[""rec_text""] = rec_text\n        out_dic[""rec_token""] = rec_token\n        out_dic[""rec_tokenid""] = rec_tokenid\n        out_dic[""score""] = score\n\n        # add to list of N-best result dicts\n        new_js[""output""].append(out_dic)\n\n        # show 1-best result\n        if n == 1:\n            if ""text"" in out_dic.keys():\n                logging.info(""groundtruth: %s"" % out_dic[""text""])\n            logging.info(""prediction : %s"" % out_dic[""rec_text""])\n\n    return new_js\n\n\ndef plot_spectrogram(\n    plt,\n    spec,\n    mode=""db"",\n    fs=None,\n    frame_shift=None,\n    bottom=True,\n    left=True,\n    right=True,\n    top=False,\n    labelbottom=True,\n    labelleft=True,\n    labelright=True,\n    labeltop=False,\n    cmap=""inferno"",\n):\n    """"""Plot spectrogram using matplotlib.\n\n    Args:\n        plt (matplotlib.pyplot): pyplot object.\n        spec (numpy.ndarray): Input stft (Freq, Time)\n        mode (str): db or linear.\n        fs (int): Sample frequency. To convert y-axis to kHz unit.\n        frame_shift (int): The frame shift of stft. To convert x-axis to second unit.\n        bottom (bool):Whether to draw the respective ticks.\n        left (bool):\n        right (bool):\n        top (bool):\n        labelbottom (bool):Whether to draw the respective tick labels.\n        labelleft (bool):\n        labelright (bool):\n        labeltop (bool):\n        cmap (str): Colormap defined in matplotlib.\n\n    """"""\n    spec = np.abs(spec)\n    if mode == ""db"":\n        x = 20 * np.log10(spec + np.finfo(spec.dtype).eps)\n    elif mode == ""linear"":\n        x = spec\n    else:\n        raise ValueError(mode)\n\n    if fs is not None:\n        ytop = fs / 2000\n        ylabel = ""kHz""\n    else:\n        ytop = x.shape[0]\n        ylabel = ""bin""\n\n    if frame_shift is not None and fs is not None:\n        xtop = x.shape[1] * frame_shift / fs\n        xlabel = ""s""\n    else:\n        xtop = x.shape[1]\n        xlabel = ""frame""\n\n    extent = (0, xtop, 0, ytop)\n    plt.imshow(x[::-1], cmap=cmap, extent=extent)\n\n    if labelbottom:\n        plt.xlabel(""time [{}]"".format(xlabel))\n    if labelleft:\n        plt.ylabel(""freq [{}]"".format(ylabel))\n    plt.colorbar().set_label(""{}"".format(mode))\n\n    plt.tick_params(\n        bottom=bottom,\n        left=left,\n        right=right,\n        top=top,\n        labelbottom=labelbottom,\n        labelleft=labelleft,\n        labelright=labelright,\n        labeltop=labeltop,\n    )\n    plt.axis(""auto"")\n\n\n# * ------------------ recognition related ------------------ *\ndef format_mulenc_args(args):\n    """"""Format args for multi-encoder setup.\n\n    It deals with following situations:  (when args.num_encs=2):\n    1. args.elayers = None -> args.elayers = [4, 4];\n    2. args.elayers = 4 -> args.elayers = [4, 4];\n    3. args.elayers = [4, 4, 4] -> args.elayers = [4, 4].\n\n    """"""\n    # default values when None is assigned.\n    default_dict = {\n        ""etype"": ""blstmp"",\n        ""elayers"": 4,\n        ""eunits"": 300,\n        ""subsample"": ""1"",\n        ""dropout_rate"": 0.0,\n        ""atype"": ""dot"",\n        ""adim"": 320,\n        ""awin"": 5,\n        ""aheads"": 4,\n        ""aconv_chans"": -1,\n        ""aconv_filts"": 100,\n    }\n    for k in default_dict.keys():\n        if isinstance(vars(args)[k], list):\n            if len(vars(args)[k]) != args.num_encs:\n                logging.warning(\n                    ""Length mismatch {}: Convert {} to {}."".format(\n                        k, vars(args)[k], vars(args)[k][: args.num_encs]\n                    )\n                )\n            vars(args)[k] = vars(args)[k][: args.num_encs]\n        else:\n            if not vars(args)[k]:\n                # assign default value if it is None\n                vars(args)[k] = default_dict[k]\n                logging.warning(\n                    ""{} is not specified, use default value {}."".format(\n                        k, default_dict[k]\n                    )\n                )\n            # duplicate\n            logging.warning(\n                ""Type mismatch {}: Convert {} to {}."".format(\n                    k, vars(args)[k], [vars(args)[k] for _ in range(args.num_encs)]\n                )\n            )\n            vars(args)[k] = [vars(args)[k] for _ in range(args.num_encs)]\n    return args\n'"
espnet/bin/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/bin/asr_enhance.py,0,"b'#!/usr/bin/env python3\nimport configargparse\nfrom distutils.util import strtobool\nimport logging\nimport os\nimport random\nimport sys\n\nimport numpy as np\n\nfrom espnet.asr.pytorch_backend.asr import enhance\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser():\n    parser = configargparse.ArgumentParser(\n        description=""Enhance noisy speech for speech recognition"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""second config file path that overwrites the settings in `--config`."",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""third config file path that overwrites the settings ""\n        ""in `--config` and `--config2`."",\n    )\n\n    parser.add_argument(""--ngpu"", default=0, type=int, help=""Number of GPUs"")\n    parser.add_argument(\n        ""--backend"",\n        default=""chainer"",\n        type=str,\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(""--debugmode"", default=1, type=int, help=""Debugmode"")\n    parser.add_argument(""--seed"", default=1, type=int, help=""Random seed"")\n    parser.add_argument(""--verbose"", ""-V"", default=1, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--batchsize"",\n        default=1,\n        type=int,\n        help=""Batch size for beam search (0: means no batch processing)"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    # task related\n    parser.add_argument(\n        ""--recog-json"", type=str, help=""Filename of recognition data (json)""\n    )\n    # model (parameter) related\n    parser.add_argument(\n        ""--model"", type=str, required=True, help=""Model file parameters to read""\n    )\n    parser.add_argument(\n        ""--model-conf"", type=str, default=None, help=""Model config file""\n    )\n\n    # Outputs configuration\n    parser.add_argument(\n        ""--enh-wspecifier"",\n        type=str,\n        default=None,\n        help=""Specify the output way for enhanced speech.""\n        ""e.g. ark,scp:outdir,wav.scp"",\n    )\n    parser.add_argument(\n        ""--enh-filetype"",\n        type=str,\n        default=""sound"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for enhanced speech. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(""--fs"", type=int, default=16000, help=""The sample frequency"")\n    parser.add_argument(\n        ""--keep-length"",\n        type=strtobool,\n        default=True,\n        help=""Adjust the output length to match "" ""with the input for enhanced speech"",\n    )\n    parser.add_argument(\n        ""--image-dir"", type=str, default=None, help=""The directory saving the images.""\n    )\n    parser.add_argument(\n        ""--num-images"",\n        type=int,\n        default=20,\n        help=""The number of images files to be saved. ""\n        ""If negative, all samples are to be saved."",\n    )\n\n    # IStft\n    parser.add_argument(\n        ""--apply-istft"",\n        type=strtobool,\n        default=True,\n        help=""Apply istft to the output from the network"",\n    )\n    parser.add_argument(\n        ""--istft-win-length"",\n        type=int,\n        default=512,\n        help=""The window length for istft. ""\n        ""This option is ignored ""\n        ""if stft is found in the preprocess-conf"",\n    )\n    parser.add_argument(\n        ""--istft-n-shift"",\n        type=str,\n        default=256,\n        help=""The window type for istft. ""\n        ""This option is ignored ""\n        ""if stft is found in the preprocess-conf"",\n    )\n    parser.add_argument(\n        ""--istft-window"",\n        type=str,\n        default=""hann"",\n        help=""The window type for istft. ""\n        ""This option is ignored ""\n        ""if stft is found in the preprocess-conf"",\n    )\n    return parser\n\n\ndef main(args):\n    parser = get_parser()\n    args = parser.parse_args(args)\n\n    # logging info\n    if args.verbose == 1:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    elif args.verbose == 2:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check CUDA_VISIBLE_DEVICES\n    if args.ngpu > 0:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is None:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n        elif args.ngpu != len(cvd.split("","")):\n            logging.error(""#gpus is not matched with CUDA_VISIBLE_DEVICES."")\n            sys.exit(1)\n\n        # TODO(kamo): support of multiple GPUs\n        if args.ngpu > 1:\n            logging.error(""The program only supports ngpu=1."")\n            sys.exit(1)\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # seed setting\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    logging.info(""set random seed = %d"" % args.seed)\n\n    # recog\n    logging.info(""backend = "" + args.backend)\n    if args.backend == ""pytorch"":\n        enhance(args)\n    else:\n        raise ValueError(""Only pytorch is supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/asr_recog.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""End-to-end speech recognition model decoding script.""""""\n\nimport configargparse\nimport logging\nimport os\nimport random\nimport sys\n\nimport numpy as np\n\nfrom espnet.utils.cli_utils import strtobool\n\n# NOTE: you need this func to generate our sphinx doc\n\n\ndef get_parser():\n    """"""Get default arguments.""""""\n    parser = configargparse.ArgumentParser(\n        description=""Transcribe text from speech using ""\n        ""a speech recognition model on one CPU or GPU"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""Config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""Second config file path that overwrites the settings in `--config`"",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""Third config file path that overwrites the settings ""\n        ""in `--config` and `--config2`"",\n    )\n\n    parser.add_argument(""--ngpu"", type=int, default=0, help=""Number of GPUs"")\n    parser.add_argument(\n        ""--dtype"",\n        choices=(""float16"", ""float32"", ""float64""),\n        default=""float32"",\n        help=""Float precision (only available in --api v2)"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        type=str,\n        default=""chainer"",\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(""--debugmode"", type=int, default=1, help=""Debugmode"")\n    parser.add_argument(""--seed"", type=int, default=1, help=""Random seed"")\n    parser.add_argument(""--verbose"", ""-V"", type=int, default=1, help=""Verbose option"")\n    parser.add_argument(\n        ""--batchsize"",\n        type=int,\n        default=1,\n        help=""Batch size for beam search (0: means no batch processing)"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""--api"",\n        default=""v1"",\n        choices=[""v1"", ""v2""],\n        help=""Beam search APIs ""\n        ""v1: Default API. It only supports the ASRInterface.recognize method ""\n        ""and DefaultRNNLM. ""\n        ""v2: Experimental API. It supports any models that implements ScorerInterface."",\n    )\n    # task related\n    parser.add_argument(\n        ""--recog-json"", type=str, help=""Filename of recognition data (json)""\n    )\n    parser.add_argument(\n        ""--result-label"",\n        type=str,\n        required=True,\n        help=""Filename of result label data (json)"",\n    )\n    # model (parameter) related\n    parser.add_argument(\n        ""--model"", type=str, required=True, help=""Model file parameters to read""\n    )\n    parser.add_argument(\n        ""--model-conf"", type=str, default=None, help=""Model config file""\n    )\n    parser.add_argument(\n        ""--num-spkrs"",\n        type=int,\n        default=1,\n        choices=[1, 2],\n        help=""Number of speakers in the speech"",\n    )\n    parser.add_argument(\n        ""--num-encs"", default=1, type=int, help=""Number of encoders in the model.""\n    )\n    # search related\n    parser.add_argument(""--nbest"", type=int, default=1, help=""Output N-best hypotheses"")\n    parser.add_argument(""--beam-size"", type=int, default=1, help=""Beam size"")\n    parser.add_argument(""--penalty"", type=float, default=0.0, help=""Incertion penalty"")\n    parser.add_argument(\n        ""--maxlenratio"",\n        type=float,\n        default=0.0,\n        help=""""""Input length ratio to obtain max output length.\n                        If maxlenratio=0.0 (default), it uses a end-detect function\n                        to automatically find maximum hypothesis lengths"""""",\n    )\n    parser.add_argument(\n        ""--minlenratio"",\n        type=float,\n        default=0.0,\n        help=""Input length ratio to obtain min output length"",\n    )\n    parser.add_argument(\n        ""--ctc-weight"", type=float, default=0.0, help=""CTC weight in joint decoding""\n    )\n    parser.add_argument(\n        ""--weights-ctc-dec"",\n        type=float,\n        action=""append"",\n        help=""ctc weight assigned to each encoder during decoding.""\n        ""[in multi-encoder mode only]"",\n    )\n    parser.add_argument(\n        ""--ctc-window-margin"",\n        type=int,\n        default=0,\n        help=""""""Use CTC window with margin parameter to accelerate\n                        CTC/attention decoding especially on GPU. Smaller magin\n                        makes decoding faster, but may increase search errors.\n                        If margin=0 (default), this function is disabled"""""",\n    )\n    # transducer related\n    parser.add_argument(\n        ""--score-norm-transducer"",\n        type=strtobool,\n        nargs=""?"",\n        default=True,\n        help=""Normalize transducer scores by length"",\n    )\n    # rnnlm related\n    parser.add_argument(\n        ""--rnnlm"", type=str, default=None, help=""RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--rnnlm-conf"", type=str, default=None, help=""RNNLM model config file to read""\n    )\n    parser.add_argument(\n        ""--word-rnnlm"", type=str, default=None, help=""Word RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--word-rnnlm-conf"",\n        type=str,\n        default=None,\n        help=""Word RNNLM model config file to read"",\n    )\n    parser.add_argument(""--word-dict"", type=str, default=None, help=""Word list to read"")\n    parser.add_argument(""--lm-weight"", type=float, default=0.1, help=""RNNLM weight"")\n    # streaming related\n    parser.add_argument(\n        ""--streaming-mode"",\n        type=str,\n        default=None,\n        choices=[""window"", ""segment""],\n        help=""""""Use streaming recognizer for inference.\n                        `--batchsize` must be set to 0 to enable this mode"""""",\n    )\n    parser.add_argument(""--streaming-window"", type=int, default=10, help=""Window size"")\n    parser.add_argument(\n        ""--streaming-min-blank-dur"",\n        type=int,\n        default=10,\n        help=""Minimum blank duration threshold"",\n    )\n    parser.add_argument(\n        ""--streaming-onset-margin"", type=int, default=1, help=""Onset margin""\n    )\n    parser.add_argument(\n        ""--streaming-offset-margin"", type=int, default=1, help=""Offset margin""\n    )\n    return parser\n\n\ndef main(args):\n    """"""Run the main decoding function.""""""\n    parser = get_parser()\n    args = parser.parse_args(args)\n\n    if args.ngpu == 0 and args.dtype == ""float16"":\n        raise ValueError(f""--dtype {args.dtype} does not support the CPU backend."")\n\n    # logging info\n    if args.verbose == 1:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    elif args.verbose == 2:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check CUDA_VISIBLE_DEVICES\n    if args.ngpu > 0:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is None:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n        elif args.ngpu != len(cvd.split("","")):\n            logging.error(""#gpus is not matched with CUDA_VISIBLE_DEVICES."")\n            sys.exit(1)\n\n        # TODO(mn5k): support of multiple GPUs\n        if args.ngpu > 1:\n            logging.error(""The program only supports ngpu=1."")\n            sys.exit(1)\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # seed setting\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    logging.info(""set random seed = %d"" % args.seed)\n\n    # validate rnn options\n    if args.rnnlm is not None and args.word_rnnlm is not None:\n        logging.error(\n            ""It seems that both --rnnlm and --word-rnnlm are specified. ""\n            ""Please use either option.""\n        )\n        sys.exit(1)\n\n    # recog\n    logging.info(""backend = "" + args.backend)\n    if args.num_spkrs == 1:\n        if args.backend == ""chainer"":\n            from espnet.asr.chainer_backend.asr import recog\n\n            recog(args)\n        elif args.backend == ""pytorch"":\n            if args.num_encs == 1:\n                # Experimental API that supports custom LMs\n                if args.api == ""v2"":\n                    from espnet.asr.pytorch_backend.recog import recog_v2\n\n                    recog_v2(args)\n                else:\n                    from espnet.asr.pytorch_backend.asr import recog\n\n                    if args.dtype != ""float32"":\n                        raise NotImplementedError(\n                            f""`--dtype {args.dtype}` is only available with `--api v2`""\n                        )\n                    recog(args)\n            else:\n                if args.api == ""v2"":\n                    raise NotImplementedError(\n                        f""--num-encs {args.num_encs} > 1 is not supported in --api v2""\n                    )\n                else:\n                    from espnet.asr.pytorch_backend.asr import recog\n\n                    recog(args)\n        else:\n            raise ValueError(""Only chainer and pytorch are supported."")\n    elif args.num_spkrs == 2:\n        if args.backend == ""pytorch"":\n            from espnet.asr.pytorch_backend.asr_mix import recog\n\n            recog(args)\n        else:\n            raise ValueError(""Only pytorch is supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/asr_train.py,1,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Automatic speech recognition model training script.""""""\n\nimport logging\nimport os\nimport random\nimport subprocess\nimport sys\n\nfrom distutils.version import LooseVersion\n\nimport configargparse\nimport numpy as np\nimport torch\n\nfrom espnet.utils.cli_utils import strtobool\nfrom espnet.utils.training.batchfy import BATCH_COUNT_CHOICES\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(""1.2"")\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser(parser=None, required=True):\n    """"""Get default arguments.""""""\n    if parser is None:\n        parser = configargparse.ArgumentParser(\n            description=""Train an automatic speech recognition (ASR) model on one CPU, ""\n            ""one or multiple GPUs"",\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n        )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""second config file path that overwrites the settings in `--config`."",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""third config file path that overwrites the settings in ""\n        ""`--config` and `--config2`."",\n    )\n\n    parser.add_argument(\n        ""--ngpu"",\n        default=None,\n        type=int,\n        help=""Number of GPUs. If not given, use all visible devices"",\n    )\n    parser.add_argument(\n        ""--train-dtype"",\n        default=""float32"",\n        choices=[""float16"", ""float32"", ""float64"", ""O0"", ""O1"", ""O2"", ""O3""],\n        help=""Data type for training (only pytorch backend). ""\n        ""O0,O1,.. flags require apex. ""\n        ""See https://nvidia.github.io/apex/amp.html#opt-levels"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        default=""chainer"",\n        type=str,\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(\n        ""--outdir"", type=str, required=required, help=""Output directory""\n    )\n    parser.add_argument(""--debugmode"", default=1, type=int, help=""Debugmode"")\n    parser.add_argument(""--dict"", required=required, help=""Dictionary"")\n    parser.add_argument(""--seed"", default=1, type=int, help=""Random seed"")\n    parser.add_argument(""--debugdir"", type=str, help=""Output directory for debugging"")\n    parser.add_argument(\n        ""--resume"",\n        ""-r"",\n        default="""",\n        nargs=""?"",\n        help=""Resume the training from snapshot"",\n    )\n    parser.add_argument(\n        ""--minibatches"",\n        ""-N"",\n        type=int,\n        default=""-1"",\n        help=""Process only N minibatches (for debug)"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--tensorboard-dir"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Tensorboard log dir path"",\n    )\n    parser.add_argument(\n        ""--report-interval-iters"",\n        default=100,\n        type=int,\n        help=""Report interval iterations"",\n    )\n    parser.add_argument(\n        ""--save-interval-iters"",\n        default=0,\n        type=int,\n        help=""Save snapshot interval iterations"",\n    )\n    # task related\n    parser.add_argument(\n        ""--train-json"",\n        type=str,\n        default=None,\n        help=""Filename of train label data (json)"",\n    )\n    parser.add_argument(\n        ""--valid-json"",\n        type=str,\n        default=None,\n        help=""Filename of validation label data (json)"",\n    )\n    # network architecture\n    parser.add_argument(\n        ""--model-module"",\n        type=str,\n        default=None,\n        help=""model defined module (default: espnet.nets.xxx_backend.e2e_asr:E2E)"",\n    )\n    # encoder\n    parser.add_argument(\n        ""--num-encs"", default=1, type=int, help=""Number of encoders in the model.""\n    )\n    # loss related\n    parser.add_argument(\n        ""--ctc_type"",\n        default=""warpctc"",\n        type=str,\n        choices=[""builtin"", ""warpctc""],\n        help=""Type of CTC implementation to calculate loss."",\n    )\n    parser.add_argument(\n        ""--mtlalpha"",\n        default=0.5,\n        type=float,\n        help=""Multitask learning coefficient, ""\n        ""alpha: alpha*ctc_loss + (1-alpha)*att_loss "",\n    )\n    parser.add_argument(\n        ""--lsm-weight"", default=0.0, type=float, help=""Label smoothing weight""\n    )\n    # recognition options to compute CER/WER\n    parser.add_argument(\n        ""--report-cer"",\n        default=False,\n        action=""store_true"",\n        help=""Compute CER on development set"",\n    )\n    parser.add_argument(\n        ""--report-wer"",\n        default=False,\n        action=""store_true"",\n        help=""Compute WER on development set"",\n    )\n    parser.add_argument(""--nbest"", type=int, default=1, help=""Output N-best hypotheses"")\n    parser.add_argument(""--beam-size"", type=int, default=4, help=""Beam size"")\n    parser.add_argument(""--penalty"", default=0.0, type=float, help=""Incertion penalty"")\n    parser.add_argument(\n        ""--maxlenratio"",\n        default=0.0,\n        type=float,\n        help=""""""Input length ratio to obtain max output length.\n                        If maxlenratio=0.0 (default), it uses a end-detect function\n                        to automatically find maximum hypothesis lengths"""""",\n    )\n    parser.add_argument(\n        ""--minlenratio"",\n        default=0.0,\n        type=float,\n        help=""Input length ratio to obtain min output length"",\n    )\n    parser.add_argument(\n        ""--ctc-weight"", default=0.3, type=float, help=""CTC weight in joint decoding""\n    )\n    parser.add_argument(\n        ""--rnnlm"", type=str, default=None, help=""RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--rnnlm-conf"", type=str, default=None, help=""RNNLM model config file to read""\n    )\n    parser.add_argument(""--lm-weight"", default=0.1, type=float, help=""RNNLM weight."")\n    parser.add_argument(""--sym-space"", default=""<space>"", type=str, help=""Space symbol"")\n    parser.add_argument(""--sym-blank"", default=""<blank>"", type=str, help=""Blank symbol"")\n    # minibatch related\n    parser.add_argument(\n        ""--sortagrad"",\n        default=0,\n        type=int,\n        nargs=""?"",\n        help=""How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs"",\n    )\n    parser.add_argument(\n        ""--batch-count"",\n        default=""auto"",\n        choices=BATCH_COUNT_CHOICES,\n        help=""How to count batch_size. ""\n        ""The default (auto) will find how to count by args."",\n    )\n    parser.add_argument(\n        ""--batch-size"",\n        ""--batch-seqs"",\n        ""-b"",\n        default=0,\n        type=int,\n        help=""Maximum seqs in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-bins"",\n        default=0,\n        type=int,\n        help=""Maximum bins in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-in"",\n        default=0,\n        type=int,\n        help=""Maximum input frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-out"",\n        default=0,\n        type=int,\n        help=""Maximum output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-inout"",\n        default=0,\n        type=int,\n        help=""Maximum input+output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--maxlen-in"",\n        ""--batch-seq-maxlen-in"",\n        default=800,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, ""\n        ""batch size is reduced if the input sequence length > ML."",\n    )\n    parser.add_argument(\n        ""--maxlen-out"",\n        ""--batch-seq-maxlen-out"",\n        default=150,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, ""\n        ""batch size is reduced if the output sequence length > ML"",\n    )\n    parser.add_argument(\n        ""--n-iter-processes"",\n        default=0,\n        type=int,\n        help=""Number of processes of iterator"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""The configuration file for the pre-processing"",\n    )\n    # optimization related\n    parser.add_argument(\n        ""--opt"",\n        default=""adadelta"",\n        type=str,\n        choices=[""adadelta"", ""adam"", ""noam""],\n        help=""Optimizer"",\n    )\n    parser.add_argument(\n        ""--accum-grad"", default=1, type=int, help=""Number of gradient accumuration""\n    )\n    parser.add_argument(\n        ""--eps"", default=1e-8, type=float, help=""Epsilon constant for optimizer""\n    )\n    parser.add_argument(\n        ""--eps-decay"", default=0.01, type=float, help=""Decaying ratio of epsilon""\n    )\n    parser.add_argument(\n        ""--weight-decay"", default=0.0, type=float, help=""Weight decay ratio""\n    )\n    parser.add_argument(\n        ""--criterion"",\n        default=""acc"",\n        type=str,\n        choices=[""loss"", ""acc""],\n        help=""Criterion to perform epsilon decay"",\n    )\n    parser.add_argument(\n        ""--threshold"", default=1e-4, type=float, help=""Threshold to stop iteration""\n    )\n    parser.add_argument(\n        ""--epochs"", ""-e"", default=30, type=int, help=""Maximum number of epochs""\n    )\n    parser.add_argument(\n        ""--early-stop-criterion"",\n        default=""validation/main/acc"",\n        type=str,\n        nargs=""?"",\n        help=""Value to monitor to trigger an early stopping of the training"",\n    )\n    parser.add_argument(\n        ""--patience"",\n        default=3,\n        type=int,\n        nargs=""?"",\n        help=""Number of epochs to wait without improvement ""\n        ""before stopping the training"",\n    )\n    parser.add_argument(\n        ""--grad-clip"", default=5, type=float, help=""Gradient norm threshold to clip""\n    )\n    parser.add_argument(\n        ""--num-save-attention"",\n        default=3,\n        type=int,\n        help=""Number of samples of attention to be saved"",\n    )\n    parser.add_argument(\n        ""--grad-noise"",\n        type=strtobool,\n        default=False,\n        help=""The flag to switch to use noise injection to gradients during training"",\n    )\n    # asr_mix related\n    parser.add_argument(\n        ""--num-spkrs"",\n        default=1,\n        type=int,\n        choices=[1, 2],\n        help=""Number of speakers in the speech."",\n    )\n    # decoder related\n    parser.add_argument(\n        ""--context-residual"",\n        default=False,\n        type=strtobool,\n        nargs=""?"",\n        help=""The flag to switch to use context vector residual in the decoder network"",\n    )\n    # finetuning related\n    parser.add_argument(\n        ""--enc-init"",\n        default=None,\n        type=str,\n        help=""Pre-trained ASR model to initialize encoder."",\n    )\n    parser.add_argument(\n        ""--enc-init-mods"",\n        default=""enc.enc."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of encoder modules to initialize, separated by a comma."",\n    )\n    parser.add_argument(\n        ""--dec-init"",\n        default=None,\n        type=str,\n        help=""Pre-trained ASR, MT or LM model to initialize decoder."",\n    )\n    parser.add_argument(\n        ""--dec-init-mods"",\n        default=""att., dec."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of decoder modules to initialize, separated by a comma."",\n    )\n    # front end related\n    parser.add_argument(\n        ""--use-frontend"",\n        type=strtobool,\n        default=False,\n        help=""The flag to switch to use frontend system."",\n    )\n\n    # WPE related\n    parser.add_argument(\n        ""--use-wpe"",\n        type=strtobool,\n        default=False,\n        help=""Apply Weighted Prediction Error"",\n    )\n    parser.add_argument(\n        ""--wtype"",\n        default=""blstmp"",\n        type=str,\n        choices=[\n            ""lstm"",\n            ""blstm"",\n            ""lstmp"",\n            ""blstmp"",\n            ""vgglstmp"",\n            ""vggblstmp"",\n            ""vgglstm"",\n            ""vggblstm"",\n            ""gru"",\n            ""bgru"",\n            ""grup"",\n            ""bgrup"",\n            ""vgggrup"",\n            ""vggbgrup"",\n            ""vgggru"",\n            ""vggbgru"",\n        ],\n        help=""Type of encoder network architecture ""\n        ""of the mask estimator for WPE. ""\n        """",\n    )\n    parser.add_argument(""--wlayers"", type=int, default=2, help="""")\n    parser.add_argument(""--wunits"", type=int, default=300, help="""")\n    parser.add_argument(""--wprojs"", type=int, default=300, help="""")\n    parser.add_argument(""--wdropout-rate"", type=float, default=0.0, help="""")\n    parser.add_argument(""--wpe-taps"", type=int, default=5, help="""")\n    parser.add_argument(""--wpe-delay"", type=int, default=3, help="""")\n    parser.add_argument(\n        ""--use-dnn-mask-for-wpe"",\n        type=strtobool,\n        default=False,\n        help=""Use DNN to estimate the power spectrogram. ""\n        ""This option is experimental."",\n    )\n    # Beamformer related\n    parser.add_argument(""--use-beamformer"", type=strtobool, default=True, help="""")\n    parser.add_argument(\n        ""--btype"",\n        default=""blstmp"",\n        type=str,\n        choices=[\n            ""lstm"",\n            ""blstm"",\n            ""lstmp"",\n            ""blstmp"",\n            ""vgglstmp"",\n            ""vggblstmp"",\n            ""vgglstm"",\n            ""vggblstm"",\n            ""gru"",\n            ""bgru"",\n            ""grup"",\n            ""bgrup"",\n            ""vgggrup"",\n            ""vggbgrup"",\n            ""vgggru"",\n            ""vggbgru"",\n        ],\n        help=""Type of encoder network architecture ""\n        ""of the mask estimator for Beamformer."",\n    )\n    parser.add_argument(""--blayers"", type=int, default=2, help="""")\n    parser.add_argument(""--bunits"", type=int, default=300, help="""")\n    parser.add_argument(""--bprojs"", type=int, default=300, help="""")\n    parser.add_argument(""--badim"", type=int, default=320, help="""")\n    parser.add_argument(\n        ""--bnmask"",\n        type=int,\n        default=2,\n        help=""Number of beamforming masks, "" ""default is 2 for [speech, noise]."",\n    )\n    parser.add_argument(\n        ""--ref-channel"",\n        type=int,\n        default=-1,\n        help=""The reference channel used for beamformer. ""\n        ""By default, the channel is estimated by DNN."",\n    )\n    parser.add_argument(""--bdropout-rate"", type=float, default=0.0, help="""")\n    # Feature transform: Normalization\n    parser.add_argument(\n        ""--stats-file"",\n        type=str,\n        default=None,\n        help=""The stats file for the feature normalization"",\n    )\n    parser.add_argument(\n        ""--apply-uttmvn"",\n        type=strtobool,\n        default=True,\n        help=""Apply utterance level mean "" ""variance normalization."",\n    )\n    parser.add_argument(""--uttmvn-norm-means"", type=strtobool, default=True, help="""")\n    parser.add_argument(""--uttmvn-norm-vars"", type=strtobool, default=False, help="""")\n    # Feature transform: Fbank\n    parser.add_argument(\n        ""--fbank-fs"",\n        type=int,\n        default=16000,\n        help=""The sample frequency used for "" ""the mel-fbank creation."",\n    )\n    parser.add_argument(\n        ""--n-mels"", type=int, default=80, help=""The number of mel-frequency bins.""\n    )\n    parser.add_argument(""--fbank-fmin"", type=float, default=0.0, help="""")\n    parser.add_argument(""--fbank-fmax"", type=float, default=None, help="""")\n    return parser\n\n\ndef main(cmd_args):\n    """"""Run the main training function.""""""\n    parser = get_parser()\n    args, _ = parser.parse_known_args(cmd_args)\n    if args.backend == ""chainer"" and args.train_dtype != ""float32"":\n        raise NotImplementedError(\n            f""chainer backend does not support --train-dtype {args.train_dtype}.""\n            ""Use --dtype float32.""\n        )\n    if args.ngpu == 0 and args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3"", ""float16""):\n        raise ValueError(\n            f""--train-dtype {args.train_dtype} does not support the CPU backend.""\n        )\n\n    from espnet.utils.dynamic_import import dynamic_import\n\n    if args.model_module is None:\n        model_module = ""espnet.nets."" + args.backend + ""_backend.e2e_asr:E2E""\n    else:\n        model_module = args.model_module\n    model_class = dynamic_import(model_module)\n    model_class.add_arguments(parser)\n\n    args = parser.parse_args(cmd_args)\n    args.model_module = model_module\n    if ""chainer_backend"" in args.model_module:\n        args.backend = ""chainer""\n    if ""pytorch_backend"" in args.model_module:\n        args.backend = ""pytorch""\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # If --ngpu is not given,\n    #   1. if CUDA_VISIBLE_DEVICES is set, all visible devices\n    #   2. if nvidia-smi exists, use all devices\n    #   3. else ngpu=0\n    if args.ngpu is None:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is not None:\n            ngpu = len(cvd.split("",""))\n        else:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n            try:\n                p = subprocess.run(\n                    [""nvidia-smi"", ""-L""], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                ngpu = 0\n            else:\n                ngpu = len(p.stderr.decode().split(""\\n"")) - 1\n    else:\n        if is_torch_1_2_plus and args.ngpu != 1:\n            logging.debug(\n                ""There are some bugs with multi-GPU processing in PyTorch 1.2+""\n                + "" (see https://github.com/pytorch/pytorch/issues/21108)""\n            )\n        ngpu = args.ngpu\n    logging.info(f""ngpu: {ngpu}"")\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # set random seed\n    logging.info(""random seed = %d"" % args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    # load dictionary for debug log\n    if args.dict is not None:\n        with open(args.dict, ""rb"") as f:\n            dictionary = f.readlines()\n        char_list = [entry.decode(""utf-8"").split("" "")[0] for entry in dictionary]\n        char_list.insert(0, ""<blank>"")\n        char_list.append(""<eos>"")\n        args.char_list = char_list\n    else:\n        args.char_list = None\n\n    # train\n    logging.info(""backend = "" + args.backend)\n\n    if args.num_spkrs == 1:\n        if args.backend == ""chainer"":\n            from espnet.asr.chainer_backend.asr import train\n\n            train(args)\n        elif args.backend == ""pytorch"":\n            from espnet.asr.pytorch_backend.asr import train\n\n            train(args)\n        else:\n            raise ValueError(""Only chainer and pytorch are supported."")\n    else:\n        # FIXME(kamo): Support --model-module\n        if args.backend == ""pytorch"":\n            from espnet.asr.pytorch_backend.asr_mix import train\n\n            train(args)\n        else:\n            raise ValueError(""Only pytorch is supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/lm_train.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n# This code is ported from the following implementation written in Torch.\n# https://github.com/chainer/chainer/blob/master/examples/ptb/train_ptb_custom_loop.py\n\n""""""Language model training script.""""""\n\nimport logging\nimport os\nimport random\nimport subprocess\nimport sys\n\nimport configargparse\nimport numpy as np\n\nfrom espnet.nets.lm_interface import dynamic_import_lm\nfrom espnet.optimizer.factory import dynamic_import_optimizer\nfrom espnet.scheduler.scheduler import dynamic_import_scheduler\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser(parser=None, required=True):\n    """"""Get parser.""""""\n    if parser is None:\n        parser = configargparse.ArgumentParser(\n            description=""Train a new language model on one CPU or one GPU"",\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n        )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""second config file path that overwrites the settings in `--config`."",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""third config file path that overwrites the settings ""\n        ""in `--config` and `--config2`."",\n    )\n\n    parser.add_argument(\n        ""--ngpu"",\n        default=None,\n        type=int,\n        help=""Number of GPUs. If not given, use all visible devices"",\n    )\n    parser.add_argument(\n        ""--train-dtype"",\n        default=""float32"",\n        choices=[""float16"", ""float32"", ""float64"", ""O0"", ""O1"", ""O2"", ""O3""],\n        help=""Data type for training (only pytorch backend). ""\n        ""O0,O1,.. flags require apex. ""\n        ""See https://nvidia.github.io/apex/amp.html#opt-levels"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        default=""chainer"",\n        type=str,\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(\n        ""--outdir"", type=str, required=required, help=""Output directory""\n    )\n    parser.add_argument(""--debugmode"", default=1, type=int, help=""Debugmode"")\n    parser.add_argument(""--dict"", type=str, required=required, help=""Dictionary"")\n    parser.add_argument(""--seed"", default=1, type=int, help=""Random seed"")\n    parser.add_argument(\n        ""--resume"",\n        ""-r"",\n        default="""",\n        nargs=""?"",\n        help=""Resume the training from snapshot"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--tensorboard-dir"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Tensorboard log dir path"",\n    )\n    parser.add_argument(\n        ""--report-interval-iters"",\n        default=100,\n        type=int,\n        help=""Report interval iterations"",\n    )\n    # task related\n    parser.add_argument(\n        ""--train-label"",\n        type=str,\n        required=required,\n        help=""Filename of train label data"",\n    )\n    parser.add_argument(\n        ""--valid-label"",\n        type=str,\n        required=required,\n        help=""Filename of validation label data"",\n    )\n    parser.add_argument(""--test-label"", type=str, help=""Filename of test label data"")\n    parser.add_argument(\n        ""--dump-hdf5-path"",\n        type=str,\n        default=None,\n        help=""Path to dump a preprocessed dataset as hdf5"",\n    )\n    # training configuration\n    parser.add_argument(""--opt"", default=""sgd"", type=str, help=""Optimizer"")\n    parser.add_argument(\n        ""--sortagrad"",\n        default=0,\n        type=int,\n        nargs=""?"",\n        help=""How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs"",\n    )\n    parser.add_argument(\n        ""--batchsize"",\n        ""-b"",\n        type=int,\n        default=300,\n        help=""Number of examples in each mini-batch"",\n    )\n    parser.add_argument(\n        ""--accum-grad"", type=int, default=1, help=""Number of gradient accumueration""\n    )\n    parser.add_argument(\n        ""--epoch"",\n        ""-e"",\n        type=int,\n        default=20,\n        help=""Number of sweeps over the dataset to train"",\n    )\n    parser.add_argument(\n        ""--early-stop-criterion"",\n        default=""validation/main/loss"",\n        type=str,\n        nargs=""?"",\n        help=""Value to monitor to trigger an early stopping of the training"",\n    )\n    parser.add_argument(\n        ""--patience"",\n        default=3,\n        type=int,\n        nargs=""?"",\n        help=""Number of epochs ""\n        ""to wait without improvement before stopping the training"",\n    )\n    parser.add_argument(\n        ""--schedulers"",\n        default=None,\n        action=""append"",\n        type=lambda kv: kv.split(""=""),\n        help=""optimizer schedulers, you can configure params like:""\n        "" <optimizer-param>-<scheduler-name>-<schduler-param>""\n        \' e.g., ""--schedulers lr=noam --lr-noam-warmup 1000"".\',\n    )\n    parser.add_argument(\n        ""--gradclip"",\n        ""-c"",\n        type=float,\n        default=5,\n        help=""Gradient norm threshold to clip"",\n    )\n    parser.add_argument(\n        ""--maxlen"",\n        type=int,\n        default=40,\n        help=""Batch size is reduced if the input sequence > ML"",\n    )\n    parser.add_argument(\n        ""--model-module"",\n        type=str,\n        default=""default"",\n        help=""model defined module ""\n        ""(default: espnet.nets.xxx_backend.lm.default:DefaultRNNLM)"",\n    )\n    return parser\n\n\ndef main(cmd_args):\n    """"""Train LM.""""""\n    parser = get_parser()\n    args, _ = parser.parse_known_args(cmd_args)\n    if args.backend == ""chainer"" and args.train_dtype != ""float32"":\n        raise NotImplementedError(\n            f""chainer backend does not support --train-dtype {args.train_dtype}.""\n            ""Use --dtype float32.""\n        )\n    if args.ngpu == 0 and args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3"", ""float16""):\n        raise ValueError(\n            f""--train-dtype {args.train_dtype} does not support the CPU backend.""\n        )\n\n    # parse arguments dynamically\n    model_class = dynamic_import_lm(args.model_module, args.backend)\n    model_class.add_arguments(parser)\n    if args.schedulers is not None:\n        for k, v in args.schedulers:\n            scheduler_class = dynamic_import_scheduler(v)\n            scheduler_class.add_arguments(k, parser)\n\n    opt_class = dynamic_import_optimizer(args.opt, args.backend)\n    opt_class.add_arguments(parser)\n\n    args = parser.parse_args(cmd_args)\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # If --ngpu is not given,\n    #   1. if CUDA_VISIBLE_DEVICES is set, all visible devices\n    #   2. if nvidia-smi exists, use all devices\n    #   3. else ngpu=0\n    if args.ngpu is None:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is not None:\n            ngpu = len(cvd.split("",""))\n        else:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n            try:\n                p = subprocess.run(\n                    [""nvidia-smi"", ""-L""], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                ngpu = 0\n            else:\n                ngpu = len(p.stderr.decode().split(""\\n"")) - 1\n        args.ngpu = ngpu\n    else:\n        ngpu = args.ngpu\n    logging.info(f""ngpu: {ngpu}"")\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # seed setting\n    nseed = args.seed\n    random.seed(nseed)\n    np.random.seed(nseed)\n\n    # load dictionary\n    with open(args.dict, ""rb"") as f:\n        dictionary = f.readlines()\n    char_list = [entry.decode(""utf-8"").split("" "")[0] for entry in dictionary]\n    char_list.insert(0, ""<blank>"")\n    char_list.append(""<eos>"")\n    args.char_list_dict = {x: i for i, x in enumerate(char_list)}\n    args.n_vocab = len(char_list)\n\n    # train\n    logging.info(""backend = "" + args.backend)\n    if args.backend == ""chainer"":\n        from espnet.lm.chainer_backend.lm import train\n\n        train(args)\n    elif args.backend == ""pytorch"":\n        from espnet.lm.pytorch_backend.lm import train\n\n        train(args)\n    else:\n        raise ValueError(""Only chainer and pytorch are supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/mt_train.py,1,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Neural machine translation model training script.""""""\n\nimport logging\nimport os\nimport random\nimport subprocess\nimport sys\n\nfrom distutils.version import LooseVersion\n\nimport configargparse\nimport numpy as np\nimport torch\n\nfrom espnet.utils.cli_utils import strtobool\nfrom espnet.utils.training.batchfy import BATCH_COUNT_CHOICES\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(""1.2"")\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser(parser=None, required=True):\n    """"""Get default arguments.""""""\n    if parser is None:\n        parser = configargparse.ArgumentParser(\n            description=""Train a neural machine translation (NMT) model on one CPU, ""\n            ""one or multiple GPUs"",\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n        )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""second config file path that overwrites the settings in `--config`."",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""third config file path that overwrites the settings ""\n        ""in `--config` and `--config2`."",\n    )\n\n    parser.add_argument(\n        ""--ngpu"",\n        default=None,\n        type=int,\n        help=""Number of GPUs. If not given, use all visible devices"",\n    )\n    parser.add_argument(\n        ""--train-dtype"",\n        default=""float32"",\n        choices=[""float16"", ""float32"", ""float64"", ""O0"", ""O1"", ""O2"", ""O3""],\n        help=""Data type for training (only pytorch backend). ""\n        ""O0,O1,.. flags require apex. ""\n        ""See https://nvidia.github.io/apex/amp.html#opt-levels"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        default=""chainer"",\n        type=str,\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(\n        ""--outdir"", type=str, required=required, help=""Output directory""\n    )\n    parser.add_argument(""--debugmode"", default=1, type=int, help=""Debugmode"")\n    parser.add_argument(\n        ""--dict"", required=required, help=""Dictionary for source/target languages""\n    )\n    parser.add_argument(""--seed"", default=1, type=int, help=""Random seed"")\n    parser.add_argument(""--debugdir"", type=str, help=""Output directory for debugging"")\n    parser.add_argument(\n        ""--resume"",\n        ""-r"",\n        default="""",\n        nargs=""?"",\n        help=""Resume the training from snapshot"",\n    )\n    parser.add_argument(\n        ""--minibatches"",\n        ""-N"",\n        type=int,\n        default=""-1"",\n        help=""Process only N minibatches (for debug)"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--tensorboard-dir"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Tensorboard log dir path"",\n    )\n    parser.add_argument(\n        ""--report-interval-iters"",\n        default=100,\n        type=int,\n        help=""Report interval iterations"",\n    )\n    parser.add_argument(\n        ""--save-interval-iters"",\n        default=0,\n        type=int,\n        help=""Save snapshot interval iterations"",\n    )\n    # task related\n    parser.add_argument(\n        ""--train-json"",\n        type=str,\n        default=None,\n        help=""Filename of train label data (json)"",\n    )\n    parser.add_argument(\n        ""--valid-json"",\n        type=str,\n        default=None,\n        help=""Filename of validation label data (json)"",\n    )\n    # network architecture\n    parser.add_argument(\n        ""--model-module"",\n        type=str,\n        default=None,\n        help=""model defined module (default: espnet.nets.xxx_backend.e2e_mt:E2E)"",\n    )\n    # loss related\n    parser.add_argument(\n        ""--lsm-weight"", default=0.0, type=float, help=""Label smoothing weight""\n    )\n    # translations options to compute BLEU\n    parser.add_argument(\n        ""--report-bleu"",\n        default=True,\n        action=""store_true"",\n        help=""Compute BLEU on development set"",\n    )\n    parser.add_argument(""--nbest"", type=int, default=1, help=""Output N-best hypotheses"")\n    parser.add_argument(""--beam-size"", type=int, default=4, help=""Beam size"")\n    parser.add_argument(""--penalty"", default=0.0, type=float, help=""Incertion penalty"")\n    parser.add_argument(\n        ""--maxlenratio"",\n        default=0.0,\n        type=float,\n        help=""""""Input length ratio to obtain max output length.\n                        If maxlenratio=0.0 (default), it uses a end-detect function\n                        to automatically find maximum hypothesis lengths"""""",\n    )\n    parser.add_argument(\n        ""--minlenratio"",\n        default=0.0,\n        type=float,\n        help=""Input length ratio to obtain min output length"",\n    )\n    parser.add_argument(\n        ""--rnnlm"", type=str, default=None, help=""RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--rnnlm-conf"", type=str, default=None, help=""RNNLM model config file to read""\n    )\n    parser.add_argument(""--lm-weight"", default=0.0, type=float, help=""RNNLM weight."")\n    parser.add_argument(""--sym-space"", default=""<space>"", type=str, help=""Space symbol"")\n    parser.add_argument(""--sym-blank"", default=""<blank>"", type=str, help=""Blank symbol"")\n    # minibatch related\n    parser.add_argument(\n        ""--sortagrad"",\n        default=0,\n        type=int,\n        nargs=""?"",\n        help=""How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs"",\n    )\n    parser.add_argument(\n        ""--batch-count"",\n        default=""auto"",\n        choices=BATCH_COUNT_CHOICES,\n        help=""How to count batch_size. ""\n        ""The default (auto) will find how to count by args."",\n    )\n    parser.add_argument(\n        ""--batch-size"",\n        ""--batch-seqs"",\n        ""-b"",\n        default=0,\n        type=int,\n        help=""Maximum seqs in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-bins"",\n        default=0,\n        type=int,\n        help=""Maximum bins in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-in"",\n        default=0,\n        type=int,\n        help=""Maximum input frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-out"",\n        default=0,\n        type=int,\n        help=""Maximum output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-inout"",\n        default=0,\n        type=int,\n        help=""Maximum input+output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--maxlen-in"",\n        ""--batch-seq-maxlen-in"",\n        default=100,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, ""\n        ""batch size is reduced if the input sequence length > ML."",\n    )\n    parser.add_argument(\n        ""--maxlen-out"",\n        ""--batch-seq-maxlen-out"",\n        default=100,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, ""\n        ""batch size is reduced if the output sequence length > ML"",\n    )\n    parser.add_argument(\n        ""--n-iter-processes"",\n        default=0,\n        type=int,\n        help=""Number of processes of iterator"",\n    )\n    # optimization related\n    parser.add_argument(\n        ""--opt"",\n        default=""adadelta"",\n        type=str,\n        choices=[""adadelta"", ""adam"", ""noam""],\n        help=""Optimizer"",\n    )\n    parser.add_argument(\n        ""--accum-grad"", default=1, type=int, help=""Number of gradient accumuration""\n    )\n    parser.add_argument(\n        ""--eps"", default=1e-8, type=float, help=""Epsilon constant for optimizer""\n    )\n    parser.add_argument(\n        ""--eps-decay"", default=0.01, type=float, help=""Decaying ratio of epsilon""\n    )\n    parser.add_argument(\n        ""--lr"", default=1e-3, type=float, help=""Learning rate for optimizer""\n    )\n    parser.add_argument(\n        ""--lr-decay"", default=1.0, type=float, help=""Decaying ratio of learning rate""\n    )\n    parser.add_argument(\n        ""--weight-decay"", default=0.0, type=float, help=""Weight decay ratio""\n    )\n    parser.add_argument(\n        ""--criterion"",\n        default=""acc"",\n        type=str,\n        choices=[""loss"", ""acc""],\n        help=""Criterion to perform epsilon decay"",\n    )\n    parser.add_argument(\n        ""--threshold"", default=1e-4, type=float, help=""Threshold to stop iteration""\n    )\n    parser.add_argument(\n        ""--epochs"", ""-e"", default=30, type=int, help=""Maximum number of epochs""\n    )\n    parser.add_argument(\n        ""--early-stop-criterion"",\n        default=""validation/main/acc"",\n        type=str,\n        nargs=""?"",\n        help=""Value to monitor to trigger an early stopping of the training"",\n    )\n    parser.add_argument(\n        ""--patience"",\n        default=3,\n        type=int,\n        nargs=""?"",\n        help=""Number of epochs to wait ""\n        ""without improvement before stopping the training"",\n    )\n    parser.add_argument(\n        ""--grad-clip"", default=5, type=float, help=""Gradient norm threshold to clip""\n    )\n    parser.add_argument(\n        ""--num-save-attention"",\n        default=3,\n        type=int,\n        help=""Number of samples of attention to be saved"",\n    )\n    # decoder related\n    parser.add_argument(\n        ""--context-residual"",\n        default=False,\n        type=strtobool,\n        nargs=""?"",\n        help=""The flag to switch to use context vector residual in the decoder network"",\n    )\n    parser.add_argument(\n        ""--tie-src-tgt-embedding"",\n        default=False,\n        type=strtobool,\n        nargs=""?"",\n        help=""Tie parameters of source embedding and target embedding."",\n    )\n    parser.add_argument(\n        ""--tie-classifier"",\n        default=False,\n        type=strtobool,\n        nargs=""?"",\n        help=""Tie parameters of target embedding and output projection layer."",\n    )\n    # finetuning related\n    parser.add_argument(\n        ""--enc-init"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Pre-trained ASR model to initialize encoder."",\n    )\n    parser.add_argument(\n        ""--enc-init-mods"",\n        default=""enc.enc."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of encoder modules to initialize, separated by a comma."",\n    )\n    parser.add_argument(\n        ""--dec-init"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Pre-trained ASR, MT or LM model to initialize decoder."",\n    )\n    parser.add_argument(\n        ""--dec-init-mods"",\n        default=""att., dec."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of decoder modules to initialize, separated by a comma."",\n    )\n    # multilingual related\n    parser.add_argument(\n        ""--multilingual"",\n        default=False,\n        type=strtobool,\n        help=""Prepend target language ID to the source sentence. ""\n        ""Both source/target language IDs must be prepend in the pre-processing stage."",\n    )\n    parser.add_argument(\n        ""--replace-sos"",\n        default=False,\n        type=strtobool,\n        help=""Replace <sos> in the decoder with a target language ID ""\n        ""(the first token in the target sequence)"",\n    )\n\n    return parser\n\n\ndef main(cmd_args):\n    """"""Run the main training function.""""""\n    parser = get_parser()\n    args, _ = parser.parse_known_args(cmd_args)\n    if args.backend == ""chainer"" and args.train_dtype != ""float32"":\n        raise NotImplementedError(\n            f""chainer backend does not support --train-dtype {args.train_dtype}.""\n            ""Use --dtype float32.""\n        )\n    if args.ngpu == 0 and args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3"", ""float16""):\n        raise ValueError(\n            f""--train-dtype {args.train_dtype} does not support the CPU backend.""\n        )\n\n    from espnet.utils.dynamic_import import dynamic_import\n\n    if args.model_module is None:\n        model_module = ""espnet.nets."" + args.backend + ""_backend.e2e_mt:E2E""\n    else:\n        model_module = args.model_module\n    model_class = dynamic_import(model_module)\n    model_class.add_arguments(parser)\n\n    args = parser.parse_args(cmd_args)\n    args.model_module = model_module\n    if ""chainer_backend"" in args.model_module:\n        args.backend = ""chainer""\n    if ""pytorch_backend"" in args.model_module:\n        args.backend = ""pytorch""\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # If --ngpu is not given,\n    #   1. if CUDA_VISIBLE_DEVICES is set, all visible devices\n    #   2. if nvidia-smi exists, use all devices\n    #   3. else ngpu=0\n    if args.ngpu is None:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is not None:\n            ngpu = len(cvd.split("",""))\n        else:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n            try:\n                p = subprocess.run(\n                    [""nvidia-smi"", ""-L""], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                ngpu = 0\n            else:\n                ngpu = len(p.stderr.decode().split(""\\n"")) - 1\n        args.ngpu = ngpu\n    else:\n        if is_torch_1_2_plus and args.ngpu != 1:\n            logging.debug(\n                ""There are some bugs with multi-GPU processing in PyTorch 1.2+""\n                + "" (see https://github.com/pytorch/pytorch/issues/21108)""\n            )\n        ngpu = args.ngpu\n    logging.info(f""ngpu: {ngpu}"")\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # set random seed\n    logging.info(""random seed = %d"" % args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    # load dictionary for debug log\n    if args.dict is not None:\n        with open(args.dict, ""rb"") as f:\n            dictionary = f.readlines()\n        char_list = [entry.decode(""utf-8"").split("" "")[0] for entry in dictionary]\n        char_list.insert(0, ""<blank>"")\n        char_list.append(""<eos>"")\n        args.char_list = char_list\n    else:\n        args.char_list = None\n\n    # train\n    logging.info(""backend = "" + args.backend)\n\n    if args.backend == ""pytorch"":\n        from espnet.mt.pytorch_backend.mt import train\n\n        train(args)\n    else:\n        raise ValueError(""Only pytorch are supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/mt_trans.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Neural machine translation model decoding script.""""""\n\nimport configargparse\nimport logging\nimport os\nimport random\nimport sys\n\nimport numpy as np\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser():\n    """"""Get default arguments.""""""\n    parser = configargparse.ArgumentParser(\n        description=""Translate text from speech ""\n        ""using a speech translation model on one CPU or GPU"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""Config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""Second config file path that overwrites the settings in `--config`"",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""Third config file path ""\n        ""that overwrites the settings in `--config` and `--config2`"",\n    )\n\n    parser.add_argument(""--ngpu"", type=int, default=0, help=""Number of GPUs"")\n    parser.add_argument(\n        ""--dtype"",\n        choices=(""float16"", ""float32"", ""float64""),\n        default=""float32"",\n        help=""Float precision (only available in --api v2)"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        type=str,\n        default=""chainer"",\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(""--debugmode"", type=int, default=1, help=""Debugmode"")\n    parser.add_argument(""--seed"", type=int, default=1, help=""Random seed"")\n    parser.add_argument(""--verbose"", ""-V"", type=int, default=1, help=""Verbose option"")\n    parser.add_argument(\n        ""--batchsize"",\n        type=int,\n        default=1,\n        help=""Batch size for beam search (0: means no batch processing)"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""--api"",\n        default=""v1"",\n        choices=[""v1"", ""v2""],\n        help=""Beam search APIs ""\n        ""v1: Default API. It only supports ""\n        ""the ASRInterface.recognize method and DefaultRNNLM. ""\n        ""v2: Experimental API. ""\n        ""It supports any models that implements ScorerInterface."",\n    )\n    # task related\n    parser.add_argument(\n        ""--trans-json"", type=str, help=""Filename of translation data (json)""\n    )\n    parser.add_argument(\n        ""--result-label"",\n        type=str,\n        required=True,\n        help=""Filename of result label data (json)"",\n    )\n    # model (parameter) related\n    parser.add_argument(\n        ""--model"", type=str, required=True, help=""Model file parameters to read""\n    )\n    parser.add_argument(\n        ""--model-conf"", type=str, default=None, help=""Model config file""\n    )\n    # search related\n    parser.add_argument(""--nbest"", type=int, default=1, help=""Output N-best hypotheses"")\n    parser.add_argument(""--beam-size"", type=int, default=1, help=""Beam size"")\n    parser.add_argument(""--penalty"", type=float, default=0.1, help=""Incertion penalty"")\n    parser.add_argument(\n        ""--maxlenratio"",\n        type=float,\n        default=3.0,\n        help=""""""Input length ratio to obtain max output length.\n                        If maxlenratio=0.0 (default), it uses a end-detect function\n                        to automatically find maximum hypothesis lengths"""""",\n    )\n    parser.add_argument(\n        ""--minlenratio"",\n        type=float,\n        default=0.0,\n        help=""Input length ratio to obtain min output length"",\n    )\n    # rnnlm related\n    parser.add_argument(\n        ""--rnnlm"", type=str, default=None, help=""RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--rnnlm-conf"", type=str, default=None, help=""RNNLM model config file to read""\n    )\n    parser.add_argument(""--lm-weight"", type=float, default=0.0, help=""RNNLM weight"")\n    # multilingual related\n    parser.add_argument(\n        ""--tgt-lang"",\n        default=False,\n        type=str,\n        help=""target language ID (e.g., <en>, <de>, and <fr> etc.)"",\n    )\n    return parser\n\n\ndef main(args):\n    """"""Run the main decoding function.""""""\n    parser = get_parser()\n    args = parser.parse_args(args)\n\n    # logging info\n    if args.verbose == 1:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    elif args.verbose == 2:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check CUDA_VISIBLE_DEVICES\n    if args.ngpu > 0:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is None:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n        elif args.ngpu != len(cvd.split("","")):\n            logging.error(""#gpus is not matched with CUDA_VISIBLE_DEVICES."")\n            sys.exit(1)\n\n        # TODO(mn5k): support of multiple GPUs\n        if args.ngpu > 1:\n            logging.error(""The program only supports ngpu=1."")\n            sys.exit(1)\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # seed setting\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    logging.info(""set random seed = %d"" % args.seed)\n\n    # trans\n    logging.info(""backend = "" + args.backend)\n    if args.backend == ""pytorch"":\n        # Experimental API that supports custom LMs\n        from espnet.mt.pytorch_backend.mt import trans\n\n        if args.dtype != ""float32"":\n            raise NotImplementedError(\n                f""`--dtype {args.dtype}` is only available with `--api v2`""\n            )\n        trans(args)\n    else:\n        raise ValueError(""Only pytorch are supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/st_train.py,1,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""End-to-end speech translation model training script.""""""\n\nfrom distutils.version import LooseVersion\nimport logging\nimport os\nimport random\nimport subprocess\nimport sys\n\nimport configargparse\nimport numpy as np\nimport torch\n\nfrom espnet.utils.cli_utils import strtobool\nfrom espnet.utils.training.batchfy import BATCH_COUNT_CHOICES\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(""1.2"")\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser(parser=None, required=True):\n    """"""Get default arguments.""""""\n    if parser is None:\n        parser = configargparse.ArgumentParser(\n            description=""Train a speech translation (ST) model on one CPU, ""\n            ""one or multiple GPUs"",\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n        )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""second config file path that overwrites the settings in `--config`."",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""third config file path that overwrites the settings ""\n        ""in `--config` and `--config2`."",\n    )\n\n    parser.add_argument(\n        ""--ngpu"",\n        default=None,\n        type=int,\n        help=""Number of GPUs. If not given, use all visible devices"",\n    )\n    parser.add_argument(\n        ""--train-dtype"",\n        default=""float32"",\n        choices=[""float16"", ""float32"", ""float64"", ""O0"", ""O1"", ""O2"", ""O3""],\n        help=""Data type for training (only pytorch backend). ""\n        ""O0,O1,.. flags require apex. ""\n        ""See https://nvidia.github.io/apex/amp.html#opt-levels"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        default=""chainer"",\n        type=str,\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(\n        ""--outdir"", type=str, required=required, help=""Output directory""\n    )\n    parser.add_argument(""--debugmode"", default=1, type=int, help=""Debugmode"")\n    parser.add_argument(""--dict"", required=required, help=""Dictionary"")\n    parser.add_argument(""--seed"", default=1, type=int, help=""Random seed"")\n    parser.add_argument(""--debugdir"", type=str, help=""Output directory for debugging"")\n    parser.add_argument(\n        ""--resume"",\n        ""-r"",\n        default="""",\n        nargs=""?"",\n        help=""Resume the training from snapshot"",\n    )\n    parser.add_argument(\n        ""--minibatches"",\n        ""-N"",\n        type=int,\n        default=""-1"",\n        help=""Process only N minibatches (for debug)"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--tensorboard-dir"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Tensorboard log dir path"",\n    )\n    parser.add_argument(\n        ""--report-interval-iters"",\n        default=100,\n        type=int,\n        help=""Report interval iterations"",\n    )\n    parser.add_argument(\n        ""--save-interval-iters"",\n        default=0,\n        type=int,\n        help=""Save snapshot interval iterations"",\n    )\n    # task related\n    parser.add_argument(\n        ""--train-json"",\n        type=str,\n        default=None,\n        help=""Filename of train label data (json)"",\n    )\n    parser.add_argument(\n        ""--valid-json"",\n        type=str,\n        default=None,\n        help=""Filename of validation label data (json)"",\n    )\n    # network architecture\n    parser.add_argument(\n        ""--model-module"",\n        type=str,\n        default=None,\n        help=""model defined module (default: espnet.nets.xxx_backend.e2e_st:E2E)"",\n    )\n    # loss related\n    parser.add_argument(\n        ""--ctc_type"",\n        default=""warpctc"",\n        type=str,\n        choices=[""builtin"", ""warpctc""],\n        help=""Type of CTC implementation to calculate loss."",\n    )\n    parser.add_argument(\n        ""--mtlalpha"",\n        default=0.0,\n        type=float,\n        help=""Multitask learning coefficient, alpha: \\\n                                alpha*ctc_loss + (1-alpha)*att_loss"",\n    )\n    parser.add_argument(\n        ""--asr-weight"",\n        default=0.0,\n        type=float,\n        help=""Multitask learning coefficient for ASR task, weight: ""\n        "" asr_weight*(alpha*ctc_loss + (1-alpha)*att_loss)""\n        "" + (1-asr_weight-mt_weight)*st_loss"",\n    )\n    parser.add_argument(\n        ""--mt-weight"",\n        default=0.0,\n        type=float,\n        help=""Multitask learning coefficient for MT task, weight: \\\n                                mt_weight*mt_loss + (1-mt_weight-asr_weight)*st_loss"",\n    )\n    parser.add_argument(\n        ""--lsm-weight"", default=0.0, type=float, help=""Label smoothing weight""\n    )\n    # recognition options to compute CER/WER\n    parser.add_argument(\n        ""--report-cer"",\n        default=False,\n        action=""store_true"",\n        help=""Compute CER on development set"",\n    )\n    parser.add_argument(\n        ""--report-wer"",\n        default=False,\n        action=""store_true"",\n        help=""Compute WER on development set"",\n    )\n    # translations options to compute BLEU\n    parser.add_argument(\n        ""--report-bleu"",\n        default=True,\n        action=""store_true"",\n        help=""Compute BLEU on development set"",\n    )\n    parser.add_argument(""--nbest"", type=int, default=1, help=""Output N-best hypotheses"")\n    parser.add_argument(""--beam-size"", type=int, default=4, help=""Beam size"")\n    parser.add_argument(""--penalty"", default=0.0, type=float, help=""Incertion penalty"")\n    parser.add_argument(\n        ""--maxlenratio"",\n        default=0.0,\n        type=float,\n        help=""""""Input length ratio to obtain max output length.\n                        If maxlenratio=0.0 (default), it uses a end-detect function\n                        to automatically find maximum hypothesis lengths"""""",\n    )\n    parser.add_argument(\n        ""--minlenratio"",\n        default=0.0,\n        type=float,\n        help=""Input length ratio to obtain min output length"",\n    )\n    parser.add_argument(\n        ""--rnnlm"", type=str, default=None, help=""RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--rnnlm-conf"", type=str, default=None, help=""RNNLM model config file to read""\n    )\n    parser.add_argument(""--lm-weight"", default=0.0, type=float, help=""RNNLM weight."")\n    parser.add_argument(""--sym-space"", default=""<space>"", type=str, help=""Space symbol"")\n    parser.add_argument(""--sym-blank"", default=""<blank>"", type=str, help=""Blank symbol"")\n    # minibatch related\n    parser.add_argument(\n        ""--sortagrad"",\n        default=0,\n        type=int,\n        nargs=""?"",\n        help=""How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs"",\n    )\n    parser.add_argument(\n        ""--batch-count"",\n        default=""auto"",\n        choices=BATCH_COUNT_CHOICES,\n        help=""How to count batch_size. ""\n        ""The default (auto) will find how to count by args."",\n    )\n    parser.add_argument(\n        ""--batch-size"",\n        ""--batch-seqs"",\n        ""-b"",\n        default=0,\n        type=int,\n        help=""Maximum seqs in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-bins"",\n        default=0,\n        type=int,\n        help=""Maximum bins in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-in"",\n        default=0,\n        type=int,\n        help=""Maximum input frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-out"",\n        default=0,\n        type=int,\n        help=""Maximum output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-inout"",\n        default=0,\n        type=int,\n        help=""Maximum input+output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--maxlen-in"",\n        ""--batch-seq-maxlen-in"",\n        default=800,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, batch size is reduced ""\n        ""if the input sequence length > ML."",\n    )\n    parser.add_argument(\n        ""--maxlen-out"",\n        ""--batch-seq-maxlen-out"",\n        default=150,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, ""\n        ""batch size is reduced if the output sequence length > ML"",\n    )\n    parser.add_argument(\n        ""--n-iter-processes"",\n        default=0,\n        type=int,\n        help=""Number of processes of iterator"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""The configuration file for the pre-processing"",\n    )\n    # optimization related\n    parser.add_argument(\n        ""--opt"",\n        default=""adadelta"",\n        type=str,\n        choices=[""adadelta"", ""adam"", ""noam""],\n        help=""Optimizer"",\n    )\n    parser.add_argument(\n        ""--accum-grad"", default=1, type=int, help=""Number of gradient accumuration""\n    )\n    parser.add_argument(\n        ""--eps"", default=1e-8, type=float, help=""Epsilon constant for optimizer""\n    )\n    parser.add_argument(\n        ""--eps-decay"", default=0.01, type=float, help=""Decaying ratio of epsilon""\n    )\n    parser.add_argument(\n        ""--lr"", default=1e-3, type=float, help=""Learning rate for optimizer""\n    )\n    parser.add_argument(\n        ""--lr-decay"", default=1.0, type=float, help=""Decaying ratio of learning rate""\n    )\n    parser.add_argument(\n        ""--weight-decay"", default=0.0, type=float, help=""Weight decay ratio""\n    )\n    parser.add_argument(\n        ""--criterion"",\n        default=""acc"",\n        type=str,\n        choices=[""loss"", ""acc""],\n        help=""Criterion to perform epsilon decay"",\n    )\n    parser.add_argument(\n        ""--threshold"", default=1e-4, type=float, help=""Threshold to stop iteration""\n    )\n    parser.add_argument(\n        ""--epochs"", ""-e"", default=30, type=int, help=""Maximum number of epochs""\n    )\n    parser.add_argument(\n        ""--early-stop-criterion"",\n        default=""validation/main/acc"",\n        type=str,\n        nargs=""?"",\n        help=""Value to monitor to trigger an early stopping of the training"",\n    )\n    parser.add_argument(\n        ""--patience"",\n        default=3,\n        type=int,\n        nargs=""?"",\n        help=""Number of epochs to wait ""\n        ""without improvement before stopping the training"",\n    )\n    parser.add_argument(\n        ""--grad-clip"", default=5, type=float, help=""Gradient norm threshold to clip""\n    )\n    parser.add_argument(\n        ""--num-save-attention"",\n        default=3,\n        type=int,\n        help=""Number of samples of attention to be saved"",\n    )\n    parser.add_argument(\n        ""--grad-noise"",\n        type=strtobool,\n        default=False,\n        help=""The flag to switch to use noise injection to gradients during training"",\n    )\n    # speech translation related\n    parser.add_argument(\n        ""--context-residual"",\n        default=False,\n        type=strtobool,\n        nargs=""?"",\n        help=""The flag to switch to use context vector residual in the decoder network"",\n    )\n    # finetuning related\n    parser.add_argument(\n        ""--enc-init"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Pre-trained ASR model to initialize encoder."",\n    )\n    parser.add_argument(\n        ""--enc-init-mods"",\n        default=""enc.enc."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of encoder modules to initialize, separated by a comma."",\n    )\n    parser.add_argument(\n        ""--dec-init"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Pre-trained ASR, MT or LM model to initialize decoder."",\n    )\n    parser.add_argument(\n        ""--dec-init-mods"",\n        default=""att., dec."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of decoder modules to initialize, separated by a comma."",\n    )\n    # multilingual related\n    parser.add_argument(\n        ""--multilingual"",\n        default=False,\n        type=strtobool,\n        help=""Prepend target language ID to the source sentence. ""\n        "" Both source/target language IDs must be prepend in the pre-processing stage."",\n    )\n    parser.add_argument(\n        ""--replace-sos"",\n        default=False,\n        type=strtobool,\n        help=""Replace <sos> in the decoder with a target language ID \\\n                              (the first token in the target sequence)"",\n    )\n    # Feature transform: Normalization\n    parser.add_argument(\n        ""--stats-file"",\n        type=str,\n        default=None,\n        help=""The stats file for the feature normalization"",\n    )\n    parser.add_argument(\n        ""--apply-uttmvn"",\n        type=strtobool,\n        default=True,\n        help=""Apply utterance level mean "" ""variance normalization."",\n    )\n    parser.add_argument(""--uttmvn-norm-means"", type=strtobool, default=True, help="""")\n    parser.add_argument(""--uttmvn-norm-vars"", type=strtobool, default=False, help="""")\n    # Feature transform: Fbank\n    parser.add_argument(\n        ""--fbank-fs"",\n        type=int,\n        default=16000,\n        help=""The sample frequency used for "" ""the mel-fbank creation."",\n    )\n    parser.add_argument(\n        ""--n-mels"", type=int, default=80, help=""The number of mel-frequency bins.""\n    )\n    parser.add_argument(""--fbank-fmin"", type=float, default=0.0, help="""")\n    parser.add_argument(""--fbank-fmax"", type=float, default=None, help="""")\n    return parser\n\n\ndef main(cmd_args):\n    """"""Run the main training function.""""""\n    parser = get_parser()\n    args, _ = parser.parse_known_args(cmd_args)\n    if args.backend == ""chainer"" and args.train_dtype != ""float32"":\n        raise NotImplementedError(\n            f""chainer backend does not support --train-dtype {args.train_dtype}.""\n            ""Use --dtype float32.""\n        )\n    if args.ngpu == 0 and args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3"", ""float16""):\n        raise ValueError(\n            f""--train-dtype {args.train_dtype} does not support the CPU backend.""\n        )\n\n    from espnet.utils.dynamic_import import dynamic_import\n\n    if args.model_module is None:\n        model_module = ""espnet.nets."" + args.backend + ""_backend.e2e_st:E2E""\n    else:\n        model_module = args.model_module\n    model_class = dynamic_import(model_module)\n    model_class.add_arguments(parser)\n\n    args = parser.parse_args(cmd_args)\n    args.model_module = model_module\n    if ""chainer_backend"" in args.model_module:\n        args.backend = ""chainer""\n    if ""pytorch_backend"" in args.model_module:\n        args.backend = ""pytorch""\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # If --ngpu is not given,\n    #   1. if CUDA_VISIBLE_DEVICES is set, all visible devices\n    #   2. if nvidia-smi exists, use all devices\n    #   3. else ngpu=0\n    if args.ngpu is None:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is not None:\n            ngpu = len(cvd.split("",""))\n        else:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n            try:\n                p = subprocess.run(\n                    [""nvidia-smi"", ""-L""], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                ngpu = 0\n            else:\n                ngpu = len(p.stderr.decode().split(""\\n"")) - 1\n        args.ngpu = ngpu\n    else:\n        if is_torch_1_2_plus and args.ngpu != 1:\n            logging.debug(\n                ""There are some bugs with multi-GPU processing in PyTorch 1.2+""\n                + "" (see https://github.com/pytorch/pytorch/issues/21108)""\n            )\n        ngpu = args.ngpu\n    logging.info(f""ngpu: {ngpu}"")\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # set random seed\n    logging.info(""random seed = %d"" % args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    # load dictionary for debug log\n    if args.dict is not None:\n        with open(args.dict, ""rb"") as f:\n            dictionary = f.readlines()\n        char_list = [entry.decode(""utf-8"").split("" "")[0] for entry in dictionary]\n        char_list.insert(0, ""<blank>"")\n        char_list.append(""<eos>"")\n        args.char_list = char_list\n    else:\n        args.char_list = None\n\n    # train\n    logging.info(""backend = "" + args.backend)\n\n    if args.backend == ""pytorch"":\n        from espnet.st.pytorch_backend.st import train\n\n        train(args)\n    else:\n        raise ValueError(""Only pytorch are supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/st_trans.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""End-to-end speech translation model decoding script.""""""\n\nimport logging\nimport os\nimport random\nimport sys\n\nimport configargparse\nimport numpy as np\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser():\n    """"""Get default arguments.""""""\n    parser = configargparse.ArgumentParser(\n        description=""Translate text from speech using a speech translation ""\n        ""model on one CPU or GPU"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""Config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""Second config file path that overwrites the settings in `--config`"",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""Third config file path that overwrites ""\n        ""the settings in `--config` and `--config2`"",\n    )\n\n    parser.add_argument(""--ngpu"", type=int, default=0, help=""Number of GPUs"")\n    parser.add_argument(\n        ""--dtype"",\n        choices=(""float16"", ""float32"", ""float64""),\n        default=""float32"",\n        help=""Float precision (only available in --api v2)"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        type=str,\n        default=""chainer"",\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(""--debugmode"", type=int, default=1, help=""Debugmode"")\n    parser.add_argument(""--seed"", type=int, default=1, help=""Random seed"")\n    parser.add_argument(""--verbose"", ""-V"", type=int, default=1, help=""Verbose option"")\n    parser.add_argument(\n        ""--batchsize"",\n        type=int,\n        default=1,\n        help=""Batch size for beam search (0: means no batch processing)"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""--api"",\n        default=""v1"",\n        choices=[""v1"", ""v2""],\n        help=""Beam search APIs ""\n        ""v1: Default API. ""\n        ""It only supports the ASRInterface.recognize method and DefaultRNNLM. ""\n        ""v2: Experimental API. ""\n        ""It supports any models that implements ScorerInterface."",\n    )\n    # task related\n    parser.add_argument(\n        ""--trans-json"", type=str, help=""Filename of translation data (json)""\n    )\n    parser.add_argument(\n        ""--result-label"",\n        type=str,\n        required=True,\n        help=""Filename of result label data (json)"",\n    )\n    # model (parameter) related\n    parser.add_argument(\n        ""--model"", type=str, required=True, help=""Model file parameters to read""\n    )\n    # search related\n    parser.add_argument(""--nbest"", type=int, default=1, help=""Output N-best hypotheses"")\n    parser.add_argument(""--beam-size"", type=int, default=1, help=""Beam size"")\n    parser.add_argument(""--penalty"", type=float, default=0.0, help=""Incertion penalty"")\n    parser.add_argument(\n        ""--maxlenratio"",\n        type=float,\n        default=0.0,\n        help=""""""Input length ratio to obtain max output length.\n                        If maxlenratio=0.0 (default), it uses a end-detect function\n                        to automatically find maximum hypothesis lengths"""""",\n    )\n    parser.add_argument(\n        ""--minlenratio"",\n        type=float,\n        default=0.0,\n        help=""Input length ratio to obtain min output length"",\n    )\n    # rnnlm related\n    parser.add_argument(\n        ""--rnnlm"", type=str, default=None, help=""RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--rnnlm-conf"", type=str, default=None, help=""RNNLM model config file to read""\n    )\n    parser.add_argument(\n        ""--word-rnnlm"", type=str, default=None, help=""Word RNNLM model file to read""\n    )\n    parser.add_argument(\n        ""--word-rnnlm-conf"",\n        type=str,\n        default=None,\n        help=""Word RNNLM model config file to read"",\n    )\n    parser.add_argument(""--word-dict"", type=str, default=None, help=""Word list to read"")\n    parser.add_argument(""--lm-weight"", type=float, default=0.1, help=""RNNLM weight"")\n    # multilingual related\n    parser.add_argument(\n        ""--tgt-lang"",\n        default=False,\n        type=str,\n        help=""target language ID (e.g., <en>, <de>, and <fr> etc.)"",\n    )\n    return parser\n\n\ndef main(args):\n    """"""Run the main decoding function.""""""\n    parser = get_parser()\n    args = parser.parse_args(args)\n\n    # logging info\n    if args.verbose == 1:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    elif args.verbose == 2:\n        logging.basicConfig(\n            level=logging.DEBUG,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check CUDA_VISIBLE_DEVICES\n    if args.ngpu > 0:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is None:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n        elif args.ngpu != len(cvd.split("","")):\n            logging.error(""#gpus is not matched with CUDA_VISIBLE_DEVICES."")\n            sys.exit(1)\n\n        # TODO(mn5k): support of multiple GPUs\n        if args.ngpu > 1:\n            logging.error(""The program only supports ngpu=1."")\n            sys.exit(1)\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # seed setting\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    logging.info(""set random seed = %d"" % args.seed)\n\n    # validate rnn options\n    if args.rnnlm is not None and args.word_rnnlm is not None:\n        logging.error(\n            ""It seems that both --rnnlm and --word-rnnlm are specified. ""\n            ""Please use either option.""\n        )\n        sys.exit(1)\n\n    # trans\n    logging.info(""backend = "" + args.backend)\n    if args.backend == ""pytorch"":\n        # Experimental API that supports custom LMs\n        from espnet.st.pytorch_backend.st import trans\n\n        if args.dtype != ""float32"":\n            raise NotImplementedError(\n                f""`--dtype {args.dtype}` is only available with `--api v2`""\n            )\n        trans(args)\n    else:\n        raise ValueError(""Only pytorch are supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/tts_decode.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""TTS decoding script.""""""\n\nimport configargparse\nimport logging\nimport os\nimport platform\nimport subprocess\nimport sys\n\nfrom espnet.utils.cli_utils import strtobool\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser():\n    """"""Get parser of decoding arguments.""""""\n    parser = configargparse.ArgumentParser(\n        description=""Synthesize speech from text using a TTS model on one CPU"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""second config file path that overwrites the settings in `--config`."",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""third config file path that overwrites ""\n        ""the settings in `--config` and `--config2`."",\n    )\n\n    parser.add_argument(""--ngpu"", default=0, type=int, help=""Number of GPUs"")\n    parser.add_argument(\n        ""--backend"",\n        default=""pytorch"",\n        type=str,\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(""--debugmode"", default=1, type=int, help=""Debugmode"")\n    parser.add_argument(""--seed"", default=1, type=int, help=""Random seed"")\n    parser.add_argument(""--out"", type=str, required=True, help=""Output filename"")\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    # task related\n    parser.add_argument(\n        ""--json"", type=str, required=True, help=""Filename of train label data (json)""\n    )\n    parser.add_argument(\n        ""--model"", type=str, required=True, help=""Model file parameters to read""\n    )\n    parser.add_argument(\n        ""--model-conf"", type=str, default=None, help=""Model config file""\n    )\n    # decoding related\n    parser.add_argument(\n        ""--maxlenratio"", type=float, default=5, help=""Maximum length ratio in decoding""\n    )\n    parser.add_argument(\n        ""--minlenratio"", type=float, default=0, help=""Minimum length ratio in decoding""\n    )\n    parser.add_argument(\n        ""--threshold"", type=float, default=0.5, help=""Threshold value in decoding""\n    )\n    parser.add_argument(\n        ""--use-att-constraint"",\n        type=strtobool,\n        default=False,\n        help=""Whether to use the attention constraint"",\n    )\n    parser.add_argument(\n        ""--backward-window"",\n        type=int,\n        default=1,\n        help=""Backward window size in the attention constraint"",\n    )\n    parser.add_argument(\n        ""--forward-window"",\n        type=int,\n        default=3,\n        help=""Forward window size in the attention constraint"",\n    )\n    parser.add_argument(\n        ""--fastspeech-alpha"",\n        type=float,\n        default=1.0,\n        help=""Alpha to change the speed for FastSpeech"",\n    )\n    # save related\n    parser.add_argument(\n        ""--save-durations"",\n        default=False,\n        type=strtobool,\n        help=""Whether to save durations converted from attentions"",\n    )\n    parser.add_argument(\n        ""--save-focus-rates"",\n        default=False,\n        type=strtobool,\n        help=""Whether to save focus rates of attentions"",\n    )\n    return parser\n\n\ndef main(args):\n    """"""Run deocding.""""""\n    parser = get_parser()\n    args = parser.parse_args(args)\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check CUDA_VISIBLE_DEVICES\n    if args.ngpu > 0:\n        # python 2 case\n        if platform.python_version_tuple()[0] == ""2"":\n            if ""clsp.jhu.edu"" in subprocess.check_output([""hostname"", ""-f""]):\n                cvd = subprocess.check_output(\n                    [""/usr/local/bin/free-gpu"", ""-n"", str(args.ngpu)]\n                ).strip()\n                logging.info(""CLSP: use gpu"" + cvd)\n                os.environ[""CUDA_VISIBLE_DEVICES""] = cvd\n        # python 3 case\n        else:\n            if ""clsp.jhu.edu"" in subprocess.check_output([""hostname"", ""-f""]).decode():\n                cvd = (\n                    subprocess.check_output(\n                        [""/usr/local/bin/free-gpu"", ""-n"", str(args.ngpu)]\n                    )\n                    .decode()\n                    .strip()\n                )\n                logging.info(""CLSP: use gpu"" + cvd)\n                os.environ[""CUDA_VISIBLE_DEVICES""] = cvd\n\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is None:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n        elif args.ngpu != len(cvd.split("","")):\n            logging.error(""#gpus is not matched with CUDA_VISIBLE_DEVICES."")\n            sys.exit(1)\n\n    # display PYTHONPATH\n    logging.info(""python path = "" + os.environ.get(""PYTHONPATH"", ""(None)""))\n\n    # extract\n    logging.info(""backend = "" + args.backend)\n    if args.backend == ""pytorch"":\n        from espnet.tts.pytorch_backend.tts import decode\n\n        decode(args)\n    else:\n        raise NotImplementedError(""Only pytorch is supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/bin/tts_train.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Text-to-speech model training script.""""""\n\nimport logging\nimport os\nimport random\nimport subprocess\nimport sys\n\nimport configargparse\nimport numpy as np\n\nfrom espnet.nets.tts_interface import TTSInterface\nfrom espnet.utils.cli_utils import strtobool\nfrom espnet.utils.training.batchfy import BATCH_COUNT_CHOICES\n\n\n# NOTE: you need this func to generate our sphinx doc\ndef get_parser():\n    """"""Get parser of training arguments.""""""\n    parser = configargparse.ArgumentParser(\n        description=""Train a new text-to-speech (TTS) model on one CPU, ""\n        ""one or multiple GPUs"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    # general configuration\n    parser.add(""--config"", is_config_file=True, help=""config file path"")\n    parser.add(\n        ""--config2"",\n        is_config_file=True,\n        help=""second config file path that overwrites the settings in `--config`."",\n    )\n    parser.add(\n        ""--config3"",\n        is_config_file=True,\n        help=""third config file path that overwrites ""\n        ""the settings in `--config` and `--config2`."",\n    )\n\n    parser.add_argument(\n        ""--ngpu"",\n        default=None,\n        type=int,\n        help=""Number of GPUs. If not given, use all visible devices"",\n    )\n    parser.add_argument(\n        ""--backend"",\n        default=""pytorch"",\n        type=str,\n        choices=[""chainer"", ""pytorch""],\n        help=""Backend library"",\n    )\n    parser.add_argument(""--outdir"", type=str, required=True, help=""Output directory"")\n    parser.add_argument(""--debugmode"", default=1, type=int, help=""Debugmode"")\n    parser.add_argument(""--seed"", default=1, type=int, help=""Random seed"")\n    parser.add_argument(\n        ""--resume"",\n        ""-r"",\n        default="""",\n        type=str,\n        nargs=""?"",\n        help=""Resume the training from snapshot"",\n    )\n    parser.add_argument(\n        ""--minibatches"",\n        ""-N"",\n        type=int,\n        default=""-1"",\n        help=""Process only N minibatches (for debug)"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--tensorboard-dir"",\n        default=None,\n        type=str,\n        nargs=""?"",\n        help=""Tensorboard log directory path"",\n    )\n    parser.add_argument(\n        ""--eval-interval-epochs"", default=1, type=int, help=""Evaluation interval epochs""\n    )\n    parser.add_argument(\n        ""--save-interval-epochs"", default=1, type=int, help=""Save interval epochs""\n    )\n    parser.add_argument(\n        ""--report-interval-iters"",\n        default=100,\n        type=int,\n        help=""Report interval iterations"",\n    )\n    # task related\n    parser.add_argument(\n        ""--train-json"", type=str, required=True, help=""Filename of training json""\n    )\n    parser.add_argument(\n        ""--valid-json"", type=str, required=True, help=""Filename of validation json""\n    )\n    # network architecture\n    parser.add_argument(\n        ""--model-module"",\n        type=str,\n        default=""espnet.nets.pytorch_backend.e2e_tts_tacotron2:Tacotron2"",\n        help=""model defined module"",\n    )\n    # minibatch related\n    parser.add_argument(\n        ""--sortagrad"",\n        default=0,\n        type=int,\n        nargs=""?"",\n        help=""How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs"",\n    )\n    parser.add_argument(\n        ""--batch-sort-key"",\n        default=""shuffle"",\n        type=str,\n        choices=[""shuffle"", ""output"", ""input""],\n        nargs=""?"",\n        help=\'Batch sorting key. ""shuffle"" only work with --batch-count ""seq"".\',\n    )\n    parser.add_argument(\n        ""--batch-count"",\n        default=""auto"",\n        choices=BATCH_COUNT_CHOICES,\n        help=""How to count batch_size. ""\n        ""The default (auto) will find how to count by args."",\n    )\n    parser.add_argument(\n        ""--batch-size"",\n        ""--batch-seqs"",\n        ""-b"",\n        default=0,\n        type=int,\n        help=""Maximum seqs in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-bins"",\n        default=0,\n        type=int,\n        help=""Maximum bins in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-in"",\n        default=0,\n        type=int,\n        help=""Maximum input frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-out"",\n        default=0,\n        type=int,\n        help=""Maximum output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--batch-frames-inout"",\n        default=0,\n        type=int,\n        help=""Maximum input+output frames in a minibatch (0 to disable)"",\n    )\n    parser.add_argument(\n        ""--maxlen-in"",\n        ""--batch-seq-maxlen-in"",\n        default=100,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, ""\n        ""batch size is reduced if the input sequence length > ML."",\n    )\n    parser.add_argument(\n        ""--maxlen-out"",\n        ""--batch-seq-maxlen-out"",\n        default=200,\n        type=int,\n        metavar=""ML"",\n        help=""When --batch-count=seq, ""\n        ""batch size is reduced if the output sequence length > ML"",\n    )\n    parser.add_argument(\n        ""--num-iter-processes"",\n        default=0,\n        type=int,\n        help=""Number of processes of iterator"",\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""--use-speaker-embedding"",\n        default=False,\n        type=strtobool,\n        help=""Whether to use speaker embedding"",\n    )\n    parser.add_argument(\n        ""--use-second-target"",\n        default=False,\n        type=strtobool,\n        help=""Whether to use second target"",\n    )\n    # optimization related\n    parser.add_argument(\n        ""--opt"", default=""adam"", type=str, choices=[""adam"", ""noam""], help=""Optimizer""\n    )\n    parser.add_argument(\n        ""--accum-grad"", default=1, type=int, help=""Number of gradient accumuration""\n    )\n    parser.add_argument(\n        ""--lr"", default=1e-3, type=float, help=""Learning rate for optimizer""\n    )\n    parser.add_argument(""--eps"", default=1e-6, type=float, help=""Epsilon for optimizer"")\n    parser.add_argument(\n        ""--weight-decay"",\n        default=1e-6,\n        type=float,\n        help=""Weight decay coefficient for optimizer"",\n    )\n    parser.add_argument(\n        ""--epochs"", ""-e"", default=30, type=int, help=""Number of maximum epochs""\n    )\n    parser.add_argument(\n        ""--early-stop-criterion"",\n        default=""validation/main/loss"",\n        type=str,\n        nargs=""?"",\n        help=""Value to monitor to trigger an early stopping of the training"",\n    )\n    parser.add_argument(\n        ""--patience"",\n        default=3,\n        type=int,\n        nargs=""?"",\n        help=""Number of epochs to wait ""\n        ""without improvement before stopping the training"",\n    )\n    parser.add_argument(\n        ""--grad-clip"", default=1, type=float, help=""Gradient norm threshold to clip""\n    )\n    parser.add_argument(\n        ""--num-save-attention"",\n        default=5,\n        type=int,\n        help=""Number of samples of attention to be saved"",\n    )\n    parser.add_argument(\n        ""--keep-all-data-on-mem"",\n        default=False,\n        type=strtobool,\n        help=""Whether to keep all data on memory"",\n    )\n    # finetuning related\n    parser.add_argument(\n        ""--enc-init"",\n        default=None,\n        type=str,\n        help=""Pre-trained TTS model path to initialize encoder."",\n    )\n    parser.add_argument(\n        ""--enc-init-mods"",\n        default=""enc."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of encoder modules to initialize, separated by a comma."",\n    )\n    parser.add_argument(\n        ""--dec-init"",\n        default=None,\n        type=str,\n        help=""Pre-trained TTS model path to initialize decoder."",\n    )\n    parser.add_argument(\n        ""--dec-init-mods"",\n        default=""dec."",\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of decoder modules to initialize, separated by a comma."",\n    )\n    parser.add_argument(\n        ""--freeze-mods"",\n        default=None,\n        type=lambda s: [str(mod) for mod in s.split("","") if s != """"],\n        help=""List of modules to freeze (not to train), separated by a comma."",\n    )\n\n    return parser\n\n\ndef main(cmd_args):\n    """"""Run training.""""""\n    parser = get_parser()\n    args, _ = parser.parse_known_args(cmd_args)\n\n    from espnet.utils.dynamic_import import dynamic_import\n\n    model_class = dynamic_import(args.model_module)\n    assert issubclass(model_class, TTSInterface)\n    model_class.add_arguments(parser)\n    args = parser.parse_args(cmd_args)\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # If --ngpu is not given,\n    #   1. if CUDA_VISIBLE_DEVICES is set, all visible devices\n    #   2. if nvidia-smi exists, use all devices\n    #   3. else ngpu=0\n    if args.ngpu is None:\n        cvd = os.environ.get(""CUDA_VISIBLE_DEVICES"")\n        if cvd is not None:\n            ngpu = len(cvd.split("",""))\n        else:\n            logging.warning(""CUDA_VISIBLE_DEVICES is not set."")\n            try:\n                p = subprocess.run(\n                    [""nvidia-smi"", ""-L""], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n                )\n            except (subprocess.CalledProcessError, FileNotFoundError):\n                ngpu = 0\n            else:\n                ngpu = len(p.stderr.decode().split(""\\n"")) - 1\n        args.ngpu = ngpu\n    else:\n        ngpu = args.ngpu\n    logging.info(f""ngpu: {ngpu}"")\n\n    # set random seed\n    logging.info(""random seed = %d"" % args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    if args.backend == ""pytorch"":\n        from espnet.tts.pytorch_backend.tts import train\n\n        train(args)\n    else:\n        raise NotImplementedError(""Only pytorch is supported."")\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
espnet/lm/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/lm/lm_utils.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n# This code is ported from the following implementation written in Torch.\n# https://github.com/chainer/chainer/blob/master/examples/ptb/train_ptb_custom_loop.py\n\nimport chainer\nimport h5py\nimport logging\nimport numpy as np\nimport os\nimport random\nimport six\nfrom tqdm import tqdm\n\nfrom chainer.training import extension\n\n\ndef load_dataset(path, label_dict, outdir=None):\n    """"""Load and save HDF5 that contains a dataset and stats for LM\n\n    Args:\n        path (str): The path of an input text dataset file\n        label_dict (dict[str, int]):\n            dictionary that maps token label string to its ID number\n        outdir (str): The path of an output dir\n\n    Returns:\n        tuple[list[np.ndarray], int, int]: Tuple of\n            token IDs in np.int32 converted by `read_tokens`\n            the number of tokens by `count_tokens`,\n            and the number of OOVs by `count_tokens`\n    """"""\n    if outdir is not None:\n        os.makedirs(outdir, exist_ok=True)\n        filename = outdir + ""/"" + os.path.basename(path) + "".h5""\n        if os.path.exists(filename):\n            logging.info(f""loading binary dataset: {filename}"")\n            f = h5py.File(filename, ""r"")\n            return f[""data""][:], f[""n_tokens""][()], f[""n_oovs""][()]\n    else:\n        logging.info(""skip dump/load HDF5 because the output dir is not specified"")\n    logging.info(f""reading text dataset: {path}"")\n    ret = read_tokens(path, label_dict)\n    n_tokens, n_oovs = count_tokens(ret, label_dict[""<unk>""])\n    if outdir is not None:\n        logging.info(f""saving binary dataset: {filename}"")\n        with h5py.File(filename, ""w"") as f:\n            # http://docs.h5py.org/en/stable/special.html#arbitrary-vlen-data\n            data = f.create_dataset(\n                ""data"", (len(ret),), dtype=h5py.special_dtype(vlen=np.int32)\n            )\n            data[:] = ret\n            f[""n_tokens""] = n_tokens\n            f[""n_oovs""] = n_oovs\n    return ret, n_tokens, n_oovs\n\n\ndef read_tokens(filename, label_dict):\n    """"""Read tokens as a sequence of sentences\n\n    :param str filename : The name of the input file\n    :param dict label_dict : dictionary that maps token label string to its ID number\n    :return list of ID sequences\n    :rtype list\n    """"""\n\n    data = []\n    unk = label_dict[""<unk>""]\n    for ln in tqdm(open(filename, ""r"", encoding=""utf-8"")):\n        data.append(\n            np.array(\n                [label_dict.get(label, unk) for label in ln.split()], dtype=np.int32\n            )\n        )\n    return data\n\n\ndef count_tokens(data, unk_id=None):\n    """"""Count tokens and oovs in token ID sequences.\n\n    Args:\n        data (list[np.ndarray]): list of token ID sequences\n        unk_id (int): ID of unknown token\n\n    Returns:\n        tuple: tuple of number of token occurrences and number of oov tokens\n\n    """"""\n\n    n_tokens = 0\n    n_oovs = 0\n    for sentence in data:\n        n_tokens += len(sentence)\n        if unk_id is not None:\n            n_oovs += np.count_nonzero(sentence == unk_id)\n    return n_tokens, n_oovs\n\n\ndef compute_perplexity(result):\n    """"""Computes and add the perplexity to the LogReport\n\n    :param dict result: The current observations\n    """"""\n    # Routine to rewrite the result dictionary of LogReport to add perplexity values\n    result[""perplexity""] = np.exp(result[""main/loss""] / result[""main/count""])\n    if ""validation/main/loss"" in result:\n        result[""val_perplexity""] = np.exp(result[""validation/main/loss""])\n\n\nclass ParallelSentenceIterator(chainer.dataset.Iterator):\n    """"""Dataset iterator to create a batch of sentences.\n\n       This iterator returns a pair of sentences, where one token is shifted\n       between the sentences like \'<sos> w1 w2 w3\' and \'w1 w2 w3 <eos>\'\n       Sentence batches are made in order of longer sentences, and then\n       randomly shuffled.\n    """"""\n\n    def __init__(\n        self, dataset, batch_size, max_length=0, sos=0, eos=0, repeat=True, shuffle=True\n    ):\n        self.dataset = dataset\n        self.batch_size = batch_size  # batch size\n        # Number of completed sweeps over the dataset. In this case, it is\n        # incremented if every word is visited at least once after the last\n        # increment.\n        self.epoch = 0\n        # True if the epoch is incremented at the last iteration.\n        self.is_new_epoch = False\n        self.repeat = repeat\n        length = len(dataset)\n        self.batch_indices = []\n        # make mini-batches\n        if batch_size > 1:\n            indices = sorted(range(len(dataset)), key=lambda i: -len(dataset[i]))\n            bs = 0\n            while bs < length:\n                be = min(bs + batch_size, length)\n                # batch size is automatically reduced if the sentence length\n                # is larger than max_length\n                if max_length > 0:\n                    sent_length = len(dataset[indices[bs]])\n                    be = min(\n                        be, bs + max(batch_size // (sent_length // max_length + 1), 1)\n                    )\n                self.batch_indices.append(np.array(indices[bs:be]))\n                bs = be\n            if shuffle:\n                # shuffle batches\n                random.shuffle(self.batch_indices)\n        else:\n            self.batch_indices = [np.array([i]) for i in six.moves.range(length)]\n\n        # NOTE: this is not a count of parameter updates. It is just a count of\n        # calls of ``__next__``.\n        self.iteration = 0\n        self.sos = sos\n        self.eos = eos\n        # use -1 instead of None internally\n        self._previous_epoch_detail = -1.0\n\n    def __next__(self):\n        # This iterator returns a list representing a mini-batch. Each item\n        # indicates a sentence pair like \'<sos> w1 w2 w3\' and \'w1 w2 w3 <eos>\'\n        # represented by token IDs.\n        n_batches = len(self.batch_indices)\n        if not self.repeat and self.iteration >= n_batches:\n            # If not self.repeat, this iterator stops at the end of the first\n            # epoch (i.e., when all words are visited once).\n            raise StopIteration\n\n        batch = []\n        for idx in self.batch_indices[self.iteration % n_batches]:\n            batch.append(\n                (\n                    np.append([self.sos], self.dataset[idx]),\n                    np.append(self.dataset[idx], [self.eos]),\n                )\n            )\n\n        self._previous_epoch_detail = self.epoch_detail\n        self.iteration += 1\n\n        epoch = self.iteration // n_batches\n        self.is_new_epoch = self.epoch < epoch\n        if self.is_new_epoch:\n            self.epoch = epoch\n\n        return batch\n\n    def start_shuffle(self):\n        random.shuffle(self.batch_indices)\n\n    @property\n    def epoch_detail(self):\n        # Floating point version of epoch.\n        return self.iteration / len(self.batch_indices)\n\n    @property\n    def previous_epoch_detail(self):\n        if self._previous_epoch_detail < 0:\n            return None\n        return self._previous_epoch_detail\n\n    def serialize(self, serializer):\n        # It is important to serialize the state to be recovered on resume.\n        self.iteration = serializer(""iteration"", self.iteration)\n        self.epoch = serializer(""epoch"", self.epoch)\n        try:\n            self._previous_epoch_detail = serializer(\n                ""previous_epoch_detail"", self._previous_epoch_detail\n            )\n        except KeyError:\n            # guess previous_epoch_detail for older version\n            self._previous_epoch_detail = self.epoch + (\n                self.current_position - 1\n            ) / len(self.batch_indices)\n            if self.epoch_detail > 0:\n                self._previous_epoch_detail = max(self._previous_epoch_detail, 0.0)\n            else:\n                self._previous_epoch_detail = -1.0\n\n\nclass MakeSymlinkToBestModel(extension.Extension):\n    """"""Extension that makes a symbolic link to the best model\n\n    :param str key: Key of value\n    :param str prefix: Prefix of model files and link target\n    :param str suffix: Suffix of link target\n    """"""\n\n    def __init__(self, key, prefix=""model"", suffix=""best""):\n        super(MakeSymlinkToBestModel, self).__init__()\n        self.best_model = -1\n        self.min_loss = 0.0\n        self.key = key\n        self.prefix = prefix\n        self.suffix = suffix\n\n    def __call__(self, trainer):\n        observation = trainer.observation\n        if self.key in observation:\n            loss = observation[self.key]\n            if self.best_model == -1 or loss < self.min_loss:\n                self.min_loss = loss\n                self.best_model = trainer.updater.epoch\n                src = ""%s.%d"" % (self.prefix, self.best_model)\n                dest = os.path.join(trainer.out, ""%s.%s"" % (self.prefix, self.suffix))\n                if os.path.lexists(dest):\n                    os.remove(dest)\n                os.symlink(src, dest)\n                logging.info(""best model is "" + src)\n\n    def serialize(self, serializer):\n        if isinstance(serializer, chainer.serializer.Serializer):\n            serializer(""_best_model"", self.best_model)\n            serializer(""_min_loss"", self.min_loss)\n            serializer(""_key"", self.key)\n            serializer(""_prefix"", self.prefix)\n            serializer(""_suffix"", self.suffix)\n        else:\n            self.best_model = serializer(""_best_model"", -1)\n            self.min_loss = serializer(""_min_loss"", 0.0)\n            self.key = serializer(""_key"", """")\n            self.prefix = serializer(""_prefix"", ""model"")\n            self.suffix = serializer(""_suffix"", ""best"")\n\n\n# TODO(Hori): currently it only works with character-word level LM.\n#             need to consider any types of subwords-to-word mapping.\ndef make_lexical_tree(word_dict, subword_dict, word_unk):\n    """"""Make a lexical tree to compute word-level probabilities\n\n    """"""\n    # node [dict(subword_id -> node), word_id, word_set[start-1, end]]\n    root = [{}, -1, None]\n    for w, wid in word_dict.items():\n        if wid > 0 and wid != word_unk:  # skip <blank> and <unk>\n            if True in [c not in subword_dict for c in w]:  # skip unknown subword\n                continue\n            succ = root[0]  # get successors from root node\n            for i, c in enumerate(w):\n                cid = subword_dict[c]\n                if cid not in succ:  # if next node does not exist, make a new node\n                    succ[cid] = [{}, -1, (wid - 1, wid)]\n                else:\n                    prev = succ[cid][2]\n                    succ[cid][2] = (min(prev[0], wid - 1), max(prev[1], wid))\n                if i == len(w) - 1:  # if word end, set word id\n                    succ[cid][1] = wid\n                succ = succ[cid][0]  # move to the child successors\n    return root\n'"
espnet/mt/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/mt/mt_utils.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Utility funcitons for the text translation task.""""""\n\nimport logging\n\n\n# * ------------------ recognition related ------------------ *\ndef parse_hypothesis(hyp, char_list):\n    """"""Parse hypothesis.\n\n    :param list hyp: recognition hypothesis\n    :param list char_list: list of characters\n    :return: recognition text string\n    :return: recognition token string\n    :return: recognition tokenid string\n    """"""\n    # remove sos and get results\n    tokenid_as_list = list(map(int, hyp[""yseq""][1:]))\n    token_as_list = [char_list[idx] for idx in tokenid_as_list]\n    score = float(hyp[""score""])\n\n    # convert to string\n    tokenid = "" "".join([str(idx) for idx in tokenid_as_list])\n    token = "" "".join(token_as_list)\n    text = """".join(token_as_list).replace(""<space>"", "" "")\n\n    return text, token, tokenid, score\n\n\ndef add_results_to_json(js, nbest_hyps, char_list):\n    """"""Add N-best results to json.\n\n    :param dict js: groundtruth utterance dict\n    :param list nbest_hyps: list of hypothesis\n    :param list char_list: list of characters\n    :return: N-best results added utterance dict\n    """"""\n    # copy old json info\n    new_js = dict()\n    if ""utt2spk"" in js.keys():\n        new_js[""utt2spk""] = js[""utt2spk""]\n    new_js[""output""] = []\n\n    for n, hyp in enumerate(nbest_hyps, 1):\n        # parse hypothesis\n        rec_text, rec_token, rec_tokenid, score = parse_hypothesis(hyp, char_list)\n\n        # copy ground-truth\n        if len(js[""output""]) > 0:\n            out_dic = dict(js[""output""][0].items())\n        else:\n            out_dic = {""name"": """"}\n\n        # update name\n        out_dic[""name""] += ""[%d]"" % n\n\n        # add recognition results\n        out_dic[""rec_text""] = rec_text\n        out_dic[""rec_token""] = rec_token\n        out_dic[""rec_tokenid""] = rec_tokenid\n        out_dic[""score""] = score\n\n        # add source reference\n        out_dic[""text_src""] = js[""output""][1][""text""]\n        out_dic[""token_src""] = js[""output""][1][""token""]\n        out_dic[""tokenid_src""] = js[""output""][1][""tokenid""]\n\n        # add to list of N-best result dicts\n        new_js[""output""].append(out_dic)\n\n        # show 1-best result\n        if n == 1:\n            if ""text"" in out_dic.keys():\n                logging.info(""groundtruth: %s"" % out_dic[""text""])\n            logging.info(""prediction : %s"" % out_dic[""rec_text""])\n            logging.info(""source : %s"" % out_dic[""token_src""])\n\n    return new_js\n'"
espnet/nets/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/asr_interface.py,8,"b'""""""ASR Interface module.""""""\nimport argparse\n\nfrom espnet.bin.asr_train import get_parser\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass ASRInterface:\n    """"""ASR Interface for ESPnet model implementation.""""""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments to parser.""""""\n        return parser\n\n    @classmethod\n    def build(cls, idim: int, odim: int, **kwargs):\n        """"""Initialize this class with python-level args.\n\n        Args:\n            idim (int): The number of an input feature dim.\n            odim (int): The number of output vocab.\n\n        Returns:\n            ASRinterface: A new instance of ASRInterface.\n\n        """"""\n\n        def wrap(parser):\n            return get_parser(parser, required=False)\n\n        args = argparse.Namespace(**kwargs)\n        args = fill_missing_args(args, wrap)\n        args = fill_missing_args(args, cls.add_arguments)\n        return cls(idim, odim, args)\n\n    def forward(self, xs, ilens, ys):\n        """"""Compute loss for training.\n\n        :param xs:\n            For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim)\n            For chainer, list of source sequences chainer.Variable\n        :param ilens: batch of lengths of source sequences (B)\n            For pytorch, torch.Tensor\n            For chainer, list of int\n        :param ys:\n            For pytorch, batch of padded source sequences torch.Tensor (B, Lmax)\n            For chainer, list of source sequences chainer.Variable\n        :return: loss value\n        :rtype: torch.Tensor for pytorch, chainer.Variable for chainer\n        """"""\n        raise NotImplementedError(""forward method is not implemented"")\n\n    def recognize(self, x, recog_args, char_list=None, rnnlm=None):\n        """"""Recognize x for evaluation.\n\n        :param ndarray x: input acouctic feature (B, T, D) or (T, D)\n        :param namespace recog_args: argment namespace contraining options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        raise NotImplementedError(""recognize method is not implemented"")\n\n    def recognize_batch(self, x, recog_args, char_list=None, rnnlm=None):\n        """"""Beam search implementation for batch.\n\n        :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc)\n        :param namespace recog_args: argument namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        raise NotImplementedError(""Batch decoding is not supported yet."")\n\n    def calculate_all_attentions(self, xs, ilens, ys):\n        """"""Caluculate attention.\n\n        :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...]\n        :param ndarray ilens: batch of lengths of input sequences (B)\n        :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...]\n        :return: attention weights (B, Lmax, Tmax)\n        :rtype: float ndarray\n        """"""\n        raise NotImplementedError(""calculate_all_attentions method is not implemented"")\n\n    @property\n    def attention_plot_class(self):\n        """"""Get attention plot class.""""""\n        from espnet.asr.asr_utils import PlotAttentionReport\n\n        return PlotAttentionReport\n\n    def encode(self, feat):\n        """"""Encode feature in `beam_search` (optional).\n\n        Args:\n            x (numpy.ndarray): input feature (T, D)\n        Returns:\n            torch.Tensor for pytorch, chainer.Variable for chainer:\n                encoded feature (T, D)\n\n        """"""\n        raise NotImplementedError(""encode method is not implemented"")\n\n    def scorers(self):\n        """"""Get scorers for `beam_search` (optional).\n\n        Returns:\n            dict[str, ScorerInterface]: dict of `ScorerInterface` objects\n\n        """"""\n        raise NotImplementedError(""decoders method is not implemented"")\n\n\npredefined_asr = {\n    ""pytorch"": {\n        ""rnn"": ""espnet.nets.pytorch_backend.e2e_asr:E2E"",\n        ""transformer"": ""espnet.nets.pytorch_backend.e2e_asr_transformer:E2E"",\n    },\n    ""chainer"": {\n        ""rnn"": ""espnet.nets.chainer_backend.e2e_asr:E2E"",\n        ""transformer"": ""espnet.nets.chainer_backend.e2e_asr_transformer:E2E"",\n    },\n}\n\n\ndef dynamic_import_asr(module, backend):\n    """"""Import ASR models dynamically.\n\n    Args:\n        module (str): module_name:class_name or alias in `predefined_asr`\n        backend (str): NN backend. e.g., pytorch, chainer\n\n    Returns:\n        type: ASR class\n\n    """"""\n    model_class = dynamic_import(module, predefined_asr.get(backend, dict()))\n    assert issubclass(\n        model_class, ASRInterface\n    ), f""{module} does not implement ASRInterface""\n    return model_class\n'"
espnet/nets/batch_beam_search.py,27,"b'""""""Parallel beam search module.""""""\n\nimport logging\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import NamedTuple\nfrom typing import Tuple\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom espnet.nets.beam_search import BeamSearch\nfrom espnet.nets.beam_search import Hypothesis\n\n\nclass BatchHypothesis(NamedTuple):\n    """"""Batchfied/Vectorized hypothesis data type.""""""\n\n    yseq: torch.Tensor = torch.tensor([])  # (batch, maxlen)\n    score: torch.Tensor = torch.tensor([])  # (batch,)\n    length: torch.Tensor = torch.tensor([])  # (batch,)\n    scores: Dict[str, torch.Tensor] = dict()  # values: (batch,)\n    states: Dict[str, Dict] = dict()\n\n    def __len__(self) -> int:\n        """"""Return a batch size.""""""\n        return len(self.length)\n\n\nclass BatchBeamSearch(BeamSearch):\n    """"""Batch beam search implementation.""""""\n\n    def batchfy(self, hyps: List[Hypothesis]) -> BatchHypothesis:\n        """"""Convert list to batch.""""""\n        if len(hyps) == 0:\n            return BatchHypothesis()\n        return BatchHypothesis(\n            yseq=pad_sequence(\n                [h.yseq for h in hyps], batch_first=True, padding_value=self.eos\n            ),\n            length=torch.tensor([len(h.yseq) for h in hyps], dtype=torch.int64),\n            score=torch.tensor([h.score for h in hyps]),\n            scores={k: torch.tensor([h.scores[k] for h in hyps]) for k in self.scorers},\n            states={k: [h.states[k] for h in hyps] for k in self.scorers},\n        )\n\n    def _batch_select(self, hyps: BatchHypothesis, ids: List[int]) -> BatchHypothesis:\n        return BatchHypothesis(\n            yseq=hyps.yseq[ids],\n            score=hyps.score[ids],\n            length=hyps.length[ids],\n            scores={k: v[ids] for k, v in hyps.scores.items()},\n            states={\n                k: [self.scorers[k].select_state(v, i) for i in ids]\n                for k, v in hyps.states.items()\n            },\n        )\n\n    def _select(self, hyps: BatchHypothesis, i: int) -> Hypothesis:\n        return Hypothesis(\n            yseq=hyps.yseq[i, : hyps.length[i]],\n            score=hyps.score[i],\n            scores={k: v[i] for k, v in hyps.scores.items()},\n            states={\n                k: self.scorers[k].select_state(v, i) for k, v in hyps.states.items()\n            },\n        )\n\n    def unbatchfy(self, batch_hyps: BatchHypothesis) -> List[Hypothesis]:\n        """"""Revert batch to list.""""""\n        return [\n            Hypothesis(\n                yseq=batch_hyps.yseq[i][: batch_hyps.length[i]],\n                score=batch_hyps.score[i],\n                scores={k: batch_hyps.scores[k][i] for k in self.scorers},\n                states={\n                    k: v.select_state(batch_hyps.states[k], i)\n                    for k, v in self.scorers.items()\n                },\n            )\n            for i in range(len(batch_hyps.length))\n        ]\n\n    def batch_beam(\n        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        """"""Batch-compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n                Its shape is `(n_beam, self.vocab_size)`.\n            ids (torch.Tensor): The partial token ids to compute topk.\n                Its shape is `(n_beam, self.pre_beam_size)`.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n                The topk full (prev_hyp, new_token) ids\n                and partial (prev_hyp, new_token) ids.\n                Their shapes are all `(self.beam_size,)`\n\n        """"""\n        if not self.do_pre_beam:\n            top_ids = weighted_scores.view(-1).topk(self.beam_size)[1]\n            # Because of the flatten above, `top_ids` is organized as:\n            # [hyp1 * V + token1, hyp2 * V + token2, ..., hypK * V + tokenK],\n            # where V is `self.n_vocab` and K is `self.beam_size`\n            prev_hyp_ids = top_ids // self.n_vocab\n            new_token_ids = top_ids % self.n_vocab\n            return prev_hyp_ids, new_token_ids, prev_hyp_ids, new_token_ids\n\n        raise NotImplementedError(\n            ""batch decoding with PartialScorer is not supported yet.""\n        )\n\n    def init_hyp(self, x: torch.Tensor) -> BatchHypothesis:\n        """"""Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        """"""\n        return self.batchfy(super().init_hyp(x))\n\n    def score_full(\n        self, hyp: BatchHypothesis, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        """"""Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        """"""\n        scores = dict()\n        states = dict()\n        for k, d in self.full_scorers.items():\n            scores[k], states[k] = d.batch_score(hyp.yseq, hyp.states[k], x)\n        return scores, states\n\n    def search(self, running_hyps: BatchHypothesis, x: torch.Tensor) -> BatchHypothesis:\n        """"""Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (BatchHypothesis): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            BatchHypothesis: Best sorted hypotheses\n\n        """"""\n        n_batch = len(running_hyps)\n\n        # batch scoring\n        scores, states = self.score_full(running_hyps, x.expand(n_batch, *x.shape))\n        if self.do_pre_beam:\n            part_ids = torch.topk(\n                scores[self.pre_beam_score_key], self.pre_beam_size, dim=-1\n            )[1]\n        else:\n            part_ids = torch.arange(self.n_vocab, device=x.device).expand(\n                n_batch, self.n_vocab\n            )\n        part_scores, part_states = self.score_partial(running_hyps, part_ids, x)\n\n        # weighted sum scores\n        weighted_scores = torch.zeros(\n            n_batch, self.n_vocab, dtype=x.dtype, device=x.device\n        )\n        for k in self.full_scorers:\n            weighted_scores += self.weights[k] * scores[k]\n        for k in self.part_scorers:\n            weighted_scores[part_ids] += self.weights[k] * part_scores[k]\n        weighted_scores += running_hyps.score.to(\n            dtype=x.dtype, device=x.device\n        ).unsqueeze(1)\n\n        # TODO(karita): do not use list. use batch instead\n        # see also https://github.com/espnet/espnet/pull/1402#discussion_r354561029\n        # update hyps\n        best_hyps = []\n        prev_hyps = self.unbatchfy(running_hyps)\n        for (\n            full_prev_hyp_id,\n            full_new_token_id,\n            part_prev_hyp_id,\n            part_new_token_id,\n        ) in zip(*self.batch_beam(weighted_scores, part_ids)):\n            prev_hyp = prev_hyps[full_prev_hyp_id]\n            best_hyps.append(\n                Hypothesis(\n                    score=weighted_scores[full_prev_hyp_id, full_new_token_id],\n                    yseq=self.append_token(prev_hyp.yseq, full_new_token_id),\n                    scores=self.merge_scores(\n                        prev_hyp.scores,\n                        {k: v[full_prev_hyp_id] for k, v in scores.items()},\n                        full_new_token_id,\n                        {k: v[part_prev_hyp_id] for k, v in part_scores.items()},\n                        part_new_token_id,\n                    ),\n                    states=self.merge_states(\n                        {\n                            k: self.full_scorers[k].select_state(v, full_prev_hyp_id)\n                            for k, v in states.items()\n                        },\n                        {\n                            k: self.part_scorers[k].select_state(v, part_prev_hyp_id)\n                            for k, v in part_states.items()\n                        },\n                        part_new_token_id,\n                    ),\n                )\n            )\n        return self.batchfy(best_hyps)\n\n    def post_process(\n        self,\n        i: int,\n        maxlen: int,\n        maxlenratio: float,\n        running_hyps: BatchHypothesis,\n        ended_hyps: List[Hypothesis],\n    ) -> BatchHypothesis:\n        """"""Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (BatchHypothesis): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            BatchHypothesis: The new running hypotheses.\n\n        """"""\n        n_batch, maxlen = running_hyps.yseq.shape\n        logging.debug(f""the number of running hypothes: {n_batch}"")\n        if self.token_list is not None:\n            logging.debug(\n                ""best hypo: ""\n                + """".join(\n                    [\n                        self.token_list[x]\n                        for x in running_hyps.yseq[0, 1 : running_hyps.length[0]]\n                    ]\n                )\n            )\n        # add eos in the final loop to avoid that there are no ended hyps\n        if i == maxlen - 1:\n            logging.info(""adding <eos> in the last position in the loop"")\n            running_hyps.yseq.resize_(n_batch, maxlen + 1)\n            running_hyps.yseq[:, -1] = self.eos\n            running_hyps.yseq.index_fill_(1, running_hyps.length, self.eos)\n\n        # add ended hypotheses to a final list, and removed them from current hypotheses\n        # (this will be a probmlem, number of hyps < beam)\n        is_eos = (\n            running_hyps.yseq[torch.arange(n_batch), running_hyps.length - 1]\n            == self.eos\n        )\n        for b in torch.nonzero(is_eos).view(-1):\n            hyp = self._select(running_hyps, b)\n            ended_hyps.append(hyp)\n        remained_ids = torch.nonzero(is_eos == 0).view(-1)\n        return self._batch_select(running_hyps, remained_ids)\n'"
espnet/nets/beam_search.py,42,"b'""""""Beam search module.""""""\n\nfrom itertools import chain\nimport logging\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import NamedTuple\nfrom typing import Tuple\n\nimport torch\n\nfrom espnet.nets.e2e_asr_common import end_detect\nfrom espnet.nets.scorer_interface import PartialScorerInterface\nfrom espnet.nets.scorer_interface import ScorerInterface\n\n\nclass Hypothesis(NamedTuple):\n    """"""Hypothesis data type.""""""\n\n    yseq: torch.Tensor\n    score: float = 0\n    scores: Dict[str, float] = dict()\n    states: Dict[str, Dict] = dict()\n\n    def asdict(self) -> dict:\n        """"""Convert data to JSON-friendly dict.""""""\n        return self._replace(\n            yseq=self.yseq.tolist(),\n            score=float(self.score),\n            scores={k: float(v) for k, v in self.scores.items()},\n        )._asdict()\n\n\nclass BeamSearch(torch.nn.Module):\n    """"""Beam search implementation.""""""\n\n    def __init__(\n        self,\n        scorers: Dict[str, ScorerInterface],\n        weights: Dict[str, float],\n        beam_size: int,\n        vocab_size: int,\n        sos: int,\n        eos: int,\n        token_list: List[str] = None,\n        pre_beam_ratio: float = 1.5,\n        pre_beam_score_key: str = None,\n    ):\n        """"""Initialize beam search.\n\n        Args:\n            scorers (dict[str, ScorerInterface]): Dict of decoder modules\n                e.g., Decoder, CTCPrefixScorer, LM\n                The scorer will be ignored if it is `None`\n            weights (dict[str, float]): Dict of weights for each scorers\n                The scorer will be ignored if its weight is 0\n            beam_size (int): The number of hypotheses kept during search\n            vocab_size (int): The number of vocabulary\n            sos (int): Start of sequence id\n            eos (int): End of sequence id\n            token_list (list[str]): List of tokens for debug log\n            pre_beam_score_key (str): key of scores to perform pre-beam search\n            pre_beam_ratio (float): beam size in the pre-beam search\n                will be `int(pre_beam_ratio * beam_size)`\n\n        """"""\n        super().__init__()\n        # set scorers\n        self.weights = weights\n        self.scorers = dict()\n        self.full_scorers = dict()\n        self.part_scorers = dict()\n        # this module dict is required for recursive cast\n        # `self.to(device, dtype)` in `recog.py`\n        self.nn_dict = torch.nn.ModuleDict()\n        for k, v in scorers.items():\n            w = weights.get(k, 0)\n            if w == 0 or v is None:\n                continue\n            assert isinstance(\n                v, ScorerInterface\n            ), f""{k} ({type(v)}) does not implement ScorerInterface""\n            self.scorers[k] = v\n            if isinstance(v, PartialScorerInterface):\n                self.part_scorers[k] = v\n            else:\n                self.full_scorers[k] = v\n            if isinstance(v, torch.nn.Module):\n                self.nn_dict[k] = v\n\n        # set configurations\n        self.sos = sos\n        self.eos = eos\n        self.token_list = token_list\n        self.pre_beam_size = int(pre_beam_ratio * beam_size)\n        self.beam_size = beam_size\n        self.n_vocab = vocab_size\n        if (\n            pre_beam_score_key is not None\n            and pre_beam_score_key not in self.full_scorers\n        ):\n            raise KeyError(f""{pre_beam_score_key} is not found in {self.full_scorers}"")\n        self.pre_beam_score_key = pre_beam_score_key\n        self.do_pre_beam = (\n            self.pre_beam_score_key is not None\n            and self.pre_beam_size < self.n_vocab\n            and len(self.part_scorers) > 0\n        )\n\n    def init_hyp(self, x: torch.Tensor) -> List[Hypothesis]:\n        """"""Get an initial hypothesis data.\n\n        Args:\n            x (torch.Tensor): The encoder output feature\n\n        Returns:\n            Hypothesis: The initial hypothesis.\n\n        """"""\n        init_states = dict()\n        init_scores = dict()\n        for k, d in self.scorers.items():\n            init_states[k] = d.init_state(x)\n            init_scores[k] = 0.0\n        return [\n            Hypothesis(\n                score=0.0,\n                scores=init_scores,\n                states=init_states,\n                yseq=torch.tensor([self.sos], device=x.device),\n            )\n        ]\n\n    @staticmethod\n    def append_token(xs: torch.Tensor, x: int) -> torch.Tensor:\n        """"""Append new token to prefix tokens.\n\n        Args:\n            xs (torch.Tensor): The prefix token\n            x (int): The new token to append\n\n        Returns:\n            torch.Tensor: New tensor contains: xs + [x] with xs.dtype and xs.device\n\n        """"""\n        x = torch.tensor([x], dtype=xs.dtype, device=xs.device)\n        return torch.cat((xs, x))\n\n    def score_full(\n        self, hyp: Hypothesis, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        """"""Score new hypothesis by `self.full_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.full_scorers`\n                and tensor score values of shape: `(self.n_vocab,)`,\n                and state dict that has string keys\n                and state values of `self.full_scorers`\n\n        """"""\n        scores = dict()\n        states = dict()\n        for k, d in self.full_scorers.items():\n            scores[k], states[k] = d.score(hyp.yseq, hyp.states[k], x)\n        return scores, states\n\n    def score_partial(\n        self, hyp: Hypothesis, ids: torch.Tensor, x: torch.Tensor\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n        """"""Score new hypothesis by `self.part_scorers`.\n\n        Args:\n            hyp (Hypothesis): Hypothesis with prefix tokens to score\n            ids (torch.Tensor): 1D tensor of new partial tokens to score\n            x (torch.Tensor): Corresponding input feature\n\n        Returns:\n            Tuple[Dict[str, torch.Tensor], Dict[str, Any]]: Tuple of\n                score dict of `hyp` that has string keys of `self.part_scorers`\n                and tensor score values of shape: `(len(ids),)`,\n                and state dict that has string keys\n                and state values of `self.part_scorers`\n\n        """"""\n        scores = dict()\n        states = dict()\n        for k, d in self.part_scorers.items():\n            scores[k], states[k] = d.score_partial(hyp.yseq, ids, hyp.states[k], x)\n        return scores, states\n\n    def beam(\n        self, weighted_scores: torch.Tensor, ids: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Compute topk full token ids and partial token ids.\n\n        Args:\n            weighted_scores (torch.Tensor): The weighted sum scores for each tokens.\n            Its shape is `(self.n_vocab,)`.\n            ids (torch.Tensor): The partial token ids to compute topk\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]:\n                The topk full token ids and partial token ids.\n                Their shapes are `(self.beam_size,)`\n\n        """"""\n        # no pre beam performed\n        if weighted_scores.size(0) == ids.size(0):\n            top_ids = weighted_scores.topk(self.beam_size)[1]\n            return top_ids, top_ids\n\n        # mask pruned in pre-beam not to select in topk\n        tmp = weighted_scores[ids]\n        weighted_scores[:] = -float(""inf"")\n        weighted_scores[ids] = tmp\n        top_ids = weighted_scores.topk(self.beam_size)[1]\n        local_ids = weighted_scores[ids].topk(self.beam_size)[1]\n        return top_ids, local_ids\n\n    @staticmethod\n    def merge_scores(\n        prev_scores: Dict[str, float],\n        next_full_scores: Dict[str, torch.Tensor],\n        full_idx: int,\n        next_part_scores: Dict[str, torch.Tensor],\n        part_idx: int,\n    ) -> Dict[str, torch.Tensor]:\n        """"""Merge scores for new hypothesis.\n\n        Args:\n            prev_scores (Dict[str, float]):\n                The previous hypothesis scores by `self.scorers`\n            next_full_scores (Dict[str, torch.Tensor]): scores by `self.full_scorers`\n            full_idx (int): The next token id for `next_full_scores`\n            next_part_scores (Dict[str, torch.Tensor]):\n                scores of partial tokens by `self.part_scorers`\n            part_idx (int): The new token id for `next_part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are scalar tensors by the scorers.\n\n        """"""\n        new_scores = dict()\n        for k, v in next_full_scores.items():\n            new_scores[k] = prev_scores[k] + v[full_idx]\n        for k, v in next_part_scores.items():\n            new_scores[k] = v[part_idx]\n        return new_scores\n\n    def merge_states(self, states: Any, part_states: Any, part_idx: int) -> Any:\n        """"""Merge states for new hypothesis.\n\n        Args:\n            states: states of `self.full_scorers`\n            part_states: states of `self.part_scorers`\n            part_idx (int): The new token id for `part_scores`\n\n        Returns:\n            Dict[str, torch.Tensor]: The new score dict.\n                Its keys are names of `self.full_scorers` and `self.part_scorers`.\n                Its values are states of the scorers.\n\n        """"""\n        new_states = dict()\n        for k, v in states.items():\n            new_states[k] = v\n        for k, d in self.part_scorers.items():\n            new_states[k] = d.select_state(part_states[k], part_idx)\n        return new_states\n\n    def search(\n        self, running_hyps: List[Hypothesis], x: torch.Tensor\n    ) -> List[Hypothesis]:\n        """"""Search new tokens for running hypotheses and encoded speech x.\n\n        Args:\n            running_hyps (List[Hypothesis]): Running hypotheses on beam\n            x (torch.Tensor): Encoded speech feature (T, D)\n\n        Returns:\n            List[Hypotheses]: Best sorted hypotheses\n\n        """"""\n        best_hyps = []\n        part_ids = torch.arange(self.n_vocab, device=x.device)  # no pre-beam\n        for hyp in running_hyps:\n            # scoring\n            scores, states = self.score_full(hyp, x)\n            if self.do_pre_beam:\n                part_ids = torch.topk(\n                    scores[self.pre_beam_score_key], self.pre_beam_size\n                )[1]\n            part_scores, part_states = self.score_partial(hyp, part_ids, x)\n\n            # weighted sum scores\n            weighted_scores = torch.zeros(self.n_vocab, dtype=x.dtype, device=x.device)\n            for k in self.full_scorers:\n                weighted_scores += self.weights[k] * scores[k]\n            for k in self.part_scorers:\n                weighted_scores[part_ids] += self.weights[k] * part_scores[k]\n            weighted_scores += hyp.score\n\n            # update hyps\n            for j, part_j in zip(*self.beam(weighted_scores, part_ids)):\n                # will be (2 x beam at most)\n                best_hyps.append(\n                    Hypothesis(\n                        score=weighted_scores[j],\n                        yseq=self.append_token(hyp.yseq, j),\n                        scores=self.merge_scores(\n                            hyp.scores, scores, j, part_scores, part_j\n                        ),\n                        states=self.merge_states(states, part_states, part_j),\n                    )\n                )\n\n            # sort and prune 2 x beam -> beam\n            best_hyps = sorted(best_hyps, key=lambda x: x.score, reverse=True)[\n                : min(len(best_hyps), self.beam_size)\n            ]\n        return best_hyps\n\n    def forward(\n        self, x: torch.Tensor, maxlenratio: float = 0.0, minlenratio: float = 0.0\n    ) -> List[Hypothesis]:\n        """"""Perform beam search.\n\n        Args:\n            x (torch.Tensor): Encoded speech feature (T, D)\n            maxlenratio (float): Input length ratio to obtain max output length.\n                If maxlenratio=0.0 (default), it uses a end-detect function\n                to automatically find maximum hypothesis lengths\n            minlenratio (float): Input length ratio to obtain min output length.\n\n        Returns:\n            list[Hypothesis]: N-best decoding results\n\n        """"""\n        # set length bounds\n        if maxlenratio == 0:\n            maxlen = x.shape[0]\n        else:\n            maxlen = max(1, int(maxlenratio * x.size(0)))\n        minlen = int(minlenratio * x.size(0))\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # main loop of prefix search\n        running_hyps = self.init_hyp(x)\n        ended_hyps = []\n        for i in range(maxlen):\n            logging.debug(""position "" + str(i))\n            best = self.search(running_hyps, x)\n            # post process of one iteration\n            running_hyps = self.post_process(i, maxlen, maxlenratio, best, ended_hyps)\n            # end detection\n            if maxlenratio == 0.0 and end_detect([h.asdict() for h in ended_hyps], i):\n                logging.info(f""end detected at {i}"")\n                break\n            if len(running_hyps) == 0:\n                logging.info(""no hypothesis. Finish decoding."")\n                break\n            else:\n                logging.debug(f""remeined hypothes: {len(running_hyps)}"")\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x.score, reverse=True)\n        # check number of hypotheis\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                ""there is no N-best results, perform recognition ""\n                ""again with smaller minlenratio.""\n            )\n            return (\n                []\n                if minlenratio < 0.1\n                else self.forward(x, maxlenratio, max(0.0, minlenratio - 0.1))\n            )\n\n        # report the best result\n        best = nbest_hyps[0]\n        logging.info(f""total log probability: {best.score}"")\n        logging.info(f""normalized log probability: {best.score / len(best.yseq)}"")\n        return nbest_hyps\n\n    def post_process(\n        self,\n        i: int,\n        maxlen: int,\n        maxlenratio: float,\n        running_hyps: List[Hypothesis],\n        ended_hyps: List[Hypothesis],\n    ) -> List[Hypothesis]:\n        """"""Perform post-processing of beam search iterations.\n\n        Args:\n            i (int): The length of hypothesis tokens.\n            maxlen (int): The maximum length of tokens in beam search.\n            maxlenratio (int): The maximum length ratio in beam search.\n            running_hyps (List[Hypothesis]): The running hypotheses in beam search.\n            ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.\n\n        Returns:\n            List[Hypothesis]: The new running hypotheses.\n\n        """"""\n        logging.debug(f""the number of running hypothes: {len(running_hyps)}"")\n        if self.token_list is not None:\n            logging.debug(\n                ""best hypo: ""\n                + """".join([self.token_list[x] for x in running_hyps[0].yseq[1:]])\n            )\n        # add eos in the final loop to avoid that there are no ended hyps\n        if i == maxlen - 1:\n            logging.info(""adding <eos> in the last position in the loop"")\n            running_hyps = [\n                h._replace(yseq=self.append_token(h.yseq, self.eos))\n                for h in running_hyps\n            ]\n\n        # add ended hypotheses to a final list, and removed them from current hypotheses\n        # (this will be a probmlem, number of hyps < beam)\n        remained_hyps = []\n        for hyp in running_hyps:\n            if hyp.yseq[-1] == self.eos:\n                # e.g., Word LM needs to add final <eos> score\n                for k, d in chain(self.full_scorers.items(), self.part_scorers.items()):\n                    s = d.final_score(hyp.states[k])\n                    hyp.scores[k] += s\n                    hyp = hyp._replace(score=hyp.score + self.weights[k] * s)\n                ended_hyps.append(hyp)\n            else:\n                remained_hyps.append(hyp)\n        return remained_hyps\n\n\ndef beam_search(\n    x: torch.Tensor,\n    sos: int,\n    eos: int,\n    beam_size: int,\n    vocab_size: int,\n    scorers: Dict[str, ScorerInterface],\n    weights: Dict[str, float],\n    token_list: List[str] = None,\n    maxlenratio: float = 0.0,\n    minlenratio: float = 0.0,\n    pre_beam_ratio: float = 1.5,\n    pre_beam_score_key: str = ""decoder"",\n) -> list:\n    """"""Perform beam search with scorers.\n\n    Args:\n        x (torch.Tensor): Encoded speech feature (T, D)\n        sos (int): Start of sequence id\n        eos (int): End of sequence id\n        beam_size (int): The number of hypotheses kept during search\n        vocab_size (int): The number of vocabulary\n        scorers (dict[str, ScorerInterface]): Dict of decoder modules\n            e.g., Decoder, CTCPrefixScorer, LM\n            The scorer will be ignored if it is `None`\n        weights (dict[str, float]): Dict of weights for each scorers\n            The scorer will be ignored if its weight is 0\n        token_list (list[str]): List of tokens for debug log\n        maxlenratio (float): Input length ratio to obtain max output length.\n            If maxlenratio=0.0 (default), it uses a end-detect function\n            to automatically find maximum hypothesis lengths\n        minlenratio (float): Input length ratio to obtain min output length.\n        pre_beam_score_key (str): key of scores to perform pre-beam search\n        pre_beam_ratio (float): beam size in the pre-beam search\n            will be `int(pre_beam_ratio * beam_size)`\n\n    Returns:\n        list: N-best decoding results\n\n    """"""\n    ret = BeamSearch(\n        scorers,\n        weights,\n        beam_size=beam_size,\n        vocab_size=vocab_size,\n        pre_beam_ratio=pre_beam_ratio,\n        pre_beam_score_key=pre_beam_score_key,\n        sos=sos,\n        eos=eos,\n        token_list=token_list,\n    ).forward(x=x, maxlenratio=maxlenratio, minlenratio=minlenratio)\n    return [h.asdict() for h in ret]\n'"
espnet/nets/ctc_prefix_score.py,41,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Mitsubishi Electric Research Labs (Takaaki Hori)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport torch\n\nimport numpy as np\nimport six\n\n\nclass CTCPrefixScoreTH(object):\n    """"""Batch processing of CTCPrefixScore\n\n    which is based on Algorithm 2 in WATANABE et al.\n    ""HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,""\n    but extended to efficiently compute the probablities of multiple labels\n    simultaneously\n    """"""\n\n    def __init__(self, x, xlens, blank, eos, beam, scoring_ratio=1.5, margin=0):\n        """"""Construct CTC prefix scorer\n\n        :param torch.Tensor x: input label posterior sequences (B, T, O)\n        :param torch.Tensor xlens: input lengths (B,)\n        :param int blank: blank label id\n        :param int eos: end-of-sequence id\n        :param int beam: beam size\n        :param float scoring_ratio: ratio of #scored hypos to beam size\n        :param int margin: margin parameter for windowing (0 means no windowing)\n        """"""\n        # In the comment lines,\n        # we assume T: input_length, B: batch size, W: beam width, O: output dim.\n        self.logzero = -10000000000.0\n        self.blank = blank\n        self.eos = eos\n        self.batch = x.size(0)\n        self.input_length = x.size(1)\n        self.odim = x.size(2)\n        self.beam = beam\n        self.n_bb = self.batch * beam\n        self.device = (\n            torch.device(""cuda:%d"" % x.get_device())\n            if x.is_cuda\n            else torch.device(""cpu"")\n        )\n        # Pad the rest of posteriors in the batch\n        # TODO(takaaki-hori): need a better way without for-loops\n        for i, l in enumerate(xlens):\n            if l < self.input_length:\n                x[i, l:, :] = self.logzero\n                x[i, l:, blank] = 0\n        # Set the number of scoring hypotheses (scoring_num=0 means all)\n        self.scoring_num = int(beam * scoring_ratio)\n        if self.scoring_num >= self.odim:\n            self.scoring_num = 0\n        # Expand input posteriors for fast computation\n        if self.scoring_num == 0:\n            xn = (\n                x.transpose(0, 1)\n                .unsqueeze(2)\n                .repeat(1, 1, beam, 1)\n                .view(-1, self.n_bb, self.odim)\n            )\n        else:\n            xn = x.transpose(0, 1)\n        xb = xn[:, :, self.blank].unsqueeze(2).expand(-1, -1, self.odim)\n        self.x = torch.stack([xn, xb])  # (2, T, B, O) or (2, T, BW, O)\n        # Setup CTC windowing\n        self.margin = margin\n        if margin > 0:\n            self.frame_ids = torch.arange(\n                self.input_length, dtype=torch.float32, device=self.device\n            )\n        # Precompute end frames (BW,)\n        self.end_frames = (\n            (torch.as_tensor(xlens) - 1).view(self.batch, 1).repeat(1, beam).view(-1)\n        )\n        # Precompute base indices to convert label ids to corresponding element indices\n        self.pad_b = (torch.arange(self.batch, device=self.device) * beam).view(-1, 1)\n        self.pad_bo = (\n            torch.arange(self.batch, device=self.device) * (beam * self.odim)\n        ).view(-1, 1)\n        self.pad_o = (\n            (torch.arange(self.batch, device=self.device) * self.odim)\n            .unsqueeze(1)\n            .repeat(1, beam)\n            .view(-1, 1)\n        )\n        self.bb_idx = torch.arange(self.n_bb, device=self.device).view(-1, 1)\n\n    def __call__(self, y, state, pre_scores=None, att_w=None):\n        """"""Compute CTC prefix scores for next labels\n\n        :param list y: prefix label sequences\n        :param tuple state: previous CTC state\n        :param torch.Tensor pre_scores: scores for pre-selection of hypotheses (BW, O)\n        :param torch.Tensor att_w: attention weights to decide CTC window\n        :return new_state, ctc_local_scores (BW, O)\n        """"""\n        output_length = len(y[0]) - 1  # ignore sos\n        last_ids = [yi[-1] for yi in y]  # last output label ids\n        # prepare state info\n        if state is None:\n            if self.scoring_num > 0:\n                r_prev = torch.full(\n                    (self.input_length, 2, self.batch, self.beam),\n                    self.logzero,\n                    dtype=torch.float32,\n                    device=self.device,\n                )\n                r_prev[:, 1] = torch.cumsum(self.x[0, :, :, self.blank], 0).unsqueeze(2)\n                r_prev = r_prev.view(-1, 2, self.n_bb)\n            else:\n                r_prev = torch.full(\n                    (self.input_length, 2, self.n_bb),\n                    self.logzero,\n                    dtype=torch.float32,\n                    device=self.device,\n                )\n                r_prev[:, 1] = torch.cumsum(self.x[0, :, :, self.blank], 0)\n            s_prev = 0.0\n            f_min_prev = 0\n            f_max_prev = 1\n        else:\n            r_prev, s_prev, f_min_prev, f_max_prev = state\n\n        # select input dimensions for scoring\n        if self.scoring_num > 0 and pre_scores is not None:\n            pre_scores[:, self.blank] = self.logzero  # ignore blank from pre-selection\n            scoring_ids = torch.topk(pre_scores, self.scoring_num, 1)[1]\n            scoring_idmap = torch.full(\n                (self.n_bb, self.odim), -1, dtype=torch.long, device=self.device\n            )\n            snum = scoring_ids.size(1)\n            scoring_idmap[self.bb_idx, scoring_ids] = torch.arange(\n                snum, device=self.device\n            )\n            scoring_idx = (scoring_ids + self.pad_o).view(-1)\n            x_ = torch.index_select(\n                self.x.view(2, -1, self.batch * self.odim), 2, scoring_idx\n            ).view(2, -1, self.n_bb, snum)\n        else:\n            scoring_ids = None\n            scoring_idmap = None\n            snum = self.odim\n            x_ = self.x\n\n        # new CTC forward probs are prepared as a (T x 2 x BW x S) tensor\n        # that corresponds to r_t^n(h) and r_t^b(h) in a batch.\n        r = torch.full(\n            (self.input_length, 2, self.n_bb, snum),\n            self.logzero,\n            dtype=torch.float32,\n            device=self.device,\n        )\n        if output_length == 0:\n            r[0, 0] = x_[0, 0]\n\n        r_sum = torch.logsumexp(r_prev, 1)\n        log_phi = r_sum.unsqueeze(2).repeat(1, 1, snum)\n        if scoring_ids is not None:\n            for idx in range(self.n_bb):\n                pos = scoring_idmap[idx, last_ids[idx]]\n                if pos >= 0:\n                    log_phi[:, idx, pos] = r_prev[:, 1, idx]\n        else:\n            for idx in range(self.n_bb):\n                log_phi[:, idx, last_ids[idx]] = r_prev[:, 1, idx]\n\n        # decide start and end frames based on attention weights\n        if att_w is not None and self.margin > 0:\n            f_arg = torch.matmul(att_w, self.frame_ids)\n            f_min = max(int(f_arg.min().cpu()), f_min_prev)\n            f_max = max(int(f_arg.max().cpu()), f_max_prev)\n            start = min(f_max_prev, max(f_min - self.margin, output_length, 1))\n            end = min(f_max + self.margin, self.input_length)\n        else:\n            f_min = f_max = 0\n            start = max(output_length, 1)\n            end = self.input_length\n\n        # compute forward probabilities log(r_t^n(h)) and log(r_t^b(h))\n        for t in range(start, end):\n            rp = r[t - 1]\n            rr = torch.stack([rp[0], log_phi[t - 1], rp[0], rp[1]]).view(\n                2, 2, self.n_bb, snum\n            )\n            r[t] = torch.logsumexp(rr, 1) + x_[:, t]\n\n        # compute log prefix probabilites log(psi)\n        log_phi_x = torch.cat((log_phi[0].unsqueeze(0), log_phi[:-1]), dim=0) + x_[0]\n        if scoring_ids is not None:\n            log_psi = torch.full(\n                (self.n_bb, self.odim), self.logzero, device=self.device\n            )\n            log_psi_ = torch.logsumexp(\n                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n                dim=0,\n            )\n            for si in range(self.n_bb):\n                log_psi[si, scoring_ids[si]] = log_psi_[si]\n        else:\n            log_psi = torch.logsumexp(\n                torch.cat((log_phi_x[start:end], r[start - 1, 0].unsqueeze(0)), dim=0),\n                dim=0,\n            )\n\n        for si in range(self.n_bb):\n            log_psi[si, self.eos] = r_sum[self.end_frames[si], si]\n\n        # exclude blank probs\n        log_psi[:, self.blank] = self.logzero\n\n        return (r, log_psi, f_min, f_max, scoring_idmap), log_psi - s_prev\n\n    def index_select_state(self, state, best_ids):\n        """"""Select CTC states according to best ids\n\n        :param state    : CTC state\n        :param best_ids : index numbers selected by beam pruning (B, W)\n        :return selected_state\n        """"""\n        r, s, f_min, f_max, scoring_idmap = state\n        # convert ids to BWO space\n        vidx = (best_ids + self.pad_bo).view(-1)\n        # select hypothesis scores\n        s_new = torch.index_select(s.view(-1), 0, vidx)\n        s_new = s_new.view(-1, 1).repeat(1, self.odim).view(self.n_bb, self.odim)\n        # convert ids to BWS space (S: scoring_num)\n        if scoring_idmap is not None:\n            snum = self.scoring_num\n            beam_idx = (torch.div(best_ids, self.odim) + self.pad_b).view(-1)\n            label_ids = torch.fmod(best_ids, self.odim).view(-1)\n            score_idx = scoring_idmap[beam_idx, label_ids]\n            score_idx[score_idx == -1] = 0\n            vidx = score_idx + beam_idx * snum\n        else:\n            snum = self.odim\n        # select forward probabilities\n        r_new = torch.index_select(r.view(-1, 2, self.n_bb * snum), 2, vidx).view(\n            -1, 2, self.n_bb\n        )\n        return r_new, s_new, f_min, f_max\n\n\nclass CTCPrefixScore(object):\n    """"""Compute CTC label sequence scores\n\n    which is based on Algorithm 2 in WATANABE et al.\n    ""HYBRID CTC/ATTENTION ARCHITECTURE FOR END-TO-END SPEECH RECOGNITION,""\n    but extended to efficiently compute the probablities of multiple labels\n    simultaneously\n    """"""\n\n    def __init__(self, x, blank, eos, xp):\n        self.xp = xp\n        self.logzero = -10000000000.0\n        self.blank = blank\n        self.eos = eos\n        self.input_length = len(x)\n        self.x = x\n\n    def initial_state(self):\n        """"""Obtain an initial CTC state\n\n        :return: CTC state\n        """"""\n        # initial CTC state is made of a frame x 2 tensor that corresponds to\n        # r_t^n(<sos>) and r_t^b(<sos>), where 0 and 1 of axis=1 represent\n        # superscripts n and b (non-blank and blank), respectively.\n        r = self.xp.full((self.input_length, 2), self.logzero, dtype=np.float32)\n        r[0, 1] = self.x[0, self.blank]\n        for i in six.moves.range(1, self.input_length):\n            r[i, 1] = r[i - 1, 1] + self.x[i, self.blank]\n        return r\n\n    def __call__(self, y, cs, r_prev):\n        """"""Compute CTC prefix scores for next labels\n\n        :param y     : prefix label sequence\n        :param cs    : array of next labels\n        :param r_prev: previous CTC state\n        :return ctc_scores, ctc_states\n        """"""\n        # initialize CTC states\n        output_length = len(y) - 1  # ignore sos\n        # new CTC states are prepared as a frame x (n or b) x n_labels tensor\n        # that corresponds to r_t^n(h) and r_t^b(h).\n        r = self.xp.ndarray((self.input_length, 2, len(cs)), dtype=np.float32)\n        xs = self.x[:, cs]\n        if output_length == 0:\n            r[0, 0] = xs[0]\n            r[0, 1] = self.logzero\n        else:\n            r[output_length - 1] = self.logzero\n\n        # prepare forward probabilities for the last label\n        r_sum = self.xp.logaddexp(\n            r_prev[:, 0], r_prev[:, 1]\n        )  # log(r_t^n(g) + r_t^b(g))\n        last = y[-1]\n        if output_length > 0 and last in cs:\n            log_phi = self.xp.ndarray((self.input_length, len(cs)), dtype=np.float32)\n            for i in six.moves.range(len(cs)):\n                log_phi[:, i] = r_sum if cs[i] != last else r_prev[:, 1]\n        else:\n            log_phi = r_sum\n\n        # compute forward probabilities log(r_t^n(h)), log(r_t^b(h)),\n        # and log prefix probabilites log(psi)\n        start = max(output_length, 1)\n        log_psi = r[start - 1, 0]\n        for t in six.moves.range(start, self.input_length):\n            r[t, 0] = self.xp.logaddexp(r[t - 1, 0], log_phi[t - 1]) + xs[t]\n            r[t, 1] = (\n                self.xp.logaddexp(r[t - 1, 0], r[t - 1, 1]) + self.x[t, self.blank]\n            )\n            log_psi = self.xp.logaddexp(log_psi, log_phi[t - 1] + xs[t])\n\n        # get P(...eos|X) that ends with the prefix itself\n        eos_pos = self.xp.where(cs == self.eos)[0]\n        if len(eos_pos) > 0:\n            log_psi[eos_pos] = r_sum[-1]  # log(r_T^n(g) + r_T^b(g))\n\n        # exclude blank probs\n        blank_pos = self.xp.where(cs == self.blank)[0]\n        if len(blank_pos) > 0:\n            log_psi[blank_pos] = self.logzero\n\n        # return the log prefix probability and CTC states, where the label axis\n        # of the CTC states is moved to the first axis to slice it easily\n        return log_psi, self.xp.rollaxis(r, 2)\n'"
espnet/nets/e2e_asr_common.py,14,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Common functions for ASR.""""""\n\nimport argparse\nimport editdistance\nimport json\nimport logging\nimport numpy as np\nimport six\nimport sys\n\nfrom itertools import groupby\n\n\ndef end_detect(ended_hyps, i, M=3, D_end=np.log(1 * np.exp(-10))):\n    """"""End detection.\n\n    desribed in Eq. (50) of S. Watanabe et al\n    ""Hybrid CTC/Attention Architecture for End-to-End Speech Recognition""\n\n    :param ended_hyps:\n    :param i:\n    :param M:\n    :param D_end:\n    :return:\n    """"""\n    if len(ended_hyps) == 0:\n        return False\n    count = 0\n    best_hyp = sorted(ended_hyps, key=lambda x: x[""score""], reverse=True)[0]\n    for m in six.moves.range(M):\n        # get ended_hyps with their length is i - m\n        hyp_length = i - m\n        hyps_same_length = [x for x in ended_hyps if len(x[""yseq""]) == hyp_length]\n        if len(hyps_same_length) > 0:\n            best_hyp_same_length = sorted(\n                hyps_same_length, key=lambda x: x[""score""], reverse=True\n            )[0]\n            if best_hyp_same_length[""score""] - best_hyp[""score""] < D_end:\n                count += 1\n\n    if count == M:\n        return True\n    else:\n        return False\n\n\n# TODO(takaaki-hori): add different smoothing methods\ndef label_smoothing_dist(odim, lsm_type, transcript=None, blank=0):\n    """"""Obtain label distribution for loss smoothing.\n\n    :param odim:\n    :param lsm_type:\n    :param blank:\n    :param transcript:\n    :return:\n    """"""\n    if transcript is not None:\n        with open(transcript, ""rb"") as f:\n            trans_json = json.load(f)[""utts""]\n\n    if lsm_type == ""unigram"":\n        assert transcript is not None, (\n            ""transcript is required for %s label smoothing"" % lsm_type\n        )\n        labelcount = np.zeros(odim)\n        for k, v in trans_json.items():\n            ids = np.array([int(n) for n in v[""output""][0][""tokenid""].split()])\n            # to avoid an error when there is no text in an uttrance\n            if len(ids) > 0:\n                labelcount[ids] += 1\n        labelcount[odim - 1] = len(transcript)  # count <eos>\n        labelcount[labelcount == 0] = 1  # flooring\n        labelcount[blank] = 0  # remove counts for blank\n        labeldist = labelcount.astype(np.float32) / np.sum(labelcount)\n    else:\n        logging.error(""Error: unexpected label smoothing type: %s"" % lsm_type)\n        sys.exit()\n\n    return labeldist\n\n\ndef get_vgg2l_odim(idim, in_channel=3, out_channel=128):\n    """"""Return the output size of the VGG frontend.\n\n    :param in_channel: input channel size\n    :param out_channel: output channel size\n    :return: output size\n    :rtype int\n    """"""\n    idim = idim / in_channel\n    idim = np.ceil(np.array(idim, dtype=np.float32) / 2)  # 1st max pooling\n    idim = np.ceil(np.array(idim, dtype=np.float32) / 2)  # 2nd max pooling\n    return int(idim) * out_channel  # numer of channels\n\n\nclass ErrorCalculator(object):\n    """"""Calculate CER and WER for E2E_ASR and CTC models during training.\n\n    :param y_hats: numpy array with predicted text\n    :param y_pads: numpy array with true (target) text\n    :param char_list:\n    :param sym_space:\n    :param sym_blank:\n    :return:\n    """"""\n\n    def __init__(\n        self, char_list, sym_space, sym_blank, report_cer=False, report_wer=False\n    ):\n        """"""Construct an ErrorCalculator object.""""""\n        super(ErrorCalculator, self).__init__()\n\n        self.report_cer = report_cer\n        self.report_wer = report_wer\n\n        self.char_list = char_list\n        self.space = sym_space\n        self.blank = sym_blank\n        self.idx_blank = self.char_list.index(self.blank)\n        if self.space in self.char_list:\n            self.idx_space = self.char_list.index(self.space)\n        else:\n            self.idx_space = None\n\n    def __call__(self, ys_hat, ys_pad, is_ctc=False):\n        """"""Calculate sentence-level WER/CER score.\n\n        :param torch.Tensor ys_hat: prediction (batch, seqlen)\n        :param torch.Tensor ys_pad: reference (batch, seqlen)\n        :param bool is_ctc: calculate CER score for CTC\n        :return: sentence-level WER score\n        :rtype float\n        :return: sentence-level CER score\n        :rtype float\n        """"""\n        cer, wer = None, None\n        if is_ctc:\n            return self.calculate_cer_ctc(ys_hat, ys_pad)\n        elif not self.report_cer and not self.report_wer:\n            return cer, wer\n\n        seqs_hat, seqs_true = self.convert_to_char(ys_hat, ys_pad)\n        if self.report_cer:\n            cer = self.calculate_cer(seqs_hat, seqs_true)\n\n        if self.report_wer:\n            wer = self.calculate_wer(seqs_hat, seqs_true)\n        return cer, wer\n\n    def calculate_cer_ctc(self, ys_hat, ys_pad):\n        """"""Calculate sentence-level CER score for CTC.\n\n        :param torch.Tensor ys_hat: prediction (batch, seqlen)\n        :param torch.Tensor ys_pad: reference (batch, seqlen)\n        :return: average sentence-level CER score\n        :rtype float\n        """"""\n        cers, char_ref_lens = [], []\n        for i, y in enumerate(ys_hat):\n            y_hat = [x[0] for x in groupby(y)]\n            y_true = ys_pad[i]\n            seq_hat, seq_true = [], []\n            for idx in y_hat:\n                idx = int(idx)\n                if idx != -1 and idx != self.idx_blank and idx != self.idx_space:\n                    seq_hat.append(self.char_list[int(idx)])\n\n            for idx in y_true:\n                idx = int(idx)\n                if idx != -1 and idx != self.idx_blank and idx != self.idx_space:\n                    seq_true.append(self.char_list[int(idx)])\n\n            hyp_chars = """".join(seq_hat)\n            ref_chars = """".join(seq_true)\n            if len(ref_chars) > 0:\n                cers.append(editdistance.eval(hyp_chars, ref_chars))\n                char_ref_lens.append(len(ref_chars))\n\n        cer_ctc = float(sum(cers)) / sum(char_ref_lens) if cers else None\n        return cer_ctc\n\n    def convert_to_char(self, ys_hat, ys_pad):\n        """"""Convert index to character.\n\n        :param torch.Tensor seqs_hat: prediction (batch, seqlen)\n        :param torch.Tensor seqs_true: reference (batch, seqlen)\n        :return: token list of prediction\n        :rtype list\n        :return: token list of reference\n        :rtype list\n        """"""\n        seqs_hat, seqs_true = [], []\n        for i, y_hat in enumerate(ys_hat):\n            y_true = ys_pad[i]\n            eos_true = np.where(y_true == -1)[0]\n            ymax = eos_true[0] if len(eos_true) > 0 else len(y_true)\n            # NOTE: padding index (-1) in y_true is used to pad y_hat\n            seq_hat = [self.char_list[int(idx)] for idx in y_hat[:ymax]]\n            seq_true = [self.char_list[int(idx)] for idx in y_true if int(idx) != -1]\n            seq_hat_text = """".join(seq_hat).replace(self.space, "" "")\n            seq_hat_text = seq_hat_text.replace(self.blank, """")\n            seq_true_text = """".join(seq_true).replace(self.space, "" "")\n            seqs_hat.append(seq_hat_text)\n            seqs_true.append(seq_true_text)\n        return seqs_hat, seqs_true\n\n    def calculate_cer(self, seqs_hat, seqs_true):\n        """"""Calculate sentence-level CER score.\n\n        :param list seqs_hat: prediction\n        :param list seqs_true: reference\n        :return: average sentence-level CER score\n        :rtype float\n        """"""\n        char_eds, char_ref_lens = [], []\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_chars = seq_hat_text.replace("" "", """")\n            ref_chars = seq_true_text.replace("" "", """")\n            char_eds.append(editdistance.eval(hyp_chars, ref_chars))\n            char_ref_lens.append(len(ref_chars))\n        return float(sum(char_eds)) / sum(char_ref_lens)\n\n    def calculate_wer(self, seqs_hat, seqs_true):\n        """"""Calculate sentence-level WER score.\n\n        :param list seqs_hat: prediction\n        :param list seqs_true: reference\n        :return: average sentence-level WER score\n        :rtype float\n        """"""\n        word_eds, word_ref_lens = [], []\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_words = seq_hat_text.split()\n            ref_words = seq_true_text.split()\n            word_eds.append(editdistance.eval(hyp_words, ref_words))\n            word_ref_lens.append(len(ref_words))\n        return float(sum(word_eds)) / sum(word_ref_lens)\n\n\nclass ErrorCalculatorTrans(object):\n    """"""Calculate CER and WER for transducer models.\n\n    Args:\n        decoder (nn.Module): decoder module\n        args (Namespace): argument Namespace containing options\n        report_cer (boolean): compute CER option\n        report_wer (boolean): compute WER option\n\n    """"""\n\n    def __init__(self, decoder, args, report_cer=False, report_wer=False):\n        """"""Construct an ErrorCalculator object for transducer model.""""""\n        super(ErrorCalculatorTrans, self).__init__()\n\n        self.dec = decoder\n\n        recog_args = {\n            ""beam_size"": args.beam_size,\n            ""nbest"": args.nbest,\n            ""space"": args.sym_space,\n            ""score_norm_transducer"": args.score_norm_transducer,\n        }\n\n        self.recog_args = argparse.Namespace(**recog_args)\n\n        self.char_list = args.char_list\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n\n        self.report_cer = args.report_cer\n        self.report_wer = args.report_wer\n\n    def __call__(self, hs_pad, ys_pad):\n        """"""Calculate sentence-level WER/CER score for transducer models.\n\n        Args:\n            hs_pad (torch.Tensor): batch of padded input sequence (batch, T, D)\n            ys_pad (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (float): sentence-level CER score\n            (float): sentence-level WER score\n\n        """"""\n        cer, wer = None, None\n\n        if not self.report_cer and not self.report_wer:\n            return cer, wer\n\n        batchsize = int(hs_pad.size(0))\n        batch_nbest = []\n\n        for b in six.moves.range(batchsize):\n            if self.recog_args.beam_size == 1:\n                nbest_hyps = self.dec.recognize(hs_pad[b], self.recog_args)\n            else:\n                nbest_hyps = self.dec.recognize_beam(hs_pad[b], self.recog_args)\n            batch_nbest.append(nbest_hyps)\n\n        ys_hat = [nbest_hyp[0][""yseq""][1:] for nbest_hyp in batch_nbest]\n\n        seqs_hat, seqs_true = self.convert_to_char(ys_hat, ys_pad.cpu())\n\n        if self.report_cer:\n            cer = self.calculate_cer(seqs_hat, seqs_true)\n\n        if self.report_wer:\n            wer = self.calculate_wer(seqs_hat, seqs_true)\n\n        return cer, wer\n\n    def convert_to_char(self, ys_hat, ys_pad):\n        """"""Convert index to character.\n\n        Args:\n            ys_hat (torch.Tensor): prediction (batch, seqlen)\n            ys_pad (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (list): token list of prediction\n            (list): token list of reference\n\n        """"""\n        seqs_hat, seqs_true = [], []\n\n        for i, y_hat in enumerate(ys_hat):\n            y_true = ys_pad[i]\n\n            eos_true = np.where(y_true == -1)[0]\n            eos_true = eos_true[0] if len(eos_true) > 0 else len(y_true)\n\n            seq_hat = [self.char_list[int(idx)] for idx in y_hat[:eos_true]]\n            seq_true = [self.char_list[int(idx)] for idx in y_true if int(idx) != -1]\n\n            seq_hat_text = """".join(seq_hat).replace(self.space, "" "")\n            seq_hat_text = seq_hat_text.replace(self.blank, """")\n            seq_true_text = """".join(seq_true).replace(self.space, "" "")\n\n            seqs_hat.append(seq_hat_text)\n            seqs_true.append(seq_true_text)\n\n        return seqs_hat, seqs_true\n\n    def calculate_cer(self, seqs_hat, seqs_true):\n        """"""Calculate sentence-level CER score for transducer model.\n\n        Args:\n            seqs_hat (torch.Tensor): prediction (batch, seqlen)\n            seqs_true (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (float): average sentence-level CER score\n\n        """"""\n        char_eds, char_ref_lens = [], []\n\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_chars = seq_hat_text.replace("" "", """")\n            ref_chars = seq_true_text.replace("" "", """")\n\n            char_eds.append(editdistance.eval(hyp_chars, ref_chars))\n            char_ref_lens.append(len(ref_chars))\n\n        return float(sum(char_eds)) / sum(char_ref_lens)\n\n    def calculate_wer(self, seqs_hat, seqs_true):\n        """"""Calculate sentence-level WER score for transducer model.\n\n        Args:\n            seqs_hat (torch.Tensor): prediction (batch, seqlen)\n            seqs_true (torch.Tensor): reference (batch, seqlen)\n\n        Returns:\n            (float): average sentence-level WER score\n\n        """"""\n        word_eds, word_ref_lens = [], []\n\n        for i, seq_hat_text in enumerate(seqs_hat):\n            seq_true_text = seqs_true[i]\n            hyp_words = seq_hat_text.split()\n            ref_words = seq_true_text.split()\n\n            word_eds.append(editdistance.eval(hyp_words, ref_words))\n            word_ref_lens.append(len(ref_words))\n\n        return float(sum(word_eds)) / sum(word_ref_lens)\n'"
espnet/nets/e2e_mt_common.py,4,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Common functions for ST and MT.""""""\n\nimport nltk\nimport numpy as np\n\n\nclass ErrorCalculator(object):\n    """"""Calculate BLEU for ST and MT models during training.\n\n    :param y_hats: numpy array with predicted text\n    :param y_pads: numpy array with true (target) text\n    :param char_list: vocabulary list\n    :param sym_space: space symbol\n    :param sym_pad: pad symbol\n    :param report_bleu: report BLUE score if True\n    """"""\n\n    def __init__(self, char_list, sym_space, sym_pad, report_bleu=False):\n        """"""Construct an ErrorCalculator object.""""""\n        super(ErrorCalculator, self).__init__()\n        self.char_list = char_list\n        self.space = sym_space\n        self.pad = sym_pad\n        self.report_bleu = report_bleu\n        if self.space in self.char_list:\n            self.idx_space = self.char_list.index(self.space)\n        else:\n            self.idx_space = None\n\n    def __call__(self, ys_hat, ys_pad):\n        """"""Calculate corpus-level BLEU score.\n\n        :param torch.Tensor ys_hat: prediction (batch, seqlen)\n        :param torch.Tensor ys_pad: reference (batch, seqlen)\n        :return: corpus-level BLEU score in a mini-batch\n        :rtype float\n        """"""\n        bleu = None\n        if not self.report_bleu:\n            return bleu\n\n        bleu = self.calculate_corpus_bleu(ys_hat, ys_pad)\n        return bleu\n\n    def calculate_corpus_bleu(self, ys_hat, ys_pad):\n        """"""Calculate corpus-level BLEU score in a mini-batch.\n\n        :param torch.Tensor seqs_hat: prediction (batch, seqlen)\n        :param torch.Tensor seqs_true: reference (batch, seqlen)\n        :return: corpus-level BLEU score\n        :rtype float\n        """"""\n        seqs_hat, seqs_true = [], []\n        for i, y_hat in enumerate(ys_hat):\n            y_true = ys_pad[i]\n            eos_true = np.where(y_true == -1)[0]\n            ymax = eos_true[0] if len(eos_true) > 0 else len(y_true)\n            # NOTE: padding index (-1) in y_true is used to pad y_hat\n            # because y_hats is not padded with -1\n            seq_hat = [self.char_list[int(idx)] for idx in y_hat[:ymax]]\n            seq_true = [self.char_list[int(idx)] for idx in y_true if int(idx) != -1]\n            seq_hat_text = """".join(seq_hat).replace(self.space, "" "")\n            seq_hat_text = seq_hat_text.replace(self.pad, """")\n            seq_true_text = """".join(seq_true).replace(self.space, "" "")\n            seqs_hat.append(seq_hat_text)\n            seqs_true.append(seq_true_text)\n        bleu = nltk.bleu_score.corpus_bleu([[ref] for ref in seqs_true], seqs_hat)\n        return bleu * 100\n'"
espnet/nets/lm_interface.py,3,"b'""""""Language model interface.""""""\n\nimport argparse\n\nfrom espnet.nets.scorer_interface import ScorerInterface\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass LMInterface(ScorerInterface):\n    """"""LM Interface for ESPnet model implementation.""""""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments to command line argument parser.""""""\n        return parser\n\n    @classmethod\n    def build(cls, n_vocab: int, **kwargs):\n        """"""Initialize this class with python-level args.\n\n        Args:\n            idim (int): The number of vocabulary.\n\n        Returns:\n            LMinterface: A new instance of LMInterface.\n\n        """"""\n        # local import to avoid cyclic import in lm_train\n        from espnet.bin.lm_train import get_parser\n\n        def wrap(parser):\n            return get_parser(parser, required=False)\n\n        args = argparse.Namespace(**kwargs)\n        args = fill_missing_args(args, wrap)\n        args = fill_missing_args(args, cls.add_arguments)\n        return cls(n_vocab, args)\n\n    def forward(self, x, t):\n        """"""Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        """"""\n        raise NotImplementedError(""forward method is not implemented"")\n\n\npredefined_lms = {\n    ""pytorch"": {\n        ""default"": ""espnet.nets.pytorch_backend.lm.default:DefaultRNNLM"",\n        ""seq_rnn"": ""espnet.nets.pytorch_backend.lm.seq_rnn:SequentialRNNLM"",\n        ""transformer"": ""espnet.nets.pytorch_backend.lm.transformer:TransformerLM"",\n    },\n    ""chainer"": {""default"": ""espnet.lm.chainer_backend.lm:DefaultRNNLM""},\n}\n\n\ndef dynamic_import_lm(module, backend):\n    """"""Import LM class dynamically.\n\n    Args:\n        module (str): module_name:class_name or alias in `predefined_lms`\n        backend (str): NN backend. e.g., pytorch, chainer\n\n    Returns:\n        type: LM class\n\n    """"""\n    model_class = dynamic_import(module, predefined_lms.get(backend, dict()))\n    assert issubclass(\n        model_class, LMInterface\n    ), f""{module} does not implement LMInterface""\n    return model_class\n'"
espnet/nets/mt_interface.py,7,"b'""""""MT Interface module.""""""\nimport argparse\n\nfrom espnet.bin.asr_train import get_parser\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass MTInterface:\n    """"""MT Interface for ESPnet model implementation.""""""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments to parser.""""""\n        return parser\n\n    @classmethod\n    def build(cls, idim: int, odim: int, **kwargs):\n        """"""Initialize this class with python-level args.\n\n        Args:\n            idim (int): The number of an input feature dim.\n            odim (int): The number of output vocab.\n\n        Returns:\n            ASRinterface: A new instance of ASRInterface.\n\n        """"""\n\n        def wrap(parser):\n            return get_parser(parser, required=False)\n\n        args = argparse.Namespace(**kwargs)\n        args = fill_missing_args(args, wrap)\n        args = fill_missing_args(args, cls.add_arguments)\n        return cls(idim, odim, args)\n\n    def forward(self, xs, ilens, ys):\n        """"""Compute loss for training.\n\n        :param xs:\n            For pytorch, batch of padded source sequences torch.Tensor (B, Tmax, idim)\n            For chainer, list of source sequences chainer.Variable\n        :param ilens: batch of lengths of source sequences (B)\n            For pytorch, torch.Tensor\n            For chainer, list of int\n        :param ys:\n            For pytorch, batch of padded source sequences torch.Tensor (B, Lmax)\n            For chainer, list of source sequences chainer.Variable\n        :return: loss value\n        :rtype: torch.Tensor for pytorch, chainer.Variable for chainer\n        """"""\n        raise NotImplementedError(""forward method is not implemented"")\n\n    def translate(self, x, trans_args, char_list=None, rnnlm=None):\n        """"""Translate x for evaluation.\n\n        :param ndarray x: input acouctic feature (B, T, D) or (T, D)\n        :param namespace trans_args: argment namespace contraining options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        raise NotImplementedError(""translate method is not implemented"")\n\n    def translate_batch(self, x, trans_args, char_list=None, rnnlm=None):\n        """"""Beam search implementation for batch.\n\n        :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc)\n        :param namespace trans_args: argument namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        raise NotImplementedError(""Batch decoding is not supported yet."")\n\n    def calculate_all_attentions(self, xs, ilens, ys):\n        """"""Caluculate attention.\n\n        :param list xs_pad: list of padded input sequences [(T1, idim), (T2, idim), ...]\n        :param ndarray ilens: batch of lengths of input sequences (B)\n        :param list ys: list of character id sequence tensor [(L1), (L2), (L3), ...]\n        :return: attention weights (B, Lmax, Tmax)\n        :rtype: float ndarray\n        """"""\n        raise NotImplementedError(""calculate_all_attentions method is not implemented"")\n\n    @property\n    def attention_plot_class(self):\n        """"""Get attention plot class.""""""\n        from espnet.asr.asr_utils import PlotAttentionReport\n\n        return PlotAttentionReport\n'"
espnet/nets/scorer_interface.py,18,"b'""""""Scorer interface module.""""""\n\nfrom typing import Any\nfrom typing import List\nfrom typing import Tuple\n\nimport torch\n\n\nclass ScorerInterface:\n    """"""Scorer interface for beam search.\n\n    The scorer performs scoring of the all tokens in vocabulary.\n\n    Examples:\n        * Search heuristics\n            * :class:`espnet.nets.scorers.length_bonus.LengthBonus`\n        * Decoder networks of the sequence-to-sequence models\n            * :class:`espnet.nets.pytorch_backend.nets.transformer.decoder.Decoder`\n            * :class:`espnet.nets.pytorch_backend.nets.rnn.decoders.Decoder`\n        * Neural language models\n            * :class:`espnet.nets.pytorch_backend.lm.transformer.TransformerLM`\n            * :class:`espnet.nets.pytorch_backend.lm.default.DefaultRNNLM`\n            * :class:`espnet.nets.pytorch_backend.lm.seq_rnn.SequentialRNNLM`\n\n    """"""\n\n    def init_state(self, x: torch.Tensor) -> Any:\n        """"""Get an initial state for decoding (optional).\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        """"""\n        return None\n\n    def select_state(self, state: Any, i: int) -> Any:\n        """"""Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n\n        Returns:\n            state: pruned state\n\n        """"""\n        return None if state is None else state[i]\n\n    def score(\n        self, y: torch.Tensor, state: Any, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Any]:\n        """"""Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                scores for next token that has a shape of `(n_vocab)`\n                and next state for ys\n\n        """"""\n        raise NotImplementedError\n\n    def final_score(self, state: Any) -> float:\n        """"""Score eos (optional).\n\n        Args:\n            state: Scorer state for prefix tokens\n\n        Returns:\n            float: final score\n\n        """"""\n        return 0.0\n\n\nclass BatchScorerInterface(ScorerInterface):\n    """"""Batch scorer interface.""""""\n\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        """"""Score new token batch (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        """"""\n        raise NotImplementedError\n\n\nclass PartialScorerInterface(ScorerInterface):\n    """"""Partial scorer interface for beam search.\n\n    The partial scorer performs scoring when non-partial scorer finished scoring,\n    and recieves pre-pruned next tokens to score because it is too heavy to score\n    all the tokens.\n\n    Examples:\n         * Prefix search for connectionist-temporal-classification models\n             * :class:`espnet.nets.scorers.ctc.CTCPrefixScorer`\n\n    """"""\n\n    def score_partial(\n        self, y: torch.Tensor, next_tokens: torch.Tensor, state: Any, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Any]:\n        """"""Score new token (required).\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): The encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        """"""\n        raise NotImplementedError\n'"
espnet/nets/st_interface.py,3,"b'""""""ST Interface module.""""""\n\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.utils.dynamic_import import dynamic_import\n\n\nclass STInterface(ASRInterface):\n    """"""ST Interface for ESPnet model implementation.\n\n    NOTE: This class is inherited from ASRInterface to enable joint translation\n    and recognition when performing multi-task learning with the ASR task.\n\n    """"""\n\n    def translate(self, x, trans_args, char_list=None, rnnlm=None, ensemble_models=[]):\n        """"""Recognize x for evaluation.\n\n        :param ndarray x: input acouctic feature (B, T, D) or (T, D)\n        :param namespace trans_args: argment namespace contraining options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        raise NotImplementedError(""translate method is not implemented"")\n\n    def translate_batch(self, x, trans_args, char_list=None, rnnlm=None):\n        """"""Beam search implementation for batch.\n\n        :param torch.Tensor x: encoder hidden state sequences (B, Tmax, Henc)\n        :param namespace trans_args: argument namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        raise NotImplementedError(""Batch decoding is not supported yet."")\n\n\npredefined_st = {\n    ""pytorch"": {\n        ""rnn"": ""espnet.nets.pytorch_backend.e2e_st:E2E"",\n        ""transformer"": ""espnet.nets.pytorch_backend.e2e_st_transformer:E2E"",\n    },\n    # ""chainer"": {\n    #     ""rnn"": ""espnet.nets.chainer_backend.e2e_st:E2E"",\n    #     ""transformer"": ""espnet.nets.chainer_backend.e2e_st_transformer:E2E"",\n    # }\n}\n\n\ndef dynamic_import_st(module, backend):\n    """"""Import ST models dynamically.\n\n    Args:\n        module (str): module_name:class_name or alias in `predefined_st`\n        backend (str): NN backend. e.g., pytorch, chainer\n\n    Returns:\n        type: ST class\n\n    """"""\n    model_class = dynamic_import(module, predefined_st.get(backend, dict()))\n    assert issubclass(\n        model_class, STInterface\n    ), f""{module} does not implement STInterface""\n    return model_class\n'"
espnet/nets/tts_interface.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""TTS Interface realted modules.""""""\n\nimport chainer\n\nfrom espnet.asr.asr_utils import torch_load\n\n\nclass Reporter(chainer.Chain):\n    """"""Reporter module.""""""\n\n    def report(self, dicts):\n        """"""Report values from a given dict.""""""\n        for d in dicts:\n            chainer.reporter.report(d, self)\n\n\nclass TTSInterface(object):\n    """"""TTS Interface for ESPnet model implementation.""""""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add model specific argments to parser.""""""\n        return parser\n\n    def __init__(self):\n        """"""Initilize TTS module.""""""\n        self.reporter = Reporter()\n\n    def forward(self, *args, **kwargs):\n        """"""Calculate TTS forward propagation.\n\n        Returns:\n            Tensor: Loss value.\n\n        """"""\n        raise NotImplementedError(""forward method is not implemented"")\n\n    def inference(self, *args, **kwargs):\n        """"""Generate the sequence of features given the sequences of characters.\n\n        Returns:\n            Tensor: The sequence of generated features (L, odim).\n            Tensor: The sequence of stop probabilities (L,).\n            Tensor: The sequence of attention weights (L, T).\n\n        """"""\n        raise NotImplementedError(""inference method is not implemented"")\n\n    def calculate_all_attentions(self, *args, **kwargs):\n        """"""Calculate TTS attention weights.\n\n        Args:\n            Tensor: Batch of attention weights (B, Lmax, Tmax).\n\n        """"""\n        raise NotImplementedError(""calculate_all_attentions method is not implemented"")\n\n    def load_pretrained_model(self, model_path):\n        """"""Load pretrained model parameters.""""""\n        torch_load(model_path, self)\n\n    @property\n    def attention_plot_class(self):\n        """"""Plot attention weights.""""""\n        from espnet.asr.asr_utils import PlotAttentionReport\n\n        return PlotAttentionReport\n\n    @property\n    def base_plot_keys(self):\n        """"""Return base key names to plot during training.\n\n        The keys should match what `chainer.reporter` reports.\n        if you add the key `loss`,\n        the reporter will report `main/loss` and `validation/main/loss` values.\n        also `loss.png` will be created as a figure visulizing `main/loss`\n        and `validation/main/loss` values.\n\n        Returns:\n            list[str]:  Base keys to plot during training.\n\n        """"""\n        return [""loss""]\n'"
espnet/optimizer/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/optimizer/chainer.py,0,"b'""""""Chainer optimizer builders.""""""\nimport argparse\n\nimport chainer\nfrom chainer.optimizer_hooks import WeightDecay\n\nfrom espnet.optimizer.factory import OptimizerFactoryInterface\nfrom espnet.optimizer.parser import adadelta\nfrom espnet.optimizer.parser import adam\nfrom espnet.optimizer.parser import sgd\n\n\nclass AdamFactory(OptimizerFactoryInterface):\n    """"""Adam factory.""""""\n\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        """"""Register args.""""""\n        return adam(parser)\n\n    @staticmethod\n    def from_args(target, args: argparse.Namespace):\n        """"""Initialize optimizer from argparse Namespace.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n            args (argparse.Namespace): parsed command-line args\n\n        """"""\n        opt = chainer.optimizers.Adam(\n            alpha=args.lr, beta1=args.beta1, beta2=args.beta2,\n        )\n        opt.setup(target)\n        opt.add_hook(WeightDecay(args.weight_decay))\n        return opt\n\n\nclass SGDFactory(OptimizerFactoryInterface):\n    """"""SGD factory.""""""\n\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        """"""Register args.""""""\n        return sgd(parser)\n\n    @staticmethod\n    def from_args(target, args: argparse.Namespace):\n        """"""Initialize optimizer from argparse Namespace.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n            args (argparse.Namespace): parsed command-line args\n\n        """"""\n        opt = chainer.optimizers.SGD(lr=args.lr,)\n        opt.setup(target)\n        opt.add_hook(WeightDecay(args.weight_decay))\n        return opt\n\n\nclass AdadeltaFactory(OptimizerFactoryInterface):\n    """"""Adadelta factory.""""""\n\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        """"""Register args.""""""\n        return adadelta(parser)\n\n    @staticmethod\n    def from_args(target, args: argparse.Namespace):\n        """"""Initialize optimizer from argparse Namespace.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n            args (argparse.Namespace): parsed command-line args\n\n        """"""\n        opt = chainer.optimizers.AdaDelta(rho=args.rho, eps=args.eps,)\n        opt.setup(target)\n        opt.add_hook(WeightDecay(args.weight_decay))\n        return opt\n\n\nOPTIMIZER_FACTORY_DICT = {\n    ""adam"": AdamFactory,\n    ""sgd"": SGDFactory,\n    ""adadelta"": AdadeltaFactory,\n}\n'"
espnet/optimizer/factory.py,0,"b'""""""Import optimizer class dynamically.""""""\nimport argparse\n\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass OptimizerFactoryInterface:\n    """"""Optimizer adaptor.""""""\n\n    @staticmethod\n    def from_args(target, args: argparse.Namespace):\n        """"""Initialize optimizer from argparse Namespace.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n            args (argparse.Namespace): parsed command-line args\n\n        """"""\n        raise NotImplementedError()\n\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        """"""Register args.""""""\n        return parser\n\n    @classmethod\n    def build(cls, target, **kwargs):\n        """"""Initialize optimizer with python-level args.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n\n        Returns:\n            new Optimizer\n\n        """"""\n        args = argparse.Namespace(**kwargs)\n        args = fill_missing_args(args, cls.add_arguments)\n        return cls.from_args(target, args)\n\n\ndef dynamic_import_optimizer(name: str, backend: str) -> OptimizerFactoryInterface:\n    """"""Import optimizer class dynamically.\n\n    Args:\n        name (str): alias name or dynamic import syntax `module:class`\n        backend (str): backend name e.g., chainer or pytorch\n\n    Returns:\n        OptimizerFactoryInterface or FunctionalOptimizerAdaptor\n\n    """"""\n    if backend == ""pytorch"":\n        from espnet.optimizer.pytorch import OPTIMIZER_FACTORY_DICT\n\n        return OPTIMIZER_FACTORY_DICT[name]\n    elif backend == ""chainer"":\n        from espnet.optimizer.chainer import OPTIMIZER_FACTORY_DICT\n\n        return OPTIMIZER_FACTORY_DICT[name]\n    else:\n        raise NotImplementedError(f""unsupported backend: {backend}"")\n\n    factory_class = dynamic_import(name)\n    assert issubclass(factory_class, OptimizerFactoryInterface)\n    return factory_class\n'"
espnet/optimizer/parser.py,0,"b'""""""Common optimizer default config for multiple backends.""""""\n\n\ndef sgd(parser):\n    """"""Add arguments.""""""\n    parser.add_argument(""--lr"", type=float, default=1.0, help=""Learning rate"")\n    parser.add_argument(""--weight-decay"", type=float, default=0.0, help=""Weight decay"")\n    return parser\n\n\ndef adam(parser):\n    """"""Add arguments.""""""\n    parser.add_argument(""--lr"", type=float, default=1e-3, help=""Learning rate"")\n    parser.add_argument(""--beta1"", type=float, default=0.9, help=""Beta1"")\n    parser.add_argument(""--beta2"", type=float, default=0.999, help=""Beta2"")\n    parser.add_argument(""--weight-decay"", type=float, default=0.0, help=""Weight decay"")\n    return parser\n\n\ndef adadelta(parser):\n    """"""Add arguments.""""""\n    parser.add_argument(""--rho"", type=float, default=0.95, help=""Rho"")\n    parser.add_argument(""--eps"", type=float, default=1e-8, help=""Eps"")\n    parser.add_argument(""--weight-decay"", type=float, default=0.0, help=""Weight decay"")\n    return parser\n'"
espnet/optimizer/pytorch.py,3,"b'""""""PyTorch optimizer builders.""""""\nimport argparse\n\nimport torch\n\nfrom espnet.optimizer.factory import OptimizerFactoryInterface\nfrom espnet.optimizer.parser import adadelta\nfrom espnet.optimizer.parser import adam\nfrom espnet.optimizer.parser import sgd\n\n\nclass AdamFactory(OptimizerFactoryInterface):\n    """"""Adam factory.""""""\n\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        """"""Register args.""""""\n        return adam(parser)\n\n    @staticmethod\n    def from_args(target, args: argparse.Namespace):\n        """"""Initialize optimizer from argparse Namespace.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n            args (argparse.Namespace): parsed command-line args\n\n        """"""\n        return torch.optim.Adam(\n            target,\n            lr=args.lr,\n            weight_decay=args.weight_decay,\n            betas=(args.beta1, args.beta2),\n        )\n\n\nclass SGDFactory(OptimizerFactoryInterface):\n    """"""SGD factory.""""""\n\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        """"""Register args.""""""\n        return sgd(parser)\n\n    @staticmethod\n    def from_args(target, args: argparse.Namespace):\n        """"""Initialize optimizer from argparse Namespace.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n            args (argparse.Namespace): parsed command-line args\n\n        """"""\n        return torch.optim.SGD(target, lr=args.lr, weight_decay=args.weight_decay,)\n\n\nclass AdadeltaFactory(OptimizerFactoryInterface):\n    """"""Adadelta factory.""""""\n\n    @staticmethod\n    def add_arguments(parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        """"""Register args.""""""\n        return adadelta(parser)\n\n    @staticmethod\n    def from_args(target, args: argparse.Namespace):\n        """"""Initialize optimizer from argparse Namespace.\n\n        Args:\n            target: for pytorch `model.parameters()`,\n                for chainer `model`\n            args (argparse.Namespace): parsed command-line args\n\n        """"""\n        return torch.optim.Adadelta(\n            target, rho=args.rho, eps=args.eps, weight_decay=args.weight_decay,\n        )\n\n\nOPTIMIZER_FACTORY_DICT = {\n    ""adam"": AdamFactory,\n    ""sgd"": SGDFactory,\n    ""adadelta"": AdadeltaFactory,\n}\n'"
espnet/scheduler/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/scheduler/chainer.py,0,"b'""""""Chainer optimizer schdulers.""""""\n\nfrom typing import List\n\nfrom chainer.optimizer import Optimizer\n\nfrom espnet.scheduler.scheduler import SchedulerInterface\n\n\nclass ChainerScheduler:\n    """"""Chainer optimizer scheduler.""""""\n\n    def __init__(self, schedulers: List[SchedulerInterface], optimizer: Optimizer):\n        """"""Initialize class.""""""\n        self.schedulers = schedulers\n        self.optimizer = optimizer\n        self.init_values = dict()\n        for s in self.schedulers:\n            self.init_values[s.key] = getattr(self.optimizer, s.key)\n\n    def step(self, n_iter: int):\n        """"""Update optimizer by scheduling.""""""\n        for s in self.schedulers:\n            new_val = self.init_values[s.key] * s.scale(n_iter)\n            setattr(self.optimizer, s.key, new_val)\n'"
espnet/scheduler/pytorch.py,1,"b'""""""PyTorch optimizer schdulers.""""""\n\nfrom typing import List\n\nfrom torch.optim import Optimizer\n\nfrom espnet.scheduler.scheduler import SchedulerInterface\n\n\nclass PyTorchScheduler:\n    """"""PyTorch optimizer scheduler.""""""\n\n    def __init__(self, schedulers: List[SchedulerInterface], optimizer: Optimizer):\n        """"""Initialize class.""""""\n        self.schedulers = schedulers\n        self.optimizer = optimizer\n        for s in self.schedulers:\n            for group in optimizer.param_groups:\n                group.setdefault(""initial_"" + s.key, group[s.key])\n\n    def step(self, n_iter: int):\n        """"""Update optimizer by scheduling.""""""\n        for s in self.schedulers:\n            for group in self.optimizer.param_groups:\n                group[s.key] = group[""initial_"" + s.key] * s.scale(n_iter)\n'"
espnet/scheduler/scheduler.py,0,"b'""""""Schedulers.""""""\n\nimport argparse\n\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass _PrefixParser:\n    def __init__(self, parser, prefix):\n        self.parser = parser\n        self.prefix = prefix\n\n    def add_argument(self, name, **kwargs):\n        assert name.startswith(""--"")\n        self.parser.add_argument(self.prefix + name[2:], **kwargs)\n\n\nclass SchedulerInterface:\n    """"""Scheduler interface.""""""\n\n    alias = """"\n\n    def __init__(self, key: str, args: argparse.Namespace):\n        """"""Initialize class.""""""\n        self.key = key\n        prefix = key + ""_"" + self.alias + ""_""\n        for k, v in vars(args).items():\n            if k.startswith(prefix):\n                setattr(self, k[len(prefix) :], v)\n\n    def get_arg(self, name):\n        """"""Get argument without prefix.""""""\n        return getattr(self.args, f""{self.key}_{self.alias}_{name}"")\n\n    @classmethod\n    def add_arguments(cls, key: str, parser: argparse.ArgumentParser):\n        """"""Add arguments for CLI.""""""\n        group = parser.add_argument_group(f""{cls.alias} scheduler"")\n        cls._add_arguments(_PrefixParser(parser=group, prefix=f""--{key}-{cls.alias}-""))\n        return parser\n\n    @staticmethod\n    def _add_arguments(parser: _PrefixParser):\n        pass\n\n    @classmethod\n    def build(cls, key: str, **kwargs):\n        """"""Initialize this class with python-level args.\n\n        Args:\n            key (str): key of hyper parameter\n\n        Returns:\n            LMinterface: A new instance of LMInterface.\n\n        """"""\n\n        def add(parser):\n            return cls.add_arguments(key, parser)\n\n        kwargs = {f""{key}_{cls.alias}_"" + k: v for k, v in kwargs.items()}\n        args = argparse.Namespace(**kwargs)\n        args = fill_missing_args(args, add)\n        return cls(key, args)\n\n    def scale(self, n_iter: int) -> float:\n        """"""Scale at `n_iter`.\n\n        Args:\n            n_iter (int): number of current iterations.\n\n        Returns:\n            float: current scale of learning rate.\n\n        """"""\n        raise NotImplementedError()\n\n\nSCHEDULER_DICT = {}\n\n\ndef register_scheduler(cls):\n    """"""Register scheduler.""""""\n    SCHEDULER_DICT[cls.alias] = cls.__module__ + "":"" + cls.__name__\n    return cls\n\n\ndef dynamic_import_scheduler(module):\n    """"""Import Scheduler class dynamically.\n\n    Args:\n        module (str): module_name:class_name or alias in `SCHEDULER_DICT`\n\n    Returns:\n        type: Scheduler class\n\n    """"""\n    model_class = dynamic_import(module, SCHEDULER_DICT)\n    assert issubclass(\n        model_class, SchedulerInterface\n    ), f""{module} does not implement SchedulerInterface""\n    return model_class\n\n\n@register_scheduler\nclass NoScheduler(SchedulerInterface):\n    """"""Scheduler which does nothing.""""""\n\n    alias = ""none""\n\n    def scale(self, n_iter):\n        """"""Scale of lr.""""""\n        return 1.0\n\n\n@register_scheduler\nclass NoamScheduler(SchedulerInterface):\n    """"""Warmup + InverseSqrt decay scheduler.\n\n    Args:\n        noam_warmup (int): number of warmup iterations.\n\n    """"""\n\n    alias = ""noam""\n\n    @staticmethod\n    def _add_arguments(parser: _PrefixParser):\n        """"""Add scheduler args.""""""\n        parser.add_argument(\n            ""--warmup"", type=int, default=1000, help=""Number of warmup iterations.""\n        )\n\n    def __init__(self, key, args):\n        """"""Initialize class.""""""\n        super().__init__(key, args)\n        self.normalize = 1 / (self.warmup * self.warmup ** -1.5)\n\n    def scale(self, step):\n        """"""Scale of lr.""""""\n        step += 1  # because step starts from 0\n        return self.normalize * min(step ** -0.5, step * self.warmup ** -1.5)\n\n\n@register_scheduler\nclass CyclicCosineScheduler(SchedulerInterface):\n    """"""Cyclic cosine annealing.\n\n    Args:\n        cosine_warmup (int): number of warmup iterations.\n        cosine_total (int): number of total annealing iterations.\n\n    Notes:\n        Proposed in https://openreview.net/pdf?id=BJYwwY9ll\n        (and https://arxiv.org/pdf/1608.03983.pdf).\n        Used in the GPT2 config of Megatron-LM https://github.com/NVIDIA/Megatron-LM\n\n    """"""\n\n    alias = ""cosine""\n\n    @staticmethod\n    def _add_arguments(parser: _PrefixParser):\n        """"""Add scheduler args.""""""\n        parser.add_argument(\n            ""--warmup"", type=int, default=1000, help=""Number of warmup iterations.""\n        )\n        parser.add_argument(\n            ""--total"",\n            type=int,\n            default=100000,\n            help=""Number of total annealing iterations."",\n        )\n\n    def scale(self, n_iter):\n        """"""Scale of lr.""""""\n        import math\n\n        return 0.5 * (math.cos(math.pi * (n_iter - self.warmup) / self.total) + 1)\n'"
espnet/st/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/transform/__init__.py,0,"b'""""""Initialize main package.""""""\n'"
espnet/transform/add_deltas.py,0,"b'import numpy as np\n\n\ndef delta(feat, window):\n    assert window > 0\n    delta_feat = np.zeros_like(feat)\n    for i in range(1, window + 1):\n        delta_feat[:-i] += i * feat[i:]\n        delta_feat[i:] += -i * feat[:-i]\n        delta_feat[-i:] += i * feat[-1]\n        delta_feat[:i] += -i * feat[0]\n    delta_feat /= 2 * sum(i ** 2 for i in range(1, window + 1))\n    return delta_feat\n\n\ndef add_deltas(x, window=2, order=2):\n    feats = [x]\n    for _ in range(order):\n        feats.append(delta(feats[-1], window))\n    return np.concatenate(feats, axis=1)\n\n\nclass AddDeltas(object):\n    def __init__(self, window=2, order=2):\n        self.window = window\n        self.order = order\n\n    def __repr__(self):\n        return ""{name}(window={window}, order={order}"".format(\n            name=self.__class__.__name__, window=self.window, order=self.order\n        )\n\n    def __call__(self, x):\n        return add_deltas(x, window=self.window, order=self.order)\n'"
espnet/transform/channel_selector.py,0,"b'import numpy\n\n\nclass ChannelSelector(object):\n    """"""Select 1ch from multi-channel signal """"""\n\n    def __init__(self, train_channel=""random"", eval_channel=0, axis=1):\n        self.train_channel = train_channel\n        self.eval_channel = eval_channel\n        self.axis = axis\n\n    def __repr__(self):\n        return (\n            ""{name}(train_channel={train_channel}, ""\n            ""eval_channel={eval_channel}, axis={axis})"".format(\n                name=self.__class__.__name__,\n                train_channel=self.train_channel,\n                eval_channel=self.eval_channel,\n                axis=self.axis,\n            )\n        )\n\n    def __call__(self, x, train=True):\n        # Assuming x: [Time, Channel] by default\n\n        if x.ndim <= self.axis:\n            # If the dimension is insufficient, then unsqueeze\n            # (e.g [Time] -> [Time, 1])\n            ind = tuple(\n                slice(None) if i < x.ndim else None for i in range(self.axis + 1)\n            )\n            x = x[ind]\n\n        if train:\n            channel = self.train_channel\n        else:\n            channel = self.eval_channel\n\n        if channel == ""random"":\n            ch = numpy.random.randint(0, x.shape[self.axis])\n        else:\n            ch = channel\n\n        ind = tuple(slice(None) if i != self.axis else ch for i in range(x.ndim))\n        return x[ind]\n'"
espnet/transform/cmvn.py,0,"b'import io\n\nimport h5py\nimport kaldiio\nimport numpy as np\n\n\nclass CMVN(object):\n    def __init__(\n        self,\n        stats,\n        norm_means=True,\n        norm_vars=False,\n        filetype=""mat"",\n        utt2spk=None,\n        spk2utt=None,\n        reverse=False,\n        std_floor=1.0e-20,\n    ):\n        self.stats_file = stats\n        self.norm_means = norm_means\n        self.norm_vars = norm_vars\n        self.reverse = reverse\n\n        if isinstance(stats, dict):\n            stats_dict = dict(stats)\n        else:\n            # Use for global CMVN\n            if filetype == ""mat"":\n                stats_dict = {None: kaldiio.load_mat(stats)}\n            # Use for global CMVN\n            elif filetype == ""npy"":\n                stats_dict = {None: np.load(stats)}\n            # Use for speaker CMVN\n            elif filetype == ""ark"":\n                self.accept_uttid = True\n                stats_dict = dict(kaldiio.load_ark(stats))\n            # Use for speaker CMVN\n            elif filetype == ""hdf5"":\n                self.accept_uttid = True\n                stats_dict = h5py.File(stats)\n            else:\n                raise ValueError(""Not supporting filetype={}"".format(filetype))\n\n        if utt2spk is not None:\n            self.utt2spk = {}\n            with io.open(utt2spk, ""r"", encoding=""utf-8"") as f:\n                for line in f:\n                    utt, spk = line.rstrip().split(None, 1)\n                    self.utt2spk[utt] = spk\n        elif spk2utt is not None:\n            self.utt2spk = {}\n            with io.open(spk2utt, ""r"", encoding=""utf-8"") as f:\n                for line in f:\n                    spk, utts = line.rstrip().split(None, 1)\n                    for utt in utts.split():\n                        self.utt2spk[utt] = spk\n        else:\n            self.utt2spk = None\n\n        # Kaldi makes a matrix for CMVN which has a shape of (2, feat_dim + 1),\n        # and the first vector contains the sum of feats and the second is\n        # the sum of squares. The last value of the first, i.e. stats[0,-1],\n        # is the number of samples for this statistics.\n        self.bias = {}\n        self.scale = {}\n        for spk, stats in stats_dict.items():\n            assert len(stats) == 2, stats.shape\n\n            count = stats[0, -1]\n\n            # If the feature has two or more dimensions\n            if not (np.isscalar(count) or isinstance(count, (int, float))):\n                # The first is only used\n                count = count.flatten()[0]\n\n            mean = stats[0, :-1] / count\n            # V(x) = E(x^2) - (E(x))^2\n            var = stats[1, :-1] / count - mean * mean\n            std = np.maximum(np.sqrt(var), std_floor)\n            self.bias[spk] = -mean\n            self.scale[spk] = 1 / std\n\n    def __repr__(self):\n        return (\n            ""{name}(stats_file={stats_file}, ""\n            ""norm_means={norm_means}, norm_vars={norm_vars}, ""\n            ""reverse={reverse})"".format(\n                name=self.__class__.__name__,\n                stats_file=self.stats_file,\n                norm_means=self.norm_means,\n                norm_vars=self.norm_vars,\n                reverse=self.reverse,\n            )\n        )\n\n    def __call__(self, x, uttid=None):\n        if self.utt2spk is not None:\n            spk = self.utt2spk[uttid]\n        else:\n            spk = uttid\n\n        if not self.reverse:\n            if self.norm_means:\n                x = np.add(x, self.bias[spk])\n            if self.norm_vars:\n                x = np.multiply(x, self.scale[spk])\n\n        else:\n            if self.norm_vars:\n                x = np.divide(x, self.scale[spk])\n            if self.norm_means:\n                x = np.subtract(x, self.bias[spk])\n\n        return x\n\n\nclass UtteranceCMVN(object):\n    def __init__(self, norm_means=True, norm_vars=False, std_floor=1.0e-20):\n        self.norm_means = norm_means\n        self.norm_vars = norm_vars\n        self.std_floor = std_floor\n\n    def __repr__(self):\n        return ""{name}(norm_means={norm_means}, norm_vars={norm_vars})"".format(\n            name=self.__class__.__name__,\n            norm_means=self.norm_means,\n            norm_vars=self.norm_vars,\n        )\n\n    def __call__(self, x, uttid=None):\n        # x: [Time, Dim]\n        square_sums = (x ** 2).sum(axis=0)\n        mean = x.mean(axis=0)\n\n        if self.norm_means:\n            x = np.subtract(x, mean)\n\n        if self.norm_vars:\n            var = square_sums / x.shape[0] - mean ** 2\n            std = np.maximum(np.sqrt(var), self.std_floor)\n            x = np.divide(x, std)\n\n        return x\n'"
espnet/transform/functional.py,0,"b'import inspect\n\nfrom espnet.transform.transform_interface import TransformInterface\nfrom espnet.utils.check_kwargs import check_kwargs\n\n\nclass FuncTrans(TransformInterface):\n    """"""Functional Transformation\n\n    WARNING:\n        Builtin or C/C++ functions may not work properly\n        because this class heavily depends on the `inspect` module.\n\n    Usage:\n\n    >>> def foo_bar(x, a=1, b=2):\n    ...     \'\'\'Foo bar\n    ...     :param x: input\n    ...     :param int a: default 1\n    ...     :param int b: default 2\n    ...     \'\'\'\n    ...     return x + a - b\n\n\n    >>> class FooBar(FuncTrans):\n    ...     _func = foo_bar\n    ...     __doc__ = foo_bar.__doc__\n    """"""\n\n    _func = None\n\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n        check_kwargs(self.func, kwargs)\n\n    def __call__(self, x):\n        return self.func(x, **self.kwargs)\n\n    @classmethod\n    def add_arguments(cls, parser):\n        fname = cls._func.__name__.replace(""_"", ""-"")\n        group = parser.add_argument_group(fname + "" transformation setting"")\n        for k, v in cls.default_params().items():\n            # TODO(karita): get help and choices from docstring?\n            attr = k.replace(""_"", ""-"")\n            group.add_argument(f""--{fname}-{attr}"", default=v, type=type(v))\n        return parser\n\n    @property\n    def func(self):\n        return type(self)._func\n\n    @classmethod\n    def default_params(cls):\n        try:\n            d = dict(inspect.signature(cls._func).parameters)\n        except ValueError:\n            d = dict()\n        return {\n            k: v.default for k, v in d.items() if v.default != inspect.Parameter.empty\n        }\n\n    def __repr__(self):\n        params = self.default_params()\n        params.update(**self.kwargs)\n        ret = self.__class__.__name__ + ""(""\n        if len(params) == 0:\n            return ret + "")""\n        for k, v in params.items():\n            ret += ""{}={}, "".format(k, v)\n        return ret[:-2] + "")""\n'"
espnet/transform/perturb.py,0,"b'import librosa\nimport numpy\nimport scipy\nimport soundfile\n\nfrom espnet.utils.io_utils import SoundHDF5File\n\n\nclass SpeedPerturbation(object):\n    """"""SpeedPerturbation\n\n    The speed perturbation in kaldi uses sox-speed instead of sox-tempo,\n    and sox-speed just to resample the input,\n    i.e pitch and tempo are changed both.\n\n    ""Why use speed option instead of tempo -s in SoX for speed perturbation""\n    https://groups.google.com/forum/#!topic/kaldi-help/8OOG7eE4sZ8\n\n    Warning:\n        This function is very slow because of resampling.\n        I recommmend to apply speed-perturb outside the training using sox.\n\n    """"""\n\n    def __init__(\n        self,\n        lower=0.9,\n        upper=1.1,\n        utt2ratio=None,\n        keep_length=True,\n        res_type=""kaiser_best"",\n        seed=None,\n    ):\n        self.res_type = res_type\n        self.keep_length = keep_length\n        self.state = numpy.random.RandomState(seed)\n\n        if utt2ratio is not None:\n            self.utt2ratio = {}\n            # Use the scheduled ratio for each utterances\n            self.utt2ratio_file = utt2ratio\n            self.lower = None\n            self.upper = None\n            self.accept_uttid = True\n\n            with open(utt2ratio, ""r"") as f:\n                for line in f:\n                    utt, ratio = line.rstrip().split(None, 1)\n                    ratio = float(ratio)\n                    self.utt2ratio[utt] = ratio\n        else:\n            self.utt2ratio = None\n            # The ratio is given on runtime randomly\n            self.lower = lower\n            self.upper = upper\n\n    def __repr__(self):\n        if self.utt2ratio is None:\n            return ""{}(lower={}, upper={}, "" ""keep_length={}, res_type={})"".format(\n                self.__class__.__name__,\n                self.lower,\n                self.upper,\n                self.keep_length,\n                self.res_type,\n            )\n        else:\n            return ""{}({}, res_type={})"".format(\n                self.__class__.__name__, self.utt2ratio_file, self.res_type\n            )\n\n    def __call__(self, x, uttid=None, train=True):\n        if not train:\n            return x\n\n        x = x.astype(numpy.float32)\n        if self.accept_uttid:\n            ratio = self.utt2ratio[uttid]\n        else:\n            ratio = self.state.uniform(self.lower, self.upper)\n\n        # Note1: resample requires the sampling-rate of input and output,\n        #        but actually only the ratio is used.\n        y = librosa.resample(x, ratio, 1, res_type=self.res_type)\n\n        if self.keep_length:\n            diff = abs(len(x) - len(y))\n            if len(y) > len(x):\n                # Truncate noise\n                y = y[diff // 2 : -((diff + 1) // 2)]\n            elif len(y) < len(x):\n                # Assume the time-axis is the first: (Time, Channel)\n                pad_width = [(diff // 2, (diff + 1) // 2)] + [\n                    (0, 0) for _ in range(y.ndim - 1)\n                ]\n                y = numpy.pad(\n                    y, pad_width=pad_width, constant_values=0, mode=""constant""\n                )\n        return y\n\n\nclass BandpassPerturbation(object):\n    """"""BandpassPerturbation\n\n    Randomly dropout along the frequency axis.\n\n    The original idea comes from the following:\n        ""randomly-selected frequency band was cut off under the constraint of\n         leaving at least 1,000 Hz band within the range of less than 4,000Hz.""\n        (The Hitachi/JHU CHiME-5 system: Advances in speech recognition for\n         everyday home environments using multiple microphone arrays;\n         http://spandh.dcs.shef.ac.uk/chime_workshop/papers/CHiME_2018_paper_kanda.pdf)\n\n    """"""\n\n    def __init__(self, lower=0.0, upper=0.75, seed=None, axes=(-1,)):\n        self.lower = lower\n        self.upper = upper\n        self.state = numpy.random.RandomState(seed)\n        # x_stft: (Time, Channel, Freq)\n        self.axes = axes\n\n    def __repr__(self):\n        return ""{}(lower={}, upper={})"".format(\n            self.__class__.__name__, self.lower, self.upper\n        )\n\n    def __call__(self, x_stft, uttid=None, train=True):\n        if not train:\n            return x_stft\n\n        if x_stft.ndim == 1:\n            raise RuntimeError(\n                ""Input in time-freq domain: "" ""(Time, Channel, Freq) or (Time, Freq)""\n            )\n\n        ratio = self.state.uniform(self.lower, self.upper)\n        axes = [i if i >= 0 else x_stft.ndim - i for i in self.axes]\n        shape = [s if i in axes else 1 for i, s in enumerate(x_stft.shape)]\n\n        mask = self.state.randn(*shape) > ratio\n        x_stft *= mask\n        return x_stft\n\n\nclass VolumePerturbation(object):\n    def __init__(self, lower=-1.6, upper=1.6, utt2ratio=None, dbunit=True, seed=None):\n        self.dbunit = dbunit\n        self.utt2ratio_file = utt2ratio\n        self.lower = lower\n        self.upper = upper\n        self.state = numpy.random.RandomState(seed)\n\n        if utt2ratio is not None:\n            # Use the scheduled ratio for each utterances\n            self.utt2ratio = {}\n            self.lower = None\n            self.upper = None\n            self.accept_uttid = True\n\n            with open(utt2ratio, ""r"") as f:\n                for line in f:\n                    utt, ratio = line.rstrip().split(None, 1)\n                    ratio = float(ratio)\n                    self.utt2ratio[utt] = ratio\n        else:\n            # The ratio is given on runtime randomly\n            self.utt2ratio = None\n\n    def __repr__(self):\n        if self.utt2ratio is None:\n            return ""{}(lower={}, upper={}, dbunit={})"".format(\n                self.__class__.__name__, self.lower, self.upper, self.dbunit\n            )\n        else:\n            return \'{}(""{}"", dbunit={})\'.format(\n                self.__class__.__name__, self.utt2ratio_file, self.dbunit\n            )\n\n    def __call__(self, x, uttid=None, train=True):\n        if not train:\n            return x\n\n        x = x.astype(numpy.float32)\n\n        if self.accept_uttid:\n            ratio = self.utt2ratio[uttid]\n        else:\n            ratio = self.state.uniform(self.lower, self.upper)\n        if self.dbunit:\n            ratio = 10 ** (ratio / 20)\n        return x * ratio\n\n\nclass NoiseInjection(object):\n    """"""Add isotropic noise""""""\n\n    def __init__(\n        self,\n        utt2noise=None,\n        lower=-20,\n        upper=-5,\n        utt2ratio=None,\n        filetype=""list"",\n        dbunit=True,\n        seed=None,\n    ):\n        self.utt2noise_file = utt2noise\n        self.utt2ratio_file = utt2ratio\n        self.filetype = filetype\n        self.dbunit = dbunit\n        self.lower = lower\n        self.upper = upper\n        self.state = numpy.random.RandomState(seed)\n\n        if utt2ratio is not None:\n            # Use the scheduled ratio for each utterances\n            self.utt2ratio = {}\n            with open(utt2noise, ""r"") as f:\n                for line in f:\n                    utt, snr = line.rstrip().split(None, 1)\n                    snr = float(snr)\n                    self.utt2ratio[utt] = snr\n        else:\n            # The ratio is given on runtime randomly\n            self.utt2ratio = None\n\n        if utt2noise is not None:\n            self.utt2noise = {}\n            if filetype == ""list"":\n                with open(utt2noise, ""r"") as f:\n                    for line in f:\n                        utt, filename = line.rstrip().split(None, 1)\n                        signal, rate = soundfile.read(filename, dtype=""int16"")\n                        # Load all files in memory\n                        self.utt2noise[utt] = (signal, rate)\n\n            elif filetype == ""sound.hdf5"":\n                self.utt2noise = SoundHDF5File(utt2noise, ""r"")\n            else:\n                raise ValueError(filetype)\n        else:\n            self.utt2noise = None\n\n        if utt2noise is not None and utt2ratio is not None:\n            if set(self.utt2ratio) != set(self.utt2noise):\n                raise RuntimeError(\n                    ""The uttids mismatch between {} and {}"".format(utt2ratio, utt2noise)\n                )\n\n    def __repr__(self):\n        if self.utt2ratio is None:\n            return ""{}(lower={}, upper={}, dbunit={})"".format(\n                self.__class__.__name__, self.lower, self.upper, self.dbunit\n            )\n        else:\n            return \'{}(""{}"", dbunit={})\'.format(\n                self.__class__.__name__, self.utt2ratio_file, self.dbunit\n            )\n\n    def __call__(self, x, uttid=None, train=True):\n        if not train:\n            return x\n        x = x.astype(numpy.float32)\n\n        # 1. Get ratio of noise to signal in sound pressure level\n        if uttid is not None and self.utt2ratio is not None:\n            ratio = self.utt2ratio[uttid]\n        else:\n            ratio = self.state.uniform(self.lower, self.upper)\n\n        if self.dbunit:\n            ratio = 10 ** (ratio / 20)\n        scale = ratio * numpy.sqrt((x ** 2).mean())\n\n        # 2. Get noise\n        if self.utt2noise is not None:\n            # Get noise from the external source\n            if uttid is not None:\n                noise, rate = self.utt2noise[uttid]\n            else:\n                # Randomly select the noise source\n                noise = self.state.choice(list(self.utt2noise.values()))\n            # Normalize the level\n            noise /= numpy.sqrt((noise ** 2).mean())\n\n            # Adjust the noise length\n            diff = abs(len(x) - len(noise))\n            offset = self.state.randint(0, diff)\n            if len(noise) > len(x):\n                # Truncate noise\n                noise = noise[offset : -(diff - offset)]\n            else:\n                noise = numpy.pad(noise, pad_width=[offset, diff - offset], mode=""wrap"")\n\n        else:\n            # Generate white noise\n            noise = self.state.normal(0, 1, x.shape)\n\n        # 3. Add noise to signal\n        return x + noise * scale\n\n\nclass RIRConvolve(object):\n    def __init__(self, utt2rir, filetype=""list""):\n        self.utt2rir_file = utt2rir\n        self.filetype = filetype\n\n        self.utt2rir = {}\n        if filetype == ""list"":\n            with open(utt2rir, ""r"") as f:\n                for line in f:\n                    utt, filename = line.rstrip().split(None, 1)\n                    signal, rate = soundfile.read(filename, dtype=""int16"")\n                    self.utt2rir[utt] = (signal, rate)\n\n        elif filetype == ""sound.hdf5"":\n            self.utt2rir = SoundHDF5File(utt2rir, ""r"")\n        else:\n            raise NotImplementedError(filetype)\n\n    def __repr__(self):\n        return \'{}(""{}"")\'.format(self.__class__.__name__, self.utt2rir_file)\n\n    def __call__(self, x, uttid=None, train=True):\n        if not train:\n            return x\n\n        x = x.astype(numpy.float32)\n\n        if x.ndim != 1:\n            # Must be single channel\n            raise RuntimeError(\n                ""Input x must be one dimensional array, but got {}"".format(x.shape)\n            )\n\n        rir, rate = self.utt2rir[uttid]\n        if rir.ndim == 2:\n            # FIXME(kamo): Use chainer.convolution_1d?\n            # return [Time, Channel]\n            return numpy.stack(\n                [scipy.convolve(x, r, mode=""same"") for r in rir], axis=-1\n            )\n        else:\n            return scipy.convolve(x, rir, mode=""same"")\n'"
espnet/transform/spec_augment.py,1,"b'""""""Spec Augment module for preprocessing i.e., data augmentation""""""\n\nimport random\n\nimport numpy\nfrom PIL import Image\nfrom PIL.Image import BICUBIC\n\nfrom espnet.transform.functional import FuncTrans\n\n\ndef time_warp(x, max_time_warp=80, inplace=False, mode=""PIL""):\n    """"""time warp for spec augment\n\n    move random center frame by the random width ~ uniform(-window, window)\n    :param numpy.ndarray x: spectrogram (time, freq)\n    :param int max_time_warp: maximum time frames to warp\n    :param bool inplace: overwrite x with the result\n    :param str mode: ""PIL"" (default, fast, not differentiable) or ""sparse_image_warp""\n        (slow, differentiable)\n    :returns numpy.ndarray: time warped spectrogram (time, freq)\n    """"""\n    window = max_time_warp\n    if mode == ""PIL"":\n        t = x.shape[0]\n        if t - window <= window:\n            return x\n        # NOTE: randrange(a, b) emits a, a + 1, ..., b - 1\n        center = random.randrange(window, t - window)\n        warped = random.randrange(center - window, center + window) + 1  # 1 ... t - 1\n\n        left = Image.fromarray(x[:center]).resize((x.shape[1], warped), BICUBIC)\n        right = Image.fromarray(x[center:]).resize((x.shape[1], t - warped), BICUBIC)\n        if inplace:\n            x[:warped] = left\n            x[warped:] = right\n            return x\n        return numpy.concatenate((left, right), 0)\n    elif mode == ""sparse_image_warp"":\n        import torch\n\n        from espnet.utils import spec_augment\n\n        # TODO(karita): make this differentiable again\n        return spec_augment.time_warp(torch.from_numpy(x), window).numpy()\n    else:\n        raise NotImplementedError(\n            ""unknown resize mode: ""\n            + mode\n            + "", choose one from (PIL, sparse_image_warp).""\n        )\n\n\nclass TimeWarp(FuncTrans):\n    _func = time_warp\n    __doc__ = time_warp.__doc__\n\n    def __call__(self, x, train):\n        if not train:\n            return x\n        return super().__call__(x)\n\n\ndef freq_mask(x, F=30, n_mask=2, replace_with_zero=True, inplace=False):\n    """"""freq mask for spec agument\n\n    :param numpy.ndarray x: (time, freq)\n    :param int n_mask: the number of masks\n    :param bool inplace: overwrite\n    :param bool replace_with_zero: pad zero on mask if true else use mean\n    """"""\n    if inplace:\n        cloned = x\n    else:\n        cloned = x.copy()\n\n    num_mel_channels = cloned.shape[1]\n    fs = numpy.random.randint(0, F, size=(n_mask, 2))\n\n    for f, mask_end in fs:\n        f_zero = random.randrange(0, num_mel_channels - f)\n        mask_end += f_zero\n\n        # avoids randrange error if values are equal and range is empty\n        if f_zero == f_zero + f:\n            continue\n\n        if replace_with_zero:\n            cloned[:, f_zero:mask_end] = 0\n        else:\n            cloned[:, f_zero:mask_end] = cloned.mean()\n    return cloned\n\n\nclass FreqMask(FuncTrans):\n    _func = freq_mask\n    __doc__ = freq_mask.__doc__\n\n    def __call__(self, x, train):\n        if not train:\n            return x\n        return super().__call__(x)\n\n\ndef time_mask(spec, T=40, n_mask=2, replace_with_zero=True, inplace=False):\n    """"""freq mask for spec agument\n\n    :param numpy.ndarray spec: (time, freq)\n    :param int n_mask: the number of masks\n    :param bool inplace: overwrite\n    :param bool replace_with_zero: pad zero on mask if true else use mean\n    """"""\n    if inplace:\n        cloned = spec\n    else:\n        cloned = spec.copy()\n    len_spectro = cloned.shape[0]\n    ts = numpy.random.randint(0, T, size=(n_mask, 2))\n    for t, mask_end in ts:\n        # avoid randint range error\n        if len_spectro - t <= 0:\n            continue\n        t_zero = random.randrange(0, len_spectro - t)\n\n        # avoids randrange error if values are equal and range is empty\n        if t_zero == t_zero + t:\n            continue\n\n        mask_end += t_zero\n        if replace_with_zero:\n            cloned[t_zero:mask_end] = 0\n        else:\n            cloned[t_zero:mask_end] = cloned.mean()\n    return cloned\n\n\nclass TimeMask(FuncTrans):\n    _func = time_mask\n    __doc__ = time_mask.__doc__\n\n    def __call__(self, x, train):\n        if not train:\n            return x\n        return super().__call__(x)\n\n\ndef spec_augment(\n    x,\n    resize_mode=""PIL"",\n    max_time_warp=80,\n    max_freq_width=27,\n    n_freq_mask=2,\n    max_time_width=100,\n    n_time_mask=2,\n    inplace=True,\n    replace_with_zero=True,\n):\n    """"""spec agument\n\n    apply random time warping and time/freq masking\n    default setting is based on LD (Librispeech double) in Table 2\n        https://arxiv.org/pdf/1904.08779.pdf\n\n    :param numpy.ndarray x: (time, freq)\n    :param str resize_mode: ""PIL"" (fast, nondifferentiable) or ""sparse_image_warp""\n        (slow, differentiable)\n    :param int max_time_warp: maximum frames to warp the center frame in spectrogram (W)\n    :param int freq_mask_width: maximum width of the random freq mask (F)\n    :param int n_freq_mask: the number of the random freq mask (m_F)\n    :param int time_mask_width: maximum width of the random time mask (T)\n    :param int n_time_mask: the number of the random time mask (m_T)\n    :param bool inplace: overwrite intermediate array\n    :param bool replace_with_zero: pad zero on mask if true else use mean\n    """"""\n    assert isinstance(x, numpy.ndarray)\n    assert x.ndim == 2\n    x = time_warp(x, max_time_warp, inplace=inplace, mode=resize_mode)\n    x = freq_mask(\n        x,\n        max_freq_width,\n        n_freq_mask,\n        inplace=inplace,\n        replace_with_zero=replace_with_zero,\n    )\n    x = time_mask(\n        x,\n        max_time_width,\n        n_time_mask,\n        inplace=inplace,\n        replace_with_zero=replace_with_zero,\n    )\n    return x\n\n\nclass SpecAugment(FuncTrans):\n    _func = spec_augment\n    __doc__ = spec_augment.__doc__\n\n    def __call__(self, x, train):\n        if not train:\n            return x\n        return super().__call__(x)\n'"
espnet/transform/spectrogram.py,0,"b'import librosa\nimport numpy as np\n\n\ndef stft(\n    x, n_fft, n_shift, win_length=None, window=""hann"", center=True, pad_mode=""reflect""\n):\n    # x: [Time, Channel]\n    if x.ndim == 1:\n        single_channel = True\n        # x: [Time] -> [Time, Channel]\n        x = x[:, None]\n    else:\n        single_channel = False\n    x = x.astype(np.float32)\n\n    # FIXME(kamo): librosa.stft can\'t use multi-channel?\n    # x: [Time, Channel, Freq]\n    x = np.stack(\n        [\n            librosa.stft(\n                x[:, ch],\n                n_fft=n_fft,\n                hop_length=n_shift,\n                win_length=win_length,\n                window=window,\n                center=center,\n                pad_mode=pad_mode,\n            ).T\n            for ch in range(x.shape[1])\n        ],\n        axis=1,\n    )\n\n    if single_channel:\n        # x: [Time, Channel, Freq] -> [Time, Freq]\n        x = x[:, 0]\n    return x\n\n\ndef istft(x, n_shift, win_length=None, window=""hann"", center=True):\n    # x: [Time, Channel, Freq]\n    if x.ndim == 2:\n        single_channel = True\n        # x: [Time, Freq] -> [Time, Channel, Freq]\n        x = x[:, None, :]\n    else:\n        single_channel = False\n\n    # x: [Time, Channel]\n    x = np.stack(\n        [\n            librosa.istft(\n                x[:, ch].T,  # [Time, Freq] -> [Freq, Time]\n                hop_length=n_shift,\n                win_length=win_length,\n                window=window,\n                center=center,\n            )\n            for ch in range(x.shape[1])\n        ],\n        axis=1,\n    )\n\n    if single_channel:\n        # x: [Time, Channel] -> [Time]\n        x = x[:, 0]\n    return x\n\n\ndef stft2logmelspectrogram(x_stft, fs, n_mels, n_fft, fmin=None, fmax=None, eps=1e-10):\n    # x_stft: (Time, Channel, Freq) or (Time, Freq)\n    fmin = 0 if fmin is None else fmin\n    fmax = fs / 2 if fmax is None else fmax\n\n    # spc: (Time, Channel, Freq) or (Time, Freq)\n    spc = np.abs(x_stft)\n    # mel_basis: (Mel_freq, Freq)\n    mel_basis = librosa.filters.mel(fs, n_fft, n_mels, fmin, fmax)\n    # lmspc: (Time, Channel, Mel_freq) or (Time, Mel_freq)\n    lmspc = np.log10(np.maximum(eps, np.dot(spc, mel_basis.T)))\n\n    return lmspc\n\n\ndef spectrogram(x, n_fft, n_shift, win_length=None, window=""hann""):\n    # x: (Time, Channel) -> spc: (Time, Channel, Freq)\n    spc = np.abs(stft(x, n_fft, n_shift, win_length, window=window))\n    return spc\n\n\ndef logmelspectrogram(\n    x,\n    fs,\n    n_mels,\n    n_fft,\n    n_shift,\n    win_length=None,\n    window=""hann"",\n    fmin=None,\n    fmax=None,\n    eps=1e-10,\n    pad_mode=""reflect"",\n):\n    # stft: (Time, Channel, Freq) or (Time, Freq)\n    x_stft = stft(\n        x,\n        n_fft=n_fft,\n        n_shift=n_shift,\n        win_length=win_length,\n        window=window,\n        pad_mode=pad_mode,\n    )\n\n    return stft2logmelspectrogram(\n        x_stft, fs=fs, n_mels=n_mels, n_fft=n_fft, fmin=fmin, fmax=fmax, eps=eps\n    )\n\n\nclass Spectrogram(object):\n    def __init__(self, n_fft, n_shift, win_length=None, window=""hann""):\n        self.n_fft = n_fft\n        self.n_shift = n_shift\n        self.win_length = win_length\n        self.window = window\n\n    def __repr__(self):\n        return (\n            ""{name}(n_fft={n_fft}, n_shift={n_shift}, ""\n            ""win_length={win_length}, window={window})"".format(\n                name=self.__class__.__name__,\n                n_fft=self.n_fft,\n                n_shift=self.n_shift,\n                win_length=self.win_length,\n                window=self.window,\n            )\n        )\n\n    def __call__(self, x):\n        return spectrogram(\n            x,\n            n_fft=self.n_fft,\n            n_shift=self.n_shift,\n            win_length=self.win_length,\n            window=self.window,\n        )\n\n\nclass LogMelSpectrogram(object):\n    def __init__(\n        self,\n        fs,\n        n_mels,\n        n_fft,\n        n_shift,\n        win_length=None,\n        window=""hann"",\n        fmin=None,\n        fmax=None,\n        eps=1e-10,\n    ):\n        self.fs = fs\n        self.n_mels = n_mels\n        self.n_fft = n_fft\n        self.n_shift = n_shift\n        self.win_length = win_length\n        self.window = window\n        self.fmin = fmin\n        self.fmax = fmax\n        self.eps = eps\n\n    def __repr__(self):\n        return (\n            ""{name}(fs={fs}, n_mels={n_mels}, n_fft={n_fft}, ""\n            ""n_shift={n_shift}, win_length={win_length}, window={window}, ""\n            ""fmin={fmin}, fmax={fmax}, eps={eps}))"".format(\n                name=self.__class__.__name__,\n                fs=self.fs,\n                n_mels=self.n_mels,\n                n_fft=self.n_fft,\n                n_shift=self.n_shift,\n                win_length=self.win_length,\n                window=self.window,\n                fmin=self.fmin,\n                fmax=self.fmax,\n                eps=self.eps,\n            )\n        )\n\n    def __call__(self, x):\n        return logmelspectrogram(\n            x,\n            fs=self.fs,\n            n_mels=self.n_mels,\n            n_fft=self.n_fft,\n            n_shift=self.n_shift,\n            win_length=self.win_length,\n            window=self.window,\n        )\n\n\nclass Stft2LogMelSpectrogram(object):\n    def __init__(self, fs, n_mels, n_fft, fmin=None, fmax=None, eps=1e-10):\n        self.fs = fs\n        self.n_mels = n_mels\n        self.n_fft = n_fft\n        self.fmin = fmin\n        self.fmax = fmax\n        self.eps = eps\n\n    def __repr__(self):\n        return (\n            ""{name}(fs={fs}, n_mels={n_mels}, n_fft={n_fft}, ""\n            ""fmin={fmin}, fmax={fmax}, eps={eps}))"".format(\n                name=self.__class__.__name__,\n                fs=self.fs,\n                n_mels=self.n_mels,\n                n_fft=self.n_fft,\n                fmin=self.fmin,\n                fmax=self.fmax,\n                eps=self.eps,\n            )\n        )\n\n    def __call__(self, x):\n        return stft2logmelspectrogram(\n            x,\n            fs=self.fs,\n            n_mels=self.n_mels,\n            n_fft=self.n_fft,\n            fmin=self.fmin,\n            fmax=self.fmax,\n        )\n\n\nclass Stft(object):\n    def __init__(\n        self,\n        n_fft,\n        n_shift,\n        win_length=None,\n        window=""hann"",\n        center=True,\n        pad_mode=""reflect"",\n    ):\n        self.n_fft = n_fft\n        self.n_shift = n_shift\n        self.win_length = win_length\n        self.window = window\n        self.center = center\n        self.pad_mode = pad_mode\n\n    def __repr__(self):\n        return (\n            ""{name}(n_fft={n_fft}, n_shift={n_shift}, ""\n            ""win_length={win_length}, window={window},""\n            ""center={center}, pad_mode={pad_mode})"".format(\n                name=self.__class__.__name__,\n                n_fft=self.n_fft,\n                n_shift=self.n_shift,\n                win_length=self.win_length,\n                window=self.window,\n                center=self.center,\n                pad_mode=self.pad_mode,\n            )\n        )\n\n    def __call__(self, x):\n        return stft(\n            x,\n            self.n_fft,\n            self.n_shift,\n            win_length=self.win_length,\n            window=self.window,\n            center=self.center,\n            pad_mode=self.pad_mode,\n        )\n\n\nclass IStft(object):\n    def __init__(self, n_shift, win_length=None, window=""hann"", center=True):\n        self.n_shift = n_shift\n        self.win_length = win_length\n        self.window = window\n        self.center = center\n\n    def __repr__(self):\n        return (\n            ""{name}(n_shift={n_shift}, ""\n            ""win_length={win_length}, window={window},""\n            ""center={center})"".format(\n                name=self.__class__.__name__,\n                n_shift=self.n_shift,\n                win_length=self.win_length,\n                window=self.window,\n                center=self.center,\n            )\n        )\n\n    def __call__(self, x):\n        return istft(\n            x,\n            self.n_shift,\n            win_length=self.win_length,\n            window=self.window,\n            center=self.center,\n        )\n'"
espnet/transform/transform_interface.py,0,"b'# TODO(karita): add this to all the transform impl.\nclass TransformInterface:\n    """"""Transform Interface""""""\n\n    def __call__(self, x):\n        raise NotImplementedError(""__call__ method is not implemented"")\n\n    @classmethod\n    def add_arguments(cls, parser):\n        return parser\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""()""\n\n\nclass Identity(TransformInterface):\n    """"""Identity Function""""""\n\n    def __call__(self, x):\n        return x\n'"
espnet/transform/transformation.py,0,"b'from collections import OrderedDict\nimport copy\nimport io\nimport logging\nimport sys\n\nimport yaml\n\nfrom espnet.utils.dynamic_import import dynamic_import\n\n\nPY2 = sys.version_info[0] == 2\n\nif PY2:\n    from collections import Sequence\n    from funcsigs import signature\nelse:\n    # The ABCs from \'collections\' will stop working in 3.8\n    from collections.abc import Sequence\n    from inspect import signature\n\n\n# TODO(karita): inherit TransformInterface\n# TODO(karita): register cmd arguments in asr_train.py\nimport_alias = dict(\n    identity=""espnet.transform.transform_interface:Identity"",\n    time_warp=""espnet.transform.spec_augment:TimeWarp"",\n    time_mask=""espnet.transform.spec_augment:TimeMask"",\n    freq_mask=""espnet.transform.spec_augment:FreqMask"",\n    spec_augment=""espnet.transform.spec_augment:SpecAugment"",\n    speed_perturbation=""espnet.transform.perturb:SpeedPerturbation"",\n    volume_perturbation=""espnet.transform.perturb:VolumePerturbation"",\n    noise_injection=""espnet.transform.perturb:NoiseInjection"",\n    bandpass_perturbation=""espnet.transform.perturb:BandpassPerturbation"",\n    rir_convolve=""espnet.transform.perturb:RIRConvolve"",\n    delta=""espnet.transform.add_deltas:AddDeltas"",\n    cmvn=""espnet.transform.cmvn:CMVN"",\n    utterance_cmvn=""espnet.transform.cmvn:UtteranceCMVN"",\n    fbank=""espnet.transform.spectrogram:LogMelSpectrogram"",\n    spectrogram=""espnet.transform.spectrogram:Spectrogram"",\n    stft=""espnet.transform.spectrogram:Stft"",\n    istft=""espnet.transform.spectrogram:IStft"",\n    stft2fbank=""espnet.transform.spectrogram:Stft2LogMelSpectrogram"",\n    wpe=""espnet.transform.wpe:WPE"",\n    channel_selector=""espnet.transform.channel_selector:ChannelSelector"",\n)\n\n\nclass Transformation(object):\n    """"""Apply some functions to the mini-batch\n\n    Examples:\n        >>> kwargs = {""process"": [{""type"": ""fbank"",\n        ...                        ""n_mels"": 80,\n        ...                        ""fs"": 16000},\n        ...                       {""type"": ""cmvn"",\n        ...                        ""stats"": ""data/train/cmvn.ark"",\n        ...                        ""norm_vars"": True},\n        ...                       {""type"": ""delta"", ""window"": 2, ""order"": 2}]}\n        >>> transform = Transformation(kwargs)\n        >>> bs = 10\n        >>> xs = [np.random.randn(100, 80).astype(np.float32)\n        ...       for _ in range(bs)]\n        >>> xs = transform(xs)\n    """"""\n\n    def __init__(self, conffile=None):\n        if conffile is not None:\n            if isinstance(conffile, dict):\n                self.conf = copy.deepcopy(conffile)\n            else:\n                with io.open(conffile, encoding=""utf-8"") as f:\n                    self.conf = yaml.safe_load(f)\n                    assert isinstance(self.conf, dict), type(self.conf)\n        else:\n            self.conf = {""mode"": ""sequential"", ""process"": []}\n\n        self.functions = OrderedDict()\n        if self.conf.get(""mode"", ""sequential"") == ""sequential"":\n            for idx, process in enumerate(self.conf[""process""]):\n                assert isinstance(process, dict), type(process)\n                opts = dict(process)\n                process_type = opts.pop(""type"")\n                class_obj = dynamic_import(process_type, import_alias)\n                # TODO(karita): assert issubclass(class_obj, TransformInterface)\n                try:\n                    self.functions[idx] = class_obj(**opts)\n                except TypeError:\n                    try:\n                        signa = signature(class_obj)\n                    except ValueError:\n                        # Some function, e.g. built-in function, are failed\n                        pass\n                    else:\n                        logging.error(\n                            ""Expected signature: {}({})"".format(\n                                class_obj.__name__, signa\n                            )\n                        )\n                    raise\n        else:\n            raise NotImplementedError(\n                ""Not supporting mode={}"".format(self.conf[""mode""])\n            )\n\n    def __repr__(self):\n        rep = ""\\n"" + ""\\n"".join(\n            ""    {}: {}"".format(k, v) for k, v in self.functions.items()\n        )\n        return ""{}({})"".format(self.__class__.__name__, rep)\n\n    def __call__(self, xs, uttid_list=None, **kwargs):\n        """"""Return new mini-batch\n\n        :param Union[Sequence[np.ndarray], np.ndarray] xs:\n        :param Union[Sequence[str], str] uttid_list:\n        :return: batch:\n        :rtype: List[np.ndarray]\n        """"""\n        if not isinstance(xs, Sequence):\n            is_batch = False\n            xs = [xs]\n        else:\n            is_batch = True\n\n        if isinstance(uttid_list, str):\n            uttid_list = [uttid_list for _ in range(len(xs))]\n\n        if self.conf.get(""mode"", ""sequential"") == ""sequential"":\n            for idx in range(len(self.conf[""process""])):\n                func = self.functions[idx]\n                # TODO(karita): use TrainingTrans and UttTrans to check __call__ args\n                # Derive only the args which the func has\n                try:\n                    param = signature(func).parameters\n                except ValueError:\n                    # Some function, e.g. built-in function, are failed\n                    param = {}\n                _kwargs = {k: v for k, v in kwargs.items() if k in param}\n                try:\n                    if uttid_list is not None and ""uttid"" in param:\n                        xs = [func(x, u, **_kwargs) for x, u in zip(xs, uttid_list)]\n                    else:\n                        xs = [func(x, **_kwargs) for x in xs]\n                except Exception:\n                    logging.fatal(\n                        ""Catch a exception from {}th func: {}"".format(idx, func)\n                    )\n                    raise\n        else:\n            raise NotImplementedError(\n                ""Not supporting mode={}"".format(self.conf[""mode""])\n            )\n\n        if is_batch:\n            return xs\n        else:\n            return xs[0]\n'"
espnet/transform/wpe.py,0,"b'from nara_wpe.wpe import wpe\n\n\nclass WPE(object):\n    def __init__(\n        self, taps=10, delay=3, iterations=3, psd_context=0, statistics_mode=""full""\n    ):\n        self.taps = taps\n        self.delay = delay\n        self.iterations = iterations\n        self.psd_context = psd_context\n        self.statistics_mode = statistics_mode\n\n    def __repr__(self):\n        return (\n            ""{name}(taps={taps}, delay={delay}""\n            ""iterations={iterations}, psd_context={psd_context}, ""\n            ""statistics_mode={statistics_mode})"".format(\n                name=self.__class__.__name__,\n                taps=self.taps,\n                delay=self.delay,\n                iterations=self.iterations,\n                psd_context=self.psd_context,\n                statistics_mode=self.statistics_mode,\n            )\n        )\n\n    def __call__(self, xs):\n        """"""Return enhanced\n\n        :param np.ndarray xs: (Time, Channel, Frequency)\n        :return: enhanced_xs\n        :rtype: np.ndarray\n\n        """"""\n        # nara_wpe.wpe: (F, C, T)\n        xs = wpe(\n            xs.transpose((2, 1, 0)),\n            taps=self.taps,\n            delay=self.delay,\n            iterations=self.iterations,\n            psd_context=self.psd_context,\n            statistics_mode=self.statistics_mode,\n        )\n        return xs.transpose(2, 1, 0)\n'"
espnet/tts/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/utils/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/utils/check_kwargs.py,0,"b'import inspect\n\n\ndef check_kwargs(func, kwargs, name=None):\n    """"""check kwargs are valid for func\n\n    If kwargs are invalid, raise TypeError as same as python default\n    :param function func: function to be validated\n    :param dict kwargs: keyword arguments for func\n    :param str name: name used in TypeError (default is func name)\n    """"""\n    try:\n        params = inspect.signature(func).parameters\n    except ValueError:\n        return\n    if name is None:\n        name = func.__name__\n    for k in kwargs.keys():\n        if k not in params:\n            raise TypeError(f""{name}() got an unexpected keyword argument \'{k}\'"")\n'"
espnet/utils/cli_readers.py,0,"b'import io\nimport logging\nimport sys\n\nimport h5py\nimport kaldiio\nimport soundfile\n\nfrom espnet.utils.io_utils import SoundHDF5File\n\n\ndef file_reader_helper(\n    rspecifier: str,\n    filetype: str = ""mat"",\n    return_shape: bool = False,\n    segments: str = None,\n):\n    """"""Read uttid and array in kaldi style\n\n    This function might be a bit confusing as ""ark"" is used\n    for HDF5 to imitate ""kaldi-rspecifier"".\n\n    Args:\n        rspecifier: Give as ""ark:feats.ark"" or ""scp:feats.scp""\n        filetype: ""mat"" is kaldi-martix, ""hdf5"": HDF5\n        return_shape: Return the shape of the matrix,\n            instead of the matrix. This can reduce IO cost for HDF5.\n    Returns:\n        Generator[Tuple[str, np.ndarray], None, None]:\n\n    Examples:\n        Read from kaldi-matrix ark file:\n\n        >>> for u, array in file_reader_helper(\'ark:feats.ark\', \'mat\'):\n        ...     array\n\n        Read from HDF5 file:\n\n        >>> for u, array in file_reader_helper(\'ark:feats.h5\', \'hdf5\'):\n        ...     array\n\n    """"""\n    if filetype == ""mat"":\n        return KaldiReader(rspecifier, return_shape=return_shape, segments=segments)\n    elif filetype == ""hdf5"":\n        return HDF5Reader(rspecifier, return_shape=return_shape)\n    elif filetype == ""sound.hdf5"":\n        return SoundHDF5Reader(rspecifier, return_shape=return_shape)\n    elif filetype == ""sound"":\n        return SoundReader(rspecifier, return_shape=return_shape)\n    else:\n        raise NotImplementedError(f""filetype={filetype}"")\n\n\nclass KaldiReader:\n    def __init__(self, rspecifier, return_shape=False, segments=None):\n        self.rspecifier = rspecifier\n        self.return_shape = return_shape\n        self.segments = segments\n\n    def __iter__(self):\n        with kaldiio.ReadHelper(self.rspecifier, segments=self.segments) as reader:\n            for key, array in reader:\n                if self.return_shape:\n                    array = array.shape\n                yield key, array\n\n\nclass HDF5Reader:\n    def __init__(self, rspecifier, return_shape=False):\n        if "":"" not in rspecifier:\n            raise ValueError(\n                \'Give ""rspecifier"" such as ""ark:some.ark: {}""\'.format(self.rspecifier)\n            )\n        self.rspecifier = rspecifier\n        self.ark_or_scp, self.filepath = self.rspecifier.split("":"", 1)\n        if self.ark_or_scp not in [""ark"", ""scp""]:\n            raise ValueError(f""Must be scp or ark: {self.ark_or_scp}"")\n\n        self.return_shape = return_shape\n\n    def __iter__(self):\n        if self.ark_or_scp == ""scp"":\n            hdf5_dict = {}\n            with open(self.filepath, ""r"", encoding=""utf-8"") as f:\n                for line in f:\n                    key, value = line.rstrip().split(None, 1)\n\n                    if "":"" not in value:\n                        raise RuntimeError(\n                            ""scp file for hdf5 should be like: ""\n                            \'""uttid filepath.h5:key"": {}({})\'.format(\n                                line, self.filepath\n                            )\n                        )\n                    path, h5_key = value.split("":"", 1)\n\n                    hdf5_file = hdf5_dict.get(path)\n                    if hdf5_file is None:\n                        try:\n                            hdf5_file = h5py.File(path, ""r"")\n                        except Exception:\n                            logging.error(""Error when loading {}"".format(path))\n                            raise\n                        hdf5_dict[path] = hdf5_file\n\n                    try:\n                        data = hdf5_file[h5_key]\n                    except Exception:\n                        logging.error(\n                            ""Error when loading {} with key={}"".format(path, h5_key)\n                        )\n                        raise\n\n                    if self.return_shape:\n                        yield key, data.shape\n                    else:\n                        yield key, data[()]\n\n            # Closing all files\n            for k in hdf5_dict:\n                try:\n                    hdf5_dict[k].close()\n                except Exception:\n                    pass\n\n        else:\n            if self.filepath == ""-"":\n                # Required h5py>=2.9\n                filepath = io.BytesIO(sys.stdin.buffer.read())\n            else:\n                filepath = self.filepath\n            with h5py.File(filepath, ""r"") as f:\n                for key in f:\n                    if self.return_shape:\n                        yield key, f[key].shape\n                    else:\n                        yield key, f[key][()]\n\n\nclass SoundHDF5Reader:\n    def __init__(self, rspecifier, return_shape=False):\n        if "":"" not in rspecifier:\n            raise ValueError(\n                \'Give ""rspecifier"" such as ""ark:some.ark: {}""\'.format(rspecifier)\n            )\n        self.ark_or_scp, self.filepath = rspecifier.split("":"", 1)\n        if self.ark_or_scp not in [""ark"", ""scp""]:\n            raise ValueError(f""Must be scp or ark: {self.ark_or_scp}"")\n        self.return_shape = return_shape\n\n    def __iter__(self):\n        if self.ark_or_scp == ""scp"":\n            hdf5_dict = {}\n            with open(self.filepath, ""r"", encoding=""utf-8"") as f:\n                for line in f:\n                    key, value = line.rstrip().split(None, 1)\n\n                    if "":"" not in value:\n                        raise RuntimeError(\n                            ""scp file for hdf5 should be like: ""\n                            \'""uttid filepath.h5:key"": {}({})\'.format(\n                                line, self.filepath\n                            )\n                        )\n                    path, h5_key = value.split("":"", 1)\n\n                    hdf5_file = hdf5_dict.get(path)\n                    if hdf5_file is None:\n                        try:\n                            hdf5_file = SoundHDF5File(path, ""r"")\n                        except Exception:\n                            logging.error(""Error when loading {}"".format(path))\n                            raise\n                        hdf5_dict[path] = hdf5_file\n\n                    try:\n                        data = hdf5_file[h5_key]\n                    except Exception:\n                        logging.error(\n                            ""Error when loading {} with key={}"".format(path, h5_key)\n                        )\n                        raise\n\n                    # Change Tuple[ndarray, int] -> Tuple[int, ndarray]\n                    # (soundfile style -> scipy style)\n                    array, rate = data\n                    if self.return_shape:\n                        array = array.shape\n                    yield key, (rate, array)\n\n            # Closing all files\n            for k in hdf5_dict:\n                try:\n                    hdf5_dict[k].close()\n                except Exception:\n                    pass\n\n        else:\n            if self.filepath == ""-"":\n                # Required h5py>=2.9\n                filepath = io.BytesIO(sys.stdin.buffer.read())\n            else:\n                filepath = self.filepath\n            for key, (a, r) in SoundHDF5File(filepath, ""r"").items():\n                if self.return_shape:\n                    a = a.shape\n                yield key, (r, a)\n\n\nclass SoundReader:\n    def __init__(self, rspecifier, return_shape=False):\n        if "":"" not in rspecifier:\n            raise ValueError(\n                \'Give ""rspecifier"" such as ""scp:some.scp: {}""\'.format(rspecifier)\n            )\n        self.ark_or_scp, self.filepath = rspecifier.split("":"", 1)\n        if self.ark_or_scp != ""scp"":\n            raise ValueError(\n                \'Only supporting ""scp"" for sound file: {}\'.format(self.ark_or_scp)\n            )\n        self.return_shape = return_shape\n\n    def __iter__(self):\n        with open(self.filepath, ""r"", encoding=""utf-8"") as f:\n            for line in f:\n                key, sound_file_path = line.rstrip().split(None, 1)\n                # Assume PCM16\n                array, rate = soundfile.read(sound_file_path, dtype=""int16"")\n                # Change Tuple[ndarray, int] -> Tuple[int, ndarray]\n                # (soundfile style -> scipy style)\n                if self.return_shape:\n                    array = array.shape\n                yield key, (rate, array)\n'"
espnet/utils/cli_utils.py,0,"b'from collections.abc import Sequence\nfrom distutils.util import strtobool as dist_strtobool\nimport sys\n\nimport numpy\n\n\ndef strtobool(x):\n    # distutils.util.strtobool returns integer, but it\'s confusing,\n    return bool(dist_strtobool(x))\n\n\ndef get_commandline_args():\n    extra_chars = [\n        "" "",\n        "";"",\n        ""&"",\n        ""("",\n        "")"",\n        ""|"",\n        ""^"",\n        ""<"",\n        "">"",\n        ""?"",\n        ""*"",\n        ""["",\n        ""]"",\n        ""$"",\n        ""`"",\n        \'""\',\n        ""\\\\"",\n        ""!"",\n        ""{"",\n        ""}"",\n    ]\n\n    # Escape the extra characters for shell\n    argv = [\n        arg.replace(""\'"", ""\'\\\\\'\'"")\n        if all(char not in arg for char in extra_chars)\n        else ""\'"" + arg.replace(""\'"", ""\'\\\\\'\'"") + ""\'""\n        for arg in sys.argv\n    ]\n\n    return sys.executable + "" "" + "" "".join(argv)\n\n\ndef is_scipy_wav_style(value):\n    # If Tuple[int, numpy.ndarray] or not\n    return (\n        isinstance(value, Sequence)\n        and len(value) == 2\n        and isinstance(value[0], int)\n        and isinstance(value[1], numpy.ndarray)\n    )\n\n\ndef assert_scipy_wav_style(value):\n    assert is_scipy_wav_style(\n        value\n    ), ""Must be Tuple[int, numpy.ndarray], but got {}"".format(\n        type(value)\n        if not isinstance(value, Sequence)\n        else ""{}[{}]"".format(type(value), "", "".join(str(type(v)) for v in value))\n    )\n'"
espnet/utils/cli_writers.py,0,"b'from pathlib import Path\nfrom typing import Dict\n\nimport h5py\nimport kaldiio\nimport numpy\nimport soundfile\n\nfrom espnet.utils.cli_utils import assert_scipy_wav_style\nfrom espnet.utils.io_utils import SoundHDF5File\n\n\ndef file_writer_helper(\n    wspecifier: str,\n    filetype: str = ""mat"",\n    write_num_frames: str = None,\n    compress: bool = False,\n    compression_method: int = 2,\n    pcm_format: str = ""wav"",\n):\n    """"""Write matrices in kaldi style\n\n    Args:\n        wspecifier: e.g. ark,scp:out.ark,out.scp\n        filetype: ""mat"" is kaldi-martix, ""hdf5"": HDF5\n        write_num_frames: e.g. \'ark,t:num_frames.txt\'\n        compress: Compress or not\n        compression_method: Specify compression level\n\n    Write in kaldi-matrix-ark with ""kaldi-scp"" file:\n\n    >>> with file_writer_helper(\'ark,scp:out.ark,out.scp\') as f:\n    >>>     f[\'uttid\'] = array\n\n    This ""scp"" has the following format:\n\n        uttidA out.ark:1234\n        uttidB out.ark:2222\n\n    where, 1234 and 2222 points the strating byte address of the matrix.\n    (For detail, see official documentation of Kaldi)\n\n    Write in HDF5 with ""scp"" file:\n\n    >>> with file_writer_helper(\'ark,scp:out.h5,out.scp\', \'hdf5\') as f:\n    >>>     f[\'uttid\'] = array\n\n    This ""scp"" file is created as:\n\n        uttidA out.h5:uttidA\n        uttidB out.h5:uttidB\n\n    HDF5 can be, unlike ""kaldi-ark"", accessed to any keys,\n    so originally ""scp"" is not required for random-reading.\n    Nevertheless we create ""scp"" for HDF5 because it is useful\n    for some use-case. e.g. Concatenation, Splitting.\n\n    """"""\n    if filetype == ""mat"":\n        return KaldiWriter(\n            wspecifier,\n            write_num_frames=write_num_frames,\n            compress=compress,\n            compression_method=compression_method,\n        )\n    elif filetype == ""hdf5"":\n        return HDF5Writer(\n            wspecifier, write_num_frames=write_num_frames, compress=compress\n        )\n    elif filetype == ""sound.hdf5"":\n        return SoundHDF5Writer(\n            wspecifier, write_num_frames=write_num_frames, pcm_format=pcm_format\n        )\n    elif filetype == ""sound"":\n        return SoundWriter(\n            wspecifier, write_num_frames=write_num_frames, pcm_format=pcm_format\n        )\n    else:\n        raise NotImplementedError(f""filetype={filetype}"")\n\n\nclass BaseWriter:\n    def __setitem__(self, key, value):\n        raise NotImplementedError\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        try:\n            self.writer.close()\n        except Exception:\n            pass\n\n        if self.writer_scp is not None:\n            try:\n                self.writer_scp.close()\n            except Exception:\n                pass\n\n        if self.writer_nframe is not None:\n            try:\n                self.writer_nframe.close()\n            except Exception:\n                pass\n\n\ndef get_num_frames_writer(write_num_frames: str):\n    """"""get_num_frames_writer\n\n    Examples:\n        >>> get_num_frames_writer(\'ark,t:num_frames.txt\')\n    """"""\n    if write_num_frames is not None:\n        if "":"" not in write_num_frames:\n            raise ValueError(\n                \'Must include "":"", write_num_frames={}\'.format(write_num_frames)\n            )\n\n        nframes_type, nframes_file = write_num_frames.split("":"", 1)\n        if nframes_type != ""ark,t"":\n            raise ValueError(\n                ""Only supporting text mode. ""\n                ""e.g. --write-num-frames=ark,t:foo.txt :""\n                ""{}"".format(nframes_type)\n            )\n\n    return open(nframes_file, ""w"", encoding=""utf-8"")\n\n\nclass KaldiWriter(BaseWriter):\n    def __init__(\n        self, wspecifier, write_num_frames=None, compress=False, compression_method=2\n    ):\n        if compress:\n            self.writer = kaldiio.WriteHelper(\n                wspecifier, compression_method=compression_method\n            )\n        else:\n            self.writer = kaldiio.WriteHelper(wspecifier)\n        self.writer_scp = None\n        if write_num_frames is not None:\n            self.writer_nframe = get_num_frames_writer(write_num_frames)\n        else:\n            self.writer_nframe = None\n\n    def __setitem__(self, key, value):\n        self.writer[key] = value\n        if self.writer_nframe is not None:\n            self.writer_nframe.write(f""{key} {len(value)}\\n"")\n\n\ndef parse_wspecifier(wspecifier: str) -> Dict[str, str]:\n    """"""Parse wspecifier to dict\n\n    Examples:\n        >>> parse_wspecifier(\'ark,scp:out.ark,out.scp\')\n        {\'ark\': \'out.ark\', \'scp\': \'out.scp\'}\n\n    """"""\n    ark_scp, filepath = wspecifier.split("":"", 1)\n    if ark_scp not in [""ark"", ""scp,ark"", ""ark,scp""]:\n        raise ValueError(""{} is not allowed: {}"".format(ark_scp, wspecifier))\n    ark_scps = ark_scp.split("","")\n    filepaths = filepath.split("","")\n    if len(ark_scps) != len(filepaths):\n        raise ValueError(""Mismatch: {} and {}"".format(ark_scp, filepath))\n    spec_dict = dict(zip(ark_scps, filepaths))\n    return spec_dict\n\n\nclass HDF5Writer(BaseWriter):\n    """"""HDF5Writer\n\n    Examples:\n        >>> with HDF5Writer(\'ark:out.h5\', compress=True) as f:\n        ...     f[\'key\'] = array\n    """"""\n\n    def __init__(self, wspecifier, write_num_frames=None, compress=False):\n        spec_dict = parse_wspecifier(wspecifier)\n        self.filename = spec_dict[""ark""]\n\n        if compress:\n            self.kwargs = {""compression"": ""gzip""}\n        else:\n            self.kwargs = {}\n        self.writer = h5py.File(spec_dict[""ark""], ""w"")\n        if ""scp"" in spec_dict:\n            self.writer_scp = open(spec_dict[""scp""], ""w"", encoding=""utf-8"")\n        else:\n            self.writer_scp = None\n        if write_num_frames is not None:\n            self.writer_nframe = get_num_frames_writer(write_num_frames)\n        else:\n            self.writer_nframe = None\n\n    def __setitem__(self, key, value):\n        self.writer.create_dataset(key, data=value, **self.kwargs)\n\n        if self.writer_scp is not None:\n            self.writer_scp.write(f""{key} {self.filename}:{key}\\n"")\n        if self.writer_nframe is not None:\n            self.writer_nframe.write(f""{key} {len(value)}\\n"")\n\n\nclass SoundHDF5Writer(BaseWriter):\n    """"""SoundHDF5Writer\n\n    Examples:\n        >>> fs = 16000\n        >>> with SoundHDF5Writer(\'ark:out.h5\') as f:\n        ...     f[\'key\'] = fs, array\n    """"""\n\n    def __init__(self, wspecifier, write_num_frames=None, pcm_format=""wav""):\n        self.pcm_format = pcm_format\n        spec_dict = parse_wspecifier(wspecifier)\n        self.filename = spec_dict[""ark""]\n        self.writer = SoundHDF5File(spec_dict[""ark""], ""w"", format=self.pcm_format)\n        if ""scp"" in spec_dict:\n            self.writer_scp = open(spec_dict[""scp""], ""w"", encoding=""utf-8"")\n        else:\n            self.writer_scp = None\n        if write_num_frames is not None:\n            self.writer_nframe = get_num_frames_writer(write_num_frames)\n        else:\n            self.writer_nframe = None\n\n    def __setitem__(self, key, value):\n        assert_scipy_wav_style(value)\n        # Change Tuple[int, ndarray] -> Tuple[ndarray, int]\n        # (scipy style -> soundfile style)\n        value = (value[1], value[0])\n        self.writer.create_dataset(key, data=value)\n\n        if self.writer_scp is not None:\n            self.writer_scp.write(f""{key} {self.filename}:{key}\\n"")\n        if self.writer_nframe is not None:\n            self.writer_nframe.write(f""{key} {len(value[0])}\\n"")\n\n\nclass SoundWriter(BaseWriter):\n    """"""SoundWriter\n\n    Examples:\n        >>> fs = 16000\n        >>> with SoundWriter(\'ark,scp:outdir,out.scp\') as f:\n        ...     f[\'key\'] = fs, array\n    """"""\n\n    def __init__(self, wspecifier, write_num_frames=None, pcm_format=""wav""):\n        self.pcm_format = pcm_format\n        spec_dict = parse_wspecifier(wspecifier)\n        # e.g. ark,scp:dirname,wav.scp\n        # -> The wave files are found in dirname/*.wav\n        self.dirname = spec_dict[""ark""]\n        Path(self.dirname).mkdir(parents=True, exist_ok=True)\n        self.writer = None\n\n        if ""scp"" in spec_dict:\n            self.writer_scp = open(spec_dict[""scp""], ""w"", encoding=""utf-8"")\n        else:\n            self.writer_scp = None\n        if write_num_frames is not None:\n            self.writer_nframe = get_num_frames_writer(write_num_frames)\n        else:\n            self.writer_nframe = None\n\n    def __setitem__(self, key, value):\n        assert_scipy_wav_style(value)\n        rate, signal = value\n        wavfile = Path(self.dirname) / (key + ""."" + self.pcm_format)\n        soundfile.write(wavfile, signal.astype(numpy.int16), rate)\n\n        if self.writer_scp is not None:\n            self.writer_scp.write(f""{key} {wavfile}\\n"")\n        if self.writer_nframe is not None:\n            self.writer_nframe.write(f""{key} {len(signal)}\\n"")\n'"
espnet/utils/dataset.py,5,"b'#!/usr/bin/env python\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""pytorch dataset and dataloader implementation for chainer training.""""""\n\nimport torch\nimport torch.utils.data\n\n\nclass TransformDataset(torch.utils.data.Dataset):\n    """"""Transform Dataset for pytorch backend.\n\n    Args:\n        data: list object from make_batchset\n        transfrom: transform function\n\n    """"""\n\n    def __init__(self, data, transform):\n        """"""Init function.""""""\n        super(TransformDataset).__init__()\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        """"""Len function.""""""\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        """"""[] operator.""""""\n        return self.transform(self.data[idx])\n\n\nclass ChainerDataLoader(object):\n    """"""Pytorch dataloader in chainer style.\n\n    Args:\n        all args for torch.utils.data.dataloader.Dataloader\n\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""Init function.""""""\n        self.loader = torch.utils.data.dataloader.DataLoader(**kwargs)\n        self.len = len(kwargs[""dataset""])\n        self.current_position = 0\n        self.epoch = 0\n        self.iter = None\n        self.kwargs = kwargs\n\n    def next(self):\n        """"""Implement next function.""""""\n        if self.iter is None:\n            self.iter = iter(self.loader)\n        try:\n            ret = next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return self.next()\n        self.current_position += 1\n        if self.current_position == self.len:\n            self.epoch = self.epoch + 1\n            self.current_position = 0\n        return ret\n\n    def __iter__(self):\n        """"""Implement iter function.""""""\n        for batch in self.loader:\n            yield batch\n\n    @property\n    def epoch_detail(self):\n        """"""Epoch_detail required by chainer.""""""\n        return self.epoch + self.current_position / self.len\n\n    def serialize(self, serializer):\n        """"""Serialize and deserialize function.""""""\n        epoch = serializer(""epoch"", self.epoch)\n        current_position = serializer(""current_position"", self.current_position)\n        self.epoch = epoch\n        self.current_position = current_position\n\n    def start_shuffle(self):\n        """"""Shuffle function for sortagrad.""""""\n        self.kwargs[""shuffle""] = True\n        self.loader = torch.utils.data.dataloader.DataLoader(**self.kwargs)\n\n    def finalize(self):\n        """"""Implement finalize function.""""""\n        del self.loader\n'"
espnet/utils/deterministic_utils.py,5,"b'import logging\nimport os\n\nimport chainer\nimport torch\n\n\ndef set_deterministic_pytorch(args):\n    """"""Ensures pytorch produces deterministic results depending on the program arguments\n\n    :param Namespace args: The program arguments\n    """"""\n    # seed setting\n    torch.manual_seed(args.seed)\n\n    # debug mode setting\n    # 0 would be fastest, but 1 seems to be reasonable\n    # considering reproducibility\n    # remove type check\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = (\n        False  # https://github.com/pytorch/pytorch/issues/6351\n    )\n    if args.debugmode < 2:\n        chainer.config.type_check = False\n        logging.info(""torch type check is disabled"")\n    # use deterministic computation or not\n    if args.debugmode < 1:\n        torch.backends.cudnn.deterministic = False\n        torch.backends.cudnn.benchmark = True\n        logging.info(""torch cudnn deterministic is disabled"")\n\n\ndef set_deterministic_chainer(args):\n    """"""Ensures chainer produces deterministic results depending on the program arguments\n\n    :param Namespace args: The program arguments\n    """"""\n    # seed setting (chainer seed may not need it)\n    os.environ[""CHAINER_SEED""] = str(args.seed)\n    logging.info(""chainer seed = "" + os.environ[""CHAINER_SEED""])\n\n    # debug mode setting\n    # 0 would be fastest, but 1 seems to be reasonable\n    # considering reproducibility\n    # remove type check\n    if args.debugmode < 2:\n        chainer.config.type_check = False\n        logging.info(""chainer type check is disabled"")\n    # use deterministic computation or not\n    if args.debugmode < 1:\n        chainer.config.cudnn_deterministic = False\n        logging.info(""chainer cudnn deterministic is disabled"")\n    else:\n        chainer.config.cudnn_deterministic = True\n'"
espnet/utils/dynamic_import.py,0,"b'import importlib\n\n\ndef dynamic_import(import_path, alias=dict()):\n    """"""dynamic import module and class\n\n    :param str import_path: syntax \'module_name:class_name\'\n        e.g., \'espnet.transform.add_deltas:AddDeltas\'\n    :param dict alias: shortcut for registered class\n    :return: imported class\n    """"""\n    if import_path not in alias and "":"" not in import_path:\n        raise ValueError(\n            ""import_path should be one of {} or ""\n            \'include "":"", e.g. ""espnet.transform.add_deltas:AddDeltas"" : \'\n            ""{}"".format(set(alias), import_path)\n        )\n    if "":"" not in import_path:\n        import_path = alias[import_path]\n\n    module_name, objname = import_path.split("":"")\n    m = importlib.import_module(module_name)\n    return getattr(m, objname)\n'"
espnet/utils/fill_missing_args.py,0,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport logging\n\n\ndef fill_missing_args(args, add_arguments):\n    """"""Fill missing arguments in args.\n\n    Args:\n        args (Namespace or None): Namesapce containing hyperparameters.\n        add_arguments (function): Function to add arguments.\n\n    Returns:\n        Namespace: Arguments whose missing ones are filled with default value.\n\n    Examples:\n        >>> from argparse import Namespace\n        >>> from espnet.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2\n        >>> args = Namespace()\n        >>> fill_missing_args(args, Tacotron2.add_arguments_fn)\n        Namespace(aconv_chans=32, aconv_filts=15, adim=512, atype=\'location\', ...)\n\n    """"""\n    # check argument type\n    assert isinstance(args, argparse.Namespace) or args is None\n    assert callable(add_arguments)\n\n    # get default arguments\n    default_args, _ = add_arguments(argparse.ArgumentParser()).parse_known_args()\n\n    # convert to dict\n    args = {} if args is None else vars(args)\n    default_args = vars(default_args)\n\n    for key, value in default_args.items():\n        if key not in args:\n            logging.info(\n                \'attribute ""%s"" does not exist. use default %s.\' % (key, str(value))\n            )\n            args[key] = value\n\n    return argparse.Namespace(**args)\n'"
espnet/utils/io_utils.py,0,"b'from collections import OrderedDict\nimport io\nimport logging\nimport os\n\nimport h5py\nimport kaldiio\nimport numpy as np\nimport soundfile\n\nfrom espnet.transform.transformation import Transformation\n\n\nclass LoadInputsAndTargets(object):\n    """"""Create a mini-batch from a list of dicts\n\n    >>> batch = [(\'utt1\',\n    ...           dict(input=[dict(feat=\'some.ark:123\',\n    ...                            filetype=\'mat\',\n    ...                            name=\'input1\',\n    ...                            shape=[100, 80])],\n    ...                output=[dict(tokenid=\'1 2 3 4\',\n    ...                             name=\'target1\',\n    ...                             shape=[4, 31])]]))\n    >>> l = LoadInputsAndTargets()\n    >>> feat, target = l(batch)\n\n    :param: str mode: Specify the task mode, ""asr"" or ""tts""\n    :param: str preprocess_conf: The path of a json file for pre-processing\n    :param: bool load_input: If False, not to load the input data\n    :param: bool load_output: If False, not to load the output data\n    :param: bool sort_in_input_length: Sort the mini-batch in descending order\n        of the input length\n    :param: bool use_speaker_embedding: Used for tts mode only\n    :param: bool use_second_target: Used for tts mode only\n    :param: dict preprocess_args: Set some optional arguments for preprocessing\n    :param: Optional[dict] preprocess_args: Used for tts mode only\n    """"""\n\n    def __init__(\n        self,\n        mode=""asr"",\n        preprocess_conf=None,\n        load_input=True,\n        load_output=True,\n        sort_in_input_length=True,\n        use_speaker_embedding=False,\n        use_second_target=False,\n        preprocess_args=None,\n        keep_all_data_on_mem=False,\n    ):\n        self._loaders = {}\n        if mode not in [""asr"", ""tts"", ""mt""]:\n            raise ValueError(""Only asr or tts are allowed: mode={}"".format(mode))\n        if preprocess_conf is not None:\n            self.preprocessing = Transformation(preprocess_conf)\n            logging.warning(\n                ""[Experimental feature] Some preprocessing will be done ""\n                ""for the mini-batch creation using {}"".format(self.preprocessing)\n            )\n        else:\n            # If conf doesn\'t exist, this function don\'t touch anything.\n            self.preprocessing = None\n\n        if use_second_target and use_speaker_embedding and mode == ""tts"":\n            raise ValueError(\n                \'Choose one of ""use_second_target"" and \' \'""use_speaker_embedding ""\'\n            )\n        if (use_second_target or use_speaker_embedding) and mode != ""tts"":\n            logging.warning(\n                \'""use_second_target"" and ""use_speaker_embedding"" is \'\n                ""used only for tts mode""\n            )\n\n        self.mode = mode\n        self.load_output = load_output\n        self.load_input = load_input\n        self.sort_in_input_length = sort_in_input_length\n        self.use_speaker_embedding = use_speaker_embedding\n        self.use_second_target = use_second_target\n        if preprocess_args is None:\n            self.preprocess_args = {}\n        else:\n            assert isinstance(preprocess_args, dict), type(preprocess_args)\n            self.preprocess_args = dict(preprocess_args)\n\n        self.keep_all_data_on_mem = keep_all_data_on_mem\n\n    def __call__(self, batch):\n        """"""Function to load inputs and targets from list of dicts\n\n        :param List[Tuple[str, dict]] batch: list of dict which is subset of\n            loaded data.json\n        :return: list of input token id sequences [(L_1), (L_2), ..., (L_B)]\n        :return: list of input feature sequences\n            [(T_1, D), (T_2, D), ..., (T_B, D)]\n        :rtype: list of float ndarray\n        :return: list of target token id sequences [(L_1), (L_2), ..., (L_B)]\n        :rtype: list of int ndarray\n\n        """"""\n        x_feats_dict = OrderedDict()  # OrderedDict[str, List[np.ndarray]]\n        y_feats_dict = OrderedDict()  # OrderedDict[str, List[np.ndarray]]\n        uttid_list = []  # List[str]\n\n        for uttid, info in batch:\n            uttid_list.append(uttid)\n\n            if self.load_input:\n                # Note(kamo): This for-loop is for multiple inputs\n                for idx, inp in enumerate(info[""input""]):\n                    # {""input"":\n                    #  [{""feat"": ""some/path.h5:F01_050C0101_PED_REAL"",\n                    #    ""filetype"": ""hdf5"",\n                    #    ""name"": ""input1"", ...}], ...}\n                    x = self._get_from_loader(\n                        filepath=inp[""feat""], filetype=inp.get(""filetype"", ""mat"")\n                    )\n                    x_feats_dict.setdefault(inp[""name""], []).append(x)\n            # FIXME(kamo): Dirty way to load only speaker_embedding\n            elif self.mode == ""tts"" and self.use_speaker_embedding:\n                for idx, inp in enumerate(info[""input""]):\n                    if idx != 1 and len(info[""input""]) > 1:\n                        x = None\n                    else:\n                        x = self._get_from_loader(\n                            filepath=inp[""feat""], filetype=inp.get(""filetype"", ""mat"")\n                        )\n                    x_feats_dict.setdefault(inp[""name""], []).append(x)\n\n            if self.load_output:\n                if self.mode == ""mt"":\n                    x = np.fromiter(\n                        map(int, info[""output""][1][""tokenid""].split()), dtype=np.int64\n                    )\n                    x_feats_dict.setdefault(info[""output""][1][""name""], []).append(x)\n\n                for idx, inp in enumerate(info[""output""]):\n                    if ""tokenid"" in inp:\n                        # ======= Legacy format for output =======\n                        # {""output"": [{""tokenid"": ""1 2 3 4""}])\n                        x = np.fromiter(\n                            map(int, inp[""tokenid""].split()), dtype=np.int64\n                        )\n                    else:\n                        # ======= New format =======\n                        # {""input"":\n                        #  [{""feat"": ""some/path.h5:F01_050C0101_PED_REAL"",\n                        #    ""filetype"": ""hdf5"",\n                        #    ""name"": ""target1"", ...}], ...}\n                        x = self._get_from_loader(\n                            filepath=inp[""feat""], filetype=inp.get(""filetype"", ""mat"")\n                        )\n\n                    y_feats_dict.setdefault(inp[""name""], []).append(x)\n\n        if self.mode == ""asr"":\n            return_batch, uttid_list = self._create_batch_asr(\n                x_feats_dict, y_feats_dict, uttid_list\n            )\n        elif self.mode == ""tts"":\n            _, info = batch[0]\n            eos = int(info[""output""][0][""shape""][1]) - 1\n            return_batch, uttid_list = self._create_batch_tts(\n                x_feats_dict, y_feats_dict, uttid_list, eos\n            )\n        elif self.mode == ""mt"":\n            return_batch, uttid_list = self._create_batch_mt(\n                x_feats_dict, y_feats_dict, uttid_list\n            )\n        else:\n            raise NotImplementedError\n\n        if self.preprocessing is not None:\n            # Apply pre-processing all input features\n            for x_name in return_batch.keys():\n                if x_name.startswith(""input""):\n                    return_batch[x_name] = self.preprocessing(\n                        return_batch[x_name], uttid_list, **self.preprocess_args\n                    )\n\n        # Doesn\'t return the names now.\n        return tuple(return_batch.values())\n\n    def _create_batch_asr(self, x_feats_dict, y_feats_dict, uttid_list):\n        """"""Create a OrderedDict for the mini-batch\n\n        :param OrderedDict x_feats_dict:\n            e.g. {""input1"": [ndarray, ndarray, ...],\n                  ""input2"": [ndarray, ndarray, ...]}\n        :param OrderedDict y_feats_dict:\n            e.g. {""target1"": [ndarray, ndarray, ...],\n                  ""target2"": [ndarray, ndarray, ...]}\n        :param: List[str] uttid_list:\n            Give uttid_list to sort in the same order as the mini-batch\n        :return: batch, uttid_list\n        :rtype: Tuple[OrderedDict, List[str]]\n        """"""\n        # handle single-input and multi-input (paralell) asr mode\n        xs = list(x_feats_dict.values())\n\n        if self.load_output:\n            if len(y_feats_dict) == 1:\n                ys = list(y_feats_dict.values())[0]\n                assert len(xs[0]) == len(ys), (len(xs[0]), len(ys))\n\n                # get index of non-zero length samples\n                nonzero_idx = list(filter(lambda i: len(ys[i]) > 0, range(len(ys))))\n            elif len(y_feats_dict) > 1:  # multi-speaker asr mode\n                ys = list(y_feats_dict.values())\n                assert len(xs[0]) == len(ys[0]), (len(xs[0]), len(ys[0]))\n\n                # get index of non-zero length samples\n                nonzero_idx = list(\n                    filter(lambda i: len(ys[0][i]) > 0, range(len(ys[0])))\n                )\n                for n in range(1, len(y_feats_dict)):\n                    nonzero_idx = filter(lambda i: len(ys[n][i]) > 0, nonzero_idx)\n        else:\n            # Note(kamo): Be careful not to make nonzero_idx to a generator\n            nonzero_idx = list(range(len(xs[0])))\n\n        if self.sort_in_input_length:\n            # sort in input lengths based on the first input\n            nonzero_sorted_idx = sorted(nonzero_idx, key=lambda i: -len(xs[0][i]))\n        else:\n            nonzero_sorted_idx = nonzero_idx\n\n        if len(nonzero_sorted_idx) != len(xs[0]):\n            logging.warning(\n                ""Target sequences include empty tokenid (batch {} -> {})."".format(\n                    len(xs[0]), len(nonzero_sorted_idx)\n                )\n            )\n\n        # remove zero-length samples\n        xs = [[x[i] for i in nonzero_sorted_idx] for x in xs]\n        uttid_list = [uttid_list[i] for i in nonzero_sorted_idx]\n\n        x_names = list(x_feats_dict.keys())\n        if self.load_output:\n            if len(y_feats_dict) == 1:\n                ys = [ys[i] for i in nonzero_sorted_idx]\n            elif len(y_feats_dict) > 1:  # multi-speaker asr mode\n                ys = zip(*[[y[i] for i in nonzero_sorted_idx] for y in ys])\n\n            y_name = list(y_feats_dict.keys())[0]\n\n            # Keeping x_name and y_name, e.g. input1, for future extension\n            return_batch = OrderedDict(\n                [*[(x_name, x) for x_name, x in zip(x_names, xs)], (y_name, ys)]\n            )\n        else:\n            return_batch = OrderedDict([(x_name, x) for x_name, x in zip(x_names, xs)])\n        return return_batch, uttid_list\n\n    def _create_batch_mt(self, x_feats_dict, y_feats_dict, uttid_list):\n        """"""Create a OrderedDict for the mini-batch\n\n        :param OrderedDict x_feats_dict:\n        :param OrderedDict y_feats_dict:\n        :return: batch, uttid_list\n        :rtype: Tuple[OrderedDict, List[str]]\n        """"""\n        # Create a list from the first item\n        xs = list(x_feats_dict.values())[0]\n\n        if self.load_output:\n            ys = list(y_feats_dict.values())[0]\n            assert len(xs) == len(ys), (len(xs), len(ys))\n\n            # get index of non-zero length samples\n            nonzero_idx = filter(lambda i: len(ys[i]) > 0, range(len(ys)))\n        else:\n            nonzero_idx = range(len(xs))\n\n        if self.sort_in_input_length:\n            # sort in input lengths\n            nonzero_sorted_idx = sorted(nonzero_idx, key=lambda i: -len(xs[i]))\n        else:\n            nonzero_sorted_idx = nonzero_idx\n\n        if len(nonzero_sorted_idx) != len(xs):\n            logging.warning(\n                ""Target sequences include empty tokenid (batch {} -> {})."".format(\n                    len(xs), len(nonzero_sorted_idx)\n                )\n            )\n\n        # remove zero-length samples\n        xs = [xs[i] for i in nonzero_sorted_idx]\n        uttid_list = [uttid_list[i] for i in nonzero_sorted_idx]\n\n        x_name = list(x_feats_dict.keys())[0]\n        if self.load_output:\n            ys = [ys[i] for i in nonzero_sorted_idx]\n            y_name = list(y_feats_dict.keys())[0]\n\n            return_batch = OrderedDict([(x_name, xs), (y_name, ys)])\n        else:\n            return_batch = OrderedDict([(x_name, xs)])\n        return return_batch, uttid_list\n\n    def _create_batch_tts(self, x_feats_dict, y_feats_dict, uttid_list, eos):\n        """"""Create a OrderedDict for the mini-batch\n\n        :param OrderedDict x_feats_dict:\n            e.g. {""input1"": [ndarray, ndarray, ...],\n                  ""input2"": [ndarray, ndarray, ...]}\n        :param OrderedDict y_feats_dict:\n            e.g. {""target1"": [ndarray, ndarray, ...],\n                  ""target2"": [ndarray, ndarray, ...]}\n        :param: List[str] uttid_list:\n        :param int eos:\n        :return: batch, uttid_list\n        :rtype: Tuple[OrderedDict, List[str]]\n        """"""\n        # Use the output values as the input feats for tts mode\n        xs = list(y_feats_dict.values())[0]\n        # get index of non-zero length samples\n        nonzero_idx = list(filter(lambda i: len(xs[i]) > 0, range(len(xs))))\n        # sort in input lengths\n        if self.sort_in_input_length:\n            # sort in input lengths\n            nonzero_sorted_idx = sorted(nonzero_idx, key=lambda i: -len(xs[i]))\n        else:\n            nonzero_sorted_idx = nonzero_idx\n        # remove zero-length samples\n        xs = [xs[i] for i in nonzero_sorted_idx]\n        uttid_list = [uttid_list[i] for i in nonzero_sorted_idx]\n        # Added eos into input sequence\n        xs = [np.append(x, eos) for x in xs]\n\n        if self.load_input:\n            ys = list(x_feats_dict.values())[0]\n            assert len(xs) == len(ys), (len(xs), len(ys))\n            ys = [ys[i] for i in nonzero_sorted_idx]\n\n            spembs = None\n            spcs = None\n            spembs_name = ""spembs_none""\n            spcs_name = ""spcs_none""\n\n            if self.use_second_target:\n                spcs = list(x_feats_dict.values())[1]\n                spcs = [spcs[i] for i in nonzero_sorted_idx]\n                spcs_name = list(x_feats_dict.keys())[1]\n\n            if self.use_speaker_embedding:\n                spembs = list(x_feats_dict.values())[1]\n                spembs = [spembs[i] for i in nonzero_sorted_idx]\n                spembs_name = list(x_feats_dict.keys())[1]\n\n            x_name = list(y_feats_dict.keys())[0]\n            y_name = list(x_feats_dict.keys())[0]\n\n            return_batch = OrderedDict(\n                [(x_name, xs), (y_name, ys), (spembs_name, spembs), (spcs_name, spcs)]\n            )\n        elif self.use_speaker_embedding:\n            if len(x_feats_dict) == 0:\n                raise IndexError(""No speaker embedding is provided"")\n            elif len(x_feats_dict) == 1:\n                spembs_idx = 0\n            else:\n                spembs_idx = 1\n\n            spembs = list(x_feats_dict.values())[spembs_idx]\n            spembs = [spembs[i] for i in nonzero_sorted_idx]\n\n            x_name = list(y_feats_dict.keys())[0]\n            spembs_name = list(x_feats_dict.keys())[spembs_idx]\n\n            return_batch = OrderedDict([(x_name, xs), (spembs_name, spembs)])\n        else:\n            x_name = list(y_feats_dict.keys())[0]\n\n            return_batch = OrderedDict([(x_name, xs)])\n        return return_batch, uttid_list\n\n    def _get_from_loader(self, filepath, filetype):\n        """"""Return ndarray\n\n        In order to make the fds to be opened only at the first referring,\n        the loader are stored in self._loaders\n\n        >>> ndarray = loader.get_from_loader(\n        ...     \'some/path.h5:F01_050C0101_PED_REAL\', filetype=\'hdf5\')\n\n        :param: str filepath:\n        :param: str filetype:\n        :return:\n        :rtype: np.ndarray\n        """"""\n        if filetype == ""hdf5"":\n            # e.g.\n            #    {""input"": [{""feat"": ""some/path.h5:F01_050C0101_PED_REAL"",\n            #                ""filetype"": ""hdf5"",\n            # -> filepath = ""some/path.h5"", key = ""F01_050C0101_PED_REAL""\n            filepath, key = filepath.split("":"", 1)\n\n            loader = self._loaders.get(filepath)\n            if loader is None:\n                # To avoid disk access, create loader only for the first time\n                loader = h5py.File(filepath, ""r"")\n                self._loaders[filepath] = loader\n            return loader[key][()]\n        elif filetype == ""sound.hdf5"":\n            # e.g.\n            #    {""input"": [{""feat"": ""some/path.h5:F01_050C0101_PED_REAL"",\n            #                ""filetype"": ""sound.hdf5"",\n            # -> filepath = ""some/path.h5"", key = ""F01_050C0101_PED_REAL""\n            filepath, key = filepath.split("":"", 1)\n\n            loader = self._loaders.get(filepath)\n            if loader is None:\n                # To avoid disk access, create loader only for the first time\n                loader = SoundHDF5File(filepath, ""r"", dtype=""int16"")\n                self._loaders[filepath] = loader\n            array, rate = loader[key]\n            return array\n        elif filetype == ""sound"":\n            # e.g.\n            #    {""input"": [{""feat"": ""some/path.wav"",\n            #                ""filetype"": ""sound""},\n            # Assume PCM16\n            if not self.keep_all_data_on_mem:\n                array, _ = soundfile.read(filepath, dtype=""int16"")\n                return array\n            if filepath not in self._loaders:\n                array, _ = soundfile.read(filepath, dtype=""int16"")\n                self._loaders[filepath] = array\n            return self._loaders[filepath]\n        elif filetype == ""npz"":\n            # e.g.\n            #    {""input"": [{""feat"": ""some/path.npz:F01_050C0101_PED_REAL"",\n            #                ""filetype"": ""npz"",\n            filepath, key = filepath.split("":"", 1)\n\n            loader = self._loaders.get(filepath)\n            if loader is None:\n                # To avoid disk access, create loader only for the first time\n                loader = np.load(filepath)\n                self._loaders[filepath] = loader\n            return loader[key]\n        elif filetype == ""npy"":\n            # e.g.\n            #    {""input"": [{""feat"": ""some/path.npy"",\n            #                ""filetype"": ""npy""},\n            if not self.keep_all_data_on_mem:\n                return np.load(filepath)\n            if filepath not in self._loaders:\n                self._loaders[filepath] = np.load(filepath)\n            return self._loaders[filepath]\n        elif filetype in [""mat"", ""vec""]:\n            # e.g.\n            #    {""input"": [{""feat"": ""some/path.ark:123"",\n            #                ""filetype"": ""mat""}]},\n            # In this case, ""123"" indicates the starting points of the matrix\n            # load_mat can load both matrix and vector\n            if not self.keep_all_data_on_mem:\n                return kaldiio.load_mat(filepath)\n            if filepath not in self._loaders:\n                self._loaders[filepath] = kaldiio.load_mat(filepath)\n            return self._loaders[filepath]\n        elif filetype == ""scp"":\n            # e.g.\n            #    {""input"": [{""feat"": ""some/path.scp:F01_050C0101_PED_REAL"",\n            #                ""filetype"": ""scp"",\n            filepath, key = filepath.split("":"", 1)\n            loader = self._loaders.get(filepath)\n            if loader is None:\n                # To avoid disk access, create loader only for the first time\n                loader = kaldiio.load_scp(filepath)\n                self._loaders[filepath] = loader\n            return loader[key]\n        else:\n            raise NotImplementedError(""Not supported: loader_type={}"".format(filetype))\n\n\nclass SoundHDF5File(object):\n    """"""Collecting sound files to a HDF5 file\n\n    >>> f = SoundHDF5File(\'a.flac.h5\', mode=\'a\')\n    >>> array = np.random.randint(0, 100, 100, dtype=np.int16)\n    >>> f[\'id\'] = (array, 16000)\n    >>> array, rate = f[\'id\']\n\n\n    :param: str filepath:\n    :param: str mode:\n    :param: str format: The type used when saving wav. flac, nist, htk, etc.\n    :param: str dtype:\n\n    """"""\n\n    def __init__(self, filepath, mode=""r+"", format=None, dtype=""int16"", **kwargs):\n        self.filepath = filepath\n        self.mode = mode\n        self.dtype = dtype\n\n        self.file = h5py.File(filepath, mode, **kwargs)\n        if format is None:\n            # filepath = a.flac.h5 -> format = flac\n            second_ext = os.path.splitext(os.path.splitext(filepath)[0])[1]\n            format = second_ext[1:]\n            if format.upper() not in soundfile.available_formats():\n                # If not found, flac is selected\n                format = ""flac""\n\n        # This format affects only saving\n        self.format = format\n\n    def __repr__(self):\n        return \'<SoundHDF5 file ""{}"" (mode {}, format {}, type {})>\'.format(\n            self.filepath, self.mode, self.format, self.dtype\n        )\n\n    def create_dataset(self, name, shape=None, data=None, **kwds):\n        f = io.BytesIO()\n        array, rate = data\n        soundfile.write(f, array, rate, format=self.format)\n        self.file.create_dataset(name, shape=shape, data=np.void(f.getvalue()), **kwds)\n\n    def __setitem__(self, name, data):\n        self.create_dataset(name, data=data)\n\n    def __getitem__(self, key):\n        data = self.file[key][()]\n        f = io.BytesIO(data.tobytes())\n        array, rate = soundfile.read(f, dtype=self.dtype)\n        return array, rate\n\n    def keys(self):\n        return self.file.keys()\n\n    def values(self):\n        for k in self.file:\n            yield self[k]\n\n    def items(self):\n        for k in self.file:\n            yield k, self[k]\n\n    def __iter__(self):\n        return iter(self.file)\n\n    def __contains__(self, item):\n        return item in self.file\n\n    def __len__(self, item):\n        return len(self.file)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.file.close()\n\n    def close(self):\n        self.file.close()\n'"
espnet/utils/spec_augment.py,61,"b'# -*- coding: utf-8 -*-\n\n""""""\nThis implementation is modified from https://github.com/zcaceres/spec_augment\n\nMIT License\n\nCopyright (c) 2019 Zach Caceres\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETjjHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n""""""\n\nimport random\n\nimport torch\n\n\ndef specaug(\n    spec, W=5, F=30, T=40, num_freq_masks=2, num_time_masks=2, replace_with_zero=False\n):\n    """"""SpecAugment\n\n    Reference:\n        SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\n        (https://arxiv.org/pdf/1904.08779.pdf)\n\n    This implementation modified from https://github.com/zcaceres/spec_augment\n\n    :param torch.Tensor spec: input tensor with the shape (T, dim)\n    :param int W: time warp parameter\n    :param int F: maximum width of each freq mask\n    :param int T: maximum width of each time mask\n    :param int num_freq_masks: number of frequency masks\n    :param int num_time_masks: number of time masks\n    :param bool replace_with_zero: if True, masked parts will be filled with 0,\n        if False, filled with mean\n    """"""\n    return time_mask(\n        freq_mask(\n            time_warp(spec, W=W),\n            F=F,\n            num_masks=num_freq_masks,\n            replace_with_zero=replace_with_zero,\n        ),\n        T=T,\n        num_masks=num_time_masks,\n        replace_with_zero=replace_with_zero,\n    )\n\n\ndef time_warp(spec, W=5):\n    """"""Time warping\n\n    :param torch.Tensor spec: input tensor with shape (T, dim)\n    :param int W: time warp parameter\n    """"""\n    spec = spec.unsqueeze(0)\n    spec_len = spec.shape[1]\n    num_rows = spec.shape[2]\n    device = spec.device\n\n    y = num_rows // 2\n    horizontal_line_at_ctr = spec[0, :, y]\n    assert len(horizontal_line_at_ctr) == spec_len\n\n    point_to_warp = horizontal_line_at_ctr[random.randrange(W, spec_len - W)]\n    assert isinstance(point_to_warp, torch.Tensor)\n\n    # Uniform distribution from (0,W) with chance to be up to W negative\n    dist_to_warp = random.randrange(-W, W)\n    src_pts, dest_pts = (\n        torch.tensor([[[point_to_warp, y]]], device=device),\n        torch.tensor([[[point_to_warp + dist_to_warp, y]]], device=device),\n    )\n    warped_spectro, dense_flows = sparse_image_warp(spec, src_pts, dest_pts)\n    return warped_spectro.squeeze(3).squeeze(0)\n\n\ndef freq_mask(spec, F=30, num_masks=1, replace_with_zero=False):\n    """"""Frequency masking\n\n    :param torch.Tensor spec: input tensor with shape (T, dim)\n    :param int F: maximum width of each mask\n    :param int num_masks: number of masks\n    :param bool replace_with_zero: if True, masked parts will be filled with 0,\n        if False, filled with mean\n    """"""\n    cloned = spec.unsqueeze(0).clone()\n    num_mel_channels = cloned.shape[2]\n\n    for i in range(0, num_masks):\n        f = random.randrange(0, F)\n        f_zero = random.randrange(0, num_mel_channels - f)\n\n        # avoids randrange error if values are equal and range is empty\n        if f_zero == f_zero + f:\n            return cloned.squeeze(0)\n\n        mask_end = random.randrange(f_zero, f_zero + f)\n        if replace_with_zero:\n            cloned[0][:, f_zero:mask_end] = 0\n        else:\n            cloned[0][:, f_zero:mask_end] = cloned.mean()\n    return cloned.squeeze(0)\n\n\ndef time_mask(spec, T=40, num_masks=1, replace_with_zero=False):\n    """"""Time masking\n\n    :param torch.Tensor spec: input tensor with shape (T, dim)\n    :param int T: maximum width of each mask\n    :param int num_masks: number of masks\n    :param bool replace_with_zero: if True, masked parts will be filled with 0,\n        if False, filled with mean\n    """"""\n    cloned = spec.unsqueeze(0).clone()\n    len_spectro = cloned.shape[1]\n\n    for i in range(0, num_masks):\n        t = random.randrange(0, T)\n        t_zero = random.randrange(0, len_spectro - t)\n\n        # avoids randrange error if values are equal and range is empty\n        if t_zero == t_zero + t:\n            return cloned.squeeze(0)\n\n        mask_end = random.randrange(t_zero, t_zero + t)\n        if replace_with_zero:\n            cloned[0][t_zero:mask_end, :] = 0\n        else:\n            cloned[0][t_zero:mask_end, :] = cloned.mean()\n    return cloned.squeeze(0)\n\n\ndef sparse_image_warp(\n    img_tensor,\n    source_control_point_locations,\n    dest_control_point_locations,\n    interpolation_order=2,\n    regularization_weight=0.0,\n    num_boundaries_points=0,\n):\n    device = img_tensor.device\n    control_point_flows = dest_control_point_locations - source_control_point_locations\n\n    batch_size, image_height, image_width = img_tensor.shape\n    flattened_grid_locations = get_flat_grid_locations(\n        image_height, image_width, device\n    )\n\n    flattened_flows = interpolate_spline(\n        dest_control_point_locations,\n        control_point_flows,\n        flattened_grid_locations,\n        interpolation_order,\n        regularization_weight,\n    )\n\n    dense_flows = create_dense_flows(\n        flattened_flows, batch_size, image_height, image_width\n    )\n\n    warped_image = dense_image_warp(img_tensor, dense_flows)\n\n    return warped_image, dense_flows\n\n\ndef get_grid_locations(image_height, image_width, device):\n    y_range = torch.linspace(0, image_height - 1, image_height, device=device)\n    x_range = torch.linspace(0, image_width - 1, image_width, device=device)\n    y_grid, x_grid = torch.meshgrid(y_range, x_range)\n    return torch.stack((y_grid, x_grid), -1)\n\n\ndef flatten_grid_locations(grid_locations, image_height, image_width):\n    return torch.reshape(grid_locations, [image_height * image_width, 2])\n\n\ndef get_flat_grid_locations(image_height, image_width, device):\n    y_range = torch.linspace(0, image_height - 1, image_height, device=device)\n    x_range = torch.linspace(0, image_width - 1, image_width, device=device)\n    y_grid, x_grid = torch.meshgrid(y_range, x_range)\n    return torch.stack((y_grid, x_grid), -1).reshape([image_height * image_width, 2])\n\n\ndef create_dense_flows(flattened_flows, batch_size, image_height, image_width):\n    # possibly .view\n    return torch.reshape(flattened_flows, [batch_size, image_height, image_width, 2])\n\n\ndef interpolate_spline(\n    train_points, train_values, query_points, order, regularization_weight=0.0,\n):\n    # First, fit the spline to the observed data.\n    w, v = solve_interpolation(train_points, train_values, order, regularization_weight)\n    # Then, evaluate the spline at the query locations.\n    query_values = apply_interpolation(query_points, train_points, w, v, order)\n\n    return query_values\n\n\ndef solve_interpolation(train_points, train_values, order, regularization_weight):\n    device = train_points.device\n    b, n, d = train_points.shape\n    k = train_values.shape[-1]\n\n    c = train_points\n    f = train_values.float()\n\n    matrix_a = phi(cross_squared_distance_matrix(c, c), order).unsqueeze(0)  # [b, n, n]\n\n    # Append ones to the feature values for the bias term in the linear model.\n    ones = torch.ones(1, dtype=train_points.dtype, device=device).view([-1, 1, 1])\n    matrix_b = torch.cat((c, ones), 2).float()  # [b, n, d + 1]\n\n    # [b, n + d + 1, n]\n    left_block = torch.cat((matrix_a, torch.transpose(matrix_b, 2, 1)), 1)\n\n    num_b_cols = matrix_b.shape[2]  # d + 1\n\n    # In Tensorflow, zeros are used here. Pytorch solve fails with zeros\n    # for some reason we don\'t understand.\n    # So instead we use very tiny randn values (variance of one, zero mean)\n    # on one side of our multiplication.\n    lhs_zeros = torch.randn((b, num_b_cols, num_b_cols), device=device) / 1e10\n    right_block = torch.cat((matrix_b, lhs_zeros), 1)  # [b, n + d + 1, d + 1]\n    lhs = torch.cat((left_block, right_block), 2)  # [b, n + d + 1, n + d + 1]\n\n    rhs_zeros = torch.zeros(\n        (b, d + 1, k), dtype=train_points.dtype, device=device\n    ).float()\n    rhs = torch.cat((f, rhs_zeros), 1)  # [b, n + d + 1, k]\n\n    # Then, solve the linear system and unpack the results.\n    X, LU = torch.gesv(rhs, lhs)\n    w = X[:, :n, :]\n    v = X[:, n:, :]\n\n    return w, v\n\n\ndef cross_squared_distance_matrix(x, y):\n    """"""Pairwise squared distance between two (batch) matrices\' rows (2nd dim).\n\n        Computes the pairwise distances between rows of x and rows of y\n        Args:\n        x: [batch_size, n, d] float `Tensor`\n        y: [batch_size, m, d] float `Tensor`\n        Returns:\n        squared_dists: [batch_size, n, m] float `Tensor`, where\n        squared_dists[b,i,j] = ||x[b,i,:] - y[b,j,:]||^2\n    """"""\n    x_norm_squared = torch.sum(torch.mul(x, x))\n    y_norm_squared = torch.sum(torch.mul(y, y))\n\n    x_y_transpose = torch.matmul(x.squeeze(0), y.squeeze(0).transpose(0, 1))\n\n    # squared_dists[b,i,j] = ||x_bi - y_bj||^2 = x_bi\'x_bi- 2x_bi\'x_bj + x_bj\'x_bj\n    squared_dists = x_norm_squared - 2 * x_y_transpose + y_norm_squared\n\n    return squared_dists.float()\n\n\ndef phi(r, order):\n    """"""Coordinate-wise nonlinearity used to define the order of the interpolation.\n\n    See https://en.wikipedia.org/wiki/Polyharmonic_spline for the definition.\n    Args:\n    r: input op\n    order: interpolation order\n    Returns:\n    phi_k evaluated coordinate-wise on r, for k = r\n    """"""\n    EPSILON = torch.tensor(1e-10, device=r.device)\n    # using EPSILON prevents log(0), sqrt0), etc.\n    # sqrt(0) is well-defined, but its gradient is not\n    if order == 1:\n        r = torch.max(r, EPSILON)\n        r = torch.sqrt(r)\n        return r\n    elif order == 2:\n        return 0.5 * r * torch.log(torch.max(r, EPSILON))\n    elif order == 4:\n        return 0.5 * torch.square(r) * torch.log(torch.max(r, EPSILON))\n    elif order % 2 == 0:\n        r = torch.max(r, EPSILON)\n        return 0.5 * torch.pow(r, 0.5 * order) * torch.log(r)\n    else:\n        r = torch.max(r, EPSILON)\n        return torch.pow(r, 0.5 * order)\n\n\ndef apply_interpolation(query_points, train_points, w, v, order):\n    """"""Apply polyharmonic interpolation model to data.\n\n    Notes:\n        Given coefficients w and v for the interpolation model, we evaluate\n        interpolated function values at query_points.\n\n    Args:\n        query_points: `[b, m, d]` x values to evaluate the interpolation at\n        train_points: `[b, n, d]` x values that act as the interpolation centers\n            ( the c variables in the wikipedia article)\n            w: `[b, n, k]` weights on each interpolation center\n            v: `[b, d, k]` weights on each input dimension\n        order: order of the interpolation\n\n    Returns:\n        Polyharmonic interpolation evaluated at points defined in query_points.\n    """"""\n    query_points = query_points.unsqueeze(0)\n    # First, compute the contribution from the rbf term.\n    pairwise_dists = cross_squared_distance_matrix(\n        query_points.float(), train_points.float()\n    )\n    phi_pairwise_dists = phi(pairwise_dists, order)\n\n    rbf_term = torch.matmul(phi_pairwise_dists, w)\n\n    # Then, compute the contribution from the linear term.\n    # Pad query_points with ones, for the bias term in the linear model.\n    ones = torch.ones_like(query_points[..., :1])\n    query_points_pad = torch.cat((query_points, ones), 2).float()\n    linear_term = torch.matmul(query_points_pad, v)\n\n    return rbf_term + linear_term\n\n\ndef dense_image_warp(image, flow):\n    """"""Image warping using per-pixel flow vectors.\n\n    Apply a non-linear warp to the image, where the warp is specified by a dense\n    flow field of offset vectors that define the correspondences of pixel values\n    in the output image back to locations in the  source image. Specifically, the\n    pixel value at output[b, j, i, c] is\n    images[b, j - flow[b, j, i, 0], i - flow[b, j, i, 1], c].\n    The locations specified by this formula do not necessarily map to an int\n    index. Therefore, the pixel value is obtained by bilinear\n    interpolation of the 4 nearest pixels around\n    (b, j - flow[b, j, i, 0], i - flow[b, j, i, 1]). For locations outside\n    of the image, we use the nearest pixel values at the image boundary.\n    Args:\n    image: 4-D float `Tensor` with shape `[batch, height, width, channels]`.\n    flow: A 4-D float `Tensor` with shape `[batch, height, width, 2]`.\n    name: A name for the operation (optional).\n    Note that image and flow can be of type tf.half, tf.float32, or tf.float64,\n    and do not necessarily have to be the same type.\n    Returns:\n    A 4-D float `Tensor` with shape`[batch, height, width, channels]`\n    and same type as input image.\n    Raises:\n    ValueError: if height < 2 or width < 2 or the inputs have the wrong number\n    of dimensions.\n    """"""\n    image = image.unsqueeze(3)  # add a single channel dimension to image tensor\n    batch_size, height, width, channels = image.shape\n    device = image.device\n\n    # The flow is defined on the image grid. Turn the flow into a list of query\n    # points in the grid space.\n    grid_x, grid_y = torch.meshgrid(\n        torch.arange(width, device=device), torch.arange(height, device=device)\n    )\n\n    stacked_grid = torch.stack((grid_y, grid_x), dim=2).float()\n\n    batched_grid = stacked_grid.unsqueeze(-1).permute(3, 1, 0, 2)\n\n    query_points_on_grid = batched_grid - flow\n    query_points_flattened = torch.reshape(\n        query_points_on_grid, [batch_size, height * width, 2]\n    )\n    # Compute values at the query points, then reshape the result back to the\n    # image grid.\n    interpolated = interpolate_bilinear(image, query_points_flattened)\n    interpolated = torch.reshape(interpolated, [batch_size, height, width, channels])\n    return interpolated\n\n\ndef interpolate_bilinear(\n    grid, query_points, name=""interpolate_bilinear"", indexing=""ij""\n):\n    """"""Similar to Matlab\'s interp2 function.\n\n    Notes:\n        Finds values for query points on a grid using bilinear interpolation.\n\n    Args:\n        grid: a 4-D float `Tensor` of shape `[batch, height, width, channels]`.\n        query_points: a 3-D float `Tensor` of N points with shape `[batch, N, 2]`.\n        name: a name for the operation (optional).\n        indexing: whether the query points are specified as row and column (ij),\n            or Cartesian coordinates (xy).\n\n    Returns:\n        values: a 3-D `Tensor` with shape `[batch, N, channels]`\n\n    Raises:\n        ValueError: if the indexing mode is invalid, or if the shape of the inputs\n        invalid.\n    """"""\n    if indexing != ""ij"" and indexing != ""xy"":\n        raise ValueError(""Indexing mode must be \'ij\' or \'xy\'"")\n\n    shape = grid.shape\n    if len(shape) != 4:\n        msg = ""Grid must be 4 dimensional. Received size: ""\n        raise ValueError(msg + str(grid.shape))\n\n    batch_size, height, width, channels = grid.shape\n\n    shape = [batch_size, height, width, channels]\n    query_type = query_points.dtype\n    grid_type = grid.dtype\n    grid_device = grid.device\n\n    num_queries = query_points.shape[1]\n\n    alphas = []\n    floors = []\n    ceils = []\n    index_order = [0, 1] if indexing == ""ij"" else [1, 0]\n    unstacked_query_points = query_points.unbind(2)\n\n    for dim in index_order:\n        queries = unstacked_query_points[dim]\n\n        size_in_indexing_dimension = shape[dim + 1]\n\n        # max_floor is size_in_indexing_dimension - 2 so that max_floor + 1\n        # is still a valid index into the grid.\n        max_floor = torch.tensor(\n            size_in_indexing_dimension - 2, dtype=query_type, device=grid_device\n        )\n        min_floor = torch.tensor(0.0, dtype=query_type, device=grid_device)\n        maxx = torch.max(min_floor, torch.floor(queries))\n        floor = torch.min(maxx, max_floor)\n        int_floor = floor.long()\n        floors.append(int_floor)\n        ceil = int_floor + 1\n        ceils.append(ceil)\n\n        # alpha has the same type as the grid, as we will directly use alpha\n        # when taking linear combinations of pixel values from the image.\n\n        alpha = torch.tensor((queries - floor), dtype=grid_type, device=grid_device)\n        min_alpha = torch.tensor(0.0, dtype=grid_type, device=grid_device)\n        max_alpha = torch.tensor(1.0, dtype=grid_type, device=grid_device)\n        alpha = torch.min(torch.max(min_alpha, alpha), max_alpha)\n\n        # Expand alpha to [b, n, 1] so we can use broadcasting\n        # (since the alpha values don\'t depend on the channel).\n        alpha = torch.unsqueeze(alpha, 2)\n        alphas.append(alpha)\n\n    flattened_grid = torch.reshape(grid, [batch_size * height * width, channels])\n    batch_offsets = torch.reshape(\n        torch.arange(batch_size, device=grid_device) * height * width, [batch_size, 1]\n    )\n\n    # This wraps array_ops.gather. We reshape the image data such that the\n    # batch, y, and x coordinates are pulled into the first dimension.\n    # Then we gather. Finally, we reshape the output back. It\'s possible this\n    # code would be made simpler by using array_ops.gather_nd.\n    def gather(y_coords, x_coords, name):\n        linear_coordinates = batch_offsets + y_coords * width + x_coords\n        gathered_values = torch.gather(flattened_grid.t(), 1, linear_coordinates)\n        return torch.reshape(gathered_values, [batch_size, num_queries, channels])\n\n    # grab the pixel values in the 4 corners around each query point\n    top_left = gather(floors[0], floors[1], ""top_left"")\n    top_right = gather(floors[0], ceils[1], ""top_right"")\n    bottom_left = gather(ceils[0], floors[1], ""bottom_left"")\n    bottom_right = gather(ceils[0], ceils[1], ""bottom_right"")\n\n    interp_top = alphas[1] * (top_right - top_left) + top_left\n    interp_bottom = alphas[1] * (bottom_right - bottom_left) + bottom_left\n    interp = alphas[0] * (interp_bottom - interp_top) + interp_top\n\n    return interp\n'"
espnet2/asr/__init__.py,0,b''
espnet2/asr/ctc.py,11,"b'import torch\nimport torch.nn.functional as F\nfrom typeguard import check_argument_types\n\n\nclass CTC(torch.nn.Module):\n    """"""CTC module.\n\n    Args:\n        odim: dimension of outputs\n        encoder_output_sizse: number of encoder projection units\n        dropout_rate: dropout rate (0.0 ~ 1.0)\n        ctc_type: builtin or warpctc\n        reduce: reduce the CTC loss into a scalar\n    """"""\n\n    def __init__(\n        self,\n        odim: int,\n        encoder_output_sizse: int,\n        dropout_rate: float = 0.0,\n        ctc_type: str = ""builtin"",\n        reduce: bool = True,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        eprojs = encoder_output_sizse\n        self.dropout_rate = dropout_rate\n        self.ctc_lo = torch.nn.Linear(eprojs, odim)\n        self.ctc_type = ctc_type\n\n        if self.ctc_type == ""builtin"":\n            reduction_type = ""sum"" if reduce else ""none""\n            self.ctc_loss = torch.nn.CTCLoss(reduction=reduction_type)\n        elif self.ctc_type == ""warpctc"":\n            import warpctc_pytorch as warp_ctc\n\n            self.ctc_loss = warp_ctc.CTCLoss(size_average=True, reduce=reduce)\n        else:\n            raise ValueError(\n                f\'ctc_type must be ""builtin"" or ""warpctc"": {self.ctc_type}\'\n            )\n\n        self.reduce = reduce\n\n    def loss_fn(self, th_pred, th_target, th_ilen, th_olen) -> torch.Tensor:\n        if self.ctc_type == ""builtin"":\n            th_pred = th_pred.log_softmax(2)\n            loss = self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n            # Batch-size average\n            loss = loss / th_pred.size(1)\n            return loss\n        elif self.ctc_type == ""warpctc"":\n            # warpctc only supports float32\n            th_pred = th_pred.to(dtype=torch.float32)\n\n            th_target = th_target.cpu().int()\n            th_ilen = th_ilen.cpu().int()\n            th_olen = th_olen.cpu().int()\n            loss = self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n            if self.reduce:\n                # NOTE: sum() is needed to keep consistency since warpctc\n                # return as tensor w/ shape (1,)\n                # but builtin return as tensor w/o shape (scalar).\n                loss = loss.sum()\n            return loss\n        else:\n            raise NotImplementedError\n\n    def forward(self, hs_pad, hlens, ys_pad, ys_lens):\n        """"""Calculate CTC loss.\n\n        Args:\n            hs_pad: batch of padded hidden state sequences (B, Tmax, D)\n            hlens: batch of lengths of hidden state sequences (B)\n            ys_pad: batch of padded character id sequence tensor (B, Lmax)\n            ys_lens: batch of lengths of character sequence (B)\n        """"""\n        # hs_pad: (B, L, NProj) -> ys_hat: (B, L, Nvocab)\n        ys_hat = self.ctc_lo(F.dropout(hs_pad, p=self.dropout_rate))\n        # ys_hat: (B, L, D) -> (L, B, D)\n        ys_hat = ys_hat.transpose(0, 1)\n\n        # (B, L) -> (BxL,)\n        ys_true = torch.cat([ys_pad[i, :l] for i, l in enumerate(ys_lens)])\n\n        loss = self.loss_fn(ys_hat, ys_true, hlens, ys_lens).to(\n            device=hs_pad.device, dtype=hs_pad.dtype\n        )\n\n        return loss\n\n    def log_softmax(self, hs_pad):\n        """"""log_softmax of frame activations\n\n        Args:\n            Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        Returns:\n            torch.Tensor: log softmax applied 3d tensor (B, Tmax, odim)\n        """"""\n        return F.log_softmax(self.ctc_lo(hs_pad), dim=2)\n\n    def argmax(self, hs_pad):\n        """"""argmax of frame activations\n\n        Args:\n            torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        Returns:\n            torch.Tensor: argmax applied 2d tensor (B, Tmax)\n        """"""\n        return torch.argmax(self.ctc_lo(hs_pad), dim=2)\n'"
espnet2/asr/espnet_model.py,26,"b'from typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.e2e_asr_common import ErrorCalculator\nfrom espnet.nets.pytorch_backend.nets_utils import th_accuracy\nfrom espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\nfrom espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (\n    LabelSmoothingLoss,  # noqa: H301\n)\nfrom espnet2.asr.ctc import CTC\nfrom espnet2.asr.decoder.abs_decoder import AbsDecoder\nfrom espnet2.asr.encoder.abs_encoder import AbsEncoder\nfrom espnet2.asr.frontend.abs_frontend import AbsFrontend\nfrom espnet2.asr.specaug.abs_specaug import AbsSpecAug\nfrom espnet2.layers.abs_normalize import AbsNormalize\nfrom espnet2.torch_utils.device_funcs import force_gatherable\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\n\n\nclass ESPnetASRModel(AbsESPnetModel):\n    """"""CTC-attention hybrid Encoder-Decoder model""""""\n\n    def __init__(\n        self,\n        vocab_size: int,\n        token_list: Union[Tuple[str, ...], List[str]],\n        frontend: Optional[AbsFrontend],\n        specaug: Optional[AbsSpecAug],\n        normalize: Optional[AbsNormalize],\n        encoder: AbsEncoder,\n        decoder: AbsDecoder,\n        ctc: CTC,\n        rnnt_decoder: None,\n        ctc_weight: float = 0.5,\n        ignore_id: int = -1,\n        lsm_weight: float = 0.0,\n        length_normalized_loss: bool = False,\n        report_cer: bool = True,\n        report_wer: bool = True,\n        sym_space: str = ""<space>"",\n        sym_blank: str = ""<blank>"",\n    ):\n        assert check_argument_types()\n        assert 0.0 <= ctc_weight <= 1.0, ctc_weight\n        assert rnnt_decoder is None, ""Not implemented""\n\n        super().__init__()\n        # note that eos is the same as sos (equivalent ID)\n        self.sos = vocab_size - 1\n        self.eos = vocab_size - 1\n        self.vocab_size = vocab_size\n        self.ignore_id = ignore_id\n        self.ctc_weight = ctc_weight\n        self.token_list = token_list.copy()\n\n        self.frontend = frontend\n        self.specaug = specaug\n        self.normalize = normalize\n        self.encoder = encoder\n        self.decoder = decoder\n        if ctc_weight == 0.0:\n            self.ctc = None\n        else:\n            self.ctc = ctc\n        self.rnnt_decoder = rnnt_decoder\n        self.criterion_att = LabelSmoothingLoss(\n            size=vocab_size,\n            padding_idx=ignore_id,\n            smoothing=lsm_weight,\n            normalize_length=length_normalized_loss,\n        )\n\n        if report_cer or report_wer:\n            self.error_calculator = ErrorCalculator(\n                token_list, sym_space, sym_blank, report_cer, report_wer\n            )\n        else:\n            self.error_calculator = None\n\n    def forward(\n        self,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:\n        """"""Frontend + Encoder + Decoder + Calc loss\n\n        Args:\n            speech: (Batch, Length, ...)\n            speech_lengths: (Batch, )\n            text: (Batch, Length)\n            text_lengths: (Batch,)\n        """"""\n        assert text_lengths.dim() == 1, text_lengths.shape\n        # Check that batch_size is unified\n        assert (\n            speech.shape[0]\n            == speech_lengths.shape[0]\n            == text.shape[0]\n            == text_lengths.shape[0]\n        ), (speech.shape, speech_lengths.shape, text.shape, text_lengths.shape)\n        batch_size = speech.shape[0]\n\n        # for data-parallel\n        text = text[:, : text_lengths.max()]\n\n        # 1. Encoder\n        encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)\n\n        # 2a. Attention-decoder branch\n        if self.ctc_weight == 1.0:\n            loss_att, acc_att, cer_att, wer_att = None, None, None, None\n        else:\n            loss_att, acc_att, cer_att, wer_att = self._calc_att_loss(\n                encoder_out, encoder_out_lens, text, text_lengths\n            )\n\n        # 2b. CTC branch\n        if self.ctc_weight == 0.0:\n            loss_ctc, cer_ctc = None, None\n        else:\n            loss_ctc, cer_ctc = self._calc_ctc_loss(\n                encoder_out, encoder_out_lens, text, text_lengths\n            )\n\n        # 2c. RNN-T branch\n        if self.rnnt_decoder is not None:\n            _ = self._calc_rnnt_loss(encoder_out, encoder_out_lens, text, text_lengths)\n\n        if self.ctc_weight == 0.0:\n            loss = loss_att\n        elif self.ctc_weight == 1.0:\n            loss = loss_ctc\n        else:\n            loss = self.ctc_weight * loss_ctc + (1 - self.ctc_weight) * loss_att\n\n        stats = dict(\n            loss=loss.detach(),\n            loss_att=loss_att.detach() if loss_att is not None else None,\n            loss_ctc=loss_ctc.detach() if loss_ctc is not None else None,\n            acc=acc_att,\n            cer=cer_att,\n            wer=wer_att,\n            cer_ctc=cer_ctc,\n        )\n\n        # force_gatherable: to-device and to-tensor if scalar for DataParallel\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n        return loss, stats, weight\n\n    def collect_feats(\n        self,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n    ) -> Dict[str, torch.Tensor]:\n        feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n        return {""feats"": feats, ""feats_lengths"": feats_lengths}\n\n    def encode(\n        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Frontend + Encoder. Note that this method is used by asr_inference.py\n\n        Args:\n            speech: (Batch, Length, ...)\n            speech_lengths: (Batch, )\n        """"""\n        # 1. Extract feats\n        feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n\n        # 2. Data augmentation for spectrogram\n        if self.specaug is not None and self.training:\n            feats, feats_lengths = self.specaug(feats, feats_lengths)\n\n        # 3. Normalization for feature: e.g. Global-CMVN, Utterance-CMVN\n        if self.normalize is not None:\n            feats, feats_lengths = self.normalize(feats, feats_lengths)\n\n        # 4. Forward encoder\n        # feats: (Batch, Length, Dim)\n        # -> encoder_out: (Batch, Length2, Dim2)\n        encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)\n\n        assert encoder_out.size(0) == speech.size(0), (\n            encoder_out.size(),\n            speech.size(0),\n        )\n        assert encoder_out.size(1) <= encoder_out_lens.max(), (\n            encoder_out.size(),\n            encoder_out_lens.max(),\n        )\n\n        return encoder_out, encoder_out_lens\n\n    def _extract_feats(\n        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        assert speech_lengths.dim() == 1, speech_lengths.shape\n\n        # for data-parallel\n        speech = speech[:, : speech_lengths.max()]\n\n        if self.frontend is not None:\n            # Frontend\n            #  e.g. STFT and Feature extract\n            #       data_loader may send time-domain signal in this case\n            # speech (Batch, NSamples) -> feats: (Batch, NFrames, Dim)\n            feats, feats_lengths = self.frontend(speech, speech_lengths)\n        else:\n            # No frontend and no feature extract\n            feats, feats_lengths = speech, speech_lengths\n        return feats, feats_lengths\n\n    def _calc_att_loss(\n        self,\n        encoder_out: torch.Tensor,\n        encoder_out_lens: torch.Tensor,\n        ys_pad: torch.Tensor,\n        ys_pad_lens: torch.Tensor,\n    ):\n        ys_in_pad, ys_out_pad = add_sos_eos(ys_pad, self.sos, self.eos, self.ignore_id)\n        ys_in_lens = ys_pad_lens + 1\n\n        # 1. Forward decoder\n        decoder_out, _ = self.decoder(\n            encoder_out, encoder_out_lens, ys_in_pad, ys_in_lens\n        )\n\n        # 2. Compute attention loss\n        loss_att = self.criterion_att(decoder_out, ys_out_pad)\n        acc_att = th_accuracy(\n            decoder_out.view(-1, self.vocab_size),\n            ys_out_pad,\n            ignore_label=self.ignore_id,\n        )\n\n        # Compute cer/wer using attention-decoder\n        if self.training or self.error_calculator is None:\n            cer_att, wer_att = None, None\n        else:\n            ys_hat = decoder_out.argmax(dim=-1)\n            cer_att, wer_att = self.error_calculator(ys_hat.cpu(), ys_pad.cpu())\n\n        return loss_att, acc_att, cer_att, wer_att\n\n    def _calc_ctc_loss(\n        self,\n        encoder_out: torch.Tensor,\n        encoder_out_lens: torch.Tensor,\n        ys_pad: torch.Tensor,\n        ys_pad_lens: torch.Tensor,\n    ):\n        # Calc CTC loss\n        loss_ctc = self.ctc(encoder_out, encoder_out_lens, ys_pad, ys_pad_lens)\n\n        # Calc CER using CTC\n        cer_ctc = None\n        if not self.training and self.error_calculator is not None:\n            ys_hat = self.ctc.argmax(encoder_out).data\n            cer_ctc = self.error_calculator(ys_hat.cpu(), ys_pad.cpu(), is_ctc=True)\n        return loss_ctc, cer_ctc\n\n    def _calc_rnnt_loss(\n        self,\n        encoder_out: torch.Tensor,\n        encoder_out_lens: torch.Tensor,\n        ys_pad: torch.Tensor,\n        ys_pad_lens: torch.Tensor,\n    ):\n        raise NotImplementedError\n'"
espnet2/bin/__init__.py,0,b''
espnet2/bin/aggregate_stats_dirs.py,0,"b'import argparse\nimport logging\nfrom pathlib import Path\nimport sys\nfrom typing import Iterable\nfrom typing import Union\n\nimport numpy as np\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\n\ndef aggregate_stats_dirs(\n    input_dir: Iterable[Union[str, Path]], output_dir: Union[str, Path], log_level: str,\n):\n    logging.basicConfig(\n        level=log_level,\n        format=""%(asctime)s (%(module)s:%(lineno)d) (levelname)s: %(message)s"",\n    )\n\n    input_dirs = [Path(p) for p in input_dir]\n    output_dir = Path(output_dir)\n\n    for mode in [""train"", ""valid""]:\n        with (input_dirs[0] / mode / ""batch_keys"").open(""r"", encoding=""utf-8"") as f:\n            batch_keys = [line.strip() for line in f if line.strip() != """"]\n        with (input_dirs[0] / mode / ""stats_keys"").open(""r"", encoding=""utf-8"") as f:\n            stats_keys = [line.strip() for line in f if line.strip() != """"]\n        (output_dir / mode).mkdir(parents=True, exist_ok=True)\n\n        for key in batch_keys:\n            with (output_dir / mode / f""{key}_shape"").open(\n                ""w"", encoding=""utf-8""\n            ) as fout:\n                for idir in input_dirs:\n                    with (idir / mode / f""{key}_shape"").open(\n                        ""r"", encoding=""utf-8""\n                    ) as fin:\n                        for line in fin:\n                            fout.write(line)\n\n        for key in stats_keys:\n            sum_stats = None\n            for idir in input_dirs:\n                stats = np.load(idir / mode / f""{key}_stats.npz"")\n                if sum_stats is None:\n                    sum_stats = dict(**stats)\n                else:\n                    for k in stats:\n                        sum_stats[k] += stats[k]\n\n            np.savez(output_dir / mode / f""{key}_stats.npz"", **sum_stats)\n\n            # if --write_collected_feats=true\n            p = Path(mode) / ""collect_feats"" / f""{key}.scp""\n            scp = input_dirs[0] / p\n            if scp.exists():\n                (output_dir / p).parent.mkdir(parents=True, exist_ok=True)\n                with (output_dir / p).open(""w"", encoding=""utf-8"") as fout:\n                    for idir in input_dirs:\n                        with (idir / p).open(""r"", encoding=""utf-8"") as fin:\n                            for line in fin:\n                                fout.write(line)\n\n\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(\n        description=""Aggregate statistics directories into one directory"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--log_level"",\n        type=lambda x: x.upper(),\n        default=""INFO"",\n        choices=(""INFO"", ""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""),\n        help=""The verbose level of logging"",\n    )\n\n    parser.add_argument(""--input_dir"", action=""append"", help=""Input directories"")\n    parser.add_argument(""--output_dir"", required=True, help=""Output directory"")\n    return parser\n\n\ndef main(cmd=None):\n    print(get_commandline_args(), file=sys.stderr)\n    parser = get_parser()\n    args = parser.parse_args(cmd)\n    kwargs = vars(args)\n    aggregate_stats_dirs(**kwargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/asr_inference.py,2,"b'#!/usr/bin/env python3\nimport logging\nimport sys\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nimport configargparse\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.beam_search import BeamSearch\nfrom espnet.nets.beam_search import Hypothesis\nfrom espnet.nets.scorers.ctc import CTCPrefixScorer\nfrom espnet.nets.scorers.length_bonus import LengthBonus\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet2.tasks.asr import ASRTask\nfrom espnet2.tasks.lm import LMTask\nfrom espnet2.text.build_tokenizer import build_tokenizer\nfrom espnet2.text.token_id_converter import TokenIDConverter\nfrom espnet2.torch_utils.device_funcs import to_device\nfrom espnet2.torch_utils.set_all_random_seed import set_all_random_seed\nfrom espnet2.utils.fileio import DatadirWriter\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str2triple_str\nfrom espnet2.utils.types import str_or_none\n\n\ndef inference(\n    output_dir: str,\n    maxlenratio: float,\n    minlenratio: float,\n    batch_size: int,\n    dtype: str,\n    beam_size: int,\n    ngpu: int,\n    seed: int,\n    ctc_weight: float,\n    lm_weight: float,\n    penalty: float,\n    nbest: int,\n    num_workers: int,\n    log_level: Union[int, str],\n    data_path_and_name_and_type: Sequence[Tuple[str, str, str]],\n    key_file: Optional[str],\n    asr_train_config: str,\n    asr_model_file: str,\n    lm_train_config: Optional[str],\n    lm_file: Optional[str],\n    word_lm_train_config: Optional[str],\n    word_lm_file: Optional[str],\n    blank_symbol: str,\n    token_type: Optional[str],\n    bpemodel: Optional[str],\n    allow_variable_data_keys: bool,\n):\n    assert check_argument_types()\n    if batch_size > 1:\n        raise NotImplementedError(""batch decoding is not implemented"")\n    if word_lm_train_config is not None:\n        raise NotImplementedError(""Word LM is not implemented"")\n    if ngpu > 1:\n        raise NotImplementedError(""only single GPU decoding is supported"")\n\n    logging.basicConfig(\n        level=log_level,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n\n    if ngpu >= 1:\n        device = ""cuda""\n    else:\n        device = ""cpu""\n\n    # 1. Set random-seed\n    set_all_random_seed(seed)\n\n    # 2. Build ASR model\n    scorers = {}\n    asr_model, asr_train_args = ASRTask.build_model_from_file(\n        asr_train_config, asr_model_file, device\n    )\n    asr_model.eval()\n\n    decoder = asr_model.decoder\n    ctc = CTCPrefixScorer(ctc=asr_model.ctc, eos=asr_model.eos)\n    token_list = asr_model.token_list\n    scorers.update(\n        decoder=decoder, ctc=ctc, length_bonus=LengthBonus(len(token_list)),\n    )\n\n    # 3. Build Language model\n    if lm_train_config is not None:\n        lm, lm_train_args = LMTask.build_model_from_file(\n            lm_train_config, lm_file, device\n        )\n        scorers[""lm""] = lm.lm\n\n    # 4. Build BeamSearch object\n    weights = dict(\n        decoder=1.0 - ctc_weight, ctc=ctc_weight, lm=lm_weight, length_bonus=penalty,\n    )\n    beam_search = BeamSearch(\n        beam_size=beam_size,\n        weights=weights,\n        scorers=scorers,\n        sos=asr_model.sos,\n        eos=asr_model.eos,\n        vocab_size=len(token_list),\n        token_list=token_list,\n    )\n    beam_search.to(device=device, dtype=getattr(torch, dtype)).eval()\n    for scorer in scorers.values():\n        if isinstance(scorer, torch.nn.Module):\n            scorer.to(device=device, dtype=getattr(torch, dtype)).eval()\n    logging.info(f""Beam_search: {beam_search}"")\n    logging.info(f""Decoding device={device}, dtype={dtype}"")\n\n    # 5. Build data-iterator\n    loader, _, _ = ASRTask.build_non_sorted_iterator(\n        data_path_and_name_and_type,\n        dtype=dtype,\n        batch_size=batch_size,\n        key_file=key_file,\n        num_workers=num_workers,\n        preprocess_fn=ASRTask.build_preprocess_fn(asr_train_args, False),\n        collate_fn=ASRTask.build_collate_fn(asr_train_args),\n        allow_variable_data_keys=allow_variable_data_keys,\n    )\n\n    # 6. [Optional] Build Text converter: e.g. bpe-sym -> Text\n    if token_type is None:\n        token_type = asr_train_args.token_type\n    if bpemodel is None:\n        bpemodel = asr_train_args.bpemodel\n\n    if token_type is None:\n        tokenizer = None\n    elif token_type == ""bpe"":\n        if bpemodel is not None:\n            tokenizer = build_tokenizer(token_type=token_type, bpemodel=bpemodel)\n        else:\n            tokenizer = None\n    else:\n        tokenizer = build_tokenizer(token_type=token_type)\n    converter = TokenIDConverter(token_list=token_list)\n    logging.info(f""Text tokenizer: {tokenizer}"")\n\n    # 7 .Start for-loop\n    # FIXME(kamo): The output format should be discussed about\n    with DatadirWriter(output_dir) as writer:\n        for keys, batch in loader:\n            assert isinstance(batch, dict), type(batch)\n            assert all(isinstance(s, str) for s in keys), keys\n            _bs = len(next(iter(batch.values())))\n            assert len(keys) == _bs, f""{len(keys)} != {_bs}""\n\n            with torch.no_grad():\n                # a. To device\n                batch = to_device(batch, device)\n\n                # b. Forward Encoder\n                enc, _ = asr_model.encode(**batch)\n                assert len(enc) == batch_size, len(enc)\n\n                # c. Passed the encoder result and the beam search\n                nbest_hyps = beam_search(\n                    x=enc[0], maxlenratio=maxlenratio, minlenratio=minlenratio\n                )\n                nbest_hyps = nbest_hyps[:nbest]\n\n            # Only supporting batch_size==1\n            key = keys[0]\n            for n in range(1, nbest + 1):\n                hyp = nbest_hyps[n - 1]\n                assert isinstance(hyp, Hypothesis), type(hyp)\n\n                # remove sos/eos and get results\n                token_int = hyp.yseq[1:-1].tolist()\n\n                # remove blank symbol id, which is assumed to be 0\n                token_int = list(filter(lambda x: x != 0, token_int))\n\n                # Change integer-ids to tokens\n                token = converter.ids2tokens(token_int)\n\n                # Create a directory: outdir/{n}best_recog\n                ibest_writer = writer[f""{n}best_recog""]\n\n                # Write the result to each files\n                ibest_writer[""token""][key] = "" "".join(token)\n                ibest_writer[""token_int""][key] = "" "".join(map(str, token_int))\n                ibest_writer[""score""][key] = str(hyp.score)\n\n                if tokenizer is not None:\n                    text = tokenizer.tokens2text(token)\n                    ibest_writer[""text""][key] = text\n\n\ndef get_parser():\n    parser = configargparse.ArgumentParser(\n        description=""ASR Decoding"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    # Note(kamo): Use \'_\' instead of \'-\' as separator.\n    # \'-\' is confusing if written in yaml.\n    parser.add_argument(""--config"", is_config_file=True, help=""config file path"")\n\n    parser.add_argument(\n        ""--log_level"",\n        type=lambda x: x.upper(),\n        default=""INFO"",\n        choices=(""INFO"", ""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""),\n        help=""The verbose level of logging"",\n    )\n\n    parser.add_argument(""--output_dir"", type=str, required=True)\n    parser.add_argument(\n        ""--ngpu"", type=int, default=0, help=""The number of gpus. 0 indicates CPU mode"",\n    )\n    parser.add_argument(""--seed"", type=int, default=0, help=""Random seed"")\n    parser.add_argument(\n        ""--dtype"",\n        default=""float32"",\n        choices=[""float16"", ""float32"", ""float64""],\n        help=""Data type"",\n    )\n    parser.add_argument(\n        ""--num_workers"",\n        type=int,\n        default=1,\n        help=""The number of workers used for DataLoader"",\n    )\n\n    group = parser.add_argument_group(""Input data related"")\n    group.add_argument(\n        ""--data_path_and_name_and_type"",\n        type=str2triple_str,\n        required=True,\n        action=""append"",\n    )\n    group.add_argument(""--key_file"", type=str_or_none)\n    group.add_argument(""--allow_variable_data_keys"", type=str2bool, default=False)\n\n    group = parser.add_argument_group(""The model configuration related"")\n    group.add_argument(""--asr_train_config"", type=str, required=True)\n    group.add_argument(""--asr_model_file"", type=str, required=True)\n    group.add_argument(""--lm_train_config"", type=str)\n    group.add_argument(""--lm_file"", type=str)\n    group.add_argument(""--word_lm_train_config"", type=str)\n    group.add_argument(""--word_lm_file"", type=str)\n\n    group = parser.add_argument_group(""Beam-search related"")\n    group.add_argument(\n        ""--batch_size"", type=int, default=1, help=""The batch size for inference"",\n    )\n    group.add_argument(""--nbest"", type=int, default=1, help=""Output N-best hypotheses"")\n    group.add_argument(""--beam_size"", type=int, default=20, help=""Beam size"")\n    group.add_argument(""--penalty"", type=float, default=0.0, help=""Insertion penalty"")\n    group.add_argument(\n        ""--maxlenratio"",\n        type=float,\n        default=0.0,\n        help=""Input length ratio to obtain max output length. ""\n        ""If maxlenratio=0.0 (default), it uses a end-detect ""\n        ""function ""\n        ""to automatically find maximum hypothesis lengths"",\n    )\n    group.add_argument(\n        ""--minlenratio"",\n        type=float,\n        default=0.0,\n        help=""Input length ratio to obtain min output length"",\n    )\n    group.add_argument(\n        ""--ctc_weight"", type=float, default=0.5, help=""CTC weight in joint decoding"",\n    )\n    group.add_argument(""--lm_weight"", type=float, default=1.0, help=""RNNLM weight"")\n    group.add_argument(\n        ""--blank_symbol"",\n        type=str,\n        default=""<blank>"",\n        help=""The token symbol represents CTC-blank"",\n    )\n\n    group = parser.add_argument_group(""Text converter related"")\n    group.add_argument(\n        ""--token_type"",\n        type=str_or_none,\n        default=None,\n        choices=[""char"", ""bpe"", None],\n        help=""The token type for ASR model. ""\n        ""If not given, refers from the training args"",\n    )\n    group.add_argument(\n        ""--bpemodel"",\n        type=str_or_none,\n        default=None,\n        help=""The model path of sentencepiece. ""\n        ""If not given, refers from the training args"",\n    )\n\n    return parser\n\n\ndef main(cmd=None):\n    print(get_commandline_args(), file=sys.stderr)\n    parser = get_parser()\n    args = parser.parse_args(cmd)\n    kwargs = vars(args)\n    kwargs.pop(""config"", None)\n    inference(**kwargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/asr_train.py,0,"b'#!/usr/bin/env python3\nfrom espnet2.tasks.asr import ASRTask\n\n\ndef get_parser():\n    parser = ASRTask.get_parser()\n    return parser\n\n\ndef main(cmd=None):\n    r""""""ASR training.\n\n    Example:\n\n        % python asr_train.py asr --print_config --optim adadelta \\\n                > conf/train_asr.yaml\n        % python asr_train.py --config conf/train_asr.yaml\n    """"""\n    ASRTask.main(cmd=cmd)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/launch.py,5,"b'#!/usr/bin/env python3\nimport argparse\nimport logging\nimport os\nfrom pathlib import Path\nimport shlex\nimport shutil\nimport subprocess\nimport sys\nimport uuid\n\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str_or_none\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""Launch distributed process with appropriate options. "",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--cmd"",\n        help=""The path of cmd script of Kaldi: run.pl. queue.pl, or slurm.pl"",\n        default=""utils/run.pl"",\n    )\n    parser.add_argument(\n        ""--log"", help=""The path of log file used by cmd"", default=""run.log"",\n    )\n    parser.add_argument(\n        ""--max_num_log_files"",\n        help=""The maximum number of log-files to be kept"",\n        default=1000,\n    )\n    parser.add_argument(\n        ""--ngpu"", type=int, default=1, help=""The number of GPUs per node""\n    )\n    egroup = parser.add_mutually_exclusive_group()\n    egroup.add_argument(""--num_nodes"", type=int, default=1, help=""The number of nodes"")\n    egroup.add_argument(\n        ""--host"",\n        type=str,\n        default=None,\n        help=""Directly specify the host names.  The job are submitted via SSH. ""\n        ""Multiple host names can be specified by splitting by comma. e.g. host1,host2""\n        "" You can also the device id after the host name with \':\'. e.g. ""\n        ""host1:0:2:3,host2:0:2. If the device ids are specified in this way, ""\n        ""the value of --ngpu is ignored."",\n    )\n    parser.add_argument(\n        ""--envfile"",\n        type=str_or_none,\n        default=""path.sh"",\n        help=""Source the shell script before executing command. ""\n        ""This option is used when --host is specified."",\n    )\n\n    parser.add_argument(\n        ""--multiprocessing_distributed"",\n        type=str2bool,\n        default=True,\n        help=""Distributed method is used when single-node mode."",\n    )\n    parser.add_argument(\n        ""--master_port"",\n        type=int,\n        default=None,\n        help=""Specify the port number of master""\n        ""Master is a host machine has RANK0 process."",\n    )\n    parser.add_argument(\n        ""--master_addr"",\n        type=str,\n        default=None,\n        help=""Specify the address s of master. ""\n        ""Master is a host machine has RANK0 process."",\n    )\n    parser.add_argument(\n        ""--init_file_prefix"",\n        type=str,\n        default="".dist_init_"",\n        help=""The file name prefix for init_file, which is used for ""\n        ""\'Shared-file system initialization\'. ""\n        ""This option is used when --port is not specified"",\n    )\n    parser.add_argument(""args"", type=str, nargs=""+"")\n    return parser\n\n\ndef main(cmd=None):\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    parser = get_parser()\n    args = parser.parse_args(cmd)\n    args.cmd = shlex.split(args.cmd)\n\n    if args.host is None and shutil.which(args.cmd[0]) is None:\n        raise RuntimeError(\n            f""The first args of --cmd should be a script path. e.g. utils/run.pl: ""\n            f""{args.cmd[0]}""\n        )\n\n    # Specify init_method:\n    #   See: https://pytorch.org/docs/stable/distributed.html#initialization\n    if args.host is None and args.num_nodes <= 1:\n        # Automatically set init_method if num_node=1\n        init_method = None\n    else:\n        if args.master_port is None:\n            # Try ""shared-file system initialization"" if master_port is not specified\n            # Give random name to avoid reusing previous file\n            init_file = args.init_file_prefix + str(uuid.uuid4())\n            init_file = Path(init_file).absolute()\n            Path(init_file).parent.mkdir(exist_ok=True, parents=True)\n            init_method = [""--dist_init_method"", f""file://{init_file}""]\n        else:\n            init_method = [""--dist_master_port"", str(args.master_port)]\n\n            # This can be omitted if slurm mode\n            if args.master_addr is not None:\n                init_method += [""--dist_master_addr"", args.master_addr]\n            elif args.host is not None:\n                init_method += [\n                    ""--dist_master_addr"",\n                    args.host.split("","")[0].split("":"")[0],\n                ]\n\n    # Log-rotation\n    for i in range(args.max_num_log_files - 1, -1, -1):\n        if i == 0:\n            p = Path(args.log)\n            pn = p.parent / (p.stem + "".1"" + p.suffix)\n        else:\n            _p = Path(args.log)\n            p = _p.parent / (_p.stem + f"".{i}"" + _p.suffix)\n            pn = _p.parent / (_p.stem + f"".{i + 1}"" + _p.suffix)\n\n        if p.exists():\n            if i == args.max_num_log_files - 1:\n                p.unlink()\n            else:\n                shutil.move(p, pn)\n\n    processes = []\n    # Submit command via SSH\n    if args.host is not None:\n        hosts = []\n        ids_list = []\n        # e.g. args.host = ""host1:0:2,host2:0:1""\n        for host in args.host.split("",""):\n            # e.g host = ""host1:0:2""\n            sps = host.split("":"")\n            host = sps[0]\n            if len(sps) > 1:\n                ids = [int(x) for x in sps[1:]]\n            else:\n                ids = list(range(args.ngpu))\n            hosts.append(host)\n            ids_list.append(ids)\n\n        world_size = sum(max(len(x), 1) for x in ids_list)\n        logging.info(f""{len(hosts)}nodes with world_size={world_size} via SSH"")\n\n        if args.envfile is not None:\n            env = f""source {args.envfile}""\n        else:\n            env = """"\n\n        if args.log != ""-"":\n            Path(args.log).parent.mkdir(parents=True, exist_ok=True)\n            f = Path(args.log).open(""w"", encoding=""utf-8"")\n        else:\n            # Output to stdout/stderr\n            f = None\n\n        rank = 0\n        for host, ids in zip(hosts, ids_list):\n            ngpu = 1 if len(ids) > 0 else 0\n            ids = ids if len(ids) > 0 else [""none""]\n\n            for local_rank in ids:\n                cmd = (\n                    args.args\n                    + [\n                        ""--ngpu"",\n                        str(ngpu),\n                        ""--multiprocessing_distributed"",\n                        ""false"",\n                        ""--local_rank"",\n                        str(local_rank),\n                        ""--dist_rank"",\n                        str(rank),\n                        ""--dist_world_size"",\n                        str(world_size),\n                    ]\n                    + init_method\n                )\n                if ngpu == 0:\n                    # Gloo supports both GPU and CPU mode.\n                    #   See: https://pytorch.org/docs/stable/distributed.html\n                    cmd += [""--dist_backend"", ""gloo""]\n\n                heredoc = f""""""<< EOF\nset -euo pipefail\ncd {os.getcwd()}\n{env}\n{"" "".join([c if len(c) != 0 else ""\'\'"" for c in cmd])}\nEOF\n""""""\n\n                # FIXME(kamo): The process will be alive\n                #  even if this program is stopped because we don\'t set -t here,\n                #  i.e. not assigning pty,\n                #  and the program is not killed when SSH connection is closed.\n                process = subprocess.Popen(\n                    [""ssh"", host, ""bash"", heredoc], stdout=f, stderr=f,\n                )\n\n                processes.append(process)\n\n                rank += 1\n\n    # If Single node\n    elif args.num_nodes <= 1:\n        if args.ngpu > 1:\n            if args.multiprocessing_distributed:\n                # NOTE:\n                #   If multiprocessing_distributed=true,\n                # -> Distributed mode, which is multi-process and Multi-GPUs.\n                #    and TCP initializetion is used if single-node case:\n                #      e.g. init_method=""tcp://localhost:20000""\n                logging.info(f""single-node with {args.ngpu}gpu on distributed mode"")\n            else:\n                # NOTE:\n                #   If multiprocessing_distributed=false\n                # -> ""DataParallel"" mode, which is single-process\n                #    and Multi-GPUs with threading.\n                # See:\n                # https://discuss.pytorch.org/t/why-torch-nn-parallel-distributeddataparallel-runs-faster-than-torch-nn-dataparallel-on-single-machine-with-multi-gpu/32977/2\n                logging.info(f""single-node with {args.ngpu}gpu using DataParallel"")\n\n        # Using cmd as it is simply\n        cmd = (\n            args.cmd\n            # arguments for ${cmd}\n            + [""--gpu"", str(args.ngpu), args.log]\n            # arguments for *_train.py\n            + args.args\n            + [\n                ""--ngpu"",\n                str(args.ngpu),\n                ""--multiprocessing_distributed"",\n                str(args.multiprocessing_distributed),\n            ]\n        )\n        process = subprocess.Popen(cmd)\n        processes.append(process)\n\n    elif Path(args.cmd[0]).name == ""run.pl"":\n        raise RuntimeError(""run.pl doesn\'t support submitting to the other nodes."")\n\n    elif Path(args.cmd[0]).name == ""ssh.pl"":\n        raise RuntimeError(""Use --host option instead of ssh.pl"")\n\n    # If Slurm\n    elif Path(args.cmd[0]).name == ""slurm.pl"":\n        logging.info(f""{args.num_nodes}nodes and {args.ngpu}gpu-per-node using srun"")\n        cmd = (\n            args.cmd\n            # arguments for ${cmd}\n            + [\n                ""--gpu"",\n                str(args.ngpu),\n                ""--num_threads"",\n                str(max(args.ngpu, 1)),\n                ""--num_nodes"",\n                str(args.num_nodes),\n                args.log,\n                ""srun"",\n                # Inherit all enviroment variable from parent process\n                ""--export=ALL"",\n            ]\n            # arguments for *_train.py\n            + args.args\n            + [\n                ""--ngpu"",\n                str(args.ngpu),\n                ""--multiprocessing_distributed"",\n                ""true"",\n                ""--dist_launcher"",\n                ""slurm"",\n            ]\n            + init_method\n        )\n        if args.ngpu == 0:\n            # Gloo supports both GPU and CPU mode.\n            #   See: https://pytorch.org/docs/stable/distributed.html\n            cmd += [""--dist_backend"", ""gloo""]\n        process = subprocess.Popen(cmd)\n        processes.append(process)\n\n    else:\n        # This pattern can also works with Slurm.\n\n        logging.info(f""{args.num_nodes}nodes and {args.ngpu}gpu-per-node using mpirun"")\n        cmd = (\n            args.cmd\n            # arguments for ${cmd}\n            + [\n                ""--gpu"",\n                str(args.ngpu),\n                ""--num_threads"",\n                str(max(args.ngpu, 1)),\n                # Make sure scheduler setting, i.e. conf/queue.conf\n                # so that --num_nodes requires 1process-per-node\n                ""--num_nodes"",\n                str(args.num_nodes),\n                args.log,\n                ""mpirun"",\n                # -np option can be omitted with Torque/PBS\n                ""-np"",\n                str(args.num_nodes),\n            ]\n            # arguments for *_train.py\n            + args.args\n            + [\n                ""--ngpu"",\n                str(args.ngpu),\n                ""--multiprocessing_distributed"",\n                ""true"",\n                ""--dist_launcher"",\n                ""mpi"",\n            ]\n            + init_method\n        )\n        if args.ngpu == 0:\n            # Gloo supports both GPU and CPU mode.\n            #   See: https://pytorch.org/docs/stable/distributed.html\n            cmd += [""--dist_backend"", ""gloo""]\n        process = subprocess.Popen(cmd)\n        processes.append(process)\n\n    logging.info(f""log file: {args.log}"")\n\n    failed = False\n    while any(p.returncode is None for p in processes):\n        for process in processes:\n            # If any process is failed, try to kill the other processes too\n            if failed and process.returncode is not None:\n                process.kill()\n            else:\n                try:\n                    process.wait(0.5)\n                except subprocess.TimeoutExpired:\n                    pass\n\n                if process.returncode is not None and process.returncode != 0:\n                    failed = True\n\n    for process in processes:\n        if process.returncode != 0:\n            print(\n                subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd),\n                file=sys.stderr,\n            )\n            p = Path(args.log)\n            if p.exists():\n                with p.open() as f:\n                    lines = list(f)\n                raise RuntimeError(\n                    f""\\n################### The last 1000 lines of {args.log} ""\n                    f""###################\\n"" + """".join(lines[-1000:])\n                )\n            else:\n                raise RuntimeError\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/lm_calc_perplexity.py,2,"b'#!/usr/bin/env python3\nimport logging\nfrom pathlib import Path\nimport sys\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nimport configargparse\nimport numpy as np\nimport torch\nfrom torch.nn.parallel import data_parallel\nfrom typeguard import check_argument_types\n\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet2.tasks.lm import LMTask\nfrom espnet2.torch_utils.device_funcs import to_device\nfrom espnet2.torch_utils.forward_adaptor import ForwardAdaptor\nfrom espnet2.torch_utils.set_all_random_seed import set_all_random_seed\nfrom espnet2.utils.fileio import DatadirWriter\nfrom espnet2.utils.types import float_or_none\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str2triple_str\nfrom espnet2.utils.types import str_or_none\n\n\ndef calc_perplexity(\n    output_dir: str,\n    batch_size: int,\n    dtype: str,\n    ngpu: int,\n    seed: int,\n    num_workers: int,\n    log_level: Union[int, str],\n    data_path_and_name_and_type: Sequence[Tuple[str, str, str]],\n    key_file: Optional[str],\n    train_config: Optional[str],\n    model_file: Optional[str],\n    log_base: Optional[float],\n    allow_variable_data_keys: bool,\n):\n    assert check_argument_types()\n    logging.basicConfig(\n        level=log_level,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n\n    if ngpu >= 1:\n        device = ""cuda""\n    else:\n        device = ""cpu""\n\n    # 1. Set random-seed\n    set_all_random_seed(seed)\n\n    # 2. Build LM\n    model, train_args = LMTask.build_model_from_file(train_config, model_file, device)\n    # Wrape model to make model.nll() data-parallel\n    wrapped_model = ForwardAdaptor(model, ""nll"")\n    wrapped_model.to(dtype=getattr(torch, dtype)).eval()\n    logging.info(f""Model:\\n{model}"")\n\n    # 3. Build data-iterator\n    loader, _, _ = LMTask.build_non_sorted_iterator(\n        data_path_and_name_and_type,\n        dtype=dtype,\n        batch_size=batch_size,\n        key_file=key_file,\n        num_workers=num_workers,\n        preprocess_fn=LMTask.build_preprocess_fn(train_args, False),\n        collate_fn=LMTask.build_collate_fn(train_args),\n        allow_variable_data_keys=allow_variable_data_keys,\n    )\n\n    # 4. Start for-loop\n    with DatadirWriter(output_dir) as writer:\n        total_nll = 0.0\n        total_ntokens = 0\n        for keys, batch in loader:\n            assert isinstance(batch, dict), type(batch)\n            assert all(isinstance(s, str) for s in keys), keys\n            _bs = len(next(iter(batch.values())))\n            assert len(keys) == _bs, f""{len(keys)} != {_bs}""\n\n            with torch.no_grad():\n                batch = to_device(batch, device)\n                if ngpu <= 1:\n                    # NOTE(kamo): data_parallel also should work with ngpu=1,\n                    # but for debuggability it\'s better to keep this block.\n                    nll, lengths = wrapped_model(**batch)\n                else:\n                    nll, lengths = data_parallel(\n                        wrapped_model, (), range(ngpu), module_kwargs=batch\n                    )\n\n            assert _bs == len(nll) == len(lengths), (_bs, len(nll), len(lengths))\n            # nll: (B, L) -> (B,)\n            nll = nll.detach().cpu().numpy().sum(1)\n            # lengths: (B,)\n            lengths = lengths.detach().cpu().numpy()\n            total_nll += nll.sum()\n            total_ntokens += lengths.sum()\n\n            for key, _nll, ntoken in zip(keys, nll, lengths):\n                if log_base is None:\n                    utt_ppl = np.exp(_nll / ntoken)\n                else:\n                    utt_ppl = log_base ** (_nll / ntoken / np.log(log_base))\n\n                # Write PPL of each utts for debugging or analysis\n                writer[""utt2ppl""][key] = str(utt_ppl)\n                writer[""utt2ntokens""][key] = str(ntoken)\n\n        if log_base is None:\n            ppl = np.exp(total_nll / total_ntokens)\n        else:\n            ppl = log_base ** (total_nll / total_ntokens / np.log(log_base))\n\n        with (Path(output_dir) / ""ppl"").open(""w"", encoding=""utf-8"") as f:\n            f.write(f""{ppl}\\n"")\n        with (Path(output_dir) / ""base"").open(""w"", encoding=""utf-8"") as f:\n            if log_base is None:\n                _log_base = np.e\n            else:\n                _log_base = log_base\n            f.write(f""{_log_base}\\n"")\n        logging.info(f""PPL={ppl}"")\n\n\ndef get_parser():\n    parser = configargparse.ArgumentParser(\n        description=""Calc perplexity"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    # Note(kamo): Use \'_\' instead of \'-\' as separator.\n    # \'-\' is confusing if written in yaml.\n    parser.add_argument(""--config"", is_config_file=True, help=""config file path"")\n\n    parser.add_argument(\n        ""--log_level"",\n        type=lambda x: x.upper(),\n        default=""INFO"",\n        choices=(""INFO"", ""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""),\n        help=""The verbose level of logging"",\n    )\n\n    parser.add_argument(""--output_dir"", type=str, required=True)\n    parser.add_argument(\n        ""--ngpu"", type=int, default=0, help=""The number of gpus. 0 indicates CPU mode"",\n    )\n    parser.add_argument(""--seed"", type=int, default=0, help=""Random seed"")\n    parser.add_argument(\n        ""--dtype"",\n        default=""float32"",\n        choices=[""float16"", ""float32"", ""float64""],\n        help=""Data type"",\n    )\n    parser.add_argument(\n        ""--num_workers"",\n        type=int,\n        default=1,\n        help=""The number of workers used for DataLoader"",\n    )\n    parser.add_argument(\n        ""--batch_size"", type=int, default=1, help=""The batch size for inference"",\n    )\n    parser.add_argument(\n        ""--log_base"",\n        type=float_or_none,\n        default=None,\n        help=""The base of logarithm for Perplexity. ""\n        ""If None, napier\'s constant is used."",\n    )\n\n    group = parser.add_argument_group(""Input data related"")\n    group.add_argument(\n        ""--data_path_and_name_and_type"",\n        type=str2triple_str,\n        required=True,\n        action=""append"",\n    )\n    group.add_argument(""--key_file"", type=str_or_none)\n    group.add_argument(""--allow_variable_data_keys"", type=str2bool, default=False)\n\n    group = parser.add_argument_group(""The model configuration related"")\n    group.add_argument(""--train_config"", type=str)\n    group.add_argument(""--model_file"", type=str)\n\n    return parser\n\n\ndef main(cmd=None):\n    print(get_commandline_args(), file=sys.stderr)\n    parser = get_parser()\n    args = parser.parse_args(cmd)\n    kwargs = vars(args)\n    kwargs.pop(""config"", None)\n    calc_perplexity(**kwargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/lm_train.py,0,"b'#!/usr/bin/env python3\nfrom espnet2.tasks.lm import LMTask\n\n\ndef get_parser():\n    parser = LMTask.get_parser()\n    return parser\n\n\ndef main(cmd=None):\n    """"""LM training.\n\n    Example:\n\n        % python lm_train.py asr --print_config --optim adadelta\n        % python lm_train.py --config conf/train_asr.yaml\n    """"""\n    LMTask.main(cmd=cmd)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/pack.py,0,"b'import argparse\nfrom typing import Type\n\nfrom espnet2.utils.pack_funcs import pack\n\n\nclass PackedContents:\n    files = []\n    yaml_files = []\n\n\nclass ASRPackedContents(PackedContents):\n    files = [""asr_model_file.pth"", ""lm_file.pth""]\n    yaml_files = [""asr_train_config.yaml"", ""lm_train_config.yaml""]\n\n\nclass TTSPackedContents(PackedContents):\n    files = [""model_file.pth""]\n    yaml_files = [""train_config.yaml""]\n\n\ndef add_arguments(parser: argparse.ArgumentParser, contents: Type[PackedContents]):\n    parser.add_argument(""--outpath"", type=str, required=True)\n    for key in contents.yaml_files:\n        parser.add_argument(f""--{key}"", type=str, default=None)\n    for key in contents.files:\n        parser.add_argument(f""--{key}"", type=str, default=None)\n    parser.add_argument(""--option"", type=str, action=""append"", default=[])\n    parser.add_argument(\n        ""--mode"",\n        type=str,\n        default=""w:gz"",\n        choices=[""w"", ""w:gz"", ""w:bz2"", ""w:xz""],\n        help=""Compression mode"",\n    )\n\n\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(\n        description=""Pack input files to archive format. If the external file path ""\n        ""are written in the input yaml files, then the paths are ""\n        ""rewritten to the archived name"",\n    )\n    subparsers = parser.add_subparsers()\n\n    # Create subparser for ASR\n    for name, contents in [(""asr"", ASRPackedContents), (""tts"", TTSPackedContents)]:\n        parser_asr = subparsers.add_parser(\n            name, formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        )\n        add_arguments(parser_asr, contents)\n        parser_asr.set_defaults(contents=contents)\n    return parser\n\n\ndef main(cmd=None):\n    parser = get_parser()\n    args = parser.parse_args(cmd)\n    if not hasattr(args, ""contents""):\n        parser.print_help()\n        parser.exit(2)\n\n    yaml_files = {\n        y: getattr(args, y)\n        for y in args.contents.yaml_files\n        if getattr(args, y) is not None\n    }\n    files = {\n        y: getattr(args, y) for y in args.contents.files if getattr(args, y) is not None\n    }\n    pack(\n        yaml_files=yaml_files,\n        files=files,\n        option=args.option,\n        outpath=args.outpath,\n        mode=args.mode,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/tokenize_text.py,0,"b'import argparse\nfrom collections import Counter\nimport logging\nfrom pathlib import Path\nimport sys\nfrom typing import List\nfrom typing import Optional\n\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet2.text.build_tokenizer import build_tokenizer\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str_or_none\n\n\ndef field2slice(field: Optional[str]) -> slice:\n    """"""Convert field string to slice\n\n    Note that field string accepts 1-based integer.\n\n    Examples:\n        >>> field2slice(""1-"")\n        slice(0, None, None)\n        >>> field2slice(""1-3"")\n        slice(0, 3, None)\n        >>> field2slice(""-3"")\n        slice(None, 3, None)\n\n    """"""\n    field = field.strip()\n    try:\n        if ""-"" in field:\n            # e.g. ""2-"" or ""2-5"" or ""-7""\n            s1, s2 = field.split(""-"", maxsplit=1)\n            if s1.strip() == """":\n                s1 = None\n            else:\n                s1 = int(s1)\n                if s1 == 0:\n                    raise ValueError(""1-based string"")\n            if s2.strip() == """":\n                s2 = None\n            else:\n                s2 = int(s2)\n        else:\n            # e.g. ""2""\n            s1 = int(field)\n            s2 = s1 + 1\n            if s1 == 0:\n                raise ValueError(""must be 1 or more value"")\n    except ValueError:\n        raise RuntimeError(f""Format error: e.g. \'2-\', \'2-5\', or \'-5\': {field}"")\n\n    # -1 because of 1-based integer following ""cut"" command\n    # e.g ""1-3"" -> slice(0, 3)\n    slic = slice(s1 - 1, s2)\n    return slic\n\n\ndef tokenize(\n    input: str,\n    output: str,\n    field: Optional[str],\n    delimiter: Optional[str],\n    token_type: str,\n    space_symbol: str,\n    non_linguistic_symbols: Optional[str],\n    bpemodel: Optional[str],\n    log_level: str,\n    write_vocabulary: bool,\n    vocabulary_size: int,\n    remove_non_linguistic_symbols: bool,\n    cutoff: int,\n    add_symbol: List[str],\n):\n    logging.basicConfig(\n        level=log_level,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n    if input == ""-"":\n        fin = sys.stdin\n    else:\n        fin = Path(input).open(""r"", encoding=""utf-8"")\n    if output == ""-"":\n        fout = sys.stdout\n    else:\n        p = Path(output)\n        p.parent.mkdir(parents=True, exist_ok=True)\n        fout = p.open(""w"", encoding=""utf-8"")\n\n    tokenizer = build_tokenizer(\n        token_type=token_type,\n        bpemodel=bpemodel,\n        delimiter=delimiter,\n        space_symbol=space_symbol,\n        non_linguistic_symbols=non_linguistic_symbols,\n        remove_non_linguistic_symbols=remove_non_linguistic_symbols,\n    )\n\n    counter = Counter()\n    if field is not None:\n        field = field2slice(field)\n\n    for line in fin:\n        line = line.rstrip()\n        if field is not None:\n            # e.g. field=""2-""\n            # uttidA hello world!! -> hello world!!\n            tokens = line.split(delimiter)\n            tokens = tokens[field]\n            if delimiter is None:\n                line = "" "".join(tokens)\n            else:\n                line = delimiter.join(tokens)\n\n        tokens = tokenizer.text2tokens(line)\n        if not write_vocabulary:\n            fout.write("" "".join(tokens) + ""\\n"")\n        else:\n            for t in tokens:\n                counter[t] += 1\n\n    if not write_vocabulary:\n        return\n\n    # ======= write_vocabulary mode from here =======\n    # Sort by the number of occurrences\n    words_and_counts = list(sorted(counter.items(), key=lambda x: x[1]))\n\n    for symbol_and_id in add_symbol:\n        # e.g symbol=""<blank>:0""\n        try:\n            symbol, idx = symbol_and_id.split("":"")\n            idx = int(idx)\n        except ValueError:\n            raise RuntimeError(f""Format error: e.g. \'<blank>:0\': {symbol_and_id}"")\n        symbol = symbol.strip()\n\n        # e.g. idx=0  -> append as the first symbol\n        # e.g. idx=-1 -> append as the last symbol\n        if idx < 0:\n            idx = len(words_and_counts) + 1 + idx\n        words_and_counts.insert(idx, (symbol, None))\n\n    total_count = sum(counter.values())\n    invocab_count = 0\n    for nvocab, (w, c) in enumerate(words_and_counts, 1):\n        fout.write(w + ""\\n"")\n        if c is not None:\n            invocab_count += c\n            if c <= cutoff:\n                break\n\n        # Note that nvocab includes appended symbol, e.g. even <blank> or <sos/eos>\n        if nvocab >= vocabulary_size > 0:\n            break\n\n    logging.info(f""OOV rate = {(total_count - invocab_count) / total_count * 100} %"")\n\n\ndef get_parser() -> argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(\n        description=""Tokenize texts"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--log_level"",\n        type=lambda x: x.upper(),\n        default=""INFO"",\n        choices=(""INFO"", ""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""),\n        help=""The verbose level of logging"",\n    )\n\n    parser.add_argument(\n        ""--input"", ""-i"", required=True, help=""Input text. - indicates sys.stdin""\n    )\n    parser.add_argument(\n        ""--output"", ""-o"", required=True, help=""Output text. - indicates sys.stdout""\n    )\n    parser.add_argument(\n        ""--field"",\n        ""-f"",\n        help=""The target columns of the input text as 1-based integer. e.g 2-"",\n    )\n    parser.add_argument(\n        ""--token_type"",\n        ""-t"",\n        default=""char"",\n        choices=[""char"", ""bpe"", ""word""],\n        help=""Token type"",\n    )\n    parser.add_argument(""--delimiter"", ""-d"", default=None, help=""The delimiter"")\n    parser.add_argument(""--space_symbol"", default=""<space>"", help=""The space symbol"")\n    parser.add_argument(""--bpemodel"", default=None, help=""The bpemodel file path"")\n    parser.add_argument(\n        ""--non_linguistic_symbols"",\n        type=str_or_none,\n        help=""non_linguistic_symbols file path"",\n    )\n    parser.add_argument(\n        ""--remove_non_linguistic_symbols"",\n        type=str2bool,\n        default=False,\n        help=""Remove non-language-symbols from tokens"",\n    )\n\n    group = parser.add_argument_group(""write_vocabulary mode related"")\n    group.add_argument(\n        ""--write_vocabulary"",\n        type=str2bool,\n        default=False,\n        help=""Write tokens list instead of tokenized text per line"",\n    )\n    group.add_argument(""--vocabulary_size"", type=int, default=0, help=""Vocabulary size"")\n    group.add_argument(\n        ""--cutoff"",\n        default=0,\n        type=int,\n        help=""cut-off frequency used for write-vocabulary mode"",\n    )\n    group.add_argument(\n        ""--add_symbol"",\n        type=str,\n        default=[],\n        action=""append"",\n        help=""Append symbol e.g. --add_symbol \'<blank>:0\' --add_symbol \'<unk>:1\'"",\n    )\n\n    return parser\n\n\ndef main(cmd=None):\n    print(get_commandline_args(), file=sys.stderr)\n    parser = get_parser()\n    args = parser.parse_args(cmd)\n    kwargs = vars(args)\n    tokenize(**kwargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/tts_inference.py,1,"b'#!/usr/bin/env python3\n\n""""""TTS mode decoding.""""""\n\nimport logging\nfrom pathlib import Path\nimport sys\nimport time\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nimport configargparse\nimport kaldiio\nimport soundfile as sf\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet2.tasks.tts import TTSTask\nfrom espnet2.torch_utils.device_funcs import to_device\nfrom espnet2.torch_utils.set_all_random_seed import set_all_random_seed\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\nfrom espnet2.utils.griffin_lim import Spectrogram2Waveform\nfrom espnet2.utils.nested_dict_action import NestedDictAction\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str2triple_str\nfrom espnet2.utils.types import str_or_none\n\n\n@torch.no_grad()\ndef inference(\n    output_dir: str,\n    batch_size: int,\n    dtype: str,\n    ngpu: int,\n    seed: int,\n    num_workers: int,\n    log_level: Union[int, str],\n    data_path_and_name_and_type: Sequence[Tuple[str, str, str]],\n    key_file: Optional[str],\n    train_config: Optional[str],\n    model_file: Optional[str],\n    threshold: float,\n    minlenratio: float,\n    maxlenratio: float,\n    use_att_constraint: bool,\n    backward_window: int,\n    forward_window: int,\n    allow_variable_data_keys: bool,\n    vocoder_conf: dict,\n):\n    """"""Perform TTS model decoding.""""""\n    assert check_argument_types()\n    if batch_size > 1:\n        raise NotImplementedError(""batch decoding is not implemented"")\n    if ngpu > 1:\n        raise NotImplementedError(""only single GPU decoding is supported"")\n    logging.basicConfig(\n        level=log_level,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n\n    if ngpu >= 1:\n        device = ""cuda""\n    else:\n        device = ""cpu""\n\n    # 1. Set random-seed\n    set_all_random_seed(seed)\n\n    # 2. Build model\n    model, train_args = TTSTask.build_model_from_file(train_config, model_file, device)\n    model.to(dtype=getattr(torch, dtype)).eval()\n    tts = model.tts\n    normalize = model.normalize\n    logging.info(f""Normalization:\\n{normalize}"")\n    logging.info(f""TTS:\\n{tts}"")\n\n    # 3. Build data-iterator\n    loader, _, batch_sampler = TTSTask.build_non_sorted_iterator(\n        data_path_and_name_and_type,\n        dtype=dtype,\n        batch_size=batch_size,\n        key_file=key_file,\n        num_workers=num_workers,\n        preprocess_fn=TTSTask.build_preprocess_fn(train_args, False),\n        collate_fn=TTSTask.build_collate_fn(train_args),\n        allow_variable_data_keys=allow_variable_data_keys,\n    )\n\n    # 4. Build converter from spectrogram to waveform\n    if model.feats_extract is not None:\n        vocoder_conf.update(model.feats_extract.get_parameters())\n    if ""n_fft"" in vocoder_conf and ""n_shift"" in vocoder_conf and ""fs"" in vocoder_conf:\n        spc2wav = Spectrogram2Waveform(**vocoder_conf)\n        logging.info(f""Vocoder: {spc2wav}"")\n    else:\n        spc2wav = None\n        logging.info(""Vocoder is not used because vocoder_conf is not sufficient"")\n\n    # 5. Start for-loop\n    output_dir = Path(output_dir)\n    (output_dir / ""norm"").mkdir(parents=True, exist_ok=True)\n    (output_dir / ""denorm"").mkdir(parents=True, exist_ok=True)\n    (output_dir / ""wav"").mkdir(parents=True, exist_ok=True)\n\n    # FIXME(kamo): I think we shouldn\'t depend on kaldi-format any more.\n    #  How about numpy or HDF5?\n    #  >>> with NpyScpWriter() as f:\n    with kaldiio.WriteHelper(\n        ""ark,scp:{o}.ark,{o}.scp"".format(o=output_dir / ""norm/feats"")\n    ) as f, kaldiio.WriteHelper(\n        ""ark,scp:{o}.ark,{o}.scp"".format(o=output_dir / ""denorm/feats"")\n    ) as g:\n        for idx, (keys, batch) in enumerate(loader, 1):\n            assert isinstance(batch, dict), type(batch)\n            assert all(isinstance(s, str) for s in keys), keys\n            _bs = len(next(iter(batch.values())))\n            assert len(keys) == _bs, f""{len(keys)} != {_bs}""\n            batch = to_device(batch, device)\n\n            key = keys[0]\n            # Change to single sequence and remove *_length\n            # because inference() requires 1-seq, not mini-batch.\n            _data = {k: v[0] for k, v in batch.items() if not k.endswith(""_lengths"")}\n            start_time = time.perf_counter()\n\n            # TODO(kamo): Now att_ws is not used.\n            outs, probs, att_ws = tts.inference(\n                **_data,\n                threshold=threshold,\n                maxlenratio=maxlenratio,\n                minlenratio=minlenratio,\n            )\n            outs_denorm = normalize.inverse(outs[None])[0][0]\n            insize = next(iter(_data.values())).size(0)\n            logging.info(\n                ""inference speed = {} msec / frame."".format(\n                    (time.perf_counter() - start_time) / (int(outs.size(0)) * 1000)\n                )\n            )\n            if outs.size(0) == insize * maxlenratio:\n                logging.warning(f""output length reaches maximum length ({key})."")\n            logging.info(\n                f""({idx}/{len(batch_sampler)}) {key} ""\n                f""(size:{insize}->{outs.size(0)})""\n            )\n            f[key] = outs.cpu().numpy()\n            g[key] = outs_denorm.cpu().numpy()\n\n            # TODO(kamo): Write scp\n            if spc2wav is not None:\n                wav = spc2wav(outs_denorm.cpu().numpy())\n                sf.write(f""{output_dir}/wav/{key}.wav"", wav, spc2wav.fs, ""PCM_16"")\n\n\ndef get_parser():\n    """"""Get argument parser.""""""\n    parser = configargparse.ArgumentParser(\n        description=""TTS Decode"",\n        config_file_parser_class=configargparse.YAMLConfigFileParser,\n        formatter_class=configargparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    # Note(kamo): Use ""_"" instead of ""-"" as separator.\n    # ""-"" is confusing if written in yaml.\n    parser.add_argument(""--config"", is_config_file=True, help=""config file path"")\n\n    parser.add_argument(\n        ""--log_level"",\n        type=lambda x: x.upper(),\n        default=""INFO"",\n        choices=(""INFO"", ""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""),\n        help=""The verbose level of logging"",\n    )\n\n    parser.add_argument(\n        ""--output_dir"", type=str, required=True, help=""The path of output directory"",\n    )\n    parser.add_argument(\n        ""--ngpu"", type=int, default=0, help=""The number of gpus. 0 indicates CPU mode"",\n    )\n    parser.add_argument(""--seed"", type=int, default=0, help=""Random seed"")\n    parser.add_argument(\n        ""--dtype"",\n        default=""float32"",\n        choices=[""float16"", ""float32"", ""float64""],\n        help=""Data type"",\n    )\n    parser.add_argument(\n        ""--num_workers"",\n        type=int,\n        default=1,\n        help=""The number of workers used for DataLoader"",\n    )\n    parser.add_argument(\n        ""--batch_size"", type=int, default=1, help=""The batch size for inference"",\n    )\n\n    group = parser.add_argument_group(""Input data related"")\n    group.add_argument(\n        ""--data_path_and_name_and_type"",\n        type=str2triple_str,\n        required=True,\n        action=""append"",\n    )\n    group.add_argument(""--key_file"", type=str_or_none)\n    group.add_argument(""--allow_variable_data_keys"", type=str2bool, default=False)\n\n    group = parser.add_argument_group(""The model configuration related"")\n    group.add_argument(""--train_config"", type=str)\n    group.add_argument(""--model_file"", type=str)\n\n    group = parser.add_argument_group(""Decoding related"")\n    group.add_argument(\n        ""--maxlenratio"",\n        type=float,\n        default=10.0,\n        help=""Maximum length ratio in decoding"",\n    )\n    group.add_argument(\n        ""--minlenratio"",\n        type=float,\n        default=0.0,\n        help=""Minimum length ratio in decoding"",\n    )\n    group.add_argument(\n        ""--threshold"", type=float, default=0.5, help=""Threshold value in decoding"",\n    )\n    group.add_argument(\n        ""--use_att_constraint"",\n        type=str2bool,\n        default=False,\n        help=""Whether to use attention constraint"",\n    )\n    group.add_argument(\n        ""--backward_window"",\n        type=int,\n        default=1,\n        help=""Backward window value in attention constraint"",\n    )\n    group.add_argument(\n        ""--forward_window"",\n        type=int,\n        default=3,\n        help=""Forward window value in attention constraint"",\n    )\n\n    group = parser.add_argument_group("" Grriffin-Lim related"")\n    group.add_argument(\n        ""--vocoder_conf"",\n        action=NestedDictAction,\n        default=get_default_kwargs(Spectrogram2Waveform),\n        help=""The configuration for Grriffin-Lim"",\n    )\n    return parser\n\n\ndef main(cmd=None):\n    """"""Run TTS model decoding.""""""\n    print(get_commandline_args(), file=sys.stderr)\n    parser = get_parser()\n    args = parser.parse_args(cmd)\n    kwargs = vars(args)\n    kwargs.pop(""config"", None)\n    inference(**kwargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/bin/tts_train.py,0,"b'#!/usr/bin/env python3\nfrom espnet2.tasks.tts import TTSTask\n\n\ndef get_parser():\n    parser = TTSTask.get_parser()\n    return parser\n\n\ndef main(cmd=None):\n    """"""TTS training\n\n    Example:\n\n        % python tts_train.py asr --print_config --optim adadelta\n        % python tts_train.py --config conf/train_asr.yaml\n    """"""\n    TTSTask.main(cmd=cmd)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
espnet2/iterators/__init__.py,0,b''
espnet2/iterators/abs_iter_factory.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\n\n\nclass AbsIterFactory(ABC):\n    @abstractmethod\n    def build_iter(self, epoch: int, shuffle: bool = None):\n        raise NotImplementedError\n'"
espnet2/iterators/chunk_iter_factory.py,3,"b'import logging\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet2.iterators.abs_iter_factory import AbsIterFactory\nfrom espnet2.iterators.sequence_iter_factory import SequenceIterFactory\n\n\nclass ChunkIterFactory(AbsIterFactory):\n    """"""Creates chunks from a sequence\n\n    Examples:\n        >>> batches = [[""id1""], [""id2""], ...]\n        >>> batch_size = 128\n        >>> chunk_length = 1000\n        >>> iter_factory = ChunkIterFactory(dataset, batches, batch_size, chunk_length)\n        >>> it = iter_factory.build_iter(epoch)\n        >>> for ids, batch in it:\n        ...     ...\n\n    - The number of mini-batches are varied in each epochs and\n      we can\'t get the number in advance\n      because IterFactory doesn\'t be given to the length information.\n    - Since the first reason, ""num_iters_per_epoch"" can\'t be implemented\n      for this iterator. Instead of it, ""num_samples_per_epoch"" is implemented.\n\n    """"""\n\n    def __init__(\n        self,\n        dataset,\n        batches: Sequence[Sequence[Any]],\n        batch_size: int,\n        chunk_length: Union[int, str],\n        chunk_shift_ratio: float = 0.5,\n        num_cache_chunks: int = 1024,\n        num_samples_per_epoch: int = None,\n        seed: int = 0,\n        shuffle: bool = False,\n        num_workers: int = 0,\n        collate_fn=None,\n        pin_memory: bool = False,\n    ):\n        assert check_argument_types()\n        assert all(len(x) == 1 for x in batches), ""batch-size must be 1""\n\n        self.per_sample_iter_factory = SequenceIterFactory(\n            dataset=dataset,\n            batches=batches,\n            num_iters_per_epoch=num_samples_per_epoch,\n            seed=seed,\n            shuffle=shuffle,\n            num_workers=num_workers,\n            collate_fn=collate_fn,\n            pin_memory=pin_memory,\n        )\n\n        self.num_cache_chunks = max(num_cache_chunks, batch_size)\n        if isinstance(chunk_length, str):\n            if len(chunk_length) == 0:\n                raise ValueError(""e.g. 5,8 or 3-5: but got empty string"")\n\n            self.chunk_lengths = []\n            for x in chunk_length.split("",""):\n                try:\n                    sps = list(map(int, x.split(""-"")))\n                except ValueError:\n                    raise ValueError(f""e.g. 5,8 or 3-5: but got {chunk_length}"")\n\n                if len(sps) > 2:\n                    raise ValueError(f""e.g. 5,8 or 3-5: but got {chunk_length}"")\n                elif len(sps) == 2:\n                    # Append all numbers between the range into the candidates\n                    self.chunk_lengths += list(range(sps[0], sps[1] + 1))\n                else:\n                    self.chunk_lengths += [sps[0]]\n        else:\n            # Single candidates: Fixed chunk length\n            self.chunk_lengths = [chunk_length]\n\n        self.chunk_shift_ratio = chunk_shift_ratio\n        self.batch_size = batch_size\n        self.seed = seed\n        self.shuffle = shuffle\n\n    def build_iter(\n        self, epoch: int, shuffle: bool = None,\n    ) -> Iterator[Tuple[List[str], Dict[str, torch.Tensor]]]:\n        per_sample_loader = self.per_sample_iter_factory.build_iter(epoch, shuffle)\n\n        if shuffle is None:\n            shuffle = self.shuffle\n        state = np.random.RandomState(epoch + self.seed)\n\n        # NOTE(kamo):\n        #   This iterator supports multiple chunk lengths and\n        #   keep chunks for each lenghts here until collecting specified numbers\n        cache_chunks_dict = {}\n        cache_id_list_dict = {}\n        for ids, batch in per_sample_loader:\n            # Must be per-sample-loader\n            assert len(ids) == 1, f""Must be per-sample-loader: {len(ids)}""\n            assert all(len(x) == 1 for x in batch.values())\n            # Get keys of sequence data\n            sequence_keys = []\n            for key in batch:\n                if key + ""_lengths"" in batch:\n                    sequence_keys.append(key)\n            # Remove lengths data and get the first sample\n            batch = {k: v[0] for k, v in batch.items() if not k.endswith(""_lengths"")}\n            id_ = ids[0]\n\n            for key in sequence_keys:\n                if len(batch[key]) != len(batch[sequence_keys[0]]):\n                    raise RuntimeError(\n                        f""All sequences must has same length: ""\n                        f""{len(batch[key])} != {len(batch[sequence_keys[0]])}""\n                    )\n\n            L = len(batch[sequence_keys[0]])\n            # Select chunk length\n            chunk_lengths = [lg for lg in self.chunk_lengths if lg < L]\n            if len(chunk_lengths) == 0:\n                logging.warning(\n                    f""The length of \'{id_}\' is {L}, but it is shorter than ""\n                    f""any candidates of chunk-length: {self.chunk_lengths}""\n                )\n                continue\n\n            W = int(state.choice(chunk_lengths, 1))\n            cache_id_list = cache_id_list_dict.setdefault(W, [])\n            cache_chunks = cache_chunks_dict.setdefault(W, {})\n\n            # Shift width to the next chunk\n            S = int(L * self.chunk_shift_ratio)\n            # Number of chunks\n            N = (L - W) // S + 1\n            if shuffle:\n                Z = state.randint(0, (L - W) % S + 1)\n            else:\n                Z = 0\n\n            # Split a sequence into chunks.\n            # Note that the marginal frames divided by chunk length are discarded\n            for k, v in batch.items():\n                if k not in cache_chunks:\n                    cache_chunks[k] = []\n                if k in sequence_keys:\n                    # Shift chunks with overlapped length for data augmentation\n                    cache_chunks[k] += [v[Z + i * S : Z + i * S + W] for i in range(N)]\n                else:\n                    # If not sequence, use whole data instead of chunk\n                    cache_chunks[k] += [v for _ in range(N)]\n            cache_id_list += [id_ for _ in range(N)]\n\n            if len(cache_id_list) > self.num_cache_chunks:\n                cache_id_list, cache_chunks = yield from self._generate_mini_batches(\n                    cache_id_list, cache_chunks, shuffle, state,\n                )\n\n            cache_id_list_dict[W] = cache_id_list\n            cache_chunks_dict[W] = cache_chunks\n\n        else:\n            for W in cache_id_list_dict:\n                cache_id_list = cache_id_list_dict.setdefault(W, [])\n                cache_chunks = cache_chunks_dict.setdefault(W, {})\n\n                yield from self._generate_mini_batches(\n                    cache_id_list, cache_chunks, shuffle, state,\n                )\n\n    def _generate_mini_batches(\n        self,\n        id_list: List[str],\n        batches: Dict[str, List[torch.Tensor]],\n        shuffle: bool,\n        state: np.random.RandomState,\n    ):\n        if shuffle:\n            indices = np.arange(0, len(id_list))\n            state.shuffle(indices)\n            batches = {k: [v[i] for i in indices] for k, v in batches.items()}\n            id_list = [id_list[i] for i in indices]\n\n        bs = self.batch_size\n        while len(id_list) >= bs:\n            # Make mini-batch and yield\n            yield (\n                id_list[:bs],\n                {k: torch.stack(v[:bs], 0) for k, v in batches.items()},\n            )\n            id_list = id_list[bs:]\n            batches = {k: v[bs:] for k, v in batches.items()}\n        return id_list, batches\n'"
espnet2/iterators/sequence_iter_factory.py,2,"b'from typing import Any\nfrom typing import Sequence\n\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom typeguard import check_argument_types\n\nfrom espnet2.iterators.abs_iter_factory import AbsIterFactory\n\n\nclass SequenceIterFactory(AbsIterFactory):\n    """"""Build iterator for each epoch.\n\n    This class simply creates pytorch DataLoader except for the following points:\n    - The random seed is decided according to the number of epochs. This feature\n      guarantees reproducibility when resuming from middle of training process.\n    - Enable to restrict the number of samples for one epoch. This features\n      controls the interval number between training and evaluation.\n\n    """"""\n\n    def __init__(\n        self,\n        dataset,\n        batches: Sequence[Sequence[Any]],\n        num_iters_per_epoch: int = None,\n        seed: int = 0,\n        shuffle: bool = False,\n        num_workers: int = 0,\n        collate_fn=None,\n        pin_memory: bool = False,\n    ):\n        assert check_argument_types()\n\n        self.batches = list(batches)\n        self.dataset = dataset\n        if num_iters_per_epoch is not None and num_iters_per_epoch < len(batches):\n            self.num_iters_per_epoch = num_iters_per_epoch\n        else:\n            self.num_iters_per_epoch = None\n        self.shuffle = shuffle\n        self.seed = seed\n        self.num_workers = num_workers\n        self.collate_fn = collate_fn\n        # https://discuss.pytorch.org/t/what-is-the-disadvantage-of-using-pin-memory/1702\n        self.pin_memory = pin_memory\n\n    def build_iter(self, epoch: int, shuffle: bool = None) -> DataLoader:\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        if self.num_iters_per_epoch is not None:\n            N = len(self.batches)\n            real_epoch, offset = divmod(self.num_iters_per_epoch * epoch, N)\n\n            if offset >= self.num_iters_per_epoch:\n                current_batches = list(self.batches)\n                if shuffle:\n                    np.random.RandomState(real_epoch + self.seed).shuffle(\n                        current_batches\n                    )\n                batches = current_batches[offset - self.num_iters_per_epoch : offset]\n            else:\n                prev_batches = list(self.batches)\n                current_batches = list(self.batches)\n                if shuffle:\n                    np.random.RandomState(real_epoch - 1 + self.seed).shuffle(\n                        prev_batches\n                    )\n                    np.random.RandomState(real_epoch + self.seed).shuffle(\n                        current_batches\n                    )\n                batches = (\n                    prev_batches[offset - self.num_iters_per_epoch :]\n                    + current_batches[:offset]\n                )\n        else:\n            batches = list(self.batches)\n            if shuffle:\n                np.random.RandomState(epoch + self.seed).shuffle(batches)\n\n        # For backward compatibility for pytorch DataLoader\n        if self.collate_fn is not None:\n            kwargs = dict(collate_fn=self.collate_fn)\n        else:\n            kwargs = {}\n\n        return DataLoader(\n            dataset=self.dataset,\n            batch_sampler=batches,\n            num_workers=self.num_workers,\n            pin_memory=self.pin_memory,\n            **kwargs,\n        )\n'"
espnet2/layers/__init__.py,0,b''
espnet2/layers/abs_normalize.py,3,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Tuple\n\nimport torch\n\n\nclass AbsNormalize(torch.nn.Module, ABC):\n    @abstractmethod\n    def forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # return output, output_lengths\n        raise NotImplementedError\n'"
espnet2/layers/global_mvn.py,6,"b'from pathlib import Path\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet2.layers.abs_normalize import AbsNormalize\nfrom espnet2.layers.inversible_interface import InversibleInterface\n\n\nclass GlobalMVN(AbsNormalize, InversibleInterface):\n    """"""Apply global mean and variance normalization\n\n    TODO(kamo): Make this class portable somehow\n\n    Args:\n        stats_file: npy file\n        norm_means: Apply mean normalization\n        norm_vars: Apply var normalization\n        eps:\n    """"""\n\n    def __init__(\n        self,\n        stats_file: Union[Path, str],\n        norm_means: bool = True,\n        norm_vars: bool = True,\n        eps: float = 1.0e-20,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self.norm_means = norm_means\n        self.norm_vars = norm_vars\n        self.eps = eps\n        stats_file = Path(stats_file)\n\n        self.stats_file = stats_file\n        stats = np.load(stats_file)\n        if isinstance(stats, np.ndarray):\n            # Kaldi like stats\n            count = stats[0].flatten()[-1]\n            mean = stats[0, :-1] / count\n            var = stats[1, :-1] / count - mean * mean\n        else:\n            # New style: Npz file\n            count = stats[""count""]\n            sum_v = stats[""sum""]\n            sum_square_v = stats[""sum_square""]\n            mean = sum_v / count\n            var = sum_square_v / count - mean * mean\n        std = np.maximum(np.sqrt(var), eps)\n\n        self.register_buffer(""mean"", torch.from_numpy(mean))\n        self.register_buffer(""std"", torch.from_numpy(std))\n\n    def extra_repr(self):\n        return (\n            f""stats_file={self.stats_file}, ""\n            f""norm_means={self.norm_means}, norm_vars={self.norm_vars}""\n        )\n\n    def forward(\n        self, x: torch.Tensor, ilens: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Forward function\n\n        Args:\n            x: (B, L, ...)\n            ilens: (B,)\n        """"""\n        if ilens is None:\n            ilens = x.new_full([x.size(0)], x.size(1))\n        norm_means = self.norm_means\n        norm_vars = self.norm_vars\n        self.mean = self.mean.to(x.device, x.dtype)\n        self.std = self.std.to(x.device, x.dtype)\n        mask = make_pad_mask(ilens, x, 1)\n\n        # feat: (B, T, D)\n        if norm_means:\n            if x.requires_grad:\n                x = x - self.mean\n            else:\n                x -= self.mean\n        if x.requires_grad:\n            x = x.masked_fill(mask, 0.0)\n        else:\n            x.masked_fill_(mask, 0.0)\n\n        if norm_vars:\n            x /= self.std\n\n        return x, ilens\n\n    def inverse(\n        self, x: torch.Tensor, ilens: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if ilens is None:\n            ilens = x.new_full([x.size(0)], x.size(1))\n        norm_means = self.norm_means\n        norm_vars = self.norm_vars\n        self.mean = self.mean.to(x.device, x.dtype)\n        self.std = self.std.to(x.device, x.dtype)\n        mask = make_pad_mask(ilens, x, 1)\n\n        if x.requires_grad:\n            x = x.masked_fill(mask, 0.0)\n        else:\n            x.masked_fill_(mask, 0.0)\n\n        if norm_vars:\n            x *= self.std\n\n        # feat: (B, T, D)\n        if norm_means:\n            x += self.mean\n            x.masked_fill_(make_pad_mask(ilens, x, 1), 0.0)\n        return x, ilens\n'"
espnet2/layers/inversible_interface.py,2,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Tuple\n\nimport torch\n\n\nclass InversibleInterface(ABC):\n    @abstractmethod\n    def inverse(\n        self, input: torch.Tensor, input_lengths: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # return output, output_lengths\n        raise NotImplementedError\n'"
espnet2/layers/log_mel.py,7,"b'import librosa\nimport numpy as np\nimport torch\nfrom typing import Tuple\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\n\n\nclass LogMel(torch.nn.Module):\n    """"""Convert STFT to fbank feats\n\n    The arguments is same as librosa.filters.mel\n\n    Args:\n        fs: number > 0 [scalar] sampling rate of the incoming signal\n        n_fft: int > 0 [scalar] number of FFT components\n        n_mels: int > 0 [scalar] number of Mel bands to generate\n        fmin: float >= 0 [scalar] lowest frequency (in Hz)\n        fmax: float >= 0 [scalar] highest frequency (in Hz).\n            If `None`, use `fmax = fs / 2.0`\n        htk: use HTK formula instead of Slaney\n        norm: {None, 1, np.inf} [scalar]\n            if 1, divide the triangular mel weights by the width of the mel band\n            (area normalization).  Otherwise, leave all the triangles aiming for\n            a peak value of 1.0\n\n    """"""\n\n    def __init__(\n        self,\n        fs: int = 16000,\n        n_fft: int = 512,\n        n_mels: int = 80,\n        fmin: float = None,\n        fmax: float = None,\n        htk: bool = False,\n        norm=1,\n    ):\n        super().__init__()\n\n        fmin = 0 if fmin is None else fmin\n        fmax = fs / 2 if fmax is None else fmax\n        _mel_options = dict(\n            sr=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, htk=htk, norm=norm\n        )\n        self.mel_options = _mel_options\n\n        # Note(kamo): The mel matrix of librosa is different from kaldi.\n        melmat = librosa.filters.mel(**_mel_options)\n        # melmat: (D2, D1) -> (D1, D2)\n        self.register_buffer(""melmat"", torch.from_numpy(melmat.T).float())\n        inv_mel = np.linalg.pinv(melmat)\n        self.register_buffer(""inv_melmat"", torch.from_numpy(inv_mel.T).float())\n\n    def extra_repr(self):\n        return "", "".join(f""{k}={v}"" for k, v in self.mel_options.items())\n\n    def forward(\n        self, feat: torch.Tensor, ilens: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)\n        mel_feat = torch.matmul(feat, self.melmat)\n\n        logmel_feat = (mel_feat + 1e-20).log()\n        # Zero padding\n        if ilens is not None:\n            logmel_feat = logmel_feat.masked_fill(\n                make_pad_mask(ilens, logmel_feat, 1), 0.0\n            )\n        else:\n            ilens = feat.new_full(\n                [feat.size(0)], fill_value=feat.size(1), dtype=torch.long\n            )\n        return logmel_feat, ilens\n'"
espnet2/layers/mask_along_axis.py,7,"b'import torch\nfrom typeguard import check_argument_types\nfrom typing import Sequence\nfrom typing import Union\n\n\ndef mask_along_axis(\n    spec: torch.Tensor,\n    spec_lengths: torch.Tensor,\n    mask_width_range: Sequence[int] = (0, 30),\n    dim: int = 1,\n    num_mask: int = 2,\n    replace_with_zero: bool = True,\n):\n    """"""Apply mask along the specified direction.\n\n    Args:\n        spec: (Batch, Length, Freq)\n        spec_lengths: (Length): Not using lenghts in this implementation\n        mask_width_range: Select the width randomly between this range\n    """"""\n\n    org_size = spec.size()\n    if spec.dim() == 4:\n        # spec: (Batch, Channel, Length, Freq) -> (Batch * Channel, Length, Freq)\n        spec = spec.view(-1, spec.size(2), spec.size(3))\n\n    B = spec.shape[0]\n    # D = Length or Freq\n    D = spec.shape[dim]\n    # mask_length: (B, num_mask, 1)\n    mask_length = torch.randint(\n        mask_width_range[0], mask_width_range[1], (B, num_mask), device=spec.device,\n    ).unsqueeze(2)\n\n    # mask_pos: (B, num_mask, 1)\n    mask_pos = torch.randint(\n        0, max(1, D - mask_length.max()), (B, num_mask), device=spec.device\n    ).unsqueeze(2)\n\n    # aran: (1, 1, D)\n    aran = torch.arange(D, device=spec.device)[None, None, :]\n    # mask: (Batch, num_mask, D)\n    mask = (mask_pos <= aran) * (aran < (mask_pos + mask_length))\n    # Multiply masks: (Batch, num_mask, D) -> (Batch, D)\n    mask = mask.any(dim=1)\n    if dim == 1:\n        # mask: (Batch, Length, 1)\n        mask = mask.unsqueeze(2)\n    elif dim == 2:\n        # mask: (Batch, 1, Freq)\n        mask = mask.unsqueeze(1)\n\n    if replace_with_zero:\n        value = 0.0\n    else:\n        value = spec.mean()\n\n    if spec.requires_grad:\n        spec = spec.masked_fill(mask, value)\n    else:\n        spec = spec.masked_fill_(mask, value)\n    spec = spec.view(*org_size)\n    return spec, spec_lengths\n\n\nclass MaskAlongAxis(torch.nn.Module):\n    def __init__(\n        self,\n        mask_width_range: Union[int, Sequence[int]] = (0, 30),\n        num_mask: int = 2,\n        dim: Union[int, str] = ""time"",\n        replace_with_zero: bool = True,\n    ):\n        assert check_argument_types()\n        if isinstance(mask_width_range, int):\n            mask_width_range = (0, mask_width_range)\n        if len(mask_width_range) != 2:\n            raise TypeError(\n                f""mask_width_range must be a tuple of int and int values: ""\n                f""{mask_width_range}"",\n            )\n\n        assert mask_width_range[1] > mask_width_range[0]\n        if isinstance(dim, str):\n            if dim == ""time"":\n                dim = 1\n            elif dim == ""freq"":\n                dim = 2\n            else:\n                raise ValueError(""dim must be int, \'time\' or \'freq\'"")\n        if dim == 1:\n            self.mask_axis = ""time""\n        elif dim == 2:\n            self.mask_axis = ""freq""\n        else:\n            self.mask_axis = ""unknown""\n\n        super().__init__()\n        self.mask_width_range = mask_width_range\n        self.num_mask = num_mask\n        self.dim = dim\n        self.replace_with_zero = replace_with_zero\n\n    def extra_repr(self):\n        return (\n            f""mask_width_range={self.mask_width_range}, ""\n            f""num_mask={self.num_mask}, axis={self.mask_axis}""\n        )\n\n    def forward(self, spec: torch.Tensor, spec_lengths: torch.Tensor = None):\n        """"""Forward function.\n\n        Args:\n            spec: (Batch, Length, Freq)\n        """"""\n\n        return mask_along_axis(\n            spec,\n            spec_lengths,\n            mask_width_range=self.mask_width_range,\n            dim=self.dim,\n            num_mask=self.num_mask,\n            replace_with_zero=self.replace_with_zero,\n        )\n'"
espnet2/layers/stft.py,6,"b'from typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet2.layers.inversible_interface import InversibleInterface\n\n\nclass Stft(torch.nn.Module, InversibleInterface):\n    def __init__(\n        self,\n        n_fft: int = 512,\n        win_length: Union[int, None] = 512,\n        hop_length: int = 128,\n        center: bool = True,\n        pad_mode: str = ""reflect"",\n        normalized: bool = False,\n        onesided: bool = True,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self.n_fft = n_fft\n        if win_length is None:\n            self.win_length = n_fft\n        else:\n            self.win_length = win_length\n        self.hop_length = hop_length\n        self.center = center\n        self.pad_mode = pad_mode\n        self.normalized = normalized\n        self.onesided = onesided\n\n    def extra_repr(self):\n        return (\n            f""n_fft={self.n_fft}, ""\n            f""win_length={self.win_length}, ""\n            f""hop_length={self.hop_length}, ""\n            f""center={self.center}, ""\n            f""pad_mode={self.pad_mode}, ""\n            f""normalized={self.normalized}, ""\n            f""onesided={self.onesided}""\n        )\n\n    def forward(\n        self, input: torch.Tensor, ilens: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        """"""STFT forward function.\n\n        Args:\n            input: (Batch, Nsamples) or (Batch, Nsample, Channels)\n            ilens: (Batch)\n        Returns:\n            output: (Batch, Frames, Freq, 2) or (Batch, Frames, Channels, Freq, 2)\n\n        """"""\n        bs = input.size(0)\n        if input.dim() == 3:\n            multi_channel = True\n            # input: (Batch, Nsample, Channels) -> (Batch * Channels, Nsample)\n            input = input.transpose(1, 2).reshape(-1, input.size(1))\n        else:\n            multi_channel = False\n\n        # output: (Batch, Freq, Frames, 2=real_imag)\n        # or (Batch, Channel, Freq, Frames, 2=real_imag)\n        output = torch.stft(\n            input,\n            n_fft=self.n_fft,\n            win_length=self.win_length,\n            hop_length=self.hop_length,\n            center=self.center,\n            pad_mode=self.pad_mode,\n            normalized=self.normalized,\n            onesided=self.onesided,\n        )\n        # output: (Batch, Freq, Frames, 2=real_imag)\n        # -> (Batch, Frames, Freq, 2=real_imag)\n        output = output.transpose(1, 2)\n        if multi_channel:\n            # output: (Batch * Channel, Frames, Freq, 2=real_imag)\n            # -> (Batch, Frame, Channel, Freq, 2=real_imag)\n            output = output.view(bs, -1, output.size(1), output.size(2), 2).transpose(\n                1, 2\n            )\n\n        if ilens is not None:\n            if self.center:\n                pad = self.win_length // 2\n                ilens = ilens + 2 * pad\n\n            olens = (ilens - self.win_length) // self.hop_length + 1\n            output.masked_fill_(make_pad_mask(olens, output, 1), 0.0)\n        else:\n            olens = None\n\n        return output, olens\n\n    def inverse(\n        self, input: torch.Tensor, ilens: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        # TODO(kamo): torch audio?\n        raise NotImplementedError\n'"
espnet2/layers/time_warp.py,11,"b'from distutils.version import LooseVersion\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\nif LooseVersion(torch.__version__) >= LooseVersion(""1.1""):\n    DEFAULT_TIME_WARP_MODE = ""bicubic""\nelse:\n    # pytorch1.0 doesn\'t implement bicubic\n    DEFAULT_TIME_WARP_MODE = ""bilinear""\n\n\ndef time_warp(x: torch.Tensor, window: int = 80, mode: str = DEFAULT_TIME_WARP_MODE):\n    """"""Time warping using torch.interpolate.\n\n    Args:\n        x: (Batch, Time, Freq)\n        window: time warp parameter\n        mode: Interpolate mode\n    """"""\n\n    # bicubic supports 4D or more dimension tensor\n    org_size = x.size()\n    if x.dim() == 3:\n        # x: (Batch, Time, Freq) -> (Batch, 1, Time, Freq)\n        x = x[:, None]\n\n    t = x.shape[2]\n    if t - window <= window:\n        return x.view(*org_size)\n\n    center = torch.randint(window, t - window, (1,))[0]\n    warped = torch.randint(center - window, center + window, (1,))[0] + 1\n\n    # left: (Batch, Channel, warped, Freq)\n    # right: (Batch, Channel, time - warped, Freq)\n    left = torch.nn.functional.interpolate(\n        x[:, :, :center], (warped, x.shape[3]), mode=mode, align_corners=False\n    )\n    right = torch.nn.functional.interpolate(\n        x[:, :, center:], (t - warped, x.shape[3]), mode=mode, align_corners=False\n    )\n\n    if x.requires_grad:\n        x = torch.cat([left, right], dim=-2)\n    else:\n        x[:, :, :warped] = left\n        x[:, :, warped:] = right\n\n    return x.view(*org_size)\n\n\nclass TimeWarp(torch.nn.Module):\n    """"""Time warping using torch.interpolate.\n\n    Args:\n        window: time warp parameter\n        mode: Interpolate mode\n    """"""\n\n    def __init__(self, window: int = 80, mode: str = DEFAULT_TIME_WARP_MODE):\n        super().__init__()\n        self.window = window\n        self.mode = mode\n\n    def extra_repr(self):\n        return f""window={self.window}, mode={self.mode}""\n\n    def forward(self, x: torch.Tensor, x_lengths: torch.Tensor = None):\n        """"""Forward function.\n\n        Args:\n            x: (Batch, Time, Freq)\n            x_lengths: (Batch,)\n        """"""\n\n        if x_lengths is None or all(le == x_lengths[0] for le in x_lengths):\n            # Note that applying same warping for each sample\n            y = time_warp(x, window=self.window, mode=self.mode)\n        else:\n            # FIXME(kamo): I have no idea to batchify Timewarp\n            ys = []\n            for i in range(x.size(0)):\n                _y = time_warp(\n                    x[i][None, : x_lengths[i]], window=self.window, mode=self.mode,\n                )[0]\n                ys.append(_y)\n            y = pad_list(ys, 0.0)\n\n        return y, x_lengths\n'"
espnet2/layers/utterance_mvn.py,7,"b'from typing import Tuple\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet2.layers.abs_normalize import AbsNormalize\n\n\nclass UtteranceMVN(AbsNormalize):\n    def __init__(\n        self, norm_means: bool = True, norm_vars: bool = False, eps: float = 1.0e-20,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self.norm_means = norm_means\n        self.norm_vars = norm_vars\n        self.eps = eps\n\n    def extra_repr(self):\n        return f""norm_means={self.norm_means}, norm_vars={self.norm_vars}""\n\n    def forward(\n        self, x: torch.Tensor, ilens: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Forward function\n\n        Args:\n            x: (B, L, ...)\n            ilens: (B,)\n\n        """"""\n        return utterance_mvn(\n            x,\n            ilens,\n            norm_means=self.norm_means,\n            norm_vars=self.norm_vars,\n            eps=self.eps,\n        )\n\n\ndef utterance_mvn(\n    x: torch.Tensor,\n    ilens: torch.Tensor = None,\n    norm_means: bool = True,\n    norm_vars: bool = False,\n    eps: float = 1.0e-20,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    """"""Apply utterance mean and variance normalization\n\n    Args:\n        x: (B, T, D), assumed zero padded\n        ilens: (B,)\n        norm_means:\n        norm_vars:\n        eps:\n\n    """"""\n    if ilens is None:\n        ilens = x.new_full([x.size(0)], x.size(1))\n    ilens_ = ilens.to(x.device, x.dtype).view(-1, *[1 for _ in range(x.dim() - 1)])\n    # Zero padding\n    if x.requires_grad:\n        x = x.masked_fill(make_pad_mask(ilens, x, 1), 0.0)\n    else:\n        x.masked_fill_(make_pad_mask(ilens, x, 1), 0.0)\n    # mean: (B, 1, D)\n    mean = x.sum(dim=1, keepdim=True) / ilens_\n\n    if norm_means:\n        x -= mean\n\n        if norm_vars:\n            var = x.pow(2).sum(dim=1, keepdim=True) / ilens_\n            std = torch.clamp(var.sqrt(), min=eps)\n            x = x / std.sqrt()\n        return x, ilens\n    else:\n        if norm_vars:\n            y = x - mean\n            y.masked_fill_(make_pad_mask(ilens, y, 1), 0.0)\n            var = y.pow(2).sum(dim=1, keepdim=True) / ilens_\n            std = torch.clamp(var.sqrt(), min=eps)\n            x /= std\n        return x, ilens\n'"
espnet2/lm/__init__.py,0,b''
espnet2/lm/abs_model.py,3,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Tuple\n\nimport torch\n\nfrom espnet.nets.scorer_interface import ScorerInterface\n\n\nclass AbsLM(torch.nn.Module, ScorerInterface, ABC):\n    """"""The abstract LM class\n\n    To share the loss calculation way among different models,\n    We uses delegate pattern here:\n    The instance of this class should be passed to ""LanguageModel""\n\n    >>> from espnet2.lm.abs_model import LanguageESPnetModel\n    >>> lm = AbsLM()\n    >>> model = LanguageESPnetModel(lm=lm)\n\n    This ""model"" is one of mediator objects for ""Task"" class.\n\n    """"""\n\n    @abstractmethod\n    def forward(\n        self, input: torch.Tensor, hidden: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        raise NotImplementedError\n'"
espnet2/lm/espnet_model.py,7,"b'from typing import Dict\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet2.lm.abs_model import AbsLM\nfrom espnet2.torch_utils.device_funcs import force_gatherable\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\n\n\nclass ESPnetLanguageModel(AbsESPnetModel):\n    def __init__(self, lm: AbsLM, vocab_size: int, ignore_id: int = 0):\n        assert check_argument_types()\n        super().__init__()\n        self.lm = lm\n        self.sos = vocab_size - 1\n        self.eos = vocab_size - 1\n\n        # ignore_id may be assumed as 0, shared with CTC-blank symbol for ASR.\n        self.ignore_id = ignore_id\n\n    def nll(\n        self, text: torch.Tensor, text_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        batch_size = text.size(0)\n        # For data parallel\n        text = text[:, : text_lengths.max()]\n\n        # 1. Create a sentence pair like \'<sos> w1 w2 w3\' and \'w1 w2 w3 <eos>\'\n        # text: (Batch, Length) -> x, y: (Batch, Length + 1)\n        x = F.pad(text, [1, 0], ""constant"", self.eos)\n        t = F.pad(text, [0, 1], ""constant"", self.ignore_id)\n        for i, l in enumerate(text_lengths):\n            t[i, l] = self.sos\n        x_lengths = text_lengths + 1\n\n        # 2. Forward Language model\n        # x: (Batch, Length) -> y: (Batch, Length, NVocab)\n        y, _ = self.lm(x, None)\n\n        # 3. Calc negative log likelihood\n        # nll: (BxL,)\n        nll = F.cross_entropy(y.view(-1, y.shape[-1]), t.view(-1), reduction=""none"")\n        # nll: (BxL,) -> (BxL,)\n        nll.masked_fill_(make_pad_mask(x_lengths).to(nll.device).view(-1), 0.0)\n        # nll: (BxL,) -> (B, L)\n        nll = nll.view(batch_size, -1)\n        return nll, x_lengths\n\n    def forward(\n        self, text: torch.Tensor, text_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:\n        nll, y_lengths = self.nll(text, text_lengths)\n        ntokens = y_lengths.sum()\n        loss = nll.sum() / ntokens\n        stats = dict(loss=loss.detach())\n\n        # force_gatherable: to-device and to-tensor if scalar for DataParallel\n        loss, stats, weight = force_gatherable((loss, stats, ntokens), loss.device)\n        return loss, stats, weight\n\n    def collect_feats(\n        self, text: torch.Tensor, text_lengths: torch.Tensor\n    ) -> Dict[str, torch.Tensor]:\n        return {}\n'"
espnet2/lm/seq_rnn.py,10,"b'""""""Sequential implementation of Recurrent Neural Network Language Model.""""""\nfrom typing import Tuple\nfrom typing import Union\n\nimport torch\nimport torch.nn as nn\nfrom typeguard import check_argument_types\n\nfrom espnet2.lm.abs_model import AbsLM\n\n\nclass SequentialRNNLM(AbsLM):\n    """"""Sequential RNNLM.\n\n    See also:\n        https://github.com/pytorch/examples/blob/4581968193699de14b56527296262dd76ab43557/word_language_model/model.py\n\n    """"""\n\n    def __init__(\n        self,\n        vocab_size: int,\n        unit: int = 650,\n        nhid: int = None,\n        nlayers: int = 2,\n        dropout_rate: float = 0.0,\n        tie_weights: bool = False,\n        rnn_type: str = ""lstm"",\n        ignore_id: int = 0,\n    ):\n        assert check_argument_types()\n        super().__init__()\n\n        ninp = unit\n        if nhid is None:\n            nhid = unit\n        rnn_type = rnn_type.upper()\n\n        self.drop = nn.Dropout(dropout_rate)\n        self.encoder = nn.Embedding(vocab_size, ninp, padding_idx=ignore_id)\n        if rnn_type in [""LSTM"", ""GRU""]:\n            rnn_class = getattr(nn, rnn_type)\n            self.rnn = rnn_class(\n                ninp, nhid, nlayers, dropout=dropout_rate, batch_first=True\n            )\n        else:\n            try:\n                nonlinearity = {""RNN_TANH"": ""tanh"", ""RNN_RELU"": ""relu""}[rnn_type]\n            except KeyError:\n                raise ValueError(\n                    """"""An invalid option for `--model` was supplied,\n                    options are [\'LSTM\', \'GRU\', \'RNN_TANH\' or \'RNN_RELU\']""""""\n                )\n            self.rnn = nn.RNN(\n                ninp,\n                nhid,\n                nlayers,\n                nonlinearity=nonlinearity,\n                dropout=dropout_rate,\n                batch_first=True,\n            )\n        self.decoder = nn.Linear(nhid, vocab_size)\n\n        # Optionally tie weights as in:\n        # ""Using the Output Embedding to Improve Language Models""\n        # (Press & Wolf 2016) https://arxiv.org/abs/1608.05859\n        # and\n        # ""Tying Word Vectors and Word Classifiers:\n        # A Loss Framework for Language Modeling"" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            if nhid != ninp:\n                raise ValueError(\n                    ""When using the tied flag, nhid must be equal to emsize""\n                )\n            self.decoder.weight = self.encoder.weight\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def forward(\n        self, input: torch.Tensor, hidden: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(\n            output.contiguous().view(output.size(0) * output.size(1), output.size(2))\n        )\n        return (\n            decoded.view(output.size(0), output.size(1), decoded.size(1)),\n            hidden,\n        )\n\n    def init_state(self, x):\n        """"""Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        """"""\n        bsz = 1\n        weight = next(self.parameters())\n        if self.rnn_type == ""LSTM"":\n            return (\n                weight.new_zeros(self.nlayers, bsz, self.nhid),\n                weight.new_zeros(self.nlayers, bsz, self.nhid),\n            )\n        else:\n            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n\n    def score(\n        self,\n        y: torch.Tensor,\n        state: Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]],\n        x: torch.Tensor,\n    ) -> Tuple[torch.Tensor, Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]:\n        """"""Score new token.\n\n        Args:\n            y: 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x: 2D encoder feature that generates ys.\n\n        Returns:\n            Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and next state for ys\n\n        """"""\n        y, new_state = self(y[-1].view(1, 1), state)\n        logp = y.log_softmax(dim=-1).view(-1)\n        return logp, new_state\n'"
espnet2/main_funcs/__init__.py,0,b''
espnet2/main_funcs/average_nbest_models.py,3,"b'import logging\nfrom pathlib import Path\nfrom typing import Sequence\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet2.train.reporter import Reporter\n\n\n@torch.no_grad()\ndef average_nbest_models(\n    output_dir: Path,\n    reporter: Reporter,\n    best_model_criterion: Sequence[Sequence[str]],\n    nbest: int,\n) -> None:\n    assert check_argument_types()\n    # 1. Get nbests: List[Tuple[str, str, List[Tuple[epoch, value]]]]\n    nbest_epochs = [\n        (ph, k, reporter.sort_epochs_and_values(ph, k, m)[:nbest])\n        for ph, k, m in best_model_criterion\n        if reporter.has(ph, k)\n    ]\n\n    _loaded = {}\n    for ph, cr, epoch_and_values in nbest_epochs:\n        # Note that len(epoch_and_values) doesn\'t always equal to nbest.\n        op = output_dir / f""{ph}.{cr}.ave_{len(epoch_and_values)}best.pth""\n        logging.info(\n            f""Averaging {len(epoch_and_values)}best models: ""\n            f\'criterion=""{ph}.{cr}"": {op}\'\n        )\n\n        if len(epoch_and_values) == 0:\n            continue\n        elif len(epoch_and_values) == 1:\n            # The averaged model is same as the best model\n            e, _ = epoch_and_values[0]\n            op = output_dir / f""{e}epoch.pth""\n            for sym_op in [\n                output_dir / f""{ph}.{cr}.ave.pth"",\n                output_dir / f""{ph}.{cr}.ave_{len(epoch_and_values)}best.pth"",\n            ]:\n                if sym_op.is_symlink() or sym_op.exists():\n                    sym_op.unlink()\n                sym_op.symlink_to(op.name)\n        else:\n            avg = None\n            # 2.a Averaging model\n            for e, _ in epoch_and_values:\n                if e not in _loaded:\n                    _loaded[e] = torch.load(\n                        output_dir / f""{e}epoch.pth"", map_location=""cpu"",\n                    )\n                states = _loaded[e]\n\n                if avg is None:\n                    avg = states\n                else:\n                    # Accumulated\n                    for k in avg:\n                        avg[k] += states[k]\n            for k in avg:\n                avg[k] /= len(epoch_and_values)\n\n            # 2.b Save the ave model and create a symlink\n            torch.save(avg, op)\n            sym_op = output_dir / f""{ph}.{cr}.ave.pth""\n            if sym_op.is_symlink() or sym_op.exists():\n                sym_op.unlink()\n            sym_op.symlink_to(op.name)\n'"
espnet2/main_funcs/calculate_all_attentions.py,6,"b'from collections import defaultdict\nfrom typing import Dict\nfrom typing import List\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttAdd\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttCov\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttCovLoc\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttDot\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttForward\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttForwardTA\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttLoc\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttLoc2D\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttLocRec\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttMultiHeadAdd\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttMultiHeadDot\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttMultiHeadLoc\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttMultiHeadMultiResLoc\nfrom espnet.nets.pytorch_backend.rnn.attentions import NoAtt\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n\n\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\n\n\n@torch.no_grad()\ndef calculate_all_attentions(\n    model: AbsESPnetModel, batch: Dict[str, torch.Tensor]\n) -> Dict[str, List[torch.Tensor]]:\n    """"""Derive the outputs from the all attention layers\n\n    Args:\n        model:\n        batch: same as forward\n    Returns:\n        return_dict: A dict of a list of tensor.\n        key_names x batch x (D1, D2, ...)\n\n    """"""\n    bs = len(next(iter(batch.values())))\n    assert all(len(v) == bs for v in batch.values()), {\n        k: v.shape for k, v in batch.items()\n    }\n\n    # 1. Register forward_hook fn to save the output from specific layers\n    outputs = {}\n    handles = {}\n    for name, modu in model.named_modules():\n\n        def hook(module, input, output, name=name):\n            if isinstance(module, MultiHeadedAttention):\n                # att_w: (B, Tout, Tin)\n                att_w = output\n                outputs[name] = att_w.detach().cpu()\n            elif isinstance(module, AttLoc2D):\n                c, w = output\n                # w: previous concate attentions\n                # w: (B, nprev, Tin)\n                att_w = w[:, -1].detach().cpu()\n                outputs.setdefault(name, []).append(att_w)\n            elif isinstance(module, (AttCov, AttCovLoc)):\n                c, w = output\n                assert isinstance(w, list), type(w)\n                # w: list of previous attentions\n                # w: nprev x (B, Tin)\n                att_w = w[-1].detach().cpu()\n                outputs.setdefault(name, []).append(att_w)\n            elif isinstance(module, AttLocRec):\n                # w: (B, Tin)\n                c, (w, (att_h, att_c)) = output\n                att_w = w.detach().cpu()\n                outputs.setdefault(name, []).append(att_w)\n            elif isinstance(\n                module,\n                (\n                    AttMultiHeadDot,\n                    AttMultiHeadAdd,\n                    AttMultiHeadLoc,\n                    AttMultiHeadMultiResLoc,\n                ),\n            ):\n                c, w = output\n                # w: nhead x (B, Tin)\n                assert isinstance(w, list), type(w)\n                att_w = [_w.detach().cpu() for _w in w]\n                outputs.setdefault(name, []).append(att_w)\n            elif isinstance(\n                module, (AttAdd, AttDot, AttForward, AttForwardTA, AttLoc, NoAtt,),\n            ):\n                c, w = output\n                att_w = w.detach().cpu()\n                outputs.setdefault(name, []).append(att_w)\n\n        handle = modu.register_forward_hook(hook)\n        handles[name] = handle\n\n    # 2. Just forward one by one sample.\n    # Batch-mode can\'t be used to keep requirements small for each models.\n    keys = []\n    for k in batch:\n        if not k.endswith(""_lengths""):\n            keys.append(k)\n\n    return_dict = defaultdict(list)\n    for ibatch in range(bs):\n        # *: (B, L, ...) -> (1, L2, ...)\n        _sample = {\n            k: batch[k][ibatch, None, : batch[k + ""_lengths""][ibatch]]\n            if k + ""_lengths"" in batch\n            else batch[k][ibatch, None]\n            for k in keys\n        }\n\n        # *_lengths: (B,) -> (1,)\n        _sample.update(\n            {\n                k + ""_lengths"": batch[k + ""_lengths""][ibatch, None]\n                for k in keys\n                if k + ""_lengths"" in batch\n            }\n        )\n        model(**_sample)\n\n        # Derive the attention results\n        for name, output in outputs.items():\n            if isinstance(output, list):\n                if isinstance(output[0], list):\n                    # output: nhead x (Tout, Tin)\n                    output = torch.stack(\n                        [\n                            # Tout x (1, Tin) -> (Tout, Tin)\n                            torch.cat([o[idx] for o in output], dim=0)\n                            for idx in range(len(output[0]))\n                        ],\n                        dim=0,\n                    )\n                else:\n                    # Tout x (1, Tin) -> (Tout, Tin)\n                    output = torch.cat(output, dim=0)\n            return_dict[name].append(output)\n        outputs.clear()\n\n    # 3. Remove all hooks\n    for _, handle in handles.items():\n        handle.remove()\n\n    return dict(return_dict)\n'"
espnet2/main_funcs/collect_stats.py,5,"b'from collections import defaultdict\nimport logging\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.nn.parallel import data_parallel\nfrom torch.utils.data import DataLoader\nfrom typeguard import check_argument_types\n\nfrom espnet2.torch_utils.device_funcs import to_device\nfrom espnet2.torch_utils.forward_adaptor import ForwardAdaptor\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\nfrom espnet2.utils.fileio import DatadirWriter\nfrom espnet2.utils.fileio import NpyScpWriter\n\n\n@torch.no_grad()\ndef collect_stats(\n    model: AbsESPnetModel,\n    train_iter: DataLoader and Iterable[Tuple[List[str], Dict[str, torch.Tensor]]],\n    valid_iter: DataLoader and Iterable[Tuple[List[str], Dict[str, torch.Tensor]]],\n    output_dir: Path,\n    ngpu: Optional[int],\n    log_interval: Optional[int],\n    write_collected_feats: bool,\n) -> None:\n    """"""Perform on collect_stats mode.\n\n    Running for deriving the shape information from data\n    and gathering statistics.\n    This method is used before executing train().\n\n    """"""\n    assert check_argument_types()\n\n    npy_scp_writers = {}\n    for itr, mode in zip([train_iter, valid_iter], [""train"", ""valid""]):\n        if log_interval is None:\n            try:\n                log_interval = max(len(itr) // 20, 10)\n            except TypeError:\n                log_interval = 100\n\n        sum_dict = defaultdict(lambda: 0)\n        sq_dict = defaultdict(lambda: 0)\n        count_dict = defaultdict(lambda: 0)\n\n        with DatadirWriter(output_dir / mode) as datadir_writer:\n            for iiter, (keys, batch) in enumerate(itr, 1):\n                batch = to_device(batch, ""cuda"" if ngpu > 0 else ""cpu"")\n\n                # 1. Write shape file\n                for name in batch:\n                    if name.endswith(""_lengths""):\n                        continue\n                    for i, (key, data) in enumerate(zip(keys, batch[name])):\n                        if f""{name}_lengths"" in batch:\n                            lg = int(batch[f""{name}_lengths""][i])\n                            data = data[:lg]\n                        datadir_writer[f""{name}_shape""][key] = "","".join(\n                            map(str, data.shape)\n                        )\n\n                # 2. Extract feats\n                if ngpu <= 1:\n                    data = model.collect_feats(**batch)\n                else:\n                    # Note that data_parallel can parallelize only ""forward()""\n                    data = data_parallel(\n                        ForwardAdaptor(model, ""collect_feats""),\n                        (),\n                        range(ngpu),\n                        module_kwargs=batch,\n                    )\n\n                # 3. Calculate sum and square sum\n                for key, v in data.items():\n                    for i, (uttid, seq) in enumerate(zip(keys, v.cpu().numpy())):\n                        # Truncate zero-padding region\n                        if f""{key}_lengths"" in data:\n                            length = data[f""{key}_lengths""][i]\n                            # seq: (Length, Dim, ...)\n                            seq = seq[:length]\n                        else:\n                            # seq: (Dim, ...) -> (1, Dim, ...)\n                            seq = seq[None]\n                        # Accumulate value, its square, and count\n                        sum_dict[key] += seq.sum(0)\n                        sq_dict[key] += (seq ** 2).sum(0)\n                        count_dict[key] += len(seq)\n\n                        # 4. [Option] Write derived features as npy format file.\n                        if write_collected_feats:\n                            # Instantiate NpyScpWriter for the first iteration\n                            if (key, mode) not in npy_scp_writers:\n                                p = output_dir / mode / ""collect_feats""\n                                npy_scp_writers[(key, mode)] = NpyScpWriter(\n                                    p / f""data_{key}"", p / f""{key}.scp""\n                                )\n                            # Save array as npy file\n                            npy_scp_writers[(key, mode)][uttid] = seq\n\n                if iiter % log_interval == 0:\n                    logging.info(f""Niter: {iiter}"")\n\n        for key in sum_dict:\n            np.savez(\n                output_dir / mode / f""{key}_stats.npz"",\n                count=count_dict[key],\n                sum=sum_dict[key],\n                sum_square=sq_dict[key],\n            )\n\n        # batch_keys and stats_keys are used by aggregate_stats_dirs.py\n        with (output_dir / mode / ""batch_keys"").open(""w"", encoding=""utf-8"") as f:\n            f.write(\n                ""\\n"".join(filter(lambda x: not x.endswith(""_lengths""), batch)) + ""\\n""\n            )\n        with (output_dir / mode / ""stats_keys"").open(""w"", encoding=""utf-8"") as f:\n            f.write(""\\n"".join(sum_dict) + ""\\n"")\n'"
espnet2/optimizers/__init__.py,0,b''
espnet2/optimizers/sgd.py,2,"b'import torch\nfrom typeguard import check_argument_types\n\n\nclass SGD(torch.optim.SGD):\n    """"""Thin inheritance of torch.optim.SGD to bind the required arguments, \'lr\'\n\n    Note that\n    the arguments of the optimizer invoked by AbsTask.main()\n    must have default value except for \'param\'.\n\n    I can\'t understand why only SGD.lr doesn\'t have the default value.\n    """"""\n\n    def __init__(\n        self,\n        params,\n        lr: float = 0.1,\n        momentum: float = 0.0,\n        dampening: float = 0.0,\n        weight_decay: float = 0.0,\n        nesterov: bool = False,\n    ):\n        assert check_argument_types()\n        super().__init__(\n            params,\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n        )\n'"
espnet2/samplers/__init__.py,0,b''
espnet2/samplers/abs_sampler.py,1,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Iterator\nfrom typing import Tuple\n\nfrom torch.utils.data import Sampler\n\n\nclass AbsSampler(Sampler, ABC):\n    @abstractmethod\n    def __len__(self) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def __iter__(self) -> Iterator[Tuple[str, ...]]:\n        raise NotImplementedError\n'"
espnet2/samplers/build_batch_sampler.py,0,"b'from typing import List\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nfrom espnet2.samplers.abs_sampler import AbsSampler\nfrom espnet2.samplers.folded_batch_sampler import FoldedBatchSampler\nfrom espnet2.samplers.length_batch_sampler import LengthBatchSampler\nfrom espnet2.samplers.num_elements_batch_sampler import NumElementsBatchSampler\nfrom espnet2.samplers.sorted_batch_sampler import SortedBatchSampler\nfrom espnet2.samplers.unsorted_batch_sampler import UnsortedBatchSampler\n\n\nBATCH_TYPES = dict(\n    unsorted=""UnsortedBatchSampler has nothing in paticular feature and ""\n    ""just creates mini-batches which has constant batch_size. ""\n    ""This sampler doesn\'t require any length ""\n    ""information for each feature. ""\n    ""\'key_file\' is just a text file which describes each sample name.""\n    ""\\n\\n""\n    ""    utterance_id_a\\n""\n    ""    utterance_id_b\\n""\n    ""    utterance_id_c\\n""\n    ""\\n""\n    ""The fist column is referred, so \'shape file\' can be used, too.\\n\\n""\n    ""    utterance_id_a 100,80\\n""\n    ""    utterance_id_b 400,80\\n""\n    ""    utterance_id_c 512,80\\n"",\n    sorted=""SortedBatchSampler sorts samples by the length of the first input ""\n    "" in order to make each sample in a mini-batch has close length. ""\n    ""This sampler requires a text file which describes the length for each sample ""\n    ""\\n\\n""\n    ""    utterance_id_a 1000\\n""\n    ""    utterance_id_b 1453\\n""\n    ""    utterance_id_c 1241\\n""\n    ""\\n""\n    ""The first element of feature dimensions is referred, ""\n    ""so \'shape_file\' can be also used.\\n\\n""\n    ""    utterance_id_a 1000,80\\n""\n    ""    utterance_id_b 1453,80\\n""\n    ""    utterance_id_c 1241,80\\n"",\n    folded=""FoldedBatchSampler supports variable batch_size. ""\n    ""The batch_size is decided by\\n""\n    ""    batch_size = base_batch_size // (L // fold_length)\\n""\n    ""L is referred to the largest length of samples in the mini-batch. ""\n    ""This samples requires length information as same as SortedBatchSampler\\n"",\n    length=""LengthBatchSampler supports variable batch_size. ""\n    ""This sampler makes mini-batches which have same number of \'bins\' as possible ""\n    ""counting by the total lengths of each feature in the mini-batch. ""\n    ""This sampler requires a text file which describes the length for each sample. ""\n    ""\\n\\n""\n    ""    utterance_id_a 1000\\n""\n    ""    utterance_id_b 1453\\n""\n    ""    utterance_id_c 1241\\n""\n    ""\\n""\n    ""The first element of feature dimensions is referred, ""\n    ""so \'shape_file\' can be also used.\\n\\n""\n    ""    utterance_id_a 1000,80\\n""\n    ""    utterance_id_b 1453,80\\n""\n    ""    utterance_id_c 1241,80\\n"",\n    numel=""NumElementsBatchSampler supports variable batch_size. ""\n    ""Just like LengthBatchSampler, this sampler makes mini-batches""\n    "" which have same number of \'bins\' as possible ""\n    ""counting by the total number of elements of each feature ""\n    ""instead of the length. ""\n    ""Thus this sampler requires the full information of the dimension of the features. ""\n    ""\\n\\n""\n    ""    utterance_id_a 1000,80\\n""\n    ""    utterance_id_b 1453,80\\n""\n    ""    utterance_id_c 1241,80\\n"",\n)\n\n\ndef build_batch_sampler(\n    type: str,\n    batch_size: int,\n    batch_bins: int,\n    shape_files: Union[Tuple[str, ...], List[str]],\n    sort_in_batch: str = ""descending"",\n    sort_batch: str = ""ascending"",\n    drop_last: bool = False,\n    min_batch_size: int = 1,\n    fold_lengths: Sequence[int] = (),\n    padding: bool = True,\n) -> AbsSampler:\n    """"""Helper function to instantiate BatchSampler.\n\n    Args:\n        type: mini-batch type. ""unsorted"", ""sorted"", ""folded"", ""numel"", or, ""length""\n        batch_size: The mini-batch size. Used for ""unsorted"", ""sorted"", ""folded"" mode\n        batch_bins: Used for ""numel"" model\n        shape_files: Text files describing the length and dimension\n            of each features. e.g. uttA 1330,80\n        sort_in_batch:\n        sort_batch:\n        drop_last:\n        min_batch_size:  Used for ""numel"" or ""folded"" mode\n        fold_lengths: Used for ""folded"" mode\n        padding: Whether sequences are input as a padded tensor or not.\n            used for ""numel"" mode\n    """"""\n    assert check_argument_types()\n\n    if type == ""unsorted"":\n        retval = UnsortedBatchSampler(\n            batch_size=batch_size, key_file=shape_files[0], drop_last=drop_last\n        )\n\n    elif type == ""sorted"":\n        retval = SortedBatchSampler(\n            batch_size=batch_size,\n            shape_file=shape_files[0],\n            sort_in_batch=sort_in_batch,\n            sort_batch=sort_batch,\n            drop_last=drop_last,\n        )\n\n    elif type == ""folded"":\n        if len(fold_lengths) != len(shape_files):\n            raise ValueError(\n                f""The number of fold_lengths must be equal to ""\n                f""the number of shape_files: ""\n                f""{len(fold_lengths)} != {len(shape_files)}""\n            )\n        retval = FoldedBatchSampler(\n            batch_size=batch_size,\n            shape_files=shape_files,\n            fold_lengths=fold_lengths,\n            sort_in_batch=sort_in_batch,\n            sort_batch=sort_batch,\n            drop_last=drop_last,\n            min_batch_size=min_batch_size,\n        )\n\n    elif type == ""numel"":\n        retval = NumElementsBatchSampler(\n            batch_bins=batch_bins,\n            shape_files=shape_files,\n            sort_in_batch=sort_in_batch,\n            sort_batch=sort_batch,\n            drop_last=drop_last,\n            padding=padding,\n            min_batch_size=min_batch_size,\n        )\n\n    elif type == ""length"":\n        retval = LengthBatchSampler(\n            batch_bins=batch_bins,\n            shape_files=shape_files,\n            sort_in_batch=sort_in_batch,\n            sort_batch=sort_batch,\n            drop_last=drop_last,\n            padding=padding,\n            min_batch_size=min_batch_size,\n        )\n\n    else:\n        raise ValueError(f""Not supported: {type}"")\n    assert check_return_type(retval)\n    return retval\n'"
espnet2/samplers/folded_batch_sampler.py,0,"b'from typing import Iterator\nfrom typing import List\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nfrom typeguard import check_argument_types\n\nfrom espnet2.samplers.abs_sampler import AbsSampler\nfrom espnet2.utils.fileio import load_num_sequence_text\n\n\nclass FoldedBatchSampler(AbsSampler):\n    def __init__(\n        self,\n        batch_size: int,\n        shape_files: Union[Tuple[str, ...], List[str]],\n        fold_lengths: Sequence[int],\n        min_batch_size: int = 1,\n        sort_in_batch: str = ""descending"",\n        sort_batch: str = ""ascending"",\n        drop_last: bool = False,\n    ):\n        assert check_argument_types()\n        assert batch_size > 0\n        if sort_batch != ""ascending"" and sort_batch != ""descending"":\n            raise ValueError(\n                f""sort_batch must be ascending or descending: {sort_batch}""\n            )\n        if sort_in_batch != ""descending"" and sort_in_batch != ""ascending"":\n            raise ValueError(\n                f""sort_in_batch must be ascending or descending: {sort_in_batch}""\n            )\n\n        self.batch_size = batch_size\n        self.shape_files = shape_files\n        self.sort_in_batch = sort_in_batch\n        self.sort_batch = sort_batch\n        self.drop_last = drop_last\n\n        # utt2shape: (Length, ...)\n        #    uttA 100,...\n        #    uttB 201,...\n        utt2shapes = [\n            load_num_sequence_text(s, loader_type=""csv_int"") for s in shape_files\n        ]\n\n        first_utt2shape = utt2shapes[0]\n        for s, d in zip(shape_files, utt2shapes):\n            if set(d) != set(first_utt2shape):\n                raise RuntimeError(\n                    f""keys are mismatched between {s} != {shape_files[0]}""\n                )\n\n        # Sort samples in ascending order\n        # (shape order should be like (Length, Dim))\n        keys = sorted(first_utt2shape, key=lambda k: first_utt2shape[k][0])\n        if len(keys) == 0:\n            raise RuntimeError(f""0 lines found: {shape_files[0]}"")\n\n        # Decide batch-sizes\n        start = 0\n        batch_sizes = []\n        while True:\n            k = keys[start]\n            factor = max(int(d[k][0] / m) for d, m in zip(utt2shapes, fold_lengths))\n            bs = max(min_batch_size, int(batch_size / (1 + factor)))\n            if self.drop_last and start + bs > len(keys):\n                # This if-block avoids 0-batches\n                if len(self.batch_list) > 0:\n                    break\n\n            bs = min(len(keys) - start, bs)\n            batch_sizes.append(bs)\n            start += bs\n            if start >= len(keys):\n                break\n\n        if len(batch_sizes) == 0:\n            # Maybe we can\'t reach here\n            raise RuntimeError(""0 batches"")\n\n        # If the last batch-size is smaller than minimum batch_size,\n        # the samples are redistributed to the other mini-batches\n        if len(batch_sizes) > 1 and batch_sizes[-1] < min_batch_size:\n            for i in range(batch_sizes.pop(-1)):\n                batch_sizes[-(i % len(batch_sizes)) - 2] += 1\n\n        if not self.drop_last:\n            # Bug check\n            assert sum(batch_sizes) == len(keys), f""{sum(batch_sizes)} != {len(keys)}""\n\n        # Set mini-batch\n        self.batch_list = []\n        start = 0\n        for bs in batch_sizes:\n            assert len(keys) >= start + bs, ""Bug""\n            minibatch_keys = keys[start : start + bs]\n            start += bs\n            if sort_in_batch == ""descending"":\n                minibatch_keys.reverse()\n            elif sort_in_batch == ""ascending"":\n                # Key are already sorted in ascending\n                pass\n            else:\n                raise ValueError(\n                    f""sort_in_batch must be ascending or descending: {sort_in_batch}""\n                )\n            self.batch_list.append(tuple(minibatch_keys))\n\n        if sort_batch == ""ascending"":\n            pass\n        elif sort_batch == ""descending"":\n            self.batch_list.reverse()\n        else:\n            raise ValueError(\n                f""sort_batch must be ascending or descending: {sort_batch}""\n            )\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(""\n            f""N-batch={len(self)}, ""\n            f""batch_size={self.batch_size}, ""\n            f""shape_files={self.shape_files}, ""\n            f""sort_in_batch={self.sort_in_batch}, ""\n            f""sort_batch={self.sort_batch})""\n        )\n\n    def __len__(self):\n        return len(self.batch_list)\n\n    def __iter__(self) -> Iterator[Tuple[str, ...]]:\n        return iter(self.batch_list)\n'"
espnet2/samplers/length_batch_sampler.py,0,"b'from typing import Iterator\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nfrom typeguard import check_argument_types\n\nfrom espnet2.samplers.abs_sampler import AbsSampler\nfrom espnet2.utils.fileio import load_num_sequence_text\n\n\nclass LengthBatchSampler(AbsSampler):\n    def __init__(\n        self,\n        batch_bins: int,\n        shape_files: Union[Tuple[str, ...], List[str]],\n        min_batch_size: int = 1,\n        sort_in_batch: str = ""descending"",\n        sort_batch: str = ""ascending"",\n        drop_last: bool = False,\n        padding: bool = True,\n    ):\n        assert check_argument_types()\n        assert batch_bins > 0\n        if sort_batch != ""ascending"" and sort_batch != ""descending"":\n            raise ValueError(\n                f""sort_batch must be ascending or descending: {sort_batch}""\n            )\n        if sort_in_batch != ""descending"" and sort_in_batch != ""ascending"":\n            raise ValueError(\n                f""sort_in_batch must be ascending or descending: {sort_in_batch}""\n            )\n\n        self.batch_bins = batch_bins\n        self.shape_files = shape_files\n        self.sort_in_batch = sort_in_batch\n        self.sort_batch = sort_batch\n        self.drop_last = drop_last\n\n        # utt2shape: (Length, ...)\n        #    uttA 100,...\n        #    uttB 201,...\n        utt2shapes = [\n            load_num_sequence_text(s, loader_type=""csv_int"") for s in shape_files\n        ]\n\n        first_utt2shape = utt2shapes[0]\n        for s, d in zip(shape_files, utt2shapes):\n            if set(d) != set(first_utt2shape):\n                raise RuntimeError(\n                    f""keys are mismatched between {s} != {shape_files[0]}""\n                )\n\n        # Sort samples in ascending order\n        # (shape order should be like (Length, Dim))\n        keys = sorted(first_utt2shape, key=lambda k: first_utt2shape[k][0])\n        if len(keys) == 0:\n            raise RuntimeError(f""0 lines found: {shape_files[0]}"")\n\n        # Decide batch-sizes\n        start = 0\n        batch_sizes = []\n        bs = 1\n        while True:\n            # shape: (Length, dim1, dim2, ...)\n            if padding:\n                max_lengths = [\n                    max(d[keys[i]][0] for i in range(start, start + bs))\n                    for d in utt2shapes\n                ]\n                # bins = bs x max_length\n                bins = sum(bs * lg for lg in max_lengths)\n            else:\n                # bins = sum of lengths\n                bins = sum(\n                    d[keys[i]][0] for i in range(start, start + bs) for d in utt2shapes\n                )\n\n            if bins > batch_bins and bs >= min_batch_size:\n                batch_sizes.append(bs)\n                start += bs\n                bs = 1\n            else:\n                bs += 1\n            if start >= len(keys):\n                break\n\n            if start + bs > len(keys):\n                if not self.drop_last or len(batch_sizes) == 0:\n                    batch_sizes.append(len(keys) - start)\n                break\n\n        if len(batch_sizes) == 0:\n            # Maybe we can\'t reach here\n            raise RuntimeError(""0 batches"")\n\n        # If the last batch-size is smaller than minimum batch_size,\n        # the samples are redistributed to the other mini-batches\n        if len(batch_sizes) > 1 and batch_sizes[-1] < min_batch_size:\n            for i in range(batch_sizes.pop(-1)):\n                batch_sizes[-(i % len(batch_sizes)) - 2] += 1\n\n        if not self.drop_last:\n            # Bug check\n            assert sum(batch_sizes) == len(keys), f""{sum(batch_sizes)} != {len(keys)}""\n\n        # Set mini-batch\n        self.batch_list = []\n        start = 0\n        for bs in batch_sizes:\n            assert len(keys) >= start + bs, ""Bug""\n            minibatch_keys = keys[start : start + bs]\n            start += bs\n            if sort_in_batch == ""descending"":\n                minibatch_keys.reverse()\n            elif sort_in_batch == ""ascending"":\n                # Key are already sorted in ascending\n                pass\n            else:\n                raise ValueError(\n                    f""sort_in_batch must be ascending or descending: {sort_in_batch}""\n                )\n            self.batch_list.append(tuple(minibatch_keys))\n\n        if sort_batch == ""ascending"":\n            pass\n        elif sort_batch == ""descending"":\n            self.batch_list.reverse()\n        else:\n            raise ValueError(\n                f""sort_batch must be ascending or descending: {sort_batch}""\n            )\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(""\n            f""N-batch={len(self)}, ""\n            f""batch_bins={self.batch_bins}, ""\n            f""sort_in_batch={self.sort_in_batch}, ""\n            f""sort_batch={self.sort_batch})""\n        )\n\n    def __len__(self):\n        return len(self.batch_list)\n\n    def __iter__(self) -> Iterator[Tuple[str, ...]]:\n        return iter(self.batch_list)\n'"
espnet2/samplers/num_elements_batch_sampler.py,0,"b'from typing import Iterator\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nfrom typeguard import check_argument_types\n\nfrom espnet2.samplers.abs_sampler import AbsSampler\nfrom espnet2.utils.fileio import load_num_sequence_text\n\n\nclass NumElementsBatchSampler(AbsSampler):\n    def __init__(\n        self,\n        batch_bins: int,\n        shape_files: Union[Tuple[str, ...], List[str]],\n        min_batch_size: int = 1,\n        sort_in_batch: str = ""descending"",\n        sort_batch: str = ""ascending"",\n        drop_last: bool = False,\n        padding: bool = True,\n    ):\n        assert check_argument_types()\n        assert batch_bins > 0\n        if sort_batch != ""ascending"" and sort_batch != ""descending"":\n            raise ValueError(\n                f""sort_batch must be ascending or descending: {sort_batch}""\n            )\n        if sort_in_batch != ""descending"" and sort_in_batch != ""ascending"":\n            raise ValueError(\n                f""sort_in_batch must be ascending or descending: {sort_in_batch}""\n            )\n\n        self.batch_bins = batch_bins\n        self.shape_files = shape_files\n        self.sort_in_batch = sort_in_batch\n        self.sort_batch = sort_batch\n        self.drop_last = drop_last\n\n        # utt2shape: (Length, ...)\n        #    uttA 100,...\n        #    uttB 201,...\n        utt2shapes = [\n            load_num_sequence_text(s, loader_type=""csv_int"") for s in shape_files\n        ]\n\n        first_utt2shape = utt2shapes[0]\n        for s, d in zip(shape_files, utt2shapes):\n            if set(d) != set(first_utt2shape):\n                raise RuntimeError(\n                    f""keys are mismatched between {s} != {shape_files[0]}""\n                )\n\n        # Sort samples in ascending order\n        # (shape order should be like (Length, Dim))\n        keys = sorted(first_utt2shape, key=lambda k: first_utt2shape[k][0])\n        if len(keys) == 0:\n            raise RuntimeError(f""0 lines found: {shape_files[0]}"")\n\n        if padding:\n            for d, s in zip(utt2shapes, shape_files):\n                # shape: (Length, dim1, dim2, ...)\n                if not all(tuple(d[k][1:]) == tuple(d[keys[0]][1:]) for k in keys):\n                    raise RuntimeError(\n                        ""If padding=True, the feature dimension must be unified: {s}"",\n                    )\n            # If padding case, the feat-dim must be same over whole corpus,\n            # therefore the first sample is referred\n            feat_dims = [np.prod(d[keys[0]][1:]) for d in utt2shapes]\n        else:\n            feat_dims = None\n\n        # Decide batch-sizes\n        start = 0\n        batch_sizes = []\n        bs = 1\n        while True:\n            # shape: (Length, dim1, dim2, ...)\n            if padding:\n                max_lengths = [\n                    max(d[keys[i]][0] for i in range(start, start + bs))\n                    for d in utt2shapes\n                ]\n                bins = sum(bs * lg * d for lg, d in zip(max_lengths, feat_dims))\n            else:\n                bins = sum(\n                    np.prod(d[keys[i]])\n                    for i in range(start, start + bs)\n                    for d in utt2shapes\n                )\n\n            if bins > batch_bins and bs >= min_batch_size:\n                batch_sizes.append(bs)\n                start += bs\n                bs = 1\n            else:\n                bs += 1\n            if start >= len(keys):\n                break\n\n            if start + bs > len(keys):\n                if not self.drop_last or len(batch_sizes) == 0:\n                    batch_sizes.append(len(keys) - start)\n                break\n\n        if len(batch_sizes) == 0:\n            # Maybe we can\'t reach here\n            raise RuntimeError(""0 batches"")\n\n        # If the last batch-size is smaller than minimum batch_size,\n        # the samples are redistributed to the other mini-batches\n        if len(batch_sizes) > 1 and batch_sizes[-1] < min_batch_size:\n            for i in range(batch_sizes.pop(-1)):\n                batch_sizes[-(i % len(batch_sizes)) - 2] += 1\n\n        if not self.drop_last:\n            # Bug check\n            assert sum(batch_sizes) == len(keys), f""{sum(batch_sizes)} != {len(keys)}""\n\n        # Set mini-batch\n        self.batch_list = []\n        start = 0\n        for bs in batch_sizes:\n            assert len(keys) >= start + bs, ""Bug""\n            minibatch_keys = keys[start : start + bs]\n            start += bs\n            if sort_in_batch == ""descending"":\n                minibatch_keys.reverse()\n            elif sort_in_batch == ""ascending"":\n                # Key are already sorted in ascending\n                pass\n            else:\n                raise ValueError(\n                    f""sort_in_batch must be ascending or descending: {sort_in_batch}""\n                )\n            self.batch_list.append(tuple(minibatch_keys))\n\n        if sort_batch == ""ascending"":\n            pass\n        elif sort_batch == ""descending"":\n            self.batch_list.reverse()\n        else:\n            raise ValueError(\n                f""sort_batch must be ascending or descending: {sort_batch}""\n            )\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(""\n            f""N-batch={len(self)}, ""\n            f""batch_bins={self.batch_bins}, ""\n            f""sort_in_batch={self.sort_in_batch}, ""\n            f""sort_batch={self.sort_batch})""\n        )\n\n    def __len__(self):\n        return len(self.batch_list)\n\n    def __iter__(self) -> Iterator[Tuple[str, ...]]:\n        return iter(self.batch_list)\n'"
espnet2/samplers/sorted_batch_sampler.py,0,"b'import logging\nfrom typing import Iterator\nfrom typing import Tuple\n\nfrom typeguard import check_argument_types\n\nfrom espnet2.samplers.abs_sampler import AbsSampler\nfrom espnet2.utils.fileio import load_num_sequence_text\n\n\nclass SortedBatchSampler(AbsSampler):\n    """"""BatchSampler with sorted samples by length.\n\n    Args:\n        batch_size:\n        shape_file:\n        sort_in_batch: \'descending\', \'ascending\' or None.\n        sort_batch:\n    """"""\n\n    def __init__(\n        self,\n        batch_size: int,\n        shape_file: str,\n        sort_in_batch: str = ""descending"",\n        sort_batch: str = ""ascending"",\n        drop_last: bool = False,\n    ):\n        assert check_argument_types()\n        assert batch_size > 0\n        self.batch_size = batch_size\n        self.shape_file = shape_file\n        self.sort_in_batch = sort_in_batch\n        self.sort_batch = sort_batch\n        self.drop_last = drop_last\n\n        # utt2shape: (Length, ...)\n        #    uttA 100,...\n        #    uttB 201,...\n        utt2shape = load_num_sequence_text(shape_file, loader_type=""csv_int"")\n        if sort_in_batch == ""descending"":\n            # Sort samples in descending order (required by RNN)\n            keys = sorted(utt2shape, key=lambda k: -utt2shape[k][0])\n        elif sort_in_batch == ""ascending"":\n            # Sort samples in ascending order\n            keys = sorted(utt2shape, key=lambda k: utt2shape[k][0])\n        else:\n            raise ValueError(\n                f""sort_in_batch must be either one of ""\n                f""ascending, descending, or None: {sort_in_batch}""\n            )\n        if len(keys) == 0:\n            raise RuntimeError(f""0 lines found: {shape_file}"")\n\n        # Apply max(, 1) to avoid 0-batches\n        N = max(len(keys) // batch_size, 1)\n        if not self.drop_last:\n            # Split keys evenly as possible as. Note that If N != 1,\n            # the these batches always have size of batch_size at minimum.\n            self.batch_list = [\n                keys[i * len(keys) // N : (i + 1) * len(keys) // N] for i in range(N)\n            ]\n        else:\n            self.batch_list = [\n                tuple(keys[i * batch_size : (i + 1) * batch_size]) for i in range(N)\n            ]\n\n        if len(self.batch_list) == 0:\n            logging.warning(f""{shape_file} is empty"")\n\n        if sort_in_batch != sort_batch:\n            if sort_batch not in (""ascending"", ""descending""):\n                raise ValueError(\n                    f""sort_batch must be ascending or descending: {sort_batch}""\n                )\n            self.batch_list.reverse()\n\n        if len(self.batch_list) == 0:\n            raise RuntimeError(""0 batches"")\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(""\n            f""N-batch={len(self)}, ""\n            f""batch_size={self.batch_size}, ""\n            f""shape_file={self.shape_file}, ""\n            f""sort_in_batch={self.sort_in_batch}, ""\n            f""sort_batch={self.sort_batch})""\n        )\n\n    def __len__(self):\n        return len(self.batch_list)\n\n    def __iter__(self) -> Iterator[Tuple[str, ...]]:\n        return iter(self.batch_list)\n'"
espnet2/samplers/unsorted_batch_sampler.py,0,"b'import logging\nfrom typing import Iterator\nfrom typing import Tuple\n\nfrom typeguard import check_argument_types\n\nfrom espnet2.samplers.abs_sampler import AbsSampler\nfrom espnet2.utils.fileio import read_2column_text\n\n\nclass UnsortedBatchSampler(AbsSampler):\n    """"""BatchSampler with constant batch-size.\n\n    Any sorting is not done in this class,\n    so no length information is required,\n    This class is convenient for decoding mode,\n    or not seq2seq learning e.g. classification.\n\n    Args:\n        batch_size:\n        key_file:\n    """"""\n\n    def __init__(self, batch_size: int, key_file: str, drop_last: bool = False):\n        assert check_argument_types()\n        assert batch_size > 0\n        self.batch_size = batch_size\n        self.key_file = key_file\n        self.drop_last = drop_last\n\n        # utt2shape:\n        #    uttA <anything is o.k>\n        #    uttB <anything is o.k>\n        utt2any = read_2column_text(key_file)\n        if len(utt2any) == 0:\n            logging.warning(f""{key_file} is empty"")\n        # In this case the, the first column in only used\n        keys = list(utt2any)\n        if len(keys) == 0:\n            raise RuntimeError(f""0 lines found: {key_file}"")\n\n        # Apply max(, 1) to avoid 0-batches\n        N = max(len(keys) // batch_size, 1)\n        if not self.drop_last:\n            # Split keys evenly as possible as. Note that If N != 1,\n            # the these batches always have size of batch_size at minimum.\n            self.batch_list = [\n                keys[i * len(keys) // N : (i + 1) * len(keys) // N] for i in range(N)\n            ]\n        else:\n            self.batch_list = [\n                tuple(keys[i * batch_size : (i + 1) * batch_size]) for i in range(N)\n            ]\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(""\n            f""N-batch={len(self)}, ""\n            f""batch_size={self.batch_size}, ""\n            f""key_file={self.key_file}, ""\n        )\n\n    def __len__(self):\n        return len(self.batch_list)\n\n    def __iter__(self) -> Iterator[Tuple[str, ...]]:\n        return iter(self.batch_list)\n'"
espnet2/schedulers/__init__.py,0,b''
espnet2/schedulers/abs_scheduler.py,2,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom distutils.version import LooseVersion\n\nimport torch\nimport torch.optim.lr_scheduler as L\n\n\nclass AbsScheduler(ABC):\n    @abstractmethod\n    def step(self, epoch: int = None):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state):\n        pass\n\n\n# If you need to define custom scheduler, please inherit these classes\nclass AbsBatchStepScheduler(AbsScheduler):\n    @abstractmethod\n    def step(self, epoch: int = None):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state):\n        pass\n\n\nclass AbsEpochStepScheduler(AbsScheduler):\n    @abstractmethod\n    def step(self, epoch: int = None):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state):\n        pass\n\n\nclass AbsValEpochStepScheduler(AbsEpochStepScheduler):\n    @abstractmethod\n    def step(self, val, epoch: int = None):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state):\n        pass\n\n\n# Create alias type to check the type\n# Note(kamo): Currently PyTorch doesn\'t provide the base class\n# to judge these classes.\nAbsValEpochStepScheduler.register(L.ReduceLROnPlateau)\nfor s in [\n    L.ReduceLROnPlateau,\n    L.LambdaLR,\n    L.StepLR,\n    L.MultiStepLR,\n    L.MultiStepLR,\n    L.ExponentialLR,\n    L.CosineAnnealingLR,\n]:\n    AbsEpochStepScheduler.register(s)\nif LooseVersion(torch.__version__) >= LooseVersion(""1.3.0""):\n    for s in [L.CyclicLR, L.OneCycleLR, L.CosineAnnealingWarmRestarts]:\n        AbsBatchStepScheduler.register(s)\n'"
espnet2/schedulers/noam_lr.py,4,"b'from distutils.version import LooseVersion\nfrom typing import Union\nimport warnings\n\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom typeguard import check_argument_types\n\nfrom espnet2.schedulers.abs_scheduler import AbsBatchStepScheduler\n\n\nclass NoamLR(_LRScheduler, AbsBatchStepScheduler):\n    """"""The LR scheduler proposed by Noam\n\n    Ref:\n        ""Attention Is All You Need"", https://arxiv.org/pdf/1706.03762.pdf\n\n    FIXME(kamo): PyTorch doesn\'t provide _LRScheduler as public class,\n     thus the behaviour isn\'t guaranteed at forward PyTorch version.\n\n    NOTE(kamo): The ""model_size"" in original implementation is derived from\n     the model, but in this implementation, this parameter is a constant value.\n     You need to change it if the model is changed.\n\n    """"""\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        model_size: Union[int, float] = 320,\n        warmup_steps: Union[int, float] = 25000,\n        last_epoch: int = -1,\n    ):\n        if LooseVersion(torch.__version__) < LooseVersion(""1.1.0""):\n            raise NotImplementedError(f""Require PyTorch>=1.1.0: {torch.__version__}"")\n        assert check_argument_types()\n        self.model_size = model_size\n        self.warmup_steps = warmup_steps\n\n        lr = list(optimizer.param_groups)[0][""lr""]\n        new_lr = self.lr_for_WarmupLR(lr)\n        warnings.warn(\n            f""NoamLR is deprecated. ""\n            f""Use WarmupLR(warmup_steps={warmup_steps}) with Optimizer(lr={new_lr})"",\n        )\n\n        # __init__() must be invoked before setting field\n        # because step() is also invoked in __init__()\n        super().__init__(optimizer, last_epoch)\n\n    def lr_for_WarmupLR(self, lr: float) -> float:\n        return lr / self.model_size ** 0.5 / self.warmup_steps ** 0.5\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(model_size={self.model_size}, ""\n            f""warmup_steps={self.warmup_steps})""\n        )\n\n    def get_lr(self):\n        step_num = self.last_epoch + 1\n        return [\n            lr\n            * self.model_size ** -0.5\n            * min(step_num ** -0.5, step_num * self.warmup_steps ** -1.5)\n            for lr in self.base_lrs\n        ]\n'"
espnet2/schedulers/warmup_lr.py,4,"b'from distutils.version import LooseVersion\nfrom typing import Union\n\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom typeguard import check_argument_types\n\nfrom espnet2.schedulers.abs_scheduler import AbsBatchStepScheduler\n\n\nclass WarmupLR(_LRScheduler, AbsBatchStepScheduler):\n    """"""The WarmupLR scheduler\n\n    This scheduler is almost same as NoamLR Scheduler except for following difference:\n\n    NoamLR:\n        lr = optimizer.lr * model_size ** -0.5\n             * min(step ** -0.5, step * warmup_step ** -1.5)\n    WarmupLR:\n        lr = optimizer.lr * warmup_step ** 0.5\n             * min(step ** -0.5, step * warmup_step ** -1.5)\n\n    Note that the maximum lr equals to optimizer.lr in this scheduler.\n\n    """"""\n\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        warmup_steps: Union[int, float] = 25000,\n        last_epoch: int = -1,\n    ):\n        if LooseVersion(torch.__version__) < LooseVersion(""1.1.0""):\n            raise NotImplementedError(f""Require PyTorch>=1.1.0: {torch.__version__}"")\n\n        assert check_argument_types()\n        self.warmup_steps = warmup_steps\n\n        # __init__() must be invoked before setting field\n        # because step() is also invoked in __init__()\n        super().__init__(optimizer, last_epoch)\n\n    def __repr__(self):\n        return f""{self.__class__.__name__}(warmup_steps={self.warmup_steps})""\n\n    def get_lr(self):\n        step_num = self.last_epoch + 1\n        return [\n            lr\n            * self.warmup_steps ** 0.5\n            * min(step_num ** -0.5, step_num * self.warmup_steps ** -1.5)\n            for lr in self.base_lrs\n        ]\n'"
espnet2/tasks/__init__.py,0,b''
espnet2/tasks/abs_task.py,51,"b'from abc import ABC\nfrom abc import abstractmethod\nimport argparse\nfrom distutils.version import LooseVersion\nimport logging\nimport os\nfrom pathlib import Path\nimport sys\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\n\nimport configargparse\nimport humanfriendly\nimport numpy as np\nimport torch\nimport torch.multiprocessing\nimport torch.nn\nimport torch.optim\nfrom torch.utils.data import DataLoader\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\nimport yaml\n\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet2.iterators.abs_iter_factory import AbsIterFactory\nfrom espnet2.iterators.chunk_iter_factory import ChunkIterFactory\nfrom espnet2.iterators.sequence_iter_factory import SequenceIterFactory\nfrom espnet2.main_funcs.average_nbest_models import average_nbest_models\nfrom espnet2.main_funcs.collect_stats import collect_stats\nfrom espnet2.optimizers.sgd import SGD\nfrom espnet2.samplers.build_batch_sampler import BATCH_TYPES\nfrom espnet2.samplers.build_batch_sampler import build_batch_sampler\nfrom espnet2.samplers.unsorted_batch_sampler import UnsortedBatchSampler\nfrom espnet2.schedulers.noam_lr import NoamLR\nfrom espnet2.schedulers.warmup_lr import WarmupLR\nfrom espnet2.torch_utils.load_pretrained_model import load_pretrained_model\nfrom espnet2.torch_utils.pytorch_version import pytorch_cudnn_version\nfrom espnet2.torch_utils.set_all_random_seed import set_all_random_seed\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\nfrom espnet2.train.class_choices import ClassChoices\nfrom espnet2.train.dataset import DATA_TYPES\nfrom espnet2.train.dataset import ESPnetDataset\nfrom espnet2.train.distributed_utils import DistributedOption\nfrom espnet2.train.distributed_utils import free_port\nfrom espnet2.train.distributed_utils import get_master_port\nfrom espnet2.train.distributed_utils import get_node_rank\nfrom espnet2.train.distributed_utils import get_num_nodes\nfrom espnet2.train.distributed_utils import resolve_distributed_mode\nfrom espnet2.train.reporter import Reporter\nfrom espnet2.train.trainer import Trainer\nfrom espnet2.utils.build_dataclass import build_dataclass\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\nfrom espnet2.utils.nested_dict_action import NestedDictAction\nfrom espnet2.utils.types import humanfriendly_parse_size_or_none\nfrom espnet2.utils.types import int_or_none\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str2triple_str\nfrom espnet2.utils.types import str_or_int\nfrom espnet2.utils.types import str_or_none\nfrom espnet2.utils.yaml_no_alias_safe_dump import yaml_no_alias_safe_dump\n\nif LooseVersion(torch.__version__) >= LooseVersion(""1.5.0""):\n    from torch.multiprocessing.spawn import ProcessContext\nelse:\n    from torch.multiprocessing.spawn import SpawnContext as ProcessContext\n\n\noptim_classes = dict(\n    adam=torch.optim.Adam,\n    sgd=SGD,\n    adadelta=torch.optim.Adadelta,\n    adagrad=torch.optim.Adagrad,\n    adamax=torch.optim.Adamax,\n    asgd=torch.optim.ASGD,\n    lbfgs=torch.optim.LBFGS,\n    rmsprop=torch.optim.RMSprop,\n    rprop=torch.optim.Rprop,\n)\nif LooseVersion(torch.__version__) >= LooseVersion(""1.2.0""):\n    optim_classes[""adamw""] = torch.optim.AdamW\ntry:\n    import torch_optimizer\n\n    optim_classes.update(\n        accagd=torch_optimizer.AccSGD,\n        adabound=torch_optimizer.AdaBound,\n        adamod=torch_optimizer.AdaMod,\n        diffgrad=torch_optimizer.DiffGrad,\n        lamb=torch_optimizer.Lamb,\n        novograd=torch_optimizer.NovoGrad,\n        pid=torch_optimizer.PID,\n        # torch_optimizer<=0.0.1a10 doesn\'t support\n        # qhadam=torch_optimizer.QHAdam,\n        qhm=torch_optimizer.QHM,\n        radam=torch_optimizer.RAdam,\n        sgdw=torch_optimizer.SGDW,\n        yogi=torch_optimizer.Yogi,\n    )\n    del torch_optimizer\nexcept ImportError:\n    pass\ntry:\n    import apex\n\n    optim_classes.update(\n        fusedadam=apex.optimizers.FusedAdam,\n        fusedlamb=apex.optimizers.FusedLAMB,\n        fusednovograd=apex.optimizers.FusedNovoGrad,\n        fusedsgd=apex.optimizers.FusedSGD,\n    )\n    del apex\nexcept ImportError:\n    pass\n\nscheduler_classes = dict(\n    ReduceLROnPlateau=torch.optim.lr_scheduler.ReduceLROnPlateau,\n    lambdalr=torch.optim.lr_scheduler.LambdaLR,\n    steplr=torch.optim.lr_scheduler.StepLR,\n    multisteplr=torch.optim.lr_scheduler.MultiStepLR,\n    exponentiallr=torch.optim.lr_scheduler.ExponentialLR,\n    CosineAnnealingLR=torch.optim.lr_scheduler.CosineAnnealingLR,\n)\nif LooseVersion(torch.__version__) >= LooseVersion(""1.1.0""):\n    scheduler_classes.update(\n        noamlr=NoamLR, warmuplr=WarmupLR,\n    )\nif LooseVersion(torch.__version__) >= LooseVersion(""1.3.0""):\n    CosineAnnealingWarmRestarts = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n    scheduler_classes.update(\n        cycliclr=torch.optim.lr_scheduler.CyclicLR,\n        onecyclelr=torch.optim.lr_scheduler.OneCycleLR,\n        CosineAnnealingWarmRestarts=CosineAnnealingWarmRestarts,\n    )\n# To lower keys\noptim_classes = {k.lower(): v for k, v in optim_classes.items()}\nscheduler_classes = {k.lower(): v for k, v in scheduler_classes.items()}\n\n\nclass AbsTask(ABC):\n    # Use @staticmethod, or @classmethod,\n    # instead of instance method to avoid God classes\n\n    # If you need more than one optimizers, change this value in inheritance\n    num_optimizers: int = 1\n    trainer = Trainer\n    class_choices_list: List[ClassChoices] = []\n\n    def __init__(self):\n        raise RuntimeError(""This class can\'t be instantiated."")\n\n    @classmethod\n    @abstractmethod\n    def add_task_arguments(cls, parser: argparse.ArgumentParser):\n        pass\n\n    @classmethod\n    @abstractmethod\n    def build_collate_fn(\n        cls, args: argparse.Namespace\n    ) -> Callable[[Sequence[Dict[str, np.ndarray]]], Dict[str, torch.Tensor]]:\n        """"""Return ""collate_fn"", which is a callable object and given to DataLoader.\n\n        >>> from torch.utils.data import DataLoader\n        >>> loader = DataLoader(collate_fn=cls.build_collate_fn(args), ...)\n\n        In many cases, you can use our common collate_fn.\n        """"""\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def build_preprocess_fn(\n        cls, args: argparse.Namespace, train: bool\n    ) -> Optional[Callable[[str, Dict[str, np.array]], Dict[str, np.ndarray]]]:\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def required_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        """"""Define the required names by Task\n\n        This function is used by\n        >>> cls.check_task_requirements()\n        If your model is defined as following,\n\n        >>> from espnet2.train.abs_espnet_model import AbsESPnetModel\n        >>> class Model(AbsESPnetModel):\n        ...     def forward(self, input, output, opt=None):  pass\n\n        then ""required_data_names"" should be as\n\n        >>> required_data_names = (\'input\', \'output\')\n        """"""\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def optional_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        """"""Define the optional names by Task\n\n        This function is used by\n        >>> cls.check_task_requirements()\n        If your model is defined as following,\n\n        >>> from espnet2.train.abs_espnet_model import AbsESPnetModel\n        >>> class Model(AbsESPnetModel):\n        ...     def forward(self, input, output, opt=None):  pass\n\n        then ""optional_data_names"" should be as\n\n        >>> optional_data_names = (\'opt\',)\n        """"""\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def build_model(cls, args: argparse.Namespace) -> AbsESPnetModel:\n        raise NotImplementedError\n\n    @classmethod\n    def get_parser(cls) -> configargparse.ArgumentParser:\n        assert check_argument_types()\n\n        class ArgumentDefaultsRawTextHelpFormatter(\n            configargparse.RawTextHelpFormatter,\n            configargparse.ArgumentDefaultsHelpFormatter,\n        ):\n            pass\n\n        parser = configargparse.ArgumentParser(\n            description=""base parser"",\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\n            formatter_class=ArgumentDefaultsRawTextHelpFormatter,\n        )\n\n        # NOTE(kamo): Use \'_\' instead of \'-\' to avoid confusion.\n        #  I think \'-\' looks really confusing if it\'s written in yaml.\n\n        # NOTE(kamo): add_arguments(..., required=True) can\'t be used\n        #  to provide --print_config mode. Instead of it, do as\n        parser.set_defaults(required=[""output_dir""])\n\n        group = parser.add_argument_group(""Common configuration"")\n\n        group.add_argument(""--config"", is_config_file=True, help=""config file path"")\n        group.add_argument(\n            ""--print_config"",\n            action=""store_true"",\n            help=""Print the config file and exit"",\n        )\n        group.add_argument(\n            ""--log_level"",\n            type=lambda x: x.upper(),\n            default=""INFO"",\n            choices=(""ERROR"", ""WARNING"", ""INFO"", ""DEBUG"", ""NOTSET""),\n            help=""The verbose level of logging"",\n        )\n        group.add_argument(\n            ""--dry_run"",\n            type=str2bool,\n            default=False,\n            help=""Perform process without training"",\n        )\n        group.add_argument(\n            ""--iterator_type"",\n            type=str,\n            choices=[""sequence"", ""none""],\n            default=""sequence"",\n            help=""Specify iterator type"",\n        )\n\n        group.add_argument(""--output_dir"", type=str_or_none, default=None)\n        group.add_argument(\n            ""--ngpu"",\n            type=int,\n            default=0,\n            help=""The number of gpus. 0 indicates CPU mode"",\n        )\n        group.add_argument(""--seed"", type=int, default=0, help=""Random seed"")\n        group.add_argument(\n            ""--num_workers"",\n            type=int,\n            default=1,\n            help=""The number of workers used for DataLoader"",\n        )\n        group.add_argument(\n            ""--num_att_plot"",\n            type=int,\n            default=3,\n            help=""The number images to plot the outputs from attention. ""\n            ""This option makes sense only when attention-based model"",\n        )\n\n        group = parser.add_argument_group(""distributed training related"")\n        group.add_argument(\n            ""--dist_backend"", default=""nccl"", type=str, help=""distributed backend"",\n        )\n        group.add_argument(\n            ""--dist_init_method"",\n            type=str,\n            default=""env://"",\n            help=\'if init_method=""env://"", env values of ""MASTER_PORT"", ""MASTER_ADDR"", \'\n            \'""WORLD_SIZE"", and ""RANK"" are referred.\',\n        )\n        group.add_argument(\n            ""--dist_world_size"",\n            default=None,\n            type=int_or_none,\n            help=""number of nodes for distributed training"",\n        )\n        group.add_argument(\n            ""--dist_rank"",\n            type=int_or_none,\n            default=None,\n            help=""node rank for distributed training"",\n        )\n        group.add_argument(\n            # Not starting with ""dist_"" for compatibility to launch.py\n            ""--local_rank"",\n            type=int_or_none,\n            default=None,\n            help=""local rank for distributed training. This option is used if ""\n            ""--multiprocessing_distributed=false"",\n        )\n        group.add_argument(\n            ""--dist_master_addr"",\n            default=None,\n            type=str_or_none,\n            help=""The master address for distributed training. ""\n            ""This value is used when dist_init_method == \'env://\'"",\n        )\n        group.add_argument(\n            ""--dist_master_port"",\n            default=None,\n            type=int_or_none,\n            help=""The master port for distributed training""\n            ""This value is used when dist_init_method == \'env://\'"",\n        )\n        group.add_argument(\n            ""--dist_launcher"",\n            default=None,\n            type=str_or_none,\n            choices=[""slurm"", ""mpi"", None],\n            help=""The launcher type for distributed training"",\n        )\n        group.add_argument(\n            ""--multiprocessing_distributed"",\n            default=False,\n            type=str2bool,\n            help=""Use multi-processing distributed training to launch ""\n            ""N processes per node, which has N GPUs. This is the ""\n            ""fastest way to use PyTorch for either single node or ""\n            ""multi node data parallel training"",\n        )\n\n        group = parser.add_argument_group(""cudnn mode related"")\n        group.add_argument(\n            ""--cudnn_enabled"",\n            type=str2bool,\n            default=torch.backends.cudnn.enabled,\n            help=""Enable CUDNN"",\n        )\n        group.add_argument(\n            ""--cudnn_benchmark"",\n            type=str2bool,\n            default=torch.backends.cudnn.benchmark,\n            help=""Enable cudnn-benchmark mode"",\n        )\n        group.add_argument(\n            ""--cudnn_deterministic"",\n            type=str2bool,\n            default=True,\n            help=""Enable cudnn-deterministic mode"",\n        )\n\n        group = parser.add_argument_group(""collect stats mode related"")\n        group.add_argument(\n            ""--collect_stats"",\n            type=str2bool,\n            default=False,\n            help=\'Perform on ""collect stats"" mode\',\n        )\n        group.add_argument(\n            ""--write_collected_feats"",\n            type=str2bool,\n            default=False,\n            help=\'Write the output features from the model when ""collect stats"" mode\',\n        )\n\n        group = parser.add_argument_group(""Trainer related"")\n        group.add_argument(\n            ""--max_epoch"",\n            type=int,\n            default=40,\n            help=""The maximum number epoch to train"",\n        )\n        group.add_argument(\n            ""--patience"",\n            type=int_or_none,\n            default=None,\n            help=""Number of epochs to wait without improvement ""\n            ""before stopping the training"",\n        )\n        group.add_argument(\n            ""--val_scheduler_criterion"",\n            type=str,\n            nargs=2,\n            default=(""valid"", ""loss""),\n            help=""The criterion used for the value given to the lr scheduler. ""\n            \'Give a pair referring the phase, ""train"" or ""valid"",\'\n            \'and the criterion name. The mode specifying ""min"" or ""max"" can \'\n            ""be changed by --scheduler_conf"",\n        )\n        group.add_argument(\n            ""--early_stopping_criterion"",\n            type=str,\n            nargs=3,\n            default=(""valid"", ""loss"", ""min""),\n            help=""The criterion used for judging of early stopping. ""\n            \'Give a pair referring the phase, ""train"" or ""valid"",\'\n            \'the criterion name and the mode, ""min"" or ""max"", e.g. ""acc,max"".\',\n        )\n        group.add_argument(\n            ""--best_model_criterion"",\n            type=str2triple_str,\n            nargs=""+"",\n            default=[\n                (""train"", ""loss"", ""min""),\n                (""valid"", ""loss"", ""min""),\n                (""train"", ""acc"", ""max""),\n                (""valid"", ""acc"", ""max""),\n            ],\n            help=""The criterion used for judging of the best model. ""\n            \'Give a pair referring the phase, ""train"" or ""valid"",\'\n            \'the criterion name, and the mode, ""min"" or ""max"", e.g. ""acc,max"".\',\n        )\n        group.add_argument(\n            ""--keep_nbest_models"",\n            type=int,\n            default=10,\n            help=""Remove previous snapshots excluding the n-best scored epochs"",\n        )\n        group.add_argument(\n            ""--grad_clip"",\n            type=float,\n            default=5.0,\n            help=""Gradient norm threshold to clip"",\n        )\n        group.add_argument(\n            ""--grad_noise"",\n            type=str2bool,\n            default=False,\n            help=""The flag to switch to use noise injection to ""\n            ""gradients during training"",\n        )\n        group.add_argument(\n            ""--accum_grad"",\n            type=int,\n            default=1,\n            help=""The number of gradient accumulation"",\n        )\n        group.add_argument(\n            ""--no_forward_run"",\n            type=str2bool,\n            default=False,\n            help=""Just only iterating data loading without ""\n            ""model forwarding and training"",\n        )\n        group.add_argument(\n            ""--resume"",\n            type=str2bool,\n            default=False,\n            help=""Enable resuming if checkpoint is existing"",\n        )\n        group.add_argument(\n            ""--train_dtype"",\n            default=""float32"",\n            choices=[""float16"", ""float32"", ""float64"", ""O0"", ""O1"", ""O2"", ""O3""],\n            help=""Data type for training. O0,O1,.. flags require apex. ""\n            ""See https://nvidia.github.io/apex/amp.html#opt-levels"",\n        )\n        group.add_argument(\n            ""--log_interval"",\n            type=int_or_none,\n            default=None,\n            help=""Show the logs every the number iterations in each epochs at the ""\n            ""training phase. If None is given, it is decided according the number ""\n            ""of training samples automatically ."",\n        )\n\n        group = parser.add_argument_group(""Pretraining model related"")\n        group.add_argument(""--pretrain_path"", type=str, default=[], nargs=""*"")\n        group.add_argument(""--pretrain_key"", type=str_or_none, default=[], nargs=""*"")\n\n        group = parser.add_argument_group(""BatchSampler related"")\n        group.add_argument(\n            ""--num_iters_per_epoch"",\n            type=int_or_none,\n            default=None,\n            help=""Restrict the number of iterations for training per epoch"",\n        )\n        group.add_argument(\n            ""--batch_size"",\n            type=int,\n            default=20,\n            help=""The mini-batch size used for training. Used if batch_type=\'unsorted\',""\n            "" \'sorted\', or \'folded\'."",\n        )\n        group.add_argument(\n            ""--valid_batch_size"",\n            type=int_or_none,\n            default=None,\n            help=""If not given, the value of --batch_size is used"",\n        )\n        group.add_argument(\n            ""--batch_bins"",\n            type=int,\n            default=1000000,\n            help=""The number of batch bins. Used if batch_type=\'length\' or \'numel\'"",\n        )\n        group.add_argument(\n            ""--valid_batch_bins"",\n            type=int_or_none,\n            default=None,\n            help=""If not given, the value of --batch_bins is used"",\n        )\n\n        group.add_argument(""--train_shape_file"", type=str, action=""append"", default=[])\n        group.add_argument(""--valid_shape_file"", type=str, action=""append"", default=[])\n\n        group = parser.add_argument_group(""Sequence iterator related"")\n        _batch_type_help = """"\n        for key, value in BATCH_TYPES.items():\n            _batch_type_help += f\'""{key}"":\\n{value}\\n\'\n        group.add_argument(\n            ""--batch_type"",\n            type=str,\n            default=""folded"",\n            choices=list(BATCH_TYPES),\n            help=_batch_type_help,\n        )\n        group.add_argument(\n            ""--valid_batch_type"",\n            type=str_or_none,\n            default=None,\n            choices=list(BATCH_TYPES) + [None],\n            help=""If not given, the value of --batch_type is used"",\n        )\n        group.add_argument(""--fold_length"", type=int, action=""append"", default=[])\n        group.add_argument(\n            ""--sort_in_batch"",\n            type=str,\n            default=""descending"",\n            choices=[""descending"", ""ascending""],\n            help=""Sort the samples in each mini-batches by the sample ""\n            \'lengths. To enable this, ""shape_file"" must have the length information.\',\n        )\n        group.add_argument(\n            ""--sort_batch"",\n            type=str,\n            default=""descending"",\n            choices=[""descending"", ""ascending""],\n            help=""Sort mini-batches by the sample lengths"",\n        )\n\n        group = parser.add_argument_group(""Chunk iterator related"")\n        group.add_argument(\n            ""--chunk_length"",\n            type=str_or_int,\n            default=500,\n            help=""Specify chunk length. e.g. \'300\', \'300,400,500\', or \'300-400\'.""\n            ""If multiple numbers separated by command are given, ""\n            ""one of them is selected randomly for each samples. ""\n            ""If two numbers are given with \'-\', it indicates the range of the choices. ""\n            ""Note that if the sequence length is shorter than the all chunk_lengths, ""\n            ""the sample is discarded. "",\n        )\n        group.add_argument(\n            ""--chunk_shift_ratio"",\n            type=float,\n            default=0.5,\n            help=""Specify the shift width of chunks. If it\'s less than 1, ""\n            ""allows the overlapping and if bigger than 1, there are some gaps ""\n            ""between each chunk."",\n        )\n        group.add_argument(\n            ""--num_cache_chunks"",\n            type=int,\n            default=1024,\n            help=""Shuffle in the specified number of chunks and generate mini-batches ""\n            ""More larger this value, more randomness can be obtained."",\n        )\n\n        group = parser.add_argument_group(""Dataset related"")\n        _data_path_and_name_and_type_help = (\n            ""Give three words splitted by comma. It\'s used for the training data. ""\n            ""e.g. \'--train_data_path_and_name_and_type some/path/a.scp,foo,sound\'. ""\n            ""The first value, some/path/a.scp, indicates the file path, ""\n            ""and the second, foo, is the key name used for the mini-batch data, ""\n            ""and the last, sound, decides the file type. ""\n            ""This option is repeatable, so you can input any number of features ""\n            ""for your task. Supported file types are as follows:\\n\\n""\n        )\n        for key, dic in DATA_TYPES.items():\n            _data_path_and_name_and_type_help += f\'""{key}"":\\n{dic[""help""]}\\n\\n\'\n\n        group.add_argument(\n            ""--train_data_path_and_name_and_type"",\n            type=str2triple_str,\n            action=""append"",\n            default=[],\n            help=_data_path_and_name_and_type_help,\n        )\n        group.add_argument(\n            ""--valid_data_path_and_name_and_type"",\n            type=str2triple_str,\n            action=""append"",\n            default=[],\n        )\n        group.add_argument(\n            ""--allow_variable_data_keys"",\n            type=str2bool,\n            default=False,\n            help=""Allow the arbitrary keys for mini-batch with ignoring ""\n            ""the task requirements"",\n        )\n        group.add_argument(\n            ""--max_cache_size"",\n            type=humanfriendly.parse_size,\n            default=0.0,\n            help=""The maximum cache size for data loader. e.g. 10MB, 20GB."",\n        )\n        group.add_argument(\n            ""--valid_max_cache_size"",\n            type=humanfriendly_parse_size_or_none,\n            default=None,\n            help=""The maximum cache size for validation data loader. e.g. 10MB, 20GB. ""\n            ""If None, the 5 percent size of --max_cache_size"",\n        )\n\n        group = parser.add_argument_group(""Optimizer related"")\n        for i in range(1, cls.num_optimizers + 1):\n            suf = """" if i == 1 else str(i)\n            group.add_argument(\n                f""--optim{suf}"",\n                type=lambda x: x.lower(),\n                default=""adadelta"",\n                choices=list(optim_classes),\n                help=""The optimizer type"",\n            )\n            group.add_argument(\n                f""--optim{suf}_conf"",\n                action=NestedDictAction,\n                default=dict(),\n                help=""The keyword arguments for optimizer"",\n            )\n            group.add_argument(\n                f""--scheduler{suf}"",\n                type=lambda x: str_or_none(x.lower()),\n                default=None,\n                choices=list(scheduler_classes) + [None],\n                help=""The lr scheduler type"",\n            )\n            group.add_argument(\n                f""--scheduler{suf}_conf"",\n                action=NestedDictAction,\n                default=dict(),\n                help=""The keyword arguments for lr scheduler"",\n            )\n\n        cls.trainer.add_arguments(parser)\n        cls.add_task_arguments(parser)\n\n        assert check_return_type(parser)\n        return parser\n\n    @classmethod\n    def build_optimizers(\n        cls, args: argparse.Namespace, model: torch.nn.Module,\n    ) -> List[torch.optim.Optimizer]:\n        if cls.num_optimizers != 1:\n            raise RuntimeError(\n                ""build_optimizers() must be overridden if num_optimizers != 1""\n            )\n\n        optim_class = optim_classes.get(args.optim)\n        if optim_class is None:\n            raise ValueError(f""must be one of {list(optim_classes)}: {args.optim}"")\n        optim = optim_class(model.parameters(), **args.optim_conf)\n        optimizers = [optim]\n        return optimizers\n\n    @classmethod\n    def exclude_opts(cls) -> Tuple[str, ...]:\n        """"""The options not to be shown by --print_config""""""\n        return ""required"", ""print_config"", ""config"", ""ngpu""\n\n    @classmethod\n    def get_default_config(cls) -> Dict[str, Any]:\n        """"""Return the configuration as dict.\n\n        This method is used by print_config()\n        """"""\n\n        def get_class_type(name: str, classes: dict):\n            _cls = classes.get(name)\n            if _cls is None:\n                raise ValueError(f""must be one of {list(classes)}: {name}"")\n            return _cls\n\n        # This method is used only for --print_config\n        assert check_argument_types()\n        parser = cls.get_parser()\n        args, _ = parser.parse_known_args()\n        config = vars(args)\n        # Excludes the options not to be shown\n        for k in AbsTask.exclude_opts():\n            config.pop(k)\n\n        for i in range(1, cls.num_optimizers + 1):\n            suf = """" if i == 1 else str(i)\n            name = config[f""optim{suf}""]\n            optim_class = get_class_type(name, optim_classes)\n            conf = get_default_kwargs(optim_class)\n            # Overwrite the default by the arguments,\n            conf.update(config[f""optim{suf}_conf""])\n            # and set it again\n            config[f""optim{suf}_conf""] = conf\n\n            name = config[f""scheduler{suf}""]\n            if name is not None:\n                scheduler_class = get_class_type(name, scheduler_classes)\n                conf = get_default_kwargs(scheduler_class)\n                # Overwrite the default by the arguments,\n                conf.update(config[f""scheduler{suf}_conf""])\n                # and set it again\n                config[f""scheduler{suf}_conf""] = conf\n\n        for class_choices in cls.class_choices_list:\n            if getattr(args, class_choices.name) is not None:\n                class_obj = class_choices.get_class(getattr(args, class_choices.name))\n                conf = get_default_kwargs(class_obj)\n                name = class_choices.name\n                # Overwrite the default by the arguments,\n                conf.update(config[f""{name}_conf""])\n                # and set it again\n                config[f""{name}_conf""] = conf\n        return config\n\n    @classmethod\n    def check_required_command_args(cls, args: argparse.Namespace):\n        assert check_argument_types()\n        for k in vars(args):\n            if ""-"" in k:\n                raise RuntimeError(f\'Use ""_"" instead of ""-"": parser.get_parser(""{k}"")\')\n        if len(args.pretrain_path) != len(args.pretrain_key):\n            raise RuntimeError(\n                ""The number of --pretrain_path and --pretrain_key must be same""\n            )\n\n        required = "", "".join(\n            f""--{a}"" for a in args.required if getattr(args, a) is None\n        )\n\n        if len(required) != 0:\n            parser = cls.get_parser()\n            parser.print_help(file=sys.stderr)\n            p = Path(sys.argv[0]).name\n            print(file=sys.stderr)\n            print(\n                f""{p}: error: the following arguments are required: "" f""{required}"",\n                file=sys.stderr,\n            )\n            sys.exit(2)\n\n    @classmethod\n    def check_task_requirements(\n        cls,\n        dataset: ESPnetDataset,\n        allow_variable_data_keys: bool,\n        inference: bool = False,\n    ) -> None:\n        """"""Check if the dataset satisfy the requirement of current Task""""""\n        assert check_argument_types()\n        mes = (\n            f""If you intend to use an additional input, modify ""\n            f\'""{cls.__name__}.required_data_names()"" or \'\n            f\'""{cls.__name__}.optional_data_names()"". \'\n            f""Otherwise you need to set --allow_variable_data_keys true ""\n        )\n\n        for k in cls.required_data_names(inference):\n            if not dataset.has_name(k):\n                raise RuntimeError(\n                    f\'""{cls.required_data_names(inference)}"" are required for\'\n                    f\' {cls.__name__}. but ""{dataset.names()}"" are input.\\n{mes}\'\n                )\n        if not allow_variable_data_keys:\n            task_keys = cls.required_data_names(inference) + cls.optional_data_names(\n                inference\n            )\n            for k in dataset.names():\n                if k not in task_keys:\n                    raise RuntimeError(\n                        f""The data-name must be one of {task_keys} ""\n                        f\'for {cls.__name__}: ""{k}"" is not allowed.\\n{mes}\'\n                    )\n\n    @classmethod\n    def print_config(cls, file=sys.stdout) -> None:\n        assert check_argument_types()\n        # Shows the config: e.g. python train.py asr --print_config\n        config = cls.get_default_config()\n        file.write(yaml_no_alias_safe_dump(config, indent=4, sort_keys=False))\n\n    @classmethod\n    def main(cls, args: argparse.Namespace = None, cmd: Sequence[str] = None):\n        if cls.num_optimizers != cls.trainer.num_optimizers:\n            raise RuntimeError(\n                f""Task.num_optimizers != Task.trainer.num_optimizers: ""\n                f""{cls.num_optimizers} != {cls.trainer.num_optimizers}""\n            )\n        assert check_argument_types()\n        print(get_commandline_args(), file=sys.stderr)\n        if args is None:\n            parser = cls.get_parser()\n            args = parser.parse_args(cmd)\n        if args.print_config:\n            cls.print_config()\n            sys.exit(0)\n        cls.check_required_command_args(args)\n\n        # ""distributed"" is decided using the other command args\n        resolve_distributed_mode(args)\n        if not args.distributed or not args.multiprocessing_distributed:\n            cls.main_worker(args)\n\n        else:\n            assert args.ngpu > 1, args.ngpu\n            # Multi-processing distributed mode: e.g. 2node-4process-4GPU\n            # |   Host1     |    Host2    |\n            # |   Process1  |   Process2  |  <= Spawn processes\n            # |Child1|Child2|Child1|Child2|\n            # |GPU1  |GPU2  |GPU1  |GPU2  |\n\n            # See also the following usage of --multiprocessing-distributed:\n            # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n            num_nodes = get_num_nodes(args.dist_world_size, args.dist_launcher)\n            if num_nodes == 1:\n                args.dist_master_addr = ""localhost""\n                args.dist_rank = 0\n                # Single node distributed training with multi-GPUs\n                if (\n                    args.dist_init_method == ""env://""\n                    and get_master_port(args.dist_master_port) is None\n                ):\n                    # Get the unused port\n                    args.dist_master_port = free_port()\n\n            # Assume that nodes use same number of GPUs each other\n            args.dist_world_size = args.ngpu * num_nodes\n            node_rank = get_node_rank(args.dist_rank, args.dist_launcher)\n\n            # The following block is copied from:\n            # https://github.com/pytorch/pytorch/blob/master/torch/multiprocessing/spawn.py\n            error_queues = []\n            processes = []\n            mp = torch.multiprocessing.get_context(""spawn"")\n            for i in range(args.ngpu):\n                # Copy args\n                local_args = argparse.Namespace(**vars(args))\n\n                local_args.local_rank = i\n                local_args.dist_rank = args.ngpu * node_rank + i\n                local_args.ngpu = 1\n\n                process = mp.Process(\n                    target=cls.main_worker, args=(local_args,), daemon=False,\n                )\n                process.start()\n                processes.append(process)\n                error_queues.append(mp.SimpleQueue())\n            # Loop on join until it returns True or raises an exception.\n            while not ProcessContext(processes, error_queues).join():\n                pass\n\n    @classmethod\n    def main_worker(cls, args: argparse.Namespace):\n        assert check_argument_types()\n\n        # 0. Init distributed process\n        distributed_option = build_dataclass(DistributedOption, args)\n        distributed_option.init()\n\n        # NOTE(kamo): Don\'t use logging before invoking logging.basicConfig()\n        if not distributed_option.distributed or distributed_option.dist_rank == 0:\n            if not distributed_option.distributed:\n                _rank = """"\n            else:\n                _rank = (\n                    f"":{distributed_option.dist_rank}/""\n                    f""{distributed_option.dist_world_size}""\n                )\n\n            # NOTE(kamo):\n            # logging.basicConfig() is invoked in main_worker() instead of main()\n            # because it can be invoked only once in a process.\n            # FIXME(kamo): Should we use logging.getLogger()?\n            logging.basicConfig(\n                level=args.log_level,\n                format=f""[{os.uname()[1].split(\'.\')[0]}{_rank}]""\n                f"" %(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n            )\n        else:\n            # Suppress logging if RANK != 0\n            logging.basicConfig(\n                level=""ERROR"",\n                format=f""[{os.uname()[1].split(\'.\')[0]}""\n                f"":{distributed_option.dist_rank}/{distributed_option.dist_world_size}]""\n                f"" %(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n            )\n\n        # 1. Set random-seed\n        set_all_random_seed(args.seed)\n        torch.backends.cudnn.enabled = args.cudnn_enabled\n        torch.backends.cudnn.benchmark = args.cudnn_benchmark\n        torch.backends.cudnn.deterministic = args.cudnn_deterministic\n\n        common_iter_kwargs = dict(\n            iterator_type=args.iterator_type,\n            train_dtype=args.train_dtype,\n            num_workers=args.num_workers,\n            seed=args.seed,\n            allow_variable_data_keys=args.allow_variable_data_keys,\n            ngpu=args.ngpu,\n            fold_length=args.fold_length,\n            sort_in_batch=args.sort_in_batch,\n            sort_batch=args.sort_batch,\n            chunk_length=args.chunk_length,\n            chunk_shift_ratio=args.chunk_shift_ratio,\n            num_cache_chunks=args.num_cache_chunks,\n        )\n\n        # 2. Build iterator factories\n        train_iter_factory, _, _ = cls.build_iter_factory(\n            data_path_and_name_and_type=args.train_data_path_and_name_and_type,\n            shape_files=args.train_shape_file,\n            batch_size=args.batch_size,\n            batch_bins=args.batch_bins,\n            batch_type=args.batch_type,\n            train=not args.collect_stats,\n            preprocess_fn=cls.build_preprocess_fn(args, train=True),\n            collate_fn=cls.build_collate_fn(args),\n            num_iters_per_epoch=args.num_iters_per_epoch,\n            max_cache_size=args.max_cache_size,\n            distributed=distributed_option.distributed,\n            name=""train"",\n            **common_iter_kwargs,\n        )\n        if args.valid_batch_type is None:\n            args.valid_batch_type = args.batch_type\n        if args.valid_batch_size is None:\n            args.valid_batch_size = args.batch_size\n        if args.valid_batch_bins is None:\n            args.valid_batch_bins = args.batch_bins\n        if args.valid_max_cache_size is None:\n            # Cache 5% of maximum size for validation loader\n            args.valid_max_cache_size = 0.05 * args.max_cache_size\n        valid_iter_factory, _, _ = cls.build_iter_factory(\n            data_path_and_name_and_type=args.valid_data_path_and_name_and_type,\n            shape_files=args.valid_shape_file,\n            batch_size=args.valid_batch_size,\n            batch_bins=args.valid_batch_bins,\n            batch_type=args.batch_type,\n            train=False,\n            preprocess_fn=cls.build_preprocess_fn(args, train=False),\n            collate_fn=cls.build_collate_fn(args),\n            num_iters_per_epoch=None,\n            max_cache_size=args.valid_max_cache_size,\n            distributed=distributed_option.distributed,\n            name=""valid"",\n            **common_iter_kwargs,\n        )\n        if args.num_att_plot != 0:\n            plot_attention_iter_factory, _, _ = cls.build_iter_factory(\n                data_path_and_name_and_type=args.valid_data_path_and_name_and_type,\n                shape_files=args.valid_shape_file,\n                batch_type=""unsorted"",\n                batch_size=1,\n                batch_bins=0,\n                train=False,\n                preprocess_fn=cls.build_preprocess_fn(args, train=False),\n                collate_fn=cls.build_collate_fn(args),\n                num_batches=args.num_att_plot,\n                num_iters_per_epoch=None,\n                # num_att_plot should be a few sample ~ 3, so cache all data.\n                max_cache_size=np.inf if args.max_cache_size != 0.0 else 0.0,\n                # always False because plot_attention performs on RANK0\n                distributed=False,\n                name=""plot_att"",\n                **common_iter_kwargs,\n            )\n        else:\n            plot_attention_iter_factory = None\n\n        # 3. Build model\n        model = cls.build_model(args=args)\n        if not isinstance(model, AbsESPnetModel):\n            raise RuntimeError(\n                f""model must inherit {AbsESPnetModel.__name__}, but got {type(model)}""\n            )\n        if args.train_dtype in (""float16"", ""float32"", ""float64""):\n            dtype = getattr(torch, args.train_dtype)\n        else:\n            dtype = torch.float32\n        model = model.to(dtype=dtype, device=""cuda"" if args.ngpu > 0 else ""cpu"")\n\n        # 4. Build optimizer\n        optimizers = cls.build_optimizers(args, model=model)\n\n        # For apex support\n        use_apex = args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3"")\n        if use_apex:\n            try:\n                from apex import amp\n            except ImportError:\n                logging.error(\n                    ""You need to install apex. ""\n                    ""See https://github.com/NVIDIA/apex#linux""\n                )\n                raise\n            model, optimizers = amp.initialize(\n                model, optimizers, opt_level=args.train_dtype\n            )\n\n        # 5. Build schedulers\n        schedulers = []\n        for i, optim in enumerate(optimizers, 1):\n            suf = """" if i == 1 else str(i)\n            name = getattr(args, f""scheduler{suf}"")\n            conf = getattr(args, f""scheduler{suf}_conf"")\n            if name is not None:\n                cls_ = scheduler_classes.get(name)\n                if cls_ is None:\n                    raise ValueError(\n                        f""must be one of {list(scheduler_classes)}: {name}""\n                    )\n                scheduler = cls_(optim, **conf)\n            else:\n                scheduler = None\n\n            schedulers.append(scheduler)\n\n        logging.info(pytorch_cudnn_version())\n        logging.info(f""Model:\\n{model}"")\n        for i, (o, s) in enumerate(zip(optimizers, schedulers), 1):\n            suf = """" if i == 1 else str(i)\n            logging.info(f""Optimizer{suf}:\\n{o}"")\n            logging.info(f""Scheduler{suf}: {s}"")\n\n        # 6. Dump ""args"" to config.yaml\n        # NOTE(kamo): ""args"" should be saved after object-buildings are done\n        #  because they are allowed to modify ""args"".\n        output_dir = Path(args.output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        with (output_dir / ""config.yaml"").open(""w"", encoding=""utf-8"") as f:\n            logging.info(f\'Saving the configuration in {output_dir / ""config.yaml""}\')\n            yaml_no_alias_safe_dump(vars(args), f, indent=4, sort_keys=False)\n\n        # 7. Loads pre-trained model\n        for p, k in zip(args.pretrain_path, args.pretrain_key):\n            load_pretrained_model(\n                model=model,\n                # Directly specify the model path e.g. exp/train/loss.best.pt\n                pretrain_path=p,\n                # if pretrain_key is None -> model\n                # elif pretrain_key is str e.g. ""encoder"" -> model.encoder\n                pretrain_key=k,\n                # NOTE(kamo): ""cuda"" for torch.load always indicates cuda:0\n                #   in PyTorch<=1.4\n                map_location=f""cuda:{torch.cuda.current_device()}""\n                if args.ngpu > 0\n                else ""cpu"",\n            )\n\n        # 8. Resume the training state from the previous epoch\n        reporter = Reporter()\n        if args.resume and (output_dir / ""checkpoint.pth"").exists():\n            states = torch.load(\n                output_dir / ""checkpoint.pth"",\n                map_location=f""cuda:{torch.cuda.current_device()}""\n                if args.ngpu > 0\n                else ""cpu"",\n            )\n            model.load_state_dict(states[""model""])\n            reporter.load_state_dict(states[""reporter""])\n            for optimizer, state in zip(optimizers, states[""optimizers""]):\n                optimizer.load_state_dict(state)\n            for scheduler, state in zip(schedulers, states[""schedulers""]):\n                if scheduler is not None:\n                    scheduler.load_state_dict(state)\n            if use_apex and states[""amp""] is not None:\n                try:\n                    from apex import amp\n                except ImportError:\n                    logging.error(\n                        ""You need to install apex. ""\n                        ""See https://github.com/NVIDIA/apex#linux""\n                    )\n                amp.load_state_dict(states[""amp""])\n\n            logging.info(\n                f""The training was resumed using {output_dir / \'checkpoint.pth\'}""\n            )\n\n        # 9. Run\n        if args.dry_run:\n            pass\n        elif args.collect_stats:\n            # Perform on collect_stats mode. This mode has two roles\n            # - Derive the length and dimension of all input data\n            # - Accumulate feats, square values, and the length for whitening\n            collect_stats(\n                model=model,\n                train_iter=train_iter_factory.build_iter(1),\n                valid_iter=valid_iter_factory.build_iter(1),\n                output_dir=output_dir,\n                ngpu=args.ngpu,\n                log_interval=args.log_interval,\n                write_collected_feats=args.write_collected_feats,\n            )\n        else:\n            # Don\'t give args to run() directly!!!\n            # Instead of it, define ""Options"" object and build here.\n            trainer_options = cls.trainer.build_options(args)\n\n            # Start training\n            cls.trainer.run(\n                model=model,\n                optimizers=optimizers,\n                schedulers=schedulers,\n                train_iter_factory=train_iter_factory,\n                valid_iter_factory=valid_iter_factory,\n                plot_attention_iter_factory=plot_attention_iter_factory,\n                reporter=reporter,\n                output_dir=output_dir,\n                max_epoch=args.max_epoch,\n                seed=args.seed,\n                patience=args.patience,\n                keep_nbest_models=args.keep_nbest_models,\n                early_stopping_criterion=args.early_stopping_criterion,\n                best_model_criterion=args.best_model_criterion,\n                val_scheduler_criterion=args.val_scheduler_criterion,\n                trainer_options=trainer_options,\n                distributed_option=distributed_option,\n            )\n\n            if not distributed_option.distributed or distributed_option.dist_rank == 0:\n                # Generated n-best averaged model\n                average_nbest_models(\n                    reporter=reporter,\n                    output_dir=output_dir,\n                    best_model_criterion=args.best_model_criterion,\n                    nbest=args.keep_nbest_models,\n                )\n\n    @classmethod\n    def build_iter_factory(\n        cls,\n        iterator_type: str,\n        batch_size: int,\n        batch_bins: int,\n        preprocess_fn,\n        collate_fn,\n        train_dtype: str,\n        num_workers: int,\n        seed: int,\n        allow_variable_data_keys: bool,\n        ngpu: int,\n        data_path_and_name_and_type,\n        shape_files: Union[Tuple[str, ...], List[str]],\n        batch_type: str,\n        train: bool,\n        num_iters_per_epoch: Optional[int],\n        max_cache_size: float,\n        distributed: bool,\n        name: str,\n        fold_length: Sequence[int],\n        sort_in_batch: str,\n        sort_batch: str,\n        chunk_length: Union[int, str],\n        chunk_shift_ratio: float,\n        num_cache_chunks: int,\n        num_batches: int = None,\n    ) -> Union[\n        Tuple[AbsIterFactory, ESPnetDataset, List[Tuple[str, ...]]],\n        Tuple[None, None, None],\n    ]:\n        """"""Build a factory object of mini-batch iterator.\n\n        This object is invoked at every epochs to build the iterator for each epoch\n        as following:\n\n        >>> iter_factory, _, _ = cls.build_iter_factory(...)\n        >>> for epoch in range(1, max_epoch):\n        ...     for keys, batch in iter_fatory.build_iter(epoch):\n        ...         model(**batch)\n\n        The mini-batches for each epochs are fully controlled by this class.\n        Note that the random seed used for shuffling is decided as ""seed + epoch"" and\n        the generated mini-batches can be reproduces when resuming.\n\n        Note that the definition of ""epoch"" doesn\'t always indicate\n        to run out of the whole training corpus.\n        ""--num_iters_per_epoch"" option restricts the number of iterations for each epoch\n        and the rest of samples for the originally epoch are left for the next epoch.\n        e.g. If The number of mini-batches equals to 4, the following two are same:\n\n        - 1 epoch without ""--num_iters_per_epoch""\n        - 4 epoch with ""--num_iters_per_epoch"" == 4\n\n        """"""\n        assert check_argument_types()\n\n        kwargs = dict(\n            data_path_and_name_and_type=data_path_and_name_and_type,\n            shape_files=shape_files,\n            train=train,\n            preprocess_fn=preprocess_fn,\n            collate_fn=collate_fn,\n            num_batches=num_batches,\n            num_iters_per_epoch=num_iters_per_epoch,\n            max_cache_size=max_cache_size,\n            distributed=distributed,\n            name=name,\n            batch_size=batch_size,\n            train_dtype=train_dtype,\n            num_workers=num_workers,\n            seed=seed,\n            allow_variable_data_keys=allow_variable_data_keys,\n            ngpu=ngpu,\n        )\n\n        if iterator_type == ""sequence"":\n            return cls.build_sequence_iter_factory(\n                **kwargs,\n                batch_type=batch_type,\n                batch_bins=batch_bins,\n                fold_length=fold_length,\n                sort_in_batch=sort_in_batch,\n                sort_batch=sort_batch,\n            )\n        elif iterator_type == ""chunk"":\n            return cls.build_chunk_iter_factory(\n                **kwargs,\n                chunk_length=chunk_length,\n                chunk_shift_ratio=chunk_shift_ratio,\n                num_cache_chunks=num_cache_chunks,\n            )\n        elif iterator_type == ""none"":\n            # This branch is used for --dry_run mode\n            return None, None, None\n        else:\n            raise RuntimeError(f""Not supported: iterator_type={iterator_type}"")\n\n    @classmethod\n    def build_sequence_iter_factory(\n        cls,\n        data_path_and_name_and_type,\n        shape_files: Union[Tuple[str, ...], List[str]],\n        batch_type: str,\n        train: bool,\n        preprocess_fn,\n        batch_size: int,\n        batch_bins: int,\n        collate_fn,\n        train_dtype: str,\n        fold_length: Sequence[int],\n        num_workers: int,\n        sort_in_batch: str,\n        sort_batch: str,\n        seed: int,\n        allow_variable_data_keys: bool,\n        ngpu: int,\n        num_batches: Optional[int],\n        num_iters_per_epoch: Optional[int],\n        max_cache_size: float,\n        distributed: bool,\n        name: str,\n    ) -> Tuple[AbsIterFactory, ESPnetDataset, List[Tuple[str, ...]]]:\n        assert check_argument_types()\n        if train_dtype in (""float32"", ""O0"", ""O1"", ""O2"", ""O3""):\n            train_dtype = ""float32""\n\n        dataset = ESPnetDataset(\n            data_path_and_name_and_type,\n            float_dtype=train_dtype,\n            preprocess=preprocess_fn,\n            max_cache_size=max_cache_size,\n        )\n        cls.check_task_requirements(dataset, allow_variable_data_keys)\n\n        batch_sampler = build_batch_sampler(\n            type=batch_type,\n            shape_files=shape_files,\n            fold_lengths=fold_length,\n            batch_size=batch_size,\n            batch_bins=batch_bins,\n            sort_in_batch=sort_in_batch,\n            sort_batch=sort_batch,\n            drop_last=False,\n            min_batch_size=torch.distributed.get_world_size() if distributed else 1,\n        )\n\n        batches = list(batch_sampler)\n        if num_batches is not None:\n            batches = batches[:num_batches]\n\n        bs_list = [len(batch) for batch in batches]\n\n        logging.info(f""[{name}] dataset:\\n{dataset}"")\n        logging.info(f""[{name}] Batch sampler: {batch_sampler}"")\n        logging.info(\n            f""[{name}] mini-batch sizes summary: N-batch={len(bs_list)}, ""\n            f""mean={np.mean(bs_list):.1f}, min={np.min(bs_list)}, max={np.max(bs_list)}""\n        )\n\n        if distributed:\n            world_size = torch.distributed.get_world_size()\n            rank = torch.distributed.get_rank()\n            for batch in batches:\n                if len(batch) < world_size:\n                    raise RuntimeError(\n                        f""The batch-size must be equal or more than world_size: ""\n                        f""{len(batch)} < {world_size}""\n                    )\n            batches = [batch[rank::world_size] for batch in batches]\n\n        return (\n            SequenceIterFactory(\n                dataset=dataset,\n                batches=batches,\n                seed=seed,\n                num_iters_per_epoch=num_iters_per_epoch,\n                shuffle=train,\n                num_workers=num_workers,\n                collate_fn=collate_fn,\n                pin_memory=ngpu > 0,\n            ),\n            dataset,\n            batches,\n        )\n\n    @classmethod\n    def build_chunk_iter_factory(\n        cls,\n        data_path_and_name_and_type,\n        shape_files: Union[Tuple[str, ...], List[str]],\n        train: bool,\n        preprocess_fn,\n        collate_fn,\n        train_dtype: str,\n        num_workers: int,\n        seed: int,\n        allow_variable_data_keys: bool,\n        batch_size: int,\n        ngpu: int,\n        chunk_length: Union[int, str],\n        chunk_shift_ratio: float,\n        num_cache_chunks: int,\n        num_batches: Optional[int],\n        num_iters_per_epoch: Optional[int],\n        max_cache_size: float,\n        distributed: bool,\n        name: str,\n    ) -> Tuple[AbsIterFactory, ESPnetDataset, List[Tuple[str, ...]]]:\n        assert check_argument_types()\n        if train_dtype in (""float32"", ""O0"", ""O1"", ""O2"", ""O3""):\n            train_dtype = ""float32""\n\n        dataset = ESPnetDataset(\n            data_path_and_name_and_type,\n            float_dtype=train_dtype,\n            preprocess=preprocess_fn,\n            max_cache_size=max_cache_size,\n        )\n        cls.check_task_requirements(dataset, allow_variable_data_keys)\n\n        if len(shape_files) == 0:\n            key_file = data_path_and_name_and_type[0][0]\n        else:\n            key_file = shape_files[0]\n\n        batch_sampler = UnsortedBatchSampler(batch_size=1, key_file=key_file)\n        batches = list(batch_sampler)\n        if num_batches is not None:\n            batches = batches[:num_batches]\n        logging.info(f""[{name}] dataset:\\n{dataset}"")\n\n        if distributed:\n            world_size = torch.distributed.get_world_size()\n            rank = torch.distributed.get_rank()\n            if len(batches) < world_size:\n                raise RuntimeError(""Number of samples is smaller than world_size"")\n            if batch_size < world_size:\n                raise RuntimeError(""batch_size must be equal or more than world_size"")\n\n            if rank < batch_size % world_size:\n                batch_size = batch_size // world_size + 1\n            else:\n                batch_size = batch_size // world_size\n            num_cache_chunks = num_cache_chunks // world_size\n            # NOTE(kamo): Split whole corpus by sample numbers without considering\n            #   each of the lengths, therefore the number of iteration counts are not\n            #   always equal to each other and the iterations are limitted\n            #   by the fewest iterations.\n            #   i.e. the samples over the counts are discarded.\n            batches = batches[rank::world_size]\n\n        return (\n            ChunkIterFactory(\n                dataset=dataset,\n                batches=batches,\n                seed=seed,\n                # For chunk iterator,\n                # --num_iters_per_epoch doesn\'t indicate the number of iterations,\n                # but indicates the number of samples.\n                num_samples_per_epoch=num_iters_per_epoch,\n                shuffle=train,\n                num_workers=num_workers,\n                collate_fn=collate_fn,\n                pin_memory=ngpu > 0,\n                batch_size=batch_size,\n                chunk_length=chunk_length,\n                chunk_shift_ratio=chunk_shift_ratio,\n                num_cache_chunks=num_cache_chunks,\n            ),\n            dataset,\n            batches,\n        )\n\n    # ~~~~~~~~~ The methods below are mainly used for inference ~~~~~~~~~\n    @classmethod\n    def build_model_from_file(\n        cls,\n        config_file: Union[Path, str],\n        model_file: Union[Path, str] = None,\n        device: str = ""cpu"",\n    ) -> Tuple[AbsESPnetModel, argparse.Namespace]:\n        """"""This method is used for inference or fine-tuning.\n\n        Args:\n            config_file: The yaml file saved when training.\n            model_file: The model file saved when training.\n            device:\n\n        """"""\n        assert check_argument_types()\n        config_file = Path(config_file)\n\n        with config_file.open(""r"", encoding=""utf-8"") as f:\n            args = yaml.safe_load(f)\n        args = argparse.Namespace(**args)\n        model = cls.build_model(args)\n        if not isinstance(model, AbsESPnetModel):\n            raise RuntimeError(\n                f""model must inherit {AbsESPnetModel.__name__}, but got {type(model)}""\n            )\n        model.to(device)\n        if model_file is not None:\n            if device == ""cuda"":\n                # NOTE(kamo): ""cuda"" for torch.load always indicates cuda:0\n                #   in PyTorch<=1.4\n                device = f""cuda:{torch.cuda.current_device()}""\n            model.load_state_dict(torch.load(model_file, map_location=device))\n\n        return model, args\n\n    @classmethod\n    def build_non_sorted_iterator(\n        cls,\n        data_path_and_name_and_type,\n        batch_size: int = 1,\n        dtype: str = ""float32"",\n        key_file: str = None,\n        num_workers: int = 1,\n        pin_memory: bool = False,\n        preprocess_fn=None,\n        collate_fn=None,\n        inference: bool = True,\n        allow_variable_data_keys: bool = False,\n    ) -> Tuple[DataLoader, ESPnetDataset, UnsortedBatchSampler]:\n        """"""Create mini-batch iterator w/o shuffling and sorting by sequence lengths.\n\n        Note that unlike the iterator for training, the shape files are not required\n        for this iterator because any sorting is not done here.\n\n        """"""\n        assert check_argument_types()\n        if dtype in (""float32"", ""O0"", ""O1"", ""O2"", ""O3""):\n            dtype = ""float32""\n\n        dataset = ESPnetDataset(\n            data_path_and_name_and_type, float_dtype=dtype, preprocess=preprocess_fn,\n        )\n        cls.check_task_requirements(dataset, allow_variable_data_keys, inference)\n\n        if key_file is None:\n            key_file, _, _ = data_path_and_name_and_type[0]\n        batch_sampler = UnsortedBatchSampler(batch_size=batch_size, key_file=key_file)\n\n        logging.info(f""dataset:\\n{dataset}"")\n        logging.info(f""Batch sampler: {batch_sampler}"")\n\n        # For backward compatibility for pytorch DataLoader\n        if collate_fn is not None:\n            kwargs = dict(collate_fn=collate_fn)\n        else:\n            kwargs = {}\n\n        return (\n            DataLoader(\n                dataset=dataset,\n                batch_sampler=batch_sampler,\n                num_workers=num_workers,\n                pin_memory=pin_memory,\n                **kwargs,\n            ),\n            dataset,\n            batch_sampler,\n        )\n'"
espnet2/tasks/asr.py,1,"b'import argparse\nimport logging\nfrom typing import Callable\nfrom typing import Collection\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nfrom espnet2.asr.ctc import CTC\nfrom espnet2.asr.decoder.abs_decoder import AbsDecoder\nfrom espnet2.asr.decoder.rnn_decoder import RNNDecoder\nfrom espnet2.asr.decoder.transformer_decoder import TransformerDecoder\nfrom espnet2.asr.encoder.abs_encoder import AbsEncoder\nfrom espnet2.asr.encoder.rnn_encoder import RNNEncoder\nfrom espnet2.asr.encoder.transformer_encoder import TransformerEncoder\nfrom espnet2.asr.encoder.vgg_rnn_encoder import VGGRNNEncoder\nfrom espnet2.asr.espnet_model import ESPnetASRModel\nfrom espnet2.asr.frontend.abs_frontend import AbsFrontend\nfrom espnet2.asr.frontend.default import DefaultFrontend\nfrom espnet2.asr.specaug.abs_specaug import AbsSpecAug\nfrom espnet2.asr.specaug.specaug import SpecAug\nfrom espnet2.layers.abs_normalize import AbsNormalize\nfrom espnet2.layers.global_mvn import GlobalMVN\nfrom espnet2.layers.utterance_mvn import UtteranceMVN\nfrom espnet2.tasks.abs_task import AbsTask\nfrom espnet2.torch_utils.initialize import initialize\nfrom espnet2.train.class_choices import ClassChoices\nfrom espnet2.train.collate_fn import CommonCollateFn\nfrom espnet2.train.preprocessor import CommonPreprocessor\nfrom espnet2.train.trainer import Trainer\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\nfrom espnet2.utils.nested_dict_action import NestedDictAction\nfrom espnet2.utils.types import int_or_none\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str_or_none\n\nfrontend_choices = ClassChoices(\n    name=""frontend"",\n    classes=dict(default=DefaultFrontend),\n    type_check=AbsFrontend,\n    default=""default"",\n)\nspecaug_choices = ClassChoices(\n    name=""specaug"",\n    classes=dict(specaug=SpecAug),\n    type_check=AbsSpecAug,\n    default=None,\n    optional=True,\n)\nnormalize_choices = ClassChoices(\n    ""normalize"",\n    classes=dict(global_mvn=GlobalMVN, utterance_mvn=UtteranceMVN,),\n    type_check=AbsNormalize,\n    default=""utterance_mvn"",\n    optional=True,\n)\nencoder_choices = ClassChoices(\n    ""encoder"",\n    classes=dict(\n        transformer=TransformerEncoder, vgg_rnn=VGGRNNEncoder, rnn=RNNEncoder,\n    ),\n    type_check=AbsEncoder,\n    default=""rnn"",\n)\ndecoder_choices = ClassChoices(\n    ""decoder"",\n    classes=dict(transformer=TransformerDecoder, rnn=RNNDecoder),\n    type_check=AbsDecoder,\n    default=""rnn"",\n)\n\n\nclass ASRTask(AbsTask):\n    # If you need more than one optimizers, change this value\n    num_optimizers: int = 1\n\n    # Add variable objects configurations\n    class_choices_list = [\n        # --frontend and --frontend_conf\n        frontend_choices,\n        # --specaug and --specaug_conf\n        specaug_choices,\n        # --normalize and --normalize_conf\n        normalize_choices,\n        # --encoder and --encoder_conf\n        encoder_choices,\n        # --decoder and --decoder_conf\n        decoder_choices,\n    ]\n\n    # If you need to modify train() or eval() procedures, change Trainer class here\n    trainer = Trainer\n\n    @classmethod\n    def add_task_arguments(cls, parser: argparse.ArgumentParser):\n        group = parser.add_argument_group(description=""Task related"")\n\n        # NOTE(kamo): add_arguments(..., required=True) can\'t be used\n        # to provide --print_config mode. Instead of it, do as\n        required = parser.get_default(""required"")\n        required += [""token_list""]\n\n        group.add_argument(\n            ""--token_list"",\n            type=str_or_none,\n            default=None,\n            help=""A text mapping int-id to token"",\n        )\n        group.add_argument(\n            ""--init"",\n            type=lambda x: str_or_none(x.lower()),\n            default=None,\n            help=""The initialization method"",\n            choices=[\n                ""chainer"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n                None,\n            ],\n        )\n\n        group.add_argument(\n            ""--input_size"",\n            type=int_or_none,\n            default=None,\n            help=""The number of input dimension of the feature"",\n        )\n\n        group.add_argument(\n            ""--ctc_conf"",\n            action=NestedDictAction,\n            default=get_default_kwargs(CTC),\n            help=""The keyword arguments for CTC class."",\n        )\n        group.add_argument(\n            ""--model_conf"",\n            action=NestedDictAction,\n            default=get_default_kwargs(ESPnetASRModel),\n            help=""The keyword arguments for model class."",\n        )\n\n        group = parser.add_argument_group(description=""Preprocess related"")\n        group.add_argument(\n            ""--use_preprocessor"",\n            type=str2bool,\n            default=False,\n            help=""Apply preprocessing to data or not"",\n        )\n        group.add_argument(\n            ""--token_type"",\n            type=str,\n            default=""bpe"",\n            choices=[""bpe"", ""char"", ""word""],\n            help=""The text will be tokenized "" ""in the specified level token"",\n        )\n        group.add_argument(\n            ""--bpemodel"",\n            type=str_or_none,\n            default=None,\n            help=""The model file of sentencepiece"",\n        )\n        parser.add_argument(\n            ""--non_linguistic_symbols"",\n            type=str_or_none,\n            help=""non_linguistic_symbols file path"",\n        )\n\n        for class_choices in cls.class_choices_list:\n            # Append --<name> and --<name>_conf.\n            # e.g. --encoder and --encoder_conf\n            class_choices.add_arguments(group)\n\n    @classmethod\n    def build_collate_fn(\n        cls, args: argparse.Namespace\n    ) -> Callable[\n        [Collection[Tuple[str, Dict[str, np.ndarray]]]],\n        Tuple[List[str], Dict[str, torch.Tensor]],\n    ]:\n        assert check_argument_types()\n        # NOTE(kamo): int value = 0 is reserved by CTC-blank symbol\n        return CommonCollateFn(float_pad_value=0.0, int_pad_value=-1)\n\n    @classmethod\n    def build_preprocess_fn(\n        cls, args: argparse.Namespace, train: bool\n    ) -> Optional[Callable[[str, Dict[str, np.array]], Dict[str, np.ndarray]]]:\n        assert check_argument_types()\n        if args.use_preprocessor:\n            retval = CommonPreprocessor(\n                train=train,\n                token_type=args.token_type,\n                token_list=args.token_list,\n                bpemodel=args.bpemodel,\n                non_linguistic_symbols=args.non_linguistic_symbols,\n            )\n        else:\n            retval = None\n        assert check_return_type(retval)\n        return retval\n\n    @classmethod\n    def required_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        if not inference:\n            retval = (""speech"", ""text"")\n        else:\n            # Recognition mode\n            retval = (""speech"",)\n        return retval\n\n    @classmethod\n    def optional_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        retval = ()\n        assert check_return_type(retval)\n        return retval\n\n    @classmethod\n    def build_model(cls, args: argparse.Namespace) -> ESPnetASRModel:\n        assert check_argument_types()\n        if isinstance(args.token_list, str):\n            with open(args.token_list, encoding=""utf-8"") as f:\n                token_list = [line.rstrip() for line in f]\n\n            # Overwriting token_list to keep it as ""portable"".\n            args.token_list = list(token_list)\n        elif isinstance(args.token_list, (tuple, list)):\n            token_list = list(args.token_list)\n        else:\n            raise RuntimeError(""token_list must be str or list"")\n        vocab_size = len(token_list)\n        logging.info(f""Vocabulary size: {vocab_size }"")\n\n        # 1. frontend\n        if args.input_size is None:\n            # Extract features in the model\n            frontend_class = frontend_choices.get_class(args.frontend)\n            frontend = frontend_class(**args.frontend_conf)\n            input_size = frontend.output_size()\n        else:\n            # Give features from data-loader\n            args.frontend = None\n            args.frontend_conf = {}\n            frontend = None\n            input_size = args.input_size\n\n        # 2. Data augmentation for spectrogram\n        if args.specaug is not None:\n            specaug_class = specaug_choices.get_class(args.specaug)\n            specaug = specaug_class(**args.specaug_conf)\n        else:\n            specaug = None\n\n        # 3. Normalization layer\n        if args.normalize is not None:\n            normalize_class = normalize_choices.get_class(args.normalize)\n            normalize = normalize_class(**args.normalize_conf)\n        else:\n            normalize = None\n\n        # 4. Encoder\n        encoder_class = encoder_choices.get_class(args.encoder)\n        encoder = encoder_class(input_size=input_size, **args.encoder_conf)\n\n        # 5. Decoder\n        decoder_class = decoder_choices.get_class(args.decoder)\n\n        decoder = decoder_class(\n            vocab_size=vocab_size,\n            encoder_output_size=encoder.output_size(),\n            **args.decoder_conf,\n        )\n\n        # 6. CTC\n        ctc = CTC(\n            odim=vocab_size, encoder_output_sizse=encoder.output_size(), **args.ctc_conf\n        )\n\n        # 7. RNN-T Decoder (Not implemented)\n        rnnt_decoder = None\n\n        # 8. Build model\n        model = ESPnetASRModel(\n            vocab_size=vocab_size,\n            frontend=frontend,\n            specaug=specaug,\n            normalize=normalize,\n            encoder=encoder,\n            decoder=decoder,\n            ctc=ctc,\n            rnnt_decoder=rnnt_decoder,\n            token_list=token_list,\n            **args.model_conf,\n        )\n\n        # FIXME(kamo): Should be done in model?\n        # 9. Initialize\n        if args.init is not None:\n            initialize(model, args.init)\n\n        assert check_return_type(model)\n        return model\n'"
espnet2/tasks/lm.py,1,"b'import argparse\nimport logging\nfrom typing import Callable\nfrom typing import Collection\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nfrom espnet2.lm.abs_model import AbsLM\nfrom espnet2.lm.espnet_model import ESPnetLanguageModel\nfrom espnet2.lm.seq_rnn import SequentialRNNLM\nfrom espnet2.tasks.abs_task import AbsTask\nfrom espnet2.torch_utils.initialize import initialize\nfrom espnet2.train.class_choices import ClassChoices\nfrom espnet2.train.collate_fn import CommonCollateFn\nfrom espnet2.train.preprocessor import CommonPreprocessor\nfrom espnet2.train.trainer import Trainer\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\nfrom espnet2.utils.nested_dict_action import NestedDictAction\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str_or_none\n\n\nlm_choices = ClassChoices(\n    ""lm"", classes=dict(seq_rnn=SequentialRNNLM), type_check=AbsLM, default=""seq_rnn""\n)\n\n\nclass LMTask(AbsTask):\n    # If you need more than one optimizers, change this value\n    num_optimizers: int = 1\n\n    # Add variable objects configurations\n    class_choices_list = [lm_choices]\n\n    # If you need to modify train() or eval() procedures, change Trainer class here\n    trainer = Trainer\n\n    @classmethod\n    def add_task_arguments(cls, parser: argparse.ArgumentParser):\n        # NOTE(kamo): Use \'_\' instead of \'-\' to avoid confusion\n        assert check_argument_types()\n        group = parser.add_argument_group(description=""Task related"")\n\n        # NOTE(kamo): add_arguments(..., required=True) can\'t be used\n        # to provide --print_config mode. Instead of it, do as\n        required = parser.get_default(""required"")\n        required += [""token_list""]\n\n        group.add_argument(\n            ""--token_list"",\n            type=str_or_none,\n            default=None,\n            help=""A text mapping int-id to token"",\n        )\n        group.add_argument(\n            ""--init"",\n            type=lambda x: str_or_none(x.lower()),\n            default=None,\n            help=""The initialization method"",\n            choices=[\n                ""chainer"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n                None,\n            ],\n        )\n        group.add_argument(\n            ""--model_conf"",\n            action=NestedDictAction,\n            default=get_default_kwargs(ESPnetLanguageModel),\n            help=""The keyword arguments for model class."",\n        )\n\n        group = parser.add_argument_group(description=""Preprocess related"")\n        group.add_argument(\n            ""--use_preprocessor"",\n            type=str2bool,\n            default=False,\n            help=""Apply preprocessing to data or not"",\n        )\n        group.add_argument(\n            ""--token_type"",\n            type=str,\n            default=""bpe"",\n            choices=[""bpe"", ""char"", ""word""],\n            help="""",\n        )\n        group.add_argument(\n            ""--bpemodel"",\n            type=str_or_none,\n            default=None,\n            help=""The model file fo sentencepiece"",\n        )\n        parser.add_argument(\n            ""--non_linguistic_symbols"",\n            type=str_or_none,\n            help=""non_linguistic_symbols file path"",\n        )\n        for class_choices in cls.class_choices_list:\n            # Append --<name> and --<name>_conf.\n            # e.g. --encoder and --encoder_conf\n            class_choices.add_arguments(group)\n\n        assert check_return_type(parser)\n        return parser\n\n    @classmethod\n    def build_collate_fn(\n        cls, args: argparse.Namespace\n    ) -> Callable[\n        [Collection[Tuple[str, Dict[str, np.ndarray]]]],\n        Tuple[List[str], Dict[str, torch.Tensor]],\n    ]:\n        assert check_argument_types()\n        return CommonCollateFn(int_pad_value=0)\n\n    @classmethod\n    def build_preprocess_fn(\n        cls, args: argparse.Namespace, train: bool\n    ) -> Optional[Callable[[str, Dict[str, np.array]], Dict[str, np.ndarray]]]:\n        assert check_argument_types()\n        if args.use_preprocessor:\n            retval = CommonPreprocessor(\n                train=train,\n                token_type=args.token_type,\n                token_list=args.token_list,\n                bpemodel=args.bpemodel,\n            )\n        else:\n            retval = None\n        assert check_return_type(retval)\n        return retval\n\n    @classmethod\n    def required_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        retval = (""text"",)\n        return retval\n\n    @classmethod\n    def optional_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        retval = ()\n        return retval\n\n    @classmethod\n    def build_model(cls, args: argparse.Namespace) -> ESPnetLanguageModel:\n        assert check_argument_types()\n        if isinstance(args.token_list, str):\n            with open(args.token_list, encoding=""utf-8"") as f:\n                token_list = [line.rstrip() for line in f]\n\n            # ""args"" is saved as it is in a yaml file by BaseTask.main().\n            # Overwriting token_list to keep it as ""portable"".\n            args.token_list = token_list.copy()\n        elif isinstance(args.token_list, (tuple, list)):\n            token_list = args.token_list.copy()\n        else:\n            raise RuntimeError(""token_list must be str or dict"")\n\n        vocab_size = len(token_list)\n        logging.info(f""Vocabulary size: {vocab_size }"")\n\n        # 1. Build LM model\n        lm_class = lm_choices.get_class(args.lm)\n        lm = lm_class(vocab_size=vocab_size, **args.lm_conf)\n\n        # 2. Build ESPnetModel\n        # Assume the last-id is sos_and_eos\n        model = ESPnetLanguageModel(lm=lm, vocab_size=vocab_size, **args.model_conf)\n\n        # FIXME(kamo): Should be done in model?\n        # 3. Initialize\n        if args.init is not None:\n            initialize(model, args.init)\n\n        assert check_return_type(model)\n        return model\n'"
espnet2/tasks/tts.py,1,"b'import argparse\nimport logging\nfrom typing import Callable\nfrom typing import Collection\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nfrom espnet2.layers.abs_normalize import AbsNormalize\nfrom espnet2.layers.global_mvn import GlobalMVN\nfrom espnet2.tasks.abs_task import AbsTask\nfrom espnet2.train.class_choices import ClassChoices\nfrom espnet2.train.collate_fn import CommonCollateFn\nfrom espnet2.train.preprocessor import CommonPreprocessor\nfrom espnet2.train.trainer import Trainer\nfrom espnet2.tts.abs_tts import AbsTTS\nfrom espnet2.tts.espnet_model import ESPnetTTSModel\nfrom espnet2.tts.feats_extract.abs_feats_extract import AbsFeatsExtract\nfrom espnet2.tts.feats_extract.log_mel_fbank import LogMelFbank\nfrom espnet2.tts.feats_extract.log_spectrogram import LogSpectrogram\nfrom espnet2.tts.tacotron2 import Tacotron2\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\nfrom espnet2.utils.nested_dict_action import NestedDictAction\nfrom espnet2.utils.types import int_or_none\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str_or_none\n\nfeats_extractor_choices = ClassChoices(\n    ""feats_extract"",\n    classes=dict(fbank=LogMelFbank, spectrogram=LogSpectrogram),\n    type_check=AbsFeatsExtract,\n    default=""fbank"",\n)\nnormalize_choices = ClassChoices(\n    ""normalize"",\n    classes=dict(global_mvn=GlobalMVN),\n    type_check=AbsNormalize,\n    default=""global_mvn"",\n    optional=True,\n)\ntts_choices = ClassChoices(\n    ""tts"", classes=dict(tacotron2=Tacotron2), type_check=AbsTTS, default=""tacotron2""\n)\n\n\nclass TTSTask(AbsTask):\n    # If you need more than one optimizers, change this value\n    num_optimizers: int = 1\n\n    # Add variable objects configurations\n    class_choices_list = [\n        # --feats_extractor and --feats_extractor_conf\n        feats_extractor_choices,\n        # --normalize and --normalize_conf\n        normalize_choices,\n        # --tts and --tts_conf\n        tts_choices,\n    ]\n\n    # If you need to modify train() or eval() procedures, change Trainer class here\n    trainer = Trainer\n\n    @classmethod\n    def add_task_arguments(cls, parser: argparse.ArgumentParser):\n        # NOTE(kamo): Use \'_\' instead of \'-\' to avoid confusion\n        assert check_argument_types()\n        group = parser.add_argument_group(description=""Task related"")\n\n        # NOTE(kamo): add_arguments(..., required=True) can\'t be used\n        # to provide --print_config mode. Instead of it, do as\n        required = parser.get_default(""required"")\n        required += [""token_list""]\n\n        group.add_argument(\n            ""--token_list"",\n            type=str_or_none,\n            default=None,\n            help=""A text mapping int-id to token"",\n        )\n        group.add_argument(\n            ""--odim"",\n            type=int_or_none,\n            default=None,\n            help=""The number of dimension of output feature"",\n        )\n        group.add_argument(\n            ""--model_conf"",\n            action=NestedDictAction,\n            default=get_default_kwargs(ESPnetTTSModel),\n            help=""The keyword arguments for model class."",\n        )\n\n        group = parser.add_argument_group(description=""Preprocess related"")\n        group.add_argument(\n            ""--use_preprocessor"",\n            type=str2bool,\n            default=False,\n            help=""Apply preprocessing to data or not"",\n        )\n        group.add_argument(\n            ""--token_type"",\n            type=str,\n            default=""bpe"",\n            choices=[""bpe"", ""char"", ""word""],\n            help=""The text will be tokenized "" ""in the specified level token"",\n        )\n        group.add_argument(\n            ""--bpemodel"",\n            type=str_or_none,\n            default=None,\n            help=""The model file of sentencepiece"",\n        )\n        parser.add_argument(\n            ""--non_linguistic_symbols"",\n            type=str_or_none,\n            help=""non_linguistic_symbols file path"",\n        )\n        for class_choices in cls.class_choices_list:\n            # Append --<name> and --<name>_conf.\n            # e.g. --encoder and --encoder_conf\n            class_choices.add_arguments(group)\n\n    @classmethod\n    def build_collate_fn(\n        cls, args: argparse.Namespace\n    ) -> Callable[\n        [Collection[Tuple[str, Dict[str, np.ndarray]]]],\n        Tuple[List[str], Dict[str, torch.Tensor]],\n    ]:\n        assert check_argument_types()\n        return CommonCollateFn(\n            float_pad_value=0.0, int_pad_value=0, not_sequence=[""spembs""]\n        )\n\n    @classmethod\n    def build_preprocess_fn(\n        cls, args: argparse.Namespace, train: bool\n    ) -> Optional[Callable[[str, Dict[str, np.array]], Dict[str, np.ndarray]]]:\n        assert check_argument_types()\n        if args.use_preprocessor:\n            retval = CommonPreprocessor(\n                train=train,\n                token_type=args.token_type,\n                token_list=args.token_list,\n                bpemodel=args.bpemodel,\n                non_linguistic_symbols=args.non_linguistic_symbols,\n            )\n        else:\n            retval = None\n        assert check_return_type(retval)\n        return retval\n\n    @classmethod\n    def required_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        if not inference:\n            retval = (""text"", ""speech"")\n        else:\n            # Inference mode\n            retval = (""text"",)\n        return retval\n\n    @classmethod\n    def optional_data_names(cls, inference: bool = False) -> Tuple[str, ...]:\n        if not inference:\n            retval = (""spembs"", ""spcs"")\n        else:\n            # Inference mode\n            retval = (""spembs"",)\n        return retval\n\n    @classmethod\n    def build_model(cls, args: argparse.Namespace) -> ESPnetTTSModel:\n        assert check_argument_types()\n        if isinstance(args.token_list, str):\n            with open(args.token_list, encoding=""utf-8"") as f:\n                token_list = [line.rstrip() for line in f]\n\n            # ""args"" is saved as it is in a yaml file by BaseTask.main().\n            # Overwriting token_list to keep it as ""portable"".\n            args.token_list = token_list.copy()\n        elif isinstance(args.token_list, (tuple, list)):\n            token_list = args.token_list.copy()\n        else:\n            raise RuntimeError(""token_list must be str or dict"")\n\n        vocab_size = len(token_list)\n        logging.info(f""Vocabulary size: {vocab_size }"")\n\n        # 1. feats_extract\n        if args.odim is None:\n            # Extract features in the model\n            feats_extract_class = feats_extractor_choices.get_class(args.feats_extract)\n            feats_extract = feats_extract_class(**args.feats_extract_conf)\n            odim = feats_extract.output_size()\n        else:\n            # Give features from data-loader\n            args.feats_extract = None\n            args.feats_extract_conf = None\n            feats_extract = None\n            odim = args.odim\n\n        # 2. Normalization layer\n        if args.normalize is not None:\n            normalize_class = normalize_choices.get_class(args.normalize)\n            normalize = normalize_class(**args.normalize_conf)\n        else:\n            normalize = None\n\n        # 3. TTS\n        tts_class = tts_choices.get_class(args.tts)\n        tts = tts_class(idim=vocab_size, odim=odim, **args.tts_conf)\n\n        # 4. Build model\n        model = ESPnetTTSModel(\n            feats_extract=feats_extract,\n            normalize=normalize,\n            tts=tts,\n            **args.model_conf,\n        )\n        assert check_return_type(model)\n        return model\n'"
espnet2/text/__init__.py,0,b''
espnet2/text/abs_tokenizer.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Iterable\nfrom typing import List\n\n\nclass AbsTokenizer(ABC):\n    @abstractmethod\n    def text2tokens(self, line: str) -> List[str]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def tokens2text(self, tokens: Iterable[str]) -> str:\n        raise NotImplementedError\n'"
espnet2/text/build_tokenizer.py,0,"b'from pathlib import Path\nfrom typing import Iterable\nfrom typing import Union\n\nfrom typeguard import check_argument_types\n\nfrom espnet2.text.abs_tokenizer import AbsTokenizer\nfrom espnet2.text.char_tokenizer import CharTokenizer\nfrom espnet2.text.sentencepiece_tokenizer import SentencepiecesTokenizer\nfrom espnet2.text.word_tokenizer import WordTokenizer\n\n\ndef build_tokenizer(\n    token_type: str,\n    bpemodel: Union[Path, str, Iterable[str]] = None,\n    non_linguistic_symbols: Union[Path, str, Iterable[str]] = None,\n    remove_non_linguistic_symbols: bool = False,\n    space_symbol: str = ""<space>"",\n    delimiter: str = None,\n) -> AbsTokenizer:\n    """"""A helper function to instantiate Tokenizer""""""\n    assert check_argument_types()\n    if token_type == ""bpe"":\n        if bpemodel is None:\n            raise ValueError(\'bpemodel is required if token_type = ""bpe""\')\n\n        if remove_non_linguistic_symbols:\n            raise RuntimeError(\n                ""remove_non_linguistic_symbols is not implemented for token_type=bpe""\n            )\n        return SentencepiecesTokenizer(bpemodel)\n\n    elif token_type == ""word"":\n        if remove_non_linguistic_symbols and non_linguistic_symbols is not None:\n            return WordTokenizer(\n                delimiter=delimiter,\n                non_linguistic_symbols=non_linguistic_symbols,\n                remove_non_linguistic_symbols=True,\n            )\n        else:\n            return WordTokenizer(delimiter=delimiter)\n\n    elif token_type == ""char"":\n        return CharTokenizer(\n            non_linguistic_symbols=non_linguistic_symbols,\n            space_symbol=space_symbol,\n            remove_non_linguistic_symbols=remove_non_linguistic_symbols,\n        )\n\n    else:\n        raise ValueError(\n            f""token_mode must be one of bpe, word, or char: "" f""{token_type}""\n        )\n'"
espnet2/text/char_tokenizer.py,0,"b'from pathlib import Path\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Union\n\nfrom typeguard import check_argument_types\n\nfrom espnet2.text.abs_tokenizer import AbsTokenizer\n\n\nclass CharTokenizer(AbsTokenizer):\n    def __init__(\n        self,\n        non_linguistic_symbols: Union[Path, str, Iterable[str]] = None,\n        space_symbol: str = ""<space>"",\n        remove_non_linguistic_symbols: bool = False,\n    ):\n        assert check_argument_types()\n        self.space_symbol = space_symbol\n        if non_linguistic_symbols is None:\n            self.non_linguistic_symbols = set()\n        elif isinstance(non_linguistic_symbols, (Path, str)):\n            non_linguistic_symbols = Path(non_linguistic_symbols)\n            with non_linguistic_symbols.open(""r"", encoding=""utf-8"") as f:\n                self.non_linguistic_symbols = set(line.rstrip() for line in f)\n        else:\n            self.non_linguistic_symbols = set(non_linguistic_symbols)\n        self.remove_non_linguistic_symbols = remove_non_linguistic_symbols\n\n    def __repr__(self):\n        return (\n            f""{self.__class__.__name__}(""\n            f\'space_symbol=""{self.space_symbol}""\'\n            f\'non_linguistic_symbols=""{self.non_linguistic_symbols}""\'\n            f"")""\n        )\n\n    def text2tokens(self, line: str) -> List[str]:\n        tokens = []\n        while len(line) != 0:\n            for w in self.non_linguistic_symbols:\n                if line.startswith(w):\n                    if not self.remove_non_linguistic_symbols:\n                        tokens.append(line[: len(w)])\n                    line = line[len(w) :]\n                    break\n            else:\n                t = line[0]\n                if t == "" "":\n                    t = ""<space>""\n                tokens.append(t)\n                line = line[1:]\n        return tokens\n\n    def tokens2text(self, tokens: Iterable[str]) -> str:\n        tokens = [t if t != self.space_symbol else "" "" for t in tokens]\n        return """".join(tokens)\n'"
espnet2/text/sentencepiece_tokenizer.py,0,"b'from pathlib import Path\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Union\n\nimport sentencepiece as spm\nfrom typeguard import check_argument_types\n\nfrom espnet2.text.abs_tokenizer import AbsTokenizer\n\n\nclass SentencepiecesTokenizer(AbsTokenizer):\n    def __init__(self, model: Union[Path, str]):\n        assert check_argument_types()\n        self.model = str(model)\n        # NOTE(kamo):\n        # Don\'t build SentencePieceProcessor in __init__()\n        # because it\'s not picklable and it may cause following error,\n        # ""TypeError: can\'t pickle SwigPyObject objects"",\n        # when giving it as argument of ""multiprocessing.Process()"".\n        self.sp = None\n\n    def __repr__(self):\n        return f\'{self.__class__.__name__}(model=""{self.model}"")\'\n\n    def _build_sentence_piece_processor(self):\n        # Build SentencePieceProcessor lazily.\n        if self.sp is None:\n            self.sp = spm.SentencePieceProcessor()\n            self.sp.load(self.model)\n\n    def text2tokens(self, line: str) -> List[str]:\n        self._build_sentence_piece_processor()\n        return self.sp.EncodeAsPieces(line)\n\n    def tokens2text(self, tokens: Iterable[str]) -> str:\n        self._build_sentence_piece_processor()\n        return self.sp.DecodePieces(list(tokens))\n'"
espnet2/text/token_id_converter.py,0,"b'from pathlib import Path\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Union\n\nimport numpy as np\nfrom typeguard import check_argument_types\n\n\nclass TokenIDConverter:\n    def __init__(\n        self, token_list: Union[Path, str, Iterable[str]], unk_symbol: str = ""<unk>"",\n    ):\n        assert check_argument_types()\n\n        if isinstance(token_list, (Path, str)):\n            token_list = Path(token_list)\n            self.token_list_repr = str(token_list)\n            self.token_list: List[str] = []\n\n            with token_list.open(""r"", encoding=""utf-8"") as f:\n                for idx, line in enumerate(f):\n                    line = line.rstrip()\n                    self.token_list.append(line)\n\n        else:\n            self.token_list: List[str] = list(token_list)\n            self.token_list_repr = """"\n            for i, t in enumerate(self.token_list):\n                if i == 3:\n                    break\n                self.token_list_repr += f""{t}, ""\n            self.token_list_repr += f""... (NVocab={(len(self.token_list))})""\n\n        self.token2id: Dict[str, int] = {}\n        for i, t in enumerate(self.token_list):\n            if t in self.token2id:\n                raise RuntimeError(f\'Symbol ""{t}"" is duplicated\')\n            self.token2id[t] = i\n\n        self.unk_symbol = unk_symbol\n        if self.unk_symbol not in self.token2id:\n            raise RuntimeError(\n                f""Unknown symbol \'{unk_symbol}\' doesn\'t exist in the token_list""\n            )\n        self.unk_id = self.token2id[self.unk_symbol]\n\n    def get_num_vocabulary_size(self) -> int:\n        return len(self.token_list)\n\n    def ids2tokens(self, integers: Union[np.ndarray, Iterable[int]]) -> List[str]:\n        if isinstance(integers, np.ndarray) and integers.ndim != 1:\n            raise ValueError(f""Must be 1 dim ndarray, but got {integers.ndim}"")\n        return [self.token_list[i] for i in integers]\n\n    def tokens2ids(self, tokens: Iterable[str]) -> List[int]:\n        return [self.token2id.get(i, self.unk_id) for i in tokens]\n'"
espnet2/text/word_tokenizer.py,0,"b'from pathlib import Path\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Union\nimport warnings\n\nfrom typeguard import check_argument_types\n\nfrom espnet2.text.abs_tokenizer import AbsTokenizer\n\n\nclass WordTokenizer(AbsTokenizer):\n    def __init__(\n        self,\n        delimiter: str = None,\n        non_linguistic_symbols: Union[Path, str, Iterable[str]] = None,\n        remove_non_linguistic_symbols: bool = False,\n    ):\n        assert check_argument_types()\n        self.delimiter = delimiter\n\n        if not remove_non_linguistic_symbols and non_linguistic_symbols is not None:\n            warnings.warn(\n                ""non_linguistic_symbols is only used ""\n                ""when remove_non_linguistic_symbols = True""\n            )\n\n        if non_linguistic_symbols is None:\n            self.non_linguistic_symbols = set()\n        elif isinstance(non_linguistic_symbols, (Path, str)):\n            non_linguistic_symbols = Path(non_linguistic_symbols)\n            with non_linguistic_symbols.open(""r"", encoding=""utf-8"") as f:\n                self.non_linguistic_symbols = set(line.rstrip() for line in f)\n        else:\n            self.non_linguistic_symbols = set(non_linguistic_symbols)\n        self.remove_non_linguistic_symbols = remove_non_linguistic_symbols\n\n    def __repr__(self):\n        return f\'{self.__class__.__name__}(delimiter=""{self.delimiter}"")\'\n\n    def text2tokens(self, line: str) -> List[str]:\n        tokens = []\n        for t in line.split(self.delimiter):\n            if self.remove_non_linguistic_symbols and t in self.non_linguistic_symbols:\n                continue\n            tokens.append(t)\n        return tokens\n\n    def tokens2text(self, tokens: Iterable[str]) -> str:\n        if self.delimiter is None:\n            delimiter = "" ""\n        else:\n            delimiter = self.delimiter\n        return delimiter.join(tokens)\n'"
espnet2/torch_utils/__init__.py,0,b''
espnet2/torch_utils/add_gradient_noise.py,2,"b'import torch\n\n\ndef add_gradient_noise(\n    model: torch.nn.Module,\n    iteration: int,\n    duration: float = 100,\n    eta: float = 1.0,\n    scale_factor: float = 0.55,\n):\n    """"""Adds noise from a standard normal distribution to the gradients.\n\n    The standard deviation (`sigma`) is controlled\n    by the three hyper-parameters below.\n    `sigma` goes to zero (no noise) with more iterations.\n\n    Args:\n        model: Model.\n        iteration: Number of iterations.\n        duration: {100, 1000}: Number of durations to control\n            the interval of the `sigma` change.\n        eta: {0.01, 0.3, 1.0}: The magnitude of `sigma`.\n        scale_factor: {0.55}: The scale of `sigma`.\n    """"""\n    interval = (iteration // duration) + 1\n    sigma = eta / interval ** scale_factor\n    for param in model.parameters():\n        if param.grad is not None:\n            _shape = param.grad.size()\n            noise = sigma * torch.randn(_shape).to(param.device)\n            param.grad += noise\n'"
espnet2/torch_utils/device_funcs.py,9,"b'import dataclasses\nimport warnings\n\nimport numpy as np\nimport torch\n\n\ndef to_device(data, device=None, dtype=None, non_blocking=False, copy=False):\n    """"""Change the device of object recursively""""""\n    if isinstance(data, dict):\n        return {\n            k: to_device(v, device, dtype, non_blocking, copy) for k, v in data.items()\n        }\n    elif dataclasses.is_dataclass(data) and not isinstance(data, type):\n        return type(data)(\n            *[\n                to_device(v, device, dtype, non_blocking, copy)\n                for v in dataclasses.astuple(data)\n            ]\n        )\n    # maybe namedtuple. I don\'t know the correct way to judge namedtuple.\n    elif isinstance(data, tuple) and type(data) is not tuple:\n        return type(data)(\n            *[to_device(o, device, dtype, non_blocking, copy) for o in data]\n        )\n    elif isinstance(data, (list, tuple)):\n        return type(data)(to_device(v, device, dtype, non_blocking, copy) for v in data)\n    elif isinstance(data, np.ndarray):\n        return to_device(torch.from_numpy(data), device, dtype, non_blocking, copy)\n    elif isinstance(data, torch.Tensor):\n        return data.to(device, dtype, non_blocking, copy)\n    else:\n        return data\n\n\ndef force_gatherable(data, device):\n    """"""Change object to gatherable in torch.nn.DataParallel recursively\n\n    The difference from to_device() is changing to torch.Tensor if float or int\n    value is found.\n\n    The restriction to the returned value in DataParallel:\n        The object must be\n        - torch.cuda.Tensor\n        - 1 or more dimension. 0-dimension-tensor sends warning.\n        or a list, tuple, dict.\n\n    """"""\n    if isinstance(data, dict):\n        return {k: force_gatherable(v, device) for k, v in data.items()}\n    # DataParallel can\'t handle NamedTuple well\n    elif isinstance(data, tuple) and type(data) is not tuple:\n        return type(data)(*[force_gatherable(o, device) for o in data])\n    elif isinstance(data, (list, tuple, set)):\n        return type(data)(force_gatherable(v, device) for v in data)\n    elif isinstance(data, np.ndarray):\n        return force_gatherable(torch.from_numpy(data), device)\n    elif isinstance(data, torch.Tensor):\n        if data.dim() == 0:\n            # To 1-dim array\n            data = data[None]\n        return data.to(device)\n    elif isinstance(data, float):\n        return torch.tensor([data], dtype=torch.float, device=device)\n    elif isinstance(data, int):\n        return torch.tensor([data], dtype=torch.long, device=device)\n    elif data is None:\n        return None\n    else:\n        warnings.warn(f""{type(data)} may not be gatherable by DataParallel"")\n        return data\n'"
espnet2/torch_utils/forward_adaptor.py,6,"b'import torch\nfrom typeguard import check_argument_types\n\n\nclass ForwardAdaptor(torch.nn.Module):\n    """"""Wrapped module to parallelize specified method\n\n    torch.nn.DataParallel parallelizes only ""forward()""\n    and, maybe, the method having the other name can\'t be applied\n    except for wrapping the module just like this class.\n\n    Examples:\n        >>> class A(torch.nn.Module):\n        ...     def foo(self, x):\n        ...         ...\n        >>> model = A()\n        >>> model = ForwardAdaptor(model, ""foo"")\n        >>> model = torch.nn.DataParallel(model, device_ids=[0, 1])\n        >>> x = torch.randn(2, 10)\n        >>> model(x)\n    """"""\n\n    def __init__(self, module: torch.nn.Module, name: str):\n        assert check_argument_types()\n        super().__init__()\n        self.module = module\n        self.name = name\n        if not hasattr(module, name):\n            raise ValueError(f""{module} doesn\'t have {name}"")\n\n    def forward(self, *args, **kwargs):\n        func = getattr(self.module, self.name)\n        return func(*args, **kwargs)\n'"
espnet2/torch_utils/initialize.py,9,"b'import math\n\nimport torch\nfrom typeguard import check_argument_types\n\n\ndef initialize(model: torch.nn.Module, init: str):\n    assert check_argument_types()\n\n    if init == ""chainer"":\n        # 1. lecun_normal_init_parameters\n        for p in model.parameters():\n            data = p.data\n            if data.dim() == 1:\n                # bias\n                data.zero_()\n            elif data.dim() == 2:\n                # linear weight\n                n = data.size(1)\n                stdv = 1.0 / math.sqrt(n)\n                data.normal_(0, stdv)\n            elif data.dim() in (3, 4):\n                # conv weight\n                n = data.size(1)\n                for k in data.size()[2:]:\n                    n *= k\n                stdv = 1.0 / math.sqrt(n)\n                data.normal_(0, stdv)\n            else:\n                raise NotImplementedError\n\n        for mod in model.modules():\n            # 2. embed weight ~ Normal(0, 1)\n            if isinstance(mod, torch.nn.Embedding):\n                mod.weight.data.normal_(0, 1)\n            # 3. forget-bias = 1.0\n            elif isinstance(mod, torch.nn.RNNCellBase):\n                n = mod.bias_ih.size(0)\n                mod.bias_ih.data[n // 4 : n // 2].fill_(1.0)\n            elif isinstance(mod, torch.nn.RNNBase):\n                for name, param in mod.named_parameters():\n                    if ""bias"" in name:\n                        n = param.size(0)\n                        param.data[n // 4 : n // 2].fill_(1.0)\n\n    else:\n        # weight init\n        for p in model.parameters():\n            if p.dim() > 1:\n                if init == ""xavier_uniform"":\n                    torch.nn.init.xavier_uniform_(p.data)\n                elif init == ""xavier_normal"":\n                    torch.nn.init.xavier_normal_(p.data)\n                elif init == ""kaiming_uniform"":\n                    torch.nn.init.kaiming_uniform_(p.data, nonlinearity=""relu"")\n                elif init == ""kaiming_normal"":\n                    torch.nn.init.kaiming_normal_(p.data, nonlinearity=""relu"")\n                else:\n                    raise ValueError(""Unknown initialization: "" + init)\n        # bias init\n        for p in model.parameters():\n            if p.dim() == 1:\n                p.data.zero_()\n\n        # reset some modules with default init\n        for m in model.modules():\n            if isinstance(m, (torch.nn.Embedding, torch.nn.LayerNorm)):\n                m.reset_parameters()\n'"
espnet2/torch_utils/load_pretrained_model.py,6,"b'from pathlib import Path\nfrom typing import Any\nfrom typing import Union\n\nimport torch\nimport torch.nn\nimport torch.optim\n\n\ndef load_pretrained_model(\n    pretrain_path: Union[str, Path],\n    model: torch.nn.Module,\n    pretrain_key: str = None,\n    map_location: str = ""cpu"",\n    ignore_not_existing_keys: bool = True,\n):\n    """"""Load a model state and set it to the model.\n\n    Examples:\n        >>> load_pretrained_model(""somewhere/model.pth"", model)\n        >>> load_pretrained_model(""somewhere/encoder.pth"", model, ""encoder"")\n    """"""\n    if pretrain_key is None:\n        obj = model\n    else:\n\n        def get_attr(obj: Any, key: str):\n            """"""Get an nested attribute.\n\n            >>> class A(torch.nn.Module):\n            ...     def __init__(self):\n            ...         super().__init__()\n            ...         self.linear = torch.nn.Linear(10, 10)\n            >>> a = A()\n            >>> assert A.linear.weight is get_attr(A, \'linear.weight\')\n\n            """"""\n            if key.strip() == """":\n                return obj\n            for k in key.split("".""):\n                obj = getattr(obj, k)\n            return obj\n\n        obj = get_attr(model, pretrain_key)\n\n    state_dict = obj.state_dict()\n    pretrained_dict = torch.load(pretrain_path, map_location=map_location)\n    if ignore_not_existing_keys:\n        # Ignores the parameters not existing in the train-model\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in state_dict}\n    state_dict.update(pretrained_dict)\n    obj.load_state_dict(state_dict)\n'"
espnet2/torch_utils/pytorch_version.py,6,"b'import torch\n\n\ndef pytorch_cudnn_version() -> str:\n    message = (\n        f""pytorch.version={torch.__version__}, ""\n        f""cuda.available={torch.cuda.is_available()}, ""\n    )\n\n    if torch.backends.cudnn.enabled:\n        message += (\n            f""cudnn.version={torch.backends.cudnn.version()}, ""\n            f""cudnn.benchmark={torch.backends.cudnn.benchmark}, ""\n            f""cudnn.deterministic={torch.backends.cudnn.benchmark}""\n        )\n    return message\n'"
espnet2/torch_utils/recursive_op.py,11,"b'from distutils.version import LooseVersion\n\nimport torch\n\nif torch.distributed.is_available():\n    if LooseVersion(torch.__version__) > LooseVersion(""1.0.1""):\n        from torch.distributed import ReduceOp\n    else:\n        from torch.distributed import reduce_op as ReduceOp\nelse:\n    ReduceOp = None\n\n\ndef recursive_sum(obj, weight: torch.Tensor, distributed: bool = False):\n    assert weight.dim() == 1, weight.size()\n    if isinstance(obj, (tuple, list)):\n        return type(obj)(recursive_sum(v, weight, distributed) for v in obj)\n    elif isinstance(obj, dict):\n        return {k: recursive_sum(v, weight, distributed) for k, v in obj.items()}\n    elif isinstance(obj, torch.Tensor):\n        assert obj.size() == weight.size(), (obj.size(), weight.size())\n        obj = (obj * weight.type(obj.dtype)).sum()\n        if distributed:\n            torch.distributed.all_reduce(obj, op=ReduceOp.SUM)\n        return obj\n    elif obj is None:\n        return None\n    else:\n        raise ValueError(type(obj))\n\n\ndef recursive_divide(a, b: torch.Tensor):\n    if isinstance(a, (tuple, list)):\n        return type(a)(recursive_divide(v, b) for v in a)\n    elif isinstance(a, dict):\n        return {k: recursive_divide(v, b) for k, v in a.items()}\n    elif isinstance(a, torch.Tensor):\n        assert a.size() == b.size(), (a.size(), b.size())\n        return a / b.type(a.dtype)\n    elif a is None:\n        return None\n    else:\n        raise ValueError(type(a))\n\n\ndef recursive_average(obj, weight: torch.Tensor, distributed: bool = False):\n    obj = recursive_sum(obj, weight, distributed)\n    weight = weight.sum()\n    if distributed:\n        torch.distributed.all_reduce(weight, op=ReduceOp.SUM)\n    # Normalize weight to be sum-to-1\n    obj = recursive_divide(obj, weight)\n    return obj, weight\n'"
espnet2/torch_utils/set_all_random_seed.py,1,b'import random\n\nimport numpy as np\nimport torch\n\n\ndef set_all_random_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n'
espnet2/train/__init__.py,0,b''
espnet2/train/abs_espnet_model.py,5,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Dict\nfrom typing import Tuple\n\nimport torch\n\n\nclass AbsESPnetModel(torch.nn.Module, ABC):\n    """"""The common abstract class among each tasks\n\n    ""ESPnetModel"" is referred to a class which inherits torch.nn.Module,\n    and makes the dnn-models forward as its member field,\n    a.k.a delegate pattern,\n    and defines ""loss"", ""stats"", and ""weight"" for the task.\n\n    If you intend to implement new task in ESPNet,\n    the model must inherit this class.\n    In other words, the ""mediator"" objects between\n    our training system and the your task class are\n    just only these three values, loss, stats, and weight.\n\n    Example:\n        >>> from espnet2.tasks.abs_task import AbsTask\n        >>> class YourESPnetModel(AbsESPnetModel):\n        ...     def forward(self, input, input_lengths):\n        ...         ...\n        ...         return loss, stats, weight\n        >>> class YourTask(AbsTask):\n        ...     @classmethod\n        ...     def build_model(cls, args: argparse.Namespace) -> YourESPnetModel:\n    """"""\n\n    @abstractmethod\n    def forward(\n        self, **batch: torch.Tensor\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def collect_feats(self, **batch: torch.Tensor) -> Dict[str, torch.Tensor]:\n        raise NotImplementedError\n'"
espnet2/train/class_choices.py,0,"b'from typing import Mapping\nfrom typing import Optional\nfrom typing import Tuple\n\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nfrom espnet2.utils.nested_dict_action import NestedDictAction\nfrom espnet2.utils.types import str_or_none\n\n\nclass ClassChoices:\n    """"""Helper class to manage the options for variable objects and its configuration.\n\n    Example:\n\n    >>> class A:\n    ...     def __init__(self, foo=3):  pass\n    >>> class B:\n    ...     def __init__(self, bar=""aaaa""):  pass\n    >>> choices = ClassChoices(""var"", dict(a=A, b=B), default=""a"")\n    >>> import argparse\n    >>> parser = argparse.ArgumentParser()\n    >>> choices.add_arguments(parser)\n    >>> args = parser.parse_args([""--var"", ""a"", ""--var_conf"", ""foo=4"")\n    >>> args.var\n    a\n    >>> args.var_conf\n    {""foo"": 4}\n    >>> class_obj = choices.get_class(args.var)\n    >>> a_object = class_obj(**args.var_conf)\n\n    """"""\n\n    def __init__(\n        self,\n        name: str,\n        classes: Mapping[str, type],\n        type_check: type = None,\n        default: str = None,\n        optional: bool = False,\n    ):\n        assert check_argument_types()\n        self.name = name\n        self.base_type = type_check\n        self.classes = {k.lower(): v for k, v in classes.items()}\n        if ""none"" in self.classes or ""nil"" in self.classes or ""null"" in self.classes:\n            raise ValueError(\'""none"", ""nil"", and ""null"" are reserved.\')\n        if type_check is not None:\n            for v in self.classes.values():\n                if not issubclass(v, type_check):\n                    raise ValueError(f""must be {type_check.__name__}, but got {v}"")\n\n        self.optional = optional\n        self.default = default\n        if default is None:\n            self.optional = True\n\n    def choices(self) -> Tuple[Optional[str], ...]:\n        retval = tuple(self.classes)\n        if self.optional:\n            return retval + (None,)\n        else:\n            return retval\n\n    def get_class(self, name: Optional[str]) -> Optional[type]:\n        assert check_argument_types()\n        if name is None or (self.optional and name.lower() == (""none"", ""null"", ""nil"")):\n            retval = None\n        elif name.lower() in self.classes:\n            class_obj = self.classes[name]\n            assert check_return_type(class_obj)\n            retval = class_obj\n        else:\n            raise ValueError(\n                f""--{self.name} must be one of {self.choices()}: ""\n                f""--{self.name} {name.lower()}""\n            )\n\n        return retval\n\n    def add_arguments(self, parser):\n        parser.add_argument(\n            f""--{self.name}"",\n            type=lambda x: str_or_none(x.lower()),\n            default=self.default,\n            choices=self.choices(),\n            help=f""The {self.name} type"",\n        )\n        parser.add_argument(\n            f""--{self.name}_conf"",\n            action=NestedDictAction,\n            default=dict(),\n            help=f""The keyword arguments for {self.name}"",\n        )\n'"
espnet2/train/collate_fn.py,5,"b'from typing import Collection\nfrom typing import Dict\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\nclass CommonCollateFn:\n    """"""Functor class of common_collate_fn()""""""\n\n    def __init__(\n        self,\n        float_pad_value: Union[float, int] = 0.0,\n        int_pad_value: int = -32768,\n        not_sequence: Collection[str] = (),\n    ):\n        assert check_argument_types()\n        self.float_pad_value = float_pad_value\n        self.int_pad_value = int_pad_value\n        self.not_sequence = set(not_sequence)\n\n    def __repr__(self):\n        return (\n            f""{self.__class__}(float_pad_value={self.float_pad_value}, ""\n            f""int_pad_value={self.float_pad_value})""\n        )\n\n    def __call__(\n        self, data: Collection[Tuple[str, Dict[str, np.ndarray]]]\n    ) -> Tuple[List[str], Dict[str, torch.Tensor]]:\n        return common_collate_fn(\n            data,\n            float_pad_value=self.float_pad_value,\n            int_pad_value=self.int_pad_value,\n            not_sequence=self.not_sequence,\n        )\n\n\ndef common_collate_fn(\n    data: Collection[Tuple[str, Dict[str, np.ndarray]]],\n    float_pad_value: Union[float, int] = 0.0,\n    int_pad_value: int = -32768,\n    not_sequence: Collection[str] = (),\n) -> Tuple[List[str], Dict[str, torch.Tensor]]:\n    """"""Concatenate ndarray-list to an array and convert to torch.Tensor.\n\n    Examples:\n        >>> from espnet2.samplers.constant_batch_sampler import ConstantBatchSampler,\n        >>> import espnet2.tasks.abs_task\n        >>> from espnet2.train.dataset import ESPnetDataset\n        >>> sampler = ConstantBatchSampler(...)\n        >>> dataset = ESPnetDataset(...)\n        >>> keys = next(iter(sampler)\n        >>> batch = [dataset[key] for key in keys]\n        >>> batch = common_collate_fn(batch)\n        >>> model(**batch)\n\n        Note that the dict-keys of batch are propagated from\n        that of the dataset as they are.\n\n    """"""\n    assert check_argument_types()\n    uttids = [u for u, _ in data]\n    data = [d for _, d in data]\n\n    assert all(set(data[0]) == set(d) for d in data), ""dict-keys mismatching""\n    assert all(\n        not k.endswith(""_lengths"") for k in data[0]\n    ), f""*_lengths is reserved: {list(data[0])}""\n\n    output = {}\n    for key in data[0]:\n        # NOTE(kamo):\n        # Each models, which accepts these values finally, are responsible\n        # to repaint the pad_value to the desired value for each tasks.\n        if data[0][key].dtype.kind == ""i"":\n            pad_value = int_pad_value\n        else:\n            pad_value = float_pad_value\n\n        array_list = [d[key] for d in data]\n\n        # Assume the first axis is length:\n        # tensor_list: Batch x (Length, ...)\n        tensor_list = [torch.from_numpy(a) for a in array_list]\n        # tensor: (Batch, Length, ...)\n        tensor = pad_list(tensor_list, pad_value)\n        output[key] = tensor\n\n        assert all(len(d[key]) != 0 for d in data), [len(d[key]) for d in data]\n\n        # lens: (Batch,)\n        if key not in not_sequence:\n            lens = torch.tensor([d[key].shape[0] for d in data], dtype=torch.long)\n            output[key + ""_lengths""] = lens\n\n    output = (uttids, output)\n    assert check_return_type(output)\n    return output\n'"
espnet2/train/dataset.py,16,"b'import collections\nimport copy\nimport functools\nimport logging\nimport numbers\nimport re\nfrom typing import Callable\nfrom typing import Collection\nfrom typing import Dict\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Tuple\nfrom typing import Union\n\nimport h5py\nimport humanfriendly\nimport kaldiio\nimport numpy as np\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nfrom espnet2.utils.fileio import load_num_sequence_text\nfrom espnet2.utils.fileio import NpyScpReader\nfrom espnet2.utils.fileio import read_2column_text\nfrom espnet2.utils.fileio import SoundScpReader\nfrom espnet2.utils.rand_gen_dataset import FloatRandomGenerateDataset\nfrom espnet2.utils.rand_gen_dataset import IntRandomGenerateDataset\nfrom espnet2.utils.sized_dict import SizedDict\n\n\nclass AdapterForSoundScpReader(collections.abc.Mapping):\n    def __init__(self, loader, dtype=None):\n        assert check_argument_types()\n        self.loader = loader\n        self.dtype = dtype\n        self.rate = None\n\n    def keys(self):\n        return self.loader.keys()\n\n    def __len__(self):\n        return len(self.loader)\n\n    def __iter__(self):\n        return iter(self.loader)\n\n    def __getitem__(self, key: str) -> np.ndarray:\n        rate, array = self.loader[key]\n        if self.rate is not None and self.rate != rate:\n            raise RuntimeError(f""Sampling rates are mismatched: {self.rate} != {rate}"")\n        self.rate = rate\n        # Multichannel wave fie\n        # array: (NSample, Channel) or (Nsample)\n        if self.dtype is not None:\n            array = array.astype(self.dtype)\n        return array\n\n\nclass H5FileWrapper:\n    def __init__(self, path: str):\n        self.path = path\n        self.h5_file = h5py.File(path, ""r"")\n\n    def __repr__(self) -> str:\n        return str(self.h5_file)\n\n    def __len__(self) -> int:\n        return len(self.h5_file)\n\n    def __iter__(self):\n        return iter(self.h5_file)\n\n    def __getitem__(self, key) -> Union[np.ndarray, Dict[str, np.ndarray]]:\n        value = self.h5_file[key]\n        if isinstance(value, h5py.Group):\n            for k, v in value.items():\n                if not isinstance(v, h5py.Dataset):\n                    raise RuntimeError(\n                        f""Invalid h5-file. Must be 1 or 2 level HDF5: {self.path}""\n                    )\n            return {k: v[()] for k, v in value.items()}\n        else:\n            return value[()]\n\n\ndef sound_loader(path, float_dtype):\n    # The file is as follows:\n    #   utterance_id_A /some/where/a.wav\n    #   utterance_id_B /some/where/a.flac\n\n    # NOTE(kamo): SoundScpReader doesn\'t support pipe-fashion\n    # like Kaldi e.g. ""cat a.wav |"".\n    # NOTE(kamo): The audio signal is normalized to [-1,1] range.\n    loader = SoundScpReader(path, normalize=True, always_2d=False)\n\n    # SoundScpReader.__getitem__() returns Tuple[int, ndarray],\n    # but ndarray is desired, so Adapter class is inserted here\n    return AdapterForSoundScpReader(loader, float_dtype)\n\n\ndef pipe_wav_loader(path, float_dtype):\n    # The file is as follows:\n    #   utterance_id_A cat a.wav |\n    #   utterance_id_B cat b.wav |\n\n    # NOTE(kamo): I don\'t think this case is practical\n    # because subprocess takes much times due to fork().\n\n    # NOTE(kamo): kaldiio doesn\'t normalize the signal.\n    loader = kaldiio.load_scp(path)\n    return AdapterForSoundScpReader(loader, float_dtype)\n\n\ndef rand_int_loader(filepath, loader_type):\n    # e.g. rand_int_3_10\n    try:\n        low, high = map(int, loader_type[len(""rand_int_"") :].split(""_""))\n    except ValueError:\n        raise RuntimeError(f""e.g rand_int_3_10: but got {loader_type}"")\n    return IntRandomGenerateDataset(filepath, low, high)\n\n\ndef imagefolder_loader(filepath, loader_type):\n    # torchvision is not mandatory for espnet\n    import torchvision\n\n    # e.g. imagefolder_256x256\n    # /\n    #   |- horse/\n    #   \xe2\x94\x82    |- 8537.png\n    #   \xe2\x94\x82    |- ...\n    #   |- butterfly/\n    #   \xe2\x94\x82    |- 2857.png\n    #   \xe2\x94\x82    |- ...\n    try:\n        _, image_size = loader_type.split(""_"")\n        height, width = map(int, image_size.split(""x""))\n    except ValueError:\n        raise RuntimeError(f""e.g imagefolder_256x256: but got {loader_type}"")\n\n    # folder dataset\n    return torchvision.datasets.ImageFolder(\n        root=filepath,\n        transform=torchvision.transforms.Compose(\n            [\n                torchvision.transforms.Resize([height, width]),\n                torchvision.transforms.ToTensor(),\n            ]\n        ),\n    )\n\n\ndef mnist_loader(filepath, loader_type):\n    # torchvision is not mandatory for espnet\n    import torchvision\n\n    # e.g. mnist_train_128x128\n    try:\n        _, train_test, image_size = loader_type.split(""_"")\n        if train_test not in [""train"", ""test""]:\n            raise ValueError\n        height, width = map(int, image_size.split(""x""))\n    except ValueError:\n        raise RuntimeError(f""e.g mnist_train_256x256: but got {loader_type}"")\n\n    return torchvision.datasets.MNIST(\n        root=filepath,\n        train=train_test == ""train"",\n        download=True,\n        transform=torchvision.transforms.Compose(\n            [\n                torchvision.transforms.Resize([height, width]),\n                torchvision.transforms.ToTensor(),\n            ]\n        ),\n    )\n\n\nDATA_TYPES = {\n    ""sound"": dict(\n        func=sound_loader,\n        kwargs=[""float_dtype""],\n        help=""Audio format types which supported by sndfile wav, flac, etc.""\n        ""\\n\\n""\n        ""   utterance_id_a a.wav\\n""\n        ""   utterance_id_b b.wav\\n""\n        ""   ..."",\n    ),\n    ""pipe_wav"": dict(\n        func=pipe_wav_loader,\n        kwargs=[""float_dtype""],\n        help=""Kaldi wav.scp file. If the file doesn\'t include a pipe, \'|\' ""\n        ""for each line, use \'sound\' instead.""\n        "":\\n\\n""\n        ""   utterance_id_a cat a.wav |\\n""\n        ""   utterance_id_b cat b.wav |\\n""\n        ""   ..."",\n    ),\n    ""kaldi_ark"": dict(\n        func=kaldiio.load_scp,\n        kwargs=[],\n        help=""Kaldi-ark file type.""\n        ""\\n\\n""\n        ""   utterance_id_A /some/where/a.ark:123\\n""\n        ""   utterance_id_B /some/where/a.ark:456\\n""\n        ""   ..."",\n    ),\n    ""npy"": dict(\n        func=NpyScpReader,\n        kwargs=[],\n        help=""Npy file format.""\n        ""\\n\\n""\n        ""   utterance_id_A /some/where/a.npy\\n""\n        ""   utterance_id_B /some/where/b.npy\\n""\n        ""   ..."",\n    ),\n    ""text_int"": dict(\n        func=functools.partial(load_num_sequence_text, loader_type=""text_int""),\n        kwargs=[],\n        help=""A text file in which is written a sequence of interger numbers ""\n        ""separated by space.""\n        ""\\n\\n""\n        ""   utterance_id_A 12 0 1 3\\n""\n        ""   utterance_id_B 3 3 1\\n""\n        ""   ..."",\n    ),\n    ""csv_int"": dict(\n        func=functools.partial(load_num_sequence_text, loader_type=""csv_int""),\n        kwargs=[],\n        help=""A text file in which is written a sequence of interger numbers ""\n        ""separated by comma.""\n        ""\\n\\n""\n        ""   utterance_id_A 100,80\\n""\n        ""   utterance_id_B 143,80\\n""\n        ""   ..."",\n    ),\n    ""text_float"": dict(\n        func=functools.partial(load_num_sequence_text, loader_type=""text_float""),\n        kwargs=[],\n        help=""A text file in which is written a sequence of float numbers ""\n        ""separated by space.""\n        ""\\n\\n""\n        ""   utterance_id_A 12. 3.1 3.4 4.4\\n""\n        ""   utterance_id_B 3. 3.12 1.1\\n""\n        ""   ..."",\n    ),\n    ""csv_float"": dict(\n        func=functools.partial(load_num_sequence_text, loader_type=""csv_float""),\n        kwargs=[],\n        help=""A text file in which is written a sequence of float numbers ""\n        ""separated by comma.""\n        ""\\n\\n""\n        ""   utterance_id_A 12.,3.1,3.4,4.4\\n""\n        ""   utterance_id_B 3.,3.12,1.1\\n""\n        ""   ..."",\n    ),\n    ""text"": dict(\n        func=read_2column_text,\n        kwargs=[],\n        help=""Return text as is. The text must be converted to ndarray ""\n        ""by \'preprocess\'.""\n        ""\\n\\n""\n        ""   utterance_id_A hello world\\n""\n        ""   utterance_id_B foo bar\\n""\n        ""   ..."",\n    ),\n    ""hdf5"": dict(\n        func=H5FileWrapper,\n        kwargs=[],\n        help=""A HDF5 file which contains arrays at the first level or the second level.""\n        ""\\n\\n""\n        ""   1-level HDF5 file example.\\n""\n        ""   >>> f = h5py.File(\'file.h5\')\\n""\n        ""   >>> array1 = f[\'utterance_id_A\']\\n""\n        ""   >>> array2 = f[\'utterance_id_B\']\\n""\n        ""\\n""\n        ""   2-level HDF5 file example.\\n""\n        ""   >>> f = h5py.File(\'file.h5\')\\n""\n        ""   >>> values = f[\'utterance_id_A\']\\n""\n        ""   >>> input_array = values[\'input\']\\n""\n        ""   >>> target_array = values[\'target\']"",\n    ),\n    ""rand_float"": dict(\n        func=FloatRandomGenerateDataset,\n        kwargs=[],\n        help=""Generate random float-ndarray which has the given shapes ""\n        ""in the file.""\n        ""\\n\\n""\n        ""   utterance_id_A 3,4\\n""\n        ""   utterance_id_B 10,4\\n""\n        ""   ..."",\n    ),\n    ""rand_int_\\\\d+_\\\\d+"": dict(\n        func=rand_int_loader,\n        kwargs=[""loader_type""],\n        help=""e.g. \'rand_int_0_10\'. Generate random int-ndarray which has the given ""\n        ""shapes in the path. ""\n        ""Give the lower and upper value by the file type. e.g. ""\n        ""rand_int_0_10 -> Generate integers from 0 to 10.""\n        ""\\n\\n""\n        ""   utterance_id_A 3,4\\n""\n        ""   utterance_id_B 10,4\\n""\n        ""   ..."",\n    ),\n    ""imagefolder_\\\\d+x\\\\d+"": dict(\n        func=imagefolder_loader,\n        kwargs=[""loader_type""],\n        help=""e.g. \'imagefolder_32x32\'. Using torchvision.datasets.ImageFolder."",\n    ),\n    ""mnist_train_\\\\d+x\\\\d+"": dict(\n        func=mnist_loader,\n        kwargs=[""loader_type""],\n        help=""e.g. \'mnist_train_32x32\'. MNIST train data"",\n    ),\n    ""mnist_test_\\\\d+x\\\\d+"": dict(\n        func=mnist_loader,\n        kwargs=[""loader_type""],\n        help=""e.g. \'mnist_test_32x32\'. MNIST test data"",\n    ),\n}\n\n\nclass ESPnetDataset(Dataset):\n    """"""Pytorch Dataset class for ESPNet.\n\n    Examples:\n        >>> dataset = ESPnetDataset([(\'wav.scp\', \'input\', \'sound\'),\n        ...                          (\'token_int\', \'output\', \'text_int\')],\n        ...                         )\n        ... uttid, data = dataset[\'uttid\']\n        {\'input\': per_utt_array, \'output\': per_utt_array}\n    """"""\n\n    def __init__(\n        self,\n        path_name_type_list: Collection[Tuple[str, str, str]],\n        preprocess: Callable[\n            [str, Dict[str, np.ndarray]], Dict[str, np.ndarray]\n        ] = None,\n        float_dtype: str = ""float32"",\n        int_dtype: str = ""long"",\n        max_cache_size: Union[float, int, str] = 0.0,\n    ):\n        assert check_argument_types()\n        if len(path_name_type_list) == 0:\n            raise ValueError(\n                \'1 or more elements are required for ""path_name_type_list""\'\n            )\n\n        path_name_type_list = copy.deepcopy(path_name_type_list)\n        self.preprocess = preprocess\n\n        self.float_dtype = float_dtype\n        self.int_dtype = int_dtype\n\n        self.loader_dict = {}\n        self.debug_info = {}\n        for path, name, _type in path_name_type_list:\n            if name in self.loader_dict:\n                raise RuntimeError(f\'""{name}"" is duplicated for data-key\')\n\n            loader = self._build_loader(path, _type)\n            self.loader_dict[name] = loader\n            self.debug_info[name] = path, _type\n            if len(self.loader_dict[name]) == 0:\n                raise RuntimeError(f""{path} has no samples"")\n\n            # TODO(kamo): Should check consistency of each utt-keys?\n\n        if isinstance(max_cache_size, str):\n            max_cache_size = humanfriendly.parse_size(max_cache_size)\n        self.max_cache_size = max_cache_size\n        if max_cache_size > 0:\n            self.cache = SizedDict(shared=True)\n        else:\n            self.cache = None\n\n    def _build_loader(\n        self, path: str, loader_type: str\n    ) -> Mapping[\n        str,\n        Union[\n            np.ndarray,\n            torch.Tensor,\n            str,\n            numbers.Number,\n            Tuple[Union[np.ndarray, torch.Tensor, str, numbers.Number]],\n            List[Union[np.ndarray, torch.Tensor, str, numbers.Number]],\n            Dict[str, Union[np.ndarray, torch.Tensor, str, numbers.Number]],\n        ],\n    ]:\n        """"""Helper function to instantiate Loader.\n\n        Args:\n            path:  The file path\n            loader_type:  loader_type. sound, npy, text_int, text_float, etc\n        """"""\n        for key, dic in DATA_TYPES.items():\n            # e.g. loader_type=""sound""\n            # -> return DATA_TYPES[""sound""][""func""](path)\n            if re.match(key, loader_type):\n                kwargs = {}\n                for key2 in dic[""kwargs""]:\n                    if key2 == ""loader_type"":\n                        kwargs[""loader_type""] = loader_type\n                    elif key2 == ""float_dtype"":\n                        kwargs[""float_dtype""] = self.float_dtype\n                    elif key2 == ""int_dtype"":\n                        kwargs[""int_dtype""] = self.int_dtype\n                    else:\n                        raise RuntimeError(f""Not implemented keyword argument: {key2}"")\n\n                func = dic[""func""]\n                try:\n                    return func(path, **kwargs)\n                except Exception:\n                    if hasattr(func, ""__name__""):\n                        name = func.__name__\n                    else:\n                        name = str(func)\n                    logging.error(f""An error happend with {name}({path})"")\n                    raise\n        else:\n            raise RuntimeError(f""Not supported: loader_type={loader_type}"")\n\n    def has_name(self, name) -> bool:\n        return name in self.loader_dict\n\n    def names(self) -> Tuple[str, ...]:\n        return tuple(self.loader_dict)\n\n    def __repr__(self):\n        _mes = self.__class__.__name__\n        _mes += ""(""\n        for name, (path, _type) in self.debug_info.items():\n            _mes += f\'\\n  {name}: {{""path"": ""{path}"", ""type"": ""{_type}""}}\'\n        _mes += f""\\n  preprocess: {self.preprocess})""\n        return _mes\n\n    def __len__(self):\n        return len(list(self.loader_dict.values())[0])\n\n    # NOTE(kamo):\n    # Typically pytorch\'s Dataset.__getitem__ accepts an inger index,\n    # however this Dataset handle a string, which represents a sample-id.\n    def __getitem__(\n        self, uid: Union[str, int]\n    ) -> Tuple[Union[str, int], Dict[str, np.ndarray]]:\n        assert check_argument_types()\n\n        if self.cache is not None and uid in self.cache:\n            data = self.cache[uid]\n            return uid, data\n\n        data = {}\n        # 1. Load data from each loaders\n        for name, loader in self.loader_dict.items():\n            try:\n                value = loader[uid]\n                if isinstance(value, dict):\n                    for v in value.values():\n                        if not isinstance(\n                            v, (np.ndarray, torch.Tensor, str, numbers.Number)\n                        ):\n                            raise TypeError(\n                                f""Must be ndarray, torch.Tensor, str or Number: ""\n                                f""{type(v)}""\n                            )\n                elif isinstance(value, (tuple, list)):\n                    for v in value:\n                        if not isinstance(\n                            v, (np.ndarray, torch.Tensor, str, numbers.Number)\n                        ):\n                            raise TypeError(\n                                f""Must be ndarray, torch.Tensor, str or Number: ""\n                                f""{type(v)}""\n                            )\n                elif not isinstance(\n                    value, (np.ndarray, torch.Tensor, str, numbers.Number)\n                ):\n                    raise TypeError(\n                        f""Must be ndarray, torch.Tensor, str or Number: {type(value)}""\n                    )\n            except Exception:\n                path, _type = self.debug_info[name]\n                logging.error(\n                    f""Error happened with path={path}, type={_type}, id={uid}""\n                )\n                raise\n\n            if isinstance(value, (np.ndarray, torch.Tensor, str, numbers.Number)):\n                # torch.Tensor is converted to ndarray\n                if isinstance(value, torch.Tensor):\n                    value = value.numpy()\n                elif isinstance(value, numbers.Number):\n                    value = np.array([value])\n                data[name] = value\n\n            # The return value of ESPnet dataset must be a dict of ndarrays,\n            # so we need to parse a container of ndarrays\n            # if dict:\n            #   e.g. ""name"": {""foo"": array, ""bar"": arrray}\n            #   => ""name_foo"", ""name_bar""\n            elif isinstance(value, dict):\n                for k, v in value.items():\n                    new_key = f""{name}_{k}""\n                    if new_key in self.loader_dict:\n                        raise RuntimeError(f""Use another name: {new_key}"")\n                    if isinstance(v, torch.Tensor):\n                        v = v.numpy()\n                    elif isinstance(v, numbers.Number):\n                        v = np.array([v])\n                    data[new_key] = v\n\n            # if tuple or list:\n            #   e.g. ""name"": [array, array]\n            #   => ""name_0"", ""name_1""\n            elif isinstance(value, (tuple, list)):\n                for i, v in enumerate(value):\n                    new_key = f""{name}_{i}""\n                    if new_key in self.loader_dict:\n                        raise RuntimeError(f""Use another name: {new_key}"")\n                    if isinstance(v, torch.Tensor):\n                        v = v.numpy()\n                    elif isinstance(v, numbers.Number):\n                        v = np.array([v])\n                    data[new_key] = v\n\n        # 2. [Option] Apply preprocessing\n        #   e.g. espnet2.train.preprocessor:CommonPreprocessor\n        if self.preprocess is not None:\n            data = self.preprocess(uid, data)\n\n        # 3. Force data-precision\n        for name in data:\n            value = data[name]\n            if not isinstance(value, np.ndarray):\n                raise RuntimeError(\n                    f""All values must be converted to np.ndarray object ""\n                    f\'by preprocessing, but ""{name}"" is still {type(value)}.\'\n                )\n\n            # Cast to desired type\n            if value.dtype.kind == ""f"":\n                value = value.astype(self.float_dtype)\n            elif value.dtype.kind == ""i"":\n                value = value.astype(self.int_dtype)\n            else:\n                raise NotImplementedError(f""Not supported dtype: {value.dtype}"")\n            data[name] = value\n\n        if self.cache is not None and self.cache.size < self.max_cache_size:\n            self.cache[uid] = data\n\n        retval = uid, data\n        assert check_return_type(retval)\n        return retval\n'"
espnet2/train/distributed_utils.py,8,"b'import dataclasses\nimport os\nimport socket\nfrom typing import Optional\n\nimport torch\nimport torch.distributed\n\n\n@dataclasses.dataclass\nclass DistributedOption:\n    # Enable distributed Training\n    distributed: bool = False\n    # torch.distributed.Backend: ""nccl"", ""mpi"", ""gloo"", or ""tcp""\n    dist_backend: str = ""nccl""\n    # if init_method=""env://"",\n    # env values of ""MASTER_PORT"", ""MASTER_ADDR"", ""WORLD_SIZE"", and ""RANK"" are referred.\n    dist_init_method: str = ""env://""\n    dist_world_size: Optional[int] = None\n    dist_rank: Optional[int] = None\n    local_rank: Optional[int] = None\n    ngpu: int = 0\n    dist_master_addr: Optional[str] = None\n    dist_master_port: Optional[int] = None\n    dist_launcher: Optional[str] = None\n    multiprocessing_distributed: bool = True\n\n    def init(self):\n        if self.distributed:\n            if self.dist_init_method == ""env://"":\n                if get_master_addr(self.dist_master_addr, self.dist_launcher) is None:\n                    raise RuntimeError(\n                        ""--dist_master_addr or MASTER_ADDR must be set ""\n                        ""if --dist_init_method == \'env://\'""\n                    )\n                if get_master_port(self.dist_master_port) is None:\n                    raise RuntimeError(\n                        ""--dist_master_port or MASTER_PORT must be set ""\n                        ""if --dist_init_port == \'env://\'""\n                    )\n\n            # About priority order:\n            # If --dist_* is specified:\n            #    Use the value of --dist_rank and overwrite it environ just in case.\n            # elif environ is set:\n            #    Use the value of environ and set it to self\n            self.dist_rank = get_rank(self.dist_rank, self.dist_launcher)\n            self.dist_world_size = get_world_size(\n                self.dist_world_size, self.dist_launcher\n            )\n            self.local_rank = get_local_rank(self.local_rank, self.dist_launcher)\n\n            if self.local_rank is not None:\n                if self.ngpu > 1:\n                    raise RuntimeError(f""Assuming 1GPU in this case: ngpu={self.ngpu}"")\n                if ""CUDA_VISIBLE_DEVICES"" in os.environ:\n                    cvd = os.environ[""CUDA_VISIBLE_DEVICES""]\n                    if self.local_rank >= len(cvd.split("","")):\n                        raise RuntimeError(\n                            f""LOCAL_RANK={self.local_rank} is bigger ""\n                            f""than the number of visible devices: {cvd}""\n                        )\n\n            if (\n                self.dist_rank is not None\n                and self.dist_world_size is not None\n                and self.dist_rank >= self.dist_world_size\n            ):\n                raise RuntimeError(\n                    f""RANK >= WORLD_SIZE: {self.dist_rank} >= {self.dist_world_size}""\n                )\n\n            if self.dist_init_method == ""env://"":\n                self.dist_master_addr = get_master_addr(\n                    self.dist_master_addr, self.dist_launcher\n                )\n                self.dist_master_port = get_master_port(self.dist_master_port)\n                if (\n                    self.dist_master_addr is not None\n                    and self.dist_master_port is not None\n                ):\n                    self.dist_init_method = (\n                        f""tcp://{self.dist_master_addr}:{self.dist_master_port}""\n                    )\n\n            # See:\n            # https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html\n            os.environ.setdefault(""NCCL_DEBUG"", ""INFO"")\n\n            # See:\n            # https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group\n            os.environ.setdefault(""NCCL_BLOCKING_WAIT"", ""1"")\n\n            torch.distributed.init_process_group(\n                backend=self.dist_backend,\n                init_method=self.dist_init_method,\n                world_size=self.dist_world_size,\n                rank=self.dist_rank,\n            )\n\n            # About distributed model:\n            # if self.local_rank is not None and ngpu == 1\n            #    => Distributed with n-Process and n-GPU\n            # if self.local_rank is None and ngpu >= 1\n            #    => Distributed with 1-Process and n-GPU\n            if self.local_rank is not None and self.ngpu > 0:\n                torch.cuda.set_device(self.local_rank)\n\n\ndef resolve_distributed_mode(args):\n    # Note that args.distributed is set by only this function.\n    # and ArgumentParser doesn\'t have such option\n\n    if args.multiprocessing_distributed:\n        num_nodes = get_num_nodes(args.dist_world_size, args.dist_launcher)\n        # a. multi-node\n        if num_nodes > 1:\n            args.distributed = True\n        # b. single-node and multi-gpu with multiprocessing_distributed mode\n        elif args.ngpu > 1:\n            args.distributed = True\n        # c. single-node and single-gpu\n        else:\n            args.distributed = False\n\n        if args.ngpu <= 1:\n            # Disable multiprocessing_distributed mode if 1process per node or cpu mode\n            args.multiprocessing_distributed = False\n        if args.ngpu == 1:\n            # If the number of GPUs equals to 1 with multiprocessing_distributed mode,\n            # LOCAL_RANK is always 0\n            args.local_rank = 0\n\n        if num_nodes > 1 and get_node_rank(args.dist_rank, args.dist_launcher) is None:\n            raise RuntimeError(\n                ""--dist_rank or RANK must be set ""\n                ""if --multiprocessing_distributed == true""\n            )\n\n        # Note that RANK, LOCAL_RANK, and WORLD_SIZE is automatically set,\n        # so we don\'t need to check here\n    else:\n        # d. multiprocess and multi-gpu with external launcher\n        #    e.g. torch.distributed.launch\n        if get_world_size(args.dist_world_size, args.dist_launcher) > 1:\n            args.distributed = True\n        # e. single-process\n        else:\n            args.distributed = False\n\n        if args.distributed and args.ngpu > 0:\n            if get_local_rank(args.local_rank, args.dist_launcher) is None:\n                raise RuntimeError(\n                    ""--local_rank or LOCAL_RANK must be set ""\n                    ""if --multiprocessing_distributed == false""\n                )\n        if args.distributed:\n            if get_node_rank(args.dist_rank, args.dist_launcher) is None:\n                raise RuntimeError(\n                    ""--dist_rank or RANK must be set ""\n                    ""if --multiprocessing_distributed == false""\n                )\n    if args.distributed and args.dist_launcher == ""slurm"" and not is_in_slurm_step():\n        raise RuntimeError(""Launch by \'srun\' command if --dist_launcher=\'slurm\'"")\n\n\ndef is_in_slurm_job() -> bool:\n    return ""SLURM_PROCID"" in os.environ and ""SLURM_NTASKS"" in os.environ\n\n\ndef is_in_slurm_step() -> bool:\n    return (\n        is_in_slurm_job()\n        and ""SLURM_STEP_NUM_NODES"" in os.environ\n        and ""SLURM_STEP_NODELIST"" in os.environ\n    )\n\n\ndef _int_or_none(x: Optional[str]) -> Optional[int]:\n    if x is None:\n        return x\n    return int(x)\n\n\ndef free_port():\n    """"""Find free port using bind().\n\n    There are some interval between finding this port and using it\n    and the other process might catch the port by that time.\n    Thus it is not guaranteed that the port is really empty.\n\n    """"""\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n        sock.bind(("""", 0))\n        return sock.getsockname()[1]\n\n\ndef get_rank(prior=None, launcher: str = None) -> Optional[int]:\n    if prior is None:\n        if launcher == ""slurm"":\n            if not is_in_slurm_step():\n                raise RuntimeError(""This process seems not to be launched by \'srun\'"")\n            prior = os.environ[""SLURM_PROCID""]\n        elif launcher == ""mpi"":\n            raise RuntimeError(\n                ""launcher=mpi is used for \'multiprocessing-distributed\' mode""\n            )\n        elif launcher is not None:\n            raise RuntimeError(f""launcher=\'{launcher}\' is not supported"")\n\n    if prior is not None:\n        return int(prior)\n    else:\n        # prior is None and RANK is None -> RANK = None\n        return _int_or_none(os.environ.get(""RANK""))\n\n\ndef get_world_size(prior=None, launcher: str = None) -> int:\n    if prior is None:\n        if launcher == ""slurm"":\n            if not is_in_slurm_step():\n                raise RuntimeError(""This process seems not to be launched by \'srun\'"")\n            prior = int(os.environ[""SLURM_NTASKS""])\n        elif launcher == ""mpi"":\n            raise RuntimeError(\n                ""launcher=mpi is used for \'multiprocessing-distributed\' mode""\n            )\n        elif launcher is not None:\n            raise RuntimeError(f""launcher=\'{launcher}\' is not supported"")\n\n    if prior is not None:\n        return int(prior)\n    else:\n        # prior is None and WORLD_SIZE is None -> WORLD_SIZE = 1\n        return int(os.environ.get(""WORLD_SIZE"", ""1""))\n\n\ndef get_local_rank(prior=None, launcher: str = None) -> Optional[int]:\n    # LOCAL_RANK is same as GPU device id\n\n    if prior is None:\n        if launcher == ""slurm"":\n            if not is_in_slurm_step():\n                raise RuntimeError(""This process seems not to be launched by \'srun\'"")\n\n            prior = int(os.environ[""SLURM_LOCALID""])\n        elif launcher == ""mpi"":\n            raise RuntimeError(\n                ""launcher=mpi is used for \'multiprocessing-distributed\' mode""\n            )\n        elif launcher is not None:\n            raise RuntimeError(f""launcher=\'{launcher}\' is not supported"")\n\n    if prior is not None:\n        return int(prior)\n\n    elif ""LOCAL_RANK"" in os.environ:\n        return int(os.environ[""LOCAL_RANK""])\n\n    elif ""CUDA_VISIBLE_DEVICES"" in os.environ:\n        # There are two possibility:\n        # - ""CUDA_VISIBLE_DEVICES"" is set to multiple GPU ids. e.g. ""0.1,2""\n        #   => This intends to specify multiple devices to to be used exactly\n        #      and local_rank information is possibly insufficient.\n        # - ""CUDA_VISIBLE_DEVICES"" is set to an id. e.g. ""1""\n        #   => This could be used for LOCAL_RANK\n        cvd = os.environ[""CUDA_VISIBLE_DEVICES""].split("","")\n        if len(cvd) == 1 and ""LOCAL_RANK"" not in os.environ:\n            # If CUDA_VISIBLE_DEVICES is set and LOCAL_RANK is not set,\n            # then use it as LOCAL_RANK.\n\n            # Unset CUDA_VISIBLE_DEVICES\n            # because the other device must be visible to communicate\n            return int(os.environ.pop(""CUDA_VISIBLE_DEVICES""))\n        else:\n            return None\n    else:\n        return None\n\n\ndef get_master_addr(prior=None, launcher: str = None) -> Optional[str]:\n    if prior is None:\n        if launcher == ""slurm"":\n            if not is_in_slurm_step():\n                raise RuntimeError(""This process seems not to be launched by \'srun\'"")\n\n            # e.g nodelist = foo[1-10],bar[3-8] or foo4,bar[2-10]\n            nodelist = os.environ[""SLURM_STEP_NODELIST""]\n            prior = nodelist.split("","")[0].split(""-"")[0].replace(""["", """")\n\n    if prior is not None:\n        return str(prior)\n    else:\n        return os.environ.get(""MASTER_ADDR"")\n\n\ndef get_master_port(prior=None) -> Optional[int]:\n    if prior is not None:\n        return prior\n    else:\n        return _int_or_none(os.environ.get(""MASTER_PORT""))\n\n\ndef get_node_rank(prior=None, launcher: str = None) -> Optional[int]:\n    """"""Get Node Rank.\n\n    Use for ""multiprocessing distributed"" mode.\n    The initial RANK equals to the Node id in this case and\n    the real Rank is set as (nGPU * NodeID) + LOCAL_RANK in torch.distributed.\n\n    """"""\n    if prior is not None:\n        return prior\n    elif launcher == ""slurm"":\n        if not is_in_slurm_step():\n            raise RuntimeError(""This process seems not to be launched by \'srun\'"")\n\n        # Assume ntasks_per_node == 1\n        if os.environ[""SLURM_STEP_NUM_NODES""] != os.environ[""SLURM_NTASKS""]:\n            raise RuntimeError(\n                ""Run with --ntasks_per_node=1 if mutliprocessing_distributed=true""\n            )\n        return int(os.environ[""SLURM_NODEID""])\n    elif launcher == ""mpi"":\n        # Use mpi4py only for initialization and not using for communication\n        from mpi4py import MPI\n\n        comm = MPI.COMM_WORLD\n        # Assume ntasks_per_node == 1 (We can\'t check whether it is or not)\n        return comm.Get_rank()\n    elif launcher is not None:\n        raise RuntimeError(f""launcher=\'{launcher}\' is not supported"")\n    else:\n        return _int_or_none(os.environ.get(""RANK""))\n\n\ndef get_num_nodes(prior=None, launcher: str = None) -> Optional[int]:\n    """"""Get the number of nodes.\n\n    Use for ""multiprocessing distributed"" mode.\n    RANK equals to the Node id in this case and\n    the real Rank is set as (nGPU * NodeID) + LOCAL_RANK in torch.distributed.\n\n    """"""\n    if prior is not None:\n        return prior\n    elif launcher == ""slurm"":\n        if not is_in_slurm_step():\n            raise RuntimeError(""This process seems not to be launched by \'srun\'"")\n\n        # Assume ntasks_per_node == 1\n        if os.environ[""SLURM_STEP_NUM_NODES""] != os.environ[""SLURM_NTASKS""]:\n            raise RuntimeError(\n                ""Run with --ntasks_per_node=1 if mutliprocessing_distributed=true""\n            )\n        return int(os.environ[""SLURM_STEP_NUM_NODES""])\n    elif launcher == ""mpi"":\n        # Use mpi4py only for initialization and not using for communication\n        from mpi4py import MPI\n\n        comm = MPI.COMM_WORLD\n        # Assume ntasks_per_node == 1 (We can\'t check whether it is or not)\n        return comm.Get_size()\n    elif launcher is not None:\n        raise RuntimeError(f""launcher=\'{launcher}\' is not supported"")\n    else:\n        # prior is None -> NUM_NODES = 1\n        return int(os.environ.get(""WORLD_SIZE"", 1))\n'"
espnet2/train/preprocessor.py,0,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Union\n\nimport numpy as np\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\nfrom typing import Iterable\n\nfrom espnet2.text.build_tokenizer import build_tokenizer\nfrom espnet2.text.token_id_converter import TokenIDConverter\n\n\nclass AbsPreprocessor(ABC):\n    def __init__(self, train: bool):\n        self.train = train\n\n    @abstractmethod\n    def __call__(\n        self, uid: str, data: Dict[str, Union[str, np.ndarray]]\n    ) -> Dict[str, np.ndarray]:\n        raise NotImplementedError\n\n\nclass CommonPreprocessor(AbsPreprocessor):\n    def __init__(\n        self,\n        train: bool,\n        token_type: str = None,\n        token_list: Union[Path, str, Iterable[str]] = None,\n        bpemodel: Union[Path, str, Iterable[str]] = None,\n        unk_symbol: str = ""<unk>"",\n        space_symbol: str = ""<space>"",\n        non_linguistic_symbols: Union[Path, str, Iterable[str]] = None,\n        delimiter: str = None,\n        speech_name: str = ""speech"",\n        text_name: str = ""text"",\n    ):\n        super().__init__(train)\n        self.train = train\n        self.speech_name = speech_name\n        self.text_name = text_name\n\n        if token_type is not None:\n            if token_list is None:\n                raise ValueError(""token_list is required if token_type is not None"")\n\n            self.tokenizer = build_tokenizer(\n                token_type=token_type,\n                bpemodel=bpemodel,\n                delimiter=delimiter,\n                space_symbol=space_symbol,\n                non_linguistic_symbols=non_linguistic_symbols,\n            )\n            self.token_id_converter = TokenIDConverter(\n                token_list=token_list, unk_symbol=unk_symbol,\n            )\n        else:\n            self.tokenizer = None\n            self.token_id_converter = None\n\n    def __call__(\n        self, uid: str, data: Dict[str, Union[str, np.ndarray]]\n    ) -> Dict[str, np.ndarray]:\n        assert check_argument_types()\n\n        if self.speech_name in data:\n            # Nothing now: candidates:\n            # - STFT\n            # - Fbank\n            # - CMVN\n            # - Data augmentation\n            pass\n\n        if self.text_name in data and self.tokenizer is not None:\n            text = data[self.text_name]\n            tokens = self.tokenizer.text2tokens(text)\n            text_ints = self.token_id_converter.tokens2ids(tokens)\n            data[self.text_name] = np.array(text_ints, dtype=np.int64)\n        assert check_return_type(data)\n        return data\n'"
espnet2/train/reporter.py,5,"b'from collections import defaultdict\nfrom contextlib import contextmanager\nimport dataclasses\nimport datetime\nfrom distutils.version import LooseVersion\nimport logging\nfrom pathlib import Path\nimport time\nfrom typing import ContextManager\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Union\nimport warnings\n\nimport humanfriendly\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\nif LooseVersion(torch.__version__) >= LooseVersion(""1.1.0""):\n    from torch.utils.tensorboard import SummaryWriter\nelse:\n    from tensorboardX import SummaryWriter\n\nNum = Union[float, int, complex, torch.Tensor, np.ndarray]\n\n\n_reserved = {""time"", ""total_count""}\n\n\ndef to_reported_value(v: Num, weight: Num = None) -> ""ReportedValue"":\n    assert check_argument_types()\n    if isinstance(v, (torch.Tensor, np.ndarray)):\n        if np.prod(v.shape) != 1:\n            raise ValueError(f""v must be 0 or 1 dimension: {len(v.shape)}"")\n        v = v.item()\n\n    if isinstance(weight, (torch.Tensor, np.ndarray)):\n        if np.prod(weight.shape) != 1:\n            raise ValueError(f""weight must be 0 or 1 dimension: {len(weight.shape)}"")\n        weight = weight.item()\n\n    if weight is not None:\n        retval = WeightedAverage(v, weight)\n    else:\n        retval = Average(v)\n    assert check_return_type(retval)\n    return retval\n\n\ndef aggregate(values: Sequence[""ReportedValue""]) -> Num:\n    assert check_argument_types()\n\n    for v in values:\n        if not isinstance(v, type(values[0])):\n            raise ValueError(\n                f""Can\'t use different Reported type together: ""\n                f""{type(v)} != {type(values[0])}""\n            )\n\n    if len(values) == 0:\n        warnings.warn(""No stats found"")\n        retval = np.nan\n\n    elif isinstance(values[0], Average):\n        retval = np.nanmean([v.value for v in values])\n\n    elif isinstance(values[0], WeightedAverage):\n        # Excludes non finite values\n        invalid_indices = set()\n        for i, v in enumerate(values):\n            if not np.isfinite(v.value) or not np.isfinite(v.weight):\n                invalid_indices.add(i)\n        values = [v for i, v in enumerate(values) if i not in invalid_indices]\n\n        if len(values) != 0:\n            # Calc weighed average. Weights are changed to sum-to-1.\n            sum_weights = sum(v.weight for i, v in enumerate(values))\n            sum_value = sum(v.value * v.weight for i, v in enumerate(values))\n            if sum_weights == 0:\n                warnings.warn(""weight is zero"")\n                retval = np.nan\n            else:\n                retval = sum_value / sum_weights\n        else:\n            warnings.warn(""No valid stats found"")\n            retval = np.nan\n\n    else:\n        raise NotImplementedError(f""type={type(values[0])}"")\n    assert check_return_type(retval)\n    return retval\n\n\nclass ReportedValue:\n    pass\n\n\n@dataclasses.dataclass(frozen=True)\nclass Average(ReportedValue):\n    value: Num\n\n\n@dataclasses.dataclass(frozen=True)\nclass WeightedAverage(ReportedValue):\n    value: Tuple[Num, Num]\n    weight: Num\n\n\nclass SubReporter:\n    """"""This class is used in Reporter.\n\n    See the docstring of Reporter for the usage.\n    """"""\n\n    def __init__(self, key: str, epoch: int, total_count: int):\n        assert check_argument_types()\n        self.key = key\n        self.epoch = epoch\n        self.start_time = time.perf_counter()\n        self.stats = defaultdict(list)\n        self._finished = False\n        self.total_count = total_count\n        self.count = 0\n        self.prev_count = 0\n        self.prev_positions = {}\n\n    def get_total_count(self) -> int:\n        """"""Returns the number of iterations over all epochs.""""""\n        return self.total_count\n\n    def get_epoch(self) -> int:\n        return self.epoch\n\n    def register(\n        self,\n        stats: Dict[str, Optional[Union[Num, Dict[str, Num]]]],\n        weight: Num = None,\n        not_increment_count: bool = False,\n    ) -> None:\n        assert check_argument_types()\n        if self._finished:\n            raise RuntimeError(""Already finished"")\n        if not not_increment_count:\n            self.total_count += 1\n            self.count += 1\n\n        for key2, v in stats.items():\n            if key2 in _reserved:\n                raise RuntimeError(f""{key2} is reserved."")\n            # if None value, the key is not registered\n            if v is None:\n                continue\n            r = to_reported_value(v, weight)\n            self.stats[key2].append(r)\n\n    def log_message(self) -> str:\n        if self._finished:\n            raise RuntimeError(""Already finished"")\n        if self.count == 0:\n            return """"\n\n        message = (\n            f""{self.epoch}epoch:{self.key}:""\n            f""{self.prev_count + 1}-{self.count}batch: ""\n        )\n\n        stats = list(self.stats.items())\n\n        for idx, (key2, stats) in enumerate(stats):\n            # values: List[ReportValue]\n            pos = self.prev_positions.setdefault(key2, 0)\n            self.prev_positions[key2] = len(stats)\n            values = stats[pos:]\n            if idx != 0 and idx != len(stats):\n                message += "", ""\n\n            v = aggregate(values)\n            if abs(v) > 1.0e3:\n                message += f""{key2}={v:.3e}""\n            elif abs(v) > 1.0e-3:\n                message += f""{key2}={v:.3f}""\n            else:\n                message += f""{key2}={v:.3e}""\n\n        self.prev_count = self.count\n        return message\n\n    def finished(self) -> None:\n        self._finished = True\n\n    @contextmanager\n    def measure_time(self, name: str):\n        start = time.perf_counter()\n        yield start\n        t = time.perf_counter() - start\n        self.register({name: t}, not_increment_count=True)\n\n    def measure_iter_time(self, iterable, name: str):\n        iterator = iter(iterable)\n        while True:\n            try:\n                start = time.perf_counter()\n                retval = next(iterator)\n                t = time.perf_counter() - start\n                self.register({name: t}, not_increment_count=True)\n                yield retval\n            except StopIteration:\n                break\n\n\nclass Reporter:\n    """"""Reporter class.\n\n    Examples:\n\n        >>> reporter = Reporter()\n        >>> with reporter.observe(\'train\') as sub_reporter:\n        ...     for batch in iterator:\n        ...         stats = dict(loss=0.2)\n        ...         sub_reporter.register(stats)\n\n    """"""\n\n    def __init__(self, epoch: int = 0):\n        assert check_argument_types()\n        if epoch < 0:\n            raise ValueError(f""epoch must be 0 or more: {epoch}"")\n        self.epoch = epoch\n        # stats: Dict[int, Dict[str, Dict[str, float]]]\n        # e.g. self.stats[epoch][\'train\'][\'loss\']\n        self.stats = {}\n\n    def get_epoch(self) -> int:\n        return self.epoch\n\n    def set_epoch(self, epoch: int) -> None:\n        if epoch < 0:\n            raise ValueError(f""epoch must be 0 or more: {epoch}"")\n        self.epoch = epoch\n\n    @contextmanager\n    def observe(self, key: str, epoch: int = None) -> ContextManager[SubReporter]:\n        sub_reporter = self.start_epoch(key, epoch)\n        yield sub_reporter\n        # Receive the stats from sub_reporter\n        self.finish_epoch(sub_reporter)\n\n    def start_epoch(self, key: str, epoch: int = None) -> SubReporter:\n        if epoch is not None:\n            if epoch < 0:\n                raise ValueError(f""epoch must be 0 or more: {epoch}"")\n            self.epoch = epoch\n\n        if self.epoch - 1 not in self.stats or key not in self.stats[self.epoch - 1]:\n            # If the previous epoch doesn\'t exist for some reason,\n            # maybe due to bug, this case also indicates 0-count.\n            if self.epoch - 1 != 0:\n                warnings.warn(\n                    f""The stats of the previous epoch={self.epoch - 1}""\n                    f""doesn\'t exist.""\n                )\n            total_count = 0\n        else:\n            total_count = self.stats[self.epoch - 1][key][""total_count""]\n\n        sub_reporter = SubReporter(key, self.epoch, total_count)\n        # Clear the stats for the next epoch if it exists\n        self.stats.pop(epoch, None)\n        return sub_reporter\n\n    def finish_epoch(self, sub_reporter: SubReporter) -> None:\n        if self.epoch != sub_reporter.epoch:\n            raise RuntimeError(\n                f""Don\'t change epoch during observation: ""\n                f""{self.epoch} != {sub_reporter.epoch}""\n            )\n\n        # Calc mean of current stats and set it as previous epochs stats\n        stats = {}\n        for key2, values in sub_reporter.stats.items():\n            v = aggregate(values)\n            stats[key2] = v\n\n        stats[""time""] = datetime.timedelta(\n            seconds=time.perf_counter() - sub_reporter.start_time\n        )\n        stats[""total_count""] = sub_reporter.total_count\n\n        self.stats.setdefault(self.epoch, {})[sub_reporter.key] = stats\n        sub_reporter.finished()\n\n    def sort_epochs_and_values(\n        self, key: str, key2: str, mode: str\n    ) -> List[Tuple[int, float]]:\n        """"""Return the epoch which resulted the best value.\n\n        Example:\n            >>> val = reporter.sort_epochs_and_values(\'eval\', \'loss\', \'min\')\n            >>> e_1best, v_1best = val[0]\n            >>> e_2best, v_2best = val[1]\n        """"""\n        if mode not in (""min"", ""max""):\n            raise ValueError(f""mode must min or max: {mode}"")\n        if not self.has(key, key2):\n            raise KeyError(f""{key}.{key2} is not found: {self.get_all_keys()}"")\n\n        # iterate from the last epoch\n        values = [(e, self.stats[e][key][key2]) for e in self.stats]\n\n        if mode == ""min"":\n            values = sorted(values, key=lambda x: x[1])\n        else:\n            values = sorted(values, key=lambda x: -x[1])\n        return values\n\n    def sort_epochs(self, key: str, key2: str, mode: str) -> List[int]:\n        return [e for e, v in self.sort_epochs_and_values(key, key2, mode)]\n\n    def sort_values(self, key: str, key2: str, mode: str) -> List[float]:\n        return [v for e, v in self.sort_epochs_and_values(key, key2, mode)]\n\n    def get_best_epoch(self, key: str, key2: str, mode: str, nbest: int = 0) -> int:\n        return self.sort_epochs(key, key2, mode)[nbest]\n\n    def check_early_stopping(\n        self,\n        patience: int,\n        key1: str,\n        key2: str,\n        mode: str,\n        epoch: int = None,\n        logger=None,\n    ) -> bool:\n        if logger is None:\n            logger = logging\n        if epoch is None:\n            epoch = self.get_epoch()\n\n        best_epoch = self.get_best_epoch(key1, key2, mode)\n        if epoch - best_epoch > patience:\n            logger.info(\n                f""[Early stopping] {key1}.{key2} has not been ""\n                f""improved {epoch - best_epoch} epochs continuously. ""\n                f""The training was stopped at {epoch}epoch""\n            )\n            return True\n        else:\n            return False\n\n    def has(self, key: str, key2: str, epoch: int = None) -> bool:\n        if epoch is None:\n            epoch = self.get_epoch()\n        return (\n            epoch in self.stats\n            and key in self.stats[epoch]\n            and key2 in self.stats[epoch][key]\n        )\n\n    def log_message(self, epoch: int = None) -> str:\n        if epoch is None:\n            epoch = self.get_epoch()\n\n        message = """"\n        for key, d in self.stats[epoch].items():\n            _message = """"\n            for key2, v in d.items():\n                if v is not None:\n                    if len(_message) != 0:\n                        _message += "", ""\n                    if isinstance(v, float):\n                        if abs(v) > 1.0e3:\n                            _message += f""{key2}={v:.3e}""\n                        elif abs(v) > 1.0e-3:\n                            _message += f""{key2}={v:.3f}""\n                        else:\n                            _message += f""{key2}={v:.3e}""\n                    elif isinstance(v, datetime.timedelta):\n                        _v = humanfriendly.format_timespan(v)\n                        _message += f""{key2}={_v}""\n                    else:\n                        _message += f""{key2}={v}""\n            if len(_message) != 0:\n                if len(message) == 0:\n                    message += f""{epoch}epoch results: ""\n                else:\n                    message += "", ""\n                message += f""[{key}] {_message}""\n        return message\n\n    def get_value(self, key: str, key2: str, epoch: int = None):\n        if not self.has(key, key2):\n            raise KeyError(f""{key}.{key2} is not found in stats: {self.get_all_keys()}"")\n        if epoch is None:\n            epoch = self.get_epoch()\n        return self.stats[epoch][key][key2]\n\n    def get_keys(self, epoch: int = None) -> Tuple[str, ...]:\n        """"""Returns keys1 e.g. train,eval.""""""\n        if epoch is None:\n            epoch = self.get_epoch()\n        return tuple(self.stats[epoch])\n\n    def get_keys2(self, key: str, epoch: int = None) -> Tuple[str, ...]:\n        """"""Returns keys2 e.g. loss,acc.""""""\n        if epoch is None:\n            epoch = self.get_epoch()\n        d = self.stats[epoch][key]\n        keys2 = tuple(k for k in d if k not in (""time"", ""total_count""))\n        return keys2\n\n    def get_all_keys(self, epoch: int = None) -> Tuple[Tuple[str, str], ...]:\n        if epoch is None:\n            epoch = self.get_epoch()\n        all_keys = []\n        for key in self.stats[epoch]:\n            for key2 in self.stats[epoch][key]:\n                all_keys.append((key, key2))\n        return tuple(all_keys)\n\n    def matplotlib_plot(self, output_dir: Union[str, Path]):\n        """"""Plot stats using Matplotlib and save images.""""""\n        keys2 = set.union(*[set(self.get_keys2(k)) for k in self.get_keys()])\n        for key2 in keys2:\n            keys = [k for k in self.get_keys() if key2 in self.get_keys2(k)]\n            plt = self._plot_stats(keys, key2)\n            p = output_dir / f""{key2}.png""\n            p.parent.mkdir(parents=True, exist_ok=True)\n            plt.savefig(p)\n\n    def _plot_stats(self, keys: Sequence[str], key2: str):\n        assert check_argument_types()\n        # str is also Sequence[str]\n        if isinstance(keys, str):\n            raise TypeError(f""Input as [{keys}]"")\n\n        import matplotlib\n\n        matplotlib.use(""agg"")\n        import matplotlib.pyplot as plt\n        import matplotlib.ticker as ticker\n\n        plt.clf()\n\n        epochs = np.arange(1, self.get_epoch() + 1)\n        for key in keys:\n            y = [\n                self.stats[e][key][key2]\n                if e in self.stats\n                and key in self.stats[e]\n                and key2 in self.stats[e][key]\n                else np.nan\n                for e in epochs\n            ]\n            assert len(epochs) == len(y), ""Bug?""\n\n            plt.plot(epochs, y, label=key, marker=""x"")\n        plt.legend()\n        plt.title(f""epoch vs {key2}"")\n        # Force integer tick for x-axis\n        plt.gca().get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))\n        plt.xlabel(""epoch"")\n        plt.ylabel(key2)\n        plt.grid()\n\n        return plt\n\n    def tensorboard_add_scalar(self, summary_writer: SummaryWriter, epoch: int = None):\n        if epoch is None:\n            epoch = self.get_epoch()\n\n        keys2 = set.union(*[set(self.get_keys2(k)) for k in self.get_keys()])\n        for key2 in keys2:\n            summary_writer.add_scalars(\n                key2,\n                {\n                    k: self.stats[epoch][k][key2]\n                    for k in self.get_keys(epoch)\n                    if key2 in self.stats[epoch][k]\n                },\n                epoch,\n            )\n\n    def state_dict(self):\n        return {""stats"": self.stats, ""epoch"": self.epoch}\n\n    def load_state_dict(self, state_dict: dict):\n        self.epoch = state_dict[""epoch""]\n        self.stats = state_dict[""stats""]\n'"
espnet2/train/trainer.py,38,"b'import argparse\nimport dataclasses\nfrom dataclasses import is_dataclass\nfrom distutils.version import LooseVersion\nimport logging\nfrom pathlib import Path\nimport time\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import List\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\n\nimport humanfriendly\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.optim\nfrom typeguard import check_argument_types\n\nfrom espnet2.iterators.abs_iter_factory import AbsIterFactory\nfrom espnet2.main_funcs.calculate_all_attentions import calculate_all_attentions\nfrom espnet2.schedulers.abs_scheduler import AbsBatchStepScheduler\nfrom espnet2.schedulers.abs_scheduler import AbsEpochStepScheduler\nfrom espnet2.schedulers.abs_scheduler import AbsScheduler\nfrom espnet2.schedulers.abs_scheduler import AbsValEpochStepScheduler\nfrom espnet2.torch_utils.add_gradient_noise import add_gradient_noise\nfrom espnet2.torch_utils.device_funcs import to_device\nfrom espnet2.torch_utils.recursive_op import recursive_average\nfrom espnet2.torch_utils.set_all_random_seed import set_all_random_seed\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\nfrom espnet2.train.distributed_utils import DistributedOption\nfrom espnet2.train.reporter import Reporter\nfrom espnet2.train.reporter import SubReporter\nfrom espnet2.utils.build_dataclass import build_dataclass\n\nif LooseVersion(torch.__version__) >= LooseVersion(""1.1.0""):\n    from torch.utils.tensorboard import SummaryWriter\nelse:\n    from tensorboardX import SummaryWriter\nif torch.distributed.is_available():\n    if LooseVersion(torch.__version__) > LooseVersion(""1.0.1""):\n        from torch.distributed import ReduceOp\n    else:\n        from torch.distributed import reduce_op as ReduceOp\nelse:\n    ReduceOp = None\n\n\n@dataclasses.dataclass\nclass TrainerOptions:\n    ngpu: int\n    train_dtype: str\n    grad_noise: bool\n    accum_grad: int\n    grad_clip: float\n    log_interval: Optional[int]\n    no_forward_run: bool\n\n\nclass Trainer:\n    """"""Trainer having a optimizer.\n\n    If you\'d like to use multiple optimizers, then inherit this class\n    and override the methods if necessary - at least ""train_one_epoch()""\n\n    >>> class TwoOptimizerTrainer(Trainer):\n    ...     num_optimizers: int = 1\n    ...\n    ...     @classmethod\n    ...     def add_arguments(cls, parser):\n    ...         ...\n    ...\n    ...     @classmethod\n    ...     def train_one_epoch(cls, model, optimizers, ...):\n    ...         loss1 = model.model1(...)\n    ...         loss1.backward()\n    ...         optimizers[0].step()\n    ...\n    ...         loss2 = model.model2(...)\n    ...         loss2.backward()\n    ...         optimizers[1].step()\n\n    """"""\n\n    # If you need more than one optimizers, change this value in inheritance\n    num_optimizers: int = 1\n\n    def __init__(self):\n        raise RuntimeError(""This class can\'t be instantiated."")\n\n    @classmethod\n    def build_options(cls, args: argparse.Namespace) -> TrainerOptions:\n        """"""Build options consumed by train(), eval(), and plot_attention()""""""\n        assert check_argument_types()\n        return build_dataclass(TrainerOptions, args)\n\n    @classmethod\n    def add_arguments(cls, parser: argparse.ArgumentParser):\n        """"""Reserved for future development of another Trainer""""""\n        pass\n\n    @classmethod\n    def run(\n        cls,\n        model: AbsESPnetModel,\n        optimizers: Sequence[torch.optim.Optimizer],\n        schedulers: Sequence[Optional[AbsScheduler]],\n        train_iter_factory: AbsIterFactory,\n        valid_iter_factory: AbsIterFactory,\n        plot_attention_iter_factory: Optional[AbsIterFactory],\n        reporter: Reporter,\n        output_dir: Path,\n        max_epoch: int,\n        seed: int,\n        patience: Optional[int],\n        keep_nbest_models: int,\n        early_stopping_criterion: Sequence[str],\n        best_model_criterion: Sequence[Sequence[str]],\n        val_scheduler_criterion: Sequence[str],\n        trainer_options,\n        distributed_option: DistributedOption,\n    ) -> None:\n        """"""Perform training. This method performs the main process of training.""""""\n        assert check_argument_types()\n        # NOTE(kamo): Don\'t check the type more strictly as far trainer_options\n        assert is_dataclass(trainer_options), type(trainer_options)\n\n        # NOTE(kamo): trainer_options doesn\'t always have ""train_dtype""\n        use_apex = getattr(trainer_options, ""train_dtype"", """") in (\n            ""O0"",\n            ""O1"",\n            ""O2"",\n            ""O3"",\n        )\n        if use_apex:\n            try:\n                from apex import amp\n            except ImportError:\n                logging.error(\n                    ""You need to install apex. ""\n                    ""See https://github.com/NVIDIA/apex#linux""\n                )\n\n        start_epoch = reporter.get_epoch() + 1\n        if start_epoch == max_epoch + 1:\n            logging.warning(\n                f""The training has already reached at max_epoch: {start_epoch}""\n            )\n\n        if distributed_option.distributed:\n            # Use torch DDP instead of apex DDP\n            # https://github.com/NVIDIA/apex/issues/494\n            dp_model = torch.nn.parallel.DistributedDataParallel(\n                model,\n                device_ids=(\n                    # Perform multi-Process with multi-GPUs\n                    [torch.cuda.current_device()]\n                    if distributed_option.ngpu == 1\n                    # Perform single-Process with multi-GPUs\n                    else None\n                ),\n                output_device=(\n                    torch.cuda.current_device()\n                    if distributed_option.ngpu == 1\n                    else None\n                ),\n            )\n        elif distributed_option.ngpu > 1:\n            # apex.amp supports DataParallel now.\n            dp_model = torch.nn.parallel.DataParallel(\n                model, device_ids=list(range(distributed_option.ngpu)),\n            )\n        else:\n            # NOTE(kamo): DataParallel also should work with ngpu=1,\n            # but for debuggability it\'s better to keep this block.\n            dp_model = model\n\n        if not distributed_option.distributed or distributed_option.dist_rank == 0:\n            summary_writer = SummaryWriter(str(output_dir / ""tensorboard""))\n        else:\n            summary_writer = None\n\n        start_time = time.perf_counter()\n        for iepoch in range(start_epoch, max_epoch + 1):\n            if iepoch != start_epoch:\n                logging.info(\n                    ""{}/{}epoch started. Estimated time to finish: {}"".format(\n                        iepoch,\n                        max_epoch,\n                        humanfriendly.format_timespan(\n                            (time.perf_counter() - start_time)\n                            / (iepoch - start_epoch)\n                            * (max_epoch - iepoch + 1)\n                        ),\n                    )\n                )\n            else:\n                logging.info(f""{iepoch}/{max_epoch}epoch started"")\n            set_all_random_seed(seed + iepoch)\n\n            reporter.set_epoch(iepoch)\n            # 1. Train and validation for one-epoch\n            with reporter.observe(""train"") as sub_reporter:\n                all_steps_are_invalid = cls.train_one_epoch(\n                    model=dp_model,\n                    optimizers=optimizers,\n                    schedulers=schedulers,\n                    iterator=train_iter_factory.build_iter(iepoch),\n                    reporter=sub_reporter,\n                    options=trainer_options,\n                )\n\n            with reporter.observe(""valid"") as sub_reporter:\n                cls.validate_one_epoch(\n                    model=dp_model,\n                    iterator=valid_iter_factory.build_iter(iepoch),\n                    reporter=sub_reporter,\n                    options=trainer_options,\n                )\n\n            if not distributed_option.distributed or distributed_option.dist_rank == 0:\n                # att_plot doesn\'t support distributed\n                if plot_attention_iter_factory is not None:\n                    with reporter.observe(""att_plot"") as sub_reporter:\n                        cls.plot_attention(\n                            model=model,\n                            output_dir=output_dir / ""att_ws"",\n                            summary_writer=summary_writer,\n                            iterator=plot_attention_iter_factory.build_iter(iepoch),\n                            reporter=sub_reporter,\n                            options=trainer_options,\n                        )\n\n            # 2. LR Scheduler step\n            for scheduler in schedulers:\n                if isinstance(scheduler, AbsValEpochStepScheduler):\n                    scheduler.step(reporter.get_value(*val_scheduler_criterion))\n                elif isinstance(scheduler, AbsEpochStepScheduler):\n                    scheduler.step()\n\n            if not distributed_option.distributed or distributed_option.dist_rank == 0:\n                # 3. Report the results\n                logging.info(reporter.log_message())\n                reporter.matplotlib_plot(output_dir / ""images"")\n                reporter.tensorboard_add_scalar(summary_writer)\n\n                # 4. Save/Update the checkpoint\n                torch.save(\n                    {\n                        ""model"": model.state_dict(),\n                        ""reporter"": reporter.state_dict(),\n                        ""optimizers"": [o.state_dict() for o in optimizers],\n                        ""schedulers"": [\n                            s.state_dict() if s is not None else None\n                            for s in schedulers\n                        ],\n                        ""amp"": amp.state_dict() if use_apex else None,\n                    },\n                    output_dir / ""checkpoint.pth"",\n                )\n\n                # 5. Save the model and update the link to the best model\n                torch.save(model.state_dict(), output_dir / f""{iepoch}epoch.pth"")\n\n                # Creates a sym link latest.pth -> {iepoch}epoch.pth\n                p = output_dir / ""latest.pth""\n                if p.is_symlink() or p.exists():\n                    p.unlink()\n                p.symlink_to(f""{iepoch}epoch.pth"")\n\n                _improved = []\n                for _phase, k, _mode in best_model_criterion:\n                    # e.g. _phase, k, _mode = ""train"", ""loss"", ""min""\n                    if reporter.has(_phase, k):\n                        best_epoch = reporter.get_best_epoch(_phase, k, _mode)\n                        # Creates sym links if it\'s the best result\n                        if best_epoch == iepoch:\n                            p = output_dir / f""{_phase}.{k}.best.pth""\n                            if p.is_symlink() or p.exists():\n                                p.unlink()\n                            p.symlink_to(f""{iepoch}epoch.pth"")\n                            _improved.append(f""{_phase}.{k}"")\n                if len(_improved) == 0:\n                    logging.info(""There are no improvements in this epoch"")\n                else:\n                    logging.info(\n                        ""The best model has been updated: "" + "", "".join(_improved)\n                    )\n\n                # 6. Remove the model files excluding n-best epoch and latest epoch\n                _removed = []\n                # Get the union set of the n-best among multiple criterion\n                nbests = set().union(\n                    *[\n                        set(reporter.sort_epochs(ph, k, m)[:keep_nbest_models])\n                        for ph, k, m in best_model_criterion\n                        if reporter.has(ph, k)\n                    ]\n                )\n                for e in range(1, iepoch):\n                    p = output_dir / f""{e}epoch.pth""\n                    if p.exists() and e not in nbests:\n                        p.unlink()\n                        _removed.append(str(p))\n                if len(_removed) != 0:\n                    logging.info(""The model files were removed: "" + "", "".join(_removed))\n\n            # 7. If any updating haven\'t happened, stops the training\n            if all_steps_are_invalid:\n                logging.warning(\n                    f""The gradients at all steps are invalid in this epoch. ""\n                    f""Something seems wrong. This training was stopped at {iepoch}epoch""\n                )\n                break\n\n            # 8. Check early stopping\n            if patience is not None:\n                if reporter.check_early_stopping(patience, *early_stopping_criterion):\n                    break\n\n        else:\n            logging.info(f""The training was finished at {max_epoch} epochs "")\n\n    @classmethod\n    def train_one_epoch(\n        cls,\n        model: torch.nn.Module,\n        iterator: Iterable[Tuple[List[str], Dict[str, torch.Tensor]]],\n        optimizers: Sequence[torch.optim.Optimizer],\n        schedulers: Sequence[Optional[AbsScheduler]],\n        reporter: SubReporter,\n        options: TrainerOptions,\n    ) -> bool:\n        assert check_argument_types()\n\n        # Note(kamo): assumes one optimizer\n        assert cls.num_optimizers == 1, cls.num_optimizers\n        assert len(optimizers) == 1, len(optimizers)\n        optimizer = optimizers[0]\n        scheduler = schedulers[0]\n\n        grad_noise = options.grad_noise\n        accum_grad = options.accum_grad\n        grad_clip = options.grad_clip\n        log_interval = options.log_interval\n        no_forward_run = options.no_forward_run\n        ngpu = options.ngpu\n        distributed = isinstance(model, torch.nn.parallel.DistributedDataParallel)\n        use_apex = options.train_dtype in (""O0"", ""O1"", ""O2"", ""O3"")\n\n        if log_interval is None:\n            try:\n                log_interval = max(len(iterator) // 20, 10)\n            except TypeError:\n                log_interval = 100\n\n        model.train()\n        all_steps_are_invalid = True\n        # [For distributed] Because iteration counts are not always equals between\n        # processes, send stop-flag to the other processes if iterator is finished\n        iterator_stop = torch.tensor(0).to(""cuda"" if ngpu > 0 else ""cpu"")\n\n        start_time = time.perf_counter()\n        for iiter, (_, batch) in enumerate(\n            reporter.measure_iter_time(iterator, ""iter_time""), 1\n        ):\n            assert isinstance(batch, dict), type(batch)\n\n            if distributed:\n                torch.distributed.all_reduce(iterator_stop, ReduceOp.SUM)\n                if iterator_stop > 0:\n                    break\n\n            batch = to_device(batch, ""cuda"" if ngpu > 0 else ""cpu"")\n            if no_forward_run:\n                all_steps_are_invalid = False\n                reporter.register({})\n                continue\n\n            with reporter.measure_time(""forward_time""):\n                loss, stats, weight = model(**batch)\n            if ngpu > 1 or distributed:\n                # Apply weighted averaging for loss and stats\n                loss = (loss * weight.type(loss.dtype)).sum()\n\n                # if distributed, this method can also apply all_reduce()\n                stats, weight = recursive_average(stats, weight, distributed)\n\n                # Now weight is summation over all workers\n                loss /= weight\n            if distributed:\n                # NOTE(kamo): Multiply world_size because DistributedDataParallel\n                # automatically normalizes the gradient by world_size.\n                loss *= torch.distributed.get_world_size()\n\n            reporter.register(stats, weight)\n\n            loss /= accum_grad\n            with reporter.measure_time(""backward_time""):\n                if use_apex:\n                    try:\n                        from apex import amp\n                    except ImportError:\n                        logging.error(\n                            ""You need to install apex. ""\n                            ""See https://github.com/NVIDIA/apex#linux""\n                        )\n\n                    with amp.scale_loss(loss, optimizers) as scaled_loss:\n                        scaled_loss.backward()\n                else:\n                    loss.backward()\n\n            if iiter % accum_grad == 0:\n                # gradient noise injection\n                if grad_noise:\n                    add_gradient_noise(\n                        model,\n                        reporter.get_total_count(),\n                        duration=100,\n                        eta=1.0,\n                        scale_factor=0.55,\n                    )\n\n                # compute the gradient norm to check if it is normal or not\n                grad_norm = torch.nn.utils.clip_grad_norm_(\n                    model.parameters(), grad_clip\n                )\n                # PyTorch<=1.4, clip_grad_norm_ returns float value\n                if not isinstance(grad_norm, torch.Tensor):\n                    grad_norm = torch.tensor(grad_norm)\n\n                if not torch.isfinite(grad_norm):\n                    logging.warning(\n                        f""The grad norm is {grad_norm}. Skipping updating the model.""\n                    )\n                else:\n                    all_steps_are_invalid = False\n                    with reporter.measure_time(""optim_step_time""):\n                        optimizer.step()\n                    if isinstance(scheduler, AbsBatchStepScheduler):\n                        scheduler.step()\n                optimizer.zero_grad()\n\n                # Register lr and train/load time[sec/step],\n                # where step refers to accum_grad * mini-batch\n                reporter.register(\n                    dict(\n                        {\n                            f""lr_{i}"": pg[""lr""]\n                            for i, pg in enumerate(optimizer.param_groups)\n                            if ""lr"" in pg\n                        },\n                        train_time=time.perf_counter() - start_time,\n                    ),\n                    # Suppress to increment the internal counter.\n                    not_increment_count=True,\n                )\n                start_time = time.perf_counter()\n\n            if iiter % log_interval == 0:\n                logging.info(reporter.log_message())\n\n        else:\n            if distributed:\n                iterator_stop.fill_(1)\n                torch.distributed.all_reduce(iterator_stop, ReduceOp.SUM)\n\n        return all_steps_are_invalid\n\n    @classmethod\n    @torch.no_grad()\n    def validate_one_epoch(\n        cls,\n        model: torch.nn.Module,\n        iterator: Iterable[Dict[str, torch.Tensor]],\n        reporter: SubReporter,\n        options: TrainerOptions,\n    ) -> None:\n        assert check_argument_types()\n        ngpu = options.ngpu\n        no_forward_run = options.no_forward_run\n        distributed = isinstance(model, torch.nn.parallel.DistributedDataParallel)\n\n        model.eval()\n\n        # [For distributed] Because iteration counts are not always equals between\n        # processes, send stop-flag to the other processes if iterator is finished\n        iterator_stop = torch.tensor(0).to(""cuda"" if ngpu > 0 else ""cpu"")\n        for (_, batch) in iterator:\n            assert isinstance(batch, dict), type(batch)\n            if distributed:\n                torch.distributed.all_reduce(iterator_stop, ReduceOp.SUM)\n                if iterator_stop > 0:\n                    break\n\n            batch = to_device(batch, ""cuda"" if ngpu > 0 else ""cpu"")\n            if no_forward_run:\n                reporter.register({})\n                continue\n\n            _, stats, weight = model(**batch)\n            if ngpu > 1 or distributed:\n                # Apply weighted averaging for stats.\n                # if distributed, this method can also apply all_reduce()\n                stats, weight = recursive_average(stats, weight, distributed)\n\n            reporter.register(stats, weight)\n\n        else:\n            if distributed:\n                iterator_stop.fill_(1)\n                torch.distributed.all_reduce(iterator_stop, ReduceOp.SUM)\n\n    @classmethod\n    @torch.no_grad()\n    def plot_attention(\n        cls,\n        model: torch.nn.Module,\n        output_dir: Optional[Path],\n        summary_writer: Optional[SummaryWriter],\n        iterator: Iterable[Tuple[List[str], Dict[str, torch.Tensor]]],\n        reporter: SubReporter,\n        options: TrainerOptions,\n    ) -> None:\n        assert check_argument_types()\n        import matplotlib\n\n        ngpu = options.ngpu\n        no_forward_run = options.no_forward_run\n\n        matplotlib.use(""Agg"")\n        import matplotlib.pyplot as plt\n        from matplotlib.ticker import MaxNLocator\n\n        model.eval()\n        for ids, batch in iterator:\n            assert isinstance(batch, dict), type(batch)\n            assert len(next(iter(batch.values()))) == len(ids), (\n                len(next(iter(batch.values()))),\n                len(ids),\n            )\n            batch = to_device(batch, ""cuda"" if ngpu > 0 else ""cpu"")\n            if no_forward_run:\n                continue\n\n            # 1. Forwarding model and gathering all attentions\n            #    calculate_all_attentions() uses single gpu only.\n            att_dict = calculate_all_attentions(model, batch)\n\n            # 2. Plot attentions: This part is slow due to matplotlib\n            for k, att_list in att_dict.items():\n                assert len(att_list) == len(ids), (len(att_list), len(ids))\n                for id_, att_w in zip(ids, att_list):\n\n                    if isinstance(att_w, torch.Tensor):\n                        att_w = att_w.detach().cpu().numpy()\n\n                    if att_w.ndim == 2:\n                        att_w = att_w[None]\n                    elif att_w.ndim > 3 or att_w.ndim == 1:\n                        raise RuntimeError(f""Must be 2 or 3 dimension: {att_w.ndim}"")\n\n                    w, h = plt.figaspect(1.0 / len(att_w))\n                    fig = plt.Figure(figsize=(w * 1.3, h * 1.3))\n                    axes = fig.subplots(1, len(att_w))\n                    if len(att_w) == 1:\n                        axes = [axes]\n\n                    for ax, aw in zip(axes, att_w):\n                        ax.imshow(aw.astype(np.float32), aspect=""auto"")\n                        ax.set_title(f""{k}_{id_}"")\n                        ax.set_xlabel(""Input"")\n                        ax.set_ylabel(""Output"")\n                        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n                        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n\n                    if output_dir is not None:\n                        p = output_dir / id_ / f""{k}.{reporter.get_epoch()}ep.png""\n                        p.parent.mkdir(parents=True, exist_ok=True)\n                        fig.savefig(p)\n\n                    if summary_writer is not None:\n                        summary_writer.add_figure(\n                            f""{k}_{id_}"", fig, reporter.get_epoch()\n                        )\n\n                    # Dummy register() stimulates to increment the counter\n                    reporter.register({})\n'"
espnet2/tts/__init__.py,0,b''
espnet2/tts/abs_tts.py,12,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Dict\nfrom typing import Tuple\n\nimport torch\n\n\nclass AbsTTS(torch.nn.Module, ABC):\n    @abstractmethod\n    def forward(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        spembs: torch.Tensor = None,\n        spcs: torch.Tensor = None,\n        spcs_lengths: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def inference(\n        self,\n        text: torch.Tensor,\n        threshold: float,\n        minlenratio: float,\n        maxlenratio: float,\n        spembs: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        raise NotImplementedError\n'"
espnet2/tts/espnet_model.py,18,"b'from typing import Dict\nfrom typing import Optional\nfrom typing import Tuple\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet2.layers.abs_normalize import AbsNormalize\nfrom espnet2.layers.inversible_interface import InversibleInterface\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\nfrom espnet2.tts.abs_tts import AbsTTS\nfrom espnet2.tts.feats_extract.abs_feats_extract import AbsFeatsExtract\n\n\nclass ESPnetTTSModel(AbsESPnetModel):\n    def __init__(\n        self,\n        feats_extract: Optional[AbsFeatsExtract],\n        normalize: Optional[AbsNormalize and InversibleInterface],\n        tts: AbsTTS,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self.feats_extract = feats_extract\n        self.normalize = normalize\n        self.tts = tts\n\n    def forward(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        spembs: torch.Tensor = None,\n        spcs: torch.Tensor = None,\n        spcs_lengths: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:\n        feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n\n        if self.normalize is not None:\n            feats, feats_lengths = self.normalize(feats, feats_lengths)\n\n        return self.tts(\n            text=text,\n            text_lengths=text_lengths,\n            speech=feats,\n            speech_lengths=feats_lengths,\n            spembs=spembs,\n            spcs=spcs,\n            spcs_lengths=spcs_lengths,\n        )\n\n    def collect_feats(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        spembs: torch.Tensor = None,\n        spcs: torch.Tensor = None,\n        spcs_lengths: torch.Tensor = None,\n    ) -> Dict[str, torch.Tensor]:\n        feats, feats_lengths = self._extract_feats(speech, speech_lengths)\n        return {""feats"": feats, ""feats_lengths"": feats_lengths}\n\n    def _extract_feats(\n        self, speech: torch.Tensor, speech_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if self.feats_extract is not None:\n            feats, feats_lengths = self.feats_extract(speech, speech_lengths)\n        else:\n            feats, feats_lengths = speech, speech_lengths\n        return feats, feats_lengths\n'"
espnet2/tts/tacotron2.py,14,"b'# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Tacotron 2 related modules.""""""\n\nimport logging\nfrom typing import Dict\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.e2e_tts_tacotron2 import GuidedAttentionLoss\nfrom espnet.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2Loss\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttForward\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttForwardTA\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttLoc\nfrom espnet.nets.pytorch_backend.tacotron2.cbhg import CBHG\nfrom espnet.nets.pytorch_backend.tacotron2.cbhg import CBHGLoss\nfrom espnet.nets.pytorch_backend.tacotron2.decoder import Decoder\nfrom espnet.nets.pytorch_backend.tacotron2.encoder import Encoder\nfrom espnet2.torch_utils.device_funcs import force_gatherable\nfrom espnet2.tts.abs_tts import AbsTTS\n\n\nclass Tacotron2(AbsTTS):\n    """"""Tacotron2 module for end-to-end text-to-speech.\n\n    This is a module of Spectrogram prediction network in Tacotron2 described\n    in `Natural TTS Synthesis\n    by Conditioning WaveNet on Mel Spectrogram Predictions`_, which converts\n    the sequence of characters into the sequence of Mel-filterbanks.\n\n    .. _`Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`:\n       https://arxiv.org/abs/1712.05884\n\n    Args:\n        idim: Dimension of the inputs.\n        odim: Dimension of the outputs.\n        spk_embed_dim: Dimension of the speaker embedding.\n        embed_dim: Dimension of character embedding.\n        elayers: The number of encoder blstm layers.\n        eunits: The number of encoder blstm units.\n        econv_layers: The number of encoder conv layers.\n        econv_filts: The number of encoder conv filter size.\n        econv_chans: The number of encoder conv filter channels.\n        dlayers: The number of decoder lstm layers.\n        dunits: The number of decoder lstm units.\n        prenet_layers: The number of prenet layers.\n        prenet_units: The number of prenet units.\n        postnet_layers: The number of postnet layers.\n        postnet_filts: The number of postnet filter size.\n        postnet_chans: The number of postnet filter channels.\n        output_activation: The name of activation function for outputs.\n        adim: The number of dimension of mlp in attention.\n        aconv_chans: The number of attention conv filter channels.\n        aconv_filts: The number of attention conv filter size.\n        cumulate_att_w: Whether to cumulate previous attention weight.\n        use_batch_norm: Whether to use batch normalization.\n        use_concate: Whether to concatenate encoder embedding with decoder\n            lstm outputs.\n        dropout_rate: Dropout rate.\n        zoneout_rate: Zoneout rate.\n        reduction_factor: Reduction factor.\n        spk_embed_dim: Number of speaker embedding dimenstions.\n        spc_dim: Number of spectrogram embedding dimenstions\n            (only for use_cbhg=True).\n        use_cbhg: Whether to use CBHG module.\n        cbhg_conv_bank_layers: The number of convoluional banks in CBHG.\n        cbhg_conv_bank_chans: The number of channels of convolutional bank in\n            CBHG.\n        cbhg_proj_filts: The number of filter size of projection layeri in\n            CBHG.\n        cbhg_proj_chans: The number of channels of projection layer in CBHG.\n        cbhg_highway_layers: The number of layers of highway network in CBHG.\n        cbhg_highway_units: The number of units of highway network in CBHG.\n        cbhg_gru_units: The number of units of GRU in CBHG.\n        use_masking: Whether to mask padded part in loss calculation.\n        use_weighted_masking: Whether to apply weighted masking in\n            loss calculation.\n        bce_pos_weight: Weight of positive sample of stop token\n            (only for use_masking=True).\n        use_guided_attn_loss: Whether to use guided attention loss.\n        guided_attn_loss_sigma: Sigma in guided attention loss.\n        guided_attn_loss_lamdba: Lambda in guided attention loss.\n    """"""\n\n    def __init__(\n        self,\n        idim: int,\n        odim: int,\n        embed_dim: int = 512,\n        elayers: int = 1,\n        eunits: int = 512,\n        econv_layers: int = 3,\n        econv_chans: int = 512,\n        econv_filts: int = 5,\n        atype: str = ""location"",\n        adim: int = 512,\n        aconv_chans: int = 32,\n        aconv_filts: int = 15,\n        cumulate_att_w: bool = True,\n        dlayers: int = 2,\n        dunits: int = 1024,\n        prenet_layers: int = 2,\n        prenet_units: int = 256,\n        postnet_layers: int = 5,\n        postnet_chans: int = 512,\n        postnet_filts: int = 5,\n        output_activation: str = None,\n        use_cbhg: bool = False,\n        cbhg_conv_bank_layers: int = 8,\n        cbhg_conv_bank_chans: int = 128,\n        cbhg_conv_proj_filts: int = 3,\n        cbhg_conv_proj_chans: int = 256,\n        cbhg_highway_layers: int = 4,\n        cbhg_highway_units: int = 128,\n        cbhg_gru_units: int = 256,\n        use_batch_norm: bool = True,\n        use_concate: bool = True,\n        use_residual: bool = False,\n        dropout_rate: float = 0.5,\n        zoneout_rate: float = 0.1,\n        reduction_factor: int = 1,\n        spk_embed_dim: int = None,\n        spc_dim: int = None,\n        use_masking: bool = True,\n        use_weighted_masking: bool = False,\n        bce_pos_weight: float = 5.0,\n        use_guided_attn_loss: bool = True,\n        guided_attn_loss_sigma: float = 0.4,\n        guided_attn_loss_lambda: float = 1.0,\n    ):\n        assert check_argument_types()\n        super().__init__()\n\n        # store hyperparameters\n        self.idim = idim\n        self.odim = odim\n        self.eos = idim - 1\n        self.spk_embed_dim = spk_embed_dim\n        self.cumulate_att_w = cumulate_att_w\n        self.reduction_factor = reduction_factor\n        self.use_cbhg = use_cbhg\n        self.use_guided_attn_loss = use_guided_attn_loss\n\n        # define activation function for the final output\n        if output_activation is None:\n            self.output_activation_fn = None\n        elif hasattr(F, output_activation):\n            self.output_activation_fn = getattr(F, output_activation)\n        else:\n            raise ValueError(\n                f""there is no such an activation function. "" f""({output_activation})""\n            )\n\n        # set padding idx\n        padding_idx = 0\n        self.padding_idx = padding_idx\n\n        # define network modules\n        self.enc = Encoder(\n            idim=idim,\n            embed_dim=embed_dim,\n            elayers=elayers,\n            eunits=eunits,\n            econv_layers=econv_layers,\n            econv_chans=econv_chans,\n            econv_filts=econv_filts,\n            use_batch_norm=use_batch_norm,\n            use_residual=use_residual,\n            dropout_rate=dropout_rate,\n            padding_idx=padding_idx,\n        )\n\n        dec_idim = eunits if spk_embed_dim is None else eunits + spk_embed_dim\n        if atype == ""location"":\n            att = AttLoc(dec_idim, dunits, adim, aconv_chans, aconv_filts)\n        elif atype == ""forward"":\n            att = AttForward(dec_idim, dunits, adim, aconv_chans, aconv_filts)\n            if self.cumulate_att_w:\n                logging.warning(\n                    ""cumulation of attention weights is disabled ""\n                    ""in forward attention.""\n                )\n                self.cumulate_att_w = False\n        elif atype == ""forward_ta"":\n            att = AttForwardTA(dec_idim, dunits, adim, aconv_chans, aconv_filts, odim)\n            if self.cumulate_att_w:\n                logging.warning(\n                    ""cumulation of attention weights is disabled ""\n                    ""in forward attention.""\n                )\n                self.cumulate_att_w = False\n        else:\n            raise NotImplementedError(""Support only location or forward"")\n        self.dec = Decoder(\n            idim=dec_idim,\n            odim=odim,\n            att=att,\n            dlayers=dlayers,\n            dunits=dunits,\n            prenet_layers=prenet_layers,\n            prenet_units=prenet_units,\n            postnet_layers=postnet_layers,\n            postnet_chans=postnet_chans,\n            postnet_filts=postnet_filts,\n            output_activation_fn=self.output_activation_fn,\n            cumulate_att_w=self.cumulate_att_w,\n            use_batch_norm=use_batch_norm,\n            use_concate=use_concate,\n            dropout_rate=dropout_rate,\n            zoneout_rate=zoneout_rate,\n            reduction_factor=reduction_factor,\n        )\n        self.taco2_loss = Tacotron2Loss(\n            use_masking=use_masking,\n            use_weighted_masking=use_weighted_masking,\n            bce_pos_weight=bce_pos_weight,\n        )\n        if self.use_guided_attn_loss:\n            self.attn_loss = GuidedAttentionLoss(\n                sigma=guided_attn_loss_sigma, alpha=guided_attn_loss_lambda,\n            )\n        if self.use_cbhg:\n            self.cbhg = CBHG(\n                idim=odim,\n                odim=spc_dim,\n                conv_bank_layers=cbhg_conv_bank_layers,\n                conv_bank_chans=cbhg_conv_bank_chans,\n                conv_proj_filts=cbhg_conv_proj_filts,\n                conv_proj_chans=cbhg_conv_proj_chans,\n                highway_layers=cbhg_highway_layers,\n                highway_units=cbhg_highway_units,\n                gru_units=cbhg_gru_units,\n            )\n            self.cbhg_loss = CBHGLoss(use_masking=use_masking)\n\n    def forward(\n        self,\n        text: torch.Tensor,\n        text_lengths: torch.Tensor,\n        speech: torch.Tensor,\n        speech_lengths: torch.Tensor,\n        spembs: torch.Tensor = None,\n        spcs: torch.Tensor = None,\n        spcs_lengths: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:\n        """"""Calculate forward propagation.\n\n        Args:\n            text: Batch of padded character ids (B, Tmax).\n            text_lengths: Batch of lengths of each input batch (B,).\n            speech: Batch of padded target features (B, Lmax, odim).\n            speech_lengths: Batch of the lengths of each target (B,).\n            spembs: Batch of speaker embedding vectors (B, spk_embed_dim).\n            spcs: Batch of ground-truth spectrogram (B, Lmax, spc_dim).\n            spcs_lengths:\n        """"""\n        text = text[:, : text_lengths.max()]  # for data-parallel\n        speech = speech[:, : speech_lengths.max()]  # for data-parallel\n\n        batch_size = text.size(0)\n        # Add eos at the last of sequence\n        xs = F.pad(text, [0, 1], ""constant"", 0.0)\n        for i, l in enumerate(text_lengths):\n            xs[i, l] = self.eos\n        ilens = text_lengths + 1\n\n        ys = speech\n        olens = speech_lengths\n\n        # make labels for stop prediction\n        labels = make_pad_mask(olens).to(ys.device, ys.dtype)\n\n        # calculate tacotron2 outputs\n        hs, hlens = self.enc(xs, ilens)\n        if self.spk_embed_dim is not None:\n            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n            hs = torch.cat([hs, spembs], dim=-1)\n        after_outs, before_outs, logits, att_ws = self.dec(hs, hlens, ys)\n\n        # modify mod part of groundtruth\n        if self.reduction_factor > 1:\n            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])\n            max_out = max(olens)\n            ys = ys[:, :max_out]\n            labels = labels[:, :max_out]\n            labels[:, -1] = 1.0  # make sure at least one frame has 1\n\n        # calculate taco2 loss\n        l1_loss, mse_loss, bce_loss = self.taco2_loss(\n            after_outs, before_outs, logits, ys, labels, olens\n        )\n        loss = l1_loss + mse_loss + bce_loss\n\n        stats = dict(\n            l1_loss=l1_loss.item(), mse_loss=mse_loss.item(), bce_loss=bce_loss.item(),\n        )\n\n        # calculate attention loss\n        if self.use_guided_attn_loss:\n            # NOTE(kan-bayashi): length of output for auto-regressive\n            # input will be changed when r > 1\n            if self.reduction_factor > 1:\n                olens_in = olens.new([olen // self.reduction_factor for olen in olens])\n            else:\n                olens_in = olens\n            attn_loss = self.attn_loss(att_ws, ilens, olens_in)\n            loss = loss + attn_loss\n            stats.update(attn_loss=attn_loss.item())\n\n        # caluculate cbhg loss\n        if self.use_cbhg:\n            # remove unnecessary padded part (for multi-gpus)\n            if max_out != spcs.shape[1]:\n                spcs = spcs[:, :max_out]\n\n            # caluculate cbhg outputs & loss and report them\n            cbhg_outs, _ = self.cbhg(after_outs, olens)\n            cbhg_l1_loss, cbhg_mse_loss = self.cbhg_loss(cbhg_outs, spcs, olens)\n            loss = loss + cbhg_l1_loss + cbhg_mse_loss\n            stats.update(\n                cbhg_l1_loss=cbhg_l1_loss.item(), cbhg_mse_loss=cbhg_mse_loss.item(),\n            )\n\n        stats.update(loss=loss.item())\n\n        loss, stats, weight = force_gatherable((loss, stats, batch_size), loss.device)\n        return loss, stats, weight\n\n    def inference(\n        self,\n        text: torch.Tensor,\n        spembs: torch.Tensor = None,\n        threshold: float = 0.5,\n        minlenratio: float = 0.0,\n        maxlenratio: float = 10.0,\n        use_att_constraint: bool = False,\n        backward_window: int = 1,\n        forward_window: int = 3,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        """"""Generate the sequence of features given the sequences of characters.\n\n        Args:\n            text: Input sequence of characters (T,).\n            spembs: Speaker embedding vector (spk_embed_dim,).\n            threshold: Threshold in inference.\n            minlenratio: Minimum length ratio in inference.\n            maxlenratio: Maximum length ratio in inference.\n            use_att_constraint: Whether to apply attention constraint.\n            backward_window: Backward window in attention constraint.\n            forward_window: Forward window in attention constraint.\n\n        Returns:\n            Tensor: Output sequence of features (L, odim).\n            Tensor: Output sequence of stop probabilities (L,).\n            Tensor: Attention weights (L, T).\n\n        """"""\n        x = text\n        spemb = spembs\n\n        # inference\n        h = self.enc.inference(x)\n        if self.spk_embed_dim is not None:\n            spemb = F.normalize(spemb, dim=0).unsqueeze(0).expand(h.size(0), -1)\n            h = torch.cat([h, spemb], dim=-1)\n        outs, probs, att_ws = self.dec.inference(\n            h,\n            threshold=threshold,\n            minlenratio=minlenratio,\n            maxlenratio=maxlenratio,\n            use_att_constraint=use_att_constraint,\n            backward_window=backward_window,\n            forward_window=forward_window,\n        )\n\n        if self.use_cbhg:\n            cbhg_outs = self.cbhg.inference(outs)\n            return cbhg_outs, probs, att_ws\n        else:\n            return outs, probs, att_ws\n'"
espnet2/utils/__init__.py,0,b''
espnet2/utils/build_dataclass.py,0,"b'import argparse\nimport dataclasses\n\nfrom typeguard import check_type\n\n\ndef build_dataclass(dataclass, args: argparse.Namespace):\n    """"""Helper function to build dataclass from \'args\'.""""""\n    kwargs = {}\n    for field in dataclasses.fields(dataclass):\n        if not hasattr(args, field.name):\n            raise ValueError(\n                f""args doesn\'t have {field.name}. You need to set it to ArgumentsParser""\n            )\n        check_type(field.name, getattr(args, field.name), field.type)\n        kwargs[field.name] = getattr(args, field.name)\n    return dataclass(**kwargs)\n'"
espnet2/utils/fileio.py,0,"b'import collections.abc\nfrom io import StringIO\nimport logging\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import Union\nimport warnings\n\nimport numpy as np\nimport soundfile\nfrom typeguard import check_argument_types\nfrom typeguard import check_return_type\n\n\nclass DatadirWriter:\n    """"""Writer class to create kaldi like data directory.\n\n    Examples:\n        >>> with DatadirWriter(""output"") as writer:\n        ...     # output/sub.txt is created here\n        ...     subwriter = writer[""sub.txt""]\n        ...     # Write ""uttidA some/where/a.wav""\n        ...     subwriter[""uttidA""] = ""some/where/a.wav""\n        ...     subwriter[""uttidB""] = ""some/where/b.wav""\n\n    """"""\n\n    def __init__(self, p: Union[Path, str]):\n        assert check_argument_types()\n        self.path = Path(p)\n        self.chilidren = {}\n        self.fd = None\n        self.has_children = False\n        self.keys = set()\n\n    def __enter__(self):\n        return self\n\n    def __getitem__(self, key: str) -> ""DatadirWriter"":\n        assert check_argument_types()\n        if self.fd is not None:\n            raise RuntimeError(""This writer points out a file"")\n\n        if key not in self.chilidren:\n            w = DatadirWriter((self.path / key))\n            self.chilidren[key] = w\n            self.has_children = True\n\n        retval = self.chilidren[key]\n        assert check_return_type(retval)\n        return retval\n\n    def __setitem__(self, key: str, value: str):\n        assert check_argument_types()\n        if self.has_children:\n            raise RuntimeError(""This writer points out a directory"")\n        if key in self.keys:\n            warnings.warn(f""Duplicated: {key}"")\n\n        if self.fd is None:\n            self.path.parent.mkdir(parents=True, exist_ok=True)\n            self.fd = self.path.open(""w"", encoding=""utf-8"")\n\n        self.keys.add(key)\n        self.fd.write(f""{key} {value}\\n"")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        if self.has_children:\n            prev_child = None\n            for child in self.chilidren.values():\n                child.close()\n                if prev_child is not None and prev_child.keys != child.keys:\n                    warnings.warn(\n                        f""Ids are mismatching between ""\n                        f""{prev_child.path} and {child.path}""\n                    )\n                prev_child = child\n\n        elif self.fd is not None:\n            self.fd.close()\n\n\ndef read_2column_text(path: Union[Path, str]) -> Dict[str, str]:\n    """"""Read a text file having 2 column as dict object.\n\n    Examples:\n        wav.scp:\n            key1 /some/path/a.wav\n            key2 /some/path/b.wav\n\n        >>> read_2column_text(\'wav.scp\')\n        {\'key1\': \'/some/path/a.wav\', \'key2\': \'/some/path/b.wav\'}\n\n    """"""\n    assert check_argument_types()\n\n    data = {}\n    with Path(path).open(""r"", encoding=""utf-8"") as f:\n        for linenum, line in enumerate(f, 1):\n            sps = line.rstrip().split(maxsplit=1)\n            if len(sps) != 2:\n                raise RuntimeError(\n                    f""scp file must have two or more columns: ""\n                    f""{line} ({path}:{linenum})""\n                )\n            k, v = sps\n            if k in data:\n                raise RuntimeError(f""{k} is duplicated ({path}:{linenum})"")\n            data[k] = v.rstrip()\n    assert check_return_type(data)\n    return data\n\n\ndef load_num_sequence_text(\n    path: Union[Path, str], loader_type: str = ""csv_int""\n) -> Dict[str, np.ndarray]:\n    assert check_argument_types()\n    if loader_type == ""text_int"":\n        delimiter = "" ""\n        dtype = np.long\n    elif loader_type == ""text_float"":\n        delimiter = "" ""\n        dtype = np.float32\n    elif loader_type == ""csv_int"":\n        delimiter = "",""\n        dtype = np.long\n    elif loader_type == ""csv_float"":\n        delimiter = "",""\n        dtype = np.float32\n    else:\n        raise ValueError(f""Not supported loader_type={loader_type}"")\n\n    # path looks like:\n    #   utta 1,0\n    #   uttb 3,4,5\n    # -> return {\'utta\': np.ndarray([1, 0]),\n    #            \'uttb\': np.ndarray([3, 4, 5])}\n    d = read_2column_text(path)\n\n    # Using for-loop instead of dict-comprehension for debuggability\n    retval = {}\n    for k, v in d.items():\n        try:\n            retval[k] = np.loadtxt(\n                StringIO(v), ndmin=1, dtype=dtype, delimiter=delimiter\n            )\n        except ValueError:\n            logging.error(\n                f\'Error happened with path=""{path}"", \' f\'id=""{k}"", value=""{v}""\'\n            )\n            raise\n    assert check_return_type(retval)\n    return retval\n\n\nclass SoundScpReader(collections.abc.Mapping):\n    """"""Reader class for \'wav.scp\'.\n\n    Examples:\n        key1 /some/path/a.wav\n        key2 /some/path/b.wav\n        key3 /some/path/c.wav\n        key4 /some/path/d.wav\n        ...\n\n        >>> reader = SoundScpReader(\'wav.scp\')\n        >>> rate, array = reader[\'key1\']\n\n    """"""\n\n    def __init__(\n        self, fname, dtype=np.int16, always_2d: bool = False, normalize: bool = False,\n    ):\n        assert check_argument_types()\n        self.fname = fname\n        self.dtype = dtype\n        self.always_2d = always_2d\n        self.normalize = normalize\n        self.data = read_2column_text(fname)\n\n    def __getitem__(self, key):\n        wav = self.data[key]\n        if self.normalize:\n            # soundfile.read normalizes data to [-1,1] if dtype is not given\n            array, rate = soundfile.read(wav, always_2d=self.always_2d)\n        else:\n            array, rate = soundfile.read(\n                wav, dtype=self.dtype, always_2d=self.always_2d\n            )\n\n        return rate, array\n\n    def get_path(self, key):\n        return self.data[key]\n\n    def __contains__(self, item):\n        return item\n\n    def __len__(self):\n        return len(self.data)\n\n    def __iter__(self):\n        return iter(self.data)\n\n    def keys(self):\n        return self.data.keys()\n\n\nclass SoundScpWriter:\n    """"""Writer class for \'wav.scp\'\n\n    Examples:\n        key1 /some/path/a.wav\n        key2 /some/path/b.wav\n        key3 /some/path/c.wav\n        key4 /some/path/d.wav\n        ...\n\n        >>> writer = SoundScpWriter(\'./data/\', \'./data/feat.scp\')\n        >>> writer[\'aa\'] = 16000, numpy_array\n        >>> writer[\'bb\'] = 16000, numpy_array\n\n    """"""\n\n    def __init__(\n        self,\n        outdir: Union[Path, str],\n        scpfile: Union[Path, str],\n        format=""wav"",\n        dtype=None,\n    ):\n        assert check_argument_types()\n        self.dir = Path(outdir)\n        self.dir.mkdir(parents=True, exist_ok=True)\n        scpfile = Path(scpfile)\n        scpfile.parent.mkdir(parents=True, exist_ok=True)\n        self.fscp = scpfile.open(""w"", encoding=""utf-8"")\n        self.format = format\n        self.dtype = dtype\n\n        self.data = {}\n\n    def __setitem__(self, key: str, value):\n        rate, signal = value\n        assert isinstance(rate, int), type(rate)\n        assert isinstance(signal, np.ndarray), type(signal)\n        if signal.ndim not in (1, 2):\n            raise RuntimeError(f""Input signal must be 1 or 2 dimension: {signal.ndim}"")\n        if signal.ndim == 1:\n            signal = signal[:, None]\n\n        wav = self.dir / f""{key}.{self.format}""\n        wav.parent.mkdir(parents=True, exist_ok=True)\n        soundfile.write(str(wav), signal, rate)\n\n        self.fscp.write(f""{key} {wav}\\n"")\n\n        # Store the file path\n        self.data[key] = str(wav)\n\n    def get_path(self, key):\n        return self.data[key]\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        self.fscp.close()\n\n\nclass NpyScpWriter:\n    """"""Writer class for a scp file of numpy file.\n\n    Examples:\n        key1 /some/path/a.npy\n        key2 /some/path/b.npy\n        key3 /some/path/c.npy\n        key4 /some/path/d.npy\n        ...\n\n        >>> writer = NpyScpWriter(\'./data/\', \'./data/feat.scp\')\n        >>> writer[\'aa\'] = numpy_array\n        >>> writer[\'bb\'] = numpy_array\n\n    """"""\n\n    def __init__(self, outdir: Union[Path, str], scpfile: Union[Path, str]):\n        assert check_argument_types()\n        self.dir = Path(outdir)\n        self.dir.mkdir(parents=True, exist_ok=True)\n        scpfile = Path(scpfile)\n        scpfile.parent.mkdir(parents=True, exist_ok=True)\n        self.fscp = scpfile.open(""w"", encoding=""utf-8"")\n\n        self.data = {}\n\n    def get_path(self, key):\n        return self.data[key]\n\n    def __setitem__(self, key, value):\n        assert isinstance(value, np.ndarray), type(value)\n        p = self.dir / f""{key}.npy""\n        p.parent.mkdir(parents=True, exist_ok=True)\n        np.save(str(p), value)\n        self.fscp.write(f""{key} {p}\\n"")\n\n        # Store the file path\n        self.data[key] = str(p)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        self.fscp.close()\n\n\nclass NpyScpReader(collections.abc.Mapping):\n    """"""Reader class for a scp file of numpy file.\n\n    Examples:\n        key1 /some/path/a.npy\n        key2 /some/path/b.npy\n        key3 /some/path/c.npy\n        key4 /some/path/d.npy\n        ...\n\n        >>> reader = NpyScpReader(\'npy.scp\')\n        >>> array = reader[\'key1\']\n\n    """"""\n\n    def __init__(self, fname: Union[Path, str]):\n        assert check_argument_types()\n        self.fname = Path(fname)\n        self.data = read_2column_text(fname)\n\n    def get_path(self, key):\n        return self.data[key]\n\n    def __getitem__(self, key) -> np.ndarray:\n        p = self.data[key]\n        return np.load(p)\n\n    def __contains__(self, item):\n        return item\n\n    def __len__(self):\n        return len(self.data)\n\n    def __iter__(self):\n        return iter(self.data)\n\n    def keys(self):\n        return self.data.keys()\n'"
espnet2/utils/get_default_kwargs.py,0,"b'import inspect\n\n\nclass Invalid:\n    """"""Marker object for not serializable-object""""""\n\n\ndef get_default_kwargs(func):\n    """"""Get the default values of the input function.\n\n    Examples:\n        >>> def func(a, b=3):  pass\n        >>> get_default_kwargs(func)\n        {\'b\': 3}\n\n    """"""\n\n    def yaml_serializable(value):\n        # isinstance(x, tuple) includes namedtuple, so type is used here\n        if type(value) is tuple:\n            return yaml_serializable(list(value))\n        elif isinstance(value, set):\n            return yaml_serializable(list(value))\n        elif isinstance(value, dict):\n            if not all(isinstance(k, str) for k in value):\n                return Invalid\n            retval = {}\n            for k, v in value.items():\n                v2 = yaml_serializable(v)\n                # Register only valid object\n                if v2 not in (Invalid, inspect.Parameter.empty):\n                    retval[k] = v2\n            return retval\n        elif isinstance(value, list):\n            retval = []\n            for v in value:\n                v2 = yaml_serializable(v)\n                # If any elements in the list are invalid,\n                # the list also becomes invalid\n                if v2 is Invalid:\n                    return Invalid\n                else:\n                    retval.append(v2)\n            return retval\n        elif value in (inspect.Parameter.empty, None):\n            return value\n        elif isinstance(value, (float, int, complex, bool, str, bytes)):\n            return value\n        else:\n            return Invalid\n\n    # params: An ordered mapping of inspect.Parameter\n    params = inspect.signature(func).parameters\n    data = {p.name: p.default for p in params.values()}\n    # Remove not yaml-serializable object\n    data = yaml_serializable(data)\n    return data\n'"
espnet2/utils/griffin_lim.py,1,"b'#!/usr/bin/env python3\n\n""""""Griffin-Lim related modules.""""""\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport logging\n\nfrom distutils.version import LooseVersion\nfrom functools import partial\nfrom typeguard import check_argument_types\nfrom typing import Optional\n\nimport librosa\nimport numpy as np\n\nEPS = 1e-10\n\n\ndef logmel2linear(\n    lmspc: np.ndarray,\n    fs: int,\n    n_fft: int,\n    n_mels: int,\n    fmin: int = None,\n    fmax: int = None,\n) -> np.ndarray:\n    """"""Convert log Mel filterbank to linear spectrogram.\n\n    Args:\n        lmspc: Log Mel filterbank (T, n_mels).\n        fs: Sampling frequency.\n        n_fft: The number of FFT points.\n        n_mels: The number of mel basis.\n        f_min: Minimum frequency to analyze.\n        f_max: Maximum frequency to analyze.\n\n    Returns:\n        Linear spectrogram (T, n_fft // 2 + 1).\n\n    """"""\n    assert lmspc.shape[1] == n_mels\n    fmin = 0 if fmin is None else fmin\n    fmax = fs / 2 if fmax is None else fmax\n    mspc = np.power(10.0, lmspc)\n    mel_basis = librosa.filters.mel(fs, n_fft, n_mels, fmin, fmax)\n    inv_mel_basis = np.linalg.pinv(mel_basis)\n    return np.maximum(EPS, np.dot(inv_mel_basis, mspc.T).T)\n\n\ndef griffin_lim(\n    spc: np.ndarray,\n    n_fft: int,\n    n_shift: int,\n    win_length: int = None,\n    window: Optional[str] = ""hann"",\n    n_iter: Optional[int] = 32,\n) -> np.ndarray:\n    """"""Convert linear spectrogram into waveform using Griffin-Lim.\n\n    Args:\n        spc: Linear spectrogram (T, n_fft // 2 + 1).\n        n_fft: The number of FFT points.\n        n_shift: Shift size in points.\n        win_length: Window length in points.\n        window: Window function type.\n        n_iter: The number of iterations.\n\n    Returns:\n        Reconstructed waveform (N,).\n\n    """"""\n    # assert the size of input linear spectrogram\n    assert spc.shape[1] == n_fft // 2 + 1\n\n    if LooseVersion(librosa.__version__) >= LooseVersion(""0.7.0""):\n        # use librosa\'s fast Grriffin-Lim algorithm\n        spc = np.abs(spc.T)\n        y = librosa.griffinlim(\n            S=spc,\n            n_iter=n_iter,\n            hop_length=n_shift,\n            win_length=win_length,\n            window=window,\n            center=True if spc.shape[1] > 1 else False,\n        )\n    else:\n        # use slower version of Grriffin-Lim algorithm\n        logging.warning(\n            ""librosa version is old. use slow version of Grriffin-Lim algorithm.""\n            ""if you want to use fast Griffin-Lim, please update librosa via ""\n            ""`source ./path.sh && pip install librosa==0.7.0`.""\n        )\n        cspc = np.abs(spc).astype(np.complex).T\n        angles = np.exp(2j * np.pi * np.random.rand(*cspc.shape))\n        y = librosa.istft(cspc * angles, n_shift, win_length, window=window)\n        for i in range(n_iter):\n            angles = np.exp(\n                1j\n                * np.angle(librosa.stft(y, n_fft, n_shift, win_length, window=window))\n            )\n            y = librosa.istft(cspc * angles, n_shift, win_length, window=window)\n\n    return y\n\n\n# TODO(kan-bayashi): write as torch.nn.Module\nclass Spectrogram2Waveform(object):\n    """"""Spectrogram to waveform conversion module.""""""\n\n    def __init__(\n        self,\n        n_fft: int,\n        n_shift: int,\n        fs: int = None,\n        n_mels: int = None,\n        win_length: int = None,\n        window: Optional[str] = ""hann"",\n        fmin: int = None,\n        fmax: int = None,\n        griffin_lim_iters: Optional[int] = 32,\n    ):\n        """"""Initialize module.\n\n        Args:\n            fs: Sampling frequency.\n            n_fft: The number of FFT points.\n            n_shift: Shift size in points.\n            n_mels: The number of mel basis.\n            win_length: Window length in points.\n            window: Window function type.\n            f_min: Minimum frequency to analyze.\n            f_max: Maximum frequency to analyze.\n            griffin_lim_iters: The number of iterations.\n\n        """"""\n        assert check_argument_types()\n        self.fs = fs\n        self.logmel2linear = (\n            partial(\n                logmel2linear, fs=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax\n            )\n            if n_mels is not None\n            else None\n        )\n        self.griffin_lim = partial(\n            griffin_lim,\n            n_fft=n_fft,\n            n_shift=n_shift,\n            win_length=win_length,\n            window=window,\n            n_iter=griffin_lim_iters,\n        )\n        self.params = dict(\n            n_fft=n_fft,\n            n_shift=n_shift,\n            win_length=win_length,\n            window=window,\n            n_iter=griffin_lim_iters,\n        )\n        if n_mels is not None:\n            self.params.update(fs=fs, n_mels=n_mels, fmin=fmin, fmax=fmax)\n\n    def __repr__(self):\n        retval = f""{self.__class__.__name__}(""\n        for k, v in self.params.items():\n            retval += f""{k}={v}, ""\n        retval += "")""\n        return retval\n\n    def __call__(self, spc):\n        """"""Convert spectrogram to waveform.\n\n        Args:\n            spc: Log Mel filterbank (T, n_mels)\n                or linear spectrogram (T, n_fft // 2 + 1).\n\n        Returns:\n            Reconstructed waveform (N,).\n\n        """"""\n        if self.logmel2linear is not None:\n            spc = self.logmel2linear(spc)\n        return self.griffin_lim(spc)\n'"
espnet2/utils/nested_dict_action.py,0,"b'import argparse\nimport copy\n\nimport yaml\n\n\nclass NestedDictAction(argparse.Action):\n    """"""Action class to append items to dict object.\n\n    Examples:\n        >>> parser = argparse.ArgumentParser()\n        >>> _ = parser.add_argument(\'--conf\', action=NestedDictAction,\n        ...                         default={\'a\': 4})\n        >>> parser.parse_args([\'--conf\', \'a=3\', \'--conf\', \'c=4\'])\n        Namespace(conf={\'a\': 3, \'c\': 4})\n        >>> parser.parse_args([\'--conf\', \'c.d=4\'])\n        Namespace(conf={\'a\': 4, \'c\': {\'d\': 4}})\n        >>> parser.parse_args([\'--conf\', \'c.d=4\', \'--conf\', \'c=2\'])\n        Namespace(conf={\'a\': 4, \'c\': 2})\n        >>> parser.parse_args([\'--conf\', \'{d: 5, e: 9}\'])\n        Namespace(conf={\'d\': 5, \'e\': 9})\n\n    """"""\n\n    _syntax = """"""Syntax:\n  {op} <key>=<yaml-string>\n  {op} <key>.<key2>=<yaml-string>\n  {op} <python-dict>\n  {op} <yaml-string>\ne.g.\n  {op} a=4\n  {op} a.b={{c: true}}\n  {op} {{""c"": True}}\n  {op} {{a: 34.5}}\n""""""\n\n    def __init__(\n        self,\n        option_strings,\n        dest,\n        nargs=None,\n        default=None,\n        choices=None,\n        required=False,\n        help=None,\n        metavar=None,\n    ):\n        super().__init__(\n            option_strings=option_strings,\n            dest=dest,\n            nargs=nargs,\n            default=copy.deepcopy(default),\n            type=None,\n            choices=choices,\n            required=required,\n            help=help,\n            metavar=metavar,\n        )\n\n    def __call__(self, parser, namespace, values, option_strings=None):\n        # --{option} a.b=3 -> {\'a\': {\'b\': 3}}\n        if ""="" in values:\n            indict = copy.deepcopy(getattr(namespace, self.dest, {}))\n            key, value = values.split(""="", maxsplit=1)\n            if not value.strip() == """":\n                value = yaml.load(value, Loader=yaml.Loader)\n            if not isinstance(indict, dict):\n                indict = {}\n\n            keys = key.split(""."")\n            d = indict\n            for idx, k in enumerate(keys):\n                if idx == len(keys) - 1:\n                    d[k] = value\n                else:\n                    v = d.setdefault(k, {})\n                    if not isinstance(v, dict):\n                        # Remove the existing value and recreates as empty dict\n                        d[k] = {}\n                    d = d[k]\n\n            # Update the value\n            setattr(namespace, self.dest, indict)\n        else:\n            setattr(namespace, self.dest, values)\n            try:\n                # At the first, try eval(), i.e. Python syntax dict.\n                # e.g. --{option} ""{\'a\': 3}"" -> {\'a\': 3}\n                # This is workaround for internal behaviour of configargparse.\n                value = eval(values, {}, {})\n                if not isinstance(value, dict):\n                    syntax = self._syntax.format(op=option_strings)\n                    mes = f""must be interpreted as dict: but got {values}\\n{syntax}""\n                    raise argparse.ArgumentTypeError(self, mes)\n            except Exception:\n                # and the second, try yaml.load\n                value = yaml.load(values, Loader=yaml.Loader)\n                if not isinstance(value, dict):\n                    syntax = self._syntax.format(op=option_strings)\n                    mes = f""must be interpreted as dict: but got {values}\\n{syntax}""\n                    raise argparse.ArgumentError(self, mes)\n            # Remove existing params, and overwrite\n            setattr(namespace, self.dest, value)\n'"
espnet2/utils/pack_funcs.py,1,"b'from collections import defaultdict\nfrom datetime import datetime\nfrom io import BytesIO\nfrom io import TextIOWrapper\nimport os\nfrom pathlib import Path\nimport sys\nimport tarfile\nfrom typing import Dict\nfrom typing import Iterable\nfrom typing import Union\n\nimport yaml\n\nDIRNAME = Path(""packed"")\n\n\ndef find_path_and_change_it_recursive(value, src: str, tgt: str):\n    if isinstance(value, dict):\n        return {\n            k: find_path_and_change_it_recursive(v, src, tgt) for k, v in value.items()\n        }\n    elif isinstance(value, (list, tuple)):\n        return [find_path_and_change_it_recursive(v, src, tgt) for v in value]\n    elif isinstance(value, str) and Path(value) == Path(src):\n        return tgt\n    else:\n        return value\n\n\ndef default_tarinfo(name) -> tarfile.TarInfo:\n    """"""Generate TarInfo using system information""""""\n    tarinfo = tarfile.TarInfo(str(name))\n    if os.name == ""posix"":\n        tarinfo.gid = os.getgid()\n        tarinfo.uid = os.getuid()\n    tarinfo.mtime = datetime.now().timestamp()\n    # Keep mode as default\n    return tarinfo\n\n\ndef unpack(\n    input_tarfile: Union[Path, str], outpath: Union[Path, str]\n) -> Dict[str, str]:\n    """"""Scan all files in the archive file and return as a dict of files.\n\n    Examples:\n        tarfile:\n           packed/asr_model_file.pth\n           packed/option/some1.file\n           packed/option/some2.file\n\n        >>> unpack(""tarfile"", ""out"")\n        {\'asr_model_file\': \'out/packed/asr_model_file.pth\',\n         \'option\': [\'out/packed/option/some1.file\', \'out/packed/option/some2.file\']}\n    """"""\n    input_tarfile = Path(input_tarfile)\n    outpath = Path(outpath)\n\n    with tarfile.open(input_tarfile) as tar:\n        for tarinfo in tar:\n            if tarinfo.name == str(DIRNAME / ""meta.yaml""):\n                d = yaml.safe_load(TextIOWrapper(tar.extractfile(tarinfo)))\n                yaml_files = d[""yaml_files""]\n                break\n        else:\n            raise RuntimeError(""Format error: not found meta.yaml"")\n\n        retval = defaultdict(list)\n        for tarinfo in tar:\n            outname = outpath / tarinfo.name\n            outname.parent.mkdir(parents=True, exist_ok=True)\n            if tarinfo.name in yaml_files:\n                d = yaml.safe_load(TextIOWrapper(tar.extractfile(tarinfo)))\n                # Rewrite yaml\n                for tarinfo2 in tar:\n                    d = find_path_and_change_it_recursive(\n                        d, tarinfo2.name, str(outpath / tarinfo2.name)\n                    )\n                with outname.open(""w"", encoding=""utf-8"") as f:\n                    yaml.safe_dump(d, f)\n            else:\n                tar.extract(tarinfo, path=outpath)\n\n            key = tarinfo.name.split(""/"")[1]\n            key = Path(key).stem\n            retval[key].append(str(outname))\n        retval = {k: v[0] if len(v) == 1 else v for k, v in retval.items()}\n        return retval\n\n\ndef pack(\n    files: Dict[str, Union[str, Path]],\n    yaml_files: Dict[str, Union[str, Path]],\n    outpath: Union[str, Path],\n    option: Iterable[Union[str, Path]] = (),\n    mode: str = ""w:gz"",\n):\n    for v in list(files.values()) + list(yaml_files.values()) + list(option):\n        if not Path(v).exists():\n            raise FileNotFoundError(f""No such file or directory: {v}"")\n\n    files_map = {}\n    for name, src in list(files.items()):\n        # Save as e.g. packed/asr_model_file.pth\n        dst = str(DIRNAME / name)\n        files_map[dst] = src\n\n    for src in option:\n        # Save as packed/option/${basename}\n        idx = 0\n        while True:\n            p = Path(src)\n            if idx == 0:\n                dst = str(DIRNAME / ""option"" / p.name)\n            else:\n                dst = str(DIRNAME / ""option"" / f""{p.stem}.{idx}{p.suffix}"")\n            if dst not in files_map:\n                files_map[dst] = src\n                break\n            idx += 1\n\n    # Read yaml and Change the file path to the archived path\n    yaml_files_map = {}\n    for name, path in yaml_files.items():\n        with open(path, ""r"", encoding=""utf-8"") as f:\n            dic = yaml.safe_load(f)\n            for dst, src in files_map.items():\n                dic = find_path_and_change_it_recursive(dic, src, dst)\n            dst = str(DIRNAME / name)\n            yaml_files_map[dst] = dic\n\n    meta_objs = dict(\n        files=list(files_map),\n        yaml_files=list(yaml_files_map),\n        timestamp=datetime.now().timestamp(),\n        python=sys.version,\n    )\n\n    try:\n        import torch\n\n        meta_objs.update(torch=torch.__version__)\n    except ImportError:\n        pass\n    try:\n        import espnet\n\n        meta_objs.update(espnet=espnet.__version__)\n    except ImportError:\n        pass\n\n    Path(outpath).parent.mkdir(parents=True, exist_ok=True)\n    with tarfile.open(outpath, mode=mode) as tar:\n        # Write packed/meta.yaml\n        fileobj = BytesIO(yaml.safe_dump(meta_objs).encode())\n        tarinfo = default_tarinfo(DIRNAME / ""meta.yaml"")\n        tarinfo.size = fileobj.getbuffer().nbytes\n        tar.addfile(tarinfo, fileobj=fileobj)\n\n        for dst, dic in yaml_files_map.items():\n            # Dump dict as yaml-bytes\n            fileobj = BytesIO(yaml.safe_dump(dic).encode())\n            # Embed the yaml-bytes in tarfile\n            tarinfo = default_tarinfo(dst)\n            tarinfo.size = fileobj.getbuffer().nbytes\n            tar.addfile(tarinfo, fileobj=fileobj)\n        for dst, src in files_map.items():\n            # Resolve to avoid symbolic link\n            tar.add(Path(src).resolve(), dst)\n'"
espnet2/utils/rand_gen_dataset.py,0,"b'import collections\nfrom pathlib import Path\nfrom typing import Union\n\nimport numpy as np\nfrom typeguard import check_argument_types\n\nfrom espnet2.utils.fileio import load_num_sequence_text\n\n\nclass FloatRandomGenerateDataset(collections.abc.Mapping):\n    """"""Generate float array from shape.txt.\n\n    Examples:\n        shape.txt\n        uttA 123,83\n        uttB 34,83\n        >>> dataset = FloatRandomGenerateDataset(""shape.txt"")\n        >>> array = dataset[""uttA""]\n        >>> assert array.shape == (123, 83)\n        >>> array = dataset[""uttB""]\n        >>> assert array.shape == (34, 83)\n\n    """"""\n\n    def __init__(\n        self,\n        shape_file: Union[Path, str],\n        dtype: Union[str, np.dtype] = ""float32"",\n        loader_type: str = ""csv_int"",\n    ):\n        assert check_argument_types()\n        shape_file = Path(shape_file)\n        self.utt2shape = load_num_sequence_text(shape_file, loader_type)\n        self.dtype = np.dtype(dtype)\n\n    def __iter__(self):\n        return iter(self.utt2shape)\n\n    def __len__(self):\n        return len(self.utt2shape)\n\n    def __getitem__(self, item) -> np.ndarray:\n        shape = self.utt2shape[item]\n        return np.random.randn(*shape).astype(self.dtype)\n\n\nclass IntRandomGenerateDataset(collections.abc.Mapping):\n    """"""Generate float array from shape.txt\n\n    Examples:\n        shape.txt\n        uttA 123,83\n        uttB 34,83\n        >>> dataset = IntRandomGenerateDataset(""shape.txt"", low=0, high=10)\n        >>> array = dataset[""uttA""]\n        >>> assert array.shape == (123, 83)\n        >>> array = dataset[""uttB""]\n        >>> assert array.shape == (34, 83)\n\n    """"""\n\n    def __init__(\n        self,\n        shape_file: Union[Path, str],\n        low: int,\n        high: int = None,\n        dtype: Union[str, np.dtype] = ""int64"",\n        loader_type: str = ""csv_int"",\n    ):\n        assert check_argument_types()\n        shape_file = Path(shape_file)\n        self.utt2shape = load_num_sequence_text(shape_file, loader_type)\n        self.dtype = np.dtype(dtype)\n        self.low = low\n        self.high = high\n\n    def __iter__(self):\n        return iter(self.utt2shape)\n\n    def __len__(self):\n        return len(self.utt2shape)\n\n    def __getitem__(self, item) -> np.ndarray:\n        shape = self.utt2shape[item]\n        return np.random.randint(self.low, self.high, size=shape, dtype=self.dtype)\n'"
espnet2/utils/sized_dict.py,0,"b'import collections\nimport sys\n\nfrom torch import multiprocessing\n\n\ndef get_size(obj, seen=None):\n    """"""Recursively finds size of objects\n\n    Taken from https://github.com/bosswissam/pysize\n\n    """"""\n\n    size = sys.getsizeof(obj)\n    if seen is None:\n        seen = set()\n\n    obj_id = id(obj)\n    if obj_id in seen:\n        return 0\n\n    # Important mark as seen *before* entering recursion to gracefully handle\n    # self-referential objects\n    seen.add(obj_id)\n\n    if isinstance(obj, dict):\n        size += sum([get_size(v, seen) for v in obj.values()])\n        size += sum([get_size(k, seen) for k in obj.keys()])\n    elif hasattr(obj, ""__dict__""):\n        size += get_size(obj.__dict__, seen)\n    elif isinstance(obj, (list, set, tuple)):\n        size += sum([get_size(i, seen) for i in obj])\n\n    return size\n\n\nclass SizedDict(collections.abc.MutableMapping):\n    def __init__(self, shared: bool = False, data: dict = None):\n        if data is None:\n            data = {}\n\n        if shared:\n            # NOTE(kamo): Don\'t set manager as a field because Manager, which includes\n            # weakref object, causes following error with method=""spawn"",\n            # ""TypeError: can\'t pickle weakref objects""\n            self.cache = multiprocessing.Manager().dict(**data)\n        else:\n            self.manager = None\n            self.cache = dict(**data)\n        self.size = 0\n\n    def __setitem__(self, key, value):\n        if key in self.cache:\n            self.size -= get_size(self.cache[key])\n        else:\n            self.size += sys.getsizeof(key)\n        self.size += get_size(value)\n        self.cache[key] = value\n\n    def __getitem__(self, key):\n        return self.cache[key]\n\n    def __delitem__(self, key):\n        self.size -= get_size(self.cache[key])\n        self.size -= sys.getsizeof(key)\n        del self.cache[key]\n\n    def __iter__(self):\n        return iter(self.cache)\n\n    def __contains__(self, key):\n        return key in self.cache\n\n    def __len__(self):\n        return len(self.cache)\n'"
espnet2/utils/types.py,0,"b'from distutils.util import strtobool\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport humanfriendly\n\n\ndef str2bool(value: str) -> bool:\n    return bool(strtobool(value))\n\n\ndef remove_parenthesis(value: str):\n    value = value.strip()\n    if value.startswith(""("") and value.endswith("")""):\n        value = value[1:-1]\n    elif value.startswith(""["") and value.endswith(""]""):\n        value = value[1:-1]\n    return value\n\n\ndef remove_quotes(value: str):\n    value = value.strip()\n    if value.startswith(\'""\') and value.endswith(\'""\'):\n        value = value[1:-1]\n    elif value.startswith(""\'"") and value.endswith(""\'""):\n        value = value[1:-1]\n    return value\n\n\ndef int_or_none(value: str) -> Optional[int]:\n    """"""int_or_none.\n\n    Examples:\n        >>> import argparse\n        >>> parser = argparse.ArgumentParser()\n        >>> _ = parser.add_argument(\'--foo\', type=int_or_none)\n        >>> parser.parse_args([\'--foo\', \'456\'])\n        Namespace(foo=456)\n        >>> parser.parse_args([\'--foo\', \'none\'])\n        Namespace(foo=None)\n        >>> parser.parse_args([\'--foo\', \'null\'])\n        Namespace(foo=None)\n        >>> parser.parse_args([\'--foo\', \'nil\'])\n        Namespace(foo=None)\n\n    """"""\n    if value.strip().lower() in (""none"", ""null"", ""nil""):\n        return None\n    return int(value)\n\n\ndef float_or_none(value: str) -> Optional[float]:\n    """"""float_or_none.\n\n    Examples:\n        >>> import argparse\n        >>> parser = argparse.ArgumentParser()\n        >>> _ = parser.add_argument(\'--foo\', type=float_or_none)\n        >>> parser.parse_args([\'--foo\', \'4.5\'])\n        Namespace(foo=4.5)\n        >>> parser.parse_args([\'--foo\', \'none\'])\n        Namespace(foo=None)\n        >>> parser.parse_args([\'--foo\', \'null\'])\n        Namespace(foo=None)\n        >>> parser.parse_args([\'--foo\', \'nil\'])\n        Namespace(foo=None)\n\n    """"""\n    if value.strip().lower() in (""none"", ""null"", ""nil""):\n        return None\n    return float(value)\n\n\ndef humanfriendly_parse_size_or_none(value) -> Optional[float]:\n    if value.strip().lower() in (""none"", ""null"", ""nil""):\n        return None\n    return humanfriendly.parse_size(value)\n\n\ndef str_or_int(value: str) -> Union[str, int]:\n    try:\n        return int(value)\n    except ValueError:\n        return value\n\n\ndef str_or_none(value: str) -> Optional[str]:\n    """"""str_or_none.\n\n    Examples:\n        >>> import argparse\n        >>> parser = argparse.ArgumentParser()\n        >>> _ = parser.add_argument(\'--foo\', type=str_or_none)\n        >>> parser.parse_args([\'--foo\', \'aaa\'])\n        Namespace(foo=\'aaa\')\n        >>> parser.parse_args([\'--foo\', \'none\'])\n        Namespace(foo=None)\n        >>> parser.parse_args([\'--foo\', \'null\'])\n        Namespace(foo=None)\n        >>> parser.parse_args([\'--foo\', \'nil\'])\n        Namespace(foo=None)\n\n    """"""\n    if value.strip().lower() in (""none"", ""null"", ""nil""):\n        return None\n    return value\n\n\ndef str2pair_str(value: str) -> Tuple[str, str]:\n    """"""str2pair_str.\n\n    Examples:\n        >>> import argparse\n        >>> str2pair_str(\'abc,def \')\n        (\'abc\', \'def\')\n        >>> parser = argparse.ArgumentParser()\n        >>> _ = parser.add_argument(\'--foo\', type=str2pair_str)\n        >>> parser.parse_args([\'--foo\', \'abc,def\'])\n        Namespace(foo=(\'abc\', \'def\'))\n\n    """"""\n    value = remove_parenthesis(value)\n    a, b = value.split("","")\n\n    # Workaround for configargparse issues:\n    # If the list values are given from yaml file,\n    # the value givent to type() is shaped as python-list,\n    # e.g. [\'a\', \'b\', \'c\'],\n    # so we need to remove double quotes from it.\n    return remove_quotes(a), remove_quotes(b)\n\n\ndef str2triple_str(value: str) -> Tuple[str, str, str]:\n    """"""str2triple_str.\n\n    Examples:\n        >>> str2triple_str(\'abc,def ,ghi\')\n        (\'abc\', \'def\', \'ghi\')\n    """"""\n    value = remove_parenthesis(value)\n    a, b, c = value.split("","")\n\n    # Workaround for configargparse issues:\n    # If the list values are given from yaml file,\n    # the value givent to type() is shaped as python-list,\n    # e.g. [\'a\', \'b\', \'c\'],\n    # so we need to remove quotes from it.\n    return remove_quotes(a), remove_quotes(b), remove_quotes(c)\n'"
espnet2/utils/yaml_no_alias_safe_dump.py,0,"b'import yaml\n\n\nclass NoAliasSafeDumper(yaml.SafeDumper):\n    # Disable anchor/alias in yaml because looks ugly\n    def ignore_aliases(self, data):\n        return True\n\n\ndef yaml_no_alias_safe_dump(data, stream=None, **kwargs):\n    """"""Safe-dump in yaml with no anchor/alias""""""\n    return yaml.dump(data, stream, Dumper=NoAliasSafeDumper, **kwargs)\n'"
test/espnet2/__init__.py,0,b''
espnet/asr/chainer_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/asr/chainer_backend/asr.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport json\nimport logging\nimport os\nimport six\n\n# chainer related\nimport chainer\n\nfrom chainer import training\n\nfrom chainer.datasets import TransformDataset\nfrom chainer.training import extensions\n\n# espnet related\nfrom espnet.asr.asr_utils import adadelta_eps_decay\nfrom espnet.asr.asr_utils import add_results_to_json\nfrom espnet.asr.asr_utils import chainer_load\nfrom espnet.asr.asr_utils import CompareValueTrigger\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import restore_snapshot\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.utils.deterministic_utils import set_deterministic_chainer\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.training.batchfy import make_batchset\nfrom espnet.utils.training.evaluator import BaseEvaluator\nfrom espnet.utils.training.iterators import ShufflingEnabler\nfrom espnet.utils.training.iterators import ToggleableShufflingMultiprocessIterator\nfrom espnet.utils.training.iterators import ToggleableShufflingSerialIterator\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\n# rnnlm\nimport espnet.lm.chainer_backend.extlm as extlm_chainer\nimport espnet.lm.chainer_backend.lm as lm_chainer\n\n# numpy related\nimport matplotlib\n\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom tensorboardX import SummaryWriter\n\nmatplotlib.use(""Agg"")\n\n\ndef train(args):\n    """"""Train with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    # display chainer version\n    logging.info(""chainer version = "" + chainer.__version__)\n\n    set_deterministic_chainer(args)\n\n    # check cuda and cudnn availability\n    if not chainer.cuda.available:\n        logging.warning(""cuda is not available"")\n    if not chainer.cuda.cudnn_enabled:\n        logging.warning(""cudnn is not available"")\n\n    # get input and output dimension info\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n    utts = list(valid_json.keys())\n    idim = int(valid_json[utts[0]][""input""][0][""shape""][1])\n    odim = int(valid_json[utts[0]][""output""][0][""shape""][1])\n    logging.info(""#input dims : "" + str(idim))\n    logging.info(""#output dims: "" + str(odim))\n\n    # specify attention, CTC, hybrid mode\n    if args.mtlalpha == 1.0:\n        mtl_mode = ""ctc""\n        logging.info(""Pure CTC mode"")\n    elif args.mtlalpha == 0.0:\n        mtl_mode = ""att""\n        logging.info(""Pure attention mode"")\n    else:\n        mtl_mode = ""mtl""\n        logging.info(""Multitask learning mode"")\n\n    # specify model architecture\n    logging.info(""import model module: "" + args.model_module)\n    model_class = dynamic_import(args.model_module)\n    model = model_class(idim, odim, args, flag_return=False)\n    assert isinstance(model, ASRInterface)\n\n    # write model config\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to "" + model_conf)\n        f.write(\n            json.dumps(\n                (idim, odim, vars(args)), indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n    for key in sorted(vars(args).keys()):\n        logging.info(""ARGS: "" + key + "": "" + str(vars(args)[key]))\n\n    # Set gpu\n    ngpu = args.ngpu\n    if ngpu == 1:\n        gpu_id = 0\n        # Make a specified GPU current\n        chainer.cuda.get_device_from_id(gpu_id).use()\n        model.to_gpu()  # Copy the model to the GPU\n        logging.info(""single gpu calculation."")\n    elif ngpu > 1:\n        gpu_id = 0\n        devices = {""main"": gpu_id}\n        for gid in six.moves.xrange(1, ngpu):\n            devices[""sub_%d"" % gid] = gid\n        logging.info(""multi gpu calculation (#gpus = %d)."" % ngpu)\n        logging.warning(\n            ""batch size is automatically increased (%d -> %d)""\n            % (args.batch_size, args.batch_size * args.ngpu)\n        )\n    else:\n        gpu_id = -1\n        logging.info(""cpu calculation"")\n\n    # Setup an optimizer\n    if args.opt == ""adadelta"":\n        optimizer = chainer.optimizers.AdaDelta(eps=args.eps)\n    elif args.opt == ""adam"":\n        optimizer = chainer.optimizers.Adam()\n    elif args.opt == ""noam"":\n        optimizer = chainer.optimizers.Adam(alpha=0, beta1=0.9, beta2=0.98, eps=1e-9)\n    else:\n        raise NotImplementedError(""args.opt={}"".format(args.opt))\n\n    optimizer.setup(model)\n    optimizer.add_hook(chainer.optimizer.GradientClipping(args.grad_clip))\n\n    # Setup a converter\n    converter = model.custom_converter(subsampling_factor=model.subsample[0])\n\n    # read json data\n    with open(args.train_json, ""rb"") as f:\n        train_json = json.load(f)[""utts""]\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n\n    # set up training iterator and updater\n    load_tr = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": True},  # Switch the mode of preprocessing\n    )\n    load_cv = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": False},  # Switch the mode of preprocessing\n    )\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n    accum_grad = args.accum_grad\n    if ngpu <= 1:\n        # make minibatch list (variable length)\n        train = make_batchset(\n            train_json,\n            args.batch_size,\n            args.maxlen_in,\n            args.maxlen_out,\n            args.minibatches,\n            min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n            shortest_first=use_sortagrad,\n            count=args.batch_count,\n            batch_bins=args.batch_bins,\n            batch_frames_in=args.batch_frames_in,\n            batch_frames_out=args.batch_frames_out,\n            batch_frames_inout=args.batch_frames_inout,\n            iaxis=0,\n            oaxis=0,\n        )\n        # hack to make batchsize argument as 1\n        # actual batchsize is included in a list\n        if args.n_iter_processes > 0:\n            train_iters = [\n                ToggleableShufflingMultiprocessIterator(\n                    TransformDataset(train, load_tr),\n                    batch_size=1,\n                    n_processes=args.n_iter_processes,\n                    n_prefetch=8,\n                    maxtasksperchild=20,\n                    shuffle=not use_sortagrad,\n                )\n            ]\n        else:\n            train_iters = [\n                ToggleableShufflingSerialIterator(\n                    TransformDataset(train, load_tr),\n                    batch_size=1,\n                    shuffle=not use_sortagrad,\n                )\n            ]\n\n        # set up updater\n        updater = model.custom_updater(\n            train_iters[0],\n            optimizer,\n            converter=converter,\n            device=gpu_id,\n            accum_grad=accum_grad,\n        )\n    else:\n        if args.batch_count not in (""auto"", ""seq"") and args.batch_size == 0:\n            raise NotImplementedError(\n                ""--batch-count \'bin\' and \'frame\' are not implemented ""\n                ""in chainer multi gpu""\n            )\n        # set up minibatches\n        train_subsets = []\n        for gid in six.moves.xrange(ngpu):\n            # make subset\n            train_json_subset = {\n                k: v for i, (k, v) in enumerate(train_json.items()) if i % ngpu == gid\n            }\n            # make minibatch list (variable length)\n            train_subsets += [\n                make_batchset(\n                    train_json_subset,\n                    args.batch_size,\n                    args.maxlen_in,\n                    args.maxlen_out,\n                    args.minibatches,\n                )\n            ]\n\n        # each subset must have same length for MultiprocessParallelUpdater\n        maxlen = max([len(train_subset) for train_subset in train_subsets])\n        for train_subset in train_subsets:\n            if maxlen != len(train_subset):\n                for i in six.moves.xrange(maxlen - len(train_subset)):\n                    train_subset += [train_subset[i]]\n\n        # hack to make batchsize argument as 1\n        # actual batchsize is included in a list\n        if args.n_iter_processes > 0:\n            train_iters = [\n                ToggleableShufflingMultiprocessIterator(\n                    TransformDataset(train_subsets[gid], load_tr),\n                    batch_size=1,\n                    n_processes=args.n_iter_processes,\n                    n_prefetch=8,\n                    maxtasksperchild=20,\n                    shuffle=not use_sortagrad,\n                )\n                for gid in six.moves.xrange(ngpu)\n            ]\n        else:\n            train_iters = [\n                ToggleableShufflingSerialIterator(\n                    TransformDataset(train_subsets[gid], load_tr),\n                    batch_size=1,\n                    shuffle=not use_sortagrad,\n                )\n                for gid in six.moves.xrange(ngpu)\n            ]\n\n        # set up updater\n        updater = model.custom_parallel_updater(\n            train_iters, optimizer, converter=converter, devices=devices\n        )\n\n    # Set up a trainer\n    trainer = training.Trainer(updater, (args.epochs, ""epoch""), out=args.outdir)\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler(train_iters),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, ""epoch""),\n        )\n    if args.opt == ""noam"":\n        from espnet.nets.chainer_backend.transformer.training import VaswaniRule\n\n        trainer.extend(\n            VaswaniRule(\n                ""alpha"",\n                d=args.adim,\n                warmup_steps=args.transformer_warmup_steps,\n                scale=args.transformer_lr,\n            ),\n            trigger=(1, ""iteration""),\n        )\n    # Resume from a snapshot\n    if args.resume:\n        chainer.serializers.load_npz(args.resume, trainer)\n\n    # set up validation iterator\n    valid = make_batchset(\n        valid_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        iaxis=0,\n        oaxis=0,\n    )\n\n    if args.n_iter_processes > 0:\n        valid_iter = chainer.iterators.MultiprocessIterator(\n            TransformDataset(valid, load_cv),\n            batch_size=1,\n            repeat=False,\n            shuffle=False,\n            n_processes=args.n_iter_processes,\n            n_prefetch=8,\n            maxtasksperchild=20,\n        )\n    else:\n        valid_iter = chainer.iterators.SerialIterator(\n            TransformDataset(valid, load_cv), batch_size=1, repeat=False, shuffle=False\n        )\n\n    # Evaluate the model with the test dataset for each epoch\n    trainer.extend(BaseEvaluator(valid_iter, model, converter=converter, device=gpu_id))\n\n    # Save attention weight each epoch\n    if args.num_save_attention > 0 and args.mtlalpha != 1.0:\n        data = sorted(\n            list(valid_json.items())[: args.num_save_attention],\n            key=lambda x: int(x[1][""input""][0][""shape""][1]),\n            reverse=True,\n        )\n        if hasattr(model, ""module""):\n            att_vis_fn = model.module.calculate_all_attentions\n            plot_class = model.module.attention_plot_class\n        else:\n            att_vis_fn = model.calculate_all_attentions\n            plot_class = model.attention_plot_class\n        logging.info(""Using custom PlotAttentionReport"")\n        att_reporter = plot_class(\n            att_vis_fn,\n            data,\n            args.outdir + ""/att_ws"",\n            converter=converter,\n            transform=load_cv,\n            device=gpu_id,\n        )\n        trainer.extend(att_reporter, trigger=(1, ""epoch""))\n    else:\n        att_reporter = None\n\n    # Take a snapshot for each specified epoch\n    trainer.extend(\n        extensions.snapshot(filename=""snapshot.ep.{.updater.epoch}""),\n        trigger=(1, ""epoch""),\n    )\n\n    # Make a plot for training and validation values\n    trainer.extend(\n        extensions.PlotReport(\n            [\n                ""main/loss"",\n                ""validation/main/loss"",\n                ""main/loss_ctc"",\n                ""validation/main/loss_ctc"",\n                ""main/loss_att"",\n                ""validation/main/loss_att"",\n            ],\n            ""epoch"",\n            file_name=""loss.png"",\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/acc"", ""validation/main/acc""], ""epoch"", file_name=""acc.png""\n        )\n    )\n\n    # Save best models\n    trainer.extend(\n        extensions.snapshot_object(model, ""model.loss.best""),\n        trigger=training.triggers.MinValueTrigger(""validation/main/loss""),\n    )\n    if mtl_mode != ""ctc"":\n        trainer.extend(\n            extensions.snapshot_object(model, ""model.acc.best""),\n            trigger=training.triggers.MaxValueTrigger(""validation/main/acc""),\n        )\n\n    # epsilon decay in the optimizer\n    if args.opt == ""adadelta"":\n        if args.criterion == ""acc"" and mtl_mode != ""ctc"":\n            trainer.extend(\n                restore_snapshot(model, args.outdir + ""/model.acc.best""),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n        elif args.criterion == ""loss"":\n            trainer.extend(\n                restore_snapshot(model, args.outdir + ""/model.loss.best""),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n\n    # Write a log of evaluation statistics for each epoch\n    trainer.extend(\n        extensions.LogReport(trigger=(args.report_interval_iters, ""iteration""))\n    )\n    report_keys = [\n        ""epoch"",\n        ""iteration"",\n        ""main/loss"",\n        ""main/loss_ctc"",\n        ""main/loss_att"",\n        ""validation/main/loss"",\n        ""validation/main/loss_ctc"",\n        ""validation/main/loss_att"",\n        ""main/acc"",\n        ""validation/main/acc"",\n        ""elapsed_time"",\n    ]\n    if args.opt == ""adadelta"":\n        trainer.extend(\n            extensions.observe_value(\n                ""eps"", lambda trainer: trainer.updater.get_optimizer(""main"").eps\n            ),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n        report_keys.append(""eps"")\n    trainer.extend(\n        extensions.PrintReport(report_keys),\n        trigger=(args.report_interval_iters, ""iteration""),\n    )\n\n    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))\n\n    set_early_stop(trainer, args)\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        writer = SummaryWriter(args.tensorboard_dir)\n        trainer.extend(\n            TensorboardLogger(writer, att_reporter),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n\n    # Run the training\n    trainer.run()\n    check_early_stop(trainer, args.epochs)\n\n\ndef recog(args):\n    """"""Decode with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    # display chainer version\n    logging.info(""chainer version = "" + chainer.__version__)\n\n    set_deterministic_chainer(args)\n\n    # read training config\n    idim, odim, train_args = get_model_conf(args.model, args.model_conf)\n\n    for key in sorted(vars(args).keys()):\n        logging.info(""ARGS: "" + key + "": "" + str(vars(args)[key]))\n\n    # specify model architecture\n    logging.info(""reading model parameters from "" + args.model)\n    # To be compatible with v.0.3.0 models\n    if hasattr(train_args, ""model_module""):\n        model_module = train_args.model_module\n    else:\n        model_module = ""espnet.nets.chainer_backend.e2e_asr:E2E""\n    model_class = dynamic_import(model_module)\n    model = model_class(idim, odim, train_args)\n    assert isinstance(model, ASRInterface)\n    chainer_load(args.model, model)\n\n    # read rnnlm\n    if args.rnnlm:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        rnnlm = lm_chainer.ClassifierWithState(\n            lm_chainer.RNNLM(\n                len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit\n            )\n        )\n        chainer_load(args.rnnlm, rnnlm)\n    else:\n        rnnlm = None\n\n    if args.word_rnnlm:\n        rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)\n        word_dict = rnnlm_args.char_list_dict\n        char_dict = {x: i for i, x in enumerate(train_args.char_list)}\n        word_rnnlm = lm_chainer.ClassifierWithState(\n            lm_chainer.RNNLM(len(word_dict), rnnlm_args.layer, rnnlm_args.unit)\n        )\n        chainer_load(args.word_rnnlm, word_rnnlm)\n\n        if rnnlm is not None:\n            rnnlm = lm_chainer.ClassifierWithState(\n                extlm_chainer.MultiLevelLM(\n                    word_rnnlm.predictor, rnnlm.predictor, word_dict, char_dict\n                )\n            )\n        else:\n            rnnlm = lm_chainer.ClassifierWithState(\n                extlm_chainer.LookAheadWordLM(\n                    word_rnnlm.predictor, word_dict, char_dict\n                )\n            )\n\n    # read json data\n    with open(args.recog_json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=False,\n        sort_in_input_length=False,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={""train"": False},  # Switch the mode of preprocessing\n    )\n\n    # decode each utterance\n    new_js = {}\n    with chainer.no_backprop_mode():\n        for idx, name in enumerate(js.keys(), 1):\n            logging.info(""(%d/%d) decoding "" + name, idx, len(js.keys()))\n            batch = [(name, js[name])]\n            feat = load_inputs_and_targets(batch)[0][0]\n            nbest_hyps = model.recognize(feat, args, train_args.char_list, rnnlm)\n            new_js[name] = add_results_to_json(\n                js[name], nbest_hyps, train_args.char_list\n            )\n\n    with open(args.result_label, ""wb"") as f:\n        f.write(\n            json.dumps(\n                {""utts"": new_js}, indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n'"
espnet/asr/pytorch_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/asr/pytorch_backend/asr.py,51,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Training/decoding definition for the speech recognition task.""""""\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport sys\n\nfrom chainer import reporter as reporter_module\nfrom chainer import training\nfrom chainer.training import extensions\nfrom chainer.training.updater import StandardUpdater\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nimport torch\nfrom torch.nn.parallel import data_parallel\n\nfrom espnet.asr.asr_utils import adadelta_eps_decay\nfrom espnet.asr.asr_utils import add_results_to_json\nfrom espnet.asr.asr_utils import CompareValueTrigger\nfrom espnet.asr.asr_utils import format_mulenc_args\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import plot_spectrogram\nfrom espnet.asr.asr_utils import restore_snapshot\nfrom espnet.asr.asr_utils import snapshot_object\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.asr.asr_utils import torch_resume\nfrom espnet.asr.asr_utils import torch_snapshot\nfrom espnet.asr.pytorch_backend.asr_init import load_trained_model\nfrom espnet.asr.pytorch_backend.asr_init import load_trained_modules\nimport espnet.lm.pytorch_backend.extlm as extlm_pytorch\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.pytorch_backend.e2e_asr import pad_list\nimport espnet.nets.pytorch_backend.lm.default as lm_pytorch\nfrom espnet.nets.pytorch_backend.streaming.segment import SegmentStreamingE2E\nfrom espnet.nets.pytorch_backend.streaming.window import WindowStreamingE2E\nfrom espnet.transform.spectrogram import IStft\nfrom espnet.transform.transformation import Transformation\nfrom espnet.utils.cli_writers import file_writer_helper\nfrom espnet.utils.dataset import ChainerDataLoader\nfrom espnet.utils.dataset import TransformDataset\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.training.batchfy import make_batchset\nfrom espnet.utils.training.evaluator import BaseEvaluator\nfrom espnet.utils.training.iterators import ShufflingEnabler\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\nimport matplotlib\n\nmatplotlib.use(""Agg"")\n\nif sys.version_info[0] == 2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest as zip_longest\n\n\ndef _recursive_to(xs, device):\n    if torch.is_tensor(xs):\n        return xs.to(device)\n    if isinstance(xs, tuple):\n        return tuple(_recursive_to(x, device) for x in xs)\n    return xs\n\n\nclass CustomEvaluator(BaseEvaluator):\n    """"""Custom Evaluator for Pytorch.\n\n    Args:\n        model (torch.nn.Module): The model to evaluate.\n        iterator (chainer.dataset.Iterator) : The train iterator.\n\n        target (link | dict[str, link]) :Link object or a dictionary of\n            links to evaluate. If this is just a link object, the link is\n            registered by the name ``\'main\'``.\n\n        device (torch.device): The device used.\n        ngpu (int): The number of GPUs.\n\n    """"""\n\n    def __init__(self, model, iterator, target, device, ngpu=None):\n        super(CustomEvaluator, self).__init__(iterator, target)\n        self.model = model\n        self.device = device\n        if ngpu is not None:\n            self.ngpu = ngpu\n        elif device.type == ""cpu"":\n            self.ngpu = 0\n        else:\n            self.ngpu = 1\n\n    # The core part of the update routine can be customized by overriding\n    def evaluate(self):\n        """"""Main evaluate routine for CustomEvaluator.""""""\n        iterator = self._iterators[""main""]\n\n        if self.eval_hook:\n            self.eval_hook(self)\n\n        if hasattr(iterator, ""reset""):\n            iterator.reset()\n            it = iterator\n        else:\n            it = copy.copy(iterator)\n\n        summary = reporter_module.DictSummary()\n\n        self.model.eval()\n        with torch.no_grad():\n            for batch in it:\n                x = _recursive_to(batch, self.device)\n                observation = {}\n                with reporter_module.report_scope(observation):\n                    # read scp files\n                    # x: original json with loaded features\n                    #    will be converted to chainer variable later\n                    if self.ngpu == 0:\n                        self.model(*x)\n                    else:\n                        # apex does not support torch.nn.DataParallel\n                        data_parallel(self.model, x, range(self.ngpu))\n\n                summary.add(observation)\n        self.model.train()\n\n        return summary.compute_mean()\n\n\nclass CustomUpdater(StandardUpdater):\n    """"""Custom Updater for Pytorch.\n\n    Args:\n        model (torch.nn.Module): The model to update.\n        grad_clip_threshold (float): The gradient clipping value to use.\n        train_iter (chainer.dataset.Iterator): The training iterator.\n        optimizer (torch.optim.optimizer): The training optimizer.\n\n        device (torch.device): The device to use.\n        ngpu (int): The number of gpus to use.\n        use_apex (bool): The flag to use Apex in backprop.\n\n    """"""\n\n    def __init__(\n        self,\n        model,\n        grad_clip_threshold,\n        train_iter,\n        optimizer,\n        device,\n        ngpu,\n        grad_noise=False,\n        accum_grad=1,\n        use_apex=False,\n    ):\n        super(CustomUpdater, self).__init__(train_iter, optimizer)\n        self.model = model\n        self.grad_clip_threshold = grad_clip_threshold\n        self.device = device\n        self.ngpu = ngpu\n        self.accum_grad = accum_grad\n        self.forward_count = 0\n        self.grad_noise = grad_noise\n        self.iteration = 0\n        self.use_apex = use_apex\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        """"""Main update routine of the CustomUpdater.""""""\n        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n        # they are automatically named \'main\'.\n        train_iter = self.get_iterator(""main"")\n        optimizer = self.get_optimizer(""main"")\n        epoch = train_iter.epoch\n\n        # Get the next batch (a list of json files)\n        batch = train_iter.next()\n        # self.iteration += 1 # Increase may result in early report,\n        # which is done in other place automatically.\n        x = _recursive_to(batch, self.device)\n        is_new_epoch = train_iter.epoch != epoch\n        # When the last minibatch in the current epoch is given,\n        # gradient accumulation is turned off in order to evaluate the model\n        # on the validation set in every epoch.\n        # see details in https://github.com/espnet/espnet/pull/1388\n\n        # Compute the loss at this time step and accumulate it\n        if self.ngpu == 0:\n            loss = self.model(*x).mean() / self.accum_grad\n        else:\n            # apex does not support torch.nn.DataParallel\n            loss = (\n                data_parallel(self.model, x, range(self.ngpu)).mean() / self.accum_grad\n            )\n        if self.use_apex:\n            from apex import amp\n\n            # NOTE: for a compatibility with noam optimizer\n            opt = optimizer.optimizer if hasattr(optimizer, ""optimizer"") else optimizer\n            with amp.scale_loss(loss, opt) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n        # gradient noise injection\n        if self.grad_noise:\n            from espnet.asr.asr_utils import add_gradient_noise\n\n            add_gradient_noise(\n                self.model, self.iteration, duration=100, eta=1.0, scale_factor=0.55\n            )\n\n        # update parameters\n        self.forward_count += 1\n        if not is_new_epoch and self.forward_count != self.accum_grad:\n            return\n        self.forward_count = 0\n        # compute the gradient norm to check if it is normal or not\n        grad_norm = torch.nn.utils.clip_grad_norm_(\n            self.model.parameters(), self.grad_clip_threshold\n        )\n        logging.info(""grad norm={}"".format(grad_norm))\n        if math.isnan(grad_norm):\n            logging.warning(""grad norm is nan. Do not update model."")\n        else:\n            optimizer.step()\n        optimizer.zero_grad()\n\n    def update(self):\n        self.update_core()\n        # #iterations with accum_grad > 1\n        # Ref.: https://github.com/espnet/espnet/issues/777\n        if self.forward_count == 0:\n            self.iteration += 1\n\n\nclass CustomConverter(object):\n    """"""Custom batch converter for Pytorch.\n\n    Args:\n        subsampling_factor (int): The subsampling factor.\n        dtype (torch.dtype): Data type to convert.\n\n    """"""\n\n    def __init__(self, subsampling_factor=1, dtype=torch.float32):\n        """"""Construct a CustomConverter object.""""""\n        self.subsampling_factor = subsampling_factor\n        self.ignore_id = -1\n        self.dtype = dtype\n\n    def __call__(self, batch, device=torch.device(""cpu"")):\n        """"""Transform a batch and send it to a device.\n\n        Args:\n            batch (list): The batch to transform.\n            device (torch.device): The device to send to.\n\n        Returns:\n            tuple(torch.Tensor, torch.Tensor, torch.Tensor)\n\n        """"""\n        # batch should be located in list\n        assert len(batch) == 1\n        xs, ys = batch[0]\n\n        # perform subsampling\n        if self.subsampling_factor > 1:\n            xs = [x[:: self.subsampling_factor, :] for x in xs]\n\n        # get batch of lengths of input sequences\n        ilens = np.array([x.shape[0] for x in xs])\n\n        # perform padding and convert to tensor\n        # currently only support real number\n        if xs[0].dtype.kind == ""c"":\n            xs_pad_real = pad_list(\n                [torch.from_numpy(x.real).float() for x in xs], 0\n            ).to(device, dtype=self.dtype)\n            xs_pad_imag = pad_list(\n                [torch.from_numpy(x.imag).float() for x in xs], 0\n            ).to(device, dtype=self.dtype)\n            # Note(kamo):\n            # {\'real\': ..., \'imag\': ...} will be changed to ComplexTensor in E2E.\n            # Don\'t create ComplexTensor and give it E2E here\n            # because torch.nn.DataParellel can\'t handle it.\n            xs_pad = {""real"": xs_pad_real, ""imag"": xs_pad_imag}\n        else:\n            xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(\n                device, dtype=self.dtype\n            )\n\n        ilens = torch.from_numpy(ilens).to(device)\n        # NOTE: this is for multi-output (e.g., speech translation)\n        ys_pad = pad_list(\n            [\n                torch.from_numpy(\n                    np.array(y[0][:]) if isinstance(y, tuple) else y\n                ).long()\n                for y in ys\n            ],\n            self.ignore_id,\n        ).to(device)\n\n        return xs_pad, ilens, ys_pad\n\n\nclass CustomConverterMulEnc(object):\n    """"""Custom batch converter for Pytorch in multi-encoder case.\n\n    Args:\n        subsampling_factors (list): List of subsampling factors for each encoder.\n        dtype (torch.dtype): Data type to convert.\n\n    """"""\n\n    def __init__(self, subsamping_factors=[1, 1], dtype=torch.float32):\n        """"""Initialize the converter.""""""\n        self.subsamping_factors = subsamping_factors\n        self.ignore_id = -1\n        self.dtype = dtype\n        self.num_encs = len(subsamping_factors)\n\n    def __call__(self, batch, device=torch.device(""cpu"")):\n        """"""Transform a batch and send it to a device.\n\n        Args:\n            batch (list): The batch to transform.\n            device (torch.device): The device to send to.\n\n        Returns:\n            tuple( list(torch.Tensor), list(torch.Tensor), torch.Tensor)\n\n        """"""\n        # batch should be located in list\n        assert len(batch) == 1\n        xs_list = batch[0][: self.num_encs]\n        ys = batch[0][-1]\n\n        # perform subsampling\n        if np.sum(self.subsamping_factors) > self.num_encs:\n            xs_list = [\n                [x[:: self.subsampling_factors[i], :] for x in xs_list[i]]\n                for i in range(self.num_encs)\n            ]\n\n        # get batch of lengths of input sequences\n        ilens_list = [\n            np.array([x.shape[0] for x in xs_list[i]]) for i in range(self.num_encs)\n        ]\n\n        # perform padding and convert to tensor\n        # currently only support real number\n        xs_list_pad = [\n            pad_list([torch.from_numpy(x).float() for x in xs_list[i]], 0).to(\n                device, dtype=self.dtype\n            )\n            for i in range(self.num_encs)\n        ]\n\n        ilens_list = [\n            torch.from_numpy(ilens_list[i]).to(device) for i in range(self.num_encs)\n        ]\n        # NOTE: this is for multi-task learning (e.g., speech translation)\n        ys_pad = pad_list(\n            [\n                torch.from_numpy(np.array(y[0]) if isinstance(y, tuple) else y).long()\n                for y in ys\n            ],\n            self.ignore_id,\n        ).to(device)\n\n        return xs_list_pad, ilens_list, ys_pad\n\n\ndef train(args):\n    """"""Train with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n    if args.num_encs > 1:\n        args = format_mulenc_args(args)\n\n    # check cuda availability\n    if not torch.cuda.is_available():\n        logging.warning(""cuda is not available"")\n\n    # get input and output dimension info\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n    utts = list(valid_json.keys())\n    idim_list = [\n        int(valid_json[utts[0]][""input""][i][""shape""][-1]) for i in range(args.num_encs)\n    ]\n    odim = int(valid_json[utts[0]][""output""][0][""shape""][-1])\n    for i in range(args.num_encs):\n        logging.info(""stream{}: input dims : {}"".format(i + 1, idim_list[i]))\n    logging.info(""#output dims: "" + str(odim))\n\n    # specify attention, CTC, hybrid mode\n    if args.mtlalpha == 1.0:\n        mtl_mode = ""ctc""\n        logging.info(""Pure CTC mode"")\n    elif args.mtlalpha == 0.0:\n        mtl_mode = ""att""\n        logging.info(""Pure attention mode"")\n    else:\n        mtl_mode = ""mtl""\n        logging.info(""Multitask learning mode"")\n\n    if (args.enc_init is not None or args.dec_init is not None) and args.num_encs == 1:\n        model = load_trained_modules(idim_list[0], odim, args)\n    else:\n        model_class = dynamic_import(args.model_module)\n        model = model_class(\n            idim_list[0] if args.num_encs == 1 else idim_list, odim, args\n        )\n    assert isinstance(model, ASRInterface)\n\n    if args.rnnlm is not None:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(len(args.char_list), rnnlm_args.layer, rnnlm_args.unit)\n        )\n        torch_load(args.rnnlm, rnnlm)\n        model.rnnlm = rnnlm\n\n    # write model config\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to "" + model_conf)\n        f.write(\n            json.dumps(\n                (idim_list[0] if args.num_encs == 1 else idim_list, odim, vars(args)),\n                indent=4,\n                ensure_ascii=False,\n                sort_keys=True,\n            ).encode(""utf_8"")\n        )\n    for key in sorted(vars(args).keys()):\n        logging.info(""ARGS: "" + key + "": "" + str(vars(args)[key]))\n\n    reporter = model.reporter\n\n    # check the use of multi-gpu\n    if args.ngpu > 1:\n        if args.batch_size != 0:\n            logging.warning(\n                ""batch size is automatically increased (%d -> %d)""\n                % (args.batch_size, args.batch_size * args.ngpu)\n            )\n            args.batch_size *= args.ngpu\n        if args.num_encs > 1:\n            # TODO(ruizhili): implement data parallel for multi-encoder setup.\n            raise NotImplementedError(\n                ""Data parallel is not supported for multi-encoder setup.""\n            )\n\n    # set torch device\n    device = torch.device(""cuda"" if args.ngpu > 0 else ""cpu"")\n    if args.train_dtype in (""float16"", ""float32"", ""float64""):\n        dtype = getattr(torch, args.train_dtype)\n    else:\n        dtype = torch.float32\n    model = model.to(device=device, dtype=dtype)\n\n    # Setup an optimizer\n    if args.opt == ""adadelta"":\n        optimizer = torch.optim.Adadelta(\n            model.parameters(), rho=0.95, eps=args.eps, weight_decay=args.weight_decay\n        )\n    elif args.opt == ""adam"":\n        optimizer = torch.optim.Adam(model.parameters(), weight_decay=args.weight_decay)\n    elif args.opt == ""noam"":\n        from espnet.nets.pytorch_backend.transformer.optimizer import get_std_opt\n\n        optimizer = get_std_opt(\n            model, args.adim, args.transformer_warmup_steps, args.transformer_lr\n        )\n    else:\n        raise NotImplementedError(""unknown optimizer: "" + args.opt)\n\n    # setup apex.amp\n    if args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3""):\n        try:\n            from apex import amp\n        except ImportError as e:\n            logging.error(\n                f""You need to install apex for --train-dtype {args.train_dtype}. ""\n                ""See https://github.com/NVIDIA/apex#linux""\n            )\n            raise e\n        if args.opt == ""noam"":\n            model, optimizer.optimizer = amp.initialize(\n                model, optimizer.optimizer, opt_level=args.train_dtype\n            )\n        else:\n            model, optimizer = amp.initialize(\n                model, optimizer, opt_level=args.train_dtype\n            )\n        use_apex = True\n\n        from espnet.nets.pytorch_backend.ctc import CTC\n\n        amp.register_float_function(CTC, ""loss_fn"")\n        amp.init()\n        logging.warning(""register ctc as float function"")\n    else:\n        use_apex = False\n\n    # FIXME: TOO DIRTY HACK\n    setattr(optimizer, ""target"", reporter)\n    setattr(optimizer, ""serialize"", lambda s: reporter.serialize(s))\n\n    # Setup a converter\n    if args.num_encs == 1:\n        converter = CustomConverter(subsampling_factor=model.subsample[0], dtype=dtype)\n    else:\n        converter = CustomConverterMulEnc(\n            [i[0] for i in model.subsample_list], dtype=dtype\n        )\n\n    # read json data\n    with open(args.train_json, ""rb"") as f:\n        train_json = json.load(f)[""utts""]\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n    # make minibatch list (variable length)\n    train = make_batchset(\n        train_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        shortest_first=use_sortagrad,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        iaxis=0,\n        oaxis=0,\n    )\n    valid = make_batchset(\n        valid_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        iaxis=0,\n        oaxis=0,\n    )\n\n    load_tr = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": True},  # Switch the mode of preprocessing\n    )\n    load_cv = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": False},  # Switch the mode of preprocessing\n    )\n    # hack to make batchsize argument as 1\n    # actual bathsize is included in a list\n    # default collate function converts numpy array to pytorch tensor\n    # we used an empty collate function instead which returns list\n    train_iter = ChainerDataLoader(\n        dataset=TransformDataset(train, lambda data: converter([load_tr(data)])),\n        batch_size=1,\n        num_workers=args.n_iter_processes,\n        shuffle=not use_sortagrad,\n        collate_fn=lambda x: x[0],\n    )\n    valid_iter = ChainerDataLoader(\n        dataset=TransformDataset(valid, lambda data: converter([load_cv(data)])),\n        batch_size=1,\n        shuffle=False,\n        collate_fn=lambda x: x[0],\n        num_workers=args.n_iter_processes,\n    )\n\n    # Set up a trainer\n    updater = CustomUpdater(\n        model,\n        args.grad_clip,\n        {""main"": train_iter},\n        optimizer,\n        device,\n        args.ngpu,\n        args.grad_noise,\n        args.accum_grad,\n        use_apex=use_apex,\n    )\n    trainer = training.Trainer(updater, (args.epochs, ""epoch""), out=args.outdir)\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler([train_iter]),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, ""epoch""),\n        )\n\n    # Resume from a snapshot\n    if args.resume:\n        logging.info(""resumed from %s"" % args.resume)\n        torch_resume(args.resume, trainer)\n\n    # Evaluate the model with the test dataset for each epoch\n    if args.save_interval_iters > 0:\n        trainer.extend(\n            CustomEvaluator(model, {""main"": valid_iter}, reporter, device, args.ngpu),\n            trigger=(args.save_interval_iters, ""iteration""),\n        )\n    else:\n        trainer.extend(\n            CustomEvaluator(model, {""main"": valid_iter}, reporter, device, args.ngpu)\n        )\n\n    # Save attention weight each epoch\n    if args.num_save_attention > 0 and args.mtlalpha != 1.0:\n        data = sorted(\n            list(valid_json.items())[: args.num_save_attention],\n            key=lambda x: int(x[1][""input""][0][""shape""][1]),\n            reverse=True,\n        )\n        if hasattr(model, ""module""):\n            att_vis_fn = model.module.calculate_all_attentions\n            plot_class = model.module.attention_plot_class\n        else:\n            att_vis_fn = model.calculate_all_attentions\n            plot_class = model.attention_plot_class\n        att_reporter = plot_class(\n            att_vis_fn,\n            data,\n            args.outdir + ""/att_ws"",\n            converter=converter,\n            transform=load_cv,\n            device=device,\n        )\n        trainer.extend(att_reporter, trigger=(1, ""epoch""))\n    else:\n        att_reporter = None\n\n    # Make a plot for training and validation values\n    if args.num_encs > 1:\n        report_keys_loss_ctc = [\n            ""main/loss_ctc{}"".format(i + 1) for i in range(model.num_encs)\n        ] + [""validation/main/loss_ctc{}"".format(i + 1) for i in range(model.num_encs)]\n        report_keys_cer_ctc = [\n            ""main/cer_ctc{}"".format(i + 1) for i in range(model.num_encs)\n        ] + [""validation/main/cer_ctc{}"".format(i + 1) for i in range(model.num_encs)]\n    trainer.extend(\n        extensions.PlotReport(\n            [\n                ""main/loss"",\n                ""validation/main/loss"",\n                ""main/loss_ctc"",\n                ""validation/main/loss_ctc"",\n                ""main/loss_att"",\n                ""validation/main/loss_att"",\n            ]\n            + ([] if args.num_encs == 1 else report_keys_loss_ctc),\n            ""epoch"",\n            file_name=""loss.png"",\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/acc"", ""validation/main/acc""], ""epoch"", file_name=""acc.png""\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/cer_ctc"", ""validation/main/cer_ctc""]\n            + ([] if args.num_encs == 1 else report_keys_loss_ctc),\n            ""epoch"",\n            file_name=""cer.png"",\n        )\n    )\n\n    # Save best models\n    trainer.extend(\n        snapshot_object(model, ""model.loss.best""),\n        trigger=training.triggers.MinValueTrigger(""validation/main/loss""),\n    )\n    if mtl_mode != ""ctc"":\n        trainer.extend(\n            snapshot_object(model, ""model.acc.best""),\n            trigger=training.triggers.MaxValueTrigger(""validation/main/acc""),\n        )\n\n    # save snapshot which contains model and optimizer states\n    if args.save_interval_iters > 0:\n        trainer.extend(\n            torch_snapshot(filename=""snapshot.iter.{.updater.iteration}""),\n            trigger=(args.save_interval_iters, ""iteration""),\n        )\n    else:\n        trainer.extend(torch_snapshot(), trigger=(1, ""epoch""))\n\n    # epsilon decay in the optimizer\n    if args.opt == ""adadelta"":\n        if args.criterion == ""acc"" and mtl_mode != ""ctc"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.acc.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n        elif args.criterion == ""loss"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.loss.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n\n    # Write a log of evaluation statistics for each epoch\n    trainer.extend(\n        extensions.LogReport(trigger=(args.report_interval_iters, ""iteration""))\n    )\n    report_keys = [\n        ""epoch"",\n        ""iteration"",\n        ""main/loss"",\n        ""main/loss_ctc"",\n        ""main/loss_att"",\n        ""validation/main/loss"",\n        ""validation/main/loss_ctc"",\n        ""validation/main/loss_att"",\n        ""main/acc"",\n        ""validation/main/acc"",\n        ""main/cer_ctc"",\n        ""validation/main/cer_ctc"",\n        ""elapsed_time"",\n    ] + ([] if args.num_encs == 1 else report_keys_cer_ctc + report_keys_loss_ctc)\n    if args.opt == ""adadelta"":\n        trainer.extend(\n            extensions.observe_value(\n                ""eps"",\n                lambda trainer: trainer.updater.get_optimizer(""main"").param_groups[0][\n                    ""eps""\n                ],\n            ),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n        report_keys.append(""eps"")\n    if args.report_cer:\n        report_keys.append(""validation/main/cer"")\n    if args.report_wer:\n        report_keys.append(""validation/main/wer"")\n    trainer.extend(\n        extensions.PrintReport(report_keys),\n        trigger=(args.report_interval_iters, ""iteration""),\n    )\n\n    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))\n    set_early_stop(trainer, args)\n\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        trainer.extend(\n            TensorboardLogger(SummaryWriter(args.tensorboard_dir), att_reporter),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n    # Run the training\n    trainer.run()\n    check_early_stop(trainer, args.epochs)\n\n\ndef recog(args):\n    """"""Decode with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n    model, train_args = load_trained_model(args.model)\n    assert isinstance(model, ASRInterface)\n    model.recog_args = args\n\n    if args.streaming_mode and ""transformer"" in train_args.model_module:\n        raise NotImplementedError(""streaming mode for transformer is not implemented"")\n\n    # read rnnlm\n    if args.rnnlm:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        if getattr(rnnlm_args, ""model_module"", ""default"") != ""default"":\n            raise ValueError(\n                ""use \'--api v2\' option to decode with non-default language model""\n            )\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(train_args.char_list),\n                rnnlm_args.layer,\n                rnnlm_args.unit,\n                getattr(rnnlm_args, ""embed_unit"", None),  # for backward compatibility\n            )\n        )\n        torch_load(args.rnnlm, rnnlm)\n        rnnlm.eval()\n    else:\n        rnnlm = None\n\n    if args.word_rnnlm:\n        rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)\n        word_dict = rnnlm_args.char_list_dict\n        char_dict = {x: i for i, x in enumerate(train_args.char_list)}\n        word_rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(word_dict),\n                rnnlm_args.layer,\n                rnnlm_args.unit,\n                getattr(rnnlm_args, ""embed_unit"", None),  # for backward compatibility\n            )\n        )\n        torch_load(args.word_rnnlm, word_rnnlm)\n        word_rnnlm.eval()\n\n        if rnnlm is not None:\n            rnnlm = lm_pytorch.ClassifierWithState(\n                extlm_pytorch.MultiLevelLM(\n                    word_rnnlm.predictor, rnnlm.predictor, word_dict, char_dict\n                )\n            )\n        else:\n            rnnlm = lm_pytorch.ClassifierWithState(\n                extlm_pytorch.LookAheadWordLM(\n                    word_rnnlm.predictor, word_dict, char_dict\n                )\n            )\n\n    # gpu\n    if args.ngpu == 1:\n        gpu_id = list(range(args.ngpu))\n        logging.info(""gpu id: "" + str(gpu_id))\n        model.cuda()\n        if rnnlm:\n            rnnlm.cuda()\n\n    # read json data\n    with open(args.recog_json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n    new_js = {}\n\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=False,\n        sort_in_input_length=False,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={""train"": False},\n    )\n\n    if args.batchsize == 0:\n        with torch.no_grad():\n            for idx, name in enumerate(js.keys(), 1):\n                logging.info(""(%d/%d) decoding "" + name, idx, len(js.keys()))\n                batch = [(name, js[name])]\n                feat = load_inputs_and_targets(batch)\n                feat = (\n                    feat[0][0]\n                    if args.num_encs == 1\n                    else [feat[idx][0] for idx in range(model.num_encs)]\n                )\n                if args.streaming_mode == ""window"" and args.num_encs == 1:\n                    logging.info(\n                        ""Using streaming recognizer with window size %d frames"",\n                        args.streaming_window,\n                    )\n                    se2e = WindowStreamingE2E(e2e=model, recog_args=args, rnnlm=rnnlm)\n                    for i in range(0, feat.shape[0], args.streaming_window):\n                        logging.info(\n                            ""Feeding frames %d - %d"", i, i + args.streaming_window\n                        )\n                        se2e.accept_input(feat[i : i + args.streaming_window])\n                    logging.info(""Running offline attention decoder"")\n                    se2e.decode_with_attention_offline()\n                    logging.info(""Offline attention decoder finished"")\n                    nbest_hyps = se2e.retrieve_recognition()\n                elif args.streaming_mode == ""segment"" and args.num_encs == 1:\n                    logging.info(\n                        ""Using streaming recognizer with threshold value %d"",\n                        args.streaming_min_blank_dur,\n                    )\n                    nbest_hyps = []\n                    for n in range(args.nbest):\n                        nbest_hyps.append({""yseq"": [], ""score"": 0.0})\n                    se2e = SegmentStreamingE2E(e2e=model, recog_args=args, rnnlm=rnnlm)\n                    r = np.prod(model.subsample)\n                    for i in range(0, feat.shape[0], r):\n                        hyps = se2e.accept_input(feat[i : i + r])\n                        if hyps is not None:\n                            text = """".join(\n                                [\n                                    train_args.char_list[int(x)]\n                                    for x in hyps[0][""yseq""][1:-1]\n                                    if int(x) != -1\n                                ]\n                            )\n                            text = text.replace(\n                                ""\\u2581"", "" ""\n                            ).strip()  # for SentencePiece\n                            text = text.replace(model.space, "" "")\n                            text = text.replace(model.blank, """")\n                            logging.info(text)\n                            for n in range(args.nbest):\n                                nbest_hyps[n][""yseq""].extend(hyps[n][""yseq""])\n                                nbest_hyps[n][""score""] += hyps[n][""score""]\n                else:\n                    nbest_hyps = model.recognize(\n                        feat, args, train_args.char_list, rnnlm\n                    )\n                new_js[name] = add_results_to_json(\n                    js[name], nbest_hyps, train_args.char_list\n                )\n\n    else:\n\n        def grouper(n, iterable, fillvalue=None):\n            kargs = [iter(iterable)] * n\n            return zip_longest(*kargs, fillvalue=fillvalue)\n\n        # sort data if batchsize > 1\n        keys = list(js.keys())\n        if args.batchsize > 1:\n            feat_lens = [js[key][""input""][0][""shape""][0] for key in keys]\n            sorted_index = sorted(range(len(feat_lens)), key=lambda i: -feat_lens[i])\n            keys = [keys[i] for i in sorted_index]\n\n        with torch.no_grad():\n            for names in grouper(args.batchsize, keys, None):\n                names = [name for name in names if name]\n                batch = [(name, js[name]) for name in names]\n                feats = (\n                    load_inputs_and_targets(batch)[0]\n                    if args.num_encs == 1\n                    else load_inputs_and_targets(batch)\n                )\n                if args.streaming_mode == ""window"" and args.num_encs == 1:\n                    raise NotImplementedError\n                elif args.streaming_mode == ""segment"" and args.num_encs == 1:\n                    if args.batchsize > 1:\n                        raise NotImplementedError\n                    feat = feats[0]\n                    nbest_hyps = []\n                    for n in range(args.nbest):\n                        nbest_hyps.append({""yseq"": [], ""score"": 0.0})\n                    se2e = SegmentStreamingE2E(e2e=model, recog_args=args, rnnlm=rnnlm)\n                    r = np.prod(model.subsample)\n                    for i in range(0, feat.shape[0], r):\n                        hyps = se2e.accept_input(feat[i : i + r])\n                        if hyps is not None:\n                            text = """".join(\n                                [\n                                    train_args.char_list[int(x)]\n                                    for x in hyps[0][""yseq""][1:-1]\n                                    if int(x) != -1\n                                ]\n                            )\n                            text = text.replace(\n                                ""\\u2581"", "" ""\n                            ).strip()  # for SentencePiece\n                            text = text.replace(model.space, "" "")\n                            text = text.replace(model.blank, """")\n                            logging.info(text)\n                            for n in range(args.nbest):\n                                nbest_hyps[n][""yseq""].extend(hyps[n][""yseq""])\n                                nbest_hyps[n][""score""] += hyps[n][""score""]\n                    nbest_hyps = [nbest_hyps]\n                else:\n                    nbest_hyps = model.recognize_batch(\n                        feats, args, train_args.char_list, rnnlm=rnnlm\n                    )\n\n                for i, nbest_hyp in enumerate(nbest_hyps):\n                    name = names[i]\n                    new_js[name] = add_results_to_json(\n                        js[name], nbest_hyp, train_args.char_list\n                    )\n\n    with open(args.result_label, ""wb"") as f:\n        f.write(\n            json.dumps(\n                {""utts"": new_js}, indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n\n\ndef enhance(args):\n    """"""Dumping enhanced speech and mask.\n\n    Args:\n        args (namespace): The program arguments.\n    """"""\n    set_deterministic_pytorch(args)\n    # read training config\n    idim, odim, train_args = get_model_conf(args.model, args.model_conf)\n\n    # TODO(ruizhili): implement enhance for multi-encoder model\n    assert args.num_encs == 1, ""number of encoder should be 1 ({} is given)"".format(\n        args.num_encs\n    )\n\n    # load trained model parameters\n    logging.info(""reading model parameters from "" + args.model)\n    model_class = dynamic_import(train_args.model_module)\n    model = model_class(idim, odim, train_args)\n    assert isinstance(model, ASRInterface)\n    torch_load(args.model, model)\n    model.recog_args = args\n\n    # gpu\n    if args.ngpu == 1:\n        gpu_id = list(range(args.ngpu))\n        logging.info(""gpu id: "" + str(gpu_id))\n        model.cuda()\n\n    # read json data\n    with open(args.recog_json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=False,\n        sort_in_input_length=False,\n        preprocess_conf=None,  # Apply pre_process in outer func\n    )\n    if args.batchsize == 0:\n        args.batchsize = 1\n\n    # Creates writers for outputs from the network\n    if args.enh_wspecifier is not None:\n        enh_writer = file_writer_helper(args.enh_wspecifier, filetype=args.enh_filetype)\n    else:\n        enh_writer = None\n\n    # Creates a Transformation instance\n    preprocess_conf = (\n        train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf\n    )\n    if preprocess_conf is not None:\n        logging.info(f""Use preprocessing: {preprocess_conf}"")\n        transform = Transformation(preprocess_conf)\n    else:\n        transform = None\n\n    # Creates a IStft instance\n    istft = None\n    frame_shift = args.istft_n_shift  # Used for plot the spectrogram\n    if args.apply_istft:\n        if preprocess_conf is not None:\n            # Read the conffile and find stft setting\n            with open(preprocess_conf) as f:\n                # Json format: e.g.\n                #    {""process"": [{""type"": ""stft"",\n                #                  ""win_length"": 400,\n                #                  ""n_fft"": 512, ""n_shift"": 160,\n                #                  ""window"": ""han""},\n                #                 {""type"": ""foo"", ...}, ...]}\n                conf = json.load(f)\n                assert ""process"" in conf, conf\n                # Find stft setting\n                for p in conf[""process""]:\n                    if p[""type""] == ""stft"":\n                        istft = IStft(\n                            win_length=p[""win_length""],\n                            n_shift=p[""n_shift""],\n                            window=p.get(""window"", ""hann""),\n                        )\n                        logging.info(\n                            ""stft is found in {}. ""\n                            ""Setting istft config from it\\n{}"".format(\n                                preprocess_conf, istft\n                            )\n                        )\n                        frame_shift = p[""n_shift""]\n                        break\n        if istft is None:\n            # Set from command line arguments\n            istft = IStft(\n                win_length=args.istft_win_length,\n                n_shift=args.istft_n_shift,\n                window=args.istft_window,\n            )\n            logging.info(\n                ""Setting istft config from the command line args\\n{}"".format(istft)\n            )\n\n    # sort data\n    keys = list(js.keys())\n    feat_lens = [js[key][""input""][0][""shape""][0] for key in keys]\n    sorted_index = sorted(range(len(feat_lens)), key=lambda i: -feat_lens[i])\n    keys = [keys[i] for i in sorted_index]\n\n    def grouper(n, iterable, fillvalue=None):\n        kargs = [iter(iterable)] * n\n        return zip_longest(*kargs, fillvalue=fillvalue)\n\n    num_images = 0\n    if not os.path.exists(args.image_dir):\n        os.makedirs(args.image_dir)\n\n    for names in grouper(args.batchsize, keys, None):\n        batch = [(name, js[name]) for name in names]\n\n        # May be in time region: (Batch, [Time, Channel])\n        org_feats = load_inputs_and_targets(batch)[0]\n        if transform is not None:\n            # May be in time-freq region: : (Batch, [Time, Channel, Freq])\n            feats = transform(org_feats, train=False)\n        else:\n            feats = org_feats\n\n        with torch.no_grad():\n            enhanced, mask, ilens = model.enhance(feats)\n\n        for idx, name in enumerate(names):\n            # Assuming mask, feats : [Batch, Time, Channel. Freq]\n            #          enhanced    : [Batch, Time, Freq]\n            enh = enhanced[idx][: ilens[idx]]\n            mas = mask[idx][: ilens[idx]]\n            feat = feats[idx]\n\n            # Plot spectrogram\n            if args.image_dir is not None and num_images < args.num_images:\n                import matplotlib.pyplot as plt\n\n                num_images += 1\n                ref_ch = 0\n\n                plt.figure(figsize=(20, 10))\n                plt.subplot(4, 1, 1)\n                plt.title(""Mask [ref={}ch]"".format(ref_ch))\n                plot_spectrogram(\n                    plt,\n                    mas[:, ref_ch].T,\n                    fs=args.fs,\n                    mode=""linear"",\n                    frame_shift=frame_shift,\n                    bottom=False,\n                    labelbottom=False,\n                )\n\n                plt.subplot(4, 1, 2)\n                plt.title(""Noisy speech [ref={}ch]"".format(ref_ch))\n                plot_spectrogram(\n                    plt,\n                    feat[:, ref_ch].T,\n                    fs=args.fs,\n                    mode=""db"",\n                    frame_shift=frame_shift,\n                    bottom=False,\n                    labelbottom=False,\n                )\n\n                plt.subplot(4, 1, 3)\n                plt.title(""Masked speech [ref={}ch]"".format(ref_ch))\n                plot_spectrogram(\n                    plt,\n                    (feat[:, ref_ch] * mas[:, ref_ch]).T,\n                    frame_shift=frame_shift,\n                    fs=args.fs,\n                    mode=""db"",\n                    bottom=False,\n                    labelbottom=False,\n                )\n\n                plt.subplot(4, 1, 4)\n                plt.title(""Enhanced speech"")\n                plot_spectrogram(\n                    plt, enh.T, fs=args.fs, mode=""db"", frame_shift=frame_shift\n                )\n\n                plt.savefig(os.path.join(args.image_dir, name + "".png""))\n                plt.clf()\n\n            # Write enhanced wave files\n            if enh_writer is not None:\n                if istft is not None:\n                    enh = istft(enh)\n                else:\n                    enh = enh\n\n                if args.keep_length:\n                    if len(org_feats[idx]) < len(enh):\n                        # Truncate the frames added by stft padding\n                        enh = enh[: len(org_feats[idx])]\n                    elif len(org_feats) > len(enh):\n                        padwidth = [(0, (len(org_feats[idx]) - len(enh)))] + [\n                            (0, 0)\n                        ] * (enh.ndim - 1)\n                        enh = np.pad(enh, padwidth, mode=""constant"")\n\n                if args.enh_filetype in (""sound"", ""sound.hdf5""):\n                    enh_writer[name] = (args.fs, enh)\n                else:\n                    # Hint: To dump stft_signal, mask or etc,\n                    # enh_filetype=\'hdf5\' might be convenient.\n                    enh_writer[name] = enh\n\n            if num_images >= args.num_images and enh_writer is None:\n                logging.info(""Breaking the process."")\n                break\n'"
espnet/asr/pytorch_backend/asr_init.py,2,"b'""""""Finetuning methods.""""""\n\nimport logging\nimport os\nimport torch\n\nfrom collections import OrderedDict\n\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import torch_load\n\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.mt_interface import MTInterface\nfrom espnet.nets.tts_interface import TTSInterface\n\nfrom espnet.utils.dynamic_import import dynamic_import\n\n\ndef transfer_verification(model_state_dict, partial_state_dict, modules):\n    """"""Verify tuples (key, shape) for input model modules match specified modules.\n\n    Args:\n        model_state_dict (OrderedDict): the initial model state_dict\n        partial_state_dict (OrderedDict): the trained model state_dict\n        modules (list): specified module list for transfer\n\n    Return:\n        (boolean): allow transfer\n\n    """"""\n    modules_model = []\n    partial_modules = []\n\n    for key_p, value_p in partial_state_dict.items():\n        if any(key_p.startswith(m) for m in modules):\n            partial_modules += [(key_p, value_p.shape)]\n\n    for key_m, value_m in model_state_dict.items():\n        if any(key_m.startswith(m) for m in modules):\n            modules_model += [(key_m, value_m.shape)]\n\n    len_match = len(modules_model) == len(partial_modules)\n\n    module_match = sorted(modules_model, key=lambda x: (x[0], x[1])) == sorted(\n        partial_modules, key=lambda x: (x[0], x[1])\n    )\n\n    return len_match and module_match\n\n\ndef get_partial_state_dict(model_state_dict, modules):\n    """"""Create state_dict with specified modules matching input model modules.\n\n    Note that get_partial_lm_state_dict is used if a LM specified.\n\n    Args:\n        model_state_dict (OrderedDict): trained model state_dict\n        modules (list): specified module list for transfer\n\n    Return:\n        new_state_dict (OrderedDict): the updated state_dict\n\n    """"""\n    new_state_dict = OrderedDict()\n\n    for key, value in model_state_dict.items():\n        if any(key.startswith(m) for m in modules):\n            new_state_dict[key] = value\n\n    return new_state_dict\n\n\ndef get_partial_lm_state_dict(model_state_dict, modules):\n    """"""Create compatible ASR state_dict from model_state_dict (LM).\n\n    The keys for specified modules are modified to match ASR decoder modules keys.\n\n    Args:\n        model_state_dict (OrderedDict): trained model state_dict\n        modules (list): specified module list for transfer\n\n    Return:\n        new_state_dict (OrderedDict): the updated state_dict\n        new_mods (list): the updated module list\n\n    """"""\n    new_state_dict = OrderedDict()\n    new_modules = []\n\n    for key, value in list(model_state_dict.items()):\n        if key == ""predictor.embed.weight"" and ""predictor.embed."" in modules:\n            new_key = ""dec.embed.weight""\n            new_state_dict[new_key] = value\n            new_modules += [new_key]\n        elif ""predictor.rnn."" in key and ""predictor.rnn."" in modules:\n            new_key = ""dec.decoder."" + key.split(""predictor.rnn."", 1)[1]\n            new_state_dict[new_key] = value\n            new_modules += [new_key]\n\n    return new_state_dict, new_modules\n\n\ndef filter_modules(model_state_dict, modules):\n    """"""Filter non-matched modules in module_state_dict.\n\n    Args:\n        model_state_dict (OrderedDict): trained model state_dict\n        modules (list): specified module list for transfer\n\n    Return:\n        new_mods (list): the update module list\n\n    """"""\n    new_mods = []\n    incorrect_mods = []\n\n    mods_model = list(model_state_dict.keys())\n    for mod in modules:\n        if any(key.startswith(mod) for key in mods_model):\n            new_mods += [mod]\n        else:\n            incorrect_mods += [mod]\n\n    if incorrect_mods:\n        logging.warning(\n            ""module(s) %s don\'t match or (partially match) ""\n            ""available modules in model."",\n            incorrect_mods,\n        )\n        logging.warning(""for information, the existing modules in model are:"")\n        logging.warning(""%s"", mods_model)\n\n    return new_mods\n\n\ndef load_trained_model(model_path):\n    """"""Load the trained model for recognition.\n\n    Args:\n        model_path (str): Path to model.***.best\n\n    """"""\n    idim, odim, train_args = get_model_conf(\n        model_path, os.path.join(os.path.dirname(model_path), ""model.json"")\n    )\n\n    logging.warning(""reading model parameters from "" + model_path)\n\n    if hasattr(train_args, ""model_module""):\n        model_module = train_args.model_module\n    else:\n        model_module = ""espnet.nets.pytorch_backend.e2e_asr:E2E""\n    model_class = dynamic_import(model_module)\n    model = model_class(idim, odim, train_args)\n\n    torch_load(model_path, model)\n\n    return model, train_args\n\n\ndef get_trained_model_state_dict(model_path):\n    """"""Extract the trained model state dict for pre-initialization.\n\n    Args:\n        model_path (str): Path to model.***.best\n\n    Return:\n        model.state_dict() (OrderedDict): the loaded model state_dict\n        (bool): Boolean defining whether the model is an LM\n\n    """"""\n    conf_path = os.path.join(os.path.dirname(model_path), ""model.json"")\n    if ""rnnlm"" in model_path:\n        logging.warning(""reading model parameters from %s"", model_path)\n\n        return torch.load(model_path), True\n\n    idim, odim, args = get_model_conf(model_path, conf_path)\n\n    logging.warning(""reading model parameters from "" + model_path)\n\n    if hasattr(args, ""model_module""):\n        model_module = args.model_module\n    else:\n        model_module = ""espnet.nets.pytorch_backend.e2e_asr:E2E""\n\n    model_class = dynamic_import(model_module)\n    model = model_class(idim, odim, args)\n    torch_load(model_path, model)\n    assert (\n        isinstance(model, MTInterface)\n        or isinstance(model, ASRInterface)\n        or isinstance(model, TTSInterface)\n    )\n\n    return model.state_dict(), False\n\n\ndef load_trained_modules(idim, odim, args, interface=ASRInterface):\n    """"""Load model encoder or/and decoder modules with ESPNET pre-trained model(s).\n\n    Args:\n        idim (int): initial input dimension.\n        odim (int): initial output dimension.\n        args (Namespace): The initial model arguments.\n        interface (Interface): ASRInterface or STInterface or TTSInterface.\n\n    Return:\n        model (torch.nn.Module): The model with pretrained modules.\n\n    """"""\n\n    def print_new_keys(state_dict, modules, model_path):\n        logging.warning(""loading %s from model: %s"", modules, model_path)\n\n        for k in state_dict.keys():\n            logging.warning(""override %s"" % k)\n\n    enc_model_path = args.enc_init\n    dec_model_path = args.dec_init\n    enc_modules = args.enc_init_mods\n    dec_modules = args.dec_init_mods\n\n    model_class = dynamic_import(args.model_module)\n    main_model = model_class(idim, odim, args)\n    assert isinstance(main_model, interface)\n\n    main_state_dict = main_model.state_dict()\n\n    logging.warning(""model(s) found for pre-initialization"")\n    for model_path, modules in [\n        (enc_model_path, enc_modules),\n        (dec_model_path, dec_modules),\n    ]:\n        if model_path is not None:\n            if os.path.isfile(model_path):\n                model_state_dict, is_lm = get_trained_model_state_dict(model_path)\n\n                modules = filter_modules(model_state_dict, modules)\n                if is_lm:\n                    partial_state_dict, modules = get_partial_lm_state_dict(\n                        model_state_dict, modules\n                    )\n                    print_new_keys(partial_state_dict, modules, model_path)\n                else:\n                    partial_state_dict = get_partial_state_dict(\n                        model_state_dict, modules\n                    )\n\n                    if partial_state_dict:\n                        if transfer_verification(\n                            main_state_dict, partial_state_dict, modules\n                        ):\n                            print_new_keys(partial_state_dict, modules, model_path)\n                            main_state_dict.update(partial_state_dict)\n                        else:\n                            logging.warning(\n                                f""modules {modules} in model {model_path} ""\n                                f""don\'t match your training config"",\n                            )\n            else:\n                logging.warning(""model was not found : %s"", model_path)\n\n    main_model.load_state_dict(main_state_dict)\n\n    return main_model\n'"
espnet/asr/pytorch_backend/asr_mix.py,32,"b'#!/usr/bin/env python3\n\n""""""\nThis script is used for multi-speaker speech recognition.\n\nCopyright 2017 Johns Hopkins University (Shinji Watanabe)\n Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n""""""\nimport json\nimport logging\nimport os\n\n# chainer related\nfrom chainer import training\nfrom chainer.training import extensions\nfrom itertools import zip_longest as zip_longest\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nimport torch\n\nfrom espnet.asr.asr_mix_utils import add_results_to_json\nfrom espnet.asr.asr_utils import adadelta_eps_decay\n\nfrom espnet.asr.asr_utils import CompareValueTrigger\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import restore_snapshot\nfrom espnet.asr.asr_utils import snapshot_object\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.asr.asr_utils import torch_resume\nfrom espnet.asr.asr_utils import torch_snapshot\nfrom espnet.asr.pytorch_backend.asr import CustomEvaluator\nfrom espnet.asr.pytorch_backend.asr import CustomUpdater\nfrom espnet.asr.pytorch_backend.asr import load_trained_model\nimport espnet.lm.pytorch_backend.extlm as extlm_pytorch\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.pytorch_backend.e2e_asr_mix import E2E\nfrom espnet.nets.pytorch_backend.e2e_asr_mix import pad_list\nimport espnet.nets.pytorch_backend.lm.default as lm_pytorch\nfrom espnet.utils.dataset import ChainerDataLoader\nfrom espnet.utils.dataset import TransformDataset\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.training.batchfy import make_batchset\nfrom espnet.utils.training.iterators import ShufflingEnabler\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\nimport matplotlib\n\nmatplotlib.use(""Agg"")\n\n\nclass CustomConverter(object):\n    """"""Custom batch converter for Pytorch.\n\n    Args:\n        subsampling_factor (int): The subsampling factor.\n        dtype (torch.dtype): Data type to convert.\n\n    """"""\n\n    def __init__(self, subsampling_factor=1, dtype=torch.float32):\n        """"""Initialize the converter.""""""\n        self.subsampling_factor = subsampling_factor\n        self.ignore_id = -1\n        self.dtype = dtype\n\n    def __call__(self, batch, device=torch.device(""cpu"")):\n        """"""Transform a batch and send it to a device.\n\n        Args:\n            batch (list(tuple(str, dict[str, dict[str, Any]]))): The batch to transform.\n            device (torch.device): The device to send to.\n\n        Returns:\n            tuple(torch.Tensor, torch.Tensor, torch.Tensor): Transformed batch.\n\n        """"""\n        # batch should be located in list\n        assert len(batch) == 1\n        xs, ys = batch[0]\n        # Convert zip object to list in python 3.x\n        ys = list(ys)\n\n        # perform subsampling\n        if self.subsampling_factor > 1:\n            xs = [x[:: self.subsampling_factor, :] for x in xs]\n\n        # get batch of lengths of input sequences\n        ilens = np.array([x.shape[0] for x in xs])\n\n        # perform padding and convert to tensor\n        # currently only support real number\n        if xs[0].dtype.kind == ""c"":\n            xs_pad_real = pad_list(\n                [torch.from_numpy(x.real).float() for x in xs], 0\n            ).to(device, dtype=self.dtype)\n            xs_pad_imag = pad_list(\n                [torch.from_numpy(x.imag).float() for x in xs], 0\n            ).to(device, dtype=self.dtype)\n            # Note(kamo):\n            # {\'real\': ..., \'imag\': ...} will be changed to ComplexTensor in E2E.\n            # Don\'t create ComplexTensor and give it to E2E here\n            # because torch.nn.DataParallel can\'t handle it.\n            xs_pad = {""real"": xs_pad_real, ""imag"": xs_pad_imag}\n        else:\n            xs_pad = pad_list([torch.from_numpy(x).float() for x in xs], 0).to(\n                device, dtype=self.dtype\n            )\n\n        ilens = torch.from_numpy(ilens).to(device)\n        # TODO(Xuankai): try to make this neat\n        if not isinstance(ys[0], np.ndarray):\n            ys_pad = [torch.from_numpy(y[0]).long() for y in ys] + [\n                torch.from_numpy(y[1]).long() for y in ys\n            ]\n            ys_pad = pad_list(ys_pad, self.ignore_id)\n            ys_pad = (\n                ys_pad.view(2, -1, ys_pad.size(1)).transpose(0, 1).to(device)\n            )  # (num_spkrs, B, Tmax)\n        else:\n            ys_pad = pad_list(\n                [torch.from_numpy(y).long() for y in ys], self.ignore_id\n            ).to(device)\n\n        return xs_pad, ilens, ys_pad\n\n\ndef train(args):\n    """"""Train with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n\n    # check cuda availability\n    if not torch.cuda.is_available():\n        logging.warning(""cuda is not available"")\n\n    # get input and output dimension info\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n    utts = list(valid_json.keys())\n    idim = int(valid_json[utts[0]][""input""][0][""shape""][-1])\n    odim = int(valid_json[utts[0]][""output""][0][""shape""][-1])\n    logging.info(""#input dims : "" + str(idim))\n    logging.info(""#output dims: "" + str(odim))\n\n    # specify attention, CTC, hybrid mode\n    if args.mtlalpha == 1.0:\n        mtl_mode = ""ctc""\n        logging.info(""Pure CTC mode"")\n    elif args.mtlalpha == 0.0:\n        mtl_mode = ""att""\n        logging.info(""Pure attention mode"")\n    else:\n        mtl_mode = ""mtl""\n        logging.info(""Multitask learning mode"")\n\n    # specify model architecture\n    model = E2E(idim, odim, args)\n    subsampling_factor = model.subsample[0]\n\n    if args.rnnlm is not None:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(args.char_list),\n                rnnlm_args.layer,\n                rnnlm_args.unit,\n                getattr(rnnlm_args, ""embed_unit"", None),  # for backward compatibility\n            )\n        )\n        torch.load(args.rnnlm, rnnlm)\n        model.rnnlm = rnnlm\n\n    # write model config\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to "" + model_conf)\n        f.write(\n            json.dumps(\n                (idim, odim, vars(args)), indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n    for key in sorted(vars(args).keys()):\n        logging.info(""ARGS: "" + key + "": "" + str(vars(args)[key]))\n\n    reporter = model.reporter\n\n    # check the use of multi-gpu\n    if args.ngpu > 1:\n        if args.batch_size != 0:\n            logging.warning(\n                ""batch size is automatically increased (%d -> %d)""\n                % (args.batch_size, args.batch_size * args.ngpu)\n            )\n            args.batch_size *= args.ngpu\n\n    # set torch device\n    device = torch.device(""cuda"" if args.ngpu > 0 else ""cpu"")\n    if args.train_dtype in (""float16"", ""float32"", ""float64""):\n        dtype = getattr(torch, args.train_dtype)\n    else:\n        dtype = torch.float32\n    model = model.to(device=device, dtype=dtype)\n\n    # Setup an optimizer\n    if args.opt == ""adadelta"":\n        optimizer = torch.optim.Adadelta(\n            model.parameters(), rho=0.95, eps=args.eps, weight_decay=args.weight_decay\n        )\n    elif args.opt == ""adam"":\n        optimizer = torch.optim.Adam(model.parameters(), weight_decay=args.weight_decay)\n    elif args.opt == ""noam"":\n        from espnet.nets.pytorch_backend.transformer.optimizer import get_std_opt\n\n        optimizer = get_std_opt(\n            model, args.adim, args.transformer_warmup_steps, args.transformer_lr\n        )\n    else:\n        raise NotImplementedError(""unknown optimizer: "" + args.opt)\n\n    # setup apex.amp\n    if args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3""):\n        try:\n            from apex import amp\n        except ImportError as e:\n            logging.error(\n                f""You need to install apex for --train-dtype {args.train_dtype}. ""\n                ""See https://github.com/NVIDIA/apex#linux""\n            )\n            raise e\n        if args.opt == ""noam"":\n            model, optimizer.optimizer = amp.initialize(\n                model, optimizer.optimizer, opt_level=args.train_dtype\n            )\n        else:\n            model, optimizer = amp.initialize(\n                model, optimizer, opt_level=args.train_dtype\n            )\n        use_apex = True\n    else:\n        use_apex = False\n\n    # FIXME: TOO DIRTY HACK\n    setattr(optimizer, ""target"", reporter)\n    setattr(optimizer, ""serialize"", lambda s: reporter.serialize(s))\n\n    # Setup a converter\n    converter = CustomConverter(subsampling_factor=subsampling_factor, dtype=dtype)\n\n    # read json data\n    with open(args.train_json, ""rb"") as f:\n        train_json = json.load(f)[""utts""]\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n    # make minibatch list (variable length)\n    train = make_batchset(\n        train_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        shortest_first=use_sortagrad,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        iaxis=0,\n        oaxis=-1,\n    )\n    valid = make_batchset(\n        valid_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        iaxis=0,\n        oaxis=-1,\n    )\n\n    load_tr = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": True},  # Switch the mode of preprocessing\n    )\n    load_cv = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": False},  # Switch the mode of preprocessing\n    )\n    # hack to make batchsize argument as 1\n    # actual bathsize is included in a list\n    # default collate function converts numpy array to pytorch tensor\n    # we used an empty collate function instead which returns list\n    train_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(train, lambda data: converter([load_tr(data)])),\n            batch_size=1,\n            num_workers=args.n_iter_processes,\n            shuffle=True,\n            collate_fn=lambda x: x[0],\n        )\n    }\n    valid_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(valid, lambda data: converter([load_cv(data)])),\n            batch_size=1,\n            shuffle=False,\n            collate_fn=lambda x: x[0],\n            num_workers=args.n_iter_processes,\n        )\n    }\n\n    # Set up a trainer\n    updater = CustomUpdater(\n        model,\n        args.grad_clip,\n        train_iter,\n        optimizer,\n        device,\n        args.ngpu,\n        args.grad_noise,\n        args.accum_grad,\n        use_apex=use_apex,\n    )\n    trainer = training.Trainer(updater, (args.epochs, ""epoch""), out=args.outdir)\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler([train_iter]),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, ""epoch""),\n        )\n\n    # Resume from a snapshot\n    if args.resume:\n        logging.info(""resumed from %s"" % args.resume)\n        torch_resume(args.resume, trainer)\n\n    # Evaluate the model with the test dataset for each epoch\n    trainer.extend(CustomEvaluator(model, valid_iter, reporter, device, args.ngpu))\n\n    # Save attention weight each epoch\n    if args.num_save_attention > 0 and args.mtlalpha != 1.0:\n        data = sorted(\n            list(valid_json.items())[: args.num_save_attention],\n            key=lambda x: int(x[1][""input""][0][""shape""][1]),\n            reverse=True,\n        )\n        if hasattr(model, ""module""):\n            att_vis_fn = model.module.calculate_all_attentions\n            plot_class = model.module.attention_plot_class\n        else:\n            att_vis_fn = model.calculate_all_attentions\n            plot_class = model.attention_plot_class\n        att_reporter = plot_class(\n            att_vis_fn,\n            data,\n            args.outdir + ""/att_ws"",\n            converter=converter,\n            transform=load_cv,\n            device=device,\n        )\n        trainer.extend(att_reporter, trigger=(1, ""epoch""))\n    else:\n        att_reporter = None\n\n    # Make a plot for training and validation values\n    trainer.extend(\n        extensions.PlotReport(\n            [\n                ""main/loss"",\n                ""validation/main/loss"",\n                ""main/loss_ctc"",\n                ""validation/main/loss_ctc"",\n                ""main/loss_att"",\n                ""validation/main/loss_att"",\n            ],\n            ""epoch"",\n            file_name=""loss.png"",\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/acc"", ""validation/main/acc""], ""epoch"", file_name=""acc.png""\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/cer_ctc"", ""validation/main/cer_ctc""], ""epoch"", file_name=""cer.png""\n        )\n    )\n\n    # Save best models\n    trainer.extend(\n        snapshot_object(model, ""model.loss.best""),\n        trigger=training.triggers.MinValueTrigger(""validation/main/loss""),\n    )\n    if mtl_mode != ""ctc"":\n        trainer.extend(\n            snapshot_object(model, ""model.acc.best""),\n            trigger=training.triggers.MaxValueTrigger(""validation/main/acc""),\n        )\n\n    # save snapshot which contains model and optimizer states\n    trainer.extend(torch_snapshot(), trigger=(1, ""epoch""))\n\n    # epsilon decay in the optimizer\n    if args.opt == ""adadelta"":\n        if args.criterion == ""acc"" and mtl_mode != ""ctc"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.acc.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n        elif args.criterion == ""loss"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.loss.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n\n    # Write a log of evaluation statistics for each epoch\n    trainer.extend(\n        extensions.LogReport(trigger=(args.report_interval_iters, ""iteration""))\n    )\n    report_keys = [\n        ""epoch"",\n        ""iteration"",\n        ""main/loss"",\n        ""main/loss_ctc"",\n        ""main/loss_att"",\n        ""validation/main/loss"",\n        ""validation/main/loss_ctc"",\n        ""validation/main/loss_att"",\n        ""main/acc"",\n        ""validation/main/acc"",\n        ""main/cer_ctc"",\n        ""validation/main/cer_ctc"",\n        ""elapsed_time"",\n    ]\n    if args.opt == ""adadelta"":\n        trainer.extend(\n            extensions.observe_value(\n                ""eps"",\n                lambda trainer: trainer.updater.get_optimizer(""main"").param_groups[0][\n                    ""eps""\n                ],\n            ),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n        report_keys.append(""eps"")\n    if args.report_cer:\n        report_keys.append(""validation/main/cer"")\n    if args.report_wer:\n        report_keys.append(""validation/main/wer"")\n    trainer.extend(\n        extensions.PrintReport(report_keys),\n        trigger=(args.report_interval_iters, ""iteration""),\n    )\n\n    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))\n    set_early_stop(trainer, args)\n\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        trainer.extend(\n            TensorboardLogger(SummaryWriter(args.tensorboard_dir), att_reporter),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n    # Run the training\n    trainer.run()\n    check_early_stop(trainer, args.epochs)\n\n\ndef recog(args):\n    """"""Decode with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n    model, train_args = load_trained_model(args.model)\n    assert isinstance(model, ASRInterface)\n    model.recog_args = args\n\n    # read rnnlm\n    if args.rnnlm:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        if getattr(rnnlm_args, ""model_module"", ""default"") != ""default"":\n            raise ValueError(\n                ""use \'--api v2\' option to decode with non-default language model""\n            )\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(train_args.char_list),\n                rnnlm_args.layer,\n                rnnlm_args.unit,\n                getattr(rnnlm_args, ""embed_unit"", None),  # for backward compatibility\n            )\n        )\n        torch_load(args.rnnlm, rnnlm)\n        rnnlm.eval()\n    else:\n        rnnlm = None\n\n    if args.word_rnnlm:\n        rnnlm_args = get_model_conf(args.word_rnnlm, args.word_rnnlm_conf)\n        word_dict = rnnlm_args.char_list_dict\n        char_dict = {x: i for i, x in enumerate(train_args.char_list)}\n        word_rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(len(word_dict), rnnlm_args.layer, rnnlm_args.unit)\n        )\n        torch_load(args.word_rnnlm, word_rnnlm)\n        word_rnnlm.eval()\n\n        if rnnlm is not None:\n            rnnlm = lm_pytorch.ClassifierWithState(\n                extlm_pytorch.MultiLevelLM(\n                    word_rnnlm.predictor, rnnlm.predictor, word_dict, char_dict\n                )\n            )\n        else:\n            rnnlm = lm_pytorch.ClassifierWithState(\n                extlm_pytorch.LookAheadWordLM(\n                    word_rnnlm.predictor, word_dict, char_dict\n                )\n            )\n\n    # gpu\n    if args.ngpu == 1:\n        gpu_id = list(range(args.ngpu))\n        logging.info(""gpu id: "" + str(gpu_id))\n        model.cuda()\n        if rnnlm:\n            rnnlm.cuda()\n\n    # read json data\n    with open(args.recog_json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n    new_js = {}\n\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=False,\n        sort_in_input_length=False,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={""train"": False},\n    )\n\n    if args.batchsize == 0:\n        with torch.no_grad():\n            for idx, name in enumerate(js.keys(), 1):\n                logging.info(""(%d/%d) decoding "" + name, idx, len(js.keys()))\n                batch = [(name, js[name])]\n                feat = load_inputs_and_targets(batch)[0][0]\n                nbest_hyps = model.recognize(feat, args, train_args.char_list, rnnlm)\n                new_js[name] = add_results_to_json(\n                    js[name], nbest_hyps, train_args.char_list\n                )\n\n    else:\n\n        def grouper(n, iterable, fillvalue=None):\n            kargs = [iter(iterable)] * n\n            return zip_longest(*kargs, fillvalue=fillvalue)\n\n        # sort data if batchsize > 1\n        keys = list(js.keys())\n        if args.batchsize > 1:\n            feat_lens = [js[key][""input""][0][""shape""][0] for key in keys]\n            sorted_index = sorted(range(len(feat_lens)), key=lambda i: -feat_lens[i])\n            keys = [keys[i] for i in sorted_index]\n\n        with torch.no_grad():\n            for names in grouper(args.batchsize, keys, None):\n                names = [name for name in names if name]\n                batch = [(name, js[name]) for name in names]\n                feats = load_inputs_and_targets(batch)[0]\n                nbest_hyps = model.recognize_batch(\n                    feats, args, train_args.char_list, rnnlm=rnnlm\n                )\n\n                for i, name in enumerate(names):\n                    nbest_hyp = [hyp[i] for hyp in nbest_hyps]\n                    new_js[name] = add_results_to_json(\n                        js[name], nbest_hyp, train_args.char_list\n                    )\n\n    with open(args.result_label, ""wb"") as f:\n        f.write(\n            json.dumps(\n                {""utts"": new_js}, indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n'"
espnet/asr/pytorch_backend/recog.py,2,"b'""""""V2 backend for `asr_recog.py` using py:class:`espnet.nets.beam_search.BeamSearch`.""""""\n\nimport json\nimport logging\n\nimport torch\n\nfrom espnet.asr.asr_utils import add_results_to_json\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.asr.pytorch_backend.asr import load_trained_model\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.batch_beam_search import BatchBeamSearch\nfrom espnet.nets.beam_search import BeamSearch\nfrom espnet.nets.lm_interface import dynamic_import_lm\nfrom espnet.nets.scorer_interface import BatchScorerInterface\nfrom espnet.nets.scorers.length_bonus import LengthBonus\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.utils.io_utils import LoadInputsAndTargets\n\n\ndef recog_v2(args):\n    """"""Decode with custom models that implements ScorerInterface.\n\n    Notes:\n        The previous backend espnet.asr.pytorch_backend.asr.recog\n        only supports E2E and RNNLM\n\n    Args:\n        args (namespace): The program arguments.\n        See py:func:`espnet.bin.asr_recog.get_parser` for details\n\n    """"""\n    logging.warning(""experimental API for custom LMs is selected by --api v2"")\n    if args.batchsize > 1:\n        raise NotImplementedError(""multi-utt batch decoding is not implemented"")\n    if args.streaming_mode is not None:\n        raise NotImplementedError(""streaming mode is not implemented"")\n    if args.word_rnnlm:\n        raise NotImplementedError(""word LM is not implemented"")\n\n    set_deterministic_pytorch(args)\n    model, train_args = load_trained_model(args.model)\n    assert isinstance(model, ASRInterface)\n    model.eval()\n\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=False,\n        sort_in_input_length=False,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={""train"": False},\n    )\n\n    if args.rnnlm:\n        lm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        # NOTE: for a compatibility with less than 0.5.0 version models\n        lm_model_module = getattr(lm_args, ""model_module"", ""default"")\n        lm_class = dynamic_import_lm(lm_model_module, lm_args.backend)\n        lm = lm_class(len(train_args.char_list), lm_args)\n        torch_load(args.rnnlm, lm)\n        lm.eval()\n    else:\n        lm = None\n\n    scorers = model.scorers()\n    scorers[""lm""] = lm\n    scorers[""length_bonus""] = LengthBonus(len(train_args.char_list))\n    weights = dict(\n        decoder=1.0 - args.ctc_weight,\n        ctc=args.ctc_weight,\n        lm=args.lm_weight,\n        length_bonus=args.penalty,\n    )\n\n    beam_search = BeamSearch(\n        beam_size=args.beam_size,\n        vocab_size=len(train_args.char_list),\n        weights=weights,\n        scorers=scorers,\n        sos=model.sos,\n        eos=model.eos,\n        token_list=train_args.char_list,\n        pre_beam_score_key=None if args.ctc_weight == 1.0 else ""decoder"",\n    )\n    # TODO(karita): make all scorers batchfied\n    if args.batchsize == 1:\n        non_batch = [\n            k\n            for k, v in beam_search.full_scorers.items()\n            if not isinstance(v, BatchScorerInterface)\n        ]\n        if len(non_batch) == 0:\n            beam_search.__class__ = BatchBeamSearch\n            logging.info(""BatchBeamSearch implementation is selected."")\n        else:\n            logging.warning(\n                f""As non-batch scorers {non_batch} are found, ""\n                f""fall back to non-batch implementation.""\n            )\n\n    if args.ngpu > 1:\n        raise NotImplementedError(""only single GPU decoding is supported"")\n    if args.ngpu == 1:\n        device = ""cuda""\n    else:\n        device = ""cpu""\n    dtype = getattr(torch, args.dtype)\n    logging.info(f""Decoding device={device}, dtype={dtype}"")\n    model.to(device=device, dtype=dtype).eval()\n    beam_search.to(device=device, dtype=dtype).eval()\n\n    # read json data\n    with open(args.recog_json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n    new_js = {}\n    with torch.no_grad():\n        for idx, name in enumerate(js.keys(), 1):\n            logging.info(""(%d/%d) decoding "" + name, idx, len(js.keys()))\n            batch = [(name, js[name])]\n            feat = load_inputs_and_targets(batch)[0][0]\n            enc = model.encode(torch.as_tensor(feat).to(device=device, dtype=dtype))\n            nbest_hyps = beam_search(\n                x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio\n            )\n            nbest_hyps = [\n                h.asdict() for h in nbest_hyps[: min(len(nbest_hyps), args.nbest)]\n            ]\n            new_js[name] = add_results_to_json(\n                js[name], nbest_hyps, train_args.char_list\n            )\n\n    with open(args.result_label, ""wb"") as f:\n        f.write(\n            json.dumps(\n                {""utts"": new_js}, indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n'"
espnet/lm/chainer_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/lm/chainer_backend/extlm.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Mitsubishi Electric Research Laboratories (Takaaki Hori)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport chainer\nimport chainer.functions as F\nfrom espnet.lm.lm_utils import make_lexical_tree\n\n\n# Definition of a multi-level (subword/word) language model\nclass MultiLevelLM(chainer.Chain):\n    logzero = -10000000000.0\n    zero = 1.0e-10\n\n    def __init__(\n        self,\n        wordlm,\n        subwordlm,\n        word_dict,\n        subword_dict,\n        subwordlm_weight=0.8,\n        oov_penalty=1.0,\n        open_vocab=True,\n    ):\n        super(MultiLevelLM, self).__init__()\n        self.wordlm = wordlm\n        self.subwordlm = subwordlm\n        self.word_eos = word_dict[""<eos>""]\n        self.word_unk = word_dict[""<unk>""]\n        self.xp_word_eos = self.xp.full(1, self.word_eos, ""i"")\n        self.xp_word_unk = self.xp.full(1, self.word_unk, ""i"")\n        self.space = subword_dict[""<space>""]\n        self.eos = subword_dict[""<eos>""]\n        self.lexroot = make_lexical_tree(word_dict, subword_dict, self.word_unk)\n        self.log_oov_penalty = math.log(oov_penalty)\n        self.open_vocab = open_vocab\n        self.subword_dict_size = len(subword_dict)\n        self.subwordlm_weight = subwordlm_weight\n        self.normalized = True\n\n    def __call__(self, state, x):\n        # update state with input label x\n        if state is None:  # make initial states and log-prob vectors\n            wlm_state, z_wlm = self.wordlm(None, self.xp_word_eos)\n            wlm_logprobs = F.log_softmax(z_wlm).data\n            clm_state, z_clm = self.subwordlm(None, x)\n            log_y = F.log_softmax(z_clm).data * self.subwordlm_weight\n            new_node = self.lexroot\n            clm_logprob = 0.0\n            xi = self.space\n        else:\n            clm_state, wlm_state, wlm_logprobs, node, log_y, clm_logprob = state\n            xi = int(x)\n            if xi == self.space:  # inter-word transition\n                if node is not None and node[1] >= 0:  # check if the node is word end\n                    w = self.xp.full(1, node[1], ""i"")\n                else:  # this node is not a word end, which means <unk>\n                    w = self.xp_word_unk\n                # update wordlm state and log-prob vector\n                wlm_state, z_wlm = self.wordlm(wlm_state, w)\n                wlm_logprobs = F.log_softmax(z_wlm).data\n                new_node = self.lexroot  # move to the tree root\n                clm_logprob = 0.0\n            elif node is not None and xi in node[0]:  # intra-word transition\n                new_node = node[0][xi]\n                clm_logprob += log_y[0, xi]\n            elif self.open_vocab:  # if no path in the tree, enter open-vocabulary mode\n                new_node = None\n                clm_logprob += log_y[0, xi]\n            else:  # if open_vocab flag is disabled, return 0 probabilities\n                log_y = self.xp.full((1, self.subword_dict_size), self.logzero, ""f"")\n                return (clm_state, wlm_state, None, log_y, 0.0), log_y\n\n            clm_state, z_clm = self.subwordlm(clm_state, x)\n            log_y = F.log_softmax(z_clm).data * self.subwordlm_weight\n\n        # apply word-level probabilies for <space> and <eos> labels\n        if xi != self.space:\n            if new_node is not None and new_node[1] >= 0:  # if new node is word end\n                wlm_logprob = wlm_logprobs[:, new_node[1]] - clm_logprob\n            else:\n                wlm_logprob = wlm_logprobs[:, self.word_unk] + self.log_oov_penalty\n            log_y[:, self.space] = wlm_logprob\n            log_y[:, self.eos] = wlm_logprob\n        else:\n            log_y[:, self.space] = self.logzero\n            log_y[:, self.eos] = self.logzero\n\n        return (clm_state, wlm_state, wlm_logprobs, new_node, log_y, clm_logprob), log_y\n\n    def final(self, state):\n        clm_state, wlm_state, wlm_logprobs, node, log_y, clm_logprob = state\n        if node is not None and node[1] >= 0:  # check if the node is word end\n            w = self.xp.full(1, node[1], ""i"")\n        else:  # this node is not a word end, which means <unk>\n            w = self.xp_word_unk\n        wlm_state, z_wlm = self.wordlm(wlm_state, w)\n        return F.log_softmax(z_wlm).data[:, self.word_eos]\n\n\n# Definition of a look-ahead word language model\nclass LookAheadWordLM(chainer.Chain):\n    logzero = -10000000000.0\n    zero = 1.0e-10\n\n    def __init__(\n        self, wordlm, word_dict, subword_dict, oov_penalty=0.0001, open_vocab=True\n    ):\n        super(LookAheadWordLM, self).__init__()\n        self.wordlm = wordlm\n        self.word_eos = word_dict[""<eos>""]\n        self.word_unk = word_dict[""<unk>""]\n        self.xp_word_eos = self.xp.full(1, self.word_eos, ""i"")\n        self.xp_word_unk = self.xp.full(1, self.word_unk, ""i"")\n        self.space = subword_dict[""<space>""]\n        self.eos = subword_dict[""<eos>""]\n        self.lexroot = make_lexical_tree(word_dict, subword_dict, self.word_unk)\n        self.oov_penalty = oov_penalty\n        self.open_vocab = open_vocab\n        self.subword_dict_size = len(subword_dict)\n        self.normalized = True\n\n    def __call__(self, state, x):\n        # update state with input label x\n        if state is None:  # make initial states and cumlative probability vector\n            wlm_state, z_wlm = self.wordlm(None, self.xp_word_eos)\n            cumsum_probs = self.xp.cumsum(F.softmax(z_wlm).data, axis=1)\n            new_node = self.lexroot\n            xi = self.space\n        else:\n            wlm_state, cumsum_probs, node = state\n            xi = int(x)\n            if xi == self.space:  # inter-word transition\n                if node is not None and node[1] >= 0:  # check if the node is word end\n                    w = self.xp.full(1, node[1], ""i"")\n                else:  # this node is not a word end, which means <unk>\n                    w = self.xp_word_unk\n                # update wordlm state and cumlative probability vector\n                wlm_state, z_wlm = self.wordlm(wlm_state, w)\n                cumsum_probs = self.xp.cumsum(F.softmax(z_wlm).data, axis=1)\n                new_node = self.lexroot  # move to the tree root\n            elif node is not None and xi in node[0]:  # intra-word transition\n                new_node = node[0][xi]\n            elif self.open_vocab:  # if no path in the tree, enter open-vocabulary mode\n                new_node = None\n            else:  # if open_vocab flag is disabled, return 0 probabilities\n                log_y = self.xp.full((1, self.subword_dict_size), self.logzero, ""f"")\n                return (wlm_state, None, None), log_y\n\n        if new_node is not None:\n            succ, wid, wids = new_node\n            # compute parent node probability\n            sum_prob = (\n                (cumsum_probs[:, wids[1]] - cumsum_probs[:, wids[0]])\n                if wids is not None\n                else 1.0\n            )\n            if sum_prob < self.zero:\n                log_y = self.xp.full((1, self.subword_dict_size), self.logzero, ""f"")\n                return (wlm_state, cumsum_probs, new_node), log_y\n            # set <unk> probability as a default value\n            unk_prob = (\n                cumsum_probs[:, self.word_unk] - cumsum_probs[:, self.word_unk - 1]\n            )\n            y = self.xp.full(\n                (1, self.subword_dict_size), unk_prob * self.oov_penalty, ""f""\n            )\n            # compute transition probabilities to child nodes\n            for cid, nd in succ.items():\n                y[:, cid] = (\n                    cumsum_probs[:, nd[2][1]] - cumsum_probs[:, nd[2][0]]\n                ) / sum_prob\n            # apply word-level probabilies for <space> and <eos> labels\n            if wid >= 0:\n                wlm_prob = (cumsum_probs[:, wid] - cumsum_probs[:, wid - 1]) / sum_prob\n                y[:, self.space] = wlm_prob\n                y[:, self.eos] = wlm_prob\n            elif xi == self.space:\n                y[:, self.space] = self.zero\n                y[:, self.eos] = self.zero\n            log_y = self.xp.log(\n                self.xp.clip(y, self.zero, None)\n            )  # clip to avoid log(0)\n        else:  # if no path in the tree, transition probability is one\n            log_y = self.xp.zeros((1, self.subword_dict_size), ""f"")\n        return (wlm_state, cumsum_probs, new_node), log_y\n\n    def final(self, state):\n        wlm_state, cumsum_probs, node = state\n        if node is not None and node[1] >= 0:  # check if the node is word end\n            w = self.xp.full(1, node[1], ""i"")\n        else:  # this node is not a word end, which means <unk>\n            w = self.xp_word_unk\n        wlm_state, z_wlm = self.wordlm(wlm_state, w)\n        return F.log_softmax(z_wlm).data[:, self.word_eos]\n'"
espnet/lm/chainer_backend/lm.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n# This code is ported from the following implementation written in Torch.\n# https://github.com/chainer/chainer/blob/master/examples/ptb/train_ptb_custom_loop.py\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\nimport json\nimport logging\nimport numpy as np\nimport six\n\nimport chainer\nfrom chainer.dataset import convert\nimport chainer.functions as F\nimport chainer.links as L\n\n# for classifier link\nfrom chainer.functions.loss import softmax_cross_entropy\nfrom chainer import link\nfrom chainer import reporter\nfrom chainer import training\nfrom chainer.training import extensions\n\nfrom espnet.lm.lm_utils import compute_perplexity\nfrom espnet.lm.lm_utils import count_tokens\nfrom espnet.lm.lm_utils import MakeSymlinkToBestModel\nfrom espnet.lm.lm_utils import ParallelSentenceIterator\nfrom espnet.lm.lm_utils import read_tokens\n\nimport espnet.nets.chainer_backend.deterministic_embed_id as DL\nfrom espnet.nets.lm_interface import LMInterface\nfrom espnet.optimizer.factory import dynamic_import_optimizer\nfrom espnet.scheduler.chainer import ChainerScheduler\nfrom espnet.scheduler.scheduler import dynamic_import_scheduler\n\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom tensorboardX import SummaryWriter\n\nfrom espnet.utils.deterministic_utils import set_deterministic_chainer\nfrom espnet.utils.training.evaluator import BaseEvaluator\nfrom espnet.utils.training.iterators import ShufflingEnabler\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\n\n# TODO(karita): reimplement RNNLM with new interface\nclass DefaultRNNLM(LMInterface, link.Chain):\n    """"""Default RNNLM wrapper to compute reduce framewise loss values.\n\n    Args:\n        n_vocab (int): The size of the vocabulary\n        args (argparse.Namespace): configurations. see `add_arguments`\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        parser.add_argument(\n            ""--type"",\n            type=str,\n            default=""lstm"",\n            nargs=""?"",\n            choices=[""lstm"", ""gru""],\n            help=""Which type of RNN to use"",\n        )\n        parser.add_argument(\n            ""--layer"", ""-l"", type=int, default=2, help=""Number of hidden layers""\n        )\n        parser.add_argument(\n            ""--unit"", ""-u"", type=int, default=650, help=""Number of hidden units""\n        )\n        return parser\n\n\nclass ClassifierWithState(link.Chain):\n    """"""A wrapper for a chainer RNNLM\n\n    :param link.Chain predictor : The RNNLM\n    :param function lossfun: The loss function to use\n    :param int/str label_key:\n    """"""\n\n    def __init__(\n        self,\n        predictor,\n        lossfun=softmax_cross_entropy.softmax_cross_entropy,\n        label_key=-1,\n    ):\n        if not (isinstance(label_key, (int, str))):\n            raise TypeError(""label_key must be int or str, but is %s"" % type(label_key))\n\n        super(ClassifierWithState, self).__init__()\n        self.lossfun = lossfun\n        self.y = None\n        self.loss = None\n        self.label_key = label_key\n\n        with self.init_scope():\n            self.predictor = predictor\n\n    def __call__(self, state, *args, **kwargs):\n        """"""Computes the loss value for an input and label pair.\n\n            It also computes accuracy and stores it to the attribute.\n            When ``label_key`` is ``int``, the corresponding element in ``args``\n            is treated as ground truth labels. And when it is ``str``, the\n            element in ``kwargs`` is used.\n            The all elements of ``args`` and ``kwargs`` except the groundtruth\n            labels are features.\n            It feeds features to the predictor and compare the result\n            with ground truth labels.\n\n        :param state : The LM state\n        :param list[chainer.Variable] args : Input minibatch\n        :param dict[chainer.Variable] kwargs : Input minibatch\n        :return loss value\n        :rtype chainer.Variable\n        """"""\n\n        if isinstance(self.label_key, int):\n            if not (-len(args) <= self.label_key < len(args)):\n                msg = ""Label key %d is out of bounds"" % self.label_key\n                raise ValueError(msg)\n            t = args[self.label_key]\n            if self.label_key == -1:\n                args = args[:-1]\n            else:\n                args = args[: self.label_key] + args[self.label_key + 1 :]\n        elif isinstance(self.label_key, str):\n            if self.label_key not in kwargs:\n                msg = \'Label key ""%s"" is not found\' % self.label_key\n                raise ValueError(msg)\n            t = kwargs[self.label_key]\n            del kwargs[self.label_key]\n\n        self.y = None\n        self.loss = None\n        state, self.y = self.predictor(state, *args, **kwargs)\n        self.loss = self.lossfun(self.y, t)\n        return state, self.loss\n\n    def predict(self, state, x):\n        """"""Predict log probabilities for given state and input x using the predictor\n\n        :param state : the state\n        :param x : the input\n        :return a tuple (state, log prob vector)\n        :rtype cupy/numpy array\n        """"""\n        if hasattr(self.predictor, ""normalized"") and self.predictor.normalized:\n            return self.predictor(state, x)\n        else:\n            state, z = self.predictor(state, x)\n            return state, F.log_softmax(z).data\n\n    def final(self, state):\n        """"""Predict final log probabilities for given state using the predictor\n\n        :param state : the state\n        :return log probability vector\n        :rtype cupy/numpy array\n\n        """"""\n        if hasattr(self.predictor, ""final""):\n            return self.predictor.final(state)\n        else:\n            return 0.0\n\n\n# Definition of a recurrent net for language modeling\nclass RNNLM(chainer.Chain):\n    """"""A chainer RNNLM\n\n    :param int n_vocab: The size of the vocabulary\n    :param int n_layers: The number of layers to create\n    :param int n_units: The number of units per layer\n    :param str type: The RNN type\n    """"""\n\n    def __init__(self, n_vocab, n_layers, n_units, typ=""lstm""):\n        super(RNNLM, self).__init__()\n        with self.init_scope():\n            self.embed = DL.EmbedID(n_vocab, n_units)\n            self.rnn = (\n                chainer.ChainList(\n                    *[L.StatelessLSTM(n_units, n_units) for _ in range(n_layers)]\n                )\n                if typ == ""lstm""\n                else chainer.ChainList(\n                    *[L.StatelessGRU(n_units, n_units) for _ in range(n_layers)]\n                )\n            )\n            self.lo = L.Linear(n_units, n_vocab)\n\n        for param in self.params():\n            param.data[...] = np.random.uniform(-0.1, 0.1, param.data.shape)\n        self.n_layers = n_layers\n        self.n_units = n_units\n        self.typ = typ\n\n    def __call__(self, state, x):\n        if state is None:\n            if self.typ == ""lstm"":\n                state = {""c"": [None] * self.n_layers, ""h"": [None] * self.n_layers}\n            else:\n                state = {""h"": [None] * self.n_layers}\n\n        h = [None] * self.n_layers\n        emb = self.embed(x)\n        if self.typ == ""lstm"":\n            c = [None] * self.n_layers\n            c[0], h[0] = self.rnn[0](state[""c""][0], state[""h""][0], F.dropout(emb))\n            for n in six.moves.range(1, self.n_layers):\n                c[n], h[n] = self.rnn[n](\n                    state[""c""][n], state[""h""][n], F.dropout(h[n - 1])\n                )\n            state = {""c"": c, ""h"": h}\n        else:\n            if state[""h""][0] is None:\n                xp = self.xp\n                with chainer.backends.cuda.get_device_from_id(self._device_id):\n                    state[""h""][0] = chainer.Variable(\n                        xp.zeros((emb.shape[0], self.n_units), dtype=emb.dtype)\n                    )\n            h[0] = self.rnn[0](state[""h""][0], F.dropout(emb))\n            for n in six.moves.range(1, self.n_layers):\n                if state[""h""][n] is None:\n                    xp = self.xp\n                    with chainer.backends.cuda.get_device_from_id(self._device_id):\n                        state[""h""][n] = chainer.Variable(\n                            xp.zeros(\n                                (h[n - 1].shape[0], self.n_units), dtype=h[n - 1].dtype\n                            )\n                        )\n                h[n] = self.rnn[n](state[""h""][n], F.dropout(h[n - 1]))\n            state = {""h"": h}\n        y = self.lo(F.dropout(h[-1]))\n        return state, y\n\n\nclass BPTTUpdater(training.updaters.StandardUpdater):\n    """"""An updater for a chainer LM\n\n    :param chainer.dataset.Iterator train_iter : The train iterator\n    :param optimizer:\n    :param schedulers:\n    :param int device : The device id\n    :param int accum_grad :\n    """"""\n\n    def __init__(self, train_iter, optimizer, schedulers, device, accum_grad):\n        super(BPTTUpdater, self).__init__(train_iter, optimizer, device=device)\n        self.scheduler = ChainerScheduler(schedulers, optimizer)\n        self.accum_grad = accum_grad\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n        # they are automatically named \'main\'.\n        train_iter = self.get_iterator(""main"")\n        optimizer = self.get_optimizer(""main"")\n\n        count = 0\n        sum_loss = 0\n        optimizer.target.cleargrads()  # Clear the parameter gradients\n        for _ in range(self.accum_grad):\n            # Progress the dataset iterator for sentences at each iteration.\n            batch = train_iter.__next__()\n            x, t = convert.concat_examples(batch, device=self.device, padding=(0, -1))\n            # Concatenate the token IDs to matrices and send them to the device\n            # self.converter does this job\n            # (it is chainer.dataset.concat_examples by default)\n            xp = chainer.backends.cuda.get_array_module(x)\n            loss = 0\n            state = None\n            batch_size, sequence_length = x.shape\n            for i in six.moves.range(sequence_length):\n                # Compute the loss at this time step and accumulate it\n                state, loss_batch = optimizer.target(\n                    state, chainer.Variable(x[:, i]), chainer.Variable(t[:, i])\n                )\n                non_zeros = xp.count_nonzero(x[:, i])\n                loss += loss_batch * non_zeros\n                count += int(non_zeros)\n            # backward\n            loss /= batch_size * self.accum_grad  # normalized by batch size\n            sum_loss += float(loss.data)\n            loss.backward()  # Backprop\n            loss.unchain_backward()  # Truncate the graph\n\n        reporter.report({""loss"": sum_loss}, optimizer.target)\n        reporter.report({""count"": count}, optimizer.target)\n        # update\n        optimizer.update()  # Update the parameters\n        self.scheduler.step(self.iteration)\n\n\nclass LMEvaluator(BaseEvaluator):\n    """"""A custom evaluator for a chainer LM\n\n    :param chainer.dataset.Iterator val_iter : The validation iterator\n    :param eval_model : The model to evaluate\n    :param int device : The device id to use\n    """"""\n\n    def __init__(self, val_iter, eval_model, device):\n        super(LMEvaluator, self).__init__(val_iter, eval_model, device=device)\n\n    def evaluate(self):\n        val_iter = self.get_iterator(""main"")\n        target = self.get_target(""main"")\n        loss = 0\n        count = 0\n        for batch in copy.copy(val_iter):\n            x, t = convert.concat_examples(batch, device=self.device, padding=(0, -1))\n            xp = chainer.backends.cuda.get_array_module(x)\n            state = None\n            for i in six.moves.range(len(x[0])):\n                state, loss_batch = target(state, x[:, i], t[:, i])\n                non_zeros = xp.count_nonzero(x[:, i])\n                loss += loss_batch.data * non_zeros\n                count += int(non_zeros)\n        # report validation loss\n        observation = {}\n        with reporter.report_scope(observation):\n            reporter.report({""loss"": float(loss / count)}, target)\n        return observation\n\n\ndef train(args):\n    """"""Train with the given args\n\n    :param Namespace args: The program arguments\n    """"""\n    # TODO(karita): support this\n    if args.model_module != ""default"":\n        raise NotImplementedError(""chainer backend does not support --model-module"")\n\n    # display chainer version\n    logging.info(""chainer version = "" + chainer.__version__)\n\n    set_deterministic_chainer(args)\n\n    # check cuda and cudnn availability\n    if not chainer.cuda.available:\n        logging.warning(""cuda is not available"")\n    if not chainer.cuda.cudnn_enabled:\n        logging.warning(""cudnn is not available"")\n\n    # get special label ids\n    unk = args.char_list_dict[""<unk>""]\n    eos = args.char_list_dict[""<eos>""]\n    # read tokens as a sequence of sentences\n    train = read_tokens(args.train_label, args.char_list_dict)\n    val = read_tokens(args.valid_label, args.char_list_dict)\n    # count tokens\n    n_train_tokens, n_train_oovs = count_tokens(train, unk)\n    n_val_tokens, n_val_oovs = count_tokens(val, unk)\n    logging.info(""#vocab = "" + str(args.n_vocab))\n    logging.info(""#sentences in the training data = "" + str(len(train)))\n    logging.info(""#tokens in the training data = "" + str(n_train_tokens))\n    logging.info(\n        ""oov rate in the training data = %.2f %%""\n        % (n_train_oovs / n_train_tokens * 100)\n    )\n    logging.info(""#sentences in the validation data = "" + str(len(val)))\n    logging.info(""#tokens in the validation data = "" + str(n_val_tokens))\n    logging.info(\n        ""oov rate in the validation data = %.2f %%"" % (n_val_oovs / n_val_tokens * 100)\n    )\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n\n    # Create the dataset iterators\n    train_iter = ParallelSentenceIterator(\n        train,\n        args.batchsize,\n        max_length=args.maxlen,\n        sos=eos,\n        eos=eos,\n        shuffle=not use_sortagrad,\n    )\n    val_iter = ParallelSentenceIterator(\n        val, args.batchsize, max_length=args.maxlen, sos=eos, eos=eos, repeat=False\n    )\n    epoch_iters = int(len(train_iter.batch_indices) / args.accum_grad)\n    logging.info(""#iterations per epoch = %d"" % epoch_iters)\n    logging.info(""#total iterations = "" + str(args.epoch * epoch_iters))\n    # Prepare an RNNLM model\n    rnn = RNNLM(args.n_vocab, args.layer, args.unit, args.type)\n    model = ClassifierWithState(rnn)\n    if args.ngpu > 1:\n        logging.warning(""currently, multi-gpu is not supported. use single gpu."")\n    if args.ngpu > 0:\n        # Make the specified GPU current\n        gpu_id = 0\n        chainer.cuda.get_device_from_id(gpu_id).use()\n        model.to_gpu()\n    else:\n        gpu_id = -1\n\n    # Save model conf to json\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to "" + model_conf)\n        f.write(\n            json.dumps(vars(args), indent=4, ensure_ascii=False, sort_keys=True).encode(\n                ""utf_8""\n            )\n        )\n\n    # Set up an optimizer\n    opt_class = dynamic_import_optimizer(args.opt, args.backend)\n    optimizer = opt_class.from_args(model, args)\n    if args.schedulers is None:\n        schedulers = []\n    else:\n        schedulers = [dynamic_import_scheduler(v)(k, args) for k, v in args.schedulers]\n\n    optimizer.setup(model)\n    optimizer.add_hook(chainer.optimizer.GradientClipping(args.gradclip))\n\n    updater = BPTTUpdater(train_iter, optimizer, schedulers, gpu_id, args.accum_grad)\n    trainer = training.Trainer(updater, (args.epoch, ""epoch""), out=args.outdir)\n    trainer.extend(LMEvaluator(val_iter, model, device=gpu_id))\n    trainer.extend(\n        extensions.LogReport(\n            postprocess=compute_perplexity,\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n    )\n    trainer.extend(\n        extensions.PrintReport(\n            [""epoch"", ""iteration"", ""perplexity"", ""val_perplexity"", ""elapsed_time""]\n        ),\n        trigger=(args.report_interval_iters, ""iteration""),\n    )\n    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))\n    trainer.extend(extensions.snapshot(filename=""snapshot.ep.{.updater.epoch}""))\n    trainer.extend(extensions.snapshot_object(model, ""rnnlm.model.{.updater.epoch}""))\n    # MEMO(Hori): wants to use MinValueTrigger, but it seems to fail in resuming\n    trainer.extend(MakeSymlinkToBestModel(""validation/main/loss"", ""rnnlm.model""))\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler([train_iter]),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epoch, ""epoch""),\n        )\n\n    if args.resume:\n        logging.info(""resumed from %s"" % args.resume)\n        chainer.serializers.load_npz(args.resume, trainer)\n\n    set_early_stop(trainer, args, is_lm=True)\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        writer = SummaryWriter(args.tensorboard_dir)\n        trainer.extend(\n            TensorboardLogger(writer), trigger=(args.report_interval_iters, ""iteration"")\n        )\n\n    trainer.run()\n    check_early_stop(trainer, args.epoch)\n\n    # compute perplexity for test set\n    if args.test_label:\n        logging.info(""test the best model"")\n        chainer.serializers.load_npz(args.outdir + ""/rnnlm.model.best"", model)\n        test = read_tokens(args.test_label, args.char_list_dict)\n        n_test_tokens, n_test_oovs = count_tokens(test, unk)\n        logging.info(""#sentences in the test data = "" + str(len(test)))\n        logging.info(""#tokens in the test data = "" + str(n_test_tokens))\n        logging.info(\n            ""oov rate in the test data = %.2f %%"" % (n_test_oovs / n_test_tokens * 100)\n        )\n        test_iter = ParallelSentenceIterator(\n            test, args.batchsize, max_length=args.maxlen, sos=eos, eos=eos, repeat=False\n        )\n        evaluator = LMEvaluator(test_iter, model, device=gpu_id)\n        with chainer.using_config(""train"", False):\n            result = evaluator()\n        logging.info(""test perplexity: "" + str(np.exp(float(result[""main/loss""]))))\n'"
espnet/lm/pytorch_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/lm/pytorch_backend/extlm.py,19,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Mitsubishi Electric Research Laboratories (Takaaki Hori)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom espnet.lm.lm_utils import make_lexical_tree\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\n\n\n# Definition of a multi-level (subword/word) language model\nclass MultiLevelLM(nn.Module):\n    logzero = -10000000000.0\n    zero = 1.0e-10\n\n    def __init__(\n        self,\n        wordlm,\n        subwordlm,\n        word_dict,\n        subword_dict,\n        subwordlm_weight=0.8,\n        oov_penalty=1.0,\n        open_vocab=True,\n    ):\n        super(MultiLevelLM, self).__init__()\n        self.wordlm = wordlm\n        self.subwordlm = subwordlm\n        self.word_eos = word_dict[""<eos>""]\n        self.word_unk = word_dict[""<unk>""]\n        self.var_word_eos = torch.LongTensor([self.word_eos])\n        self.var_word_unk = torch.LongTensor([self.word_unk])\n        self.space = subword_dict[""<space>""]\n        self.eos = subword_dict[""<eos>""]\n        self.lexroot = make_lexical_tree(word_dict, subword_dict, self.word_unk)\n        self.log_oov_penalty = math.log(oov_penalty)\n        self.open_vocab = open_vocab\n        self.subword_dict_size = len(subword_dict)\n        self.subwordlm_weight = subwordlm_weight\n        self.normalized = True\n\n    def forward(self, state, x):\n        # update state with input label x\n        if state is None:  # make initial states and log-prob vectors\n            self.var_word_eos = to_device(self, self.var_word_eos)\n            self.var_word_unk = to_device(self, self.var_word_eos)\n            wlm_state, z_wlm = self.wordlm(None, self.var_word_eos)\n            wlm_logprobs = F.log_softmax(z_wlm, dim=1)\n            clm_state, z_clm = self.subwordlm(None, x)\n            log_y = F.log_softmax(z_clm, dim=1) * self.subwordlm_weight\n            new_node = self.lexroot\n            clm_logprob = 0.0\n            xi = self.space\n        else:\n            clm_state, wlm_state, wlm_logprobs, node, log_y, clm_logprob = state\n            xi = int(x)\n            if xi == self.space:  # inter-word transition\n                if node is not None and node[1] >= 0:  # check if the node is word end\n                    w = to_device(self, torch.LongTensor([node[1]]))\n                else:  # this node is not a word end, which means <unk>\n                    w = self.var_word_unk\n                # update wordlm state and log-prob vector\n                wlm_state, z_wlm = self.wordlm(wlm_state, w)\n                wlm_logprobs = F.log_softmax(z_wlm, dim=1)\n                new_node = self.lexroot  # move to the tree root\n                clm_logprob = 0.0\n            elif node is not None and xi in node[0]:  # intra-word transition\n                new_node = node[0][xi]\n                clm_logprob += log_y[0, xi]\n            elif self.open_vocab:  # if no path in the tree, enter open-vocabulary mode\n                new_node = None\n                clm_logprob += log_y[0, xi]\n            else:  # if open_vocab flag is disabled, return 0 probabilities\n                log_y = to_device(\n                    self, torch.full((1, self.subword_dict_size), self.logzero)\n                )\n                return (clm_state, wlm_state, wlm_logprobs, None, log_y, 0.0), log_y\n\n            clm_state, z_clm = self.subwordlm(clm_state, x)\n            log_y = F.log_softmax(z_clm, dim=1) * self.subwordlm_weight\n\n        # apply word-level probabilies for <space> and <eos> labels\n        if xi != self.space:\n            if new_node is not None and new_node[1] >= 0:  # if new node is word end\n                wlm_logprob = wlm_logprobs[:, new_node[1]] - clm_logprob\n            else:\n                wlm_logprob = wlm_logprobs[:, self.word_unk] + self.log_oov_penalty\n            log_y[:, self.space] = wlm_logprob\n            log_y[:, self.eos] = wlm_logprob\n        else:\n            log_y[:, self.space] = self.logzero\n            log_y[:, self.eos] = self.logzero\n\n        return (\n            (clm_state, wlm_state, wlm_logprobs, new_node, log_y, float(clm_logprob)),\n            log_y,\n        )\n\n    def final(self, state):\n        clm_state, wlm_state, wlm_logprobs, node, log_y, clm_logprob = state\n        if node is not None and node[1] >= 0:  # check if the node is word end\n            w = to_device(self, torch.LongTensor([node[1]]))\n        else:  # this node is not a word end, which means <unk>\n            w = self.var_word_unk\n        wlm_state, z_wlm = self.wordlm(wlm_state, w)\n        return float(F.log_softmax(z_wlm, dim=1)[:, self.word_eos])\n\n\n# Definition of a look-ahead word language model\nclass LookAheadWordLM(nn.Module):\n    logzero = -10000000000.0\n    zero = 1.0e-10\n\n    def __init__(\n        self, wordlm, word_dict, subword_dict, oov_penalty=0.0001, open_vocab=True\n    ):\n        super(LookAheadWordLM, self).__init__()\n        self.wordlm = wordlm\n        self.word_eos = word_dict[""<eos>""]\n        self.word_unk = word_dict[""<unk>""]\n        self.var_word_eos = torch.LongTensor([self.word_eos])\n        self.var_word_unk = torch.LongTensor([self.word_unk])\n        self.space = subword_dict[""<space>""]\n        self.eos = subword_dict[""<eos>""]\n        self.lexroot = make_lexical_tree(word_dict, subword_dict, self.word_unk)\n        self.oov_penalty = oov_penalty\n        self.open_vocab = open_vocab\n        self.subword_dict_size = len(subword_dict)\n        self.zero_tensor = torch.FloatTensor([self.zero])\n        self.normalized = True\n\n    def forward(self, state, x):\n        # update state with input label x\n        if state is None:  # make initial states and cumlative probability vector\n            self.var_word_eos = to_device(self, self.var_word_eos)\n            self.var_word_unk = to_device(self, self.var_word_eos)\n            self.zero_tensor = to_device(self, self.zero_tensor)\n            wlm_state, z_wlm = self.wordlm(None, self.var_word_eos)\n            cumsum_probs = torch.cumsum(F.softmax(z_wlm, dim=1), dim=1)\n            new_node = self.lexroot\n            xi = self.space\n        else:\n            wlm_state, cumsum_probs, node = state\n            xi = int(x)\n            if xi == self.space:  # inter-word transition\n                if node is not None and node[1] >= 0:  # check if the node is word end\n                    w = to_device(self, torch.LongTensor([node[1]]))\n                else:  # this node is not a word end, which means <unk>\n                    w = self.var_word_unk\n                # update wordlm state and cumlative probability vector\n                wlm_state, z_wlm = self.wordlm(wlm_state, w)\n                cumsum_probs = torch.cumsum(F.softmax(z_wlm, dim=1), dim=1)\n                new_node = self.lexroot  # move to the tree root\n            elif node is not None and xi in node[0]:  # intra-word transition\n                new_node = node[0][xi]\n            elif self.open_vocab:  # if no path in the tree, enter open-vocabulary mode\n                new_node = None\n            else:  # if open_vocab flag is disabled, return 0 probabilities\n                log_y = to_device(\n                    self, torch.full((1, self.subword_dict_size), self.logzero)\n                )\n                return (wlm_state, None, None), log_y\n\n        if new_node is not None:\n            succ, wid, wids = new_node\n            # compute parent node probability\n            sum_prob = (\n                (cumsum_probs[:, wids[1]] - cumsum_probs[:, wids[0]])\n                if wids is not None\n                else 1.0\n            )\n            if sum_prob < self.zero:\n                log_y = to_device(\n                    self, torch.full((1, self.subword_dict_size), self.logzero)\n                )\n                return (wlm_state, cumsum_probs, new_node), log_y\n            # set <unk> probability as a default value\n            unk_prob = (\n                cumsum_probs[:, self.word_unk] - cumsum_probs[:, self.word_unk - 1]\n            )\n            y = to_device(\n                self,\n                torch.full(\n                    (1, self.subword_dict_size), float(unk_prob) * self.oov_penalty\n                ),\n            )\n            # compute transition probabilities to child nodes\n            for cid, nd in succ.items():\n                y[:, cid] = (\n                    cumsum_probs[:, nd[2][1]] - cumsum_probs[:, nd[2][0]]\n                ) / sum_prob\n            # apply word-level probabilies for <space> and <eos> labels\n            if wid >= 0:\n                wlm_prob = (cumsum_probs[:, wid] - cumsum_probs[:, wid - 1]) / sum_prob\n                y[:, self.space] = wlm_prob\n                y[:, self.eos] = wlm_prob\n            elif xi == self.space:\n                y[:, self.space] = self.zero\n                y[:, self.eos] = self.zero\n            log_y = torch.log(torch.max(y, self.zero_tensor))  # clip to avoid log(0)\n        else:  # if no path in the tree, transition probability is one\n            log_y = to_device(self, torch.zeros(1, self.subword_dict_size))\n        return (wlm_state, cumsum_probs, new_node), log_y\n\n    def final(self, state):\n        wlm_state, cumsum_probs, node = state\n        if node is not None and node[1] >= 0:  # check if the node is word end\n            w = to_device(self, torch.LongTensor([node[1]]))\n        else:  # this node is not a word end, which means <unk>\n            w = self.var_word_unk\n        wlm_state, z_wlm = self.wordlm(wlm_state, w)\n        return float(F.log_softmax(z_wlm, dim=1)[:, self.word_eos])\n'"
espnet/lm/pytorch_backend/lm.py,13,"b'#!/usr/bin/env python3\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n# This code is ported from the following implementation written in Torch.\n# https://github.com/chainer/chainer/blob/master/examples/ptb/train_ptb_custom_loop.py\n\n""""""LM training in pytorch.""""""\n\nimport copy\nimport json\nimport logging\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.parallel import data_parallel\n\nfrom chainer import Chain\nfrom chainer.dataset import convert\nfrom chainer import reporter\nfrom chainer import training\nfrom chainer.training import extensions\n\nfrom espnet.lm.lm_utils import count_tokens\nfrom espnet.lm.lm_utils import load_dataset\nfrom espnet.lm.lm_utils import MakeSymlinkToBestModel\nfrom espnet.lm.lm_utils import ParallelSentenceIterator\nfrom espnet.lm.lm_utils import read_tokens\nfrom espnet.nets.lm_interface import dynamic_import_lm\nfrom espnet.nets.lm_interface import LMInterface\nfrom espnet.optimizer.factory import dynamic_import_optimizer\nfrom espnet.scheduler.pytorch import PyTorchScheduler\nfrom espnet.scheduler.scheduler import dynamic_import_scheduler\n\nfrom espnet.asr.asr_utils import snapshot_object\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.asr.asr_utils import torch_resume\nfrom espnet.asr.asr_utils import torch_snapshot\n\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom tensorboardX import SummaryWriter\n\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.utils.training.evaluator import BaseEvaluator\nfrom espnet.utils.training.iterators import ShufflingEnabler\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\n\ndef compute_perplexity(result):\n    """"""Compute and add the perplexity to the LogReport.\n\n    :param dict result: The current observations\n    """"""\n    # Routine to rewrite the result dictionary of LogReport to add perplexity values\n    result[""perplexity""] = np.exp(result[""main/nll""] / result[""main/count""])\n    if ""validation/main/nll"" in result:\n        result[""val_perplexity""] = np.exp(\n            result[""validation/main/nll""] / result[""validation/main/count""]\n        )\n\n\nclass Reporter(Chain):\n    """"""Dummy module to use chainer\'s trainer.""""""\n\n    def report(self, loss):\n        """"""Report nothing.""""""\n        pass\n\n\ndef concat_examples(batch, device=None, padding=None):\n    """"""Concat examples in minibatch.\n\n    :param np.ndarray batch: The batch to concatenate\n    :param int device: The device to send to\n    :param Tuple[int,int] padding: The padding to use\n    :return: (inputs, targets)\n    :rtype (torch.Tensor, torch.Tensor)\n    """"""\n    x, t = convert.concat_examples(batch, padding=padding)\n    x = torch.from_numpy(x)\n    t = torch.from_numpy(t)\n    if device is not None and device >= 0:\n        x = x.cuda(device)\n        t = t.cuda(device)\n    return x, t\n\n\nclass BPTTUpdater(training.StandardUpdater):\n    """"""An updater for a pytorch LM.""""""\n\n    def __init__(\n        self,\n        train_iter,\n        model,\n        optimizer,\n        schedulers,\n        device,\n        gradclip=None,\n        use_apex=False,\n        accum_grad=1,\n    ):\n        """"""Initialize class.\n\n        Args:\n            train_iter (chainer.dataset.Iterator): The train iterator\n            model (LMInterface) : The model to update\n            optimizer (torch.optim.Optimizer): The optimizer for training\n            schedulers (espnet.scheduler.scheduler.SchedulerInterface):\n                The schedulers of `optimizer`\n            device (int): The device id\n            gradclip (float): The gradient clipping value to use\n            use_apex (bool): The flag to use Apex in backprop.\n            accum_grad (int): The number of gradient accumulation.\n\n        """"""\n        super(BPTTUpdater, self).__init__(train_iter, optimizer)\n        self.model = model\n        self.device = device\n        self.gradclip = gradclip\n        self.use_apex = use_apex\n        self.scheduler = PyTorchScheduler(schedulers, optimizer)\n        self.accum_grad = accum_grad\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        """"""Update the model.""""""\n        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n        # they are automatically named \'main\'.\n        train_iter = self.get_iterator(""main"")\n        optimizer = self.get_optimizer(""main"")\n        # Progress the dataset iterator for sentences at each iteration.\n        self.model.zero_grad()  # Clear the parameter gradients\n        accum = {""loss"": 0.0, ""nll"": 0.0, ""count"": 0}\n        for _ in range(self.accum_grad):\n            batch = train_iter.__next__()\n            # Concatenate the token IDs to matrices and send them to the device\n            # self.converter does this job\n            # (it is chainer.dataset.concat_examples by default)\n            x, t = concat_examples(batch, device=self.device[0], padding=(0, -100))\n            if self.device[0] == -1:\n                loss, nll, count = self.model(x, t)\n            else:\n                # apex does not support torch.nn.DataParallel\n                loss, nll, count = data_parallel(self.model, (x, t), self.device)\n\n            # backward\n            loss = loss.mean() / self.accum_grad\n            if self.use_apex:\n                from apex import amp\n\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()  # Backprop\n            # accumulate stats\n            accum[""loss""] += float(loss)\n            accum[""nll""] += float(nll.sum())\n            accum[""count""] += int(count.sum())\n\n        for k, v in accum.items():\n            reporter.report({k: v}, optimizer.target)\n        if self.gradclip is not None:\n            nn.utils.clip_grad_norm_(self.model.parameters(), self.gradclip)\n        optimizer.step()  # Update the parameters\n        self.scheduler.step(n_iter=self.iteration)\n\n\nclass LMEvaluator(BaseEvaluator):\n    """"""A custom evaluator for a pytorch LM.""""""\n\n    def __init__(self, val_iter, eval_model, reporter, device):\n        """"""Initialize class.\n\n        :param chainer.dataset.Iterator val_iter : The validation iterator\n        :param LMInterface eval_model : The model to evaluate\n        :param chainer.Reporter reporter : The observations reporter\n        :param int device : The device id to use\n\n        """"""\n        super(LMEvaluator, self).__init__(val_iter, reporter, device=-1)\n        self.model = eval_model\n        self.device = device\n\n    def evaluate(self):\n        """"""Evaluate the model.""""""\n        val_iter = self.get_iterator(""main"")\n        loss = 0\n        nll = 0\n        count = 0\n        self.model.eval()\n        with torch.no_grad():\n            for batch in copy.copy(val_iter):\n                x, t = concat_examples(batch, device=self.device[0], padding=(0, -100))\n                if self.device[0] == -1:\n                    l, n, c = self.model(x, t)\n                else:\n                    # apex does not support torch.nn.DataParallel\n                    l, n, c = data_parallel(self.model, (x, t), self.device)\n                loss += float(l.sum())\n                nll += float(n.sum())\n                count += int(c.sum())\n        self.model.train()\n        # report validation loss\n        observation = {}\n        with reporter.report_scope(observation):\n            reporter.report({""loss"": loss}, self.model.reporter)\n            reporter.report({""nll"": nll}, self.model.reporter)\n            reporter.report({""count"": count}, self.model.reporter)\n        return observation\n\n\ndef train(args):\n    """"""Train with the given args.\n\n    :param Namespace args: The program arguments\n    :param type model_class: LMInterface class for training\n    """"""\n    model_class = dynamic_import_lm(args.model_module, args.backend)\n    assert issubclass(model_class, LMInterface), ""model should implement LMInterface""\n    # display torch version\n    logging.info(""torch version = "" + torch.__version__)\n\n    set_deterministic_pytorch(args)\n\n    # check cuda and cudnn availability\n    if not torch.cuda.is_available():\n        logging.warning(""cuda is not available"")\n\n    # get special label ids\n    unk = args.char_list_dict[""<unk>""]\n    eos = args.char_list_dict[""<eos>""]\n    # read tokens as a sequence of sentences\n    val, n_val_tokens, n_val_oovs = load_dataset(\n        args.valid_label, args.char_list_dict, args.dump_hdf5_path\n    )\n    train, n_train_tokens, n_train_oovs = load_dataset(\n        args.train_label, args.char_list_dict, args.dump_hdf5_path\n    )\n    logging.info(""#vocab = "" + str(args.n_vocab))\n    logging.info(""#sentences in the training data = "" + str(len(train)))\n    logging.info(""#tokens in the training data = "" + str(n_train_tokens))\n    logging.info(\n        ""oov rate in the training data = %.2f %%""\n        % (n_train_oovs / n_train_tokens * 100)\n    )\n    logging.info(""#sentences in the validation data = "" + str(len(val)))\n    logging.info(""#tokens in the validation data = "" + str(n_val_tokens))\n    logging.info(\n        ""oov rate in the validation data = %.2f %%"" % (n_val_oovs / n_val_tokens * 100)\n    )\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n    # Create the dataset iterators\n    batch_size = args.batchsize * max(args.ngpu, 1)\n    if batch_size * args.accum_grad > args.batchsize:\n        logging.info(\n            f""batch size is automatically increased ""\n            f""({args.batchsize} -> {batch_size * args.accum_grad})""\n        )\n    train_iter = ParallelSentenceIterator(\n        train,\n        batch_size,\n        max_length=args.maxlen,\n        sos=eos,\n        eos=eos,\n        shuffle=not use_sortagrad,\n    )\n    val_iter = ParallelSentenceIterator(\n        val, batch_size, max_length=args.maxlen, sos=eos, eos=eos, repeat=False\n    )\n    epoch_iters = int(len(train_iter.batch_indices) / args.accum_grad)\n    logging.info(""#iterations per epoch = %d"" % epoch_iters)\n    logging.info(""#total iterations = "" + str(args.epoch * epoch_iters))\n    # Prepare an RNNLM model\n    if args.train_dtype in (""float16"", ""float32"", ""float64""):\n        dtype = getattr(torch, args.train_dtype)\n    else:\n        dtype = torch.float32\n    model = model_class(args.n_vocab, args).to(dtype=dtype)\n    if args.ngpu > 0:\n        model.to(""cuda"")\n        gpu_id = list(range(args.ngpu))\n    else:\n        gpu_id = [-1]\n\n    # Save model conf to json\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to "" + model_conf)\n        f.write(\n            json.dumps(vars(args), indent=4, ensure_ascii=False, sort_keys=True).encode(\n                ""utf_8""\n            )\n        )\n\n    # Set up an optimizer\n    opt_class = dynamic_import_optimizer(args.opt, args.backend)\n    optimizer = opt_class.from_args(model.parameters(), args)\n    if args.schedulers is None:\n        schedulers = []\n    else:\n        schedulers = [dynamic_import_scheduler(v)(k, args) for k, v in args.schedulers]\n\n    # setup apex.amp\n    if args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3""):\n        try:\n            from apex import amp\n        except ImportError as e:\n            logging.error(\n                f""You need to install apex for --train-dtype {args.train_dtype}. ""\n                ""See https://github.com/NVIDIA/apex#linux""\n            )\n            raise e\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.train_dtype)\n        use_apex = True\n    else:\n        use_apex = False\n\n    # FIXME: TOO DIRTY HACK\n    reporter = Reporter()\n    setattr(model, ""reporter"", reporter)\n    setattr(optimizer, ""target"", reporter)\n    setattr(optimizer, ""serialize"", lambda s: reporter.serialize(s))\n\n    updater = BPTTUpdater(\n        train_iter,\n        model,\n        optimizer,\n        schedulers,\n        gpu_id,\n        gradclip=args.gradclip,\n        use_apex=use_apex,\n        accum_grad=args.accum_grad,\n    )\n    trainer = training.Trainer(updater, (args.epoch, ""epoch""), out=args.outdir)\n    trainer.extend(LMEvaluator(val_iter, model, reporter, device=gpu_id))\n    trainer.extend(\n        extensions.LogReport(\n            postprocess=compute_perplexity,\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n    )\n    trainer.extend(\n        extensions.PrintReport(\n            [\n                ""epoch"",\n                ""iteration"",\n                ""main/loss"",\n                ""perplexity"",\n                ""val_perplexity"",\n                ""elapsed_time"",\n            ]\n        ),\n        trigger=(args.report_interval_iters, ""iteration""),\n    )\n    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))\n    # Save best models\n    trainer.extend(torch_snapshot(filename=""snapshot.ep.{.updater.epoch}""))\n    trainer.extend(snapshot_object(model, ""rnnlm.model.{.updater.epoch}""))\n    # T.Hori: MinValueTrigger should be used, but it fails when resuming\n    trainer.extend(MakeSymlinkToBestModel(""validation/main/loss"", ""rnnlm.model""))\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler([train_iter]),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epoch, ""epoch""),\n        )\n    if args.resume:\n        logging.info(""resumed from %s"" % args.resume)\n        torch_resume(args.resume, trainer)\n\n    set_early_stop(trainer, args, is_lm=True)\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        writer = SummaryWriter(args.tensorboard_dir)\n        trainer.extend(\n            TensorboardLogger(writer), trigger=(args.report_interval_iters, ""iteration"")\n        )\n\n    trainer.run()\n    check_early_stop(trainer, args.epoch)\n\n    # compute perplexity for test set\n    if args.test_label:\n        logging.info(""test the best model"")\n        torch_load(args.outdir + ""/rnnlm.model.best"", model)\n        test = read_tokens(args.test_label, args.char_list_dict)\n        n_test_tokens, n_test_oovs = count_tokens(test, unk)\n        logging.info(""#sentences in the test data = "" + str(len(test)))\n        logging.info(""#tokens in the test data = "" + str(n_test_tokens))\n        logging.info(\n            ""oov rate in the test data = %.2f %%"" % (n_test_oovs / n_test_tokens * 100)\n        )\n        test_iter = ParallelSentenceIterator(\n            test, batch_size, max_length=args.maxlen, sos=eos, eos=eos, repeat=False\n        )\n        evaluator = LMEvaluator(test_iter, model, reporter, device=gpu_id)\n        result = evaluator()\n        compute_perplexity(result)\n        logging.info(f""test perplexity: {result[\'perplexity\']}"")\n'"
espnet/mt/pytorch_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/mt/pytorch_backend/mt.py,18,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Training/decoding definition for the text translation task.""""""\n\nimport json\nimport logging\nimport os\nimport sys\n\nfrom chainer import training\nfrom chainer.training import extensions\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nimport torch\n\nfrom espnet.asr.asr_utils import adadelta_eps_decay\nfrom espnet.asr.asr_utils import adam_lr_decay\nfrom espnet.asr.asr_utils import add_results_to_json\nfrom espnet.asr.asr_utils import CompareValueTrigger\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import restore_snapshot\nfrom espnet.asr.asr_utils import snapshot_object\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.asr.asr_utils import torch_resume\nfrom espnet.asr.asr_utils import torch_snapshot\nimport espnet.lm.pytorch_backend.lm as lm_pytorch\nfrom espnet.nets.mt_interface import MTInterface\nfrom espnet.nets.pytorch_backend.e2e_asr import pad_list\nfrom espnet.utils.dataset import ChainerDataLoader\nfrom espnet.utils.dataset import TransformDataset\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.training.batchfy import make_batchset\nfrom espnet.utils.training.iterators import ShufflingEnabler\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\nfrom espnet.asr.pytorch_backend.asr import CustomEvaluator\nfrom espnet.asr.pytorch_backend.asr import CustomUpdater\nfrom espnet.asr.pytorch_backend.asr import load_trained_model\n\nimport matplotlib\n\nmatplotlib.use(""Agg"")\n\nif sys.version_info[0] == 2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest as zip_longest\n\n\nclass CustomConverter(object):\n    """"""Custom batch converter for Pytorch.""""""\n\n    def __init__(self):\n        """"""Construct a CustomConverter object.""""""\n        self.ignore_id = -1\n        self.pad = 0\n        # NOTE: we reserve index:0 for <pad> although this is reserved for a blank class\n        # in ASR. However,\n        # blank labels are not used in NMT. To keep the vocabulary size,\n        # we use index:0 for padding instead of adding one more class.\n\n    def __call__(self, batch, device=torch.device(""cpu"")):\n        """"""Transform a batch and send it to a device.\n\n        Args:\n            batch (list): The batch to transform.\n            device (torch.device): The device to send to.\n\n        Returns:\n            tuple(torch.Tensor, torch.Tensor, torch.Tensor)\n\n        """"""\n        # batch should be located in list\n        assert len(batch) == 1\n        xs, ys = batch[0]\n\n        # get batch of lengths of input sequences\n        ilens = np.array([x.shape[0] for x in xs])\n\n        # perform padding and convert to tensor\n        xs_pad = pad_list([torch.from_numpy(x).long() for x in xs], self.pad).to(device)\n        ilens = torch.from_numpy(ilens).to(device)\n        ys_pad = pad_list([torch.from_numpy(y).long() for y in ys], self.ignore_id).to(\n            device\n        )\n\n        return xs_pad, ilens, ys_pad\n\n\ndef train(args):\n    """"""Train with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n\n    # check cuda availability\n    if not torch.cuda.is_available():\n        logging.warning(""cuda is not available"")\n\n    # get input and output dimension info\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n    utts = list(valid_json.keys())\n    idim = int(valid_json[utts[0]][""output""][1][""shape""][1])\n    odim = int(valid_json[utts[0]][""output""][0][""shape""][1])\n    logging.info(""#input dims : "" + str(idim))\n    logging.info(""#output dims: "" + str(odim))\n\n    # specify model architecture\n    model_class = dynamic_import(args.model_module)\n    model = model_class(idim, odim, args)\n    assert isinstance(model, MTInterface)\n\n    if args.rnnlm is not None:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(args.char_list),\n                rnnlm_args.layer,\n                rnnlm_args.unit,\n                getattr(rnnlm_args, ""embed_unit"", None),  # for backward compatibility\n            )\n        )\n        torch_load(args.rnnlm, rnnlm)\n        model.rnnlm = rnnlm\n\n    # write model config\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to "" + model_conf)\n        f.write(\n            json.dumps(\n                (idim, odim, vars(args)), indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n    for key in sorted(vars(args).keys()):\n        logging.info(""ARGS: "" + key + "": "" + str(vars(args)[key]))\n\n    reporter = model.reporter\n\n    # check the use of multi-gpu\n    if args.ngpu > 1:\n        if args.batch_size != 0:\n            logging.warning(\n                ""batch size is automatically increased (%d -> %d)""\n                % (args.batch_size, args.batch_size * args.ngpu)\n            )\n            args.batch_size *= args.ngpu\n\n    # set torch device\n    device = torch.device(""cuda"" if args.ngpu > 0 else ""cpu"")\n    if args.train_dtype in (""float16"", ""float32"", ""float64""):\n        dtype = getattr(torch, args.train_dtype)\n    else:\n        dtype = torch.float32\n    model = model.to(device=device, dtype=dtype)\n\n    # Setup an optimizer\n    if args.opt == ""adadelta"":\n        optimizer = torch.optim.Adadelta(\n            model.parameters(), rho=0.95, eps=args.eps, weight_decay=args.weight_decay\n        )\n    elif args.opt == ""adam"":\n        optimizer = torch.optim.Adam(\n            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n        )\n    elif args.opt == ""noam"":\n        from espnet.nets.pytorch_backend.transformer.optimizer import get_std_opt\n\n        optimizer = get_std_opt(\n            model, args.adim, args.transformer_warmup_steps, args.transformer_lr\n        )\n    else:\n        raise NotImplementedError(""unknown optimizer: "" + args.opt)\n\n    # setup apex.amp\n    if args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3""):\n        try:\n            from apex import amp\n        except ImportError as e:\n            logging.error(\n                f""You need to install apex for --train-dtype {args.train_dtype}. ""\n                ""See https://github.com/NVIDIA/apex#linux""\n            )\n            raise e\n        if args.opt == ""noam"":\n            model, optimizer.optimizer = amp.initialize(\n                model, optimizer.optimizer, opt_level=args.train_dtype\n            )\n        else:\n            model, optimizer = amp.initialize(\n                model, optimizer, opt_level=args.train_dtype\n            )\n        use_apex = True\n    else:\n        use_apex = False\n\n    # FIXME: TOO DIRTY HACK\n    setattr(optimizer, ""target"", reporter)\n    setattr(optimizer, ""serialize"", lambda s: reporter.serialize(s))\n\n    # Setup a converter\n    converter = CustomConverter()\n\n    # read json data\n    with open(args.train_json, ""rb"") as f:\n        train_json = json.load(f)[""utts""]\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n    # make minibatch list (variable length)\n    train = make_batchset(\n        train_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        shortest_first=use_sortagrad,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        mt=True,\n        iaxis=1,\n        oaxis=0,\n    )\n    valid = make_batchset(\n        valid_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        mt=True,\n        iaxis=1,\n        oaxis=0,\n    )\n\n    load_tr = LoadInputsAndTargets(mode=""mt"", load_output=True)\n    load_cv = LoadInputsAndTargets(mode=""mt"", load_output=True)\n    # hack to make batchsize argument as 1\n    # actual bathsize is included in a list\n    # default collate function converts numpy array to pytorch tensor\n    # we used an empty collate function instead which returns list\n    train_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(train, lambda data: converter([load_tr(data)])),\n            batch_size=1,\n            num_workers=args.n_iter_processes,\n            shuffle=not use_sortagrad,\n            collate_fn=lambda x: x[0],\n        )\n    }\n    valid_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(valid, lambda data: converter([load_cv(data)])),\n            batch_size=1,\n            shuffle=False,\n            collate_fn=lambda x: x[0],\n            num_workers=args.n_iter_processes,\n        )\n    }\n\n    # Set up a trainer\n    updater = CustomUpdater(\n        model,\n        args.grad_clip,\n        train_iter,\n        optimizer,\n        device,\n        args.ngpu,\n        False,\n        args.accum_grad,\n        use_apex=use_apex,\n    )\n    trainer = training.Trainer(updater, (args.epochs, ""epoch""), out=args.outdir)\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler([train_iter]),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, ""epoch""),\n        )\n\n    # Resume from a snapshot\n    if args.resume:\n        logging.info(""resumed from %s"" % args.resume)\n        torch_resume(args.resume, trainer)\n\n    # Evaluate the model with the test dataset for each epoch\n    if args.save_interval_iters > 0:\n        trainer.extend(\n            CustomEvaluator(model, valid_iter, reporter, device, args.ngpu),\n            trigger=(args.save_interval_iters, ""iteration""),\n        )\n    else:\n        trainer.extend(CustomEvaluator(model, valid_iter, reporter, device, args.ngpu))\n\n    # Save attention weight each epoch\n    if args.num_save_attention > 0:\n        # NOTE: sort it by output lengths\n        data = sorted(\n            list(valid_json.items())[: args.num_save_attention],\n            key=lambda x: int(x[1][""output""][0][""shape""][0]),\n            reverse=True,\n        )\n        if hasattr(model, ""module""):\n            att_vis_fn = model.module.calculate_all_attentions\n            plot_class = model.module.attention_plot_class\n        else:\n            att_vis_fn = model.calculate_all_attentions\n            plot_class = model.attention_plot_class\n        att_reporter = plot_class(\n            att_vis_fn,\n            data,\n            args.outdir + ""/att_ws"",\n            converter=converter,\n            transform=load_cv,\n            device=device,\n            ikey=""output"",\n            iaxis=1,\n        )\n        trainer.extend(att_reporter, trigger=(1, ""epoch""))\n    else:\n        att_reporter = None\n\n    # Make a plot for training and validation values\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/loss"", ""validation/main/loss""], ""epoch"", file_name=""loss.png""\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/acc"", ""validation/main/acc""], ""epoch"", file_name=""acc.png""\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/ppl"", ""validation/main/ppl""], ""epoch"", file_name=""ppl.png""\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/bleu"", ""validation/main/bleu""], ""epoch"", file_name=""bleu.png""\n        )\n    )\n\n    # Save best models\n    trainer.extend(\n        snapshot_object(model, ""model.loss.best""),\n        trigger=training.triggers.MinValueTrigger(""validation/main/loss""),\n    )\n    trainer.extend(\n        snapshot_object(model, ""model.acc.best""),\n        trigger=training.triggers.MaxValueTrigger(""validation/main/acc""),\n    )\n\n    # save snapshot which contains model and optimizer states\n    if args.save_interval_iters > 0:\n        trainer.extend(\n            torch_snapshot(filename=""snapshot.iter.{.updater.iteration}""),\n            trigger=(args.save_interval_iters, ""iteration""),\n        )\n    else:\n        trainer.extend(torch_snapshot(), trigger=(1, ""epoch""))\n\n    # epsilon decay in the optimizer\n    if args.opt == ""adadelta"":\n        if args.criterion == ""acc"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.acc.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n        elif args.criterion == ""loss"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.loss.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n    elif args.opt == ""adam"":\n        if args.criterion == ""acc"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.acc.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n            trainer.extend(\n                adam_lr_decay(args.lr_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n        elif args.criterion == ""loss"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.loss.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n            trainer.extend(\n                adam_lr_decay(args.lr_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n\n    # Write a log of evaluation statistics for each epoch\n    trainer.extend(\n        extensions.LogReport(trigger=(args.report_interval_iters, ""iteration""))\n    )\n    report_keys = [\n        ""epoch"",\n        ""iteration"",\n        ""main/loss"",\n        ""validation/main/loss"",\n        ""main/acc"",\n        ""validation/main/acc"",\n        ""main/ppl"",\n        ""validation/main/ppl"",\n        ""elapsed_time"",\n    ]\n    if args.opt == ""adadelta"":\n        trainer.extend(\n            extensions.observe_value(\n                ""eps"",\n                lambda trainer: trainer.updater.get_optimizer(""main"").param_groups[0][\n                    ""eps""\n                ],\n            ),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n        report_keys.append(""eps"")\n    elif args.opt in [""adam"", ""noam""]:\n        trainer.extend(\n            extensions.observe_value(\n                ""lr"",\n                lambda trainer: trainer.updater.get_optimizer(""main"").param_groups[0][\n                    ""lr""\n                ],\n            ),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n        report_keys.append(""lr"")\n    if args.report_bleu:\n        report_keys.append(""main/bleu"")\n        report_keys.append(""validation/main/bleu"")\n    trainer.extend(\n        extensions.PrintReport(report_keys),\n        trigger=(args.report_interval_iters, ""iteration""),\n    )\n\n    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))\n    set_early_stop(trainer, args)\n\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        trainer.extend(\n            TensorboardLogger(SummaryWriter(args.tensorboard_dir), att_reporter),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n    # Run the training\n    trainer.run()\n    check_early_stop(trainer, args.epochs)\n\n\ndef trans(args):\n    """"""Decode with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n    model, train_args = load_trained_model(args.model)\n    assert isinstance(model, MTInterface)\n    model.trans_args = args\n\n    # read rnnlm\n    if args.rnnlm:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        if getattr(rnnlm_args, ""model_module"", ""default"") != ""default"":\n            raise ValueError(\n                ""use \'--api v2\' option to decode with non-default language model""\n            )\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit\n            )\n        )\n        torch_load(args.rnnlm, rnnlm)\n        rnnlm.eval()\n    else:\n        rnnlm = None\n\n    # gpu\n    if args.ngpu == 1:\n        gpu_id = list(range(args.ngpu))\n        logging.info(""gpu id: "" + str(gpu_id))\n        model.cuda()\n        if rnnlm:\n            rnnlm.cuda()\n\n    # read json data\n    with open(args.trans_json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n    new_js = {}\n\n    # remove enmpy utterances\n    if train_args.multilingual:\n        js = {\n            k: v\n            for k, v in js.items()\n            if v[""output""][0][""shape""][0] > 1 and v[""output""][1][""shape""][0] > 1\n        }\n    else:\n        js = {\n            k: v\n            for k, v in js.items()\n            if v[""output""][0][""shape""][0] > 0 and v[""output""][1][""shape""][0] > 0\n        }\n\n    if args.batchsize == 0:\n        with torch.no_grad():\n            for idx, name in enumerate(js.keys(), 1):\n                logging.info(""(%d/%d) decoding "" + name, idx, len(js.keys()))\n                feat = [js[name][""output""][1][""tokenid""].split()]\n                nbest_hyps = model.translate(feat, args, train_args.char_list, rnnlm)\n                new_js[name] = add_results_to_json(\n                    js[name], nbest_hyps, train_args.char_list\n                )\n\n    else:\n\n        def grouper(n, iterable, fillvalue=None):\n            kargs = [iter(iterable)] * n\n            return zip_longest(*kargs, fillvalue=fillvalue)\n\n        # sort data\n        keys = list(js.keys())\n        feat_lens = [js[key][""output""][1][""shape""][0] for key in keys]\n        sorted_index = sorted(range(len(feat_lens)), key=lambda i: -feat_lens[i])\n        keys = [keys[i] for i in sorted_index]\n\n        with torch.no_grad():\n            for names in grouper(args.batchsize, keys, None):\n                names = [name for name in names if name]\n                feats = [\n                    np.fromiter(\n                        map(int, js[name][""output""][1][""tokenid""].split()),\n                        dtype=np.int64,\n                    )\n                    for name in names\n                ]\n                nbest_hyps = model.translate_batch(\n                    feats, args, train_args.char_list, rnnlm=rnnlm\n                )\n\n                for i, nbest_hyp in enumerate(nbest_hyps):\n                    name = names[i]\n                    new_js[name] = add_results_to_json(\n                        js[name], nbest_hyp, train_args.char_list\n                    )\n\n    with open(args.result_label, ""wb"") as f:\n        f.write(\n            json.dumps(\n                {""utts"": new_js}, indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n'"
espnet/nets/chainer_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/chainer_backend/asr_interface.py,0,"b'""""""ASR Interface module.""""""\nimport chainer\n\nfrom espnet.nets.asr_interface import ASRInterface\n\n\nclass ChainerASRInterface(ASRInterface, chainer.Chain):\n    """"""ASR Interface for ESPnet model implementation.""""""\n\n    @staticmethod\n    def custom_converter(*args, **kw):\n        """"""Get customconverter of the model (Chainer only).""""""\n        raise NotImplementedError(""custom converter method is not implemented"")\n\n    @staticmethod\n    def custom_updater(*args, **kw):\n        """"""Get custom_updater of the model (Chainer only).""""""\n        raise NotImplementedError(""custom updater method is not implemented"")\n\n    @staticmethod\n    def custom_parallel_updater(*args, **kw):\n        """"""Get custom_parallel_updater of the model (Chainer only).""""""\n        raise NotImplementedError(""custom parallel updater method is not implemented"")\n'"
espnet/nets/chainer_backend/ctc.py,0,"b'import logging\n\nimport chainer\nfrom chainer import cuda\nimport chainer.functions as F\nimport chainer.links as L\nimport numpy as np\n\n\nclass CTC(chainer.Chain):\n    """"""Chainer implementation of ctc layer.\n\n    Args:\n        odim (int): The output dimension.\n        eprojs (int | None): Dimension of input vectors from encoder.\n        dropout_rate (float): Dropout rate.\n\n    """"""\n\n    def __init__(self, odim, eprojs, dropout_rate):\n        super(CTC, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.loss = None\n\n        with self.init_scope():\n            self.ctc_lo = L.Linear(eprojs, odim)\n\n    def __call__(self, hs, ys):\n        """"""CTC forward.\n\n        Args:\n            hs (list of chainer.Variable | N-dimension array):\n                Input variable from encoder.\n            ys (list of chainer.Variable | N-dimension array):\n                Input variable of decoder.\n\n        Returns:\n            chainer.Variable: A variable holding a scalar value of the CTC loss.\n\n        """"""\n        self.loss = None\n        ilens = [x.shape[0] for x in hs]\n        olens = [x.shape[0] for x in ys]\n\n        # zero padding for hs\n        y_hat = self.ctc_lo(\n            F.dropout(F.pad_sequence(hs), ratio=self.dropout_rate), n_batch_axes=2\n        )\n        y_hat = F.separate(y_hat, axis=1)  # ilen list of batch x hdim\n\n        # zero padding for ys\n        y_true = F.pad_sequence(ys, padding=-1)  # batch x olen\n\n        # get length info\n        input_length = chainer.Variable(self.xp.array(ilens, dtype=np.int32))\n        label_length = chainer.Variable(self.xp.array(olens, dtype=np.int32))\n        logging.info(\n            self.__class__.__name__ + "" input lengths:  "" + str(input_length.data)\n        )\n        logging.info(\n            self.__class__.__name__ + "" output lengths: "" + str(label_length.data)\n        )\n\n        # get ctc loss\n        self.loss = F.connectionist_temporal_classification(\n            y_hat, y_true, 0, input_length, label_length\n        )\n        logging.info(""ctc loss:"" + str(self.loss.data))\n\n        return self.loss\n\n    def log_softmax(self, hs):\n        """"""Log_softmax of frame activations.\n\n        Args:\n            hs (list of chainer.Variable | N-dimension array):\n                Input variable from encoder.\n\n        Returns:\n            chainer.Variable: A n-dimension float array.\n\n        """"""\n        y_hat = self.ctc_lo(F.pad_sequence(hs), n_batch_axes=2)\n        return F.log_softmax(y_hat.reshape(-1, y_hat.shape[-1])).reshape(y_hat.shape)\n\n\nclass WarpCTC(chainer.Chain):\n    """"""Chainer implementation of warp-ctc layer.\n\n    Args:\n        odim (int): The output dimension.\n        eproj (int | None): Dimension of input vector from encoder.\n        dropout_rate (float): Dropout rate.\n\n    """"""\n\n    def __init__(self, odim, eprojs, dropout_rate):\n        super(WarpCTC, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.loss = None\n\n        with self.init_scope():\n            self.ctc_lo = L.Linear(eprojs, odim)\n\n    def __call__(self, hs, ys):\n        """"""Core function of the Warp-CTC layer.\n\n        Args:\n            hs (iterable of chainer.Variable | N-dimention array):\n                Input variable from encoder.\n            ys (iterable of chainer.Variable | N-dimension array):\n                Input variable of decoder.\n\n        Returns:\n           chainer.Variable: A variable holding a scalar value of the CTC loss.\n\n        """"""\n        self.loss = None\n        ilens = [x.shape[0] for x in hs]\n        olens = [x.shape[0] for x in ys]\n\n        # zero padding for hs\n        y_hat = self.ctc_lo(\n            F.dropout(F.pad_sequence(hs), ratio=self.dropout_rate), n_batch_axes=2\n        )\n        y_hat = y_hat.transpose(1, 0, 2)  # batch x frames x hdim\n\n        # get length info\n        logging.info(self.__class__.__name__ + "" input lengths:  "" + str(ilens))\n        logging.info(self.__class__.__name__ + "" output lengths: "" + str(olens))\n\n        # get ctc loss\n        from chainer_ctc.warpctc import ctc as warp_ctc\n\n        self.loss = warp_ctc(y_hat, ilens, [cuda.to_cpu(y.data) for y in ys])[0]\n        logging.info(""ctc loss:"" + str(self.loss.data))\n\n        return self.loss\n\n    def log_softmax(self, hs):\n        """"""Log_softmax of frame activations.\n\n        Args:\n            hs (list of chainer.Variable | N-dimension array):\n                Input variable from encoder.\n\n        Returns:\n            chainer.Variable: A n-dimension float array.\n\n        """"""\n        y_hat = self.ctc_lo(F.pad_sequence(hs), n_batch_axes=2)\n        return F.log_softmax(y_hat.reshape(-1, y_hat.shape[-1])).reshape(y_hat.shape)\n\n    def argmax(self, hs_pad):\n        """"""argmax of frame activations\n\n        :param chainer variable hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: argmax applied 2d tensor (B, Tmax)\n        :rtype: chainer.Variable\n        """"""\n        return F.argmax(self.ctc_lo(F.pad_sequence(hs_pad), n_batch_axes=2), axis=-1)\n\n\ndef ctc_for(args, odim):\n    """"""Return the CTC layer corresponding to the args.\n\n    Args:\n        args (Namespace): The program arguments.\n        odim (int): The output dimension.\n\n    Returns:\n        The CTC module.\n\n    """"""\n    ctc_type = args.ctc_type\n    if ctc_type == ""builtin"":\n        logging.info(""Using chainer CTC implementation"")\n        ctc = CTC(odim, args.eprojs, args.dropout_rate)\n    elif ctc_type == ""warpctc"":\n        logging.info(""Using warpctc CTC implementation"")\n        ctc = WarpCTC(odim, args.eprojs, args.dropout_rate)\n    else:\n        raise ValueError(\'ctc_type must be ""builtin"" or ""warpctc"": {}\'.format(ctc_type))\n    return ctc\n'"
espnet/nets/chainer_backend/deterministic_embed_id.py,0,"b'import numpy\nimport six\n\nimport chainer\nfrom chainer import cuda\nfrom chainer import function_node\nfrom chainer.initializers import normal\n\n# from chainer.functions.connection import embed_id\nfrom chainer import link\nfrom chainer.utils import type_check\nfrom chainer import variable\n\n""""""Deterministic EmbedID link and function\n\n   copied from chainer/links/connection/embed_id.py\n   and chainer/functions/connection/embed_id.py,\n   and modified not to use atomicAdd operation\n""""""\n\n\nclass EmbedIDFunction(function_node.FunctionNode):\n    def __init__(self, ignore_label=None):\n        self.ignore_label = ignore_label\n        self._w_shape = None\n\n    def check_type_forward(self, in_types):\n        type_check.expect(in_types.size() == 2)\n        x_type, w_type = in_types\n        type_check.expect(\n            x_type.dtype.kind == ""i"", x_type.ndim >= 1,\n        )\n        type_check.expect(w_type.dtype == numpy.float32, w_type.ndim == 2)\n\n    def forward(self, inputs):\n        self.retain_inputs((0,))\n        x, W = inputs\n        self._w_shape = W.shape\n\n        if not type_check.same_types(*inputs):\n            raise ValueError(\n                ""numpy and cupy must not be used together\\n""\n                ""type(W): {0}, type(x): {1}"".format(type(W), type(x))\n            )\n\n        xp = cuda.get_array_module(*inputs)\n        if chainer.is_debug():\n            valid_x = xp.logical_and(0 <= x, x < len(W))\n            if self.ignore_label is not None:\n                valid_x = xp.logical_or(valid_x, x == self.ignore_label)\n            if not valid_x.all():\n                raise ValueError(\n                    ""Each not ignored `x` value need to satisfy"" ""`0 <= x < len(W)`""\n                )\n\n        if self.ignore_label is not None:\n            mask = x == self.ignore_label\n            return (xp.where(mask[..., None], 0, W[xp.where(mask, 0, x)]),)\n\n        return (W[x],)\n\n    def backward(self, indexes, grad_outputs):\n        inputs = self.get_retained_inputs()\n        gW = EmbedIDGrad(self._w_shape, self.ignore_label).apply(inputs + grad_outputs)[\n            0\n        ]\n        return None, gW\n\n\nclass EmbedIDGrad(function_node.FunctionNode):\n    def __init__(self, w_shape, ignore_label=None):\n        self.w_shape = w_shape\n        self.ignore_label = ignore_label\n        self._gy_shape = None\n\n    def forward(self, inputs):\n        self.retain_inputs((0,))\n        xp = cuda.get_array_module(*inputs)\n        x, gy = inputs\n        self._gy_shape = gy.shape\n        gW = xp.zeros(self.w_shape, dtype=gy.dtype)\n\n        if xp is numpy:\n            # It is equivalent to `numpy.add.at(gW, x, gy)` but ufunc.at is\n            # too slow.\n            for ix, igy in six.moves.zip(x.ravel(), gy.reshape(x.size, -1)):\n                if ix == self.ignore_label:\n                    continue\n                gW[ix] += igy\n        else:\n            """"""\n            # original code based on cuda elementwise method\n            if self.ignore_label is None:\n                cuda.elementwise(\n                    \'T gy, S x, S n_out\', \'raw T gW\',\n                    \'ptrdiff_t w_ind[] = {x, i % n_out};\'\n                    \'atomicAdd(&gW[w_ind], gy)\',\n                    \'embed_id_bwd\')(\n                        gy, xp.expand_dims(x, -1), gW.shape[1], gW)\n            else:\n                cuda.elementwise(\n                    \'T gy, S x, S n_out, S ignore\', \'raw T gW\',\n                    \'\'\'\n                    if (x != ignore) {\n                      ptrdiff_t w_ind[] = {x, i % n_out};\n                      atomicAdd(&gW[w_ind], gy);\n                    }\n                    \'\'\',\n                    \'embed_id_bwd_ignore_label\')(\n                        gy, xp.expand_dims(x, -1), gW.shape[1],\n                        self.ignore_label, gW)\n            """"""\n            # EmbedID gradient alternative without atomicAdd, which simply\n            # creates a one-hot vector and applies dot product\n            xi = xp.zeros((x.size, len(gW)), dtype=numpy.float32)\n            idx = xp.arange(x.size, dtype=numpy.int32) * len(gW) + x.ravel()\n            xi.ravel()[idx] = 1.0\n            if self.ignore_label is not None:\n                xi[:, self.ignore_label] = 0.0\n            gW = xi.T.dot(gy.reshape(x.size, -1)).astype(gW.dtype, copy=False)\n\n        return (gW,)\n\n    def backward(self, indexes, grads):\n        xp = cuda.get_array_module(*grads)\n        x = self.get_retained_inputs()[0].data\n        ggW = grads[0]\n\n        if self.ignore_label is not None:\n            mask = x == self.ignore_label\n            # To prevent index out of bounds, we need to check if ignore_label\n            # is inside of W.\n            if not (0 <= self.ignore_label < self.w_shape[1]):\n                x = xp.where(mask, 0, x)\n\n        ggy = ggW[x]\n\n        if self.ignore_label is not None:\n            mask, zero, _ = xp.broadcast_arrays(\n                mask[..., None], xp.zeros((), ""f""), ggy.data\n            )\n            ggy = chainer.functions.where(mask, zero, ggy)\n        return None, ggy\n\n\ndef embed_id(x, W, ignore_label=None):\n    r""""""Efficient linear function for one-hot input.\n\n    This function implements so called *word embeddings*. It takes two\n    arguments: a set of IDs (words) ``x`` in :math:`B` dimensional integer\n    vector, and a set of all ID (word) embeddings ``W`` in :math:`V \\\\times d`\n    float32 matrix. It outputs :math:`B \\\\times d` matrix whose ``i``-th\n    column is the ``x[i]``-th column of ``W``.\n    This function is only differentiable on the input ``W``.\n\n    Args:\n        x (chainer.Variable | np.ndarray): Batch vectors of IDs. Each\n            element must be signed integer.\n        W (chainer.Variable | np.ndarray): Distributed representation\n            of each ID (a.k.a. word embeddings).\n        ignore_label (int): If ignore_label is an int value, i-th column\n            of return value is filled with 0.\n\n    Returns:\n        chainer.Variable: Embedded variable.\n\n\n    .. rubric:: :class:`~chainer.links.EmbedID`\n\n    Examples:\n\n        >>> x = np.array([2, 1]).astype(\'i\')\n        >>> x\n        array([2, 1], dtype=int32)\n        >>> W = np.array([[0, 0, 0],\n        ...               [1, 1, 1],\n        ...               [2, 2, 2]]).astype(\'f\')\n        >>> W\n        array([[ 0.,  0.,  0.],\n               [ 1.,  1.,  1.],\n               [ 2.,  2.,  2.]], dtype=float32)\n        >>> F.embed_id(x, W).data\n        array([[ 2.,  2.,  2.],\n               [ 1.,  1.,  1.]], dtype=float32)\n        >>> F.embed_id(x, W, ignore_label=1).data\n        array([[ 2.,  2.,  2.],\n               [ 0.,  0.,  0.]], dtype=float32)\n\n    """"""\n    return EmbedIDFunction(ignore_label=ignore_label).apply((x, W))[0]\n\n\nclass EmbedID(link.Link):\n    """"""Efficient linear layer for one-hot input.\n\n    This is a link that wraps the :func:`~chainer.functions.embed_id` function.\n    This link holds the ID (word) embedding matrix ``W`` as a parameter.\n\n    Args:\n        in_size (int): Number of different identifiers (a.k.a. vocabulary size).\n        out_size (int): Output dimension.\n        initialW (Initializer): Initializer to initialize the weight.\n        ignore_label (int): If `ignore_label` is an int value, i-th column of\n            return value is filled with 0.\n\n    .. rubric:: :func:`~chainer.functions.embed_id`\n\n    Attributes:\n        W (~chainer.Variable): Embedding parameter matrix.\n\n    Examples:\n\n        >>> W = np.array([[0, 0, 0],\n        ...               [1, 1, 1],\n        ...               [2, 2, 2]]).astype(\'f\')\n        >>> W\n        array([[ 0.,  0.,  0.],\n               [ 1.,  1.,  1.],\n               [ 2.,  2.,  2.]], dtype=float32)\n        >>> l = L.EmbedID(W.shape[0], W.shape[1], initialW=W)\n        >>> x = np.array([2, 1]).astype(\'i\')\n        >>> x\n        array([2, 1], dtype=int32)\n        >>> y = l(x)\n        >>> y.data\n        array([[ 2.,  2.,  2.],\n               [ 1.,  1.,  1.]], dtype=float32)\n\n    """"""\n\n    ignore_label = None\n\n    def __init__(self, in_size, out_size, initialW=None, ignore_label=None):\n        super(EmbedID, self).__init__()\n        self.ignore_label = ignore_label\n\n        with self.init_scope():\n            if initialW is None:\n                initialW = normal.Normal(1.0)\n            self.W = variable.Parameter(initialW, (in_size, out_size))\n\n    def __call__(self, x):\n        """"""Extracts the word embedding of given IDs.\n\n        Args:\n            x (chainer.Variable): Batch vectors of IDs.\n\n        Returns:\n            chainer.Variable: Batch of corresponding embeddings.\n\n        """"""\n        return embed_id(x, self.W, ignore_label=self.ignore_label)\n'"
espnet/nets/chainer_backend/e2e_asr.py,1,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""RNN sequence-to-sequence speech recognition model (chainer).""""""\n\nimport logging\nimport math\n\nimport chainer\nfrom chainer import reporter\nimport numpy as np\n\nfrom espnet.nets.chainer_backend.asr_interface import ChainerASRInterface\nfrom espnet.nets.chainer_backend.ctc import ctc_for\nfrom espnet.nets.chainer_backend.rnn.attentions import att_for\nfrom espnet.nets.chainer_backend.rnn.decoders import decoder_for\nfrom espnet.nets.chainer_backend.rnn.encoders import encoder_for\nfrom espnet.nets.e2e_asr_common import label_smoothing_dist\nfrom espnet.nets.pytorch_backend.e2e_asr import E2E as E2E_pytorch\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\n\nCTC_LOSS_THRESHOLD = 10000\n\n\nclass E2E(ChainerASRInterface):\n    """"""E2E module for chainer backend.\n\n    Args:\n        idim (int): Dimension of the inputs.\n        odim (int): Dimension of the outputs.\n        args (parser.args): Training config.\n        flag_return (bool): If True, train() would return\n            additional metrics in addition to the training\n            loss.\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        return E2E_pytorch.add_arguments(parser)\n\n    def __init__(self, idim, odim, args, flag_return=True):\n        """"""Construct an E2E object.\n\n        :param int idim: dimension of inputs\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        """"""\n        chainer.Chain.__init__(self)\n        self.mtlalpha = args.mtlalpha\n        assert 0 <= self.mtlalpha <= 1, ""mtlalpha must be [0,1]""\n        self.etype = args.etype\n        self.verbose = args.verbose\n        self.char_list = args.char_list\n        self.outdir = args.outdir\n\n        # below means the last number becomes eos/sos ID\n        # note that sos/eos IDs are identical\n        self.sos = odim - 1\n        self.eos = odim - 1\n\n        # subsample info\n        self.subsample = get_subsample(args, mode=""asr"", arch=""rnn"")\n\n        # label smoothing info\n        if args.lsm_type:\n            logging.info(""Use label smoothing with "" + args.lsm_type)\n            labeldist = label_smoothing_dist(\n                odim, args.lsm_type, transcript=args.train_json\n            )\n        else:\n            labeldist = None\n\n        with self.init_scope():\n            # encoder\n            self.enc = encoder_for(args, idim, self.subsample)\n            # ctc\n            self.ctc = ctc_for(args, odim)\n            # attention\n            self.att = att_for(args)\n            # decoder\n            self.dec = decoder_for(args, odim, self.sos, self.eos, self.att, labeldist)\n\n        self.acc = None\n        self.loss = None\n        self.flag_return = flag_return\n\n    def forward(self, xs, ilens, ys):\n        """"""E2E forward propagation.\n\n        Args:\n            xs (chainer.Variable): Batch of padded charactor ids. (B, Tmax)\n            ilens (chainer.Variable): Batch of length of each input batch. (B,)\n            ys (chainer.Variable): Batch of padded target features. (B, Lmax, odim)\n\n        Returns:\n            float: Loss that calculated by attention and ctc loss.\n            float (optional): Ctc loss.\n            float (optional): Attention loss.\n            float (optional): Accuracy.\n\n        """"""\n        # 1. encoder\n        hs, ilens = self.enc(xs, ilens)\n\n        # 3. CTC loss\n        if self.mtlalpha == 0:\n            loss_ctc = None\n        else:\n            loss_ctc = self.ctc(hs, ys)\n\n        # 4. attention loss\n        if self.mtlalpha == 1:\n            loss_att = None\n            acc = None\n        else:\n            loss_att, acc = self.dec(hs, ys)\n\n        self.acc = acc\n        alpha = self.mtlalpha\n        if alpha == 0:\n            self.loss = loss_att\n        elif alpha == 1:\n            self.loss = loss_ctc\n        else:\n            self.loss = alpha * loss_ctc + (1 - alpha) * loss_att\n\n        if self.loss.data < CTC_LOSS_THRESHOLD and not math.isnan(self.loss.data):\n            reporter.report({""loss_ctc"": loss_ctc}, self)\n            reporter.report({""loss_att"": loss_att}, self)\n            reporter.report({""acc"": acc}, self)\n\n            logging.info(""mtl loss:"" + str(self.loss.data))\n            reporter.report({""loss"": self.loss}, self)\n        else:\n            logging.warning(""loss (=%f) is not correct"", self.loss.data)\n        if self.flag_return:\n            return self.loss, loss_ctc, loss_att, acc\n        else:\n            return self.loss\n\n    def recognize(self, x, recog_args, char_list, rnnlm=None):\n        """"""E2E greedy/beam search.\n\n        Args:\n            x (chainer.Variable): Input tensor for recognition.\n            recog_args (parser.args): Arguments of config file.\n            char_list (List[str]): List of Charactors.\n            rnnlm (Module): RNNLM module defined at `espnet.lm.chainer_backend.lm`.\n\n        Returns:\n            List[Dict[str, Any]]: Result of recognition.\n\n        """"""\n        # subsample frame\n        x = x[:: self.subsample[0], :]\n        ilen = self.xp.array(x.shape[0], dtype=np.int32)\n        h = chainer.Variable(self.xp.array(x, dtype=np.float32))\n\n        with chainer.no_backprop_mode(), chainer.using_config(""train"", False):\n            # 1. encoder\n            # make a utt list (1) to use the same interface for encoder\n            h, _ = self.enc([h], [ilen])\n\n            # calculate log P(z_t|X) for CTC scores\n            if recog_args.ctc_weight > 0.0:\n                lpz = self.ctc.log_softmax(h).data[0]\n            else:\n                lpz = None\n\n            # 2. decoder\n            # decode the first utterance\n            y = self.dec.recognize_beam(h[0], lpz, recog_args, char_list, rnnlm)\n\n            return y\n\n    def calculate_all_attentions(self, xs, ilens, ys):\n        """"""E2E attention calculation.\n\n        Args:\n            xs (List): List of padded input sequences. [(T1, idim), (T2, idim), ...]\n            ilens (np.ndarray): Batch of lengths of input sequences. (B)\n            ys (List): List of character id sequence tensor. [(L1), (L2), (L3), ...]\n\n        Returns:\n            float np.ndarray: Attention weights. (B, Lmax, Tmax)\n\n        """"""\n        hs, ilens = self.enc(xs, ilens)\n        att_ws = self.dec.calculate_all_attentions(hs, ys)\n\n        return att_ws\n\n    @staticmethod\n    def custom_converter(subsampling_factor=0):\n        """"""Get customconverter of the model.""""""\n        from espnet.nets.chainer_backend.rnn.training import CustomConverter\n\n        return CustomConverter(subsampling_factor=subsampling_factor)\n\n    @staticmethod\n    def custom_updater(iters, optimizer, converter, device=-1, accum_grad=1):\n        """"""Get custom_updater of the model.""""""\n        from espnet.nets.chainer_backend.rnn.training import CustomUpdater\n\n        return CustomUpdater(\n            iters, optimizer, converter=converter, device=device, accum_grad=accum_grad\n        )\n\n    @staticmethod\n    def custom_parallel_updater(iters, optimizer, converter, devices, accum_grad=1):\n        """"""Get custom_parallel_updater of the model.""""""\n        from espnet.nets.chainer_backend.rnn.training import CustomParallelUpdater\n\n        return CustomParallelUpdater(\n            iters,\n            optimizer,\n            converter=converter,\n            devices=devices,\n            accum_grad=accum_grad,\n        )\n'"
espnet/nets/chainer_backend/e2e_asr_transformer.py,0,"b'# encoding: utf-8\n""""""Transformer-based model for End-to-end ASR.""""""\n\nfrom argparse import Namespace\nfrom distutils.util import strtobool\nimport logging\nimport math\n\nimport chainer\nimport chainer.functions as F\nfrom chainer import reporter\nimport numpy as np\nimport six\n\nfrom espnet.nets.chainer_backend.asr_interface import ChainerASRInterface\nfrom espnet.nets.chainer_backend.transformer.attention import MultiHeadAttention\nfrom espnet.nets.chainer_backend.transformer import ctc\nfrom espnet.nets.chainer_backend.transformer.decoder import Decoder\nfrom espnet.nets.chainer_backend.transformer.encoder import Encoder\nfrom espnet.nets.chainer_backend.transformer.label_smoothing_loss import (\n    LabelSmoothingLoss,  # noqa: H301\n)\nfrom espnet.nets.chainer_backend.transformer.plot import PlotAttentionReport\nfrom espnet.nets.chainer_backend.transformer.training import CustomConverter\nfrom espnet.nets.chainer_backend.transformer.training import CustomUpdater\nfrom espnet.nets.chainer_backend.transformer.training import (\n    CustomParallelUpdater,  # noqa: H301\n)\nfrom espnet.nets.ctc_prefix_score import CTCPrefixScore\nfrom espnet.nets.e2e_asr_common import end_detect\nfrom espnet.nets.e2e_asr_common import ErrorCalculator\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\n\nCTC_SCORING_RATIO = 1.5\nMAX_DECODER_OUTPUT = 5\n\n\nclass E2E(ChainerASRInterface):\n    """"""E2E module.\n\n    Args:\n        idim (int): Input dimmensions.\n        odim (int): Output dimmensions.\n        args (Namespace): Training config.\n        ignore_id (int, optional): Id for ignoring a character.\n        flag_return (bool, optional): If true, return a list with (loss,\n        loss_ctc, loss_att, acc) in forward. Otherwise, return loss.\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Customize flags for transformer setup.\n\n        Args:\n            parser (Namespace): Training config.\n\n        """"""\n        group = parser.add_argument_group(""transformer model setting"")\n        group.add_argument(\n            ""--transformer-init"",\n            type=str,\n            default=""pytorch"",\n            help=""how to initialize transformer parameters"",\n        )\n        group.add_argument(\n            ""--transformer-input-layer"",\n            type=str,\n            default=""conv2d"",\n            choices=[""conv2d"", ""linear"", ""embed""],\n            help=""transformer input layer type"",\n        )\n        group.add_argument(\n            ""--transformer-attn-dropout-rate"",\n            default=None,\n            type=float,\n            help=""dropout in transformer attention. use --dropout-rate if None is set"",\n        )\n        group.add_argument(\n            ""--transformer-lr"",\n            default=10.0,\n            type=float,\n            help=""Initial value of learning rate"",\n        )\n        group.add_argument(\n            ""--transformer-warmup-steps"",\n            default=25000,\n            type=int,\n            help=""optimizer warmup steps"",\n        )\n        group.add_argument(\n            ""--transformer-length-normalized-loss"",\n            default=True,\n            type=strtobool,\n            help=""normalize loss by length"",\n        )\n\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        # Encoder\n        group.add_argument(\n            ""--elayers"",\n            default=4,\n            type=int,\n            help=""Number of encoder layers (for shared recognition part ""\n            ""in multi-speaker asr mode)"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=300,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        # Attention\n        group.add_argument(\n            ""--adim"",\n            default=320,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        # Decoder\n        group.add_argument(\n            ""--dlayers"", default=1, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=320, type=int, help=""Number of decoder hidden units""\n        )\n        return parser\n\n    def __init__(self, idim, odim, args, ignore_id=-1, flag_return=True):\n        """"""Initialize the transformer.""""""\n        chainer.Chain.__init__(self)\n        self.mtlalpha = args.mtlalpha\n        assert 0 <= self.mtlalpha <= 1, ""mtlalpha must be [0,1]""\n        if args.transformer_attn_dropout_rate is None:\n            args.transformer_attn_dropout_rate = args.dropout_rate\n        self.use_label_smoothing = False\n        self.char_list = args.char_list\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n        self.scale_emb = args.adim ** 0.5\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.subsample = get_subsample(args, mode=""asr"", arch=""transformer"")\n        self.ignore_id = ignore_id\n        self.reset_parameters(args)\n        with self.init_scope():\n            self.encoder = Encoder(\n                idim=idim,\n                attention_dim=args.adim,\n                attention_heads=args.aheads,\n                linear_units=args.eunits,\n                input_layer=args.transformer_input_layer,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                attention_dropout_rate=args.transformer_attn_dropout_rate,\n                initialW=self.initialW,\n                initial_bias=self.initialB,\n            )\n            self.decoder = Decoder(\n                odim, args, initialW=self.initialW, initial_bias=self.initialB\n            )\n            self.criterion = LabelSmoothingLoss(\n                args.lsm_weight,\n                len(args.char_list),\n                args.transformer_length_normalized_loss,\n            )\n            if args.mtlalpha > 0.0:\n                if args.ctc_type == ""builtin"":\n                    logging.info(""Using chainer CTC implementation"")\n                    self.ctc = ctc.CTC(odim, args.adim, args.dropout_rate)\n                elif args.ctc_type == ""warpctc"":\n                    logging.info(""Using warpctc CTC implementation"")\n                    self.ctc = ctc.WarpCTC(odim, args.adim, args.dropout_rate)\n                else:\n                    raise ValueError(\n                        \'ctc_type must be ""builtin"" or ""warpctc"": {}\'.format(\n                            args.ctc_type\n                        )\n                    )\n            else:\n                self.ctc = None\n        self.dims = args.adim\n        self.odim = odim\n        self.flag_return = flag_return\n        if args.report_cer or args.report_wer:\n            self.error_calculator = ErrorCalculator(\n                args.char_list,\n                args.sym_space,\n                args.sym_blank,\n                args.report_cer,\n                args.report_wer,\n            )\n        else:\n            self.error_calculator = None\n        if ""Namespace"" in str(type(args)):\n            self.verbose = 0 if ""verbose"" not in args else args.verbose\n        else:\n            self.verbose = 0 if args.verbose is None else args.verbose\n\n    def reset_parameters(self, args):\n        """"""Initialize the Weight according to the give initialize-type.\n\n        Args:\n            args (Namespace): Transformer config.\n\n        """"""\n        type_init = args.transformer_init\n        if type_init == ""lecun_uniform"":\n            logging.info(""Using LeCunUniform as Parameter initializer"")\n            self.initialW = chainer.initializers.LeCunUniform\n        elif type_init == ""lecun_normal"":\n            logging.info(""Using LeCunNormal as Parameter initializer"")\n            self.initialW = chainer.initializers.LeCunNormal\n        elif type_init == ""gorot_uniform"":\n            logging.info(""Using GlorotUniform as Parameter initializer"")\n            self.initialW = chainer.initializers.GlorotUniform\n        elif type_init == ""gorot_normal"":\n            logging.info(""Using GlorotNormal as Parameter initializer"")\n            self.initialW = chainer.initializers.GlorotNormal\n        elif type_init == ""he_uniform"":\n            logging.info(""Using HeUniform as Parameter initializer"")\n            self.initialW = chainer.initializers.HeUniform\n        elif type_init == ""he_normal"":\n            logging.info(""Using HeNormal as Parameter initializer"")\n            self.initialW = chainer.initializers.HeNormal\n        elif type_init == ""pytorch"":\n            logging.info(""Using Pytorch initializer"")\n            self.initialW = chainer.initializers.Uniform\n        else:\n            logging.info(""Using Chainer default as Parameter initializer"")\n            self.initialW = chainer.initializers.Uniform\n        self.initialB = chainer.initializers.Uniform\n\n    def forward(self, xs, ilens, ys_pad, calculate_attentions=False):\n        """"""E2E forward propagation.\n\n        Args:\n            xs (chainer.Variable): Batch of padded charactor ids. (B, Tmax)\n            ilens (chainer.Variable): Batch of length of each input batch. (B,)\n            ys (chainer.Variable): Batch of padded target features. (B, Lmax, odim)\n            calculate_attentions (bool): If true, return value is the output of encoder.\n\n        Returns:\n            float: Training loss.\n            float (optional): Training loss for ctc.\n            float (optional): Training loss for attention.\n            float (optional): Accuracy.\n            chainer.Variable (Optional): Output of the encoder.\n\n        """"""\n        alpha = self.mtlalpha\n\n        # 1. Encoder\n        xs, x_mask, ilens = self.encoder(xs, ilens)\n\n        # 2. CTC loss\n        cer_ctc = None\n        if alpha == 0.0:\n            loss_ctc = None\n        else:\n            _ys = [y.astype(np.int32) for y in ys_pad]\n            loss_ctc = self.ctc(xs, _ys)\n            if self.error_calculator is not None:\n                with chainer.no_backprop_mode():\n                    ys_hat = chainer.backends.cuda.to_cpu(self.ctc.argmax(xs).data)\n                cer_ctc = self.error_calculator(ys_hat, ys_pad, is_ctc=True)\n\n        # 3. Decoder\n        if calculate_attentions:\n            self.calculate_attentions(xs, x_mask, ys_pad)\n        ys = self.decoder(ys_pad, xs, x_mask)\n\n        # 4. Attention Loss\n        cer, wer = None, None\n        if alpha == 1:\n            loss_att = None\n            acc = None\n        else:\n            # Make target\n            eos = np.array([self.eos], ""i"")\n            with chainer.no_backprop_mode():\n                ys_pad_out = [np.concatenate([y, eos], axis=0) for y in ys_pad]\n                ys_pad_out = F.pad_sequence(ys_pad_out, padding=-1).data\n                ys_pad_out = self.xp.array(ys_pad_out)\n\n            loss_att = self.criterion(ys, ys_pad_out)\n            acc = F.accuracy(\n                ys.reshape(-1, self.odim), ys_pad_out.reshape(-1), ignore_label=-1\n            )\n            if (not chainer.config.train) and (self.error_calculator is not None):\n                cer, wer = self.error_calculator(ys, ys_pad)\n\n        if alpha == 0.0:\n            self.loss = loss_att\n            loss_att_data = loss_att.data\n            loss_ctc_data = None\n        elif alpha == 1.0:\n            self.loss = loss_ctc\n            loss_att_data = None\n            loss_ctc_data = loss_ctc.data\n        else:\n            self.loss = alpha * loss_ctc + (1 - alpha) * loss_att\n            loss_att_data = loss_att.data\n            loss_ctc_data = loss_ctc.data\n        loss_data = self.loss.data\n\n        if not math.isnan(loss_data):\n            reporter.report({""loss_ctc"": loss_ctc_data}, self)\n            reporter.report({""loss_att"": loss_att_data}, self)\n            reporter.report({""acc"": acc}, self)\n\n            reporter.report({""cer_ctc"": cer_ctc}, self)\n            reporter.report({""cer"": cer}, self)\n            reporter.report({""wer"": wer}, self)\n\n            logging.info(""mtl loss:"" + str(loss_data))\n            reporter.report({""loss"": loss_data}, self)\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n\n        if self.flag_return:\n            loss_ctc = None\n            return self.loss, loss_ctc, loss_att, acc\n        else:\n            return self.loss\n\n    def calculate_attentions(self, xs, x_mask, ys_pad):\n        """"""Calculate Attentions.""""""\n        self.decoder(ys_pad, xs, x_mask)\n\n    def recognize(self, x_block, recog_args, char_list=None, rnnlm=None):\n        """"""E2E recognition function.\n\n        Args:\n            x (ndarray): Input acouctic feature (B, T, D) or (T, D).\n            recog_args (Namespace): Argment namespace contraining options.\n            char_list (List[str]): List of characters.\n            rnnlm (chainer.Chain): Language model module defined at\n            `espnet.lm.chainer_backend.lm`.\n\n        Returns:\n            List: N-best decoding results.\n\n        """"""\n        with chainer.no_backprop_mode(), chainer.using_config(""train"", False):\n            # 1. encoder\n            ilens = [x_block.shape[0]]\n            batch = len(ilens)\n            xs, _, _ = self.encoder(x_block[None, :, :], ilens)\n\n            # calculate log P(z_t|X) for CTC scores\n            if recog_args.ctc_weight > 0.0:\n                lpz = self.ctc.log_softmax(xs.reshape(batch, -1, self.dims)).data[0]\n            else:\n                lpz = None\n            # 2. decoder\n            if recog_args.lm_weight == 0.0:\n                rnnlm = None\n            y = self.recognize_beam(xs, lpz, recog_args, char_list, rnnlm)\n\n        return y\n\n    def recognize_beam(self, h, lpz, recog_args, char_list=None, rnnlm=None):\n        """"""E2E beam search.\n\n        Args:\n            h (ndarray): Encoder ouput features (B, T, D) or (T, D).\n            lpz (ndarray): Log probabilities from CTC.\n            recog_args (Namespace): Argment namespace contraining options.\n            char_list (List[str]): List of characters.\n            rnnlm (chainer.Chain): Language model module defined at\n            `espnet.lm.chainer_backend.lm`.\n\n        Returns:\n            List: N-best decoding results.\n\n        """"""\n        logging.info(""input lengths: "" + str(h.shape[1]))\n\n        # initialization\n        n_len = h.shape[1]\n        xp = self.xp\n        h_mask = xp.ones((1, n_len))\n\n        # search parms\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = recog_args.ctc_weight\n\n        # prepare sos\n        y = self.sos\n        if recog_args.maxlenratio == 0:\n            maxlen = n_len\n        else:\n            maxlen = max(1, int(recog_args.maxlenratio * n_len))\n        minlen = int(recog_args.minlenratio * n_len)\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {""score"": 0.0, ""yseq"": [y], ""rnnlm_prev"": None}\n        else:\n            hyp = {""score"": 0.0, ""yseq"": [y]}\n\n        if lpz is not None:\n            ctc_prefix_score = CTCPrefixScore(lpz, 0, self.eos, self.xp)\n            hyp[""ctc_state_prev""] = ctc_prefix_score.initial_state()\n            hyp[""ctc_score_prev""] = 0.0\n            if ctc_weight != 1.0:\n                # pre-pruning based on attention scores\n                ctc_beam = min(lpz.shape[-1], int(beam * CTC_SCORING_RATIO))\n            else:\n                ctc_beam = lpz.shape[-1]\n\n        hyps = [hyp]\n        ended_hyps = []\n\n        for i in six.moves.range(maxlen):\n            logging.debug(""position "" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                ys = F.expand_dims(xp.array(hyp[""yseq""]), axis=0).data\n                out = self.decoder(ys, h, h_mask)\n\n                # get nbest local scores and their ids\n                local_att_scores = F.log_softmax(out[:, -1], axis=-1).data\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(\n                        hyp[""rnnlm_prev""], hyp[""yseq""][i]\n                    )\n                    local_scores = (\n                        local_att_scores + recog_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                if lpz is not None:\n                    local_best_ids = xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :ctc_beam\n                    ]\n                    ctc_scores, ctc_states = ctc_prefix_score(\n                        hyp[""yseq""], local_best_ids, hyp[""ctc_state_prev""]\n                    )\n                    local_scores = (1.0 - ctc_weight) * local_att_scores[\n                        :, local_best_ids\n                    ] + ctc_weight * (ctc_scores - hyp[""ctc_score_prev""])\n                    if rnnlm:\n                        local_scores += (\n                            recog_args.lm_weight * local_lm_scores[:, local_best_ids]\n                        )\n                    joint_best_ids = xp.argsort(local_scores, axis=1)[0, ::-1][:beam]\n                    local_best_scores = local_scores[:, joint_best_ids]\n                    local_best_ids = local_best_ids[joint_best_ids]\n                else:\n                    local_best_ids = self.xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :beam\n                    ]\n                    local_best_scores = local_scores[:, local_best_ids]\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    new_hyp[""score""] = hyp[""score""] + float(local_best_scores[0, j])\n                    new_hyp[""yseq""] = [0] * (1 + len(hyp[""yseq""]))\n                    new_hyp[""yseq""][: len(hyp[""yseq""])] = hyp[""yseq""]\n                    new_hyp[""yseq""][len(hyp[""yseq""])] = int(local_best_ids[j])\n                    if rnnlm:\n                        new_hyp[""rnnlm_prev""] = rnnlm_state\n                    if lpz is not None:\n                        new_hyp[""ctc_state_prev""] = ctc_states[joint_best_ids[j]]\n                        new_hyp[""ctc_score_prev""] = ctc_scores[joint_best_ids[j]]\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[""score""], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(""number of pruned hypothesis: "" + str(len(hyps)))\n            if char_list is not None:\n                logging.debug(\n                    ""best hypo: ""\n                    + """".join([char_list[int(x)] for x in hyps[0][""yseq""][1:]])\n                    + "" score: ""\n                    + str(hyps[0][""score""])\n                )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(""adding <eos> in the last postion in the loop"")\n                for hyp in hyps:\n                    hyp[""yseq""].append(self.eos)\n\n            # add ended hypothes to a final list, and removed them from current hypothes\n            # (this will be a probmlem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[""yseq""][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[""yseq""]) > minlen:\n                        hyp[""score""] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[""score""] += recog_args.lm_weight * rnnlm.final(\n                                hyp[""rnnlm_prev""]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n            if end_detect(ended_hyps, i) and recog_args.maxlenratio == 0.0:\n                logging.info(""end detected at %d"", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(""remained hypothes: "" + str(len(hyps)))\n            else:\n                logging.info(""no hypothesis. Finish decoding."")\n                break\n            if char_list is not None:\n                for hyp in hyps:\n                    logging.debug(\n                        ""hypo: "" + """".join([char_list[int(x)] for x in hyp[""yseq""][1:]])\n                    )\n\n            logging.debug(""number of ended hypothes: "" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(\n            ended_hyps, key=lambda x: x[""score""], reverse=True\n        )  # [:min(len(ended_hyps), recog_args.nbest)]\n\n        logging.debug(nbest_hyps)\n        # check number of hypotheis\n        if len(nbest_hyps) == 0:\n            logging.warn(\n                ""there is no N-best results, perform recognition ""\n                ""again with smaller minlenratio.""\n            )\n            # should copy becasuse Namespace will be overwritten globally\n            recog_args = Namespace(**vars(recog_args))\n            recog_args.minlenratio = max(0.0, recog_args.minlenratio - 0.1)\n            return self.recognize_beam(h, lpz, recog_args, char_list, rnnlm)\n\n        logging.info(""total log probability: "" + str(nbest_hyps[0][""score""]))\n        logging.info(\n            ""normalized log probability: ""\n            + str(nbest_hyps[0][""score""] / len(nbest_hyps[0][""yseq""]))\n        )\n        # remove sos\n        return nbest_hyps\n\n    def calculate_all_attentions(self, xs, ilens, ys):\n        """"""E2E attention calculation.\n\n        Args:\n            xs_pad (List[tuple()]): List of padded input sequences.\n                [(T1, idim), (T2, idim), ...]\n            ilens (ndarray): Batch of lengths of input sequences. (B)\n            ys (List): List of character id sequence tensor. [(L1), (L2), (L3), ...]\n\n        Returns:\n            float ndarray: Attention weights. (B, Lmax, Tmax)\n\n        """"""\n        with chainer.no_backprop_mode():\n            self(xs, ilens, ys, calculate_attentions=True)\n        ret = dict()\n        for name, m in self.namedlinks():\n            if isinstance(m, MultiHeadAttention):\n                var = m.attn\n                var.to_cpu()\n                _name = name[1:].replace(""/"", ""_"")\n                ret[_name] = var.data\n        return ret\n\n    @property\n    def attention_plot_class(self):\n        """"""Attention plot function.\n\n        Redirects to PlotAttentionReport\n\n        Returns:\n            PlotAttentionReport\n\n        """"""\n        return PlotAttentionReport\n\n    @staticmethod\n    def custom_converter(subsampling_factor=0):\n        """"""Get customconverter of the model.""""""\n        return CustomConverter()\n\n    @staticmethod\n    def custom_updater(iters, optimizer, converter, device=-1, accum_grad=1):\n        """"""Get custom_updater of the model.""""""\n        return CustomUpdater(\n            iters, optimizer, converter=converter, device=device, accum_grad=accum_grad\n        )\n\n    @staticmethod\n    def custom_parallel_updater(iters, optimizer, converter, devices, accum_grad=1):\n        """"""Get custom_parallel_updater of the model.""""""\n        return CustomParallelUpdater(\n            iters,\n            optimizer,\n            converter=converter,\n            devices=devices,\n            accum_grad=accum_grad,\n        )\n'"
espnet/nets/chainer_backend/nets_utils.py,0,"b'import chainer.functions as F\n\n\ndef _subsamplex(x, n):\n    x = [F.get_item(xx, (slice(None, None, n), slice(None))) for xx in x]\n    ilens = [xx.shape[0] for xx in x]\n    return x, ilens\n'"
espnet/nets/pytorch_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/ctc.py,22,"b'from distutils.version import LooseVersion\nimport logging\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\n\n\nclass CTC(torch.nn.Module):\n    """"""CTC module\n\n    :param int odim: dimension of outputs\n    :param int eprojs: number of encoder projection units\n    :param float dropout_rate: dropout rate (0.0 ~ 1.0)\n    :param str ctc_type: builtin or warpctc\n    :param bool reduce: reduce the CTC loss into a scalar\n    """"""\n\n    def __init__(self, odim, eprojs, dropout_rate, ctc_type=""warpctc"", reduce=True):\n        super().__init__()\n        self.dropout_rate = dropout_rate\n        self.loss = None\n        self.ctc_lo = torch.nn.Linear(eprojs, odim)\n\n        # In case of Pytorch >= 1.2.0, CTC will be always builtin\n        self.ctc_type = (\n            ctc_type\n            if LooseVersion(torch.__version__) < LooseVersion(""1.2.0"")\n            else ""builtin""\n        )\n        if ctc_type != self.ctc_type:\n            logging.warning(f""CTC was set to {self.ctc_type} due to PyTorch version."")\n        if self.ctc_type == ""builtin"":\n            reduction_type = ""sum"" if reduce else ""none""\n            self.ctc_loss = torch.nn.CTCLoss(reduction=reduction_type)\n        elif self.ctc_type == ""warpctc"":\n            import warpctc_pytorch as warp_ctc\n\n            self.ctc_loss = warp_ctc.CTCLoss(size_average=True, reduce=reduce)\n        else:\n            raise ValueError(\n                \'ctc_type must be ""builtin"" or ""warpctc"": {}\'.format(self.ctc_type)\n            )\n\n        self.ignore_id = -1\n        self.reduce = reduce\n\n    def loss_fn(self, th_pred, th_target, th_ilen, th_olen):\n        if self.ctc_type == ""builtin"":\n            th_pred = th_pred.log_softmax(2)\n            # Use the deterministic CuDNN implementation of CTC loss to avoid\n            #  [issue#17798](https://github.com/pytorch/pytorch/issues/17798)\n            with torch.backends.cudnn.flags(deterministic=True):\n                loss = self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n            # Batch-size average\n            loss = loss / th_pred.size(1)\n            return loss\n        elif self.ctc_type == ""warpctc"":\n            return self.ctc_loss(th_pred, th_target, th_ilen, th_olen)\n        else:\n            raise NotImplementedError\n\n    def forward(self, hs_pad, hlens, ys_pad):\n        """"""CTC forward\n\n        :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D)\n        :param torch.Tensor hlens: batch of lengths of hidden state sequences (B)\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, Lmax)\n        :return: ctc loss value\n        :rtype: torch.Tensor\n        """"""\n        # TODO(kan-bayashi): need to make more smart way\n        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n\n        self.loss = None\n        hlens = torch.from_numpy(np.fromiter(hlens, dtype=np.int32))\n        olens = torch.from_numpy(np.fromiter((x.size(0) for x in ys), dtype=np.int32))\n\n        # zero padding for hs\n        ys_hat = self.ctc_lo(F.dropout(hs_pad, p=self.dropout_rate))\n\n        # zero padding for ys\n        ys_true = torch.cat(ys).cpu().int()  # batch x olen\n\n        # get length info\n        logging.info(\n            self.__class__.__name__\n            + "" input lengths:  ""\n            + """".join(str(hlens).split(""\\n""))\n        )\n        logging.info(\n            self.__class__.__name__\n            + "" output lengths: ""\n            + """".join(str(olens).split(""\\n""))\n        )\n\n        # get ctc loss\n        # expected shape of seqLength x batchSize x alphabet_size\n        dtype = ys_hat.dtype\n        ys_hat = ys_hat.transpose(0, 1)\n        if self.ctc_type == ""warpctc"" or dtype == torch.float16:\n            # warpctc only supports float32\n            # torch.ctc does not support float16 (#1751)\n            ys_hat = ys_hat.to(dtype=torch.float32)\n        if self.ctc_type == ""builtin"":\n            # use GPU when using the cuDNN implementation\n            ys_true = to_device(self, ys_true)\n        self.loss = to_device(self, self.loss_fn(ys_hat, ys_true, hlens, olens)).to(\n            dtype=dtype\n        )\n        if self.reduce:\n            # NOTE: sum() is needed to keep consistency\n            # since warpctc return as tensor w/ shape (1,)\n            # but builtin return as tensor w/o shape (scalar).\n            self.loss = self.loss.sum()\n            logging.info(""ctc loss:"" + str(float(self.loss)))\n\n        return self.loss\n\n    def log_softmax(self, hs_pad):\n        """"""log_softmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: log softmax applied 3d tensor (B, Tmax, odim)\n        :rtype: torch.Tensor\n        """"""\n        return F.log_softmax(self.ctc_lo(hs_pad), dim=2)\n\n    def argmax(self, hs_pad):\n        """"""argmax of frame activations\n\n        :param torch.Tensor hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: argmax applied 2d tensor (B, Tmax)\n        :rtype: torch.Tensor\n        """"""\n        return torch.argmax(self.ctc_lo(hs_pad), dim=2)\n\n\ndef ctc_for(args, odim, reduce=True):\n    """"""Returns the CTC module for the given args and output dimension\n\n    :param Namespace args: the program args\n    :param int odim : The output dimension\n    :param bool reduce : return the CTC loss in a scalar\n    :return: the corresponding CTC module\n    """"""\n    num_encs = getattr(args, ""num_encs"", 1)  # use getattr to keep compatibility\n    if num_encs == 1:\n        # compatible with single encoder asr mode\n        return CTC(\n            odim, args.eprojs, args.dropout_rate, ctc_type=args.ctc_type, reduce=reduce\n        )\n    elif num_encs >= 1:\n        ctcs_list = torch.nn.ModuleList()\n        if args.share_ctc:\n            # use dropout_rate of the first encoder\n            ctc = CTC(\n                odim,\n                args.eprojs,\n                args.dropout_rate[0],\n                ctc_type=args.ctc_type,\n                reduce=reduce,\n            )\n            ctcs_list.append(ctc)\n        else:\n            for idx in range(num_encs):\n                ctc = CTC(\n                    odim,\n                    args.eprojs,\n                    args.dropout_rate[idx],\n                    ctc_type=args.ctc_type,\n                    reduce=reduce,\n                )\n                ctcs_list.append(ctc)\n        return ctcs_list\n    else:\n        raise ValueError(\n            ""Number of encoders needs to be more than one. {}"".format(num_encs)\n        )\n'"
espnet/nets/pytorch_backend/e2e_asr.py,19,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""RNN sequence-to-sequence speech recognition model (pytorch).""""""\n\nimport argparse\nfrom itertools import groupby\nimport logging\nimport math\nimport os\n\nimport chainer\nfrom chainer import reporter\nimport editdistance\nimport numpy as np\nimport six\nimport torch\n\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.e2e_asr_common import label_smoothing_dist\nfrom espnet.nets.pytorch_backend.ctc import ctc_for\nfrom espnet.nets.pytorch_backend.frontends.feature_transform import (\n    feature_transform_for,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.frontends.frontend import frontend_for\nfrom espnet.nets.pytorch_backend.initialization import lecun_normal_init_parameters\nfrom espnet.nets.pytorch_backend.initialization import set_forget_bias_to_one\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.nets_utils import to_torch_tensor\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_for\nfrom espnet.nets.pytorch_backend.rnn.decoders import decoder_for\nfrom espnet.nets.pytorch_backend.rnn.encoders import encoder_for\nfrom espnet.nets.scorers.ctc import CTCPrefixScorer\n\nCTC_LOSS_THRESHOLD = 10000\n\n\nclass Reporter(chainer.Chain):\n    """"""A chainer reporter wrapper.""""""\n\n    def report(self, loss_ctc, loss_att, acc, cer_ctc, cer, wer, mtl_loss):\n        """"""Report at every step.""""""\n        reporter.report({""loss_ctc"": loss_ctc}, self)\n        reporter.report({""loss_att"": loss_att}, self)\n        reporter.report({""acc"": acc}, self)\n        reporter.report({""cer_ctc"": cer_ctc}, self)\n        reporter.report({""cer"": cer}, self)\n        reporter.report({""wer"": wer}, self)\n        logging.info(""mtl loss:"" + str(mtl_loss))\n        reporter.report({""loss"": mtl_loss}, self)\n\n\nclass E2E(ASRInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param int idim: dimension of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        E2E.encoder_add_arguments(parser)\n        E2E.attention_add_arguments(parser)\n        E2E.decoder_add_arguments(parser)\n        return parser\n\n    @staticmethod\n    def encoder_add_arguments(parser):\n        """"""Add arguments for the encoder.""""""\n        group = parser.add_argument_group(""E2E encoder setting"")\n        # encoder\n        group.add_argument(\n            ""--etype"",\n            default=""blstmp"",\n            type=str,\n            choices=[\n                ""lstm"",\n                ""blstm"",\n                ""lstmp"",\n                ""blstmp"",\n                ""vgglstmp"",\n                ""vggblstmp"",\n                ""vgglstm"",\n                ""vggblstm"",\n                ""gru"",\n                ""bgru"",\n                ""grup"",\n                ""bgrup"",\n                ""vgggrup"",\n                ""vggbgrup"",\n                ""vgggru"",\n                ""vggbgru"",\n            ],\n            help=""Type of encoder network architecture"",\n        )\n        group.add_argument(\n            ""--elayers"",\n            default=4,\n            type=int,\n            help=""Number of encoder layers ""\n            ""(for shared recognition part in multi-speaker asr mode)"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=300,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        group.add_argument(\n            ""--eprojs"", default=320, type=int, help=""Number of encoder projection units""\n        )\n        group.add_argument(\n            ""--subsample"",\n            default=""1"",\n            type=str,\n            help=""Subsample input frames x_y_z means ""\n            ""subsample every x frame at 1st layer, ""\n            ""every y frame at 2nd layer etc."",\n        )\n        return parser\n\n    @staticmethod\n    def attention_add_arguments(parser):\n        """"""Add arguments for the attention.""""""\n        group = parser.add_argument_group(""E2E attention setting"")\n        # attention\n        group.add_argument(\n            ""--atype"",\n            default=""dot"",\n            type=str,\n            choices=[\n                ""noatt"",\n                ""dot"",\n                ""add"",\n                ""location"",\n                ""coverage"",\n                ""coverage_location"",\n                ""location2d"",\n                ""location_recurrent"",\n                ""multi_head_dot"",\n                ""multi_head_add"",\n                ""multi_head_loc"",\n                ""multi_head_multi_res_loc"",\n            ],\n            help=""Type of attention architecture"",\n        )\n        group.add_argument(\n            ""--adim"",\n            default=320,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--awin"", default=5, type=int, help=""Window size for location2d attention""\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        group.add_argument(\n            ""--aconv-chans"",\n            default=-1,\n            type=int,\n            help=""Number of attention convolution channels \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--aconv-filts"",\n            default=100,\n            type=int,\n            help=""Number of attention convolution filters \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        return parser\n\n    @staticmethod\n    def decoder_add_arguments(parser):\n        """"""Add arguments for the decoder.""""""\n        group = parser.add_argument_group(""E2E encoder setting"")\n        group.add_argument(\n            ""--dtype"",\n            default=""lstm"",\n            type=str,\n            choices=[""lstm"", ""gru""],\n            help=""Type of decoder network architecture"",\n        )\n        group.add_argument(\n            ""--dlayers"", default=1, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=320, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--dropout-rate-decoder"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the decoder"",\n        )\n        group.add_argument(\n            ""--sampling-probability"",\n            default=0.0,\n            type=float,\n            help=""Ratio of predicted labels fed back to decoder"",\n        )\n        group.add_argument(\n            ""--lsm-type"",\n            const="""",\n            default="""",\n            type=str,\n            nargs=""?"",\n            choices=["""", ""unigram""],\n            help=""Apply label smoothing with a specified distribution type"",\n        )\n        return parser\n\n    def __init__(self, idim, odim, args):\n        """"""Construct an E2E object.\n\n        :param int idim: dimension of inputs\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        """"""\n        super(E2E, self).__init__()\n        torch.nn.Module.__init__(self)\n        self.mtlalpha = args.mtlalpha\n        assert 0.0 <= self.mtlalpha <= 1.0, ""mtlalpha should be [0.0, 1.0]""\n        self.etype = args.etype\n        self.verbose = args.verbose\n        # NOTE: for self.build method\n        args.char_list = getattr(args, ""char_list"", None)\n        self.char_list = args.char_list\n        self.outdir = args.outdir\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n        self.reporter = Reporter()\n\n        # below means the last number becomes eos/sos ID\n        # note that sos/eos IDs are identical\n        self.sos = odim - 1\n        self.eos = odim - 1\n\n        # subsample info\n        self.subsample = get_subsample(args, mode=""asr"", arch=""rnn"")\n\n        # label smoothing info\n        if args.lsm_type and os.path.isfile(args.train_json):\n            logging.info(""Use label smoothing with "" + args.lsm_type)\n            labeldist = label_smoothing_dist(\n                odim, args.lsm_type, transcript=args.train_json\n            )\n        else:\n            labeldist = None\n\n        if getattr(args, ""use_frontend"", False):  # use getattr to keep compatibility\n            self.frontend = frontend_for(args, idim)\n            self.feature_transform = feature_transform_for(args, (idim - 1) * 2)\n            idim = args.n_mels\n        else:\n            self.frontend = None\n\n        # encoder\n        self.enc = encoder_for(args, idim, self.subsample)\n        # ctc\n        self.ctc = ctc_for(args, odim)\n        # attention\n        self.att = att_for(args)\n        # decoder\n        self.dec = decoder_for(args, odim, self.sos, self.eos, self.att, labeldist)\n\n        # weight initialization\n        self.init_like_chainer()\n\n        # options for beam search\n        if args.report_cer or args.report_wer:\n            recog_args = {\n                ""beam_size"": args.beam_size,\n                ""penalty"": args.penalty,\n                ""ctc_weight"": args.ctc_weight,\n                ""maxlenratio"": args.maxlenratio,\n                ""minlenratio"": args.minlenratio,\n                ""lm_weight"": args.lm_weight,\n                ""rnnlm"": args.rnnlm,\n                ""nbest"": args.nbest,\n                ""space"": args.sym_space,\n                ""blank"": args.sym_blank,\n            }\n\n            self.recog_args = argparse.Namespace(**recog_args)\n            self.report_cer = args.report_cer\n            self.report_wer = args.report_wer\n        else:\n            self.report_cer = False\n            self.report_wer = False\n        self.rnnlm = None\n\n        self.logzero = -10000000000.0\n        self.loss = None\n        self.acc = None\n\n    def init_like_chainer(self):\n        """"""Initialize weight like chainer.\n\n        chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0\n        pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5)\n        however, there are two exceptions as far as I know.\n        - EmbedID.W ~ Normal(0, 1)\n        - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM)\n        """"""\n        lecun_normal_init_parameters(self)\n        # exceptions\n        # embed weight ~ Normal(0, 1)\n        self.dec.embed.weight.data.normal_(0, 1)\n        # forget-bias = 1.0\n        # https://discuss.pytorch.org/t/set-forget-gate-bias-of-lstm/1745\n        for i in six.moves.range(len(self.dec.decoder)):\n            set_forget_bias_to_one(self.dec.decoder[i].bias_ih)\n\n    def forward(self, xs_pad, ilens, ys_pad):\n        """"""E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: loss value\n        :rtype: torch.Tensor\n        """"""\n        # 0. Frontend\n        if self.frontend is not None:\n            hs_pad, hlens, mask = self.frontend(to_torch_tensor(xs_pad), ilens)\n            hs_pad, hlens = self.feature_transform(hs_pad, hlens)\n        else:\n            hs_pad, hlens = xs_pad, ilens\n\n        # 1. Encoder\n        hs_pad, hlens, _ = self.enc(hs_pad, hlens)\n\n        # 2. CTC loss\n        if self.mtlalpha == 0:\n            self.loss_ctc = None\n        else:\n            self.loss_ctc = self.ctc(hs_pad, hlens, ys_pad)\n\n        # 3. attention loss\n        if self.mtlalpha == 1:\n            self.loss_att, acc = None, None\n        else:\n            self.loss_att, acc, _ = self.dec(hs_pad, hlens, ys_pad)\n        self.acc = acc\n\n        # 4. compute cer without beam search\n        if self.mtlalpha == 0 or self.char_list is None:\n            cer_ctc = None\n        else:\n            cers = []\n\n            y_hats = self.ctc.argmax(hs_pad).data\n            for i, y in enumerate(y_hats):\n                y_hat = [x[0] for x in groupby(y)]\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = """".join(seq_hat).replace(self.space, "" "")\n                seq_hat_text = seq_hat_text.replace(self.blank, """")\n                seq_true_text = """".join(seq_true).replace(self.space, "" "")\n\n                hyp_chars = seq_hat_text.replace("" "", """")\n                ref_chars = seq_true_text.replace("" "", """")\n                if len(ref_chars) > 0:\n                    cers.append(\n                        editdistance.eval(hyp_chars, ref_chars) / len(ref_chars)\n                    )\n\n            cer_ctc = sum(cers) / len(cers) if cers else None\n\n        # 5. compute cer/wer\n        if self.training or not (self.report_cer or self.report_wer):\n            cer, wer = 0.0, 0.0\n            # oracle_cer, oracle_wer = 0.0, 0.0\n        else:\n            if self.recog_args.ctc_weight > 0.0:\n                lpz = self.ctc.log_softmax(hs_pad).data\n            else:\n                lpz = None\n\n            word_eds, word_ref_lens, char_eds, char_ref_lens = [], [], [], []\n            nbest_hyps = self.dec.recognize_beam_batch(\n                hs_pad,\n                torch.tensor(hlens),\n                lpz,\n                self.recog_args,\n                self.char_list,\n                self.rnnlm,\n            )\n            # remove <sos> and <eos>\n            y_hats = [nbest_hyp[0][""yseq""][1:-1] for nbest_hyp in nbest_hyps]\n            for i, y_hat in enumerate(y_hats):\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = """".join(seq_hat).replace(self.recog_args.space, "" "")\n                seq_hat_text = seq_hat_text.replace(self.recog_args.blank, """")\n                seq_true_text = """".join(seq_true).replace(self.recog_args.space, "" "")\n\n                hyp_words = seq_hat_text.split()\n                ref_words = seq_true_text.split()\n                word_eds.append(editdistance.eval(hyp_words, ref_words))\n                word_ref_lens.append(len(ref_words))\n                hyp_chars = seq_hat_text.replace("" "", """")\n                ref_chars = seq_true_text.replace("" "", """")\n                char_eds.append(editdistance.eval(hyp_chars, ref_chars))\n                char_ref_lens.append(len(ref_chars))\n\n            wer = (\n                0.0\n                if not self.report_wer\n                else float(sum(word_eds)) / sum(word_ref_lens)\n            )\n            cer = (\n                0.0\n                if not self.report_cer\n                else float(sum(char_eds)) / sum(char_ref_lens)\n            )\n\n        alpha = self.mtlalpha\n        if alpha == 0:\n            self.loss = self.loss_att\n            loss_att_data = float(self.loss_att)\n            loss_ctc_data = None\n        elif alpha == 1:\n            self.loss = self.loss_ctc\n            loss_att_data = None\n            loss_ctc_data = float(self.loss_ctc)\n        else:\n            self.loss = alpha * self.loss_ctc + (1 - alpha) * self.loss_att\n            loss_att_data = float(self.loss_att)\n            loss_ctc_data = float(self.loss_ctc)\n\n        loss_data = float(self.loss)\n        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):\n            self.reporter.report(\n                loss_ctc_data, loss_att_data, acc, cer_ctc, cer, wer, loss_data\n            )\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def scorers(self):\n        """"""Scorers.""""""\n        return dict(decoder=self.dec, ctc=CTCPrefixScorer(self.ctc, self.eos))\n\n    def encode(self, x):\n        """"""Encode acoustic features.\n\n        :param ndarray x: input acoustic feature (T, D)\n        :return: encoder outputs\n        :rtype: torch.Tensor\n        """"""\n        self.eval()\n        ilens = [x.shape[0]]\n\n        # subsample frame\n        x = x[:: self.subsample[0], :]\n        p = next(self.parameters())\n        h = torch.as_tensor(x, device=p.device, dtype=p.dtype)\n        # make a utt list (1) to use the same interface for encoder\n        hs = h.contiguous().unsqueeze(0)\n\n        # 0. Frontend\n        if self.frontend is not None:\n            enhanced, hlens, mask = self.frontend(hs, ilens)\n            hs, hlens = self.feature_transform(enhanced, hlens)\n        else:\n            hs, hlens = hs, ilens\n\n        # 1. encoder\n        hs, _, _ = self.enc(hs, hlens)\n        return hs.squeeze(0)\n\n    def recognize(self, x, recog_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param ndarray x: input acoustic feature (T, D)\n        :param Namespace recog_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        hs = self.encode(x).unsqueeze(0)\n        # calculate log P(z_t|X) for CTC scores\n        if recog_args.ctc_weight > 0.0:\n            lpz = self.ctc.log_softmax(hs)[0]\n        else:\n            lpz = None\n\n        # 2. Decoder\n        # decode the first utterance\n        y = self.dec.recognize_beam(hs[0], lpz, recog_args, char_list, rnnlm)\n        return y\n\n    def recognize_batch(self, xs, recog_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param list xs: list of input acoustic feature arrays [(T_1, D), (T_2, D), ...]\n        :param Namespace recog_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        prev = self.training\n        self.eval()\n        ilens = np.fromiter((xx.shape[0] for xx in xs), dtype=np.int64)\n\n        # subsample frame\n        xs = [xx[:: self.subsample[0], :] for xx in xs]\n        xs = [to_device(self, to_torch_tensor(xx).float()) for xx in xs]\n        xs_pad = pad_list(xs, 0.0)\n\n        # 0. Frontend\n        if self.frontend is not None:\n            enhanced, hlens, mask = self.frontend(xs_pad, ilens)\n            hs_pad, hlens = self.feature_transform(enhanced, hlens)\n        else:\n            hs_pad, hlens = xs_pad, ilens\n\n        # 1. Encoder\n        hs_pad, hlens, _ = self.enc(hs_pad, hlens)\n\n        # calculate log P(z_t|X) for CTC scores\n        if recog_args.ctc_weight > 0.0:\n            lpz = self.ctc.log_softmax(hs_pad)\n            normalize_score = False\n        else:\n            lpz = None\n            normalize_score = True\n\n        # 2. Decoder\n        hlens = torch.tensor(list(map(int, hlens)))  # make sure hlens is tensor\n        y = self.dec.recognize_beam_batch(\n            hs_pad,\n            hlens,\n            lpz,\n            recog_args,\n            char_list,\n            rnnlm,\n            normalize_score=normalize_score,\n        )\n\n        if prev:\n            self.train()\n        return y\n\n    def enhance(self, xs):\n        """"""Forward only in the frontend stage.\n\n        :param ndarray xs: input acoustic feature (T, C, F)\n        :return: enhaned feature\n        :rtype: torch.Tensor\n        """"""\n        if self.frontend is None:\n            raise RuntimeError(""Frontend does\'t exist"")\n        prev = self.training\n        self.eval()\n        ilens = np.fromiter((xx.shape[0] for xx in xs), dtype=np.int64)\n\n        # subsample frame\n        xs = [xx[:: self.subsample[0], :] for xx in xs]\n        xs = [to_device(self, to_torch_tensor(xx).float()) for xx in xs]\n        xs_pad = pad_list(xs, 0.0)\n        enhanced, hlensm, mask = self.frontend(xs_pad, ilens)\n        if prev:\n            self.train()\n        return enhanced.cpu().numpy(), mask.cpu().numpy(), ilens\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        """"""E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n        """"""\n        with torch.no_grad():\n            # 0. Frontend\n            if self.frontend is not None:\n                hs_pad, hlens, mask = self.frontend(to_torch_tensor(xs_pad), ilens)\n                hs_pad, hlens = self.feature_transform(hs_pad, hlens)\n            else:\n                hs_pad, hlens = xs_pad, ilens\n\n            # 1. Encoder\n            hpad, hlens, _ = self.enc(hs_pad, hlens)\n\n            # 2. Decoder\n            att_ws = self.dec.calculate_all_attentions(hpad, hlens, ys_pad)\n\n        return att_ws\n\n    def subsample_frames(self, x):\n        """"""Subsample speeh frames in the encoder.""""""\n        # subsample frame\n        x = x[:: self.subsample[0], :]\n        ilen = [x.shape[0]]\n        h = to_device(self, torch.from_numpy(np.array(x, dtype=np.float32)))\n        h.contiguous()\n        return h, ilen\n'"
espnet/nets/pytorch_backend/e2e_asr_mix.py,39,"b'#!/usr/bin/env python3\n\n""""""\nThis script is used to construct End-to-End models of multi-speaker ASR.\n\nCopyright 2017 Johns Hopkins University (Shinji Watanabe)\n Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n""""""\n\nimport argparse\nimport logging\nimport math\nimport os\nimport sys\n\nimport chainer\nfrom chainer import reporter\nimport editdistance\nimport numpy as np\nimport six\nimport torch\n\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.e2e_asr_common import get_vgg2l_odim\nfrom espnet.nets.e2e_asr_common import label_smoothing_dist\nfrom espnet.nets.pytorch_backend.ctc import ctc_for\nfrom espnet.nets.pytorch_backend.e2e_asr import E2E as E2E_ASR\nfrom espnet.nets.pytorch_backend.frontends.feature_transform import (\n    feature_transform_for,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.frontends.frontend import frontend_for\n\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.nets_utils import to_torch_tensor\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_for\nfrom espnet.nets.pytorch_backend.rnn.decoders import decoder_for\nfrom espnet.nets.pytorch_backend.rnn.encoders import encoder_for as encoder_for_single\nfrom espnet.nets.pytorch_backend.rnn.encoders import RNNP\nfrom espnet.nets.pytorch_backend.rnn.encoders import VGG2L\n\nCTC_LOSS_THRESHOLD = 10000\n\n\nclass Reporter(chainer.Chain):\n    """"""A chainer reporter wrapper.""""""\n\n    def report(self, loss_ctc, loss_att, acc, cer, wer, mtl_loss):\n        """"""Define reporter.""""""\n        reporter.report({""loss_ctc"": loss_ctc}, self)\n        reporter.report({""loss_att"": loss_att}, self)\n        reporter.report({""acc"": acc}, self)\n        reporter.report({""cer"": cer}, self)\n        reporter.report({""wer"": wer}, self)\n        logging.info(""mtl loss:"" + str(mtl_loss))\n        reporter.report({""loss"": mtl_loss}, self)\n\n\nclass PIT(object):\n    """"""Permutation Invariant Training (PIT) module.\n\n    :parameter int num_spkrs: number of speakers for PIT process (2 or 3)\n    """"""\n\n    def __init__(self, num_spkrs):\n        """"""Initialize PIT module.""""""\n        self.num_spkrs = num_spkrs\n        if self.num_spkrs == 2:\n            self.perm_choices = [[0, 1], [1, 0]]\n        elif self.num_spkrs == 3:\n            self.perm_choices = [\n                [0, 1, 2],\n                [0, 2, 1],\n                [1, 2, 0],\n                [1, 0, 2],\n                [2, 0, 1],\n                [2, 1, 0],\n            ]\n        else:\n            raise ValueError\n\n    def min_pit_sample(self, loss):\n        """"""Compute the PIT loss for each sample.\n\n        :param 1-D torch.Tensor loss: list of losses for one sample,\n            including [h1r1, h1r2, h2r1, h2r2] or\n            [h1r1, h1r2, h1r3, h2r1, h2r2, h2r3, h3r1, h3r2, h3r3]\n        :return minimum loss of best permutation\n        :rtype torch.Tensor (1)\n        :return the best permutation\n        :rtype List: len=2\n\n        """"""\n        if self.num_spkrs == 2:\n            score_perms = (\n                torch.stack([loss[0] + loss[3], loss[1] + loss[2]]) / self.num_spkrs\n            )\n        elif self.num_spkrs == 3:\n            score_perms = (\n                torch.stack(\n                    [\n                        loss[0] + loss[4] + loss[8],\n                        loss[0] + loss[5] + loss[7],\n                        loss[1] + loss[5] + loss[6],\n                        loss[1] + loss[3] + loss[8],\n                        loss[2] + loss[3] + loss[7],\n                        loss[2] + loss[4] + loss[6],\n                    ]\n                )\n                / self.num_spkrs\n            )\n\n        perm_loss, min_idx = torch.min(score_perms, 0)\n        permutation = self.perm_choices[min_idx]\n\n        return perm_loss, permutation\n\n    def pit_process(self, losses):\n        """"""Compute the PIT loss for a batch.\n\n        :param torch.Tensor losses: losses (B, 1|4|9)\n        :return minimum losses of a batch with best permutation\n        :rtype torch.Tensor (B)\n        :return the best permutation\n        :rtype torch.LongTensor (B, 1|2|3)\n\n        """"""\n        bs = losses.size(0)\n        ret = [self.min_pit_sample(losses[i]) for i in range(bs)]\n\n        loss_perm = torch.stack([r[0] for r in ret], dim=0).to(losses.device)  # (B)\n        permutation = torch.tensor([r[1] for r in ret]).long().to(losses.device)\n\n        return torch.mean(loss_perm), permutation\n\n\nclass E2E(E2E_ASR, ASRInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param int idim: dimension of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        E2E.encoder_add_arguments(parser)\n        E2E.encoder_mix_add_arguments(parser)\n        E2E.attention_add_arguments(parser)\n        E2E.decoder_add_arguments(parser)\n        return parser\n\n    @staticmethod\n    def encoder_mix_add_arguments(parser):\n        """"""Add arguments for multi-speaker encoder.""""""\n        group = parser.add_argument_group(""E2E encoder setting for multi-speaker"")\n        # asr-mix encoder\n        group.add_argument(\n            ""--spa"",\n            action=""store_true"",\n            help=""Enable speaker parallel attention ""\n            ""for multi-speaker speech recognition task."",\n        )\n        group.add_argument(\n            ""--elayers-sd"",\n            default=4,\n            type=int,\n            help=""Number of speaker differentiate encoder layers""\n            ""for multi-speaker speech recognition task."",\n        )\n        return parser\n\n    def __init__(self, idim, odim, args):\n        """"""Initialize multi-speaker E2E module.""""""\n        torch.nn.Module.__init__(self)\n        self.mtlalpha = args.mtlalpha\n        assert 0.0 <= self.mtlalpha <= 1.0, ""mtlalpha should be [0.0, 1.0]""\n        self.etype = args.etype\n        self.verbose = args.verbose\n        self.char_list = args.char_list\n        self.outdir = args.outdir\n        self.reporter = Reporter()\n        self.num_spkrs = args.num_spkrs\n        self.spa = args.spa\n        self.pit = PIT(self.num_spkrs)\n\n        # below means the last number becomes eos/sos ID\n        # note that sos/eos IDs are identical\n        self.sos = odim - 1\n        self.eos = odim - 1\n\n        # subsample info\n        self.subsample = get_subsample(args, mode=""asr"", arch=""rnn_mix"")\n\n        # label smoothing info\n        if args.lsm_type and os.path.isfile(args.train_json):\n            logging.info(""Use label smoothing with "" + args.lsm_type)\n            labeldist = label_smoothing_dist(\n                odim, args.lsm_type, transcript=args.train_json\n            )\n        else:\n            labeldist = None\n\n        if getattr(args, ""use_frontend"", False):  # use getattr to keep compatibility\n            self.frontend = frontend_for(args, idim)\n            self.feature_transform = feature_transform_for(args, (idim - 1) * 2)\n            idim = args.n_mels\n        else:\n            self.frontend = None\n\n        # encoder\n        self.enc = encoder_for(args, idim, self.subsample)\n        # ctc\n        self.ctc = ctc_for(args, odim, reduce=False)\n        # attention\n        num_att = self.num_spkrs if args.spa else 1\n        self.att = att_for(args, num_att)\n        # decoder\n        self.dec = decoder_for(args, odim, self.sos, self.eos, self.att, labeldist)\n\n        # weight initialization\n        self.init_like_chainer()\n\n        # options for beam search\n        if ""report_cer"" in vars(args) and (args.report_cer or args.report_wer):\n            recog_args = {\n                ""beam_size"": args.beam_size,\n                ""penalty"": args.penalty,\n                ""ctc_weight"": args.ctc_weight,\n                ""maxlenratio"": args.maxlenratio,\n                ""minlenratio"": args.minlenratio,\n                ""lm_weight"": args.lm_weight,\n                ""rnnlm"": args.rnnlm,\n                ""nbest"": args.nbest,\n                ""space"": args.sym_space,\n                ""blank"": args.sym_blank,\n            }\n\n            self.recog_args = argparse.Namespace(**recog_args)\n            self.report_cer = args.report_cer\n            self.report_wer = args.report_wer\n        else:\n            self.report_cer = False\n            self.report_wer = False\n        self.rnnlm = None\n\n        self.logzero = -10000000000.0\n        self.loss = None\n        self.acc = None\n\n    def init_like_chainer(self):\n        """"""Initialize weight like chainer.\n\n        chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0\n        pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5)\n\n        however, there are two exceptions as far as I know.\n        - EmbedID.W ~ Normal(0, 1)\n        - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM)\n        """"""\n\n        def lecun_normal_init_parameters(module):\n            for p in module.parameters():\n                data = p.data\n                if data.dim() == 1:\n                    # bias\n                    data.zero_()\n                elif data.dim() == 2:\n                    # linear weight\n                    n = data.size(1)\n                    stdv = 1.0 / math.sqrt(n)\n                    data.normal_(0, stdv)\n                elif data.dim() == 4:\n                    # conv weight\n                    n = data.size(1)\n                    for k in data.size()[2:]:\n                        n *= k\n                    stdv = 1.0 / math.sqrt(n)\n                    data.normal_(0, stdv)\n                else:\n                    raise NotImplementedError\n\n        def set_forget_bias_to_one(bias):\n            n = bias.size(0)\n            start, end = n // 4, n // 2\n            bias.data[start:end].fill_(1.0)\n\n        lecun_normal_init_parameters(self)\n        # exceptions\n        # embed weight ~ Normal(0, 1)\n        self.dec.embed.weight.data.normal_(0, 1)\n        # forget-bias = 1.0\n        # https://discuss.pytorch.org/t/set-forget-gate-bias-of-lstm/1745\n        for i in six.moves.range(len(self.dec.decoder)):\n            set_forget_bias_to_one(self.dec.decoder[i].bias_ih)\n\n    def forward(self, xs_pad, ilens, ys_pad):\n        """"""E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, num_spkrs, Lmax)\n        :return: ctc loss value\n        :rtype: torch.Tensor\n        :return: attention loss value\n        :rtype: torch.Tensor\n        :return: accuracy in attention decoder\n        :rtype: float\n        """"""\n        # 0. Frontend\n        if self.frontend is not None:\n            hs_pad, hlens, mask = self.frontend(to_torch_tensor(xs_pad), ilens)\n            if isinstance(hs_pad, list):\n                hlens_n = [None] * self.num_spkrs\n                for i in range(self.num_spkrs):\n                    hs_pad[i], hlens_n[i] = self.feature_transform(hs_pad[i], hlens)\n                hlens = hlens_n\n            else:\n                hs_pad, hlens = self.feature_transform(hs_pad, hlens)\n        else:\n            hs_pad, hlens = xs_pad, ilens\n\n        # 1. Encoder\n        if not isinstance(\n            hs_pad, list\n        ):  # single-channel input xs_pad (single- or multi-speaker)\n            hs_pad, hlens, _ = self.enc(hs_pad, hlens)\n        else:  # multi-channel multi-speaker input xs_pad\n            for i in range(self.num_spkrs):\n                hs_pad[i], hlens[i], _ = self.enc(hs_pad[i], hlens[i])\n\n        # 2. CTC loss\n        if self.mtlalpha == 0:\n            loss_ctc, min_perm = None, None\n        else:\n            if not isinstance(hs_pad, list):  # single-speaker input xs_pad\n                loss_ctc = torch.mean(self.ctc(hs_pad, hlens, ys_pad))\n            else:  # multi-speaker input xs_pad\n                ys_pad = ys_pad.transpose(0, 1)  # (num_spkrs, B, Lmax)\n                loss_ctc_perm = torch.stack(\n                    [\n                        self.ctc(\n                            hs_pad[i // self.num_spkrs],\n                            hlens[i // self.num_spkrs],\n                            ys_pad[i % self.num_spkrs],\n                        )\n                        for i in range(self.num_spkrs ** 2)\n                    ],\n                    dim=1,\n                )  # (B, num_spkrs^2)\n                loss_ctc, min_perm = self.pit.pit_process(loss_ctc_perm)\n                logging.info(""ctc loss:"" + str(float(loss_ctc)))\n\n        # 3. attention loss\n        if self.mtlalpha == 1:\n            loss_att = None\n            acc = None\n        else:\n            if not isinstance(hs_pad, list):  # single-speaker input xs_pad\n                loss_att, acc, _ = self.dec(hs_pad, hlens, ys_pad)\n            else:\n                for i in range(ys_pad.size(1)):  # B\n                    ys_pad[:, i] = ys_pad[min_perm[i], i]\n                rslt = [\n                    self.dec(hs_pad[i], hlens[i], ys_pad[i], strm_idx=i)\n                    for i in range(self.num_spkrs)\n                ]\n                loss_att = sum([r[0] for r in rslt]) / float(len(rslt))\n                acc = sum([r[1] for r in rslt]) / float(len(rslt))\n        self.acc = acc\n\n        # 5. compute cer/wer\n        if (\n            self.training\n            or not (self.report_cer or self.report_wer)\n            or not isinstance(hs_pad, list)\n        ):\n            cer, wer = 0.0, 0.0\n            # oracle_cer, oracle_wer = 0.0, 0.0\n        else:\n            if self.recog_args.ctc_weight > 0.0:\n                lpz = [\n                    self.ctc.log_softmax(hs_pad[i]).data for i in range(self.num_spkrs)\n                ]\n            else:\n                lpz = None\n\n            word_eds, char_eds, word_ref_lens, char_ref_lens = [], [], [], []\n            nbest_hyps = [\n                self.dec.recognize_beam_batch(\n                    hs_pad[i],\n                    torch.tensor(hlens[i]),\n                    lpz[i],\n                    self.recog_args,\n                    self.char_list,\n                    self.rnnlm,\n                    strm_idx=i,\n                )\n                for i in range(self.num_spkrs)\n            ]\n            # remove <sos> and <eos>\n            y_hats = [\n                [nbest_hyp[0][""yseq""][1:-1] for nbest_hyp in nbest_hyps[i]]\n                for i in range(self.num_spkrs)\n            ]\n            for i in range(len(y_hats[0])):\n                hyp_words = []\n                hyp_chars = []\n                ref_words = []\n                ref_chars = []\n                for ns in range(self.num_spkrs):\n                    y_hat = y_hats[ns][i]\n                    y_true = ys_pad[ns][i]\n\n                    seq_hat = [\n                        self.char_list[int(idx)] for idx in y_hat if int(idx) != -1\n                    ]\n                    seq_true = [\n                        self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                    ]\n                    seq_hat_text = """".join(seq_hat).replace(self.recog_args.space, "" "")\n                    seq_hat_text = seq_hat_text.replace(self.recog_args.blank, """")\n                    seq_true_text = """".join(seq_true).replace(\n                        self.recog_args.space, "" ""\n                    )\n\n                    hyp_words.append(seq_hat_text.split())\n                    ref_words.append(seq_true_text.split())\n                    hyp_chars.append(seq_hat_text.replace("" "", """"))\n                    ref_chars.append(seq_true_text.replace("" "", """"))\n\n                tmp_word_ed = [\n                    editdistance.eval(\n                        hyp_words[ns // self.num_spkrs], ref_words[ns % self.num_spkrs]\n                    )\n                    for ns in range(self.num_spkrs ** 2)\n                ]  # h1r1,h1r2,h2r1,h2r2\n                tmp_char_ed = [\n                    editdistance.eval(\n                        hyp_chars[ns // self.num_spkrs], ref_chars[ns % self.num_spkrs]\n                    )\n                    for ns in range(self.num_spkrs ** 2)\n                ]  # h1r1,h1r2,h2r1,h2r2\n\n                word_eds.append(self.pit.min_pit_sample(torch.tensor(tmp_word_ed))[0])\n                word_ref_lens.append(len(sum(ref_words, [])))\n                char_eds.append(self.pit.min_pit_sample(torch.tensor(tmp_char_ed))[0])\n                char_ref_lens.append(len("""".join(ref_chars)))\n\n            wer = (\n                0.0\n                if not self.report_wer\n                else float(sum(word_eds)) / sum(word_ref_lens)\n            )\n            cer = (\n                0.0\n                if not self.report_cer\n                else float(sum(char_eds)) / sum(char_ref_lens)\n            )\n\n        alpha = self.mtlalpha\n        if alpha == 0:\n            self.loss = loss_att\n            loss_att_data = float(loss_att)\n            loss_ctc_data = None\n        elif alpha == 1:\n            self.loss = loss_ctc\n            loss_att_data = None\n            loss_ctc_data = float(loss_ctc)\n        else:\n            self.loss = alpha * loss_ctc + (1 - alpha) * loss_att\n            loss_att_data = float(loss_att)\n            loss_ctc_data = float(loss_ctc)\n\n        loss_data = float(self.loss)\n        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):\n            self.reporter.report(loss_ctc_data, loss_att_data, acc, cer, wer, loss_data)\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def recognize(self, x, recog_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param ndarray x: input acoustic feature (T, D)\n        :param Namespace recog_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        prev = self.training\n        self.eval()\n        ilens = [x.shape[0]]\n\n        # subsample frame\n        x = x[:: self.subsample[0], :]\n        h = to_device(self, to_torch_tensor(x).float())\n        # make a utt list (1) to use the same interface for encoder\n        hs = h.contiguous().unsqueeze(0)\n\n        # 0. Frontend\n        if self.frontend is not None:\n            hs, hlens, mask = self.frontend(hs, ilens)\n            hlens_n = [None] * self.num_spkrs\n            for i in range(self.num_spkrs):\n                hs[i], hlens_n[i] = self.feature_transform(hs[i], hlens)\n            hlens = hlens_n\n        else:\n            hs, hlens = hs, ilens\n\n        # 1. Encoder\n        if not isinstance(hs, list):  # single-channel multi-speaker input x\n            hs, hlens, _ = self.enc(hs, hlens)\n        else:  # multi-channel multi-speaker input x\n            for i in range(self.num_spkrs):\n                hs[i], hlens[i], _ = self.enc(hs[i], hlens[i])\n\n        # calculate log P(z_t|X) for CTC scores\n        if recog_args.ctc_weight > 0.0:\n            lpz = [self.ctc.log_softmax(i)[0] for i in hs]\n        else:\n            lpz = None\n\n        # 2. decoder\n        # decode the first utterance\n        y = [\n            self.dec.recognize_beam(\n                hs[i][0], lpz[i], recog_args, char_list, rnnlm, strm_idx=i\n            )\n            for i in range(self.num_spkrs)\n        ]\n\n        if prev:\n            self.train()\n        return y\n\n    def recognize_batch(self, xs, recog_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param ndarray xs: input acoustic feature (T, D)\n        :param Namespace recog_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        prev = self.training\n        self.eval()\n        ilens = np.fromiter((xx.shape[0] for xx in xs), dtype=np.int64)\n\n        # subsample frame\n        xs = [xx[:: self.subsample[0], :] for xx in xs]\n        xs = [to_device(self, to_torch_tensor(xx).float()) for xx in xs]\n        xs_pad = pad_list(xs, 0.0)\n\n        # 0. Frontend\n        if self.frontend is not None:\n            hs_pad, hlens, mask = self.frontend(xs_pad, ilens)\n            hlens_n = [None] * self.num_spkrs\n            for i in range(self.num_spkrs):\n                hs_pad[i], hlens_n[i] = self.feature_transform(hs_pad[i], hlens)\n            hlens = hlens_n\n        else:\n            hs_pad, hlens = xs_pad, ilens\n\n        # 1. Encoder\n        if not isinstance(hs_pad, list):  # single-channel multi-speaker input x\n            hs_pad, hlens, _ = self.enc(hs_pad, hlens)\n        else:  # multi-channel multi-speaker input x\n            for i in range(self.num_spkrs):\n                hs_pad[i], hlens[i], _ = self.enc(hs_pad[i], hlens[i])\n\n        # calculate log P(z_t|X) for CTC scores\n        if recog_args.ctc_weight > 0.0:\n            lpz = [self.ctc.log_softmax(hs_pad[i]) for i in range(self.num_spkrs)]\n            normalize_score = False\n        else:\n            lpz = None\n            normalize_score = True\n\n        # 2. decoder\n        y = [\n            self.dec.recognize_beam_batch(\n                hs_pad[i],\n                hlens[i],\n                lpz[i],\n                recog_args,\n                char_list,\n                rnnlm,\n                normalize_score=normalize_score,\n                strm_idx=i,\n            )\n            for i in range(self.num_spkrs)\n        ]\n\n        if prev:\n            self.train()\n        return y\n\n    def enhance(self, xs):\n        """"""Forward only the frontend stage.\n\n        :param ndarray xs: input acoustic feature (T, C, F)\n        """"""\n        if self.frontend is None:\n            raise RuntimeError(""Frontend doesn\'t exist"")\n        prev = self.training\n        self.eval()\n        ilens = np.fromiter((xx.shape[0] for xx in xs), dtype=np.int64)\n\n        # subsample frame\n        xs = [xx[:: self.subsample[0], :] for xx in xs]\n        xs = [to_device(self, to_torch_tensor(xx).float()) for xx in xs]\n        xs_pad = pad_list(xs, 0.0)\n        enhanced, hlensm, mask = self.frontend(xs_pad, ilens)\n        if prev:\n            self.train()\n\n        if isinstance(enhanced, (tuple, list)):\n            enhanced = list(enhanced)\n            mask = list(mask)\n            for idx in range(len(enhanced)):  # number of speakers\n                enhanced[idx] = enhanced[idx].cpu().numpy()\n                mask[idx] = mask[idx].cpu().numpy()\n            return enhanced, mask, ilens\n        return enhanced.cpu().numpy(), mask.cpu().numpy(), ilens\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        """"""E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, num_spkrs, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n        """"""\n        with torch.no_grad():\n            # 0. Frontend\n            if self.frontend is not None:\n                hs_pad, hlens, mask = self.frontend(to_torch_tensor(xs_pad), ilens)\n                hlens_n = [None] * self.num_spkrs\n                for i in range(self.num_spkrs):\n                    hs_pad[i], hlens_n[i] = self.feature_transform(hs_pad[i], hlens)\n                hlens = hlens_n\n            else:\n                hs_pad, hlens = xs_pad, ilens\n\n            # 1. Encoder\n            if not isinstance(hs_pad, list):  # single-channel multi-speaker input x\n                hs_pad, hlens, _ = self.enc(hs_pad, hlens)\n            else:  # multi-channel multi-speaker input x\n                for i in range(self.num_spkrs):\n                    hs_pad[i], hlens[i], _ = self.enc(hs_pad[i], hlens[i])\n\n            # Permutation\n            ys_pad = ys_pad.transpose(0, 1)  # (num_spkrs, B, Lmax)\n            if self.num_spkrs <= 3:\n                loss_ctc = torch.stack(\n                    [\n                        self.ctc(\n                            hs_pad[i // self.num_spkrs],\n                            hlens[i // self.num_spkrs],\n                            ys_pad[i % self.num_spkrs],\n                        )\n                        for i in range(self.num_spkrs ** 2)\n                    ],\n                    1,\n                )  # (B, num_spkrs^2)\n                loss_ctc, min_perm = self.pit.pit_process(loss_ctc)\n            for i in range(ys_pad.size(1)):  # B\n                ys_pad[:, i] = ys_pad[min_perm[i], i]\n\n            # 2. Decoder\n            att_ws = [\n                self.dec.calculate_all_attentions(\n                    hs_pad[i], hlens[i], ys_pad[i], strm_idx=i\n                )\n                for i in range(self.num_spkrs)\n            ]\n\n        return att_ws\n\n\nclass EncoderMix(torch.nn.Module):\n    """"""Encoder module for the case of multi-speaker mixture speech.\n\n    :param str etype: type of encoder network\n    :param int idim: number of dimensions of encoder network\n    :param int elayers_sd:\n        number of layers of speaker differentiate part in encoder network\n    :param int elayers_rec:\n        number of layers of shared recognition part in encoder network\n    :param int eunits: number of lstm units of encoder network\n    :param int eprojs: number of projection units of encoder network\n    :param np.ndarray subsample: list of subsampling numbers\n    :param float dropout: dropout rate\n    :param int in_channel: number of input channels\n    :param int num_spkrs: number of number of speakers\n    """"""\n\n    def __init__(\n        self,\n        etype,\n        idim,\n        elayers_sd,\n        elayers_rec,\n        eunits,\n        eprojs,\n        subsample,\n        dropout,\n        num_spkrs=2,\n        in_channel=1,\n    ):\n        """"""Initialize the encoder of single-channel multi-speaker ASR.""""""\n        super(EncoderMix, self).__init__()\n        typ = etype.lstrip(""vgg"").rstrip(""p"")\n        if typ not in [""lstm"", ""gru"", ""blstm"", ""bgru""]:\n            logging.error(""Error: need to specify an appropriate encoder architecture"")\n        if etype.startswith(""vgg""):\n            if etype[-1] == ""p"":\n                self.enc_mix = torch.nn.ModuleList([VGG2L(in_channel)])\n                self.enc_sd = torch.nn.ModuleList(\n                    [\n                        torch.nn.ModuleList(\n                            [\n                                RNNP(\n                                    get_vgg2l_odim(idim, in_channel=in_channel),\n                                    elayers_sd,\n                                    eunits,\n                                    eprojs,\n                                    subsample[: elayers_sd + 1],\n                                    dropout,\n                                    typ=typ,\n                                )\n                            ]\n                        )\n                        for i in range(num_spkrs)\n                    ]\n                )\n                self.enc_rec = torch.nn.ModuleList(\n                    [\n                        RNNP(\n                            eprojs,\n                            elayers_rec,\n                            eunits,\n                            eprojs,\n                            subsample[elayers_sd:],\n                            dropout,\n                            typ=typ,\n                        )\n                    ]\n                )\n                logging.info(""Use CNN-VGG + B"" + typ.upper() + ""P for encoder"")\n            else:\n                logging.error(\n                    f""Error: need to specify an appropriate encoder architecture. ""\n                    f""Illegal name {etype}""\n                )\n                sys.exit()\n        else:\n            logging.error(\n                f""Error: need to specify an appropriate encoder architecture. ""\n                f""Illegal name {etype}""\n            )\n            sys.exit()\n\n        self.num_spkrs = num_spkrs\n\n    def forward(self, xs_pad, ilens):\n        """"""Encodermix forward.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :return: list: batch of hidden state sequences [num_spkrs x (B, Tmax, eprojs)]\n        :rtype: torch.Tensor\n        """"""\n        # mixture encoder\n        for module in self.enc_mix:\n            xs_pad, ilens, _ = module(xs_pad, ilens)\n\n        # SD and Rec encoder\n        xs_pad_sd = [xs_pad for i in range(self.num_spkrs)]\n        ilens_sd = [ilens for i in range(self.num_spkrs)]\n        for ns in range(self.num_spkrs):\n            # Encoder_SD: speaker differentiate encoder\n            for module in self.enc_sd[ns]:\n                xs_pad_sd[ns], ilens_sd[ns], _ = module(xs_pad_sd[ns], ilens_sd[ns])\n            # Encoder_Rec: recognition encoder\n            for module in self.enc_rec:\n                xs_pad_sd[ns], ilens_sd[ns], _ = module(xs_pad_sd[ns], ilens_sd[ns])\n\n        # make mask to remove bias value in padded part\n        mask = to_device(self, make_pad_mask(ilens_sd[0]).unsqueeze(-1))\n\n        return [x.masked_fill(mask, 0.0) for x in xs_pad_sd], ilens_sd, None\n\n\ndef encoder_for(args, idim, subsample):\n    """"""Construct the encoder.""""""\n    if getattr(args, ""use_frontend"", False):  # use getattr to keep compatibility\n        # with frontend, the mixed speech are separated as streams for each speaker\n        return encoder_for_single(args, idim, subsample)\n    else:\n        return EncoderMix(\n            args.etype,\n            idim,\n            args.elayers_sd,\n            args.elayers,\n            args.eunits,\n            args.eprojs,\n            subsample,\n            args.dropout_rate,\n            args.num_spkrs,\n        )\n'"
espnet/nets/pytorch_backend/e2e_asr_mulenc.py,19,"b'#!/usr/bin/env python3\n\n""""""Define e2e module for multi-encoder network. https://arxiv.org/pdf/1811.04903.pdf.""""""\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n# Copyright 2017 Johns Hopkins University (Ruizhi Li)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\nimport argparse\nfrom itertools import groupby\nimport logging\nimport math\nimport os\n\nimport chainer\nfrom chainer import reporter\nimport editdistance\nimport numpy as np\nimport torch\n\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.e2e_asr_common import label_smoothing_dist\nfrom espnet.nets.pytorch_backend.ctc import ctc_for\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.nets_utils import to_torch_tensor\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_for\nfrom espnet.nets.pytorch_backend.rnn.decoders import decoder_for\nfrom espnet.nets.pytorch_backend.rnn.encoders import encoder_for\nfrom espnet.nets.scorers.ctc import CTCPrefixScorer\nfrom espnet.utils.cli_utils import strtobool\n\nCTC_LOSS_THRESHOLD = 10000\n\n\nclass Reporter(chainer.Chain):\n    """"""Define a chainer reporter wrapper.""""""\n\n    def report(self, loss_ctc_list, loss_att, acc, cer_ctc_list, cer, wer, mtl_loss):\n        """"""Define a chainer reporter function.""""""\n        # loss_ctc_list = [weighted CTC, CTC1, CTC2, ... CTCN]\n        # cer_ctc_list = [weighted cer_ctc, cer_ctc_1, cer_ctc_2, ... cer_ctc_N]\n        num_encs = len(loss_ctc_list) - 1\n        reporter.report({""loss_ctc"": loss_ctc_list[0]}, self)\n        for i in range(num_encs):\n            reporter.report({""loss_ctc{}"".format(i + 1): loss_ctc_list[i + 1]}, self)\n        reporter.report({""loss_att"": loss_att}, self)\n        reporter.report({""acc"": acc}, self)\n        reporter.report({""cer_ctc"": cer_ctc_list[0]}, self)\n        for i in range(num_encs):\n            reporter.report({""cer_ctc{}"".format(i + 1): cer_ctc_list[i + 1]}, self)\n        reporter.report({""cer"": cer}, self)\n        reporter.report({""wer"": wer}, self)\n        logging.info(""mtl loss:"" + str(mtl_loss))\n        reporter.report({""loss"": mtl_loss}, self)\n\n\nclass E2E(ASRInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param List idims: List of dimensions of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments for multi-encoder setting.""""""\n        E2E.encoder_add_arguments(parser)\n        E2E.attention_add_arguments(parser)\n        E2E.decoder_add_arguments(parser)\n        E2E.ctc_add_arguments(parser)\n        return parser\n\n    @staticmethod\n    def encoder_add_arguments(parser):\n        """"""Add arguments for encoders in multi-encoder setting.""""""\n        group = parser.add_argument_group(""E2E encoder setting"")\n        group.add_argument(\n            ""--etype"",\n            action=""append"",\n            type=str,\n            choices=[\n                ""lstm"",\n                ""blstm"",\n                ""lstmp"",\n                ""blstmp"",\n                ""vgglstmp"",\n                ""vggblstmp"",\n                ""vgglstm"",\n                ""vggblstm"",\n                ""gru"",\n                ""bgru"",\n                ""grup"",\n                ""bgrup"",\n                ""vgggrup"",\n                ""vggbgrup"",\n                ""vgggru"",\n                ""vggbgru"",\n            ],\n            help=""Type of encoder network architecture"",\n        )\n        group.add_argument(\n            ""--elayers"",\n            type=int,\n            action=""append"",\n            help=""Number of encoder layers ""\n            ""(for shared recognition part in multi-speaker asr mode)"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            type=int,\n            action=""append"",\n            help=""Number of encoder hidden units"",\n        )\n        group.add_argument(\n            ""--eprojs"", default=320, type=int, help=""Number of encoder projection units""\n        )\n        group.add_argument(\n            ""--subsample"",\n            type=str,\n            action=""append"",\n            help=""Subsample input frames x_y_z means ""\n            ""subsample every x frame at 1st layer, ""\n            ""every y frame at 2nd layer etc."",\n        )\n        return parser\n\n    @staticmethod\n    def attention_add_arguments(parser):\n        """"""Add arguments for attentions in multi-encoder setting.""""""\n        group = parser.add_argument_group(""E2E attention setting"")\n        # attention\n        group.add_argument(\n            ""--atype"",\n            type=str,\n            action=""append"",\n            choices=[\n                ""noatt"",\n                ""dot"",\n                ""add"",\n                ""location"",\n                ""coverage"",\n                ""coverage_location"",\n                ""location2d"",\n                ""location_recurrent"",\n                ""multi_head_dot"",\n                ""multi_head_add"",\n                ""multi_head_loc"",\n                ""multi_head_multi_res_loc"",\n            ],\n            help=""Type of attention architecture"",\n        )\n        group.add_argument(\n            ""--adim"",\n            type=int,\n            action=""append"",\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--awin"",\n            type=int,\n            action=""append"",\n            help=""Window size for location2d attention"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            type=int,\n            action=""append"",\n            help=""Number of heads for multi head attention"",\n        )\n        group.add_argument(\n            ""--aconv-chans"",\n            type=int,\n            action=""append"",\n            help=""Number of attention convolution channels \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--aconv-filts"",\n            type=int,\n            action=""append"",\n            help=""Number of attention convolution filters \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--dropout-rate"",\n            type=float,\n            action=""append"",\n            help=""Dropout rate for the encoder"",\n        )\n        # hierarchical attention network (HAN)\n        group.add_argument(\n            ""--han-type"",\n            default=""dot"",\n            type=str,\n            choices=[\n                ""noatt"",\n                ""dot"",\n                ""add"",\n                ""location"",\n                ""coverage"",\n                ""coverage_location"",\n                ""location2d"",\n                ""location_recurrent"",\n                ""multi_head_dot"",\n                ""multi_head_add"",\n                ""multi_head_loc"",\n                ""multi_head_multi_res_loc"",\n            ],\n            help=""Type of attention architecture (multi-encoder asr mode only)"",\n        )\n        group.add_argument(\n            ""--han-dim"",\n            default=320,\n            type=int,\n            help=""Number of attention transformation dimensions in HAN"",\n        )\n        group.add_argument(\n            ""--han-win"",\n            default=5,\n            type=int,\n            help=""Window size for location2d attention in HAN"",\n        )\n        group.add_argument(\n            ""--han-heads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention in HAN"",\n        )\n        group.add_argument(\n            ""--han-conv-chans"",\n            default=-1,\n            type=int,\n            help=""Number of attention convolution channels  in HAN \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--han-conv-filts"",\n            default=100,\n            type=int,\n            help=""Number of attention convolution filters in HAN \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        return parser\n\n    @staticmethod\n    def decoder_add_arguments(parser):\n        """"""Add arguments for decoder in multi-encoder setting.""""""\n        group = parser.add_argument_group(""E2E decoder setting"")\n        group.add_argument(\n            ""--dtype"",\n            default=""lstm"",\n            type=str,\n            choices=[""lstm"", ""gru""],\n            help=""Type of decoder network architecture"",\n        )\n        group.add_argument(\n            ""--dlayers"", default=1, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=320, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--dropout-rate-decoder"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the decoder"",\n        )\n        group.add_argument(\n            ""--sampling-probability"",\n            default=0.0,\n            type=float,\n            help=""Ratio of predicted labels fed back to decoder"",\n        )\n        group.add_argument(\n            ""--lsm-type"",\n            const="""",\n            default="""",\n            type=str,\n            nargs=""?"",\n            choices=["""", ""unigram""],\n            help=""Apply label smoothing with a specified distribution type"",\n        )\n        return parser\n\n    @staticmethod\n    def ctc_add_arguments(parser):\n        """"""Add arguments for ctc in multi-encoder setting.""""""\n        group = parser.add_argument_group(""E2E multi-ctc setting"")\n        group.add_argument(\n            ""--share-ctc"",\n            type=strtobool,\n            default=False,\n            help=""The flag to switch to share ctc across multiple encoders ""\n            ""(multi-encoder asr mode only)."",\n        )\n        group.add_argument(\n            ""--weights-ctc-train"",\n            type=float,\n            action=""append"",\n            help=""ctc weight assigned to each encoder during training."",\n        )\n        group.add_argument(\n            ""--weights-ctc-dec"",\n            type=float,\n            action=""append"",\n            help=""ctc weight assigned to each encoder during decoding."",\n        )\n        return parser\n\n    def __init__(self, idims, odim, args):\n        """"""Initialize this class with python-level args.\n\n        Args:\n            idims (list): list of the number of an input feature dim.\n            odim (int): The number of output vocab.\n            args (Namespace): arguments\n\n        """"""\n        super(E2E, self).__init__()\n        torch.nn.Module.__init__(self)\n        self.mtlalpha = args.mtlalpha\n        assert 0.0 <= self.mtlalpha <= 1.0, ""mtlalpha should be [0.0, 1.0]""\n        self.verbose = args.verbose\n        # NOTE: for self.build method\n        args.char_list = getattr(args, ""char_list"", None)\n        self.char_list = args.char_list\n        self.outdir = args.outdir\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n        self.reporter = Reporter()\n        self.num_encs = args.num_encs\n        self.share_ctc = args.share_ctc\n\n        # below means the last number becomes eos/sos ID\n        # note that sos/eos IDs are identical\n        self.sos = odim - 1\n        self.eos = odim - 1\n\n        # subsample info\n        self.subsample_list = get_subsample(args, mode=""asr"", arch=""rnn_mulenc"")\n\n        # label smoothing info\n        if args.lsm_type and os.path.isfile(args.train_json):\n            logging.info(""Use label smoothing with "" + args.lsm_type)\n            labeldist = label_smoothing_dist(\n                odim, args.lsm_type, transcript=args.train_json\n            )\n        else:\n            labeldist = None\n\n        # speech translation related\n        self.replace_sos = getattr(\n            args, ""replace_sos"", False\n        )  # use getattr to keep compatibility\n\n        self.frontend = None\n\n        # encoder\n        self.enc = encoder_for(args, idims, self.subsample_list)\n        # ctc\n        self.ctc = ctc_for(args, odim)\n        # attention\n        self.att = att_for(args)\n        # hierarchical attention network\n        han = att_for(args, han_mode=True)\n        self.att.append(han)\n        # decoder\n        self.dec = decoder_for(args, odim, self.sos, self.eos, self.att, labeldist)\n\n        if args.mtlalpha > 0 and self.num_encs > 1:\n            # weights-ctc,\n            # e.g. ctc_loss = w_1*ctc_1_loss + w_2 * ctc_2_loss + w_N * ctc_N_loss\n            self.weights_ctc_train = args.weights_ctc_train / np.sum(\n                args.weights_ctc_train\n            )  # normalize\n            self.weights_ctc_dec = args.weights_ctc_dec / np.sum(\n                args.weights_ctc_dec\n            )  # normalize\n            logging.info(\n                ""ctc weights (training during training): ""\n                + "" "".join([str(x) for x in self.weights_ctc_train])\n            )\n            logging.info(\n                ""ctc weights (decoding during training): ""\n                + "" "".join([str(x) for x in self.weights_ctc_dec])\n            )\n        else:\n            self.weights_ctc_dec = [1.0]\n            self.weights_ctc_train = [1.0]\n\n        # weight initialization\n        self.init_like_chainer()\n\n        # options for beam search\n        if args.report_cer or args.report_wer:\n            recog_args = {\n                ""beam_size"": args.beam_size,\n                ""penalty"": args.penalty,\n                ""ctc_weight"": args.ctc_weight,\n                ""maxlenratio"": args.maxlenratio,\n                ""minlenratio"": args.minlenratio,\n                ""lm_weight"": args.lm_weight,\n                ""rnnlm"": args.rnnlm,\n                ""nbest"": args.nbest,\n                ""space"": args.sym_space,\n                ""blank"": args.sym_blank,\n                ""tgt_lang"": False,\n                ""ctc_weights_dec"": self.weights_ctc_dec,\n            }\n\n            self.recog_args = argparse.Namespace(**recog_args)\n            self.report_cer = args.report_cer\n            self.report_wer = args.report_wer\n        else:\n            self.report_cer = False\n            self.report_wer = False\n        self.rnnlm = None\n\n        self.logzero = -10000000000.0\n        self.loss = None\n        self.acc = None\n\n    def init_like_chainer(self):\n        """"""Initialize weight like chainer.\n\n        chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0\n        pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5)\n\n        however, there are two exceptions as far as I know.\n        - EmbedID.W ~ Normal(0, 1)\n        - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM)\n        """"""\n\n        def lecun_normal_init_parameters(module):\n            for p in module.parameters():\n                data = p.data\n                if data.dim() == 1:\n                    # bias\n                    data.zero_()\n                elif data.dim() == 2:\n                    # linear weight\n                    n = data.size(1)\n                    stdv = 1.0 / math.sqrt(n)\n                    data.normal_(0, stdv)\n                elif data.dim() in (3, 4):\n                    # conv weight\n                    n = data.size(1)\n                    for k in data.size()[2:]:\n                        n *= k\n                    stdv = 1.0 / math.sqrt(n)\n                    data.normal_(0, stdv)\n                else:\n                    raise NotImplementedError\n\n        def set_forget_bias_to_one(bias):\n            n = bias.size(0)\n            start, end = n // 4, n // 2\n            bias.data[start:end].fill_(1.0)\n\n        lecun_normal_init_parameters(self)\n        # exceptions\n        # embed weight ~ Normal(0, 1)\n        self.dec.embed.weight.data.normal_(0, 1)\n        # forget-bias = 1.0\n        # https://discuss.pytorch.org/t/set-forget-gate-bias-of-lstm/1745\n        for i in range(len(self.dec.decoder)):\n            set_forget_bias_to_one(self.dec.decoder[i].bias_ih)\n\n    def forward(self, xs_pad_list, ilens_list, ys_pad):\n        """"""E2E forward.\n\n        :param List xs_pad_list: list of batch (torch.Tensor) of padded input sequences\n                                [(B, Tmax_1, idim), (B, Tmax_2, idim),..]\n        :param List ilens_list:\n            list of batch (torch.Tensor) of lengths of input sequences [(B), (B), ..]\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, Lmax)\n        :return: loss value\n        :rtype: torch.Tensor\n        """"""\n        if self.replace_sos:\n            tgt_lang_ids = ys_pad[:, 0:1]\n            ys_pad = ys_pad[:, 1:]  # remove target language ID in the beginning\n        else:\n            tgt_lang_ids = None\n\n        hs_pad_list, hlens_list, self.loss_ctc_list = [], [], []\n        for idx in range(self.num_encs):\n            # 1. Encoder\n            hs_pad, hlens, _ = self.enc[idx](xs_pad_list[idx], ilens_list[idx])\n\n            # 2. CTC loss\n            if self.mtlalpha == 0:\n                self.loss_ctc_list.append(None)\n            else:\n                ctc_idx = 0 if self.share_ctc else idx\n                loss_ctc = self.ctc[ctc_idx](hs_pad, hlens, ys_pad)\n                self.loss_ctc_list.append(loss_ctc)\n            hs_pad_list.append(hs_pad)\n            hlens_list.append(hlens)\n\n        # 3. attention loss\n        if self.mtlalpha == 1:\n            self.loss_att, acc = None, None\n        else:\n            self.loss_att, acc, _ = self.dec(\n                hs_pad_list, hlens_list, ys_pad, lang_ids=tgt_lang_ids\n            )\n        self.acc = acc\n\n        # 4. compute cer without beam search\n        if self.mtlalpha == 0 or self.char_list is None:\n            cer_ctc_list = [None] * (self.num_encs + 1)\n        else:\n            cer_ctc_list = []\n            for ind in range(self.num_encs):\n                cers = []\n                ctc_idx = 0 if self.share_ctc else ind\n                y_hats = self.ctc[ctc_idx].argmax(hs_pad_list[ind]).data\n                for i, y in enumerate(y_hats):\n                    y_hat = [x[0] for x in groupby(y)]\n                    y_true = ys_pad[i]\n\n                    seq_hat = [\n                        self.char_list[int(idx)] for idx in y_hat if int(idx) != -1\n                    ]\n                    seq_true = [\n                        self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                    ]\n                    seq_hat_text = """".join(seq_hat).replace(self.space, "" "")\n                    seq_hat_text = seq_hat_text.replace(self.blank, """")\n                    seq_true_text = """".join(seq_true).replace(self.space, "" "")\n\n                    hyp_chars = seq_hat_text.replace("" "", """")\n                    ref_chars = seq_true_text.replace("" "", """")\n                    if len(ref_chars) > 0:\n                        cers.append(\n                            editdistance.eval(hyp_chars, ref_chars) / len(ref_chars)\n                        )\n\n                cer_ctc = sum(cers) / len(cers) if cers else None\n                cer_ctc_list.append(cer_ctc)\n            cer_ctc_weighted = np.sum(\n                [\n                    item * self.weights_ctc_train[i]\n                    for i, item in enumerate(cer_ctc_list)\n                ]\n            )\n            cer_ctc_list = [float(cer_ctc_weighted)] + [\n                float(item) for item in cer_ctc_list\n            ]\n\n        # 5. compute cer/wer\n        if self.training or not (self.report_cer or self.report_wer):\n            cer, wer = 0.0, 0.0\n            # oracle_cer, oracle_wer = 0.0, 0.0\n        else:\n            if self.recog_args.ctc_weight > 0.0:\n                lpz_list = []\n                for idx in range(self.num_encs):\n                    ctc_idx = 0 if self.share_ctc else idx\n                    lpz = self.ctc[ctc_idx].log_softmax(hs_pad_list[idx]).data\n                    lpz_list.append(lpz)\n            else:\n                lpz_list = None\n\n            word_eds, word_ref_lens, char_eds, char_ref_lens = [], [], [], []\n            nbest_hyps = self.dec.recognize_beam_batch(\n                hs_pad_list,\n                hlens_list,\n                lpz_list,\n                self.recog_args,\n                self.char_list,\n                self.rnnlm,\n                lang_ids=tgt_lang_ids.squeeze(1).tolist() if self.replace_sos else None,\n            )\n            # remove <sos> and <eos>\n            y_hats = [nbest_hyp[0][""yseq""][1:-1] for nbest_hyp in nbest_hyps]\n            for i, y_hat in enumerate(y_hats):\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = """".join(seq_hat).replace(self.recog_args.space, "" "")\n                seq_hat_text = seq_hat_text.replace(self.recog_args.blank, """")\n                seq_true_text = """".join(seq_true).replace(self.recog_args.space, "" "")\n\n                hyp_words = seq_hat_text.split()\n                ref_words = seq_true_text.split()\n                word_eds.append(editdistance.eval(hyp_words, ref_words))\n                word_ref_lens.append(len(ref_words))\n                hyp_chars = seq_hat_text.replace("" "", """")\n                ref_chars = seq_true_text.replace("" "", """")\n                char_eds.append(editdistance.eval(hyp_chars, ref_chars))\n                char_ref_lens.append(len(ref_chars))\n\n            wer = (\n                0.0\n                if not self.report_wer\n                else float(sum(word_eds)) / sum(word_ref_lens)\n            )\n            cer = (\n                0.0\n                if not self.report_cer\n                else float(sum(char_eds)) / sum(char_ref_lens)\n            )\n\n        alpha = self.mtlalpha\n        if alpha == 0:\n            self.loss = self.loss_att\n            loss_att_data = float(self.loss_att)\n            loss_ctc_data_list = [None] * (self.num_encs + 1)\n        elif alpha == 1:\n            self.loss = torch.sum(\n                torch.cat(\n                    [\n                        (item * self.weights_ctc_train[i]).unsqueeze(0)\n                        for i, item in enumerate(self.loss_ctc_list)\n                    ]\n                )\n            )\n            loss_att_data = None\n            loss_ctc_data_list = [float(self.loss)] + [\n                float(item) for item in self.loss_ctc_list\n            ]\n        else:\n            self.loss_ctc = torch.sum(\n                torch.cat(\n                    [\n                        (item * self.weights_ctc_train[i]).unsqueeze(0)\n                        for i, item in enumerate(self.loss_ctc_list)\n                    ]\n                )\n            )\n            self.loss = alpha * self.loss_ctc + (1 - alpha) * self.loss_att\n            loss_att_data = float(self.loss_att)\n            loss_ctc_data_list = [float(self.loss_ctc)] + [\n                float(item) for item in self.loss_ctc_list\n            ]\n\n        loss_data = float(self.loss)\n        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):\n            self.reporter.report(\n                loss_ctc_data_list,\n                loss_att_data,\n                acc,\n                cer_ctc_list,\n                cer,\n                wer,\n                loss_data,\n            )\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def scorers(self):\n        """"""Get scorers for `beam_search` (optional).\n\n        Returns:\n            dict[str, ScorerInterface]: dict of `ScorerInterface` objects\n\n        """"""\n        return dict(decoder=self.dec, ctc=CTCPrefixScorer(self.ctc, self.eos))\n\n    def encode(self, x_list):\n        """"""Encode feature.\n\n        Args:\n            x_list (list): input feature [(T1, D), (T2, D), ... ]\n        Returns:\n            list\n                encoded feature [(T1, D), (T2, D), ... ]\n\n        """"""\n        self.eval()\n        ilens_list = [[x_list[idx].shape[0]] for idx in range(self.num_encs)]\n\n        # subsample frame\n        x_list = [\n            x_list[idx][:: self.subsample_list[idx][0], :]\n            for idx in range(self.num_encs)\n        ]\n        p = next(self.parameters())\n        x_list = [\n            torch.as_tensor(x_list[idx], device=p.device, dtype=p.dtype)\n            for idx in range(self.num_encs)\n        ]\n        # make a utt list (1) to use the same interface for encoder\n        xs_list = [\n            x_list[idx].contiguous().unsqueeze(0) for idx in range(self.num_encs)\n        ]\n\n        # 1. encoder\n        hs_list = []\n        for idx in range(self.num_encs):\n            hs, _, _ = self.enc[idx](xs_list[idx], ilens_list[idx])\n            hs_list.append(hs[0])\n        return hs_list\n\n    def recognize(self, x_list, recog_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param list of ndarray x: list of input acoustic feature [(T1, D), (T2,D),...]\n        :param Namespace recog_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        hs_list = self.encode(x_list)\n        # calculate log P(z_t|X) for CTC scores\n        if recog_args.ctc_weight > 0.0:\n            if self.share_ctc:\n                lpz_list = [\n                    self.ctc[0].log_softmax(hs_list[idx].unsqueeze(0))[0]\n                    for idx in range(self.num_encs)\n                ]\n            else:\n                lpz_list = [\n                    self.ctc[idx].log_softmax(hs_list[idx].unsqueeze(0))[0]\n                    for idx in range(self.num_encs)\n                ]\n        else:\n            lpz_list = None\n\n        # 2. Decoder\n        # decode the first utterance\n        y = self.dec.recognize_beam(hs_list, lpz_list, recog_args, char_list, rnnlm)\n        return y\n\n    def recognize_batch(self, xs_list, recog_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param list xs_list: list of list of input acoustic feature arrays\n                [[(T1_1, D), (T1_2, D), ...],[(T2_1, D), (T2_2, D), ...], ...]\n        :param Namespace recog_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        prev = self.training\n        self.eval()\n        ilens_list = [\n            np.fromiter((xx.shape[0] for xx in xs_list[idx]), dtype=np.int64)\n            for idx in range(self.num_encs)\n        ]\n\n        # subsample frame\n        xs_list = [\n            [xx[:: self.subsample_list[idx][0], :] for xx in xs_list[idx]]\n            for idx in range(self.num_encs)\n        ]\n\n        xs_list = [\n            [to_device(self, to_torch_tensor(xx).float()) for xx in xs_list[idx]]\n            for idx in range(self.num_encs)\n        ]\n        xs_pad_list = [pad_list(xs_list[idx], 0.0) for idx in range(self.num_encs)]\n\n        # 1. Encoder\n        hs_pad_list, hlens_list = [], []\n        for idx in range(self.num_encs):\n            hs_pad, hlens, _ = self.enc[idx](xs_pad_list[idx], ilens_list[idx])\n            hs_pad_list.append(hs_pad)\n            hlens_list.append(hlens)\n\n        # calculate log P(z_t|X) for CTC scores\n        if recog_args.ctc_weight > 0.0:\n            if self.share_ctc:\n                lpz_list = [\n                    self.ctc[0].log_softmax(hs_pad_list[idx])\n                    for idx in range(self.num_encs)\n                ]\n            else:\n                lpz_list = [\n                    self.ctc[idx].log_softmax(hs_pad_list[idx])\n                    for idx in range(self.num_encs)\n                ]\n            normalize_score = False\n        else:\n            lpz_list = None\n            normalize_score = True\n\n        # 2. Decoder\n        hlens_list = [\n            torch.tensor(list(map(int, hlens_list[idx])))\n            for idx in range(self.num_encs)\n        ]  # make sure hlens is tensor\n        y = self.dec.recognize_beam_batch(\n            hs_pad_list,\n            hlens_list,\n            lpz_list,\n            recog_args,\n            char_list,\n            rnnlm,\n            normalize_score=normalize_score,\n        )\n\n        if prev:\n            self.train()\n        return y\n\n    def calculate_all_attentions(self, xs_pad_list, ilens_list, ys_pad):\n        """"""E2E attention calculation.\n\n        :param List xs_pad_list: list of batch (torch.Tensor) of padded input sequences\n                                [(B, Tmax_1, idim), (B, Tmax_2, idim),..]\n        :param List ilens_list:\n            list of batch (torch.Tensor) of lengths of input sequences [(B), (B), ..]\n        :param torch.Tensor ys_pad:\n            batch of padded character id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) multi-encoder case\n                => [(B, Lmax, Tmax1), (B, Lmax, Tmax2), ..., (B, Lmax, NumEncs)]\n            3) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray or list\n        """"""\n        with torch.no_grad():\n            # 1. Encoder\n            if self.replace_sos:\n                tgt_lang_ids = ys_pad[:, 0:1]\n                ys_pad = ys_pad[:, 1:]  # remove target language ID in the beggining\n            else:\n                tgt_lang_ids = None\n\n            hs_pad_list, hlens_list = [], []\n            for idx in range(self.num_encs):\n                hs_pad, hlens, _ = self.enc[idx](xs_pad_list[idx], ilens_list[idx])\n                hs_pad_list.append(hs_pad)\n                hlens_list.append(hlens)\n\n            # 2. Decoder\n            att_ws = self.dec.calculate_all_attentions(\n                hs_pad_list, hlens_list, ys_pad, lang_ids=tgt_lang_ids\n            )\n\n        return att_ws\n'"
espnet/nets/pytorch_backend/e2e_asr_transducer.py,15,"b'""""""Transducer speech recognition model (pytorch).""""""\n\nfrom distutils.util import strtobool\nimport logging\nimport math\n\nimport chainer\nfrom chainer import reporter\nimport torch\n\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.nets_utils import to_torch_tensor\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_for\nfrom espnet.nets.pytorch_backend.rnn.encoders import encoder_for\nfrom espnet.nets.pytorch_backend.transducer.initializer import initializer\nfrom espnet.nets.pytorch_backend.transducer.loss import TransLoss\nfrom espnet.nets.pytorch_backend.transducer.rnn_decoders import decoder_for\nfrom espnet.nets.pytorch_backend.transducer.transformer_decoder import Decoder\nfrom espnet.nets.pytorch_backend.transducer.utils import prepare_loss_inputs\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.mask import target_mask\n\n\nclass Reporter(chainer.Chain):\n    """"""A chainer reporter wrapper for transducer models.""""""\n\n    def report(self, loss, cer, wer):\n        """"""Instantiate reporter attributes.""""""\n        reporter.report({""cer"": cer}, self)\n        reporter.report({""wer"": wer}, self)\n        reporter.report({""loss"": loss}, self)\n\n        logging.info(""loss:"" + str(loss))\n\n\nclass E2E(ASRInterface, torch.nn.Module):\n    """"""E2E module.\n\n    Args:\n        idim (int): dimension of inputs\n        odim (int): dimension of outputs\n        args (Namespace): argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Extend arguments for transducer models.\n\n        Both Transformer and RNN modules are supported.\n        General options encapsulate both modules options.\n\n        """"""\n        group = parser.add_argument_group(""transformer model setting"")\n\n        # Encoder - general\n        group.add_argument(\n            ""--etype"",\n            default=""blstmp"",\n            type=str,\n            choices=[\n                ""transformer"",\n                ""lstm"",\n                ""blstm"",\n                ""lstmp"",\n                ""blstmp"",\n                ""vgglstmp"",\n                ""vggblstmp"",\n                ""vgglstm"",\n                ""vggblstm"",\n                ""gru"",\n                ""bgru"",\n                ""grup"",\n                ""bgrup"",\n                ""vgggrup"",\n                ""vggbgrup"",\n                ""vgggru"",\n                ""vggbgru"",\n            ],\n            help=""Type of encoder network architecture"",\n        )\n        group.add_argument(\n            ""--elayers"",\n            default=4,\n            type=int,\n            help=""Number of encoder layers (for shared recognition part ""\n            ""in multi-speaker asr mode)"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=300,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        # Encoder - RNN\n        group.add_argument(\n            ""--eprojs"", default=320, type=int, help=""Number of encoder projection units""\n        )\n        group.add_argument(\n            ""--subsample"",\n            default=""1"",\n            type=str,\n            help=""Subsample input frames x_y_z means subsample every x frame ""\n            ""at 1st layer, every y frame at 2nd layer etc."",\n        )\n        # Attention - general\n        group.add_argument(\n            ""--adim"",\n            default=320,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        group.add_argument(\n            ""--transformer-attn-dropout-rate-encoder"",\n            default=0.0,\n            type=float,\n            help=""dropout in transformer decoder attention."",\n        )\n        group.add_argument(\n            ""--transformer-attn-dropout-rate-decoder"",\n            default=0.0,\n            type=float,\n            help=""dropout in transformer decoder attention."",\n        )\n        # Attention - RNN\n        group.add_argument(\n            ""--atype"",\n            default=""location"",\n            type=str,\n            choices=[\n                ""noatt"",\n                ""dot"",\n                ""add"",\n                ""location"",\n                ""coverage"",\n                ""coverage_location"",\n                ""location2d"",\n                ""location_recurrent"",\n                ""multi_head_dot"",\n                ""multi_head_add"",\n                ""multi_head_loc"",\n                ""multi_head_multi_res_loc"",\n            ],\n            help=""Type of attention architecture"",\n        )\n        group.add_argument(\n            ""--awin"", default=5, type=int, help=""Window size for location2d attention""\n        )\n        group.add_argument(\n            ""--aconv-chans"",\n            default=10,\n            type=int,\n            help=""Number of attention convolution channels ""\n            ""(negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--aconv-filts"",\n            default=100,\n            type=int,\n            help=""Number of attention convolution filters ""\n            ""(negative value indicates no location-aware attention)"",\n        )\n        # Decoder - general\n        group.add_argument(\n            ""--dtype"",\n            default=""lstm"",\n            type=str,\n            choices=[""lstm"", ""gru"", ""transformer""],\n            help=""Type of decoder to use."",\n        )\n        group.add_argument(\n            ""--dlayers"", default=1, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=320, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--dropout-rate-decoder"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the decoder"",\n        )\n        # Decoder - RNN\n        group.add_argument(\n            ""--dec-embed-dim"",\n            default=320,\n            type=int,\n            help=""Number of decoder embeddings dimensions"",\n        )\n        group.add_argument(\n            ""--dropout-rate-embed-decoder"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the decoder embeddings"",\n        )\n        # Transformer\n        group.add_argument(\n            ""--transformer-warmup-steps"",\n            default=25000,\n            type=int,\n            help=""optimizer warmup steps"",\n        )\n        group.add_argument(\n            ""--transformer-init"",\n            type=str,\n            default=""pytorch"",\n            choices=[\n                ""pytorch"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n            ],\n            help=""how to initialize transformer parameters"",\n        )\n        group.add_argument(\n            ""--transformer-input-layer"",\n            type=str,\n            default=""conv2d"",\n            choices=[""conv2d"", ""vgg2l"", ""linear"", ""embed""],\n            help=""transformer encoder input layer type"",\n        )\n        group.add_argument(\n            ""--transformer-dec-input-layer"",\n            type=str,\n            default=""embed"",\n            choices=[""linear"", ""embed""],\n            help=""transformer decoder input layer type"",\n        )\n        group.add_argument(\n            ""--transformer-lr"",\n            default=10.0,\n            type=float,\n            help=""Initial value of learning rate"",\n        )\n        # Transducer\n        group.add_argument(\n            ""--trans-type"",\n            default=""warp-transducer"",\n            type=str,\n            choices=[""warp-transducer""],\n            help=""Type of transducer implementation to calculate loss."",\n        )\n        group.add_argument(\n            ""--rnnt-mode"",\n            default=""rnnt"",\n            type=str,\n            choices=[""rnnt"", ""rnnt-att""],\n            help=""Transducer mode for RNN decoder."",\n        )\n        group.add_argument(\n            ""--joint-dim"",\n            default=320,\n            type=int,\n            help=""Number of dimensions in joint space"",\n        )\n        group.add_argument(\n            ""--score-norm-transducer"",\n            type=strtobool,\n            nargs=""?"",\n            default=True,\n            help=""Normalize transducer scores by length"",\n        )\n\n        return parser\n\n    def __init__(self, idim, odim, args, ignore_id=-1, blank_id=0):\n        """"""Construct an E2E object for transducer model.\n\n        Args:\n            idim (int): dimension of inputs\n            odim (int): dimension of outputs\n            args (Namespace): argument Namespace containing options\n\n        """"""\n        torch.nn.Module.__init__(self)\n\n        if args.etype == ""transformer"":\n            self.subsample = get_subsample(args, mode=""asr"", arch=""transformer"")\n\n            self.encoder = Encoder(\n                idim=idim,\n                attention_dim=args.adim,\n                attention_heads=args.aheads,\n                linear_units=args.eunits,\n                num_blocks=args.elayers,\n                input_layer=args.transformer_input_layer,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                attention_dropout_rate=args.transformer_attn_dropout_rate_encoder,\n            )\n        else:\n            self.subsample = get_subsample(args, mode=""asr"", arch=""rnn-t"")\n\n            self.enc = encoder_for(args, idim, self.subsample)\n\n        if args.dtype == ""transformer"":\n            self.decoder = Decoder(\n                odim=odim,\n                jdim=args.joint_dim,\n                attention_dim=args.adim,\n                attention_heads=args.aheads,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                input_layer=args.transformer_dec_input_layer,\n                dropout_rate=args.dropout_rate_decoder,\n                positional_dropout_rate=args.dropout_rate_decoder,\n                attention_dropout_rate=args.transformer_attn_dropout_rate_decoder,\n            )\n        else:\n            if args.etype == ""transformer"":\n                args.eprojs = args.adim\n\n            if args.rnnt_mode == ""rnnt-att"":\n                self.att = att_for(args)\n                self.dec = decoder_for(args, odim, self.att)\n            else:\n                self.dec = decoder_for(args, odim)\n\n        self.etype = args.etype\n        self.dtype = args.dtype\n        self.rnnt_mode = args.rnnt_mode\n\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.blank_id = blank_id\n        self.ignore_id = ignore_id\n\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n\n        self.odim = odim\n        self.adim = args.adim\n\n        self.reporter = Reporter()\n\n        self.criterion = TransLoss(args.trans_type, self.blank_id)\n\n        self.default_parameters(args)\n\n        if args.report_cer or args.report_wer:\n            from espnet.nets.e2e_asr_common import ErrorCalculatorTrans\n\n            if self.dtype == ""transformer"":\n                self.error_calculator = ErrorCalculatorTrans(self.decoder, args)\n            else:\n                self.error_calculator = ErrorCalculatorTrans(self.dec, args)\n        else:\n            self.error_calculator = None\n\n        self.logzero = -10000000000.0\n        self.loss = None\n        self.rnnlm = None\n\n    def default_parameters(self, args):\n        """"""Initialize/reset parameters for transducer.""""""\n        initializer(self, args)\n\n    def forward(self, xs_pad, ilens, ys_pad):\n        """"""E2E forward.\n\n        Args:\n            xs_pad (torch.Tensor): batch of padded source sequences (B, Tmax, idim)\n            ilens (torch.Tensor): batch of lengths of input sequences (B)\n            ys_pad (torch.Tensor): batch of padded target sequences (B, Lmax)\n\n        Returns:\n            loss (torch.Tensor): transducer loss value\n\n        """"""\n        # 1. encoder\n        if self.etype == ""transformer"":\n            xs_pad = xs_pad[:, : max(ilens)]\n            src_mask = make_non_pad_mask(ilens.tolist()).to(xs_pad.device).unsqueeze(-2)\n\n            hs_pad, hs_mask = self.encoder(xs_pad, src_mask)\n        else:\n            hs_pad, hs_mask, _ = self.enc(xs_pad, ilens)\n        self.hs_pad = hs_pad\n\n        # 1.5. transducer preparation related\n        ys_in_pad, target, pred_len, target_len = prepare_loss_inputs(ys_pad, hs_mask)\n\n        # 2. decoder\n        if self.dtype == ""transformer"":\n            ys_mask = target_mask(ys_in_pad, self.blank_id)\n            pred_pad, _ = self.decoder(ys_in_pad, ys_mask, hs_pad)\n        else:\n            if self.rnnt_mode == ""rnnt"":\n                pred_pad = self.dec(hs_pad, ys_in_pad)\n            else:\n                pred_pad = self.dec(hs_pad, ys_in_pad, pred_len)\n        self.pred_pad = pred_pad\n\n        # 3. loss computation\n        loss = self.criterion(pred_pad, target, pred_len, target_len)\n\n        self.loss = loss\n        loss_data = float(self.loss)\n\n        # 4. compute cer/wer\n        if self.training or self.error_calculator is None:\n            cer, wer = None, None\n        else:\n            cer, wer = self.error_calculator(hs_pad, ys_pad)\n\n        if not math.isnan(loss_data):\n            self.reporter.report(loss_data, cer, wer)\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n\n        return self.loss\n\n    def encode_transformer(self, x):\n        """"""Encode acoustic features.\n\n        Args:\n            x (ndarray): input acoustic feature (T, D)\n\n        Returns:\n            x (torch.Tensor): encoded features (T, attention_dim)\n\n        """"""\n        self.eval()\n\n        x = torch.as_tensor(x).unsqueeze(0)\n        enc_output, _ = self.encoder(x, None)\n\n        return enc_output.squeeze(0)\n\n    def encode_rnn(self, x):\n        """"""Encode acoustic features.\n\n        Args:\n            x (ndarray): input acoustic feature (T, D)\n\n        Returns:\n            x (torch.Tensor): encoded features (T, attention_dim)\n\n        """"""\n        self.eval()\n\n        ilens = [x.shape[0]]\n\n        x = x[:: self.subsample[0], :]\n        h = to_device(self, to_torch_tensor(x).float())\n        hs = h.contiguous().unsqueeze(0)\n\n        h, _, _ = self.enc(hs, ilens)\n\n        return h[0]\n\n    def recognize(self, x, recog_args, char_list=None, rnnlm=None):\n        """"""Recognize input features.\n\n        Args:\n            x (ndarray): input acoustic feature (T, D)\n            recog_args (namespace): argument Namespace containing options\n            char_list (list): list of characters\n            rnnlm (torch.nn.Module): language model module\n\n        Returns:\n            y (list): n-best decoding results\n\n        """"""\n        if self.etype == ""transformer"":\n            h = self.encode_transformer(x)\n        else:\n            h = self.encode_rnn(x)\n        params = [h, recog_args]\n\n        if recog_args.beam_size == 1:\n            if self.dtype == ""transformer"":\n                nbest_hyps = self.decoder.recognize(*params)\n            else:\n                nbest_hyps = self.dec.recognize(*params)\n        else:\n            params.append(rnnlm)\n            if self.dtype == ""transformer"":\n                nbest_hyps = self.decoder.recognize_beam(*params)\n            else:\n                nbest_hyps = self.dec.recognize_beam(*params)\n\n        return nbest_hyps\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        """"""E2E attention calculation.\n\n        Args:\n            xs_pad (torch.Tensor): batch of padded input sequences (B, Tmax, idim)\n            ilens (torch.Tensor): batch of lengths of input sequences (B)\n            ys_pad (torch.Tensor):\n                batch of padded character id sequence tensor (B, Lmax)\n\n        Returns:\n            ret (ndarray): attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n\n        """"""\n        if (\n            self.etype == ""transformer""\n            and self.dtype != ""transformer""\n            and self.rnnt_mode == ""rnnt-att""\n        ):\n            raise NotImplementedError(\n                ""Transformer encoder with rnn attention decoder"" ""is not supported yet.""\n            )\n        elif self.etype != ""transformer"" and self.dtype != ""transformer"":\n            if self.rnnt_mode == ""rnnt"":\n                return []\n            else:\n                with torch.no_grad():\n                    hs_pad, hlens = xs_pad, ilens\n                    hpad, hlens, _ = self.enc(hs_pad, hlens)\n\n                    ret = self.dec.calculate_all_attentions(hpad, hlens, ys_pad)\n        else:\n            with torch.no_grad():\n                self.forward(xs_pad, ilens, ys_pad)\n\n                ret = dict()\n                for name, m in self.named_modules():\n                    if isinstance(m, MultiHeadedAttention):\n                        ret[name] = m.attn.cpu().numpy()\n\n        return ret\n'"
espnet/nets/pytorch_backend/e2e_asr_transformer.py,20,"b'# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Transformer speech recognition model (pytorch).""""""\n\nfrom argparse import Namespace\nfrom distutils.util import strtobool\nimport logging\nimport math\n\nimport numpy\nimport torch\n\nfrom espnet.nets.asr_interface import ASRInterface\nfrom espnet.nets.ctc_prefix_score import CTCPrefixScore\nfrom espnet.nets.e2e_asr_common import end_detect\nfrom espnet.nets.e2e_asr_common import ErrorCalculator\nfrom espnet.nets.pytorch_backend.ctc import CTC\nfrom espnet.nets.pytorch_backend.e2e_asr import CTC_LOSS_THRESHOLD\nfrom espnet.nets.pytorch_backend.e2e_asr import Reporter\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import th_accuracy\nfrom espnet.nets.pytorch_backend.rnn.decoders import CTC_SCORING_RATIO\nfrom espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.decoder import Decoder\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.initializer import initialize\nfrom espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (\n    LabelSmoothingLoss,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.pytorch_backend.transformer.mask import target_mask\nfrom espnet.nets.pytorch_backend.transformer.plot import PlotAttentionReport\nfrom espnet.nets.scorers.ctc import CTCPrefixScorer\n\n\nclass E2E(ASRInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param int idim: dimension of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""transformer model setting"")\n\n        group.add_argument(\n            ""--transformer-init"",\n            type=str,\n            default=""pytorch"",\n            choices=[\n                ""pytorch"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n            ],\n            help=""how to initialize transformer parameters"",\n        )\n        group.add_argument(\n            ""--transformer-input-layer"",\n            type=str,\n            default=""conv2d"",\n            choices=[""conv2d"", ""linear"", ""embed""],\n            help=""transformer input layer type"",\n        )\n        group.add_argument(\n            ""--transformer-attn-dropout-rate"",\n            default=None,\n            type=float,\n            help=""dropout in transformer attention. use --dropout-rate if None is set"",\n        )\n        group.add_argument(\n            ""--transformer-lr"",\n            default=10.0,\n            type=float,\n            help=""Initial value of learning rate"",\n        )\n        group.add_argument(\n            ""--transformer-warmup-steps"",\n            default=25000,\n            type=int,\n            help=""optimizer warmup steps"",\n        )\n        group.add_argument(\n            ""--transformer-length-normalized-loss"",\n            default=True,\n            type=strtobool,\n            help=""normalize loss by length"",\n        )\n\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        # Encoder\n        group.add_argument(\n            ""--elayers"",\n            default=4,\n            type=int,\n            help=""Number of encoder layers (for shared recognition part ""\n            ""in multi-speaker asr mode)"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=300,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        # Attention\n        group.add_argument(\n            ""--adim"",\n            default=320,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        # Decoder\n        group.add_argument(\n            ""--dlayers"", default=1, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=320, type=int, help=""Number of decoder hidden units""\n        )\n        return parser\n\n    @property\n    def attention_plot_class(self):\n        """"""Return PlotAttentionReport.""""""\n        return PlotAttentionReport\n\n    def __init__(self, idim, odim, args, ignore_id=-1):\n        """"""Construct an E2E object.\n\n        :param int idim: dimension of inputs\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        """"""\n        torch.nn.Module.__init__(self)\n        if args.transformer_attn_dropout_rate is None:\n            args.transformer_attn_dropout_rate = args.dropout_rate\n        self.encoder = Encoder(\n            idim=idim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=args.transformer_input_layer,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n        )\n        self.decoder = Decoder(\n            odim=odim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.dunits,\n            num_blocks=args.dlayers,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n        )\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.odim = odim\n        self.ignore_id = ignore_id\n        self.subsample = get_subsample(args, mode=""asr"", arch=""transformer"")\n        self.reporter = Reporter()\n\n        # self.lsm_weight = a\n        self.criterion = LabelSmoothingLoss(\n            self.odim,\n            self.ignore_id,\n            args.lsm_weight,\n            args.transformer_length_normalized_loss,\n        )\n        # self.verbose = args.verbose\n        self.reset_parameters(args)\n        self.adim = args.adim\n        self.mtlalpha = args.mtlalpha\n        if args.mtlalpha > 0.0:\n            self.ctc = CTC(\n                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n            )\n        else:\n            self.ctc = None\n\n        if args.report_cer or args.report_wer:\n            self.error_calculator = ErrorCalculator(\n                args.char_list,\n                args.sym_space,\n                args.sym_blank,\n                args.report_cer,\n                args.report_wer,\n            )\n        else:\n            self.error_calculator = None\n        self.rnnlm = None\n\n    def reset_parameters(self, args):\n        """"""Initialize parameters.""""""\n        # initialize parameters\n        initialize(self, args.transformer_init)\n\n    def forward(self, xs_pad, ilens, ys_pad):\n        """"""E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded source sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of source sequences (B)\n        :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n        :return: ctc loass value\n        :rtype: torch.Tensor\n        :return: attention loss value\n        :rtype: torch.Tensor\n        :return: accuracy in attention decoder\n        :rtype: float\n        """"""\n        # 1. forward encoder\n        xs_pad = xs_pad[:, : max(ilens)]  # for data parallel\n        src_mask = make_non_pad_mask(ilens.tolist()).to(xs_pad.device).unsqueeze(-2)\n        hs_pad, hs_mask = self.encoder(xs_pad, src_mask)\n        self.hs_pad = hs_pad\n\n        # 2. forward decoder\n        ys_in_pad, ys_out_pad = add_sos_eos(ys_pad, self.sos, self.eos, self.ignore_id)\n        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n        pred_pad, pred_mask = self.decoder(ys_in_pad, ys_mask, hs_pad, hs_mask)\n        self.pred_pad = pred_pad\n\n        # 3. compute attention loss\n        loss_att = self.criterion(pred_pad, ys_out_pad)\n        self.acc = th_accuracy(\n            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n        )\n\n        # TODO(karita) show predicted text\n        # TODO(karita) calculate these stats\n        cer_ctc = None\n        if self.mtlalpha == 0.0:\n            loss_ctc = None\n        else:\n            batch_size = xs_pad.size(0)\n            hs_len = hs_mask.view(batch_size, -1).sum(1)\n            loss_ctc = self.ctc(hs_pad.view(batch_size, -1, self.adim), hs_len, ys_pad)\n            if self.error_calculator is not None:\n                ys_hat = self.ctc.argmax(hs_pad.view(batch_size, -1, self.adim)).data\n                cer_ctc = self.error_calculator(ys_hat.cpu(), ys_pad.cpu(), is_ctc=True)\n\n        # 5. compute cer/wer\n        if self.training or self.error_calculator is None:\n            cer, wer = None, None\n        else:\n            ys_hat = pred_pad.argmax(dim=-1)\n            cer, wer = self.error_calculator(ys_hat.cpu(), ys_pad.cpu())\n\n        # copyied from e2e_asr\n        alpha = self.mtlalpha\n        if alpha == 0:\n            self.loss = loss_att\n            loss_att_data = float(loss_att)\n            loss_ctc_data = None\n        elif alpha == 1:\n            self.loss = loss_ctc\n            loss_att_data = None\n            loss_ctc_data = float(loss_ctc)\n        else:\n            self.loss = alpha * loss_ctc + (1 - alpha) * loss_att\n            loss_att_data = float(loss_att)\n            loss_ctc_data = float(loss_ctc)\n\n        loss_data = float(self.loss)\n        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):\n            self.reporter.report(\n                loss_ctc_data, loss_att_data, self.acc, cer_ctc, cer, wer, loss_data\n            )\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def scorers(self):\n        """"""Scorers.""""""\n        return dict(decoder=self.decoder, ctc=CTCPrefixScorer(self.ctc, self.eos))\n\n    def encode(self, x):\n        """"""Encode acoustic features.\n\n        :param ndarray x: source acoustic feature (T, D)\n        :return: encoder outputs\n        :rtype: torch.Tensor\n        """"""\n        self.eval()\n        x = torch.as_tensor(x).unsqueeze(0)\n        enc_output, _ = self.encoder(x, None)\n        return enc_output.squeeze(0)\n\n    def recognize(self, x, recog_args, char_list=None, rnnlm=None, use_jit=False):\n        """"""Recognize input speech.\n\n        :param ndnarray x: input acoustic feature (B, T, D) or (T, D)\n        :param Namespace recog_args: argment Namespace contraining options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        enc_output = self.encode(x).unsqueeze(0)\n        if recog_args.ctc_weight > 0.0:\n            lpz = self.ctc.log_softmax(enc_output)\n            lpz = lpz.squeeze(0)\n        else:\n            lpz = None\n\n        h = enc_output.squeeze(0)\n\n        logging.info(""input lengths: "" + str(h.size(0)))\n        # search parms\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = recog_args.ctc_weight\n\n        # preprare sos\n        y = self.sos\n        vy = h.new_zeros(1).long()\n\n        if recog_args.maxlenratio == 0:\n            maxlen = h.shape[0]\n        else:\n            # maxlen >= 1\n            maxlen = max(1, int(recog_args.maxlenratio * h.size(0)))\n        minlen = int(recog_args.minlenratio * h.size(0))\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {""score"": 0.0, ""yseq"": [y], ""rnnlm_prev"": None}\n        else:\n            hyp = {""score"": 0.0, ""yseq"": [y]}\n        if lpz is not None:\n            ctc_prefix_score = CTCPrefixScore(lpz.detach().numpy(), 0, self.eos, numpy)\n            hyp[""ctc_state_prev""] = ctc_prefix_score.initial_state()\n            hyp[""ctc_score_prev""] = 0.0\n            if ctc_weight != 1.0:\n                # pre-pruning based on attention scores\n                ctc_beam = min(lpz.shape[-1], int(beam * CTC_SCORING_RATIO))\n            else:\n                ctc_beam = lpz.shape[-1]\n        hyps = [hyp]\n        ended_hyps = []\n\n        import six\n\n        traced_decoder = None\n        for i in six.moves.range(maxlen):\n            logging.debug(""position "" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                vy[0] = hyp[""yseq""][i]\n\n                # get nbest local scores and their ids\n                ys_mask = subsequent_mask(i + 1).unsqueeze(0)\n                ys = torch.tensor(hyp[""yseq""]).unsqueeze(0)\n                # FIXME: jit does not match non-jit result\n                if use_jit:\n                    if traced_decoder is None:\n                        traced_decoder = torch.jit.trace(\n                            self.decoder.forward_one_step, (ys, ys_mask, enc_output)\n                        )\n                    local_att_scores = traced_decoder(ys, ys_mask, enc_output)[0]\n                else:\n                    local_att_scores = self.decoder.forward_one_step(\n                        ys, ys_mask, enc_output\n                    )[0]\n\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(hyp[""rnnlm_prev""], vy)\n                    local_scores = (\n                        local_att_scores + recog_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                if lpz is not None:\n                    local_best_scores, local_best_ids = torch.topk(\n                        local_att_scores, ctc_beam, dim=1\n                    )\n                    ctc_scores, ctc_states = ctc_prefix_score(\n                        hyp[""yseq""], local_best_ids[0], hyp[""ctc_state_prev""]\n                    )\n                    local_scores = (1.0 - ctc_weight) * local_att_scores[\n                        :, local_best_ids[0]\n                    ] + ctc_weight * torch.from_numpy(\n                        ctc_scores - hyp[""ctc_score_prev""]\n                    )\n                    if rnnlm:\n                        local_scores += (\n                            recog_args.lm_weight * local_lm_scores[:, local_best_ids[0]]\n                        )\n                    local_best_scores, joint_best_ids = torch.topk(\n                        local_scores, beam, dim=1\n                    )\n                    local_best_ids = local_best_ids[:, joint_best_ids[0]]\n                else:\n                    local_best_scores, local_best_ids = torch.topk(\n                        local_scores, beam, dim=1\n                    )\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    new_hyp[""score""] = hyp[""score""] + float(local_best_scores[0, j])\n                    new_hyp[""yseq""] = [0] * (1 + len(hyp[""yseq""]))\n                    new_hyp[""yseq""][: len(hyp[""yseq""])] = hyp[""yseq""]\n                    new_hyp[""yseq""][len(hyp[""yseq""])] = int(local_best_ids[0, j])\n                    if rnnlm:\n                        new_hyp[""rnnlm_prev""] = rnnlm_state\n                    if lpz is not None:\n                        new_hyp[""ctc_state_prev""] = ctc_states[joint_best_ids[0, j]]\n                        new_hyp[""ctc_score_prev""] = ctc_scores[joint_best_ids[0, j]]\n                    # will be (2 x beam) hyps at most\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[""score""], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(""number of pruned hypothes: "" + str(len(hyps)))\n            if char_list is not None:\n                logging.debug(\n                    ""best hypo: ""\n                    + """".join([char_list[int(x)] for x in hyps[0][""yseq""][1:]])\n                )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(""adding <eos> in the last postion in the loop"")\n                for hyp in hyps:\n                    hyp[""yseq""].append(self.eos)\n\n            # add ended hypothes to a final list, and removed them from current hypothes\n            # (this will be a probmlem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[""yseq""][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[""yseq""]) > minlen:\n                        hyp[""score""] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[""score""] += recog_args.lm_weight * rnnlm.final(\n                                hyp[""rnnlm_prev""]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n\n            if end_detect(ended_hyps, i) and recog_args.maxlenratio == 0.0:\n                logging.info(""end detected at %d"", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(""remeined hypothes: "" + str(len(hyps)))\n            else:\n                logging.info(""no hypothesis. Finish decoding."")\n                break\n\n            if char_list is not None:\n                for hyp in hyps:\n                    logging.debug(\n                        ""hypo: "" + """".join([char_list[int(x)] for x in hyp[""yseq""][1:]])\n                    )\n\n            logging.debug(""number of ended hypothes: "" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x[""score""], reverse=True)[\n            : min(len(ended_hyps), recog_args.nbest)\n        ]\n\n        # check number of hypotheis\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                ""there is no N-best results, perform recognition ""\n                ""again with smaller minlenratio.""\n            )\n            # should copy becasuse Namespace will be overwritten globally\n            recog_args = Namespace(**vars(recog_args))\n            recog_args.minlenratio = max(0.0, recog_args.minlenratio - 0.1)\n            return self.recognize(x, recog_args, char_list, rnnlm)\n\n        logging.info(""total log probability: "" + str(nbest_hyps[0][""score""]))\n        logging.info(\n            ""normalized log probability: ""\n            + str(nbest_hyps[0][""score""] / len(nbest_hyps[0][""yseq""]))\n        )\n        return nbest_hyps\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        """"""E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n        """"""\n        with torch.no_grad():\n            self.forward(xs_pad, ilens, ys_pad)\n        ret = dict()\n        for name, m in self.named_modules():\n            if isinstance(m, MultiHeadedAttention):\n                ret[name] = m.attn.cpu().numpy()\n        return ret\n'"
espnet/nets/pytorch_backend/e2e_mt.py,30,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""RNN sequence-to-sequence text translation model (pytorch).""""""\n\nimport argparse\nimport logging\nimport math\nimport os\n\nimport chainer\nfrom chainer import reporter\nimport nltk\nimport numpy as np\nimport torch\n\nfrom espnet.nets.e2e_asr_common import label_smoothing_dist\nfrom espnet.nets.mt_interface import MTInterface\nfrom espnet.nets.pytorch_backend.initialization import uniform_init_parameters\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_for\nfrom espnet.nets.pytorch_backend.rnn.decoders import decoder_for\nfrom espnet.nets.pytorch_backend.rnn.encoders import encoder_for\n\n\nclass Reporter(chainer.Chain):\n    """"""A chainer reporter wrapper.""""""\n\n    def report(self, loss, acc, ppl, bleu):\n        """"""Report at every step.""""""\n        reporter.report({""loss"": loss}, self)\n        reporter.report({""acc"": acc}, self)\n        reporter.report({""ppl"": ppl}, self)\n        reporter.report({""bleu"": bleu}, self)\n\n\nclass E2E(MTInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param int idim: dimension of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        E2E.encoder_add_arguments(parser)\n        E2E.attention_add_arguments(parser)\n        E2E.decoder_add_arguments(parser)\n        return parser\n\n    @staticmethod\n    def encoder_add_arguments(parser):\n        """"""Add arguments for the encoder.""""""\n        group = parser.add_argument_group(""E2E encoder setting"")\n        # encoder\n        group.add_argument(\n            ""--etype"",\n            default=""blstmp"",\n            type=str,\n            choices=[\n                ""lstm"",\n                ""blstm"",\n                ""lstmp"",\n                ""blstmp"",\n                ""vgglstmp"",\n                ""vggblstmp"",\n                ""vgglstm"",\n                ""vggblstm"",\n                ""gru"",\n                ""bgru"",\n                ""grup"",\n                ""bgrup"",\n                ""vgggrup"",\n                ""vggbgrup"",\n                ""vgggru"",\n                ""vggbgru"",\n            ],\n            help=""Type of encoder network architecture"",\n        )\n        group.add_argument(\n            ""--elayers"", default=4, type=int, help=""Number of encoder layers"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=300,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        group.add_argument(\n            ""--eprojs"", default=320, type=int, help=""Number of encoder projection units""\n        )\n        group.add_argument(\n            ""--subsample"",\n            default=""1"",\n            type=str,\n            help=""Subsample input frames x_y_z means ""\n            ""subsample every x frame at 1st layer, ""\n            ""every y frame at 2nd layer etc."",\n        )\n        return parser\n\n    @staticmethod\n    def attention_add_arguments(parser):\n        """"""Add arguments for the attention.""""""\n        group = parser.add_argument_group(""E2E attention setting"")\n        # attention\n        group.add_argument(\n            ""--atype"",\n            default=""dot"",\n            type=str,\n            choices=[\n                ""noatt"",\n                ""dot"",\n                ""add"",\n                ""location"",\n                ""coverage"",\n                ""coverage_location"",\n                ""location2d"",\n                ""location_recurrent"",\n                ""multi_head_dot"",\n                ""multi_head_add"",\n                ""multi_head_loc"",\n                ""multi_head_multi_res_loc"",\n            ],\n            help=""Type of attention architecture"",\n        )\n        group.add_argument(\n            ""--adim"",\n            default=320,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--awin"", default=5, type=int, help=""Window size for location2d attention""\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        group.add_argument(\n            ""--aconv-chans"",\n            default=-1,\n            type=int,\n            help=""Number of attention convolution channels \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--aconv-filts"",\n            default=100,\n            type=int,\n            help=""Number of attention convolution filters \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        return parser\n\n    @staticmethod\n    def decoder_add_arguments(parser):\n        """"""Add arguments for the decoder.""""""\n        group = parser.add_argument_group(""E2E encoder setting"")\n        group.add_argument(\n            ""--dtype"",\n            default=""lstm"",\n            type=str,\n            choices=[""lstm"", ""gru""],\n            help=""Type of decoder network architecture"",\n        )\n        group.add_argument(\n            ""--dlayers"", default=1, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=320, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--dropout-rate-decoder"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the decoder"",\n        )\n        group.add_argument(\n            ""--sampling-probability"",\n            default=0.0,\n            type=float,\n            help=""Ratio of predicted labels fed back to decoder"",\n        )\n        group.add_argument(\n            ""--lsm-type"",\n            const="""",\n            default="""",\n            type=str,\n            nargs=""?"",\n            choices=["""", ""unigram""],\n            help=""Apply label smoothing with a specified distribution type"",\n        )\n        return parser\n\n    def __init__(self, idim, odim, args):\n        """"""Construct an E2E object.\n\n        :param int idim: dimension of inputs\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        """"""\n        super(E2E, self).__init__()\n        torch.nn.Module.__init__(self)\n        self.etype = args.etype\n        self.verbose = args.verbose\n        # NOTE: for self.build method\n        args.char_list = getattr(args, ""char_list"", None)\n        self.char_list = args.char_list\n        self.outdir = args.outdir\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n        self.reporter = Reporter()\n\n        # below means the last number becomes eos/sos ID\n        # note that sos/eos IDs are identical\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.pad = 0\n        # NOTE: we reserve index:0 for <pad> although this is reserved for a blank class\n        # in ASR. However, blank labels are not used in MT.\n        # To keep the vocabulary size,\n        # we use index:0 for padding instead of adding one more class.\n\n        # subsample info\n        self.subsample = get_subsample(args, mode=""mt"", arch=""rnn"")\n\n        # label smoothing info\n        if args.lsm_type and os.path.isfile(args.train_json):\n            logging.info(""Use label smoothing with "" + args.lsm_type)\n            labeldist = label_smoothing_dist(\n                odim, args.lsm_type, transcript=args.train_json\n            )\n        else:\n            labeldist = None\n\n        # multilingual related\n        self.multilingual = getattr(args, ""multilingual"", False)\n        self.replace_sos = getattr(args, ""replace_sos"", False)\n\n        # encoder\n        self.embed = torch.nn.Embedding(idim, args.eunits, padding_idx=self.pad)\n        self.dropout = torch.nn.Dropout(p=args.dropout_rate)\n        self.enc = encoder_for(args, args.eunits, self.subsample)\n        # attention\n        self.att = att_for(args)\n        # decoder\n        self.dec = decoder_for(args, odim, self.sos, self.eos, self.att, labeldist)\n\n        # tie source and target emeddings\n        if args.tie_src_tgt_embedding:\n            if idim != odim:\n                raise ValueError(\n                    ""When using tie_src_tgt_embedding, idim and odim must be equal.""\n                )\n            if args.eunits != args.dunits:\n                raise ValueError(\n                    ""When using tie_src_tgt_embedding, eunits and dunits must be equal.""\n                )\n            self.embed.weight = self.dec.embed.weight\n\n        # tie emeddings and the classfier\n        if args.tie_classifier:\n            if args.context_residual:\n                raise ValueError(\n                    ""When using tie_classifier, context_residual must be turned off.""\n                )\n            self.dec.output.weight = self.dec.embed.weight\n\n        # weight initialization\n        self.init_like_fairseq()\n\n        # options for beam search\n        if args.report_bleu:\n            trans_args = {\n                ""beam_size"": args.beam_size,\n                ""penalty"": args.penalty,\n                ""ctc_weight"": 0,\n                ""maxlenratio"": args.maxlenratio,\n                ""minlenratio"": args.minlenratio,\n                ""lm_weight"": args.lm_weight,\n                ""rnnlm"": args.rnnlm,\n                ""nbest"": args.nbest,\n                ""space"": args.sym_space,\n                ""blank"": args.sym_blank,\n                ""tgt_lang"": False,\n            }\n\n            self.trans_args = argparse.Namespace(**trans_args)\n            self.report_bleu = args.report_bleu\n        else:\n            self.report_bleu = False\n        self.rnnlm = None\n\n        self.logzero = -10000000000.0\n        self.loss = None\n        self.acc = None\n\n    def init_like_fairseq(self):\n        """"""Initialize weight like Fairseq.\n\n        Fairseq basically uses W, b, EmbedID.W ~ Uniform(-0.1, 0.1),\n        """"""\n        uniform_init_parameters(self)\n        # exceptions\n        # embed weight ~ Normal(-0.1, 0.1)\n        torch.nn.init.uniform_(self.embed.weight, -0.1, 0.1)\n        torch.nn.init.constant_(self.embed.weight[self.pad], 0)\n        torch.nn.init.uniform_(self.dec.embed.weight, -0.1, 0.1)\n        torch.nn.init.constant_(self.dec.embed.weight[self.pad], 0)\n\n    def forward(self, xs_pad, ilens, ys_pad):\n        """"""E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: loss value\n        :rtype: torch.Tensor\n        """"""\n        # 1. Encoder\n        xs_pad, ys_pad = self.target_language_biasing(xs_pad, ilens, ys_pad)\n        hs_pad, hlens, _ = self.enc(self.dropout(self.embed(xs_pad)), ilens)\n\n        # 3. attention loss\n        self.loss, self.acc, self.ppl = self.dec(hs_pad, hlens, ys_pad)\n\n        # 4. compute bleu\n        if self.training or not self.report_bleu:\n            self.bleu = 0.0\n        else:\n            lpz = None\n\n            nbest_hyps = self.dec.recognize_beam_batch(\n                hs_pad,\n                torch.tensor(hlens),\n                lpz,\n                self.trans_args,\n                self.char_list,\n                self.rnnlm,\n            )\n            # remove <sos> and <eos>\n            list_of_refs = []\n            hyps = []\n            y_hats = [nbest_hyp[0][""yseq""][1:-1] for nbest_hyp in nbest_hyps]\n            for i, y_hat in enumerate(y_hats):\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = """".join(seq_hat).replace(self.trans_args.space, "" "")\n                seq_hat_text = seq_hat_text.replace(self.trans_args.blank, """")\n                seq_true_text = """".join(seq_true).replace(self.trans_args.space, "" "")\n\n                hyps += [seq_hat_text.split("" "")]\n                list_of_refs += [[seq_true_text.split("" "")]]\n\n            self.bleu = nltk.corpus_bleu(list_of_refs, hyps) * 100\n\n        loss_data = float(self.loss)\n        if not math.isnan(loss_data):\n            self.reporter.report(loss_data, self.acc, self.ppl, self.bleu)\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def target_language_biasing(self, xs_pad, ilens, ys_pad):\n        """"""Prepend target language IDs to source sentences for multilingual MT.\n\n        These tags are prepended in source/target sentences as pre-processing.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :return: source text without language IDs\n        :rtype: torch.Tensor\n        :return: target text without language IDs\n        :rtype: torch.Tensor\n        :return: target language IDs\n        :rtype: torch.Tensor (B, 1)\n        """"""\n        if self.multilingual:\n            # remove language ID in the beggining\n            tgt_lang_ids = ys_pad[:, 0].unsqueeze(1)\n            xs_pad = xs_pad[:, 1:]  # remove source language IDs here\n            ys_pad = ys_pad[:, 1:]\n\n            # prepend target language ID to source sentences\n            xs_pad = torch.cat([tgt_lang_ids, xs_pad], dim=1)\n        return xs_pad, ys_pad\n\n    def translate(self, x, trans_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param ndarray x: input source text feature (B, T, D)\n        :param Namespace trans_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        prev = self.training\n        self.eval()\n\n        # 1. encoder\n        # make a utt list (1) to use the same interface for encoder\n        if self.multilingual:\n            ilen = [len(x[0][1:])]\n            h = to_device(\n                self, torch.from_numpy(np.fromiter(map(int, x[0][1:]), dtype=np.int64))\n            )\n        else:\n            ilen = [len(x[0])]\n            h = to_device(\n                self, torch.from_numpy(np.fromiter(map(int, x[0]), dtype=np.int64))\n            )\n        hs, _, _ = self.enc(self.dropout(self.embed(h.unsqueeze(0))), ilen)\n\n        # 2. decoder\n        # decode the first utterance\n        y = self.dec.recognize_beam(hs[0], None, trans_args, char_list, rnnlm)\n\n        if prev:\n            self.train()\n        return y\n\n    def translate_batch(self, xs, trans_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param list xs:\n            list of input source text feature arrays [(T_1, D), (T_2, D), ...]\n        :param Namespace trans_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        prev = self.training\n        self.eval()\n\n        # 1. Encoder\n        if self.multilingual:\n            ilens = np.fromiter((len(xx[1:]) for xx in xs), dtype=np.int64)\n            hs = [to_device(self, torch.from_numpy(xx[1:])) for xx in xs]\n        else:\n            ilens = np.fromiter((len(xx) for xx in xs), dtype=np.int64)\n            hs = [to_device(self, torch.from_numpy(xx)) for xx in xs]\n        xpad = pad_list(hs, self.pad)\n        hs_pad, hlens, _ = self.enc(self.dropout(self.embed(xpad)), ilens)\n\n        # 2. Decoder\n        hlens = torch.tensor(list(map(int, hlens)))  # make sure hlens is tensor\n        y = self.dec.recognize_beam_batch(\n            hs_pad, hlens, None, trans_args, char_list, rnnlm\n        )\n\n        if prev:\n            self.train()\n        return y\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        """"""E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n        """"""\n        with torch.no_grad():\n            # 1. Encoder\n            xs_pad, ys_pad = self.target_language_biasing(xs_pad, ilens, ys_pad)\n            hpad, hlens, _ = self.enc(self.dropout(self.embed(xs_pad)), ilens)\n\n            # 2. Decoder\n            att_ws = self.dec.calculate_all_attentions(hpad, hlens, ys_pad)\n\n        return att_ws\n'"
espnet/nets/pytorch_backend/e2e_mt_transformer.py,27,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Transformer text translation model (pytorch).""""""\n\nfrom argparse import Namespace\nfrom distutils.util import strtobool\nimport logging\nimport math\n\nimport numpy as np\nimport torch\n\nfrom espnet.nets.e2e_asr_common import end_detect\nfrom espnet.nets.e2e_mt_common import ErrorCalculator\nfrom espnet.nets.mt_interface import MTInterface\nfrom espnet.nets.pytorch_backend.e2e_mt import Reporter\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import th_accuracy\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.decoder import Decoder\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.initializer import initialize\nfrom espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (\n    LabelSmoothingLoss,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.pytorch_backend.transformer.mask import target_mask\nfrom espnet.nets.pytorch_backend.transformer.plot import PlotAttentionReport\n\n\nclass E2E(MTInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param int idim: dimension of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""transformer model setting"")\n\n        group.add_argument(\n            ""--transformer-init"",\n            type=str,\n            default=""xavier_uniform"",\n            choices=[\n                ""pytorch"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n            ],\n            help=""how to initialize transformer parameters"",\n        )\n        group.add_argument(\n            ""--transformer-attn-dropout-rate"",\n            default=None,\n            type=float,\n            help=""dropout in transformer attention. use --dropout-rate if None is set"",\n        )\n        group.add_argument(\n            ""--transformer-lr"",\n            default=1.0,\n            type=float,\n            help=""Initial value of learning rate"",\n        )\n        group.add_argument(\n            ""--transformer-warmup-steps"",\n            default=4000,\n            type=int,\n            help=""optimizer warmup steps"",\n        )\n        group.add_argument(\n            ""--transformer-length-normalized-loss"",\n            default=False,\n            type=strtobool,\n            help=""normalize loss by length"",\n        )\n\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        # Encoder\n        group.add_argument(\n            ""--elayers"", default=6, type=int, help=""Number of encoder layers"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=2048,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        # Attention\n        group.add_argument(\n            ""--adim"",\n            default=256,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        # Decoder\n        group.add_argument(\n            ""--dlayers"", default=6, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=2048, type=int, help=""Number of decoder hidden units""\n        )\n        return parser\n\n    @property\n    def attention_plot_class(self):\n        """"""Return PlotAttentionReport.""""""\n        return PlotAttentionReport\n\n    def __init__(self, idim, odim, args, ignore_id=-1):\n        """"""Construct an E2E object.\n\n        :param int idim: dimension of inputs\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        """"""\n        torch.nn.Module.__init__(self)\n        if args.transformer_attn_dropout_rate is None:\n            args.transformer_attn_dropout_rate = args.dropout_rate\n        self.encoder = Encoder(\n            idim=idim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=""embed"",\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n        )\n        self.decoder = Decoder(\n            odim=odim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.dunits,\n            num_blocks=args.dlayers,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n        )\n        self.pad = 0  # use <blank> for padding\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.odim = odim\n        self.ignore_id = ignore_id\n        self.subsample = get_subsample(args, mode=""mt"", arch=""transformer"")\n        self.reporter = Reporter()\n\n        # tie source and target emeddings\n        if args.tie_src_tgt_embedding:\n            if idim != odim:\n                raise ValueError(\n                    ""When using tie_src_tgt_embedding, idim and odim must be equal.""\n                )\n            self.encoder.embed[0].weight = self.decoder.embed[0].weight\n\n        # tie emeddings and the classfier\n        if args.tie_classifier:\n            self.decoder.output_layer.weight = self.decoder.embed[0].weight\n\n        self.criterion = LabelSmoothingLoss(\n            self.odim,\n            self.ignore_id,\n            args.lsm_weight,\n            args.transformer_length_normalized_loss,\n        )\n        self.normalize_length = args.transformer_length_normalized_loss  # for PPL\n        self.reset_parameters(args)\n        self.adim = args.adim\n        self.error_calculator = ErrorCalculator(\n            args.char_list, args.sym_space, args.sym_blank, args.report_bleu\n        )\n        self.rnnlm = None\n\n        # multilingual MT related\n        self.multilingual = args.multilingual\n\n    def reset_parameters(self, args):\n        """"""Initialize parameters.""""""\n        # initialize parameters\n        initialize(self, args.transformer_init)\n        torch.nn.init.normal_(\n            self.encoder.embed[0].weight, mean=0, std=args.adim ** -0.5\n        )\n        torch.nn.init.constant_(self.encoder.embed[0].weight[self.pad], 0)\n        torch.nn.init.normal_(\n            self.decoder.embed[0].weight, mean=0, std=args.adim ** -0.5\n        )\n        torch.nn.init.constant_(self.decoder.embed[0].weight[self.pad], 0)\n\n    def forward(self, xs_pad, ilens, ys_pad):\n        """"""E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded source sequences (B, Tmax)\n        :param torch.Tensor ilens: batch of lengths of source sequences (B)\n        :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n        :rtype: torch.Tensor\n        :return: attention loss value\n        :rtype: torch.Tensor\n        :return: accuracy in attention decoder\n        :rtype: float\n        """"""\n        # 1. forward encoder\n        xs_pad = xs_pad[:, : max(ilens)]  # for data parallel\n        src_mask = (~make_pad_mask(ilens.tolist())).to(xs_pad.device).unsqueeze(-2)\n        xs_pad, ys_pad = self.target_forcing(xs_pad, ys_pad)\n        hs_pad, hs_mask = self.encoder(xs_pad, src_mask)\n\n        # 2. forward decoder\n        ys_in_pad, ys_out_pad = add_sos_eos(ys_pad, self.sos, self.eos, self.ignore_id)\n        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n        pred_pad, pred_mask = self.decoder(ys_in_pad, ys_mask, hs_pad, hs_mask)\n\n        # 3. compute attention loss\n        self.loss = self.criterion(pred_pad, ys_out_pad)\n        self.acc = th_accuracy(\n            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n        )\n\n        # 4. compute corpus-level bleu in a mini-batch\n        if self.training:\n            self.bleu = None\n        else:\n            ys_hat = pred_pad.argmax(dim=-1)\n            self.bleu = self.error_calculator(ys_hat.cpu(), ys_pad.cpu())\n\n        loss_data = float(self.loss)\n        if self.normalize_length:\n            self.ppl = np.exp(loss_data)\n        else:\n            batch_size = ys_out_pad.size(0)\n            ys_out_pad = ys_out_pad.view(-1)\n            ignore = ys_out_pad == self.ignore_id  # (B*T,)\n            total_n_tokens = len(ys_out_pad) - ignore.sum().item()\n            self.ppl = np.exp(loss_data * batch_size / total_n_tokens)\n        if not math.isnan(loss_data):\n            self.reporter.report(loss_data, self.acc, self.ppl, self.bleu)\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def scorers(self):\n        """"""Scorers.""""""\n        return dict(decoder=self.decoder)\n\n    def encode(self, xs):\n        """"""Encode source sentences.""""""\n        self.eval()\n        xs = torch.as_tensor(xs).unsqueeze(0)\n        enc_output, _ = self.encoder(xs, None)\n        return enc_output.squeeze(0)\n\n    def target_forcing(self, xs_pad, ys_pad=None, tgt_lang=None):\n        """"""Prepend target language IDs to source sentences for multilingual MT.\n\n        These tags are prepended in source/target sentences as pre-processing.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax)\n        :return: source text without language IDs\n        :rtype: torch.Tensor\n        :return: target text without language IDs\n        :rtype: torch.Tensor\n        :return: target language IDs\n        :rtype: torch.Tensor (B, 1)\n        """"""\n        if self.multilingual:\n            xs_pad = xs_pad[:, 1:]  # remove source language IDs here\n            if ys_pad is not None:\n                # remove language ID in the beginning\n                lang_ids = ys_pad[:, 0].unsqueeze(1)\n                ys_pad = ys_pad[:, 1:]\n            elif tgt_lang is not None:\n                lang_ids = xs_pad.new_zeros(xs_pad.size(0), 1).fill_(tgt_lang)\n            else:\n                raise ValueError(""Set ys_pad or tgt_lang."")\n\n            # prepend target language ID to source sentences\n            xs_pad = torch.cat([lang_ids, xs_pad], dim=1)\n        return xs_pad, ys_pad\n\n    def translate(self, x, trans_args, char_list=None, rnnlm=None, use_jit=False):\n        """"""Translate source text.\n\n        :param list x: input source text feature (T,)\n        :param Namespace trans_args: argment Namespace contraining options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        self.eval()  # NOTE: this is important because self.encode() is not used\n        assert isinstance(x, list)\n\n        # make a utt list (1) to use the same interface for encoder\n        if self.multilingual:\n            x = to_device(\n                self, torch.from_numpy(np.fromiter(map(int, x[0][1:]), dtype=np.int64))\n            )\n        else:\n            x = to_device(\n                self, torch.from_numpy(np.fromiter(map(int, x[0]), dtype=np.int64))\n            )\n\n        xs_pad = x.unsqueeze(0)\n        tgt_lang = None\n        if trans_args.tgt_lang:\n            tgt_lang = char_list.index(trans_args.tgt_lang)\n        xs_pad, _ = self.target_forcing(xs_pad, tgt_lang=tgt_lang)\n        enc_output, _ = self.encoder(xs_pad, None)\n        h = enc_output.squeeze(0)\n\n        logging.info(""input lengths: "" + str(h.size(0)))\n        # search parms\n        beam = trans_args.beam_size\n        penalty = trans_args.penalty\n\n        # preprare sos\n        y = self.sos\n        vy = h.new_zeros(1).long()\n\n        if trans_args.maxlenratio == 0:\n            maxlen = h.shape[0]\n        else:\n            # maxlen >= 1\n            maxlen = max(1, int(trans_args.maxlenratio * h.size(0)))\n        minlen = int(trans_args.minlenratio * h.size(0))\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {""score"": 0.0, ""yseq"": [y], ""rnnlm_prev"": None}\n        else:\n            hyp = {""score"": 0.0, ""yseq"": [y]}\n        hyps = [hyp]\n        ended_hyps = []\n\n        import six\n\n        traced_decoder = None\n        for i in six.moves.range(maxlen):\n            logging.debug(""position "" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                vy[0] = hyp[""yseq""][i]\n\n                # get nbest local scores and their ids\n                ys_mask = subsequent_mask(i + 1).unsqueeze(0)\n                ys = torch.tensor(hyp[""yseq""]).unsqueeze(0)\n                # FIXME: jit does not match non-jit result\n                if use_jit:\n                    if traced_decoder is None:\n                        traced_decoder = torch.jit.trace(\n                            self.decoder.forward_one_step, (ys, ys_mask, enc_output)\n                        )\n                    local_att_scores = traced_decoder(ys, ys_mask, enc_output)[0]\n                else:\n                    local_att_scores = self.decoder.forward_one_step(\n                        ys, ys_mask, enc_output\n                    )[0]\n\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(hyp[""rnnlm_prev""], vy)\n                    local_scores = (\n                        local_att_scores + trans_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                local_best_scores, local_best_ids = torch.topk(\n                    local_scores, beam, dim=1\n                )\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    new_hyp[""score""] = hyp[""score""] + float(local_best_scores[0, j])\n                    new_hyp[""yseq""] = [0] * (1 + len(hyp[""yseq""]))\n                    new_hyp[""yseq""][: len(hyp[""yseq""])] = hyp[""yseq""]\n                    new_hyp[""yseq""][len(hyp[""yseq""])] = int(local_best_ids[0, j])\n                    if rnnlm:\n                        new_hyp[""rnnlm_prev""] = rnnlm_state\n                    # will be (2 x beam) hyps at most\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[""score""], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(""number of pruned hypothes: "" + str(len(hyps)))\n            if char_list is not None:\n                logging.debug(\n                    ""best hypo: ""\n                    + """".join([char_list[int(x)] for x in hyps[0][""yseq""][1:]])\n                )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(""adding <eos> in the last postion in the loop"")\n                for hyp in hyps:\n                    hyp[""yseq""].append(self.eos)\n\n            # add ended hypothes to a final list, and removed them from current hypothes\n            # (this will be a probmlem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[""yseq""][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[""yseq""]) > minlen:\n                        hyp[""score""] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[""score""] += trans_args.lm_weight * rnnlm.final(\n                                hyp[""rnnlm_prev""]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n            if end_detect(ended_hyps, i) and trans_args.maxlenratio == 0.0:\n                logging.info(""end detected at %d"", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(""remeined hypothes: "" + str(len(hyps)))\n            else:\n                logging.info(""no hypothesis. Finish decoding."")\n                break\n\n            if char_list is not None:\n                for hyp in hyps:\n                    logging.debug(\n                        ""hypo: "" + """".join([char_list[int(x)] for x in hyp[""yseq""][1:]])\n                    )\n\n            logging.debug(""number of ended hypothes: "" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x[""score""], reverse=True)[\n            : min(len(ended_hyps), trans_args.nbest)\n        ]\n\n        # check number of hypotheis\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                ""there is no N-best results, perform translation ""\n                ""again with smaller minlenratio.""\n            )\n            # should copy becasuse Namespace will be overwritten globally\n            trans_args = Namespace(**vars(trans_args))\n            trans_args.minlenratio = max(0.0, trans_args.minlenratio - 0.1)\n            return self.translate(x, trans_args, char_list, rnnlm)\n\n        logging.info(""total log probability: "" + str(nbest_hyps[0][""score""]))\n        logging.info(\n            ""normalized log probability: ""\n            + str(nbest_hyps[0][""score""] / len(nbest_hyps[0][""yseq""]))\n        )\n        return nbest_hyps\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad):\n        """"""E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n        """"""\n        with torch.no_grad():\n            self.forward(xs_pad, ilens, ys_pad)\n        ret = dict()\n        for name, m in self.named_modules():\n            if isinstance(m, MultiHeadedAttention) and m.attn is not None:\n                ret[name] = m.attn.cpu().numpy()\n        return ret\n'"
espnet/nets/pytorch_backend/e2e_st.py,22,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""RNN sequence-to-sequence speech translation model (pytorch).""""""\n\nimport argparse\nimport copy\nimport logging\nimport math\nimport os\n\nimport editdistance\nimport nltk\n\nimport chainer\nimport numpy as np\nimport six\nimport torch\n\nfrom itertools import groupby\n\nfrom chainer import reporter\n\nfrom espnet.nets.e2e_asr_common import label_smoothing_dist\nfrom espnet.nets.pytorch_backend.ctc import CTC\nfrom espnet.nets.pytorch_backend.initialization import lecun_normal_init_parameters\nfrom espnet.nets.pytorch_backend.initialization import set_forget_bias_to_one\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.nets_utils import to_torch_tensor\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_for\nfrom espnet.nets.pytorch_backend.rnn.decoders import decoder_for\nfrom espnet.nets.pytorch_backend.rnn.encoders import encoder_for\nfrom espnet.nets.st_interface import STInterface\n\nCTC_LOSS_THRESHOLD = 10000\n\n\nclass Reporter(chainer.Chain):\n    """"""A chainer reporter wrapper.""""""\n\n    def report(\n        self,\n        loss_asr,\n        loss_mt,\n        loss_st,\n        acc_asr,\n        acc_mt,\n        acc,\n        cer_ctc,\n        cer,\n        wer,\n        bleu,\n        mtl_loss,\n    ):\n        """"""Report at every step.""""""\n        reporter.report({""loss_asr"": loss_asr}, self)\n        reporter.report({""loss_mt"": loss_mt}, self)\n        reporter.report({""loss_st"": loss_st}, self)\n        reporter.report({""acc_asr"": acc_asr}, self)\n        reporter.report({""acc_mt"": acc_mt}, self)\n        reporter.report({""acc"": acc}, self)\n        reporter.report({""cer_ctc"": cer_ctc}, self)\n        reporter.report({""cer"": cer}, self)\n        reporter.report({""wer"": wer}, self)\n        reporter.report({""bleu"": bleu}, self)\n        logging.info(""mtl loss:"" + str(mtl_loss))\n        reporter.report({""loss"": mtl_loss}, self)\n\n\nclass E2E(STInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param int idim: dimension of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        E2E.encoder_add_arguments(parser)\n        E2E.attention_add_arguments(parser)\n        E2E.decoder_add_arguments(parser)\n        return parser\n\n    @staticmethod\n    def encoder_add_arguments(parser):\n        """"""Add arguments for the encoder.""""""\n        group = parser.add_argument_group(""E2E encoder setting"")\n        # encoder\n        group.add_argument(\n            ""--etype"",\n            default=""blstmp"",\n            type=str,\n            choices=[\n                ""lstm"",\n                ""blstm"",\n                ""lstmp"",\n                ""blstmp"",\n                ""vgglstmp"",\n                ""vggblstmp"",\n                ""vgglstm"",\n                ""vggblstm"",\n                ""gru"",\n                ""bgru"",\n                ""grup"",\n                ""bgrup"",\n                ""vgggrup"",\n                ""vggbgrup"",\n                ""vgggru"",\n                ""vggbgru"",\n            ],\n            help=""Type of encoder network architecture"",\n        )\n        group.add_argument(\n            ""--elayers"", default=4, type=int, help=""Number of encoder layers"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=300,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        group.add_argument(\n            ""--eprojs"", default=320, type=int, help=""Number of encoder projection units""\n        )\n        group.add_argument(\n            ""--subsample"",\n            default=""1"",\n            type=str,\n            help=""Subsample input frames x_y_z means ""\n            ""subsample every x frame at 1st layer, ""\n            ""every y frame at 2nd layer etc."",\n        )\n        return parser\n\n    @staticmethod\n    def attention_add_arguments(parser):\n        """"""Add arguments for the attention.""""""\n        group = parser.add_argument_group(""E2E attention setting"")\n        # attention\n        group.add_argument(\n            ""--atype"",\n            default=""dot"",\n            type=str,\n            choices=[\n                ""noatt"",\n                ""dot"",\n                ""add"",\n                ""location"",\n                ""coverage"",\n                ""coverage_location"",\n                ""location2d"",\n                ""location_recurrent"",\n                ""multi_head_dot"",\n                ""multi_head_add"",\n                ""multi_head_loc"",\n                ""multi_head_multi_res_loc"",\n            ],\n            help=""Type of attention architecture"",\n        )\n        group.add_argument(\n            ""--adim"",\n            default=320,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--awin"", default=5, type=int, help=""Window size for location2d attention""\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        group.add_argument(\n            ""--aconv-chans"",\n            default=-1,\n            type=int,\n            help=""Number of attention convolution channels \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--aconv-filts"",\n            default=100,\n            type=int,\n            help=""Number of attention convolution filters \\\n                           (negative value indicates no location-aware attention)"",\n        )\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        return parser\n\n    @staticmethod\n    def decoder_add_arguments(parser):\n        """"""Add arguments for the decoder.""""""\n        group = parser.add_argument_group(""E2E encoder setting"")\n        group.add_argument(\n            ""--dtype"",\n            default=""lstm"",\n            type=str,\n            choices=[""lstm"", ""gru""],\n            help=""Type of decoder network architecture"",\n        )\n        group.add_argument(\n            ""--dlayers"", default=1, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=320, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--dropout-rate-decoder"",\n            default=0.0,\n            type=float,\n            help=""Dropout rate for the decoder"",\n        )\n        group.add_argument(\n            ""--sampling-probability"",\n            default=0.0,\n            type=float,\n            help=""Ratio of predicted labels fed back to decoder"",\n        )\n        group.add_argument(\n            ""--lsm-type"",\n            const="""",\n            default="""",\n            type=str,\n            nargs=""?"",\n            choices=["""", ""unigram""],\n            help=""Apply label smoothing with a specified distribution type"",\n        )\n        return parser\n\n    def __init__(self, idim, odim, args):\n        """"""Construct an E2E object.\n\n        :param int idim: dimension of inputs\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        """"""\n        super(E2E, self).__init__()\n        torch.nn.Module.__init__(self)\n        self.asr_weight = getattr(args, ""asr_weight"", 0)\n        self.mt_weight = getattr(args, ""mt_weight"", 0)\n        self.mtlalpha = args.mtlalpha\n        assert 0.0 <= self.asr_weight < 1.0, ""asr_weight should be [0.0, 1.0)""\n        assert 0.0 <= self.mt_weight < 1.0, ""mt_weight should be [0.0, 1.0)""\n        assert 0.0 <= self.mtlalpha <= 1.0, ""mtlalpha should be [0.0, 1.0]""\n        self.etype = args.etype\n        self.verbose = args.verbose\n        # NOTE: for self.build method\n        args.char_list = getattr(args, ""char_list"", None)\n        self.char_list = args.char_list\n        self.outdir = args.outdir\n        self.space = args.sym_space\n        self.blank = args.sym_blank\n        self.reporter = Reporter()\n\n        # below means the last number becomes eos/sos ID\n        # note that sos/eos IDs are identical\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.pad = 0\n        # NOTE: we reserve index:0 for <pad> although this is reserved for a blank class\n        # in ASR. However, blank labels are not used in MT.\n        # To keep the vocabulary size,\n        # we use index:0 for padding instead of adding one more class.\n\n        # subsample info\n        self.subsample = get_subsample(args, mode=""st"", arch=""rnn"")\n\n        # label smoothing info\n        if args.lsm_type and os.path.isfile(args.train_json):\n            logging.info(""Use label smoothing with "" + args.lsm_type)\n            labeldist = label_smoothing_dist(\n                odim, args.lsm_type, transcript=args.train_json\n            )\n        else:\n            labeldist = None\n\n        # multilingual related\n        self.multilingual = getattr(args, ""multilingual"", False)\n        self.replace_sos = getattr(args, ""replace_sos"", False)\n\n        # encoder\n        self.enc = encoder_for(args, idim, self.subsample)\n        # attention (ST)\n        self.att = att_for(args)\n        # decoder (ST)\n        self.dec = decoder_for(args, odim, self.sos, self.eos, self.att, labeldist)\n\n        # submodule for ASR task\n        self.ctc = None\n        self.att_asr = None\n        self.dec_asr = None\n        if self.asr_weight > 0:\n            if self.mtlalpha > 0.0:\n                self.ctc = CTC(\n                    odim,\n                    args.eprojs,\n                    args.dropout_rate,\n                    ctc_type=args.ctc_type,\n                    reduce=True,\n                )\n            if self.mtlalpha < 1.0:\n                # attention (asr)\n                self.att_asr = att_for(args)\n                # decoder (asr)\n                args_asr = copy.deepcopy(args)\n                args_asr.atype = ""location""  # TODO(hirofumi0810): make this option\n                self.dec_asr = decoder_for(\n                    args_asr, odim, self.sos, self.eos, self.att_asr, labeldist\n                )\n\n        # submodule for MT task\n        if self.mt_weight > 0:\n            self.embed_mt = torch.nn.Embedding(odim, args.eunits, padding_idx=self.pad)\n            self.dropout_mt = torch.nn.Dropout(p=args.dropout_rate)\n            self.enc_mt = encoder_for(\n                args, args.eunits, subsample=np.ones(args.elayers + 1, dtype=np.int)\n            )\n\n        # weight initialization\n        self.init_like_chainer()\n\n        # options for beam search\n        if self.asr_weight > 0 and args.report_cer or args.report_wer:\n            recog_args = {\n                ""beam_size"": args.beam_size,\n                ""penalty"": args.penalty,\n                ""ctc_weight"": args.ctc_weight,\n                ""maxlenratio"": args.maxlenratio,\n                ""minlenratio"": args.minlenratio,\n                ""lm_weight"": args.lm_weight,\n                ""rnnlm"": args.rnnlm,\n                ""nbest"": args.nbest,\n                ""space"": args.sym_space,\n                ""blank"": args.sym_blank,\n                ""tgt_lang"": False,\n            }\n\n            self.recog_args = argparse.Namespace(**recog_args)\n            self.report_cer = args.report_cer\n            self.report_wer = args.report_wer\n        else:\n            self.report_cer = False\n            self.report_wer = False\n        if args.report_bleu:\n            trans_args = {\n                ""beam_size"": args.beam_size,\n                ""penalty"": args.penalty,\n                ""ctc_weight"": 0,\n                ""maxlenratio"": args.maxlenratio,\n                ""minlenratio"": args.minlenratio,\n                ""lm_weight"": args.lm_weight,\n                ""rnnlm"": args.rnnlm,\n                ""nbest"": args.nbest,\n                ""space"": args.sym_space,\n                ""blank"": args.sym_blank,\n                ""tgt_lang"": False,\n            }\n\n            self.trans_args = argparse.Namespace(**trans_args)\n            self.report_bleu = args.report_bleu\n        else:\n            self.report_bleu = False\n        self.rnnlm = None\n\n        self.logzero = -10000000000.0\n        self.loss = None\n        self.acc = None\n\n    def init_like_chainer(self):\n        """"""Initialize weight like chainer.\n\n        chainer basically uses LeCun way: W ~ Normal(0, fan_in ** -0.5), b = 0\n        pytorch basically uses W, b ~ Uniform(-fan_in**-0.5, fan_in**-0.5)\n        however, there are two exceptions as far as I know.\n        - EmbedID.W ~ Normal(0, 1)\n        - LSTM.upward.b[forget_gate_range] = 1 (but not used in NStepLSTM)\n        """"""\n        lecun_normal_init_parameters(self)\n        # exceptions\n        # embed weight ~ Normal(0, 1)\n        self.dec.embed.weight.data.normal_(0, 1)\n        # forget-bias = 1.0\n        # https://discuss.pytorch.org/t/set-forget-gate-bias-of-lstm/1745\n        for i in six.moves.range(len(self.dec.decoder)):\n            set_forget_bias_to_one(self.dec.decoder[i].bias_ih)\n\n    def forward(self, xs_pad, ilens, ys_pad, ys_pad_src):\n        """"""E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: loss value\n        :rtype: torch.Tensor\n        """"""\n        # 0. Extract target language ID\n        if self.multilingual:\n            tgt_lang_ids = ys_pad[:, 0:1]\n            ys_pad = ys_pad[:, 1:]  # remove target language ID in the beggining\n        else:\n            tgt_lang_ids = None\n\n        # 1. Encoder\n        hs_pad, hlens, _ = self.enc(xs_pad, ilens)\n\n        # 2. ST attention loss\n        self.loss_st, self.acc, _ = self.dec(\n            hs_pad, hlens, ys_pad, lang_ids=tgt_lang_ids\n        )\n\n        # 2. ASR CTC loss\n        if self.asr_weight == 0 or self.mtlalpha == 0:\n            self.loss_ctc = 0.0\n        else:\n            self.loss_ctc = self.ctc(hs_pad, hlens, ys_pad_src)\n\n        # 3. ASR attention loss\n        if self.asr_weight == 0 or self.mtlalpha == 1:\n            self.loss_asr = 0.0\n            acc_asr = 0.0\n        else:\n            self.loss_asr, acc_asr, _ = self.dec_asr(hs_pad, hlens, ys_pad_src)\n            acc_asr = acc_asr\n\n        # 3. MT attention loss\n        if self.mt_weight == 0:\n            self.loss_mt = 0.0\n            acc_mt = 0.0\n        else:\n            # ys_pad_src, ys_pad = self.target_forcing(ys_pad_src, ys_pad)\n            ilens_mt = torch.sum(ys_pad_src != -1, dim=1).cpu().numpy()\n            # NOTE: ys_pad_src is padded with -1\n            ys_src = [y[y != -1] for y in ys_pad_src]  # parse padded ys_src\n            ys_zero_pad_src = pad_list(ys_src, self.pad)  # re-pad with zero\n            hs_pad_mt, hlens_mt, _ = self.enc_mt(\n                self.dropout_mt(self.embed_mt(ys_zero_pad_src)), ilens_mt\n            )\n            self.loss_mt, acc_mt, _ = self.dec(hs_pad_mt, hlens_mt, ys_pad)\n            acc_mt = acc_mt\n\n        # 4. compute cer without beam search\n        if (self.asr_weight == 0 or self.mtlalpha == 0) or self.char_list is None:\n            cer_ctc = None\n        else:\n            cers = []\n\n            y_hats = self.ctc.argmax(hs_pad).data\n            for i, y in enumerate(y_hats):\n                y_hat = [x[0] for x in groupby(y)]\n                y_true = ys_pad_src[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = """".join(seq_hat).replace(self.space, "" "")\n                seq_hat_text = seq_hat_text.replace(self.blank, """")\n                seq_true_text = """".join(seq_true).replace(self.space, "" "")\n\n                hyp_chars = seq_hat_text.replace("" "", """")\n                ref_chars = seq_true_text.replace("" "", """")\n                if len(ref_chars) > 0:\n                    cers.append(\n                        editdistance.eval(hyp_chars, ref_chars) / len(ref_chars)\n                    )\n\n            cer_ctc = sum(cers) / len(cers) if cers else None\n\n        # 5. compute cer/wer\n        if self.training or (\n            self.asr_weight == 0\n            or self.mtlalpha == 1\n            or not (self.report_cer or self.report_wer)\n        ):\n            cer, wer = 0.0, 0.0\n        else:\n            if (\n                self.asr_weight > 0 and self.mtlalpha > 0\n            ) and self.recog_args.ctc_weight > 0.0:\n                lpz = self.ctc.log_softmax(hs_pad).data\n            else:\n                lpz = None\n\n            word_eds, word_ref_lens, char_eds, char_ref_lens = [], [], [], []\n            nbest_hyps_asr = self.dec_asr.recognize_beam_batch(\n                hs_pad,\n                torch.tensor(hlens),\n                lpz,\n                self.recog_args,\n                self.char_list,\n                self.rnnlm,\n            )\n            # remove <sos> and <eos>\n            y_hats = [nbest_hyp[0][""yseq""][1:-1] for nbest_hyp in nbest_hyps_asr]\n            for i, y_hat in enumerate(y_hats):\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = """".join(seq_hat).replace(self.recog_args.space, "" "")\n                seq_hat_text = seq_hat_text.replace(self.recog_args.blank, """")\n                seq_true_text = """".join(seq_true).replace(self.recog_args.space, "" "")\n\n                hyp_words = seq_hat_text.split()\n                ref_words = seq_true_text.split()\n                word_eds.append(editdistance.eval(hyp_words, ref_words))\n                word_ref_lens.append(len(ref_words))\n                hyp_chars = seq_hat_text.replace("" "", """")\n                ref_chars = seq_true_text.replace("" "", """")\n                char_eds.append(editdistance.eval(hyp_chars, ref_chars))\n                char_ref_lens.append(len(ref_chars))\n\n            wer = (\n                0.0\n                if not self.report_wer\n                else float(sum(word_eds)) / sum(word_ref_lens)\n            )\n            cer = (\n                0.0\n                if not self.report_cer\n                else float(sum(char_eds)) / sum(char_ref_lens)\n            )\n\n        # 6. compute bleu\n        if self.training or not self.report_bleu:\n            self.bleu = 0.0\n        else:\n            lpz = None\n\n            nbest_hyps = self.dec.recognize_beam_batch(\n                hs_pad,\n                torch.tensor(hlens),\n                lpz,\n                self.trans_args,\n                self.char_list,\n                self.rnnlm,\n                lang_ids=tgt_lang_ids.squeeze(1).tolist()\n                if self.multilingual\n                else None,\n            )\n            # remove <sos> and <eos>\n            list_of_refs = []\n            hyps = []\n            y_hats = [nbest_hyp[0][""yseq""][1:-1] for nbest_hyp in nbest_hyps]\n            for i, y_hat in enumerate(y_hats):\n                y_true = ys_pad[i]\n\n                seq_hat = [self.char_list[int(idx)] for idx in y_hat if int(idx) != -1]\n                seq_true = [\n                    self.char_list[int(idx)] for idx in y_true if int(idx) != -1\n                ]\n                seq_hat_text = """".join(seq_hat).replace(self.trans_args.space, "" "")\n                seq_hat_text = seq_hat_text.replace(self.trans_args.blank, """")\n                seq_true_text = """".join(seq_true).replace(self.trans_args.space, "" "")\n\n                hyps += [seq_hat_text.split("" "")]\n                list_of_refs += [[seq_true_text.split("" "")]]\n\n            self.bleu = nltk.corpus_bleu(list_of_refs, hyps) * 100\n\n        alpha = self.mtlalpha\n        self.loss = (\n            (1 - self.asr_weight - self.mt_weight) * self.loss_st\n            + self.asr_weight * (alpha * self.loss_ctc + (1 - alpha) * self.loss_asr)\n            + self.mt_weight * self.loss_mt\n        )\n        loss_st_data = float(self.loss_st)\n        loss_asr_data = float(alpha * self.loss_ctc + (1 - alpha) * self.loss_asr)\n        loss_mt_data = None if self.mt_weight == 0 else float(self.loss_mt)\n\n        loss_data = float(self.loss)\n        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):\n            self.reporter.report(\n                loss_asr_data,\n                loss_mt_data,\n                loss_st_data,\n                acc_asr,\n                acc_mt,\n                self.acc,\n                cer_ctc,\n                cer,\n                wer,\n                self.bleu,\n                loss_data,\n            )\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def scorers(self):\n        """"""Scorers.""""""\n        return dict(decoder=self.dec)\n\n    def encode(self, x):\n        """"""Encode acoustic features.\n\n        :param ndarray x: input acoustic feature (T, D)\n        :return: encoder outputs\n        :rtype: torch.Tensor\n        """"""\n        self.eval()\n        ilens = [x.shape[0]]\n\n        # subsample frame\n        x = x[:: self.subsample[0], :]\n        p = next(self.parameters())\n        h = torch.as_tensor(x, device=p.device, dtype=p.dtype)\n        # make a utt list (1) to use the same interface for encoder\n        hs = h.contiguous().unsqueeze(0)\n\n        # 1. encoder\n        hs, _, _ = self.enc(hs, ilens)\n        return hs.squeeze(0)\n\n    def translate(self, x, trans_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param ndarray x: input acoustic feature (T, D)\n        :param Namespace trans_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        hs = self.encode(x).unsqueeze(0)\n\n        # 2. Decoder\n        # decode the first utterance\n        y = self.dec.recognize_beam(hs[0], None, trans_args, char_list, rnnlm)\n        return y\n\n    def translate_batch(self, xs, trans_args, char_list, rnnlm=None):\n        """"""E2E beam search.\n\n        :param list xs: list of input acoustic feature arrays [(T_1, D), (T_2, D), ...]\n        :param Namespace trans_args: argument Namespace containing options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        prev = self.training\n        self.eval()\n        ilens = np.fromiter((xx.shape[0] for xx in xs), dtype=np.int64)\n\n        # subsample frame\n        xs = [xx[:: self.subsample[0], :] for xx in xs]\n        xs = [to_device(self, to_torch_tensor(xx).float()) for xx in xs]\n        xs_pad = pad_list(xs, 0.0)\n\n        # 1. Encoder\n        hs_pad, hlens, _ = self.enc(xs_pad, ilens)\n\n        # 2. Decoder\n        hlens = torch.tensor(list(map(int, hlens)))  # make sure hlens is tensor\n        y = self.dec.recognize_beam_batch(\n            hs_pad, hlens, None, trans_args, char_list, rnnlm\n        )\n\n        if prev:\n            self.train()\n        return y\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad, ys_pad_src):\n        """"""E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n        """"""\n        with torch.no_grad():\n            # 1. Encoder\n            if self.multilingual:\n                tgt_lang_ids = ys_pad[:, 0:1]\n                ys_pad = ys_pad[:, 1:]  # remove target language ID in the beggining\n            else:\n                tgt_lang_ids = None\n            hpad, hlens, _ = self.enc(xs_pad, ilens)\n\n            # 2. Decoder\n            att_ws = self.dec.calculate_all_attentions(\n                hpad, hlens, ys_pad, lang_ids=tgt_lang_ids\n            )\n\n        return att_ws\n\n    def subsample_frames(self, x):\n        """"""Subsample speeh frames in the encoder.""""""\n        # subsample frame\n        x = x[:: self.subsample[0], :]\n        ilen = [x.shape[0]]\n        h = to_device(self, torch.from_numpy(np.array(x, dtype=np.float32)))\n        h.contiguous()\n        return h, ilen\n'"
espnet/nets/pytorch_backend/e2e_st_transformer.py,23,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Transformer speech recognition model (pytorch).""""""\n\nfrom argparse import Namespace\nfrom distutils.util import strtobool\nimport logging\nimport math\n\nimport torch\n\nfrom espnet.nets.e2e_asr_common import end_detect\nfrom espnet.nets.e2e_asr_common import ErrorCalculator as ASRErrorCalculator\nfrom espnet.nets.e2e_mt_common import ErrorCalculator as MTErrorCalculator\nfrom espnet.nets.pytorch_backend.ctc import CTC\nfrom espnet.nets.pytorch_backend.e2e_asr import CTC_LOSS_THRESHOLD\nfrom espnet.nets.pytorch_backend.e2e_st import Reporter\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import th_accuracy\nfrom espnet.nets.pytorch_backend.transformer.add_sos_eos import add_sos_eos\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.decoder import Decoder\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.initializer import initialize\nfrom espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (\n    LabelSmoothingLoss,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.pytorch_backend.transformer.mask import target_mask\nfrom espnet.nets.pytorch_backend.transformer.plot import PlotAttentionReport\nfrom espnet.nets.st_interface import STInterface\n\n\nclass E2E(STInterface, torch.nn.Module):\n    """"""E2E module.\n\n    :param int idim: dimension of inputs\n    :param int odim: dimension of outputs\n    :param Namespace args: argument Namespace containing options\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments.""""""\n        group = parser.add_argument_group(""transformer model setting"")\n\n        group.add_argument(\n            ""--transformer-init"",\n            type=str,\n            default=""pytorch"",\n            choices=[\n                ""pytorch"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n            ],\n            help=""how to initialize transformer parameters"",\n        )\n        group.add_argument(\n            ""--transformer-input-layer"",\n            type=str,\n            default=""conv2d"",\n            choices=[""conv2d"", ""linear"", ""embed""],\n            help=""transformer input layer type"",\n        )\n        group.add_argument(\n            ""--transformer-attn-dropout-rate"",\n            default=None,\n            type=float,\n            help=""dropout in transformer attention. use --dropout-rate if None is set"",\n        )\n        group.add_argument(\n            ""--transformer-lr"",\n            default=10.0,\n            type=float,\n            help=""Initial value of learning rate"",\n        )\n        group.add_argument(\n            ""--transformer-warmup-steps"",\n            default=25000,\n            type=int,\n            help=""optimizer warmup steps"",\n        )\n        group.add_argument(\n            ""--transformer-length-normalized-loss"",\n            default=False,\n            type=strtobool,\n            help=""normalize loss by length"",\n        )\n\n        group.add_argument(\n            ""--dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for the encoder"",\n        )\n        # Encoder\n        group.add_argument(\n            ""--elayers"", default=4, type=int, help=""Number of encoder layers"",\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=2048,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        # Attention\n        group.add_argument(\n            ""--adim"",\n            default=256,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        # Decoder\n        group.add_argument(\n            ""--dlayers"", default=6, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=2048, type=int, help=""Number of decoder hidden units""\n        )\n        return parser\n\n    @property\n    def attention_plot_class(self):\n        """"""Return PlotAttentionReport.""""""\n        return PlotAttentionReport\n\n    def __init__(self, idim, odim, args, ignore_id=-1):\n        """"""Construct an E2E object.\n\n        :param int idim: dimension of inputs\n        :param int odim: dimension of outputs\n        :param Namespace args: argument Namespace containing options\n        """"""\n        torch.nn.Module.__init__(self)\n        if args.transformer_attn_dropout_rate is None:\n            args.transformer_attn_dropout_rate = args.dropout_rate\n        self.encoder = Encoder(\n            idim=idim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=args.transformer_input_layer,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            attention_dropout_rate=args.transformer_attn_dropout_rate,\n        )\n        self.decoder = Decoder(\n            odim=odim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.dunits,\n            num_blocks=args.dlayers,\n            dropout_rate=args.dropout_rate,\n            positional_dropout_rate=args.dropout_rate,\n            self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n        )\n        self.pad = 0  # use <blank> for padding\n        self.sos = odim - 1\n        self.eos = odim - 1\n        self.odim = odim\n        self.ignore_id = ignore_id\n        self.subsample = get_subsample(args, mode=""st"", arch=""transformer"")\n        self.reporter = Reporter()\n\n        self.criterion = LabelSmoothingLoss(\n            self.odim,\n            self.ignore_id,\n            args.lsm_weight,\n            args.transformer_length_normalized_loss,\n        )\n        self.adim = args.adim\n        # submodule for ASR task\n        self.mtlalpha = args.mtlalpha\n        self.asr_weight = getattr(args, ""asr_weight"", 0.0)\n        if self.asr_weight > 0 and args.mtlalpha < 1:\n            self.decoder_asr = Decoder(\n                odim=odim,\n                attention_dim=args.adim,\n                attention_heads=args.aheads,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                self_attention_dropout_rate=args.transformer_attn_dropout_rate,\n                src_attention_dropout_rate=args.transformer_attn_dropout_rate,\n            )\n        # submodule for MT task\n        self.mt_weight = getattr(args, ""mt_weight"", 0.0)\n        if self.mt_weight > 0:\n            self.encoder_mt = Encoder(\n                idim=odim,\n                attention_dim=args.adim,\n                attention_heads=args.aheads,\n                linear_units=args.dunits,\n                num_blocks=args.dlayers,\n                input_layer=""embed"",\n                dropout_rate=args.dropout_rate,\n                positional_dropout_rate=args.dropout_rate,\n                attention_dropout_rate=args.transformer_attn_dropout_rate,\n                padding_idx=0,\n            )\n        self.reset_parameters(args)  # place after the submodule initialization\n        if self.asr_weight > 0 and args.mtlalpha > 0.0:\n            self.ctc = CTC(\n                odim, args.adim, args.dropout_rate, ctc_type=args.ctc_type, reduce=True\n            )\n        else:\n            self.ctc = None\n\n        # translation error calculator\n        self.error_calculator = MTErrorCalculator(\n            args.char_list, args.sym_space, args.sym_blank, args.report_bleu\n        )\n\n        # recognition error calculator\n        self.error_calculator_asr = ASRErrorCalculator(\n            args.char_list,\n            args.sym_space,\n            args.sym_blank,\n            args.report_cer,\n            args.report_wer,\n        )\n        self.rnnlm = None\n\n        # multilingual E2E-ST related\n        self.multilingual = getattr(args, ""multilingual"", False)\n        self.replace_sos = getattr(args, ""replace_sos"", False)\n\n    def reset_parameters(self, args):\n        """"""Initialize parameters.""""""\n        # initialize parameters\n        initialize(self, args.transformer_init)\n        if self.mt_weight > 0:\n            torch.nn.init.normal_(\n                self.encoder_mt.embed[0].weight, mean=0, std=args.adim ** -0.5\n            )\n            torch.nn.init.constant_(self.encoder_mt.embed[0].weight[self.pad], 0)\n\n    def forward(self, xs_pad, ilens, ys_pad, ys_pad_src):\n        """"""E2E forward.\n\n        :param torch.Tensor xs_pad: batch of padded source sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of source sequences (B)\n        :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n        :param torch.Tensor ys_pad_src: batch of padded target sequences (B, Lmax)\n        :return: ctc loass value\n        :rtype: torch.Tensor\n        :return: attention loss value\n        :rtype: torch.Tensor\n        :return: accuracy in attention decoder\n        :rtype: float\n        """"""\n        # 0. Extract target language ID\n        tgt_lang_ids = None\n        if self.multilingual:\n            tgt_lang_ids = ys_pad[:, 0:1]\n            ys_pad = ys_pad[:, 1:]  # remove target language ID in the beggining\n\n        # 1. forward encoder\n        xs_pad = xs_pad[:, : max(ilens)]  # for data parallel\n        src_mask = (~make_pad_mask(ilens.tolist())).to(xs_pad.device).unsqueeze(-2)\n        hs_pad, hs_mask = self.encoder(xs_pad, src_mask)\n\n        # 2. forward decoder\n        ys_in_pad, ys_out_pad = add_sos_eos(ys_pad, self.sos, self.eos, self.ignore_id)\n        # replace <sos> with target language ID\n        if self.replace_sos:\n            ys_in_pad = torch.cat([tgt_lang_ids, ys_in_pad[:, 1:]], dim=1)\n        ys_mask = target_mask(ys_in_pad, self.ignore_id)\n        pred_pad, pred_mask = self.decoder(ys_in_pad, ys_mask, hs_pad, hs_mask)\n\n        # 3. compute ST loss\n        loss_asr_att, loss_asr_ctc, loss_mt = 0.0, 0.0, 0.0\n        acc_asr, acc_mt = 0.0, 0.0\n        loss_att = self.criterion(pred_pad, ys_out_pad)\n\n        self.acc = th_accuracy(\n            pred_pad.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n        )\n\n        # 4. compute corpus-level bleu in a mini-batch\n        if self.training:\n            self.bleu = None\n        else:\n            ys_hat = pred_pad.argmax(dim=-1)\n            self.bleu = self.error_calculator(ys_hat.cpu(), ys_pad.cpu())\n\n        # 5. compute auxiliary ASR loss\n        cer, wer = None, None\n        cer_ctc = None\n        if self.asr_weight > 0:\n            # attention\n            if self.mtlalpha < 1:\n                ys_in_pad_asr, ys_out_pad_asr = add_sos_eos(\n                    ys_pad_src, self.sos, self.eos, self.ignore_id\n                )\n                ys_mask_asr = target_mask(ys_in_pad_asr, self.ignore_id)\n                pred_pad_asr, _ = self.decoder_asr(\n                    ys_in_pad_asr, ys_mask_asr, hs_pad, hs_mask\n                )\n                loss_asr_att = self.criterion(pred_pad_asr, ys_out_pad_asr)\n\n                acc_asr = th_accuracy(\n                    pred_pad_asr.view(-1, self.odim),\n                    ys_out_pad_asr,\n                    ignore_label=self.ignore_id,\n                )\n                if not self.training:\n                    ys_hat_asr = pred_pad_asr.argmax(dim=-1)\n                    cer, wer = self.error_calculator_asr(\n                        ys_hat_asr.cpu(), ys_pad_src.cpu()\n                    )\n\n            # CTC\n            if self.mtlalpha > 0:\n                batch_size = xs_pad.size(0)\n                hs_len = hs_mask.view(batch_size, -1).sum(1)\n                loss_asr_ctc = self.ctc(\n                    hs_pad.view(batch_size, -1, self.adim), hs_len, ys_pad_src\n                )\n                ys_hat_ctc = self.ctc.argmax(\n                    hs_pad.view(batch_size, -1, self.adim)\n                ).data\n                if not self.training:\n                    cer_ctc = self.error_calculator_asr(\n                        ys_hat_ctc.cpu(), ys_pad_src.cpu(), is_ctc=True\n                    )\n\n        # 6. compute auxiliary MT loss\n        if self.mt_weight > 0:\n            ilens_mt = torch.sum(ys_pad_src != self.ignore_id, dim=1).cpu().numpy()\n            # NOTE: ys_pad_src is padded with -1\n            ys_src = [y[y != self.ignore_id] for y in ys_pad_src]  # parse padded ys_src\n            ys_zero_pad_src = pad_list(ys_src, self.pad)  # re-pad with zero\n            ys_zero_pad_src = ys_zero_pad_src[:, : max(ilens_mt)]  # for data parallel\n            src_mask_mt = (\n                (~make_pad_mask(ilens_mt.tolist()))\n                .to(ys_zero_pad_src.device)\n                .unsqueeze(-2)\n            )\n            hs_pad_mt, hs_mask_mt = self.encoder_mt(ys_zero_pad_src, src_mask_mt)\n            pred_pad_mt, _ = self.decoder(ys_in_pad, ys_mask, hs_pad_mt, hs_mask_mt)\n            loss_mt = self.criterion(pred_pad_mt, ys_out_pad)\n\n            acc_mt = th_accuracy(\n                pred_pad_mt.view(-1, self.odim), ys_out_pad, ignore_label=self.ignore_id\n            )\n\n        alpha = self.mtlalpha\n        self.loss = (\n            (1 - self.asr_weight - self.mt_weight) * loss_att\n            + self.asr_weight * (alpha * loss_asr_ctc + (1 - alpha) * loss_asr_att)\n            + self.mt_weight * loss_mt\n        )\n        loss_asr_data = float(alpha * loss_asr_ctc + (1 - alpha) * loss_asr_att)\n        loss_mt_data = None if self.mt_weight == 0 else float(loss_mt)\n        loss_st_data = float(loss_att)\n\n        loss_data = float(self.loss)\n        if loss_data < CTC_LOSS_THRESHOLD and not math.isnan(loss_data):\n            self.reporter.report(\n                loss_asr_data,\n                loss_mt_data,\n                loss_st_data,\n                acc_asr,\n                acc_mt,\n                self.acc,\n                cer_ctc,\n                cer,\n                wer,\n                self.bleu,\n                loss_data,\n            )\n        else:\n            logging.warning(""loss (=%f) is not correct"", loss_data)\n        return self.loss\n\n    def scorers(self):\n        """"""Scorers.""""""\n        return dict(decoder=self.decoder)\n\n    def encode(self, x):\n        """"""Encode source acoustic features.\n\n        :param ndarray x: source acoustic feature (T, D)\n        :return: encoder outputs\n        :rtype: torch.Tensor\n        """"""\n        self.eval()\n        x = torch.as_tensor(x).unsqueeze(0)\n        enc_output, _ = self.encoder(x, None)\n        return enc_output.squeeze(0)\n\n    def translate(\n        self, x, trans_args, char_list=None, rnnlm=None, use_jit=False,\n    ):\n        """"""Translate input speech.\n\n        :param ndnarray x: input acoustic feature (B, T, D) or (T, D)\n        :param Namespace trans_args: argment Namespace contraining options\n        :param list char_list: list of characters\n        :param torch.nn.Module rnnlm: language model module\n        :return: N-best decoding results\n        :rtype: list\n        """"""\n        # preprate sos\n        if getattr(trans_args, ""tgt_lang"", False):\n            if self.replace_sos:\n                y = char_list.index(trans_args.tgt_lang)\n        else:\n            y = self.sos\n        logging.info(""<sos> index: "" + str(y))\n        logging.info(""<sos> mark: "" + char_list[y])\n\n        enc_output = self.encode(x).unsqueeze(0)\n        h = enc_output.squeeze(0)\n\n        logging.info(""input lengths: "" + str(h.size(0)))\n        # search parms\n        beam = trans_args.beam_size\n        penalty = trans_args.penalty\n\n        vy = h.new_zeros(1).long()\n\n        if trans_args.maxlenratio == 0:\n            maxlen = h.shape[0]\n        else:\n            # maxlen >= 1\n            maxlen = max(1, int(trans_args.maxlenratio * h.size(0)))\n        minlen = int(trans_args.minlenratio * h.size(0))\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {""score"": 0.0, ""yseq"": [y], ""rnnlm_prev"": None}\n        else:\n            hyp = {""score"": 0.0, ""yseq"": [y]}\n        hyps = [hyp]\n        ended_hyps = []\n\n        import six\n\n        traced_decoder = None\n        for i in six.moves.range(maxlen):\n            logging.debug(""position "" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                vy[0] = hyp[""yseq""][i]\n\n                # get nbest local scores and their ids\n                ys_mask = subsequent_mask(i + 1).unsqueeze(0)\n                ys = torch.tensor(hyp[""yseq""]).unsqueeze(0)\n                # FIXME: jit does not match non-jit result\n                if use_jit:\n                    if traced_decoder is None:\n                        traced_decoder = torch.jit.trace(\n                            self.decoder.forward_one_step, (ys, ys_mask, enc_output)\n                        )\n                    local_att_scores = traced_decoder(ys, ys_mask, enc_output)[0]\n                else:\n                    local_att_scores = self.decoder.forward_one_step(\n                        ys, ys_mask, enc_output\n                    )[0]\n\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(hyp[""rnnlm_prev""], vy)\n                    local_scores = (\n                        local_att_scores + trans_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                local_best_scores, local_best_ids = torch.topk(\n                    local_scores, beam, dim=1\n                )\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    new_hyp[""score""] = hyp[""score""] + float(local_best_scores[0, j])\n                    new_hyp[""yseq""] = [0] * (1 + len(hyp[""yseq""]))\n                    new_hyp[""yseq""][: len(hyp[""yseq""])] = hyp[""yseq""]\n                    new_hyp[""yseq""][len(hyp[""yseq""])] = int(local_best_ids[0, j])\n                    if rnnlm:\n                        new_hyp[""rnnlm_prev""] = rnnlm_state\n                    # will be (2 x beam) hyps at most\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[""score""], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(""number of pruned hypothes: "" + str(len(hyps)))\n            if char_list is not None:\n                logging.debug(\n                    ""best hypo: ""\n                    + """".join([char_list[int(x)] for x in hyps[0][""yseq""][1:]])\n                )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(""adding <eos> in the last postion in the loop"")\n                for hyp in hyps:\n                    hyp[""yseq""].append(self.eos)\n\n            # add ended hypothes to a final list, and removed them from current hypothes\n            # (this will be a probmlem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[""yseq""][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[""yseq""]) > minlen:\n                        hyp[""score""] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[""score""] += trans_args.lm_weight * rnnlm.final(\n                                hyp[""rnnlm_prev""]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n            if end_detect(ended_hyps, i) and trans_args.maxlenratio == 0.0:\n                logging.info(""end detected at %d"", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(""remeined hypothes: "" + str(len(hyps)))\n            else:\n                logging.info(""no hypothesis. Finish decoding."")\n                break\n\n            if char_list is not None:\n                for hyp in hyps:\n                    logging.debug(\n                        ""hypo: "" + """".join([char_list[int(x)] for x in hyp[""yseq""][1:]])\n                    )\n\n            logging.debug(""number of ended hypothes: "" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x[""score""], reverse=True)[\n            : min(len(ended_hyps), trans_args.nbest)\n        ]\n\n        # check number of hypotheis\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                ""there is no N-best results, perform translation ""\n                ""again with smaller minlenratio.""\n            )\n            # should copy becasuse Namespace will be overwritten globally\n            trans_args = Namespace(**vars(trans_args))\n            trans_args.minlenratio = max(0.0, trans_args.minlenratio - 0.1)\n            return self.translate(x, trans_args, char_list, rnnlm)\n\n        logging.info(""total log probability: "" + str(nbest_hyps[0][""score""]))\n        logging.info(\n            ""normalized log probability: ""\n            + str(nbest_hyps[0][""score""] / len(nbest_hyps[0][""yseq""]))\n        )\n        return nbest_hyps\n\n    def calculate_all_attentions(self, xs_pad, ilens, ys_pad, ys_pad_src):\n        """"""E2E attention calculation.\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor ys_pad: batch of padded token id sequence tensor (B, Lmax)\n        :param torch.Tensor ys_pad_src:\n            batch of padded token id sequence tensor (B, Lmax)\n        :return: attention weights with the following shape,\n            1) multi-head case => attention weights (B, H, Lmax, Tmax),\n            2) other case => attention weights (B, Lmax, Tmax).\n        :rtype: float ndarray\n        """"""\n        with torch.no_grad():\n            self.forward(xs_pad, ilens, ys_pad, ys_pad_src)\n        ret = dict()\n        for name, m in self.named_modules():\n            if (\n                isinstance(m, MultiHeadedAttention) and m.attn is not None\n            ):  # skip MHA for submodules\n                ret[name] = m.attn.cpu().numpy()\n        return ret\n'"
espnet/nets/pytorch_backend/e2e_tts_fastspeech.py,18,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""FastSpeech related modules.""""""\n\nimport logging\n\nimport torch\nimport torch.nn.functional as F\n\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.nets.pytorch_backend.e2e_tts_transformer import TTSPlot\nfrom espnet.nets.pytorch_backend.fastspeech.duration_calculator import (\n    DurationCalculator,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.fastspeech.duration_predictor import DurationPredictor\nfrom espnet.nets.pytorch_backend.fastspeech.duration_predictor import (\n    DurationPredictorLoss,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.fastspeech.length_regulator import LengthRegulator\nfrom espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.tacotron2.decoder import Postnet\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.embedding import ScaledPositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.initializer import initialize\nfrom espnet.nets.tts_interface import TTSInterface\nfrom espnet.utils.cli_utils import strtobool\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass FeedForwardTransformerLoss(torch.nn.Module):\n    """"""Loss function module for feed-forward Transformer.""""""\n\n    def __init__(self, use_masking=True, use_weighted_masking=False):\n        """"""Initialize feed-forward Transformer loss module.\n\n        Args:\n            use_masking (bool):\n                Whether to apply masking for padded part in loss calculation.\n            use_weighted_masking (bool):\n                Whether to weighted masking in loss calculation.\n\n        """"""\n        super(FeedForwardTransformerLoss, self).__init__()\n        assert (use_masking != use_weighted_masking) or not use_masking\n        self.use_masking = use_masking\n        self.use_weighted_masking = use_weighted_masking\n\n        # define criterions\n        reduction = ""none"" if self.use_weighted_masking else ""mean""\n        self.l1_criterion = torch.nn.L1Loss(reduction=reduction)\n        self.duration_criterion = DurationPredictorLoss(reduction=reduction)\n\n    def forward(self, after_outs, before_outs, d_outs, ys, ds, ilens, olens):\n        """"""Calculate forward propagation.\n\n        Args:\n            after_outs (Tensor): Batch of outputs after postnets (B, Lmax, odim).\n            before_outs (Tensor): Batch of outputs before postnets (B, Lmax, odim).\n            d_outs (Tensor): Batch of outputs of duration predictor (B, Tmax).\n            ys (Tensor): Batch of target features (B, Lmax, odim).\n            ds (Tensor): Batch of durations (B, Tmax).\n            ilens (LongTensor): Batch of the lengths of each input (B,).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n\n        Returns:\n            Tensor: L1 loss value.\n            Tensor: Duration predictor loss value.\n\n        """"""\n        # apply mask to remove padded part\n        if self.use_masking:\n            duration_masks = make_non_pad_mask(ilens).to(ys.device)\n            d_outs = d_outs.masked_select(duration_masks)\n            ds = ds.masked_select(duration_masks)\n            out_masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)\n            before_outs = before_outs.masked_select(out_masks)\n            after_outs = (\n                after_outs.masked_select(out_masks) if after_outs is not None else None\n            )\n            ys = ys.masked_select(out_masks)\n\n        # calculate loss\n        l1_loss = self.l1_criterion(before_outs, ys)\n        if after_outs is not None:\n            l1_loss += self.l1_criterion(after_outs, ys)\n        duration_loss = self.duration_criterion(d_outs, ds)\n\n        # make weighted mask and apply it\n        if self.use_weighted_masking:\n            out_masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)\n            out_weights = out_masks.float() / out_masks.sum(dim=1, keepdim=True).float()\n            out_weights /= ys.size(0) * ys.size(2)\n            duration_masks = make_non_pad_mask(ilens).to(ys.device)\n            duration_weights = (\n                duration_masks.float() / duration_masks.sum(dim=1, keepdim=True).float()\n            )\n            duration_weights /= ds.size(0)\n\n            # apply weight\n            l1_loss = l1_loss.mul(out_weights).masked_select(out_masks).sum()\n            duration_loss = (\n                duration_loss.mul(duration_weights).masked_select(duration_masks).sum()\n            )\n\n        return l1_loss, duration_loss\n\n\nclass FeedForwardTransformer(TTSInterface, torch.nn.Module):\n    """"""Feed Forward Transformer for TTS a.k.a. FastSpeech.\n\n    This is a module of FastSpeech,\n    feed-forward Transformer with duration predictor described in\n    `FastSpeech: Fast, Robust and Controllable Text to Speech`_,\n    which does not require any auto-regressive\n    processing during inference,\n    resulting in fast decoding compared with auto-regressive Transformer.\n\n    .. _`FastSpeech: Fast, Robust and Controllable Text to Speech`:\n        https://arxiv.org/pdf/1905.09263.pdf\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        group = parser.add_argument_group(""feed-forward transformer model setting"")\n        # network structure related\n        group.add_argument(\n            ""--adim"",\n            default=384,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        group.add_argument(\n            ""--elayers"", default=6, type=int, help=""Number of encoder layers""\n        )\n        group.add_argument(\n            ""--eunits"", default=1536, type=int, help=""Number of encoder hidden units""\n        )\n        group.add_argument(\n            ""--dlayers"", default=6, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=1536, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--positionwise-layer-type"",\n            default=""linear"",\n            type=str,\n            choices=[""linear"", ""conv1d"", ""conv1d-linear""],\n            help=""Positionwise layer type."",\n        )\n        group.add_argument(\n            ""--positionwise-conv-kernel-size"",\n            default=3,\n            type=int,\n            help=""Kernel size of positionwise conv1d layer"",\n        )\n        group.add_argument(\n            ""--postnet-layers"", default=0, type=int, help=""Number of postnet layers""\n        )\n        group.add_argument(\n            ""--postnet-chans"", default=256, type=int, help=""Number of postnet channels""\n        )\n        group.add_argument(\n            ""--postnet-filts"", default=5, type=int, help=""Filter size of postnet""\n        )\n        group.add_argument(\n            ""--use-batch-norm"",\n            default=True,\n            type=strtobool,\n            help=""Whether to use batch normalization"",\n        )\n        group.add_argument(\n            ""--use-scaled-pos-enc"",\n            default=True,\n            type=strtobool,\n            help=""Use trainable scaled positional encoding ""\n            ""instead of the fixed scale one"",\n        )\n        group.add_argument(\n            ""--encoder-normalize-before"",\n            default=False,\n            type=strtobool,\n            help=""Whether to apply layer norm before encoder block"",\n        )\n        group.add_argument(\n            ""--decoder-normalize-before"",\n            default=False,\n            type=strtobool,\n            help=""Whether to apply layer norm before decoder block"",\n        )\n        group.add_argument(\n            ""--encoder-concat-after"",\n            default=False,\n            type=strtobool,\n            help=""Whether to concatenate attention layer\'s input and output in encoder"",\n        )\n        group.add_argument(\n            ""--decoder-concat-after"",\n            default=False,\n            type=strtobool,\n            help=""Whether to concatenate attention layer\'s input and output in decoder"",\n        )\n        group.add_argument(\n            ""--duration-predictor-layers"",\n            default=2,\n            type=int,\n            help=""Number of layers in duration predictor"",\n        )\n        group.add_argument(\n            ""--duration-predictor-chans"",\n            default=384,\n            type=int,\n            help=""Number of channels in duration predictor"",\n        )\n        group.add_argument(\n            ""--duration-predictor-kernel-size"",\n            default=3,\n            type=int,\n            help=""Kernel size in duration predictor"",\n        )\n        group.add_argument(\n            ""--teacher-model"",\n            default=None,\n            type=str,\n            nargs=""?"",\n            help=""Teacher model file path"",\n        )\n        group.add_argument(\n            ""--reduction-factor"", default=1, type=int, help=""Reduction factor""\n        )\n        group.add_argument(\n            ""--spk-embed-dim"",\n            default=None,\n            type=int,\n            help=""Number of speaker embedding dimensions"",\n        )\n        group.add_argument(\n            ""--spk-embed-integration-type"",\n            type=str,\n            default=""add"",\n            choices=[""add"", ""concat""],\n            help=""How to integrate speaker embedding"",\n        )\n        # training related\n        group.add_argument(\n            ""--transformer-init"",\n            type=str,\n            default=""pytorch"",\n            choices=[\n                ""pytorch"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n            ],\n            help=""How to initialize transformer parameters"",\n        )\n        group.add_argument(\n            ""--initial-encoder-alpha"",\n            type=float,\n            default=1.0,\n            help=""Initial alpha value in encoder\'s ScaledPositionalEncoding"",\n        )\n        group.add_argument(\n            ""--initial-decoder-alpha"",\n            type=float,\n            default=1.0,\n            help=""Initial alpha value in decoder\'s ScaledPositionalEncoding"",\n        )\n        group.add_argument(\n            ""--transformer-lr"",\n            default=1.0,\n            type=float,\n            help=""Initial value of learning rate"",\n        )\n        group.add_argument(\n            ""--transformer-warmup-steps"",\n            default=4000,\n            type=int,\n            help=""Optimizer warmup steps"",\n        )\n        group.add_argument(\n            ""--transformer-enc-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder except for attention"",\n        )\n        group.add_argument(\n            ""--transformer-enc-positional-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder positional encoding"",\n        )\n        group.add_argument(\n            ""--transformer-enc-attn-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder self-attention"",\n        )\n        group.add_argument(\n            ""--transformer-dec-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer decoder except ""\n            ""for attention and pos encoding"",\n        )\n        group.add_argument(\n            ""--transformer-dec-positional-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer decoder positional encoding"",\n        )\n        group.add_argument(\n            ""--transformer-dec-attn-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer decoder self-attention"",\n        )\n        group.add_argument(\n            ""--transformer-enc-dec-attn-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder-decoder attention"",\n        )\n        group.add_argument(\n            ""--duration-predictor-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for duration predictor"",\n        )\n        group.add_argument(\n            ""--postnet-dropout-rate"",\n            default=0.5,\n            type=float,\n            help=""Dropout rate in postnet"",\n        )\n        group.add_argument(\n            ""--transfer-encoder-from-teacher"",\n            default=True,\n            type=strtobool,\n            help=""Whether to transfer teacher\'s parameters"",\n        )\n        group.add_argument(\n            ""--transferred-encoder-module"",\n            default=""all"",\n            type=str,\n            choices=[""all"", ""embed""],\n            help=""Encoder modeules to be trasferred from teacher"",\n        )\n        # loss related\n        group.add_argument(\n            ""--use-masking"",\n            default=True,\n            type=strtobool,\n            help=""Whether to use masking in calculation of loss"",\n        )\n        group.add_argument(\n            ""--use-weighted-masking"",\n            default=False,\n            type=strtobool,\n            help=""Whether to use weighted masking in calculation of loss"",\n        )\n        return parser\n\n    def __init__(self, idim, odim, args=None):\n        """"""Initialize feed-forward Transformer module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n            odim (int): Dimension of the outputs.\n            args (Namespace, optional):\n                - elayers (int): Number of encoder layers.\n                - eunits (int): Number of encoder hidden units.\n                - adim (int): Number of attention transformation dimensions.\n                - aheads (int): Number of heads for multi head attention.\n                - dlayers (int): Number of decoder layers.\n                - dunits (int): Number of decoder hidden units.\n                - use_scaled_pos_enc (bool):\n                    Whether to use trainable scaled positional encoding.\n                - encoder_normalize_before (bool):\n                    Whether to perform layer normalization before encoder block.\n                - decoder_normalize_before (bool):\n                    Whether to perform layer normalization before decoder block.\n                - encoder_concat_after (bool): Whether to concatenate attention\n                    layer\'s input and output in encoder.\n                - decoder_concat_after (bool): Whether to concatenate attention\n                    layer\'s input and output in decoder.\n                - duration_predictor_layers (int): Number of duration predictor layers.\n                - duration_predictor_chans (int): Number of duration predictor channels.\n                - duration_predictor_kernel_size (int):\n                    Kernel size of duration predictor.\n                - spk_embed_dim (int): Number of speaker embedding dimensions.\n                - spk_embed_integration_type: How to integrate speaker embedding.\n                - teacher_model (str): Teacher auto-regressive transformer model path.\n                - reduction_factor (int): Reduction factor.\n                - transformer_init (float): How to initialize transformer parameters.\n                - transformer_lr (float): Initial value of learning rate.\n                - transformer_warmup_steps (int): Optimizer warmup steps.\n                - transformer_enc_dropout_rate (float):\n                    Dropout rate in encoder except attention & positional encoding.\n                - transformer_enc_positional_dropout_rate (float):\n                    Dropout rate after encoder positional encoding.\n                - transformer_enc_attn_dropout_rate (float):\n                    Dropout rate in encoder self-attention module.\n                - transformer_dec_dropout_rate (float):\n                    Dropout rate in decoder except attention & positional encoding.\n                - transformer_dec_positional_dropout_rate (float):\n                    Dropout rate after decoder positional encoding.\n                - transformer_dec_attn_dropout_rate (float):\n                    Dropout rate in deocoder self-attention module.\n                - transformer_enc_dec_attn_dropout_rate (float):\n                    Dropout rate in encoder-deocoder attention module.\n                - use_masking (bool):\n                    Whether to apply masking for padded part in loss calculation.\n                - use_weighted_masking (bool):\n                    Whether to apply weighted masking in loss calculation.\n                - transfer_encoder_from_teacher:\n                    Whether to transfer encoder using teacher encoder parameters.\n                - transferred_encoder_module:\n                    Encoder module to be initialized using teacher parameters.\n\n        """"""\n        # initialize base classes\n        TTSInterface.__init__(self)\n        torch.nn.Module.__init__(self)\n\n        # fill missing arguments\n        args = fill_missing_args(args, self.add_arguments)\n\n        # store hyperparameters\n        self.idim = idim\n        self.odim = odim\n        self.reduction_factor = args.reduction_factor\n        self.use_scaled_pos_enc = args.use_scaled_pos_enc\n        self.spk_embed_dim = args.spk_embed_dim\n        if self.spk_embed_dim is not None:\n            self.spk_embed_integration_type = args.spk_embed_integration_type\n\n        # use idx 0 as padding idx\n        padding_idx = 0\n\n        # get positional encoding class\n        pos_enc_class = (\n            ScaledPositionalEncoding if self.use_scaled_pos_enc else PositionalEncoding\n        )\n\n        # define encoder\n        encoder_input_layer = torch.nn.Embedding(\n            num_embeddings=idim, embedding_dim=args.adim, padding_idx=padding_idx\n        )\n        self.encoder = Encoder(\n            idim=idim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=encoder_input_layer,\n            dropout_rate=args.transformer_enc_dropout_rate,\n            positional_dropout_rate=args.transformer_enc_positional_dropout_rate,\n            attention_dropout_rate=args.transformer_enc_attn_dropout_rate,\n            pos_enc_class=pos_enc_class,\n            normalize_before=args.encoder_normalize_before,\n            concat_after=args.encoder_concat_after,\n            positionwise_layer_type=args.positionwise_layer_type,\n            positionwise_conv_kernel_size=args.positionwise_conv_kernel_size,\n        )\n\n        # define additional projection for speaker embedding\n        if self.spk_embed_dim is not None:\n            if self.spk_embed_integration_type == ""add"":\n                self.projection = torch.nn.Linear(self.spk_embed_dim, args.adim)\n            else:\n                self.projection = torch.nn.Linear(\n                    args.adim + self.spk_embed_dim, args.adim\n                )\n\n        # define duration predictor\n        self.duration_predictor = DurationPredictor(\n            idim=args.adim,\n            n_layers=args.duration_predictor_layers,\n            n_chans=args.duration_predictor_chans,\n            kernel_size=args.duration_predictor_kernel_size,\n            dropout_rate=args.duration_predictor_dropout_rate,\n        )\n\n        # define length regulator\n        self.length_regulator = LengthRegulator()\n\n        # define decoder\n        # NOTE: we use encoder as decoder\n        # because fastspeech\'s decoder is the same as encoder\n        self.decoder = Encoder(\n            idim=0,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.dunits,\n            num_blocks=args.dlayers,\n            input_layer=None,\n            dropout_rate=args.transformer_dec_dropout_rate,\n            positional_dropout_rate=args.transformer_dec_positional_dropout_rate,\n            attention_dropout_rate=args.transformer_dec_attn_dropout_rate,\n            pos_enc_class=pos_enc_class,\n            normalize_before=args.decoder_normalize_before,\n            concat_after=args.decoder_concat_after,\n            positionwise_layer_type=args.positionwise_layer_type,\n            positionwise_conv_kernel_size=args.positionwise_conv_kernel_size,\n        )\n\n        # define final projection\n        self.feat_out = torch.nn.Linear(args.adim, odim * args.reduction_factor)\n\n        # define postnet\n        self.postnet = (\n            None\n            if args.postnet_layers == 0\n            else Postnet(\n                idim=idim,\n                odim=odim,\n                n_layers=args.postnet_layers,\n                n_chans=args.postnet_chans,\n                n_filts=args.postnet_filts,\n                use_batch_norm=args.use_batch_norm,\n                dropout_rate=args.postnet_dropout_rate,\n            )\n        )\n\n        # initialize parameters\n        self._reset_parameters(\n            init_type=args.transformer_init,\n            init_enc_alpha=args.initial_encoder_alpha,\n            init_dec_alpha=args.initial_decoder_alpha,\n        )\n\n        # define teacher model\n        if args.teacher_model is not None:\n            self.teacher = self._load_teacher_model(args.teacher_model)\n        else:\n            self.teacher = None\n\n        # define duration calculator\n        if self.teacher is not None:\n            self.duration_calculator = DurationCalculator(self.teacher)\n        else:\n            self.duration_calculator = None\n\n        # transfer teacher parameters\n        if self.teacher is not None and args.transfer_encoder_from_teacher:\n            self._transfer_from_teacher(args.transferred_encoder_module)\n\n        # define criterions\n        self.criterion = FeedForwardTransformerLoss(\n            use_masking=args.use_masking, use_weighted_masking=args.use_weighted_masking\n        )\n\n    def _forward(\n        self,\n        xs,\n        ilens,\n        ys=None,\n        olens=None,\n        spembs=None,\n        ds=None,\n        is_inference=False,\n        alpha=1.0,\n    ):\n        # forward encoder\n        x_masks = self._source_mask(ilens)\n        hs, _ = self.encoder(xs, x_masks)  # (B, Tmax, adim)\n\n        # integrate speaker embedding\n        if self.spk_embed_dim is not None:\n            hs = self._integrate_with_spk_embed(hs, spembs)\n\n        # forward duration predictor and length regulator\n        d_masks = make_pad_mask(ilens).to(xs.device)\n        if is_inference:\n            d_outs = self.duration_predictor.inference(hs, d_masks)  # (B, Tmax)\n            hs = self.length_regulator(hs, d_outs, ilens, alpha)  # (B, Lmax, adim)\n        else:\n            if ds is None:\n                with torch.no_grad():\n                    ds = self.duration_calculator(\n                        xs, ilens, ys, olens, spembs\n                    )  # (B, Tmax)\n            d_outs = self.duration_predictor(hs, d_masks)  # (B, Tmax)\n            hs = self.length_regulator(hs, ds, ilens)  # (B, Lmax, adim)\n\n        # forward decoder\n        if olens is not None:\n            if self.reduction_factor > 1:\n                olens_in = olens.new([olen // self.reduction_factor for olen in olens])\n            else:\n                olens_in = olens\n            h_masks = self._source_mask(olens_in)\n        else:\n            h_masks = None\n        zs, _ = self.decoder(hs, h_masks)  # (B, Lmax, adim)\n        before_outs = self.feat_out(zs).view(\n            zs.size(0), -1, self.odim\n        )  # (B, Lmax, odim)\n\n        # postnet -> (B, Lmax//r * r, odim)\n        if self.postnet is None:\n            after_outs = before_outs\n        else:\n            after_outs = before_outs + self.postnet(\n                before_outs.transpose(1, 2)\n            ).transpose(1, 2)\n\n        if is_inference:\n            return before_outs, after_outs, d_outs\n        else:\n            return before_outs, after_outs, ds, d_outs\n\n    def forward(self, xs, ilens, ys, olens, spembs=None, extras=None, *args, **kwargs):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of padded character ids (B, Tmax).\n            ilens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor): Batch of padded target features (B, Lmax, odim).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n            spembs (Tensor, optional):\n                Batch of speaker embedding vectors (B, spk_embed_dim).\n            extras (Tensor, optional): Batch of precalculated durations (B, Tmax, 1).\n\n        Returns:\n            Tensor: Loss value.\n\n        """"""\n        # remove unnecessary padded part (for multi-gpus)\n        xs = xs[:, : max(ilens)]\n        ys = ys[:, : max(olens)]\n        if extras is not None:\n            extras = extras[:, : max(ilens)].squeeze(-1)\n\n        # forward propagation\n        before_outs, after_outs, ds, d_outs = self._forward(\n            xs, ilens, ys, olens, spembs=spembs, ds=extras, is_inference=False\n        )\n\n        # modifiy mod part of groundtruth\n        if self.reduction_factor > 1:\n            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])\n            max_olen = max(olens)\n            ys = ys[:, :max_olen]\n\n        # calculate loss\n        if self.postnet is None:\n            l1_loss, duration_loss = self.criterion(\n                None, before_outs, d_outs, ys, ds, ilens, olens\n            )\n        else:\n            l1_loss, duration_loss = self.criterion(\n                after_outs, before_outs, d_outs, ys, ds, ilens, olens\n            )\n        loss = l1_loss + duration_loss\n        report_keys = [\n            {""l1_loss"": l1_loss.item()},\n            {""duration_loss"": duration_loss.item()},\n            {""loss"": loss.item()},\n        ]\n\n        # report extra information\n        if self.use_scaled_pos_enc:\n            report_keys += [\n                {""encoder_alpha"": self.encoder.embed[-1].alpha.data.item()},\n                {""decoder_alpha"": self.decoder.embed[-1].alpha.data.item()},\n            ]\n        self.reporter.report(report_keys)\n\n        return loss\n\n    def calculate_all_attentions(\n        self, xs, ilens, ys, olens, spembs=None, extras=None, *args, **kwargs\n    ):\n        """"""Calculate all of the attention weights.\n\n        Args:\n            xs (Tensor): Batch of padded character ids (B, Tmax).\n            ilens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor): Batch of padded target features (B, Lmax, odim).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n            spembs (Tensor, optional):\n                Batch of speaker embedding vectors (B, spk_embed_dim).\n            extras (Tensor, optional): Batch of precalculated durations (B, Tmax, 1).\n\n        Returns:\n            dict: Dict of attention weights and outputs.\n\n        """"""\n        with torch.no_grad():\n            # remove unnecessary padded part (for multi-gpus)\n            xs = xs[:, : max(ilens)]\n            ys = ys[:, : max(olens)]\n            if extras is not None:\n                extras = extras[:, : max(ilens)].squeeze(-1)\n\n            # forward propagation\n            outs = self._forward(\n                xs, ilens, ys, olens, spembs=spembs, ds=extras, is_inference=False\n            )[1]\n\n        att_ws_dict = dict()\n        for name, m in self.named_modules():\n            if isinstance(m, MultiHeadedAttention):\n                attn = m.attn.cpu().numpy()\n                if ""encoder"" in name:\n                    attn = [a[:, :l, :l] for a, l in zip(attn, ilens.tolist())]\n                elif ""decoder"" in name:\n                    if ""src"" in name:\n                        attn = [\n                            a[:, :ol, :il]\n                            for a, il, ol in zip(attn, ilens.tolist(), olens.tolist())\n                        ]\n                    elif ""self"" in name:\n                        attn = [a[:, :l, :l] for a, l in zip(attn, olens.tolist())]\n                    else:\n                        logging.warning(""unknown attention module: "" + name)\n                else:\n                    logging.warning(""unknown attention module: "" + name)\n                att_ws_dict[name] = attn\n        att_ws_dict[""predicted_fbank""] = [\n            m[:l].T for m, l in zip(outs.cpu().numpy(), olens.tolist())\n        ]\n\n        return att_ws_dict\n\n    def inference(self, x, inference_args, spemb=None, *args, **kwargs):\n        """"""Generate the sequence of features given the sequences of characters.\n\n        Args:\n            x (Tensor): Input sequence of characters (T,).\n            inference_args (Namespace): Dummy for compatibility.\n            spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim).\n\n        Returns:\n            Tensor: Output sequence of features (L, odim).\n            None: Dummy for compatibility.\n            None: Dummy for compatibility.\n\n        """"""\n        # setup batch axis\n        ilens = torch.tensor([x.shape[0]], dtype=torch.long, device=x.device)\n        xs = x.unsqueeze(0)\n        if spemb is not None:\n            spembs = spemb.unsqueeze(0)\n        else:\n            spembs = None\n\n        # get option\n        alpha = getattr(inference_args, ""fastspeech_alpha"", 1.0)\n\n        # inference\n        _, outs, _ = self._forward(\n            xs, ilens, spembs=spembs, is_inference=True, alpha=alpha,\n        )  # (1, L, odim)\n\n        return outs[0], None, None\n\n    def _integrate_with_spk_embed(self, hs, spembs):\n        """"""Integrate speaker embedding with hidden states.\n\n        Args:\n            hs (Tensor): Batch of hidden state sequences (B, Tmax, adim).\n            spembs (Tensor): Batch of speaker embeddings (B, spk_embed_dim).\n\n        Returns:\n            Tensor: Batch of integrated hidden state sequences (B, Tmax, adim)\n\n        """"""\n        if self.spk_embed_integration_type == ""add"":\n            # apply projection and then add to hidden states\n            spembs = self.projection(F.normalize(spembs))\n            hs = hs + spembs.unsqueeze(1)\n        elif self.spk_embed_integration_type == ""concat"":\n            # concat hidden states with spk embeds and then apply projection\n            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n            hs = self.projection(torch.cat([hs, spembs], dim=-1))\n        else:\n            raise NotImplementedError(""support only add or concat."")\n\n        return hs\n\n    def _source_mask(self, ilens):\n        """"""Make masks for self-attention.\n\n        Args:\n            ilens (LongTensor or List): Batch of lengths (B,).\n\n        Returns:\n            Tensor: Mask tensor for self-attention.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n        Examples:\n            >>> ilens = [5, 3]\n            >>> self._source_mask(ilens)\n            tensor([[[1, 1, 1, 1, 1],\n                     [1, 1, 1, 0, 0]]], dtype=torch.uint8)\n\n        """"""\n        x_masks = make_non_pad_mask(ilens).to(next(self.parameters()).device)\n        return x_masks.unsqueeze(-2)\n\n    def _load_teacher_model(self, model_path):\n        # get teacher model config\n        idim, odim, args = get_model_conf(model_path)\n\n        # assert dimension is the same between teacher and studnet\n        assert idim == self.idim\n        assert odim == self.odim\n        assert args.reduction_factor == self.reduction_factor\n\n        # load teacher model\n        from espnet.utils.dynamic_import import dynamic_import\n\n        model_class = dynamic_import(args.model_module)\n        model = model_class(idim, odim, args)\n        torch_load(model_path, model)\n\n        # freeze teacher model parameters\n        for p in model.parameters():\n            p.requires_grad = False\n\n        return model\n\n    def _reset_parameters(self, init_type, init_enc_alpha=1.0, init_dec_alpha=1.0):\n        # initialize parameters\n        initialize(self, init_type)\n\n        # initialize alpha in scaled positional encoding\n        if self.use_scaled_pos_enc:\n            self.encoder.embed[-1].alpha.data = torch.tensor(init_enc_alpha)\n            self.decoder.embed[-1].alpha.data = torch.tensor(init_dec_alpha)\n\n    def _transfer_from_teacher(self, transferred_encoder_module):\n        if transferred_encoder_module == ""all"":\n            for (n1, p1), (n2, p2) in zip(\n                self.encoder.named_parameters(), self.teacher.encoder.named_parameters()\n            ):\n                assert n1 == n2, ""It seems that encoder structure is different.""\n                assert p1.shape == p2.shape, ""It seems that encoder size is different.""\n                p1.data.copy_(p2.data)\n        elif transferred_encoder_module == ""embed"":\n            student_shape = self.encoder.embed[0].weight.data.shape\n            teacher_shape = self.teacher.encoder.embed[0].weight.data.shape\n            assert (\n                student_shape == teacher_shape\n            ), ""It seems that embed dimension is different.""\n            self.encoder.embed[0].weight.data.copy_(\n                self.teacher.encoder.embed[0].weight.data\n            )\n        else:\n            raise NotImplementedError(""Support only all or embed."")\n\n    @property\n    def attention_plot_class(self):\n        """"""Return plot class for attention weight plot.""""""\n        return TTSPlot\n\n    @property\n    def base_plot_keys(self):\n        """"""Return base key names to plot during training.\n\n        keys should match what `chainer.reporter` reports.\n        If you add the key `loss`,\n        the reporter will report `main/loss` and `validation/main/loss` values.\n        also `loss.png` will be created as a figure visulizing `main/loss`\n        and `validation/main/loss` values.\n\n        Returns:\n            list: List of strings which are base keys to plot during training.\n\n        """"""\n        plot_keys = [""loss"", ""l1_loss"", ""duration_loss""]\n        if self.use_scaled_pos_enc:\n            plot_keys += [""encoder_alpha"", ""decoder_alpha""]\n\n        return plot_keys\n'"
espnet/nets/pytorch_backend/e2e_tts_tacotron2.py,23,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Tacotron 2 related modules.""""""\n\nimport logging\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttForward\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttForwardTA\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttLoc\nfrom espnet.nets.pytorch_backend.tacotron2.cbhg import CBHG\nfrom espnet.nets.pytorch_backend.tacotron2.cbhg import CBHGLoss\nfrom espnet.nets.pytorch_backend.tacotron2.decoder import Decoder\nfrom espnet.nets.pytorch_backend.tacotron2.encoder import Encoder\nfrom espnet.nets.tts_interface import TTSInterface\nfrom espnet.utils.cli_utils import strtobool\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass GuidedAttentionLoss(torch.nn.Module):\n    """"""Guided attention loss function module.\n\n    This module calculates the guided attention loss described\n    in `Efficiently Trainable Text-to-Speech System Based\n    on Deep Convolutional Networks with Guided Attention`_,\n    which forces the attention to be diagonal.\n\n    .. _`Efficiently Trainable Text-to-Speech System\n        Based on Deep Convolutional Networks with Guided Attention`:\n        https://arxiv.org/abs/1710.08969\n\n    """"""\n\n    def __init__(self, sigma=0.4, alpha=1.0, reset_always=True):\n        """"""Initialize guided attention loss module.\n\n        Args:\n            sigma (float, optional): Standard deviation to control\n                how close attention to a diagonal.\n            alpha (float, optional): Scaling coefficient (lambda).\n            reset_always (bool, optional): Whether to always reset masks.\n\n        """"""\n        super(GuidedAttentionLoss, self).__init__()\n        self.sigma = sigma\n        self.alpha = alpha\n        self.reset_always = reset_always\n        self.guided_attn_masks = None\n        self.masks = None\n\n    def _reset_masks(self):\n        self.guided_attn_masks = None\n        self.masks = None\n\n    def forward(self, att_ws, ilens, olens):\n        """"""Calculate forward propagation.\n\n        Args:\n            att_ws (Tensor): Batch of attention weights (B, T_max_out, T_max_in).\n            ilens (LongTensor): Batch of input lenghts (B,).\n            olens (LongTensor): Batch of output lenghts (B,).\n\n        Returns:\n            Tensor: Guided attention loss value.\n\n        """"""\n        if self.guided_attn_masks is None:\n            self.guided_attn_masks = self._make_guided_attention_masks(ilens, olens).to(\n                att_ws.device\n            )\n        if self.masks is None:\n            self.masks = self._make_masks(ilens, olens).to(att_ws.device)\n        losses = self.guided_attn_masks * att_ws\n        loss = torch.mean(losses.masked_select(self.masks))\n        if self.reset_always:\n            self._reset_masks()\n        return self.alpha * loss\n\n    def _make_guided_attention_masks(self, ilens, olens):\n        n_batches = len(ilens)\n        max_ilen = max(ilens)\n        max_olen = max(olens)\n        guided_attn_masks = torch.zeros((n_batches, max_olen, max_ilen))\n        for idx, (ilen, olen) in enumerate(zip(ilens, olens)):\n            guided_attn_masks[idx, :olen, :ilen] = self._make_guided_attention_mask(\n                ilen, olen, self.sigma\n            )\n        return guided_attn_masks\n\n    @staticmethod\n    def _make_guided_attention_mask(ilen, olen, sigma):\n        """"""Make guided attention mask.\n\n        Examples:\n            >>> guided_attn_mask =_make_guided_attention(5, 5, 0.4)\n            >>> guided_attn_mask.shape\n            torch.Size([5, 5])\n            >>> guided_attn_mask\n            tensor([[0.0000, 0.1175, 0.3935, 0.6753, 0.8647],\n                    [0.1175, 0.0000, 0.1175, 0.3935, 0.6753],\n                    [0.3935, 0.1175, 0.0000, 0.1175, 0.3935],\n                    [0.6753, 0.3935, 0.1175, 0.0000, 0.1175],\n                    [0.8647, 0.6753, 0.3935, 0.1175, 0.0000]])\n            >>> guided_attn_mask =_make_guided_attention(3, 6, 0.4)\n            >>> guided_attn_mask.shape\n            torch.Size([6, 3])\n            >>> guided_attn_mask\n            tensor([[0.0000, 0.2934, 0.7506],\n                    [0.0831, 0.0831, 0.5422],\n                    [0.2934, 0.0000, 0.2934],\n                    [0.5422, 0.0831, 0.0831],\n                    [0.7506, 0.2934, 0.0000],\n                    [0.8858, 0.5422, 0.0831]])\n\n        """"""\n        grid_x, grid_y = torch.meshgrid(torch.arange(olen), torch.arange(ilen))\n        grid_x, grid_y = grid_x.float(), grid_y.float()\n        return 1.0 - torch.exp(\n            -((grid_y / ilen - grid_x / olen) ** 2) / (2 * (sigma ** 2))\n        )\n\n    @staticmethod\n    def _make_masks(ilens, olens):\n        """"""Make masks indicating non-padded part.\n\n        Args:\n            ilens (LongTensor or List): Batch of lengths (B,).\n            olens (LongTensor or List): Batch of lengths (B,).\n\n        Returns:\n            Tensor: Mask tensor indicating non-padded part.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n        Examples:\n            >>> ilens, olens = [5, 2], [8, 5]\n            >>> _make_mask(ilens, olens)\n            tensor([[[1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1],\n                     [1, 1, 1, 1, 1]],\n                    [[1, 1, 0, 0, 0],\n                     [1, 1, 0, 0, 0],\n                     [1, 1, 0, 0, 0],\n                     [1, 1, 0, 0, 0],\n                     [1, 1, 0, 0, 0],\n                     [0, 0, 0, 0, 0],\n                     [0, 0, 0, 0, 0],\n                     [0, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n        """"""\n        in_masks = make_non_pad_mask(ilens)  # (B, T_in)\n        out_masks = make_non_pad_mask(olens)  # (B, T_out)\n        return out_masks.unsqueeze(-1) & in_masks.unsqueeze(-2)  # (B, T_out, T_in)\n\n\nclass Tacotron2Loss(torch.nn.Module):\n    """"""Loss function module for Tacotron2.""""""\n\n    def __init__(\n        self, use_masking=True, use_weighted_masking=False, bce_pos_weight=20.0\n    ):\n        """"""Initialize Tactoron2 loss module.\n\n        Args:\n            use_masking (bool): Whether to apply masking\n                for padded part in loss calculation.\n            use_weighted_masking (bool):\n                Whether to apply weighted masking in loss calculation.\n            bce_pos_weight (float): Weight of positive sample of stop token.\n\n        """"""\n        super(Tacotron2Loss, self).__init__()\n        assert (use_masking != use_weighted_masking) or not use_masking\n        self.use_masking = use_masking\n        self.use_weighted_masking = use_weighted_masking\n\n        # define criterions\n        reduction = ""none"" if self.use_weighted_masking else ""mean""\n        self.l1_criterion = torch.nn.L1Loss(reduction=reduction)\n        self.mse_criterion = torch.nn.MSELoss(reduction=reduction)\n        self.bce_criterion = torch.nn.BCEWithLogitsLoss(\n            reduction=reduction, pos_weight=torch.tensor(bce_pos_weight)\n        )\n\n        # NOTE(kan-bayashi): register pre hook function for the compatibility\n        self._register_load_state_dict_pre_hook(self._load_state_dict_pre_hook)\n\n    def forward(self, after_outs, before_outs, logits, ys, labels, olens):\n        """"""Calculate forward propagation.\n\n        Args:\n            after_outs (Tensor): Batch of outputs after postnets (B, Lmax, odim).\n            before_outs (Tensor): Batch of outputs before postnets (B, Lmax, odim).\n            logits (Tensor): Batch of stop logits (B, Lmax).\n            ys (Tensor): Batch of padded target features (B, Lmax, odim).\n            labels (LongTensor): Batch of the sequences of stop token labels (B, Lmax).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n\n        Returns:\n            Tensor: L1 loss value.\n            Tensor: Mean square error loss value.\n            Tensor: Binary cross entropy loss value.\n\n        """"""\n        # make mask and apply it\n        if self.use_masking:\n            masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)\n            ys = ys.masked_select(masks)\n            after_outs = after_outs.masked_select(masks)\n            before_outs = before_outs.masked_select(masks)\n            labels = labels.masked_select(masks[:, :, 0])\n            logits = logits.masked_select(masks[:, :, 0])\n\n        # calculate loss\n        l1_loss = self.l1_criterion(after_outs, ys) + self.l1_criterion(before_outs, ys)\n        mse_loss = self.mse_criterion(after_outs, ys) + self.mse_criterion(\n            before_outs, ys\n        )\n        bce_loss = self.bce_criterion(logits, labels)\n\n        # make weighted mask and apply it\n        if self.use_weighted_masking:\n            masks = make_non_pad_mask(olens).unsqueeze(-1).to(ys.device)\n            weights = masks.float() / masks.sum(dim=1, keepdim=True).float()\n            out_weights = weights.div(ys.size(0) * ys.size(2))\n            logit_weights = weights.div(ys.size(0))\n\n            # apply weight\n            l1_loss = l1_loss.mul(out_weights).masked_select(masks).sum()\n            mse_loss = mse_loss.mul(out_weights).masked_select(masks).sum()\n            bce_loss = (\n                bce_loss.mul(logit_weights.squeeze(-1))\n                .masked_select(masks.squeeze(-1))\n                .sum()\n            )\n\n        return l1_loss, mse_loss, bce_loss\n\n    def _load_state_dict_pre_hook(\n        self,\n        state_dict,\n        prefix,\n        local_metadata,\n        strict,\n        missing_keys,\n        unexpected_keys,\n        error_msgs,\n    ):\n        """"""Apply pre hook fucntion before loading state dict.\n\n        From v.0.6.1 `bce_criterion.pos_weight` param is registered as a parameter but\n        old models do not include it and as a result, it causes missing key error when\n        loading old model parameter. This function solve the issue by adding param in\n        state dict before loading as a pre hook function\n        of the `load_state_dict` method.\n\n        """"""\n        key = prefix + ""bce_criterion.pos_weight""\n        if key not in state_dict:\n            state_dict[key] = self.bce_criterion.pos_weight\n\n\nclass Tacotron2(TTSInterface, torch.nn.Module):\n    """"""Tacotron2 module for end-to-end text-to-speech (E2E-TTS).\n\n    This is a module of Spectrogram prediction network in Tacotron2 described\n    in `Natural TTS Synthesis\n    by Conditioning WaveNet on Mel Spectrogram Predictions`_,\n    which converts the sequence of characters\n    into the sequence of Mel-filterbanks.\n\n    .. _`Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`:\n       https://arxiv.org/abs/1712.05884\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        group = parser.add_argument_group(""tacotron 2 model setting"")\n        # encoder\n        group.add_argument(\n            ""--embed-dim"",\n            default=512,\n            type=int,\n            help=""Number of dimension of embedding"",\n        )\n        group.add_argument(\n            ""--elayers"", default=1, type=int, help=""Number of encoder layers""\n        )\n        group.add_argument(\n            ""--eunits"",\n            ""-u"",\n            default=512,\n            type=int,\n            help=""Number of encoder hidden units"",\n        )\n        group.add_argument(\n            ""--econv-layers"",\n            default=3,\n            type=int,\n            help=""Number of encoder convolution layers"",\n        )\n        group.add_argument(\n            ""--econv-chans"",\n            default=512,\n            type=int,\n            help=""Number of encoder convolution channels"",\n        )\n        group.add_argument(\n            ""--econv-filts"",\n            default=5,\n            type=int,\n            help=""Filter size of encoder convolution"",\n        )\n        # attention\n        group.add_argument(\n            ""--atype"",\n            default=""location"",\n            type=str,\n            choices=[""forward_ta"", ""forward"", ""location""],\n            help=""Type of attention mechanism"",\n        )\n        group.add_argument(\n            ""--adim"",\n            default=512,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aconv-chans"",\n            default=32,\n            type=int,\n            help=""Number of attention convolution channels"",\n        )\n        group.add_argument(\n            ""--aconv-filts"",\n            default=15,\n            type=int,\n            help=""Filter size of attention convolution"",\n        )\n        group.add_argument(\n            ""--cumulate-att-w"",\n            default=True,\n            type=strtobool,\n            help=""Whether or not to cumulate attention weights"",\n        )\n        # decoder\n        group.add_argument(\n            ""--dlayers"", default=2, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=1024, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--prenet-layers"", default=2, type=int, help=""Number of prenet layers""\n        )\n        group.add_argument(\n            ""--prenet-units"",\n            default=256,\n            type=int,\n            help=""Number of prenet hidden units"",\n        )\n        group.add_argument(\n            ""--postnet-layers"", default=5, type=int, help=""Number of postnet layers""\n        )\n        group.add_argument(\n            ""--postnet-chans"", default=512, type=int, help=""Number of postnet channels""\n        )\n        group.add_argument(\n            ""--postnet-filts"", default=5, type=int, help=""Filter size of postnet""\n        )\n        group.add_argument(\n            ""--output-activation"",\n            default=None,\n            type=str,\n            nargs=""?"",\n            help=""Output activation function"",\n        )\n        # cbhg\n        group.add_argument(\n            ""--use-cbhg"",\n            default=False,\n            type=strtobool,\n            help=""Whether to use CBHG module"",\n        )\n        group.add_argument(\n            ""--cbhg-conv-bank-layers"",\n            default=8,\n            type=int,\n            help=""Number of convoluional bank layers in CBHG"",\n        )\n        group.add_argument(\n            ""--cbhg-conv-bank-chans"",\n            default=128,\n            type=int,\n            help=""Number of convoluional bank channles in CBHG"",\n        )\n        group.add_argument(\n            ""--cbhg-conv-proj-filts"",\n            default=3,\n            type=int,\n            help=""Filter size of convoluional projection layer in CBHG"",\n        )\n        group.add_argument(\n            ""--cbhg-conv-proj-chans"",\n            default=256,\n            type=int,\n            help=""Number of convoluional projection channels in CBHG"",\n        )\n        group.add_argument(\n            ""--cbhg-highway-layers"",\n            default=4,\n            type=int,\n            help=""Number of highway layers in CBHG"",\n        )\n        group.add_argument(\n            ""--cbhg-highway-units"",\n            default=128,\n            type=int,\n            help=""Number of highway units in CBHG"",\n        )\n        group.add_argument(\n            ""--cbhg-gru-units"",\n            default=256,\n            type=int,\n            help=""Number of GRU units in CBHG"",\n        )\n        # model (parameter) related\n        group.add_argument(\n            ""--use-batch-norm"",\n            default=True,\n            type=strtobool,\n            help=""Whether to use batch normalization"",\n        )\n        group.add_argument(\n            ""--use-concate"",\n            default=True,\n            type=strtobool,\n            help=""Whether to concatenate encoder embedding with decoder outputs"",\n        )\n        group.add_argument(\n            ""--use-residual"",\n            default=True,\n            type=strtobool,\n            help=""Whether to use residual connection in conv layer"",\n        )\n        group.add_argument(\n            ""--dropout-rate"", default=0.5, type=float, help=""Dropout rate""\n        )\n        group.add_argument(\n            ""--zoneout-rate"", default=0.1, type=float, help=""Zoneout rate""\n        )\n        group.add_argument(\n            ""--reduction-factor"", default=1, type=int, help=""Reduction factor""\n        )\n        group.add_argument(\n            ""--spk-embed-dim"",\n            default=None,\n            type=int,\n            help=""Number of speaker embedding dimensions"",\n        )\n        group.add_argument(\n            ""--spc-dim"", default=None, type=int, help=""Number of spectrogram dimensions""\n        )\n        group.add_argument(\n            ""--pretrained-model"", default=None, type=str, help=""Pretrained model path""\n        )\n        # loss related\n        group.add_argument(\n            ""--use-masking"",\n            default=False,\n            type=strtobool,\n            help=""Whether to use masking in calculation of loss"",\n        )\n        group.add_argument(\n            ""--use-weighted-masking"",\n            default=False,\n            type=strtobool,\n            help=""Whether to use weighted masking in calculation of loss"",\n        )\n        group.add_argument(\n            ""--bce-pos-weight"",\n            default=20.0,\n            type=float,\n            help=""Positive sample weight in BCE calculation ""\n            ""(only for use-masking=True)"",\n        )\n        group.add_argument(\n            ""--use-guided-attn-loss"",\n            default=False,\n            type=strtobool,\n            help=""Whether to use guided attention loss"",\n        )\n        group.add_argument(\n            ""--guided-attn-loss-sigma"",\n            default=0.4,\n            type=float,\n            help=""Sigma in guided attention loss"",\n        )\n        group.add_argument(\n            ""--guided-attn-loss-lambda"",\n            default=1.0,\n            type=float,\n            help=""Lambda in guided attention loss"",\n        )\n        return parser\n\n    def __init__(self, idim, odim, args=None):\n        """"""Initialize Tacotron2 module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n            odim (int): Dimension of the outputs.\n            args (Namespace, optional):\n                - spk_embed_dim (int): Dimension of the speaker embedding.\n                - embed_dim (int): Dimension of character embedding.\n                - elayers (int): The number of encoder blstm layers.\n                - eunits (int): The number of encoder blstm units.\n                - econv_layers (int): The number of encoder conv layers.\n                - econv_filts (int): The number of encoder conv filter size.\n                - econv_chans (int): The number of encoder conv filter channels.\n                - dlayers (int): The number of decoder lstm layers.\n                - dunits (int): The number of decoder lstm units.\n                - prenet_layers (int): The number of prenet layers.\n                - prenet_units (int): The number of prenet units.\n                - postnet_layers (int): The number of postnet layers.\n                - postnet_filts (int): The number of postnet filter size.\n                - postnet_chans (int): The number of postnet filter channels.\n                - output_activation (int): The name of activation function for outputs.\n                - adim (int): The number of dimension of mlp in attention.\n                - aconv_chans (int): The number of attention conv filter channels.\n                - aconv_filts (int): The number of attention conv filter size.\n                - cumulate_att_w (bool): Whether to cumulate previous attention weight.\n                - use_batch_norm (bool): Whether to use batch normalization.\n                - use_concate (int): Whether to concatenate encoder embedding\n                    with decoder lstm outputs.\n                - dropout_rate (float): Dropout rate.\n                - zoneout_rate (float): Zoneout rate.\n                - reduction_factor (int): Reduction factor.\n                - spk_embed_dim (int): Number of speaker embedding dimenstions.\n                - spc_dim (int): Number of spectrogram embedding dimenstions\n                    (only for use_cbhg=True).\n                - use_cbhg (bool): Whether to use CBHG module.\n                - cbhg_conv_bank_layers (int): The number of convoluional banks in CBHG.\n                - cbhg_conv_bank_chans (int): The number of channels of\n                    convolutional bank in CBHG.\n                - cbhg_proj_filts (int):\n                    The number of filter size of projection layeri in CBHG.\n                - cbhg_proj_chans (int):\n                    The number of channels of projection layer in CBHG.\n                - cbhg_highway_layers (int):\n                    The number of layers of highway network in CBHG.\n                - cbhg_highway_units (int):\n                    The number of units of highway network in CBHG.\n                - cbhg_gru_units (int): The number of units of GRU in CBHG.\n                - use_masking (bool):\n                    Whether to apply masking for padded part in loss calculation.\n                - use_weighted_masking (bool):\n                    Whether to apply weighted masking in loss calculation.\n                - bce_pos_weight (float):\n                    Weight of positive sample of stop token (only for use_masking=True).\n                - use-guided-attn-loss (bool): Whether to use guided attention loss.\n                - guided-attn-loss-sigma (float) Sigma in guided attention loss.\n                - guided-attn-loss-lamdba (float): Lambda in guided attention loss.\n\n        """"""\n        # initialize base classes\n        TTSInterface.__init__(self)\n        torch.nn.Module.__init__(self)\n\n        # fill missing arguments\n        args = fill_missing_args(args, self.add_arguments)\n\n        # store hyperparameters\n        self.idim = idim\n        self.odim = odim\n        self.spk_embed_dim = args.spk_embed_dim\n        self.cumulate_att_w = args.cumulate_att_w\n        self.reduction_factor = args.reduction_factor\n        self.use_cbhg = args.use_cbhg\n        self.use_guided_attn_loss = args.use_guided_attn_loss\n\n        # define activation function for the final output\n        if args.output_activation is None:\n            self.output_activation_fn = None\n        elif hasattr(F, args.output_activation):\n            self.output_activation_fn = getattr(F, args.output_activation)\n        else:\n            raise ValueError(\n                ""there is no such an activation function. (%s)"" % args.output_activation\n            )\n\n        # set padding idx\n        padding_idx = 0\n\n        # define network modules\n        self.enc = Encoder(\n            idim=idim,\n            embed_dim=args.embed_dim,\n            elayers=args.elayers,\n            eunits=args.eunits,\n            econv_layers=args.econv_layers,\n            econv_chans=args.econv_chans,\n            econv_filts=args.econv_filts,\n            use_batch_norm=args.use_batch_norm,\n            use_residual=args.use_residual,\n            dropout_rate=args.dropout_rate,\n            padding_idx=padding_idx,\n        )\n        dec_idim = (\n            args.eunits\n            if args.spk_embed_dim is None\n            else args.eunits + args.spk_embed_dim\n        )\n        if args.atype == ""location"":\n            att = AttLoc(\n                dec_idim, args.dunits, args.adim, args.aconv_chans, args.aconv_filts\n            )\n        elif args.atype == ""forward"":\n            att = AttForward(\n                dec_idim, args.dunits, args.adim, args.aconv_chans, args.aconv_filts\n            )\n            if self.cumulate_att_w:\n                logging.warning(\n                    ""cumulation of attention weights is disabled in forward attention.""\n                )\n                self.cumulate_att_w = False\n        elif args.atype == ""forward_ta"":\n            att = AttForwardTA(\n                dec_idim,\n                args.dunits,\n                args.adim,\n                args.aconv_chans,\n                args.aconv_filts,\n                odim,\n            )\n            if self.cumulate_att_w:\n                logging.warning(\n                    ""cumulation of attention weights is disabled in forward attention.""\n                )\n                self.cumulate_att_w = False\n        else:\n            raise NotImplementedError(""Support only location or forward"")\n        self.dec = Decoder(\n            idim=dec_idim,\n            odim=odim,\n            att=att,\n            dlayers=args.dlayers,\n            dunits=args.dunits,\n            prenet_layers=args.prenet_layers,\n            prenet_units=args.prenet_units,\n            postnet_layers=args.postnet_layers,\n            postnet_chans=args.postnet_chans,\n            postnet_filts=args.postnet_filts,\n            output_activation_fn=self.output_activation_fn,\n            cumulate_att_w=self.cumulate_att_w,\n            use_batch_norm=args.use_batch_norm,\n            use_concate=args.use_concate,\n            dropout_rate=args.dropout_rate,\n            zoneout_rate=args.zoneout_rate,\n            reduction_factor=args.reduction_factor,\n        )\n        self.taco2_loss = Tacotron2Loss(\n            use_masking=args.use_masking,\n            use_weighted_masking=args.use_weighted_masking,\n            bce_pos_weight=args.bce_pos_weight,\n        )\n        if self.use_guided_attn_loss:\n            self.attn_loss = GuidedAttentionLoss(\n                sigma=args.guided_attn_loss_sigma, alpha=args.guided_attn_loss_lambda,\n            )\n        if self.use_cbhg:\n            self.cbhg = CBHG(\n                idim=odim,\n                odim=args.spc_dim,\n                conv_bank_layers=args.cbhg_conv_bank_layers,\n                conv_bank_chans=args.cbhg_conv_bank_chans,\n                conv_proj_filts=args.cbhg_conv_proj_filts,\n                conv_proj_chans=args.cbhg_conv_proj_chans,\n                highway_layers=args.cbhg_highway_layers,\n                highway_units=args.cbhg_highway_units,\n                gru_units=args.cbhg_gru_units,\n            )\n            self.cbhg_loss = CBHGLoss(use_masking=args.use_masking)\n\n        # load pretrained model\n        if args.pretrained_model is not None:\n            self.load_pretrained_model(args.pretrained_model)\n\n    def forward(\n        self, xs, ilens, ys, labels, olens, spembs=None, extras=None, *args, **kwargs\n    ):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of padded character ids (B, Tmax).\n            ilens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor): Batch of padded target features (B, Lmax, odim).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n            spembs (Tensor, optional):\n                Batch of speaker embedding vectors (B, spk_embed_dim).\n            extras (Tensor, optional):\n                Batch of groundtruth spectrograms (B, Lmax, spc_dim).\n\n        Returns:\n            Tensor: Loss value.\n\n        """"""\n        # remove unnecessary padded part (for multi-gpus)\n        max_in = max(ilens)\n        max_out = max(olens)\n        if max_in != xs.shape[1]:\n            xs = xs[:, :max_in]\n        if max_out != ys.shape[1]:\n            ys = ys[:, :max_out]\n            labels = labels[:, :max_out]\n\n        # calculate tacotron2 outputs\n        hs, hlens = self.enc(xs, ilens)\n        if self.spk_embed_dim is not None:\n            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n            hs = torch.cat([hs, spembs], dim=-1)\n        after_outs, before_outs, logits, att_ws = self.dec(hs, hlens, ys)\n\n        # modifiy mod part of groundtruth\n        if self.reduction_factor > 1:\n            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])\n            max_out = max(olens)\n            ys = ys[:, :max_out]\n            labels = labels[:, :max_out]\n            labels[:, -1] = 1.0  # make sure at least one frame has 1\n\n        # caluculate taco2 loss\n        l1_loss, mse_loss, bce_loss = self.taco2_loss(\n            after_outs, before_outs, logits, ys, labels, olens\n        )\n        loss = l1_loss + mse_loss + bce_loss\n        report_keys = [\n            {""l1_loss"": l1_loss.item()},\n            {""mse_loss"": mse_loss.item()},\n            {""bce_loss"": bce_loss.item()},\n        ]\n\n        # caluculate attention loss\n        if self.use_guided_attn_loss:\n            # NOTE(kan-bayashi):\n            # length of output for auto-regressive input will be changed when r > 1\n            if self.reduction_factor > 1:\n                olens_in = olens.new([olen // self.reduction_factor for olen in olens])\n            else:\n                olens_in = olens\n            attn_loss = self.attn_loss(att_ws, ilens, olens_in)\n            loss = loss + attn_loss\n            report_keys += [\n                {""attn_loss"": attn_loss.item()},\n            ]\n\n        # caluculate cbhg loss\n        if self.use_cbhg:\n            # remove unnecessary padded part (for multi-gpus)\n            if max_out != extras.shape[1]:\n                extras = extras[:, :max_out]\n\n            # caluculate cbhg outputs & loss and report them\n            cbhg_outs, _ = self.cbhg(after_outs, olens)\n            cbhg_l1_loss, cbhg_mse_loss = self.cbhg_loss(cbhg_outs, extras, olens)\n            loss = loss + cbhg_l1_loss + cbhg_mse_loss\n            report_keys += [\n                {""cbhg_l1_loss"": cbhg_l1_loss.item()},\n                {""cbhg_mse_loss"": cbhg_mse_loss.item()},\n            ]\n\n        report_keys += [{""loss"": loss.item()}]\n        self.reporter.report(report_keys)\n\n        return loss\n\n    def inference(self, x, inference_args, spemb=None, *args, **kwargs):\n        """"""Generate the sequence of features given the sequences of characters.\n\n        Args:\n            x (Tensor): Input sequence of characters (T,).\n            inference_args (Namespace):\n                - threshold (float): Threshold in inference.\n                - minlenratio (float): Minimum length ratio in inference.\n                - maxlenratio (float): Maximum length ratio in inference.\n            spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim).\n\n        Returns:\n            Tensor: Output sequence of features (L, odim).\n            Tensor: Output sequence of stop probabilities (L,).\n            Tensor: Attention weights (L, T).\n\n        """"""\n        # get options\n        threshold = inference_args.threshold\n        minlenratio = inference_args.minlenratio\n        maxlenratio = inference_args.maxlenratio\n        use_att_constraint = getattr(\n            inference_args, ""use_att_constraint"", False\n        )  # keep compatibility\n        backward_window = inference_args.backward_window if use_att_constraint else 0\n        forward_window = inference_args.forward_window if use_att_constraint else 0\n\n        # inference\n        h = self.enc.inference(x)\n        if self.spk_embed_dim is not None:\n            spemb = F.normalize(spemb, dim=0).unsqueeze(0).expand(h.size(0), -1)\n            h = torch.cat([h, spemb], dim=-1)\n        outs, probs, att_ws = self.dec.inference(\n            h,\n            threshold,\n            minlenratio,\n            maxlenratio,\n            use_att_constraint=use_att_constraint,\n            backward_window=backward_window,\n            forward_window=forward_window,\n        )\n\n        if self.use_cbhg:\n            cbhg_outs = self.cbhg.inference(outs)\n            return cbhg_outs, probs, att_ws\n        else:\n            return outs, probs, att_ws\n\n    def calculate_all_attentions(\n        self, xs, ilens, ys, spembs=None, keep_tensor=False, *args, **kwargs\n    ):\n        """"""Calculate all of the attention weights.\n\n        Args:\n            xs (Tensor): Batch of padded character ids (B, Tmax).\n            ilens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor): Batch of padded target features (B, Lmax, odim).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n            spembs (Tensor, optional):\n                Batch of speaker embedding vectors (B, spk_embed_dim).\n            keep_tensor (bool, optional): Whether to keep original tensor.\n\n        Returns:\n            Union[ndarray, Tensor]: Batch of attention weights (B, Lmax, Tmax).\n\n        """"""\n        # check ilens type (should be list of int)\n        if isinstance(ilens, torch.Tensor) or isinstance(ilens, np.ndarray):\n            ilens = list(map(int, ilens))\n\n        self.eval()\n        with torch.no_grad():\n            hs, hlens = self.enc(xs, ilens)\n            if self.spk_embed_dim is not None:\n                spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n                hs = torch.cat([hs, spembs], dim=-1)\n            att_ws = self.dec.calculate_all_attentions(hs, hlens, ys)\n        self.train()\n\n        if keep_tensor:\n            return att_ws\n        else:\n            return att_ws.cpu().numpy()\n\n    @property\n    def base_plot_keys(self):\n        """"""Return base key names to plot during training.\n\n        keys should match what `chainer.reporter` reports.\n        If you add the key `loss`, the reporter will report `main/loss`\n        and `validation/main/loss` values.\n        also `loss.png` will be created as a figure visulizing `main/loss`\n        and `validation/main/loss` values.\n\n        Returns:\n            list: List of strings which are base keys to plot during training.\n\n        """"""\n        plot_keys = [""loss"", ""l1_loss"", ""mse_loss"", ""bce_loss""]\n        if self.use_guided_attn_loss:\n            plot_keys += [""attn_loss""]\n        if self.use_cbhg:\n            plot_keys += [""cbhg_l1_loss"", ""cbhg_mse_loss""]\n        return plot_keys\n'"
espnet/nets/pytorch_backend/e2e_tts_transformer.py,33,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""TTS-Transformer related modules.""""""\n\nimport logging\n\nimport torch\nimport torch.nn.functional as F\n\nfrom espnet.nets.pytorch_backend.e2e_tts_tacotron2 import GuidedAttentionLoss\nfrom espnet.nets.pytorch_backend.e2e_tts_tacotron2 import (\n    Tacotron2Loss as TransformerLoss,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask\nfrom espnet.nets.pytorch_backend.tacotron2.decoder import Postnet\nfrom espnet.nets.pytorch_backend.tacotron2.decoder import Prenet as DecoderPrenet\nfrom espnet.nets.pytorch_backend.tacotron2.encoder import Encoder as EncoderPrenet\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.decoder import Decoder\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.embedding import ScaledPositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.initializer import initialize\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.pytorch_backend.transformer.plot import _plot_and_save_attention\nfrom espnet.nets.pytorch_backend.transformer.plot import PlotAttentionReport\nfrom espnet.nets.tts_interface import TTSInterface\nfrom espnet.utils.cli_utils import strtobool\nfrom espnet.utils.fill_missing_args import fill_missing_args\n\n\nclass GuidedMultiHeadAttentionLoss(GuidedAttentionLoss):\n    """"""Guided attention loss function module for multi head attention.\n\n    Args:\n        sigma (float, optional): Standard deviation to control\n        how close attention to a diagonal.\n        alpha (float, optional): Scaling coefficient (lambda).\n        reset_always (bool, optional): Whether to always reset masks.\n\n    """"""\n\n    def forward(self, att_ws, ilens, olens):\n        """"""Calculate forward propagation.\n\n        Args:\n            att_ws (Tensor):\n                Batch of multi head attention weights (B, H, T_max_out, T_max_in).\n            ilens (LongTensor): Batch of input lenghts (B,).\n            olens (LongTensor): Batch of output lenghts (B,).\n\n        Returns:\n            Tensor: Guided attention loss value.\n\n        """"""\n        if self.guided_attn_masks is None:\n            self.guided_attn_masks = (\n                self._make_guided_attention_masks(ilens, olens)\n                .to(att_ws.device)\n                .unsqueeze(1)\n            )\n        if self.masks is None:\n            self.masks = self._make_masks(ilens, olens).to(att_ws.device).unsqueeze(1)\n        losses = self.guided_attn_masks * att_ws\n        loss = torch.mean(losses.masked_select(self.masks))\n        if self.reset_always:\n            self._reset_masks()\n\n        return self.alpha * loss\n\n\nclass TTSPlot(PlotAttentionReport):\n    """"""Attention plot module for TTS-Transformer.""""""\n\n    def plotfn(self, data, attn_dict, outdir, suffix=""png"", savefn=None):\n        """"""Plot multi head attentions.\n\n        Args:\n            data (dict): Utts info from json file.\n            attn_dict (dict): Multi head attention dict.\n                Values should be numpy.ndarray (H, L, T)\n            outdir (str): Directory name to save figures.\n            suffix (str): Filename suffix including image type (e.g., png).\n            savefn (function): Function to save figures.\n\n        """"""\n        import matplotlib.pyplot as plt\n\n        for name, att_ws in attn_dict.items():\n            for idx, att_w in enumerate(att_ws):\n                filename = ""%s/%s.%s.%s"" % (outdir, data[idx][0], name, suffix)\n                if ""fbank"" in name:\n                    fig = plt.Figure()\n                    ax = fig.subplots(1, 1)\n                    ax.imshow(att_w, aspect=""auto"")\n                    ax.set_xlabel(""frames"")\n                    ax.set_ylabel(""fbank coeff"")\n                    fig.tight_layout()\n                else:\n                    fig = _plot_and_save_attention(att_w, filename)\n                savefn(fig, filename)\n\n\nclass Transformer(TTSInterface, torch.nn.Module):\n    """"""Text-to-Speech Transformer module.\n\n    This is a module of text-to-speech Transformer described\n    in `Neural Speech Synthesis with Transformer Network`_,\n    which convert the sequence of characters\n    or phonemes into the sequence of Mel-filterbanks.\n\n    .. _`Neural Speech Synthesis with Transformer Network`:\n        https://arxiv.org/pdf/1809.08895.pdf\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add model-specific arguments to the parser.""""""\n        group = parser.add_argument_group(""transformer model setting"")\n        # network structure related\n        group.add_argument(\n            ""--embed-dim"",\n            default=512,\n            type=int,\n            help=""Dimension of character embedding in encoder prenet"",\n        )\n        group.add_argument(\n            ""--eprenet-conv-layers"",\n            default=3,\n            type=int,\n            help=""Number of encoder prenet convolution layers"",\n        )\n        group.add_argument(\n            ""--eprenet-conv-chans"",\n            default=256,\n            type=int,\n            help=""Number of encoder prenet convolution channels"",\n        )\n        group.add_argument(\n            ""--eprenet-conv-filts"",\n            default=5,\n            type=int,\n            help=""Filter size of encoder prenet convolution"",\n        )\n        group.add_argument(\n            ""--dprenet-layers"",\n            default=2,\n            type=int,\n            help=""Number of decoder prenet layers"",\n        )\n        group.add_argument(\n            ""--dprenet-units"",\n            default=256,\n            type=int,\n            help=""Number of decoder prenet hidden units"",\n        )\n        group.add_argument(\n            ""--elayers"", default=3, type=int, help=""Number of encoder layers""\n        )\n        group.add_argument(\n            ""--eunits"", default=1536, type=int, help=""Number of encoder hidden units""\n        )\n        group.add_argument(\n            ""--adim"",\n            default=384,\n            type=int,\n            help=""Number of attention transformation dimensions"",\n        )\n        group.add_argument(\n            ""--aheads"",\n            default=4,\n            type=int,\n            help=""Number of heads for multi head attention"",\n        )\n        group.add_argument(\n            ""--dlayers"", default=3, type=int, help=""Number of decoder layers""\n        )\n        group.add_argument(\n            ""--dunits"", default=1536, type=int, help=""Number of decoder hidden units""\n        )\n        group.add_argument(\n            ""--positionwise-layer-type"",\n            default=""linear"",\n            type=str,\n            choices=[""linear"", ""conv1d"", ""conv1d-linear""],\n            help=""Positionwise layer type."",\n        )\n        group.add_argument(\n            ""--positionwise-conv-kernel-size"",\n            default=1,\n            type=int,\n            help=""Kernel size of positionwise conv1d layer"",\n        )\n        group.add_argument(\n            ""--postnet-layers"", default=5, type=int, help=""Number of postnet layers""\n        )\n        group.add_argument(\n            ""--postnet-chans"", default=256, type=int, help=""Number of postnet channels""\n        )\n        group.add_argument(\n            ""--postnet-filts"", default=5, type=int, help=""Filter size of postnet""\n        )\n        group.add_argument(\n            ""--use-scaled-pos-enc"",\n            default=True,\n            type=strtobool,\n            help=""Use trainable scaled positional encoding ""\n            ""instead of the fixed scale one."",\n        )\n        group.add_argument(\n            ""--use-batch-norm"",\n            default=True,\n            type=strtobool,\n            help=""Whether to use batch normalization"",\n        )\n        group.add_argument(\n            ""--encoder-normalize-before"",\n            default=False,\n            type=strtobool,\n            help=""Whether to apply layer norm before encoder block"",\n        )\n        group.add_argument(\n            ""--decoder-normalize-before"",\n            default=False,\n            type=strtobool,\n            help=""Whether to apply layer norm before decoder block"",\n        )\n        group.add_argument(\n            ""--encoder-concat-after"",\n            default=False,\n            type=strtobool,\n            help=""Whether to concatenate attention layer\'s input and output in encoder"",\n        )\n        group.add_argument(\n            ""--decoder-concat-after"",\n            default=False,\n            type=strtobool,\n            help=""Whether to concatenate attention layer\'s input and output in decoder"",\n        )\n        group.add_argument(\n            ""--reduction-factor"", default=1, type=int, help=""Reduction factor""\n        )\n        group.add_argument(\n            ""--spk-embed-dim"",\n            default=None,\n            type=int,\n            help=""Number of speaker embedding dimensions"",\n        )\n        group.add_argument(\n            ""--spk-embed-integration-type"",\n            type=str,\n            default=""add"",\n            choices=[""add"", ""concat""],\n            help=""How to integrate speaker embedding"",\n        )\n        # training related\n        group.add_argument(\n            ""--transformer-init"",\n            type=str,\n            default=""pytorch"",\n            choices=[\n                ""pytorch"",\n                ""xavier_uniform"",\n                ""xavier_normal"",\n                ""kaiming_uniform"",\n                ""kaiming_normal"",\n            ],\n            help=""How to initialize transformer parameters"",\n        )\n        group.add_argument(\n            ""--initial-encoder-alpha"",\n            type=float,\n            default=1.0,\n            help=""Initial alpha value in encoder\'s ScaledPositionalEncoding"",\n        )\n        group.add_argument(\n            ""--initial-decoder-alpha"",\n            type=float,\n            default=1.0,\n            help=""Initial alpha value in decoder\'s ScaledPositionalEncoding"",\n        )\n        group.add_argument(\n            ""--transformer-lr"",\n            default=1.0,\n            type=float,\n            help=""Initial value of learning rate"",\n        )\n        group.add_argument(\n            ""--transformer-warmup-steps"",\n            default=4000,\n            type=int,\n            help=""Optimizer warmup steps"",\n        )\n        group.add_argument(\n            ""--transformer-enc-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder except for attention"",\n        )\n        group.add_argument(\n            ""--transformer-enc-positional-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder positional encoding"",\n        )\n        group.add_argument(\n            ""--transformer-enc-attn-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder self-attention"",\n        )\n        group.add_argument(\n            ""--transformer-dec-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer decoder ""\n            ""except for attention and pos encoding"",\n        )\n        group.add_argument(\n            ""--transformer-dec-positional-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer decoder positional encoding"",\n        )\n        group.add_argument(\n            ""--transformer-dec-attn-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer decoder self-attention"",\n        )\n        group.add_argument(\n            ""--transformer-enc-dec-attn-dropout-rate"",\n            default=0.1,\n            type=float,\n            help=""Dropout rate for transformer encoder-decoder attention"",\n        )\n        group.add_argument(\n            ""--eprenet-dropout-rate"",\n            default=0.5,\n            type=float,\n            help=""Dropout rate in encoder prenet"",\n        )\n        group.add_argument(\n            ""--dprenet-dropout-rate"",\n            default=0.5,\n            type=float,\n            help=""Dropout rate in decoder prenet"",\n        )\n        group.add_argument(\n            ""--postnet-dropout-rate"",\n            default=0.5,\n            type=float,\n            help=""Dropout rate in postnet"",\n        )\n        group.add_argument(\n            ""--pretrained-model"", default=None, type=str, help=""Pretrained model path""\n        )\n        # loss related\n        group.add_argument(\n            ""--use-masking"",\n            default=True,\n            type=strtobool,\n            help=""Whether to use masking in calculation of loss"",\n        )\n        group.add_argument(\n            ""--use-weighted-masking"",\n            default=False,\n            type=strtobool,\n            help=""Whether to use weighted masking in calculation of loss"",\n        )\n        group.add_argument(\n            ""--loss-type"",\n            default=""L1"",\n            choices=[""L1"", ""L2"", ""L1+L2""],\n            help=""How to calc loss"",\n        )\n        group.add_argument(\n            ""--bce-pos-weight"",\n            default=5.0,\n            type=float,\n            help=""Positive sample weight in BCE calculation ""\n            ""(only for use-masking=True)"",\n        )\n        group.add_argument(\n            ""--use-guided-attn-loss"",\n            default=False,\n            type=strtobool,\n            help=""Whether to use guided attention loss"",\n        )\n        group.add_argument(\n            ""--guided-attn-loss-sigma"",\n            default=0.4,\n            type=float,\n            help=""Sigma in guided attention loss"",\n        )\n        group.add_argument(\n            ""--guided-attn-loss-lambda"",\n            default=1.0,\n            type=float,\n            help=""Lambda in guided attention loss"",\n        )\n        group.add_argument(\n            ""--num-heads-applied-guided-attn"",\n            default=2,\n            type=int,\n            help=""Number of heads in each layer to be applied guided attention loss""\n            ""if set -1, all of the heads will be applied."",\n        )\n        group.add_argument(\n            ""--num-layers-applied-guided-attn"",\n            default=2,\n            type=int,\n            help=""Number of layers to be applied guided attention loss""\n            ""if set -1, all of the layers will be applied."",\n        )\n        group.add_argument(\n            ""--modules-applied-guided-attn"",\n            type=str,\n            nargs=""+"",\n            default=[""encoder-decoder""],\n            help=""Module name list to be applied guided attention loss"",\n        )\n        return parser\n\n    @property\n    def attention_plot_class(self):\n        """"""Return plot class for attention weight plot.""""""\n        return TTSPlot\n\n    def __init__(self, idim, odim, args=None):\n        """"""Initialize TTS-Transformer module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n            odim (int): Dimension of the outputs.\n            args (Namespace, optional):\n                - embed_dim (int): Dimension of character embedding.\n                - eprenet_conv_layers (int):\n                    Number of encoder prenet convolution layers.\n                - eprenet_conv_chans (int):\n                    Number of encoder prenet convolution channels.\n                - eprenet_conv_filts (int): Filter size of encoder prenet convolution.\n                - dprenet_layers (int): Number of decoder prenet layers.\n                - dprenet_units (int): Number of decoder prenet hidden units.\n                - elayers (int): Number of encoder layers.\n                - eunits (int): Number of encoder hidden units.\n                - adim (int): Number of attention transformation dimensions.\n                - aheads (int): Number of heads for multi head attention.\n                - dlayers (int): Number of decoder layers.\n                - dunits (int): Number of decoder hidden units.\n                - postnet_layers (int): Number of postnet layers.\n                - postnet_chans (int): Number of postnet channels.\n                - postnet_filts (int): Filter size of postnet.\n                - use_scaled_pos_enc (bool):\n                    Whether to use trainable scaled positional encoding.\n                - use_batch_norm (bool):\n                    Whether to use batch normalization in encoder prenet.\n                - encoder_normalize_before (bool):\n                    Whether to perform layer normalization before encoder block.\n                - decoder_normalize_before (bool):\n                    Whether to perform layer normalization before decoder block.\n                - encoder_concat_after (bool): Whether to concatenate attention\n                    layer\'s input and output in encoder.\n                - decoder_concat_after (bool): Whether to concatenate attention\n                    layer\'s input and output in decoder.\n                - reduction_factor (int): Reduction factor.\n                - spk_embed_dim (int): Number of speaker embedding dimenstions.\n                - spk_embed_integration_type: How to integrate speaker embedding.\n                - transformer_init (float): How to initialize transformer parameters.\n                - transformer_lr (float): Initial value of learning rate.\n                - transformer_warmup_steps (int): Optimizer warmup steps.\n                - transformer_enc_dropout_rate (float):\n                    Dropout rate in encoder except attention & positional encoding.\n                - transformer_enc_positional_dropout_rate (float):\n                    Dropout rate after encoder positional encoding.\n                - transformer_enc_attn_dropout_rate (float):\n                    Dropout rate in encoder self-attention module.\n                - transformer_dec_dropout_rate (float):\n                    Dropout rate in decoder except attention & positional encoding.\n                - transformer_dec_positional_dropout_rate (float):\n                    Dropout rate after decoder positional encoding.\n                - transformer_dec_attn_dropout_rate (float):\n                    Dropout rate in deocoder self-attention module.\n                - transformer_enc_dec_attn_dropout_rate (float):\n                    Dropout rate in encoder-deocoder attention module.\n                - eprenet_dropout_rate (float): Dropout rate in encoder prenet.\n                - dprenet_dropout_rate (float): Dropout rate in decoder prenet.\n                - postnet_dropout_rate (float): Dropout rate in postnet.\n                - use_masking (bool):\n                    Whether to apply masking for padded part in loss calculation.\n                - use_weighted_masking (bool):\n                    Whether to apply weighted masking in loss calculation.\n                - bce_pos_weight (float): Positive sample weight in bce calculation\n                    (only for use_masking=true).\n                - loss_type (str): How to calculate loss.\n                - use_guided_attn_loss (bool): Whether to use guided attention loss.\n                - num_heads_applied_guided_attn (int):\n                    Number of heads in each layer to apply guided attention loss.\n                - num_layers_applied_guided_attn (int):\n                    Number of layers to apply guided attention loss.\n                - modules_applied_guided_attn (list):\n                    List of module names to apply guided attention loss.\n                - guided-attn-loss-sigma (float) Sigma in guided attention loss.\n                - guided-attn-loss-lambda (float): Lambda in guided attention loss.\n\n        """"""\n        # initialize base classes\n        TTSInterface.__init__(self)\n        torch.nn.Module.__init__(self)\n\n        # fill missing arguments\n        args = fill_missing_args(args, self.add_arguments)\n\n        # store hyperparameters\n        self.idim = idim\n        self.odim = odim\n        self.spk_embed_dim = args.spk_embed_dim\n        if self.spk_embed_dim is not None:\n            self.spk_embed_integration_type = args.spk_embed_integration_type\n        self.use_scaled_pos_enc = args.use_scaled_pos_enc\n        self.reduction_factor = args.reduction_factor\n        self.loss_type = args.loss_type\n        self.use_guided_attn_loss = args.use_guided_attn_loss\n        if self.use_guided_attn_loss:\n            if args.num_layers_applied_guided_attn == -1:\n                self.num_layers_applied_guided_attn = args.elayers\n            else:\n                self.num_layers_applied_guided_attn = (\n                    args.num_layers_applied_guided_attn\n                )\n            if args.num_heads_applied_guided_attn == -1:\n                self.num_heads_applied_guided_attn = args.aheads\n            else:\n                self.num_heads_applied_guided_attn = args.num_heads_applied_guided_attn\n            self.modules_applied_guided_attn = args.modules_applied_guided_attn\n\n        # use idx 0 as padding idx\n        padding_idx = 0\n\n        # get positional encoding class\n        pos_enc_class = (\n            ScaledPositionalEncoding if self.use_scaled_pos_enc else PositionalEncoding\n        )\n\n        # define transformer encoder\n        if args.eprenet_conv_layers != 0:\n            # encoder prenet\n            encoder_input_layer = torch.nn.Sequential(\n                EncoderPrenet(\n                    idim=idim,\n                    embed_dim=args.embed_dim,\n                    elayers=0,\n                    econv_layers=args.eprenet_conv_layers,\n                    econv_chans=args.eprenet_conv_chans,\n                    econv_filts=args.eprenet_conv_filts,\n                    use_batch_norm=args.use_batch_norm,\n                    dropout_rate=args.eprenet_dropout_rate,\n                    padding_idx=padding_idx,\n                ),\n                torch.nn.Linear(args.eprenet_conv_chans, args.adim),\n            )\n        else:\n            encoder_input_layer = torch.nn.Embedding(\n                num_embeddings=idim, embedding_dim=args.adim, padding_idx=padding_idx\n            )\n        self.encoder = Encoder(\n            idim=idim,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.eunits,\n            num_blocks=args.elayers,\n            input_layer=encoder_input_layer,\n            dropout_rate=args.transformer_enc_dropout_rate,\n            positional_dropout_rate=args.transformer_enc_positional_dropout_rate,\n            attention_dropout_rate=args.transformer_enc_attn_dropout_rate,\n            pos_enc_class=pos_enc_class,\n            normalize_before=args.encoder_normalize_before,\n            concat_after=args.encoder_concat_after,\n            positionwise_layer_type=args.positionwise_layer_type,\n            positionwise_conv_kernel_size=args.positionwise_conv_kernel_size,\n        )\n\n        # define projection layer\n        if self.spk_embed_dim is not None:\n            if self.spk_embed_integration_type == ""add"":\n                self.projection = torch.nn.Linear(self.spk_embed_dim, args.adim)\n            else:\n                self.projection = torch.nn.Linear(\n                    args.adim + self.spk_embed_dim, args.adim\n                )\n\n        # define transformer decoder\n        if args.dprenet_layers != 0:\n            # decoder prenet\n            decoder_input_layer = torch.nn.Sequential(\n                DecoderPrenet(\n                    idim=odim,\n                    n_layers=args.dprenet_layers,\n                    n_units=args.dprenet_units,\n                    dropout_rate=args.dprenet_dropout_rate,\n                ),\n                torch.nn.Linear(args.dprenet_units, args.adim),\n            )\n        else:\n            decoder_input_layer = ""linear""\n        self.decoder = Decoder(\n            odim=-1,\n            attention_dim=args.adim,\n            attention_heads=args.aheads,\n            linear_units=args.dunits,\n            num_blocks=args.dlayers,\n            dropout_rate=args.transformer_dec_dropout_rate,\n            positional_dropout_rate=args.transformer_dec_positional_dropout_rate,\n            self_attention_dropout_rate=args.transformer_dec_attn_dropout_rate,\n            src_attention_dropout_rate=args.transformer_enc_dec_attn_dropout_rate,\n            input_layer=decoder_input_layer,\n            use_output_layer=False,\n            pos_enc_class=pos_enc_class,\n            normalize_before=args.decoder_normalize_before,\n            concat_after=args.decoder_concat_after,\n        )\n\n        # define final projection\n        self.feat_out = torch.nn.Linear(args.adim, odim * args.reduction_factor)\n        self.prob_out = torch.nn.Linear(args.adim, args.reduction_factor)\n\n        # define postnet\n        self.postnet = (\n            None\n            if args.postnet_layers == 0\n            else Postnet(\n                idim=idim,\n                odim=odim,\n                n_layers=args.postnet_layers,\n                n_chans=args.postnet_chans,\n                n_filts=args.postnet_filts,\n                use_batch_norm=args.use_batch_norm,\n                dropout_rate=args.postnet_dropout_rate,\n            )\n        )\n\n        # define loss function\n        self.criterion = TransformerLoss(\n            use_masking=args.use_masking,\n            use_weighted_masking=args.use_weighted_masking,\n            bce_pos_weight=args.bce_pos_weight,\n        )\n        if self.use_guided_attn_loss:\n            self.attn_criterion = GuidedMultiHeadAttentionLoss(\n                sigma=args.guided_attn_loss_sigma, alpha=args.guided_attn_loss_lambda,\n            )\n\n        # initialize parameters\n        self._reset_parameters(\n            init_type=args.transformer_init,\n            init_enc_alpha=args.initial_encoder_alpha,\n            init_dec_alpha=args.initial_decoder_alpha,\n        )\n\n        # load pretrained model\n        if args.pretrained_model is not None:\n            self.load_pretrained_model(args.pretrained_model)\n\n    def _reset_parameters(self, init_type, init_enc_alpha=1.0, init_dec_alpha=1.0):\n        # initialize parameters\n        initialize(self, init_type)\n\n        # initialize alpha in scaled positional encoding\n        if self.use_scaled_pos_enc:\n            self.encoder.embed[-1].alpha.data = torch.tensor(init_enc_alpha)\n            self.decoder.embed[-1].alpha.data = torch.tensor(init_dec_alpha)\n\n    def _add_first_frame_and_remove_last_frame(self, ys):\n        ys_in = torch.cat(\n            [ys.new_zeros((ys.shape[0], 1, ys.shape[2])), ys[:, :-1]], dim=1\n        )\n        return ys_in\n\n    def forward(self, xs, ilens, ys, labels, olens, spembs=None, *args, **kwargs):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of padded character ids (B, Tmax).\n            ilens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor): Batch of padded target features (B, Lmax, odim).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n            spembs (Tensor, optional):\n                Batch of speaker embedding vectors (B, spk_embed_dim).\n\n        Returns:\n            Tensor: Loss value.\n\n        """"""\n        # remove unnecessary padded part (for multi-gpus)\n        max_ilen = max(ilens)\n        max_olen = max(olens)\n        if max_ilen != xs.shape[1]:\n            xs = xs[:, :max_ilen]\n        if max_olen != ys.shape[1]:\n            ys = ys[:, :max_olen]\n            labels = labels[:, :max_olen]\n\n        # forward encoder\n        x_masks = self._source_mask(ilens)\n        hs, h_masks = self.encoder(xs, x_masks)\n\n        # integrate speaker embedding\n        if self.spk_embed_dim is not None:\n            hs = self._integrate_with_spk_embed(hs, spembs)\n\n        # thin out frames for reduction factor (B, Lmax, odim) ->  (B, Lmax//r, odim)\n        if self.reduction_factor > 1:\n            ys_in = ys[:, self.reduction_factor - 1 :: self.reduction_factor]\n            olens_in = olens.new([olen // self.reduction_factor for olen in olens])\n        else:\n            ys_in, olens_in = ys, olens\n\n        # add first zero frame and remove last frame for auto-regressive\n        ys_in = self._add_first_frame_and_remove_last_frame(ys_in)\n\n        # forward decoder\n        y_masks = self._target_mask(olens_in)\n        zs, _ = self.decoder(ys_in, y_masks, hs, h_masks)\n        # (B, Lmax//r, odim * r) -> (B, Lmax//r * r, odim)\n        before_outs = self.feat_out(zs).view(zs.size(0), -1, self.odim)\n        # (B, Lmax//r, r) -> (B, Lmax//r * r)\n        logits = self.prob_out(zs).view(zs.size(0), -1)\n\n        # postnet -> (B, Lmax//r * r, odim)\n        if self.postnet is None:\n            after_outs = before_outs\n        else:\n            after_outs = before_outs + self.postnet(\n                before_outs.transpose(1, 2)\n            ).transpose(1, 2)\n\n        # modifiy mod part of groundtruth\n        if self.reduction_factor > 1:\n            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])\n            max_olen = max(olens)\n            ys = ys[:, :max_olen]\n            labels = labels[:, :max_olen]\n            labels[:, -1] = 1.0  # make sure at least one frame has 1\n\n        # caluculate loss values\n        l1_loss, l2_loss, bce_loss = self.criterion(\n            after_outs, before_outs, logits, ys, labels, olens\n        )\n        if self.loss_type == ""L1"":\n            loss = l1_loss + bce_loss\n        elif self.loss_type == ""L2"":\n            loss = l2_loss + bce_loss\n        elif self.loss_type == ""L1+L2"":\n            loss = l1_loss + l2_loss + bce_loss\n        else:\n            raise ValueError(""unknown --loss-type "" + self.loss_type)\n        report_keys = [\n            {""l1_loss"": l1_loss.item()},\n            {""l2_loss"": l2_loss.item()},\n            {""bce_loss"": bce_loss.item()},\n            {""loss"": loss.item()},\n        ]\n\n        # calculate guided attention loss\n        if self.use_guided_attn_loss:\n            # calculate for encoder\n            if ""encoder"" in self.modules_applied_guided_attn:\n                att_ws = []\n                for idx, layer_idx in enumerate(\n                    reversed(range(len(self.encoder.encoders)))\n                ):\n                    att_ws += [\n                        self.encoder.encoders[layer_idx].self_attn.attn[\n                            :, : self.num_heads_applied_guided_attn\n                        ]\n                    ]\n                    if idx + 1 == self.num_layers_applied_guided_attn:\n                        break\n                att_ws = torch.cat(att_ws, dim=1)  # (B, H*L, T_in, T_in)\n                enc_attn_loss = self.attn_criterion(att_ws, ilens, ilens)\n                loss = loss + enc_attn_loss\n                report_keys += [{""enc_attn_loss"": enc_attn_loss.item()}]\n            # calculate for decoder\n            if ""decoder"" in self.modules_applied_guided_attn:\n                att_ws = []\n                for idx, layer_idx in enumerate(\n                    reversed(range(len(self.decoder.decoders)))\n                ):\n                    att_ws += [\n                        self.decoder.decoders[layer_idx].self_attn.attn[\n                            :, : self.num_heads_applied_guided_attn\n                        ]\n                    ]\n                    if idx + 1 == self.num_layers_applied_guided_attn:\n                        break\n                att_ws = torch.cat(att_ws, dim=1)  # (B, H*L, T_out, T_out)\n                dec_attn_loss = self.attn_criterion(att_ws, olens_in, olens_in)\n                loss = loss + dec_attn_loss\n                report_keys += [{""dec_attn_loss"": dec_attn_loss.item()}]\n            # calculate for encoder-decoder\n            if ""encoder-decoder"" in self.modules_applied_guided_attn:\n                att_ws = []\n                for idx, layer_idx in enumerate(\n                    reversed(range(len(self.decoder.decoders)))\n                ):\n                    att_ws += [\n                        self.decoder.decoders[layer_idx].src_attn.attn[\n                            :, : self.num_heads_applied_guided_attn\n                        ]\n                    ]\n                    if idx + 1 == self.num_layers_applied_guided_attn:\n                        break\n                att_ws = torch.cat(att_ws, dim=1)  # (B, H*L, T_out, T_in)\n                enc_dec_attn_loss = self.attn_criterion(att_ws, ilens, olens_in)\n                loss = loss + enc_dec_attn_loss\n                report_keys += [{""enc_dec_attn_loss"": enc_dec_attn_loss.item()}]\n\n        # report extra information\n        if self.use_scaled_pos_enc:\n            report_keys += [\n                {""encoder_alpha"": self.encoder.embed[-1].alpha.data.item()},\n                {""decoder_alpha"": self.decoder.embed[-1].alpha.data.item()},\n            ]\n        self.reporter.report(report_keys)\n\n        return loss\n\n    def inference(self, x, inference_args, spemb=None, *args, **kwargs):\n        """"""Generate the sequence of features given the sequences of characters.\n\n        Args:\n            x (Tensor): Input sequence of characters (T,).\n            inference_args (Namespace):\n                - threshold (float): Threshold in inference.\n                - minlenratio (float): Minimum length ratio in inference.\n                - maxlenratio (float): Maximum length ratio in inference.\n            spemb (Tensor, optional): Speaker embedding vector (spk_embed_dim).\n\n        Returns:\n            Tensor: Output sequence of features (L, odim).\n            Tensor: Output sequence of stop probabilities (L,).\n            Tensor: Encoder-decoder (source) attention weights (#layers, #heads, L, T).\n\n        """"""\n        # get options\n        threshold = inference_args.threshold\n        minlenratio = inference_args.minlenratio\n        maxlenratio = inference_args.maxlenratio\n        use_att_constraint = getattr(\n            inference_args, ""use_att_constraint"", False\n        )  # keep compatibility\n        if use_att_constraint:\n            logging.warning(\n                ""Attention constraint is not yet supported in Transformer. Not enabled.""\n            )\n\n        # forward encoder\n        xs = x.unsqueeze(0)\n        hs, _ = self.encoder(xs, None)\n\n        # integrate speaker embedding\n        if self.spk_embed_dim is not None:\n            spembs = spemb.unsqueeze(0)\n            hs = self._integrate_with_spk_embed(hs, spembs)\n\n        # set limits of length\n        maxlen = int(hs.size(1) * maxlenratio / self.reduction_factor)\n        minlen = int(hs.size(1) * minlenratio / self.reduction_factor)\n\n        # initialize\n        idx = 0\n        ys = hs.new_zeros(1, 1, self.odim)\n        outs, probs = [], []\n\n        # forward decoder step-by-step\n        z_cache = self.decoder.init_state(x)\n        while True:\n            # update index\n            idx += 1\n\n            # calculate output and stop prob at idx-th step\n            y_masks = subsequent_mask(idx).unsqueeze(0).to(x.device)\n            z, z_cache = self.decoder.forward_one_step(\n                ys, y_masks, hs, cache=z_cache\n            )  # (B, adim)\n            outs += [\n                self.feat_out(z).view(self.reduction_factor, self.odim)\n            ]  # [(r, odim), ...]\n            probs += [torch.sigmoid(self.prob_out(z))[0]]  # [(r), ...]\n\n            # update next inputs\n            ys = torch.cat(\n                (ys, outs[-1][-1].view(1, 1, self.odim)), dim=1\n            )  # (1, idx + 1, odim)\n\n            # get attention weights\n            att_ws_ = []\n            for name, m in self.named_modules():\n                if isinstance(m, MultiHeadedAttention) and ""src"" in name:\n                    att_ws_ += [m.attn[0, :, -1].unsqueeze(1)]  # [(#heads, 1, T),...]\n            if idx == 1:\n                att_ws = att_ws_\n            else:\n                # [(#heads, l, T), ...]\n                att_ws = [\n                    torch.cat([att_w, att_w_], dim=1)\n                    for att_w, att_w_ in zip(att_ws, att_ws_)\n                ]\n\n            # check whether to finish generation\n            if int(sum(probs[-1] >= threshold)) > 0 or idx >= maxlen:\n                # check mininum length\n                if idx < minlen:\n                    continue\n                outs = (\n                    torch.cat(outs, dim=0).unsqueeze(0).transpose(1, 2)\n                )  # (L, odim) -> (1, L, odim) -> (1, odim, L)\n                if self.postnet is not None:\n                    outs = outs + self.postnet(outs)  # (1, odim, L)\n                outs = outs.transpose(2, 1).squeeze(0)  # (L, odim)\n                probs = torch.cat(probs, dim=0)\n                break\n\n        # concatenate attention weights -> (#layers, #heads, L, T)\n        att_ws = torch.stack(att_ws, dim=0)\n\n        return outs, probs, att_ws\n\n    def calculate_all_attentions(\n        self,\n        xs,\n        ilens,\n        ys,\n        olens,\n        spembs=None,\n        skip_output=False,\n        keep_tensor=False,\n        *args,\n        **kwargs\n    ):\n        """"""Calculate all of the attention weights.\n\n        Args:\n            xs (Tensor): Batch of padded character ids (B, Tmax).\n            ilens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor): Batch of padded target features (B, Lmax, odim).\n            olens (LongTensor): Batch of the lengths of each target (B,).\n            spembs (Tensor, optional):\n                Batch of speaker embedding vectors (B, spk_embed_dim).\n            skip_output (bool, optional): Whether to skip calculate the final output.\n            keep_tensor (bool, optional): Whether to keep original tensor.\n\n        Returns:\n            dict: Dict of attention weights and outputs.\n\n        """"""\n        with torch.no_grad():\n            # forward encoder\n            x_masks = self._source_mask(ilens)\n            hs, h_masks = self.encoder(xs, x_masks)\n\n            # integrate speaker embedding\n            if self.spk_embed_dim is not None:\n                hs = self._integrate_with_spk_embed(hs, spembs)\n\n            # thin out frames for reduction factor\n            # (B, Lmax, odim) ->  (B, Lmax//r, odim)\n            if self.reduction_factor > 1:\n                ys_in = ys[:, self.reduction_factor - 1 :: self.reduction_factor]\n                olens_in = olens.new([olen // self.reduction_factor for olen in olens])\n            else:\n                ys_in, olens_in = ys, olens\n\n            # add first zero frame and remove last frame for auto-regressive\n            ys_in = self._add_first_frame_and_remove_last_frame(ys_in)\n\n            # forward decoder\n            y_masks = self._target_mask(olens_in)\n            zs, _ = self.decoder(ys_in, y_masks, hs, h_masks)\n\n            # calculate final outputs\n            if not skip_output:\n                before_outs = self.feat_out(zs).view(zs.size(0), -1, self.odim)\n                if self.postnet is None:\n                    after_outs = before_outs\n                else:\n                    after_outs = before_outs + self.postnet(\n                        before_outs.transpose(1, 2)\n                    ).transpose(1, 2)\n\n        # modifiy mod part of output lengths due to reduction factor > 1\n        if self.reduction_factor > 1:\n            olens = olens.new([olen - olen % self.reduction_factor for olen in olens])\n\n        # store into dict\n        att_ws_dict = dict()\n        if keep_tensor:\n            for name, m in self.named_modules():\n                if isinstance(m, MultiHeadedAttention):\n                    att_ws_dict[name] = m.attn\n            if not skip_output:\n                att_ws_dict[""before_postnet_fbank""] = before_outs\n                att_ws_dict[""after_postnet_fbank""] = after_outs\n        else:\n            for name, m in self.named_modules():\n                if isinstance(m, MultiHeadedAttention):\n                    attn = m.attn.cpu().numpy()\n                    if ""encoder"" in name:\n                        attn = [a[:, :l, :l] for a, l in zip(attn, ilens.tolist())]\n                    elif ""decoder"" in name:\n                        if ""src"" in name:\n                            attn = [\n                                a[:, :ol, :il]\n                                for a, il, ol in zip(\n                                    attn, ilens.tolist(), olens_in.tolist()\n                                )\n                            ]\n                        elif ""self"" in name:\n                            attn = [\n                                a[:, :l, :l] for a, l in zip(attn, olens_in.tolist())\n                            ]\n                        else:\n                            logging.warning(""unknown attention module: "" + name)\n                    else:\n                        logging.warning(""unknown attention module: "" + name)\n                    att_ws_dict[name] = attn\n            if not skip_output:\n                before_outs = before_outs.cpu().numpy()\n                after_outs = after_outs.cpu().numpy()\n                att_ws_dict[""before_postnet_fbank""] = [\n                    m[:l].T for m, l in zip(before_outs, olens.tolist())\n                ]\n                att_ws_dict[""after_postnet_fbank""] = [\n                    m[:l].T for m, l in zip(after_outs, olens.tolist())\n                ]\n\n        return att_ws_dict\n\n    def _integrate_with_spk_embed(self, hs, spembs):\n        """"""Integrate speaker embedding with hidden states.\n\n        Args:\n            hs (Tensor): Batch of hidden state sequences (B, Tmax, adim).\n            spembs (Tensor): Batch of speaker embeddings (B, spk_embed_dim).\n\n        Returns:\n            Tensor: Batch of integrated hidden state sequences (B, Tmax, adim)\n\n        """"""\n        if self.spk_embed_integration_type == ""add"":\n            # apply projection and then add to hidden states\n            spembs = self.projection(F.normalize(spembs))\n            hs = hs + spembs.unsqueeze(1)\n        elif self.spk_embed_integration_type == ""concat"":\n            # concat hidden states with spk embeds and then apply projection\n            spembs = F.normalize(spembs).unsqueeze(1).expand(-1, hs.size(1), -1)\n            hs = self.projection(torch.cat([hs, spembs], dim=-1))\n        else:\n            raise NotImplementedError(""support only add or concat."")\n\n        return hs\n\n    def _source_mask(self, ilens):\n        """"""Make masks for self-attention.\n\n        Args:\n            ilens (LongTensor or List): Batch of lengths (B,).\n\n        Returns:\n            Tensor: Mask tensor for self-attention.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n        Examples:\n            >>> ilens = [5, 3]\n            >>> self._source_mask(ilens)\n            tensor([[[1, 1, 1, 1, 1],\n                    [[1, 1, 1, 0, 0]]], dtype=torch.uint8)\n\n        """"""\n        x_masks = make_non_pad_mask(ilens).to(next(self.parameters()).device)\n        return x_masks.unsqueeze(-2)\n\n    def _target_mask(self, olens):\n        """"""Make masks for masked self-attention.\n\n        Args:\n            olens (LongTensor or List): Batch of lengths (B,).\n\n        Returns:\n            Tensor: Mask tensor for masked self-attention.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n        Examples:\n            >>> olens = [5, 3]\n            >>> self._target_mask(olens)\n            tensor([[[1, 0, 0, 0, 0],\n                     [1, 1, 0, 0, 0],\n                     [1, 1, 1, 0, 0],\n                     [1, 1, 1, 1, 0],\n                     [1, 1, 1, 1, 1]],\n                    [[1, 0, 0, 0, 0],\n                     [1, 1, 0, 0, 0],\n                     [1, 1, 1, 0, 0],\n                     [1, 1, 1, 0, 0],\n                     [1, 1, 1, 0, 0]]], dtype=torch.uint8)\n\n        """"""\n        y_masks = make_non_pad_mask(olens).to(next(self.parameters()).device)\n        s_masks = subsequent_mask(y_masks.size(-1), device=y_masks.device).unsqueeze(0)\n        return y_masks.unsqueeze(-2) & s_masks\n\n    @property\n    def base_plot_keys(self):\n        """"""Return base key names to plot during training.\n\n        keys should match what `chainer.reporter` reports.\n        If you add the key `loss`, the reporter will report `main/loss`\n        and `validation/main/loss` values.\n        also `loss.png` will be created as a figure visulizing `main/loss`\n        and `validation/main/loss` values.\n\n        Returns:\n            list: List of strings which are base keys to plot during training.\n\n        """"""\n        plot_keys = [""loss"", ""l1_loss"", ""l2_loss"", ""bce_loss""]\n        if self.use_scaled_pos_enc:\n            plot_keys += [""encoder_alpha"", ""decoder_alpha""]\n        if self.use_guided_attn_loss:\n            if ""encoder"" in self.modules_applied_guided_attn:\n                plot_keys += [""enc_attn_loss""]\n            if ""decoder"" in self.modules_applied_guided_attn:\n                plot_keys += [""dec_attn_loss""]\n            if ""encoder-decoder"" in self.modules_applied_guided_attn:\n                plot_keys += [""enc_dec_attn_loss""]\n\n        return plot_keys\n'"
espnet/nets/pytorch_backend/initialization.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Initialization functions for RNN sequence-to-sequence models.""""""\n\nimport math\n\n\ndef lecun_normal_init_parameters(module):\n    """"""Initialize parameters in the LeCun\'s manner.""""""\n    for p in module.parameters():\n        data = p.data\n        if data.dim() == 1:\n            # bias\n            data.zero_()\n        elif data.dim() == 2:\n            # linear weight\n            n = data.size(1)\n            stdv = 1.0 / math.sqrt(n)\n            data.normal_(0, stdv)\n        elif data.dim() in (3, 4):\n            # conv weight\n            n = data.size(1)\n            for k in data.size()[2:]:\n                n *= k\n            stdv = 1.0 / math.sqrt(n)\n            data.normal_(0, stdv)\n        else:\n            raise NotImplementedError\n\n\ndef uniform_init_parameters(module):\n    """"""Initialize parameters with an uniform distribution.""""""\n    for p in module.parameters():\n        data = p.data\n        if data.dim() == 1:\n            # bias\n            data.uniform_(-0.1, 0.1)\n        elif data.dim() == 2:\n            # linear weight\n            data.uniform_(-0.1, 0.1)\n        elif data.dim() in (3, 4):\n            # conv weight\n            pass  # use the pytorch default\n        else:\n            raise NotImplementedError\n\n\ndef set_forget_bias_to_one(bias):\n    """"""Initialize a bias vector in the forget gate with one.""""""\n    n = bias.size(0)\n    start, end = n // 4, n // 2\n    bias.data[start:end].fill_(1.0)\n'"
espnet/nets/pytorch_backend/nets_utils.py,33,"b'# -*- coding: utf-8 -*-\n\n""""""Network related utility tools.""""""\n\nimport logging\nfrom typing import Dict\n\nimport numpy as np\nimport torch\n\n\ndef to_device(m, x):\n    """"""Send tensor into the device of the module.\n\n    Args:\n        m (torch.nn.Module): Torch module.\n        x (Tensor): Torch tensor.\n\n    Returns:\n        Tensor: Torch tensor located in the same place as torch module.\n\n    """"""\n    assert isinstance(m, torch.nn.Module)\n    device = next(m.parameters()).device\n    return x.to(device)\n\n\ndef pad_list(xs, pad_value):\n    """"""Perform padding for the list of tensors.\n\n    Args:\n        xs (List): List of Tensors [(T_1, `*`), (T_2, `*`), ..., (T_B, `*`)].\n        pad_value (float): Value for padding.\n\n    Returns:\n        Tensor: Padded tensor (B, Tmax, `*`).\n\n    Examples:\n        >>> x = [torch.ones(4), torch.ones(2), torch.ones(1)]\n        >>> x\n        [tensor([1., 1., 1., 1.]), tensor([1., 1.]), tensor([1.])]\n        >>> pad_list(x, 0)\n        tensor([[1., 1., 1., 1.],\n                [1., 1., 0., 0.],\n                [1., 0., 0., 0.]])\n\n    """"""\n    n_batch = len(xs)\n    max_len = max(x.size(0) for x in xs)\n    pad = xs[0].new(n_batch, max_len, *xs[0].size()[1:]).fill_(pad_value)\n\n    for i in range(n_batch):\n        pad[i, : xs[i].size(0)] = xs[i]\n\n    return pad\n\n\ndef make_pad_mask(lengths, xs=None, length_dim=-1):\n    """"""Make mask tensor containing indices of padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        Tensor: Mask tensor containing indices of padded part.\n                dtype=torch.uint8 in PyTorch 1.2-\n                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[0, 0, 0, 0 ,0],\n                 [0, 0, 0, 1, 1],\n                 [0, 0, 1, 1, 1]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0],\n                 [0, 0, 0, 0]],\n                [[0, 0, 0, 1],\n                 [0, 0, 0, 1]],\n                [[0, 0, 1, 1],\n                 [0, 0, 1, 1]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_pad_mask(lengths, xs)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_pad_mask(lengths, xs, 1)\n        tensor([[[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]],\n                [[0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n        >>> make_pad_mask(lengths, xs, 2)\n        tensor([[[0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1],\n                 [0, 0, 0, 0, 0, 1]],\n                [[0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1],\n                 [0, 0, 0, 1, 1, 1]],\n                [[0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1],\n                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n\n    """"""\n    if length_dim == 0:\n        raise ValueError(""length_dim cannot be 0: {}"".format(length_dim))\n\n    if not isinstance(lengths, list):\n        lengths = lengths.tolist()\n    bs = int(len(lengths))\n    if xs is None:\n        maxlen = int(max(lengths))\n    else:\n        maxlen = xs.size(length_dim)\n\n    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n    mask = seq_range_expand >= seq_length_expand\n\n    if xs is not None:\n        assert xs.size(0) == bs, (xs.size(0), bs)\n\n        if length_dim < 0:\n            length_dim = xs.dim() + length_dim\n        # ind = (:, None, ..., None, :, , None, ..., None)\n        ind = tuple(\n            slice(None) if i in (0, length_dim) else None for i in range(xs.dim())\n        )\n        mask = mask[ind].expand_as(xs).to(xs.device)\n    return mask\n\n\ndef make_non_pad_mask(lengths, xs=None, length_dim=-1):\n    """"""Make mask tensor containing indices of non-padded part.\n\n    Args:\n        lengths (LongTensor or List): Batch of lengths (B,).\n        xs (Tensor, optional): The reference tensor.\n            If set, masks will be the same shape as this tensor.\n        length_dim (int, optional): Dimension indicator of the above tensor.\n            See the example.\n\n    Returns:\n        ByteTensor: mask tensor containing indices of padded part.\n                    dtype=torch.uint8 in PyTorch 1.2-\n                    dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n\n    Examples:\n        With only lengths.\n\n        >>> lengths = [5, 3, 2]\n        >>> make_non_pad_mask(lengths)\n        masks = [[1, 1, 1, 1 ,1],\n                 [1, 1, 1, 0, 0],\n                 [1, 1, 0, 0, 0]]\n\n        With the reference tensor.\n\n        >>> xs = torch.zeros((3, 2, 4))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1],\n                 [1, 1, 1, 1]],\n                [[1, 1, 1, 0],\n                 [1, 1, 1, 0]],\n                [[1, 1, 0, 0],\n                 [1, 1, 0, 0]]], dtype=torch.uint8)\n        >>> xs = torch.zeros((3, 2, 6))\n        >>> make_non_pad_mask(lengths, xs)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n        With the reference tensor and dimension indicator.\n\n        >>> xs = torch.zeros((3, 6, 6))\n        >>> make_non_pad_mask(lengths, xs, 1)\n        tensor([[[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]],\n                [[1, 1, 1, 1, 1, 1],\n                 [1, 1, 1, 1, 1, 1],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0],\n                 [0, 0, 0, 0, 0, 0]]], dtype=torch.uint8)\n        >>> make_non_pad_mask(lengths, xs, 2)\n        tensor([[[1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0],\n                 [1, 1, 1, 1, 1, 0]],\n                [[1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0],\n                 [1, 1, 1, 0, 0, 0]],\n                [[1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0],\n                 [1, 1, 0, 0, 0, 0]]], dtype=torch.uint8)\n\n    """"""\n    return ~make_pad_mask(lengths, xs, length_dim)\n\n\ndef mask_by_length(xs, lengths, fill=0):\n    """"""Mask tensor according to length.\n\n    Args:\n        xs (Tensor): Batch of input tensor (B, `*`).\n        lengths (LongTensor or List): Batch of lengths (B,).\n        fill (int or float): Value to fill masked part.\n\n    Returns:\n        Tensor: Batch of masked input tensor (B, `*`).\n\n    Examples:\n        >>> x = torch.arange(5).repeat(3, 1) + 1\n        >>> x\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5],\n                [1, 2, 3, 4, 5]])\n        >>> lengths = [5, 3, 2]\n        >>> mask_by_length(x, lengths)\n        tensor([[1, 2, 3, 4, 5],\n                [1, 2, 3, 0, 0],\n                [1, 2, 0, 0, 0]])\n\n    """"""\n    assert xs.size(0) == len(lengths)\n    ret = xs.data.new(*xs.size()).fill_(fill)\n    for i, l in enumerate(lengths):\n        ret[i, :l] = xs[i, :l]\n    return ret\n\n\ndef th_accuracy(pad_outputs, pad_targets, ignore_label):\n    """"""Calculate accuracy.\n\n    Args:\n        pad_outputs (Tensor): Prediction tensors (B * Lmax, D).\n        pad_targets (LongTensor): Target label tensors (B, Lmax, D).\n        ignore_label (int): Ignore label id.\n\n    Returns:\n        float: Accuracy value (0.0 - 1.0).\n\n    """"""\n    pad_pred = pad_outputs.view(\n        pad_targets.size(0), pad_targets.size(1), pad_outputs.size(1)\n    ).argmax(2)\n    mask = pad_targets != ignore_label\n    numerator = torch.sum(\n        pad_pred.masked_select(mask) == pad_targets.masked_select(mask)\n    )\n    denominator = torch.sum(mask)\n    return float(numerator) / float(denominator)\n\n\ndef to_torch_tensor(x):\n    """"""Change to torch.Tensor or ComplexTensor from numpy.ndarray.\n\n    Args:\n        x: Inputs. It should be one of numpy.ndarray, Tensor, ComplexTensor, and dict.\n\n    Returns:\n        Tensor or ComplexTensor: Type converted inputs.\n\n    Examples:\n        >>> xs = np.ones(3, dtype=np.float32)\n        >>> xs = to_torch_tensor(xs)\n        tensor([1., 1., 1.])\n        >>> xs = torch.ones(3, 4, 5)\n        >>> assert to_torch_tensor(xs) is xs\n        >>> xs = {\'real\': xs, \'imag\': xs}\n        >>> to_torch_tensor(xs)\n        ComplexTensor(\n        Real:\n        tensor([1., 1., 1.])\n        Imag;\n        tensor([1., 1., 1.])\n        )\n\n    """"""\n    # If numpy, change to torch tensor\n    if isinstance(x, np.ndarray):\n        if x.dtype.kind == ""c"":\n            # Dynamically importing because torch_complex requires python3\n            from torch_complex.tensor import ComplexTensor\n\n            return ComplexTensor(x)\n        else:\n            return torch.from_numpy(x)\n\n    # If {\'real\': ..., \'imag\': ...}, convert to ComplexTensor\n    elif isinstance(x, dict):\n        # Dynamically importing because torch_complex requires python3\n        from torch_complex.tensor import ComplexTensor\n\n        if ""real"" not in x or ""imag"" not in x:\n            raise ValueError(""has \'real\' and \'imag\' keys: {}"".format(list(x)))\n        # Relative importing because of using python3 syntax\n        return ComplexTensor(x[""real""], x[""imag""])\n\n    # If torch.Tensor, as it is\n    elif isinstance(x, torch.Tensor):\n        return x\n\n    else:\n        error = (\n            ""x must be numpy.ndarray, torch.Tensor or a dict like ""\n            ""{{\'real\': torch.Tensor, \'imag\': torch.Tensor}}, ""\n            ""but got {}"".format(type(x))\n        )\n        try:\n            from torch_complex.tensor import ComplexTensor\n        except Exception:\n            # If PY2\n            raise ValueError(error)\n        else:\n            # If PY3\n            if isinstance(x, ComplexTensor):\n                return x\n            else:\n                raise ValueError(error)\n\n\ndef get_subsample(train_args, mode, arch):\n    """"""Parse the subsampling factors from the args for the specified `mode` and `arch`.\n\n    Args:\n        train_args: argument Namespace containing options.\n        mode: one of (\'asr\', \'mt\', \'st\')\n        arch: one of (\'rnn\', \'rnn-t\', \'rnn_mix\', \'rnn_mulenc\', \'transformer\')\n\n    Returns:\n        np.ndarray / List[np.ndarray]: subsampling factors.\n    """"""\n    if arch == ""transformer"":\n        return np.array([1])\n\n    elif mode == ""mt"" and arch == ""rnn"":\n        # +1 means input (+1) and layers outputs (train_args.elayer)\n        subsample = np.ones(train_args.elayers + 1, dtype=np.int)\n        logging.warning(""Subsampling is not performed for machine translation."")\n        logging.info(""subsample: "" + "" "".join([str(x) for x in subsample]))\n        return subsample\n\n    elif (\n        (mode == ""asr"" and arch in (""rnn"", ""rnn-t""))\n        or (mode == ""mt"" and arch == ""rnn"")\n        or (mode == ""st"" and arch == ""rnn"")\n    ):\n        subsample = np.ones(train_args.elayers + 1, dtype=np.int)\n        if train_args.etype.endswith(""p"") and not train_args.etype.startswith(""vgg""):\n            ss = train_args.subsample.split(""_"")\n            for j in range(min(train_args.elayers + 1, len(ss))):\n                subsample[j] = int(ss[j])\n        else:\n            logging.warning(\n                ""Subsampling is not performed for vgg*. ""\n                ""It is performed in max pooling layers at CNN.""\n            )\n        logging.info(""subsample: "" + "" "".join([str(x) for x in subsample]))\n        return subsample\n\n    elif mode == ""asr"" and arch == ""rnn_mix"":\n        subsample = np.ones(\n            train_args.elayers_sd + train_args.elayers + 1, dtype=np.int\n        )\n        if train_args.etype.endswith(""p"") and not train_args.etype.startswith(""vgg""):\n            ss = train_args.subsample.split(""_"")\n            for j in range(\n                min(train_args.elayers_sd + train_args.elayers + 1, len(ss))\n            ):\n                subsample[j] = int(ss[j])\n        else:\n            logging.warning(\n                ""Subsampling is not performed for vgg*. ""\n                ""It is performed in max pooling layers at CNN.""\n            )\n        logging.info(""subsample: "" + "" "".join([str(x) for x in subsample]))\n        return subsample\n\n    elif mode == ""asr"" and arch == ""rnn_mulenc"":\n        subsample_list = []\n        for idx in range(train_args.num_encs):\n            subsample = np.ones(train_args.elayers[idx] + 1, dtype=np.int)\n            if train_args.etype[idx].endswith(""p"") and not train_args.etype[\n                idx\n            ].startswith(""vgg""):\n                ss = train_args.subsample[idx].split(""_"")\n                for j in range(min(train_args.elayers[idx] + 1, len(ss))):\n                    subsample[j] = int(ss[j])\n            else:\n                logging.warning(\n                    ""Encoder %d: Subsampling is not performed for vgg*. ""\n                    ""It is performed in max pooling layers at CNN."",\n                    idx + 1,\n                )\n            logging.info(""subsample: "" + "" "".join([str(x) for x in subsample]))\n            subsample_list.append(subsample)\n        return subsample_list\n\n    else:\n        raise ValueError(""Invalid options: mode={}, arch={}"".format(mode, arch))\n\n\ndef rename_state_dict(\n    old_prefix: str, new_prefix: str, state_dict: Dict[str, torch.Tensor]\n):\n    """"""Replace keys of old prefix with new prefix in state dict.""""""\n    # need this list not to break the dict iterator\n    old_keys = [k for k in state_dict if k.startswith(old_prefix)]\n    if len(old_keys) > 0:\n        logging.warning(f""Rename: {old_prefix} -> {new_prefix}"")\n    for k in old_keys:\n        v = state_dict.pop(k)\n        new_k = k.replace(old_prefix, new_prefix)\n        state_dict[new_k] = v\n'"
espnet/nets/pytorch_backend/wavenet.py,8,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi (Nagoya University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""This code is based on https://github.com/kan-bayashi/PytorchWaveNetVocoder.""""""\n\nimport logging\nimport sys\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import nn\n\n\ndef encode_mu_law(x, mu=256):\n    """"""Perform mu-law encoding.\n\n    Args:\n        x (ndarray): Audio signal with the range from -1 to 1.\n        mu (int): Quantized level.\n\n    Returns:\n        ndarray: Quantized audio signal with the range from 0 to mu - 1.\n\n    """"""\n    mu = mu - 1\n    fx = np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)\n    return np.floor((fx + 1) / 2 * mu + 0.5).astype(np.int64)\n\n\ndef decode_mu_law(y, mu=256):\n    """"""Perform mu-law decoding.\n\n    Args:\n        x (ndarray): Quantized audio signal with the range from 0 to mu - 1.\n        mu (int): Quantized level.\n\n    Returns:\n        ndarray: Audio signal with the range from -1 to 1.\n\n    """"""\n    mu = mu - 1\n    fx = (y - 0.5) / mu * 2 - 1\n    x = np.sign(fx) / mu * ((1 + mu) ** np.abs(fx) - 1)\n    return x\n\n\ndef initialize(m):\n    """"""Initilize conv layers with xavier.\n\n    Args:\n        m (torch.nn.Module): Torch module.\n\n    """"""\n    if isinstance(m, nn.Conv1d):\n        nn.init.xavier_uniform_(m.weight)\n        nn.init.constant_(m.bias, 0.0)\n\n    if isinstance(m, nn.ConvTranspose2d):\n        nn.init.constant_(m.weight, 1.0)\n        nn.init.constant_(m.bias, 0.0)\n\n\nclass OneHot(nn.Module):\n    """"""Convert to one-hot vector.\n\n    Args:\n        depth (int): Dimension of one-hot vector.\n\n    """"""\n\n    def __init__(self, depth):\n        super(OneHot, self).__init__()\n        self.depth = depth\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (LongTensor): long tensor variable with the shape  (B, T)\n\n        Returns:\n            Tensor: float tensor variable with the shape (B, depth, T)\n\n        """"""\n        x = x % self.depth\n        x = torch.unsqueeze(x, 2)\n        x_onehot = x.new_zeros(x.size(0), x.size(1), self.depth).float()\n\n        return x_onehot.scatter_(2, x, 1)\n\n\nclass CausalConv1d(nn.Module):\n    """"""1D dilated causal convolution.""""""\n\n    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, bias=True):\n        super(CausalConv1d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.padding = padding = (kernel_size - 1) * dilation\n        self.conv = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding=padding,\n            dilation=dilation,\n            bias=bias,\n        )\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor with the shape (B, in_channels, T).\n\n        Returns:\n            Tensor: Tensor with the shape (B, out_channels, T)\n\n        """"""\n        x = self.conv(x)\n        if self.padding != 0:\n            x = x[:, :, : -self.padding]\n        return x\n\n\nclass UpSampling(nn.Module):\n    """"""Upsampling layer with deconvolution.\n\n    Args:\n        upsampling_factor (int): Upsampling factor.\n\n    """"""\n\n    def __init__(self, upsampling_factor, bias=True):\n        super(UpSampling, self).__init__()\n        self.upsampling_factor = upsampling_factor\n        self.bias = bias\n        self.conv = nn.ConvTranspose2d(\n            1,\n            1,\n            kernel_size=(1, self.upsampling_factor),\n            stride=(1, self.upsampling_factor),\n            bias=self.bias,\n        )\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Input tensor with the shape  (B, C, T)\n\n        Returns:\n            Tensor: Tensor with the shape (B, C, T\') where T\' = T * upsampling_factor.\n\n        """"""\n        x = x.unsqueeze(1)  # B x 1 x C x T\n        x = self.conv(x)  # B x 1 x C x T\'\n        return x.squeeze(1)\n\n\nclass WaveNet(nn.Module):\n    """"""Conditional wavenet.\n\n    Args:\n        n_quantize (int): Number of quantization.\n        n_aux (int): Number of aux feature dimension.\n        n_resch (int): Number of filter channels for residual block.\n        n_skipch (int): Number of filter channels for skip connection.\n        dilation_depth (int): Number of dilation depth\n            (e.g. if set 10, max dilation = 2^(10-1)).\n        dilation_repeat (int): Number of dilation repeat.\n        kernel_size (int): Filter size of dilated causal convolution.\n        upsampling_factor (int): Upsampling factor.\n\n    """"""\n\n    def __init__(\n        self,\n        n_quantize=256,\n        n_aux=28,\n        n_resch=512,\n        n_skipch=256,\n        dilation_depth=10,\n        dilation_repeat=3,\n        kernel_size=2,\n        upsampling_factor=0,\n    ):\n        super(WaveNet, self).__init__()\n        self.n_aux = n_aux\n        self.n_quantize = n_quantize\n        self.n_resch = n_resch\n        self.n_skipch = n_skipch\n        self.kernel_size = kernel_size\n        self.dilation_depth = dilation_depth\n        self.dilation_repeat = dilation_repeat\n        self.upsampling_factor = upsampling_factor\n\n        self.dilations = [\n            2 ** i for i in range(self.dilation_depth)\n        ] * self.dilation_repeat\n        self.receptive_field = (self.kernel_size - 1) * sum(self.dilations) + 1\n\n        # for preprocessing\n        self.onehot = OneHot(self.n_quantize)\n        self.causal = CausalConv1d(self.n_quantize, self.n_resch, self.kernel_size)\n        if self.upsampling_factor > 0:\n            self.upsampling = UpSampling(self.upsampling_factor)\n\n        # for residual blocks\n        self.dil_sigmoid = nn.ModuleList()\n        self.dil_tanh = nn.ModuleList()\n        self.aux_1x1_sigmoid = nn.ModuleList()\n        self.aux_1x1_tanh = nn.ModuleList()\n        self.skip_1x1 = nn.ModuleList()\n        self.res_1x1 = nn.ModuleList()\n        for d in self.dilations:\n            self.dil_sigmoid += [\n                CausalConv1d(self.n_resch, self.n_resch, self.kernel_size, d)\n            ]\n            self.dil_tanh += [\n                CausalConv1d(self.n_resch, self.n_resch, self.kernel_size, d)\n            ]\n            self.aux_1x1_sigmoid += [nn.Conv1d(self.n_aux, self.n_resch, 1)]\n            self.aux_1x1_tanh += [nn.Conv1d(self.n_aux, self.n_resch, 1)]\n            self.skip_1x1 += [nn.Conv1d(self.n_resch, self.n_skipch, 1)]\n            self.res_1x1 += [nn.Conv1d(self.n_resch, self.n_resch, 1)]\n\n        # for postprocessing\n        self.conv_post_1 = nn.Conv1d(self.n_skipch, self.n_skipch, 1)\n        self.conv_post_2 = nn.Conv1d(self.n_skipch, self.n_quantize, 1)\n\n    def forward(self, x, h):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (LongTensor): Quantized input waveform tensor with the shape  (B, T).\n            h (Tensor): Auxiliary feature tensor with the shape  (B, n_aux, T).\n\n        Returns:\n            Tensor: Logits with the shape (B, T, n_quantize).\n\n        """"""\n        # preprocess\n        output = self._preprocess(x)\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # residual block\n        skip_connections = []\n        for i in range(len(self.dilations)):\n            output, skip = self._residual_forward(\n                output,\n                h,\n                self.dil_sigmoid[i],\n                self.dil_tanh[i],\n                self.aux_1x1_sigmoid[i],\n                self.aux_1x1_tanh[i],\n                self.skip_1x1[i],\n                self.res_1x1[i],\n            )\n            skip_connections.append(skip)\n\n        # skip-connection part\n        output = sum(skip_connections)\n        output = self._postprocess(output)\n\n        return output\n\n    def generate(self, x, h, n_samples, interval=None, mode=""sampling""):\n        """"""Generate a waveform with fast genration algorithm.\n\n        This generation based on `Fast WaveNet Generation Algorithm`_.\n\n        Args:\n            x (LongTensor): Initial waveform tensor with the shape  (T,).\n            h (Tensor): Auxiliary feature tensor with the shape  (n_samples + T, n_aux).\n            n_samples (int): Number of samples to be generated.\n            interval (int, optional): Log interval.\n            mode (str, optional): ""sampling"" or ""argmax"".\n\n        Return:\n            ndarray: Generated quantized waveform (n_samples).\n\n        .. _`Fast WaveNet Generation Algorithm`: https://arxiv.org/abs/1611.09482\n\n        """"""\n        # reshape inputs\n        assert len(x.shape) == 1\n        assert len(h.shape) == 2 and h.shape[1] == self.n_aux\n        x = x.unsqueeze(0)\n        h = h.transpose(0, 1).unsqueeze(0)\n\n        # perform upsampling\n        if self.upsampling_factor > 0:\n            h = self.upsampling(h)\n\n        # padding for shortage\n        if n_samples > h.shape[2]:\n            h = F.pad(h, (0, n_samples - h.shape[2]), ""replicate"")\n\n        # padding if the length less than\n        n_pad = self.receptive_field - x.size(1)\n        if n_pad > 0:\n            x = F.pad(x, (n_pad, 0), ""constant"", self.n_quantize // 2)\n            h = F.pad(h, (n_pad, 0), ""replicate"")\n\n        # prepare buffer\n        output = self._preprocess(x)\n        h_ = h[:, :, : x.size(1)]\n        output_buffer = []\n        buffer_size = []\n        for i, d in enumerate(self.dilations):\n            output, _ = self._residual_forward(\n                output,\n                h_,\n                self.dil_sigmoid[i],\n                self.dil_tanh[i],\n                self.aux_1x1_sigmoid[i],\n                self.aux_1x1_tanh[i],\n                self.skip_1x1[i],\n                self.res_1x1[i],\n            )\n            if d == 2 ** (self.dilation_depth - 1):\n                buffer_size.append(self.kernel_size - 1)\n            else:\n                buffer_size.append(d * 2 * (self.kernel_size - 1))\n            output_buffer.append(output[:, :, -buffer_size[i] - 1 : -1])\n\n        # generate\n        samples = x[0]\n        start_time = time.time()\n        for i in range(n_samples):\n            output = samples[-self.kernel_size * 2 + 1 :].unsqueeze(0)\n            output = self._preprocess(output)\n            h_ = h[:, :, samples.size(0) - 1].contiguous().view(1, self.n_aux, 1)\n            output_buffer_next = []\n            skip_connections = []\n            for j, d in enumerate(self.dilations):\n                output, skip = self._generate_residual_forward(\n                    output,\n                    h_,\n                    self.dil_sigmoid[j],\n                    self.dil_tanh[j],\n                    self.aux_1x1_sigmoid[j],\n                    self.aux_1x1_tanh[j],\n                    self.skip_1x1[j],\n                    self.res_1x1[j],\n                )\n                output = torch.cat([output_buffer[j], output], dim=2)\n                output_buffer_next.append(output[:, :, -buffer_size[j] :])\n                skip_connections.append(skip)\n\n            # update buffer\n            output_buffer = output_buffer_next\n\n            # get predicted sample\n            output = sum(skip_connections)\n            output = self._postprocess(output)[0]\n            if mode == ""sampling"":\n                posterior = F.softmax(output[-1], dim=0)\n                dist = torch.distributions.Categorical(posterior)\n                sample = dist.sample().unsqueeze(0)\n            elif mode == ""argmax"":\n                sample = output.argmax(-1)\n            else:\n                logging.error(""mode should be sampling or argmax"")\n                sys.exit(1)\n            samples = torch.cat([samples, sample], dim=0)\n\n            # show progress\n            if interval is not None and (i + 1) % interval == 0:\n                elapsed_time_per_sample = (time.time() - start_time) / interval\n                logging.info(\n                    ""%d/%d estimated time = %.3f sec (%.3f sec / sample)""\n                    % (\n                        i + 1,\n                        n_samples,\n                        (n_samples - i - 1) * elapsed_time_per_sample,\n                        elapsed_time_per_sample,\n                    )\n                )\n                start_time = time.time()\n\n        return samples[-n_samples:].cpu().numpy()\n\n    def _preprocess(self, x):\n        x = self.onehot(x).transpose(1, 2)\n        output = self.causal(x)\n        return output\n\n    def _postprocess(self, x):\n        output = F.relu(x)\n        output = self.conv_post_1(output)\n        output = F.relu(output)  # B x C x T\n        output = self.conv_post_2(output).transpose(1, 2)  # B x T x C\n        return output\n\n    def _residual_forward(\n        self,\n        x,\n        h,\n        dil_sigmoid,\n        dil_tanh,\n        aux_1x1_sigmoid,\n        aux_1x1_tanh,\n        skip_1x1,\n        res_1x1,\n    ):\n        output_sigmoid = dil_sigmoid(x)\n        output_tanh = dil_tanh(x)\n        aux_output_sigmoid = aux_1x1_sigmoid(h)\n        aux_output_tanh = aux_1x1_tanh(h)\n        output = torch.sigmoid(output_sigmoid + aux_output_sigmoid) * torch.tanh(\n            output_tanh + aux_output_tanh\n        )\n        skip = skip_1x1(output)\n        output = res_1x1(output)\n        output = output + x\n        return output, skip\n\n    def _generate_residual_forward(\n        self,\n        x,\n        h,\n        dil_sigmoid,\n        dil_tanh,\n        aux_1x1_sigmoid,\n        aux_1x1_tanh,\n        skip_1x1,\n        res_1x1,\n    ):\n        output_sigmoid = dil_sigmoid(x)[:, :, -1:]\n        output_tanh = dil_tanh(x)[:, :, -1:]\n        aux_output_sigmoid = aux_1x1_sigmoid(h)\n        aux_output_tanh = aux_1x1_tanh(h)\n        output = torch.sigmoid(output_sigmoid + aux_output_sigmoid) * torch.tanh(\n            output_tanh + aux_output_tanh\n        )\n        skip = skip_1x1(output)\n        output = res_1x1(output)\n        output = output + x[:, :, -1:]  # B x C x 1\n        return output, skip\n'"
espnet/nets/scorers/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/scorers/ctc.py,9,"b'""""""ScorerInterface implementation for CTC.""""""\n\nimport numpy as np\nimport torch\n\nfrom espnet.nets.ctc_prefix_score import CTCPrefixScore\nfrom espnet.nets.scorer_interface import PartialScorerInterface\n\n\nclass CTCPrefixScorer(PartialScorerInterface):\n    """"""Decoder interface wrapper for CTCPrefixScore.""""""\n\n    def __init__(self, ctc: torch.nn.Module, eos: int):\n        """"""Initialize class.\n\n        Args:\n            ctc (torch.nn.Module): The CTC implementaiton.\n                For example, :class:`espnet.nets.pytorch_backend.ctc.CTC`\n            eos (int): The end-of-sequence id.\n\n        """"""\n        self.ctc = ctc\n        self.eos = eos\n        self.impl = None\n\n    def init_state(self, x: torch.Tensor):\n        """"""Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        """"""\n        logp = self.ctc.log_softmax(x.unsqueeze(0)).detach().squeeze(0).cpu().numpy()\n        # TODO(karita): use CTCPrefixScoreTH\n        self.impl = CTCPrefixScore(logp, 0, self.eos, np)\n        return 0, self.impl.initial_state()\n\n    def select_state(self, state, i):\n        """"""Select state with relative ids in the main beam search.\n\n        Args:\n            state: Decoder state for prefix tokens\n            i (int): Index to select a state in the main beam search\n\n        Returns:\n            state: pruned state\n\n        """"""\n        sc, st = state\n        return sc[i], st[i]\n\n    def score_partial(self, y, ids, state, x):\n        """"""Score new token.\n\n        Args:\n            y (torch.Tensor): 1D prefix token\n            next_tokens (torch.Tensor): torch.int64 next token to score\n            state: decoder state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys\n\n        Returns:\n            tuple[torch.Tensor, Any]:\n                Tuple of a score tensor for y that has a shape `(len(next_tokens),)`\n                and next state for ys\n\n        """"""\n        prev_score, state = state\n        presub_score, new_st = self.impl(y.cpu(), ids.cpu(), state)\n        tscore = torch.as_tensor(\n            presub_score - prev_score, device=x.device, dtype=x.dtype\n        )\n        return tscore, (presub_score, new_st)\n'"
espnet/nets/scorers/length_bonus.py,11,"b'""""""Length bonus module.""""""\nfrom typing import Any\nfrom typing import List\nfrom typing import Tuple\n\nimport torch\n\nfrom espnet.nets.scorer_interface import BatchScorerInterface\n\n\nclass LengthBonus(BatchScorerInterface):\n    """"""Length bonus in beam search.""""""\n\n    def __init__(self, n_vocab: int):\n        """"""Initialize class.\n\n        Args:\n            n_vocab (int): The number of tokens in vocabulary for beam search\n\n        """"""\n        self.n = n_vocab\n\n    def score(self, y, state, x):\n        """"""Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and None\n\n        """"""\n        return torch.tensor([1.0], device=x.device, dtype=x.dtype).expand(self.n), None\n\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        """"""Score new token batch.\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        """"""\n        return (\n            torch.tensor([1.0], device=xs.device, dtype=xs.dtype).expand(\n                ys.shape[0], self.n\n            ),\n            None,\n        )\n'"
espnet/st/pytorch_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/st/pytorch_backend/st.py,18,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2019 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Training/decoding definition for the speech translation task.""""""\n\nimport copy\nimport json\nimport logging\nimport os\nimport sys\n\nfrom chainer import training\nfrom chainer.training import extensions\nimport numpy as np\nfrom tensorboardX import SummaryWriter\nimport torch\n\nfrom espnet.asr.asr_utils import adadelta_eps_decay\nfrom espnet.asr.asr_utils import adam_lr_decay\nfrom espnet.asr.asr_utils import add_results_to_json\nfrom espnet.asr.asr_utils import CompareValueTrigger\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import restore_snapshot\nfrom espnet.asr.asr_utils import snapshot_object\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.asr.asr_utils import torch_resume\nfrom espnet.asr.asr_utils import torch_snapshot\nfrom espnet.asr.pytorch_backend.asr_init import load_trained_model\nfrom espnet.asr.pytorch_backend.asr_init import load_trained_modules\n\nfrom espnet.nets.pytorch_backend.e2e_asr import pad_list\nimport espnet.nets.pytorch_backend.lm.default as lm_pytorch\nfrom espnet.nets.st_interface import STInterface\nfrom espnet.utils.dataset import ChainerDataLoader\nfrom espnet.utils.dataset import TransformDataset\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.training.batchfy import make_batchset\nfrom espnet.utils.training.iterators import ShufflingEnabler\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\nfrom espnet.asr.pytorch_backend.asr import CustomConverter as ASRCustomConverter\nfrom espnet.asr.pytorch_backend.asr import CustomEvaluator\nfrom espnet.asr.pytorch_backend.asr import CustomUpdater\n\nimport matplotlib\n\nmatplotlib.use(""Agg"")\n\nif sys.version_info[0] == 2:\n    from itertools import izip_longest as zip_longest\nelse:\n    from itertools import zip_longest as zip_longest\n\n\nclass CustomConverter(ASRCustomConverter):\n    """"""Custom batch converter for Pytorch.\n\n    Args:\n        subsampling_factor (int): The subsampling factor.\n        dtype (torch.dtype): Data type to convert.\n        use_source_text (bool): use source transcription.\n\n    """"""\n\n    def __init__(\n        self, subsampling_factor=1, dtype=torch.float32, use_source_text=False\n    ):\n        """"""Construct a CustomConverter object.""""""\n        super().__init__(subsampling_factor=subsampling_factor, dtype=dtype)\n        self.use_source_text = use_source_text\n\n    def __call__(self, batch, device=torch.device(""cpu"")):\n        """"""Transform a batch and send it to a device.\n\n        Args:\n            batch (list): The batch to transform.\n            device (torch.device): The device to send to.\n\n        Returns:\n            tuple(torch.Tensor, torch.Tensor, torch.Tensor)\n\n        """"""\n        _, ys = batch[0]\n        ys_asr = copy.deepcopy(ys)\n        xs_pad, ilens, ys_pad = super().__call__(batch, device)\n        if self.use_source_text:\n            ys_pad_asr = pad_list(\n                [torch.from_numpy(np.array(y[1])).long() for y in ys_asr],\n                self.ignore_id,\n            ).to(device)\n        else:\n            ys_pad_asr = None\n\n        return xs_pad, ilens, ys_pad, ys_pad_asr\n\n\ndef train(args):\n    """"""Train with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n\n    # check cuda availability\n    if not torch.cuda.is_available():\n        logging.warning(""cuda is not available"")\n\n    # get input and output dimension info\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n    utts = list(valid_json.keys())\n    idim = int(valid_json[utts[0]][""input""][0][""shape""][-1])\n    odim = int(valid_json[utts[0]][""output""][0][""shape""][-1])\n    logging.info(""#input dims : "" + str(idim))\n    logging.info(""#output dims: "" + str(odim))\n\n    # Initialize with pre-trained ASR encoder and MT decoder\n    if args.enc_init is not None or args.dec_init is not None:\n        model = load_trained_modules(idim, odim, args, interface=STInterface)\n    else:\n        model_class = dynamic_import(args.model_module)\n        model = model_class(idim, odim, args)\n    assert isinstance(model, STInterface)\n\n    subsampling_factor = model.subsample[0]\n\n    if args.rnnlm is not None:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(args.char_list),\n                rnnlm_args.layer,\n                rnnlm_args.unit,\n                getattr(rnnlm_args, ""embed_unit"", None),  # for backward compatibility\n            )\n        )\n        torch_load(args.rnnlm, rnnlm)\n        model.rnnlm = rnnlm\n\n    # write model config\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to "" + model_conf)\n        f.write(\n            json.dumps(\n                (idim, odim, vars(args)), indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n    for key in sorted(vars(args).keys()):\n        logging.info(""ARGS: "" + key + "": "" + str(vars(args)[key]))\n\n    reporter = model.reporter\n\n    # check the use of multi-gpu\n    if args.ngpu > 1:\n        if args.batch_size != 0:\n            logging.warning(\n                ""batch size is automatically increased (%d -> %d)""\n                % (args.batch_size, args.batch_size * args.ngpu)\n            )\n            args.batch_size *= args.ngpu\n\n    # set torch device\n    device = torch.device(""cuda"" if args.ngpu > 0 else ""cpu"")\n    if args.train_dtype in (""float16"", ""float32"", ""float64""):\n        dtype = getattr(torch, args.train_dtype)\n    else:\n        dtype = torch.float32\n    model = model.to(device=device, dtype=dtype)\n\n    # Setup an optimizer\n    if args.opt == ""adadelta"":\n        optimizer = torch.optim.Adadelta(\n            model.parameters(), rho=0.95, eps=args.eps, weight_decay=args.weight_decay\n        )\n    elif args.opt == ""adam"":\n        optimizer = torch.optim.Adam(\n            model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n        )\n    elif args.opt == ""noam"":\n        from espnet.nets.pytorch_backend.transformer.optimizer import get_std_opt\n\n        optimizer = get_std_opt(\n            model, args.adim, args.transformer_warmup_steps, args.transformer_lr\n        )\n    else:\n        raise NotImplementedError(""unknown optimizer: "" + args.opt)\n\n    # setup apex.amp\n    if args.train_dtype in (""O0"", ""O1"", ""O2"", ""O3""):\n        try:\n            from apex import amp\n        except ImportError as e:\n            logging.error(\n                f""You need to install apex for --train-dtype {args.train_dtype}. ""\n                ""See https://github.com/NVIDIA/apex#linux""\n            )\n            raise e\n        if args.opt == ""noam"":\n            model, optimizer.optimizer = amp.initialize(\n                model, optimizer.optimizer, opt_level=args.train_dtype\n            )\n        else:\n            model, optimizer = amp.initialize(\n                model, optimizer, opt_level=args.train_dtype\n            )\n        use_apex = True\n    else:\n        use_apex = False\n\n    # FIXME: TOO DIRTY HACK\n    setattr(optimizer, ""target"", reporter)\n    setattr(optimizer, ""serialize"", lambda s: reporter.serialize(s))\n\n    # Setup a converter\n    converter = CustomConverter(\n        subsampling_factor=subsampling_factor,\n        dtype=dtype,\n        use_source_text=args.asr_weight > 0 or args.mt_weight > 0,\n    )\n\n    # read json data\n    with open(args.train_json, ""rb"") as f:\n        train_json = json.load(f)[""utts""]\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n    # make minibatch list (variable length)\n    train = make_batchset(\n        train_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        shortest_first=use_sortagrad,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n    )\n    valid = make_batchset(\n        valid_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n    )\n\n    load_tr = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": True},  # Switch the mode of preprocessing\n    )\n    load_cv = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=True,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": False},  # Switch the mode of preprocessing\n    )\n    # hack to make batchsize argument as 1\n    # actual bathsize is included in a list\n    # default collate function converts numpy array to pytorch tensor\n    # we used an empty collate function instead which returns list\n    train_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(train, lambda data: converter([load_tr(data)])),\n            batch_size=1,\n            num_workers=args.n_iter_processes,\n            shuffle=not use_sortagrad,\n            collate_fn=lambda x: x[0],\n        )\n    }\n    valid_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(valid, lambda data: converter([load_cv(data)])),\n            batch_size=1,\n            shuffle=False,\n            collate_fn=lambda x: x[0],\n            num_workers=args.n_iter_processes,\n        )\n    }\n\n    # Set up a trainer\n    updater = CustomUpdater(\n        model,\n        args.grad_clip,\n        train_iter,\n        optimizer,\n        device,\n        args.ngpu,\n        args.grad_noise,\n        args.accum_grad,\n        use_apex=use_apex,\n    )\n    trainer = training.Trainer(updater, (args.epochs, ""epoch""), out=args.outdir)\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler([train_iter]),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, ""epoch""),\n        )\n\n    # Resume from a snapshot\n    if args.resume:\n        logging.info(""resumed from %s"" % args.resume)\n        torch_resume(args.resume, trainer)\n\n    # Evaluate the model with the test dataset for each epoch\n    if args.save_interval_iters > 0:\n        trainer.extend(\n            CustomEvaluator(model, valid_iter, reporter, device, args.ngpu),\n            trigger=(args.save_interval_iters, ""iteration""),\n        )\n    else:\n        trainer.extend(CustomEvaluator(model, valid_iter, reporter, device, args.ngpu))\n\n    # Save attention weight each epoch\n    if args.num_save_attention > 0:\n        data = sorted(\n            list(valid_json.items())[: args.num_save_attention],\n            key=lambda x: int(x[1][""input""][0][""shape""][1]),\n            reverse=True,\n        )\n        if hasattr(model, ""module""):\n            att_vis_fn = model.module.calculate_all_attentions\n            plot_class = model.module.attention_plot_class\n        else:\n            att_vis_fn = model.calculate_all_attentions\n            plot_class = model.attention_plot_class\n        att_reporter = plot_class(\n            att_vis_fn,\n            data,\n            args.outdir + ""/att_ws"",\n            converter=converter,\n            transform=load_cv,\n            device=device,\n        )\n        trainer.extend(att_reporter, trigger=(1, ""epoch""))\n    else:\n        att_reporter = None\n\n    # Make a plot for training and validation values\n    trainer.extend(\n        extensions.PlotReport(\n            [\n                ""main/loss"",\n                ""validation/main/loss"",\n                ""main/loss_asr"",\n                ""validation/main/loss_asr"",\n                ""main/loss_mt"",\n                ""validation/main/loss_mt"",\n                ""main/loss_st"",\n                ""validation/main/loss_st"",\n            ],\n            ""epoch"",\n            file_name=""loss.png"",\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [\n                ""main/acc"",\n                ""validation/main/acc"",\n                ""main/acc_asr"",\n                ""validation/main/acc_asr"",\n                ""main/acc_mt"",\n                ""validation/main/acc_mt"",\n            ],\n            ""epoch"",\n            file_name=""acc.png"",\n        )\n    )\n    trainer.extend(\n        extensions.PlotReport(\n            [""main/bleu"", ""validation/main/bleu""], ""epoch"", file_name=""bleu.png""\n        )\n    )\n\n    # Save best models\n    trainer.extend(\n        snapshot_object(model, ""model.loss.best""),\n        trigger=training.triggers.MinValueTrigger(""validation/main/loss""),\n    )\n    trainer.extend(\n        snapshot_object(model, ""model.acc.best""),\n        trigger=training.triggers.MaxValueTrigger(""validation/main/acc""),\n    )\n\n    # save snapshot which contains model and optimizer states\n    if args.save_interval_iters > 0:\n        trainer.extend(\n            torch_snapshot(filename=""snapshot.iter.{.updater.iteration}""),\n            trigger=(args.save_interval_iters, ""iteration""),\n        )\n    else:\n        trainer.extend(torch_snapshot(), trigger=(1, ""epoch""))\n\n    # epsilon decay in the optimizer\n    if args.opt == ""adadelta"":\n        if args.criterion == ""acc"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.acc.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n        elif args.criterion == ""loss"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.loss.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n            trainer.extend(\n                adadelta_eps_decay(args.eps_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n    elif args.opt == ""adam"":\n        if args.criterion == ""acc"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.acc.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n            trainer.extend(\n                adam_lr_decay(args.lr_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/acc"",\n                    lambda best_value, current_value: best_value > current_value,\n                ),\n            )\n        elif args.criterion == ""loss"":\n            trainer.extend(\n                restore_snapshot(\n                    model, args.outdir + ""/model.loss.best"", load_fn=torch_load\n                ),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n            trainer.extend(\n                adam_lr_decay(args.lr_decay),\n                trigger=CompareValueTrigger(\n                    ""validation/main/loss"",\n                    lambda best_value, current_value: best_value < current_value,\n                ),\n            )\n\n    # Write a log of evaluation statistics for each epoch\n    trainer.extend(\n        extensions.LogReport(trigger=(args.report_interval_iters, ""iteration""))\n    )\n    report_keys = [\n        ""epoch"",\n        ""iteration"",\n        ""main/loss"",\n        ""main/loss_st"",\n        ""main/loss_asr"",\n        ""validation/main/loss"",\n        ""validation/main/loss_st"",\n        ""validation/main/loss_asr"",\n        ""main/acc"",\n        ""validation/main/acc"",\n    ]\n    if args.asr_weight > 0:\n        report_keys.append(""main/acc_asr"")\n        report_keys.append(""validation/main/acc_asr"")\n    report_keys += [""elapsed_time""]\n    if args.opt == ""adadelta"":\n        trainer.extend(\n            extensions.observe_value(\n                ""eps"",\n                lambda trainer: trainer.updater.get_optimizer(""main"").param_groups[0][\n                    ""eps""\n                ],\n            ),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n        report_keys.append(""eps"")\n    elif args.opt in [""adam"", ""noam""]:\n        trainer.extend(\n            extensions.observe_value(\n                ""lr"",\n                lambda trainer: trainer.updater.get_optimizer(""main"").param_groups[0][\n                    ""lr""\n                ],\n            ),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n        report_keys.append(""lr"")\n    if args.asr_weight > 0:\n        if args.mtlalpha > 0:\n            report_keys.append(""main/cer_ctc"")\n            report_keys.append(""validation/main/cer_ctc"")\n        if args.mtlalpha < 1:\n            if args.report_cer:\n                report_keys.append(""validation/main/cer"")\n            if args.report_wer:\n                report_keys.append(""validation/main/wer"")\n    if args.report_bleu:\n        report_keys.append(""main/bleu"")\n        report_keys.append(""validation/main/bleu"")\n    trainer.extend(\n        extensions.PrintReport(report_keys),\n        trigger=(args.report_interval_iters, ""iteration""),\n    )\n\n    trainer.extend(extensions.ProgressBar(update_interval=args.report_interval_iters))\n    set_early_stop(trainer, args)\n\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        trainer.extend(\n            TensorboardLogger(SummaryWriter(args.tensorboard_dir), att_reporter),\n            trigger=(args.report_interval_iters, ""iteration""),\n        )\n    # Run the training\n    trainer.run()\n    check_early_stop(trainer, args.epochs)\n\n\ndef trans(args):\n    """"""Decode with the given args.\n\n    Args:\n        args (namespace): The program arguments.\n\n    """"""\n    set_deterministic_pytorch(args)\n    model, train_args = load_trained_model(args.model)\n    assert isinstance(model, STInterface)\n    # args.ctc_weight = 0.0\n    model.trans_args = args\n\n    # read rnnlm\n    if args.rnnlm:\n        rnnlm_args = get_model_conf(args.rnnlm, args.rnnlm_conf)\n        if getattr(rnnlm_args, ""model_module"", ""default"") != ""default"":\n            raise ValueError(\n                ""use \'--api v2\' option to decode with non-default language model""\n            )\n        rnnlm = lm_pytorch.ClassifierWithState(\n            lm_pytorch.RNNLM(\n                len(train_args.char_list), rnnlm_args.layer, rnnlm_args.unit\n            )\n        )\n        torch_load(args.rnnlm, rnnlm)\n        rnnlm.eval()\n    else:\n        rnnlm = None\n\n    # gpu\n    if args.ngpu == 1:\n        gpu_id = list(range(args.ngpu))\n        logging.info(""gpu id: "" + str(gpu_id))\n        model.cuda()\n        if rnnlm:\n            rnnlm.cuda()\n\n    # read json data\n    with open(args.trans_json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n    new_js = {}\n\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=""asr"",\n        load_output=False,\n        sort_in_input_length=False,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={""train"": False},\n    )\n\n    if args.batchsize == 0:\n        with torch.no_grad():\n            for idx, name in enumerate(js.keys(), 1):\n                logging.info(""(%d/%d) decoding "" + name, idx, len(js.keys()))\n                batch = [(name, js[name])]\n                feat = load_inputs_and_targets(batch)[0][0]\n                nbest_hyps = model.translate(feat, args, train_args.char_list, rnnlm)\n                new_js[name] = add_results_to_json(\n                    js[name], nbest_hyps, train_args.char_list\n                )\n\n    else:\n\n        def grouper(n, iterable, fillvalue=None):\n            kargs = [iter(iterable)] * n\n            return zip_longest(*kargs, fillvalue=fillvalue)\n\n        # sort data if batchsize > 1\n        keys = list(js.keys())\n        if args.batchsize > 1:\n            feat_lens = [js[key][""input""][0][""shape""][0] for key in keys]\n            sorted_index = sorted(range(len(feat_lens)), key=lambda i: -feat_lens[i])\n            keys = [keys[i] for i in sorted_index]\n\n        with torch.no_grad():\n            for names in grouper(args.batchsize, keys, None):\n                names = [name for name in names if name]\n                batch = [(name, js[name]) for name in names]\n                feats = load_inputs_and_targets(batch)[0]\n                nbest_hyps = model.translate_batch(\n                    feats, args, train_args.char_list, rnnlm=rnnlm\n                )\n\n                for i, nbest_hyp in enumerate(nbest_hyps):\n                    name = names[i]\n                    new_js[name] = add_results_to_json(\n                        js[name], nbest_hyp, train_args.char_list\n                    )\n\n    with open(args.result_label, ""wb"") as f:\n        f.write(\n            json.dumps(\n                {""utts"": new_js}, indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n'"
espnet/tts/pytorch_backend/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/tts/pytorch_backend/tts.py,26,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""E2E-TTS training / decoding functions.""""""\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport time\n\nimport chainer\nimport kaldiio\nimport numpy as np\nimport torch\n\nfrom chainer import training\nfrom chainer.training import extensions\n\nfrom espnet.asr.asr_utils import get_model_conf\nfrom espnet.asr.asr_utils import snapshot_object\nfrom espnet.asr.asr_utils import torch_load\nfrom espnet.asr.asr_utils import torch_resume\nfrom espnet.asr.asr_utils import torch_snapshot\nfrom espnet.asr.pytorch_backend.asr_init import load_trained_modules\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.tts_interface import TTSInterface\nfrom espnet.utils.dataset import ChainerDataLoader\nfrom espnet.utils.dataset import TransformDataset\nfrom espnet.utils.dynamic_import import dynamic_import\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.training.batchfy import make_batchset\nfrom espnet.utils.training.evaluator import BaseEvaluator\n\nfrom espnet.utils.deterministic_utils import set_deterministic_pytorch\nfrom espnet.utils.training.train_utils import check_early_stop\nfrom espnet.utils.training.train_utils import set_early_stop\n\nfrom espnet.utils.training.iterators import ShufflingEnabler\n\nimport matplotlib\n\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\nfrom tensorboardX import SummaryWriter\n\nmatplotlib.use(""Agg"")\n\n\nclass CustomEvaluator(BaseEvaluator):\n    """"""Custom evaluator.""""""\n\n    def __init__(self, model, iterator, target, device):\n        """"""Initilize module.\n\n        Args:\n            model (torch.nn.Module): Pytorch model instance.\n            iterator (chainer.dataset.Iterator): Iterator for validation.\n            target (chainer.Chain): Dummy chain instance.\n            device (torch.device): The device to be used in evaluation.\n\n        """"""\n        super(CustomEvaluator, self).__init__(iterator, target)\n        self.model = model\n        self.device = device\n\n    # The core part of the update routine can be customized by overriding.\n    def evaluate(self):\n        """"""Evaluate over validation iterator.""""""\n        iterator = self._iterators[""main""]\n\n        if self.eval_hook:\n            self.eval_hook(self)\n\n        if hasattr(iterator, ""reset""):\n            iterator.reset()\n            it = iterator\n        else:\n            it = copy.copy(iterator)\n\n        summary = chainer.reporter.DictSummary()\n\n        self.model.eval()\n        with torch.no_grad():\n            for batch in it:\n                if isinstance(batch, tuple):\n                    x = tuple(arr.to(self.device) for arr in batch)\n                else:\n                    x = batch\n                    for key in x.keys():\n                        x[key] = x[key].to(self.device)\n                observation = {}\n                with chainer.reporter.report_scope(observation):\n                    # convert to torch tensor\n                    if isinstance(x, tuple):\n                        self.model(*x)\n                    else:\n                        self.model(**x)\n                summary.add(observation)\n        self.model.train()\n\n        return summary.compute_mean()\n\n\nclass CustomUpdater(training.StandardUpdater):\n    """"""Custom updater.""""""\n\n    def __init__(self, model, grad_clip, iterator, optimizer, device, accum_grad=1):\n        """"""Initilize module.\n\n        Args:\n            model (torch.nn.Module) model: Pytorch model instance.\n            grad_clip (float) grad_clip : The gradient clipping value.\n            iterator (chainer.dataset.Iterator): Iterator for training.\n            optimizer (torch.optim.Optimizer) : Pytorch optimizer instance.\n            device (torch.device): The device to be used in training.\n\n        """"""\n        super(CustomUpdater, self).__init__(iterator, optimizer)\n        self.model = model\n        self.grad_clip = grad_clip\n        self.device = device\n        self.clip_grad_norm = torch.nn.utils.clip_grad_norm_\n        self.accum_grad = accum_grad\n        self.forward_count = 0\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        """"""Update model one step.""""""\n        # When we pass one iterator and optimizer to StandardUpdater.__init__,\n        # they are automatically named \'main\'.\n        train_iter = self.get_iterator(""main"")\n        optimizer = self.get_optimizer(""main"")\n\n        # Get the next batch (a list of json files)\n        batch = train_iter.next()\n        if isinstance(batch, tuple):\n            x = tuple(arr.to(self.device) for arr in batch)\n        else:\n            x = batch\n            for key in x.keys():\n                x[key] = x[key].to(self.device)\n\n        # compute loss and gradient\n        if isinstance(x, tuple):\n            loss = self.model(*x).mean() / self.accum_grad\n        else:\n            loss = self.model(**x).mean() / self.accum_grad\n        loss.backward()\n\n        # update parameters\n        self.forward_count += 1\n        if self.forward_count != self.accum_grad:\n            return\n        self.forward_count = 0\n\n        # compute the gradient norm to check if it is normal or not\n        grad_norm = self.clip_grad_norm(self.model.parameters(), self.grad_clip)\n        logging.debug(""grad norm={}"".format(grad_norm))\n        if math.isnan(grad_norm):\n            logging.warning(""grad norm is nan. Do not update model."")\n        else:\n            optimizer.step()\n        optimizer.zero_grad()\n\n    def update(self):\n        """"""Run update function.""""""\n        self.update_core()\n        if self.forward_count == 0:\n            self.iteration += 1\n\n\nclass CustomConverter(object):\n    """"""Custom converter.""""""\n\n    def __init__(self):\n        """"""Initilize module.""""""\n        # NOTE: keep as class for future development\n        pass\n\n    def __call__(self, batch, device=torch.device(""cpu"")):\n        """"""Convert a given batch.\n\n        Args:\n            batch (list): List of ndarrays.\n            device (torch.device): The device to be send.\n\n        Returns:\n            dict: Dict of converted tensors.\n\n        Examples:\n            >>> batch = [([np.arange(5), np.arange(3)],\n                          [np.random.randn(8, 2), np.random.randn(4, 2)],\n                          None, None)]\n            >>> conveter = CustomConverter()\n            >>> conveter(batch, torch.device(""cpu""))\n            {\'xs\': tensor([[0, 1, 2, 3, 4],\n                           [0, 1, 2, 0, 0]]),\n             \'ilens\': tensor([5, 3]),\n             \'ys\': tensor([[[-0.4197, -1.1157],\n                            [-1.5837, -0.4299],\n                            [-2.0491,  0.9215],\n                            [-2.4326,  0.8891],\n                            [ 1.2323,  1.7388],\n                            [-0.3228,  0.6656],\n                            [-0.6025,  1.3693],\n                            [-1.0778,  1.3447]],\n                           [[ 0.1768, -0.3119],\n                            [ 0.4386,  2.5354],\n                            [-1.2181, -0.5918],\n                            [-0.6858, -0.8843],\n                            [ 0.0000,  0.0000],\n                            [ 0.0000,  0.0000],\n                            [ 0.0000,  0.0000],\n                            [ 0.0000,  0.0000]]]),\n             \'labels\': tensor([[0., 0., 0., 0., 0., 0., 0., 1.],\n                               [0., 0., 0., 1., 1., 1., 1., 1.]]),\n             \'olens\': tensor([8, 4])}\n\n        """"""\n        # batch should be located in list\n        assert len(batch) == 1\n        xs, ys, spembs, extras = batch[0]\n\n        # get list of lengths (must be tensor for DataParallel)\n        ilens = torch.from_numpy(np.array([x.shape[0] for x in xs])).long().to(device)\n        olens = torch.from_numpy(np.array([y.shape[0] for y in ys])).long().to(device)\n\n        # perform padding and conversion to tensor\n        xs = pad_list([torch.from_numpy(x).long() for x in xs], 0).to(device)\n        ys = pad_list([torch.from_numpy(y).float() for y in ys], 0).to(device)\n\n        # make labels for stop prediction\n        labels = ys.new_zeros(ys.size(0), ys.size(1))\n        for i, l in enumerate(olens):\n            labels[i, l - 1 :] = 1.0\n\n        # prepare dict\n        new_batch = {\n            ""xs"": xs,\n            ""ilens"": ilens,\n            ""ys"": ys,\n            ""labels"": labels,\n            ""olens"": olens,\n        }\n\n        # load speaker embedding\n        if spembs is not None:\n            spembs = torch.from_numpy(np.array(spembs)).float()\n            new_batch[""spembs""] = spembs.to(device)\n\n        # load second target\n        if extras is not None:\n            extras = pad_list([torch.from_numpy(extra).float() for extra in extras], 0)\n            new_batch[""extras""] = extras.to(device)\n\n        return new_batch\n\n\ndef train(args):\n    """"""Train E2E-TTS model.""""""\n    set_deterministic_pytorch(args)\n\n    # check cuda availability\n    if not torch.cuda.is_available():\n        logging.warning(""cuda is not available"")\n\n    # get input and output dimension info\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n    utts = list(valid_json.keys())\n\n    # reverse input and output dimension\n    idim = int(valid_json[utts[0]][""output""][0][""shape""][1])\n    odim = int(valid_json[utts[0]][""input""][0][""shape""][1])\n    logging.info(""#input dims : "" + str(idim))\n    logging.info(""#output dims: "" + str(odim))\n\n    # get extra input and output dimenstion\n    if args.use_speaker_embedding:\n        args.spk_embed_dim = int(valid_json[utts[0]][""input""][1][""shape""][0])\n    else:\n        args.spk_embed_dim = None\n    if args.use_second_target:\n        args.spc_dim = int(valid_json[utts[0]][""input""][1][""shape""][1])\n    else:\n        args.spc_dim = None\n\n    # write model config\n    if not os.path.exists(args.outdir):\n        os.makedirs(args.outdir)\n    model_conf = args.outdir + ""/model.json""\n    with open(model_conf, ""wb"") as f:\n        logging.info(""writing a model config file to"" + model_conf)\n        f.write(\n            json.dumps(\n                (idim, odim, vars(args)), indent=4, ensure_ascii=False, sort_keys=True\n            ).encode(""utf_8"")\n        )\n    for key in sorted(vars(args).keys()):\n        logging.info(""ARGS: "" + key + "": "" + str(vars(args)[key]))\n\n    # specify model architecture\n    if args.enc_init is not None or args.dec_init is not None:\n        model = load_trained_modules(idim, odim, args, TTSInterface)\n    else:\n        model_class = dynamic_import(args.model_module)\n        model = model_class(idim, odim, args)\n    assert isinstance(model, TTSInterface)\n    logging.info(model)\n    reporter = model.reporter\n\n    # check the use of multi-gpu\n    if args.ngpu > 1:\n        model = torch.nn.DataParallel(model, device_ids=list(range(args.ngpu)))\n        if args.batch_size != 0:\n            logging.warning(\n                ""batch size is automatically increased (%d -> %d)""\n                % (args.batch_size, args.batch_size * args.ngpu)\n            )\n            args.batch_size *= args.ngpu\n\n    # set torch device\n    device = torch.device(""cuda"" if args.ngpu > 0 else ""cpu"")\n    model = model.to(device)\n\n    # freeze modules, if specified\n    if args.freeze_mods:\n        if hasattr(model, ""module""):\n            freeze_mods = [""module."" + x for x in args.freeze_mods]\n        else:\n            freeze_mods = args.freeze_mods\n\n        for mod, param in model.named_parameters():\n            if any(mod.startswith(key) for key in freeze_mods):\n                logging.info(f""{mod} is frozen not to be updated."")\n                param.requires_grad = False\n\n        model_params = filter(lambda x: x.requires_grad, model.parameters())\n    else:\n        model_params = model.parameters()\n\n    # Setup an optimizer\n    if args.opt == ""adam"":\n        optimizer = torch.optim.Adam(\n            model_params, args.lr, eps=args.eps, weight_decay=args.weight_decay\n        )\n    elif args.opt == ""noam"":\n        from espnet.nets.pytorch_backend.transformer.optimizer import get_std_opt\n\n        optimizer = get_std_opt(\n            model, args.adim, args.transformer_warmup_steps, args.transformer_lr\n        )\n    else:\n        raise NotImplementedError(""unknown optimizer: "" + args.opt)\n\n    # FIXME: TOO DIRTY HACK\n    setattr(optimizer, ""target"", reporter)\n    setattr(optimizer, ""serialize"", lambda s: reporter.serialize(s))\n\n    # read json data\n    with open(args.train_json, ""rb"") as f:\n        train_json = json.load(f)[""utts""]\n    with open(args.valid_json, ""rb"") as f:\n        valid_json = json.load(f)[""utts""]\n\n    use_sortagrad = args.sortagrad == -1 or args.sortagrad > 0\n    if use_sortagrad:\n        args.batch_sort_key = ""input""\n    # make minibatch list (variable length)\n    train_batchset = make_batchset(\n        train_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        batch_sort_key=args.batch_sort_key,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        shortest_first=use_sortagrad,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        swap_io=True,\n        iaxis=0,\n        oaxis=0,\n    )\n    valid_batchset = make_batchset(\n        valid_json,\n        args.batch_size,\n        args.maxlen_in,\n        args.maxlen_out,\n        args.minibatches,\n        batch_sort_key=args.batch_sort_key,\n        min_batch_size=args.ngpu if args.ngpu > 1 else 1,\n        count=args.batch_count,\n        batch_bins=args.batch_bins,\n        batch_frames_in=args.batch_frames_in,\n        batch_frames_out=args.batch_frames_out,\n        batch_frames_inout=args.batch_frames_inout,\n        swap_io=True,\n        iaxis=0,\n        oaxis=0,\n    )\n\n    load_tr = LoadInputsAndTargets(\n        mode=""tts"",\n        use_speaker_embedding=args.use_speaker_embedding,\n        use_second_target=args.use_second_target,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": True},  # Switch the mode of preprocessing\n        keep_all_data_on_mem=args.keep_all_data_on_mem,\n    )\n\n    load_cv = LoadInputsAndTargets(\n        mode=""tts"",\n        use_speaker_embedding=args.use_speaker_embedding,\n        use_second_target=args.use_second_target,\n        preprocess_conf=args.preprocess_conf,\n        preprocess_args={""train"": False},  # Switch the mode of preprocessing\n        keep_all_data_on_mem=args.keep_all_data_on_mem,\n    )\n\n    converter = CustomConverter()\n    # hack to make batchsize argument as 1\n    # actual bathsize is included in a list\n    train_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(\n                train_batchset, lambda data: converter([load_tr(data)])\n            ),\n            batch_size=1,\n            num_workers=args.num_iter_processes,\n            shuffle=not use_sortagrad,\n            collate_fn=lambda x: x[0],\n        )\n    }\n    valid_iter = {\n        ""main"": ChainerDataLoader(\n            dataset=TransformDataset(\n                valid_batchset, lambda data: converter([load_cv(data)])\n            ),\n            batch_size=1,\n            shuffle=False,\n            collate_fn=lambda x: x[0],\n            num_workers=args.num_iter_processes,\n        )\n    }\n\n    # Set up a trainer\n    updater = CustomUpdater(\n        model, args.grad_clip, train_iter, optimizer, device, args.accum_grad\n    )\n    trainer = training.Trainer(updater, (args.epochs, ""epoch""), out=args.outdir)\n\n    # Resume from a snapshot\n    if args.resume:\n        logging.info(""resumed from %s"" % args.resume)\n        torch_resume(args.resume, trainer)\n\n    # set intervals\n    eval_interval = (args.eval_interval_epochs, ""epoch"")\n    save_interval = (args.save_interval_epochs, ""epoch"")\n    report_interval = (args.report_interval_iters, ""iteration"")\n\n    # Evaluate the model with the test dataset for each epoch\n    trainer.extend(\n        CustomEvaluator(model, valid_iter, reporter, device), trigger=eval_interval\n    )\n\n    # Save snapshot for each epoch\n    trainer.extend(torch_snapshot(), trigger=save_interval)\n\n    # Save best models\n    trainer.extend(\n        snapshot_object(model, ""model.loss.best""),\n        trigger=training.triggers.MinValueTrigger(\n            ""validation/main/loss"", trigger=eval_interval\n        ),\n    )\n\n    # Save attention figure for each epoch\n    if args.num_save_attention > 0:\n        data = sorted(\n            list(valid_json.items())[: args.num_save_attention],\n            key=lambda x: int(x[1][""output""][0][""shape""][0]),\n            reverse=True,\n        )\n        if hasattr(model, ""module""):\n            att_vis_fn = model.module.calculate_all_attentions\n            plot_class = model.module.attention_plot_class\n            reduction_factor = model.module.reduction_factor\n        else:\n            att_vis_fn = model.calculate_all_attentions\n            plot_class = model.attention_plot_class\n            reduction_factor = model.reduction_factor\n        if reduction_factor > 1:\n            # fix the length to crop attention weight plot correctly\n            data = copy.deepcopy(data)\n            for idx in range(len(data)):\n                ilen = data[idx][1][""input""][0][""shape""][0]\n                data[idx][1][""input""][0][""shape""][0] = ilen // reduction_factor\n        att_reporter = plot_class(\n            att_vis_fn,\n            data,\n            args.outdir + ""/att_ws"",\n            converter=converter,\n            transform=load_cv,\n            device=device,\n            reverse=True,\n        )\n        trainer.extend(att_reporter, trigger=eval_interval)\n    else:\n        att_reporter = None\n\n    # Make a plot for training and validation values\n    if hasattr(model, ""module""):\n        base_plot_keys = model.module.base_plot_keys\n    else:\n        base_plot_keys = model.base_plot_keys\n    plot_keys = []\n    for key in base_plot_keys:\n        plot_key = [""main/"" + key, ""validation/main/"" + key]\n        trainer.extend(\n            extensions.PlotReport(plot_key, ""epoch"", file_name=key + "".png""),\n            trigger=eval_interval,\n        )\n        plot_keys += plot_key\n    trainer.extend(\n        extensions.PlotReport(plot_keys, ""epoch"", file_name=""all_loss.png""),\n        trigger=eval_interval,\n    )\n\n    # Write a log of evaluation statistics for each epoch\n    trainer.extend(extensions.LogReport(trigger=report_interval))\n    report_keys = [""epoch"", ""iteration"", ""elapsed_time""] + plot_keys\n    trainer.extend(extensions.PrintReport(report_keys), trigger=report_interval)\n    trainer.extend(extensions.ProgressBar(), trigger=report_interval)\n\n    set_early_stop(trainer, args)\n    if args.tensorboard_dir is not None and args.tensorboard_dir != """":\n        writer = SummaryWriter(args.tensorboard_dir)\n        trainer.extend(TensorboardLogger(writer, att_reporter), trigger=report_interval)\n\n    if use_sortagrad:\n        trainer.extend(\n            ShufflingEnabler([train_iter]),\n            trigger=(args.sortagrad if args.sortagrad != -1 else args.epochs, ""epoch""),\n        )\n\n    # Run the training\n    trainer.run()\n    check_early_stop(trainer, args.epochs)\n\n\n@torch.no_grad()\ndef decode(args):\n    """"""Decode with E2E-TTS model.""""""\n    set_deterministic_pytorch(args)\n    # read training config\n    idim, odim, train_args = get_model_conf(args.model, args.model_conf)\n\n    # show arguments\n    for key in sorted(vars(args).keys()):\n        logging.info(""args: "" + key + "": "" + str(vars(args)[key]))\n\n    # define model\n    model_class = dynamic_import(train_args.model_module)\n    model = model_class(idim, odim, train_args)\n    assert isinstance(model, TTSInterface)\n    logging.info(model)\n\n    # load trained model parameters\n    logging.info(""reading model parameters from "" + args.model)\n    torch_load(args.model, model)\n    model.eval()\n\n    # set torch device\n    device = torch.device(""cuda"" if args.ngpu > 0 else ""cpu"")\n    model = model.to(device)\n\n    # read json data\n    with open(args.json, ""rb"") as f:\n        js = json.load(f)[""utts""]\n\n    # check directory\n    outdir = os.path.dirname(args.out)\n    if len(outdir) != 0 and not os.path.exists(outdir):\n        os.makedirs(outdir)\n\n    load_inputs_and_targets = LoadInputsAndTargets(\n        mode=""tts"",\n        load_input=False,\n        sort_in_input_length=False,\n        use_speaker_embedding=train_args.use_speaker_embedding,\n        preprocess_conf=train_args.preprocess_conf\n        if args.preprocess_conf is None\n        else args.preprocess_conf,\n        preprocess_args={""train"": False},  # Switch the mode of preprocessing\n    )\n\n    # define function for plot prob and att_ws\n    def _plot_and_save(array, figname, figsize=(6, 4), dpi=150):\n        import matplotlib.pyplot as plt\n\n        shape = array.shape\n        if len(shape) == 1:\n            # for eos probability\n            plt.figure(figsize=figsize, dpi=dpi)\n            plt.plot(array)\n            plt.xlabel(""Frame"")\n            plt.ylabel(""Probability"")\n            plt.ylim([0, 1])\n        elif len(shape) == 2:\n            # for tacotron 2 attention weights, whose shape is (out_length, in_length)\n            plt.figure(figsize=figsize, dpi=dpi)\n            plt.imshow(array, aspect=""auto"")\n            plt.xlabel(""Input"")\n            plt.ylabel(""Output"")\n        elif len(shape) == 4:\n            # for transformer attention weights,\n            # whose shape is (#leyers, #heads, out_length, in_length)\n            plt.figure(figsize=(figsize[0] * shape[0], figsize[1] * shape[1]), dpi=dpi)\n            for idx1, xs in enumerate(array):\n                for idx2, x in enumerate(xs, 1):\n                    plt.subplot(shape[0], shape[1], idx1 * shape[1] + idx2)\n                    plt.imshow(x, aspect=""auto"")\n                    plt.xlabel(""Input"")\n                    plt.ylabel(""Output"")\n        else:\n            raise NotImplementedError(""Support only from 1D to 4D array."")\n        plt.tight_layout()\n        if not os.path.exists(os.path.dirname(figname)):\n            # NOTE: exist_ok = True is needed for parallel process decoding\n            os.makedirs(os.path.dirname(figname), exist_ok=True)\n        plt.savefig(figname)\n        plt.close()\n\n    # define function to calculate focus rate\n    # (see section 3.3 in https://arxiv.org/abs/1905.09263)\n    def _calculate_focus_rete(att_ws):\n        if att_ws is None:\n            # fastspeech case -> None\n            return 1.0\n        elif len(att_ws.shape) == 2:\n            # tacotron 2 case -> (L, T)\n            return float(att_ws.max(dim=-1)[0].mean())\n        elif len(att_ws.shape) == 4:\n            # transformer case -> (#layers, #heads, L, T)\n            return float(att_ws.max(dim=-1)[0].mean(dim=-1).max())\n        else:\n            raise ValueError(""att_ws should be 2 or 4 dimensional tensor."")\n\n    # define function to convert attention to duration\n    def _convert_att_to_duration(att_ws):\n        if len(att_ws.shape) == 2:\n            # tacotron 2 case -> (L, T)\n            pass\n        elif len(att_ws.shape) == 4:\n            # transformer case -> (#layers, #heads, L, T)\n            # get the most diagonal head according to focus rate\n            att_ws = torch.cat(\n                [att_w for att_w in att_ws], dim=0\n            )  # (#heads * #layers, L, T)\n            diagonal_scores = att_ws.max(dim=-1)[0].mean(dim=-1)  # (#heads * #layers,)\n            diagonal_head_idx = diagonal_scores.argmax()\n            att_ws = att_ws[diagonal_head_idx]  # (L, T)\n        else:\n            raise ValueError(""att_ws should be 2 or 4 dimensional tensor."")\n        # calculate duration from 2d attention weight\n        durations = torch.stack(\n            [att_ws.argmax(-1).eq(i).sum() for i in range(att_ws.shape[1])]\n        )\n        return durations.view(-1, 1).float()\n\n    # define writer instances\n    feat_writer = kaldiio.WriteHelper(""ark,scp:{o}.ark,{o}.scp"".format(o=args.out))\n    if args.save_durations:\n        dur_writer = kaldiio.WriteHelper(\n            ""ark,scp:{o}.ark,{o}.scp"".format(o=args.out.replace(""feats"", ""durations""))\n        )\n    if args.save_focus_rates:\n        fr_writer = kaldiio.WriteHelper(\n            ""ark,scp:{o}.ark,{o}.scp"".format(o=args.out.replace(""feats"", ""focus_rates""))\n        )\n\n    # start decoding\n    for idx, utt_id in enumerate(js.keys()):\n        # setup inputs\n        batch = [(utt_id, js[utt_id])]\n        data = load_inputs_and_targets(batch)\n        x = torch.LongTensor(data[0][0]).to(device)\n        spemb = None\n        if train_args.use_speaker_embedding:\n            spemb = torch.FloatTensor(data[1][0]).to(device)\n\n        # decode and write\n        start_time = time.time()\n        outs, probs, att_ws = model.inference(x, args, spemb=spemb)\n        logging.info(\n            ""inference speed = %.1f frames / sec.""\n            % (int(outs.size(0)) / (time.time() - start_time))\n        )\n        if outs.size(0) == x.size(0) * args.maxlenratio:\n            logging.warning(""output length reaches maximum length (%s)."" % utt_id)\n        focus_rate = _calculate_focus_rete(att_ws)\n        logging.info(\n            ""(%d/%d) %s (size: %d->%d, focus rate: %.3f)""\n            % (idx + 1, len(js.keys()), utt_id, x.size(0), outs.size(0), focus_rate)\n        )\n        feat_writer[utt_id] = outs.cpu().numpy()\n        if args.save_durations:\n            ds = _convert_att_to_duration(att_ws)\n            dur_writer[utt_id] = ds.cpu().numpy()\n        if args.save_focus_rates:\n            fr_writer[utt_id] = np.array(focus_rate).reshape(1, 1)\n\n        # plot and save prob and att_ws\n        if probs is not None:\n            _plot_and_save(\n                probs.cpu().numpy(),\n                os.path.dirname(args.out) + ""/probs/%s_prob.png"" % utt_id,\n            )\n        if att_ws is not None:\n            _plot_and_save(\n                att_ws.cpu().numpy(),\n                os.path.dirname(args.out) + ""/att_ws/%s_att_ws.png"" % utt_id,\n            )\n\n    # close file object\n    feat_writer.close()\n    if args.save_durations:\n        dur_writer.close()\n    if args.save_focus_rates:\n        fr_writer.close()\n'"
espnet/utils/training/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/utils/training/batchfy.py,0,"b'import itertools\nimport logging\n\nimport numpy as np\n\n\ndef batchfy_by_seq(\n    sorted_data,\n    batch_size,\n    max_length_in,\n    max_length_out,\n    min_batch_size=1,\n    shortest_first=False,\n    ikey=""input"",\n    iaxis=0,\n    okey=""output"",\n    oaxis=0,\n):\n    """"""Make batch set from json dictionary\n\n    :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json\n    :param int batch_size: batch size\n    :param int max_length_in: maximum length of input to decide adaptive batch size\n    :param int max_length_out: maximum length of output to decide adaptive batch size\n    :param int min_batch_size: mininum batch size (for multi-gpu)\n    :param bool shortest_first: Sort from batch with shortest samples\n        to longest if true, otherwise reverse\n    :param str ikey: key to access input\n        (for ASR ikey=""input"", for TTS, MT ikey=""output"".)\n    :param int iaxis: dimension to access input\n        (for ASR, TTS iaxis=0, for MT iaxis=""1"".)\n    :param str okey: key to access output\n        (for ASR, MT okey=""output"". for TTS okey=""input"".)\n    :param int oaxis: dimension to access output\n        (for ASR, TTS, MT oaxis=0, reserved for future research, -1 means all axis.)\n    :return: List[List[Tuple[str, dict]]] list of batches\n    """"""\n    if batch_size <= 0:\n        raise ValueError(f""Invalid batch_size={batch_size}"")\n\n    # check #utts is more than min_batch_size\n    if len(sorted_data) < min_batch_size:\n        raise ValueError(\n            f""#utts({len(sorted_data)}) is less than min_batch_size({min_batch_size}).""\n        )\n\n    # make list of minibatches\n    minibatches = []\n    start = 0\n    while True:\n        _, info = sorted_data[start]\n        ilen = int(info[ikey][iaxis][""shape""][0])\n        olen = (\n            int(info[okey][oaxis][""shape""][0])\n            if oaxis >= 0\n            else max(map(lambda x: int(x[""shape""][0]), info[okey]))\n        )\n        factor = max(int(ilen / max_length_in), int(olen / max_length_out))\n        # change batchsize depending on the input and output length\n        # if ilen = 1000 and max_length_in = 800\n        # then b = batchsize / 2\n        # and max(min_batches, .) avoids batchsize = 0\n        bs = max(min_batch_size, int(batch_size / (1 + factor)))\n        end = min(len(sorted_data), start + bs)\n        minibatch = sorted_data[start:end]\n        if shortest_first:\n            minibatch.reverse()\n\n        # check each batch is more than minimum batchsize\n        if len(minibatch) < min_batch_size:\n            mod = min_batch_size - len(minibatch) % min_batch_size\n            additional_minibatch = [\n                sorted_data[i] for i in np.random.randint(0, start, mod)\n            ]\n            if shortest_first:\n                additional_minibatch.reverse()\n            minibatch.extend(additional_minibatch)\n        minibatches.append(minibatch)\n\n        if end == len(sorted_data):\n            break\n        start = end\n\n    # batch: List[List[Tuple[str, dict]]]\n    return minibatches\n\n\ndef batchfy_by_bin(\n    sorted_data,\n    batch_bins,\n    num_batches=0,\n    min_batch_size=1,\n    shortest_first=False,\n    ikey=""input"",\n    okey=""output"",\n):\n    """"""Make variably sized batch set, which maximizes\n\n    the number of bins up to `batch_bins`.\n\n    :param Dict[str, Dict[str, Any]] sorted_data: dictionary loaded from data.json\n    :param int batch_bins: Maximum frames of a batch\n    :param int num_batches: # number of batches to use (for debug)\n    :param int min_batch_size: minimum batch size (for multi-gpu)\n    :param int test: Return only every `test` batches\n    :param bool shortest_first: Sort from batch with shortest samples\n        to longest if true, otherwise reverse\n\n    :param str ikey: key to access input (for ASR ikey=""input"", for TTS ikey=""output"".)\n    :param str okey: key to access output (for ASR okey=""output"". for TTS okey=""input"".)\n\n    :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches\n    """"""\n    if batch_bins <= 0:\n        raise ValueError(f""invalid batch_bins={batch_bins}"")\n    length = len(sorted_data)\n    idim = int(sorted_data[0][1][ikey][0][""shape""][1])\n    odim = int(sorted_data[0][1][okey][0][""shape""][1])\n    logging.info(""# utts: "" + str(len(sorted_data)))\n    minibatches = []\n    start = 0\n    n = 0\n    while True:\n        # Dynamic batch size depending on size of samples\n        b = 0\n        next_size = 0\n        max_olen = 0\n        while next_size < batch_bins and (start + b) < length:\n            ilen = int(sorted_data[start + b][1][ikey][0][""shape""][0]) * idim\n            olen = int(sorted_data[start + b][1][okey][0][""shape""][0]) * odim\n            if olen > max_olen:\n                max_olen = olen\n            next_size = (max_olen + ilen) * (b + 1)\n            if next_size <= batch_bins:\n                b += 1\n            elif next_size == 0:\n                raise ValueError(\n                    f""Can\'t fit one sample in batch_bins ({batch_bins}): ""\n                    f""Please increase the value""\n                )\n        end = min(length, start + max(min_batch_size, b))\n        batch = sorted_data[start:end]\n        if shortest_first:\n            batch.reverse()\n        minibatches.append(batch)\n        # Check for min_batch_size and fixes the batches if needed\n        i = -1\n        while len(minibatches[i]) < min_batch_size:\n            missing = min_batch_size - len(minibatches[i])\n            if -i == len(minibatches):\n                minibatches[i + 1].extend(minibatches[i])\n                minibatches = minibatches[1:]\n                break\n            else:\n                minibatches[i].extend(minibatches[i - 1][:missing])\n                minibatches[i - 1] = minibatches[i - 1][missing:]\n                i -= 1\n        if end == length:\n            break\n        start = end\n        n += 1\n    if num_batches > 0:\n        minibatches = minibatches[:num_batches]\n    lengths = [len(x) for x in minibatches]\n    logging.info(\n        str(len(minibatches))\n        + "" batches containing from ""\n        + str(min(lengths))\n        + "" to ""\n        + str(max(lengths))\n        + "" samples ""\n        + ""(avg ""\n        + str(int(np.mean(lengths)))\n        + "" samples).""\n    )\n    return minibatches\n\n\ndef batchfy_by_frame(\n    sorted_data,\n    max_frames_in,\n    max_frames_out,\n    max_frames_inout,\n    num_batches=0,\n    min_batch_size=1,\n    shortest_first=False,\n    ikey=""input"",\n    okey=""output"",\n):\n    """"""Make variable batch set, which maximizes the number of frames to max_batch_frame.\n\n    :param Dict[str, Dict[str, Any]] sorteddata: dictionary loaded from data.json\n    :param int max_frames_in: Maximum input frames of a batch\n    :param int max_frames_out: Maximum output frames of a batch\n    :param int max_frames_inout: Maximum input+output frames of a batch\n    :param int num_batches: # number of batches to use (for debug)\n    :param int min_batch_size: minimum batch size (for multi-gpu)\n    :param int test: Return only every `test` batches\n    :param bool shortest_first: Sort from batch with shortest samples\n        to longest if true, otherwise reverse\n\n    :param str ikey: key to access input (for ASR ikey=""input"", for TTS ikey=""output"".)\n    :param str okey: key to access output (for ASR okey=""output"". for TTS okey=""input"".)\n\n    :return: List[Tuple[str, Dict[str, List[Dict[str, Any]]]] list of batches\n    """"""\n    if max_frames_in <= 0 and max_frames_out <= 0 and max_frames_inout <= 0:\n        raise ValueError(\n            ""At least, one of `--batch-frames-in`, `--batch-frames-out` or ""\n            ""`--batch-frames-inout` should be > 0""\n        )\n    length = len(sorted_data)\n    minibatches = []\n    start = 0\n    end = 0\n    while end != length:\n        # Dynamic batch size depending on size of samples\n        b = 0\n        max_olen = 0\n        max_ilen = 0\n        while (start + b) < length:\n            ilen = int(sorted_data[start + b][1][ikey][0][""shape""][0])\n            if ilen > max_frames_in and max_frames_in != 0:\n                raise ValueError(\n                    f""Can\'t fit one sample in --batch-frames-in ({max_frames_in}): ""\n                    f""Please increase the value""\n                )\n            olen = int(sorted_data[start + b][1][okey][0][""shape""][0])\n            if olen > max_frames_out and max_frames_out != 0:\n                raise ValueError(\n                    f""Can\'t fit one sample in --batch-frames-out ({max_frames_out}): ""\n                    f""Please increase the value""\n                )\n            if ilen + olen > max_frames_inout and max_frames_inout != 0:\n                raise ValueError(\n                    f""Can\'t fit one sample in --batch-frames-out ({max_frames_inout}): ""\n                    f""Please increase the value""\n                )\n            max_olen = max(max_olen, olen)\n            max_ilen = max(max_ilen, ilen)\n            in_ok = max_ilen * (b + 1) <= max_frames_in or max_frames_in == 0\n            out_ok = max_olen * (b + 1) <= max_frames_out or max_frames_out == 0\n            inout_ok = (max_ilen + max_olen) * (\n                b + 1\n            ) <= max_frames_inout or max_frames_inout == 0\n            if in_ok and out_ok and inout_ok:\n                # add more seq in the minibatch\n                b += 1\n            else:\n                # no more seq in the minibatch\n                break\n        end = min(length, start + b)\n        batch = sorted_data[start:end]\n        if shortest_first:\n            batch.reverse()\n        minibatches.append(batch)\n        # Check for min_batch_size and fixes the batches if needed\n        i = -1\n        while len(minibatches[i]) < min_batch_size:\n            missing = min_batch_size - len(minibatches[i])\n            if -i == len(minibatches):\n                minibatches[i + 1].extend(minibatches[i])\n                minibatches = minibatches[1:]\n                break\n            else:\n                minibatches[i].extend(minibatches[i - 1][:missing])\n                minibatches[i - 1] = minibatches[i - 1][missing:]\n                i -= 1\n        start = end\n    if num_batches > 0:\n        minibatches = minibatches[:num_batches]\n    lengths = [len(x) for x in minibatches]\n    logging.info(\n        str(len(minibatches))\n        + "" batches containing from ""\n        + str(min(lengths))\n        + "" to ""\n        + str(max(lengths))\n        + "" samples""\n        + ""(avg ""\n        + str(int(np.mean(lengths)))\n        + "" samples).""\n    )\n\n    return minibatches\n\n\ndef batchfy_shuffle(data, batch_size, min_batch_size, num_batches, shortest_first):\n    import random\n\n    logging.info(""use shuffled batch."")\n    sorted_data = random.sample(data.items(), len(data.items()))\n    logging.info(""# utts: "" + str(len(sorted_data)))\n    # make list of minibatches\n    minibatches = []\n    start = 0\n    while True:\n        end = min(len(sorted_data), start + batch_size)\n        # check each batch is more than minimum batchsize\n        minibatch = sorted_data[start:end]\n        if shortest_first:\n            minibatch.reverse()\n        if len(minibatch) < min_batch_size:\n            mod = min_batch_size - len(minibatch) % min_batch_size\n            additional_minibatch = [\n                sorted_data[i] for i in np.random.randint(0, start, mod)\n            ]\n            if shortest_first:\n                additional_minibatch.reverse()\n            minibatch.extend(additional_minibatch)\n        minibatches.append(minibatch)\n        if end == len(sorted_data):\n            break\n        start = end\n\n    # for debugging\n    if num_batches > 0:\n        minibatches = minibatches[:num_batches]\n        logging.info(""# minibatches: "" + str(len(minibatches)))\n    return minibatches\n\n\nBATCH_COUNT_CHOICES = [""auto"", ""seq"", ""bin"", ""frame""]\nBATCH_SORT_KEY_CHOICES = [""input"", ""output"", ""shuffle""]\n\n\ndef make_batchset(\n    data,\n    batch_size=0,\n    max_length_in=float(""inf""),\n    max_length_out=float(""inf""),\n    num_batches=0,\n    min_batch_size=1,\n    shortest_first=False,\n    batch_sort_key=""input"",\n    swap_io=False,\n    mt=False,\n    count=""auto"",\n    batch_bins=0,\n    batch_frames_in=0,\n    batch_frames_out=0,\n    batch_frames_inout=0,\n    iaxis=0,\n    oaxis=0,\n):\n    """"""Make batch set from json dictionary\n\n    if utts have ""category"" value,\n\n        >>> data = {\'utt1\': {\'category\': \'A\', \'input\': ...},\n        ...         \'utt2\': {\'category\': \'B\', \'input\': ...},\n        ...         \'utt3\': {\'category\': \'B\', \'input\': ...},\n        ...         \'utt4\': {\'category\': \'A\', \'input\': ...}}\n        >>> make_batchset(data, batchsize=2, ...)\n        [[(\'utt1\', ...), (\'utt4\', ...)], [(\'utt2\', ...), (\'utt3\': ...)]]\n\n    Note that if any utts doesn\'t have ""category"",\n    perform as same as batchfy_by_{count}\n\n    :param Dict[str, Dict[str, Any]] data: dictionary loaded from data.json\n    :param int batch_size: maximum number of sequences in a minibatch.\n    :param int batch_bins: maximum number of bins (frames x dim) in a minibatch.\n    :param int batch_frames_in:  maximum number of input frames in a minibatch.\n    :param int batch_frames_out: maximum number of output frames in a minibatch.\n    :param int batch_frames_out: maximum number of input+output frames in a minibatch.\n    :param str count: strategy to count maximum size of batch.\n        For choices, see espnet.asr.batchfy.BATCH_COUNT_CHOICES\n\n    :param int max_length_in: maximum length of input to decide adaptive batch size\n    :param int max_length_out: maximum length of output to decide adaptive batch size\n    :param int num_batches: # number of batches to use (for debug)\n    :param int min_batch_size: minimum batch size (for multi-gpu)\n    :param bool shortest_first: Sort from batch with shortest samples\n        to longest if true, otherwise reverse\n    :param str batch_sort_key: how to sort data before creating minibatches\n        [""input"", ""output"", ""shuffle""]\n    :param bool swap_io: if True, use ""input"" as output and ""output""\n        as input in `data` dict\n    :param bool mt: if True, use 0-axis of ""output"" as output and 1-axis of ""output""\n        as input in `data` dict\n    :param int iaxis: dimension to access input\n        (for ASR, TTS iaxis=0, for MT iaxis=""1"".)\n    :param int oaxis: dimension to access output (for ASR, TTS, MT oaxis=0,\n        reserved for future research, -1 means all axis.)\n    :return: List[List[Tuple[str, dict]]] list of batches\n    """"""\n\n    # check args\n    if count not in BATCH_COUNT_CHOICES:\n        raise ValueError(\n            f""arg \'count\' ({count}) should be one of {BATCH_COUNT_CHOICES}""\n        )\n    if batch_sort_key not in BATCH_SORT_KEY_CHOICES:\n        raise ValueError(\n            f""arg \'batch_sort_key\' ({batch_sort_key}) should be ""\n            f""one of {BATCH_SORT_KEY_CHOICES}""\n        )\n\n    # TODO(karita): remove this by creating converter from ASR to TTS json format\n    batch_sort_axis = 0\n    if swap_io:\n        # for TTS\n        ikey = ""output""\n        okey = ""input""\n        if batch_sort_key == ""input"":\n            batch_sort_key = ""output""\n        elif batch_sort_key == ""output"":\n            batch_sort_key = ""input""\n    elif mt:\n        # for MT\n        ikey = ""output""\n        okey = ""output""\n        batch_sort_key = ""output""\n        batch_sort_axis = 1\n        assert iaxis == 1\n        assert oaxis == 0\n        # NOTE: input is json[\'output\'][1] and output is json[\'output\'][0]\n    else:\n        ikey = ""input""\n        okey = ""output""\n\n    if count == ""auto"":\n        if batch_size != 0:\n            count = ""seq""\n        elif batch_bins != 0:\n            count = ""bin""\n        elif batch_frames_in != 0 or batch_frames_out != 0 or batch_frames_inout != 0:\n            count = ""frame""\n        else:\n            raise ValueError(\n                f""cannot detect `count` manually set one of {BATCH_COUNT_CHOICES}""\n            )\n        logging.info(f""count is auto detected as {count}"")\n\n    if count != ""seq"" and batch_sort_key == ""shuffle"":\n        raise ValueError(""batch_sort_key=shuffle is only available if batch_count=seq"")\n\n    category2data = {}  # Dict[str, dict]\n    for k, v in data.items():\n        category2data.setdefault(v.get(""category""), {})[k] = v\n\n    batches_list = []  # List[List[List[Tuple[str, dict]]]]\n    for d in category2data.values():\n        if batch_sort_key == ""shuffle"":\n            batches = batchfy_shuffle(\n                d, batch_size, min_batch_size, num_batches, shortest_first\n            )\n            batches_list.append(batches)\n            continue\n\n        # sort it by input lengths (long to short)\n        sorted_data = sorted(\n            d.items(),\n            key=lambda data: int(data[1][batch_sort_key][batch_sort_axis][""shape""][0]),\n            reverse=not shortest_first,\n        )\n        logging.info(""# utts: "" + str(len(sorted_data)))\n        if count == ""seq"":\n            batches = batchfy_by_seq(\n                sorted_data,\n                batch_size=batch_size,\n                max_length_in=max_length_in,\n                max_length_out=max_length_out,\n                min_batch_size=min_batch_size,\n                shortest_first=shortest_first,\n                ikey=ikey,\n                iaxis=iaxis,\n                okey=okey,\n                oaxis=oaxis,\n            )\n        if count == ""bin"":\n            batches = batchfy_by_bin(\n                sorted_data,\n                batch_bins=batch_bins,\n                min_batch_size=min_batch_size,\n                shortest_first=shortest_first,\n                ikey=ikey,\n                okey=okey,\n            )\n        if count == ""frame"":\n            batches = batchfy_by_frame(\n                sorted_data,\n                max_frames_in=batch_frames_in,\n                max_frames_out=batch_frames_out,\n                max_frames_inout=batch_frames_inout,\n                min_batch_size=min_batch_size,\n                shortest_first=shortest_first,\n                ikey=ikey,\n                okey=okey,\n            )\n        batches_list.append(batches)\n\n    if len(batches_list) == 1:\n        batches = batches_list[0]\n    else:\n        # Concat list. This way is faster than ""sum(batch_list, [])""\n        batches = list(itertools.chain(*batches_list))\n\n    # for debugging\n    if num_batches > 0:\n        batches = batches[:num_batches]\n    logging.info(""# minibatches: "" + str(len(batches)))\n\n    # batch: List[List[Tuple[str, dict]]]\n    return batches\n'"
espnet/utils/training/evaluator.py,0,"b'from chainer.training.extensions import Evaluator\n\nfrom espnet.utils.training.tensorboard_logger import TensorboardLogger\n\n\nclass BaseEvaluator(Evaluator):\n    """"""Base Evaluator in ESPnet""""""\n\n    def __call__(self, trainer=None):\n        ret = super().__call__(trainer)\n        try:\n            if trainer is not None:\n                # force tensorboard to report evaluation log\n                tb_logger = trainer.get_extension(TensorboardLogger.default_name)\n                tb_logger(trainer)\n        except ValueError:\n            pass\n        return ret\n'"
espnet/utils/training/iterators.py,2,"b'import chainer\nfrom chainer.iterators import MultiprocessIterator\nfrom chainer.iterators import SerialIterator\nfrom chainer.iterators import ShuffleOrderSampler\nfrom chainer.training.extension import Extension\n\nimport numpy as np\n\n\nclass ShufflingEnabler(Extension):\n    """"""An extension enabling shuffling on an Iterator""""""\n\n    def __init__(self, iterators):\n        """"""Inits the ShufflingEnabler\n\n        :param list[Iterator] iterators: The iterators to enable shuffling on\n        """"""\n        self.set = False\n        self.iterators = iterators\n\n    def __call__(self, trainer):\n        """"""Calls the enabler on the given iterator\n\n        :param trainer: The iterator\n        """"""\n        if not self.set:\n            for iterator in self.iterators:\n                iterator.start_shuffle()\n            self.set = True\n\n\nclass ToggleableShufflingSerialIterator(SerialIterator):\n    """"""A SerialIterator having its shuffling property activated during training""""""\n\n    def __init__(self, dataset, batch_size, repeat=True, shuffle=True):\n        """"""Init the Iterator\n\n        :param torch.nn.Tensor dataset: The dataset to take batches from\n        :param int batch_size: The batch size\n        :param bool repeat: Whether to repeat data (allow multiple epochs)\n        :param bool shuffle: Whether to shuffle the batches\n        """"""\n        super(ToggleableShufflingSerialIterator, self).__init__(\n            dataset, batch_size, repeat, shuffle\n        )\n\n    def start_shuffle(self):\n        """"""Starts shuffling (or reshuffles) the batches""""""\n        self._shuffle = True\n        if int(chainer._version.__version__[0]) <= 4:\n            self._order = np.random.permutation(len(self.dataset))\n        else:\n            self.order_sampler = ShuffleOrderSampler()\n            self._order = self.order_sampler(np.arange(len(self.dataset)), 0)\n\n\nclass ToggleableShufflingMultiprocessIterator(MultiprocessIterator):\n    """"""A MultiprocessIterator having its shuffling property activated during training""""""\n\n    def __init__(\n        self,\n        dataset,\n        batch_size,\n        repeat=True,\n        shuffle=True,\n        n_processes=None,\n        n_prefetch=1,\n        shared_mem=None,\n        maxtasksperchild=20,\n    ):\n        """"""Init the iterator\n\n        :param torch.nn.Tensor dataset: The dataset to take batches from\n        :param int batch_size: The batch size\n        :param bool repeat: Whether to repeat batches or not (enables multiple epochs)\n        :param bool shuffle: Whether to shuffle the order of the batches\n        :param int n_processes: How many processes to use\n        :param int n_prefetch: The number of prefetch to use\n        :param int shared_mem: How many memory to share between processes\n        :param int maxtasksperchild: Maximum number of tasks per child\n        """"""\n        super(ToggleableShufflingMultiprocessIterator, self).__init__(\n            dataset=dataset,\n            batch_size=batch_size,\n            repeat=repeat,\n            shuffle=shuffle,\n            n_processes=n_processes,\n            n_prefetch=n_prefetch,\n            shared_mem=shared_mem,\n            maxtasksperchild=maxtasksperchild,\n        )\n\n    def start_shuffle(self):\n        """"""Starts shuffling (or reshuffles) the batches""""""\n        self.shuffle = True\n        if int(chainer._version.__version__[0]) <= 4:\n            self._order = np.random.permutation(len(self.dataset))\n        else:\n            self.order_sampler = ShuffleOrderSampler()\n            self._order = self.order_sampler(np.arange(len(self.dataset)), 0)\n        self._set_prefetch_state()\n'"
espnet/utils/training/tensorboard_logger.py,0,"b'from chainer.training.extension import Extension\n\n\nclass TensorboardLogger(Extension):\n    """"""A tensorboard logger extension""""""\n\n    default_name = ""espnet_tensorboard_logger""\n\n    def __init__(self, logger, att_reporter=None, entries=None, epoch=0):\n        """"""Init the extension\n\n        :param SummaryWriter logger: The logger to use\n        :param PlotAttentionReporter att_reporter: The (optional) PlotAttentionReporter\n        :param entries: The entries to watch\n        :param int epoch: The starting epoch\n        """"""\n        self._entries = entries\n        self._att_reporter = att_reporter\n        self._logger = logger\n        self._epoch = epoch\n\n    def __call__(self, trainer):\n        """"""Updates the events file with the new values\n\n        :param trainer: The trainer\n        """"""\n        observation = trainer.observation\n        for k, v in observation.items():\n            if (self._entries is not None) and (k not in self._entries):\n                continue\n            if k is not None and v is not None:\n                if ""cupy"" in str(type(v)):\n                    v = v.get()\n                if ""cupy"" in str(type(k)):\n                    k = k.get()\n                self._logger.add_scalar(k, v, trainer.updater.iteration)\n        if (\n            self._att_reporter is not None\n            and trainer.updater.get_iterator(""main"").epoch > self._epoch\n        ):\n            self._epoch = trainer.updater.get_iterator(""main"").epoch\n            self._att_reporter.log_attentions(self._logger, trainer.updater.iteration)\n'"
espnet/utils/training/train_utils.py,0,"b'import chainer\nimport logging\n\n\ndef check_early_stop(trainer, epochs):\n    """"""Checks an early stopping trigger and warns the user if it\'s the case\n\n    :param trainer: The trainer used for training\n    :param epochs: The maximum number of epochs\n    """"""\n    end_epoch = trainer.updater.get_iterator(""main"").epoch\n    if end_epoch < (epochs - 1):\n        logging.warning(\n            ""Hit early stop at epoch ""\n            + str(end_epoch)\n            + ""\\nYou can change the patience or set it to 0 to run all epochs""\n        )\n\n\ndef set_early_stop(trainer, args, is_lm=False):\n    """"""Sets the early stop trigger given the program arguments\n\n    :param trainer: The trainer used for training\n    :param args: The program arguments\n    :param is_lm: If the trainer is for a LM (epoch instead of epochs)\n    """"""\n    patience = args.patience\n    criterion = args.early_stop_criterion\n    epochs = args.epoch if is_lm else args.epochs\n    mode = ""max"" if ""acc"" in criterion else ""min""\n    if patience > 0:\n        trainer.stop_trigger = chainer.training.triggers.EarlyStoppingTrigger(\n            monitor=criterion,\n            mode=mode,\n            patients=patience,\n            max_trigger=(epochs, ""epoch""),\n        )\n'"
espnet2/asr/decoder/__init__.py,0,b''
espnet2/asr/decoder/abs_decoder.py,6,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Tuple\n\nimport torch\n\nfrom espnet.nets.scorer_interface import ScorerInterface\n\n\nclass AbsDecoder(torch.nn.Module, ScorerInterface, ABC):\n    @abstractmethod\n    def forward(\n        self,\n        hs_pad: torch.Tensor,\n        hlens: torch.Tensor,\n        ys_in_pad: torch.Tensor,\n        ys_in_lens: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        raise NotImplementedError\n'"
espnet2/asr/decoder/rnn_decoder.py,26,"b'import random\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.pytorch_backend.rnn.attentions import initial_att\nfrom espnet2.asr.decoder.abs_decoder import AbsDecoder\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\n\n\ndef build_attention_list(\n    eprojs: int,\n    dunits: int,\n    atype: str = ""location"",\n    num_att: int = 1,\n    num_encs: int = 1,\n    aheads: int = 4,\n    adim: int = 320,\n    awin: int = 5,\n    aconv_chans: int = 10,\n    aconv_filts: int = 100,\n    han_mode: bool = False,\n    han_type=None,\n    han_heads: int = 4,\n    han_dim: int = 320,\n    han_conv_chans: int = -1,\n    han_conv_filts: int = 100,\n    han_win: int = 5,\n):\n\n    att_list = torch.nn.ModuleList()\n    if num_encs == 1:\n        for i in range(num_att):\n            att = initial_att(\n                atype, eprojs, dunits, aheads, adim, awin, aconv_chans, aconv_filts,\n            )\n            att_list.append(att)\n    elif num_encs > 1:  # no multi-speaker mode\n        if han_mode:\n            att = initial_att(\n                han_type,\n                eprojs,\n                dunits,\n                han_heads,\n                han_dim,\n                han_win,\n                han_conv_chans,\n                han_conv_filts,\n                han_mode=True,\n            )\n            return att\n        else:\n            att_list = torch.nn.ModuleList()\n            for idx in range(num_encs):\n                att = initial_att(\n                    atype[idx],\n                    eprojs,\n                    dunits,\n                    aheads[idx],\n                    adim[idx],\n                    awin[idx],\n                    aconv_chans[idx],\n                    aconv_filts[idx],\n                )\n                att_list.append(att)\n    else:\n        raise ValueError(\n            ""Number of encoders needs to be more than one. {}"".format(num_encs)\n        )\n    return att_list\n\n\nclass RNNDecoder(AbsDecoder):\n    def __init__(\n        self,\n        vocab_size: int,\n        encoder_output_size: int,\n        rnn_type: str = ""lstm"",\n        num_layers: int = 1,\n        hidden_size: int = 320,\n        sampling_probability: float = 0.0,\n        dropout: float = 0.0,\n        context_residual: bool = False,\n        replace_sos: bool = False,\n        num_encs: int = 1,\n        att_conf: dict = get_default_kwargs(build_attention_list),\n    ):\n        # FIXME(kamo): The parts of num_spk should be refactored more more more\n        assert check_argument_types()\n        if rnn_type not in {""lstm"", ""gru""}:\n            raise ValueError(f""Not supported: rnn_type={rnn_type}"")\n\n        super().__init__()\n        eprojs = encoder_output_size\n        self.dtype = rnn_type\n        self.dunits = hidden_size\n        self.dlayers = num_layers\n        self.context_residual = context_residual\n        self.sos = vocab_size - 1\n        self.eos = vocab_size - 1\n        self.odim = vocab_size\n        self.sampling_probability = sampling_probability\n        self.dropout = dropout\n        self.num_encs = num_encs\n\n        # for multilingual translation\n        self.replace_sos = replace_sos\n\n        self.embed = torch.nn.Embedding(vocab_size, hidden_size)\n        self.dropout_emb = torch.nn.Dropout(p=dropout)\n\n        self.decoder = torch.nn.ModuleList()\n        self.dropout_dec = torch.nn.ModuleList()\n        self.decoder += [\n            torch.nn.LSTMCell(hidden_size + eprojs, hidden_size)\n            if self.dtype == ""lstm""\n            else torch.nn.GRUCell(hidden_size + eprojs, hidden_size)\n        ]\n        self.dropout_dec += [torch.nn.Dropout(p=dropout)]\n        for _ in range(1, self.dlayers):\n            self.decoder += [\n                torch.nn.LSTMCell(hidden_size, hidden_size)\n                if self.dtype == ""lstm""\n                else torch.nn.GRUCell(hidden_size, hidden_size)\n            ]\n            self.dropout_dec += [torch.nn.Dropout(p=dropout)]\n            # NOTE: dropout is applied only for the vertical connections\n            # see https://arxiv.org/pdf/1409.2329.pdf\n\n        if context_residual:\n            self.output = torch.nn.Linear(hidden_size + eprojs, vocab_size)\n        else:\n            self.output = torch.nn.Linear(hidden_size, vocab_size)\n\n        self.att_list = build_attention_list(\n            eprojs=eprojs, dunits=hidden_size, **att_conf\n        )\n\n    def zero_state(self, hs_pad):\n        return hs_pad.new_zeros(hs_pad.size(0), self.dunits)\n\n    def rnn_forward(self, ey, z_list, c_list, z_prev, c_prev):\n        if self.dtype == ""lstm"":\n            z_list[0], c_list[0] = self.decoder[0](ey, (z_prev[0], c_prev[0]))\n            for i in range(1, self.dlayers):\n                z_list[i], c_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), (z_prev[i], c_prev[i]),\n                )\n        else:\n            z_list[0] = self.decoder[0](ey, z_prev[0])\n            for i in range(1, self.dlayers):\n                z_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), z_prev[i]\n                )\n        return z_list, c_list\n\n    def forward(self, hs_pad, hlens, ys_in_pad, ys_in_lens, strm_idx=0):\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            hs_pad = [hs_pad]\n            hlens = [hlens]\n\n        # attention index for the attention module\n        # in SPA (speaker parallel attention),\n        # att_idx is used to select attention module. In other cases, it is 0.\n        att_idx = min(strm_idx, len(self.att_list) - 1)\n\n        # hlens should be list of list of integer\n        hlens = [list(map(int, hlens[idx])) for idx in range(self.num_encs)]\n\n        # get dim, length info\n        olength = ys_in_pad.size(1)\n\n        # initialization\n        c_list = [self.zero_state(hs_pad[0])]\n        z_list = [self.zero_state(hs_pad[0])]\n        for _ in range(1, self.dlayers):\n            c_list.append(self.zero_state(hs_pad[0]))\n            z_list.append(self.zero_state(hs_pad[0]))\n        z_all = []\n        if self.num_encs == 1:\n            att_w = None\n            self.att_list[att_idx].reset()  # reset pre-computation of h\n        else:\n            att_w_list = [None] * (self.num_encs + 1)  # atts + han\n            att_c_list = [None] * self.num_encs  # atts\n            for idx in range(self.num_encs + 1):\n                # reset pre-computation of h in atts and han\n                self.att_list[idx].reset()\n\n        # pre-computation of embedding\n        eys = self.dropout_emb(self.embed(ys_in_pad))  # utt x olen x zdim\n\n        # loop for an output sequence\n        for i in range(olength):\n            if self.num_encs == 1:\n                att_c, att_w = self.att_list[att_idx](\n                    hs_pad[0], hlens[0], self.dropout_dec[0](z_list[0]), att_w\n                )\n            else:\n                for idx in range(self.num_encs):\n                    att_c_list[idx], att_w_list[idx] = self.att_list[idx](\n                        hs_pad[idx],\n                        hlens[idx],\n                        self.dropout_dec[0](z_list[0]),\n                        att_w_list[idx],\n                    )\n                hs_pad_han = torch.stack(att_c_list, dim=1)\n                hlens_han = [self.num_encs] * len(ys_in_pad)\n                att_c, att_w_list[self.num_encs] = self.att_list[self.num_encs](\n                    hs_pad_han,\n                    hlens_han,\n                    self.dropout_dec[0](z_list[0]),\n                    att_w_list[self.num_encs],\n                )\n            if i > 0 and random.random() < self.sampling_probability:\n                z_out = self.output(z_all[-1])\n                z_out = np.argmax(z_out.detach().cpu(), axis=1)\n                z_out = self.dropout_emb(self.embed(to_device(self, z_out)))\n                ey = torch.cat((z_out, att_c), dim=1)  # utt x (zdim + hdim)\n            else:\n                # utt x (zdim + hdim)\n                ey = torch.cat((eys[:, i, :], att_c), dim=1)\n            z_list, c_list = self.rnn_forward(ey, z_list, c_list, z_list, c_list)\n            if self.context_residual:\n                z_all.append(\n                    torch.cat((self.dropout_dec[-1](z_list[-1]), att_c), dim=-1)\n                )  # utt x (zdim + hdim)\n            else:\n                z_all.append(self.dropout_dec[-1](z_list[-1]))  # utt x (zdim)\n\n        z_all = torch.stack(z_all, dim=1)\n        z_all = self.output(z_all)\n        z_all.masked_fill_(\n            make_pad_mask(ys_in_lens, z_all, 1), 0,\n        )\n        return z_all, ys_in_lens\n\n    def init_state(self, x):\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            x = [x]\n\n        c_list = [self.zero_state(x[0].unsqueeze(0))]\n        z_list = [self.zero_state(x[0].unsqueeze(0))]\n        for _ in range(1, self.dlayers):\n            c_list.append(self.zero_state(x[0].unsqueeze(0)))\n            z_list.append(self.zero_state(x[0].unsqueeze(0)))\n        # TODO(karita): support strm_index for `asr_mix`\n        strm_index = 0\n        att_idx = min(strm_index, len(self.att_list) - 1)\n        if self.num_encs == 1:\n            a = None\n            self.att_list[att_idx].reset()  # reset pre-computation of h\n        else:\n            a = [None] * (self.num_encs + 1)  # atts + han\n            for idx in range(self.num_encs + 1):\n                # reset pre-computation of h in atts and han\n                self.att_list[idx].reset()\n        return dict(\n            c_prev=c_list[:],\n            z_prev=z_list[:],\n            a_prev=a,\n            workspace=(att_idx, z_list, c_list),\n        )\n\n    def score(self, yseq, state, x):\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            x = [x]\n\n        att_idx, z_list, c_list = state[""workspace""]\n        vy = yseq[-1].unsqueeze(0)\n        ey = self.dropout_emb(self.embed(vy))  # utt list (1) x zdim\n        if self.num_encs == 1:\n            att_c, att_w = self.att_list[att_idx](\n                x[0].unsqueeze(0),\n                [x[0].size(0)],\n                self.dropout_dec[0](state[""z_prev""][0]),\n                state[""a_prev""],\n            )\n        else:\n            att_w = [None] * (self.num_encs + 1)  # atts + han\n            att_c_list = [None] * self.num_encs  # atts\n            for idx in range(self.num_encs):\n                att_c_list[idx], att_w[idx] = self.att_list[idx](\n                    x[idx].unsqueeze(0),\n                    [x[idx].size(0)],\n                    self.dropout_dec[0](state[""z_prev""][0]),\n                    state[""a_prev""][idx],\n                )\n            h_han = torch.stack(att_c_list, dim=1)\n            att_c, att_w[self.num_encs] = self.att_list[self.num_encs](\n                h_han,\n                [self.num_encs],\n                self.dropout_dec[0](state[""z_prev""][0]),\n                state[""a_prev""][self.num_encs],\n            )\n        ey = torch.cat((ey, att_c), dim=1)  # utt(1) x (zdim + hdim)\n        z_list, c_list = self.rnn_forward(\n            ey, z_list, c_list, state[""z_prev""], state[""c_prev""]\n        )\n        if self.context_residual:\n            logits = self.output(\n                torch.cat((self.dropout_dec[-1](z_list[-1]), att_c), dim=-1)\n            )\n        else:\n            logits = self.output(self.dropout_dec[-1](z_list[-1]))\n        logp = F.log_softmax(logits, dim=1).squeeze(0)\n        return (\n            logp,\n            dict(\n                c_prev=c_list[:],\n                z_prev=z_list[:],\n                a_prev=att_w,\n                workspace=(att_idx, z_list, c_list),\n            ),\n        )\n'"
espnet2/asr/decoder/transformer_decoder.py,21,"b'# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Decoder definition.""""""\nfrom typing import List\nfrom typing import Tuple\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.decoder_layer import DecoderLayer\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.repeat import repeat\nfrom espnet2.asr.decoder.abs_decoder import AbsDecoder\n\n\nclass TransformerDecoder(AbsDecoder):\n    """"""Transfomer decoder module.\n\n    Args:\n        vocab_size: output dim\n        encoder_output_size: dimension of attention\n        attention_heads: the number of heads of multi head attention\n        linear_units: the number of units of position-wise feed forward\n        num_blocks: the number of decoder blocks\n        dropout_rate: dropout rate\n        self_attention_dropout_rate: dropout rate for attention\n        input_layer: input layer type\n        use_output_layer: whether to use output layer\n        pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n        normalize_before: whether to use layer_norm before the first block\n        concat_after: whether to concat attention layer\'s input and output\n            if True, additional linear will be applied.\n            i.e. x -> x + linear(concat(x, att(x)))\n            if False, no additional linear will be applied.\n            i.e. x -> x + att(x)\n    """"""\n\n    def __init__(\n        self,\n        vocab_size: int,\n        encoder_output_size: int,\n        attention_heads: int = 4,\n        linear_units: int = 2048,\n        num_blocks: int = 6,\n        dropout_rate: float = 0.1,\n        positional_dropout_rate: float = 0.1,\n        self_attention_dropout_rate: float = 0.0,\n        src_attention_dropout_rate: float = 0.0,\n        input_layer: str = ""embed"",\n        use_output_layer: bool = True,\n        pos_enc_class=PositionalEncoding,\n        normalize_before: bool = True,\n        concat_after: bool = False,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        attention_dim = encoder_output_size\n\n        if input_layer == ""embed"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(vocab_size, attention_dim),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer == ""linear"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(vocab_size, attention_dim),\n                torch.nn.LayerNorm(attention_dim),\n                torch.nn.Dropout(dropout_rate),\n                torch.nn.ReLU(),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        else:\n            raise ValueError(f""only \'embed\' or \'linear\' is supported: {input_layer}"")\n\n        self.normalize_before = normalize_before\n        self.decoders = repeat(\n            num_blocks,\n            lambda: DecoderLayer(\n                attention_dim,\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, self_attention_dropout_rate\n                ),\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, src_attention_dropout_rate\n                ),\n                PositionwiseFeedForward(attention_dim, linear_units, dropout_rate),\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n        if use_output_layer:\n            self.output_layer = torch.nn.Linear(attention_dim, vocab_size)\n        else:\n            self.output_layer = None\n\n    def forward(\n        self,\n        hs_pad: torch.Tensor,\n        hlens: torch.Tensor,\n        ys_in_pad: torch.Tensor,\n        ys_in_lens: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Forward decoder.\n\n        Args:\n            hs_pad: encoded memory, float32  (batch, maxlen_in, feat)\n            hlens: (batch)\n            ys_in_pad:\n                input token ids, int64 (batch, maxlen_out)\n                if input_layer == ""embed""\n                input tensor (batch, maxlen_out, #mels) in the other cases\n            ys_in_lens: (batch)\n        Returns:\n            (tuple): tuple containing:\n\n            x: decoded token score before softmax (batch, maxlen_out, token)\n                if use_output_layer is True,\n            olens: (batch, )\n        """"""\n        tgt = ys_in_pad\n        # tgt_mask: (B, 1, L)\n        tgt_mask = (~make_pad_mask(ys_in_lens)[:, None, :]).to(tgt.device)\n        # m: (1, L, L)\n        m = subsequent_mask(tgt_mask.size(-1), device=tgt_mask.device).unsqueeze(0)\n        # tgt_mask: (B, L, L)\n        tgt_mask = tgt_mask & m\n\n        memory = hs_pad\n        memory_mask = (~make_pad_mask(hlens))[:, None, :].to(memory.device)\n\n        x = self.embed(tgt)\n        x, tgt_mask, memory, memory_mask = self.decoders(\n            x, tgt_mask, memory, memory_mask\n        )\n        if self.normalize_before:\n            x = self.after_norm(x)\n        if self.output_layer is not None:\n            x = self.output_layer(x)\n\n        olens = tgt_mask.sum(1)\n        return x, olens\n\n    def forward_one_step(\n        self,\n        tgt: torch.Tensor,\n        tgt_mask: torch.Tensor,\n        memory: torch.Tensor,\n        cache: List[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n        """"""Forward one step.\n\n        Args:\n            tgt: input token ids, int64 (batch, maxlen_out)\n            tgt_mask: input token mask,  (batch, maxlen_out)\n                      dtype=torch.uint8 in PyTorch 1.2-\n                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n            memory: encoded memory, float32  (batch, maxlen_in, feat)\n            cache: cached output list of (batch, max_time_out-1, size)\n        Returns:\n            y, cache: NN output value and cache per `self.decoders`.\n            y.shape` is (batch, maxlen_out, token)\n        """"""\n        x = self.embed(tgt)\n        if cache is None:\n            cache = self.init_state()\n        new_cache = []\n        for c, decoder in zip(cache, self.decoders):\n            x, tgt_mask, memory, memory_mask = decoder(\n                x, tgt_mask, memory, None, cache=c\n            )\n            new_cache.append(x)\n\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if self.output_layer is not None:\n            y = torch.log_softmax(self.output_layer(y), dim=-1)\n\n        return y, new_cache\n\n    # beam search API (see ScorerInterface)\n    def init_state(self, x=None):\n        """"""Get an initial state for decoding.""""""\n        return [None for i in range(len(self.decoders))]\n\n    def score(self, ys, state, x):\n        """"""Score.""""""\n        ys_mask = subsequent_mask(len(ys), device=x.device).unsqueeze(0)\n        logp, state = self.forward_one_step(\n            ys.unsqueeze(0), ys_mask, x.unsqueeze(0), cache=state\n        )\n        return logp.squeeze(0), state\n'"
espnet2/asr/encoder/__init__.py,0,b''
espnet2/asr/encoder/abs_encoder.py,5,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Optional\nfrom typing import Tuple\n\nimport torch\n\n\nclass AbsEncoder(torch.nn.Module, ABC):\n    @abstractmethod\n    def output_size(self) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def forward(\n        self,\n        xs_pad: torch.Tensor,\n        ilens: torch.Tensor,\n        prev_states: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        raise NotImplementedError\n'"
espnet2/asr/encoder/rnn_encoder.py,6,"b'from typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.rnn.encoders import RNN\nfrom espnet.nets.pytorch_backend.rnn.encoders import RNNP\nfrom espnet2.asr.encoder.abs_encoder import AbsEncoder\n\n\nclass RNNEncoder(AbsEncoder):\n    """"""RNNEncoder class.\n\n    Args:\n        input_size: The number of expected features in the input\n        output_size: The number of output features\n        hidden_size: The number of hidden features\n        bidirectional: If ``True`` becomes a bidirectional LSTM\n        use_projection: Use projection layer or not\n        num_layers: Number of recurrent layers\n        dropout: dropout probability\n\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        rnn_type: str = ""lstm"",\n        bidirectional: bool = True,\n        use_projection: bool = True,\n        num_layers: int = 4,\n        hidden_size: int = 320,\n        output_size: int = 320,\n        dropout: float = 0.0,\n        subsample: Optional[Sequence[int]] = (2, 2, 1, 1),\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self._output_size = output_size\n        self.rnn_type = rnn_type\n        self.bidirectional = bidirectional\n        self.use_projection = use_projection\n\n        if rnn_type not in {""lstm"", ""gru""}:\n            raise ValueError(f""Not supported rnn_type={rnn_type}"")\n\n        if subsample is None:\n            subsample = np.ones(num_layers + 1, dtype=np.int)\n        else:\n            subsample = subsample[:num_layers]\n            # Append 1 at the beginning because the second or later is used\n            subsample = np.pad(\n                np.array(subsample, dtype=np.int),\n                [1, num_layers - len(subsample)],\n                mode=""constant"",\n                constant_values=1,\n            )\n\n        rnn_type = (""b"" if bidirectional else """") + rnn_type\n        if use_projection:\n            self.enc = torch.nn.ModuleList(\n                [\n                    RNNP(\n                        input_size,\n                        num_layers,\n                        hidden_size,\n                        output_size,\n                        subsample,\n                        dropout,\n                        typ=rnn_type,\n                    )\n                ]\n            )\n\n        else:\n            self.enc = torch.nn.ModuleList(\n                [\n                    RNN(\n                        input_size,\n                        num_layers,\n                        hidden_size,\n                        output_size,\n                        dropout,\n                        typ=rnn_type,\n                    )\n                ]\n            )\n\n    def output_size(self) -> int:\n        return self._output_size\n\n    def forward(\n        self,\n        xs_pad: torch.Tensor,\n        ilens: torch.Tensor,\n        prev_states: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        if prev_states is None:\n            prev_states = [None] * len(self.enc)\n        assert len(prev_states) == len(self.enc)\n\n        current_states = []\n        for module, prev_state in zip(self.enc, prev_states):\n            xs_pad, ilens, states = module(xs_pad, ilens, prev_state=prev_state)\n            current_states.append(states)\n\n        if self.use_projection:\n            xs_pad.masked_fill_(make_pad_mask(ilens, xs_pad, 1), 0.0)\n        else:\n            xs_pad = xs_pad.masked_fill(make_pad_mask(ilens, xs_pad, 1), 0.0)\n        return xs_pad, ilens, current_states\n'"
espnet2/asr/encoder/transformer_encoder.py,12,"b'# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Encoder definition.""""""\nfrom typing import Optional\nfrom typing import Tuple\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.pytorch_backend.transformer.multi_layer_conv import Conv1dLinear\nfrom espnet.nets.pytorch_backend.transformer.multi_layer_conv import MultiLayeredConv1d\nfrom espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.repeat import repeat\nfrom espnet.nets.pytorch_backend.transformer.subsampling import Conv2dSubsampling\nfrom espnet2.asr.encoder.abs_encoder import AbsEncoder\n\n\nclass TransformerEncoder(AbsEncoder):\n    """"""Transformer encoder module.\n\n    Args:\n        input_size: input dim\n        output_size: dimension of attention\n        attention_heads: the number of heads of multi head attention\n        linear_units: the number of units of position-wise feed forward\n        num_blocks: the number of decoder blocks\n        dropout_rate: dropout rate\n        attention_dropout_rate: dropout rate in attention\n        positional_dropout_rate: dropout rate after adding positional encoding\n        input_layer: input layer type\n        pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n        normalize_before: whether to use layer_norm before the first block\n        concat_after: whether to concat attention layer\'s input and output\n            if True, additional linear will be applied.\n            i.e. x -> x + linear(concat(x, att(x)))\n            if False, no additional linear will be applied.\n            i.e. x -> x + att(x)\n        positionwise_layer_type: linear of conv1d\n        positionwise_conv_kernel_size: kernel size of positionwise conv1d layer\n        padding_idx: padding_idx for input_layer=embed\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int = 256,\n        attention_heads: int = 4,\n        linear_units: int = 2048,\n        num_blocks: int = 6,\n        dropout_rate: float = 0.1,\n        positional_dropout_rate: float = 0.1,\n        attention_dropout_rate: float = 0.0,\n        input_layer: Optional[str] = ""conv2d"",\n        pos_enc_class=PositionalEncoding,\n        normalize_before: bool = True,\n        concat_after: bool = False,\n        positionwise_layer_type: str = ""linear"",\n        positionwise_conv_kernel_size: int = 1,\n        padding_idx: int = -1,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self._output_size = output_size\n\n        if input_layer == ""linear"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(input_size, output_size),\n                torch.nn.LayerNorm(output_size),\n                torch.nn.Dropout(dropout_rate),\n                torch.nn.ReLU(),\n                pos_enc_class(output_size, positional_dropout_rate),\n            )\n        elif input_layer == ""conv2d"":\n            self.embed = Conv2dSubsampling(input_size, output_size, dropout_rate)\n        elif input_layer == ""embed"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(input_size, output_size, padding_idx=padding_idx),\n                pos_enc_class(output_size, positional_dropout_rate),\n            )\n        elif input_layer is None:\n            self.embed = torch.nn.Sequential(\n                pos_enc_class(output_size, positional_dropout_rate)\n            )\n        else:\n            raise ValueError(""unknown input_layer: "" + input_layer)\n        self.normalize_before = normalize_before\n        if positionwise_layer_type == ""linear"":\n            positionwise_layer = PositionwiseFeedForward\n            positionwise_layer_args = (\n                output_size,\n                linear_units,\n                dropout_rate,\n            )\n        elif positionwise_layer_type == ""conv1d"":\n            positionwise_layer = MultiLayeredConv1d\n            positionwise_layer_args = (\n                output_size,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        elif positionwise_layer_type == ""conv1d-linear"":\n            positionwise_layer = Conv1dLinear\n            positionwise_layer_args = (\n                output_size,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        else:\n            raise NotImplementedError(""Support only linear or conv1d."")\n        self.encoders = repeat(\n            num_blocks,\n            lambda: EncoderLayer(\n                output_size,\n                MultiHeadedAttention(\n                    attention_heads, output_size, attention_dropout_rate\n                ),\n                positionwise_layer(*positionwise_layer_args),\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(output_size)\n\n    def output_size(self) -> int:\n        return self._output_size\n\n    def forward(\n        self,\n        xs_pad: torch.Tensor,\n        ilens: torch.Tensor,\n        prev_states: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        """"""Embed positions in tensor.\n\n        Args:\n            xs_pad: input tensor (B, L, D)\n            ilens: input length (B)\n            prev_states: Not to be used now.\n        Returns:\n            position embedded tensor and mask\n        """"""\n        masks = (~make_pad_mask(ilens)[:, None, :]).to(xs_pad.device)\n\n        if isinstance(self.embed, Conv2dSubsampling):\n            xs_pad, masks = self.embed(xs_pad, masks)\n        else:\n            xs_pad = self.embed(xs_pad)\n        xs_pad, masks = self.encoders(xs_pad, masks)\n        if self.normalize_before:\n            xs_pad = self.after_norm(xs_pad)\n\n        olens = masks.squeeze(1).sum(1)\n        return xs_pad, olens, None\n'"
espnet2/asr/encoder/vgg_rnn_encoder.py,6,"b'from typing import Tuple\n\nimport numpy as np\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.e2e_asr_common import get_vgg2l_odim\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.rnn.encoders import RNN\nfrom espnet.nets.pytorch_backend.rnn.encoders import RNNP\nfrom espnet.nets.pytorch_backend.rnn.encoders import VGG2L\nfrom espnet2.asr.encoder.abs_encoder import AbsEncoder\n\n\nclass VGGRNNEncoder(AbsEncoder):\n    """"""VGGRNNEncoder class.\n\n    Args:\n        input_size: The number of expected features in the input\n        bidirectional: If ``True`` becomes a bidirectional LSTM\n        use_projection: Use projection layer or not\n        num_layers: Number of recurrent layers\n        hidden_size: The number of hidden features\n        output_size: The number of output features\n        dropout: dropout probability\n\n    """"""\n\n    def __init__(\n        self,\n        input_size: int,\n        rnn_type: str = ""lstm"",\n        bidirectional: bool = True,\n        use_projection: bool = True,\n        num_layers: int = 4,\n        hidden_size: int = 320,\n        output_size: int = 320,\n        dropout: float = 0.0,\n        in_channel: int = 1,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self._output_size = output_size\n        self.rnn_type = rnn_type\n        self.bidirectional = bidirectional\n        self.use_projection = use_projection\n        if rnn_type not in {""lstm"", ""gru""}:\n            raise ValueError(f""Not supported rnn_type={rnn_type}"")\n\n        # Subsample is not used for VGGRNN\n        subsample = np.ones(num_layers + 1, dtype=np.int)\n        rnn_type = (""b"" if bidirectional else """") + rnn_type\n        if use_projection:\n            self.enc = torch.nn.ModuleList(\n                [\n                    VGG2L(in_channel),\n                    RNNP(\n                        get_vgg2l_odim(input_size, in_channel=in_channel),\n                        num_layers,\n                        hidden_size,\n                        output_size,\n                        subsample,\n                        dropout,\n                        typ=rnn_type,\n                    ),\n                ]\n            )\n\n        else:\n            self.enc = torch.nn.ModuleList(\n                [\n                    VGG2L(in_channel),\n                    RNN(\n                        get_vgg2l_odim(input_size, in_channel=in_channel),\n                        num_layers,\n                        hidden_size,\n                        output_size,\n                        dropout,\n                        typ=rnn_type,\n                    ),\n                ]\n            )\n\n    def output_size(self) -> int:\n        return self._output_size\n\n    def forward(\n        self,\n        xs_pad: torch.Tensor,\n        ilens: torch.Tensor,\n        prev_states: torch.Tensor = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        if prev_states is None:\n            prev_states = [None] * len(self.enc)\n        assert len(prev_states) == len(self.enc)\n\n        current_states = []\n        for module, prev_state in zip(self.enc, prev_states):\n            xs_pad, ilens, states = module(xs_pad, ilens, prev_state=prev_state)\n            current_states.append(states)\n\n        if self.use_projection:\n            xs_pad.masked_fill_(make_pad_mask(ilens, xs_pad, 1), 0.0)\n        else:\n            xs_pad = xs_pad.masked_fill(make_pad_mask(ilens, xs_pad, 1), 0.0)\n        return xs_pad, ilens, current_states\n'"
espnet2/asr/frontend/__init__.py,0,b''
espnet2/asr/frontend/abs_frontend.py,3,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Tuple\n\nimport torch\n\n\nclass AbsFrontend(torch.nn.Module, ABC):\n    @abstractmethod\n    def output_size(self) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        raise NotImplementedError\n'"
espnet2/asr/frontend/default.py,4,"b'import copy\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport humanfriendly\nimport numpy as np\nimport torch\nfrom torch_complex.tensor import ComplexTensor\nfrom typeguard import check_argument_types\n\nfrom espnet.nets.pytorch_backend.frontends.frontend import Frontend\nfrom espnet2.asr.frontend.abs_frontend import AbsFrontend\nfrom espnet2.layers.log_mel import LogMel\nfrom espnet2.layers.stft import Stft\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\n\n\nclass DefaultFrontend(AbsFrontend):\n    """"""Conventional frontend structure for ASR\n\n    Stft -> WPE -> MVDR-Beamformer -> Power-spec -> Mel-Fbank -> CMVN\n    """"""\n\n    def __init__(\n        self,\n        fs: Union[int, str] = 16000,\n        n_fft: int = 512,\n        win_length: int = None,\n        hop_length: int = 128,\n        center: bool = True,\n        pad_mode: str = ""reflect"",\n        normalized: bool = False,\n        onesided: bool = True,\n        n_mels: int = 80,\n        fmin: int = None,\n        fmax: int = None,\n        htk: bool = False,\n        norm=1,\n        frontend_conf: Optional[dict] = get_default_kwargs(Frontend),\n    ):\n        assert check_argument_types()\n        super().__init__()\n        if isinstance(fs, str):\n            fs = humanfriendly.parse_size(fs)\n\n        # Deepcopy (In general, dict shouldn\'t be used as default arg)\n        frontend_conf = copy.deepcopy(frontend_conf)\n\n        self.stft = Stft(\n            n_fft=n_fft,\n            win_length=win_length,\n            hop_length=hop_length,\n            center=center,\n            pad_mode=pad_mode,\n            normalized=normalized,\n            onesided=onesided,\n        )\n        if frontend_conf is not None:\n            self.frontend = Frontend(idim=n_fft // 2 + 1, **frontend_conf)\n        else:\n            self.frontend = None\n\n        self.logmel = LogMel(\n            fs=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, htk=htk, norm=norm,\n        )\n        self.n_mels = n_mels\n\n    def output_size(self) -> int:\n        return self.n_mels\n\n    def forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # 1. Domain-conversion: e.g. Stft: time -> time-freq\n        input_stft, feats_lens = self.stft(input, input_lengths)\n\n        assert input_stft.dim() >= 4, input_stft.shape\n        # ""2"" refers to the real/imag parts of Complex\n        assert input_stft.shape[-1] == 2, input_stft.shape\n\n        # Change torch.Tensor to ComplexTensor\n        # input_stft: (..., F, 2) -> (..., F)\n        input_stft = ComplexTensor(input_stft[..., 0], input_stft[..., 1])\n\n        # 2. [Option] Speech enhancement\n        if self.frontend is not None:\n            assert isinstance(input_stft, ComplexTensor), type(input_stft)\n            # input_stft: (Batch, Length, [Channel], Freq)\n            input_stft, _, mask = self.frontend(input_stft, feats_lens)\n\n        # 3. [Multi channel case]: Select a channel\n        if input_stft.dim() == 4:\n            # h: (B, T, C, F) -> h: (B, T, F)\n            if self.training:\n                # Select 1ch randomly\n                ch = np.random.randint(input_stft.size(2))\n                input_stft = input_stft[:, :, ch, :]\n            else:\n                # Use the first channel\n                input_stft = input_stft[:, :, 0, :]\n\n        # 4. STFT -> Power spectrum\n        # h: ComplexTensor(B, T, F) -> torch.Tensor(B, T, F)\n        input_power = input_stft.real ** 2 + input_stft.imag ** 2\n\n        # 5. Feature transform e.g. Stft -> Log-Mel-Fbank\n        # input_power: (Batch, [Channel,] Length, Freq)\n        #       -> input_feats: (Batch, Length, Dim)\n        input_feats, _ = self.logmel(input_power, feats_lens)\n\n        return input_feats, feats_lens\n'"
espnet2/asr/specaug/abs_specaug.py,3,"b'from typing import Optional\nfrom typing import Tuple\n\nimport torch\n\n\nclass AbsSpecAug(torch.nn.Module):\n    """"""Abstract class for the augmentation of spectrogram\n\n    The process-flow:\n\n    Frontend  -> SpecAug -> Normalization -> Encoder -> Decoder\n    """"""\n\n    def forward(\n        self, x: torch.Tensor, x_lengths: torch.Tensor = None\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        raise NotImplementedError\n'"
espnet2/asr/specaug/specaug.py,2,"b'from distutils.version import LooseVersion\nfrom typing import Sequence\nfrom typing import Union\n\nimport torch\n\nfrom espnet2.asr.specaug.abs_specaug import AbsSpecAug\nfrom espnet2.layers.mask_along_axis import MaskAlongAxis\nfrom espnet2.layers.time_warp import TimeWarp\n\n\nif LooseVersion(torch.__version__) >= LooseVersion(""1.1""):\n    DEFAULT_TIME_WARP_MODE = ""bicubic""\nelse:\n    # pytorch1.0 doesn\'t implement bicubic\n    DEFAULT_TIME_WARP_MODE = ""bilinear""\n\n\nclass SpecAug(AbsSpecAug):\n    """"""Implementation of SpecAug.\n\n    Reference:\n        Daniel S. Park et al.\n        ""SpecAugment: A Simple Data\n         Augmentation Method for Automatic Speech Recognition""\n\n    .. warning::\n        When using cuda mode, time_warp doesn\'t have reproducibility\n        due to `torch.nn.functional.interpolate`.\n\n    """"""\n\n    def __init__(\n        self,\n        apply_time_warp: bool = True,\n        time_warp_window: int = 5,\n        time_warp_mode: str = DEFAULT_TIME_WARP_MODE,\n        apply_freq_mask: bool = True,\n        freq_mask_width_range: Union[int, Sequence[int]] = (0, 20),\n        num_freq_mask: int = 2,\n        apply_time_mask: bool = True,\n        time_mask_width_range: Union[int, Sequence[int]] = (0, 100),\n        num_time_mask: int = 2,\n    ):\n        if not apply_time_warp and not apply_time_mask and not apply_freq_mask:\n            raise ValueError(\n                ""Either one of time_warp, time_mask, or freq_mask should be applied"",\n            )\n        super().__init__()\n        self.apply_time_warp = apply_time_warp\n        self.apply_freq_mask = apply_freq_mask\n        self.apply_time_mask = apply_time_mask\n\n        if apply_time_warp:\n            self.time_warp = TimeWarp(window=time_warp_window, mode=time_warp_mode)\n        else:\n            self.time_warp = None\n\n        if apply_freq_mask:\n            self.freq_mask = MaskAlongAxis(\n                dim=""freq"",\n                mask_width_range=freq_mask_width_range,\n                num_mask=num_freq_mask,\n            )\n        else:\n            self.freq_mask = None\n\n        if apply_time_mask:\n            self.time_mask = MaskAlongAxis(\n                dim=""time"",\n                mask_width_range=time_mask_width_range,\n                num_mask=num_time_mask,\n            )\n        else:\n            self.time_mask = None\n\n    def forward(self, x, x_lengths=None):\n        if self.time_warp is not None:\n            x, x_lengths = self.time_warp(x, x_lengths)\n        if self.freq_mask is not None:\n            x, x_lengths = self.freq_mask(x, x_lengths)\n        if self.time_mask is not None:\n            x, x_lengths = self.time_mask(x, x_lengths)\n        return x, x_lengths\n'"
espnet2/tts/feats_extract/__init__.py,0,b''
espnet2/tts/feats_extract/abs_feats_extract.py,3,"b'from abc import ABC\nfrom abc import abstractmethod\nfrom typing import Any\nfrom typing import Dict\n\nimport torch\nfrom typing import Tuple\n\n\nclass AbsFeatsExtract(torch.nn.Module, ABC):\n    @abstractmethod\n    def output_size(self) -> int:\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_parameters(self) -> Dict[str, Any]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        raise NotImplementedError\n'"
espnet2/tts/feats_extract/log_mel_fbank.py,3,"b'from typing import Any\nfrom typing import Dict\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport humanfriendly\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet2.layers.log_mel import LogMel\nfrom espnet2.layers.stft import Stft\nfrom espnet2.tts.feats_extract.abs_feats_extract import AbsFeatsExtract\n\n\nclass LogMelFbank(AbsFeatsExtract):\n    """"""Conventional frontend structure for ASR\n\n    Stft -> amplitude-spec -> Log-Mel-Fbank\n    """"""\n\n    def __init__(\n        self,\n        fs: Union[int, str] = 16000,\n        n_fft: int = 1024,\n        win_length: int = None,\n        hop_length: int = 256,\n        center: bool = True,\n        pad_mode: str = ""reflect"",\n        normalized: bool = False,\n        onesided: bool = True,\n        n_mels: int = 80,\n        fmin: Optional[int] = 80,\n        fmax: Optional[int] = 7600,\n        htk: bool = False,\n        norm=1,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        if isinstance(fs, str):\n            fs = humanfriendly.parse_size(fs)\n\n        self.fs = fs\n        self.n_mels = n_mels\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.fmin = fmin\n        self.fmax = fmax\n\n        self.stft = Stft(\n            n_fft=n_fft,\n            win_length=win_length,\n            hop_length=hop_length,\n            center=center,\n            pad_mode=pad_mode,\n            normalized=normalized,\n            onesided=onesided,\n        )\n\n        self.logmel = LogMel(\n            fs=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, htk=htk, norm=norm\n        )\n\n    def output_size(self) -> int:\n        return self.n_mels\n\n    def get_parameters(self) -> Dict[str, Any]:\n        """"""Return the parameters required by Vocoder""""""\n        return dict(\n            fs=self.fs,\n            n_fft=self.n_fft,\n            n_shift=self.hop_length,\n            n_mels=self.n_mels,\n            win_length=self.win_length,\n            fmin=self.fmin,\n            fmax=self.fmax,\n        )\n\n    def forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # 1. Domain-conversion: e.g. Stft: time -> time-freq\n        input_stft, feats_lens = self.stft(input, input_lengths)\n\n        assert input_stft.dim() >= 4, input_stft.shape\n        # ""2"" refers to the real/imag parts of Complex\n        assert input_stft.shape[-1] == 2, input_stft.shape\n\n        # input_stft: (..., F, 2) -> (..., F)\n        input_power = input_stft[..., 0] ** 2 + input_stft[..., 1] ** 2\n        input_amp = torch.sqrt(input_power + 1.0e-20)\n        input_feats, _ = self.logmel(input_amp, feats_lens)\n        return input_feats, feats_lens\n'"
espnet2/tts/feats_extract/log_spectrogram.py,3,"b'from typing import Any\nfrom typing import Dict\nfrom typing import Tuple\n\nimport torch\nfrom typeguard import check_argument_types\n\nfrom espnet2.layers.stft import Stft\nfrom espnet2.tts.feats_extract.abs_feats_extract import AbsFeatsExtract\n\n\nclass LogSpectrogram(AbsFeatsExtract):\n    """"""Conventional frontend structure for ASR\n\n    Stft -> log-amplitude-spec\n    """"""\n\n    def __init__(\n        self,\n        n_fft: int = 1024,\n        win_length: int = None,\n        hop_length: int = 256,\n        center: bool = True,\n        pad_mode: str = ""reflect"",\n        normalized: bool = False,\n        onesided: bool = True,\n    ):\n        assert check_argument_types()\n        super().__init__()\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.stft = Stft(\n            n_fft=n_fft,\n            win_length=win_length,\n            hop_length=hop_length,\n            center=center,\n            pad_mode=pad_mode,\n            normalized=normalized,\n            onesided=onesided,\n        )\n        self.n_fft = n_fft\n\n    def output_size(self) -> int:\n        return self.n_fft // 2 + 1\n\n    def get_parameters(self) -> Dict[str, Any]:\n        """"""Return the parameters required by Vocoder""""""\n        return dict(\n            n_fft=self.n_fft, n_shift=self.hop_length, win_length=self.win_length,\n        )\n\n    def forward(\n        self, input: torch.Tensor, input_lengths: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # 1. Stft: time -> time-freq\n        input_stft, feats_lens = self.stft(input, input_lengths)\n\n        assert input_stft.dim() >= 4, input_stft.shape\n        # ""2"" refers to the real/imag parts of Complex\n        assert input_stft.shape[-1] == 2, input_stft.shape\n\n        # STFT -> Power spectrum\n        # input_stft: (..., F, 2) -> (..., F)\n        input_power = input_stft[..., 0] ** 2 + input_stft[..., 1] ** 2\n        log_amp = 0.5 * torch.log(input_power + 1.0e-20)\n        return log_amp, feats_lens\n'"
test/espnet2/asr/__init__.py,0,b''
test/espnet2/asr/test_ctc.py,4,"b'import pytest\nimport torch\n\nfrom espnet2.asr.ctc import CTC\n\n\n@pytest.fixture\ndef ctc_args():\n    bs = 2\n    h = torch.randn(bs, 10, 10)\n    h_lens = torch.LongTensor([10, 8])\n    y = torch.randint(0, 4, [2, 5])\n    y_lens = torch.LongTensor([5, 2])\n    return h, h_lens, y, y_lens\n\n\n@pytest.mark.parametrize(""ctc_type"", [""builtin"", ""warpctc""])\ndef test_ctc_forward_backward(ctc_type, ctc_args):\n    if ctc_type == ""warpctc"":\n        pytest.importorskip(""warpctc_pytorch"")\n    ctc = CTC(encoder_output_sizse=10, odim=5, ctc_type=ctc_type)\n    ctc(*ctc_args).sum().backward()\n\n\n@pytest.mark.parametrize(""ctc_type"", [""builtin"", ""warpctc""])\ndef test_ctc_log_softmax(ctc_type, ctc_args):\n    if ctc_type == ""warpctc"":\n        pytest.importorskip(""warpctc_pytorch"")\n    ctc = CTC(encoder_output_sizse=10, odim=5, ctc_type=ctc_type)\n    ctc.log_softmax(ctc_args[0])\n\n\n@pytest.mark.parametrize(""ctc_type"", [""builtin"", ""warpctc""])\ndef test_ctc_argmax(ctc_type, ctc_args):\n    if ctc_type == ""warpctc"":\n        pytest.importorskip(""warpctc_pytorch"")\n    ctc = CTC(encoder_output_sizse=10, odim=5, ctc_type=ctc_type)\n    ctc.argmax(ctc_args[0])\n'"
test/espnet2/bin/__init__.py,0,b''
test/espnet2/bin/test_aggregate_stats_dirs.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.aggregate_stats_dirs import get_parser\nfrom espnet2.bin.aggregate_stats_dirs import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_asr_inference.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.asr_inference import get_parser\nfrom espnet2.bin.asr_inference import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_asr_train.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.asr_train import get_parser\nfrom espnet2.bin.asr_train import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_lm_calc_perplexity.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.lm_calc_perplexity import get_parser\nfrom espnet2.bin.lm_calc_perplexity import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_lm_train.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.lm_train import get_parser\nfrom espnet2.bin.lm_train import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_pack.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.pack import get_parser\nfrom espnet2.bin.pack import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_tokenize_text.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.tokenize_text import get_parser\nfrom espnet2.bin.tokenize_text import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_tts_inference.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.tts_inference import get_parser\nfrom espnet2.bin.tts_inference import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/bin/test_tts_train.py,0,"b'from argparse import ArgumentParser\n\nimport pytest\n\nfrom espnet2.bin.tts_train import get_parser\nfrom espnet2.bin.tts_train import main\n\n\ndef test_get_parser():\n    assert isinstance(get_parser(), ArgumentParser)\n\n\ndef test_main():\n    with pytest.raises(SystemExit):\n        main()\n'"
test/espnet2/iterators/__init__.py,0,b''
test/espnet2/iterators/test_chunk_iter_factory.py,0,"b'from espnet2.iterators.chunk_iter_factory import ChunkIterFactory\nfrom espnet2.train.collate_fn import CommonCollateFn\n\nimport numpy as np\n\n\nclass Dataset:\n    def __init__(self):\n        self.data = {\n            ""a"": np.array([0, 1, 2, 3, 4, 5, 6, 7]),\n            ""b"": np.array([8, 9, 10, 11, 12]),\n        }\n\n    def __getitem__(self, item):\n        return item, {""dummy"": self.data[""a""]}\n\n\ndef test_ChunkIterFactory():\n    dataset = Dataset()\n    collatefn = CommonCollateFn()\n    batches = [[""a""], [""b""]]\n    iter_factory = ChunkIterFactory(\n        dataset=dataset,\n        batches=batches,\n        batch_size=2,\n        chunk_length=3,\n        collate_fn=collatefn,\n    )\n\n    for key, batch in iter_factory.build_iter(0):\n        for k, v in batch.items():\n            assert v.shape == (2, 3)\n'"
test/espnet2/iterators/test_sequence_iter_factory.py,1,"b'import pytest\nimport torch\n\nfrom espnet2.iterators.sequence_iter_factory import SequenceIterFactory\n\n\nclass Dataset:\n    def __getitem__(self, item):\n        return item\n\n\ndef collate_func(x):\n    return torch.tensor(x)\n\n\n@pytest.mark.parametrize(""collate"", [None, collate_func])\ndef test_SequenceIterFactory(collate):\n    dataset = Dataset()\n    batches = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n    iter_factory = SequenceIterFactory(\n        dataset=dataset, batches=batches, num_iters_per_epoch=3, collate_fn=collate\n    )\n\n    seq = [\n        [list(map(int, it)) for it in iter_factory.build_iter(i)] for i in range(1, 5)\n    ]\n    assert seq == [\n        [[0, 1], [2, 3], [4, 5]],\n        [[6, 7], [8, 9], [0, 1]],\n        [[2, 3], [4, 5], [6, 7]],\n        [[8, 9], [0, 1], [2, 3]],\n    ]\n\n\n@pytest.mark.parametrize(""collate"", [None, collate_func])\ndef test_SequenceIterFactory_deterministic(collate):\n    dataset = Dataset()\n    batches = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n    iter_factory = SequenceIterFactory(\n        dataset=dataset,\n        batches=batches,\n        num_iters_per_epoch=3,\n        shuffle=True,\n        collate_fn=collate,\n    )\n\n    for i in range(1, 10):\n        for v, v2 in zip(iter_factory.build_iter(i), iter_factory.build_iter(i)):\n            assert (v == v2).all()\n\n\n@pytest.mark.parametrize(""collate"", [None, collate_func])\ndef test_SequenceIterFactory_without_num_iters_per_epoch_deterministic(collate):\n    dataset = Dataset()\n    batches = [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n    iter_factory = SequenceIterFactory(\n        dataset=dataset, batches=batches, shuffle=True, collate_fn=collate\n    )\n    for i in range(1, 10):\n        for v, v2 in zip(iter_factory.build_iter(i), iter_factory.build_iter(i)):\n            assert (v == v2).all()\n'"
test/espnet2/layers/__init__.py,0,b''
test/espnet2/layers/test_global_mvn.py,6,"b'from pathlib import Path\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet2.layers.global_mvn import GlobalMVN\n\n\n@pytest.fixture()\ndef stats_file(tmp_path: Path):\n    """"""Kaldi like style""""""\n    p = tmp_path / ""stats.npy""\n\n    count = 10\n    np.random.seed(0)\n    x = np.random.randn(count, 80)\n    s = x.sum(0)\n    s = np.pad(s, [0, 1], mode=""constant"", constant_values=count)\n    s2 = (x ** 2).sum(0)\n    s2 = np.pad(s2, [0, 1], mode=""constant"", constant_values=0.0)\n\n    stats = np.stack([s, s2])\n    np.save(p, stats)\n    return p\n\n\n@pytest.fixture()\ndef stats_file2(tmp_path: Path):\n    """"""New style""""""\n    p = tmp_path / ""stats.npz""\n\n    count = 10\n    np.random.seed(0)\n    x = np.random.randn(count, 80)\n    s = x.sum(0)\n    s2 = (x ** 2).sum(0)\n\n    np.savez(p, sum=s, sum_square=s2, count=count)\n    return p\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_repl(stats_file, norm_vars, norm_means):\n    layer = GlobalMVN(stats_file, norm_means=norm_means, norm_vars=norm_vars)\n    print(layer)\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_backward_leaf_in(stats_file, norm_vars, norm_means):\n    layer = GlobalMVN(stats_file, norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(1, 2, 80, requires_grad=True)\n    y, _ = layer(x)\n    y.sum().backward()\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_backward_not_leaf_in(stats_file, norm_vars, norm_means):\n    layer = GlobalMVN(stats_file, norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 3, 80, requires_grad=True)\n    x = x + 2\n    y, _ = layer(x)\n    y.sum().backward()\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_inverse_backwar_leaf_in(stats_file, norm_vars, norm_means):\n    layer = GlobalMVN(stats_file, norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 3, 80, requires_grad=True)\n    y, _ = layer.inverse(x)\n    y.sum().backward()\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_inverse_backwar_not_leaf_in(stats_file, norm_vars, norm_means):\n    layer = GlobalMVN(stats_file, norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 3, 80, requires_grad=True)\n    x = x + 2\n    y, _ = layer.inverse(x)\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_inverse_identity(stats_file, norm_vars, norm_means):\n    layer = GlobalMVN(stats_file, norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 3, 80)\n    y, _ = layer(x)\n    x2, _ = layer.inverse(y)\n    np.testing.assert_allclose(x.numpy(), x2.numpy())\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_new_style_stats_file(stats_file, stats_file2, norm_vars, norm_means):\n    layer = GlobalMVN(stats_file, norm_means=norm_means, norm_vars=norm_vars)\n    layer2 = GlobalMVN(stats_file2, norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 3, 80)\n    y, _ = layer(x)\n    y2, _ = layer2(x)\n    np.testing.assert_allclose(y.numpy(), y2.numpy())\n'"
test/espnet2/layers/test_log_mel.py,5,"b'import torch\n\nfrom espnet2.layers.log_mel import LogMel\n\n\ndef test_repr():\n    print(LogMel())\n\n\ndef test_forward():\n    layer = LogMel(n_fft=16, n_mels=2)\n    x = torch.randn(2, 4, 9)\n    y, _ = layer(x)\n    assert y.shape == (2, 4, 2)\n    y, ylen = layer(x, torch.tensor([4, 2], dtype=torch.long))\n    assert (ylen == torch.tensor((4, 2), dtype=torch.long)).all()\n\n\ndef test_backward_leaf_in():\n    layer = LogMel(n_fft=16, n_mels=2)\n    x = torch.randn(2, 4, 9, requires_grad=True)\n    y, _ = layer(x)\n    y.sum().backward()\n\n\ndef test_backward_not_leaf_in():\n    layer = LogMel(n_fft=16, n_mels=2)\n    x = torch.randn(2, 4, 9, requires_grad=True)\n    x = x + 2\n    y, _ = layer(x)\n    y.sum().backward()\n'"
test/espnet2/layers/test_mask_along_axis.py,2,"b'import pytest\nimport torch\n\nfrom espnet2.layers.mask_along_axis import MaskAlongAxis\n\n\n@pytest.mark.parametrize(""requires_grad"", [False, True])\n@pytest.mark.parametrize(""replace_with_zero"", [False, True])\n@pytest.mark.parametrize(""dim"", [""freq"", ""time""])\ndef test_MaskAlongAxis(dim, replace_with_zero, requires_grad):\n    freq_mask = MaskAlongAxis(\n        dim=dim, mask_width_range=30, num_mask=2, replace_with_zero=replace_with_zero,\n    )\n    x = torch.randn(2, 100, 80, requires_grad=requires_grad)\n    x_lens = torch.tensor([80, 78])\n    y, y_lens = freq_mask(x, x_lens)\n    assert all(l1 == l2 for l1, l2 in zip(x_lens, y_lens))\n    if requires_grad:\n        y.sum().backward()\n\n\n@pytest.mark.parametrize(""replace_with_zero"", [False, True])\n@pytest.mark.parametrize(""dim"", [""freq"", ""time""])\ndef test_MaskAlongAxis_repr(dim, replace_with_zero):\n    freq_mask = MaskAlongAxis(\n        dim=dim, mask_width_range=30, num_mask=2, replace_with_zero=replace_with_zero,\n    )\n    print(freq_mask)\n'"
test/espnet2/layers/test_stft.py,6,"b'import pytest\nimport torch\n\nfrom espnet2.layers.stft import Stft\n\n\ndef test_repr():\n    print(Stft())\n\n\ndef test_forward():\n    layer = Stft(win_length=4, hop_length=2, n_fft=4)\n    x = torch.randn(2, 30)\n    y, _ = layer(x)\n    assert y.shape == (2, 16, 3, 2)\n    y, ylen = layer(x, torch.tensor([30, 15], dtype=torch.long))\n    assert (ylen == torch.tensor((16, 8), dtype=torch.long)).all()\n\n\ndef test_backward_leaf_in():\n    layer = Stft()\n    x = torch.randn(2, 400, requires_grad=True)\n    y, _ = layer(x)\n    y.sum().backward()\n\n\ndef test_backward_not_leaf_in():\n    layer = Stft()\n    x = torch.randn(2, 400, requires_grad=True)\n    x = x + 2\n    y, _ = layer(x)\n    y.sum().backward()\n\n\ndef test_inverse():\n    layer = Stft()\n    x = torch.randn(2, 400, requires_grad=True)\n    with pytest.raises(NotImplementedError):\n        y, _ = layer.inverse(x)\n'"
test/espnet2/layers/test_time_warp.py,2,"b'import pytest\nimport torch\n\nfrom espnet2.layers.time_warp import TimeWarp\n\n\n@pytest.mark.parametrize(""x_lens"", [None, torch.tensor([80, 78])])\n@pytest.mark.parametrize(""requires_grad"", [False, True])\ndef test_TimeWarp(x_lens, requires_grad):\n    time_warp = TimeWarp(window=10)\n    x = torch.randn(2, 100, 80, requires_grad=requires_grad)\n    y, y_lens = time_warp(x, x_lens)\n    if x_lens is not None:\n        assert all(l1 == l2 for l1, l2 in zip(x_lens, y_lens))\n    if requires_grad:\n        y.sum().backward()\n\n\ndef test_TimeWarp_repr():\n    time_warp = TimeWarp(window=10)\n    print(time_warp)\n'"
test/espnet2/layers/test_utterance_mvn.py,5,"b'import pytest\nimport torch\n\nfrom espnet2.layers.utterance_mvn import UtteranceMVN\n\n\ndef test_repr():\n    print(UtteranceMVN())\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_forward(norm_vars, norm_means):\n    layer = UtteranceMVN(norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 10, 80)\n    y, _ = layer(x)\n    assert y.shape == (2, 10, 80)\n    y, ylen = layer(x, torch.tensor([10, 8], dtype=torch.long))\n    assert (ylen == torch.tensor((10, 8), dtype=torch.long)).all()\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_backward_leaf_in(norm_vars, norm_means):\n    layer = UtteranceMVN(norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 1000, requires_grad=True)\n    y, _ = layer(x)\n    y.sum().backward()\n\n\n@pytest.mark.parametrize(\n    ""norm_vars, norm_means"",\n    [(True, True), (False, False), (True, False), (False, True)],\n)\ndef test_backward_not_leaf_in(norm_vars, norm_means):\n    layer = UtteranceMVN(norm_means=norm_means, norm_vars=norm_vars)\n    x = torch.randn(2, 1000, requires_grad=True)\n    x = x + 2\n    y, _ = layer(x)\n    y.sum().backward()\n'"
test/espnet2/lm/__init__.py,0,b''
test/espnet2/lm/test_seq_rnn.py,2,"b'import pytest\nimport torch\n\nfrom espnet2.lm.seq_rnn import SequentialRNNLM\n\n\n@pytest.mark.parametrize(""rnn_type"", [""LSTM"", ""GRU"", ""RNN_TANH"", ""RNN_RELU""])\n@pytest.mark.parametrize(""tie_weights"", [True, False])\ndef test_SequentialRNNLM_backward(rnn_type, tie_weights):\n    model = SequentialRNNLM(10, rnn_type=rnn_type, tie_weights=tie_weights)\n    input = torch.randint(0, 9, [2, 10])\n\n    out, h = model(input, None)\n    out, h = model(input, h)\n    out.sum().backward()\n\n\n@pytest.mark.parametrize(""rnn_type"", [""LSTM"", ""GRU"", ""RNN_TANH"", ""RNN_RELU""])\n@pytest.mark.parametrize(""tie_weights"", [True, False])\ndef test_SequentialRNNLM_score(rnn_type, tie_weights):\n    model = SequentialRNNLM(10, rnn_type=rnn_type, tie_weights=tie_weights)\n    input = torch.randint(0, 9, (12,))\n    state = model.init_state(None)\n    model.score(input, state, None)\n\n\ndef test_SequentialRNNLM_invalid_type():\n    with pytest.raises(ValueError):\n        SequentialRNNLM(10, rnn_type=""foooo"")\n\n\ndef test_SequentialRNNLM_tie_weights_value_error():\n    with pytest.raises(ValueError):\n        SequentialRNNLM(10, tie_weights=True, unit=20, nhid=10)\n'"
test/espnet2/main_funcs/test_calculate_all_attentions.py,10,"b'from collections import defaultdict\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttAdd\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet2.asr.decoder.rnn_decoder import RNNDecoder\nfrom espnet2.main_funcs.calculate_all_attentions import calculate_all_attentions\nfrom espnet2.train.abs_espnet_model import AbsESPnetModel\n\n\nclass Dummy(AbsESPnetModel):\n    def __init__(self):\n        super().__init__()\n        self.att1 = MultiHeadedAttention(2, 10, 0.0)\n        self.att2 = AttAdd(10, 20, 15)\n        self.desired = defaultdict(list)\n\n    def forward(self, x, x_lengths, y, y_lengths):\n        a1 = self.att1(y, x, x, None)\n        _, a2 = self.att2(x, x_lengths, y, None)\n        self.desired[""att1""].append(a1)\n        self.desired[""att2""].append(a2)\n\n    def collect_feats(self, **batch: torch.Tensor):\n        return {}\n\n\nclass Dummy2(AbsESPnetModel):\n    def __init__(self, atype):\n        super().__init__()\n        self.decoder = RNNDecoder(50, 128, att_conf=dict(atype=atype))\n\n    def forward(self, x, x_lengths, y, y_lengths):\n        self.decoder(x, x_lengths, y, y_lengths)\n\n    def collect_feats(self, **batch: torch.Tensor):\n        return {}\n\n\ndef test_calculate_all_attentions_MultiHeadedAttention():\n    model = Dummy()\n    bs = 2\n    batch = {\n        ""x"": torch.randn(bs, 3, 10),\n        ""x_lengths"": torch.tensor([3, 2], dtype=torch.long),\n        ""y"": torch.randn(bs, 2, 10),\n        ""y_lengths"": torch.tensor([4, 4], dtype=torch.long),\n    }\n    t = calculate_all_attentions(model, batch)\n    print(t)\n    for k in model.desired:\n        for i in range(bs):\n            np.testing.assert_array_equal(t[k][i].numpy(), model.desired[k][i].numpy())\n\n\n@pytest.mark.parametrize(\n    ""atype"",\n    [\n        ""noatt"",\n        ""dot"",\n        ""add"",\n        ""location"",\n        ""location2d"",\n        ""location_recurrent"",\n        ""coverage"",\n        ""coverage_location"",\n        ""multi_head_dot"",\n        ""multi_head_add"",\n        ""multi_head_loc"",\n        ""multi_head_multi_res_loc"",\n    ],\n)\ndef test_calculate_all_attentions(atype):\n    model = Dummy2(atype)\n    bs = 2\n    batch = {\n        ""x"": torch.randn(bs, 20, 128),\n        ""x_lengths"": torch.tensor([20, 17], dtype=torch.long),\n        ""y"": torch.randint(0, 50, [bs, 7]),\n        ""y_lengths"": torch.tensor([7, 5], dtype=torch.long),\n    }\n    t = calculate_all_attentions(model, batch)\n    for k, o in t.items():\n        for i, att in enumerate(o):\n            print(att.shape)\n            if att.dim() == 2:\n                att = att[None]\n            for a in att:\n                assert a.shape == (batch[""y_lengths""][i], batch[""x_lengths""][i])\n'"
test/espnet2/optimizers/__init__.py,0,b''
test/espnet2/optimizers/test_sgd.py,2,"b'import torch\n\nfrom espnet2.optimizers.sgd import SGD\n\n\ndef test_SGD():\n    linear = torch.nn.Linear(1, 1)\n    opt = SGD(linear.parameters())\n    x = torch.randn(1, 1)\n    linear(x).sum().backward()\n    opt.step()\n'"
test/espnet2/samplers/__init__.py,0,b''
test/espnet2/samplers/test_build_batch_sampler.py,0,"b'import pytest\n\nfrom espnet2.samplers.build_batch_sampler import build_batch_sampler\n\n\n@pytest.fixture()\ndef shape_files(tmp_path):\n    p1 = tmp_path / ""shape1.txt""\n    with p1.open(""w"") as f:\n        f.write(""a 1000,80\\n"")\n        f.write(""b 400,80\\n"")\n        f.write(""c 800,80\\n"")\n        f.write(""d 789,80\\n"")\n        f.write(""e 1023,80\\n"")\n        f.write(""f 999,80\\n"")\n\n    p2 = tmp_path / ""shape2.txt""\n    with p2.open(""w"") as f:\n        f.write(""a 30,30\\n"")\n        f.write(""b 50,30\\n"")\n        f.write(""c 39,30\\n"")\n        f.write(""d 49,30\\n"")\n        f.write(""e 44,30\\n"")\n        f.write(""f 99,30\\n"")\n\n    return str(p1), str(p2)\n\n\n@pytest.mark.parametrize(\n    ""type"", [""unsorted"", ""sorted"", ""folded"", ""length"", ""numel"", ""foo""]\n)\ndef test_build_batch_sampler(shape_files, type):\n    if type == ""foo"":\n        with pytest.raises(ValueError):\n            build_batch_sampler(\n                batch_bins=60000,\n                batch_size=2,\n                shape_files=shape_files,\n                fold_lengths=[800, 40],\n                type=type,\n            )\n    else:\n        sampler = build_batch_sampler(\n            batch_bins=60000,\n            batch_size=2,\n            shape_files=shape_files,\n            fold_lengths=[800, 40],\n            type=type,\n        )\n        list(sampler)\n\n\ndef test_build_batch_sampler_invalid_fold_lengths(shape_files):\n    with pytest.raises(ValueError):\n        build_batch_sampler(\n            batch_bins=60000,\n            batch_size=2,\n            shape_files=shape_files,\n            fold_lengths=[800, 40, 100],\n            type=""seq"",\n        )\n'"
test/espnet2/samplers/test_folded_batch_sampler.py,0,"b'import pytest\n\nfrom espnet2.samplers.folded_batch_sampler import FoldedBatchSampler\n\n\n@pytest.fixture()\ndef shape_files(tmp_path):\n    p1 = tmp_path / ""shape1.txt""\n    with p1.open(""w"") as f:\n        f.write(""a 1000,80\\n"")\n        f.write(""b 400,80\\n"")\n        f.write(""c 800,80\\n"")\n        f.write(""d 789,80\\n"")\n        f.write(""e 1023,80\\n"")\n        f.write(""f 999,80\\n"")\n\n    p2 = tmp_path / ""shape2.txt""\n    with p2.open(""w"") as f:\n        f.write(""a 30,30\\n"")\n        f.write(""b 50,30\\n"")\n        f.write(""c 39,30\\n"")\n        f.write(""d 49,30\\n"")\n        f.write(""e 44,30\\n"")\n        f.write(""f 99,30\\n"")\n\n    return str(p1), str(p2)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_FoldedBatchSampler(shape_files, sort_in_batch, sort_batch, drop_last):\n    sampler = FoldedBatchSampler(\n        2,\n        shape_files=shape_files,\n        fold_lengths=[500, 80],\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n    )\n    list(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_FoldedBatchSampler_repr(shape_files, sort_in_batch, sort_batch, drop_last):\n    sampler = FoldedBatchSampler(\n        2,\n        shape_files=shape_files,\n        fold_lengths=[500, 80],\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n    )\n    print(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_FoldedBatchSampler_len(shape_files, sort_in_batch, sort_batch, drop_last):\n    sampler = FoldedBatchSampler(\n        2,\n        shape_files=shape_files,\n        fold_lengths=[500, 80],\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n    )\n    len(sampler)\n'"
test/espnet2/samplers/test_length_elements_batch_sampler.py,0,"b'import pytest\n\nfrom espnet2.samplers.length_batch_sampler import LengthBatchSampler\n\n\n@pytest.fixture()\ndef shape_files(tmp_path):\n    p1 = tmp_path / ""shape1.txt""\n    with p1.open(""w"") as f:\n        f.write(""a 1000,80\\n"")\n        f.write(""b 400,80\\n"")\n        f.write(""c 800,80\\n"")\n        f.write(""d 789,80\\n"")\n        f.write(""e 1023,80\\n"")\n        f.write(""f 999,80\\n"")\n\n    p2 = tmp_path / ""shape2.txt""\n    with p2.open(""w"") as f:\n        f.write(""a 30,30\\n"")\n        f.write(""b 50,30\\n"")\n        f.write(""c 39,30\\n"")\n        f.write(""d 49,30\\n"")\n        f.write(""e 44,30\\n"")\n        f.write(""f 99,30\\n"")\n\n    return str(p1), str(p2)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\n@pytest.mark.parametrize(""padding"", [True, False])\ndef test_LengthBatchSampler(\n    shape_files, sort_in_batch, sort_batch, drop_last, padding,\n):\n    sampler = LengthBatchSampler(\n        6000,\n        shape_files=shape_files,\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n        padding=padding,\n    )\n    list(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\n@pytest.mark.parametrize(""padding"", [True, False])\ndef test_LengthBatchSampler_repr(\n    shape_files, sort_in_batch, sort_batch, drop_last, padding\n):\n    sampler = LengthBatchSampler(\n        6000,\n        shape_files=shape_files,\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n        padding=padding,\n    )\n    print(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\n@pytest.mark.parametrize(""padding"", [True, False])\ndef test_LengthBatchSampler_len(\n    shape_files, sort_in_batch, sort_batch, drop_last, padding\n):\n    sampler = LengthBatchSampler(\n        6000,\n        shape_files=shape_files,\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n        padding=padding,\n    )\n    len(sampler)\n'"
test/espnet2/samplers/test_num_elements_batch_sampler.py,0,"b'import pytest\n\nfrom espnet2.samplers.num_elements_batch_sampler import NumElementsBatchSampler\n\n\n@pytest.fixture()\ndef shape_files(tmp_path):\n    p1 = tmp_path / ""shape1.txt""\n    with p1.open(""w"") as f:\n        f.write(""a 1000,80\\n"")\n        f.write(""b 400,80\\n"")\n        f.write(""c 800,80\\n"")\n        f.write(""d 789,80\\n"")\n        f.write(""e 1023,80\\n"")\n        f.write(""f 999,80\\n"")\n\n    p2 = tmp_path / ""shape2.txt""\n    with p2.open(""w"") as f:\n        f.write(""a 30,30\\n"")\n        f.write(""b 50,30\\n"")\n        f.write(""c 39,30\\n"")\n        f.write(""d 49,30\\n"")\n        f.write(""e 44,30\\n"")\n        f.write(""f 99,30\\n"")\n\n    return str(p1), str(p2)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\n@pytest.mark.parametrize(""padding"", [True, False])\ndef test_NumElementsBatchSampler(\n    shape_files, sort_in_batch, sort_batch, drop_last, padding,\n):\n    sampler = NumElementsBatchSampler(\n        60000,\n        shape_files=shape_files,\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n        padding=padding,\n    )\n    list(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\n@pytest.mark.parametrize(""padding"", [True, False])\ndef test_NumElementsBatchSampler_repr(\n    shape_files, sort_in_batch, sort_batch, drop_last, padding\n):\n    sampler = NumElementsBatchSampler(\n        60000,\n        shape_files=shape_files,\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n        padding=padding,\n    )\n    print(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\n@pytest.mark.parametrize(""padding"", [True, False])\ndef test_NumElementsBatchSampler_len(\n    shape_files, sort_in_batch, sort_batch, drop_last, padding\n):\n    sampler = NumElementsBatchSampler(\n        60000,\n        shape_files=shape_files,\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n        padding=padding,\n    )\n    len(sampler)\n'"
test/espnet2/samplers/test_sorted_batch_sampler.py,0,"b'import pytest\n\nfrom espnet2.samplers.sorted_batch_sampler import SortedBatchSampler\n\n\n@pytest.fixture()\ndef shape_files(tmp_path):\n    p1 = tmp_path / ""shape1.txt""\n    with p1.open(""w"") as f:\n        f.write(""a 1000,80\\n"")\n        f.write(""b 400,80\\n"")\n        f.write(""c 800,80\\n"")\n        f.write(""d 789,80\\n"")\n        f.write(""e 1023,80\\n"")\n        f.write(""f 999,80\\n"")\n\n    p2 = tmp_path / ""shape2.txt""\n    with p2.open(""w"") as f:\n        f.write(""a 30,30\\n"")\n        f.write(""b 50,30\\n"")\n        f.write(""c 39,30\\n"")\n        f.write(""d 49,30\\n"")\n        f.write(""e 44,30\\n"")\n        f.write(""f 99,30\\n"")\n\n    return str(p1), str(p2)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_SortedBatchSampler(shape_files, sort_in_batch, sort_batch, drop_last):\n    sampler = SortedBatchSampler(\n        2,\n        shape_file=shape_files[0],\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n    )\n    list(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_SortedBatchSampler_repr(shape_files, sort_in_batch, sort_batch, drop_last):\n    sampler = SortedBatchSampler(\n        2,\n        shape_file=shape_files[0],\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n    )\n    print(sampler)\n\n\n@pytest.mark.parametrize(""sort_in_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""sort_batch"", [""descending"", ""ascending""])\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_SortedBatchSampler_len(shape_files, sort_in_batch, sort_batch, drop_last):\n    sampler = SortedBatchSampler(\n        2,\n        shape_file=shape_files[0],\n        sort_in_batch=sort_in_batch,\n        sort_batch=sort_batch,\n        drop_last=drop_last,\n    )\n    len(sampler)\n'"
test/espnet2/samplers/test_unsorted_batch_sampler.py,0,"b'import pytest\n\nfrom espnet2.samplers.unsorted_batch_sampler import UnsortedBatchSampler\n\n\n@pytest.fixture()\ndef shape_files(tmp_path):\n    p1 = tmp_path / ""shape1.txt""\n    with p1.open(""w"") as f:\n        f.write(""a 1000,80\\n"")\n        f.write(""b 400,80\\n"")\n        f.write(""c 800,80\\n"")\n        f.write(""d 789,80\\n"")\n        f.write(""e 1023,80\\n"")\n        f.write(""f 999,80\\n"")\n\n    p2 = tmp_path / ""shape2.txt""\n    with p2.open(""w"") as f:\n        f.write(""a 30,30\\n"")\n        f.write(""b 50,30\\n"")\n        f.write(""c 39,30\\n"")\n        f.write(""d 49,30\\n"")\n        f.write(""e 44,30\\n"")\n        f.write(""f 99,30\\n"")\n\n    return str(p1), str(p2)\n\n\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_UnsortedBatchSampler(shape_files, drop_last):\n    sampler = UnsortedBatchSampler(2, key_file=shape_files[0], drop_last=drop_last)\n    list(sampler)\n\n\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_UnsortedBatchSampler_repr(shape_files, drop_last):\n    sampler = UnsortedBatchSampler(2, key_file=shape_files[0], drop_last=drop_last)\n    print(sampler)\n\n\n@pytest.mark.parametrize(""drop_last"", [True, False])\ndef test_UnsortedBatchSampler_len(shape_files, drop_last):\n    sampler = UnsortedBatchSampler(2, key_file=shape_files[0], drop_last=drop_last)\n    len(sampler)\n'"
test/espnet2/schedulers/__init__.py,0,b''
test/espnet2/schedulers/test_noam_lr.py,3,"b'from distutils.version import LooseVersion\n\nimport pytest\nimport torch\n\nfrom espnet2.schedulers.noam_lr import NoamLR\n\n\n@pytest.mark.skipif(\n    LooseVersion(torch.__version__) < LooseVersion(""1.1.0""),\n    reason=""Require pytorch>=1.1.0"",\n)\ndef test_NoamLR():\n    linear = torch.nn.Linear(2, 2)\n    opt = torch.optim.SGD(linear.parameters(), 0.1)\n    sch = NoamLR(opt)\n    lr = opt.param_groups[0][""lr""]\n\n    opt.step()\n    sch.step()\n    lr2 = opt.param_groups[0][""lr""]\n    assert lr != lr2\n'"
test/espnet2/schedulers/test_warmup_lr.py,8,"b'from distutils.version import LooseVersion\n\nimport pytest\nimport torch\n\nfrom espnet2.schedulers.noam_lr import NoamLR\nfrom espnet2.schedulers.warmup_lr import WarmupLR\n\n\n@pytest.mark.skipif(\n    LooseVersion(torch.__version__) < LooseVersion(""1.1.0""),\n    reason=""Require pytorch>=1.1.0"",\n)\ndef test_WarumupLR():\n    linear = torch.nn.Linear(2, 2)\n    opt = torch.optim.SGD(linear.parameters(), 0.1)\n    sch = WarmupLR(opt)\n    lr = opt.param_groups[0][""lr""]\n\n    opt.step()\n    sch.step()\n    lr2 = opt.param_groups[0][""lr""]\n    assert lr != lr2\n\n\n@pytest.mark.skipif(\n    LooseVersion(torch.__version__) < LooseVersion(""1.1.0""),\n    reason=""Require pytorch>=1.1.0"",\n)\ndef test_WarumupLR_is_compatible_with_NoamLR():\n    lr = 10\n    model_size = 320\n    warmup_steps = 25000\n\n    linear = torch.nn.Linear(2, 2)\n    noam_opt = torch.optim.SGD(linear.parameters(), lr)\n    noam = NoamLR(noam_opt, model_size=model_size, warmup_steps=warmup_steps)\n    new_lr = noam.lr_for_WarmupLR(lr)\n\n    linear = torch.nn.Linear(2, 2)\n    warmup_opt = torch.optim.SGD(linear.parameters(), new_lr)\n    warmup = WarmupLR(warmup_opt)\n\n    for i in range(3 * warmup_steps):\n        warmup_opt.step()\n        warmup.step()\n\n        noam_opt.step()\n        noam.step()\n\n        lr1 = noam_opt.param_groups[0][""lr""]\n        lr2 = warmup_opt.param_groups[0][""lr""]\n\n        assert lr1 == lr2\n'"
test/espnet2/tasks/__init__.py,0,b''
test/espnet2/tasks/test_abs_task.py,0,"b'import configargparse\nimport pytest\n\nfrom espnet2.tasks.abs_task import AbsTask\n\n\n@pytest.mark.parametrize(""parser"", [configargparse.ArgumentParser(), None])\ndef test_add_arguments(parser):\n    AbsTask.get_parser()\n\n\ndef test_add_arguments_help():\n    parser = AbsTask.get_parser()\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--help""])\n\n\ndef test_main_help():\n    with pytest.raises(SystemExit):\n        AbsTask.main(cmd=[""--help""])\n\n\ndef test_main_print_config():\n    with pytest.raises(SystemExit):\n        AbsTask.main(cmd=[""--print_config""])\n\n\ndef test_main_with_no_args():\n    with pytest.raises(SystemExit):\n        AbsTask.main(cmd=[])\n\n\ndef test_print_config_and_load_it(tmp_path):\n    config_file = tmp_path / ""config.yaml""\n    with config_file.open(""w"") as f:\n        AbsTask.print_config(f)\n    parser = AbsTask.get_parser()\n    parser.parse_args([""--config"", str(config_file)])\n'"
test/espnet2/tasks/test_asr.py,0,"b'import pytest\n\nfrom espnet2.tasks.asr import ASRTask\n\n\ndef test_add_arguments():\n    ASRTask.get_parser()\n\n\ndef test_add_arguments_help():\n    parser = ASRTask.get_parser()\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--help""])\n\n\ndef test_main_help():\n    with pytest.raises(SystemExit):\n        ASRTask.main(cmd=[""--help""])\n\n\ndef test_main_print_config():\n    with pytest.raises(SystemExit):\n        ASRTask.main(cmd=[""--print_config""])\n\n\ndef test_main_with_no_args():\n    with pytest.raises(SystemExit):\n        ASRTask.main(cmd=[])\n\n\ndef test_print_config_and_load_it(tmp_path):\n    config_file = tmp_path / ""config.yaml""\n    with config_file.open(""w"") as f:\n        ASRTask.print_config(f)\n    parser = ASRTask.get_parser()\n    parser.parse_args([""--config"", str(config_file)])\n'"
test/espnet2/tasks/test_lm.py,0,"b'import pytest\n\nfrom espnet2.tasks.lm import LMTask\n\n\ndef test_add_arguments():\n    LMTask.get_parser()\n\n\ndef test_add_arguments_help():\n    parser = LMTask.get_parser()\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--help""])\n\n\ndef test_main_help():\n    with pytest.raises(SystemExit):\n        LMTask.main(cmd=[""--help""])\n\n\ndef test_main_print_config():\n    with pytest.raises(SystemExit):\n        LMTask.main(cmd=[""--print_config""])\n\n\ndef test_main_with_no_args():\n    with pytest.raises(SystemExit):\n        LMTask.main(cmd=[])\n\n\ndef test_print_config_and_load_it(tmp_path):\n    config_file = tmp_path / ""config.yaml""\n    with config_file.open(""w"") as f:\n        LMTask.print_config(f)\n    parser = LMTask.get_parser()\n    parser.parse_args([""--config"", str(config_file)])\n'"
test/espnet2/tasks/test_tts.py,0,"b'import pytest\n\nfrom espnet2.tasks.tts import TTSTask\n\n\ndef test_add_arguments():\n    TTSTask.get_parser()\n\n\ndef test_add_arguments_help():\n    parser = TTSTask.get_parser()\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--help""])\n\n\ndef test_main_help():\n    with pytest.raises(SystemExit):\n        TTSTask.main(cmd=[""--help""])\n\n\ndef test_main_print_config():\n    with pytest.raises(SystemExit):\n        TTSTask.main(cmd=[""--print_config""])\n\n\ndef test_main_with_no_args():\n    with pytest.raises(SystemExit):\n        TTSTask.main(cmd=[])\n\n\ndef test_print_config_and_load_it(tmp_path):\n    config_file = tmp_path / ""config.yaml""\n    with config_file.open(""w"") as f:\n        TTSTask.print_config(f)\n    parser = TTSTask.get_parser()\n    parser.parse_args([""--config"", str(config_file)])\n'"
test/espnet2/text/__init__.py,0,b''
test/espnet2/text/test_text_converter.py,0,"b'from pathlib import Path\nimport string\n\nimport pytest\nimport sentencepiece as spm\n\nfrom espnet2.text.char_tokenizer import CharTokenizer\nfrom espnet2.text.sentencepiece_tokenizer import SentencepiecesTokenizer\nfrom espnet2.text.word_tokenizer import WordTokenizer\n\n\n@pytest.fixture(params=[None, "" ""])\ndef word_converter(request):\n    return WordTokenizer(delimiter=request.param)\n\n\n@pytest.fixture\ndef char_converter():\n    return CharTokenizer([""[foo]""])\n\n\n@pytest.fixture\ndef spm_srcs(tmp_path: Path):\n    input_text = tmp_path / ""text""\n    vocabsize = len(string.ascii_letters) + 4\n    model_prefix = tmp_path / ""model""\n    model = str(model_prefix) + "".model""\n    input_sentence_size = 100000\n\n    with input_text.open(""w"") as f:\n        f.write(string.ascii_letters + ""\\n"")\n\n    spm.SentencePieceTrainer.Train(\n        f""--input={input_text} ""\n        f""--vocab_size={vocabsize} ""\n        f""--model_prefix={model_prefix} ""\n        f""--input_sentence_size={input_sentence_size}""\n    )\n    sp = spm.SentencePieceProcessor()\n    sp.load(model)\n\n    with input_text.open(""r"") as f:\n        vocabs = {""<unk>"", ""\xe2\x96\x81""}\n        for line in f:\n            tokens = sp.DecodePieces(list(line.strip()))\n        vocabs |= set(tokens)\n    return model, vocabs\n\n\n@pytest.fixture\ndef spm_converter(tmp_path, spm_srcs):\n    model, vocabs = spm_srcs\n    sp = spm.SentencePieceProcessor()\n    sp.load(model)\n\n    token_list = tmp_path / ""token.list""\n    with token_list.open(""w"") as f:\n        for v in vocabs:\n            f.write(f""{v}\\n"")\n    return SentencepiecesTokenizer(model=model)\n\n\ndef test_Text2Sentencepieces_repr(spm_converter: SentencepiecesTokenizer):\n    print(spm_converter)\n\n\ndef test_Text2Sentencepieces_text2tokens(spm_converter: SentencepiecesTokenizer):\n    assert spm_converter.tokens2text(spm_converter.text2tokens(""Hello"")) == ""Hello""\n\n\ndef test_Text2Words_repr(word_converter: WordTokenizer):\n    print(word_converter)\n\n\ndef test_Text2Words_text2tokens(word_converter: WordTokenizer):\n    assert word_converter.text2tokens(""Hello World!! Ummm"") == [\n        ""Hello"",\n        ""World!!"",\n        ""Ummm"",\n    ]\n\n\ndef test_Text2Words_tokens2text(word_converter: WordTokenizer):\n    assert word_converter.tokens2text(""Hello World!!"".split()) == ""Hello World!!""\n\n\ndef test_Text2Chars_repr(char_converter: CharTokenizer):\n    print(char_converter)\n\n\ndef test_Text2Chars_text2tokens(char_converter: CharTokenizer):\n    assert char_converter.text2tokens(""He[foo]llo"") == [\n        ""H"",\n        ""e"",\n        ""[foo]"",\n        ""l"",\n        ""l"",\n        ""o"",\n    ]\n'"
test/espnet2/text/test_token_id_converter.py,0,"b'from pathlib import Path\n\nimport numpy as np\nimport pytest\n\nfrom espnet2.text.token_id_converter import TokenIDConverter\n\n\ndef test_tokens2ids():\n    converter = TokenIDConverter([""a"", ""b"", ""c"", ""<unk>""])\n    assert converter.tokens2ids(""abc"") == [0, 1, 2]\n\n\ndef test_idstokens():\n    converter = TokenIDConverter([""a"", ""b"", ""c"", ""<unk>""])\n    assert converter.ids2tokens([0, 1, 2]) == [""a"", ""b"", ""c""]\n\n\ndef test_get_num_vocabulary_size():\n    converter = TokenIDConverter([""a"", ""b"", ""c"", ""<unk>""])\n    assert converter.get_num_vocabulary_size() == 4\n\n\ndef test_from_file(tmp_path: Path):\n    with (tmp_path / ""tokens.txt"").open(""w"") as f:\n        f.write(""a\\n"")\n        f.write(""b\\n"")\n        f.write(""c\\n"")\n        f.write(""<unk>\\n"")\n    converter = TokenIDConverter(tmp_path / ""tokens.txt"")\n    assert converter.tokens2ids(""abc"") == [0, 1, 2]\n\n\ndef test_duplicated():\n    with pytest.raises(RuntimeError):\n        TokenIDConverter([""a"", ""a"", ""c""])\n\n\ndef test_no_unk():\n    with pytest.raises(RuntimeError):\n        TokenIDConverter([""a"", ""b"", ""c""])\n\n\ndef test_input_2dim_array():\n    converter = TokenIDConverter([""a"", ""b"", ""c"", ""<unk>""])\n    with pytest.raises(ValueError):\n        converter.ids2tokens(np.random.randn(2, 2))\n'"
test/espnet2/torch_utils/__init__.py,0,b''
test/espnet2/torch_utils/test_add_gradient_noise.py,2,"b'import torch\n\nfrom espnet2.torch_utils.add_gradient_noise import add_gradient_noise\n\n\ndef test_add_gradient_noise():\n    linear = torch.nn.Linear(1, 1)\n    linear(torch.rand(1, 1)).sum().backward()\n    add_gradient_noise(linear, 100)\n'"
test/espnet2/torch_utils/test_device_funcs.py,9,"b'import dataclasses\nfrom typing import NamedTuple\n\nimport pytest\nimport torch\n\nfrom espnet2.torch_utils.device_funcs import force_gatherable\nfrom espnet2.torch_utils.device_funcs import to_device\n\nx = torch.tensor(10)\n\n\n@dataclasses.dataclass(frozen=True)\nclass Data:\n    x: torch.Tensor\n\n\nclass Named(NamedTuple):\n    x: torch.Tensor\n\n\n@pytest.mark.parametrize(\n    ""obj"", [x, x.numpy(), (x,), [x], {""x"": [x]}, {x}, Data(x), Named(x), 23, 3.0, None],\n)\ndef test_to_device(obj):\n    to_device(obj, ""cpu"")\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""Require cuda"")\ndef test_to_device_cuda():\n    obj = {""a"": [torch.tensor([0, 1])]}\n    obj2 = to_device(obj, ""cuda"")\n    assert obj2[""a""][0].device == torch.device(""cuda:0"")\n\n\n@pytest.mark.parametrize(\n    ""obj"", [x, x.numpy(), (x,), [x], {""x"": x}, {x}, Data(x), Named(x), 23, 3.0, None],\n)\ndef test_force_gatherable(obj):\n    force_gatherable(obj, ""cpu"")\n\n\ndef test_force_gatherable_0dim_to_1dim():\n    obj = {""a"": [3]}\n    obj2 = force_gatherable(obj, ""cpu"")\n    assert obj2[""a""][0].shape == (1,)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""Require cuda"")\ndef test_force_gatherable_cuda():\n    obj = {""a"": [torch.tensor([0, 1])]}\n    obj2 = force_gatherable(obj, ""cuda"")\n    assert obj2[""a""][0].device == torch.device(""cuda:0"")\n'"
test/espnet2/torch_utils/test_forward_adaptor.py,2,"b'import pytest\nimport torch\n\nfrom espnet2.torch_utils.forward_adaptor import ForwardAdaptor\n\n\nclass Model(torch.nn.Module):\n    def func(self, x):\n        return x\n\n\ndef test_ForwardAdaptor():\n    model = Model()\n    x = torch.randn(2, 2)\n    assert (ForwardAdaptor(model, ""func"")(x) == x).all()\n\n\ndef test_ForwardAdaptor_no_func():\n    model = Model()\n    with pytest.raises(ValueError):\n        ForwardAdaptor(model, ""aa"")\n'"
test/espnet2/torch_utils/test_initialize.py,9,"b'import pytest\nimport torch\n\nfrom espnet2.torch_utils.initialize import initialize\n\ninitialize_types = {}\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(2, 2, 3)\n        self.l1 = torch.nn.Linear(2, 2)\n        self.rnn_cell = torch.nn.LSTMCell(2, 2)\n        self.rnn = torch.nn.LSTM(2, 2)\n        self.emb = torch.nn.Embedding(1, 1)\n        self.norm = torch.nn.LayerNorm(1)\n\n\nclass Model2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv3d(2, 2, 3)\n\n\n@pytest.mark.parametrize(\n    ""init"",\n    [\n        ""chainer"",\n        ""xavier_uniform"",\n        ""xavier_normal"",\n        ""kaiming_normal"",\n        ""kaiming_uniform"",\n        ""dummy"",\n    ],\n)\ndef test_initialize(init):\n    model = Model()\n    if init == ""dummy"":\n        with pytest.raises(ValueError):\n            initialize(model, init)\n    else:\n        initialize(model, init)\n\n\ndef test_5dim():\n    model = Model2()\n    with pytest.raises(NotImplementedError):\n        initialize(model, ""chainer"")\n'"
test/espnet2/torch_utils/test_pytorch_version.py,0,b'from espnet2.torch_utils.pytorch_version import pytorch_cudnn_version\n\n\ndef test_pytorch_cudnn_version():\n    print(pytorch_cudnn_version())\n'
test/espnet2/torch_utils/test_set_all_random_seed.py,0,b'from espnet2.torch_utils.set_all_random_seed import set_all_random_seed\n\n\ndef test_set_all_random_seed():\n    set_all_random_seed(0)\n'
test/espnet2/train/__init__.py,0,b''
test/espnet2/train/test_collate_fn.py,0,"b'import numpy as np\nimport pytest\n\nfrom espnet2.train.collate_fn import common_collate_fn\nfrom espnet2.train.collate_fn import CommonCollateFn\n\n\n@pytest.mark.parametrize(\n    ""float_pad_value, int_pad_value, not_sequence"",\n    [(0.0, -1, ()), (3.0, 2, (""a"",)), (np.inf, 100, (""a"", ""b""))],\n)\ndef test_common_collate_fn(float_pad_value, int_pad_value, not_sequence):\n    data = [\n        (""id"", dict(a=np.random.randn(3, 5), b=np.random.randn(4).astype(np.long))),\n        (""id2"", dict(a=np.random.randn(2, 5), b=np.random.randn(3).astype(np.long))),\n    ]\n    t = common_collate_fn(\n        data,\n        float_pad_value=float_pad_value,\n        int_pad_value=int_pad_value,\n        not_sequence=not_sequence,\n    )\n\n    desired = dict(\n        a=np.stack(\n            [\n                data[0][1][""a""],\n                np.pad(\n                    data[1][1][""a""],\n                    [(0, 1), (0, 0)],\n                    mode=""constant"",\n                    constant_values=float_pad_value,\n                ),\n            ]\n        ),\n        b=np.stack(\n            [\n                data[0][1][""b""],\n                np.pad(\n                    data[1][1][""b""],\n                    [(0, 1)],\n                    mode=""constant"",\n                    constant_values=int_pad_value,\n                ),\n            ]\n        ),\n        a_lengths=np.array([3, 2], dtype=np.long),\n        b_lengths=np.array([4, 3], dtype=np.long),\n    )\n\n    np.testing.assert_array_equal(t[1][""a""], desired[""a""])\n    np.testing.assert_array_equal(t[1][""b""], desired[""b""])\n\n    if ""a"" not in not_sequence:\n        np.testing.assert_array_equal(t[1][""a_lengths""], desired[""a_lengths""])\n    if ""b"" not in not_sequence:\n        np.testing.assert_array_equal(t[1][""b_lengths""], desired[""b_lengths""])\n\n\n@pytest.mark.parametrize(\n    ""float_pad_value, int_pad_value, not_sequence"",\n    [(0.0, -1, ()), (3.0, 2, (""a"",)), (np.inf, 100, (""a"", ""b""))],\n)\ndef test_(float_pad_value, int_pad_value, not_sequence):\n    _common_collate_fn = CommonCollateFn(\n        float_pad_value=float_pad_value,\n        int_pad_value=int_pad_value,\n        not_sequence=not_sequence,\n    )\n    data = [\n        (""id"", dict(a=np.random.randn(3, 5), b=np.random.randn(4).astype(np.long))),\n        (""id2"", dict(a=np.random.randn(2, 5), b=np.random.randn(3).astype(np.long))),\n    ]\n    t = _common_collate_fn(data)\n\n    desired = dict(\n        a=np.stack(\n            [\n                data[0][1][""a""],\n                np.pad(\n                    data[1][1][""a""],\n                    [(0, 1), (0, 0)],\n                    mode=""constant"",\n                    constant_values=float_pad_value,\n                ),\n            ]\n        ),\n        b=np.stack(\n            [\n                data[0][1][""b""],\n                np.pad(\n                    data[1][1][""b""],\n                    [(0, 1)],\n                    mode=""constant"",\n                    constant_values=int_pad_value,\n                ),\n            ]\n        ),\n        a_lengths=np.array([3, 2], dtype=np.long),\n        b_lengths=np.array([4, 3], dtype=np.long),\n    )\n\n    np.testing.assert_array_equal(t[1][""a""], desired[""a""])\n    np.testing.assert_array_equal(t[1][""b""], desired[""b""])\n\n    if ""a"" not in not_sequence:\n        np.testing.assert_array_equal(t[1][""a_lengths""], desired[""a_lengths""])\n    if ""b"" not in not_sequence:\n        np.testing.assert_array_equal(t[1][""b_lengths""], desired[""b_lengths""])\n\n\n@pytest.mark.parametrize(\n    ""float_pad_value, int_pad_value, not_sequence"",\n    [(0.0, -1, ()), (3.0, 2, (""a"",)), (np.inf, 100, (""a"", ""b""))],\n)\ndef test_CommonCollateFn_repr(float_pad_value, int_pad_value, not_sequence):\n    print(\n        CommonCollateFn(\n            float_pad_value=float_pad_value,\n            int_pad_value=int_pad_value,\n            not_sequence=not_sequence,\n        )\n    )\n'"
test/espnet2/train/test_dataset.py,0,"b'import h5py\nimport kaldiio\nimport numpy as np\nfrom PIL import Image\nimport pytest\nimport soundfile\n\nfrom espnet2.train.dataset import ESPnetDataset\nfrom espnet2.utils.fileio import NpyScpWriter\nfrom espnet2.utils.fileio import SoundScpWriter\n\n\ndef preprocess(id: str, data):\n    new_data = {}\n    for k, v in data.items():\n        if isinstance(v, str):\n            if v == ""hello world"":\n                new_data[k] = np.array([0])\n            elif v == ""foo bar"":\n                new_data[k] = np.array([1])\n            else:\n                new_data[k] = np.array([2])\n        else:\n            new_data[k] = v\n    return new_data\n\n\n@pytest.fixture\ndef sound_scp(tmp_path):\n    p = tmp_path / ""wav.scp""\n    w = SoundScpWriter(tmp_path / ""data"", p)\n    w[""a""] = 16000, np.random.randint(-100, 100, (160000,), dtype=np.int16)\n    w[""b""] = 16000, np.random.randint(-100, 100, (80000,), dtype=np.int16)\n    return str(p)\n\n\ndef test_ESPnetDataset_sound_scp(sound_scp):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(sound_scp, ""data1"", ""sound"")], preprocess=preprocess,\n    )\n    print(dataset)\n    print(dataset.names())\n    assert len(dataset) == 2\n    assert dataset.has_name(""data1"")\n\n    _, data = dataset[""a""]\n    assert data[""data1""].shape == (160000,)\n\n    _, data = dataset[""b""]\n    assert data[""data1""].shape == (80000,)\n\n\n@pytest.fixture\ndef pipe_wav(tmp_path):\n    p = tmp_path / ""wav.scp""\n    soundfile.write(\n        tmp_path / ""a.wav"",\n        np.random.randint(-100, 100, (160000,), dtype=np.int16),\n        16000,\n    )\n    soundfile.write(\n        tmp_path / ""b.wav"",\n        np.random.randint(-100, 100, (80000,), dtype=np.int16),\n        16000,\n    )\n    with p.open(""w"") as f:\n        f.write(f""a {tmp_path / \'a.wav\'}\\n"")\n        f.write(f""b {tmp_path / \'b.wav\'}\\n"")\n    return str(p)\n\n\ndef test_ESPnetDataset_pipe_wav(pipe_wav):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(pipe_wav, ""data1"", ""pipe_wav"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert data[""data1""].shape == (160000,)\n\n    _, data = dataset[""b""]\n    assert data[""data1""].shape == (80000,)\n\n\n@pytest.fixture\ndef feats_scp(tmp_path):\n    p = tmp_path / ""feats.scp""\n    p2 = tmp_path / ""feats.ark""\n    with kaldiio.WriteHelper(f""ark,scp:{p2},{p}"") as w:\n        w[""a""] = np.random.randn(100, 80)\n        w[""b""] = np.random.randn(150, 80)\n    return str(p)\n\n\ndef test_ESPnetDataset_feats_scp(feats_scp,):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(feats_scp, ""data2"", ""kaldi_ark"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert data[""data2""].shape == (100, 80,)\n\n    _, data = dataset[""b""]\n    assert data[""data2""].shape == (150, 80,)\n\n\n@pytest.fixture\ndef npy_scp(tmp_path):\n    p = tmp_path / ""npy.scp""\n    w = NpyScpWriter(tmp_path / ""data"", p)\n    w[""a""] = np.random.randn(100, 80)\n    w[""b""] = np.random.randn(150, 80)\n    return str(p)\n\n\ndef test_ESPnetDataset_npy_scp(npy_scp):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(npy_scp, ""data3"", ""npy"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert data[""data3""].shape == (100, 80,)\n\n    _, data = dataset[""b""]\n    assert data[""data3""].shape == (150, 80,)\n\n\n@pytest.fixture\ndef h5file_1(tmp_path):\n    p = tmp_path / ""file.h5""\n    with h5py.File(p, ""w"") as w:\n        w[""a""] = np.random.randn(100, 80)\n        w[""b""] = np.random.randn(150, 80)\n    return str(p)\n\n\n@pytest.fixture\ndef h5file_2(tmp_path):\n    p = tmp_path / ""file.h5""\n    with h5py.File(p, ""w"") as w:\n        w[""a/input""] = np.random.randn(100, 80)\n        w[""a/target""] = np.random.randint(0, 10, (10,))\n        w[""b/input""] = np.random.randn(150, 80)\n        w[""b/target""] = np.random.randint(0, 10, (13,))\n    return str(p)\n\n\ndef test_ESPnetDataset_h5file_1(h5file_1):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(h5file_1, ""data4"", ""hdf5"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert data[""data4""].shape == (100, 80,)\n\n    _, data = dataset[""b""]\n    assert data[""data4""].shape == (150, 80,)\n\n\ndef test_ESPnetDataset_h5file_2(h5file_2):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(h5file_2, ""data1"", ""hdf5"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert data[""data1_input""].shape == (100, 80)\n    assert data[""data1_target""].shape == (10,)\n\n    _, data = dataset[""b""]\n    assert data[""data1_input""].shape == (150, 80)\n    assert data[""data1_target""].shape == (13,)\n\n\n@pytest.fixture\ndef shape_file(tmp_path):\n    p = tmp_path / ""shape.txt""\n    with p.open(""w"") as f:\n        f.write(""a 100,80\\n"")\n        f.write(""b 150,80\\n"")\n    return str(p)\n\n\ndef test_ESPnetDataset_rand_float(shape_file):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(shape_file, ""data5"", ""rand_float"")],\n        preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert data[""data5""].shape == (100, 80,)\n\n    _, data = dataset[""b""]\n    assert data[""data5""].shape == (150, 80,)\n\n\ndef test_ESPnetDataset_rand_int(shape_file):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(shape_file, ""data6"", ""rand_int_0_10"")],\n        preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert data[""data6""].shape == (100, 80,)\n\n    _, data = dataset[""b""]\n    assert data[""data6""].shape == (150, 80,)\n\n\n@pytest.fixture\ndef text(tmp_path):\n    p = tmp_path / ""text""\n    with p.open(""w"") as f:\n        f.write(""a hello world\\n"")\n        f.write(""b foo bar\\n"")\n    return str(p)\n\n\ndef test_ESPnetDataset_text(text):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(text, ""data7"", ""text"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert tuple(data[""data7""]) == (0,)\n\n    _, data = dataset[""b""]\n    assert tuple(data[""data7""]) == (1,)\n\n\n@pytest.fixture\ndef text_float(tmp_path):\n    p = tmp_path / ""shape.txt""\n    with p.open(""w"") as f:\n        f.write(""a 1.4 3.4\\n"")\n        f.write(""b 0.9 9.3\\n"")\n    return str(p)\n\n\ndef test_ESPnetDataset_text_float(text_float):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(text_float, ""data8"", ""text_float"")],\n        preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert all((data[""data8""]) == np.array([1.4, 3.4], dtype=np.float32))\n\n    _, data = dataset[""b""]\n    assert all((data[""data8""]) == np.array([0.9, 9.3], dtype=np.float32))\n\n\n@pytest.fixture\ndef text_int(tmp_path):\n    p = tmp_path / ""shape.txt""\n    with p.open(""w"") as f:\n        f.write(""a 0 1 2\\n"")\n        f.write(""b 2 3 4\\n"")\n    return str(p)\n\n\ndef test_ESPnetDataset_text_int(text_int):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(text_int, ""data8"", ""text_int"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert tuple(data[""data8""]) == (0, 1, 2)\n\n    _, data = dataset[""b""]\n    assert tuple(data[""data8""]) == (2, 3, 4)\n\n\n@pytest.fixture\ndef csv_float(tmp_path):\n    p = tmp_path / ""shape.txt""\n    with p.open(""w"") as f:\n        f.write(""a 1.4,3.4\\n"")\n        f.write(""b 0.9,9.3\\n"")\n    return str(p)\n\n\ndef test_ESPnetDataset_csv_float(csv_float):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(csv_float, ""data8"", ""csv_float"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert all((data[""data8""]) == np.array([1.4, 3.4], dtype=np.float32))\n\n    _, data = dataset[""b""]\n    assert all((data[""data8""]) == np.array([0.9, 9.3], dtype=np.float32))\n\n\n@pytest.fixture\ndef csv_int(tmp_path):\n    p = tmp_path / ""shape.txt""\n    with p.open(""w"") as f:\n        f.write(""a 0,1,2\\n"")\n        f.write(""b 2,3,4\\n"")\n    return str(p)\n\n\ndef test_ESPnetDataset_csv_int(csv_int):\n    dataset = ESPnetDataset(\n        path_name_type_list=[(csv_int, ""data8"", ""csv_int"")], preprocess=preprocess,\n    )\n\n    _, data = dataset[""a""]\n    assert tuple(data[""data8""]) == (0, 1, 2)\n\n    _, data = dataset[""b""]\n    assert tuple(data[""data8""]) == (2, 3, 4)\n\n\n@pytest.fixture\ndef imagefolder(tmp_path):\n    p = tmp_path / ""img""\n    (p / ""a"").mkdir(parents=True)\n    (p / ""b"").mkdir(parents=True)\n    a = np.random.rand(30, 30, 3) * 255\n    im_out = Image.fromarray(a.astype(""uint8"")).convert(""RGBA"")\n    im_out.save(p / ""a"" / ""foo.png"")\n    a = np.random.rand(30, 30, 3) * 255\n    im_out = Image.fromarray(a.astype(""uint8"")).convert(""RGBA"")\n    im_out.save(p / ""b"" / ""foo.png"")\n    return str(p)\n\n\ndef test_ESPnetDataset_imagefolder(imagefolder):\n    pytest.importorskip(""torchvision"")\n\n    dataset = ESPnetDataset(\n        path_name_type_list=[(imagefolder, ""data1"", ""imagefolder_32x32"")],\n        preprocess=preprocess,\n    )\n\n    _, data = dataset[0]\n    assert data[""data1_0""].shape == (3, 32, 32)\n    assert data[""data1_1""] == (0,)\n    _, data = dataset[1]\n    assert data[""data1_0""].shape == (3, 32, 32)\n    assert data[""data1_1""] == (1,)\n'"
test/espnet2/train/test_distributed_utils.py,0,"b'import argparse\nfrom concurrent.futures.process import ProcessPoolExecutor\nfrom concurrent.futures.thread import ThreadPoolExecutor\nimport unittest.mock\n\nimport pytest\n\nfrom espnet2.tasks.abs_task import AbsTask\nfrom espnet2.train.distributed_utils import DistributedOption\nfrom espnet2.train.distributed_utils import free_port\nfrom espnet2.train.distributed_utils import resolve_distributed_mode\nfrom espnet2.utils.build_dataclass import build_dataclass\n\n\n@pytest.fixture()\ndef dist_init_method(tmp_path):\n    return f""file://{tmp_path}/init""\n\n\ndef test_default_work():\n    parser = AbsTask.get_parser()\n    args = parser.parse_args([])\n    resolve_distributed_mode(args)\n    option = build_dataclass(DistributedOption, args)\n    option.init()\n\n\ndef test_resolve_distributed_mode1(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=False,\n        dist_world_size=2,\n        dist_rank=None,\n        ngpu=2,\n        local_rank=0,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    with pytest.raises(RuntimeError):\n        resolve_distributed_mode(args)\n\n\ndef test_resolve_distributed_mode2(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=False,\n        dist_world_size=2,\n        dist_rank=0,\n        ngpu=2,\n        local_rank=None,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    with pytest.raises(RuntimeError):\n        resolve_distributed_mode(args)\n\n\ndef test_resolve_distributed_mode3(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=False,\n        dist_world_size=None,\n        dist_rank=None,\n        ngpu=2,\n        local_rank=None,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    resolve_distributed_mode(args)\n\n\ndef test_resolve_distributed_mode4(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=False,\n        dist_world_size=2,\n        dist_rank=0,\n        ngpu=2,\n        local_rank=1,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    resolve_distributed_mode(args)\n    assert args.distributed\n\n\ndef test_resolve_distributed_mode5(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=False,\n        dist_world_size=2,\n        dist_rank=0,\n        ngpu=2,\n        local_rank=1,\n        dist_launcher=""slurm"",\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    with pytest.raises(RuntimeError):\n        resolve_distributed_mode(args)\n\n\ndef test_resolve_distributed_mode6(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=2,\n        dist_rank=None,\n        ngpu=1,\n        local_rank=None,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    with pytest.raises(RuntimeError):\n        resolve_distributed_mode(args)\n\n\ndef test_resolve_distributed_mode7(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=2,\n        dist_rank=0,\n        ngpu=1,\n        local_rank=None,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    resolve_distributed_mode(args)\n    assert args.distributed\n    assert not args.multiprocessing_distributed\n\n\ndef test_resolve_distributed_mode9(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=1,\n        dist_rank=None,\n        ngpu=2,\n        local_rank=None,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    resolve_distributed_mode(args)\n    assert args.distributed\n    assert args.multiprocessing_distributed\n\n\ndef test_resolve_distributed_mode10(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=None,\n        dist_rank=None,\n        ngpu=1,\n        local_rank=None,\n        dist_launcher=None,\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    resolve_distributed_mode(args)\n    assert not args.distributed\n    assert not args.multiprocessing_distributed\n\n\ndef test_init_cpu(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=2,\n        dist_rank=None,\n        ngpu=0,\n        local_rank=None,\n        dist_launcher=None,\n        distributed=True,\n        dist_backend=""gloo"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    args.dist_rank = 0\n    option = build_dataclass(DistributedOption, args)\n    args.dist_rank = 1\n    option2 = build_dataclass(DistributedOption, args)\n    with ProcessPoolExecutor(max_workers=2) as e:\n        fn = e.submit(option.init)\n        fn2 = e.submit(option2.init)\n        fn.result()\n        fn2.result()\n\n\ndef test_init_cpu2():\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=2,\n        dist_rank=None,\n        ngpu=0,\n        local_rank=None,\n        dist_launcher=None,\n        distributed=True,\n        dist_backend=""gloo"",\n        dist_init_method=""env://"",\n        dist_master_addr=None,\n        dist_master_port=free_port(),\n    )\n    args.dist_rank = 0\n    option = build_dataclass(DistributedOption, args)\n    args.dist_rank = 1\n    option2 = build_dataclass(DistributedOption, args)\n    with ProcessPoolExecutor(max_workers=2) as e:\n        fn = e.submit(option.init)\n        fn2 = e.submit(option2.init)\n        with pytest.raises(RuntimeError):\n            fn.result()\n            fn2.result()\n\n\ndef test_init_cpu3():\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=2,\n        dist_rank=None,\n        ngpu=0,\n        local_rank=None,\n        dist_launcher=None,\n        distributed=True,\n        dist_backend=""gloo"",\n        dist_init_method=""env://"",\n        dist_master_addr=""localhost"",\n        dist_master_port=None,\n    )\n    args.dist_rank = 0\n    option = build_dataclass(DistributedOption, args)\n    args.dist_rank = 1\n    option2 = build_dataclass(DistributedOption, args)\n    with ThreadPoolExecutor(max_workers=2) as e:\n        fn = e.submit(option.init)\n        fn2 = e.submit(option2.init)\n        with pytest.raises(RuntimeError):\n            fn.result()\n            fn2.result()\n\n\ndef test_init_cpu4():\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=2,\n        dist_rank=None,\n        ngpu=0,\n        local_rank=None,\n        dist_launcher=None,\n        distributed=True,\n        dist_backend=""gloo"",\n        dist_init_method=""env://"",\n        dist_master_addr=""localhost"",\n        dist_master_port=free_port(),\n    )\n    args.dist_rank = 0\n    option = build_dataclass(DistributedOption, args)\n    args.dist_rank = 1\n    option2 = build_dataclass(DistributedOption, args)\n    with ProcessPoolExecutor(max_workers=2) as e:\n        fn = e.submit(option.init)\n        fn2 = e.submit(option2.init)\n        fn.result()\n        fn2.result()\n\n\ndef test_init_cpu5():\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=2,\n        dist_rank=None,\n        ngpu=0,\n        local_rank=None,\n        dist_launcher=None,\n        distributed=True,\n        dist_backend=""gloo"",\n        dist_init_method=""env://"",\n        dist_master_addr=""localhost"",\n        dist_master_port=free_port(),\n    )\n    args.dist_rank = 0\n    option = build_dataclass(DistributedOption, args)\n    args.dist_rank = 1\n    option2 = build_dataclass(DistributedOption, args)\n    with ProcessPoolExecutor(max_workers=2) as e:\n        fn = e.submit(option.init)\n        fn2 = e.submit(option2.init)\n        fn.result()\n        fn2.result()\n\n\ndef test_resolve_distributed_mode_slurm1(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=False,\n        dist_world_size=None,\n        dist_rank=None,\n        ngpu=2,\n        local_rank=None,\n        dist_launcher=""slurm"",\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    with unittest.mock.patch.dict(\n        ""os.environ"",\n        dict(\n            SLURM_PROCID=""0"",\n            SLURM_NTASKS=""2"",\n            SLURM_STEP_NUM_NODES=""2"",\n            SLURM_STEP_NODELIST=""host1"",\n            SLURM_NODEID=""0"",\n            SLURM_LOCALID=""0"",\n            CUDA_VISIBLE_DEVICES=""0,1"",\n        ),\n    ):\n        resolve_distributed_mode(args)\n\n\ndef test_resolve_distributed_mode_slurm2(dist_init_method):\n    args = argparse.Namespace(\n        multiprocessing_distributed=False,\n        dist_world_size=None,\n        dist_rank=None,\n        ngpu=2,\n        local_rank=None,\n        dist_launcher=""slurm"",\n        dist_backend=""nccl"",\n        dist_init_method=dist_init_method,\n        dist_master_addr=None,\n        dist_master_port=None,\n    )\n    with unittest.mock.patch.dict(\n        ""os.environ"",\n        dict(\n            SLURM_PROCID=""0"",\n            SLURM_NTASKS=""2"",\n            SLURM_STEP_NUM_NODES=""1"",\n            SLURM_STEP_NODELIST=""host1"",\n            SLURM_NODEID=""0"",\n            SLURM_LOCALID=""0"",\n            CUDA_VISIBLE_DEVICES=""0,1"",\n        ),\n    ):\n        with pytest.raises(RuntimeError):\n            resolve_distributed_mode(args)\n\n\ndef test_resolve_distributed_mode_slurm3():\n    args = argparse.Namespace(\n        multiprocessing_distributed=True,\n        dist_world_size=None,\n        dist_rank=None,\n        ngpu=1,\n        local_rank=None,\n        dist_launcher=""slurm"",\n        dist_backend=""nccl"",\n        dist_init_method=""env://"",\n        dist_master_addr=None,\n        dist_master_port=10000,\n    )\n    env = dict(\n        SLURM_PROCID=""0"",\n        SLURM_NTASKS=""1"",\n        SLURM_STEP_NUM_NODES=""1"",\n        SLURM_STEP_NODELIST=""localhost"",\n        SLURM_NODEID=""0"",\n        CUDA_VISIBLE_DEVICES=""0,1"",\n    )\n\n    e = ProcessPoolExecutor(max_workers=2)\n    with unittest.mock.patch.dict(""os.environ"", dict(env, SLURM_LOCALID=""0"")):\n        resolve_distributed_mode(args)\n        option = build_dataclass(DistributedOption, args)\n        fn = e.submit(option.init)\n\n    with unittest.mock.patch.dict(""os.environ"", dict(env, SLURM_LOCALID=""0"")):\n        option2 = build_dataclass(DistributedOption, args)\n        fn2 = e.submit(option2.init)\n\n    fn.result()\n    fn2.result()\n'"
test/espnet2/train/test_reporter.py,4,"b'from distutils.version import LooseVersion\nimport logging\nfrom pathlib import Path\nimport uuid\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom espnet2.train.reporter import aggregate\nfrom espnet2.train.reporter import Average\nfrom espnet2.train.reporter import ReportedValue\nfrom espnet2.train.reporter import Reporter\n\n\n@pytest.mark.parametrize(""weight1,weight2"", [(None, None), (19, np.array(9))])\ndef test_register(weight1, weight2):\n    reporter = Reporter()\n    reporter.set_epoch(1)\n    with reporter.observe(uuid.uuid4().hex) as sub:\n        stats1 = {\n            ""float"": 0.6,\n            ""int"": 6,\n            ""np"": np.random.random(),\n            ""torch"": torch.rand(1),\n            ""none"": None,\n        }\n        sub.register(stats1, weight1)\n        stats2 = {\n            ""float"": 0.3,\n            ""int"": 100,\n            ""np"": np.random.random(),\n            ""torch"": torch.rand(1),\n            ""none"": None,\n        }\n        sub.register(stats2, weight2)\n        assert sub.get_epoch() == 1\n    with pytest.raises(RuntimeError):\n        sub.register({})\n\n    desired = {}\n    for k in stats1:\n        if stats1[k] is None:\n            continue\n\n        if weight1 is None:\n            desired[k] = (stats1[k] + stats2[k]) / 2\n        else:\n            weight1 = float(weight1)\n            weight2 = float(weight2)\n            desired[k] = float(weight1 * stats1[k] + weight2 * stats2[k])\n            desired[k] /= weight1 + weight2\n\n    for k1, k2 in reporter.get_all_keys():\n        if k2 in (""time"", ""total_count""):\n            continue\n        np.testing.assert_allclose(reporter.get_value(k1, k2), desired[k2])\n\n\n@pytest.mark.parametrize(""mode"", [""min"", ""max"", ""foo""])\ndef test_sort_epochs_and_values(mode):\n    reporter = Reporter()\n    key1 = uuid.uuid4().hex\n    stats_list = [{""aa"": 0.3}, {""aa"": 0.5}, {""aa"": 0.2}]\n    for e in range(len(stats_list)):\n        reporter.set_epoch(e + 1)\n        with reporter.observe(key1) as sub:\n            sub.register(stats_list[e])\n    if mode not in (""min"", ""max""):\n        with pytest.raises(ValueError):\n            reporter.sort_epochs_and_values(key1, ""aa"", mode)\n        return\n    else:\n        sort_values = reporter.sort_epochs_and_values(key1, ""aa"", mode)\n\n    if mode == ""min"":\n        sign = 1\n    else:\n        sign = -1\n    desired = sorted(\n        [(e + 1, stats_list[e][""aa""]) for e in range(len(stats_list))],\n        key=lambda x: sign * x[1],\n    )\n\n    for e in range(len(stats_list)):\n        assert sort_values[e] == desired[e]\n\n\ndef test_sort_epochs_and_values_no_key():\n    reporter = Reporter()\n    key1 = uuid.uuid4().hex\n    stats_list = [{""aa"": 0.3}, {""aa"": 0.5}, {""aa"": 0.2}]\n    for e in range(len(stats_list)):\n        reporter.set_epoch(e + 1)\n        with reporter.observe(key1) as sub:\n            sub.register(stats_list[e])\n    with pytest.raises(KeyError):\n        reporter.sort_epochs_and_values(""foo"", ""bar"", ""min"")\n\n\ndef test_get_value_not_found():\n    reporter = Reporter()\n    with pytest.raises(KeyError):\n        reporter.get_value(""a"", ""b"")\n\n\ndef test_sort_values():\n    mode = ""min""\n    reporter = Reporter()\n    key1 = uuid.uuid4().hex\n    stats_list = [{""aa"": 0.3}, {""aa"": 0.5}, {""aa"": 0.2}]\n    for e in range(len(stats_list)):\n        reporter.set_epoch(e + 1)\n        with reporter.observe(key1) as sub:\n            sub.register(stats_list[e])\n    sort_values = reporter.sort_values(key1, ""aa"", mode)\n\n    desired = sorted([stats_list[e][""aa""] for e in range(len(stats_list))],)\n\n    for e in range(len(stats_list)):\n        assert sort_values[e] == desired[e]\n\n\ndef test_sort_epochs():\n    mode = ""min""\n    reporter = Reporter()\n    key1 = uuid.uuid4().hex\n    stats_list = [{""aa"": 0.3}, {""aa"": 0.5}, {""aa"": 0.2}]\n    for e in range(len(stats_list)):\n        reporter.set_epoch(e + 1)\n        with reporter.observe(key1) as sub:\n            sub.register(stats_list[e])\n    sort_values = reporter.sort_epochs(key1, ""aa"", mode)\n\n    desired = sorted(\n        [(e + 1, stats_list[e][""aa""]) for e in range(len(stats_list))],\n        key=lambda x: x[1],\n    )\n\n    for e in range(len(stats_list)):\n        assert sort_values[e] == desired[e][0]\n\n\ndef test_best_epoch():\n    mode = ""min""\n    reporter = Reporter()\n    key1 = uuid.uuid4().hex\n    stats_list = [{""aa"": 0.3}, {""aa"": 0.5}, {""aa"": 0.2}]\n    for e in range(len(stats_list)):\n        reporter.set_epoch(e + 1)\n        with reporter.observe(key1) as sub:\n            sub.register(stats_list[e])\n    best_epoch = reporter.get_best_epoch(key1, ""aa"", mode)\n    assert best_epoch == 3\n\n\ndef test_check_early_stopping():\n    mode = ""min""\n    reporter = Reporter()\n    key1 = uuid.uuid4().hex\n    stats_list = [{""aa"": 0.3}, {""aa"": 0.2}, {""aa"": 0.4}, {""aa"": 0.3}]\n    patience = 1\n\n    results = []\n    for e in range(len(stats_list)):\n        reporter.set_epoch(e + 1)\n        with reporter.observe(key1) as sub:\n            sub.register(stats_list[e])\n        truefalse = reporter.check_early_stopping(patience, key1, ""aa"", mode)\n        results.append(truefalse)\n    assert results == [False, False, False, True]\n\n\ndef test_logging():\n    reporter = Reporter()\n    key1 = uuid.uuid4().hex\n    key2 = uuid.uuid4().hex\n    stats_list = [\n        {""aa"": 0.3, ""bb"": 3.0},\n        {""aa"": 0.5, ""bb"": 3.0},\n        {""aa"": 0.2, ""bb"": 3.0},\n    ]\n    for e in range(len(stats_list)):\n        reporter.set_epoch(e + 1)\n        with reporter.observe(key1) as sub:\n            sub.register(stats_list[e])\n        with reporter.observe(key2) as sub:\n            sub.register(stats_list[e])\n            logging.info(sub.log_message())\n        with pytest.raises(RuntimeError):\n            logging.info(sub.log_message())\n\n    logging.info(reporter.log_message())\n\n\ndef test_has_key():\n    reporter = Reporter()\n    reporter.set_epoch(1)\n    key1 = uuid.uuid4().hex\n    with reporter.observe(key1) as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n    assert reporter.has(key1, ""aa"")\n\n\ndef test_get_Keys():\n    reporter = Reporter()\n    reporter.set_epoch(1)\n    key1 = uuid.uuid4().hex\n    with reporter.observe(key1) as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n    assert reporter.get_keys() == (key1,)\n\n\ndef test_get_Keys2():\n    reporter = Reporter()\n    reporter.set_epoch(1)\n    key1 = uuid.uuid4().hex\n    with reporter.observe(key1) as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n    assert reporter.get_keys2(key1) == (""aa"",)\n\n\ndef test_matplotlib_plot(tmp_path: Path):\n    reporter = Reporter()\n    reporter.set_epoch(1)\n    key1 = uuid.uuid4().hex\n    with reporter.observe(key1) as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n\n    reporter.set_epoch(1)\n    with reporter.observe(key1) as sub:\n        # Skip epoch=2\n        sub.register({})\n\n    reporter.set_epoch(3)\n    with reporter.observe(key1) as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n\n    reporter.matplotlib_plot(tmp_path)\n    assert (tmp_path / ""aa.png"").exists()\n\n\ndef test_tensorboard_add_scalar(tmp_path: Path):\n    reporter = Reporter()\n    reporter.set_epoch(1)\n    key1 = uuid.uuid4().hex\n    with reporter.observe(key1) as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n\n    reporter.set_epoch(1)\n    with reporter.observe(key1) as sub:\n        # Skip epoch=2\n        sub.register({})\n\n    reporter.set_epoch(3)\n    with reporter.observe(key1) as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n\n    if LooseVersion(torch.__version__) >= LooseVersion(""1.1.0""):\n        from torch.utils.tensorboard import SummaryWriter\n    else:\n        from tensorboardX import SummaryWriter\n    writer = SummaryWriter(tmp_path)\n    reporter.tensorboard_add_scalar(writer)\n\n\ndef test_state_dict():\n    reporter = Reporter()\n    reporter.set_epoch(1)\n    with reporter.observe(""train"") as sub:\n        stats1 = {""aa"": 0.6}\n        sub.register(stats1)\n    with reporter.observe(""eval"") as sub:\n        stats1 = {""bb"": 0.6}\n        sub.register(stats1)\n    state = reporter.state_dict()\n\n    reporter2 = Reporter()\n    reporter2.load_state_dict(state)\n    state2 = reporter2.state_dict()\n\n    assert state == state2\n\n\ndef test_get_epoch():\n    reporter = Reporter(2)\n    assert reporter.get_epoch() == 2\n\n\ndef test_total_count():\n    reporter = Reporter(2)\n    assert reporter.get_epoch() == 2\n    with reporter.observe(""train"", 1) as sub:\n        sub.register({})\n    with reporter.observe(""train"", 2) as sub:\n        sub.register({})\n        sub.register({})\n        assert sub.get_total_count() == 3\n\n\ndef test_change_epoch():\n    reporter = Reporter()\n    with pytest.raises(RuntimeError):\n        with reporter.observe(""train"", 1):\n            reporter.set_epoch(2)\n\n\ndef test_minus_epoch():\n    with pytest.raises(ValueError):\n        Reporter(-1)\n\n\ndef test_minus_epoch2():\n    reporter = Reporter()\n    with pytest.raises(ValueError):\n        reporter.set_epoch(-1)\n    reporter.start_epoch(""aa"", 1)\n    with pytest.raises(ValueError):\n        reporter.start_epoch(""aa"", -1)\n\n\ndef test_register_array():\n    reporter = Reporter()\n    with reporter.observe(""train"", 1) as sub:\n        with pytest.raises(ValueError):\n            sub.register({""a"": np.array([0, 1])})\n        with pytest.raises(ValueError):\n            sub.register({""a"": 1}, weight=np.array([1, 2]))\n\n\ndef test_zero_weight():\n    reporter = Reporter()\n    with reporter.observe(""train"", 1) as sub:\n        sub.register({""a"": 1}, weight=0)\n\n\ndef test_register_nan():\n    reporter = Reporter()\n    with reporter.observe(""train"", 1) as sub:\n        sub.register({""a"": np.nan}, weight=1.0)\n\n\ndef test_no_register():\n    reporter = Reporter()\n    with reporter.observe(""train"", 1):\n        pass\n\n\ndef test_mismatch_key2():\n    reporter = Reporter()\n    with reporter.observe(""train"", 1) as sub:\n        sub.register({""a"": 2})\n    with reporter.observe(""train"", 2) as sub:\n        sub.register({""b"": 3})\n\n\ndef test_reserved():\n    reporter = Reporter()\n    with reporter.observe(""train"", 1) as sub:\n        with pytest.raises(RuntimeError):\n            sub.register({""time"": 2})\n        with pytest.raises(RuntimeError):\n            sub.register({""total_count"": 3})\n\n\ndef test_different_type():\n    reporter = Reporter()\n    with pytest.raises(ValueError):\n        with reporter.observe(""train"", 1) as sub:\n            sub.register({""a"": 2}, weight=1)\n            sub.register({""a"": 3})\n\n\ndef test_start_middle_epoch():\n    reporter = Reporter()\n    with reporter.observe(""train"", 2) as sub:\n        sub.register({""a"": 3})\n\n\ndef test__plot_stats_input_str():\n    reporter = Reporter()\n    with pytest.raises(TypeError):\n        reporter._plot_stats(""aaa"", ""a"")\n\n\nclass DummyReportedValue(ReportedValue):\n    pass\n\n\ndef test_aggregate():\n    vs = [Average(0.1), Average(0.3)]\n    assert aggregate(vs) == 0.2\n    vs = []\n    assert aggregate(vs) is np.nan\n    with pytest.raises(NotImplementedError):\n        vs = [DummyReportedValue()]\n        aggregate(vs)\n\n\ndef test_measure_time():\n    reporter = Reporter()\n    with reporter.observe(""train"", 2) as sub:\n        with sub.measure_time(""foo""):\n            pass\n\n\ndef test_measure_iter_time():\n    reporter = Reporter()\n    with reporter.observe(""train"", 2) as sub:\n        for _ in sub.measure_iter_time(range(3), ""foo""):\n            pass\n'"
test/espnet2/utils/__init__.py,0,b''
test/espnet2/utils/test_build_dataclass.py,0,"b'from argparse import Namespace\nimport dataclasses\n\nimport pytest\n\nfrom espnet2.utils.build_dataclass import build_dataclass\n\n\n@dataclasses.dataclass\nclass A:\n    a: str\n    b: str\n\n\ndef test_build_dataclass():\n    args = Namespace(a=""foo"", b=""bar"")\n    a = build_dataclass(A, args)\n    assert a.a == args.a\n    assert a.b == args.b\n\n\ndef test_build_dataclass_insufficient():\n    args = Namespace(a=""foo"")\n    with pytest.raises(ValueError):\n        build_dataclass(A, args)\n'"
test/espnet2/utils/test_fileio.py,0,"b'from pathlib import Path\n\nimport numpy as np\nimport pytest\nimport soundfile\n\nfrom espnet2.utils.fileio import DatadirWriter\nfrom espnet2.utils.fileio import load_num_sequence_text\nfrom espnet2.utils.fileio import NpyScpReader\nfrom espnet2.utils.fileio import NpyScpWriter\nfrom espnet2.utils.fileio import read_2column_text\nfrom espnet2.utils.fileio import SoundScpReader\nfrom espnet2.utils.fileio import SoundScpWriter\n\n\ndef test_read_2column_text(tmp_path: Path):\n    p = tmp_path / ""dummy.scp""\n    with p.open(""w"") as f:\n        f.write(""abc /some/path/a.wav\\n"")\n        f.write(""def /some/path/b.wav\\n"")\n    d = read_2column_text(p)\n    assert d == {""abc"": ""/some/path/a.wav"", ""def"": ""/some/path/b.wav""}\n\n\n@pytest.mark.parametrize(\n    ""loader_type"", [""text_int"", ""text_float"", ""csv_int"", ""csv_float"", ""dummy""]\n)\ndef test_load_num_sequence_text(loader_type: str, tmp_path: Path):\n    p = tmp_path / ""dummy.txt""\n    if ""csv"" in loader_type:\n        delimiter = "",""\n    else:\n        delimiter = "" ""\n\n    with p.open(""w"") as f:\n        f.write(""abc "" + delimiter.join([""0"", ""1"", ""2""]) + ""\\n"")\n        f.write(""def "" + delimiter.join([""3"", ""4"", ""5""]) + ""\\n"")\n    desired = {""abc"": np.array([0, 1, 2]), ""def"": np.array([3, 4, 5])}\n    if loader_type == ""dummy"":\n        with pytest.raises(ValueError):\n            load_num_sequence_text(p, loader_type=loader_type)\n        return\n    else:\n        target = load_num_sequence_text(p, loader_type=loader_type)\n    for k in desired:\n        np.testing.assert_array_equal(target[k], desired[k])\n\n\ndef test_load_num_sequence_text_invalid(tmp_path: Path):\n    p = tmp_path / ""dummy.txt""\n    with p.open(""w"") as f:\n        f.write(""abc 12.3.3.,4.44\\n"")\n    with pytest.raises(ValueError):\n        load_num_sequence_text(p)\n\n    with p.open(""w"") as f:\n        f.write(""abc\\n"")\n    with pytest.raises(RuntimeError):\n        load_num_sequence_text(p)\n\n    with p.open(""w"") as f:\n        f.write(""abc 1 2\\n"")\n        f.write(""abc 2 4\\n"")\n    with pytest.raises(RuntimeError):\n        load_num_sequence_text(p)\n\n\ndef test_DatadirWriter(tmp_path: Path):\n    writer = DatadirWriter(tmp_path)\n    # enter(), __exit__(), close()\n    with writer as f:\n        # __getitem__()\n        sub = f[""aa""]\n        # __setitem__()\n        sub[""bb""] = ""aa""\n\n        with pytest.raises(TypeError):\n            sub[""bb""] = 1\n        with pytest.raises(RuntimeError):\n            # Already has children\n            f[""aa""] = ""dd""\n        with pytest.raises(RuntimeError):\n            # Is a text\n            sub[""cc""]\n\n        # Create a directory, but set mismatched ids\n        f[""aa2""][""ccccc""] = ""aaa""\n        # Duplicated warning\n        f[""aa2""][""ccccc""] = ""def""\n\n\ndef test_SoundScpReader(tmp_path: Path):\n    audio_path1 = tmp_path / ""a1.wav""\n    audio1 = np.random.randint(-100, 100, 16, dtype=np.int16)\n    audio_path2 = tmp_path / ""a2.wav""\n    audio2 = np.random.randint(-100, 100, 16, dtype=np.int16)\n\n    soundfile.write(audio_path1, audio1, 16)\n    soundfile.write(audio_path2, audio2, 16)\n\n    p = tmp_path / ""dummy.scp""\n    with p.open(""w"") as f:\n        f.write(f""abc {audio_path1}\\n"")\n        f.write(f""def {audio_path2}\\n"")\n\n    desired = {""abc"": (16, audio1), ""def"": (16, audio2)}\n    target = SoundScpReader(p, normalize=False, dtype=np.int16)\n\n    for k in desired:\n        rate1, t = target[k]\n        rate2, d = desired[k]\n        assert rate1 == rate2\n        np.testing.assert_array_equal(t, d)\n\n    assert len(target) == len(desired)\n    assert ""abc"" in target\n    assert ""def"" in target\n    assert tuple(target.keys()) == tuple(desired)\n    assert tuple(target) == tuple(desired)\n    assert target.get_path(""abc"") == str(audio_path1)\n    assert target.get_path(""def"") == str(audio_path2)\n\n\ndef test_SoundScpReader_normalize(tmp_path: Path):\n    audio_path1 = tmp_path / ""a1.wav""\n    audio1 = np.random.randint(-100, 100, 16, dtype=np.int16)\n    audio_path2 = tmp_path / ""a2.wav""\n    audio2 = np.random.randint(-100, 100, 16, dtype=np.int16)\n\n    audio1 = audio1.astype(np.float64) / (np.iinfo(np.int16).max + 1)\n    audio2 = audio2.astype(np.float64) / (np.iinfo(np.int16).max + 1)\n\n    soundfile.write(audio_path1, audio1, 16)\n    soundfile.write(audio_path2, audio2, 16)\n\n    p = tmp_path / ""dummy.scp""\n    with p.open(""w"") as f:\n        f.write(f""abc {audio_path1}\\n"")\n        f.write(f""def {audio_path2}\\n"")\n\n    desired = {""abc"": (16, audio1), ""def"": (16, audio2)}\n    target = SoundScpReader(p, normalize=True)\n\n    for k in desired:\n        rate1, t = target[k]\n        rate2, d = desired[k]\n        assert rate1 == rate2\n        np.testing.assert_array_equal(t, d)\n\n\ndef test_SoundScpWriter(tmp_path: Path):\n    audio1 = np.random.randint(-100, 100, 16, dtype=np.int16)\n    audio2 = np.random.randint(-100, 100, 16, dtype=np.int16)\n    with SoundScpWriter(tmp_path, tmp_path / ""wav.scp"", dtype=np.int16) as writer:\n        writer[""abc""] = 16, audio1\n        writer[""def""] = 16, audio2\n        # Unsupported dimension\n        with pytest.raises(RuntimeError):\n            y = np.random.randint(-100, 100, [16, 1, 1], dtype=np.int16)\n            writer[""ghi""] = 16, y\n    target = SoundScpReader(tmp_path / ""wav.scp"", normalize=False, dtype=np.int16)\n    desired = {""abc"": (16, audio1), ""def"": (16, audio2)}\n\n    for k in desired:\n        rate1, t = target[k]\n        rate2, d = desired[k]\n        assert rate1 == rate2\n        np.testing.assert_array_equal(t, d)\n\n    assert writer.get_path(""abc"") == str(tmp_path / ""abc.wav"")\n    assert writer.get_path(""def"") == str(tmp_path / ""def.wav"")\n\n\ndef test_SoundScpWriter_normalize(tmp_path: Path):\n    audio1 = np.random.randint(-100, 100, 16, dtype=np.int16)\n    audio2 = np.random.randint(-100, 100, 16, dtype=np.int16)\n    audio1 = audio1.astype(np.float64) / (np.iinfo(np.int16).max + 1)\n    audio2 = audio2.astype(np.float64) / (np.iinfo(np.int16).max + 1)\n\n    with SoundScpWriter(tmp_path, tmp_path / ""wav.scp"", dtype=np.int16) as writer:\n        writer[""abc""] = 16, audio1\n        writer[""def""] = 16, audio2\n        # Unsupported dimension\n        with pytest.raises(RuntimeError):\n            y = np.random.randint(-100, 100, [16, 1, 1], dtype=np.int16)\n            writer[""ghi""] = 16, y\n    target = SoundScpReader(tmp_path / ""wav.scp"", normalize=True, dtype=np.float64)\n    desired = {""abc"": (16, audio1), ""def"": (16, audio2)}\n\n    for k in desired:\n        rate1, t = target[k]\n        rate2, d = desired[k]\n        assert rate1 == rate2\n        np.testing.assert_array_equal(t, d)\n\n\ndef test_NpyScpReader(tmp_path: Path):\n    npy_path1 = tmp_path / ""a1.npy""\n    array1 = np.random.randn(1)\n    npy_path2 = tmp_path / ""a2.npy""\n    array2 = np.random.randn(1, 1, 10)\n    np.save(npy_path1, array1)\n    np.save(npy_path2, array2)\n\n    p = tmp_path / ""dummy.scp""\n    with p.open(""w"") as f:\n        f.write(f""abc {npy_path1}\\n"")\n        f.write(f""def {npy_path2}\\n"")\n\n    desired = {""abc"": array1, ""def"": array2}\n    target = NpyScpReader(p)\n\n    for k in desired:\n        t = target[k]\n        d = desired[k]\n        np.testing.assert_array_equal(t, d)\n\n    assert len(target) == len(desired)\n    assert ""abc"" in target\n    assert ""def"" in target\n    assert tuple(target.keys()) == tuple(desired)\n    assert tuple(target) == tuple(desired)\n    assert target.get_path(""abc"") == str(npy_path1)\n    assert target.get_path(""def"") == str(npy_path2)\n\n\ndef test_NpyScpWriter(tmp_path: Path):\n    array1 = np.random.randn(1)\n    array2 = np.random.randn(1, 1, 10)\n    with NpyScpWriter(tmp_path, tmp_path / ""feats.scp"") as writer:\n        writer[""abc""] = array1\n        writer[""def""] = array2\n    target = NpyScpReader(tmp_path / ""feats.scp"")\n    desired = {""abc"": array1, ""def"": array2}\n\n    for k in desired:\n        t = target[k]\n        d = desired[k]\n        np.testing.assert_array_equal(t, d)\n\n    assert writer.get_path(""abc"") == str(tmp_path / ""abc.npy"")\n    assert writer.get_path(""def"") == str(tmp_path / ""def.npy"")\n'"
test/espnet2/utils/test_get_default_kwargs.py,0,"b'from typing import Any\n\nimport pytest\n\nfrom espnet2.utils.get_default_kwargs import get_default_kwargs\n\n\nclass Dummy:\n    pass\n\n\ndef func1(a, b=3):\n    pass\n\n\ndef func2(b=[{1, 2, 3}]):\n    pass\n\n\ndef func3(b=dict(c=4), d=6.7):\n    pass\n\n\ndef func4(b=Dummy()):\n    pass\n\n\ndef func5(b={3: 5}):\n    pass\n\n\ndef func6(b=(3, 5)):\n    pass\n\n\ndef func7(b=(4, Dummy())):\n    pass\n\n\n@pytest.mark.parametrize(\n    ""func, desired"",\n    [\n        (func1, {""b"": 3}),\n        (func2, {""b"": [[1, 2, 3]]}),\n        (func3, {""b"": {""c"": 4}, ""d"": 6.7}),\n        (func4, {}),\n        (func5, {}),\n        (func6, {""b"": [3, 5]}),\n        (func7, {}),\n    ],\n)\ndef test_get_defaut_kwargs(func, desired: Any):\n    assert get_default_kwargs(func) == desired\n'"
test/espnet2/utils/test_nested_dict_action.py,0,"b'import argparse\nfrom argparse import Namespace\n\nimport pytest\n\nfrom espnet2.utils.nested_dict_action import NestedDictAction\n\n\ndef test_NestedDictAction():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--conf"", action=NestedDictAction, default=3)\n\n    assert parser.parse_args([""--conf"", ""a=3"", ""--conf"", ""c=4""]) == Namespace(\n        conf={""a"": 3, ""c"": 4}\n    )\n    assert parser.parse_args([""--conf"", ""c.d=4""]) == Namespace(conf={""c"": {""d"": 4}})\n    assert parser.parse_args([""--conf"", ""c.d=4"", ""--conf"", ""c=2""]) == Namespace(\n        conf={""c"": 2}\n    )\n    assert parser.parse_args([""--conf"", ""{d: 5, e: 9}""]) == Namespace(\n        conf={""d"": 5, ""e"": 9}\n    )\n    assert parser.parse_args([""--conf"", \'{""d"": 5, ""e"": 9}\']) == Namespace(\n        conf={""d"": 5, ""e"": 9}\n    )\n    assert parser.parse_args(\n        [""--conf"", \'{""d"": 5, ""e"": 9}\', ""--conf"", ""d.e=3""]\n    ) == Namespace(conf={""d"": {""e"": 3}, ""e"": 9})\n\n\ndef test_NestedDictAction_exception():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--conf"", action=NestedDictAction, default={""a"": 4})\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--aa"", ""{d: 5, e: 9}""])\n\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--conf"", ""aaa""])\n\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--conf"", ""[0, 1, 2]""])\n\n    with pytest.raises(SystemExit):\n        parser.parse_args([""--conf"", ""[cd, e, aaa]""])\n'"
test/espnet2/utils/test_pack_funcs.py,0,"b'from pathlib import Path\nimport tarfile\n\nimport pytest\nimport yaml\n\nfrom espnet2.utils.pack_funcs import default_tarinfo\nfrom espnet2.utils.pack_funcs import find_path_and_change_it_recursive\nfrom espnet2.utils.pack_funcs import pack\nfrom espnet2.utils.pack_funcs import unpack\n\n\ndef test_find_path_and_change_it_recursive():\n    target = {""a"": [""foo/path.npy""], ""b"": 3}\n    target = find_path_and_change_it_recursive(target, ""foo/path.npy"", ""bar/path.npy"")\n    assert target == {""a"": [""bar/path.npy""], ""b"": 3}\n\n\ndef test_default_tarinfo():\n    # Just call\n    default_tarinfo(""aaa"")\n\n\ndef test_pack_unpack(tmp_path: Path):\n    files = {""abc.pth"": str(tmp_path / ""foo.pth"")}\n    with (tmp_path / ""foo.pth"").open(""w""):\n        pass\n    with (tmp_path / ""bar.yaml"").open(""w"") as f:\n        # I dared to stack ""/"" to test\n        yaml.safe_dump({""a"": str(tmp_path / ""//foo.pth"")}, f)\n    with (tmp_path / ""a"").open(""w""):\n        pass\n    (tmp_path / ""b"").mkdir(parents=True, exist_ok=True)\n    with (tmp_path / ""b"" / ""a"").open(""w""):\n        pass\n\n    pack(\n        files=files,\n        yaml_files={""def.yaml"": str(tmp_path / ""bar.yaml"")},\n        option=[tmp_path / ""a"", tmp_path / ""b"" / ""a""],\n        outpath=str(tmp_path / ""out.tgz""),\n    )\n\n    retval = unpack(str(tmp_path / ""out.tgz""), str(tmp_path))\n    assert retval == {\n        ""abc"": str(tmp_path / ""packed"" / ""abc.pth""),\n        ""def"": str(tmp_path / ""packed"" / ""def.yaml""),\n        ""option"": [\n            str(tmp_path / ""packed"" / ""option"" / ""a""),\n            str(tmp_path / ""packed"" / ""option"" / ""a.1""),\n        ],\n        ""meta"": str(tmp_path / ""packed"" / ""meta.yaml""),\n    }\n\n\ndef test_pack_not_exist_file():\n    with pytest.raises(FileNotFoundError):\n        pack(files={""a"": ""aaa""}, yaml_files={}, outpath=""out"")\n\n\ndef test_unpack_no_meta_yaml(tmp_path: Path):\n    with tarfile.open(tmp_path / ""a.tgz"", ""w:gz""):\n        pass\n    with pytest.raises(RuntimeError):\n        unpack(str(tmp_path / ""a.tgz""), ""out"")\n'"
test/espnet2/utils/test_sized_dict.py,2,"b'import multiprocessing\nimport sys\n\nimport numpy as np\nimport torch.multiprocessing\n\nfrom espnet2.utils.sized_dict import get_size\nfrom espnet2.utils.sized_dict import SizedDict\n\n\ndef test_get_size():\n    d = {}\n    x = np.random.randn(10)\n    d[""a""] = x\n    size1 = sys.getsizeof(d)\n    assert size1 + get_size(x) + get_size(""a"") == get_size(d)\n\n\ndef test_SizedDict_size():\n    d = SizedDict()\n    assert d.size == 0\n\n    x = np.random.randn(10)\n    d[""a""] = x\n    assert d.size == get_size(x) + sys.getsizeof(""a"")\n\n    y = np.random.randn(10)\n    d[""b""] = y\n    assert d.size == get_size(x) + get_size(y) + sys.getsizeof(""a"") + sys.getsizeof(""b"")\n\n    # Overwrite\n    z = np.random.randn(10)\n    d[""b""] = z\n    assert d.size == get_size(x) + get_size(z) + sys.getsizeof(""a"") + sys.getsizeof(""b"")\n\n\ndef _set(d):\n    d[""a""][0] = 10\n\n\ndef test_SizedDict_shared():\n    d = SizedDict(shared=True)\n    x = torch.randn(10)\n    d[""a""] = x\n\n    mp = multiprocessing.get_context(""forkserver"")\n    p = mp.Process(target=_set, args=(d,))\n    p.start()\n    p.join()\n    assert d[""a""][0] == 10\n\n\ndef test_SizedDict_getitem():\n    d = SizedDict(data={""a"": 2, ""b"": 5, ""c"": 10})\n    assert d[""a""] == 2\n\n\ndef test_SizedDict_iter():\n    d = SizedDict(data={""a"": 2, ""b"": 5, ""c"": 10})\n    assert list(iter(d)) == [""a"", ""b"", ""c""]\n\n\ndef test_SizedDict_contains():\n    d = SizedDict(data={""a"": 2, ""b"": 5, ""c"": 10})\n    assert ""a"" in d\n\n\ndef test_SizedDict_len():\n    d = SizedDict(data={""a"": 2, ""b"": 5, ""c"": 10})\n    assert len(d) == 3\n'"
test/espnet2/utils/test_types.py,0,"b'from contextlib import contextmanager\nfrom typing import Any\n\nimport pytest\n\nfrom espnet2.utils.types import float_or_none\nfrom espnet2.utils.types import humanfriendly_parse_size_or_none\nfrom espnet2.utils.types import int_or_none\nfrom espnet2.utils.types import remove_parenthesis\nfrom espnet2.utils.types import str2bool\nfrom espnet2.utils.types import str2pair_str\nfrom espnet2.utils.types import str2triple_str\nfrom espnet2.utils.types import str_or_int\nfrom espnet2.utils.types import str_or_none\n\n\n@contextmanager\ndef pytest_raise_or_nothing(exception_or_any: Any):\n    if isinstance(exception_or_any, type) and issubclass(exception_or_any, Exception):\n        with pytest.raises(exception_or_any):\n            yield\n    else:\n        yield\n\n\n@pytest.mark.parametrize(\n    ""value, desired"",\n    [\n        (""true"", True),\n        (""false"", False),\n        (""True"", True),\n        (""False"", False),\n        (""aa"", ValueError),\n    ],\n)\ndef test_str2bool(value: str, desired: Any):\n    with pytest_raise_or_nothing(desired):\n        assert str2bool(value) == desired\n\n\n@pytest.mark.parametrize(\n    ""value, desired"", [(""3"", 3), (""3 "", 3), (""none"", None), (""aa"", ValueError)],\n)\ndef test_int_or_none(value: str, desired: Any):\n    with pytest_raise_or_nothing(desired):\n        assert int_or_none(value) == desired\n\n\n@pytest.mark.parametrize(\n    ""value, desired"", [(""3.5"", 3.5), (""3.5 "", 3.5), (""none"", None), (""aa"", ValueError)],\n)\ndef test_float_or_none(value: str, desired: Any):\n    with pytest_raise_or_nothing(desired):\n        assert float_or_none(value) == desired\n\n\n@pytest.mark.parametrize(\n    ""value, desired"", [(""3k"", 3000), (""2m "", 2000000), (""none"", None)],\n)\ndef test_humanfriendly_parse_size_or_none(value: str, desired: Any):\n    with pytest_raise_or_nothing(desired):\n        assert humanfriendly_parse_size_or_none(value) == desired\n\n\n@pytest.mark.parametrize(\n    ""value, desired"", [(""3"", 3), (""3 "", 3), (""aa"", ""aa"")],\n)\ndef test_str_or_int(value: str, desired: Any):\n    assert str_or_int(value) == desired\n\n\n@pytest.mark.parametrize(""value, desired"", [(""none"", None), (""aa"", ""aa"")])\ndef test_str_or_none(value: str, desired: Any):\n    with pytest_raise_or_nothing(desired):\n        assert str_or_none(value) == desired\n\n\n@pytest.mark.parametrize(\n    ""value, desired"",\n    [\n        (""a, b"", (""a"", ""b"")),\n        (""a,b,c"", ValueError),\n        (""a"", ValueError),\n        (""[\'a\', \'b\']"", (""a"", ""b"")),\n    ],\n)\ndef test_str2pair_str(value: str, desired: Any):\n    with pytest_raise_or_nothing(desired):\n        assert str2pair_str(value) == desired\n\n\n@pytest.mark.parametrize(\n    ""value, desired"",\n    [\n        (""a,b, c"", (""a"", ""b"", ""c"")),\n        (""a,b"", ValueError),\n        (""a"", ValueError),\n        (""[\'a\', \'b\', \'c\']"", (""a"", ""b"", ""c"")),\n    ],\n)\ndef test_str2triple_str(value: str, desired: Any):\n    with pytest_raise_or_nothing(desired):\n        assert str2triple_str(value) == desired\n\n\n@pytest.mark.parametrize(\n    ""value, desired"", [("" (a v c) "", ""a v c""), (""[ 0999 ]"", "" 0999 "")]\n)\ndef test_remove_parenthesis(value: str, desired: Any):\n    assert remove_parenthesis(value) == desired\n'"
test/espnet2/utils/test_yaml_no_alias_safe_dump.py,0,"b'import pytest\nimport yaml\n\nfrom espnet2.utils.yaml_no_alias_safe_dump import yaml_no_alias_safe_dump\n\nd = {""a"": (1, 2, 3)}\n\n\n@pytest.mark.parametrize(\n    ""data, desired"",\n    [(d, {""a"": [1, 2, 3]}), ((d, d[""a""]), [{""a"": [1, 2, 3]}, [1, 2, 3]])],\n)\ndef test_yaml_no_alias_safe_dump(data, desired):\n    assert yaml.load(yaml_no_alias_safe_dump(data), Loader=yaml.Loader) == desired\n'"
egs/an4/asr1/local/data_prep.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2016  Allen Guo\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport os\nimport re\nimport sys\n\nif len(sys.argv) != 3:\n    print(""Usage: python data_prep.py [an4_root] [sph2pipe]"")\n    sys.exit(1)\nan4_root = sys.argv[1]\nsph2pipe = sys.argv[2]\n\nsph_dir = {""train"": ""an4_clstk"", ""test"": ""an4test_clstk""}\n\nfor x in [""train"", ""test""]:\n    with open(\n        os.path.join(an4_root, ""etc"", ""an4_"" + x + "".transcription"")\n    ) as transcript_f, open(os.path.join(""data"", x, ""text""), ""w"") as text_f, open(\n        os.path.join(""data"", x, ""wav.scp""), ""w""\n    ) as wav_scp_f, open(\n        os.path.join(""data"", x, ""utt2spk""), ""w""\n    ) as utt2spk_f:\n\n        text_f.truncate()\n        wav_scp_f.truncate()\n        utt2spk_f.truncate()\n\n        lines = sorted(transcript_f.readlines(), key=lambda s: s.split("" "")[0])\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            words = re.search(r""^(.*) \\("", line).group(1)\n            if words[:4] == ""<s> "":\n                words = words[4:]\n            if words[-5:] == "" </s>"":\n                words = words[:-5]\n            source = re.search(r""\\((.*)\\)"", line).group(1)\n            pre, mid, last = source.split(""-"")\n            utt_id = ""-"".join([mid, pre, last])\n\n            text_f.write(utt_id + "" "" + words + ""\\n"")\n            wav_scp_f.write(\n                utt_id\n                + "" ""\n                + sph2pipe\n                + "" -f wav -p -c 1 ""\n                + os.path.join(an4_root, ""wav"", sph_dir[x], mid, source + "".sph"")\n                + "" |\\n""\n            )\n            utt2spk_f.write(utt_id + "" "" + mid + ""\\n"")\n'"
egs/arctic/tts1/local/clean_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\n\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""text"", type=str, help=""text to be cleaned"")\n    args = parser.parse_args()\n    with codecs.open(args.text, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            line = line.split("" "")\n            id = line[0]\n            content = "" "".join(line[1:])\n            clean_content = custom_english_cleaners(content.rstrip())\n            print(""%s %s"" % (id, clean_content))\n'"
egs/blizzard17/tts1/local/make_lab_w_punc.py,0,"b'#!/usr/bin/env python3\r\n\r\n# Copyright 2018 Okayama University (Katsuki Inoue)\r\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\r\n\r\nimport os\r\nimport sys\r\n\r\n\r\ndef write_lab(out_dir, rfile, data):\r\n    os.makedirs(out_dir, exist_ok=True)\r\n    wfile = os.path.join(out_dir, os.path.basename(rfile))\r\n    with open(wfile, ""w"") as wf:\r\n        for data_row in range(len(data)):\r\n            wf.write(data[data_row][0] + ""\\t"")\r\n            wf.write(data[data_row][1] + ""\\t"")\r\n            wf.write(data[data_row][2] + ""\\n"")\r\n\r\n\r\ndef main():\r\n    args = sys.argv\r\n    list_file = args[1]\r\n    txt_file = args[2]\r\n    out_dir = args[3]\r\n\r\n    delimiter = "" ""\r\n    with open(list_file, ""r"") as f:\r\n        flist = list(map(lambda x: x.split(delimiter), f.read().strip().split(""\\n"")))\r\n\r\n    with open(txt_file, ""r"") as f:\r\n        new_lab = f.read().strip().split(""\\n"")\r\n\r\n    rfile = """"\r\n    for n in range(len(new_lab)):\r\n        row_num = int(flist[n][1]) - 1\r\n\r\n        if rfile != flist[n][0]:\r\n            if rfile != """":\r\n                write_lab(out_dir, rfile, data)  # noqa: F821\r\n\r\n            rfile = flist[n][0]\r\n            delimiter = ""\\t""\r\n            with open(rfile, ""r"") as rf:\r\n                data = list(\r\n                    map(lambda x: x.split(delimiter), rf.read().strip().split(""\\n""))\r\n                )\r\n\r\n        data[row_num][2] = new_lab[n]\r\n    write_lab(out_dir, rfile, data)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
egs/blizzard17/tts1/local/make_lab_wo_sil.py,0,"b'#!/usr/bin/env python3\r\n\r\n# Copyright 2018 Okayama University (Katsuki Inoue)\r\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\r\n\r\nimport csv\r\nimport sys\r\n\r\nimport librosa\r\nimport numpy as np\r\n\r\n\r\ndef get_vp(in_lab_file):\r\n    # initialize output\r\n    voice_part_old = []\r\n    utt_array = []  # uttrance array w/o \'#\'\r\n\r\n    # get voice_part & utt_array\r\n    with open(in_lab_file, ""r"") as f:\r\n        reader = csv.reader(f, delimiter=""\\t"")\r\n        for x in reader:\r\n            if x[2] != ""#"":\r\n                voice_part_old.append(np.float(x[0]))\r\n                voice_part_old.append(np.float(x[1]))\r\n                utt_array.append(x[2])\r\n    voice_part_old = np.reshape(voice_part_old, (-1, 2))\r\n\r\n    return voice_part_old, utt_array\r\n\r\n\r\ndef ignore_sil(voice_part_ana, sec=0.1):\r\n    # set length of voice_part\r\n    avn = len(voice_part_ana)  # Number of Analysed Voice part\r\n\r\n    # check short silent\r\n    check_list = np.ones(\r\n        avn + 1, np.int64\r\n    )  # 1:time to use, 0:time to ignore (=short silence)\r\n    for i in range(avn - 1):\r\n        diff = voice_part_ana[i + 1][0] - voice_part_ana[i][1]\r\n        if diff < sec:\r\n            check_list[i + 1] = 0\r\n    st_list = check_list[0:-1]\r\n    ed_list = check_list[1:]\r\n\r\n    # initialize counter\r\n    stn = 0\r\n    edn = 0\r\n\r\n    # initialize output\r\n    tvn = sum(st_list)  # Number of Tmp Voice part\r\n    voice_part_tmp = np.zeros((tvn, 2), np.float)\r\n\r\n    # ignore short silence\r\n    for i in range(avn):\r\n        if st_list[i] == 1:\r\n            voice_part_tmp[stn][0] = voice_part_ana[i][0]\r\n            stn = stn + 1\r\n        if ed_list[i] == 1:\r\n            voice_part_tmp[edn][1] = voice_part_ana[i][1]\r\n            edn = edn + 1\r\n\r\n    return voice_part_tmp\r\n\r\n\r\ndef separete_vp(voice_part_old, voice_part_tmp):\r\n    # set length of voice_part\r\n    ovn = len(voice_part_old)  # Number of Old Voice part\r\n    tvn = len(voice_part_tmp)  # Number of Tmp Voice part\r\n\r\n    # initialize output\r\n    voice_part_add = []\r\n    del_list = []\r\n    # pick up the additional separater\r\n    for i in range(ovn):\r\n        for j in range(tvn):\r\n            if (voice_part_tmp[j][0] < voice_part_old[i][0]) and (\r\n                voice_part_old[i][0] < voice_part_tmp[j][1]\r\n            ):\r\n                voice_part_add.append(voice_part_old[i][0])\r\n                voice_part_add.append(voice_part_tmp[j][1])\r\n                if not np.any(del_list == j):\r\n                    del_list.append(j)\r\n                    if (voice_part_tmp[j][0] < voice_part_old[i][1]) and (\r\n                        voice_part_old[i][1] < voice_part_tmp[j][1]\r\n                    ):\r\n                        voice_part_add.append(voice_part_old[i][0])\r\n                        voice_part_add.append(voice_part_old[i][1])\r\n\r\n            if (voice_part_tmp[j][0] < voice_part_old[i][1]) and (\r\n                voice_part_old[i][1] < voice_part_tmp[j][1]\r\n            ):\r\n                voice_part_add.append(voice_part_tmp[j][0])\r\n                voice_part_add.append(voice_part_old[i][1])\r\n                if not np.any(del_list == j):\r\n                    del_list.append(j)\r\n\r\n    # delite old separater\r\n    voice_part_res = np.delete(voice_part_tmp, del_list, 0)\r\n    rvn = len(voice_part_res)  # Number of Resodual Voice part\r\n\r\n    # merge the additional separater\r\n    for i in range(rvn):\r\n        voice_part_add.append(voice_part_res[i][0])\r\n        voice_part_add.append(voice_part_res[i][1])\r\n\r\n    return np.sort(np.reshape(voice_part_add, (-1, 2)), axis=0)\r\n\r\n\r\ndef create_vp(voice_part_old, voice_part_tmp):\r\n    # set length of voice_part\r\n    ovn = len(voice_part_old)  # Number of Old Voice part\r\n    tvn = len(voice_part_tmp)  # Number of Tmp Voice part\r\n\r\n    # initialize output\r\n    voice_part_new = np.zeros((ovn, 2), np.float)\r\n\r\n    # merge voice_part & skip non_voice_part\r\n    for i in range(ovn):\r\n        for j in range(tvn):\r\n            if (voice_part_old[i][0] <= voice_part_tmp[j][0]) and (\r\n                voice_part_tmp[j][1] <= voice_part_old[i][1]\r\n            ):\r\n                if voice_part_new[i][0] == 0.0:\r\n                    voice_part_new[i][0] = voice_part_tmp[j][0]\r\n                voice_part_new[i][1] = voice_part_tmp[j][1]\r\n\r\n    # check error\r\n    for i in range(ovn):\r\n        for j in range(2):\r\n            if voice_part_new[i][j] == 0.0:\r\n                sys.stderr.write(""Error: Element[%d][%d] is zero.\\n"" % (i, j))\r\n\r\n    return voice_part_new\r\n\r\n\r\ndef write_new_lab(out_lab_file, dur, voice_part_new, utt_array):\r\n    # set length of voice_part\r\n    nvn = len(voice_part_new)  # Number of New Voice part\r\n\r\n    with open(out_lab_file, mode=""w"") as f:\r\n        for i in range(nvn):\r\n            if i == 0:\r\n                # Head\r\n                f.write(""%.6f\\t%.6f\\t%s\\n"" % (0, np.float(voice_part_new[i][0]), ""#""))\r\n            else:\r\n                # Body of silence\r\n                f.write(\r\n                    ""%.6f\\t%.6f\\t%s\\n""\r\n                    % (\r\n                        np.float(voice_part_new[i - 1][1]),\r\n                        np.float(voice_part_new[i][0]),\r\n                        ""#"",\r\n                    )\r\n                )\r\n            # Body of voice\r\n            f.write(\r\n                ""%.6f\\t%.6f\\t%s\\n""\r\n                % (\r\n                    np.float(voice_part_new[i][0]),\r\n                    np.float(voice_part_new[i][1]),\r\n                    utt_array[i],\r\n                )\r\n            )\r\n        # Tail\r\n        f.write(""%.6f\\t%.6f\\t%s\\n"" % (np.float(voice_part_new[i][1]), dur, ""#""))\r\n\r\n\r\ndef compare_vp(voice_part_old, voice_part_new):\r\n    vn = len(voice_part_old)\r\n    base_dur = 1.2\r\n\r\n    for i in range(vn):\r\n        diff_st = voice_part_new[i][0] - voice_part_old[i][0]\r\n        diff_ed = voice_part_old[i][1] - voice_part_new[i][1]\r\n        dur_old = voice_part_old[i][1] - voice_part_old[i][0]\r\n        dur_new = voice_part_new[i][1] - voice_part_new[i][0]\r\n        diff_dur = dur_old - dur_new\r\n\r\n        if 1.0 < diff_st:\r\n            sys.stderr.write(\r\n                ""Warning: StDiff[%4f][%d] is bigger than 1.\\n"" % (diff_st, i)\r\n            )\r\n        if 1.0 < diff_ed:\r\n            sys.stderr.write(\r\n                ""Warning: EdDiff[%4f][%d] is bigger than 1.\\n"" % (diff_ed, i)\r\n            )\r\n        if dur_new < 0.5:\r\n            sys.stderr.write(\r\n                ""Warning: NewDur(%4f)[%d] is smaller than 0.5.\\n"" % (dur_new, i)\r\n            )\r\n        if (diff_dur < 0) or (base_dur < diff_dur):\r\n            sys.stderr.write(\r\n                ""Warning: Diff(%4f)[%d] is out of 0<x<%.2f.\\n"" % (diff_dur, i, base_dur)\r\n            )\r\n\r\n\r\ndef main():\r\n    args = sys.argv\r\n    in_wav_file = args[1]\r\n    in_lab_file = args[2]\r\n    out_lab_file = args[3]\r\n\r\n    # in_wav_file open\r\n    wav_form, fs = librosa.core.load(in_wav_file)  # wave form, sampling frequency\r\n    dur = len(wav_form) / fs  # duration\r\n\r\n    # extract voice_part_ana from wav_file\r\n    voice_part_ana = (\r\n        librosa.effects.split(wav_form, top_db=40, frame_length=2048, hop_length=512)\r\n        / fs\r\n    )\r\n\r\n    # in_lab_file open & get voice_part_old from lab_file\r\n    voice_part_old, utt_array = get_vp(in_lab_file)\r\n\r\n    # connect voice_part_ana to ignore short silent\r\n    voice_part_tmp = ignore_sil(voice_part_ana, sec=0.01)\r\n\r\n    # separate voice_part based on lab\r\n    voice_part_tmp = separete_vp(voice_part_old, voice_part_tmp)\r\n\r\n    # create voice_part_new by merging voice_parts and skipping non_voice_part\r\n    voice_part_new = create_vp(voice_part_old, voice_part_tmp)\r\n\r\n    # compare voice_part_old and voice_part_new (for debug)\r\n    # compare_vp(voice_part_old, voice_part_new)\r\n\r\n    # write duration of new_lab\r\n    write_new_lab(out_lab_file, dur, voice_part_new, utt_array)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
egs/chime5/asr1/local/json2text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport json\nimport logging\nimport sys\n\n\ndef hms_to_seconds(hms):\n    hour = hms.split("":"")[0]\n    minute = hms.split("":"")[1]\n    second = hms.split("":"")[2].split(""."")[0]\n\n    # .xx (10 ms order)\n    ms10 = hms.split("":"")[2].split(""."")[1]\n\n    # total seconds\n    seconds = int(hour) * 3600 + int(minute) * 60 + int(second)\n\n    return ""{:07d}"".format(int(str(seconds) + ms10))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""json"", help=""JSON transcription file"")\n    parser.add_argument(\n        ""--mictype"",\n        choices=[""ref"", ""worn"", ""u01"", ""u02"", ""u03"", ""u04"", ""u05"", ""u06""],\n        help=""Type of microphones"",\n    )\n    args = parser.parse_args()\n\n    # logging info\n    log_format = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s:%(message)s""\n    logging.basicConfig(level=logging.INFO, format=log_format)\n\n    logging.debug(""reading %s"", args.json)\n    with open(args.json, ""rt"", encoding=""utf-8"") as f:\n        j = json.load(f)\n\n    for x in j:\n        if ""[redacted]"" not in x[""words""]:\n            session_id = x[""session_id""]\n            speaker_id = x[""speaker""]\n            if args.mictype == ""ref"":\n                mictype = x[""ref""]\n            elif args.mictype == ""worn"":\n                mictype = ""original""\n            else:\n                mictype = args.mictype.upper()  # convert from u01 to U01\n\n            # add location tag for scoring (only for dev and eval sets)\n            if ""location"" in x.keys():\n                location = x[""location""].upper()\n            else:\n                location = ""NOLOCATION""\n\n            start_time = x[""start_time""][mictype]\n            end_time = x[""end_time""][mictype]\n\n            # remove meta chars and convert to lower\n            words = (\n                x[""words""]\n                .replace(\'""\', """")\n                .replace(""."", """")\n                .replace(""?"", """")\n                .replace("","", """")\n                .replace("":"", """")\n                .replace("";"", """")\n                .replace(""!"", """")\n                .lower()\n            )\n\n            # remove multiple spaces\n            words = "" "".join(words.split())\n\n            # convert to seconds, e.g., 1:10:05.55 -> 3600 + 600 + 5.55 = 4205.55\n            start_time = hms_to_seconds(start_time)\n            end_time = hms_to_seconds(end_time)\n\n            uttid = speaker_id + ""_"" + session_id\n            if not args.mictype == ""worn"":\n                uttid += ""_"" + mictype\n            uttid += ""_"" + location + ""-"" + start_time + ""-"" + end_time\n\n            if end_time > start_time:\n                sys.stdout.buffer.write((uttid + "" "" + words + ""\\n"").encode(""utf-8""))\n'"
egs/commonvoice/asr1/local/filter_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nfrom io import open\nimport sys\n\n\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--filter-list"", ""-f"", type=str, help=""filter list"")\n    args = parser.parse_args()\n\n    with open(args.filter_list, encoding=""utf-8"") as f:\n        fil = [x.rstrip() for x in f]\n\n    for x in sys.stdin:\n        # extract text parts\n        text = "" "".join(x.rstrip().split()[1:])\n        if text in fil:\n            print(x.split()[0], text)\n'"
egs/csj/asr1/local/csj_rm_tag.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nfrom io import open\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--skip-ncols"", ""-s"", default=0, type=int, help=""skip first n columns""\n    )\n    parser.add_argument(""text"", type=str, help=""input text"")\n    args = parser.parse_args()\n\n    if args.text:\n        f = open(args.text, encoding=""utf-8"")\n    else:\n        f = sys.stdin\n\n    for line in f:\n        x = line.split()\n        print("" "".join(x[: args.skip_ncols]), end="" "")\n        print("" "".join([st.split(""+"")[0] for st in x[args.skip_ncols :]]))\n'"
egs/dipco/asr1/local/json2text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport json\nimport logging\nimport sys\n\n\ndef hms_to_seconds(hms):\n    hour = hms.split("":"")[0]\n    minute = hms.split("":"")[1]\n    second = hms.split("":"")[2].split(""."")[0]\n\n    # .xx (10 ms order)\n    ms10 = hms.split("":"")[2].split(""."")[1]\n\n    # total seconds\n    seconds = int(hour) * 3600 + int(minute) * 60 + int(second)\n\n    return ""{:07d}"".format(int(str(seconds) + ms10))\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""json"", help=""JSON transcription file"")\n    parser.add_argument(\n        ""--mictype"",\n        choices=[""ref"", ""worn"", ""u01"", ""u02"", ""u03"", ""u04"", ""u05"", ""u06""],\n        help=""Type of microphones"",\n    )\n    args = parser.parse_args()\n\n    # logging info\n    log_format = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s:%(message)s""\n    logging.basicConfig(level=logging.INFO, format=log_format)\n\n    logging.debug(""reading %s"", args.json)\n    with open(args.json, ""rt"", encoding=""utf-8"") as f:\n        j = json.load(f)\n\n    for x in j:\n        if ""[redacted]"" not in x[""words""]:\n            session_id = x[""session_id""]\n            speaker_id = x[""speaker_id""]\n            if args.mictype == ""ref"":\n                mictype = x[""ref""]\n            elif args.mictype == ""worn"":\n                mictype = ""close-talk""\n            else:\n                mictype = args.mictype.upper()  # convert from u01 to U01\n\n            # add location tag for scoring (only for dev and eval sets)\n            if ""location"" in x.keys():\n                location = x[""location""].upper()\n            else:\n                location = ""NOLOCATION""\n\n            start_time = x[""start_time""][mictype]\n            end_time = x[""end_time""][mictype]\n\n            # remove meta chars and convert to lower\n            words = (\n                x[""words""]\n                .replace(\'""\', """")\n                .replace(""."", """")\n                .replace(""?"", """")\n                .replace("","", """")\n                .replace("":"", """")\n                .replace("";"", """")\n                .replace(""!"", """")\n                .lower()\n            )\n\n            # remove multiple spaces\n            words = "" "".join(words.split())\n\n            # convert to seconds, e.g., 1:10:05.55 -> 3600 + 600 + 5.55 = 4205.55\n            start_time = hms_to_seconds(start_time)\n            end_time = hms_to_seconds(end_time)\n\n            uttid = speaker_id + ""_"" + session_id\n            if args.mictype not in [""worn"", ""ref""]:\n                # Because ref is close-talk\n                uttid += ""_"" + mictype\n            uttid += ""_"" + location + ""-"" + start_time + ""-"" + end_time\n\n            if end_time > start_time:\n                sys.stdout.buffer.write((uttid + "" "" + words + ""\\n"").encode(""utf-8""))\n'"
egs/fisher_callhome_spanish/asr1/local/callhome_make_spk2gender.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2014  Gaurav Kumar.   Apache 2.0\n# Gets the unique speakers from the file created by fsp_make_trans.pl\n# Note that if a speaker appears multiple times, it is categorized as female\n\n\ntmpFileLocation = ""data/local/tmp/callhome_spk2gendertmp""\n\ntmpFile = None\n\ntry:\n    tmpFile = open(tmpFileLocation)\nexcept IOError:\n    print(""The file spk2gendertmp does not exist. Run fsp_make_trans.pl first?"")\n\nspeakers = {}\n\nfor line in tmpFile:\n    comp = line.split("" "")\n    if comp[0] in speakers:\n        speakers[comp[0]] = ""f""\n    else:\n        speakers[comp[0]] = comp[1]\n\nfor speaker, gender in speakers.items():\n    print(speaker + "" "" + gender)\n'"
egs/fisher_callhome_spanish/asr1/local/fsp_make_spk2gender.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2014  Gaurav Kumar.   Apache 2.0\n# Gets the unique speakers from the file created by fsp_make_trans.pl\n# Note that if a speaker appears multiple times, it is categorized as female\n\n\ntmpFileLocation = ""data/local/tmp/spk2gendertmp""\n\ntmpFile = None\n\ntry:\n    tmpFile = open(tmpFileLocation)\nexcept IOError:\n    print(""The file spk2gendertmp does not exist. Run fsp_make_trans.pl first?"")\n\nspeakers = {}\n\nfor line in tmpFile:\n    comp = line.split("" "")\n    if comp[0] in speakers:\n        speakers[comp[0]] = ""f""\n    else:\n        speakers[comp[0]] = comp[1]\n\nfor speaker, gender in speakers.items():\n    print(speaker + "" "" + gender)\n'"
egs/fisher_callhome_spanish/st1/local/callhome_make_spk2gender.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2014  Gaurav Kumar.   Apache 2.0\n# Gets the unique speakers from the file created by fsp_make_trans.pl\n# Note that if a speaker appears multiple times, it is categorized as female\n\ntmpFileLocation = ""data/local/tmp/callhome_spk2gendertmp""\n\ntmpFile = None\n\ntry:\n    tmpFile = open(tmpFileLocation)\nexcept IOError:\n    print(""The file spk2gendertmp does not exist. Run fsp_make_trans.pl first?"")\n\nspeakers = {}\n\nfor line in tmpFile:\n    comp = line.split("" "")\n    if comp[0] in speakers:\n        speakers[comp[0]] = ""f""\n    else:\n        speakers[comp[0]] = comp[1]\n\nfor speaker, gender in speakers.items():\n    print(speaker + "" "" + gender)\n'"
egs/fisher_callhome_spanish/st1/local/concat_short_utt.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport os\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""mapping"", type=str, help=""mapping file to concatenate multiple utterances""\n    )\n    parser.add_argument(""text"", type=str, help=""transcription file"")\n    parser.add_argument(""segments"", type=str, help=""segmentation file"")\n    args = parser.parse_args()\n\n    segments = {}\n    lineno = 1\n    session_prev = """"\n    with codecs.open(args.segments, ""r"", encoding=""utf-8"") as f:\n        for line in f:\n            line = line.strip()\n            session = line.split(""-"")[0]\n\n            # reset counter\n            if session != session_prev:\n                lineno = 1\n\n            segments[(session, lineno)] = line\n            session_prev = session\n            lineno += 1\n\n    with codecs.open(\n        os.path.join(os.path.dirname(args.segments), ""segments""), ""w"", encoding=""utf-8""\n    ) as f:\n        for line in codecs.open(args.mapping, ""r"", encoding=""utf-8""):\n            session, ids = line.strip().split()\n            if len(ids.split(""_"")) == 1:\n                line_new = segments[(session, int(ids))]\n            elif len(ids.split(""_"")) >= 2:\n                start_id = int(ids.split(""_"")[0])\n                end_id = int(ids.split(""_"")[-1])\n                segment_start, spk_start, start_t, _ = segments[\n                    (session, start_id)\n                ].split()\n                segment_end, spk_end, _, end_t = segments[(session, end_id)].split()\n                line_new = "" "".join(\n                    [\n                        ""-"".join(segment_start.split(""-"")[:3])\n                        + ""-""\n                        + segment_end.split(""-"")[-1],\n                        spk_start,\n                        start_t,\n                        end_t,\n                    ]\n                )\n            f.write(line_new + ""\\n"")\n\n    texts = {}\n    lineno = 1\n    session_prev = """"\n    with codecs.open(args.text, ""r"", encoding=""utf-8"") as f:\n        for line in f:\n            line = line.strip()\n            session = line.split(""-"")[0]\n\n            # reset counter\n            if session != session_prev:\n                lineno = 1\n\n            texts[(session, lineno)] = line\n            session_prev = session\n            lineno += 1\n\n    with codecs.open(\n        os.path.join(os.path.dirname(args.text), ""text""), ""w"", encoding=""utf-8""\n    ) as f:\n        for line in codecs.open(args.mapping, ""r"", encoding=""utf-8""):\n            session, ids = line.strip().split()\n            if len(ids.split(""_"")) == 1:\n                line_new = texts[(session, int(ids))]\n            elif len(ids.split(""_"")) >= 2:\n                start_id = int(ids.split(""_"")[0])\n                segment = texts[(session, start_id)].split()[0]\n                trans = "" "".join(texts[(session, start_id)].split()[1:])\n                for i, id in enumerate(map(int, ids.split(""_"")[1:])):\n                    segment_next = texts[(session, id)].split()[0]\n                    trans_next = "" "".join(texts[(session, id)].split()[1:])\n                    trans += "" "" + trans_next\n                    if i == len(ids.split(""_"")[1:]) - 1:\n                        line_new = "" "".join(\n                            [\n                                ""-"".join(segment.split(""-"")[:3])\n                                + ""-""\n                                + segment_next.split(""-"")[-1],\n                                trans,\n                            ]\n                        )\n            f.write(line_new + ""\\n"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/fisher_callhome_spanish/st1/local/fsp_make_spk2gender.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2014  Gaurav Kumar.   Apache 2.0\n# Gets the unique speakers from the file created by fsp_make_trans.pl\n# Note that if a speaker appears multiple times, it is categorized as female\n\ntmpFileLocation = ""data/local/tmp/spk2gendertmp""\n\ntmpFile = None\n\ntry:\n    tmpFile = open(tmpFileLocation)\nexcept IOError:\n    print(""The file spk2gendertmp does not exist. Run fsp_make_trans.pl first?"")\n\nspeakers = {}\n\nfor line in tmpFile:\n    comp = line.split("" "")\n    if comp[0] in speakers:\n        speakers[comp[0]] = ""f""\n    else:\n        speakers[comp[0]] = comp[1]\n\nfor speaker, gender in speakers.items():\n    print(speaker + "" "" + gender)\n'"
egs/fisher_swbd/asr1/local/format_acronyms_dict.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd dict to fisher convention\n# IBM to i._b._m.\n# BBC to b._b._c.\n# BBCs to b._b._c.s\n# BBC\'s to b._b._c.\'s\n\nimport argparse\nimport re\n\n__author__ = ""Minhua Wu""\n\nparser = argparse.ArgumentParser(description=""format acronyms to a._b._c."")\nparser.add_argument(""-i"", ""--input"", help=""Input lexicon"", required=True)\nparser.add_argument(\n    ""-o1"", ""--output1"", help=""Output acronym lexicon(mapped)"", required=True\n)\nparser.add_argument(\n    ""-o2"", ""--output2"", help=""Output acronym lexicon(original)"", required=True\n)\nparser.add_argument(\n    ""-L"", ""--Letter"", help=""Input single letter pronunciation"", required=True\n)\nparser.add_argument(""-M"", ""--Map"", help=""Output acronyms mapping"", required=True)\nargs = parser.parse_args()\n\n\nfin_lex = open(args.input, ""r"")\nfin_Letter = open(args.Letter, ""r"")\nfout_lex = open(args.output1, ""w"")\nfout_lex_ori = open(args.output2, ""w"")\nfout_map = open(args.Map, ""w"")\n\n# Initialise single letter dictionary\ndict_letter = {}\nfor single_letter_lex in fin_Letter:\n    items = single_letter_lex.split()\n    dict_letter[items[0]] = single_letter_lex[len(items[0]) + 1 :].strip()\nfin_Letter.close()\n# print dict_letter\n\nfor lex in fin_lex:\n    items = lex.split()\n    word = items[0]\n    lexicon = lex[len(items[0]) + 1 :].strip()\n    # find acronyms from words with only letters and \'\n    pre_match = re.match(r""^[A-Za-z]+$|^[A-Za-z]+\\\'s$|^[A-Za-z]+s$"", word)\n    if pre_match:\n        # find if words in the form of xxx\'s is acronym\n        if word[-2:] == ""\'s"" and (lexicon[-1] == ""s"" or lexicon[-1] == ""z""):\n            actual_word = word[:-2]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for w in actual_word:\n                acronym_lexicon = acronym_lexicon + dict_letter[w.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                for w in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + w.lower() + ""._""\n                acronym_mapped = acronym_mapped + actual_word[-1].lower() + "".\'s""\n                fout_map.write(word + ""\\t"" + acronym_mapped + ""\\n"")\n                fout_lex.write(acronym_mapped + "" "" + lexicon + ""\\n"")\n                fout_lex_ori.write(word + "" "" + lexicon + ""\\n"")\n            else:\n                continue\n\n        # find if words in the form of xxxs is acronym\n        elif word[-1] == ""s"" and (lexicon[-1] == ""s"" or lexicon[-1] == ""z""):\n            actual_word = word[:-1]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for w in actual_word:\n                acronym_lexicon = acronym_lexicon + dict_letter[w.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                for w in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + w.lower() + ""._""\n                acronym_mapped = acronym_mapped + actual_word[-1].lower() + "".s""\n                fout_map.write(word + ""\\t"" + acronym_mapped + ""\\n"")\n                fout_lex.write(acronym_mapped + "" "" + lexicon + ""\\n"")\n                fout_lex_ori.write(word + "" "" + lexicon + ""\\n"")\n            else:\n                continue\n\n        # find if words in the form of xxx (not ended with \'s or s) is acronym\n        elif word.find(""\'"") == -1 and word[-1] != ""s"":\n            acronym_lexicon = """"\n            for w in word:\n                acronym_lexicon = acronym_lexicon + dict_letter[w.upper()] + "" ""\n            if acronym_lexicon.strip() == lexicon:\n                acronym_mapped = """"\n                for w in word[:-1]:\n                    acronym_mapped = acronym_mapped + w.lower() + ""._""\n                acronym_mapped = acronym_mapped + word[-1].lower() + "".""\n                fout_map.write(word + ""\\t"" + acronym_mapped + ""\\n"")\n                fout_lex.write(acronym_mapped + "" "" + lexicon + ""\\n"")\n                fout_lex_ori.write(word + "" "" + lexicon + ""\\n"")\n            else:\n                continue\n        else:\n            continue\n'"
egs/fisher_swbd/asr1/local/map_acronyms_transcripts.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd transcript to fisher convention\n# accoring to first two columns in the input acronyms mapping\n\nimport argparse\nimport re\n\n__author__ = ""Minhua Wu""\n\nparser = argparse.ArgumentParser(description=""format acronyms to a._b._c."")\nparser.add_argument(""-i"", ""--input"", help=""Input transcripts"", required=True)\nparser.add_argument(""-o"", ""--output"", help=""Output transcripts"", required=True)\nparser.add_argument(""-M"", ""--Map"", help=""Input acronyms mapping"", required=True)\nargs = parser.parse_args()\n\nfin_map = open(args.Map, ""r"")\ndict_acronym = {}\ndict_acronym_noi = {}  # Mapping of acronyms without I, i\nfor pair in fin_map:\n    items = pair.split(""\\t"")\n    dict_acronym[items[0]] = items[1].strip()\n    dict_acronym_noi[items[0]] = items[1].strip()\nfin_map.close()\ndel dict_acronym_noi[""i""]\ndel dict_acronym_noi[""I""]\n\nfin_trans = open(args.input, ""r"")\nfout_trans = open(args.output, ""w"")\nfor line in fin_trans:\n    line = line.strip()\n    items = line.split()\n    L = len(items)\n    # First pass mapping to map I as part of acronym\n    for i in range(L):\n        if items[i] == ""i"":\n            x = 0\n            while i - 1 - x >= 0 and re.match(r""^[A-Z]$"", items[i - 1 - x]):\n                x += 1\n\n            y = 0\n            while i + 1 + y < L and re.match(r""^[A-Z]$"", items[i + 1 + y]):\n                y += 1\n\n            if x + y > 0:\n                for bias in range(-x, y + 1):\n                    items[i + bias] = dict_acronym[items[i + bias]]\n\n    # Second pass mapping (not mapping \'i\' and \'I\')\n    for i in range(len(items)):\n        if items[i] in dict_acronym_noi.keys():\n            items[i] = dict_acronym_noi[items[i]]\n    sentence = "" "".join(items[1:])\n    fout_trans.write(items[0] + "" "" + sentence.lower() + ""\\n"")\n\nfin_trans.close()\nfout_trans.close()\n'"
egs/hkust/asr1/local/hkust_segment.py,0,"b'#!/usr/bin/env python3\n# coding:utf-8\n\nimport sys\n\nfrom mmseg import seg_txt\n\nfor line in sys.stdin:\n    blks = str.split(line)\n    out_line = blks[0]\n    for i in range(1, len(blks)):\n        if (\n            blks[i] == ""[VOCALIZED-NOISE]""\n            or blks[i] == ""[NOISE]""\n            or blks[i] == ""[LAUGHTER]""\n        ):\n            out_line += "" "" + blks[i]\n            continue\n        for j in seg_txt(blks[i]):\n            out_line += "" "" + j\n    print(out_line)\n'"
egs/iwslt16/mt1/local/extract_recog_text.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nextract recognized texts from data.<num>.json files\n""""""\nimport argparse\nimport glob\nfrom itertools import takewhile\nimport json\nimport os\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=""my script"")\n    parser.add_argument(""--path"", ""-p"", required=True, help=""path to decode dir"")\n    args = parser.parse_args()\n    return args\n\n\ndef process_json_file(path):\n    data = json.load(open(path, ""r""))[""utts""]\n    for idx, value in data.items():\n        idx = idx.split(""_"")[-1]\n        tokens = takewhile(\n            lambda x: x != ""<eos>"", value[""output""][0][""rec_token""].split("" "")\n        )\n        txt = "" "".join(tokens)\n        print(""{}\\t{}"".format(idx, txt))\n\n    return None\n\n\ndef main(args):\n    json_files = sorted(\n        glob.glob(os.path.join(args.path, ""data.*.json"")),\n        key=lambda x: int(x.split(""."")[-2]),\n    )\n    for json_file in json_files:\n        process_json_file(json_file)\n\n\nif __name__ == ""__main__"":\n    args = get_args()\n    main(args)\n'"
egs/iwslt16/mt1/local/generate_json.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nconvert given text data to json format required by ESPNet\n""""""\nimport argparse\nimport json\nfrom logging import getLogger\nimport os\nfrom typing import Dict\nfrom typing import List\n\n\nlogger = getLogger(__name__)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=""generate json file"")\n    parser.add_argument(\n        ""--src"",\n        ""-s"",\n        type=os.path.abspath,\n        required=True,\n        help=""path to source language data"",\n    )\n    parser.add_argument(\n        ""--trg"",\n        ""-t"",\n        type=os.path.abspath,\n        required=True,\n        help=""path to target language data"",\n    )\n    parser.add_argument(\n        ""--src-vocab"",\n        ""-sv"",\n        type=os.path.abspath,\n        required=True,\n        help=""path to source vocabulary"",\n    )\n    parser.add_argument(\n        ""--trg-vocab"",\n        ""-tv"",\n        type=os.path.abspath,\n        required=True,\n        help=""path to target vocabulary"",\n    )\n    parser.add_argument(\n        ""--dest"",\n        ""-d"",\n        type=os.path.abspath,\n        default=""data.json"",\n        help=""path to output json file"",\n    )\n    args = parser.parse_args()\n    return args\n\n\ndef load_vocab_file(path: str) -> Dict[str, int]:\n    vocab: Dict[str, int] = {}\n    with open(path, ""r"") as fi:\n        for line in fi:\n            token, index = line.strip().split("" "")\n            vocab[token] = int(index)\n    return vocab\n\n\ndef convert_line_to_dict(line: str, vocab: dict, name: str) -> Dict:\n    tokens: List = line.strip().split()\n    out: Dict = {\n        ""name"": name,\n        ""shape"": [len(tokens), len(vocab) + 2],\n        ""token"": "" "".join(tokens),\n        ""tokenid"": "" "".join(\n            [str(vocab[t]) if t in vocab else str(vocab[""<unk>""]) for t in tokens]\n        ),\n    }\n    return out\n\n\ndef merge_src_and_trg_to_utts(\n    src_dicts: List, trg_dicts: List, name: str = ""iwslt""\n) -> List:\n    output_list: List = []\n    for n, (src_dict, trg_dict) in enumerate(zip(src_dicts, trg_dicts)):\n        out: Dict = {\n            ""input"": [],\n            ""output"": [trg_dict, src_dict],\n            ""utt2spk"": ""{}_{}"".format(name, n),\n        }\n        output_list.append((""{}_{}"".format(name, n), out))\n    return output_list\n\n\ndef main(args):\n    logger.info(args)\n    src_vocab: Dict = load_vocab_file(args.src_vocab)\n    trg_vocab: Dict = load_vocab_file(args.trg_vocab)\n\n    source_dicts: List = [\n        convert_line_to_dict(line, vocab=src_vocab, name=""target2"")\n        for line in open(args.src)\n    ]\n    target_dicts: List = [\n        convert_line_to_dict(line, vocab=trg_vocab, name=""target1"")\n        for line in open(args.trg)\n    ]\n\n    utt_list: List = merge_src_and_trg_to_utts(source_dicts, target_dicts, name=""iwslt"")\n    output: dict = {""utts"": {}}\n    for name, utt in utt_list:\n        output[""utts""][name] = utt\n\n    logger.info(""Saved to {}"".format(args.dest))\n    with open(args.dest, ""w"") as fo:\n        json.dump(output, fo, indent=4)\n\n\nif __name__ == ""__main__"":\n    args = get_args()\n    main(args)\n'"
egs/iwslt16/mt1/local/generate_vocab.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\n0: empty token for CTC algorithm\n1: special UNK token\nformat: token + whitespace + index\n""""""\nimport argparse\nfrom collections import defaultdict\nimport fileinput\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=""generate vocabulary"")\n    parser.add_argument(\n        ""--input"", ""-i"", default=None, help=""files to read, if empty, stdin is used""\n    )\n    args = parser.parse_args()\n    return args\n\n\ndef main(args):\n    vocab = defaultdict(lambda: len(vocab) + 1)\n    vocab[""<unk>""]\n    for line in fileinput.input(args.input):\n        tokens = line.strip().split()\n        for token in tokens:\n            vocab[token]\n    vocab[""<eos>""]\n\n    for key, value in sorted(vocab.items(), key=lambda x: x[1]):\n        print(""{} {}"".format(key, value))\n\n\nif __name__ == ""__main__"":\n    args = get_args()\n    main(args)\n'"
egs/iwslt18/st1/local/ctm2segments.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport argparse\nimport codecs\nimport re\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""text"", type=str, help=""input text"")\n    parser.add_argument(""ctm"", type=str, help=""input ctm file (ASR results)"")\n    parser.add_argument(""set"", type=str, help="""")\n    parser.add_argument(""talk_id"", type=str, help="""")\n    args = parser.parse_args()\n\n    refs = []\n    with codecs.open(args.text, encoding=""utf-8"") as f:\n        for line in f:\n            line = line.strip().lower()\n            utt_id = line.split("" "")[0].split(""_"")[0]\n            ref = "" "".join(line.split()[1:])\n            refs += [(utt_id, ref)]\n\n    ctms = []\n    with codecs.open(args.ctm, encoding=""utf-8"") as f:\n        for line in f:\n            ctms.append(re.sub(r""[\\s]+"", "" "", line.strip()))\n    ctms = sorted(ctms, key=lambda x: float(x.split()[2]))\n\n    threshold = 0.2\n\n    hyps = []\n    utt_id = 1\n    start_t = None\n    end_t = None\n    hyp = """"\n    num_lines = len(ctms)\n    for i, ctm in enumerate(ctms):\n        _, _, start_time_w, duration_w, w = ctm.split()[:5]\n        w = re.sub(r""([^\\(\\)]*)\\([^\\)]+\\)"", r""\\1"", w.replace(""$"", """"))\n\n        if start_t is not None and i < num_lines - 1:\n            if (float(start_time_w) - end_t >= threshold) and (end_t - start_t > 0.2):\n                # differnece utterance\n                hyps += [(utt_id, start_t, end_t, hyp[1:])]\n\n                # reset\n                hyp = """"\n                start_t = None\n                end_t = None\n                utt_id += 1\n\n        # normalize\n        if start_t is None:\n            start_t = float(start_time_w)\n            end_t = float(start_time_w)\n        end_t = float(start_time_w) + float(duration_w)\n        if w != """":\n            hyp += "" "" + w.lower()\n\n        # last word in the session\n        if i == num_lines - 1:\n            hyps += [(utt_id, start_t, end_t, hyp[1:])]\n\n    for i, (utt_id, start_t, end_t, hyp) in enumerate(hyps):\n        assert end_t - start_t > 0\n        print(\n            ""%s_%07d_%07d %s %.2f %.2f""\n            % (\n                args.set + ""."" + args.talk_id,\n                int(start_t * 1000 + 0.5),\n                int(end_t * 1000 + 0.5),\n                args.set + ""."" + args.talk_id,\n                start_t,\n                end_t,\n            )\n        )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/iwslt18/st1/local/join_suffix.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright  2014  Nickolay V. Shmyrev\n#            2016  Johns Hopkins University (author: Daniel Povey)\n# Apache 2.0\n\n\nimport sys\n\n# This script joins together pairs of split-up words like ""you \'re"" -> ""you\'re"".\n# The TEDLIUM transcripts are normalized in a way that\'s not traditional for\n# speech recognition.\n\nfor line in sys.stdin:\n    items = line.split()\n    new_items = []\n    i = 1\n    while i < len(items):\n        if i < len(items) - 1 and items[i + 1][0] == ""\'"":\n            new_items.append(items[i] + items[i + 1])\n            i = i + 1\n        else:\n            new_items.append(items[i])\n        i = i + 1\n    print(items[0] + "" "" + "" "".join(new_items))\n'"
egs/iwslt18/st1/local/json2trn_reorder.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport logging\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""json"", type=str, help=""json files"")\n    parser.add_argument(""dict"", type=str, help=""dict"")\n    parser.add_argument(""hyp"", type=str, help=""hyp"")\n    parser.add_argument(\n        ""file_order"",\n        type=str,\n        help=""text file which describes the order of audio files"",\n    )\n    args = parser.parse_args()\n\n    # logging info\n    logging.basicConfig(\n        level=logging.INFO,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n\n    file_order = []\n    with codecs.open(args.file_order, ""r"", encoding=""utf-8"") as f:\n        for line in f:\n            file_order.append(line.strip())\n\n    logging.info(""reading %s"", args.json)\n    with codecs.open(args.json, ""r"", encoding=""utf-8"") as f:\n        j = json.load(f)\n\n    logging.info(""reading %s"", args.dict)\n    with codecs.open(args.dict, ""r"", encoding=""utf-8"") as f:\n        dictionary = f.readlines()\n    char_list = [entry.split("" "")[0] for entry in dictionary]\n    char_list.insert(0, ""<blank>"")\n    char_list.append(""<eos>"")\n\n    logging.info(""writing hyp trn to %s"", args.hyp)\n    h = codecs.open(args.hyp, ""w"", encoding=""utf-8"")\n\n    hyps = {}\n    for x in j[""utts""]:\n        talkid = x.split(""_"")[0]\n        start_time = int(x.split(""_"")[1])\n        if talkid not in hyps.keys():\n            hyps[talkid] = {}\n\n        seq = [\n            char_list[int(i)] for i in j[""utts""][x][""output""][0][""rec_tokenid""].split()\n        ]\n        hyps[talkid][start_time] = [x, seq]\n\n    for talkid in file_order:\n        for start_time, (x, seq) in sorted(hyps[talkid].items(), key=lambda x: x[0]):\n            h.write("" "".join(seq).replace(""<eos>"", """")),\n            h.write("" ("" + j[""utts""][x][""utt2spk""].replace(""-"", ""_"") + ""-"" + x + "")\\n"")\n'"
egs/iwslt18/st1/local/parse_xml.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nfrom collections import OrderedDict\nimport os\nimport re\nimport xml.etree.ElementTree as etree\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""xml"", type=str, default=False, nargs=""?"", help=""input xml"")\n    parser.add_argument(\n        ""output"", type=str, default=False, nargs=""?"", help=""output text""\n    )\n    args = parser.parse_args()\n\n    with codecs.open(args.xml, ""r"", encoding=""utf-8"") as xml_file:\n        elem = etree.parse(xml_file).getroot()\n\n        _set = os.path.basename(args.xml).split(""."")[2]\n        lang = os.path.basename(args.xml).split(""."")[-2]\n        talk_id = None\n\n        # Parse a XML file\n        trans_dict_all = OrderedDict()\n        for e in elem.getiterator():\n            if e.tag == ""doc"":\n                talk_id = e.get(""docid"").replace("" "", """")\n                trans_dict_all[talk_id] = OrderedDict()\n            elif e.tag == ""seg"":\n                utt_id = int(e.get(""id""))\n                ref = e.text\n\n                # Remove Al Gore:, Video: etc.\n                # ref = ref.split(\':\')[-1].lstrip()\n\n                # Remove consecutive spaces\n                ref = re.sub(r""[\\s]+"", "" "", ref).lstrip().rstrip()\n\n                trans_dict_all[talk_id][utt_id] = ref\n\n    with codecs.open(args.output, ""w"", encoding=""utf-8"") as f:\n        for talk_id, trans_dict in trans_dict_all.items():\n            for utt_id, ref in trans_dict.items():\n                f.write(\n                    ""%s.%s.talkid%d_%04d %s\\n""\n                    % (_set, lang, int(talk_id), int(utt_id), ref)\n                )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/iwslt18/st1/local/reorder_text.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport logging\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""text"", type=str, help=""text"")\n    parser.add_argument(\n        ""file_order"",\n        type=str,\n        help=""text file which describes the order of audio files"",\n    )\n    args = parser.parse_args()\n\n    # logging info\n    logging.basicConfig(\n        level=logging.INFO,\n        format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n    )\n\n    file_order = []\n    with codecs.open(args.file_order, ""r"", encoding=""utf-8"") as f:\n        for line in f:\n            file_order.append(line.strip().replace("".en"", """"))\n\n    logging.info(""reading %s"", args.text)\n    with codecs.open(args.text, ""r"", encoding=""utf-8"") as f:\n        refs = f.readlines()\n\n    dic = {}\n    for line in refs:\n        utt_id = line.split()[0]\n        talk_id = utt_id.split(""_"")[0].replace("".en"", """").replace("".de"", """")\n        ref = "" "".join(line.split()[1:])\n\n        if talk_id not in dic.keys():\n            dic[talk_id] = []\n        dic[talk_id] += [ref]\n\n    for talk_id in file_order:\n        for ref in dic[talk_id]:\n            # print(talk_id + \' \' + ref)\n            print(ref)\n'"
egs/iwslt19/st1/local/join_suffix.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright  2014  Nickolay V. Shmyrev\n#            2016  Johns Hopkins University (author: Daniel Povey)\n# Apache 2.0\n\n\nimport sys\n\n# This script joins together pairs of split-up words like ""you \'re"" -> ""you\'re"".\n# The TEDLIUM transcripts are normalized in a way that\'s not traditional for\n# speech recognition.\n\nprev_line = """"\nfor line in sys.stdin:\n    if line == prev_line:\n        continue\n    items = line.split()\n    new_items = []\n    i = 0\n    while i < len(items):\n        if i < len(items) - 1 and items[i + 1][0] == ""\'"":\n            new_items.append(items[i] + items[i + 1])\n            i = i + 1\n        else:\n            new_items.append(items[i])\n        i = i + 1\n    print("" "".join(new_items))\n    prev_line = line\n'"
egs/jnas/asr1/local/clean_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Takenori Yoshimura), Ryuichi Yamamoto\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\n\nimport jaconv\nimport pyopenjtalk\n\n\ndef g2p(text, trans_type=""char""):\n    text = jaconv.normalize(text)\n    if trans_type == ""char"":\n        text = pyopenjtalk.g2p(text, kana=True)\n    elif trans_type == ""phn"":\n        text = pyopenjtalk.g2p(text, kana=False)\n    else:\n        assert False\n    return text\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""in_text"", type=str, help=""text to be cleaned"")\n    parser.add_argument(""out_text"", type=str, help=""text to be cleaned"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""kana"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    args = parser.parse_args()\n    with codecs.open(args.in_text, ""r"", ""utf-8"") as f_in, codecs.open(\n        args.out_text, ""w"", ""utf-8""\n    ) as f_out:\n        for line in f_in.readlines():\n            id = line.split("" "")[0]\n            content = """".join(line.split("" "")[1:])\n            clean_content = g2p(content, args.trans_type)\n            f_out.write(""%s %s\\n"" % (id, clean_content))\n'"
egs/jnas/asr1/local/filter_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nfrom io import open\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--filter-list"", ""-f"", type=str, help=""filter list"")\n    args = parser.parse_args()\n\n    with open(args.filter_list, encoding=""utf-8"") as f:\n        fil = [x.rstrip() for x in f]\n\n    for x in sys.stdin:\n        # extract text parts\n        text = "" "".join(x.rstrip().split()[1:])\n        if text in fil:\n            print(x.split()[0], text)\n'"
egs/jnas/asr1/local/make_eval_trans.py,0,"b'#!/usr/bin/env python3\n# Copyright 2012 Vassil Panayotov\n# Apache 2.0\n\n""""""\nTakes a ""PROMPTS"" file with lines like:\n1snoke-20120412-hge/mfc/a0405\nIT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n\n, an ID prefix and a list of audio file names\n(e.g. for above example the list will contain ""a0405"").\nIt checks if the prompts file have transcription for all audio files in the list and\nif this is the case produces a transcript line for each file in the format:\nprefix_a0405 IT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n""""""\nimport sys\n\n\ndef err(msg):\n    print(msg, file=sys.stderr)\n\n\nif len(sys.argv) < 3:\n    err(""Usage: %s <prompts-file> <id-prefix> <utt-id1> <utt-id2> ... "" % sys.argv[0])\n    sys.exit(1)\n\n# err(str(sys.argv))\nid_prefix = sys.argv[2:]\nutt2trans = dict()\n\nfor line in open(sys.argv[1], ""r"", encoding=""utf-8""):\n    u, trans = line.split(None, 1)\n    utt2trans[u] = trans.strip(""\\n"")\n\nfor uid in id_prefix:\n    if not uid.split(""_"")[-1] in utt2trans:\n        err(""No transcript found for %s"" % (uid))\n        continue\n    print(""%s %s"" % (uid, utt2trans[uid.split(""_"")[-1]]))\n'"
egs/jnas/asr1/local/make_train_trans.py,0,"b'#!/usr/bin/env python3\n# Copyright 2012 Vassil Panayotov\n# Apache 2.0\n\n""""""\nTakes a ""PROMPTS"" file with lines like:\n1snoke-20120412-hge/mfc/a0405\nIT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n\n, an ID prefix and a list of audio file names (e.g. for above example the list will\ncontain ""a0405"").\nIt checks if the prompts file have transcription for all audio files in the list and\nif this is the case produces a transcript line for each file in the format:\nprefix_a0405 IT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n""""""\nimport sys\n\n\ndef err(msg):\n    print(msg, file=sys.stderr)\n\n\nif len(sys.argv) < 3:\n    err(""Usage: %s <prompts-file> <id-prefix> <utt-id1> <utt-id2> ... "" % sys.argv[0])\n    sys.exit(1)\n\n# err(str(sys.argv))\nid_prefix = sys.argv[2]\nutt_ids = sys.argv[3:]\nutt2trans = dict()\n\nfor line in open(sys.argv[1], ""r"", encoding=""utf-8""):\n    u, trans = line.split(None, 1)\n    utt2trans[u] = trans.strip(""\\n"")\n\nfor uid in utt_ids:\n    if uid not in utt2trans:\n        err(""No transcript found for %s_%s"" % (id_prefix, uid))\n        continue\n    print(""%s_%s %s"" % (id_prefix, uid, utt2trans[uid]))\n'"
egs/jsut/tts1/local/clean_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Takenori Yoshimura), Ryuichi Yamamoto\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\n\nimport jaconv\nimport pyopenjtalk\n\n\ndef g2p(text, trans_type=""char""):\n    text = jaconv.normalize(text)\n    if trans_type == ""char"":\n        text = pyopenjtalk.g2p(text, kana=True)\n    elif trans_type == ""phn"":\n        text = pyopenjtalk.g2p(text, kana=False)\n    else:\n        assert False\n    return text\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""in_text"", type=str, help=""text to be cleaned"")\n    parser.add_argument(""out_text"", type=str, help=""text to be cleaned"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""kana"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    args = parser.parse_args()\n    with codecs.open(args.in_text, ""r"", ""utf-8"") as f_in, codecs.open(\n        args.out_text, ""w"", ""utf-8""\n    ) as f_out:\n        for line in f_in.readlines():\n            id, content = line.split("":"")\n            clean_content = g2p(content, args.trans_type)\n            f_out.write(""%s %s\\n"" % (id, clean_content))\n'"
egs/jsut/tts1/local/prep_segments.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2019 Ryuichi Yamamoto\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport os\nimport sys\n\nfrom nnmnkwii.io import hts\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""Prepare segments from HTS-style alignment files"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""wav_scp"", type=str, help=""wav scp file"")\n    return parser\n\n\nif __name__ == ""__main__"":\n    args = get_parser().parse_args(sys.argv[1:])\n\n    with open(args.wav_scp) as f:\n        for line in f:\n            recording_id, path = line.split()\n            lab_path = path.replace(""wav/"", ""lab/"").replace("".wav"", "".lab"")\n            assert os.path.exists(lab_path)\n\n            labels = hts.load(lab_path)\n            assert ""sil"" in labels[0][-1]\n            assert ""sil"" in labels[-1][-1]\n            segment_begin = ""{:.3f}"".format(labels[0][1] * 1e-7)\n            segment_end = ""{:.3f}"".format(labels[-1][0] * 1e-7)\n\n            # As we assume that there\'s only a single utterance per recording,\n            # utt_id is same as recording_id.\n            # https://kaldi-asr.org/doc/data_prep.html\n            utt_id = recording_id\n            sys.stdout.write(\n                ""{} {} {} {}\\n"".format(utt_id, recording_id, segment_begin, segment_end)\n            )\n'"
egs/jvs/tts1/local/clean_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Takenori Yoshimura), Ryuichi Yamamoto\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\n\nimport jaconv\nimport pyopenjtalk\n\n\ndef g2p(text, trans_type=""char""):\n    text = jaconv.normalize(text)\n    if trans_type == ""char"":\n        text = pyopenjtalk.g2p(text, kana=True)\n    elif trans_type == ""phn"":\n        text = pyopenjtalk.g2p(text, kana=False)\n    else:\n        assert False\n    return text\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""in_text"", type=str, help=""text to be cleaned"")\n    parser.add_argument(""out_text"", type=str, help=""text to be cleaned"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""kana"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    args = parser.parse_args()\n    with codecs.open(args.in_text, ""r"", ""utf-8"") as f_in, codecs.open(\n        args.out_text, ""w"", ""utf-8""\n    ) as f_out:\n        for line in f_in.readlines():\n            id, content = line.split("":"")\n            clean_content = g2p(content, args.trans_type)\n            f_out.write(""%s %s\\n"" % (id, clean_content))\n'"
egs/libri_trans/st1/local/concat_json_multiref.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2018 Kyoto University (Hirofumi Inaguma)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n# NOTE: this is made for machine translation\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\nis_python2 = sys.version_info[0] == 2\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(""jsons"", type=str, nargs=""+"", help=""json files"")\n    args = parser.parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    # make intersection set for utterance keys\n    num_keys = 0\n    js = {}\n    for i, x in enumerate(args.jsons):\n        with codecs.open(x, encoding=""utf-8"") as f:\n            j = json.load(f)\n        ks = j[""utts""].keys()\n        logging.debug(x + "": has "" + str(len(ks)) + "" utterances"")\n\n        num_keys += len(ks)\n        if i > 0:\n            for k in ks:\n                js[k + ""."" + str(i)] = j[""utts""][k]\n        else:\n            js = j[""utts""]\n        # js.update(j[\'utts\'])\n\n    # logging.info(\'new json has \' + str(len(js.keys())) + \' utterances\')\n    logging.info(""new json has "" + str(num_keys) + "" utterances"")\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(\n        {""utts"": js},\n        indent=4,\n        sort_keys=True,\n        ensure_ascii=False,\n        separators=("","", "": ""),\n    )\n    sys.stdout = codecs.getwriter(""utf-8"")(\n        sys.stdout if is_python2 else sys.stdout.buffer\n    )\n    print(jsonstring)\n'"
egs/ljspeech/tts1/local/clean_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport nltk\n\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""text"", type=str, help=""text to be cleaned"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""kana"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    args = parser.parse_args()\n    with codecs.open(args.text, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            id, _, content = line.split(""|"")\n            clean_content = custom_english_cleaners(content.rstrip())\n            if args.trans_type == ""phn"":\n                text = clean_content.lower()\n                clean_content = g2p(text)\n\n            print(""%s %s"" % (id, clean_content))\n'"
egs/ljspeech/tts1/local/filter_by_focus_rate.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Filter samples by focus rates.""""""\n\nimport argparse\nimport logging\nimport os\n\nimport kaldiio\n\n\ndef main():\n    """"""Run filtering by focus rate.""""""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--focus-rates-scp"", type=str, help=""scp file of focus rates"")\n    parser.add_argument(""--durations-scp"", type=str, help=""scp file of focus rates"")\n    parser.add_argument(""--feats-scp"", type=str, help=""scp file of focus rates"")\n    parser.add_argument(\n        ""--threshold"",\n        type=float,\n        default=None,\n        help=""threshold value of focus rates (0.0, 1.0)"",\n    )\n    parser.add_argument(""--verbose"", default=1, type=int, help=""verbose option"")\n    args = parser.parse_args()\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n        logging.warning(""Skip DEBUG/INFO messages"")\n\n    # check threshold is valid\n    assert args.threshold > 0 and args.threshold < 1\n\n    # load focus rates scp\n    feat_reader = kaldiio.load_scp(args.feats_scp)\n    dur_reader = kaldiio.load_scp(args.durations_scp)\n    fr_reader = kaldiio.load_scp(args.focus_rates_scp)\n\n    # define writer\n    dirname = os.path.dirname(args.feats_scp)\n    feat_fid = open(f""{dirname}/feats_filtered.scp"", ""w"")\n    dur_fid = open(f""{dirname}/durations_filtered.scp"", ""w"")\n\n    # do filtering\n    drop_count = 0\n    for utt_id in fr_reader.keys():\n        focus_rate = fr_reader[utt_id]\n        if focus_rate >= args.threshold:\n            feat_fid.write(f""{utt_id} {feat_reader._dict[utt_id]}\\n"")\n            dur_fid.write(f""{utt_id} {dur_reader._dict[utt_id]}\\n"")\n        else:\n            drop_count += 1\n            logging.info(f""{utt_id} is dropped (focus rate: {focus_rate})."")\n    logging.info(f""{drop_count} utts are dropped by filtering."")\n\n    # close writer instances\n    feat_fid.close()\n    dur_fid.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/m_ailabs/tts1/local/parse_text.py,0,"b'#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n\n# Copyright 2019 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport os\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--lang_tag"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""language tag (can be used for multi lingual case)"",\n    )\n    parser.add_argument(""--spk_tag"", type=str, help=""speaker tag"")\n    parser.add_argument(""jsons"", nargs=""+"", type=str, help=""*_mls.json filenames"")\n    parser.add_argument(""out"", type=str, help=""output filename"")\n    args = parser.parse_args()\n\n    dirname = os.path.dirname(args.out)\n    if len(dirname) != 0 and not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with codecs.open(args.out, ""w"", encoding=""utf-8"") as out:\n        for filename in sorted(args.jsons):\n            with codecs.open(filename, ""r"", encoding=""utf-8"") as f:\n                js = json.load(f)\n            for key in sorted(js.keys()):\n                uid = args.spk_tag + ""_"" + key.replace("".wav"", """")\n                text = js[key][""clean""].upper()\n                if args.lang_tag is None:\n                    line = ""%s %s\\n"" % (uid, text)\n                else:\n                    line = ""%s <%s>%s\\n"" % (uid, args.lang_tag, text)\n                out.write(line)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/mini_an4/asr1/local/data_prep.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2016  Allen Guo\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport os\nimport re\nimport sys\n\nif len(sys.argv) != 3:\n    print(""Usage: python data_prep.py [an4_root] [sph2pipe]"")\n    sys.exit(1)\nan4_root = sys.argv[1]\nsph2pipe = sys.argv[2]\n\nsph_dir = {""train"": ""an4_clstk"", ""test"": ""an4test_clstk""}\n\nfor x in [""train"", ""test""]:\n    with open(\n        os.path.join(an4_root, ""etc"", ""an4_"" + x + "".transcription"")\n    ) as transcript_f, open(os.path.join(""data"", x, ""text""), ""w"") as text_f, open(\n        os.path.join(""data"", x, ""wav.scp""), ""w""\n    ) as wav_scp_f, open(\n        os.path.join(""data"", x, ""utt2spk""), ""w""\n    ) as utt2spk_f:\n\n        text_f.truncate()\n        wav_scp_f.truncate()\n        utt2spk_f.truncate()\n\n        lines = sorted(transcript_f.readlines(), key=lambda s: s.split("" "")[0])\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            words = re.search(r""^(.*) \\("", line).group(1)\n            if words[:4] == ""<s> "":\n                words = words[4:]\n            if words[-5:] == "" </s>"":\n                words = words[:-5]\n            source = re.search(r""\\((.*)\\)"", line).group(1)\n            pre, mid, last = source.split(""-"")\n            utt_id = ""-"".join([mid, pre, last])\n\n            text_f.write(utt_id + "" "" + words + ""\\n"")\n            wav_scp_f.write(\n                utt_id\n                + "" ""\n                + sph2pipe\n                + "" -f wav -p -c 1 ""\n                + os.path.join(an4_root, ""wav"", sph_dir[x], mid, source + "".sph"")\n                + "" |\\n""\n            )\n            utt2spk_f.write(utt_id + "" "" + mid + ""\\n"")\n'"
egs/mini_an4/asr_mix1/local/data_prep.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2016  Allen Guo\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport os\nimport re\nimport sys\n\nif len(sys.argv) != 3:\n    print(""Usage: python data_prep.py [an4_root] [sph2pipe]"")\n    sys.exit(1)\nan4_root = sys.argv[1]\nsph2pipe = sys.argv[2]\n\nsph_dir = {""train"": ""an4_clstk"", ""test"": ""an4test_clstk""}\n\nfor x in [""train"", ""test""]:\n    with open(\n        os.path.join(an4_root, ""etc"", ""an4_"" + x + "".transcription"")\n    ) as transcript_f, open(os.path.join(""data"", x, ""text""), ""w"") as text_f, open(\n        os.path.join(""data"", x, ""wav.scp""), ""w""\n    ) as wav_scp_f, open(\n        os.path.join(""data"", x, ""utt2spk""), ""w""\n    ) as utt2spk_f:\n\n        text_f.truncate()\n        wav_scp_f.truncate()\n        utt2spk_f.truncate()\n\n        lines = sorted(transcript_f.readlines(), key=lambda s: s.split("" "")[0])\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            words = re.search(r""^(.*) \\("", line).group(1)\n            if words[:4] == ""<s> "":\n                words = words[4:]\n            if words[-5:] == "" </s>"":\n                words = words[:-5]\n            source = re.search(r""\\((.*)\\)"", line).group(1)\n            pre, mid, last = source.split(""-"")\n            utt_id = ""-"".join([mid, pre, last])\n\n            text_f.write(utt_id + "" "" + words + ""\\n"")\n            wav_scp_f.write(\n                utt_id\n                + "" ""\n                + sph2pipe\n                + "" -f wav -p -c 1 ""\n                + os.path.join(an4_root, ""wav"", sph_dir[x], mid, source + "".sph"")\n                + "" |\\n""\n            )\n            utt2spk_f.write(utt_id + "" "" + mid + ""\\n"")\n'"
egs/reverb/asr1/local/filterjson.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nfrom io import open\nimport json\nimport logging\nimport re\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""jsons"", type=str, nargs=""+"", help=""json files"")\n    parser.add_argument(""--filt"", ""-f"", type=str, help=""utterance ID filter"")\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    args = parser.parse_args()\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n\n    # make union set for utterance keys\n    new_dic = dict()\n    for x in args.jsons:  #\n        with open(x, ""r"", encoding=""utf_8"") as f:\n            j = json.load(f)\n        ks = j[""utts""].keys()\n        logging.info(x + "": has "" + str(len(ks)) + "" utterances"")\n        for k in ks:\n            if re.search(args.filt, k):\n                new_dic[k] = j[""utts""][k]\n    logging.info(""new json has "" + str(len(new_dic.keys())) + "" utterances"")\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(\n        {""utts"": new_dic}, indent=4, ensure_ascii=False, sort_keys=True\n    )\n    print(jsonstring)\n'"
egs/reverb/asr1/local/run_wpe.py,0,"b'#!/usr/bin/env python3\n# Copyright 2018 Johns Hopkins University (Author: Aswin Shanmugam Subramanian)\n# Apache 2.0\n# Works with both python2 and python3\n\nimport argparse\nimport errno\nimport os\nimport soundfile as sf\n\nfrom nara_wpe.utils import istft\nfrom nara_wpe.utils import stft\nfrom nara_wpe.wpe import wpe\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--files"", ""-f"", nargs=""+"")\nargs = parser.parse_args()\n\ninput_files = args.files[: len(args.files) // 2]\noutput_files = args.files[len(args.files) // 2 :]\nout_dir = os.path.dirname(output_files[0])\ntry:\n    os.makedirs(out_dir)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n\nstft_options = dict(\n    size=512,\n    shift=128,\n    window_length=None,\n    fading=True,\n    pad=True,\n    symmetric_window=False,\n)\n\nsampling_rate = 16000\ndelay = 3\niterations = 5\ntaps = 10\n\nsignal_list = [sf.read(f)[0] for f in input_files]\ny = np.stack(signal_list, axis=0)\nY = stft(y, **stft_options).transpose(2, 0, 1)\nZ = wpe(Y, iterations=iterations, statistics_mode=""full"").transpose(1, 2, 0)\nz = istft(Z, size=stft_options[""size""], shift=stft_options[""shift""])\n\nfor d in range(len(signal_list)):\n    sf.write(output_files[d], z[d, :], sampling_rate)\n'"
egs/reverb/asr1_multich/local/filterjson.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nfrom io import open\nimport json\nimport logging\nimport re\nimport sys\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""jsons"", type=str, nargs=""+"", help=""json files"")\n    parser.add_argument(""--filt"", ""-f"", type=str, help=""utterance ID filter"")\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    args = parser.parse_args()\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n\n    # make union set for utterance keys\n    new_dic = dict()\n    for x in args.jsons:  #\n        with open(x, ""r"", encoding=""utf_8"") as f:\n            j = json.load(f)\n        ks = j[""utts""].keys()\n        logging.info(x + "": has "" + str(len(ks)) + "" utterances"")\n        for k in ks:\n            if re.search(args.filt, k):\n                new_dic[k] = j[""utts""][k]\n    logging.info(""new json has "" + str(len(new_dic.keys())) + "" utterances"")\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    jsonstring = json.dumps(\n        {""utts"": new_dic}, indent=4, ensure_ascii=False, sort_keys=True\n    )\n    print(jsonstring)\n'"
egs/reverb/asr1_multich/local/run_wpe.py,0,"b'#!/usr/bin/env python3\n# Copyright 2018 Johns Hopkins University (Author: Aswin Shanmugam Subramanian)\n# Apache 2.0\n# Works with both python2 and python3\n\nimport argparse\nimport errno\nimport os\nimport soundfile as sf\n\nfrom nara_wpe.utils import istft\nfrom nara_wpe.utils import stft\nfrom nara_wpe.wpe import wpe\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--files"", ""-f"", nargs=""+"")\nargs = parser.parse_args()\n\ninput_files = args.files[: len(args.files) // 2]\noutput_files = args.files[len(args.files) // 2 :]\nout_dir = os.path.dirname(output_files[0])\ntry:\n    os.makedirs(out_dir)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\n\nstft_options = dict(\n    size=512,\n    shift=128,\n    window_length=None,\n    fading=True,\n    pad=True,\n    symmetric_window=False,\n)\n\nsampling_rate = 16000\ndelay = 3\niterations = 5\ntaps = 10\n\nsignal_list = [sf.read(f)[0] for f in input_files]\ny = np.stack(signal_list, axis=0)\nY = stft(y, **stft_options).transpose(2, 0, 1)\nZ = wpe(Y, iterations=iterations, statistics_mode=""full"").transpose(1, 2, 0)\nz = istft(Z, size=stft_options[""size""], shift=stft_options[""shift""])\n\nfor d in range(len(signal_list)):\n    sf.write(output_files[d], z[d, :], sampling_rate)\n'"
egs/ru_open_stt/asr1/local/ru_open_stt_prepare_data.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2019 University of Stuttgart (Pavel Denisov)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport csv\nimport os\nimport random\nimport subprocess\nimport sys\n\n\ndef get_uttid(wav):\n    if ""/"" in wav:\n        return wav.split(""/"")[-4] + ""_"" + wav[-21:-4].replace(""/"", """")\n\n\nidir = sys.argv[1]\n\nbad_utts = set()\n\nfor filename in [""exclude_df_youtube_1120"", ""public_exclude_file_v5""]:\n    with open(idir + ""/"" + filename + "".csv"") as bad_utts_list_file:\n        bad_utts_list = csv.DictReader(bad_utts_list_file)\n        for row in bad_utts_list:\n            bad_utts.add(get_uttid(row[""wav""]))\n\nsubsets = {""train"": {}, ""dev"": {}, ""test"": {}}\n\nwords = """"\nval_words = set()\n\nfor dataset in [\n    # first the validation datasets\n    ""asr_calls_2_val"",\n    ""buriy_audiobooks_2_val"",\n    ""public_youtube700_val"",\n    # next the training datasets\n    # (it needs all validation transcripts)\n    ""asr_public_phone_calls_1"",\n    ""asr_public_phone_calls_2"",\n    ""asr_public_stories_1"",\n    ""asr_public_stories_2"",\n    ""private_buriy_audiobooks_2"",\n    ""public_lecture_1"",\n    ""public_series_1"",\n    ""public_youtube1120"",\n    ""public_youtube1120_hq"",\n    ""public_youtube700"",\n    ""radio_2"",\n    ""ru_RU"",\n    ""russian_single"",\n    ""tts_russian_addresses_rhvoice_4voices"",\n]:\n    with open(idir + ""/"" + dataset + "".csv"") as metafile:\n        meta = csv.reader(metafile)\n\n        for row in meta:\n            wav = idir + row[1][19:][:-3] + ""mp3""\n            uttid = get_uttid(wav)\n\n            if uttid in bad_utts or not os.path.isfile(wav):\n                continue\n\n            with open(wav[:-3] + ""txt"", encoding=""utf-8"") as text_file:\n                words = text_file.read().strip().lower()\n\n                subset = ""train""\n\n                if dataset[-4:] == ""_val"":\n                    val_words.add(words)\n                    subset = ""test""\n                elif words in val_words:\n                    continue\n\n                if dataset not in subsets[subset]:\n                    subsets[subset][dataset] = []\n\n                subsets[subset][dataset].append([uttid, words, wav])\n\nfor dataset in subsets[""test""].keys():\n    subsets[dataset] = {""all"": subsets[""test""][dataset][:]}\n\nfor subset in subsets.keys():\n    if ""all"" not in subsets[subset]:\n        subsets[subset][""all""] = sum(subsets[subset].values(), [])\n\nrandom.seed(1)\nrandom.shuffle(subsets[""train""][""all""])\n\ndev_size = min(int(len(subsets[""train""][""all""]) * 0.1), len(subsets[""test""][""all""]))\nsubsets[""dev""][""all""] = subsets[""train""][""all""][:dev_size]\nsubsets[""train""][""all""] = subsets[""train""][""all""][dev_size:]\n\ndel subsets[""test""]\n\nfor subset in subsets.keys():\n    odir = ""data/"" + subset\n    os.makedirs(odir, exist_ok=True)\n\n    with open(odir + ""/text"", ""w"", encoding=""utf-8"") as text, open(\n        odir + ""/wav.scp"", ""w""\n    ) as wavscp, open(odir + ""/utt2spk"", ""w"") as utt2spk:\n\n        for utt in subsets[subset][""all""]:\n            [uttid, words, wav] = utt\n            text.write(""{} {}\\n"".format(uttid, words))\n            utt2spk.write(""{} {}\\n"".format(uttid, uttid))\n            wavscp.write(\n                ""{} sox --norm=-1 {} -r 16k -t wav -c 1 -b 16 -e signed - |\\n"".format(\n                    uttid, wav\n                )\n            )\n\n    subprocess.call(""utils/fix_data_dir.sh {}"".format(odir), shell=True)\n'"
egs/swbd/asr1/local/format_acronyms_dict.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd dict to fisher convention\n# IBM to i._b._m.\n# BBC to b._b._c.\n# BBCs to b._b._c.s\n# BBC\'s to b._b._c.\'s\n\nimport argparse\nimport re\n\n__author__ = ""Minhua Wu""\n\nparser = argparse.ArgumentParser(description=""format acronyms to a._b._c."")\nparser.add_argument(""-i"", ""--input"", help=""Input lexicon"", required=True)\nparser.add_argument(""-o"", ""--output"", help=""Output lexicon"", required=True)\nparser.add_argument(\n    ""-L"", ""--Letter"", help=""Input single letter pronunciation"", required=True\n)\nparser.add_argument(""-M"", ""--Map"", help=""Output acronyms mapping"", required=True)\nargs = parser.parse_args()\n\n\nfin_lex = open(args.input, ""r"")\nfin_Letter = open(args.Letter, ""r"")\nfout_lex = open(args.output, ""w"")\nfout_map = open(args.Map, ""w"")\n\n# Initialise single letter dictionary\ndict_letter = {}\nfor single_letter_lex in fin_Letter:\n    items = single_letter_lex.split()\n    dict_letter[items[0]] = single_letter_lex[len(items[0]) + 1 :].strip()\nfin_Letter.close()\n# print dict_letter\n\nfor lex in fin_lex:\n    items = lex.split()\n    word = items[0]\n    lexicon = lex[len(items[0]) + 1 :].strip()\n    # find acronyms from words with only letters and \'\n    pre_match = re.match(r""^[A-Za-z]+$|^[A-Za-z]+\\\'s$|^[A-Za-z]+s$"", word)\n    if pre_match:\n        # find if words in the form of xxx\'s is acronym\n        if word[-2:] == ""\'s"" and (lexicon[-1] == ""s"" or lexicon[-1] == ""z""):\n            actual_word = word[:-2]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for w in actual_word:\n                acronym_lexicon = acronym_lexicon + dict_letter[w.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                acronym_mapped_back = """"\n                for w in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + w.lower() + ""._""\n                    acronym_mapped_back = acronym_mapped_back + w.lower() + "" ""\n                acronym_mapped = acronym_mapped + actual_word[-1].lower() + "".\'s""\n                acronym_mapped_back = (\n                    acronym_mapped_back + actual_word[-1].lower() + ""\'s""\n                )\n                fout_map.write(\n                    word + ""\\t"" + acronym_mapped + ""\\t"" + acronym_mapped_back + ""\\n""\n                )\n                fout_lex.write(acronym_mapped + "" "" + lexicon + ""\\n"")\n            else:\n                fout_lex.write(lex)\n\n        # find if words in the form of xxxs is acronym\n        elif word[-1] == ""s"" and (lexicon[-1] == ""s"" or lexicon[-1] == ""z""):\n            actual_word = word[:-1]\n            actual_lexicon = lexicon[:-2]\n            acronym_lexicon = """"\n            for w in actual_word:\n                acronym_lexicon = acronym_lexicon + dict_letter[w.upper()] + "" ""\n            if acronym_lexicon.strip() == actual_lexicon:\n                acronym_mapped = """"\n                acronym_mapped_back = """"\n                for w in actual_word[:-1]:\n                    acronym_mapped = acronym_mapped + w.lower() + ""._""\n                    acronym_mapped_back = acronym_mapped_back + w.lower() + "" ""\n                acronym_mapped = acronym_mapped + actual_word[-1].lower() + "".s""\n                acronym_mapped_back = (\n                    acronym_mapped_back + actual_word[-1].lower() + ""\'s""\n                )\n                fout_map.write(\n                    word + ""\\t"" + acronym_mapped + ""\\t"" + acronym_mapped_back + ""\\n""\n                )\n                fout_lex.write(acronym_mapped + "" "" + lexicon + ""\\n"")\n            else:\n                fout_lex.write(lex)\n\n        # find if words in the form of xxx (not ended with \'s or s) is acronym\n        elif word.find(""\'"") == -1 and word[-1] != ""s"":\n            acronym_lexicon = """"\n            for w in word:\n                acronym_lexicon = acronym_lexicon + dict_letter[w.upper()] + "" ""\n            if acronym_lexicon.strip() == lexicon:\n                acronym_mapped = """"\n                acronym_mapped_back = """"\n                for w in word[:-1]:\n                    acronym_mapped = acronym_mapped + w.lower() + ""._""\n                    acronym_mapped_back = acronym_mapped_back + w.lower() + "" ""\n                acronym_mapped = acronym_mapped + word[-1].lower() + "".""\n                acronym_mapped_back = acronym_mapped_back + word[-1].lower()\n                fout_map.write(\n                    word + ""\\t"" + acronym_mapped + ""\\t"" + acronym_mapped_back + ""\\n""\n                )\n                fout_lex.write(acronym_mapped + "" "" + lexicon + ""\\n"")\n            else:\n                fout_lex.write(lex)\n        else:\n            fout_lex.write(lex)\n\n    else:\n        fout_lex.write(lex)\n'"
egs/swbd/asr1/local/map_acronyms_transcripts.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015  Minhua Wu\n# Apache 2.0\n\n# convert acronyms in swbd transcript to fisher convention\n# accoring to first two columns in the input acronyms mapping\n\nimport argparse\nimport re\n\n__author__ = ""Minhua Wu""\n\nparser = argparse.ArgumentParser(description=""format acronyms to a._b._c."")\nparser.add_argument(""-i"", ""--input"", help=""Input transcripts"", required=True)\nparser.add_argument(""-o"", ""--output"", help=""Output transcripts"", required=True)\nparser.add_argument(""-M"", ""--Map"", help=""Input acronyms mapping"", required=True)\nargs = parser.parse_args()\n\nfin_map = open(args.Map, ""r"")\ndict_acronym = {}\ndict_acronym_noi = {}  # Mapping of acronyms without I, i\nfor pair in fin_map:\n    items = pair.split(""\\t"")\n    dict_acronym[items[0]] = items[1]\n    dict_acronym_noi[items[0]] = items[1]\nfin_map.close()\ndel dict_acronym_noi[""I""]\ndel dict_acronym_noi[""i""]\n\n\nfin_trans = open(args.input, ""r"")\nfout_trans = open(args.output, ""w"")\nfor line in fin_trans:\n    items = line.split()\n    L = len(items)\n    # First pass mapping to map I as part of acronym\n    for i in range(L):\n        if items[i] == ""I"":\n            x = 0\n            while i - 1 - x >= 0 and re.match(r""^[A-Z]$"", items[i - 1 - x]):\n                x += 1\n\n            y = 0\n            while i + 1 + y < L and re.match(r""^[A-Z]$"", items[i + 1 + y]):\n                y += 1\n\n            if x + y > 0:\n                for bias in range(-x, y + 1):\n                    items[i + bias] = dict_acronym[items[i + bias]]\n\n    # Second pass mapping (not mapping \'i\' and \'I\')\n    for i in range(len(items)):\n        if items[i] in dict_acronym_noi.keys():\n            items[i] = dict_acronym_noi[items[i]]\n    sentence = "" "".join(items[1:])\n    fout_trans.write(items[0] + "" "" + sentence.lower() + ""\\n"")\n\nfin_trans.close()\nfout_trans.close()\n'"
egs/tedlium2/asr1/local/join_suffix.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright  2014  Nickolay V. Shmyrev\n#            2016  Johns Hopkins University (author: Daniel Povey)\n# Apache 2.0\n\n\nimport sys\n\n# This script joins together pairs of split-up words like ""you \'re"" -> ""you\'re"".\n# The TEDLIUM transcripts are normalized in a way that\'s not traditional for\n# speech recognition.\n\nprev_line = """"\nfor line in sys.stdin:\n    if line == prev_line:\n        continue\n    items = line.split()\n    new_items = []\n    i = 0\n    while i < len(items):\n        if i < len(items) - 1 and items[i + 1][0] == ""\'"":\n            new_items.append(items[i] + items[i + 1])\n            i = i + 1\n        else:\n            new_items.append(items[i])\n        i = i + 1\n    print("" "".join(new_items))\n    prev_line = line\n'"
egs/tedlium3/asr1/local/join_suffix.py,0,"b'#!/usr/bin/env python3\n#\n# Copyright  2014  Nickolay V. Shmyrev\n#            2016  Johns Hopkins University (author: Daniel Povey)\n# Apache 2.0\n\n\nimport sys\n\n# This script joins together pairs of split-up words like ""you \'re"" -> ""you\'re"".\n# The TEDLIUM transcripts are normalized in a way that\'s not traditional for\n# speech recognition.\n\nprev_line = """"\nfor line in sys.stdin:\n    if line == prev_line:\n        continue\n    items = line.split()\n    new_items = []\n    i = 0\n    while i < len(items):\n        if i < len(items) - 1 and items[i + 1][0] == ""\'"":\n            new_items.append(items[i] + items[i + 1])\n            i = i + 1\n        else:\n            new_items.append(items[i])\n        i = i + 1\n    print("" "".join(new_items))\n    prev_line = line\n'"
egs/tweb/tts1/local/clean_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\n\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""text"", type=str, help=""text to be cleaned"")\n    args = parser.parse_args()\n    with codecs.open(args.text, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            line = line.split("" "")\n            id = line[0]\n            content = "" "".join(line[1:])\n            clean_content = custom_english_cleaners(content.rstrip())\n            print(""%s %s"" % (id, clean_content))\n'"
egs/vais1000/tts1/local/clean_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2018 Nagoya University (Tomoki Hayashi) and K\xc3\xadnh Phan (@enamoria)\n# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\n\nfrom vietnamese_cleaner.vietnamese_cleaners import vietnamese_cleaner\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""text"", type=str, help=""text to be cleaned"")\n    args = parser.parse_args()\n\n    lines = {}\n    with codecs.open(args.text, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            id, _, content = line.split(""|"")\n\n            clean_content = vietnamese_cleaner(content)\n            lines[id] = clean_content\n\n        for id in sorted(lines.keys()):\n            print(f""{id} {lines[id]}"")\n'"
egs/vcc20/tts1_en_de/local/clean_text_mailabs.py,0,"b'#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport os\n\nimport nltk\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--lang_tag"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""language tag (can be used for multi lingual case)"",\n    )\n    parser.add_argument(""--spk_tag"", type=str, help=""speaker tag"")\n    parser.add_argument(""jsons"", nargs=""+"", type=str, help=""*_mls.json filenames"")\n    parser.add_argument(""out"", type=str, help=""output filename"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""phn"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    args = parser.parse_args()\n\n    dirname = os.path.dirname(args.out)\n    if len(dirname) != 0 and not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with codecs.open(args.out, ""w"", encoding=""utf-8"") as out:\n        for filename in sorted(args.jsons):\n            with codecs.open(filename, ""r"", encoding=""utf-8"") as f:\n                js = json.load(f)\n            for key in sorted(js.keys()):\n                uid = args.spk_tag + ""_"" + key.replace("".wav"", """")\n\n                content = js[key][""clean""]\n                text = custom_english_cleaners(content.rstrip())\n                if args.trans_type == ""phn"":\n                    clean_content = text.lower()\n                    text = g2p(clean_content)\n\n                if args.lang_tag is None:\n                    line = ""%s %s \\n"" % (uid, text)\n                else:\n                    line = ""%s <%s> %s\\n"" % (uid, args.lang_tag, text)\n                out.write(line)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/vcc20/tts1_en_fi/local/clean_text_css10.py,0,"b'#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport os\n\nimport nltk\nfrom tacotron_cleaner.cleaners import collapse_whitespace\nfrom tacotron_cleaner.cleaners import expand_abbreviations\nfrom tacotron_cleaner.cleaners import expand_numbers\nfrom tacotron_cleaner.cleaners import expand_symbols\nfrom tacotron_cleaner.cleaners import lowercase\nfrom tacotron_cleaner.cleaners import remove_unnecessary_symbols\nfrom tacotron_cleaner.cleaners import uppercase\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef custom_finnish_cleaners(text):\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = expand_symbols(text)\n    text = remove_unnecessary_symbols(text)\n    text = uppercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--lang_tag"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""language tag (can be used for multi lingual case)"",\n    )\n    parser.add_argument(""--spk_tag"", type=str, help=""speaker tag"")\n    parser.add_argument(""transcription"", type=str, help=""transcription filename"")\n    parser.add_argument(""out"", type=str, help=""output filename"")\n    parser.add_argument(\n        ""trans_type"", type=str, default=""char"", help=""transcription type (char or phn)""\n    )\n    args = parser.parse_args()\n\n    dirname = os.path.dirname(args.out)\n    if len(dirname) != 0 and not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with codecs.open(args.out, ""w"", encoding=""utf-8"") as out:\n        with codecs.open(args.transcription, ""r"", encoding=""utf-8"") as f:\n            for line in f.read().splitlines():\n                path, _, content, _ = line.split(""|"")\n                uid = args.spk_tag + ""_"" + os.path.basename(path).replace("".wav"", """")\n                text = custom_finnish_cleaners(content.rstrip())\n                if args.lang_tag is None:\n                    line = ""%s %s\\n"" % (uid, text)\n                else:\n                    line = ""%s <%s> %s\\n"" % (uid, args.lang_tag, text)\n                out.write(line)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/vcc20/tts1_en_fi/local/clean_text_mailabs.py,0,"b'#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport os\n\nimport nltk\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--lang_tag"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""language tag (can be used for multi lingual case)"",\n    )\n    parser.add_argument(""--spk_tag"", type=str, help=""speaker tag"")\n    parser.add_argument(""jsons"", nargs=""+"", type=str, help=""*_mls.json filenames"")\n    parser.add_argument(""out"", type=str, help=""output filename"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""phn"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    args = parser.parse_args()\n\n    dirname = os.path.dirname(args.out)\n    if len(dirname) != 0 and not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with codecs.open(args.out, ""w"", encoding=""utf-8"") as out:\n        for filename in sorted(args.jsons):\n            with codecs.open(filename, ""r"", encoding=""utf-8"") as f:\n                js = json.load(f)\n            for key in sorted(js.keys()):\n                uid = args.spk_tag + ""_"" + key.replace("".wav"", """")\n\n                content = js[key][""clean""]\n                text = custom_english_cleaners(content.rstrip())\n                if args.trans_type == ""phn"":\n                    clean_content = text.lower()\n                    text = g2p(clean_content)\n\n                if args.lang_tag is None:\n                    line = ""%s %s \\n"" % (uid, text)\n                else:\n                    line = ""%s <%s> %s\\n"" % (uid, args.lang_tag, text)\n                out.write(line)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/vcc20/tts1_en_zh/local/clean_text_mailabs.py,0,"b'#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport os\n\nimport nltk\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--lang_tag"",\n        type=str,\n        default=None,\n        nargs=""?"",\n        help=""language tag (can be used for multi lingual case)"",\n    )\n    parser.add_argument(""--spk_tag"", type=str, help=""speaker tag"")\n    parser.add_argument(""jsons"", nargs=""+"", type=str, help=""*_mls.json filenames"")\n    parser.add_argument(""out"", type=str, help=""output filename"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""phn"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    args = parser.parse_args()\n\n    dirname = os.path.dirname(args.out)\n    if len(dirname) != 0 and not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    with codecs.open(args.out, ""w"", encoding=""utf-8"") as out:\n        for filename in sorted(args.jsons):\n            with codecs.open(filename, ""r"", encoding=""utf-8"") as f:\n                js = json.load(f)\n            for key in sorted(js.keys()):\n                uid = args.spk_tag + ""_"" + key.replace("".wav"", """")\n\n                content = js[key][""clean""]\n                text = custom_english_cleaners(content.rstrip())\n                if args.trans_type == ""phn"":\n                    clean_content = text.lower()\n                    text = g2p(clean_content)\n\n                if args.lang_tag is None:\n                    line = ""%s %s \\n"" % (uid, text)\n                else:\n                    line = ""%s <%s> %s\\n"" % (uid, args.lang_tag, text)\n                out.write(line)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/vcc20/vc1_task1/local/clean_text_asr_result.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport nltk\n\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""transcription_path"", type=str, help=""path for the transcription text file""\n    )\n    parser.add_argument(\n        ""--lang_tag"",\n        type=str,\n        default="""",\n        help=""language tag (can be used for multi lingual case)"",\n    )\n    parser.add_argument(\n        ""--trans_type"",\n        type=str,\n        default=""char"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    parser.add_argument(\n        ""--lowercase"", type=bool, default=False, help=""Lower case the result or not""\n    )\n    args = parser.parse_args()\n\n    # clean every line in transcription file first\n    with codecs.open(args.transcription_path, ""r"", ""utf-8"") as fid:\n        for line in fid.read().splitlines():\n            segments = line.split("" "")\n\n            # clean contents\n            content = "" "".join(segments[:-1])\n            clean_content = custom_english_cleaners(content)\n\n            # get id by taking off the parentheses\n            id = segments[-1][1:-1]\n\n            if args.trans_type == ""phn"":\n                text = clean_content.lower()\n                clean_content = g2p(text)\n\n            if args.lowercase:\n                clean_content = clean_content.lower()\n\n            if args.lang_tag == """":\n                print(""{} {}"".format(id, clean_content))\n            else:\n                print(""{} {}"".format(id, ""<"" + args.lang_tag + ""> "" + clean_content))\n'"
egs/vcc20/vc1_task2/local/clean_text_asr_result.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport nltk\n\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""transcription_path"", type=str, help=""path for the transcription text file""\n    )\n    parser.add_argument(\n        ""--lang_tag"",\n        type=str,\n        default="""",\n        help=""language tag (can be used for multi lingual case)"",\n    )\n    parser.add_argument(\n        ""--trans_type"",\n        type=str,\n        default=""char"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    parser.add_argument(\n        ""--lowercase"", type=bool, default=False, help=""Lower case the result or not""\n    )\n    args = parser.parse_args()\n\n    # clean every line in transcription file first\n    with codecs.open(args.transcription_path, ""r"", ""utf-8"") as fid:\n        for line in fid.read().splitlines():\n            segments = line.split("" "")\n\n            # clean contents\n            content = "" "".join(segments[:-1])\n            clean_content = custom_english_cleaners(content)\n\n            # get id by taking off the parentheses\n            id = segments[-1][1:-1]\n\n            if args.trans_type == ""phn"":\n                text = clean_content.lower()\n                clean_content = g2p(text)\n\n            if args.lowercase:\n                clean_content = clean_content.lower()\n\n            if args.lang_tag == """":\n                print(""{} {}"".format(id, clean_content))\n            else:\n                print(""{} {}"".format(id, ""<"" + args.lang_tag + ""> "" + clean_content))\n'"
egs/vcc20/vc1_task2/local/clean_text_finnish.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport nltk\n\nfrom tacotron_cleaner.cleaners import collapse_whitespace\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\nfrom tacotron_cleaner.cleaners import expand_abbreviations\nfrom tacotron_cleaner.cleaners import expand_numbers\nfrom tacotron_cleaner.cleaners import expand_symbols\nfrom tacotron_cleaner.cleaners import lowercase\nfrom tacotron_cleaner.cleaners import remove_unnecessary_symbols\nfrom tacotron_cleaner.cleaners import uppercase\n\nE_lang_tag = ""en_US""\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\ndef custom_finnish_cleaners(text):\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = expand_symbols(text)\n    text = remove_unnecessary_symbols(text)\n    text = uppercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""transcription_path"", type=str, help=""path for the transcription text file""\n    )\n    parser.add_argument(""utt2spk"", type=str, help=""utt2spk file for the speaker"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""char"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    parser.add_argument(""lang_tag"", type=str, help=""lang tag"")\n    parser.add_argument(""spk"", type=str, help=""speaker name"")\n    parser.add_argument(\n        ""--transcription_path_en"",\n        type=str,\n        default=None,\n        help=""path for the English transcription text file"",\n    )\n    args = parser.parse_args()\n\n    # clean every line in transcription file first\n    transcription_dict = {}\n    with codecs.open(args.transcription_path, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            segments = line.split("" "")\n            lang_char = args.transcription_path.split(""/"")[-1][0]\n            id = args.spk + ""_"" + lang_char + segments[0]  # ex. TFF1_M10001\n            content = "" "".join(segments[1:])\n            text = custom_finnish_cleaners(content.rstrip())\n            if args.trans_type == ""phn"":\n                # NOTE: we don\'t have phone for Finnish yet.\n                clean_content = text\n            else:\n                clean_content = text\n\n            if args.lang_tag:\n                transcription_dict[id] = ""<"" + args.lang_tag + ""> "" + clean_content\n            else:\n                transcription_dict[id] = clean_content\n\n    if args.transcription_path_en:\n        with codecs.open(args.transcription_path_en, ""r"", ""utf-8"") as fid:\n            for line in fid.readlines():\n                segments = line.split("" "")\n                id = args.spk + ""_"" + ""E"" + segments[0]  # ex. TFF1_E10001\n                content = "" "".join(segments[1:])\n                clean_content = custom_english_cleaners(content.rstrip())\n                if args.trans_type == ""phn"":\n                    text = clean_content.lower()\n                    clean_content = g2p(text)\n\n                transcription_dict[id] = ""<"" + E_lang_tag + ""> "" + clean_content\n\n    # read the utt2spk file and actually write\n    with codecs.open(args.utt2spk, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            segments = line.split("" "")\n            id = segments[0]  # ex. E10001\n            content = transcription_dict[id]\n\n            print(""%s %s"" % (id, content))\n'"
egs/vcc20/vc1_task2/local/clean_text_german.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport nltk\n\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\n\nE_lang_tag = ""en_US""\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""transcription_path"", type=str, help=""path for the transcription text file""\n    )\n    parser.add_argument(""utt2spk"", type=str, help=""utt2spk file for the speaker"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""char"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    parser.add_argument(""lang_tag"", type=str, help=""lang tag"")\n    parser.add_argument(""spk"", type=str, help=""speaker name"")\n    parser.add_argument(\n        ""--transcription_path_en"",\n        type=str,\n        default=None,\n        help=""path for the English transcription text file"",\n    )\n    args = parser.parse_args()\n\n    # clean every line in transcription file first\n    transcription_dict = {}\n    with codecs.open(args.transcription_path, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            segments = line.split("" "")\n            lang_char = args.transcription_path.split(""/"")[-1][0]\n            id = args.spk + ""_"" + lang_char + segments[0]  # ex. TGF1_M10001\n            content = "" "".join(segments[1:])\n            text = content.rstrip().upper()\n            if args.trans_type == ""phn"":\n                # NOTE: we don\'t have phone for German yet.\n                clean_content = text\n            else:\n                clean_content = text\n\n            if args.lang_tag:\n                transcription_dict[id] = ""<"" + args.lang_tag + ""> "" + clean_content\n            else:\n                transcription_dict[id] = clean_content\n\n    if args.transcription_path_en:\n        with codecs.open(args.transcription_path_en, ""r"", ""utf-8"") as fid:\n            for line in fid.readlines():\n                segments = line.split("" "")\n                id = args.spk + ""_"" + ""E"" + segments[0]  # ex. TGF1_E10001\n                content = "" "".join(segments[1:])\n                clean_content = custom_english_cleaners(content.rstrip())\n                if args.trans_type == ""phn"":\n                    text = clean_content.lower()\n                    clean_content = g2p(text)\n\n                transcription_dict[id] = ""<"" + E_lang_tag + ""> "" + clean_content\n\n    # read the utt2spk file and actually write\n    with codecs.open(args.utt2spk, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            segments = line.split("" "")\n            id = segments[0]  # ex. E10001\n            content = transcription_dict[id]\n\n            print(""%s %s"" % (id, content))\n'"
egs/vcc20/vc1_task2/local/clean_text_mandarin.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2020 Nagoya University (Wen-Chin Huang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport nltk\n\nfrom pypinyin.contrib.neutral_tone import NeutralToneWith5Mixin\nfrom pypinyin.converter import DefaultConverter\nfrom pypinyin.core import Pinyin\nfrom pypinyin import Style\nfrom pypinyin.style._utils import get_finals\nfrom pypinyin.style._utils import get_initials\nfrom tacotron_cleaner.cleaners import custom_english_cleaners\n\n\nclass MyConverter(NeutralToneWith5Mixin, DefaultConverter):\n    pass\n\n\nmy_pinyin = Pinyin(MyConverter())\npinyin = my_pinyin.pinyin\n\nE_lang_tag = ""en_US""\n\ntry:\n    # For phoneme conversion, use https://github.com/Kyubyong/g2p.\n    from g2p_en import G2p\n\n    f_g2p = G2p()\n    f_g2p("""")\nexcept ImportError:\n    raise ImportError(\n        ""g2p_en is not installed. please run `. ./path.sh && pip install g2p_en`.""\n    )\nexcept LookupError:\n    # NOTE: we need to download dict in initial running\n    import ssl\n\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context\n    nltk.download(""punkt"")\n\n\ndef g2p(text):\n    """"""Convert grapheme to phoneme.""""""\n    tokens = filter(lambda s: s != "" "", f_g2p(text))\n    return "" "".join(tokens)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""transcription_path"", type=str, help=""path for the transcription text file""\n    )\n    parser.add_argument(""utt2spk"", type=str, help=""utt2spk file for the speaker"")\n    parser.add_argument(\n        ""trans_type"",\n        type=str,\n        default=""phn"",\n        choices=[""char"", ""phn""],\n        help=""Input transcription type"",\n    )\n    parser.add_argument(""lang_tag"", type=str, help=""lang tag"")\n    parser.add_argument(""spk"", type=str, help=""speaker name"")\n    parser.add_argument(\n        ""--transcription_path_en"",\n        type=str,\n        default=None,\n        help=""path for the English transcription text file"",\n    )\n    args = parser.parse_args()\n\n    # clean every line in transcription file first\n    transcription_dict = {}\n    with codecs.open(args.transcription_path, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            segments = line.split("" "")\n            lang_char = args.transcription_path.split(""/"")[-1][0]\n            id = args.spk + ""_"" + lang_char + segments[0]  # ex. TMF1_M10001\n            content = segments[1].replace(""\\n"", """")\n\n            # Some special rules to match CSMSC pinyin\n            text = pinyin(content, style=Style.TONE3)\n            text = [c[0] for c in text]\n            clean_content = []\n            for c in text:\n                c_init = get_initials(c, strict=True)\n                c_final = get_finals(c, strict=True)\n                for c in [c_init, c_final]:\n                    if len(c) == 0:\n                        continue\n                    c = c.replace(""\xc3\xbc"", ""v"")\n                    c = c.replace(""ui"", ""uei"")\n                    c = c.replace(""un"", ""uen"")\n                    c = c.replace(""iu"", ""iou"")\n\n                    # Special rule: ""e5n"" -> ""en5""\n                    if ""5"" in c:\n                        c = c.replace(""5"", """") + ""5""\n                    clean_content.append(c)\n\n            transcription_dict[id] = "" "".join(\n                [""<"" + args.lang_tag + "">""] + clean_content\n            )\n\n    if args.transcription_path_en:\n        with codecs.open(args.transcription_path_en, ""r"", ""utf-8"") as fid:\n            for line in fid.readlines():\n                segments = line.split("" "")\n                id = args.spk + ""_"" + ""E"" + segments[0]  # ex. TMF1_E10001\n                content = "" "".join(segments[1:])\n                clean_content = custom_english_cleaners(content.rstrip())\n                if args.trans_type == ""phn"":\n                    text = clean_content.lower()\n                    clean_content = g2p(text)\n\n                transcription_dict[id] = ""<"" + E_lang_tag + ""> "" + clean_content\n\n    # read the utt2spk file and actually write\n    with codecs.open(args.utt2spk, ""r"", ""utf-8"") as fid:\n        for line in fid.readlines():\n            segments = line.split("" "")\n            id = segments[0]  # ex. E10001\n            content = transcription_dict[id]\n\n            print(""%s %s"" % (id, content))\n'"
egs/vcc20/voc1/local/subset_data_dir.py,0,"b'#!/usr/bin/env python3\n# encoding: utf-8\n\n# This script creates a subset of data,\n# consisting of some specified number of utterances.\n\nimport argparse\nfrom io import open\nimport sys\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""creates a subset of data"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--utt_list"", type=str, help=""utt list file"")\n    parser.add_argument(""--scp"", type=str, help=""scp file"")\n    parser.add_argument(""--verbose"", ""-V"", default=1, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--out"",\n        ""-O"",\n        type=str,\n        help=""The output filename. "" ""If omitted, then output to sys.stdout"",\n    )\n    return parser\n\n\nif __name__ == ""__main__"":\n    parser = get_parser()\n    args = parser.parse_args()\n\n    with open(args.utt_list, ""r"") as f:\n        utts = [line.rsplit()[0] for line in f.readlines()]\n    with open(args.scp, ""r"") as f:\n        scps = f.readlines()\n\n    if args.out is None:\n        out = sys.stdout\n    else:\n        out = open(args.out, ""a"", encoding=""utf-8"")\n\n    for line in scps:\n        number = line.split("" "")[0].split(""_"")[1]\n        if number in utts:\n            out.write(line)\n'"
egs/voxforge/asr1/local/filter_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nfrom io import open\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--filter-list"", ""-f"", type=str, help=""filter list"")\n    args = parser.parse_args()\n\n    with open(args.filter_list, encoding=""utf-8"") as f:\n        fil = [x.rstrip() for x in f]\n\n    for x in sys.stdin:\n        # extract text parts\n        text = "" "".join(x.rstrip().split()[1:])\n        if text in fil:\n            print(x.split()[0], text)\n'"
egs/voxforge/asr1/local/make_trans.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2012 Vassil Panayotov\n# Apache 2.0\n\n""""""\nTakes a ""PROMPTS"" file with lines like:\n1snoke-20120412-hge/mfc/a0405\nIT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n\n, an ID prefix and a list of audio file names (e.g. for above example the list\nwill contain ""a0405"").\nIt checks if the prompts file have transcription for all audio files in the list and\nif this is the case produces a transcript line for each file in the format:\nprefix_a0405 IT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n""""""\nimport sys\n\n\ndef err(msg):\n    print(msg, file=sys.stderr)\n\n\nif len(sys.argv) < 3:\n    err(""Usage: %s <prompts-file> <id-prefix> <utt-id1> <utt-id2> ... "" % sys.argv[0])\n    sys.exit(1)\n\n# err(str(sys.argv))\nid_prefix = sys.argv[2]\nutt_ids = sys.argv[3:]\nutt2trans = dict()\nunnorm_utt = set()\nfor line in open(sys.argv[1], encoding=""utf-8""):\n    u, trans = line.split(None, 1)\n    u = u.strip().split(""/"")[-1]\n    trans = trans.strip().replace(""-"", "" "").upper()\n    if (\n        not trans.isupper()\n        or not trans.strip().replace("" "", """").replace(""\'"", """").isalnum()\n    ):\n        # Note(kamo): Changed from the original: isalpha() -> isalnum()\n        # not trans.strip().replace(\' \', \'\').replace(""\'"", """").isalpha():\n        err(\n            ""The transcript for \'%s\'(user \'%s\') is not properly normalized - skipped!""\n            % (u, id_prefix)\n        )\n        err(trans)\n        unnorm_utt.add(u)\n        continue\n    utt2trans[u] = trans\n\nfor uid in utt_ids:\n    if uid in unnorm_utt:\n        continue  # avoid double reporting the same problem\n    if uid not in utt2trans:\n        err(""No transcript found for %s_%s"" % (id_prefix, uid))\n        continue\n    print(""%s-%s %s"" % (id_prefix, uid, utt2trans[uid]))\n'"
egs/wsj/asr1/local/filtering_samples.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2020 Shanghai Jiao Tong University (Wangyou Zhang)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n\nfrom functools import reduce\nimport json\nfrom operator import mul\nimport sys\n\nfrom espnet.bin.asr_train import get_parser\nfrom espnet.nets.pytorch_backend.nets_utils import get_subsample\nfrom espnet.utils.dynamic_import import dynamic_import\n\nif __name__ == ""__main__"":\n    cmd_args = sys.argv[1:]\n    parser = get_parser(required=False)\n    parser.add_argument(""--data-json"", type=str, help=""data.json"")\n    parser.add_argument(\n        ""--mode-subsample"", type=str, required=True, help=\'one of (""asr"", ""mt"", ""st"")\'\n    )\n    parser.add_argument(\n        ""--arch-subsample"",\n        type=str,\n        required=True,\n        help=\'one of (""rnn"", ""rnn-t"", ""rnn_mix"", ""rnn_mulenc"", ""transformer"")\',\n    )\n    parser.add_argument(\n        ""--min-io-delta"",\n        type=float,\n        help=""an additional parameter ""\n        ""for controlling the input-output length difference"",\n        default=0.0,\n    )\n    parser.add_argument(\n        ""--output-json-path"",\n        type=str,\n        required=True,\n        help=""Output path of the filtered json file"",\n    )\n    args, _ = parser.parse_known_args(cmd_args)\n\n    if args.model_module is None:\n        model_module = ""espnet.nets."" + args.backend + ""_backend.e2e_asr:E2E""\n    else:\n        model_module = args.model_module\n    model_class = dynamic_import(model_module)\n    model_class.add_arguments(parser)\n    args = parser.parse_args(cmd_args)\n\n    # subsampling info\n    if args.etype.startswith(""vgg""):\n        # Subsampling is not performed for vgg*.\n        # It is performed in max pooling layers at CNN.\n        min_io_ratio = 4\n    else:\n        subsample = get_subsample(\n            args, mode=args.mode_subsample, arch=args.arch_subsample\n        )\n        # the minimum input-output length ratio for all samples\n        min_io_ratio = reduce(mul, subsample)\n\n    # load dictionary\n    with open(args.data_json, ""rb"") as f:\n        j = json.load(f)[""utts""]\n\n    # remove samples with IO ratio smaller than `min_io_ratio`\n    for key in list(j.keys()):\n        ilen = j[key][""input""][0][""shape""][0]\n        olen = min(x[""shape""][0] for x in j[key][""output""])\n        if float(ilen) - float(olen) * min_io_ratio < args.min_io_delta:\n            j.pop(key)\n            print(""\'{}\' removed"".format(key))\n\n    jsonstring = json.dumps({""utts"": j}, indent=4, ensure_ascii=False, sort_keys=True)\n    with open(args.output_json_path, ""w"") as f:\n        f.write(jsonstring)\n'"
egs/wsj_mix/asr1/local/merge_scp2json.py,0,"b'#!/usr/bin/env python33\n# encoding: utf-8\n\nimport argparse\nimport codecs\nfrom io import open\nimport json\nimport logging\nimport sys\n\nfrom espnet.utils.cli_utils import get_commandline_args\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getwriter(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\n# Special types:\ndef shape(x):\n    """"""Change str to List[int]\n\n    >>> shape(\'3,5\')\n    [3, 5]\n    >>> shape(\' [3, 5] \')\n    [3, 5]\n\n    """"""\n\n    # x: \' [3, 5] \' -> \'3, 5\'\n    x = x.strip()\n    if x[0] == ""["":\n        x = x[1:]\n    if x[-1] == ""]"":\n        x = x[:-1]\n\n    return list(map(int, x.split("","")))\n\n\nif __name__ == ""__main__"":\n    description = """"""\n""""""\n    parser = argparse.ArgumentParser(\n        description=""Given each file paths with such format as ""\n        ""<key>:<file>:<type>. type> can be omitted and the default ""\n        \'is ""str"". e.g. {} \'\n        ""--input-scps feat:data/feats.scp shape:data/utt2feat_shape:shape ""\n        ""--input-scps feat:data/feats2.scp shape:data/utt2feat2_shape:shape ""\n        ""--output-scps text:data/text shape:data/utt2text_shape:shape ""\n        ""--scps utt2spk:data/utt2spk"".format(sys.argv[0]),\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        ""--input-scps"",\n        type=str,\n        nargs=""*"",\n        action=""append"",\n        default=[],\n        help=""Json files for the inputs"",\n    )\n    parser.add_argument(\n        ""--output-scps"",\n        type=str,\n        nargs=""*"",\n        action=""append"",\n        default=[],\n        help=""Json files for the outputs"",\n    )\n    parser.add_argument(\n        ""--scps"",\n        type=str,\n        nargs=""+"",\n        default=[],\n        help=""The json files except for the input and outputs"",\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=1, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--out"",\n        ""-O"",\n        type=argparse.FileType(""w""),\n        default=sys.stdout,\n        help=""The output filename. "" ""If omitted, then output to sys.stdout"",\n    )\n\n    args = parser.parse_args()\n    args.scps = [args.scps]\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    # List[List[Tuple[str, str, Callable[[str], Any], str, str]]]\n    input_infos = []\n    output_infos = []\n    infos = []\n    for lis_list, key_scps_list in [\n        (input_infos, args.input_scps),\n        (output_infos, args.output_scps),\n        (infos, args.scps),\n    ]:\n        for key_scps in key_scps_list:\n            lis = []\n            for key_scp in key_scps:\n                sps = key_scp.split("":"")\n                if len(sps) == 2:\n                    key, scp = sps\n                    type_func = str\n                    type_func_str = ""str""\n                elif len(sps) == 3:\n                    key, scp, type_func_str = sps\n                    fail = False\n\n                    try:\n                        # type_func: Callable[[str], Any]\n                        # e.g. type_func_str = ""int"" -> type_func = int\n                        type_func = eval(type_func_str)\n                    except Exception:\n                        raise RuntimeError(""Unknown type: {}"".format(type_func_str))\n\n                    if not callable(type_func):\n                        raise RuntimeError(""Unknown type: {}"".format(type_func_str))\n\n                else:\n                    raise RuntimeError(\n                        ""Format <key>:<filepath> ""\n                        ""or <key>:<filepath>:<type>  ""\n                        ""e.g. feat:data/feat.scp ""\n                        ""or shape:data/feat.scp:shape: {}"".format(key_scp)\n                    )\n\n                for item in lis:\n                    if key == item[0]:\n                        raise RuntimeError(\n                            \'The key ""{}"" is duplicated: {} {}\'.format(\n                                key, item[3], key_scp\n                            )\n                        )\n\n                lis.append((key, scp, type_func, key_scp, type_func_str))\n            lis_list.append(lis)\n\n    # Open  scp files\n    input_fscps = [\n        [open(i[1], ""r"", encoding=""utf-8"") for i in il] for il in input_infos\n    ]\n    output_fscps = [\n        [open(i[1], ""r"", encoding=""utf-8"") for i in il] for il in output_infos\n    ]\n    fscps = [[open(i[1], ""r"", encoding=""utf-8"") for i in il] for il in infos]\n\n    # Note(kamo): What is done here?\n    # The final goal is creating a JSON file such as.\n    # {\n    #     ""utts"": {\n    #         ""sample_id1"": {(omitted)},\n    #         ""sample_id2"": {(omitted)},\n    #          ....\n    #     }\n    # }\n    #\n    # To reduce memory usage, reading the input text files for each lines\n    # and writing JSON elements per samples.\n    args.out.write(\'{\\n    ""utts"": {\\n\')\n    nutt = 0\n    while True:\n        nutt += 1\n        # List[List[str]]\n        input_lines = [[f.readline() for f in fl] for fl in input_fscps]\n        output_lines = [[f.readline() for f in fl] for fl in output_fscps]\n        lines = [[f.readline() for f in fl] for fl in fscps]\n\n        # Get the first line\n        concat = sum(input_lines + output_lines + lines, [])\n        if len(concat) == 0:\n            break\n        first = concat[0]\n\n        # Sanity check: Must be sorted by the first column and have same keys\n        count = 0\n        for ls_list in (input_lines, output_lines, lines):\n            for ls in ls_list:\n                for line in ls:\n                    if line == """" or first == """":\n                        if line != first:\n                            concat = sum(input_infos + output_infos + infos, [])\n                            raise RuntimeError(\n                                ""The number of lines mismatch ""\n                                \'between: ""{}"" and ""{}""\'.format(\n                                    concat[0][1], concat[count][1]\n                                )\n                            )\n\n                    elif line.split()[0] != first.split()[0]:\n                        concat = sum(input_infos + output_infos + infos, [])\n                        raise RuntimeError(\n                            ""The keys are mismatch at {}th line ""\n                            \'between ""{}"" and ""{}"":\\n>>> {}\\n>>> {}\'.format(\n                                nutt,\n                                concat[0][1],\n                                concat[count][1],\n                                first.rstrip(),\n                                line.rstrip(),\n                            )\n                        )\n                    count += 1\n\n        # The end of file\n        if first == """":\n            if nutt != 1:\n                args.out.write(""\\n"")\n            break\n        if nutt != 1:\n            args.out.write("",\\n"")\n\n        entry = {}\n        for inout, _lines, _infos in [\n            (""input"", input_lines, input_infos),\n            (""output"", output_lines, output_infos),\n            (""other"", lines, infos),\n        ]:\n\n            lis = []\n            for idx, (line_list, info_list) in enumerate(zip(_lines, _infos), 1):\n                if inout == ""input"":\n                    d = {""name"": ""input{}"".format(idx)}\n                elif inout == ""output"":\n                    d = {""name"": ""target{}"".format(idx)}\n                else:\n                    d = {}\n\n                # info_list: List[Tuple[str, str, Callable]]\n                # line_list: List[str]\n                for line, info in zip(line_list, info_list):\n                    sps = line.split(None, 1)\n                    if len(sps) < 2:\n                        raise RuntimeError(\n                            ""Format error {}th line in {}: ""\n                            \' Expecting ""<key> <value>"":\\n>>> {}\'.format(\n                                nutt, info[1], line\n                            )\n                        )\n                    uttid, value = sps\n                    key = info[0]\n                    type_func = info[2]\n                    value = value.rstrip()\n\n                    try:\n                        # type_func: Callable[[str], Any]\n                        value = type_func(value)\n                    except Exception:\n                        logging.error(\n                            \'""{}"" is an invalid function \'\n                            ""for the {} th line in {}: \\n>>> {}"".format(\n                                info[4], nutt, info[1], line\n                            )\n                        )\n                        raise\n\n                    d[key] = value\n                lis.append(d)\n\n            if inout != ""other"":\n                entry[inout] = lis\n            else:\n                # If key == \'other\'. only has the first item\n                entry.update(lis[0])\n\n        entry = json.dumps(\n            entry, indent=4, ensure_ascii=False, sort_keys=True, separators=("","", "": "")\n        )\n        # Add indent\n        indent = ""    "" * 2\n        entry = (""\\n"" + indent).join(entry.split(""\\n""))\n\n        uttid = first.split()[0]\n        args.out.write(\'        ""{}"": {}\'.format(uttid, entry))\n\n    args.out.write(""    }\\n}\\n"")\n\n    logging.info(""{} entries in {}"".format(nutt, args.out.name))\n'"
egs/wsj_mix/asr1/local/mergejson.py,0,"b'#!/usr/bin/env python2\n# encoding: utf-8\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#           2018 Xuankai Chang (Shanghai Jiao Tong University)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nimport json\nimport logging\nimport sys\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""jsons"", type=str, nargs=""+"", help=""json files"")\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(""--output-json"", default="""", type=str, help=""output json file"")\n    args = parser.parse_args()\n\n    # logging info\n    if args.verbose > 0:\n        logging.basicConfig(\n            level=logging.INFO,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n    else:\n        logging.basicConfig(\n            level=logging.WARN,\n            format=""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s"",\n        )\n\n    # make intersection set for utterance keys\n    js = []\n    intersec_ks = []\n    for x in args.jsons:\n        with open(x, ""r"") as f:\n            j = json.load(f)\n        ks = j[""utts""].keys()\n        logging.info(x + "": has "" + str(len(ks)) + "" utterances"")\n        if len(intersec_ks) > 0:\n            intersec_ks = intersec_ks.intersection(set(ks))\n        else:\n            intersec_ks = set(ks)\n        js.append(j)\n    logging.info(""new json has "" + str(len(intersec_ks)) + "" utterances"")\n\n    old_dic = dict()\n    for k in intersec_ks:\n        v = js[0][""utts""][k]\n        for j in js[1:]:\n            v.update(j[""utts""][k])\n        old_dic[k] = v\n\n    new_dic = dict()\n    for id in old_dic:\n        dic = old_dic[id]\n\n        in_dic = {}\n        # if unicode(\'idim\', \'utf-8\') in dic:\n        if ""idim"" in dic:\n            in_dic[""shape""] = (\n                int(dic[""ilen""]),\n                int(dic[""idim""]),\n            )\n        in_dic[""name""] = ""input1""\n        in_dic[""feat""] = dic[""feat""]\n\n        out_list = []\n        out_idx = 1\n        while ""text_spk%d"" % out_idx in dic:\n            out_dic = {}\n            out_dic[""name""] = ""target%d"" % out_idx\n            out_dic[""shape""] = int(dic[""olen_spk%d"" % out_idx]), int(dic[""odim""])\n            out_dic[""text""] = dic[""text_spk%d"" % out_idx]\n            out_dic[""token""] = dic[""token_spk%d"" % out_idx]\n            out_dic[""tokenid""] = dic[""tokenid_spk%d"" % out_idx]\n            out_list.append(out_dic)\n            out_idx += 1\n\n        new_dic[id] = {\n            ""input"": [in_dic],\n            ""output"": out_list,\n            ""utt2spk"": dic[""utt2spk""],\n        }\n\n    # ensure ""ensure_ascii=False"", which is a bug\n    if args.output_json:\n        with open(args.output_json, ""w"", encoding=""utf-8"") as json_file:\n            json.dumps(\n                {""utts"": new_dic},\n                json_file,\n                indent=4,\n                ensure_ascii=False,\n                sort_keys=True,\n                encoding=""utf-8"",\n            )\n    else:\n        sys.stdout = codecs.getwriter(""utf8"")(sys.stdout)\n        json.dump(\n            {""utts"": new_dic},\n            sys.stdout,\n            indent=4,\n            ensure_ascii=False,\n            sort_keys=True,\n            encoding=""utf-8"",\n        )\n'"
egs2/commonvoice/asr1/local/filter_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport codecs\nfrom io import open\nimport sys\n\n\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--filter-list"", ""-f"", type=str, help=""filter list"")\n    args = parser.parse_args()\n\n    with open(args.filter_list, encoding=""utf-8"") as f:\n        fil = [x.rstrip() for x in f]\n\n    for x in sys.stdin:\n        # extract text parts\n        text = "" "".join(x.rstrip().split()[1:])\n        if text in fil:\n            print(x.split()[0], text)\n'"
egs2/csj/asr1/local/csj_rm_tag.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nfrom io import open\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--skip-ncols"", ""-s"", default=0, type=int, help=""skip first n columns""\n    )\n    parser.add_argument(""text"", type=str, help=""input text"")\n    args = parser.parse_args()\n\n    if args.text:\n        f = open(args.text, encoding=""utf-8"")\n    else:\n        f = sys.stdin\n\n    for line in f:\n        x = line.split()\n        print("" "".join(x[: args.skip_ncols]), end="" "")\n        print("" "".join([st.split(""+"")[0] for st in x[args.skip_ncols :]]))\n'"
egs2/mini_an4/asr1/local/data_prep.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2016  Allen Guo\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport os\nimport re\nimport sys\n\nif len(sys.argv) != 3:\n    print(""Usage: python data_prep.py [an4_root] [sph2pipe]"")\n    sys.exit(1)\nan4_root = sys.argv[1]\nsph2pipe = sys.argv[2]\n\nsph_dir = {""train"": ""an4_clstk"", ""test"": ""an4test_clstk""}\n\nfor x in [""train"", ""test""]:\n    with open(\n        os.path.join(an4_root, ""etc"", ""an4_"" + x + "".transcription"")\n    ) as transcript_f, open(os.path.join(""data"", x, ""text""), ""w"") as text_f, open(\n        os.path.join(""data"", x, ""wav.scp""), ""w""\n    ) as wav_scp_f, open(\n        os.path.join(""data"", x, ""utt2spk""), ""w""\n    ) as utt2spk_f:\n\n        text_f.truncate()\n        wav_scp_f.truncate()\n        utt2spk_f.truncate()\n\n        lines = sorted(transcript_f.readlines(), key=lambda s: s.split("" "")[0])\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            words = re.search(r""^(.*) \\("", line).group(1)\n            if words[:4] == ""<s> "":\n                words = words[4:]\n            if words[-5:] == "" </s>"":\n                words = words[:-5]\n            source = re.search(r""\\((.*)\\)"", line).group(1)\n            pre, mid, last = source.split(""-"")\n            utt_id = ""-"".join([mid, pre, last])\n\n            text_f.write(utt_id + "" "" + words + ""\\n"")\n            wav_scp_f.write(\n                utt_id\n                + "" ""\n                + sph2pipe\n                + "" -f wav -p -c 1 ""\n                + os.path.join(an4_root, ""wav"", sph_dir[x], mid, source + "".sph"")\n                + "" |\\n""\n            )\n            utt2spk_f.write(utt_id + "" "" + mid + ""\\n"")\n'"
egs2/voxforge/asr1/local/filter_text.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import unicode_literals\n\nimport argparse\nimport codecs\nfrom io import open\nimport sys\n\n\nPY2 = sys.version_info[0] == 2\nsys.stdin = codecs.getreader(""utf-8"")(sys.stdin if PY2 else sys.stdin.buffer)\nsys.stdout = codecs.getwriter(""utf-8"")(sys.stdout if PY2 else sys.stdout.buffer)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--filter-list"", ""-f"", type=str, help=""filter list"")\n    args = parser.parse_args()\n\n    with open(args.filter_list, encoding=""utf-8"") as f:\n        fil = [x.rstrip() for x in f]\n\n    for x in sys.stdin:\n        # extract text parts\n        text = "" "".join(x.rstrip().split()[1:])\n        if text in fil:\n            print(x.split()[0], text)\n'"
egs2/voxforge/asr1/local/make_trans.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2012 Vassil Panayotov\n# Apache 2.0\n\n""""""\nTakes a ""PROMPTS"" file with lines like:\n1snoke-20120412-hge/mfc/a0405\nIT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n\n, an ID prefix and a list of audio file names (e.g. for above example the list\nwill contain ""a0405"").\nIt checks if the prompts file have transcription for all audio files in the list and\nif this is the case produces a transcript line for each file in the format:\nprefix_a0405 IT SEEMED THE ORDAINED ORDER OF THINGS THAT DOGS SHOULD WORK\n""""""\nimport sys\n\n\ndef err(msg):\n    print(msg, file=sys.stderr)\n\n\nif len(sys.argv) < 3:\n    err(""Usage: %s <prompts-file> <id-prefix> <utt-id1> <utt-id2> ... "" % sys.argv[0])\n    sys.exit(1)\n\n# err(str(sys.argv))\nid_prefix = sys.argv[2]\nutt_ids = sys.argv[3:]\nutt2trans = dict()\nunnorm_utt = set()\nfor line in open(sys.argv[1], encoding=""utf-8""):\n    u, trans = line.split(None, 1)\n    u = u.strip().split(""/"")[-1]\n    trans = trans.strip().replace(""-"", "" "").upper()\n    if (\n        not trans.isupper()\n        or not trans.strip().replace("" "", """").replace(""\'"", """").isalnum()\n    ):\n        # Note(kamo): Changed from the original: isalpha() -> isalnum()\n        # not trans.strip().replace(\' \', \'\').replace(""\'"", """").isalpha():\n        err(\n            ""The transcript for \'%s\'(user \'%s\') is not properly normalized - skipped!""\n            % (u, id_prefix)\n        )\n        err(trans)\n        unnorm_utt.add(u)\n        continue\n    utt2trans[u] = trans\n\nfor uid in utt_ids:\n    if uid in unnorm_utt:\n        continue  # avoid double reporting the same problem\n    if uid not in utt2trans:\n        err(""No transcript found for %s_%s"" % (id_prefix, uid))\n        continue\n    print(""%s-%s %s"" % (id_prefix, uid, utt2trans[uid]))\n'"
espnet/nets/chainer_backend/rnn/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/chainer_backend/rnn/attentions.py,0,"b'import chainer\nimport chainer.functions as F\nimport chainer.links as L\n\nimport numpy as np\n\n\n# dot product based attention\nclass AttDot(chainer.Chain):\n    """"""Compute attention based on dot product.\n\n    Args:\n        eprojs (int | None): Dimension of input vectors from encoder.\n        dunits (int | None): Dimension of input vectors for decoder.\n        att_dim (int): Dimension of input vectors for attention.\n\n    """"""\n\n    def __init__(self, eprojs, dunits, att_dim):\n        super(AttDot, self).__init__()\n        with self.init_scope():\n            self.mlp_enc = L.Linear(eprojs, att_dim)\n            self.mlp_dec = L.Linear(dunits, att_dim)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n\n    def reset(self):\n        """"""Reset states.""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n\n    def __call__(self, enc_hs, dec_z, att_prev, scaling=2.0):\n        """"""Compute AttDot forward layer.\n\n        Args:\n            enc_hs (chainer.Variable | N-dimensional array):\n                Input variable from encoder.\n            dec_z (chainer.Variable | N-dimensional array): Input variable of decoder.\n            scaling (float): Scaling weight to make attention sharp.\n\n        Returns:\n            chainer.Variable: Weighted sum over flames.\n            chainer.Variable: Attention weight.\n\n        """"""\n        batch = len(enc_hs)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None:\n            self.enc_h = F.pad_sequence(enc_hs)  # utt x frame x hdim\n            self.h_length = self.enc_h.shape[1]\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = F.tanh(self.mlp_enc(self.enc_h, n_batch_axes=2))\n\n        if dec_z is None:\n            dec_z = chainer.Variable(\n                self.xp.zeros((batch, self.dunits), dtype=np.float32)\n            )\n        else:\n            dec_z = dec_z.reshape(batch, self.dunits)\n\n        # <phi (h_t), psi (s)> for all t\n        u = F.broadcast_to(\n            F.expand_dims(F.tanh(self.mlp_dec(dec_z)), 1), self.pre_compute_enc_h.shape\n        )\n        e = F.sum(self.pre_compute_enc_h * u, axis=2)  # utt x frame\n        # Applying a minus-large-number filter\n        # to make a probability value zero for a padded area\n        # simply degrades the performance, and I gave up this implementation\n        # Apply a scaling to make an attention sharp\n        w = F.softmax(scaling * e)\n        # weighted sum over flames\n        # utt x hdim\n        c = F.sum(\n            self.enc_h * F.broadcast_to(F.expand_dims(w, 2), self.enc_h.shape), axis=1\n        )\n\n        return c, w\n\n\n# location based attention\nclass AttLoc(chainer.Chain):\n    """"""Compute location-based attention.\n\n    Args:\n        eprojs (int | None): Dimension of input vectors from encoder.\n        dunits (int | None): Dimension of input vectors for decoder.\n        att_dim (int): Dimension of input vectors for attention.\n        aconv_chans (int): Number of channels of output arrays from convolutional layer.\n        aconv_filts (int): Size of filters of convolutional layer.\n\n    """"""\n\n    def __init__(self, eprojs, dunits, att_dim, aconv_chans, aconv_filts):\n        super(AttLoc, self).__init__()\n        with self.init_scope():\n            self.mlp_enc = L.Linear(eprojs, att_dim)\n            self.mlp_dec = L.Linear(dunits, att_dim, nobias=True)\n            self.mlp_att = L.Linear(aconv_chans, att_dim, nobias=True)\n            self.loc_conv = L.Convolution2D(\n                1, aconv_chans, ksize=(1, 2 * aconv_filts + 1), pad=(0, aconv_filts)\n            )\n            self.gvec = L.Linear(att_dim, 1)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.aconv_chans = aconv_chans\n\n    def reset(self):\n        """"""Reset states.""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n\n    def __call__(self, enc_hs, dec_z, att_prev, scaling=2.0):\n        """"""Compute AttLoc forward layer.\n\n        Args:\n            enc_hs (chainer.Variable | N-dimensional array):\n                Input variable from encoders.\n            dec_z (chainer.Variable | N-dimensional array): Input variable of decoder.\n            att_prev (chainer.Variable | None): Attention weight.\n            scaling (float): Scaling weight to make attention sharp.\n\n        Returns:\n            chainer.Variable: Weighted sum over flames.\n            chainer.Variable: Attention weight.\n\n        """"""\n        batch = len(enc_hs)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None:\n            self.enc_h = F.pad_sequence(enc_hs)  # utt x frame x hdim\n            self.h_length = self.enc_h.shape[1]\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h, n_batch_axes=2)\n\n        if dec_z is None:\n            dec_z = chainer.Variable(\n                self.xp.zeros((batch, self.dunits), dtype=np.float32)\n            )\n        else:\n            dec_z = dec_z.reshape(batch, self.dunits)\n\n        # initialize attention weight with uniform dist.\n        if att_prev is None:\n            att_prev = [\n                self.xp.full(hh.shape[0], 1.0 / hh.shape[0], dtype=np.float32)\n                for hh in enc_hs\n            ]\n            att_prev = [chainer.Variable(att) for att in att_prev]\n            att_prev = F.pad_sequence(att_prev)\n\n        # att_prev: utt x frame -> utt x 1 x 1 x frame\n        # -> utt x att_conv_chans x 1 x frame\n        att_conv = self.loc_conv(att_prev.reshape(batch, 1, 1, self.h_length))\n        # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans\n        att_conv = F.swapaxes(F.squeeze(att_conv, axis=2), 1, 2)\n        # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim\n        att_conv = self.mlp_att(att_conv, n_batch_axes=2)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = F.broadcast_to(\n            F.expand_dims(self.mlp_dec(dec_z), 1), self.pre_compute_enc_h.shape\n        )\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        # TODO(watanabe) use batch_matmul\n        e = F.squeeze(\n            self.gvec(\n                F.tanh(att_conv + self.pre_compute_enc_h + dec_z_tiled), n_batch_axes=2\n            ),\n            axis=2,\n        )\n        # Applying a minus-large-number filter\n        # to make a probability value zero for a padded area\n        # simply degrades the performance, and I gave up this implementation\n        # Apply a scaling to make an attention sharp\n        w = F.softmax(scaling * e)\n\n        # weighted sum over flames\n        # utt x hdim\n        c = F.sum(\n            self.enc_h * F.broadcast_to(F.expand_dims(w, 2), self.enc_h.shape), axis=1\n        )\n\n        return c, w\n\n\nclass NoAtt(chainer.Chain):\n    """"""Compute non-attention layer.\n\n    This layer is a dummy attention layer to be compatible with other\n    attention-based models.\n\n    """"""\n\n    def __init__(self):\n        super(NoAtt, self).__init__()\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.c = None\n\n    def reset(self):\n        """"""Reset states.""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.c = None\n\n    def __call__(self, enc_hs, dec_z, att_prev):\n        """"""Compute NoAtt forward layer.\n\n        Args:\n            enc_hs (chainer.Variable | N-dimensional array):\n                Input variable from encoders.\n            dec_z: Dummy.\n            att_prev (chainer.Variable | None): Attention weight.\n\n        Returns:\n            chainer.Variable: Sum over flames.\n            chainer.Variable: Attention weight.\n\n        """"""\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None:\n            self.enc_h = F.pad_sequence(enc_hs)  # utt x frame x hdim\n            self.h_length = self.enc_h.shape[1]\n\n        # initialize attention weight with uniform dist.\n        if att_prev is None:\n            att_prev = [\n                self.xp.full(hh.shape[0], 1.0 / hh.shape[0], dtype=np.float32)\n                for hh in enc_hs\n            ]\n            att_prev = [chainer.Variable(att) for att in att_prev]\n            att_prev = F.pad_sequence(att_prev)\n            self.c = F.sum(\n                self.enc_h\n                * F.broadcast_to(F.expand_dims(att_prev, 2), self.enc_h.shape),\n                axis=1,\n            )\n\n        return self.c, att_prev\n\n\ndef att_for(args):\n    """"""Returns an attention layer given the program arguments.\n\n    Args:\n        args (Namespace): The arguments.\n\n    Returns:\n        chainer.Chain: The corresponding attention module.\n\n    """"""\n    if args.atype == ""dot"":\n        att = AttDot(args.eprojs, args.dunits, args.adim)\n    elif args.atype == ""location"":\n        att = AttLoc(\n            args.eprojs, args.dunits, args.adim, args.aconv_chans, args.aconv_filts\n        )\n    elif args.atype == ""noatt"":\n        att = NoAtt()\n    else:\n        raise NotImplementedError(\n            ""chainer supports only noatt, dot, and location attention.""\n        )\n    return att\n'"
espnet/nets/chainer_backend/rnn/decoders.py,0,"b'import logging\nimport random\nimport six\n\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport numpy as np\n\nimport espnet.nets.chainer_backend.deterministic_embed_id as DL\n\nfrom argparse import Namespace\n\nfrom espnet.nets.ctc_prefix_score import CTCPrefixScore\nfrom espnet.nets.e2e_asr_common import end_detect\n\nCTC_SCORING_RATIO = 1.5\nMAX_DECODER_OUTPUT = 5\n\n\nclass Decoder(chainer.Chain):\n    """"""Decoder layer.\n\n    Args:\n        eprojs (int): Dimension of input variables from encoder.\n        odim (int): The output dimension.\n        dtype (str): Decoder type.\n        dlayers (int): Number of layers for decoder.\n        dunits (int): Dimension of input vector of decoder.\n        sos (int): Number to indicate the start of sequences.\n        eos (int): Number to indicate the end of sequences.\n        att (Module): Attention module defined at\n            `espnet.espnet.nets.chainer_backend.attentions`.\n        verbose (int): Verbosity level.\n        char_list (List[str]): List of all charactors.\n        labeldist (numpy.array): Distributed array of counted transcript length.\n        lsm_weight (float): Weight to use when calculating the training loss.\n        sampling_probability (float): Threshold for scheduled sampling.\n\n    """"""\n\n    def __init__(\n        self,\n        eprojs,\n        odim,\n        dtype,\n        dlayers,\n        dunits,\n        sos,\n        eos,\n        att,\n        verbose=0,\n        char_list=None,\n        labeldist=None,\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n    ):\n        super(Decoder, self).__init__()\n        with self.init_scope():\n            self.embed = DL.EmbedID(odim, dunits)\n            self.rnn0 = (\n                L.StatelessLSTM(dunits + eprojs, dunits)\n                if dtype == ""lstm""\n                else L.StatelessGRU(dunits + eprojs, dunits)\n            )\n            for i in six.moves.range(1, dlayers):\n                setattr(\n                    self,\n                    ""rnn%d"" % i,\n                    L.StatelessLSTM(dunits, dunits)\n                    if dtype == ""lstm""\n                    else L.StatelessGRU(dunits, dunits),\n                )\n            self.output = L.Linear(dunits, odim)\n        self.dtype = dtype\n        self.loss = None\n        self.att = att\n        self.dlayers = dlayers\n        self.dunits = dunits\n        self.sos = sos\n        self.eos = eos\n        self.verbose = verbose\n        self.char_list = char_list\n        # for label smoothing\n        self.labeldist = labeldist\n        self.vlabeldist = None\n        self.lsm_weight = lsm_weight\n        self.sampling_probability = sampling_probability\n\n    def rnn_forward(self, ey, z_list, c_list, z_prev, c_prev):\n        if self.dtype == ""lstm"":\n            c_list[0], z_list[0] = self.rnn0(c_prev[0], z_prev[0], ey)\n            for i in six.moves.range(1, self.dlayers):\n                c_list[i], z_list[i] = self[""rnn%d"" % i](\n                    c_prev[i], z_prev[i], z_list[i - 1]\n                )\n        else:\n            if z_prev[0] is None:\n                xp = self.xp\n                with chainer.backends.cuda.get_device_from_id(self._device_id):\n                    z_prev[0] = chainer.Variable(\n                        xp.zeros((ey.shape[0], self.dunits), dtype=ey.dtype)\n                    )\n            z_list[0] = self.rnn0(z_prev[0], ey)\n            for i in six.moves.range(1, self.dlayers):\n                if z_prev[i] is None:\n                    xp = self.xp\n                    with chainer.backends.cuda.get_device_from_id(self._device_id):\n                        z_prev[i] = chainer.Variable(\n                            xp.zeros(\n                                (z_list[i - 1].shape[0], self.dunits),\n                                dtype=z_list[i - 1].dtype,\n                            )\n                        )\n                z_list[i] = self[""rnn%d"" % i](z_prev[i], z_list[i - 1])\n        return z_list, c_list\n\n    def __call__(self, hs, ys):\n        """"""Core function of Decoder layer.\n\n        Args:\n            hs (list of chainer.Variable | N-dimension array):\n                Input variable from encoder.\n            ys (list of chainer.Variable | N-dimension array):\n                Input variable of decoder.\n\n        Returns:\n            chainer.Variable: A variable holding a scalar array of the training loss.\n            chainer.Variable: A variable holding a scalar array of the accuracy.\n\n        """"""\n        self.loss = None\n        # prepare input and output word sequences with sos/eos IDs\n        eos = self.xp.array([self.eos], ""i"")\n        sos = self.xp.array([self.sos], ""i"")\n        ys_in = [F.concat([sos, y], axis=0) for y in ys]\n        ys_out = [F.concat([y, eos], axis=0) for y in ys]\n\n        # padding for ys with -1\n        # pys: utt x olen\n        pad_ys_in = F.pad_sequence(ys_in, padding=self.eos)\n        pad_ys_out = F.pad_sequence(ys_out, padding=-1)\n\n        # get dim, length info\n        batch = pad_ys_out.shape[0]\n        olength = pad_ys_out.shape[1]\n        logging.info(\n            self.__class__.__name__\n            + "" input lengths:  ""\n            + str(self.xp.array([h.shape[0] for h in hs]))\n        )\n        logging.info(\n            self.__class__.__name__\n            + "" output lengths: ""\n            + str(self.xp.array([y.shape[0] for y in ys_out]))\n        )\n\n        # initialization\n        c_list = [None]  # list of cell state of each layer\n        z_list = [None]  # list of hidden state of each layer\n        for _ in six.moves.range(1, self.dlayers):\n            c_list.append(None)\n            z_list.append(None)\n        att_w = None\n        z_all = []\n        self.att.reset()  # reset pre-computation of h\n\n        # pre-computation of embedding\n        eys = self.embed(pad_ys_in)  # utt x olen x zdim\n        eys = F.separate(eys, axis=1)\n\n        # loop for an output sequence\n        for i in six.moves.range(olength):\n            att_c, att_w = self.att(hs, z_list[0], att_w)\n            if i > 0 and random.random() < self.sampling_probability:\n                logging.info("" scheduled sampling "")\n                z_out = self.output(z_all[-1])\n                z_out = F.argmax(F.log_softmax(z_out), axis=1)\n                z_out = self.embed(z_out)\n                ey = F.hstack((z_out, att_c))  # utt x (zdim + hdim)\n            else:\n                ey = F.hstack((eys[i], att_c))  # utt x (zdim + hdim)\n            z_list, c_list = self.rnn_forward(ey, z_list, c_list, z_list, c_list)\n            z_all.append(z_list[-1])\n\n        z_all = F.stack(z_all, axis=1).reshape(batch * olength, self.dunits)\n        # compute loss\n        y_all = self.output(z_all)\n        self.loss = F.softmax_cross_entropy(y_all, F.flatten(pad_ys_out))\n        # -1: eos, which is removed in the loss computation\n        self.loss *= np.mean([len(x) for x in ys_in]) - 1\n        acc = F.accuracy(y_all, F.flatten(pad_ys_out), ignore_label=-1)\n        logging.info(""att loss:"" + str(self.loss.data))\n\n        # show predicted character sequence for debug\n        if self.verbose > 0 and self.char_list is not None:\n            y_hat = y_all.reshape(batch, olength, -1)\n            y_true = pad_ys_out\n            for (i, y_hat_), y_true_ in zip(enumerate(y_hat.data), y_true.data):\n                if i == MAX_DECODER_OUTPUT:\n                    break\n                idx_hat = self.xp.argmax(y_hat_[y_true_ != -1], axis=1)\n                idx_true = y_true_[y_true_ != -1]\n                seq_hat = [self.char_list[int(idx)] for idx in idx_hat]\n                seq_true = [self.char_list[int(idx)] for idx in idx_true]\n                seq_hat = """".join(seq_hat).replace(""<space>"", "" "")\n                seq_true = """".join(seq_true).replace(""<space>"", "" "")\n                logging.info(""groundtruth[%d]: "" % i + seq_true)\n                logging.info(""prediction [%d]: "" % i + seq_hat)\n\n        if self.labeldist is not None:\n            if self.vlabeldist is None:\n                self.vlabeldist = chainer.Variable(self.xp.asarray(self.labeldist))\n            loss_reg = -F.sum(\n                F.scale(F.log_softmax(y_all), self.vlabeldist, axis=1)\n            ) / len(ys_in)\n            self.loss = (1.0 - self.lsm_weight) * self.loss + self.lsm_weight * loss_reg\n\n        return self.loss, acc\n\n    def recognize_beam(self, h, lpz, recog_args, char_list, rnnlm=None):\n        """"""Beam search implementation.\n\n        Args:\n            h (chainer.Variable): One of the output from the encoder.\n            lpz (chainer.Variable | None): Result of net propagation.\n            recog_args (Namespace): The argument.\n            char_list (List[str]): List of all charactors.\n            rnnlm (Module): RNNLM module. Defined at `espnet.lm.chainer_backend.lm`\n\n        Returns:\n            List[Dict[str,Any]]: Result of recognition.\n\n        """"""\n        logging.info(""input lengths: "" + str(h.shape[0]))\n        # initialization\n        c_list = [None]  # list of cell state of each layer\n        z_list = [None]  # list of hidden state of each layer\n        for _ in six.moves.range(1, self.dlayers):\n            c_list.append(None)\n            z_list.append(None)\n        a = None\n        self.att.reset()  # reset pre-computation of h\n\n        # search parms\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = recog_args.ctc_weight\n\n        # preprate sos\n        y = self.xp.full(1, self.sos, ""i"")\n        if recog_args.maxlenratio == 0:\n            maxlen = h.shape[0]\n        else:\n            # maxlen >= 1\n            maxlen = max(1, int(recog_args.maxlenratio * h.shape[0]))\n        minlen = int(recog_args.minlenratio * h.shape[0])\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {\n                ""score"": 0.0,\n                ""yseq"": [y],\n                ""c_prev"": c_list,\n                ""z_prev"": z_list,\n                ""a_prev"": a,\n                ""rnnlm_prev"": None,\n            }\n        else:\n            hyp = {\n                ""score"": 0.0,\n                ""yseq"": [y],\n                ""c_prev"": c_list,\n                ""z_prev"": z_list,\n                ""a_prev"": a,\n            }\n        if lpz is not None:\n            ctc_prefix_score = CTCPrefixScore(lpz, 0, self.eos, self.xp)\n            hyp[""ctc_state_prev""] = ctc_prefix_score.initial_state()\n            hyp[""ctc_score_prev""] = 0.0\n            if ctc_weight != 1.0:\n                # pre-pruning based on attention scores\n                ctc_beam = min(lpz.shape[-1], int(beam * CTC_SCORING_RATIO))\n            else:\n                ctc_beam = lpz.shape[-1]\n        hyps = [hyp]\n        ended_hyps = []\n\n        for i in six.moves.range(maxlen):\n            logging.debug(""position "" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                ey = self.embed(hyp[""yseq""][i])  # utt list (1) x zdim\n                att_c, att_w = self.att([h], hyp[""z_prev""][0], hyp[""a_prev""])\n                ey = F.hstack((ey, att_c))  # utt(1) x (zdim + hdim)\n\n                z_list, c_list = self.rnn_forward(\n                    ey, z_list, c_list, hyp[""z_prev""], hyp[""c_prev""]\n                )\n\n                # get nbest local scores and their ids\n                local_att_scores = F.log_softmax(self.output(z_list[-1])).data\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(\n                        hyp[""rnnlm_prev""], hyp[""yseq""][i]\n                    )\n                    local_scores = (\n                        local_att_scores + recog_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                if lpz is not None:\n                    local_best_ids = self.xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :ctc_beam\n                    ]\n                    ctc_scores, ctc_states = ctc_prefix_score(\n                        hyp[""yseq""], local_best_ids, hyp[""ctc_state_prev""]\n                    )\n                    local_scores = (1.0 - ctc_weight) * local_att_scores[\n                        :, local_best_ids\n                    ] + ctc_weight * (ctc_scores - hyp[""ctc_score_prev""])\n                    if rnnlm:\n                        local_scores += (\n                            recog_args.lm_weight * local_lm_scores[:, local_best_ids]\n                        )\n                    joint_best_ids = self.xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :beam\n                    ]\n                    local_best_scores = local_scores[:, joint_best_ids]\n                    local_best_ids = local_best_ids[joint_best_ids]\n                else:\n                    local_best_ids = self.xp.argsort(local_scores, axis=1)[0, ::-1][\n                        :beam\n                    ]\n                    local_best_scores = local_scores[:, local_best_ids]\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    # do not copy {z,c}_list directly\n                    new_hyp[""z_prev""] = z_list[:]\n                    new_hyp[""c_prev""] = c_list[:]\n                    new_hyp[""a_prev""] = att_w\n                    new_hyp[""score""] = hyp[""score""] + local_best_scores[0, j]\n                    new_hyp[""yseq""] = [0] * (1 + len(hyp[""yseq""]))\n                    new_hyp[""yseq""][: len(hyp[""yseq""])] = hyp[""yseq""]\n                    new_hyp[""yseq""][len(hyp[""yseq""])] = self.xp.full(\n                        1, local_best_ids[j], ""i""\n                    )\n                    if rnnlm:\n                        new_hyp[""rnnlm_prev""] = rnnlm_state\n                    if lpz is not None:\n                        new_hyp[""ctc_state_prev""] = ctc_states[joint_best_ids[j]]\n                        new_hyp[""ctc_score_prev""] = ctc_scores[joint_best_ids[j]]\n                    # will be (2 x beam) hyps at most\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[""score""], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(""number of pruned hypotheses: "" + str(len(hyps)))\n            logging.debug(\n                ""best hypo: ""\n                + """".join([char_list[int(x)] for x in hyps[0][""yseq""][1:]]).replace(\n                    ""<space>"", "" ""\n                )\n            )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(""adding <eos> in the last position in the loop"")\n                for hyp in hyps:\n                    hyp[""yseq""].append(self.xp.full(1, self.eos, ""i""))\n\n            # add ended hypotheses to a final list,\n            # and removed them from current hypotheses\n            # (this will be a problem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[""yseq""][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[""yseq""]) > minlen:\n                        hyp[""score""] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[""score""] += recog_args.lm_weight * rnnlm.final(\n                                hyp[""rnnlm_prev""]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n            if end_detect(ended_hyps, i) and recog_args.maxlenratio == 0.0:\n                logging.info(""end detected at %d"", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(""remaining hypotheses: "" + str(len(hyps)))\n            else:\n                logging.info(""no hypothesis. Finish decoding."")\n                break\n\n            for hyp in hyps:\n                logging.debug(\n                    ""hypo: ""\n                    + """".join([char_list[int(x)] for x in hyp[""yseq""][1:]]).replace(\n                        ""<space>"", "" ""\n                    )\n                )\n\n            logging.debug(""number of ended hypotheses: "" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x[""score""], reverse=True)[\n            : min(len(ended_hyps), recog_args.nbest)\n        ]\n\n        # check number of hypotheses\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                ""there is no N-best results, ""\n                ""perform recognition again with smaller minlenratio.""\n            )\n            # should copy because Namespace will be overwritten globally\n            recog_args = Namespace(**vars(recog_args))\n            recog_args.minlenratio = max(0.0, recog_args.minlenratio - 0.1)\n            return self.recognize_beam(h, lpz, recog_args, char_list, rnnlm)\n\n        logging.info(""total log probability: "" + str(nbest_hyps[0][""score""]))\n        logging.info(\n            ""normalized log probability: ""\n            + str(nbest_hyps[0][""score""] / len(nbest_hyps[0][""yseq""]))\n        )\n\n        return nbest_hyps\n\n    def calculate_all_attentions(self, hs, ys):\n        """"""Calculate all of attentions.\n\n        Args:\n            hs (list of chainer.Variable | N-dimensional array):\n                Input variable from encoder.\n            ys (list of chainer.Variable | N-dimensional array):\n                Input variable of decoder.\n\n        Returns:\n            chainer.Variable: List of attention weights.\n\n        """"""\n        # prepare input and output word sequences with sos/eos IDs\n        eos = self.xp.array([self.eos], ""i"")\n        sos = self.xp.array([self.sos], ""i"")\n        ys_in = [F.concat([sos, y], axis=0) for y in ys]\n        ys_out = [F.concat([y, eos], axis=0) for y in ys]\n\n        # padding for ys with -1\n        # pys: utt x olen\n        pad_ys_in = F.pad_sequence(ys_in, padding=self.eos)\n        pad_ys_out = F.pad_sequence(ys_out, padding=-1)\n\n        # get length info\n        olength = pad_ys_out.shape[1]\n\n        # initialization\n        c_list = [None]  # list of cell state of each layer\n        z_list = [None]  # list of hidden state of each layer\n        for _ in six.moves.range(1, self.dlayers):\n            c_list.append(None)\n            z_list.append(None)\n        att_w = None\n        att_ws = []\n        self.att.reset()  # reset pre-computation of h\n\n        # pre-computation of embedding\n        eys = self.embed(pad_ys_in)  # utt x olen x zdim\n        eys = F.separate(eys, axis=1)\n\n        # loop for an output sequence\n        for i in six.moves.range(olength):\n            att_c, att_w = self.att(hs, z_list[0], att_w)\n            ey = F.hstack((eys[i], att_c))  # utt x (zdim + hdim)\n            z_list, c_list = self.rnn_forward(ey, z_list, c_list, z_list, c_list)\n            att_ws.append(att_w)  # for debugging\n\n        att_ws = F.stack(att_ws, axis=1)\n        att_ws.to_cpu()\n\n        return att_ws.data\n\n\ndef decoder_for(args, odim, sos, eos, att, labeldist):\n    """"""Return the decoding layer corresponding to the args.\n\n    Args:\n        args (Namespace): The program arguments.\n        odim (int): The output dimension.\n        sos (int): Number to indicate the start of sequences.\n        eos (int) Number to indicate the end of sequences.\n        att (Module):\n            Attention module defined at `espnet.nets.chainer_backend.attentions`.\n        labeldist (numpy.array): Distributed array of length od transcript.\n\n    Returns:\n        chainer.Chain: The decoder module.\n\n    """"""\n    return Decoder(\n        args.eprojs,\n        odim,\n        args.dtype,\n        args.dlayers,\n        args.dunits,\n        sos,\n        eos,\n        att,\n        args.verbose,\n        args.char_list,\n        labeldist,\n        args.lsm_weight,\n        args.sampling_probability,\n    )\n'"
espnet/nets/chainer_backend/rnn/encoders.py,0,"b'import logging\nimport six\n\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport numpy as np\n\nfrom chainer import cuda\n\nfrom espnet.nets.chainer_backend.nets_utils import _subsamplex\nfrom espnet.nets.e2e_asr_common import get_vgg2l_odim\n\n\n# TODO(watanabe) explanation of BLSTMP\nclass RNNP(chainer.Chain):\n    """"""RNN with projection layer module.\n\n    Args:\n        idim (int): Dimension of inputs.\n        elayers (int): Number of encoder layers.\n        cdim (int): Number of rnn units. (resulted in cdim * 2 if bidirectional)\n        hdim (int): Number of projection units.\n        subsample (np.ndarray): List to use sabsample the input array.\n        dropout (float): Dropout rate.\n        typ (str): The RNN type.\n\n    """"""\n\n    def __init__(self, idim, elayers, cdim, hdim, subsample, dropout, typ=""blstm""):\n        super(RNNP, self).__init__()\n        bidir = typ[0] == ""b""\n        if bidir:\n            rnn = L.NStepBiLSTM if ""lstm"" in typ else L.NStepBiGRU\n        else:\n            rnn = L.NStepLSTM if ""lstm"" in typ else L.NStepGRU\n        rnn_label = ""birnn"" if bidir else ""rnn""\n        with self.init_scope():\n            for i in six.moves.range(elayers):\n                if i == 0:\n                    inputdim = idim\n                else:\n                    inputdim = hdim\n                _cdim = 2 * cdim if bidir else cdim\n                # bottleneck layer to merge\n                setattr(\n                    self, ""{}{:d}"".format(rnn_label, i), rnn(1, inputdim, cdim, dropout)\n                )\n                setattr(self, ""bt%d"" % i, L.Linear(_cdim, hdim))\n\n        self.elayers = elayers\n        self.rnn_label = rnn_label\n        self.cdim = cdim\n        self.subsample = subsample\n        self.typ = typ\n        self.bidir = bidir\n\n    def __call__(self, xs, ilens):\n        """"""RNNP forward.\n\n        Args:\n            xs (chainer.Variable): Batch of padded charactor ids. (B, Tmax)\n            ilens (chainer.Variable): Batch of length of each input batch. (B,)\n\n        Returns:\n            xs (chainer.Variable):subsampled vector of xs.\n            chainer.Variable: Subsampled vector of ilens.\n\n        """"""\n        logging.info(self.__class__.__name__ + "" input lengths: "" + str(ilens))\n\n        for layer in six.moves.range(self.elayers):\n            if ""lstm"" in self.typ:\n                _, _, ys = self[self.rnn_label + str(layer)](None, None, xs)\n            else:\n                _, ys = self[self.rnn_label + str(layer)](None, xs)\n            # ys: utt list of frame x cdim x 2 (2: means bidirectional)\n            # TODO(watanabe) replace subsample and FC layer with CNN\n            ys, ilens = _subsamplex(ys, self.subsample[layer + 1])\n            # (sum _utt frame_utt) x dim\n            ys = self[""bt"" + str(layer)](F.vstack(ys))\n            xs = F.split_axis(ys, np.cumsum(ilens[:-1]), axis=0)\n\n        # final tanh operation\n        xs = F.split_axis(F.tanh(F.vstack(xs)), np.cumsum(ilens[:-1]), axis=0)\n\n        # 1 utterance case, it becomes an array, so need to make a utt tuple\n        if not isinstance(xs, tuple):\n            xs = [xs]\n\n        return xs, ilens  # x: utt list of frame x dim\n\n\nclass RNN(chainer.Chain):\n    """"""RNN Module.\n\n    Args:\n        idim (int): Dimension of the imput.\n        elayers (int): Number of encoder layers.\n        cdim (int): Number of rnn units.\n        hdim (int): Number of projection units.\n        dropout (float): Dropout rate.\n        typ (str): Rnn type.\n\n    """"""\n\n    def __init__(self, idim, elayers, cdim, hdim, dropout, typ=""lstm""):\n        super(RNN, self).__init__()\n        bidir = typ[0] == ""b""\n        if bidir:\n            rnn = L.NStepBiLSTM if ""lstm"" in typ else L.NStepBiGRU\n        else:\n            rnn = L.NStepLSTM if ""lstm"" in typ else L.NStepGRU\n        _cdim = 2 * cdim if bidir else cdim\n        with self.init_scope():\n            self.nbrnn = rnn(elayers, idim, cdim, dropout)\n            self.l_last = L.Linear(_cdim, hdim)\n        self.typ = typ\n        self.bidir = bidir\n\n    def __call__(self, xs, ilens):\n        """"""BRNN forward propagation.\n\n        Args:\n            xs (chainer.Variable): Batch of padded charactor ids. (B, Tmax)\n            ilens (chainer.Variable): Batch of length of each input batch. (B,)\n\n        Returns:\n            tuple(chainer.Variable): Tuple of `chainer.Variable` objects.\n            chainer.Variable: `ilens` .\n\n        """"""\n        logging.info(self.__class__.__name__ + "" input lengths: "" + str(ilens))\n        # need to move ilens to cpu\n        ilens = cuda.to_cpu(ilens)\n\n        if ""lstm"" in self.typ:\n            _, _, ys = self.nbrnn(None, None, xs)\n        else:\n            _, ys = self.nbrnn(None, xs)\n        ys = self.l_last(F.vstack(ys))  # (sum _utt frame_utt) x dim\n        xs = F.split_axis(ys, np.cumsum(ilens[:-1]), axis=0)\n\n        # final tanh operation\n        xs = F.split_axis(F.tanh(F.vstack(xs)), np.cumsum(ilens[:-1]), axis=0)\n\n        # 1 utterance case, it becomes an array, so need to make a utt tuple\n        if not isinstance(xs, tuple):\n            xs = [xs]\n\n        return xs, ilens  # x: utt list of frame x dim\n\n\n# TODO(watanabe) explanation of VGG2L, VGG2B (Block) might be better\nclass VGG2L(chainer.Chain):\n    """"""VGG motibated cnn layers.\n\n    Args:\n        in_channel (int): Number of channels.\n\n    """"""\n\n    def __init__(self, in_channel=1):\n        super(VGG2L, self).__init__()\n        with self.init_scope():\n            # CNN layer (VGG motivated)\n            self.conv1_1 = L.Convolution2D(in_channel, 64, 3, stride=1, pad=1)\n            self.conv1_2 = L.Convolution2D(64, 64, 3, stride=1, pad=1)\n            self.conv2_1 = L.Convolution2D(64, 128, 3, stride=1, pad=1)\n            self.conv2_2 = L.Convolution2D(128, 128, 3, stride=1, pad=1)\n\n        self.in_channel = in_channel\n\n    def __call__(self, xs, ilens):\n        """"""VGG2L forward propagation.\n\n        Args:\n            xs (chainer.Variable): Batch of padded charactor ids. (B, Tmax)\n            ilens (chainer.Variable): Batch of length of each features. (B,)\n\n        Returns:\n            chainer.Variable: Subsampled vector of xs.\n            chainer.Variable: Subsampled vector of ilens.\n\n        """"""\n        logging.info(self.__class__.__name__ + "" input lengths: "" + str(ilens))\n\n        # x: utt x frame x dim\n        xs = F.pad_sequence(xs)\n\n        # x: utt x 1 (input channel num) x frame x dim\n        xs = F.swapaxes(\n            xs.reshape(\n                xs.shape[0],\n                xs.shape[1],\n                self.in_channel,\n                xs.shape[2] // self.in_channel,\n            ),\n            1,\n            2,\n        )\n\n        xs = F.relu(self.conv1_1(xs))\n        xs = F.relu(self.conv1_2(xs))\n        xs = F.max_pooling_2d(xs, 2, stride=2)\n\n        xs = F.relu(self.conv2_1(xs))\n        xs = F.relu(self.conv2_2(xs))\n        xs = F.max_pooling_2d(xs, 2, stride=2)\n\n        # change ilens accordingly\n        ilens = self.xp.array(\n            self.xp.ceil(self.xp.array(ilens, dtype=np.float32) / 2), dtype=np.int32\n        )\n        ilens = self.xp.array(\n            self.xp.ceil(self.xp.array(ilens, dtype=np.float32) / 2), dtype=np.int32\n        )\n\n        # x: utt_list of frame (remove zeropaded frames) x (input channel num x dim)\n        xs = F.swapaxes(xs, 1, 2)\n        xs = xs.reshape(xs.shape[0], xs.shape[1], xs.shape[2] * xs.shape[3])\n        xs = [xs[i, : ilens[i], :] for i in range(len(ilens))]\n\n        return xs, ilens\n\n\nclass Encoder(chainer.Chain):\n    """"""Encoder network class.\n\n    Args:\n        etype (str): Type of encoder network.\n        idim (int): Number of dimensions of encoder network.\n        elayers (int): Number of layers of encoder network.\n        eunits (int): Number of lstm units of encoder network.\n        eprojs (int): Number of projection units of encoder network.\n        subsample (np.array): Subsampling number. e.g. 1_2_2_2_1\n        dropout (float): Dropout rate.\n\n    """"""\n\n    def __init__(\n        self, etype, idim, elayers, eunits, eprojs, subsample, dropout, in_channel=1\n    ):\n        super(Encoder, self).__init__()\n        typ = etype.lstrip(""vgg"").rstrip(""p"")\n        if typ not in [""lstm"", ""gru"", ""blstm"", ""bgru""]:\n            logging.error(""Error: need to specify an appropriate encoder architecture"")\n        with self.init_scope():\n            if etype.startswith(""vgg""):\n                if etype[-1] == ""p"":\n                    self.enc = chainer.Sequential(\n                        VGG2L(in_channel),\n                        RNNP(\n                            get_vgg2l_odim(idim, in_channel=in_channel),\n                            elayers,\n                            eunits,\n                            eprojs,\n                            subsample,\n                            dropout,\n                            typ=typ,\n                        ),\n                    )\n                    logging.info(""Use CNN-VGG + "" + typ.upper() + ""P for encoder"")\n                else:\n                    self.enc = chainer.Sequential(\n                        VGG2L(in_channel),\n                        RNN(\n                            get_vgg2l_odim(idim, in_channel=in_channel),\n                            elayers,\n                            eunits,\n                            eprojs,\n                            dropout,\n                            typ=typ,\n                        ),\n                    )\n                    logging.info(""Use CNN-VGG + "" + typ.upper() + "" for encoder"")\n            else:\n                if etype[-1] == ""p"":\n                    self.enc = chainer.Sequential(\n                        RNNP(idim, elayers, eunits, eprojs, subsample, dropout, typ=typ)\n                    )\n                    logging.info(\n                        typ.upper() + "" with every-layer projection for encoder""\n                    )\n                else:\n                    self.enc = chainer.Sequential(\n                        RNN(idim, elayers, eunits, eprojs, dropout, typ=typ)\n                    )\n                    logging.info(typ.upper() + "" without projection for encoder"")\n\n    def __call__(self, xs, ilens):\n        """"""Encoder forward.\n\n        Args:\n            xs (chainer.Variable): Batch of padded charactor ids. (B, Tmax)\n            ilens (chainer.variable): Batch of length of each features. (B,)\n\n        Returns:\n            chainer.Variable: Output of the encoder.\n            chainer.Variable: (Subsampled) vector of ilens.\n\n        """"""\n        xs, ilens = self.enc(xs, ilens)\n\n        return xs, ilens\n\n\ndef encoder_for(args, idim, subsample):\n    """"""Return the Encoder module.\n\n    Args:\n        idim (int): Dimension of input array.\n        subsample (numpy.array): Subsample number. egs).1_2_2_2_1\n\n    Return\n        chainer.nn.Module: Encoder module.\n\n    """"""\n    return Encoder(\n        args.etype,\n        idim,\n        args.elayers,\n        args.eunits,\n        args.eprojs,\n        subsample,\n        args.dropout_rate,\n    )\n'"
espnet/nets/chainer_backend/rnn/training.py,1,"b'# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nfrom __future__ import division\n\nimport collections\nimport logging\nimport math\nimport six\n\n# chainer related\nfrom chainer import cuda\nfrom chainer import training\nfrom chainer import Variable\n\nfrom chainer.training.updaters.multiprocess_parallel_updater import gather_grads\nfrom chainer.training.updaters.multiprocess_parallel_updater import gather_params\nfrom chainer.training.updaters.multiprocess_parallel_updater import scatter_grads\n\nimport numpy as np\n\n\n# copied from https://github.com/chainer/chainer/blob/master/chainer/optimizer.py\ndef sum_sqnorm(arr):\n    """"""Calculate the norm of the array.\n\n    Args:\n        arr (numpy.ndarray)\n\n    Returns:\n        Float: Sum of the norm calculated from the given array.\n\n    """"""\n    sq_sum = collections.defaultdict(float)\n    for x in arr:\n        with cuda.get_device_from_array(x) as dev:\n            if x is not None:\n                x = x.ravel()\n                s = x.dot(x)\n                sq_sum[int(dev)] += s\n    return sum([float(i) for i in six.itervalues(sq_sum)])\n\n\nclass CustomUpdater(training.StandardUpdater):\n    """"""Custom updater for chainer.\n\n    Args:\n        train_iter (iterator | dict[str, iterator]): Dataset iterator for the\n            training dataset. It can also be a dictionary that maps strings to\n            iterators. If this is just an iterator, then the iterator is\n            registered by the name ``\'main\'``.\n        optimizer (optimizer | dict[str, optimizer]): Optimizer to update\n            parameters. It can also be a dictionary that maps strings to\n            optimizers. If this is just an optimizer, then the optimizer is\n            registered by the name ``\'main\'``.\n        converter (espnet.asr.chainer_backend.asr.CustomConverter): Converter\n            function to build input arrays. Each batch extracted by the main\n            iterator and the ``device`` option are passed to this function.\n            :func:`chainer.dataset.concat_examples` is used by default.\n        device (int or dict): The destination device info to send variables. In the\n            case of cpu or single gpu, `device=-1 or 0`, respectively.\n            In the case of multi-gpu, `device={""main"":0, ""sub_1"": 1, ...}`.\n        accum_grad (int):The number of gradient accumulation. if set to 2, the network\n            parameters will be updated once in twice,\n            i.e. actual batchsize will be doubled.\n\n    """"""\n\n    def __init__(self, train_iter, optimizer, converter, device, accum_grad=1):\n        super(CustomUpdater, self).__init__(\n            train_iter, optimizer, converter=converter, device=device\n        )\n        self.forward_count = 0\n        self.accum_grad = accum_grad\n        self.start = True\n        # To solve #1091, it is required to set the variable inside this class.\n        self.device = device\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        """"""Main update routine for Custom Updater.""""""\n        train_iter = self.get_iterator(""main"")\n        optimizer = self.get_optimizer(""main"")\n\n        # Get batch and convert into variables\n        batch = train_iter.next()\n        x = self.converter(batch, self.device)\n        if self.start:\n            optimizer.target.cleargrads()\n            self.start = False\n\n        # Compute the loss at this time step and accumulate it\n        loss = optimizer.target(*x) / self.accum_grad\n        loss.backward()  # Backprop\n        loss.unchain_backward()  # Truncate the graph\n\n        # update parameters\n        self.forward_count += 1\n        if self.forward_count != self.accum_grad:\n            return\n        self.forward_count = 0\n        # compute the gradient norm to check if it is normal or not\n        grad_norm = np.sqrt(\n            sum_sqnorm([p.grad for p in optimizer.target.params(False)])\n        )\n        logging.info(""grad norm={}"".format(grad_norm))\n        if math.isnan(grad_norm):\n            logging.warning(""grad norm is nan. Do not update model."")\n        else:\n            optimizer.update()\n        optimizer.target.cleargrads()  # Clear the parameter gradients\n\n    def update(self):\n        self.update_core()\n        if self.forward_count == 0:\n            self.iteration += 1\n\n\nclass CustomParallelUpdater(training.updaters.MultiprocessParallelUpdater):\n    """"""Custom Parallel Updater for chainer.\n\n    Defines the main update routine.\n\n    Args:\n        train_iter (iterator | dict[str, iterator]): Dataset iterator for the\n            training dataset. It can also be a dictionary that maps strings to\n            iterators. If this is just an iterator, then the iterator is\n            registered by the name ``\'main\'``.\n        optimizer (optimizer | dict[str, optimizer]): Optimizer to update\n            parameters. It can also be a dictionary that maps strings to\n            optimizers. If this is just an optimizer, then the optimizer is\n            registered by the name ``\'main\'``.\n        converter (espnet.asr.chainer_backend.asr.CustomConverter): Converter\n            function to build input arrays. Each batch extracted by the main\n            iterator and the ``device`` option are passed to this function.\n            :func:`chainer.dataset.concat_examples` is used by default.\n        device (torch.device): Device to which the training data is sent.\n            Negative value\n            indicates the host memory (CPU).\n        accum_grad (int):The number of gradient accumulation. if set to 2,\n            the network parameters will be updated once in twice,\n            i.e. actual batchsize will be doubled.\n\n    """"""\n\n    def __init__(self, train_iters, optimizer, converter, devices, accum_grad=1):\n        super(CustomParallelUpdater, self).__init__(\n            train_iters, optimizer, converter=converter, devices=devices\n        )\n        from cupy.cuda import nccl\n\n        self.accum_grad = accum_grad\n        self.forward_count = 0\n        self.nccl = nccl\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        """"""Main Update routine of the custom parallel updater.""""""\n        self.setup_workers()\n\n        self._send_message((""update"", None))\n        with cuda.Device(self._devices[0]):\n            # For reducing memory\n\n            optimizer = self.get_optimizer(""main"")\n            batch = self.get_iterator(""main"").next()\n            x = self.converter(batch, self._devices[0])\n\n            loss = self._master(*x) / self.accum_grad\n            loss.backward()\n            loss.unchain_backward()\n\n            # NCCL: reduce grads\n            null_stream = cuda.Stream.null\n            if self.comm is not None:\n                gg = gather_grads(self._master)\n                self.comm.reduce(\n                    gg.data.ptr,\n                    gg.data.ptr,\n                    gg.size,\n                    self.nccl.NCCL_FLOAT,\n                    self.nccl.NCCL_SUM,\n                    0,\n                    null_stream.ptr,\n                )\n                scatter_grads(self._master, gg)\n                del gg\n\n            # update parameters\n            self.forward_count += 1\n            if self.forward_count != self.accum_grad:\n                return\n            self.forward_count = 0\n            # check gradient value\n            grad_norm = np.sqrt(\n                sum_sqnorm([p.grad for p in optimizer.target.params(False)])\n            )\n            logging.info(""grad norm={}"".format(grad_norm))\n\n            # update\n            if math.isnan(grad_norm):\n                logging.warning(""grad norm is nan. Do not update model."")\n            else:\n                optimizer.update()\n            self._master.cleargrads()\n\n            if self.comm is not None:\n                gp = gather_params(self._master)\n                self.comm.bcast(\n                    gp.data.ptr, gp.size, self.nccl.NCCL_FLOAT, 0, null_stream.ptr\n                )\n\n    def update(self):\n        self.update_core()\n        if self.forward_count == 0:\n            self.iteration += 1\n\n\nclass CustomConverter(object):\n    """"""Custom Converter.\n\n    Args:\n        subsampling_factor (int): The subsampling factor.\n\n    """"""\n\n    def __init__(self, subsampling_factor=1):\n        self.subsampling_factor = subsampling_factor\n\n    def __call__(self, batch, device):\n        """"""Perform sabsampling.\n\n        Args:\n            batch (list): Batch that will be sabsampled.\n            device (device): GPU device.\n\n        Returns:\n            chainer.Variable: xp.array that sabsampled from batch.\n            xp.array: xp.array of the length of the mini-batches.\n            chainer.Variable: xp.array that sabsampled from batch.\n\n        """"""\n        # set device\n        xp = cuda.cupy if device != -1 else np\n\n        # batch should be located in list\n        assert len(batch) == 1\n        xs, ys = batch[0]\n\n        # perform subsampling\n        if self.subsampling_factor > 1:\n            xs = [x[:: self.subsampling_factor, :] for x in xs]\n\n        # get batch made of lengths of input sequences\n        ilens = [x.shape[0] for x in xs]\n\n        # convert to Variable\n        xs = [Variable(xp.array(x, dtype=xp.float32)) for x in xs]\n        ilens = xp.array(ilens, dtype=xp.int32)\n        ys = [Variable(xp.array(y, dtype=xp.int32)) for y in ys]\n\n        return xs, ilens, ys\n'"
espnet/nets/chainer_backend/transformer/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/chainer_backend/transformer/attention.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Attention.""""""\n\nimport chainer\n\nimport chainer.functions as F\nimport chainer.links as L\n\nimport numpy as np\n\nMIN_VALUE = float(np.finfo(np.float32).min)\n\n\nclass MultiHeadAttention(chainer.Chain):\n    """"""Multi Head Attention Layer.\n\n    Args:\n        n_units (int): Number of input units.\n        h (int): Number of attention heads.\n        dropout (float): Dropout rate.\n        initialW: Initializer to initialize the weight.\n        initial_bias: Initializer to initialize the bias.\n\n    :param int h: the number of heads\n    :param int n_units: the number of features\n    :param float dropout_rate: dropout rate\n\n    """"""\n\n    def __init__(self, n_units, h=8, dropout=0.1, initialW=None, initial_bias=None):\n        """"""Initialize MultiHeadAttention.""""""\n        super(MultiHeadAttention, self).__init__()\n        assert n_units % h == 0\n        stvd = 1.0 / np.sqrt(n_units)\n        with self.init_scope():\n            self.linear_q = L.Linear(\n                n_units,\n                n_units,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            self.linear_k = L.Linear(\n                n_units,\n                n_units,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            self.linear_v = L.Linear(\n                n_units,\n                n_units,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            self.linear_out = L.Linear(\n                n_units,\n                n_units,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n        self.d_k = n_units // h\n        self.h = h\n        self.dropout = dropout\n        self.attn = None\n\n    def forward(self, e_var, s_var=None, mask=None, batch=1):\n        """"""Core function of the Multi-head attention layer.\n\n        Args:\n            e_var (chainer.Variable): Variable of input array.\n            s_var (chainer.Variable): Variable of source array from encoder.\n            mask (chainer.Variable): Attention mask.\n            batch (int): Batch size.\n\n        Returns:\n            chainer.Variable: Outout of multi-head attention layer.\n\n        """"""\n        xp = self.xp\n        if s_var is None:\n            # batch, head, time1/2, d_k)\n            Q = self.linear_q(e_var).reshape(batch, -1, self.h, self.d_k)\n            K = self.linear_k(e_var).reshape(batch, -1, self.h, self.d_k)\n            V = self.linear_v(e_var).reshape(batch, -1, self.h, self.d_k)\n        else:\n            Q = self.linear_q(e_var).reshape(batch, -1, self.h, self.d_k)\n            K = self.linear_k(s_var).reshape(batch, -1, self.h, self.d_k)\n            V = self.linear_v(s_var).reshape(batch, -1, self.h, self.d_k)\n        scores = F.matmul(F.swapaxes(Q, 1, 2), K.transpose(0, 2, 3, 1)) / np.sqrt(\n            self.d_k\n        )\n        if mask is not None:\n            mask = xp.stack([mask] * self.h, axis=1)\n            scores = F.where(mask, scores, xp.full(scores.shape, MIN_VALUE, ""f""))\n        self.attn = F.softmax(scores, axis=-1)\n        p_attn = F.dropout(self.attn, self.dropout)\n        x = F.matmul(p_attn, F.swapaxes(V, 1, 2))\n        x = F.swapaxes(x, 1, 2).reshape(-1, self.h * self.d_k)\n        return self.linear_out(x)\n'"
espnet/nets/chainer_backend/transformer/ctc.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s CTC.""""""\nimport logging\n\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nimport numpy as np\n\n\n# TODO(nelson): Merge chainer_backend/transformer/ctc.py in chainer_backend/ctc.py\nclass CTC(chainer.Chain):\n    """"""Chainer implementation of ctc layer.\n\n    Args:\n        odim (int): The output dimension.\n        eprojs (int | None): Dimension of input vectors from encoder.\n        dropout_rate (float): Dropout rate.\n\n    """"""\n\n    def __init__(self, odim, eprojs, dropout_rate):\n        """"""Initialize CTC.""""""\n        super(CTC, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.loss = None\n\n        with self.init_scope():\n            self.ctc_lo = L.Linear(eprojs, odim)\n\n    def __call__(self, hs, ys):\n        """"""CTC forward.\n\n        Args:\n            hs (list of chainer.Variable | N-dimension array):\n                Input variable from encoder.\n            ys (list of chainer.Variable | N-dimension array):\n                Input variable of decoder.\n\n        Returns:\n            chainer.Variable: A variable holding a scalar value of the CTC loss.\n\n        """"""\n        self.loss = None\n        ilens = [x.shape[0] for x in hs]\n        olens = [x.shape[0] for x in ys]\n\n        # zero padding for hs\n        y_hat = self.ctc_lo(\n            F.dropout(F.pad_sequence(hs), ratio=self.dropout_rate), n_batch_axes=2\n        )\n        y_hat = F.separate(y_hat, axis=1)  # ilen list of batch x hdim\n\n        # zero padding for ys\n        y_true = F.pad_sequence(ys, padding=-1)  # batch x olen\n\n        # get length info\n        input_length = chainer.Variable(self.xp.array(ilens, dtype=np.int32))\n        label_length = chainer.Variable(self.xp.array(olens, dtype=np.int32))\n        logging.info(\n            self.__class__.__name__ + "" input lengths:  "" + str(input_length.data)\n        )\n        logging.info(\n            self.__class__.__name__ + "" output lengths: "" + str(label_length.data)\n        )\n\n        # get ctc loss\n        self.loss = F.connectionist_temporal_classification(\n            y_hat, y_true, 0, input_length, label_length\n        )\n        logging.info(""ctc loss:"" + str(self.loss.data))\n\n        return self.loss\n\n    def log_softmax(self, hs):\n        """"""Log_softmax of frame activations.\n\n        Args:\n            hs (list of chainer.Variable | N-dimension array):\n                Input variable from encoder.\n\n        Returns:\n            chainer.Variable: A n-dimension float array.\n\n        """"""\n        y_hat = self.ctc_lo(F.pad_sequence(hs), n_batch_axes=2)\n        return F.log_softmax(y_hat.reshape(-1, y_hat.shape[-1])).reshape(y_hat.shape)\n\n\nclass WarpCTC(chainer.Chain):\n    """"""Chainer implementation of warp-ctc layer.\n\n    Args:\n        odim (int): The output dimension.\n        eproj (int | None): Dimension of input vector from encoder.\n        dropout_rate (float): Dropout rate.\n\n    """"""\n\n    def __init__(self, odim, eprojs, dropout_rate):\n        """"""Initialize WarpCTC.""""""\n        super(WarpCTC, self).__init__()\n        # The main difference between the ctc for transformer and\n        # the rnn is because the target (ys) is already a list of\n        # arrays located in the cpu, while in rnn routine the target is\n        # a list of variables located in cpu/gpu. If the target of rnn becomes\n        # a list of cpu arrays then this file would be no longer required.\n        from chainer_ctc.warpctc import ctc as warp_ctc\n\n        self.ctc = warp_ctc\n        self.dropout_rate = dropout_rate\n        self.loss = None\n\n        with self.init_scope():\n            self.ctc_lo = L.Linear(eprojs, odim)\n\n    def forward(self, hs, ys):\n        """"""Core function of the Warp-CTC layer.\n\n        Args:\n            hs (iterable of chainer.Variable | N-dimention array):\n                Input variable from encoder.\n            ys (iterable of N-dimension array): Input variable of decoder.\n\n        Returns:\n           chainer.Variable: A variable holding a scalar value of the CTC loss.\n\n        """"""\n        self.loss = None\n        ilens = [hs.shape[1]] * hs.shape[0]\n        olens = [x.shape[0] for x in ys]\n\n        # zero padding for hs\n        # output batch x frames x hdim > frames x batch x hdim\n        y_hat = self.ctc_lo(\n            F.dropout(hs, ratio=self.dropout_rate), n_batch_axes=2\n        ).transpose(1, 0, 2)\n\n        # get length info\n        logging.info(self.__class__.__name__ + "" input lengths:  "" + str(ilens))\n        logging.info(self.__class__.__name__ + "" output lengths: "" + str(olens))\n\n        # get ctc loss\n        self.loss = self.ctc(y_hat, ilens, ys)[0]\n        logging.info(""ctc loss:"" + str(self.loss.data))\n        return self.loss\n\n    def log_softmax(self, hs):\n        """"""Log_softmax of frame activations.\n\n        Args:\n            hs (list of chainer.Variable | N-dimension array):\n                Input variable from encoder.\n\n        Returns:\n            chainer.Variable: A n-dimension float array.\n\n        """"""\n        y_hat = self.ctc_lo(F.pad_sequence(hs), n_batch_axes=2)\n        return F.log_softmax(y_hat.reshape(-1, y_hat.shape[-1])).reshape(y_hat.shape)\n\n    def argmax(self, hs_pad):\n        """"""Argmax of frame activations.\n\n        :param chainer variable hs_pad: 3d tensor (B, Tmax, eprojs)\n        :return: argmax applied 2d tensor (B, Tmax)\n        :rtype: chainer.Variable.\n        """"""\n        return F.argmax(self.ctc_lo(F.pad_sequence(hs_pad), n_batch_axes=2), axis=-1)\n'"
espnet/nets/chainer_backend/transformer/decoder.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Decoder.""""""\n\nimport chainer\n\nimport chainer.functions as F\nimport chainer.links as L\n\nfrom espnet.nets.chainer_backend.transformer.decoder_layer import DecoderLayer\nfrom espnet.nets.chainer_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.chainer_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.chainer_backend.transformer.mask import make_history_mask\n\nimport numpy as np\n\n\nclass Decoder(chainer.Chain):\n    """"""Decoder layer.\n\n    Args:\n        odim (int): The output dimension.\n        n_layers (int): Number of ecoder layers.\n        n_units (int): Number of attention units.\n        d_units (int): Dimension of input vector of decoder.\n        h (int): Number of attention heads.\n        dropout (float): Dropout rate.\n        initialW (Initializer): Initializer to initialize the weight.\n        initial_bias (Initializer): Initializer to initialize teh bias.\n\n    """"""\n\n    def __init__(self, odim, args, initialW=None, initial_bias=None):\n        """"""Initialize Decoder.""""""\n        super(Decoder, self).__init__()\n        self.sos = odim - 1\n        self.eos = odim - 1\n        initialW = chainer.initializers.Uniform if initialW is None else initialW\n        initial_bias = (\n            chainer.initializers.Uniform if initial_bias is None else initial_bias\n        )\n        with self.init_scope():\n            self.output_norm = LayerNorm(args.adim)\n            self.pe = PositionalEncoding(args.adim, args.dropout_rate)\n            stvd = 1.0 / np.sqrt(args.adim)\n            self.output_layer = L.Linear(\n                args.adim,\n                odim,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            self.embed = L.EmbedID(\n                odim,\n                args.adim,\n                ignore_label=-1,\n                initialW=chainer.initializers.Normal(scale=1.0),\n            )\n        for i in range(args.dlayers):\n            name = ""decoders."" + str(i)\n            layer = DecoderLayer(\n                args.adim,\n                d_units=args.dunits,\n                h=args.aheads,\n                dropout=args.dropout_rate,\n                initialW=initialW,\n                initial_bias=initial_bias,\n            )\n            self.add_link(name, layer)\n        self.n_layers = args.dlayers\n\n    def make_attention_mask(self, source_block, target_block):\n        """"""Prepare the attention mask.\n\n        Args:\n            source_block (ndarray): Source block with dimensions: (B x S).\n            target_block (ndarray): Target block with dimensions: (B x T).\n        Returns:\n            ndarray: Mask with dimensions (B, S, T).\n\n        """"""\n        mask = (target_block[:, None, :] >= 0) * (source_block[:, :, None] >= 0)\n        # (batch, source_length, target_length)\n        return mask\n\n    def forward(self, ys_pad, source, x_mask):\n        """"""Forward decoder.\n\n        :param xp.array e: input token ids, int64 (batch, maxlen_out)\n        :param xp.array yy_mask: input token mask, uint8  (batch, maxlen_out)\n        :param xp.array source: encoded memory, float32  (batch, maxlen_in, feat)\n        :param xp.array xy_mask: encoded memory mask, uint8  (batch, maxlen_in)\n        :return e: decoded token score before softmax (batch, maxlen_out, token)\n        :rtype: chainer.Variable\n        """"""\n        xp = self.xp\n        sos = np.array([self.sos], np.int32)\n        ys = [np.concatenate([sos, y], axis=0) for y in ys_pad]\n        e = F.pad_sequence(ys, padding=self.eos).data\n        e = xp.array(e)\n        # mask preparation\n        xy_mask = self.make_attention_mask(e, xp.array(x_mask))\n        yy_mask = self.make_attention_mask(e, e)\n        yy_mask *= make_history_mask(xp, e)\n\n        e = self.pe(self.embed(e))\n        batch, length, dims = e.shape\n        e = e.reshape(-1, dims)\n        source = source.reshape(-1, dims)\n        for i in range(self.n_layers):\n            e = self[""decoders."" + str(i)](e, source, xy_mask, yy_mask, batch)\n        return self.output_layer(self.output_norm(e)).reshape(batch, length, -1)\n\n    def recognize(self, e, yy_mask, source):\n        """"""Process recognition function.""""""\n        e = self.forward(e, source, yy_mask)\n        return F.log_softmax(e, axis=-1)\n'"
espnet/nets/chainer_backend/transformer/decoder_layer.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Decoder Block.""""""\n\nimport chainer\n\nimport chainer.functions as F\n\nfrom espnet.nets.chainer_backend.transformer.attention import MultiHeadAttention\nfrom espnet.nets.chainer_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.chainer_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\n\n\nclass DecoderLayer(chainer.Chain):\n    """"""Single decoder layer module.\n\n    Args:\n        n_units (int): Number of input/output dimension of a FeedForward layer.\n        d_units (int): Number of units of hidden layer in a FeedForward layer.\n        h (int): Number of attention heads.\n        dropout (float): Dropout rate\n\n    """"""\n\n    def __init__(\n        self, n_units, d_units=0, h=8, dropout=0.1, initialW=None, initial_bias=None\n    ):\n        """"""Initialize DecoderLayer.""""""\n        super(DecoderLayer, self).__init__()\n        with self.init_scope():\n            self.self_attn = MultiHeadAttention(\n                n_units,\n                h,\n                dropout=dropout,\n                initialW=initialW,\n                initial_bias=initial_bias,\n            )\n            self.src_attn = MultiHeadAttention(\n                n_units,\n                h,\n                dropout=dropout,\n                initialW=initialW,\n                initial_bias=initial_bias,\n            )\n            self.feed_forward = PositionwiseFeedForward(\n                n_units,\n                d_units=d_units,\n                dropout=dropout,\n                initialW=initialW,\n                initial_bias=initial_bias,\n            )\n            self.norm1 = LayerNorm(n_units)\n            self.norm2 = LayerNorm(n_units)\n            self.norm3 = LayerNorm(n_units)\n        self.dropout = dropout\n\n    def forward(self, e, s, xy_mask, yy_mask, batch):\n        """"""Compute Encoder layer.\n\n        Args:\n            e (chainer.Variable): Batch of padded features. (B, Lmax)\n            s (chainer.Variable): Batch of padded character. (B, Tmax)\n\n        Returns:\n            chainer.Variable: Computed variable of decoder.\n\n        """"""\n        n_e = self.norm1(e)\n        n_e = self.self_attn(n_e, mask=yy_mask, batch=batch)\n        e = e + F.dropout(n_e, self.dropout)\n\n        n_e = self.norm2(e)\n        n_e = self.src_attn(n_e, s_var=s, mask=xy_mask, batch=batch)\n        e = e + F.dropout(n_e, self.dropout)\n\n        n_e = self.norm3(e)\n        n_e = self.feed_forward(n_e)\n        e = e + F.dropout(n_e, self.dropout)\n        return e\n'"
espnet/nets/chainer_backend/transformer/embedding.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Positional Encoding.""""""\n\nimport chainer\nimport chainer.functions as F\n\nimport numpy as np\n\n\nclass PositionalEncoding(chainer.Chain):\n    """"""Positional encoding module.\n\n    :param int n_units: embedding dim\n    :param float dropout: dropout rate\n    :param int length: maximum input length\n\n    """"""\n\n    def __init__(self, n_units, dropout=0.1, length=5000):\n        """"""Initialize Positional Encoding.""""""\n        # Implementation described in the paper\n        super(PositionalEncoding, self).__init__()\n        self.dropout = dropout\n        posi_block = np.arange(0, length, dtype=np.float32)[:, None]\n        unit_block = np.exp(\n            np.arange(0, n_units, 2, dtype=np.float32) * -(np.log(10000.0) / n_units)\n        )\n        self.pe = np.zeros((length, n_units), dtype=np.float32)\n        self.pe[:, ::2] = np.sin(posi_block * unit_block)\n        self.pe[:, 1::2] = np.cos(posi_block * unit_block)\n        self.scale = np.sqrt(n_units)\n\n    def forward(self, e):\n        """"""Forward Positional Encoding.""""""\n        length = e.shape[1]\n        e = e * self.scale + self.xp.array(self.pe[:length])\n        return F.dropout(e, self.dropout)\n'"
espnet/nets/chainer_backend/transformer/encoder.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Encoder.""""""\n\nimport chainer\n\nfrom chainer import links as L\n\nfrom espnet.nets.chainer_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.chainer_backend.transformer.encoder_layer import EncoderLayer\nfrom espnet.nets.chainer_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.chainer_backend.transformer.mask import make_history_mask\nfrom espnet.nets.chainer_backend.transformer.subsampling import Conv2dSubsampling\nfrom espnet.nets.chainer_backend.transformer.subsampling import LinearSampling\n\nimport logging\nimport numpy as np\n\n\nclass Encoder(chainer.Chain):\n    """"""Encoder.\n\n    Args:\n        input_type(str):\n            Sampling type. `input_type` must be `conv2d` or \'linear\' currently.\n        idim (int): Dimension of inputs.\n        n_layers (int): Number of encoder layers.\n        n_units (int): Number of input/output dimension of a FeedForward layer.\n        d_units (int): Number of units of hidden layer in a FeedForward layer.\n        h (int): Number of attention heads.\n        dropout (float): Dropout rate\n\n    """"""\n\n    def __init__(\n        self,\n        idim,\n        attention_dim=256,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        attention_dropout_rate=0.0,\n        input_layer=""conv2d"",\n        pos_enc_class=PositionalEncoding,\n        initialW=None,\n        initial_bias=None,\n    ):\n        """"""Initialize Encoder.\n\n        Args:\n            idim (int): Input dimension.\n            args (Namespace): Training config.\n            initialW (int, optional):  Initializer to initialize the weight.\n            initial_bias (bool, optional): Initializer to initialize the bias.\n\n        """"""\n        super(Encoder, self).__init__()\n        initialW = chainer.initializers.Uniform if initialW is None else initialW\n        initial_bias = (\n            chainer.initializers.Uniform if initial_bias is None else initial_bias\n        )\n        self.do_history_mask = False\n        with self.init_scope():\n            channels = 64  # Based in paper\n            if input_layer == ""conv2d"":\n                idim = int(np.ceil(np.ceil(idim / 2) / 2)) * channels\n                self.input_layer = Conv2dSubsampling(\n                    channels,\n                    idim,\n                    attention_dim,\n                    dropout=dropout_rate,\n                    initialW=initialW,\n                    initial_bias=initial_bias,\n                )\n            elif input_layer == ""linear"":\n                self.input_layer = LinearSampling(\n                    idim, attention_dim, initialW=initialW, initial_bias=initial_bias\n                )\n            elif input_layer == ""embed"":\n                self.input_layer = chainer.Sequential(\n                    L.EmbedID(idim, attention_dim, ignore_label=-1),\n                    pos_enc_class(attention_dim, positional_dropout_rate),\n                )\n                self.do_history_mask = True\n            else:\n                raise ValueError(""unknown input_layer: "" + input_layer)\n            self.norm = LayerNorm(attention_dim)\n        for i in range(num_blocks):\n            name = ""encoders."" + str(i)\n            layer = EncoderLayer(\n                attention_dim,\n                d_units=linear_units,\n                h=attention_heads,\n                dropout=attention_dropout_rate,\n                initialW=initialW,\n                initial_bias=initial_bias,\n            )\n            self.add_link(name, layer)\n        self.n_layers = num_blocks\n\n    def forward(self, e, ilens):\n        """"""Compute Encoder layer.\n\n        Args:\n            e (chainer.Variable): Batch of padded charactor. (B, Tmax)\n            ilens (chainer.Variable): Batch of length of each input batch. (B,)\n\n        Returns:\n            chainer.Variable: Computed variable of encoder.\n            numpy.array: Mask.\n            chainer.Variable: Batch of lengths of each encoder outputs.\n\n        """"""\n        if isinstance(self.input_layer, Conv2dSubsampling):\n            e, ilens = self.input_layer(e, ilens)\n        else:\n            e = self.input_layer(e)\n        batch, length, dims = e.shape\n        x_mask = np.ones([batch, length])\n        for j in range(batch):\n            x_mask[j, ilens[j] :] = -1\n        xx_mask = (x_mask[:, None, :] >= 0) * (x_mask[:, :, None] >= 0)\n        xx_mask = self.xp.array(xx_mask)\n        if self.do_history_mask:\n            history_mask = make_history_mask(self.xp, x_mask)\n            xx_mask *= history_mask\n        logging.debug(""encoders size: "" + str(e.shape))\n        e = e.reshape(-1, dims)\n        for i in range(self.n_layers):\n            e = self[""encoders."" + str(i)](e, xx_mask, batch)\n        return self.norm(e).reshape(batch, length, -1), x_mask, ilens\n'"
espnet/nets/chainer_backend/transformer/encoder_layer.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Encoder Block.""""""\n\nimport chainer\n\nimport chainer.functions as F\n\nfrom espnet.nets.chainer_backend.transformer.attention import MultiHeadAttention\nfrom espnet.nets.chainer_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.chainer_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\n\n\nclass EncoderLayer(chainer.Chain):\n    """"""Single encoder layer module.\n\n    Args:\n        n_units (int): Number of input/output dimension of a FeedForward layer.\n        d_units (int): Number of units of hidden layer in a FeedForward layer.\n        h (int): Number of attention heads.\n        dropout (float): Dropout rate\n\n    """"""\n\n    def __init__(\n        self, n_units, d_units=0, h=8, dropout=0.1, initialW=None, initial_bias=None\n    ):\n        """"""Initialize EncoderLayer.""""""\n        super(EncoderLayer, self).__init__()\n        with self.init_scope():\n            self.self_attn = MultiHeadAttention(\n                n_units,\n                h,\n                dropout=dropout,\n                initialW=initialW,\n                initial_bias=initial_bias,\n            )\n            self.feed_forward = PositionwiseFeedForward(\n                n_units,\n                d_units=d_units,\n                dropout=dropout,\n                initialW=initialW,\n                initial_bias=initial_bias,\n            )\n            self.norm1 = LayerNorm(n_units)\n            self.norm2 = LayerNorm(n_units)\n        self.dropout = dropout\n        self.n_units = n_units\n\n    def forward(self, e, xx_mask, batch):\n        """"""Forward Positional Encoding.""""""\n        n_e = self.norm1(e)\n        n_e = self.self_attn(n_e, mask=xx_mask, batch=batch)\n        e = e + F.dropout(n_e, self.dropout)\n\n        n_e = self.norm2(e)\n        n_e = self.feed_forward(n_e)\n        e = e + F.dropout(n_e, self.dropout)\n        return e\n'"
espnet/nets/chainer_backend/transformer/label_smoothing_loss.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Label Smootion loss.""""""\n\nimport logging\n\nimport chainer\n\nimport chainer.functions as F\n\n\nclass LabelSmoothingLoss(chainer.Chain):\n    """"""Label Smoothing Loss.\n\n    Args:\n        smoothing (float): smoothing rate (0.0 means the conventional CE).\n        n_target_vocab (int): number of classes.\n        normalize_length (bool): normalize loss by sequence length if True.\n\n    """"""\n\n    def __init__(self, smoothing, n_target_vocab, normalize_length=False, ignore_id=-1):\n        """"""Initialize Loss.""""""\n        super(LabelSmoothingLoss, self).__init__()\n        self.use_label_smoothing = False\n        if smoothing > 0.0:\n            logging.info(""Use label smoothing"")\n            self.smoothing = smoothing\n            self.confidence = 1.0 - smoothing\n            self.use_label_smoothing = True\n            self.n_target_vocab = n_target_vocab\n        self.normalize_length = normalize_length\n        self.ignore_id = ignore_id\n        self.acc = None\n\n    def forward(self, ys_block, ys_pad):\n        """"""Forward Loss.\n\n        Args:\n            ys_block (chainer.Variable): Predicted labels.\n            ys_pad (chainer.Variable): Target (true) labels.\n\n        Returns:\n            float: Training loss.\n\n        """"""\n        # Output (all together at once for efficiency)\n        batch, length, dims = ys_block.shape\n        concat_logit_block = ys_block.reshape(-1, dims)\n\n        # Target reshape\n        concat_t_block = ys_pad.reshape((batch * length))\n        ignore_mask = concat_t_block >= 0\n        n_token = ignore_mask.sum()\n        normalizer = n_token if self.normalize_length else batch\n\n        if not self.use_label_smoothing:\n            loss = F.softmax_cross_entropy(concat_logit_block, concat_t_block)\n            loss = loss * n_token / normalizer\n        else:\n            log_prob = F.log_softmax(concat_logit_block)\n            broad_ignore_mask = self.xp.broadcast_to(\n                ignore_mask[:, None], concat_logit_block.shape\n            )\n            pre_loss = (\n                ignore_mask * log_prob[self.xp.arange(batch * length), concat_t_block]\n            )\n            loss = -F.sum(pre_loss) / normalizer\n            label_smoothing = broad_ignore_mask * -1.0 / self.n_target_vocab * log_prob\n            label_smoothing = F.sum(label_smoothing) / normalizer\n            loss = self.confidence * loss + self.smoothing * label_smoothing\n        return loss\n'"
espnet/nets/chainer_backend/transformer/layer_norm.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Label Smootion loss.""""""\n\nimport chainer.links as L\n\n\nclass LayerNorm(L.LayerNormalization):\n    """"""Redirect to L.LayerNormalization.""""""\n\n    def __init__(self, dims, eps=1e-12):\n        """"""Initialize LayerNorm.""""""\n        super(LayerNorm, self).__init__(size=dims, eps=eps)\n\n    def __call__(self, e):\n        """"""Forward LayerNorm.""""""\n        return super(LayerNorm, self).__call__(e)\n'"
espnet/nets/chainer_backend/transformer/mask.py,0,"b'""""""Create mask for subsequent steps.""""""\n\n\ndef make_history_mask(xp, block):\n    """"""Prepare the history mask.\n\n    Args:\n        block (ndarray): Block with dimensions: (B x S).\n    Returns:\n        ndarray, np.ndarray: History mask with dimensions (B, S, S).\n\n    """"""\n    batch, length = block.shape\n    arange = xp.arange(length)\n    history_mask = (arange[None] <= arange[:, None])[\n        None,\n    ]\n    history_mask = xp.broadcast_to(history_mask, (batch, length, length))\n    return history_mask\n'"
espnet/nets/chainer_backend/transformer/plot.py,3,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Attention Plot.""""""\n\nfrom espnet.asr import asr_utils\nimport logging\nimport matplotlib.pyplot as plt\n\n\ndef savefig(plot, filename):\n    """"""Save a figure.""""""\n    plot.savefig(filename)\n    plt.clf()\n\n\ndef _plot_and_save_attention(att_w, filename):\n    """"""Plot and save an attention.""""""\n    # dynamically import matplotlib due to not found error\n    from matplotlib.ticker import MaxNLocator\n    import os\n\n    d = os.path.dirname(filename)\n    if not os.path.exists(d):\n        os.makedirs(d)\n    w, h = plt.figaspect(1.0 / len(att_w))\n    fig = plt.Figure(figsize=(w * 2, h * 2))\n    axes = fig.subplots(1, len(att_w))\n    if len(att_w) == 1:\n        axes = [axes]\n    for ax, aw in zip(axes, att_w):\n        # plt.subplot(1, len(att_w), h)\n        ax.imshow(aw, aspect=""auto"")\n        ax.set_xlabel(""Input"")\n        ax.set_ylabel(""Output"")\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n    fig.tight_layout()\n    return fig\n\n\ndef plot_multi_head_attention(data, attn_dict, outdir, suffix=""png"", savefn=savefig):\n    """"""Plot multi head attentions.\n\n    :param dict data: utts info from json file\n    :param dict[str, torch.Tensor] attn_dict: multi head attention dict.\n        values should be torch.Tensor (head, input_length, output_length)\n    :param str outdir: dir to save fig\n    :param str suffix: filename suffix including image type (e.g., png)\n    :param savefn: function to save\n\n    """"""\n    for name, att_ws in attn_dict.items():\n        for idx, att_w in enumerate(att_ws):\n            filename = ""%s/%s.%s.%s"" % (outdir, data[idx][0], name, suffix)\n            dec_len = int(data[idx][1][""output""][0][""shape""][0])\n            enc_len = int(data[idx][1][""input""][0][""shape""][0])\n            if ""encoder"" in name:\n                att_w = att_w[:, :enc_len, :enc_len]\n            elif ""decoder"" in name:\n                if ""self"" in name:\n                    att_w = att_w[:, :dec_len, :dec_len]\n                else:\n                    att_w = att_w[:, :dec_len, :enc_len]\n            else:\n                logging.warning(""unknown name for shaping attention"")\n            fig = _plot_and_save_attention(att_w, filename)\n            savefn(fig, filename)\n\n\nclass PlotAttentionReport(asr_utils.PlotAttentionReport):\n    """"""Plot an attention reporter.\n\n    Args:\n        att_vis_fn (espnet.nets.*_backend.e2e_asr.E2E.calculate_all_attentions):\n        Function of attention visualization.\n        data (list[tuple(str, dict[str, list[Any]])]): List json utt key items.\n        outdir (str): Directory to save figures.\n        converter (espnet.asr.*_backend.asr.CustomConverter): Function to convert data.\n        device (int | torch.device): Device.\n        reverse (bool): If True, input and output length are reversed.\n        ikey (str): Key to access input (for ASR ikey=""input"", for MT ikey=""output"".)\n        iaxis (int): Dimension to access input (for ASR iaxis=0, for MT iaxis=1.)\n        okey (str): Key to access output (for ASR okey=""input"", MT okay=""output"".)\n\n    """"""\n\n    def __call__(self, trainer):\n        """"""Plot and save an image file of att_ws matrix.""""""\n        attn_dict = self.get_attention_weights()\n        suffix = ""ep.{.updater.epoch}.png"".format(trainer)\n        plot_multi_head_attention(self.data, attn_dict, self.outdir, suffix, savefig)\n\n    def get_attention_weights(self):\n        """"""Return attention weights.\n\n        Returns:\n            numpy.ndarray: attention weights.float. Its shape would be\n                differ from backend.\n                * pytorch-> 1) multi-head case => (B, H, Lmax, Tmax), 2)\n                  other case => (B, Lmax, Tmax).\n                * chainer-> (B, Lmax, Tmax)\n\n        """"""\n        batch = self.converter([self.transform(self.data)], self.device)\n        return self.att_vis_fn(*batch)\n\n    def log_attentions(self, logger, step):\n        """"""Add image files of att_ws matrix to the tensorboard.""""""\n\n        def log_fig(plot, filename):\n            from os.path import basename\n\n            logger.add_figure(basename(filename), plot, step)\n            plt.clf()\n\n        attn_dict = self.get_attention_weights()\n        plot_multi_head_attention(self.data, attn_dict, self.outdir, """", log_fig)\n'"
espnet/nets/chainer_backend/transformer/positionwise_feed_forward.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Positionwise Feedforward.""""""\n\nimport chainer\n\nimport chainer.functions as F\nimport chainer.links as L\n\nimport numpy as np\n\n\nclass PositionwiseFeedForward(chainer.Chain):\n    """"""Positionwise feed forward.\n\n    Args:\n        :param int idim: input dimenstion\n        :param int hidden_units: number of hidden units\n        :param float dropout_rate: dropout rate\n\n    """"""\n\n    def __init__(\n        self, n_units, d_units=0, dropout=0.1, initialW=None, initial_bias=None\n    ):\n        """"""Initialize PositionwiseFeedForward.\n\n        Args:\n            n_units (int): Input dimension.\n            d_units (int, optional): Output dimension of hidden layer.\n            dropout (float, optional): Dropout ratio.\n            initialW (int, optional):  Initializer to initialize the weight.\n            initial_bias (bool, optional): Initializer to initialize the bias.\n\n        """"""\n        super(PositionwiseFeedForward, self).__init__()\n        n_inner_units = d_units if d_units > 0 else n_units * 4\n        with self.init_scope():\n            stvd = 1.0 / np.sqrt(n_units)\n            self.w_1 = L.Linear(\n                n_units,\n                n_inner_units,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            stvd = 1.0 / np.sqrt(n_inner_units)\n            self.w_2 = L.Linear(\n                n_inner_units,\n                n_units,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            self.act = F.relu\n        self.dropout = dropout\n\n    def __call__(self, e):\n        """"""Initialize PositionwiseFeedForward.\n\n        Args:\n            e (chainer.Variable): Input variable.\n\n        Return:\n            chainer.Variable: Output variable.\n\n        """"""\n        e = F.dropout(self.act(self.w_1(e)), self.dropout)\n        return self.w_2(e)\n'"
espnet/nets/chainer_backend/transformer/subsampling.py,0,"b'# encoding: utf-8\n""""""Class Declaration of Transformer\'s Input layers.""""""\n\nimport chainer\n\nimport chainer.functions as F\nimport chainer.links as L\n\nfrom espnet.nets.chainer_backend.transformer.embedding import PositionalEncoding\n\nimport logging\nimport numpy as np\n\n\nclass Conv2dSubsampling(chainer.Chain):\n    """"""Convolutional 2D subsampling (to 1/4 length).\n\n    :param int idim: input dim\n    :param int odim: output dim\n    :param flaot dropout_rate: dropout rate\n\n    """"""\n\n    def __init__(\n        self, channels, idim, dims, dropout=0.1, initialW=None, initial_bias=None\n    ):\n        """"""Initialize Conv2dSubsampling.""""""\n        super(Conv2dSubsampling, self).__init__()\n        self.dropout = dropout\n        with self.init_scope():\n            # Standard deviation for Conv2D with 1 channel and kernel 3 x 3.\n            n = 1 * 3 * 3\n            stvd = 1.0 / np.sqrt(n)\n            self.conv1 = L.Convolution2D(\n                1,\n                channels,\n                3,\n                stride=2,\n                pad=1,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            n = channels * 3 * 3\n            stvd = 1.0 / np.sqrt(n)\n            self.conv2 = L.Convolution2D(\n                channels,\n                channels,\n                3,\n                stride=2,\n                pad=1,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            stvd = 1.0 / np.sqrt(dims)\n            self.out = L.Linear(\n                idim,\n                dims,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            self.pe = PositionalEncoding(dims, dropout)\n\n    def forward(self, xs, ilens):\n        """"""Subsample x.\n\n        :param chainer.Variable x: input tensor\n        :return: subsampled x and mask\n\n        """"""\n        xs = self.xp.array(xs[:, None])\n        xs = F.relu(self.conv1(xs))\n        xs = F.relu(self.conv2(xs))\n        batch, _, length, _ = xs.shape\n        xs = self.out(F.swapaxes(xs, 1, 2).reshape(batch * length, -1))\n        xs = self.pe(xs.reshape(batch, length, -1))\n        # change ilens accordingly\n        ilens = np.ceil(np.array(ilens, dtype=np.float32) / 2).astype(np.int)\n        ilens = np.ceil(np.array(ilens, dtype=np.float32) / 2).astype(np.int)\n        return xs, ilens\n\n\nclass LinearSampling(chainer.Chain):\n    """"""Linear 1D subsampling.\n\n    :param int idim: input dim\n    :param int odim: output dim\n    :param flaot dropout_rate: dropout rate\n\n    """"""\n\n    def __init__(self, idim, dims, dropout=0.1, initialW=None, initial_bias=None):\n        """"""Initialize LinearSampling.""""""\n        super(LinearSampling, self).__init__()\n        stvd = 1.0 / np.sqrt(dims)\n        self.dropout = dropout\n        with self.init_scope():\n            self.linear = L.Linear(\n                idim,\n                dims,\n                initialW=initialW(scale=stvd),\n                initial_bias=initial_bias(scale=stvd),\n            )\n            self.pe = PositionalEncoding(dims, dropout)\n\n    def forward(self, xs, ilens):\n        """"""Subsample x.\n\n        :param chainer.Variable x: input tensor\n        :return: subsampled x and mask\n\n        """"""\n        logging.info(xs.shape)\n        xs = self.linear(xs, n_batch_axes=2)\n        logging.info(xs.shape)\n        xs = self.pe(xs)\n        return xs, ilens\n'"
espnet/nets/chainer_backend/transformer/training.py,1,"b'# Copyright 2017 Johns Hopkins University (Shinji Watanabe)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n""""""Class Declaration of Transformer\'s Training Subprocess.""""""\nimport collections\nimport logging\nimport math\nimport six\n\nfrom chainer import cuda\nfrom chainer import functions as F\nfrom chainer import training\nfrom chainer.training import extension\nfrom chainer.training.updaters.multiprocess_parallel_updater import gather_grads\nfrom chainer.training.updaters.multiprocess_parallel_updater import gather_params\nfrom chainer.training.updaters.multiprocess_parallel_updater import scatter_grads\nimport numpy as np\n\n\n# copied from https://github.com/chainer/chainer/blob/master/chainer/optimizer.py\ndef sum_sqnorm(arr):\n    """"""Calculate the norm of the array.\n\n    Args:\n        arr (numpy.ndarray)\n\n    Returns:\n        Float: Sum of the norm calculated from the given array.\n\n    """"""\n    sq_sum = collections.defaultdict(float)\n    for x in arr:\n        with cuda.get_device_from_array(x) as dev:\n            if x is not None:\n                x = x.ravel()\n                s = x.dot(x)\n                sq_sum[int(dev)] += s\n    return sum([float(i) for i in six.itervalues(sq_sum)])\n\n\nclass CustomUpdater(training.StandardUpdater):\n    """"""Custom updater for chainer.\n\n    Args:\n        train_iter (iterator | dict[str, iterator]): Dataset iterator for the\n            training dataset. It can also be a dictionary that maps strings to\n            iterators. If this is just an iterator, then the iterator is\n            registered by the name ``\'main\'``.\n        optimizer (optimizer | dict[str, optimizer]): Optimizer to update\n            parameters. It can also be a dictionary that maps strings to\n            optimizers. If this is just an optimizer, then the optimizer is\n            registered by the name ``\'main\'``.\n        converter (espnet.asr.chainer_backend.asr.CustomConverter): Converter\n            function to build input arrays. Each batch extracted by the main\n            iterator and the ``device`` option are passed to this function.\n            :func:`chainer.dataset.concat_examples` is used by default.\n        device (int or dict): The destination device info to send variables. In the\n            case of cpu or single gpu, `device=-1 or 0`, respectively.\n            In the case of multi-gpu, `device={""main"":0, ""sub_1"": 1, ...}`.\n        accum_grad (int):The number of gradient accumulation. if set to 2, the network\n            parameters will be updated once in twice,\n            i.e. actual batchsize will be doubled.\n\n    """"""\n\n    def __init__(self, train_iter, optimizer, converter, device, accum_grad=1):\n        """"""Initialize Custom Updater.""""""\n        super(CustomUpdater, self).__init__(\n            train_iter, optimizer, converter=converter, device=device\n        )\n        self.accum_grad = accum_grad\n        self.forward_count = 0\n        self.start = True\n        self.device = device\n        logging.debug(""using custom converter for transformer"")\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        """"""Process main update routine for Custom Updater.""""""\n        train_iter = self.get_iterator(""main"")\n        optimizer = self.get_optimizer(""main"")\n\n        # Get batch and convert into variables\n        batch = train_iter.next()\n        x = self.converter(batch, self.device)\n        if self.start:\n            optimizer.target.cleargrads()\n            self.start = False\n\n        # Compute the loss at this time step and accumulate it\n        loss = optimizer.target(*x) / self.accum_grad\n        loss.backward()  # Backprop\n\n        self.forward_count += 1\n        if self.forward_count != self.accum_grad:\n            return\n        self.forward_count = 0\n        # compute the gradient norm to check if it is normal or not\n        grad_norm = np.sqrt(\n            sum_sqnorm([p.grad for p in optimizer.target.params(False)])\n        )\n        logging.info(""grad norm={}"".format(grad_norm))\n        if math.isnan(grad_norm):\n            logging.warning(""grad norm is nan. Do not update model."")\n        else:\n            optimizer.update()\n        optimizer.target.cleargrads()  # Clear the parameter gradients\n\n    def update(self):\n        """"""Update step for Custom Updater.""""""\n        self.update_core()\n        if self.forward_count == 0:\n            self.iteration += 1\n\n\nclass CustomParallelUpdater(training.updaters.MultiprocessParallelUpdater):\n    """"""Custom Parallel Updater for chainer.\n\n    Defines the main update routine.\n\n    Args:\n        train_iter (iterator | dict[str, iterator]): Dataset iterator for the\n            training dataset. It can also be a dictionary that maps strings to\n            iterators. If this is just an iterator, then the iterator is\n            registered by the name ``\'main\'``.\n        optimizer (optimizer | dict[str, optimizer]): Optimizer to update\n            parameters. It can also be a dictionary that maps strings to\n            optimizers. If this is just an optimizer, then the optimizer is\n            registered by the name ``\'main\'``.\n        converter (espnet.asr.chainer_backend.asr.CustomConverter): Converter\n            function to build input arrays. Each batch extracted by the main\n            iterator and the ``device`` option are passed to this function.\n            :func:`chainer.dataset.concat_examples` is used by default.\n        device (torch.device): Device to which the training data is sent. Negative value\n            indicates the host memory (CPU).\n        accum_grad (int):The number of gradient accumulation. if set to 2, the network\n            parameters will be updated once in twice,\n            i.e. actual batchsize will be doubled.\n\n    """"""\n\n    def __init__(self, train_iters, optimizer, converter, devices, accum_grad=1):\n        """"""Initialize custom parallel updater.""""""\n        from cupy.cuda import nccl\n\n        super(CustomParallelUpdater, self).__init__(\n            train_iters, optimizer, converter=converter, devices=devices\n        )\n        self.accum_grad = accum_grad\n        self.forward_count = 0\n        self.nccl = nccl\n        logging.debug(""using custom parallel updater for transformer"")\n\n    # The core part of the update routine can be customized by overriding.\n    def update_core(self):\n        """"""Process main update routine for Custom Parallel Updater.""""""\n        self.setup_workers()\n\n        self._send_message((""update"", None))\n        with cuda.Device(self._devices[0]):\n            # For reducing memory\n            optimizer = self.get_optimizer(""main"")\n            batch = self.get_iterator(""main"").next()\n            x = self.converter(batch, self._devices[0])\n\n            loss = self._master(*x) / self.accum_grad\n            loss.backward()\n\n            # NCCL: reduce grads\n            null_stream = cuda.Stream.null\n            if self.comm is not None:\n                gg = gather_grads(self._master)\n                self.comm.reduce(\n                    gg.data.ptr,\n                    gg.data.ptr,\n                    gg.size,\n                    self.nccl.NCCL_FLOAT,\n                    self.nccl.NCCL_SUM,\n                    0,\n                    null_stream.ptr,\n                )\n                scatter_grads(self._master, gg)\n                del gg\n\n            # update parameters\n            self.forward_count += 1\n            if self.forward_count != self.accum_grad:\n                return\n            self.forward_count = 0\n            # check gradient value\n            grad_norm = np.sqrt(\n                sum_sqnorm([p.grad for p in optimizer.target.params(False)])\n            )\n            logging.info(""grad norm={}"".format(grad_norm))\n\n            # update\n            if math.isnan(grad_norm):\n                logging.warning(""grad norm is nan. Do not update model."")\n            else:\n                optimizer.update()\n            self._master.cleargrads()\n\n            if self.comm is not None:\n                gp = gather_params(self._master)\n                self.comm.bcast(\n                    gp.data.ptr, gp.size, self.nccl.NCCL_FLOAT, 0, null_stream.ptr\n                )\n\n    def update(self):\n        """"""Update step for Custom Parallel Updater.""""""\n        self.update_core()\n        if self.forward_count == 0:\n            self.iteration += 1\n\n\nclass VaswaniRule(extension.Extension):\n    """"""Trainer extension to shift an optimizer attribute magically by Vaswani.\n\n    Args:\n        attr (str): Name of the attribute to shift.\n        rate (float): Rate of the exponential shift. This value is multiplied\n            to the attribute at each call.\n        init (float): Initial value of the attribute. If it is ``None``, the\n            extension extracts the attribute at the first call and uses it as\n            the initial value.\n        target (float): Target value of the attribute. If the attribute reaches\n            this value, the shift stops.\n        optimizer (~chainer.Optimizer): Target optimizer to adjust the\n            attribute. If it is ``None``, the main optimizer of the updater is\n            used.\n\n    """"""\n\n    def __init__(\n        self,\n        attr,\n        d,\n        warmup_steps=4000,\n        init=None,\n        target=None,\n        optimizer=None,\n        scale=1.0,\n    ):\n        """"""Initialize Vaswani rule extension.""""""\n        self._attr = attr\n        self._d_inv05 = d ** (-0.5) * scale\n        self._warmup_steps_inv15 = warmup_steps ** (-1.5)\n        self._init = init\n        self._target = target\n        self._optimizer = optimizer\n        self._t = 0\n        self._last_value = None\n\n    def initialize(self, trainer):\n        """"""Initialize Optimizer values.""""""\n        optimizer = self._get_optimizer(trainer)\n        # ensure that _init is set\n        if self._init is None:\n            self._init = self._d_inv05 * (1.0 * self._warmup_steps_inv15)\n        if self._last_value is not None:  # resuming from a snapshot\n            self._update_value(optimizer, self._last_value)\n        else:\n            self._update_value(optimizer, self._init)\n\n    def __call__(self, trainer):\n        """"""Forward extension.""""""\n        self._t += 1\n        optimizer = self._get_optimizer(trainer)\n        value = self._d_inv05 * min(\n            self._t ** (-0.5), self._t * self._warmup_steps_inv15\n        )\n        self._update_value(optimizer, value)\n\n    def serialize(self, serializer):\n        """"""Serialize extension.""""""\n        self._t = serializer(""_t"", self._t)\n        self._last_value = serializer(""_last_value"", self._last_value)\n\n    def _get_optimizer(self, trainer):\n        """"""Obtain optimizer from trainer.""""""\n        return self._optimizer or trainer.updater.get_optimizer(""main"")\n\n    def _update_value(self, optimizer, value):\n        """"""Update requested variable values.""""""\n        setattr(optimizer, self._attr, value)\n        self._last_value = value\n\n\nclass CustomConverter(object):\n    """"""Custom Converter.\n\n    Args:\n        subsampling_factor (int): The subsampling factor.\n\n    """"""\n\n    def __init__(self):\n        """"""Initialize subsampling.""""""\n        pass\n\n    def __call__(self, batch, device):\n        """"""Perform subsampling.\n\n        Args:\n            batch (list): Batch that will be sabsampled.\n            device (chainer.backend.Device): CPU or GPU device.\n\n        Returns:\n            chainer.Variable: xp.array that are padded and subsampled from batch.\n            xp.array: xp.array of the length of the mini-batches.\n            chainer.Variable: xp.array that are padded and subsampled from batch.\n\n        """"""\n        # For transformer, data is processed in CPU.\n        # batch should be located in list\n        assert len(batch) == 1\n        xs, ys = batch[0]\n        xs = F.pad_sequence(xs, padding=-1).data\n        # get batch of lengths of input sequences\n        ilens = np.array([x.shape[0] for x in xs], dtype=np.int32)\n        return xs, ilens, ys\n'"
espnet/nets/pytorch_backend/fastspeech/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/fastspeech/duration_calculator.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Duration calculator related modules.""""""\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.e2e_tts_tacotron2 import Tacotron2\nfrom espnet.nets.pytorch_backend.e2e_tts_transformer import Transformer\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\nclass DurationCalculator(torch.nn.Module):\n    """"""Duration calculator module for FastSpeech.\n\n    Todo:\n        * Fix the duplicated calculation of diagonal head decision\n\n    """"""\n\n    def __init__(self, teacher_model):\n        """"""Initialize duration calculator module.\n\n        Args:\n            teacher_model (e2e_tts_transformer.Transformer):\n                Pretrained auto-regressive Transformer.\n\n        """"""\n        super(DurationCalculator, self).__init__()\n        if isinstance(teacher_model, Transformer):\n            self.register_buffer(""diag_head_idx"", torch.tensor(-1))\n        elif isinstance(teacher_model, Tacotron2):\n            pass\n        else:\n            raise ValueError(\n                ""teacher model should be the instance of ""\n                ""e2e_tts_transformer.Transformer or e2e_tts_tacotron2.Tacotron2.""\n            )\n        self.teacher_model = teacher_model\n\n    def forward(self, xs, ilens, ys, olens, spembs=None):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of the padded sequences of character ids (B, Tmax).\n            ilens (Tensor): Batch of lengths of each input sequence (B,).\n            ys (Tensor):\n                Batch of the padded sequence of target features (B, Lmax, odim).\n            olens (Tensor): Batch of lengths of each output sequence (B,).\n            spembs (Tensor, optional):\n                Batch of speaker embedding vectors (B, spk_embed_dim).\n\n        Returns:\n            Tensor: Batch of durations (B, Tmax).\n\n        """"""\n        if isinstance(self.teacher_model, Transformer):\n            att_ws = self._calculate_encoder_decoder_attentions(\n                xs, ilens, ys, olens, spembs=spembs\n            )\n            # TODO(kan-bayashi): fix this issue\n            # this does not work in multi-gpu case. registered buffer is not saved.\n            if int(self.diag_head_idx) == -1:\n                self._init_diagonal_head(att_ws)\n            att_ws = att_ws[:, self.diag_head_idx]\n        else:\n            # NOTE(kan-bayashi): Here we assume that the teacher is tacotron 2\n            att_ws = self.teacher_model.calculate_all_attentions(\n                xs, ilens, ys, spembs=spembs, keep_tensor=True\n            )\n        durations = [\n            self._calculate_duration(att_w, ilen, olen)\n            for att_w, ilen, olen in zip(att_ws, ilens, olens)\n        ]\n\n        return pad_list(durations, 0)\n\n    @staticmethod\n    def _calculate_duration(att_w, ilen, olen):\n        return torch.stack(\n            [att_w[:olen, :ilen].argmax(-1).eq(i).sum() for i in range(ilen)]\n        )\n\n    def _init_diagonal_head(self, att_ws):\n        diagonal_scores = att_ws.max(dim=-1)[0].mean(dim=-1).mean(dim=0)  # (H * L,)\n        self.register_buffer(""diag_head_idx"", diagonal_scores.argmax())\n\n    def _calculate_encoder_decoder_attentions(self, xs, ilens, ys, olens, spembs=None):\n        att_dict = self.teacher_model.calculate_all_attentions(\n            xs, ilens, ys, olens, spembs=spembs, skip_output=True, keep_tensor=True\n        )\n        return torch.cat(\n            [att_dict[k] for k in att_dict.keys() if ""src_attn"" in k], dim=1\n        )  # (B, H*L, Lmax, Tmax)\n'"
espnet/nets/pytorch_backend/fastspeech/duration_predictor.py,12,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Duration predictor related modules.""""""\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n\n\nclass DurationPredictor(torch.nn.Module):\n    """"""Duration predictor module.\n\n    This is a module of duration predictor described\n    in `FastSpeech: Fast, Robust and Controllable Text to Speech`_.\n    The duration predictor predicts a duration of each frame in log domain\n    from the hidden embeddings of encoder.\n\n    .. _`FastSpeech: Fast, Robust and Controllable Text to Speech`:\n        https://arxiv.org/pdf/1905.09263.pdf\n\n    Note:\n        The calculation domain of outputs is different\n        between in `forward` and in `inference`. In `forward`,\n        the outputs are calculated in log domain but in `inference`,\n        those are calculated in linear domain.\n\n    """"""\n\n    def __init__(\n        self, idim, n_layers=2, n_chans=384, kernel_size=3, dropout_rate=0.1, offset=1.0\n    ):\n        """"""Initilize duration predictor module.\n\n        Args:\n            idim (int): Input dimension.\n            n_layers (int, optional): Number of convolutional layers.\n            n_chans (int, optional): Number of channels of convolutional layers.\n            kernel_size (int, optional): Kernel size of convolutional layers.\n            dropout_rate (float, optional): Dropout rate.\n            offset (float, optional): Offset value to avoid nan in log domain.\n\n        """"""\n        super(DurationPredictor, self).__init__()\n        self.offset = offset\n        self.conv = torch.nn.ModuleList()\n        for idx in range(n_layers):\n            in_chans = idim if idx == 0 else n_chans\n            self.conv += [\n                torch.nn.Sequential(\n                    torch.nn.Conv1d(\n                        in_chans,\n                        n_chans,\n                        kernel_size,\n                        stride=1,\n                        padding=(kernel_size - 1) // 2,\n                    ),\n                    torch.nn.ReLU(),\n                    LayerNorm(n_chans, dim=1),\n                    torch.nn.Dropout(dropout_rate),\n                )\n            ]\n        self.linear = torch.nn.Linear(n_chans, 1)\n\n    def _forward(self, xs, x_masks=None, is_inference=False):\n        xs = xs.transpose(1, -1)  # (B, idim, Tmax)\n        for f in self.conv:\n            xs = f(xs)  # (B, C, Tmax)\n\n        # NOTE: calculate in log domain\n        xs = self.linear(xs.transpose(1, -1)).squeeze(-1)  # (B, Tmax)\n\n        if is_inference:\n            # NOTE: calculate in linear domain\n            xs = torch.clamp(\n                torch.round(xs.exp() - self.offset), min=0\n            ).long()  # avoid negative value\n\n        if x_masks is not None:\n            xs = xs.masked_fill(x_masks, 0.0)\n\n        return xs\n\n    def forward(self, xs, x_masks=None):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of input sequences (B, Tmax, idim).\n            x_masks (ByteTensor, optional):\n                Batch of masks indicating padded part (B, Tmax).\n\n        Returns:\n            Tensor: Batch of predicted durations in log domain (B, Tmax).\n\n        """"""\n        return self._forward(xs, x_masks, False)\n\n    def inference(self, xs, x_masks=None):\n        """"""Inference duration.\n\n        Args:\n            xs (Tensor): Batch of input sequences (B, Tmax, idim).\n            x_masks (ByteTensor, optional):\n                Batch of masks indicating padded part (B, Tmax).\n\n        Returns:\n            LongTensor: Batch of predicted durations in linear domain (B, Tmax).\n\n        """"""\n        return self._forward(xs, x_masks, True)\n\n\nclass DurationPredictorLoss(torch.nn.Module):\n    """"""Loss function module for duration predictor.\n\n    The loss value is Calculated in log domain to make it Gaussian.\n\n    """"""\n\n    def __init__(self, offset=1.0, reduction=""mean""):\n        """"""Initilize duration predictor loss module.\n\n        Args:\n            offset (float, optional): Offset value to avoid nan in log domain.\n            reduction (str): Reduction type in loss calculation.\n\n        """"""\n        super(DurationPredictorLoss, self).__init__()\n        self.criterion = torch.nn.MSELoss(reduction=reduction)\n        self.offset = offset\n\n    def forward(self, outputs, targets):\n        """"""Calculate forward propagation.\n\n        Args:\n            outputs (Tensor): Batch of prediction durations in log domain (B, T)\n            targets (LongTensor): Batch of groundtruth durations in linear domain (B, T)\n\n        Returns:\n            Tensor: Mean squared error loss value.\n\n        Note:\n            `outputs` is in log domain but `targets` is in linear domain.\n\n        """"""\n        # NOTE: outputs is in log domain while targets in linear\n        targets = torch.log(targets.float() + self.offset)\n        loss = self.criterion(outputs, targets)\n\n        return loss\n'"
espnet/nets/pytorch_backend/fastspeech/length_regulator.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Length regulator related modules.""""""\n\nimport logging\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\nclass LengthRegulator(torch.nn.Module):\n    """"""Length regulator module for feed-forward Transformer.\n\n    This is a module of length regulator described in\n    `FastSpeech: Fast, Robust and Controllable Text to Speech`_.\n    The length regulator expands char or\n    phoneme-level embedding features to frame-level by repeating each\n    feature based on the corresponding predicted durations.\n\n    .. _`FastSpeech: Fast, Robust and Controllable Text to Speech`:\n        https://arxiv.org/pdf/1905.09263.pdf\n\n    """"""\n\n    def __init__(self, pad_value=0.0):\n        """"""Initilize length regulator module.\n\n        Args:\n            pad_value (float, optional): Value used for padding.\n\n        """"""\n        super(LengthRegulator, self).__init__()\n        self.pad_value = pad_value\n\n    def forward(self, xs, ds, ilens, alpha=1.0):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of sequences of char or phoneme embeddings (B, Tmax, D).\n            ds (LongTensor): Batch of durations of each frame (B, T).\n            ilens (LongTensor): Batch of input lengths (B,).\n            alpha (float, optional): Alpha value to control speed of speech.\n\n        Returns:\n            Tensor: replicated input tensor based on durations (B, T*, D).\n\n        """"""\n        assert alpha > 0\n        if alpha != 1.0:\n            ds = torch.round(ds.float() * alpha).long()\n        xs = [x[:ilen] for x, ilen in zip(xs, ilens)]\n        ds = [d[:ilen] for d, ilen in zip(ds, ilens)]\n        xs = [self._repeat_one_sequence(x, d) for x, d in zip(xs, ds)]\n\n        return pad_list(xs, self.pad_value)\n\n    def _repeat_one_sequence(self, x, d):\n        """"""Repeat each frame according to duration.\n\n        Examples:\n            >>> x = torch.tensor([[1], [2], [3]])\n            tensor([[1],\n                    [2],\n                    [3]])\n            >>> d = torch.tensor([1, 2, 3])\n            tensor([1, 2, 3])\n            >>> self._repeat_one_sequence(x, d)\n            tensor([[1],\n                    [2],\n                    [2],\n                    [3],\n                    [3],\n                    [3]])\n\n        """"""\n        if d.sum() == 0:\n            logging.warning(""all of the predicted durations are 0. fill 0 with 1."")\n            d = d.fill_(1)\n        return torch.cat(\n            [x_.repeat(int(d_), 1) for x_, d_ in zip(x, d) if d_ != 0], dim=0\n        )\n'"
espnet/nets/pytorch_backend/frontends/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/frontends/beamformer.py,5,"b'import torch\nfrom torch_complex import functional as FC\nfrom torch_complex.tensor import ComplexTensor\n\n\ndef get_power_spectral_density_matrix(\n    xs: ComplexTensor, mask: torch.Tensor, normalization=True, eps: float = 1e-15\n) -> ComplexTensor:\n    """"""Return cross-channel power spectral density (PSD) matrix\n\n    Args:\n        xs (ComplexTensor): (..., F, C, T)\n        mask (torch.Tensor): (..., F, C, T)\n        normalization (bool):\n        eps (float):\n    Returns\n        psd (ComplexTensor): (..., F, C, C)\n\n    """"""\n    # outer product: (..., C_1, T) x (..., C_2, T) -> (..., T, C, C_2)\n    psd_Y = FC.einsum(""...ct,...et->...tce"", [xs, xs.conj()])\n\n    # Averaging mask along C: (..., C, T) -> (..., T)\n    mask = mask.mean(dim=-2)\n\n    # Normalized mask along T: (..., T)\n    if normalization:\n        # If assuming the tensor is padded with zero, the summation along\n        # the time axis is same regardless of the padding length.\n        mask = mask / (mask.sum(dim=-1, keepdim=True) + eps)\n\n    # psd: (..., T, C, C)\n    psd = psd_Y * mask[..., None, None]\n    # (..., T, C, C) -> (..., C, C)\n    psd = psd.sum(dim=-3)\n\n    return psd\n\n\ndef get_mvdr_vector(\n    psd_s: ComplexTensor,\n    psd_n: ComplexTensor,\n    reference_vector: torch.Tensor,\n    eps: float = 1e-15,\n) -> ComplexTensor:\n    """"""Return the MVDR(Minimum Variance Distortionless Response) vector:\n\n        h = (Npsd^-1 @ Spsd) / (Tr(Npsd^-1 @ Spsd)) @ u\n\n    Reference:\n        On optimal frequency-domain multichannel linear filtering\n        for noise reduction; M. Souden et al., 2010;\n        https://ieeexplore.ieee.org/document/5089420\n\n    Args:\n        psd_s (ComplexTensor): (..., F, C, C)\n        psd_n (ComplexTensor): (..., F, C, C)\n        reference_vector (torch.Tensor): (..., C)\n        eps (float):\n    Returns:\n        beamform_vector (ComplexTensor)r: (..., F, C)\n    """"""\n    # Add eps\n    C = psd_n.size(-1)\n    eye = torch.eye(C, dtype=psd_n.dtype, device=psd_n.device)\n    shape = [1 for _ in range(psd_n.dim() - 2)] + [C, C]\n    eye = eye.view(*shape)\n    psd_n += eps * eye\n\n    # numerator: (..., C_1, C_2) x (..., C_2, C_3) -> (..., C_1, C_3)\n    numerator = FC.einsum(""...ec,...cd->...ed"", [psd_n.inverse(), psd_s])\n    # ws: (..., C, C) / (...,) -> (..., C, C)\n    ws = numerator / (FC.trace(numerator)[..., None, None] + eps)\n    # h: (..., F, C_1, C_2) x (..., C_2) -> (..., F, C_1)\n    beamform_vector = FC.einsum(""...fec,...c->...fe"", [ws, reference_vector])\n    return beamform_vector\n\n\ndef apply_beamforming_vector(\n    beamform_vector: ComplexTensor, mix: ComplexTensor\n) -> ComplexTensor:\n    # (..., C) x (..., C, T) -> (..., T)\n    es = FC.einsum(""...c,...ct->...t"", [beamform_vector.conj(), mix])\n    return es\n'"
espnet/nets/pytorch_backend/frontends/dnn_beamformer.py,21,"b'from distutils.version import LooseVersion\nfrom typing import Tuple\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom espnet.nets.pytorch_backend.frontends.beamformer import apply_beamforming_vector\nfrom espnet.nets.pytorch_backend.frontends.beamformer import get_mvdr_vector\nfrom espnet.nets.pytorch_backend.frontends.beamformer import (\n    get_power_spectral_density_matrix,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.frontends.mask_estimator import MaskEstimator\nfrom torch_complex.tensor import ComplexTensor\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(""1.2.0"")\nis_torch_1_3_plus = LooseVersion(torch.__version__) >= LooseVersion(""1.3.0"")\n\n\nclass DNN_Beamformer(torch.nn.Module):\n    """"""DNN mask based Beamformer\n\n    Citation:\n        Multichannel End-to-end Speech Recognition; T. Ochiai et al., 2017;\n        https://arxiv.org/abs/1703.04783\n\n    """"""\n\n    def __init__(\n        self,\n        bidim,\n        btype=""blstmp"",\n        blayers=3,\n        bunits=300,\n        bprojs=320,\n        bnmask=2,\n        dropout_rate=0.0,\n        badim=320,\n        ref_channel: int = -1,\n        beamformer_type=""mvdr"",\n    ):\n        super().__init__()\n        self.mask = MaskEstimator(\n            btype, bidim, blayers, bunits, bprojs, dropout_rate, nmask=bnmask\n        )\n        self.ref = AttentionReference(bidim, badim)\n        self.ref_channel = ref_channel\n\n        self.nmask = bnmask\n\n        if beamformer_type != ""mvdr"":\n            raise ValueError(\n                ""Not supporting beamformer_type={}"".format(beamformer_type)\n            )\n        self.beamformer_type = beamformer_type\n\n    def forward(\n        self, data: ComplexTensor, ilens: torch.LongTensor\n    ) -> Tuple[ComplexTensor, torch.LongTensor, ComplexTensor]:\n        """"""The forward function\n\n        Notation:\n            B: Batch\n            C: Channel\n            T: Time or Sequence length\n            F: Freq\n\n        Args:\n            data (ComplexTensor): (B, T, C, F)\n            ilens (torch.Tensor): (B,)\n        Returns:\n            enhanced (ComplexTensor): (B, T, F)\n            ilens (torch.Tensor): (B,)\n\n        """"""\n\n        def apply_beamforming(data, ilens, psd_speech, psd_noise):\n            # u: (B, C)\n            if self.ref_channel < 0:\n                u, _ = self.ref(psd_speech, ilens)\n            else:\n                # (optional) Create onehot vector for fixed reference microphone\n                u = torch.zeros(\n                    *(data.size()[:-3] + (data.size(-2),)), device=data.device\n                )\n                u[..., self.ref_channel].fill_(1)\n\n            ws = get_mvdr_vector(psd_speech, psd_noise, u)\n            enhanced = apply_beamforming_vector(ws, data)\n\n            return enhanced, ws\n\n        # data (B, T, C, F) -> (B, F, C, T)\n        data = data.permute(0, 3, 2, 1)\n\n        # mask: (B, F, C, T)\n        masks, _ = self.mask(data, ilens)\n        assert self.nmask == len(masks)\n\n        if self.nmask == 2:  # (mask_speech, mask_noise)\n            mask_speech, mask_noise = masks\n\n            psd_speech = get_power_spectral_density_matrix(data, mask_speech)\n            psd_noise = get_power_spectral_density_matrix(data, mask_noise)\n\n            enhanced, ws = apply_beamforming(data, ilens, psd_speech, psd_noise)\n\n            # (..., F, T) -> (..., T, F)\n            enhanced = enhanced.transpose(-1, -2)\n            mask_speech = mask_speech.transpose(-1, -3)\n        else:  # multi-speaker case: (mask_speech1, ..., mask_noise)\n            mask_speech = list(masks[:-1])\n            mask_noise = masks[-1]\n\n            psd_speeches = [\n                get_power_spectral_density_matrix(data, mask) for mask in mask_speech\n            ]\n            psd_noise = get_power_spectral_density_matrix(data, mask_noise)\n\n            enhanced = []\n            ws = []\n            for i in range(self.nmask - 1):\n                psd_speech = psd_speeches.pop(i)\n                # treat all other speakers\' psd_speech as noises\n                enh, w = apply_beamforming(\n                    data, ilens, psd_speech, sum(psd_speeches) + psd_noise\n                )\n                psd_speeches.insert(i, psd_speech)\n\n                # (..., F, T) -> (..., T, F)\n                enh = enh.transpose(-1, -2)\n                mask_speech[i] = mask_speech[i].transpose(-1, -3)\n\n                enhanced.append(enh)\n                ws.append(w)\n\n        return enhanced, ilens, mask_speech\n\n\nclass AttentionReference(torch.nn.Module):\n    def __init__(self, bidim, att_dim):\n        super().__init__()\n        self.mlp_psd = torch.nn.Linear(bidim, att_dim)\n        self.gvec = torch.nn.Linear(att_dim, 1)\n\n    def forward(\n        self, psd_in: ComplexTensor, ilens: torch.LongTensor, scaling: float = 2.0\n    ) -> Tuple[torch.Tensor, torch.LongTensor]:\n        """"""The forward function\n\n        Args:\n            psd_in (ComplexTensor): (B, F, C, C)\n            ilens (torch.Tensor): (B,)\n            scaling (float):\n        Returns:\n            u (torch.Tensor): (B, C)\n            ilens (torch.Tensor): (B,)\n        """"""\n        B, _, C = psd_in.size()[:3]\n        assert psd_in.size(2) == psd_in.size(3), psd_in.size()\n        # psd_in: (B, F, C, C)\n        datatype = torch.bool if is_torch_1_3_plus else torch.uint8\n        datatype2 = torch.bool if is_torch_1_2_plus else torch.uint8\n        psd = psd_in.masked_fill(\n            torch.eye(C, dtype=datatype, device=psd_in.device).type(datatype2), 0\n        )\n        # psd: (B, F, C, C) -> (B, C, F)\n        psd = (psd.sum(dim=-1) / (C - 1)).transpose(-1, -2)\n\n        # Calculate amplitude\n        psd_feat = (psd.real ** 2 + psd.imag ** 2) ** 0.5\n\n        # (B, C, F) -> (B, C, F2)\n        mlp_psd = self.mlp_psd(psd_feat)\n        # (B, C, F2) -> (B, C, 1) -> (B, C)\n        e = self.gvec(torch.tanh(mlp_psd)).squeeze(-1)\n        u = F.softmax(scaling * e, dim=-1)\n        return u, ilens\n'"
espnet/nets/pytorch_backend/frontends/dnn_wpe.py,3,"b'from typing import Tuple\n\nfrom pytorch_wpe import wpe_one_iteration\nimport torch\nfrom torch_complex.tensor import ComplexTensor\n\nfrom espnet.nets.pytorch_backend.frontends.mask_estimator import MaskEstimator\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\n\n\nclass DNN_WPE(torch.nn.Module):\n    def __init__(\n        self,\n        wtype: str = ""blstmp"",\n        widim: int = 257,\n        wlayers: int = 3,\n        wunits: int = 300,\n        wprojs: int = 320,\n        dropout_rate: float = 0.0,\n        taps: int = 5,\n        delay: int = 3,\n        use_dnn_mask: bool = True,\n        iterations: int = 1,\n        normalization: bool = False,\n    ):\n        super().__init__()\n        self.iterations = iterations\n        self.taps = taps\n        self.delay = delay\n\n        self.normalization = normalization\n        self.use_dnn_mask = use_dnn_mask\n\n        self.inverse_power = True\n\n        if self.use_dnn_mask:\n            self.mask_est = MaskEstimator(\n                wtype, widim, wlayers, wunits, wprojs, dropout_rate, nmask=1\n            )\n\n    def forward(\n        self, data: ComplexTensor, ilens: torch.LongTensor\n    ) -> Tuple[ComplexTensor, torch.LongTensor, ComplexTensor]:\n        """"""The forward function\n\n        Notation:\n            B: Batch\n            C: Channel\n            T: Time or Sequence length\n            F: Freq or Some dimension of the feature vector\n\n        Args:\n            data: (B, C, T, F)\n            ilens: (B,)\n        Returns:\n            data: (B, C, T, F)\n            ilens: (B,)\n        """"""\n        # (B, T, C, F) -> (B, F, C, T)\n        enhanced = data = data.permute(0, 3, 2, 1)\n        mask = None\n\n        for i in range(self.iterations):\n            # Calculate power: (..., C, T)\n            power = enhanced.real ** 2 + enhanced.imag ** 2\n            if i == 0 and self.use_dnn_mask:\n                # mask: (B, F, C, T)\n                (mask,), _ = self.mask_est(enhanced, ilens)\n                if self.normalization:\n                    # Normalize along T\n                    mask = mask / mask.sum(dim=-1)[..., None]\n                # (..., C, T) * (..., C, T) -> (..., C, T)\n                power = power * mask\n\n            # Averaging along the channel axis: (..., C, T) -> (..., T)\n            power = power.mean(dim=-2)\n\n            # enhanced: (..., C, T) -> (..., C, T)\n            enhanced = wpe_one_iteration(\n                data.contiguous(),\n                power,\n                taps=self.taps,\n                delay=self.delay,\n                inverse_power=self.inverse_power,\n            )\n\n            enhanced.masked_fill_(make_pad_mask(ilens, enhanced.real), 0)\n\n        # (B, F, C, T) -> (B, T, C, F)\n        enhanced = enhanced.permute(0, 3, 2, 1)\n        if mask is not None:\n            mask = mask.transpose(-1, -3)\n        return enhanced, ilens, mask\n'"
espnet/nets/pytorch_backend/frontends/feature_transform.py,23,"b'from typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nimport librosa\nimport numpy as np\nimport torch\nfrom torch_complex.tensor import ComplexTensor\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\n\n\nclass FeatureTransform(torch.nn.Module):\n    def __init__(\n        self,\n        # Mel options,\n        fs: int = 16000,\n        n_fft: int = 512,\n        n_mels: int = 80,\n        fmin: float = 0.0,\n        fmax: float = None,\n        # Normalization\n        stats_file: str = None,\n        apply_uttmvn: bool = True,\n        uttmvn_norm_means: bool = True,\n        uttmvn_norm_vars: bool = False,\n    ):\n        super().__init__()\n        self.apply_uttmvn = apply_uttmvn\n\n        self.logmel = LogMel(fs=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax)\n        self.stats_file = stats_file\n        if stats_file is not None:\n            self.global_mvn = GlobalMVN(stats_file)\n        else:\n            self.global_mvn = None\n\n        if self.apply_uttmvn is not None:\n            self.uttmvn = UtteranceMVN(\n                norm_means=uttmvn_norm_means, norm_vars=uttmvn_norm_vars\n            )\n        else:\n            self.uttmvn = None\n\n    def forward(\n        self, x: ComplexTensor, ilens: Union[torch.LongTensor, np.ndarray, List[int]]\n    ) -> Tuple[torch.Tensor, torch.LongTensor]:\n        # (B, T, F) or (B, T, C, F)\n        if x.dim() not in (3, 4):\n            raise ValueError(f""Input dim must be 3 or 4: {x.dim()}"")\n        if not torch.is_tensor(ilens):\n            ilens = torch.from_numpy(np.asarray(ilens)).to(x.device)\n\n        if x.dim() == 4:\n            # h: (B, T, C, F) -> h: (B, T, F)\n            if self.training:\n                # Select 1ch randomly\n                ch = np.random.randint(x.size(2))\n                h = x[:, :, ch, :]\n            else:\n                # Use the first channel\n                h = x[:, :, 0, :]\n        else:\n            h = x\n\n        # h: ComplexTensor(B, T, F) -> torch.Tensor(B, T, F)\n        h = h.real ** 2 + h.imag ** 2\n\n        h, _ = self.logmel(h, ilens)\n        if self.stats_file is not None:\n            h, _ = self.global_mvn(h, ilens)\n        if self.apply_uttmvn:\n            h, _ = self.uttmvn(h, ilens)\n\n        return h, ilens\n\n\nclass LogMel(torch.nn.Module):\n    """"""Convert STFT to fbank feats\n\n    The arguments is same as librosa.filters.mel\n\n    Args:\n        fs: number > 0 [scalar] sampling rate of the incoming signal\n        n_fft: int > 0 [scalar] number of FFT components\n        n_mels: int > 0 [scalar] number of Mel bands to generate\n        fmin: float >= 0 [scalar] lowest frequency (in Hz)\n        fmax: float >= 0 [scalar] highest frequency (in Hz).\n            If `None`, use `fmax = fs / 2.0`\n        htk: use HTK formula instead of Slaney\n        norm: {None, 1, np.inf} [scalar]\n            if 1, divide the triangular mel weights by the width of the mel band\n            (area normalization).  Otherwise, leave all the triangles aiming for\n            a peak value of 1.0\n\n    """"""\n\n    def __init__(\n        self,\n        fs: int = 16000,\n        n_fft: int = 512,\n        n_mels: int = 80,\n        fmin: float = 0.0,\n        fmax: float = None,\n        htk: bool = False,\n        norm=1,\n    ):\n        super().__init__()\n\n        _mel_options = dict(\n            sr=fs, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, htk=htk, norm=norm\n        )\n        self.mel_options = _mel_options\n\n        # Note(kamo): The mel matrix of librosa is different from kaldi.\n        melmat = librosa.filters.mel(**_mel_options)\n        # melmat: (D2, D1) -> (D1, D2)\n        self.register_buffer(""melmat"", torch.from_numpy(melmat.T).float())\n\n    def extra_repr(self):\n        return "", "".join(f""{k}={v}"" for k, v in self.mel_options.items())\n\n    def forward(\n        self, feat: torch.Tensor, ilens: torch.LongTensor\n    ) -> Tuple[torch.Tensor, torch.LongTensor]:\n        # feat: (B, T, D1) x melmat: (D1, D2) -> mel_feat: (B, T, D2)\n        mel_feat = torch.matmul(feat, self.melmat)\n\n        logmel_feat = (mel_feat + 1e-20).log()\n        # Zero padding\n        logmel_feat = logmel_feat.masked_fill(make_pad_mask(ilens, logmel_feat, 1), 0.0)\n        return logmel_feat, ilens\n\n\nclass GlobalMVN(torch.nn.Module):\n    """"""Apply global mean and variance normalization\n\n    Args:\n        stats_file(str): npy file of 1-dim array or text file.\n            From the _first element to\n            the {(len(array) - 1) / 2}th element are treated as\n            the sum of features,\n            and the rest excluding the last elements are\n            treated as the sum of the square value of features,\n            and the last elements eqauls to the number of samples.\n        std_floor(float):\n    """"""\n\n    def __init__(\n        self,\n        stats_file: str,\n        norm_means: bool = True,\n        norm_vars: bool = True,\n        eps: float = 1.0e-20,\n    ):\n        super().__init__()\n        self.norm_means = norm_means\n        self.norm_vars = norm_vars\n\n        self.stats_file = stats_file\n        stats = np.load(stats_file)\n\n        stats = stats.astype(float)\n        assert (len(stats) - 1) % 2 == 0, stats.shape\n\n        count = stats.flatten()[-1]\n        mean = stats[: (len(stats) - 1) // 2] / count\n        var = stats[(len(stats) - 1) // 2 : -1] / count - mean * mean\n        std = np.maximum(np.sqrt(var), eps)\n\n        self.register_buffer(""bias"", torch.from_numpy(-mean.astype(np.float32)))\n        self.register_buffer(""scale"", torch.from_numpy(1 / std.astype(np.float32)))\n\n    def extra_repr(self):\n        return (\n            f""stats_file={self.stats_file}, ""\n            f""norm_means={self.norm_means}, norm_vars={self.norm_vars}""\n        )\n\n    def forward(\n        self, x: torch.Tensor, ilens: torch.LongTensor\n    ) -> Tuple[torch.Tensor, torch.LongTensor]:\n        # feat: (B, T, D)\n        if self.norm_means:\n            x += self.bias.type_as(x)\n            x.masked_fill(make_pad_mask(ilens, x, 1), 0.0)\n\n        if self.norm_vars:\n            x *= self.scale.type_as(x)\n        return x, ilens\n\n\nclass UtteranceMVN(torch.nn.Module):\n    def __init__(\n        self, norm_means: bool = True, norm_vars: bool = False, eps: float = 1.0e-20\n    ):\n        super().__init__()\n        self.norm_means = norm_means\n        self.norm_vars = norm_vars\n        self.eps = eps\n\n    def extra_repr(self):\n        return f""norm_means={self.norm_means}, norm_vars={self.norm_vars}""\n\n    def forward(\n        self, x: torch.Tensor, ilens: torch.LongTensor\n    ) -> Tuple[torch.Tensor, torch.LongTensor]:\n        return utterance_mvn(\n            x, ilens, norm_means=self.norm_means, norm_vars=self.norm_vars, eps=self.eps\n        )\n\n\ndef utterance_mvn(\n    x: torch.Tensor,\n    ilens: torch.LongTensor,\n    norm_means: bool = True,\n    norm_vars: bool = False,\n    eps: float = 1.0e-20,\n) -> Tuple[torch.Tensor, torch.LongTensor]:\n    """"""Apply utterance mean and variance normalization\n\n    Args:\n        x: (B, T, D), assumed zero padded\n        ilens: (B, T, D)\n        norm_means:\n        norm_vars:\n        eps:\n\n    """"""\n    ilens_ = ilens.type_as(x)\n    # mean: (B, D)\n    mean = x.sum(dim=1) / ilens_[:, None]\n\n    if norm_means:\n        x -= mean[:, None, :]\n        x_ = x\n    else:\n        x_ = x - mean[:, None, :]\n\n    # Zero padding\n    x_.masked_fill(make_pad_mask(ilens, x_, 1), 0.0)\n    if norm_vars:\n        var = x_.pow(2).sum(dim=1) / ilens_[:, None]\n        var = torch.clamp(var, min=eps)\n        x /= var.sqrt()[:, None, :]\n        x_ = x\n    return x_, ilens\n\n\ndef feature_transform_for(args, n_fft):\n    return FeatureTransform(\n        # Mel options,\n        fs=args.fbank_fs,\n        n_fft=n_fft,\n        n_mels=args.n_mels,\n        fmin=args.fbank_fmin,\n        fmax=args.fbank_fmax,\n        # Normalization\n        stats_file=args.stats_file,\n        apply_uttmvn=args.apply_uttmvn,\n        uttmvn_norm_means=args.uttmvn_norm_means,\n        uttmvn_norm_vars=args.uttmvn_norm_vars,\n    )\n'"
espnet/nets/pytorch_backend/frontends/frontend.py,5,"b'from typing import List\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Union\n\nimport numpy\nimport torch\nimport torch.nn as nn\nfrom torch_complex.tensor import ComplexTensor\n\nfrom espnet.nets.pytorch_backend.frontends.dnn_beamformer import DNN_Beamformer\nfrom espnet.nets.pytorch_backend.frontends.dnn_wpe import DNN_WPE\n\n\nclass Frontend(nn.Module):\n    def __init__(\n        self,\n        idim: int,\n        # WPE options\n        use_wpe: bool = False,\n        wtype: str = ""blstmp"",\n        wlayers: int = 3,\n        wunits: int = 300,\n        wprojs: int = 320,\n        wdropout_rate: float = 0.0,\n        taps: int = 5,\n        delay: int = 3,\n        use_dnn_mask_for_wpe: bool = True,\n        # Beamformer options\n        use_beamformer: bool = False,\n        btype: str = ""blstmp"",\n        blayers: int = 3,\n        bunits: int = 300,\n        bprojs: int = 320,\n        bnmask: int = 2,\n        badim: int = 320,\n        ref_channel: int = -1,\n        bdropout_rate=0.0,\n    ):\n        super().__init__()\n\n        self.use_beamformer = use_beamformer\n        self.use_wpe = use_wpe\n        self.use_dnn_mask_for_wpe = use_dnn_mask_for_wpe\n        # use frontend for all the data,\n        # e.g. in the case of multi-speaker speech separation\n        self.use_frontend_for_all = bnmask > 2\n\n        if self.use_wpe:\n            if self.use_dnn_mask_for_wpe:\n                # Use DNN for power estimation\n                # (Not observed significant gains)\n                iterations = 1\n            else:\n                # Performing as conventional WPE, without DNN Estimator\n                iterations = 2\n\n            self.wpe = DNN_WPE(\n                wtype=wtype,\n                widim=idim,\n                wunits=wunits,\n                wprojs=wprojs,\n                wlayers=wlayers,\n                taps=taps,\n                delay=delay,\n                dropout_rate=wdropout_rate,\n                iterations=iterations,\n                use_dnn_mask=use_dnn_mask_for_wpe,\n            )\n        else:\n            self.wpe = None\n\n        if self.use_beamformer:\n            self.beamformer = DNN_Beamformer(\n                btype=btype,\n                bidim=idim,\n                bunits=bunits,\n                bprojs=bprojs,\n                blayers=blayers,\n                bnmask=bnmask,\n                dropout_rate=bdropout_rate,\n                badim=badim,\n                ref_channel=ref_channel,\n            )\n        else:\n            self.beamformer = None\n\n    def forward(\n        self, x: ComplexTensor, ilens: Union[torch.LongTensor, numpy.ndarray, List[int]]\n    ) -> Tuple[ComplexTensor, torch.LongTensor, Optional[ComplexTensor]]:\n        assert len(x) == len(ilens), (len(x), len(ilens))\n        # (B, T, F) or (B, T, C, F)\n        if x.dim() not in (3, 4):\n            raise ValueError(f""Input dim must be 3 or 4: {x.dim()}"")\n        if not torch.is_tensor(ilens):\n            ilens = torch.from_numpy(numpy.asarray(ilens)).to(x.device)\n\n        mask = None\n        h = x\n        if h.dim() == 4:\n            if self.training:\n                choices = [(False, False)] if not self.use_frontend_for_all else []\n                if self.use_wpe:\n                    choices.append((True, False))\n\n                if self.use_beamformer:\n                    choices.append((False, True))\n\n                use_wpe, use_beamformer = choices[numpy.random.randint(len(choices))]\n\n            else:\n                use_wpe = self.use_wpe\n                use_beamformer = self.use_beamformer\n\n            # 1. WPE\n            if use_wpe:\n                # h: (B, T, C, F) -> h: (B, T, C, F)\n                h, ilens, mask = self.wpe(h, ilens)\n\n            # 2. Beamformer\n            if use_beamformer:\n                # h: (B, T, C, F) -> h: (B, T, F)\n                h, ilens, mask = self.beamformer(h, ilens)\n\n        return h, ilens, mask\n\n\ndef frontend_for(args, idim):\n    return Frontend(\n        idim=idim,\n        # WPE options\n        use_wpe=args.use_wpe,\n        wtype=args.wtype,\n        wlayers=args.wlayers,\n        wunits=args.wunits,\n        wprojs=args.wprojs,\n        wdropout_rate=args.wdropout_rate,\n        taps=args.wpe_taps,\n        delay=args.wpe_delay,\n        use_dnn_mask_for_wpe=args.use_dnn_mask_for_wpe,\n        # Beamformer options\n        use_beamformer=args.use_beamformer,\n        btype=args.btype,\n        blayers=args.blayers,\n        bunits=args.bunits,\n        bprojs=args.bprojs,\n        bnmask=args.bnmask,\n        badim=args.badim,\n        ref_channel=args.ref_channel,\n        bdropout_rate=args.bdropout_rate,\n    )\n'"
espnet/nets/pytorch_backend/frontends/mask_estimator.py,8,"b'from typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nfrom torch_complex.tensor import ComplexTensor\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.rnn.encoders import RNN\nfrom espnet.nets.pytorch_backend.rnn.encoders import RNNP\n\n\nclass MaskEstimator(torch.nn.Module):\n    def __init__(self, type, idim, layers, units, projs, dropout, nmask=1):\n        super().__init__()\n        subsample = np.ones(layers + 1, dtype=np.int)\n\n        typ = type.lstrip(""vgg"").rstrip(""p"")\n        if type[-1] == ""p"":\n            self.brnn = RNNP(idim, layers, units, projs, subsample, dropout, typ=typ)\n        else:\n            self.brnn = RNN(idim, layers, units, projs, dropout, typ=typ)\n\n        self.type = type\n        self.nmask = nmask\n        self.linears = torch.nn.ModuleList(\n            [torch.nn.Linear(projs, idim) for _ in range(nmask)]\n        )\n\n    def forward(\n        self, xs: ComplexTensor, ilens: torch.LongTensor\n    ) -> Tuple[Tuple[torch.Tensor, ...], torch.LongTensor]:\n        """"""The forward function\n\n        Args:\n            xs: (B, F, C, T)\n            ilens: (B,)\n        Returns:\n            hs (torch.Tensor): The hidden vector (B, F, C, T)\n            masks: A tuple of the masks. (B, F, C, T)\n            ilens: (B,)\n        """"""\n        assert xs.size(0) == ilens.size(0), (xs.size(0), ilens.size(0))\n        _, _, C, input_length = xs.size()\n        # (B, F, C, T) -> (B, C, T, F)\n        xs = xs.permute(0, 2, 3, 1)\n\n        # Calculate amplitude: (B, C, T, F) -> (B, C, T, F)\n        xs = (xs.real ** 2 + xs.imag ** 2) ** 0.5\n        # xs: (B, C, T, F) -> xs: (B * C, T, F)\n        xs = xs.contiguous().view(-1, xs.size(-2), xs.size(-1))\n        # ilens: (B,) -> ilens_: (B * C)\n        ilens_ = ilens[:, None].expand(-1, C).contiguous().view(-1)\n\n        # xs: (B * C, T, F) -> xs: (B * C, T, D)\n        xs, _, _ = self.brnn(xs, ilens_)\n        # xs: (B * C, T, D) -> xs: (B, C, T, D)\n        xs = xs.view(-1, C, xs.size(-2), xs.size(-1))\n\n        masks = []\n        for linear in self.linears:\n            # xs: (B, C, T, D) -> mask:(B, C, T, F)\n            mask = linear(xs)\n\n            mask = torch.sigmoid(mask)\n            # Zero padding\n            mask.masked_fill(make_pad_mask(ilens, mask, length_dim=2), 0)\n\n            # (B, C, T, F) -> (B, F, C, T)\n            mask = mask.permute(0, 3, 1, 2)\n\n            # Take cares of multi gpu cases: If input_length > max(ilens)\n            if mask.size(-1) < input_length:\n                mask = F.pad(mask, [0, input_length - mask.size(-1)], value=0)\n            masks.append(mask)\n\n        return tuple(masks), ilens\n'"
espnet/nets/pytorch_backend/lm/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/lm/default.py,29,"b'""""""Default Recurrent Neural Network Languge Model in `lm_train.py`.""""""\n\nfrom typing import Any\nfrom typing import List\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom espnet.nets.lm_interface import LMInterface\nfrom espnet.nets.pytorch_backend.e2e_asr import to_device\nfrom espnet.nets.scorer_interface import BatchScorerInterface\n\n\nclass DefaultRNNLM(BatchScorerInterface, LMInterface, nn.Module):\n    """"""Default RNNLM for `LMInterface` Implementation.\n\n    Note:\n        PyTorch seems to have memory leak when one GPU compute this after data parallel.\n        If parallel GPUs compute this, it seems to be fine.\n        See also https://github.com/espnet/espnet/issues/1075\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments to command line argument parser.""""""\n        parser.add_argument(\n            ""--type"",\n            type=str,\n            default=""lstm"",\n            nargs=""?"",\n            choices=[""lstm"", ""gru""],\n            help=""Which type of RNN to use"",\n        )\n        parser.add_argument(\n            ""--layer"", ""-l"", type=int, default=2, help=""Number of hidden layers""\n        )\n        parser.add_argument(\n            ""--unit"", ""-u"", type=int, default=650, help=""Number of hidden units""\n        )\n        parser.add_argument(\n            ""--embed-unit"",\n            default=None,\n            help=""Number of hidden units in embedding layer, ""\n            ""if it is not specified, it keeps the same number with hidden units."",\n        )\n        parser.add_argument(\n            ""--dropout-rate"", type=float, default=0.5, help=""dropout probability""\n        )\n        return parser\n\n    def __init__(self, n_vocab, args):\n        """"""Initialize class.\n\n        Args:\n            n_vocab (int): The size of the vocabulary\n            args (argparse.Namespace): configurations. see py:method:`add_arguments`\n\n        """"""\n        nn.Module.__init__(self)\n        # NOTE: for a compatibility with less than 0.5.0 version models\n        dropout_rate = getattr(args, ""dropout_rate"", 0.0)\n        # NOTE: for a compatibility with less than 0.6.1 version models\n        embed_unit = getattr(args, ""embed_unit"", None)\n        self.model = ClassifierWithState(\n            RNNLM(n_vocab, args.layer, args.unit, embed_unit, args.type, dropout_rate)\n        )\n\n    def state_dict(self):\n        """"""Dump state dict.""""""\n        return self.model.state_dict()\n\n    def load_state_dict(self, d):\n        """"""Load state dict.""""""\n        self.model.load_state_dict(d)\n\n    def forward(self, x, t):\n        """"""Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        """"""\n        loss = 0\n        logp = 0\n        count = torch.tensor(0).long()\n        state = None\n        batch_size, sequence_length = x.shape\n        for i in range(sequence_length):\n            # Compute the loss at this time step and accumulate it\n            state, loss_batch = self.model(state, x[:, i], t[:, i])\n            non_zeros = torch.sum(x[:, i] != 0, dtype=loss_batch.dtype)\n            loss += loss_batch.mean() * non_zeros\n            logp += torch.sum(loss_batch * non_zeros)\n            count += int(non_zeros)\n        return loss / batch_size, loss, count.to(loss.device)\n\n    def score(self, y, state, x):\n        """"""Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and next state for ys\n\n        """"""\n        new_state, scores = self.model.predict(state, y[-1].unsqueeze(0))\n        return scores.squeeze(0), new_state\n\n    def final_score(self, state):\n        """"""Score eos.\n\n        Args:\n            state: Scorer state for prefix tokens\n\n        Returns:\n            float: final score\n\n        """"""\n        return self.model.final(state)\n\n    # batch beam search API (see BatchScorerInterface)\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        """"""Score new token batch.\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        """"""\n        # merge states\n        n_batch = len(ys)\n        n_layers = self.model.predictor.n_layers\n        if self.model.predictor.typ == ""lstm"":\n            keys = (""c"", ""h"")\n        else:\n            keys = (""h"",)\n\n        if states[0] is None:\n            states = None\n        else:\n            # transpose state of [batch, key, layer] into [key, layer, batch]\n            states = {\n                k: [\n                    torch.stack([states[b][k][i] for b in range(n_batch)])\n                    for i in range(n_layers)\n                ]\n                for k in keys\n            }\n        states, logp = self.model.predict(states, ys[:, -1])\n\n        # transpose state of [key, layer, batch] into [batch, key, layer]\n        return (\n            logp,\n            [\n                {k: [states[k][i][b] for i in range(n_layers)] for k in keys}\n                for b in range(n_batch)\n            ],\n        )\n\n\nclass ClassifierWithState(nn.Module):\n    """"""A wrapper for pytorch RNNLM.""""""\n\n    def __init__(\n        self, predictor, lossfun=nn.CrossEntropyLoss(reduction=""none""), label_key=-1\n    ):\n        """"""Initialize class.\n\n        :param torch.nn.Module predictor : The RNNLM\n        :param function lossfun : The loss function to use\n        :param int/str label_key :\n\n        """"""\n        if not (isinstance(label_key, (int, str))):\n            raise TypeError(""label_key must be int or str, but is %s"" % type(label_key))\n        super(ClassifierWithState, self).__init__()\n        self.lossfun = lossfun\n        self.y = None\n        self.loss = None\n        self.label_key = label_key\n        self.predictor = predictor\n\n    def forward(self, state, *args, **kwargs):\n        """"""Compute the loss value for an input and label pair.\n\n        Notes:\n            It also computes accuracy and stores it to the attribute.\n            When ``label_key`` is ``int``, the corresponding element in ``args``\n            is treated as ground truth labels. And when it is ``str``, the\n            element in ``kwargs`` is used.\n            The all elements of ``args`` and ``kwargs`` except the groundtruth\n            labels are features.\n            It feeds features to the predictor and compare the result\n            with ground truth labels.\n\n        :param torch.Tensor state : the LM state\n        :param list[torch.Tensor] args : Input minibatch\n        :param dict[torch.Tensor] kwargs : Input minibatch\n        :return loss value\n        :rtype torch.Tensor\n\n        """"""\n        if isinstance(self.label_key, int):\n            if not (-len(args) <= self.label_key < len(args)):\n                msg = ""Label key %d is out of bounds"" % self.label_key\n                raise ValueError(msg)\n            t = args[self.label_key]\n            if self.label_key == -1:\n                args = args[:-1]\n            else:\n                args = args[: self.label_key] + args[self.label_key + 1 :]\n        elif isinstance(self.label_key, str):\n            if self.label_key not in kwargs:\n                msg = \'Label key ""%s"" is not found\' % self.label_key\n                raise ValueError(msg)\n            t = kwargs[self.label_key]\n            del kwargs[self.label_key]\n\n        self.y = None\n        self.loss = None\n        state, self.y = self.predictor(state, *args, **kwargs)\n        self.loss = self.lossfun(self.y, t)\n        return state, self.loss\n\n    def predict(self, state, x):\n        """"""Predict log probabilities for given state and input x using the predictor.\n\n        :param torch.Tensor state : The current state\n        :param torch.Tensor x : The input\n        :return a tuple (new state, log prob vector)\n        :rtype (torch.Tensor, torch.Tensor)\n        """"""\n        if hasattr(self.predictor, ""normalized"") and self.predictor.normalized:\n            return self.predictor(state, x)\n        else:\n            state, z = self.predictor(state, x)\n            return state, F.log_softmax(z, dim=1)\n\n    def buff_predict(self, state, x, n):\n        """"""Predict new tokens from buffered inputs.""""""\n        if self.predictor.__class__.__name__ == ""RNNLM"":\n            return self.predict(state, x)\n\n        new_state = []\n        new_log_y = []\n        for i in range(n):\n            state_i = None if state is None else state[i]\n            state_i, log_y = self.predict(state_i, x[i].unsqueeze(0))\n            new_state.append(state_i)\n            new_log_y.append(log_y)\n\n        return new_state, torch.cat(new_log_y)\n\n    def final(self, state, index=None):\n        """"""Predict final log probabilities for given state using the predictor.\n\n        :param state: The state\n        :return The final log probabilities\n        :rtype torch.Tensor\n        """"""\n        if hasattr(self.predictor, ""final""):\n            if index is not None:\n                return self.predictor.final(state[index])\n            else:\n                return self.predictor.final(state)\n        else:\n            return 0.0\n\n\n# Definition of a recurrent net for language modeling\nclass RNNLM(nn.Module):\n    """"""A pytorch RNNLM.""""""\n\n    def __init__(\n        self, n_vocab, n_layers, n_units, n_embed=None, typ=""lstm"", dropout_rate=0.5\n    ):\n        """"""Initialize class.\n\n        :param int n_vocab: The size of the vocabulary\n        :param int n_layers: The number of layers to create\n        :param int n_units: The number of units per layer\n        :param str typ: The RNN type\n        """"""\n        super(RNNLM, self).__init__()\n        if n_embed is None:\n            n_embed = n_units\n        self.embed = nn.Embedding(n_vocab, n_embed)\n        if typ == ""lstm"":\n            self.rnn = nn.ModuleList(\n                [nn.LSTMCell(n_embed, n_units)]\n                + [nn.LSTMCell(n_units, n_units) for _ in range(n_layers - 1)]\n            )\n        else:\n            self.rnn = nn.ModuleList(\n                [nn.GRUCell(n_embed, n_units)]\n                + [nn.GRUCell(n_units, n_units) for _ in range(n_layers - 1)]\n            )\n\n        self.dropout = nn.ModuleList(\n            [nn.Dropout(dropout_rate) for _ in range(n_layers + 1)]\n        )\n        self.lo = nn.Linear(n_units, n_vocab)\n        self.n_layers = n_layers\n        self.n_units = n_units\n        self.typ = typ\n\n        # initialize parameters from uniform distribution\n        for param in self.parameters():\n            param.data.uniform_(-0.1, 0.1)\n\n    def zero_state(self, batchsize):\n        """"""Initialize state.""""""\n        p = next(self.parameters())\n        return torch.zeros(batchsize, self.n_units).to(device=p.device, dtype=p.dtype)\n\n    def forward(self, state, x):\n        """"""Forward neural networks.""""""\n        if state is None:\n            h = [\n                to_device(self, self.zero_state(x.size(0)))\n                for n in range(self.n_layers)\n            ]\n            state = {""h"": h}\n            if self.typ == ""lstm"":\n                c = [\n                    to_device(self, self.zero_state(x.size(0)))\n                    for n in range(self.n_layers)\n                ]\n                state = {""c"": c, ""h"": h}\n\n        h = [None] * self.n_layers\n        emb = self.embed(x)\n        if self.typ == ""lstm"":\n            c = [None] * self.n_layers\n            h[0], c[0] = self.rnn[0](\n                self.dropout[0](emb), (state[""h""][0], state[""c""][0])\n            )\n            for n in range(1, self.n_layers):\n                h[n], c[n] = self.rnn[n](\n                    self.dropout[n](h[n - 1]), (state[""h""][n], state[""c""][n])\n                )\n            state = {""c"": c, ""h"": h}\n        else:\n            h[0] = self.rnn[0](self.dropout[0](emb), state[""h""][0])\n            for n in range(1, self.n_layers):\n                h[n] = self.rnn[n](self.dropout[n](h[n - 1]), state[""h""][n])\n            state = {""h"": h}\n        y = self.lo(self.dropout[-1](h[-1]))\n        return state, y\n'"
espnet/nets/pytorch_backend/lm/seq_rnn.py,12,"b'""""""Sequential implementation of Recurrent Neural Network Language Model.""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom espnet.nets.lm_interface import LMInterface\n\n\nclass SequentialRNNLM(LMInterface, torch.nn.Module):\n    """"""Sequential RNNLM.\n\n    See also:\n        https://github.com/pytorch/examples/blob/4581968193699de14b56527296262dd76ab43557/word_language_model/model.py\n\n    """"""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments to command line argument parser.""""""\n        parser.add_argument(\n            ""--type"",\n            type=str,\n            default=""lstm"",\n            nargs=""?"",\n            choices=[""lstm"", ""gru""],\n            help=""Which type of RNN to use"",\n        )\n        parser.add_argument(\n            ""--layer"", ""-l"", type=int, default=2, help=""Number of hidden layers""\n        )\n        parser.add_argument(\n            ""--unit"", ""-u"", type=int, default=650, help=""Number of hidden units""\n        )\n        parser.add_argument(\n            ""--dropout-rate"", type=float, default=0.5, help=""dropout probability""\n        )\n        return parser\n\n    def __init__(self, n_vocab, args):\n        """"""Initialize class.\n\n        Args:\n            n_vocab (int): The size of the vocabulary\n            args (argparse.Namespace): configurations. see py:method:`add_arguments`\n\n        """"""\n        torch.nn.Module.__init__(self)\n        self._setup(\n            rnn_type=args.type.upper(),\n            ntoken=n_vocab,\n            ninp=args.unit,\n            nhid=args.unit,\n            nlayers=args.layer,\n            dropout=args.dropout_rate,\n        )\n\n    def _setup(\n        self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False\n    ):\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        if rnn_type in [""LSTM"", ""GRU""]:\n            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n        else:\n            try:\n                nonlinearity = {""RNN_TANH"": ""tanh"", ""RNN_RELU"": ""relu""}[rnn_type]\n            except KeyError:\n                raise ValueError(\n                    ""An invalid option for `--model` was supplied, ""\n                    ""options are [\'LSTM\', \'GRU\', \'RNN_TANH\' or \'RNN_RELU\']""\n                )\n            self.rnn = nn.RNN(\n                ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout\n            )\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        # Optionally tie weights as in:\n        # ""Using the Output Embedding to Improve Language Models"" (Press & Wolf 2016)\n        # https://arxiv.org/abs/1608.05859\n        # and\n        # ""Tying Word Vectors and Word Classifiers:\n        #  A Loss Framework for Language Modeling"" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            if nhid != ninp:\n                raise ValueError(\n                    ""When using the tied flag, nhid must be equal to emsize""\n                )\n            self.decoder.weight = self.encoder.weight\n\n        self._init_weights()\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def _init_weights(self):\n        # NOTE: original init in pytorch/examples\n        # initrange = 0.1\n        # self.encoder.weight.data.uniform_(-initrange, initrange)\n        # self.decoder.bias.data.zero_()\n        # self.decoder.weight.data.uniform_(-initrange, initrange)\n        # NOTE: our default.py:RNNLM init\n        for param in self.parameters():\n            param.data.uniform_(-0.1, 0.1)\n\n    def forward(self, x, t):\n        """"""Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        """"""\n        y = self._before_loss(x, None)[0]\n        mask = (x != 0).to(y.dtype)\n        loss = F.cross_entropy(y.view(-1, y.shape[-1]), t.view(-1), reduction=""none"")\n        logp = loss * mask.view(-1)\n        logp = logp.sum()\n        count = mask.sum()\n        return logp / count, logp, count\n\n    def _before_loss(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(\n            output.view(output.size(0) * output.size(1), output.size(2))\n        )\n        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n\n    def init_state(self, x):\n        """"""Get an initial state for decoding.\n\n        Args:\n            x (torch.Tensor): The encoded feature tensor\n\n        Returns: initial state\n\n        """"""\n        bsz = 1\n        weight = next(self.parameters())\n        if self.rnn_type == ""LSTM"":\n            return (\n                weight.new_zeros(self.nlayers, bsz, self.nhid),\n                weight.new_zeros(self.nlayers, bsz, self.nhid),\n            )\n        else:\n            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n\n    def score(self, y, state, x):\n        """"""Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): 2D encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and next state for ys\n\n        """"""\n        y, new_state = self._before_loss(y[-1].view(1, 1), state)\n        logp = y.log_softmax(dim=-1).view(-1)\n        return logp, new_state\n'"
espnet/nets/pytorch_backend/lm/transformer.py,19,"b'""""""Transformer language model.""""""\n\nfrom typing import Any\nfrom typing import List\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom espnet.nets.lm_interface import LMInterface\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.encoder import Encoder\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.scorer_interface import BatchScorerInterface\n\n\nclass TransformerLM(nn.Module, LMInterface, BatchScorerInterface):\n    """"""Transformer language model.""""""\n\n    @staticmethod\n    def add_arguments(parser):\n        """"""Add arguments to command line argument parser.""""""\n        parser.add_argument(\n            ""--layer"", type=int, default=4, help=""Number of hidden layers""\n        )\n        parser.add_argument(\n            ""--unit"",\n            type=int,\n            default=1024,\n            help=""Number of hidden units in feedforward layer"",\n        )\n        parser.add_argument(\n            ""--att-unit"",\n            type=int,\n            default=256,\n            help=""Number of hidden units in attention layer"",\n        )\n        parser.add_argument(\n            ""--embed-unit"",\n            type=int,\n            default=128,\n            help=""Number of hidden units in embedding layer"",\n        )\n        parser.add_argument(\n            ""--head"", type=int, default=2, help=""Number of multi head attention""\n        )\n        parser.add_argument(\n            ""--dropout-rate"", type=float, default=0.5, help=""dropout probability""\n        )\n        parser.add_argument(\n            ""--pos-enc"",\n            default=""sinusoidal"",\n            choices=[""sinusoidal"", ""none""],\n            help=""positional encoding"",\n        )\n        return parser\n\n    def __init__(self, n_vocab, args):\n        """"""Initialize class.\n\n        Args:\n            n_vocab (int): The size of the vocabulary\n            args (argparse.Namespace): configurations. see py:method:`add_arguments`\n\n        """"""\n        nn.Module.__init__(self)\n        if args.pos_enc == ""sinusoidal"":\n            pos_enc_class = PositionalEncoding\n        elif args.pos_enc == ""none"":\n\n            def pos_enc_class(*args, **kwargs):\n                return nn.Sequential()  # indentity\n\n        else:\n            raise ValueError(f""unknown pos-enc option: {args.pos_enc}"")\n\n        self.embed = nn.Embedding(n_vocab, args.embed_unit)\n        self.encoder = Encoder(\n            idim=args.embed_unit,\n            attention_dim=args.att_unit,\n            attention_heads=args.head,\n            linear_units=args.unit,\n            num_blocks=args.layer,\n            dropout_rate=args.dropout_rate,\n            input_layer=""linear"",\n            pos_enc_class=pos_enc_class,\n        )\n        self.decoder = nn.Linear(args.att_unit, n_vocab)\n\n    def _target_mask(self, ys_in_pad):\n        ys_mask = ys_in_pad != 0\n        m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)\n        return ys_mask.unsqueeze(-2) & m\n\n    def forward(\n        self, x: torch.Tensor, t: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        """"""Compute LM loss value from buffer sequences.\n\n        Args:\n            x (torch.Tensor): Input ids. (batch, len)\n            t (torch.Tensor): Target ids. (batch, len)\n\n        Returns:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of\n                loss to backward (scalar),\n                negative log-likelihood of t: -log p(t) (scalar) and\n                the number of elements in x (scalar)\n\n        Notes:\n            The last two return values are used\n            in perplexity: p(t)^{-n} = exp(-log p(t) / n)\n\n        """"""\n        xm = x != 0\n        h, _ = self.encoder(self.embed(x), self._target_mask(x))\n        y = self.decoder(h)\n        loss = F.cross_entropy(y.view(-1, y.shape[-1]), t.view(-1), reduction=""none"")\n        mask = xm.to(dtype=loss.dtype)\n        logp = loss * mask.view(-1)\n        logp = logp.sum()\n        count = mask.sum()\n        return logp / count, logp, count\n\n    def score(\n        self, y: torch.Tensor, state: Any, x: torch.Tensor\n    ) -> Tuple[torch.Tensor, Any]:\n        """"""Score new token.\n\n        Args:\n            y (torch.Tensor): 1D torch.int64 prefix tokens.\n            state: Scorer state for prefix tokens\n            x (torch.Tensor): encoder feature that generates ys.\n\n        Returns:\n            tuple[torch.Tensor, Any]: Tuple of\n                torch.float32 scores for next token (n_vocab)\n                and next state for ys\n\n        """"""\n        y = y.unsqueeze(0)\n        h, _, cache = self.encoder.forward_one_step(\n            self.embed(y), self._target_mask(y), cache=state\n        )\n        h = self.decoder(h[:, -1])\n        logp = h.log_softmax(dim=-1).squeeze(0)\n        return logp, cache\n\n    # batch beam search API (see BatchScorerInterface)\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        """"""Score new token batch (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        """"""\n        # merge states\n        n_batch = len(ys)\n        n_layers = len(self.encoder.encoders)\n        if states[0] is None:\n            batch_state = None\n        else:\n            # transpose state of [batch, layer] into [layer, batch]\n            batch_state = [\n                torch.stack([states[b][i] for b in range(n_batch)])\n                for i in range(n_layers)\n            ]\n\n        # batch decoding\n        h, _, states = self.encoder.forward_one_step(\n            self.embed(ys), self._target_mask(ys), cache=batch_state\n        )\n        h = self.decoder(h[:, -1])\n        logp = h.log_softmax(dim=-1)\n\n        # transpose state of [layer, batch] into [batch, layer]\n        state_list = [[states[i][b] for i in range(n_layers)] for b in range(n_batch)]\n        return logp, state_list\n'"
espnet/nets/pytorch_backend/rnn/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/rnn/attentions.py,209,"b'""""""Attention modules for RNN.""""""\n\nimport math\nimport six\n\nimport torch\nimport torch.nn.functional as F\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\n\n\ndef _apply_attention_constraint(\n    e, last_attended_idx, backward_window=1, forward_window=3\n):\n    """"""Apply monotonic attention constraint.\n\n    This function apply the monotonic attention constraint\n    introduced in `Deep Voice 3: Scaling\n    Text-to-Speech with Convolutional Sequence Learning`_.\n\n    Args:\n        e (Tensor): Attention energy before applying softmax (1, T).\n        last_attended_idx (int): The index of the inputs of the last attended [0, T].\n        backward_window (int, optional): Backward window size in attention constraint.\n        forward_window (int, optional): Forward window size in attetion constraint.\n\n    Returns:\n        Tensor: Monotonic constrained attention energy (1, T).\n\n    .. _`Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning`:\n        https://arxiv.org/abs/1710.07654\n\n    """"""\n    if e.size(0) != 1:\n        raise NotImplementedError(""Batch attention constraining is not yet supported."")\n    backward_idx = last_attended_idx - backward_window\n    forward_idx = last_attended_idx + forward_window\n    if backward_idx > 0:\n        e[:, :backward_idx] = -float(""inf"")\n    if forward_idx < e.size(1):\n        e[:, forward_idx:] = -float(""inf"")\n    return e\n\n\nclass NoAtt(torch.nn.Module):\n    """"""No attention""""""\n\n    def __init__(self):\n        super(NoAtt, self).__init__()\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.c = None\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.c = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev):\n        """"""NoAtt forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, T_max, D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: dummy (does not use)\n        :param torch.Tensor att_prev: dummy (does not use)\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: previous attention weights\n        :rtype: torch.Tensor\n        """"""\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n\n        # initialize attention weight with uniform dist.\n        if att_prev is None:\n            # if no bias, 0 0-pad goes 0\n            mask = 1.0 - make_pad_mask(enc_hs_len).float()\n            att_prev = mask / mask.new(enc_hs_len).unsqueeze(-1)\n            att_prev = att_prev.to(self.enc_h)\n            self.c = torch.sum(\n                self.enc_h * att_prev.view(batch, self.h_length, 1), dim=1\n            )\n\n        return self.c, att_prev\n\n\nclass AttDot(torch.nn.Module):\n    """"""Dot product attention\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_enc_h\n    """"""\n\n    def __init__(self, eprojs, dunits, att_dim, han_mode=False):\n        super(AttDot, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0):\n        """"""AttDot forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: dummy (does not use)\n        :param torch.Tensor att_prev: dummy (does not use)\n        :param float scaling: scaling parameter before applying softmax\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: previous attention weight (B x T_max)\n        :rtype: torch.Tensor\n        """"""\n\n        batch = enc_hs_pad.size(0)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = torch.tanh(self.mlp_enc(self.enc_h))\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        e = torch.sum(\n            self.pre_compute_enc_h\n            * torch.tanh(self.mlp_dec(dec_z)).view(batch, 1, self.att_dim),\n            dim=2,\n        )  # utt x frame\n\n        # NOTE consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n        w = F.softmax(scaling * e, dim=1)\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n        return c, w\n\n\nclass AttAdd(torch.nn.Module):\n    """"""Additive attention\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_enc_h\n    """"""\n\n    def __init__(self, eprojs, dunits, att_dim, han_mode=False):\n        super(AttAdd, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.gvec = torch.nn.Linear(att_dim, 1)\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0):\n        """"""AttAdd forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev: dummy (does not use)\n        :param float scaling: scaling parameter before applying softmax\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: previous attention weights (B x T_max)\n        :rtype: torch.Tensor\n        """"""\n\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(torch.tanh(self.pre_compute_enc_h + dec_z_tiled)).squeeze(2)\n\n        # NOTE consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n        w = F.softmax(scaling * e, dim=1)\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n\n        return c, w\n\n\nclass AttLoc(torch.nn.Module):\n    """"""location-aware attention module.\n\n    Reference: Attention-Based Models for Speech Recognition\n        (https://arxiv.org/pdf/1506.07503.pdf)\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_enc_h\n    """"""\n\n    def __init__(\n        self, eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False\n    ):\n        super(AttLoc, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.mlp_att = torch.nn.Linear(aconv_chans, att_dim, bias=False)\n        self.loc_conv = torch.nn.Conv2d(\n            1,\n            aconv_chans,\n            (1, 2 * aconv_filts + 1),\n            padding=(0, aconv_filts),\n            bias=False,\n        )\n        self.gvec = torch.nn.Linear(att_dim, 1)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(\n        self,\n        enc_hs_pad,\n        enc_hs_len,\n        dec_z,\n        att_prev,\n        scaling=2.0,\n        last_attended_idx=None,\n        backward_window=1,\n        forward_window=3,\n    ):\n        """"""Calcualte AttLoc forward propagation.\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev: previous attention weight (B x T_max)\n        :param float scaling: scaling parameter before applying softmax\n        :param torch.Tensor forward_window:\n            forward window size when constraining attention\n        :param int last_attended_idx: index of the inputs of the last attended\n        :param int backward_window: backward window size in attention constraint\n        :param int forward_window: forward window size in attetion constraint\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: previous attention weights (B x T_max)\n        :rtype: torch.Tensor\n        """"""\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        # initialize attention weight with uniform dist.\n        if att_prev is None:\n            # if no bias, 0 0-pad goes 0\n            att_prev = 1.0 - make_pad_mask(enc_hs_len).to(\n                device=dec_z.device, dtype=dec_z.dtype\n            )\n            att_prev = att_prev / att_prev.new(enc_hs_len).unsqueeze(-1)\n\n        # att_prev: utt x frame -> utt x 1 x 1 x frame\n        # -> utt x att_conv_chans x 1 x frame\n        att_conv = self.loc_conv(att_prev.view(batch, 1, 1, self.h_length))\n        # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans\n        att_conv = att_conv.squeeze(2).transpose(1, 2)\n        # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim\n        att_conv = self.mlp_att(att_conv)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(\n            torch.tanh(att_conv + self.pre_compute_enc_h + dec_z_tiled)\n        ).squeeze(2)\n\n        # NOTE: consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n\n        # apply monotonic attention constraint (mainly for TTS)\n        if last_attended_idx is not None:\n            e = _apply_attention_constraint(\n                e, last_attended_idx, backward_window, forward_window\n            )\n\n        w = F.softmax(scaling * e, dim=1)\n\n        # weighted sum over flames\n        # utt x hdim\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n\n        return c, w\n\n\nclass AttCov(torch.nn.Module):\n    """"""Coverage mechanism attention\n\n    Reference: Get To The Point: Summarization with Pointer-Generator Network\n       (https://arxiv.org/abs/1704.04368)\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_enc_h\n    """"""\n\n    def __init__(self, eprojs, dunits, att_dim, han_mode=False):\n        super(AttCov, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.wvec = torch.nn.Linear(1, att_dim)\n        self.gvec = torch.nn.Linear(att_dim, 1)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev_list, scaling=2.0):\n        """"""AttCov forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param list att_prev_list: list of previous attention weight\n        :param float scaling: scaling parameter before applying softmax\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: list of previous attention weights\n        :rtype: list\n        """"""\n\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        # initialize attention weight with uniform dist.\n        if att_prev_list is None:\n            # if no bias, 0 0-pad goes 0\n            att_prev_list = to_device(self, (1.0 - make_pad_mask(enc_hs_len).float()))\n            att_prev_list = [\n                att_prev_list / att_prev_list.new(enc_hs_len).unsqueeze(-1)\n            ]\n\n        # att_prev_list: L\' * [B x T] => cov_vec B x T\n        cov_vec = sum(att_prev_list)\n        # cov_vec: B x T => B x T x 1 => B x T x att_dim\n        cov_vec = self.wvec(cov_vec.unsqueeze(-1))\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(\n            torch.tanh(cov_vec + self.pre_compute_enc_h + dec_z_tiled)\n        ).squeeze(2)\n\n        # NOTE consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n        w = F.softmax(scaling * e, dim=1)\n        att_prev_list += [w]\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n\n        return c, att_prev_list\n\n\nclass AttLoc2D(torch.nn.Module):\n    """"""2D location-aware attention\n\n    This attention is an extended version of location aware attention.\n    It take not only one frame before attention weights,\n    but also earlier frames into account.\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    :param int att_win: attention window size (default=5)\n    :param bool han_mode:\n        flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\n    """"""\n\n    def __init__(\n        self, eprojs, dunits, att_dim, att_win, aconv_chans, aconv_filts, han_mode=False\n    ):\n        super(AttLoc2D, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.mlp_att = torch.nn.Linear(aconv_chans, att_dim, bias=False)\n        self.loc_conv = torch.nn.Conv2d(\n            1,\n            aconv_chans,\n            (att_win, 2 * aconv_filts + 1),\n            padding=(0, aconv_filts),\n            bias=False,\n        )\n        self.gvec = torch.nn.Linear(att_dim, 1)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.aconv_chans = aconv_chans\n        self.att_win = att_win\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0):\n        """"""AttLoc2D forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev: previous attention weight (B x att_win x T_max)\n        :param float scaling: scaling parameter before applying softmax\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: previous attention weights (B x att_win x T_max)\n        :rtype: torch.Tensor\n        """"""\n\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        # initialize attention weight with uniform dist.\n        if att_prev is None:\n            # B * [Li x att_win]\n            # if no bias, 0 0-pad goes 0\n            att_prev = to_device(self, (1.0 - make_pad_mask(enc_hs_len).float()))\n            att_prev = att_prev / att_prev.new(enc_hs_len).unsqueeze(-1)\n            att_prev = att_prev.unsqueeze(1).expand(-1, self.att_win, -1)\n\n        # att_prev: B x att_win x Tmax -> B x 1 x att_win x Tmax -> B x C x 1 x Tmax\n        att_conv = self.loc_conv(att_prev.unsqueeze(1))\n        # att_conv: B x C x 1 x Tmax -> B x Tmax x C\n        att_conv = att_conv.squeeze(2).transpose(1, 2)\n        # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim\n        att_conv = self.mlp_att(att_conv)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(\n            torch.tanh(att_conv + self.pre_compute_enc_h + dec_z_tiled)\n        ).squeeze(2)\n\n        # NOTE consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n        w = F.softmax(scaling * e, dim=1)\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n\n        # update att_prev: B x att_win x Tmax -> B x att_win+1 x Tmax\n        # -> B x att_win x Tmax\n        att_prev = torch.cat([att_prev, w.unsqueeze(1)], dim=1)\n        att_prev = att_prev[:, 1:]\n\n        return c, att_prev\n\n\nclass AttLocRec(torch.nn.Module):\n    """"""location-aware recurrent attention\n\n    This attention is an extended version of location aware attention.\n    With the use of RNN,\n    it take the effect of the history of attention weights into account.\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    :param bool han_mode:\n        flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\n    """"""\n\n    def __init__(\n        self, eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False\n    ):\n        super(AttLocRec, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.loc_conv = torch.nn.Conv2d(\n            1,\n            aconv_chans,\n            (1, 2 * aconv_filts + 1),\n            padding=(0, aconv_filts),\n            bias=False,\n        )\n        self.att_lstm = torch.nn.LSTMCell(aconv_chans, att_dim, bias=False)\n        self.gvec = torch.nn.Linear(att_dim, 1)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev_states, scaling=2.0):\n        """"""AttLocRec forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param tuple att_prev_states: previous attention weight and lstm states\n                                      ((B, T_max), ((B, att_dim), (B, att_dim)))\n        :param float scaling: scaling parameter before applying softmax\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: previous attention weights and lstm states (w, (hx, cx))\n                 ((B, T_max), ((B, att_dim), (B, att_dim)))\n        :rtype: tuple\n        """"""\n\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        if att_prev_states is None:\n            # initialize attention weight with uniform dist.\n            # if no bias, 0 0-pad goes 0\n            att_prev = to_device(self, (1.0 - make_pad_mask(enc_hs_len).float()))\n            att_prev = att_prev / att_prev.new(enc_hs_len).unsqueeze(-1)\n\n            # initialize lstm states\n            att_h = enc_hs_pad.new_zeros(batch, self.att_dim)\n            att_c = enc_hs_pad.new_zeros(batch, self.att_dim)\n            att_states = (att_h, att_c)\n        else:\n            att_prev = att_prev_states[0]\n            att_states = att_prev_states[1]\n\n        # B x 1 x 1 x T -> B x C x 1 x T\n        att_conv = self.loc_conv(att_prev.view(batch, 1, 1, self.h_length))\n        # apply non-linear\n        att_conv = F.relu(att_conv)\n        # B x C x 1 x T -> B x C x 1 x 1 -> B x C\n        att_conv = F.max_pool2d(att_conv, (1, att_conv.size(3))).view(batch, -1)\n\n        att_h, att_c = self.att_lstm(att_conv, att_states)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(\n            torch.tanh(att_h.unsqueeze(1) + self.pre_compute_enc_h + dec_z_tiled)\n        ).squeeze(2)\n\n        # NOTE consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n        w = F.softmax(scaling * e, dim=1)\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n\n        return c, (w, (att_h, att_c))\n\n\nclass AttCovLoc(torch.nn.Module):\n    """"""Coverage mechanism location aware attention\n\n    This attention is a combination of coverage and location-aware attentions.\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    :param bool han_mode:\n        flag to swith on mode of hierarchical attention and not store pre_compute_enc_h\n    """"""\n\n    def __init__(\n        self, eprojs, dunits, att_dim, aconv_chans, aconv_filts, han_mode=False\n    ):\n        super(AttCovLoc, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.mlp_att = torch.nn.Linear(aconv_chans, att_dim, bias=False)\n        self.loc_conv = torch.nn.Conv2d(\n            1,\n            aconv_chans,\n            (1, 2 * aconv_filts + 1),\n            padding=(0, aconv_filts),\n            bias=False,\n        )\n        self.gvec = torch.nn.Linear(att_dim, 1)\n\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.aconv_chans = aconv_chans\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev_list, scaling=2.0):\n        """"""AttCovLoc forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param list att_prev_list: list of previous attention weight\n        :param float scaling: scaling parameter before applying softmax\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: list of previous attention weights\n        :rtype: list\n        """"""\n\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        # initialize attention weight with uniform dist.\n        if att_prev_list is None:\n            # if no bias, 0 0-pad goes 0\n            mask = 1.0 - make_pad_mask(enc_hs_len).float()\n            att_prev_list = [to_device(self, mask / mask.new(enc_hs_len).unsqueeze(-1))]\n\n        # att_prev_list: L\' * [B x T] => cov_vec B x T\n        cov_vec = sum(att_prev_list)\n\n        # cov_vec: B x T -> B x 1 x 1 x T -> B x C x 1 x T\n        att_conv = self.loc_conv(cov_vec.view(batch, 1, 1, self.h_length))\n        # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans\n        att_conv = att_conv.squeeze(2).transpose(1, 2)\n        # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim\n        att_conv = self.mlp_att(att_conv)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(\n            torch.tanh(att_conv + self.pre_compute_enc_h + dec_z_tiled)\n        ).squeeze(2)\n\n        # NOTE consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n        w = F.softmax(scaling * e, dim=1)\n        att_prev_list += [w]\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n\n        return c, att_prev_list\n\n\nclass AttMultiHeadDot(torch.nn.Module):\n    """"""Multi head dot product attention\n\n    Reference: Attention is all you need\n        (https://arxiv.org/abs/1706.03762)\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int aheads: # heads of multi head attention\n    :param int att_dim_k: dimension k in multi head attention\n    :param int att_dim_v: dimension v in multi head attention\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_k and pre_compute_v\n    """"""\n\n    def __init__(self, eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False):\n        super(AttMultiHeadDot, self).__init__()\n        self.mlp_q = torch.nn.ModuleList()\n        self.mlp_k = torch.nn.ModuleList()\n        self.mlp_v = torch.nn.ModuleList()\n        for _ in six.moves.range(aheads):\n            self.mlp_q += [torch.nn.Linear(dunits, att_dim_k)]\n            self.mlp_k += [torch.nn.Linear(eprojs, att_dim_k, bias=False)]\n            self.mlp_v += [torch.nn.Linear(eprojs, att_dim_v, bias=False)]\n        self.mlp_o = torch.nn.Linear(aheads * att_dim_v, eprojs, bias=False)\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.aheads = aheads\n        self.att_dim_k = att_dim_k\n        self.att_dim_v = att_dim_v\n        self.scaling = 1.0 / math.sqrt(att_dim_k)\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev):\n        """"""AttMultiHeadDot forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev: dummy (does not use)\n        :return: attention weighted encoder state (B x D_enc)\n        :rtype: torch.Tensor\n        :return: list of previous attention weight (B x T_max) * aheads\n        :rtype: list\n        """"""\n\n        batch = enc_hs_pad.size(0)\n        # pre-compute all k and v outside the decoder loop\n        if self.pre_compute_k is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_k = [\n                torch.tanh(self.mlp_k[h](self.enc_h))\n                for h in six.moves.range(self.aheads)\n            ]\n\n        if self.pre_compute_v is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_v = [\n                self.mlp_v[h](self.enc_h) for h in six.moves.range(self.aheads)\n            ]\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        c = []\n        w = []\n        for h in six.moves.range(self.aheads):\n            e = torch.sum(\n                self.pre_compute_k[h]\n                * torch.tanh(self.mlp_q[h](dec_z)).view(batch, 1, self.att_dim_k),\n                dim=2,\n            )  # utt x frame\n\n            # NOTE consider zero padding when compute w.\n            if self.mask is None:\n                self.mask = to_device(self, make_pad_mask(enc_hs_len))\n            e.masked_fill_(self.mask, -float(""inf""))\n            w += [F.softmax(self.scaling * e, dim=1)]\n\n            # weighted sum over flames\n            # utt x hdim\n            # NOTE use bmm instead of sum(*)\n            c += [\n                torch.sum(\n                    self.pre_compute_v[h] * w[h].view(batch, self.h_length, 1), dim=1\n                )\n            ]\n\n        # concat all of c\n        c = self.mlp_o(torch.cat(c, dim=1))\n\n        return c, w\n\n\nclass AttMultiHeadAdd(torch.nn.Module):\n    """"""Multi head additive attention\n\n    Reference: Attention is all you need\n        (https://arxiv.org/abs/1706.03762)\n\n    This attention is multi head attention using additive attention for each head.\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int aheads: # heads of multi head attention\n    :param int att_dim_k: dimension k in multi head attention\n    :param int att_dim_v: dimension v in multi head attention\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_k and pre_compute_v\n    """"""\n\n    def __init__(self, eprojs, dunits, aheads, att_dim_k, att_dim_v, han_mode=False):\n        super(AttMultiHeadAdd, self).__init__()\n        self.mlp_q = torch.nn.ModuleList()\n        self.mlp_k = torch.nn.ModuleList()\n        self.mlp_v = torch.nn.ModuleList()\n        self.gvec = torch.nn.ModuleList()\n        for _ in six.moves.range(aheads):\n            self.mlp_q += [torch.nn.Linear(dunits, att_dim_k)]\n            self.mlp_k += [torch.nn.Linear(eprojs, att_dim_k, bias=False)]\n            self.mlp_v += [torch.nn.Linear(eprojs, att_dim_v, bias=False)]\n            self.gvec += [torch.nn.Linear(att_dim_k, 1)]\n        self.mlp_o = torch.nn.Linear(aheads * att_dim_v, eprojs, bias=False)\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.aheads = aheads\n        self.att_dim_k = att_dim_k\n        self.att_dim_v = att_dim_v\n        self.scaling = 1.0 / math.sqrt(att_dim_k)\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev):\n        """"""AttMultiHeadAdd forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev: dummy (does not use)\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: list of previous attention weight (B x T_max) * aheads\n        :rtype: list\n        """"""\n\n        batch = enc_hs_pad.size(0)\n        # pre-compute all k and v outside the decoder loop\n        if self.pre_compute_k is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_k = [\n                self.mlp_k[h](self.enc_h) for h in six.moves.range(self.aheads)\n            ]\n\n        if self.pre_compute_v is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_v = [\n                self.mlp_v[h](self.enc_h) for h in six.moves.range(self.aheads)\n            ]\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        c = []\n        w = []\n        for h in six.moves.range(self.aheads):\n            e = self.gvec[h](\n                torch.tanh(\n                    self.pre_compute_k[h]\n                    + self.mlp_q[h](dec_z).view(batch, 1, self.att_dim_k)\n                )\n            ).squeeze(2)\n\n            # NOTE consider zero padding when compute w.\n            if self.mask is None:\n                self.mask = to_device(self, make_pad_mask(enc_hs_len))\n            e.masked_fill_(self.mask, -float(""inf""))\n            w += [F.softmax(self.scaling * e, dim=1)]\n\n            # weighted sum over flames\n            # utt x hdim\n            # NOTE use bmm instead of sum(*)\n            c += [\n                torch.sum(\n                    self.pre_compute_v[h] * w[h].view(batch, self.h_length, 1), dim=1\n                )\n            ]\n\n        # concat all of c\n        c = self.mlp_o(torch.cat(c, dim=1))\n\n        return c, w\n\n\nclass AttMultiHeadLoc(torch.nn.Module):\n    """"""Multi head location based attention\n\n    Reference: Attention is all you need\n        (https://arxiv.org/abs/1706.03762)\n\n    This attention is multi head attention using location-aware attention for each head.\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int aheads: # heads of multi head attention\n    :param int att_dim_k: dimension k in multi head attention\n    :param int att_dim_v: dimension v in multi head attention\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_k and pre_compute_v\n    """"""\n\n    def __init__(\n        self,\n        eprojs,\n        dunits,\n        aheads,\n        att_dim_k,\n        att_dim_v,\n        aconv_chans,\n        aconv_filts,\n        han_mode=False,\n    ):\n        super(AttMultiHeadLoc, self).__init__()\n        self.mlp_q = torch.nn.ModuleList()\n        self.mlp_k = torch.nn.ModuleList()\n        self.mlp_v = torch.nn.ModuleList()\n        self.gvec = torch.nn.ModuleList()\n        self.loc_conv = torch.nn.ModuleList()\n        self.mlp_att = torch.nn.ModuleList()\n        for _ in six.moves.range(aheads):\n            self.mlp_q += [torch.nn.Linear(dunits, att_dim_k)]\n            self.mlp_k += [torch.nn.Linear(eprojs, att_dim_k, bias=False)]\n            self.mlp_v += [torch.nn.Linear(eprojs, att_dim_v, bias=False)]\n            self.gvec += [torch.nn.Linear(att_dim_k, 1)]\n            self.loc_conv += [\n                torch.nn.Conv2d(\n                    1,\n                    aconv_chans,\n                    (1, 2 * aconv_filts + 1),\n                    padding=(0, aconv_filts),\n                    bias=False,\n                )\n            ]\n            self.mlp_att += [torch.nn.Linear(aconv_chans, att_dim_k, bias=False)]\n        self.mlp_o = torch.nn.Linear(aheads * att_dim_v, eprojs, bias=False)\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.aheads = aheads\n        self.att_dim_k = att_dim_k\n        self.att_dim_v = att_dim_v\n        self.scaling = 1.0 / math.sqrt(att_dim_k)\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev, scaling=2.0):\n        """"""AttMultiHeadLoc forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev:\n            list of previous attention weight (B x T_max) * aheads\n        :param float scaling: scaling parameter before applying softmax\n        :return: attention weighted encoder state (B x D_enc)\n        :rtype: torch.Tensor\n        :return: list of previous attention weight (B x T_max) * aheads\n        :rtype: list\n        """"""\n\n        batch = enc_hs_pad.size(0)\n        # pre-compute all k and v outside the decoder loop\n        if self.pre_compute_k is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_k = [\n                self.mlp_k[h](self.enc_h) for h in six.moves.range(self.aheads)\n            ]\n\n        if self.pre_compute_v is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_v = [\n                self.mlp_v[h](self.enc_h) for h in six.moves.range(self.aheads)\n            ]\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        if att_prev is None:\n            att_prev = []\n            for _ in six.moves.range(self.aheads):\n                # if no bias, 0 0-pad goes 0\n                mask = 1.0 - make_pad_mask(enc_hs_len).float()\n                att_prev += [to_device(self, mask / mask.new(enc_hs_len).unsqueeze(-1))]\n\n        c = []\n        w = []\n        for h in six.moves.range(self.aheads):\n            att_conv = self.loc_conv[h](att_prev[h].view(batch, 1, 1, self.h_length))\n            att_conv = att_conv.squeeze(2).transpose(1, 2)\n            att_conv = self.mlp_att[h](att_conv)\n\n            e = self.gvec[h](\n                torch.tanh(\n                    self.pre_compute_k[h]\n                    + att_conv\n                    + self.mlp_q[h](dec_z).view(batch, 1, self.att_dim_k)\n                )\n            ).squeeze(2)\n\n            # NOTE consider zero padding when compute w.\n            if self.mask is None:\n                self.mask = to_device(self, make_pad_mask(enc_hs_len))\n            e.masked_fill_(self.mask, -float(""inf""))\n            w += [F.softmax(scaling * e, dim=1)]\n\n            # weighted sum over flames\n            # utt x hdim\n            # NOTE use bmm instead of sum(*)\n            c += [\n                torch.sum(\n                    self.pre_compute_v[h] * w[h].view(batch, self.h_length, 1), dim=1\n                )\n            ]\n\n        # concat all of c\n        c = self.mlp_o(torch.cat(c, dim=1))\n\n        return c, w\n\n\nclass AttMultiHeadMultiResLoc(torch.nn.Module):\n    """"""Multi head multi resolution location based attention\n\n    Reference: Attention is all you need\n        (https://arxiv.org/abs/1706.03762)\n\n    This attention is multi head attention using location-aware attention for each head.\n    Furthermore, it uses different filter size for each head.\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int aheads: # heads of multi head attention\n    :param int att_dim_k: dimension k in multi head attention\n    :param int att_dim_v: dimension v in multi head attention\n    :param int aconv_chans: maximum # channels of attention convolution\n        each head use #ch = aconv_chans * (head + 1) / aheads\n        e.g. aheads=4, aconv_chans=100 => filter size = 25, 50, 75, 100\n    :param int aconv_filts: filter size of attention convolution\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n        and not store pre_compute_k and pre_compute_v\n    """"""\n\n    def __init__(\n        self,\n        eprojs,\n        dunits,\n        aheads,\n        att_dim_k,\n        att_dim_v,\n        aconv_chans,\n        aconv_filts,\n        han_mode=False,\n    ):\n        super(AttMultiHeadMultiResLoc, self).__init__()\n        self.mlp_q = torch.nn.ModuleList()\n        self.mlp_k = torch.nn.ModuleList()\n        self.mlp_v = torch.nn.ModuleList()\n        self.gvec = torch.nn.ModuleList()\n        self.loc_conv = torch.nn.ModuleList()\n        self.mlp_att = torch.nn.ModuleList()\n        for h in six.moves.range(aheads):\n            self.mlp_q += [torch.nn.Linear(dunits, att_dim_k)]\n            self.mlp_k += [torch.nn.Linear(eprojs, att_dim_k, bias=False)]\n            self.mlp_v += [torch.nn.Linear(eprojs, att_dim_v, bias=False)]\n            self.gvec += [torch.nn.Linear(att_dim_k, 1)]\n            afilts = aconv_filts * (h + 1) // aheads\n            self.loc_conv += [\n                torch.nn.Conv2d(\n                    1, aconv_chans, (1, 2 * afilts + 1), padding=(0, afilts), bias=False\n                )\n            ]\n            self.mlp_att += [torch.nn.Linear(aconv_chans, att_dim_k, bias=False)]\n        self.mlp_o = torch.nn.Linear(aheads * att_dim_v, eprojs, bias=False)\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.aheads = aheads\n        self.att_dim_k = att_dim_k\n        self.att_dim_v = att_dim_v\n        self.scaling = 1.0 / math.sqrt(att_dim_k)\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n        self.han_mode = han_mode\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_k = None\n        self.pre_compute_v = None\n        self.mask = None\n\n    def forward(self, enc_hs_pad, enc_hs_len, dec_z, att_prev):\n        """"""AttMultiHeadMultiResLoc forward\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev: list of previous attention weight\n            (B x T_max) * aheads\n        :return: attention weighted encoder state (B x D_enc)\n        :rtype: torch.Tensor\n        :return: list of previous attention weight (B x T_max) * aheads\n        :rtype: list\n        """"""\n\n        batch = enc_hs_pad.size(0)\n        # pre-compute all k and v outside the decoder loop\n        if self.pre_compute_k is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_k = [\n                self.mlp_k[h](self.enc_h) for h in six.moves.range(self.aheads)\n            ]\n\n        if self.pre_compute_v is None or self.han_mode:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_v = [\n                self.mlp_v[h](self.enc_h) for h in six.moves.range(self.aheads)\n            ]\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        if att_prev is None:\n            att_prev = []\n            for _ in six.moves.range(self.aheads):\n                # if no bias, 0 0-pad goes 0\n                mask = 1.0 - make_pad_mask(enc_hs_len).float()\n                att_prev += [to_device(self, mask / mask.new(enc_hs_len).unsqueeze(-1))]\n\n        c = []\n        w = []\n        for h in six.moves.range(self.aheads):\n            att_conv = self.loc_conv[h](att_prev[h].view(batch, 1, 1, self.h_length))\n            att_conv = att_conv.squeeze(2).transpose(1, 2)\n            att_conv = self.mlp_att[h](att_conv)\n\n            e = self.gvec[h](\n                torch.tanh(\n                    self.pre_compute_k[h]\n                    + att_conv\n                    + self.mlp_q[h](dec_z).view(batch, 1, self.att_dim_k)\n                )\n            ).squeeze(2)\n\n            # NOTE consider zero padding when compute w.\n            if self.mask is None:\n                self.mask = to_device(self, make_pad_mask(enc_hs_len))\n            e.masked_fill_(self.mask, -float(""inf""))\n            w += [F.softmax(self.scaling * e, dim=1)]\n\n            # weighted sum over flames\n            # utt x hdim\n            # NOTE use bmm instead of sum(*)\n            c += [\n                torch.sum(\n                    self.pre_compute_v[h] * w[h].view(batch, self.h_length, 1), dim=1\n                )\n            ]\n\n        # concat all of c\n        c = self.mlp_o(torch.cat(c, dim=1))\n\n        return c, w\n\n\nclass AttForward(torch.nn.Module):\n    """"""Forward attention module.\n\n    Reference:\n    Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\n        (https://arxiv.org/pdf/1807.06736.pdf)\n\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    """"""\n\n    def __init__(self, eprojs, dunits, att_dim, aconv_chans, aconv_filts):\n        super(AttForward, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eprojs, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.mlp_att = torch.nn.Linear(aconv_chans, att_dim, bias=False)\n        self.loc_conv = torch.nn.Conv2d(\n            1,\n            aconv_chans,\n            (1, 2 * aconv_filts + 1),\n            padding=(0, aconv_filts),\n            bias=False,\n        )\n        self.gvec = torch.nn.Linear(att_dim, 1)\n        self.dunits = dunits\n        self.eprojs = eprojs\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def reset(self):\n        """"""reset states""""""\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n\n    def forward(\n        self,\n        enc_hs_pad,\n        enc_hs_len,\n        dec_z,\n        att_prev,\n        scaling=1.0,\n        last_attended_idx=None,\n        backward_window=1,\n        forward_window=3,\n    ):\n        """"""Calculate AttForward forward propagation.\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B x T_max x D_enc)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B x D_dec)\n        :param torch.Tensor att_prev: attention weights of previous step\n        :param float scaling: scaling parameter before applying softmax\n        :param int last_attended_idx: index of the inputs of the last attended\n        :param int backward_window: backward window size in attention constraint\n        :param int forward_window: forward window size in attetion constraint\n        :return: attention weighted encoder state (B, D_enc)\n        :rtype: torch.Tensor\n        :return: previous attention weights (B x T_max)\n        :rtype: torch.Tensor\n        """"""\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        if att_prev is None:\n            # initial attention will be [1, 0, 0, ...]\n            att_prev = enc_hs_pad.new_zeros(*enc_hs_pad.size()[:2])\n            att_prev[:, 0] = 1.0\n\n        # att_prev: utt x frame -> utt x 1 x 1 x frame\n        # -> utt x att_conv_chans x 1 x frame\n        att_conv = self.loc_conv(att_prev.view(batch, 1, 1, self.h_length))\n        # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans\n        att_conv = att_conv.squeeze(2).transpose(1, 2)\n        # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim\n        att_conv = self.mlp_att(att_conv)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).unsqueeze(1)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(\n            torch.tanh(self.pre_compute_enc_h + dec_z_tiled + att_conv)\n        ).squeeze(2)\n\n        # NOTE: consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n\n        # apply monotonic attention constraint (mainly for TTS)\n        if last_attended_idx is not None:\n            e = _apply_attention_constraint(\n                e, last_attended_idx, backward_window, forward_window\n            )\n\n        w = F.softmax(scaling * e, dim=1)\n\n        # forward attention\n        att_prev_shift = F.pad(att_prev, (1, 0))[:, :-1]\n        w = (att_prev + att_prev_shift) * w\n        # NOTE: clamp is needed to avoid nan gradient\n        w = F.normalize(torch.clamp(w, 1e-6), p=1, dim=1)\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.unsqueeze(-1), dim=1)\n\n        return c, w\n\n\nclass AttForwardTA(torch.nn.Module):\n    """"""Forward attention with transition agent module.\n\n    Reference:\n    Forward attention in sequence-to-sequence acoustic modeling for speech synthesis\n        (https://arxiv.org/pdf/1807.06736.pdf)\n\n    :param int eunits: # units of encoder\n    :param int dunits: # units of decoder\n    :param int att_dim: attention dimension\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    :param int odim: output dimension\n    """"""\n\n    def __init__(self, eunits, dunits, att_dim, aconv_chans, aconv_filts, odim):\n        super(AttForwardTA, self).__init__()\n        self.mlp_enc = torch.nn.Linear(eunits, att_dim)\n        self.mlp_dec = torch.nn.Linear(dunits, att_dim, bias=False)\n        self.mlp_ta = torch.nn.Linear(eunits + dunits + odim, 1)\n        self.mlp_att = torch.nn.Linear(aconv_chans, att_dim, bias=False)\n        self.loc_conv = torch.nn.Conv2d(\n            1,\n            aconv_chans,\n            (1, 2 * aconv_filts + 1),\n            padding=(0, aconv_filts),\n            bias=False,\n        )\n        self.gvec = torch.nn.Linear(att_dim, 1)\n        self.dunits = dunits\n        self.eunits = eunits\n        self.att_dim = att_dim\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n        self.trans_agent_prob = 0.5\n\n    def reset(self):\n        self.h_length = None\n        self.enc_h = None\n        self.pre_compute_enc_h = None\n        self.mask = None\n        self.trans_agent_prob = 0.5\n\n    def forward(\n        self,\n        enc_hs_pad,\n        enc_hs_len,\n        dec_z,\n        att_prev,\n        out_prev,\n        scaling=1.0,\n        last_attended_idx=None,\n        backward_window=1,\n        forward_window=3,\n    ):\n        """"""Calculate AttForwardTA forward propagation.\n\n        :param torch.Tensor enc_hs_pad: padded encoder hidden state (B, Tmax, eunits)\n        :param list enc_hs_len: padded encoder hidden state length (B)\n        :param torch.Tensor dec_z: decoder hidden state (B, dunits)\n        :param torch.Tensor att_prev: attention weights of previous step\n        :param torch.Tensor out_prev: decoder outputs of previous step (B, odim)\n        :param float scaling: scaling parameter before applying softmax\n        :param int last_attended_idx: index of the inputs of the last attended\n        :param int backward_window: backward window size in attention constraint\n        :param int forward_window: forward window size in attetion constraint\n        :return: attention weighted encoder state (B, dunits)\n        :rtype: torch.Tensor\n        :return: previous attention weights (B, Tmax)\n        :rtype: torch.Tensor\n        """"""\n        batch = len(enc_hs_pad)\n        # pre-compute all h outside the decoder loop\n        if self.pre_compute_enc_h is None:\n            self.enc_h = enc_hs_pad  # utt x frame x hdim\n            self.h_length = self.enc_h.size(1)\n            # utt x frame x att_dim\n            self.pre_compute_enc_h = self.mlp_enc(self.enc_h)\n\n        if dec_z is None:\n            dec_z = enc_hs_pad.new_zeros(batch, self.dunits)\n        else:\n            dec_z = dec_z.view(batch, self.dunits)\n\n        if att_prev is None:\n            # initial attention will be [1, 0, 0, ...]\n            att_prev = enc_hs_pad.new_zeros(*enc_hs_pad.size()[:2])\n            att_prev[:, 0] = 1.0\n\n        # att_prev: utt x frame -> utt x 1 x 1 x frame\n        # -> utt x att_conv_chans x 1 x frame\n        att_conv = self.loc_conv(att_prev.view(batch, 1, 1, self.h_length))\n        # att_conv: utt x att_conv_chans x 1 x frame -> utt x frame x att_conv_chans\n        att_conv = att_conv.squeeze(2).transpose(1, 2)\n        # att_conv: utt x frame x att_conv_chans -> utt x frame x att_dim\n        att_conv = self.mlp_att(att_conv)\n\n        # dec_z_tiled: utt x frame x att_dim\n        dec_z_tiled = self.mlp_dec(dec_z).view(batch, 1, self.att_dim)\n\n        # dot with gvec\n        # utt x frame x att_dim -> utt x frame\n        e = self.gvec(\n            torch.tanh(att_conv + self.pre_compute_enc_h + dec_z_tiled)\n        ).squeeze(2)\n\n        # NOTE consider zero padding when compute w.\n        if self.mask is None:\n            self.mask = to_device(self, make_pad_mask(enc_hs_len))\n        e.masked_fill_(self.mask, -float(""inf""))\n\n        # apply monotonic attention constraint (mainly for TTS)\n        if last_attended_idx is not None:\n            e = _apply_attention_constraint(\n                e, last_attended_idx, backward_window, forward_window\n            )\n\n        w = F.softmax(scaling * e, dim=1)\n\n        # forward attention\n        att_prev_shift = F.pad(att_prev, (1, 0))[:, :-1]\n        w = (\n            self.trans_agent_prob * att_prev\n            + (1 - self.trans_agent_prob) * att_prev_shift\n        ) * w\n        # NOTE: clamp is needed to avoid nan gradient\n        w = F.normalize(torch.clamp(w, 1e-6), p=1, dim=1)\n\n        # weighted sum over flames\n        # utt x hdim\n        # NOTE use bmm instead of sum(*)\n        c = torch.sum(self.enc_h * w.view(batch, self.h_length, 1), dim=1)\n\n        # update transition agent prob\n        self.trans_agent_prob = torch.sigmoid(\n            self.mlp_ta(torch.cat([c, out_prev, dec_z], dim=1))\n        )\n\n        return c, w\n\n\ndef att_for(args, num_att=1, han_mode=False):\n    """"""Instantiates an attention module given the program arguments\n\n    :param Namespace args: The arguments\n    :param int num_att: number of attention modules\n        (in multi-speaker case, it can be 2 or more)\n    :param bool han_mode: switch on/off mode of hierarchical attention network (HAN)\n    :rtype torch.nn.Module\n    :return: The attention module\n    """"""\n    att_list = torch.nn.ModuleList()\n    num_encs = getattr(args, ""num_encs"", 1)  # use getattr to keep compatibility\n    aheads = getattr(args, ""aheads"", None)\n    awin = getattr(args, ""awin"", None)\n    aconv_chans = getattr(args, ""aconv_chans"", None)\n    aconv_filts = getattr(args, ""aconv_filts"", None)\n\n    if num_encs == 1:\n        for i in range(num_att):\n            att = initial_att(\n                args.atype,\n                args.eprojs,\n                args.dunits,\n                aheads,\n                args.adim,\n                awin,\n                aconv_chans,\n                aconv_filts,\n            )\n            att_list.append(att)\n    elif num_encs > 1:  # no multi-speaker mode\n        if han_mode:\n            att = initial_att(\n                args.han_type,\n                args.eprojs,\n                args.dunits,\n                args.han_heads,\n                args.han_dim,\n                args.han_win,\n                args.han_conv_chans,\n                args.han_conv_filts,\n                han_mode=True,\n            )\n            return att\n        else:\n            att_list = torch.nn.ModuleList()\n            for idx in range(num_encs):\n                att = initial_att(\n                    args.atype[idx],\n                    args.eprojs,\n                    args.dunits,\n                    aheads[idx],\n                    args.adim[idx],\n                    awin[idx],\n                    aconv_chans[idx],\n                    aconv_filts[idx],\n                )\n                att_list.append(att)\n    else:\n        raise ValueError(\n            ""Number of encoders needs to be more than one. {}"".format(num_encs)\n        )\n    return att_list\n\n\ndef initial_att(\n    atype, eprojs, dunits, aheads, adim, awin, aconv_chans, aconv_filts, han_mode=False\n):\n    """"""Instantiates a single attention module\n\n    :param str atype: attention type\n    :param int eprojs: # projection-units of encoder\n    :param int dunits: # units of decoder\n    :param int aheads: # heads of multi head attention\n    :param int adim: attention dimension\n    :param int awin: attention window size\n    :param int aconv_chans: # channels of attention convolution\n    :param int aconv_filts: filter size of attention convolution\n    :param bool han_mode: flag to swith on mode of hierarchical attention\n    :return: The attention module\n    """"""\n\n    if atype == ""noatt"":\n        att = NoAtt()\n    elif atype == ""dot"":\n        att = AttDot(eprojs, dunits, adim, han_mode)\n    elif atype == ""add"":\n        att = AttAdd(eprojs, dunits, adim, han_mode)\n    elif atype == ""location"":\n        att = AttLoc(eprojs, dunits, adim, aconv_chans, aconv_filts, han_mode)\n    elif atype == ""location2d"":\n        att = AttLoc2D(eprojs, dunits, adim, awin, aconv_chans, aconv_filts, han_mode)\n    elif atype == ""location_recurrent"":\n        att = AttLocRec(eprojs, dunits, adim, aconv_chans, aconv_filts, han_mode)\n    elif atype == ""coverage"":\n        att = AttCov(eprojs, dunits, adim, han_mode)\n    elif atype == ""coverage_location"":\n        att = AttCovLoc(eprojs, dunits, adim, aconv_chans, aconv_filts, han_mode)\n    elif atype == ""multi_head_dot"":\n        att = AttMultiHeadDot(eprojs, dunits, aheads, adim, adim, han_mode)\n    elif atype == ""multi_head_add"":\n        att = AttMultiHeadAdd(eprojs, dunits, aheads, adim, adim, han_mode)\n    elif atype == ""multi_head_loc"":\n        att = AttMultiHeadLoc(\n            eprojs, dunits, aheads, adim, adim, aconv_chans, aconv_filts, han_mode\n        )\n    elif atype == ""multi_head_multi_res_loc"":\n        att = AttMultiHeadMultiResLoc(\n            eprojs, dunits, aheads, adim, adim, aconv_chans, aconv_filts, han_mode\n        )\n    return att\n\n\ndef att_to_numpy(att_ws, att):\n    """"""Converts attention weights to a numpy array given the attention\n\n    :param list att_ws: The attention weights\n    :param torch.nn.Module att: The attention\n    :rtype: np.ndarray\n    :return: The numpy array of the attention weights\n    """"""\n    # convert to numpy array with the shape (B, Lmax, Tmax)\n    if isinstance(att, AttLoc2D):\n        # att_ws => list of previous concate attentions\n        att_ws = torch.stack([aw[:, -1] for aw in att_ws], dim=1).cpu().numpy()\n    elif isinstance(att, (AttCov, AttCovLoc)):\n        # att_ws => list of list of previous attentions\n        att_ws = (\n            torch.stack([aw[idx] for idx, aw in enumerate(att_ws)], dim=1).cpu().numpy()\n        )\n    elif isinstance(att, AttLocRec):\n        # att_ws => list of tuple of attention and hidden states\n        att_ws = torch.stack([aw[0] for aw in att_ws], dim=1).cpu().numpy()\n    elif isinstance(\n        att,\n        (AttMultiHeadDot, AttMultiHeadAdd, AttMultiHeadLoc, AttMultiHeadMultiResLoc),\n    ):\n        # att_ws => list of list of each head attention\n        n_heads = len(att_ws[0])\n        att_ws_sorted_by_head = []\n        for h in six.moves.range(n_heads):\n            att_ws_head = torch.stack([aw[h] for aw in att_ws], dim=1)\n            att_ws_sorted_by_head += [att_ws_head]\n        att_ws = torch.stack(att_ws_sorted_by_head, dim=1).cpu().numpy()\n    else:\n        # att_ws => list of attentions\n        att_ws = torch.stack(att_ws, dim=1).cpu().numpy()\n    return att_ws\n'"
espnet/nets/pytorch_backend/rnn/decoders.py,92,"b'from distutils.version import LooseVersion\nimport logging\nimport math\nimport random\nimport six\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom argparse import Namespace\n\nfrom espnet.nets.ctc_prefix_score import CTCPrefixScore\nfrom espnet.nets.ctc_prefix_score import CTCPrefixScoreTH\nfrom espnet.nets.e2e_asr_common import end_detect\n\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_to_numpy\n\nfrom espnet.nets.pytorch_backend.nets_utils import mask_by_length\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import th_accuracy\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\nfrom espnet.nets.scorer_interface import ScorerInterface\n\nMAX_DECODER_OUTPUT = 5\nCTC_SCORING_RATIO = 1.5\n\n\nclass Decoder(torch.nn.Module, ScorerInterface):\n    """"""Decoder module\n\n    :param int eprojs: encoder projection units\n    :param int odim: dimension of outputs\n    :param str dtype: gru or lstm\n    :param int dlayers: decoder layers\n    :param int dunits: decoder units\n    :param int sos: start of sequence symbol id\n    :param int eos: end of sequence symbol id\n    :param torch.nn.Module att: attention module\n    :param int verbose: verbose level\n    :param list char_list: list of character strings\n    :param ndarray labeldist: distribution of label smoothing\n    :param float lsm_weight: label smoothing weight\n    :param float sampling_probability: scheduled sampling probability\n    :param float dropout: dropout rate\n    :param float context_residual: if True, use context vector for token generation\n    :param float replace_sos: use for multilingual (speech/text) translation\n    """"""\n\n    def __init__(\n        self,\n        eprojs,\n        odim,\n        dtype,\n        dlayers,\n        dunits,\n        sos,\n        eos,\n        att,\n        verbose=0,\n        char_list=None,\n        labeldist=None,\n        lsm_weight=0.0,\n        sampling_probability=0.0,\n        dropout=0.0,\n        context_residual=False,\n        replace_sos=False,\n        num_encs=1,\n    ):\n\n        torch.nn.Module.__init__(self)\n        self.dtype = dtype\n        self.dunits = dunits\n        self.dlayers = dlayers\n        self.context_residual = context_residual\n        self.embed = torch.nn.Embedding(odim, dunits)\n        self.dropout_emb = torch.nn.Dropout(p=dropout)\n\n        self.decoder = torch.nn.ModuleList()\n        self.dropout_dec = torch.nn.ModuleList()\n        self.decoder += [\n            torch.nn.LSTMCell(dunits + eprojs, dunits)\n            if self.dtype == ""lstm""\n            else torch.nn.GRUCell(dunits + eprojs, dunits)\n        ]\n        self.dropout_dec += [torch.nn.Dropout(p=dropout)]\n        for _ in six.moves.range(1, self.dlayers):\n            self.decoder += [\n                torch.nn.LSTMCell(dunits, dunits)\n                if self.dtype == ""lstm""\n                else torch.nn.GRUCell(dunits, dunits)\n            ]\n            self.dropout_dec += [torch.nn.Dropout(p=dropout)]\n            # NOTE: dropout is applied only for the vertical connections\n            # see https://arxiv.org/pdf/1409.2329.pdf\n        self.ignore_id = -1\n\n        if context_residual:\n            self.output = torch.nn.Linear(dunits + eprojs, odim)\n        else:\n            self.output = torch.nn.Linear(dunits, odim)\n\n        self.loss = None\n        self.att = att\n        self.dunits = dunits\n        self.sos = sos\n        self.eos = eos\n        self.odim = odim\n        self.verbose = verbose\n        self.char_list = char_list\n        # for label smoothing\n        self.labeldist = labeldist\n        self.vlabeldist = None\n        self.lsm_weight = lsm_weight\n        self.sampling_probability = sampling_probability\n        self.dropout = dropout\n        self.num_encs = num_encs\n\n        # for multilingual E2E-ST\n        self.replace_sos = replace_sos\n\n        self.logzero = -10000000000.0\n\n    def zero_state(self, hs_pad):\n        return hs_pad.new_zeros(hs_pad.size(0), self.dunits)\n\n    def rnn_forward(self, ey, z_list, c_list, z_prev, c_prev):\n        if self.dtype == ""lstm"":\n            z_list[0], c_list[0] = self.decoder[0](ey, (z_prev[0], c_prev[0]))\n            for i in six.moves.range(1, self.dlayers):\n                z_list[i], c_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), (z_prev[i], c_prev[i])\n                )\n        else:\n            z_list[0] = self.decoder[0](ey, z_prev[0])\n            for i in six.moves.range(1, self.dlayers):\n                z_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), z_prev[i]\n                )\n        return z_list, c_list\n\n    def forward(self, hs_pad, hlens, ys_pad, strm_idx=0, lang_ids=None):\n        """"""Decoder forward\n\n        :param torch.Tensor hs_pad: batch of padded hidden state sequences (B, Tmax, D)\n                                    [in multi-encoder case,\n                                    list of torch.Tensor,\n                                    [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ]\n        :param torch.Tensor hlens: batch of lengths of hidden state sequences (B)\n                                   [in multi-encoder case, list of torch.Tensor,\n                                   [(B), (B), ..., ]\n        :param torch.Tensor ys_pad: batch of padded character id sequence tensor\n                                    (B, Lmax)\n        :param int strm_idx: stream index indicates the index of decoding stream.\n        :param torch.Tensor lang_ids: batch of target language id tensor (B, 1)\n        :return: attention loss value\n        :rtype: torch.Tensor\n        :return: accuracy\n        :rtype: float\n        """"""\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            hs_pad = [hs_pad]\n            hlens = [hlens]\n\n        # TODO(kan-bayashi): need to make more smart way\n        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n        # attention index for the attention module\n        # in SPA (speaker parallel attention),\n        # att_idx is used to select attention module. In other cases, it is 0.\n        att_idx = min(strm_idx, len(self.att) - 1)\n\n        # hlens should be list of list of integer\n        hlens = [list(map(int, hlens[idx])) for idx in range(self.num_encs)]\n\n        self.loss = None\n        # prepare input and output word sequences with sos/eos IDs\n        eos = ys[0].new([self.eos])\n        sos = ys[0].new([self.sos])\n        if self.replace_sos:\n            ys_in = [torch.cat([idx, y], dim=0) for idx, y in zip(lang_ids, ys)]\n        else:\n            ys_in = [torch.cat([sos, y], dim=0) for y in ys]\n        ys_out = [torch.cat([y, eos], dim=0) for y in ys]\n\n        # padding for ys with -1\n        # pys: utt x olen\n        ys_in_pad = pad_list(ys_in, self.eos)\n        ys_out_pad = pad_list(ys_out, self.ignore_id)\n\n        # get dim, length info\n        batch = ys_out_pad.size(0)\n        olength = ys_out_pad.size(1)\n        for idx in range(self.num_encs):\n            logging.info(\n                self.__class__.__name__\n                + ""Number of Encoder:{}; enc{}: input lengths: {}."".format(\n                    self.num_encs, idx + 1, hlens[idx]\n                )\n            )\n        logging.info(\n            self.__class__.__name__\n            + "" output lengths: ""\n            + str([y.size(0) for y in ys_out])\n        )\n\n        # initialization\n        c_list = [self.zero_state(hs_pad[0])]\n        z_list = [self.zero_state(hs_pad[0])]\n        for _ in six.moves.range(1, self.dlayers):\n            c_list.append(self.zero_state(hs_pad[0]))\n            z_list.append(self.zero_state(hs_pad[0]))\n        z_all = []\n        if self.num_encs == 1:\n            att_w = None\n            self.att[att_idx].reset()  # reset pre-computation of h\n        else:\n            att_w_list = [None] * (self.num_encs + 1)  # atts + han\n            att_c_list = [None] * (self.num_encs)  # atts\n            for idx in range(self.num_encs + 1):\n                self.att[idx].reset()  # reset pre-computation of h in atts and han\n\n        # pre-computation of embedding\n        eys = self.dropout_emb(self.embed(ys_in_pad))  # utt x olen x zdim\n\n        # loop for an output sequence\n        for i in six.moves.range(olength):\n            if self.num_encs == 1:\n                att_c, att_w = self.att[att_idx](\n                    hs_pad[0], hlens[0], self.dropout_dec[0](z_list[0]), att_w\n                )\n            else:\n                for idx in range(self.num_encs):\n                    att_c_list[idx], att_w_list[idx] = self.att[idx](\n                        hs_pad[idx],\n                        hlens[idx],\n                        self.dropout_dec[0](z_list[0]),\n                        att_w_list[idx],\n                    )\n                hs_pad_han = torch.stack(att_c_list, dim=1)\n                hlens_han = [self.num_encs] * len(ys_in)\n                att_c, att_w_list[self.num_encs] = self.att[self.num_encs](\n                    hs_pad_han,\n                    hlens_han,\n                    self.dropout_dec[0](z_list[0]),\n                    att_w_list[self.num_encs],\n                )\n            if i > 0 and random.random() < self.sampling_probability:\n                logging.info("" scheduled sampling "")\n                z_out = self.output(z_all[-1])\n                z_out = np.argmax(z_out.detach().cpu(), axis=1)\n                z_out = self.dropout_emb(self.embed(to_device(self, z_out)))\n                ey = torch.cat((z_out, att_c), dim=1)  # utt x (zdim + hdim)\n            else:\n                ey = torch.cat((eys[:, i, :], att_c), dim=1)  # utt x (zdim + hdim)\n            z_list, c_list = self.rnn_forward(ey, z_list, c_list, z_list, c_list)\n            if self.context_residual:\n                z_all.append(\n                    torch.cat((self.dropout_dec[-1](z_list[-1]), att_c), dim=-1)\n                )  # utt x (zdim + hdim)\n            else:\n                z_all.append(self.dropout_dec[-1](z_list[-1]))  # utt x (zdim)\n\n        z_all = torch.stack(z_all, dim=1).view(batch * olength, -1)\n        # compute loss\n        y_all = self.output(z_all)\n        if LooseVersion(torch.__version__) < LooseVersion(""1.0""):\n            reduction_str = ""elementwise_mean""\n        else:\n            reduction_str = ""mean""\n        self.loss = F.cross_entropy(\n            y_all,\n            ys_out_pad.view(-1),\n            ignore_index=self.ignore_id,\n            reduction=reduction_str,\n        )\n        # compute perplexity\n        ppl = math.exp(self.loss.item())\n        # -1: eos, which is removed in the loss computation\n        self.loss *= np.mean([len(x) for x in ys_in]) - 1\n        acc = th_accuracy(y_all, ys_out_pad, ignore_label=self.ignore_id)\n        logging.info(""att loss:"" + """".join(str(self.loss.item()).split(""\\n"")))\n\n        # show predicted character sequence for debug\n        if self.verbose > 0 and self.char_list is not None:\n            ys_hat = y_all.view(batch, olength, -1)\n            ys_true = ys_out_pad\n            for (i, y_hat), y_true in zip(\n                enumerate(ys_hat.detach().cpu().numpy()), ys_true.detach().cpu().numpy()\n            ):\n                if i == MAX_DECODER_OUTPUT:\n                    break\n                idx_hat = np.argmax(y_hat[y_true != self.ignore_id], axis=1)\n                idx_true = y_true[y_true != self.ignore_id]\n                seq_hat = [self.char_list[int(idx)] for idx in idx_hat]\n                seq_true = [self.char_list[int(idx)] for idx in idx_true]\n                seq_hat = """".join(seq_hat)\n                seq_true = """".join(seq_true)\n                logging.info(""groundtruth[%d]: "" % i + seq_true)\n                logging.info(""prediction [%d]: "" % i + seq_hat)\n\n        if self.labeldist is not None:\n            if self.vlabeldist is None:\n                self.vlabeldist = to_device(self, torch.from_numpy(self.labeldist))\n            loss_reg = -torch.sum(\n                (F.log_softmax(y_all, dim=1) * self.vlabeldist).view(-1), dim=0\n            ) / len(ys_in)\n            self.loss = (1.0 - self.lsm_weight) * self.loss + self.lsm_weight * loss_reg\n\n        return self.loss, acc, ppl\n\n    def recognize_beam(self, h, lpz, recog_args, char_list, rnnlm=None, strm_idx=0):\n        """"""beam search implementation\n\n        :param torch.Tensor h: encoder hidden state (T, eprojs)\n                                [in multi-encoder case, list of torch.Tensor,\n                                [(T1, eprojs), (T2, eprojs), ...] ]\n        :param torch.Tensor lpz: ctc log softmax output (T, odim)\n                                [in multi-encoder case, list of torch.Tensor,\n                                [(T1, odim), (T2, odim), ...] ]\n        :param Namespace recog_args: argument Namespace containing options\n        :param char_list: list of character strings\n        :param torch.nn.Module rnnlm: language module\n        :param int strm_idx:\n            stream index for speaker parallel attention in multi-speaker case\n        :return: N-best decoding results\n        :rtype: list of dicts\n        """"""\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            h = [h]\n            lpz = [lpz]\n        if self.num_encs > 1 and lpz is None:\n            lpz = [lpz] * self.num_encs\n\n        for idx in range(self.num_encs):\n            logging.info(\n                ""Number of Encoder:{}; enc{}: input lengths: {}."".format(\n                    self.num_encs, idx + 1, h[0].size(0)\n                )\n            )\n        att_idx = min(strm_idx, len(self.att) - 1)\n        # initialization\n        c_list = [self.zero_state(h[0].unsqueeze(0))]\n        z_list = [self.zero_state(h[0].unsqueeze(0))]\n        for _ in six.moves.range(1, self.dlayers):\n            c_list.append(self.zero_state(h[0].unsqueeze(0)))\n            z_list.append(self.zero_state(h[0].unsqueeze(0)))\n        if self.num_encs == 1:\n            a = None\n            self.att[att_idx].reset()  # reset pre-computation of h\n        else:\n            a = [None] * (self.num_encs + 1)  # atts + han\n            att_w_list = [None] * (self.num_encs + 1)  # atts + han\n            att_c_list = [None] * (self.num_encs)  # atts\n            for idx in range(self.num_encs + 1):\n                self.att[idx].reset()  # reset pre-computation of h in atts and han\n\n        # search parms\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = getattr(recog_args, ""ctc_weight"", False)  # for NMT\n\n        if lpz[0] is not None and self.num_encs > 1:\n            # weights-ctc,\n            # e.g. ctc_loss = w_1*ctc_1_loss + w_2 * ctc_2_loss + w_N * ctc_N_loss\n            weights_ctc_dec = recog_args.weights_ctc_dec / np.sum(\n                recog_args.weights_ctc_dec\n            )  # normalize\n            logging.info(\n                ""ctc weights (decoding): "" + "" "".join([str(x) for x in weights_ctc_dec])\n            )\n        else:\n            weights_ctc_dec = [1.0]\n\n        # preprate sos\n        if self.replace_sos and recog_args.tgt_lang:\n            y = char_list.index(recog_args.tgt_lang)\n        else:\n            y = self.sos\n        logging.info(""<sos> index: "" + str(y))\n        logging.info(""<sos> mark: "" + char_list[y])\n        vy = h[0].new_zeros(1).long()\n\n        maxlen = np.amin([h[idx].size(0) for idx in range(self.num_encs)])\n        if recog_args.maxlenratio != 0:\n            # maxlen >= 1\n            maxlen = max(1, int(recog_args.maxlenratio * maxlen))\n        minlen = int(recog_args.minlenratio * maxlen)\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # initialize hypothesis\n        if rnnlm:\n            hyp = {\n                ""score"": 0.0,\n                ""yseq"": [y],\n                ""c_prev"": c_list,\n                ""z_prev"": z_list,\n                ""a_prev"": a,\n                ""rnnlm_prev"": None,\n            }\n        else:\n            hyp = {\n                ""score"": 0.0,\n                ""yseq"": [y],\n                ""c_prev"": c_list,\n                ""z_prev"": z_list,\n                ""a_prev"": a,\n            }\n        if lpz[0] is not None:\n            ctc_prefix_score = [\n                CTCPrefixScore(lpz[idx].detach().numpy(), 0, self.eos, np)\n                for idx in range(self.num_encs)\n            ]\n            hyp[""ctc_state_prev""] = [\n                ctc_prefix_score[idx].initial_state() for idx in range(self.num_encs)\n            ]\n            hyp[""ctc_score_prev""] = [0.0] * self.num_encs\n            if ctc_weight != 1.0:\n                # pre-pruning based on attention scores\n                ctc_beam = min(lpz[0].shape[-1], int(beam * CTC_SCORING_RATIO))\n            else:\n                ctc_beam = lpz[0].shape[-1]\n        hyps = [hyp]\n        ended_hyps = []\n\n        for i in six.moves.range(maxlen):\n            logging.debug(""position "" + str(i))\n\n            hyps_best_kept = []\n            for hyp in hyps:\n                vy[0] = hyp[""yseq""][i]\n                ey = self.dropout_emb(self.embed(vy))  # utt list (1) x zdim\n                if self.num_encs == 1:\n                    att_c, att_w = self.att[att_idx](\n                        h[0].unsqueeze(0),\n                        [h[0].size(0)],\n                        self.dropout_dec[0](hyp[""z_prev""][0]),\n                        hyp[""a_prev""],\n                    )\n                else:\n                    for idx in range(self.num_encs):\n                        att_c_list[idx], att_w_list[idx] = self.att[idx](\n                            h[idx].unsqueeze(0),\n                            [h[idx].size(0)],\n                            self.dropout_dec[0](hyp[""z_prev""][0]),\n                            hyp[""a_prev""][idx],\n                        )\n                    h_han = torch.stack(att_c_list, dim=1)\n                    att_c, att_w_list[self.num_encs] = self.att[self.num_encs](\n                        h_han,\n                        [self.num_encs],\n                        self.dropout_dec[0](hyp[""z_prev""][0]),\n                        hyp[""a_prev""][self.num_encs],\n                    )\n                ey = torch.cat((ey, att_c), dim=1)  # utt(1) x (zdim + hdim)\n                z_list, c_list = self.rnn_forward(\n                    ey, z_list, c_list, hyp[""z_prev""], hyp[""c_prev""]\n                )\n\n                # get nbest local scores and their ids\n                if self.context_residual:\n                    logits = self.output(\n                        torch.cat((self.dropout_dec[-1](z_list[-1]), att_c), dim=-1)\n                    )\n                else:\n                    logits = self.output(self.dropout_dec[-1](z_list[-1]))\n                local_att_scores = F.log_softmax(logits, dim=1)\n                if rnnlm:\n                    rnnlm_state, local_lm_scores = rnnlm.predict(hyp[""rnnlm_prev""], vy)\n                    local_scores = (\n                        local_att_scores + recog_args.lm_weight * local_lm_scores\n                    )\n                else:\n                    local_scores = local_att_scores\n\n                if lpz[0] is not None:\n                    local_best_scores, local_best_ids = torch.topk(\n                        local_att_scores, ctc_beam, dim=1\n                    )\n                    ctc_scores, ctc_states = (\n                        [None] * self.num_encs,\n                        [None] * self.num_encs,\n                    )\n                    for idx in range(self.num_encs):\n                        ctc_scores[idx], ctc_states[idx] = ctc_prefix_score[idx](\n                            hyp[""yseq""], local_best_ids[0], hyp[""ctc_state_prev""][idx]\n                        )\n                    local_scores = (1.0 - ctc_weight) * local_att_scores[\n                        :, local_best_ids[0]\n                    ]\n                    if self.num_encs == 1:\n                        local_scores += ctc_weight * torch.from_numpy(\n                            ctc_scores[0] - hyp[""ctc_score_prev""][0]\n                        )\n                    else:\n                        for idx in range(self.num_encs):\n                            local_scores += (\n                                ctc_weight\n                                * weights_ctc_dec[idx]\n                                * torch.from_numpy(\n                                    ctc_scores[idx] - hyp[""ctc_score_prev""][idx]\n                                )\n                            )\n                    if rnnlm:\n                        local_scores += (\n                            recog_args.lm_weight * local_lm_scores[:, local_best_ids[0]]\n                        )\n                    local_best_scores, joint_best_ids = torch.topk(\n                        local_scores, beam, dim=1\n                    )\n                    local_best_ids = local_best_ids[:, joint_best_ids[0]]\n                else:\n                    local_best_scores, local_best_ids = torch.topk(\n                        local_scores, beam, dim=1\n                    )\n\n                for j in six.moves.range(beam):\n                    new_hyp = {}\n                    # [:] is needed!\n                    new_hyp[""z_prev""] = z_list[:]\n                    new_hyp[""c_prev""] = c_list[:]\n                    if self.num_encs == 1:\n                        new_hyp[""a_prev""] = att_w[:]\n                    else:\n                        new_hyp[""a_prev""] = [\n                            att_w_list[idx][:] for idx in range(self.num_encs + 1)\n                        ]\n                    new_hyp[""score""] = hyp[""score""] + local_best_scores[0, j]\n                    new_hyp[""yseq""] = [0] * (1 + len(hyp[""yseq""]))\n                    new_hyp[""yseq""][: len(hyp[""yseq""])] = hyp[""yseq""]\n                    new_hyp[""yseq""][len(hyp[""yseq""])] = int(local_best_ids[0, j])\n                    if rnnlm:\n                        new_hyp[""rnnlm_prev""] = rnnlm_state\n                    if lpz[0] is not None:\n                        new_hyp[""ctc_state_prev""] = [\n                            ctc_states[idx][joint_best_ids[0, j]]\n                            for idx in range(self.num_encs)\n                        ]\n                        new_hyp[""ctc_score_prev""] = [\n                            ctc_scores[idx][joint_best_ids[0, j]]\n                            for idx in range(self.num_encs)\n                        ]\n                    # will be (2 x beam) hyps at most\n                    hyps_best_kept.append(new_hyp)\n\n                hyps_best_kept = sorted(\n                    hyps_best_kept, key=lambda x: x[""score""], reverse=True\n                )[:beam]\n\n            # sort and get nbest\n            hyps = hyps_best_kept\n            logging.debug(""number of pruned hypotheses: "" + str(len(hyps)))\n            logging.debug(\n                ""best hypo: ""\n                + """".join([char_list[int(x)] for x in hyps[0][""yseq""][1:]])\n            )\n\n            # add eos in the final loop to avoid that there are no ended hyps\n            if i == maxlen - 1:\n                logging.info(""adding <eos> in the last position in the loop"")\n                for hyp in hyps:\n                    hyp[""yseq""].append(self.eos)\n\n            # add ended hypotheses to a final list,\n            # and removed them from current hypotheses\n            # (this will be a problem, number of hyps < beam)\n            remained_hyps = []\n            for hyp in hyps:\n                if hyp[""yseq""][-1] == self.eos:\n                    # only store the sequence that has more than minlen outputs\n                    # also add penalty\n                    if len(hyp[""yseq""]) > minlen:\n                        hyp[""score""] += (i + 1) * penalty\n                        if rnnlm:  # Word LM needs to add final <eos> score\n                            hyp[""score""] += recog_args.lm_weight * rnnlm.final(\n                                hyp[""rnnlm_prev""]\n                            )\n                        ended_hyps.append(hyp)\n                else:\n                    remained_hyps.append(hyp)\n\n            # end detection\n            if end_detect(ended_hyps, i) and recog_args.maxlenratio == 0.0:\n                logging.info(""end detected at %d"", i)\n                break\n\n            hyps = remained_hyps\n            if len(hyps) > 0:\n                logging.debug(""remaining hypotheses: "" + str(len(hyps)))\n            else:\n                logging.info(""no hypothesis. Finish decoding."")\n                break\n\n            for hyp in hyps:\n                logging.debug(\n                    ""hypo: "" + """".join([char_list[int(x)] for x in hyp[""yseq""][1:]])\n                )\n\n            logging.debug(""number of ended hypotheses: "" + str(len(ended_hyps)))\n\n        nbest_hyps = sorted(ended_hyps, key=lambda x: x[""score""], reverse=True)[\n            : min(len(ended_hyps), recog_args.nbest)\n        ]\n\n        # check number of hypotheses\n        if len(nbest_hyps) == 0:\n            logging.warning(\n                ""there is no N-best results, ""\n                ""perform recognition again with smaller minlenratio.""\n            )\n            # should copy because Namespace will be overwritten globally\n            recog_args = Namespace(**vars(recog_args))\n            recog_args.minlenratio = max(0.0, recog_args.minlenratio - 0.1)\n            if self.num_encs == 1:\n                return self.recognize_beam(h[0], lpz[0], recog_args, char_list, rnnlm)\n            else:\n                return self.recognize_beam(h, lpz, recog_args, char_list, rnnlm)\n\n        logging.info(""total log probability: "" + str(nbest_hyps[0][""score""]))\n        logging.info(\n            ""normalized log probability: ""\n            + str(nbest_hyps[0][""score""] / len(nbest_hyps[0][""yseq""]))\n        )\n\n        # remove sos\n        return nbest_hyps\n\n    def recognize_beam_batch(\n        self,\n        h,\n        hlens,\n        lpz,\n        recog_args,\n        char_list,\n        rnnlm=None,\n        normalize_score=True,\n        strm_idx=0,\n        lang_ids=None,\n    ):\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            h = [h]\n            hlens = [hlens]\n            lpz = [lpz]\n        if self.num_encs > 1 and lpz is None:\n            lpz = [lpz] * self.num_encs\n\n        att_idx = min(strm_idx, len(self.att) - 1)\n        for idx in range(self.num_encs):\n            logging.info(\n                ""Number of Encoder:{}; enc{}: input lengths: {}."".format(\n                    self.num_encs, idx + 1, h[idx].size(1)\n                )\n            )\n            h[idx] = mask_by_length(h[idx], hlens[idx], 0.0)\n\n        # search params\n        batch = len(hlens[0])\n        beam = recog_args.beam_size\n        penalty = recog_args.penalty\n        ctc_weight = getattr(recog_args, ""ctc_weight"", 0)  # for NMT\n        att_weight = 1.0 - ctc_weight\n        ctc_margin = getattr(\n            recog_args, ""ctc_window_margin"", 0\n        )  # use getattr to keep compatibility\n        # weights-ctc,\n        # e.g. ctc_loss = w_1*ctc_1_loss + w_2 * ctc_2_loss + w_N * ctc_N_loss\n        if lpz[0] is not None and self.num_encs > 1:\n            weights_ctc_dec = recog_args.weights_ctc_dec / np.sum(\n                recog_args.weights_ctc_dec\n            )  # normalize\n            logging.info(\n                ""ctc weights (decoding): "" + "" "".join([str(x) for x in weights_ctc_dec])\n            )\n        else:\n            weights_ctc_dec = [1.0]\n\n        n_bb = batch * beam\n        pad_b = to_device(self, torch.arange(batch) * beam).view(-1, 1)\n\n        max_hlen = np.amin([max(hlens[idx]) for idx in range(self.num_encs)])\n        if recog_args.maxlenratio == 0:\n            maxlen = max_hlen\n        else:\n            maxlen = max(1, int(recog_args.maxlenratio * max_hlen))\n        minlen = int(recog_args.minlenratio * max_hlen)\n        logging.info(""max output length: "" + str(maxlen))\n        logging.info(""min output length: "" + str(minlen))\n\n        # initialization\n        c_prev = [\n            to_device(self, torch.zeros(n_bb, self.dunits)) for _ in range(self.dlayers)\n        ]\n        z_prev = [\n            to_device(self, torch.zeros(n_bb, self.dunits)) for _ in range(self.dlayers)\n        ]\n        c_list = [\n            to_device(self, torch.zeros(n_bb, self.dunits)) for _ in range(self.dlayers)\n        ]\n        z_list = [\n            to_device(self, torch.zeros(n_bb, self.dunits)) for _ in range(self.dlayers)\n        ]\n        vscores = to_device(self, torch.zeros(batch, beam))\n\n        rnnlm_state = None\n        if self.num_encs == 1:\n            a_prev = [None]\n            att_w_list, ctc_scorer, ctc_state = [None], [None], [None]\n            self.att[att_idx].reset()  # reset pre-computation of h\n        else:\n            a_prev = [None] * (self.num_encs + 1)  # atts + han\n            att_w_list = [None] * (self.num_encs + 1)  # atts + han\n            att_c_list = [None] * (self.num_encs)  # atts\n            ctc_scorer, ctc_state = [None] * (self.num_encs), [None] * (self.num_encs)\n            for idx in range(self.num_encs + 1):\n                self.att[idx].reset()  # reset pre-computation of h in atts and han\n\n        if self.replace_sos and recog_args.tgt_lang:\n            logging.info(""<sos> index: "" + str(char_list.index(recog_args.tgt_lang)))\n            logging.info(""<sos> mark: "" + recog_args.tgt_lang)\n            yseq = [\n                [char_list.index(recog_args.tgt_lang)] for _ in six.moves.range(n_bb)\n            ]\n        elif lang_ids is not None:\n            # NOTE: used for evaluation during training\n            yseq = [\n                [lang_ids[b // recog_args.beam_size]] for b in six.moves.range(n_bb)\n            ]\n        else:\n            logging.info(""<sos> index: "" + str(self.sos))\n            logging.info(""<sos> mark: "" + char_list[self.sos])\n            yseq = [[self.sos] for _ in six.moves.range(n_bb)]\n\n        accum_odim_ids = [self.sos for _ in six.moves.range(n_bb)]\n        stop_search = [False for _ in six.moves.range(batch)]\n        nbest_hyps = [[] for _ in six.moves.range(batch)]\n        ended_hyps = [[] for _ in range(batch)]\n\n        exp_hlens = [\n            hlens[idx].repeat(beam).view(beam, batch).transpose(0, 1).contiguous()\n            for idx in range(self.num_encs)\n        ]\n        exp_hlens = [exp_hlens[idx].view(-1).tolist() for idx in range(self.num_encs)]\n        exp_h = [\n            h[idx].unsqueeze(1).repeat(1, beam, 1, 1).contiguous()\n            for idx in range(self.num_encs)\n        ]\n        exp_h = [\n            exp_h[idx].view(n_bb, h[idx].size()[1], h[idx].size()[2])\n            for idx in range(self.num_encs)\n        ]\n\n        if lpz[0] is not None:\n            scoring_ratio = (\n                CTC_SCORING_RATIO if att_weight > 0.0 and not lpz[0].is_cuda else 0\n            )\n            ctc_scorer = [\n                CTCPrefixScoreTH(\n                    lpz[idx],\n                    hlens[idx],\n                    0,\n                    self.eos,\n                    beam,\n                    scoring_ratio,\n                    margin=ctc_margin,\n                )\n                for idx in range(self.num_encs)\n            ]\n\n        for i in six.moves.range(maxlen):\n            logging.debug(""position "" + str(i))\n\n            vy = to_device(self, torch.LongTensor(self._get_last_yseq(yseq)))\n            ey = self.dropout_emb(self.embed(vy))\n            if self.num_encs == 1:\n                att_c, att_w = self.att[att_idx](\n                    exp_h[0], exp_hlens[0], self.dropout_dec[0](z_prev[0]), a_prev[0]\n                )\n                att_w_list = [att_w]\n            else:\n                for idx in range(self.num_encs):\n                    att_c_list[idx], att_w_list[idx] = self.att[idx](\n                        exp_h[idx],\n                        exp_hlens[idx],\n                        self.dropout_dec[0](z_prev[0]),\n                        a_prev[idx],\n                    )\n                exp_h_han = torch.stack(att_c_list, dim=1)\n                att_c, att_w_list[self.num_encs] = self.att[self.num_encs](\n                    exp_h_han,\n                    [self.num_encs] * n_bb,\n                    self.dropout_dec[0](z_prev[0]),\n                    a_prev[self.num_encs],\n                )\n            ey = torch.cat((ey, att_c), dim=1)\n\n            # attention decoder\n            z_list, c_list = self.rnn_forward(ey, z_list, c_list, z_prev, c_prev)\n            if self.context_residual:\n                logits = self.output(\n                    torch.cat((self.dropout_dec[-1](z_list[-1]), att_c), dim=-1)\n                )\n            else:\n                logits = self.output(self.dropout_dec[-1](z_list[-1]))\n            local_scores = att_weight * F.log_softmax(logits, dim=1)\n\n            # rnnlm\n            if rnnlm:\n                rnnlm_state, local_lm_scores = rnnlm.buff_predict(rnnlm_state, vy, n_bb)\n                local_scores = local_scores + recog_args.lm_weight * local_lm_scores\n\n            # ctc\n            if ctc_scorer[0]:\n                for idx in range(self.num_encs):\n                    att_w = att_w_list[idx]\n                    att_w_ = att_w if isinstance(att_w, torch.Tensor) else att_w[0]\n                    ctc_state[idx], local_ctc_scores = ctc_scorer[idx](\n                        yseq, ctc_state[idx], local_scores, att_w_\n                    )\n                    local_scores = (\n                        local_scores\n                        + ctc_weight * weights_ctc_dec[idx] * local_ctc_scores\n                    )\n\n            local_scores = local_scores.view(batch, beam, self.odim)\n            if i == 0:\n                local_scores[:, 1:, :] = self.logzero\n\n            # accumulate scores\n            eos_vscores = local_scores[:, :, self.eos] + vscores\n            vscores = vscores.view(batch, beam, 1).repeat(1, 1, self.odim)\n            vscores[:, :, self.eos] = self.logzero\n            vscores = (vscores + local_scores).view(batch, -1)\n\n            # global pruning\n            accum_best_scores, accum_best_ids = torch.topk(vscores, beam, 1)\n            accum_odim_ids = (\n                torch.fmod(accum_best_ids, self.odim).view(-1).data.cpu().tolist()\n            )\n            accum_padded_beam_ids = (\n                (torch.div(accum_best_ids, self.odim) + pad_b)\n                .view(-1)\n                .data.cpu()\n                .tolist()\n            )\n\n            y_prev = yseq[:][:]\n            yseq = self._index_select_list(yseq, accum_padded_beam_ids)\n            yseq = self._append_ids(yseq, accum_odim_ids)\n            vscores = accum_best_scores\n            vidx = to_device(self, torch.LongTensor(accum_padded_beam_ids))\n\n            a_prev = []\n            num_atts = self.num_encs if self.num_encs == 1 else self.num_encs + 1\n            for idx in range(num_atts):\n                if isinstance(att_w_list[idx], torch.Tensor):\n                    _a_prev = torch.index_select(\n                        att_w_list[idx].view(n_bb, *att_w_list[idx].shape[1:]), 0, vidx\n                    )\n                elif isinstance(att_w_list[idx], list):\n                    # handle the case of multi-head attention\n                    _a_prev = [\n                        torch.index_select(att_w_one.view(n_bb, -1), 0, vidx)\n                        for att_w_one in att_w_list[idx]\n                    ]\n                else:\n                    # handle the case of location_recurrent when return is a tuple\n                    _a_prev_ = torch.index_select(\n                        att_w_list[idx][0].view(n_bb, -1), 0, vidx\n                    )\n                    _h_prev_ = torch.index_select(\n                        att_w_list[idx][1][0].view(n_bb, -1), 0, vidx\n                    )\n                    _c_prev_ = torch.index_select(\n                        att_w_list[idx][1][1].view(n_bb, -1), 0, vidx\n                    )\n                    _a_prev = (_a_prev_, (_h_prev_, _c_prev_))\n                a_prev.append(_a_prev)\n            z_prev = [\n                torch.index_select(z_list[li].view(n_bb, -1), 0, vidx)\n                for li in range(self.dlayers)\n            ]\n            c_prev = [\n                torch.index_select(c_list[li].view(n_bb, -1), 0, vidx)\n                for li in range(self.dlayers)\n            ]\n\n            # pick ended hyps\n            if i >= minlen:\n                k = 0\n                penalty_i = (i + 1) * penalty\n                thr = accum_best_scores[:, -1]\n                for samp_i in six.moves.range(batch):\n                    if stop_search[samp_i]:\n                        k = k + beam\n                        continue\n                    for beam_j in six.moves.range(beam):\n                        _vscore = None\n                        if eos_vscores[samp_i, beam_j] > thr[samp_i]:\n                            yk = y_prev[k][:]\n                            if len(yk) <= min(\n                                hlens[idx][samp_i] for idx in range(self.num_encs)\n                            ):\n                                _vscore = eos_vscores[samp_i][beam_j] + penalty_i\n                        elif i == maxlen - 1:\n                            yk = yseq[k][:]\n                            _vscore = vscores[samp_i][beam_j] + penalty_i\n                        if _vscore:\n                            yk.append(self.eos)\n                            if rnnlm:\n                                _vscore += recog_args.lm_weight * rnnlm.final(\n                                    rnnlm_state, index=k\n                                )\n                            _score = _vscore.data.cpu().numpy()\n                            ended_hyps[samp_i].append(\n                                {""yseq"": yk, ""vscore"": _vscore, ""score"": _score}\n                            )\n                        k = k + 1\n\n            # end detection\n            stop_search = [\n                stop_search[samp_i] or end_detect(ended_hyps[samp_i], i)\n                for samp_i in six.moves.range(batch)\n            ]\n            stop_search_summary = list(set(stop_search))\n            if len(stop_search_summary) == 1 and stop_search_summary[0]:\n                break\n\n            if rnnlm:\n                rnnlm_state = self._index_select_lm_state(rnnlm_state, 0, vidx)\n            if ctc_scorer[0]:\n                for idx in range(self.num_encs):\n                    ctc_state[idx] = ctc_scorer[idx].index_select_state(\n                        ctc_state[idx], accum_best_ids\n                    )\n\n        torch.cuda.empty_cache()\n\n        dummy_hyps = [\n            {""yseq"": [self.sos, self.eos], ""score"": np.array([-float(""inf"")])}\n        ]\n        ended_hyps = [\n            ended_hyps[samp_i] if len(ended_hyps[samp_i]) != 0 else dummy_hyps\n            for samp_i in six.moves.range(batch)\n        ]\n        if normalize_score:\n            for samp_i in six.moves.range(batch):\n                for x in ended_hyps[samp_i]:\n                    x[""score""] /= len(x[""yseq""])\n\n        nbest_hyps = [\n            sorted(ended_hyps[samp_i], key=lambda x: x[""score""], reverse=True)[\n                : min(len(ended_hyps[samp_i]), recog_args.nbest)\n            ]\n            for samp_i in six.moves.range(batch)\n        ]\n\n        return nbest_hyps\n\n    def calculate_all_attentions(self, hs_pad, hlen, ys_pad, strm_idx=0, lang_ids=None):\n        """"""Calculate all of attentions\n\n            :param torch.Tensor hs_pad: batch of padded hidden state sequences\n                                        (B, Tmax, D)\n                                        in multi-encoder case, list of torch.Tensor,\n                                        [(B, Tmax_1, D), (B, Tmax_2, D), ..., ] ]\n            :param torch.Tensor hlen: batch of lengths of hidden state sequences (B)\n                                        [in multi-encoder case, list of torch.Tensor,\n                                        [(B), (B), ..., ]\n            :param torch.Tensor ys_pad:\n                batch of padded character id sequence tensor (B, Lmax)\n            :param int strm_idx:\n                stream index for parallel speaker attention in multi-speaker case\n            :param torch.Tensor lang_ids: batch of target language id tensor (B, 1)\n            :return: attention weights with the following shape,\n                1) multi-head case => attention weights (B, H, Lmax, Tmax),\n                2) multi-encoder case =>\n                    [(B, Lmax, Tmax1), (B, Lmax, Tmax2), ..., (B, Lmax, NumEncs)]\n                3) other case => attention weights (B, Lmax, Tmax).\n            :rtype: float ndarray\n        """"""\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            hs_pad = [hs_pad]\n            hlen = [hlen]\n\n        # TODO(kan-bayashi): need to make more smart way\n        ys = [y[y != self.ignore_id] for y in ys_pad]  # parse padded ys\n        att_idx = min(strm_idx, len(self.att) - 1)\n\n        # hlen should be list of list of integer\n        hlen = [list(map(int, hlen[idx])) for idx in range(self.num_encs)]\n\n        self.loss = None\n        # prepare input and output word sequences with sos/eos IDs\n        eos = ys[0].new([self.eos])\n        sos = ys[0].new([self.sos])\n        if self.replace_sos:\n            ys_in = [torch.cat([idx, y], dim=0) for idx, y in zip(lang_ids, ys)]\n        else:\n            ys_in = [torch.cat([sos, y], dim=0) for y in ys]\n        ys_out = [torch.cat([y, eos], dim=0) for y in ys]\n\n        # padding for ys with -1\n        # pys: utt x olen\n        ys_in_pad = pad_list(ys_in, self.eos)\n        ys_out_pad = pad_list(ys_out, self.ignore_id)\n\n        # get length info\n        olength = ys_out_pad.size(1)\n\n        # initialization\n        c_list = [self.zero_state(hs_pad[0])]\n        z_list = [self.zero_state(hs_pad[0])]\n        for _ in six.moves.range(1, self.dlayers):\n            c_list.append(self.zero_state(hs_pad[0]))\n            z_list.append(self.zero_state(hs_pad[0]))\n        att_ws = []\n        if self.num_encs == 1:\n            att_w = None\n            self.att[att_idx].reset()  # reset pre-computation of h\n        else:\n            att_w_list = [None] * (self.num_encs + 1)  # atts + han\n            att_c_list = [None] * (self.num_encs)  # atts\n            for idx in range(self.num_encs + 1):\n                self.att[idx].reset()  # reset pre-computation of h in atts and han\n\n        # pre-computation of embedding\n        eys = self.dropout_emb(self.embed(ys_in_pad))  # utt x olen x zdim\n\n        # loop for an output sequence\n        for i in six.moves.range(olength):\n            if self.num_encs == 1:\n                att_c, att_w = self.att[att_idx](\n                    hs_pad[0], hlen[0], self.dropout_dec[0](z_list[0]), att_w\n                )\n                att_ws.append(att_w)\n            else:\n                for idx in range(self.num_encs):\n                    att_c_list[idx], att_w_list[idx] = self.att[idx](\n                        hs_pad[idx],\n                        hlen[idx],\n                        self.dropout_dec[0](z_list[0]),\n                        att_w_list[idx],\n                    )\n                hs_pad_han = torch.stack(att_c_list, dim=1)\n                hlen_han = [self.num_encs] * len(ys_in)\n                att_c, att_w_list[self.num_encs] = self.att[self.num_encs](\n                    hs_pad_han,\n                    hlen_han,\n                    self.dropout_dec[0](z_list[0]),\n                    att_w_list[self.num_encs],\n                )\n                att_ws.append(att_w_list)\n            ey = torch.cat((eys[:, i, :], att_c), dim=1)  # utt x (zdim + hdim)\n            z_list, c_list = self.rnn_forward(ey, z_list, c_list, z_list, c_list)\n\n        if self.num_encs == 1:\n            # convert to numpy array with the shape (B, Lmax, Tmax)\n            att_ws = att_to_numpy(att_ws, self.att[att_idx])\n        else:\n            _att_ws = []\n            for idx, ws in enumerate(zip(*att_ws)):\n                ws = att_to_numpy(ws, self.att[idx])\n                _att_ws.append(ws)\n            att_ws = _att_ws\n        return att_ws\n\n    @staticmethod\n    def _get_last_yseq(exp_yseq):\n        last = []\n        for y_seq in exp_yseq:\n            last.append(y_seq[-1])\n        return last\n\n    @staticmethod\n    def _append_ids(yseq, ids):\n        if isinstance(ids, list):\n            for i, j in enumerate(ids):\n                yseq[i].append(j)\n        else:\n            for i in range(len(yseq)):\n                yseq[i].append(ids)\n        return yseq\n\n    @staticmethod\n    def _index_select_list(yseq, lst):\n        new_yseq = []\n        for i in lst:\n            new_yseq.append(yseq[i][:])\n        return new_yseq\n\n    @staticmethod\n    def _index_select_lm_state(rnnlm_state, dim, vidx):\n        if isinstance(rnnlm_state, dict):\n            new_state = {}\n            for k, v in rnnlm_state.items():\n                new_state[k] = [torch.index_select(vi, dim, vidx) for vi in v]\n        elif isinstance(rnnlm_state, list):\n            new_state = []\n            for i in vidx:\n                new_state.append(rnnlm_state[int(i)][:])\n        return new_state\n\n    # scorer interface methods\n    def init_state(self, x):\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            x = [x]\n\n        c_list = [self.zero_state(x[0].unsqueeze(0))]\n        z_list = [self.zero_state(x[0].unsqueeze(0))]\n        for _ in six.moves.range(1, self.dlayers):\n            c_list.append(self.zero_state(x[0].unsqueeze(0)))\n            z_list.append(self.zero_state(x[0].unsqueeze(0)))\n        # TODO(karita): support strm_index for `asr_mix`\n        strm_index = 0\n        att_idx = min(strm_index, len(self.att) - 1)\n        if self.num_encs == 1:\n            a = None\n            self.att[att_idx].reset()  # reset pre-computation of h\n        else:\n            a = [None] * (self.num_encs + 1)  # atts + han\n            for idx in range(self.num_encs + 1):\n                self.att[idx].reset()  # reset pre-computation of h in atts and han\n        return dict(\n            c_prev=c_list[:],\n            z_prev=z_list[:],\n            a_prev=a,\n            workspace=(att_idx, z_list, c_list),\n        )\n\n    def score(self, yseq, state, x):\n        # to support mutiple encoder asr mode, in single encoder mode,\n        # convert torch.Tensor to List of torch.Tensor\n        if self.num_encs == 1:\n            x = [x]\n\n        att_idx, z_list, c_list = state[""workspace""]\n        vy = yseq[-1].unsqueeze(0)\n        ey = self.dropout_emb(self.embed(vy))  # utt list (1) x zdim\n        if self.num_encs == 1:\n            att_c, att_w = self.att[att_idx](\n                x[0].unsqueeze(0),\n                [x[0].size(0)],\n                self.dropout_dec[0](state[""z_prev""][0]),\n                state[""a_prev""],\n            )\n        else:\n            att_w = [None] * (self.num_encs + 1)  # atts + han\n            att_c_list = [None] * (self.num_encs)  # atts\n            for idx in range(self.num_encs):\n                att_c_list[idx], att_w[idx] = self.att[idx](\n                    x[idx].unsqueeze(0),\n                    [x[idx].size(0)],\n                    self.dropout_dec[0](state[""z_prev""][0]),\n                    state[""a_prev""][idx],\n                )\n            h_han = torch.stack(att_c_list, dim=1)\n            att_c, att_w[self.num_encs] = self.att[self.num_encs](\n                h_han,\n                [self.num_encs],\n                self.dropout_dec[0](state[""z_prev""][0]),\n                state[""a_prev""][self.num_encs],\n            )\n        ey = torch.cat((ey, att_c), dim=1)  # utt(1) x (zdim + hdim)\n        z_list, c_list = self.rnn_forward(\n            ey, z_list, c_list, state[""z_prev""], state[""c_prev""]\n        )\n        if self.context_residual:\n            logits = self.output(\n                torch.cat((self.dropout_dec[-1](z_list[-1]), att_c), dim=-1)\n            )\n        else:\n            logits = self.output(self.dropout_dec[-1](z_list[-1]))\n        logp = F.log_softmax(logits, dim=1).squeeze(0)\n        return (\n            logp,\n            dict(\n                c_prev=c_list[:],\n                z_prev=z_list[:],\n                a_prev=att_w,\n                workspace=(att_idx, z_list, c_list),\n            ),\n        )\n\n\ndef decoder_for(args, odim, sos, eos, att, labeldist):\n    return Decoder(\n        args.eprojs,\n        odim,\n        args.dtype,\n        args.dlayers,\n        args.dunits,\n        sos,\n        eos,\n        att,\n        args.verbose,\n        args.char_list,\n        labeldist,\n        args.lsm_weight,\n        args.sampling_probability,\n        args.dropout_rate_decoder,\n        getattr(args, ""context_residual"", False),  # use getattr to keep compatibility\n        getattr(args, ""replace_sos"", False),  # use getattr to keep compatibility\n        getattr(args, ""num_encs"", 1),\n    )  # use getattr to keep compatibility\n'"
espnet/nets/pytorch_backend/rnn/encoders.py,42,"b'import logging\nimport six\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\nfrom espnet.nets.e2e_asr_common import get_vgg2l_odim\nfrom espnet.nets.pytorch_backend.nets_utils import make_pad_mask\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\n\n\nclass RNNP(torch.nn.Module):\n    """"""RNN with projection layer module\n\n    :param int idim: dimension of inputs\n    :param int elayers: number of encoder layers\n    :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional)\n    :param int hdim: number of projection units\n    :param np.ndarray subsample: list of subsampling numbers\n    :param float dropout: dropout rate\n    :param str typ: The RNN type\n    """"""\n\n    def __init__(self, idim, elayers, cdim, hdim, subsample, dropout, typ=""blstm""):\n        super(RNNP, self).__init__()\n        bidir = typ[0] == ""b""\n        for i in six.moves.range(elayers):\n            if i == 0:\n                inputdim = idim\n            else:\n                inputdim = hdim\n\n            RNN = torch.nn.LSTM if ""lstm"" in typ else torch.nn.GRU\n            rnn = RNN(\n                inputdim, cdim, num_layers=1, bidirectional=bidir, batch_first=True\n            )\n\n            setattr(self, ""%s%d"" % (""birnn"" if bidir else ""rnn"", i), rnn)\n\n            # bottleneck layer to merge\n            if bidir:\n                setattr(self, ""bt%d"" % i, torch.nn.Linear(2 * cdim, hdim))\n            else:\n                setattr(self, ""bt%d"" % i, torch.nn.Linear(cdim, hdim))\n\n        self.elayers = elayers\n        self.cdim = cdim\n        self.subsample = subsample\n        self.typ = typ\n        self.bidir = bidir\n        self.dropout = dropout\n\n    def forward(self, xs_pad, ilens, prev_state=None):\n        """"""RNNP forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor prev_state: batch of previous RNN states\n        :return: batch of hidden state sequences (B, Tmax, hdim)\n        :rtype: torch.Tensor\n        """"""\n        logging.debug(self.__class__.__name__ + "" input lengths: "" + str(ilens))\n        elayer_states = []\n        for layer in six.moves.range(self.elayers):\n            xs_pack = pack_padded_sequence(xs_pad, ilens, batch_first=True)\n            rnn = getattr(self, (""birnn"" if self.bidir else ""rnn"") + str(layer))\n            rnn.flatten_parameters()\n            if prev_state is not None and rnn.bidirectional:\n                prev_state = reset_backward_rnn_state(prev_state)\n            ys, states = rnn(\n                xs_pack, hx=None if prev_state is None else prev_state[layer]\n            )\n            elayer_states.append(states)\n            # ys: utt list of frame x cdim x 2 (2: means bidirectional)\n            ys_pad, ilens = pad_packed_sequence(ys, batch_first=True)\n            sub = self.subsample[layer + 1]\n            if sub > 1:\n                ys_pad = ys_pad[:, ::sub]\n                ilens = [int(i + 1) // sub for i in ilens]\n            # (sum _utt frame_utt) x dim\n            projection_layer = getattr(self, ""bt%d"" % layer)\n            projected = projection_layer(ys_pad.contiguous().view(-1, ys_pad.size(2)))\n            xs_pad = projected.view(ys_pad.size(0), ys_pad.size(1), -1)\n            if layer < self.elayers - 1:\n                xs_pad = torch.tanh(F.dropout(xs_pad, p=self.dropout))\n\n        return xs_pad, ilens, elayer_states  # x: utt list of frame x dim\n\n\nclass RNN(torch.nn.Module):\n    """"""RNN module\n\n    :param int idim: dimension of inputs\n    :param int elayers: number of encoder layers\n    :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional)\n    :param int hdim: number of final projection units\n    :param float dropout: dropout rate\n    :param str typ: The RNN type\n    """"""\n\n    def __init__(self, idim, elayers, cdim, hdim, dropout, typ=""blstm""):\n        super(RNN, self).__init__()\n        bidir = typ[0] == ""b""\n        self.nbrnn = (\n            torch.nn.LSTM(\n                idim,\n                cdim,\n                elayers,\n                batch_first=True,\n                dropout=dropout,\n                bidirectional=bidir,\n            )\n            if ""lstm"" in typ\n            else torch.nn.GRU(\n                idim,\n                cdim,\n                elayers,\n                batch_first=True,\n                dropout=dropout,\n                bidirectional=bidir,\n            )\n        )\n        if bidir:\n            self.l_last = torch.nn.Linear(cdim * 2, hdim)\n        else:\n            self.l_last = torch.nn.Linear(cdim, hdim)\n        self.typ = typ\n\n    def forward(self, xs_pad, ilens, prev_state=None):\n        """"""RNN forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor prev_state: batch of previous RNN states\n        :return: batch of hidden state sequences (B, Tmax, eprojs)\n        :rtype: torch.Tensor\n        """"""\n        logging.debug(self.__class__.__name__ + "" input lengths: "" + str(ilens))\n        xs_pack = pack_padded_sequence(xs_pad, ilens, batch_first=True)\n        self.nbrnn.flatten_parameters()\n        if prev_state is not None and self.nbrnn.bidirectional:\n            # We assume that when previous state is passed,\n            # it means that we\'re streaming the input\n            # and therefore cannot propagate backward BRNN state\n            # (otherwise it goes in the wrong direction)\n            prev_state = reset_backward_rnn_state(prev_state)\n        ys, states = self.nbrnn(xs_pack, hx=prev_state)\n        # ys: utt list of frame x cdim x 2 (2: means bidirectional)\n        ys_pad, ilens = pad_packed_sequence(ys, batch_first=True)\n        # (sum _utt frame_utt) x dim\n        projected = torch.tanh(\n            self.l_last(ys_pad.contiguous().view(-1, ys_pad.size(2)))\n        )\n        xs_pad = projected.view(ys_pad.size(0), ys_pad.size(1), -1)\n        return xs_pad, ilens, states  # x: utt list of frame x dim\n\n\ndef reset_backward_rnn_state(states):\n    """"""Sets backward BRNN states to zeroes\n\n    Useful in processing of sliding windows over the inputs\n    """"""\n    if isinstance(states, (list, tuple)):\n        for state in states:\n            state[1::2] = 0.0\n    else:\n        states[1::2] = 0.0\n    return states\n\n\nclass VGG2L(torch.nn.Module):\n    """"""VGG-like module\n\n    :param int in_channel: number of input channels\n    """"""\n\n    def __init__(self, in_channel=1):\n        super(VGG2L, self).__init__()\n        # CNN layer (VGG motivated)\n        self.conv1_1 = torch.nn.Conv2d(in_channel, 64, 3, stride=1, padding=1)\n        self.conv1_2 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1)\n        self.conv2_1 = torch.nn.Conv2d(64, 128, 3, stride=1, padding=1)\n        self.conv2_2 = torch.nn.Conv2d(128, 128, 3, stride=1, padding=1)\n\n        self.in_channel = in_channel\n\n    def forward(self, xs_pad, ilens, **kwargs):\n        """"""VGG2L forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :return: batch of padded hidden state sequences (B, Tmax // 4, 128 * D // 4)\n        :rtype: torch.Tensor\n        """"""\n        logging.debug(self.__class__.__name__ + "" input lengths: "" + str(ilens))\n\n        # x: utt x frame x dim\n        # xs_pad = F.pad_sequence(xs_pad)\n\n        # x: utt x 1 (input channel num) x frame x dim\n        xs_pad = xs_pad.view(\n            xs_pad.size(0),\n            xs_pad.size(1),\n            self.in_channel,\n            xs_pad.size(2) // self.in_channel,\n        ).transpose(1, 2)\n\n        # NOTE: max_pool1d ?\n        xs_pad = F.relu(self.conv1_1(xs_pad))\n        xs_pad = F.relu(self.conv1_2(xs_pad))\n        xs_pad = F.max_pool2d(xs_pad, 2, stride=2, ceil_mode=True)\n\n        xs_pad = F.relu(self.conv2_1(xs_pad))\n        xs_pad = F.relu(self.conv2_2(xs_pad))\n        xs_pad = F.max_pool2d(xs_pad, 2, stride=2, ceil_mode=True)\n        if torch.is_tensor(ilens):\n            ilens = ilens.cpu().numpy()\n        else:\n            ilens = np.array(ilens, dtype=np.float32)\n        ilens = np.array(np.ceil(ilens / 2), dtype=np.int64)\n        ilens = np.array(\n            np.ceil(np.array(ilens, dtype=np.float32) / 2), dtype=np.int64\n        ).tolist()\n\n        # x: utt_list of frame (remove zeropaded frames) x (input channel num x dim)\n        xs_pad = xs_pad.transpose(1, 2)\n        xs_pad = xs_pad.contiguous().view(\n            xs_pad.size(0), xs_pad.size(1), xs_pad.size(2) * xs_pad.size(3)\n        )\n        return xs_pad, ilens, None  # no state in this layer\n\n\nclass Encoder(torch.nn.Module):\n    """"""Encoder module\n\n    :param str etype: type of encoder network\n    :param int idim: number of dimensions of encoder network\n    :param int elayers: number of layers of encoder network\n    :param int eunits: number of lstm units of encoder network\n    :param int eprojs: number of projection units of encoder network\n    :param np.ndarray subsample: list of subsampling numbers\n    :param float dropout: dropout rate\n    :param int in_channel: number of input channels\n    """"""\n\n    def __init__(\n        self, etype, idim, elayers, eunits, eprojs, subsample, dropout, in_channel=1\n    ):\n        super(Encoder, self).__init__()\n        typ = etype.lstrip(""vgg"").rstrip(""p"")\n        if typ not in [""lstm"", ""gru"", ""blstm"", ""bgru""]:\n            logging.error(""Error: need to specify an appropriate encoder architecture"")\n\n        if etype.startswith(""vgg""):\n            if etype[-1] == ""p"":\n                self.enc = torch.nn.ModuleList(\n                    [\n                        VGG2L(in_channel),\n                        RNNP(\n                            get_vgg2l_odim(idim, in_channel=in_channel),\n                            elayers,\n                            eunits,\n                            eprojs,\n                            subsample,\n                            dropout,\n                            typ=typ,\n                        ),\n                    ]\n                )\n                logging.info(""Use CNN-VGG + "" + typ.upper() + ""P for encoder"")\n            else:\n                self.enc = torch.nn.ModuleList(\n                    [\n                        VGG2L(in_channel),\n                        RNN(\n                            get_vgg2l_odim(idim, in_channel=in_channel),\n                            elayers,\n                            eunits,\n                            eprojs,\n                            dropout,\n                            typ=typ,\n                        ),\n                    ]\n                )\n                logging.info(""Use CNN-VGG + "" + typ.upper() + "" for encoder"")\n        else:\n            if etype[-1] == ""p"":\n                self.enc = torch.nn.ModuleList(\n                    [RNNP(idim, elayers, eunits, eprojs, subsample, dropout, typ=typ)]\n                )\n                logging.info(typ.upper() + "" with every-layer projection for encoder"")\n            else:\n                self.enc = torch.nn.ModuleList(\n                    [RNN(idim, elayers, eunits, eprojs, dropout, typ=typ)]\n                )\n                logging.info(typ.upper() + "" without projection for encoder"")\n\n    def forward(self, xs_pad, ilens, prev_states=None):\n        """"""Encoder forward\n\n        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, D)\n        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n        :param torch.Tensor prev_state: batch of previous encoder hidden states (?, ...)\n        :return: batch of hidden state sequences (B, Tmax, eprojs)\n        :rtype: torch.Tensor\n        """"""\n        if prev_states is None:\n            prev_states = [None] * len(self.enc)\n        assert len(prev_states) == len(self.enc)\n\n        current_states = []\n        for module, prev_state in zip(self.enc, prev_states):\n            xs_pad, ilens, states = module(xs_pad, ilens, prev_state=prev_state)\n            current_states.append(states)\n\n        # make mask to remove bias value in padded part\n        mask = to_device(self, make_pad_mask(ilens).unsqueeze(-1))\n\n        return xs_pad.masked_fill(mask, 0.0), ilens, current_states\n\n\ndef encoder_for(args, idim, subsample):\n    """"""Instantiates an encoder module given the program arguments\n\n    :param Namespace args: The arguments\n    :param int or List of integer idim: dimension of input, e.g. 83, or\n                                        List of dimensions of inputs, e.g. [83,83]\n    :param List or List of List subsample: subsample factors, e.g. [1,2,2,1,1], or\n                                        List of subsample factors of each encoder.\n                                         e.g. [[1,2,2,1,1], [1,2,2,1,1]]\n    :rtype torch.nn.Module\n    :return: The encoder module\n    """"""\n    num_encs = getattr(args, ""num_encs"", 1)  # use getattr to keep compatibility\n    if num_encs == 1:\n        # compatible with single encoder asr mode\n        return Encoder(\n            args.etype,\n            idim,\n            args.elayers,\n            args.eunits,\n            args.eprojs,\n            subsample,\n            args.dropout_rate,\n        )\n    elif num_encs >= 1:\n        enc_list = torch.nn.ModuleList()\n        for idx in range(num_encs):\n            enc = Encoder(\n                args.etype[idx],\n                idim[idx],\n                args.elayers[idx],\n                args.eunits[idx],\n                args.eprojs,\n                subsample[idx],\n                args.dropout_rate[idx],\n            )\n            enc_list.append(enc)\n        return enc_list\n    else:\n        raise ValueError(\n            ""Number of encoders needs to be more than one. {}"".format(num_encs)\n        )\n'"
espnet/nets/pytorch_backend/streaming/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/streaming/segment.py,3,"b'import numpy as np\nimport torch\n\n\nclass SegmentStreamingE2E(object):\n    """"""SegmentStreamingE2E constructor.\n\n    :param E2E e2e: E2E ASR object\n    :param recog_args: arguments for ""recognize"" method of E2E\n    """"""\n\n    def __init__(self, e2e, recog_args, rnnlm=None):\n        self._e2e = e2e\n        self._recog_args = recog_args\n        self._char_list = e2e.char_list\n        self._rnnlm = rnnlm\n\n        self._e2e.eval()\n\n        self._blank_idx_in_char_list = -1\n        for idx in range(len(self._char_list)):\n            if self._char_list[idx] == self._e2e.blank:\n                self._blank_idx_in_char_list = idx\n                break\n\n        self._subsampling_factor = np.prod(e2e.subsample)\n        self._activates = 0\n        self._blank_dur = 0\n\n        self._previous_input = []\n        self._previous_encoder_recurrent_state = None\n        self._encoder_states = []\n        self._ctc_posteriors = []\n\n        assert (\n            self._recog_args.batchsize <= 1\n        ), ""SegmentStreamingE2E works only with batch size <= 1""\n        assert (\n            ""b"" not in self._e2e.etype\n        ), ""SegmentStreamingE2E works only with uni-directional encoders""\n\n    def accept_input(self, x):\n        """"""Call this method each time a new batch of input is available.""""""\n\n        self._previous_input.extend(x)\n        h, ilen = self._e2e.subsample_frames(x)\n\n        # Run encoder and apply greedy search on CTC softmax output\n        h, _, self._previous_encoder_recurrent_state = self._e2e.enc(\n            h.unsqueeze(0), ilen, self._previous_encoder_recurrent_state\n        )\n        z = self._e2e.ctc.argmax(h).squeeze(0)\n\n        if self._activates == 0 and z[0] != self._blank_idx_in_char_list:\n            self._activates = 1\n\n            # Rerun encoder with zero state at onset of detection\n            tail_len = self._subsampling_factor * (\n                self._recog_args.streaming_onset_margin + 1\n            )\n            h, ilen = self._e2e.subsample_frames(\n                np.reshape(\n                    self._previous_input[-tail_len:], [-1, len(self._previous_input[0])]\n                )\n            )\n            h, _, self._previous_encoder_recurrent_state = self._e2e.enc(\n                h.unsqueeze(0), ilen, None\n            )\n\n        hyp = None\n        if self._activates == 1:\n            self._encoder_states.extend(h.squeeze(0))\n            self._ctc_posteriors.extend(self._e2e.ctc.log_softmax(h).squeeze(0))\n\n            if z[0] == self._blank_idx_in_char_list:\n                self._blank_dur += 1\n            else:\n                self._blank_dur = 0\n\n            if self._blank_dur >= self._recog_args.streaming_min_blank_dur:\n                seg_len = (\n                    len(self._encoder_states)\n                    - self._blank_dur\n                    + self._recog_args.streaming_offset_margin\n                )\n                if seg_len > 0:\n                    # Run decoder with a detected segment\n                    h = torch.cat(self._encoder_states[:seg_len], dim=0).view(\n                        -1, self._encoder_states[0].size(0)\n                    )\n                    if self._recog_args.ctc_weight > 0.0:\n                        lpz = torch.cat(self._ctc_posteriors[:seg_len], dim=0).view(\n                            -1, self._ctc_posteriors[0].size(0)\n                        )\n                        if self._recog_args.batchsize > 0:\n                            lpz = lpz.unsqueeze(0)\n                        normalize_score = False\n                    else:\n                        lpz = None\n                        normalize_score = True\n\n                    if self._recog_args.batchsize == 0:\n                        hyp = self._e2e.dec.recognize_beam(\n                            h, lpz, self._recog_args, self._char_list, self._rnnlm\n                        )\n                    else:\n                        hlens = torch.tensor([h.shape[0]])\n                        hyp = self._e2e.dec.recognize_beam_batch(\n                            h.unsqueeze(0),\n                            hlens,\n                            lpz,\n                            self._recog_args,\n                            self._char_list,\n                            self._rnnlm,\n                            normalize_score=normalize_score,\n                        )[0]\n\n                    self._activates = 0\n                    self._blank_dur = 0\n\n                    tail_len = (\n                        self._subsampling_factor\n                        * self._recog_args.streaming_onset_margin\n                    )\n                    self._previous_input = self._previous_input[-tail_len:]\n                    self._encoder_states = []\n                    self._ctc_posteriors = []\n\n        return hyp\n'"
espnet/nets/pytorch_backend/streaming/window.py,3,"b'import torch\n\n\n# TODO(pzelasko): Currently allows half-streaming only;\n#  needs streaming attention decoder implementation\nclass WindowStreamingE2E(object):\n    """"""WindowStreamingE2E constructor.\n\n    :param E2E e2e: E2E ASR object\n    :param recog_args: arguments for ""recognize"" method of E2E\n    """"""\n\n    def __init__(self, e2e, recog_args, rnnlm=None):\n        self._e2e = e2e\n        self._recog_args = recog_args\n        self._char_list = e2e.char_list\n        self._rnnlm = rnnlm\n\n        self._e2e.eval()\n\n        self._offset = 0\n        self._previous_encoder_recurrent_state = None\n        self._encoder_states = []\n        self._ctc_posteriors = []\n        self._last_recognition = None\n\n        assert (\n            self._recog_args.ctc_weight > 0.0\n        ), ""WindowStreamingE2E works only with combined CTC and attention decoders.""\n\n    def accept_input(self, x):\n        """"""Call this method each time a new batch of input is available.""""""\n\n        h, ilen = self._e2e.subsample_frames(x)\n\n        # Streaming encoder\n        h, _, self._previous_encoder_recurrent_state = self._e2e.enc(\n            h.unsqueeze(0), ilen, self._previous_encoder_recurrent_state\n        )\n        self._encoder_states.append(h.squeeze(0))\n\n        # CTC posteriors for the incoming audio\n        self._ctc_posteriors.append(self._e2e.ctc.log_softmax(h).squeeze(0))\n\n    def _input_window_for_decoder(self, use_all=False):\n        if use_all:\n            return (\n                torch.cat(self._encoder_states, dim=0),\n                torch.cat(self._ctc_posteriors, dim=0),\n            )\n\n        def select_unprocessed_windows(window_tensors):\n            last_offset = self._offset\n            offset_traversed = 0\n            selected_windows = []\n            for es in window_tensors:\n                if offset_traversed > last_offset:\n                    selected_windows.append(es)\n                    continue\n                offset_traversed += es.size(1)\n            return torch.cat(selected_windows, dim=0)\n\n        return (\n            select_unprocessed_windows(self._encoder_states),\n            select_unprocessed_windows(self._ctc_posteriors),\n        )\n\n    def decode_with_attention_offline(self):\n        """"""Run the attention decoder offline.\n\n        Works even if the previous layers (encoder and CTC decoder) were\n        being run in the online mode.\n        This method should be run after all the audio has been consumed.\n        This is used mostly to compare the results between offline\n        and online implementation of the previous layers.\n        """"""\n        h, lpz = self._input_window_for_decoder(use_all=True)\n\n        return self._e2e.dec.recognize_beam(\n            h, lpz, self._recog_args, self._char_list, self._rnnlm\n        )\n'"
espnet/nets/pytorch_backend/tacotron2/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/tacotron2/cbhg.py,28,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""CBHG related modules.""""""\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\nfrom espnet.nets.pytorch_backend.nets_utils import make_non_pad_mask\n\n\nclass CBHGLoss(torch.nn.Module):\n    """"""Loss function module for CBHG.""""""\n\n    def __init__(self, use_masking=True):\n        """"""Initialize CBHG loss module.\n\n        Args:\n            use_masking (bool): Whether to mask padded part in loss calculation.\n\n        """"""\n        super(CBHGLoss, self).__init__()\n        self.use_masking = use_masking\n\n    def forward(self, cbhg_outs, spcs, olens):\n        """"""Calculate forward propagation.\n\n        Args:\n            cbhg_outs (Tensor): Batch of CBHG outputs (B, Lmax, spc_dim).\n            spcs (Tensor): Batch of groundtruth of spectrogram (B, Lmax, spc_dim).\n            olens (LongTensor): Batch of the lengths of each sequence (B,).\n\n        Returns:\n            Tensor: L1 loss value\n            Tensor: Mean square error loss value.\n\n        """"""\n        # perform masking for padded values\n        if self.use_masking:\n            mask = make_non_pad_mask(olens).unsqueeze(-1).to(spcs.device)\n            spcs = spcs.masked_select(mask)\n            cbhg_outs = cbhg_outs.masked_select(mask)\n\n        # calculate loss\n        cbhg_l1_loss = F.l1_loss(cbhg_outs, spcs)\n        cbhg_mse_loss = F.mse_loss(cbhg_outs, spcs)\n\n        return cbhg_l1_loss, cbhg_mse_loss\n\n\nclass CBHG(torch.nn.Module):\n    """"""CBHG module to convert log Mel-filterbanks to linear spectrogram.\n\n    This is a module of CBHG introduced\n    in `Tacotron: Towards End-to-End Speech Synthesis`_.\n    The CBHG converts the sequence of log Mel-filterbanks into linear spectrogram.\n\n    .. _`Tacotron: Towards End-to-End Speech Synthesis`:\n         https://arxiv.org/abs/1703.10135\n\n    """"""\n\n    def __init__(\n        self,\n        idim,\n        odim,\n        conv_bank_layers=8,\n        conv_bank_chans=128,\n        conv_proj_filts=3,\n        conv_proj_chans=256,\n        highway_layers=4,\n        highway_units=128,\n        gru_units=256,\n    ):\n        """"""Initialize CBHG module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n            odim (int): Dimension of the outputs.\n            conv_bank_layers (int, optional): The number of convolution bank layers.\n            conv_bank_chans (int, optional): The number of channels in convolution bank.\n            conv_proj_filts (int, optional):\n                Kernel size of convolutional projection layer.\n            conv_proj_chans (int, optional):\n                The number of channels in convolutional projection layer.\n            highway_layers (int, optional): The number of highway network layers.\n            highway_units (int, optional): The number of highway network units.\n            gru_units (int, optional): The number of GRU units (for both directions).\n\n        """"""\n        super(CBHG, self).__init__()\n        self.idim = idim\n        self.odim = odim\n        self.conv_bank_layers = conv_bank_layers\n        self.conv_bank_chans = conv_bank_chans\n        self.conv_proj_filts = conv_proj_filts\n        self.conv_proj_chans = conv_proj_chans\n        self.highway_layers = highway_layers\n        self.highway_units = highway_units\n        self.gru_units = gru_units\n\n        # define 1d convolution bank\n        self.conv_bank = torch.nn.ModuleList()\n        for k in range(1, self.conv_bank_layers + 1):\n            if k % 2 != 0:\n                padding = (k - 1) // 2\n            else:\n                padding = ((k - 1) // 2, (k - 1) // 2 + 1)\n            self.conv_bank += [\n                torch.nn.Sequential(\n                    torch.nn.ConstantPad1d(padding, 0.0),\n                    torch.nn.Conv1d(\n                        idim, self.conv_bank_chans, k, stride=1, padding=0, bias=True\n                    ),\n                    torch.nn.BatchNorm1d(self.conv_bank_chans),\n                    torch.nn.ReLU(),\n                )\n            ]\n\n        # define max pooling (need padding for one-side to keep same length)\n        self.max_pool = torch.nn.Sequential(\n            torch.nn.ConstantPad1d((0, 1), 0.0), torch.nn.MaxPool1d(2, stride=1)\n        )\n\n        # define 1d convolution projection\n        self.projections = torch.nn.Sequential(\n            torch.nn.Conv1d(\n                self.conv_bank_chans * self.conv_bank_layers,\n                self.conv_proj_chans,\n                self.conv_proj_filts,\n                stride=1,\n                padding=(self.conv_proj_filts - 1) // 2,\n                bias=True,\n            ),\n            torch.nn.BatchNorm1d(self.conv_proj_chans),\n            torch.nn.ReLU(),\n            torch.nn.Conv1d(\n                self.conv_proj_chans,\n                self.idim,\n                self.conv_proj_filts,\n                stride=1,\n                padding=(self.conv_proj_filts - 1) // 2,\n                bias=True,\n            ),\n            torch.nn.BatchNorm1d(self.idim),\n        )\n\n        # define highway network\n        self.highways = torch.nn.ModuleList()\n        self.highways += [torch.nn.Linear(idim, self.highway_units)]\n        for _ in range(self.highway_layers):\n            self.highways += [HighwayNet(self.highway_units)]\n\n        # define bidirectional GRU\n        self.gru = torch.nn.GRU(\n            self.highway_units,\n            gru_units // 2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        # define final projection\n        self.output = torch.nn.Linear(gru_units, odim, bias=True)\n\n    def forward(self, xs, ilens):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of the padded sequences of inputs (B, Tmax, idim).\n            ilens (LongTensor): Batch of lengths of each input sequence (B,).\n\n        Return:\n            Tensor: Batch of the padded sequence of outputs (B, Tmax, odim).\n            LongTensor: Batch of lengths of each output sequence (B,).\n\n        """"""\n        xs = xs.transpose(1, 2)  # (B, idim, Tmax)\n        convs = []\n        for k in range(self.conv_bank_layers):\n            convs += [self.conv_bank[k](xs)]\n        convs = torch.cat(convs, dim=1)  # (B, #CH * #BANK, Tmax)\n        convs = self.max_pool(convs)\n        convs = self.projections(convs).transpose(1, 2)  # (B, Tmax, idim)\n        xs = xs.transpose(1, 2) + convs\n        # + 1 for dimension adjustment layer\n        for i in range(self.highway_layers + 1):\n            xs = self.highways[i](xs)\n\n        # sort by length\n        xs, ilens, sort_idx = self._sort_by_length(xs, ilens)\n\n        # total_length needs for DataParallel\n        # (see https://github.com/pytorch/pytorch/pull/6327)\n        total_length = xs.size(1)\n        xs = pack_padded_sequence(xs, ilens, batch_first=True)\n        self.gru.flatten_parameters()\n        xs, _ = self.gru(xs)\n        xs, ilens = pad_packed_sequence(xs, batch_first=True, total_length=total_length)\n\n        # revert sorting by length\n        xs, ilens = self._revert_sort_by_length(xs, ilens, sort_idx)\n\n        xs = self.output(xs)  # (B, Tmax, odim)\n\n        return xs, ilens\n\n    def inference(self, x):\n        """"""Inference.\n\n        Args:\n            x (Tensor): The sequences of inputs (T, idim).\n\n        Return:\n            Tensor: The sequence of outputs (T, odim).\n\n        """"""\n        assert len(x.size()) == 2\n        xs = x.unsqueeze(0)\n        ilens = x.new([x.size(0)]).long()\n\n        return self.forward(xs, ilens)[0][0]\n\n    def _sort_by_length(self, xs, ilens):\n        sort_ilens, sort_idx = ilens.sort(0, descending=True)\n        return xs[sort_idx], ilens[sort_idx], sort_idx\n\n    def _revert_sort_by_length(self, xs, ilens, sort_idx):\n        _, revert_idx = sort_idx.sort(0)\n        return xs[revert_idx], ilens[revert_idx]\n\n\nclass HighwayNet(torch.nn.Module):\n    """"""Highway Network module.\n\n    This is a module of Highway Network introduced in `Highway Networks`_.\n\n    .. _`Highway Networks`: https://arxiv.org/abs/1505.00387\n\n    """"""\n\n    def __init__(self, idim):\n        """"""Initialize Highway Network module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n\n        """"""\n        super(HighwayNet, self).__init__()\n        self.idim = idim\n        self.projection = torch.nn.Sequential(\n            torch.nn.Linear(idim, idim), torch.nn.ReLU()\n        )\n        self.gate = torch.nn.Sequential(torch.nn.Linear(idim, idim), torch.nn.Sigmoid())\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of inputs (B, ..., idim).\n\n        Returns:\n            Tensor: Batch of outputs, which are the same shape as inputs (B, ..., idim).\n\n        """"""\n        proj = self.projection(x)\n        gate = self.gate(x)\n        return proj * gate + x * (1.0 - gate)\n'"
espnet/nets/pytorch_backend/tacotron2/decoder.py,48,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Tacotron2 decoder related modules.""""""\n\nimport six\n\nimport torch\nimport torch.nn.functional as F\n\nfrom espnet.nets.pytorch_backend.rnn.attentions import AttForwardTA\n\n\ndef decoder_init(m):\n    """"""Initialize decoder parameters.""""""\n    if isinstance(m, torch.nn.Conv1d):\n        torch.nn.init.xavier_uniform_(m.weight, torch.nn.init.calculate_gain(""tanh""))\n\n\nclass ZoneOutCell(torch.nn.Module):\n    """"""ZoneOut Cell module.\n\n    This is a module of zoneout described in\n    `Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations`_.\n    This code is modified from `eladhoffer/seq2seq.pytorch`_.\n\n    Examples:\n        >>> lstm = torch.nn.LSTMCell(16, 32)\n        >>> lstm = ZoneOutCell(lstm, 0.5)\n\n    .. _`Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations`:\n        https://arxiv.org/abs/1606.01305\n\n    .. _`eladhoffer/seq2seq.pytorch`:\n        https://github.com/eladhoffer/seq2seq.pytorch\n\n    """"""\n\n    def __init__(self, cell, zoneout_rate=0.1):\n        """"""Initialize zone out cell module.\n\n        Args:\n            cell (torch.nn.Module): Pytorch recurrent cell module\n                e.g. `torch.nn.Module.LSTMCell`.\n            zoneout_rate (float, optional): Probability of zoneout from 0.0 to 1.0.\n\n        """"""\n        super(ZoneOutCell, self).__init__()\n        self.cell = cell\n        self.hidden_size = cell.hidden_size\n        self.zoneout_rate = zoneout_rate\n        if zoneout_rate > 1.0 or zoneout_rate < 0.0:\n            raise ValueError(\n                ""zoneout probability must be in the range from 0.0 to 1.0.""\n            )\n\n    def forward(self, inputs, hidden):\n        """"""Calculate forward propagation.\n\n        Args:\n            inputs (Tensor): Batch of input tensor (B, input_size).\n            hidden (tuple):\n                - Tensor: Batch of initial hidden states (B, hidden_size).\n                - Tensor: Batch of initial cell states (B, hidden_size).\n\n        Returns:\n            tuple:\n                - Tensor: Batch of next hidden states (B, hidden_size).\n                - Tensor: Batch of next cell states (B, hidden_size).\n\n        """"""\n        next_hidden = self.cell(inputs, hidden)\n        next_hidden = self._zoneout(hidden, next_hidden, self.zoneout_rate)\n        return next_hidden\n\n    def _zoneout(self, h, next_h, prob):\n        # apply recursively\n        if isinstance(h, tuple):\n            num_h = len(h)\n            if not isinstance(prob, tuple):\n                prob = tuple([prob] * num_h)\n            return tuple(\n                [self._zoneout(h[i], next_h[i], prob[i]) for i in range(num_h)]\n            )\n\n        if self.training:\n            mask = h.new(*h.size()).bernoulli_(prob)\n            return mask * h + (1 - mask) * next_h\n        else:\n            return prob * h + (1 - prob) * next_h\n\n\nclass Prenet(torch.nn.Module):\n    """"""Prenet module for decoder of Spectrogram prediction network.\n\n    This is a module of Prenet in the decoder of Spectrogram prediction network,\n    which described in `Natural TTS\n    Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`_.\n    The Prenet preforms nonlinear conversion\n    of inputs before input to auto-regressive lstm,\n    which helps to learn diagonal attentions.\n\n    Note:\n        This module alway applies dropout even in evaluation.\n        See the detail in `Natural TTS Synthesis by\n        Conditioning WaveNet on Mel Spectrogram Predictions`_.\n\n    .. _`Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`:\n       https://arxiv.org/abs/1712.05884\n\n    """"""\n\n    def __init__(self, idim, n_layers=2, n_units=256, dropout_rate=0.5):\n        """"""Initialize prenet module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n            odim (int): Dimension of the outputs.\n            n_layers (int, optional): The number of prenet layers.\n            n_units (int, optional): The number of prenet units.\n\n        """"""\n        super(Prenet, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.prenet = torch.nn.ModuleList()\n        for layer in six.moves.range(n_layers):\n            n_inputs = idim if layer == 0 else n_units\n            self.prenet += [\n                torch.nn.Sequential(torch.nn.Linear(n_inputs, n_units), torch.nn.ReLU())\n            ]\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of input tensors (B, ..., idim).\n\n        Returns:\n            Tensor: Batch of output tensors (B, ..., odim).\n\n        """"""\n        for i in six.moves.range(len(self.prenet)):\n            x = F.dropout(self.prenet[i](x), self.dropout_rate)\n        return x\n\n\nclass Postnet(torch.nn.Module):\n    """"""Postnet module for Spectrogram prediction network.\n\n    This is a module of Postnet in Spectrogram prediction network,\n    which described in `Natural TTS Synthesis by\n    Conditioning WaveNet on Mel Spectrogram Predictions`_.\n    The Postnet predicts refines the predicted\n    Mel-filterbank of the decoder,\n    which helps to compensate the detail sturcture of spectrogram.\n\n    .. _`Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`:\n       https://arxiv.org/abs/1712.05884\n\n    """"""\n\n    def __init__(\n        self,\n        idim,\n        odim,\n        n_layers=5,\n        n_chans=512,\n        n_filts=5,\n        dropout_rate=0.5,\n        use_batch_norm=True,\n    ):\n        """"""Initialize postnet module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n            odim (int): Dimension of the outputs.\n            n_layers (int, optional): The number of layers.\n            n_filts (int, optional): The number of filter size.\n            n_units (int, optional): The number of filter channels.\n            use_batch_norm (bool, optional): Whether to use batch normalization..\n            dropout_rate (float, optional): Dropout rate..\n\n        """"""\n        super(Postnet, self).__init__()\n        self.postnet = torch.nn.ModuleList()\n        for layer in six.moves.range(n_layers - 1):\n            ichans = odim if layer == 0 else n_chans\n            ochans = odim if layer == n_layers - 1 else n_chans\n            if use_batch_norm:\n                self.postnet += [\n                    torch.nn.Sequential(\n                        torch.nn.Conv1d(\n                            ichans,\n                            ochans,\n                            n_filts,\n                            stride=1,\n                            padding=(n_filts - 1) // 2,\n                            bias=False,\n                        ),\n                        torch.nn.BatchNorm1d(ochans),\n                        torch.nn.Tanh(),\n                        torch.nn.Dropout(dropout_rate),\n                    )\n                ]\n            else:\n                self.postnet += [\n                    torch.nn.Sequential(\n                        torch.nn.Conv1d(\n                            ichans,\n                            ochans,\n                            n_filts,\n                            stride=1,\n                            padding=(n_filts - 1) // 2,\n                            bias=False,\n                        ),\n                        torch.nn.Tanh(),\n                        torch.nn.Dropout(dropout_rate),\n                    )\n                ]\n        ichans = n_chans if n_layers != 1 else odim\n        if use_batch_norm:\n            self.postnet += [\n                torch.nn.Sequential(\n                    torch.nn.Conv1d(\n                        ichans,\n                        odim,\n                        n_filts,\n                        stride=1,\n                        padding=(n_filts - 1) // 2,\n                        bias=False,\n                    ),\n                    torch.nn.BatchNorm1d(odim),\n                    torch.nn.Dropout(dropout_rate),\n                )\n            ]\n        else:\n            self.postnet += [\n                torch.nn.Sequential(\n                    torch.nn.Conv1d(\n                        ichans,\n                        odim,\n                        n_filts,\n                        stride=1,\n                        padding=(n_filts - 1) // 2,\n                        bias=False,\n                    ),\n                    torch.nn.Dropout(dropout_rate),\n                )\n            ]\n\n    def forward(self, xs):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of the sequences of padded input tensors (B, idim, Tmax).\n\n        Returns:\n            Tensor: Batch of padded output tensor. (B, odim, Tmax).\n\n        """"""\n        for i in six.moves.range(len(self.postnet)):\n            xs = self.postnet[i](xs)\n        return xs\n\n\nclass Decoder(torch.nn.Module):\n    """"""Decoder module of Spectrogram prediction network.\n\n    This is a module of decoder of Spectrogram prediction network in Tacotron2,\n    which described in `Natural TTS\n    Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`_.\n    The decoder generates the sequence of\n    features from the sequence of the hidden states.\n\n    .. _`Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`:\n       https://arxiv.org/abs/1712.05884\n\n    """"""\n\n    def __init__(\n        self,\n        idim,\n        odim,\n        att,\n        dlayers=2,\n        dunits=1024,\n        prenet_layers=2,\n        prenet_units=256,\n        postnet_layers=5,\n        postnet_chans=512,\n        postnet_filts=5,\n        output_activation_fn=None,\n        cumulate_att_w=True,\n        use_batch_norm=True,\n        use_concate=True,\n        dropout_rate=0.5,\n        zoneout_rate=0.1,\n        reduction_factor=1,\n    ):\n        """"""Initialize Tacotron2 decoder module.\n\n        Args:\n            idim (int): Dimension of the inputs.\n            odim (int): Dimension of the outputs.\n            att (torch.nn.Module): Instance of attention class.\n            dlayers (int, optional): The number of decoder lstm layers.\n            dunits (int, optional): The number of decoder lstm units.\n            prenet_layers (int, optional): The number of prenet layers.\n            prenet_units (int, optional): The number of prenet units.\n            postnet_layers (int, optional): The number of postnet layers.\n            postnet_filts (int, optional): The number of postnet filter size.\n            postnet_chans (int, optional): The number of postnet filter channels.\n            output_activation_fn (torch.nn.Module, optional):\n                Activation function for outputs.\n            cumulate_att_w (bool, optional):\n                Whether to cumulate previous attention weight.\n            use_batch_norm (bool, optional): Whether to use batch normalization.\n            use_concate (bool, optional): Whether to concatenate encoder embedding\n                with decoder lstm outputs.\n            dropout_rate (float, optional): Dropout rate.\n            zoneout_rate (float, optional): Zoneout rate.\n            reduction_factor (int, optional): Reduction factor.\n\n        """"""\n        super(Decoder, self).__init__()\n\n        # store the hyperparameters\n        self.idim = idim\n        self.odim = odim\n        self.att = att\n        self.output_activation_fn = output_activation_fn\n        self.cumulate_att_w = cumulate_att_w\n        self.use_concate = use_concate\n        self.reduction_factor = reduction_factor\n\n        # check attention type\n        if isinstance(self.att, AttForwardTA):\n            self.use_att_extra_inputs = True\n        else:\n            self.use_att_extra_inputs = False\n\n        # define lstm network\n        prenet_units = prenet_units if prenet_layers != 0 else odim\n        self.lstm = torch.nn.ModuleList()\n        for layer in six.moves.range(dlayers):\n            iunits = idim + prenet_units if layer == 0 else dunits\n            lstm = torch.nn.LSTMCell(iunits, dunits)\n            if zoneout_rate > 0.0:\n                lstm = ZoneOutCell(lstm, zoneout_rate)\n            self.lstm += [lstm]\n\n        # define prenet\n        if prenet_layers > 0:\n            self.prenet = Prenet(\n                idim=odim,\n                n_layers=prenet_layers,\n                n_units=prenet_units,\n                dropout_rate=dropout_rate,\n            )\n        else:\n            self.prenet = None\n\n        # define postnet\n        if postnet_layers > 0:\n            self.postnet = Postnet(\n                idim=idim,\n                odim=odim,\n                n_layers=postnet_layers,\n                n_chans=postnet_chans,\n                n_filts=postnet_filts,\n                use_batch_norm=use_batch_norm,\n                dropout_rate=dropout_rate,\n            )\n        else:\n            self.postnet = None\n\n        # define projection layers\n        iunits = idim + dunits if use_concate else dunits\n        self.feat_out = torch.nn.Linear(iunits, odim * reduction_factor, bias=False)\n        self.prob_out = torch.nn.Linear(iunits, reduction_factor)\n\n        # initialize\n        self.apply(decoder_init)\n\n    def _zero_state(self, hs):\n        init_hs = hs.new_zeros(hs.size(0), self.lstm[0].hidden_size)\n        return init_hs\n\n    def forward(self, hs, hlens, ys):\n        """"""Calculate forward propagation.\n\n        Args:\n            hs (Tensor): Batch of the sequences of padded hidden states (B, Tmax, idim).\n            hlens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor):\n                Batch of the sequences of padded target features (B, Lmax, odim).\n\n        Returns:\n            Tensor: Batch of output tensors after postnet (B, Lmax, odim).\n            Tensor: Batch of output tensors before postnet (B, Lmax, odim).\n            Tensor: Batch of logits of stop prediction (B, Lmax).\n            Tensor: Batch of attention weights (B, Lmax, Tmax).\n\n        Note:\n            This computation is performed in teacher-forcing manner.\n\n        """"""\n        # thin out frames (B, Lmax, odim) ->  (B, Lmax/r, odim)\n        if self.reduction_factor > 1:\n            ys = ys[:, self.reduction_factor - 1 :: self.reduction_factor]\n\n        # length list should be list of int\n        hlens = list(map(int, hlens))\n\n        # initialize hidden states of decoder\n        c_list = [self._zero_state(hs)]\n        z_list = [self._zero_state(hs)]\n        for _ in six.moves.range(1, len(self.lstm)):\n            c_list += [self._zero_state(hs)]\n            z_list += [self._zero_state(hs)]\n        prev_out = hs.new_zeros(hs.size(0), self.odim)\n\n        # initialize attention\n        prev_att_w = None\n        self.att.reset()\n\n        # loop for an output sequence\n        outs, logits, att_ws = [], [], []\n        for y in ys.transpose(0, 1):\n            if self.use_att_extra_inputs:\n                att_c, att_w = self.att(hs, hlens, z_list[0], prev_att_w, prev_out)\n            else:\n                att_c, att_w = self.att(hs, hlens, z_list[0], prev_att_w)\n            prenet_out = self.prenet(prev_out) if self.prenet is not None else prev_out\n            xs = torch.cat([att_c, prenet_out], dim=1)\n            z_list[0], c_list[0] = self.lstm[0](xs, (z_list[0], c_list[0]))\n            for i in six.moves.range(1, len(self.lstm)):\n                z_list[i], c_list[i] = self.lstm[i](\n                    z_list[i - 1], (z_list[i], c_list[i])\n                )\n            zcs = (\n                torch.cat([z_list[-1], att_c], dim=1)\n                if self.use_concate\n                else z_list[-1]\n            )\n            outs += [self.feat_out(zcs).view(hs.size(0), self.odim, -1)]\n            logits += [self.prob_out(zcs)]\n            att_ws += [att_w]\n            prev_out = y  # teacher forcing\n            if self.cumulate_att_w and prev_att_w is not None:\n                prev_att_w = prev_att_w + att_w  # Note: error when use +=\n            else:\n                prev_att_w = att_w\n\n        logits = torch.cat(logits, dim=1)  # (B, Lmax)\n        before_outs = torch.cat(outs, dim=2)  # (B, odim, Lmax)\n        att_ws = torch.stack(att_ws, dim=1)  # (B, Lmax, Tmax)\n\n        if self.reduction_factor > 1:\n            before_outs = before_outs.view(\n                before_outs.size(0), self.odim, -1\n            )  # (B, odim, Lmax)\n\n        if self.postnet is not None:\n            after_outs = before_outs + self.postnet(before_outs)  # (B, odim, Lmax)\n        else:\n            after_outs = before_outs\n        before_outs = before_outs.transpose(2, 1)  # (B, Lmax, odim)\n        after_outs = after_outs.transpose(2, 1)  # (B, Lmax, odim)\n        logits = logits\n\n        # apply activation function for scaling\n        if self.output_activation_fn is not None:\n            before_outs = self.output_activation_fn(before_outs)\n            after_outs = self.output_activation_fn(after_outs)\n\n        return after_outs, before_outs, logits, att_ws\n\n    def inference(\n        self,\n        h,\n        threshold=0.5,\n        minlenratio=0.0,\n        maxlenratio=10.0,\n        use_att_constraint=False,\n        backward_window=None,\n        forward_window=None,\n    ):\n        """"""Generate the sequence of features given the sequences of characters.\n\n        Args:\n            h (Tensor): Input sequence of encoder hidden states (T, C).\n            threshold (float, optional): Threshold to stop generation.\n            minlenratio (float, optional): Minimum length ratio.\n                If set to 1.0 and the length of input is 10,\n                the minimum length of outputs will be 10 * 1 = 10.\n            minlenratio (float, optional): Minimum length ratio.\n                If set to 10 and the length of input is 10,\n                the maximum length of outputs will be 10 * 10 = 100.\n            use_att_constraint (bool):\n                Whether to apply attention constraint introduced in `Deep Voice 3`_.\n            backward_window (int): Backward window size in attention constraint.\n            forward_window (int): Forward window size in attention constraint.\n\n        Returns:\n            Tensor: Output sequence of features (L, odim).\n            Tensor: Output sequence of stop probabilities (L,).\n            Tensor: Attention weights (L, T).\n\n        Note:\n            This computation is performed in auto-regressive manner.\n\n        .. _`Deep Voice 3`: https://arxiv.org/abs/1710.07654\n\n        """"""\n        # setup\n        assert len(h.size()) == 2\n        hs = h.unsqueeze(0)\n        ilens = [h.size(0)]\n        maxlen = int(h.size(0) * maxlenratio)\n        minlen = int(h.size(0) * minlenratio)\n\n        # initialize hidden states of decoder\n        c_list = [self._zero_state(hs)]\n        z_list = [self._zero_state(hs)]\n        for _ in six.moves.range(1, len(self.lstm)):\n            c_list += [self._zero_state(hs)]\n            z_list += [self._zero_state(hs)]\n        prev_out = hs.new_zeros(1, self.odim)\n\n        # initialize attention\n        prev_att_w = None\n        self.att.reset()\n\n        # setup for attention constraint\n        if use_att_constraint:\n            last_attended_idx = 0\n        else:\n            last_attended_idx = None\n\n        # loop for an output sequence\n        idx = 0\n        outs, att_ws, probs = [], [], []\n        while True:\n            # updated index\n            idx += self.reduction_factor\n\n            # decoder calculation\n            if self.use_att_extra_inputs:\n                att_c, att_w = self.att(\n                    hs,\n                    ilens,\n                    z_list[0],\n                    prev_att_w,\n                    prev_out,\n                    last_attended_idx=last_attended_idx,\n                    backward_window=backward_window,\n                    forward_window=forward_window,\n                )\n            else:\n                att_c, att_w = self.att(\n                    hs,\n                    ilens,\n                    z_list[0],\n                    prev_att_w,\n                    last_attended_idx=last_attended_idx,\n                    backward_window=backward_window,\n                    forward_window=forward_window,\n                )\n\n            att_ws += [att_w]\n            prenet_out = self.prenet(prev_out) if self.prenet is not None else prev_out\n            xs = torch.cat([att_c, prenet_out], dim=1)\n            z_list[0], c_list[0] = self.lstm[0](xs, (z_list[0], c_list[0]))\n            for i in six.moves.range(1, len(self.lstm)):\n                z_list[i], c_list[i] = self.lstm[i](\n                    z_list[i - 1], (z_list[i], c_list[i])\n                )\n            zcs = (\n                torch.cat([z_list[-1], att_c], dim=1)\n                if self.use_concate\n                else z_list[-1]\n            )\n            outs += [self.feat_out(zcs).view(1, self.odim, -1)]  # [(1, odim, r), ...]\n            probs += [torch.sigmoid(self.prob_out(zcs))[0]]  # [(r), ...]\n            if self.output_activation_fn is not None:\n                prev_out = self.output_activation_fn(outs[-1][:, :, -1])  # (1, odim)\n            else:\n                prev_out = outs[-1][:, :, -1]  # (1, odim)\n            if self.cumulate_att_w and prev_att_w is not None:\n                prev_att_w = prev_att_w + att_w  # Note: error when use +=\n            else:\n                prev_att_w = att_w\n            if use_att_constraint:\n                last_attended_idx = int(att_w.argmax())\n\n            # check whether to finish generation\n            if int(sum(probs[-1] >= threshold)) > 0 or idx >= maxlen:\n                # check mininum length\n                if idx < minlen:\n                    continue\n                outs = torch.cat(outs, dim=2)  # (1, odim, L)\n                if self.postnet is not None:\n                    outs = outs + self.postnet(outs)  # (1, odim, L)\n                outs = outs.transpose(2, 1).squeeze(0)  # (L, odim)\n                probs = torch.cat(probs, dim=0)\n                att_ws = torch.cat(att_ws, dim=0)\n                break\n\n        if self.output_activation_fn is not None:\n            outs = self.output_activation_fn(outs)\n\n        return outs, probs, att_ws\n\n    def calculate_all_attentions(self, hs, hlens, ys):\n        """"""Calculate all of the attention weights.\n\n        Args:\n            hs (Tensor): Batch of the sequences of padded hidden states (B, Tmax, idim).\n            hlens (LongTensor): Batch of lengths of each input batch (B,).\n            ys (Tensor):\n                Batch of the sequences of padded target features (B, Lmax, odim).\n\n        Returns:\n            numpy.ndarray: Batch of attention weights (B, Lmax, Tmax).\n\n        Note:\n            This computation is performed in teacher-forcing manner.\n\n        """"""\n        # thin out frames (B, Lmax, odim) ->  (B, Lmax/r, odim)\n        if self.reduction_factor > 1:\n            ys = ys[:, self.reduction_factor - 1 :: self.reduction_factor]\n\n        # length list should be list of int\n        hlens = list(map(int, hlens))\n\n        # initialize hidden states of decoder\n        c_list = [self._zero_state(hs)]\n        z_list = [self._zero_state(hs)]\n        for _ in six.moves.range(1, len(self.lstm)):\n            c_list += [self._zero_state(hs)]\n            z_list += [self._zero_state(hs)]\n        prev_out = hs.new_zeros(hs.size(0), self.odim)\n\n        # initialize attention\n        prev_att_w = None\n        self.att.reset()\n\n        # loop for an output sequence\n        att_ws = []\n        for y in ys.transpose(0, 1):\n            if self.use_att_extra_inputs:\n                att_c, att_w = self.att(hs, hlens, z_list[0], prev_att_w, prev_out)\n            else:\n                att_c, att_w = self.att(hs, hlens, z_list[0], prev_att_w)\n            att_ws += [att_w]\n            prenet_out = self.prenet(prev_out) if self.prenet is not None else prev_out\n            xs = torch.cat([att_c, prenet_out], dim=1)\n            z_list[0], c_list[0] = self.lstm[0](xs, (z_list[0], c_list[0]))\n            for i in six.moves.range(1, len(self.lstm)):\n                z_list[i], c_list[i] = self.lstm[i](\n                    z_list[i - 1], (z_list[i], c_list[i])\n                )\n            prev_out = y  # teacher forcing\n            if self.cumulate_att_w and prev_att_w is not None:\n                prev_att_w = prev_att_w + att_w  # Note: error when use +=\n            else:\n                prev_att_w = att_w\n\n        att_ws = torch.stack(att_ws, dim=1)  # (B, Lmax, Tmax)\n\n        return att_ws\n'"
espnet/nets/pytorch_backend/tacotron2/encoder.py,17,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Nagoya University (Tomoki Hayashi)\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Tacotron2 encoder related modules.""""""\n\nimport six\n\nimport torch\n\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\n\ndef encoder_init(m):\n    """"""Initialize encoder parameters.""""""\n    if isinstance(m, torch.nn.Conv1d):\n        torch.nn.init.xavier_uniform_(m.weight, torch.nn.init.calculate_gain(""relu""))\n\n\nclass Encoder(torch.nn.Module):\n    """"""Encoder module of Spectrogram prediction network.\n\n    This is a module of encoder of Spectrogram prediction network in Tacotron2,\n    which described in `Natural TTS\n    Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`_.\n    This is the encoder which converts the\n    sequence of characters into the sequence of hidden states.\n\n    .. _`Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions`:\n       https://arxiv.org/abs/1712.05884\n\n    """"""\n\n    def __init__(\n        self,\n        idim,\n        embed_dim=512,\n        elayers=1,\n        eunits=512,\n        econv_layers=3,\n        econv_chans=512,\n        econv_filts=5,\n        use_batch_norm=True,\n        use_residual=False,\n        dropout_rate=0.5,\n        padding_idx=0,\n    ):\n        """"""Initialize Tacotron2 encoder module.\n\n        Args:\n            idim (int) Dimension of the inputs.\n            embed_dim (int, optional) Dimension of character embedding.\n            elayers (int, optional) The number of encoder blstm layers.\n            eunits (int, optional) The number of encoder blstm units.\n            econv_layers (int, optional) The number of encoder conv layers.\n            econv_filts (int, optional) The number of encoder conv filter size.\n            econv_chans (int, optional) The number of encoder conv filter channels.\n            use_batch_norm (bool, optional) Whether to use batch normalization.\n            use_residual (bool, optional) Whether to use residual connection.\n            dropout_rate (float, optional) Dropout rate.\n\n        """"""\n        super(Encoder, self).__init__()\n        # store the hyperparameters\n        self.idim = idim\n        self.use_residual = use_residual\n\n        # define network layer modules\n        self.embed = torch.nn.Embedding(idim, embed_dim, padding_idx=padding_idx)\n        if econv_layers > 0:\n            self.convs = torch.nn.ModuleList()\n            for layer in six.moves.range(econv_layers):\n                ichans = embed_dim if layer == 0 else econv_chans\n                if use_batch_norm:\n                    self.convs += [\n                        torch.nn.Sequential(\n                            torch.nn.Conv1d(\n                                ichans,\n                                econv_chans,\n                                econv_filts,\n                                stride=1,\n                                padding=(econv_filts - 1) // 2,\n                                bias=False,\n                            ),\n                            torch.nn.BatchNorm1d(econv_chans),\n                            torch.nn.ReLU(),\n                            torch.nn.Dropout(dropout_rate),\n                        )\n                    ]\n                else:\n                    self.convs += [\n                        torch.nn.Sequential(\n                            torch.nn.Conv1d(\n                                ichans,\n                                econv_chans,\n                                econv_filts,\n                                stride=1,\n                                padding=(econv_filts - 1) // 2,\n                                bias=False,\n                            ),\n                            torch.nn.ReLU(),\n                            torch.nn.Dropout(dropout_rate),\n                        )\n                    ]\n        else:\n            self.convs = None\n        if elayers > 0:\n            iunits = econv_chans if econv_layers != 0 else embed_dim\n            self.blstm = torch.nn.LSTM(\n                iunits, eunits // 2, elayers, batch_first=True, bidirectional=True\n            )\n        else:\n            self.blstm = None\n\n        # initialize\n        self.apply(encoder_init)\n\n    def forward(self, xs, ilens=None):\n        """"""Calculate forward propagation.\n\n        Args:\n            xs (Tensor): Batch of the padded sequence of character ids (B, Tmax).\n                Padded value should be 0.\n            ilens (LongTensor): Batch of lengths of each input batch (B,).\n\n        Returns:\n            Tensor: Batch of the sequences of encoder states(B, Tmax, eunits).\n            LongTensor: Batch of lengths of each sequence (B,)\n\n        """"""\n        xs = self.embed(xs).transpose(1, 2)\n        if self.convs is not None:\n            for i in six.moves.range(len(self.convs)):\n                if self.use_residual:\n                    xs += self.convs[i](xs)\n                else:\n                    xs = self.convs[i](xs)\n        if self.blstm is None:\n            return xs.transpose(1, 2)\n        xs = pack_padded_sequence(xs.transpose(1, 2), ilens, batch_first=True)\n        self.blstm.flatten_parameters()\n        xs, _ = self.blstm(xs)  # (B, Tmax, C)\n        xs, hlens = pad_packed_sequence(xs, batch_first=True)\n\n        return xs, hlens\n\n    def inference(self, x):\n        """"""Inference.\n\n        Args:\n            x (Tensor): The sequeunce of character ids (T,).\n\n        Returns:\n            Tensor: The sequences of encoder states(T, eunits).\n\n        """"""\n        assert len(x.size()) == 1\n        xs = x.unsqueeze(0)\n        ilens = [x.size(0)]\n\n        return self.forward(xs, ilens)[0][0]\n'"
espnet/nets/pytorch_backend/transducer/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/transducer/initializer.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n""""""Parameter initialization for transducer RNN/Transformer parts.""""""\n\nimport six\n\nfrom espnet.nets.pytorch_backend.initialization import lecun_normal_init_parameters\nfrom espnet.nets.pytorch_backend.initialization import set_forget_bias_to_one\n\nfrom espnet.nets.pytorch_backend.transformer.initializer import initialize\n\n\ndef initializer(model, args):\n    """"""Initialize transducer model.\n\n    Args:\n        model (torch.nn.Module): transducer instance\n        args (Namespace): argument Namespace containing options\n\n    """"""\n    if args.dtype != ""transformer"":\n        if args.etype == ""transformer"":\n            initialize(model.encoder, args.transformer_init)\n            lecun_normal_init_parameters(model.dec)\n        else:\n            lecun_normal_init_parameters(model)\n\n        model.dec.embed.weight.data.normal_(0, 1)\n\n        for i in six.moves.range(len(model.dec.decoder)):\n            set_forget_bias_to_one(model.dec.decoder[i].bias_ih)\n    else:\n        if args.etype == ""transformer"":\n            initialize(model, args.transformer_init)\n        else:\n            lecun_normal_init_parameters(model.encoder)\n            initialize(model.decoder, args.transformer_init)\n'"
espnet/nets/pytorch_backend/transducer/loss.py,5,"b'#!/usr/bin/env python3\n\n""""""Transducer loss module.""""""\n\nfrom torch import nn\n\nfrom warprnnt_pytorch import RNNTLoss\n\n\nclass TransLoss(nn.Module):\n    """"""Transducer loss.\n\n    Args:\n        trans_type (str): type of transducer implementation to calculate loss.\n        blank_id (int): blank symbol id\n\n    """"""\n\n    def __init__(self, trans_type, blank_id):\n        """"""Construct an TransLoss object.""""""\n        super(TransLoss, self).__init__()\n\n        if trans_type == ""warp-transducer"":\n            self.trans_loss = RNNTLoss(blank=blank_id)\n        else:\n            raise NotImplementedError\n\n        self.blank_id = blank_id\n\n    def forward(self, pred_pad, target, pred_len, target_len):\n        """"""Compute path-aware regularization transducer loss.\n\n        Args:\n            pred_pad (torch.Tensor): Batch of predicted sequences\n                (batch, maxlen_in, maxlen_out+1, odim)\n            target (torch.Tensor): Batch of target sequences (batch, maxlen_out)\n            pred_len (torch.Tensor): batch of lengths of predicted sequences (batch)\n            target_len (torch.tensor): batch of lengths of target sequences (batch)\n\n        Returns:\n            loss (torch.Tensor): transducer loss\n\n        """"""\n        loss = self.trans_loss(pred_pad, target, pred_len, target_len)\n\n        return loss\n'"
espnet/nets/pytorch_backend/transducer/rnn_decoders.py,72,"b'""""""Transducer and transducer with attention implementation for training and decoding.""""""\n\nimport six\n\nimport torch\nimport torch.nn.functional as F\n\nfrom espnet.nets.pytorch_backend.rnn.attentions import att_to_numpy\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\n\n\nclass DecoderRNNT(torch.nn.Module):\n    """"""RNN-T Decoder module.\n\n    Args:\n        eprojs (int): # encoder projection units\n        odim (int): dimension of outputs\n        dtype (str): gru or lstm\n        dlayers (int): # prediction layers\n        dunits (int): # prediction units\n        blank (int): blank symbol id\n        embed_dim (init): dimension of embeddings\n        joint_dim (int): dimension of joint space\n        dropout (float): dropout rate\n        dropout_embed (float): embedding dropout rate\n\n    """"""\n\n    def __init__(\n        self,\n        eprojs,\n        odim,\n        dtype,\n        dlayers,\n        dunits,\n        blank,\n        embed_dim,\n        joint_dim,\n        dropout=0.0,\n        dropout_embed=0.0,\n    ):\n        """"""Transducer initializer.""""""\n        super(DecoderRNNT, self).__init__()\n\n        self.embed = torch.nn.Embedding(odim, embed_dim, padding_idx=blank)\n        self.dropout_embed = torch.nn.Dropout(p=dropout_embed)\n\n        if dtype == ""lstm"":\n            dec_net = torch.nn.LSTMCell\n        else:\n            dec_net = torch.nn.GRUCell\n\n        self.decoder = torch.nn.ModuleList([dec_net(embed_dim, dunits)])\n        self.dropout_dec = torch.nn.ModuleList([torch.nn.Dropout(p=dropout)])\n\n        for _ in six.moves.range(1, dlayers):\n            self.decoder += [dec_net(dunits, dunits)]\n            self.dropout_dec += [torch.nn.Dropout(p=dropout)]\n\n        self.lin_enc = torch.nn.Linear(eprojs, joint_dim)\n        self.lin_dec = torch.nn.Linear(dunits, joint_dim, bias=False)\n        self.lin_out = torch.nn.Linear(joint_dim, odim)\n\n        self.dlayers = dlayers\n        self.dunits = dunits\n        self.dtype = dtype\n        self.embed_dim = embed_dim\n        self.joint_dim = joint_dim\n        self.odim = odim\n\n        self.ignore_id = -1\n        self.blank = blank\n\n    def zero_state(self, ey):\n        """"""Initialize decoder states.\n\n        Args:\n            ey (torch.Tensor): batch of input features (B, Emb_dim)\n\n        Returns:\n            (list): list of L zero-init hidden and cell state (B, Hdec)\n\n        """"""\n        z_list = [ey.new_zeros(ey.size(0), self.dunits)]\n        c_list = [ey.new_zeros(ey.size(0), self.dunits)]\n\n        for _ in six.moves.range(1, self.dlayers):\n            z_list.append(ey.new_zeros(ey.size(0), self.dunits))\n            c_list.append(ey.new_zeros(ey.size(0), self.dunits))\n\n        return (z_list, c_list)\n\n    def rnn_forward(self, ey, dstate):\n        """"""RNN forward.\n\n        Args:\n            ey (torch.Tensor): batch of input features (B, Emb_dim)\n            dstate (list): list of L input hidden and cell state (B, Hdec)\n\n        Returns:\n            output (torch.Tensor): batch of output features (B, Hdec)\n            dstate (list): list of L output hidden and cell state (B, Hdec)\n\n        """"""\n        if dstate is None:\n            z_prev, c_prev = self.zero_state(ey)\n        else:\n            z_prev, c_prev = dstate\n\n        z_list, c_list = self.zero_state(ey)\n\n        if self.dtype == ""lstm"":\n            z_list[0], c_list[0] = self.decoder[0](ey, (z_prev[0], c_prev[0]))\n\n            for i in six.moves.range(1, self.dlayers):\n                z_list[i], c_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), (z_prev[i], c_prev[i])\n                )\n        else:\n            z_list[0] = self.decoder[0](ey, z_prev[0])\n\n            for i in six.moves.range(1, self.dlayers):\n                z_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), z_prev[i]\n                )\n        y = self.dropout_dec[-1](z_list[-1])\n\n        return y, (z_list, c_list)\n\n    def joint(self, h_enc, h_dec):\n        """"""Joint computation of z.\n\n        Args:\n            h_enc (torch.Tensor): batch of expanded hidden state (B, T, 1, Henc)\n            h_dec (torch.Tensor): batch of expanded hidden state (B, 1, U, Hdec)\n\n        Returns:\n            z (torch.Tensor): output (B, T, U, odim)\n\n        """"""\n        z = torch.tanh(self.lin_enc(h_enc) + self.lin_dec(h_dec))\n        z = self.lin_out(z)\n\n        return z\n\n    def forward(self, hs_pad, ys_in_pad, hlens=None):\n        """"""Forward function for transducer.\n\n        Args:\n            hs_pad (torch.Tensor):\n                batch of padded hidden state sequences (B, Tmax, D)\n            ys_in_pad (torch.Tensor):\n                batch of padded character id sequence tensor (B, Lmax+1)\n\n        Returns:\n            z (torch.Tensor): output (B, T, U, odim)\n\n        """"""\n        olength = ys_in_pad.size(1)\n\n        z_list, c_list = self.zero_state(hs_pad)\n        eys = self.dropout_embed(self.embed(ys_in_pad))\n\n        z_all = []\n        for i in six.moves.range(olength):\n            y, (z_list, c_list) = self.rnn_forward(eys[:, i, :], (z_list, c_list))\n            z_all.append(y)\n        h_dec = torch.stack(z_all, dim=1)\n\n        h_enc = hs_pad.unsqueeze(2)\n        h_dec = h_dec.unsqueeze(1)\n\n        z = self.joint(h_enc, h_dec)\n\n        return z\n\n    def recognize(self, h, recog_args):\n        """"""Greedy search implementation.\n\n        Args:\n            h (torch.Tensor): encoder hidden state sequences (Tmax, Henc)\n            recog_args (Namespace): argument Namespace containing options\n\n        Returns:\n            hyp (list of dicts): 1-best decoding results\n\n        """"""\n        z_list, c_list = self.zero_state(h.unsqueeze(0))\n        ey = to_device(self, torch.zeros((1, self.embed_dim)))\n\n        hyp = {""score"": 0.0, ""yseq"": [self.blank]}\n\n        y, (z_list, c_list) = self.rnn_forward(ey, (z_list, c_list))\n\n        for hi in h:\n            ytu = F.log_softmax(self.joint(hi, y[0]), dim=0)\n            logp, pred = torch.max(ytu, dim=0)\n\n            if pred != self.blank:\n                hyp[""yseq""].append(int(pred))\n                hyp[""score""] += float(logp)\n\n                eys = to_device(\n                    self, torch.full((1, 1), hyp[""yseq""][-1], dtype=torch.long)\n                )\n                ey = self.dropout_embed(self.embed(eys))\n\n                y, (z_list, c_list) = self.rnn_forward(ey[0], (z_list, c_list))\n\n        return [hyp]\n\n    def recognize_beam(self, h, recog_args, rnnlm=None):\n        """"""Beam search implementation.\n\n        Args:\n            h (torch.Tensor): encoder hidden state sequences (Tmax, Henc)\n            recog_args (Namespace): argument Namespace containing options\n            rnnlm (torch.nn.Module): language module\n\n        Returns:\n            nbest_hyps (list of dicts): n-best decoding results\n\n        """"""\n        beam = recog_args.beam_size\n        k_range = min(beam, self.odim)\n        nbest = recog_args.nbest\n        normscore = recog_args.score_norm_transducer\n\n        z_list, c_list = self.zero_state(h.unsqueeze(0))\n        eys = to_device(self, torch.zeros((1, self.embed_dim)))\n\n        _, (z_list, c_list) = self.rnn_forward(eys, None)\n\n        if rnnlm:\n            kept_hyps = [\n                {\n                    ""score"": 0.0,\n                    ""yseq"": [self.blank],\n                    ""z_prev"": z_list,\n                    ""c_prev"": c_list,\n                    ""lm_state"": None,\n                }\n            ]\n        else:\n            kept_hyps = [\n                {""score"": 0.0, ""yseq"": [self.blank], ""z_prev"": z_list, ""c_prev"": c_list}\n            ]\n\n        for i, hi in enumerate(h):\n            hyps = kept_hyps\n            kept_hyps = []\n\n            while True:\n                new_hyp = max(hyps, key=lambda x: x[""score""])\n                hyps.remove(new_hyp)\n\n                vy = to_device(\n                    self, torch.full((1, 1), new_hyp[""yseq""][-1], dtype=torch.long)\n                )\n                ey = self.dropout_embed(self.embed(vy))\n\n                y, (z_list, c_list) = self.rnn_forward(\n                    ey[0], (new_hyp[""z_prev""], new_hyp[""c_prev""])\n                )\n\n                ytu = F.log_softmax(self.joint(hi, y[0]), dim=0)\n\n                if rnnlm:\n                    rnnlm_state, rnnlm_scores = rnnlm.predict(\n                        new_hyp[""lm_state""], vy[0]\n                    )\n\n                for k in six.moves.range(self.odim):\n                    beam_hyp = {\n                        ""score"": new_hyp[""score""] + float(ytu[k]),\n                        ""yseq"": new_hyp[""yseq""][:],\n                        ""z_prev"": new_hyp[""z_prev""],\n                        ""c_prev"": new_hyp[""c_prev""],\n                    }\n                    if rnnlm:\n                        beam_hyp[""lm_state""] = new_hyp[""lm_state""]\n\n                    if k == self.blank:\n                        kept_hyps.append(beam_hyp)\n                    else:\n                        beam_hyp[""z_prev""] = z_list[:]\n                        beam_hyp[""c_prev""] = c_list[:]\n                        beam_hyp[""yseq""].append(int(k))\n\n                        if rnnlm:\n                            beam_hyp[""lm_state""] = rnnlm_state\n                            beam_hyp[""score""] += (\n                                recog_args.lm_weight * rnnlm_scores[0][k]\n                            )\n\n                        hyps.append(beam_hyp)\n\n                if len(kept_hyps) >= k_range:\n                    break\n\n        if normscore:\n            nbest_hyps = sorted(\n                kept_hyps, key=lambda x: x[""score""] / len(x[""yseq""]), reverse=True\n            )[:nbest]\n        else:\n            nbest_hyps = sorted(kept_hyps, key=lambda x: x[""score""], reverse=True)[\n                :nbest\n            ]\n\n        return nbest_hyps\n\n\nclass DecoderRNNTAtt(torch.nn.Module):\n    """"""RNNT-Att Decoder module.\n\n    Args:\n        eprojs (int): # encoder projection units\n        odim (int): dimension of outputs\n        dtype (str): gru or lstm\n        dlayers (int): # decoder layers\n        dunits (int): # decoder units\n        blank (int): blank symbol id\n        att (torch.nn.Module): attention module\n        embed_dim (int): dimension of embeddings\n        joint_dim (int): dimension of joint space\n        dropout (float): dropout rate\n        dropout_embed (float): embedding dropout rate\n\n    """"""\n\n    def __init__(\n        self,\n        eprojs,\n        odim,\n        dtype,\n        dlayers,\n        dunits,\n        blank,\n        att,\n        embed_dim,\n        joint_dim,\n        dropout=0.0,\n        dropout_embed=0.0,\n    ):\n        """"""Transducer with attention initializer.""""""\n        super(DecoderRNNTAtt, self).__init__()\n\n        self.embed = torch.nn.Embedding(odim, embed_dim, padding_idx=blank)\n        self.dropout_emb = torch.nn.Dropout(p=dropout_embed)\n\n        if dtype == ""lstm"":\n            dec_net = torch.nn.LSTMCell\n        else:\n            dec_net = torch.nn.GRUCell\n\n        self.decoder = torch.nn.ModuleList([dec_net((embed_dim + eprojs), dunits)])\n        self.dropout_dec = torch.nn.ModuleList([torch.nn.Dropout(p=dropout)])\n\n        for _ in six.moves.range(1, dlayers):\n            self.decoder += [dec_net(dunits, dunits)]\n            self.dropout_dec += [torch.nn.Dropout(p=dropout)]\n\n        self.lin_enc = torch.nn.Linear(eprojs, joint_dim)\n        self.lin_dec = torch.nn.Linear(dunits, joint_dim, bias=False)\n        self.lin_out = torch.nn.Linear(joint_dim, odim)\n\n        self.att = att\n\n        self.dtype = dtype\n        self.dlayers = dlayers\n        self.dunits = dunits\n        self.embed_dim = embed_dim\n        self.joint_dim = joint_dim\n        self.odim = odim\n\n        self.ignore_id = -1\n        self.blank = blank\n\n    def zero_state(self, ey):\n        """"""Initialize decoder states.\n\n        Args:\n            ey (torch.Tensor): batch of input features (B, (Emb_dim + Eprojs))\n\n        Return:\n            z_list : list of L zero-init hidden state (B, Hdec)\n            c_list : list of L zero-init cell state (B, Hdec)\n\n        """"""\n        z_list = [ey.new_zeros(ey.size(0), self.dunits)]\n        c_list = [ey.new_zeros(ey.size(0), self.dunits)]\n\n        for _ in six.moves.range(1, self.dlayers):\n            z_list.append(ey.new_zeros(ey.size(0), self.dunits))\n            c_list.append(ey.new_zeros(ey.size(0), self.dunits))\n\n        return z_list, c_list\n\n    def rnn_forward(self, ey, dstate):\n        """"""RNN forward.\n\n        Args:\n            ey (torch.Tensor): batch of input features (B, (Emb_dim + Eprojs))\n            dstate (list): list of L input hidden and cell state (B, Hdec)\n        Returns:\n            y (torch.Tensor): decoder output for one step (B, Hdec)\n            (list): list of L output hidden and cell state (B, Hdec)\n\n        """"""\n        if dstate is None:\n            z_prev, c_prev = self.zero_state(ey)\n        else:\n            z_prev, c_prev = dstate\n\n        z_list, c_list = self.zero_state(ey)\n\n        if self.dtype == ""lstm"":\n            z_list[0], c_list[0] = self.decoder[0](ey, (z_prev[0], c_prev[0]))\n\n            for i in six.moves.range(1, self.dlayers):\n                z_list[i], c_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), (z_prev[i], c_prev[i])\n                )\n        else:\n            z_list[0] = self.decoder[0](ey, z_prev[0])\n\n            for i in six.moves.range(1, self.dlayers):\n                z_list[i] = self.decoder[i](\n                    self.dropout_dec[i - 1](z_list[i - 1]), z_prev[i]\n                )\n        y = self.dropout_dec[-1](z_list[-1])\n\n        return y, (z_list, c_list)\n\n    def joint(self, h_enc, h_dec):\n        """"""Joint computation of z.\n\n        Args:\n            h_enc (torch.Tensor): batch of expanded hidden state (B, T, 1, Henc)\n            h_dec (torch.Tensor): batch of expanded hidden state (B, 1, U, Hdec)\n\n        Returns:\n            z (torch.Tensor): output (B, T, U, odim)\n\n        """"""\n        z = torch.tanh(self.lin_enc(h_enc) + self.lin_dec(h_dec))\n        z = self.lin_out(z)\n\n        return z\n\n    def forward(self, hs_pad, ys_in_pad, hlens=None):\n        """"""Forward function for transducer with attention.\n\n        Args:\n            hs_pad (torch.Tensor): batch of padded hidden state sequences (B, Tmax, D)\n            ys_in_pad (torch.Tensor):\n                batch of padded character id sequence tensor (B, Lmax+1)\n\n        Returns:\n            z (torch.Tensor): output (B, T, U, odim)\n\n        """"""\n        olength = ys_in_pad.size(1)\n\n        hlens = list(map(int, hlens))\n\n        att_w = None\n        self.att[0].reset()\n\n        z_list, c_list = self.zero_state(hs_pad)\n        eys = self.dropout_emb(self.embed(ys_in_pad))\n\n        z_all = []\n        for i in six.moves.range(olength):\n            att_c, att_w = self.att[0](\n                hs_pad, hlens, self.dropout_dec[0](z_list[0]), att_w\n            )\n\n            ey = torch.cat((eys[:, i, :], att_c), dim=1)\n            y, (z_list, c_list) = self.rnn_forward(ey, (z_list, c_list))\n            z_all.append(y)\n\n        h_dec = torch.stack(z_all, dim=1)\n\n        h_enc = hs_pad.unsqueeze(2)\n        h_dec = h_dec.unsqueeze(1)\n\n        z = self.joint(h_enc, h_dec)\n\n        return z\n\n    def recognize(self, h, recog_args):\n        """"""Greedy search implementation.\n\n        Args:\n            h (torch.Tensor): encoder hidden state sequences (Tmax, Henc)\n            recog_args (Namespace): argument Namespace containing options\n\n        Returns:\n            hyp (list of dicts): 1-best decoding results\n\n        """"""\n        self.att[0].reset()\n\n        z_list, c_list = self.zero_state(h.unsqueeze(0))\n        eys = torch.zeros((1, self.embed_dim))\n\n        att_c, att_w = self.att[0](\n            h.unsqueeze(0), [h.size(0)], self.dropout_dec[0](z_list[0]), None\n        )\n\n        ey = torch.cat((eys, att_c), dim=1)\n\n        hyp = {""score"": 0.0, ""yseq"": [self.blank]}\n\n        y, (z_list, c_list) = self.rnn_forward(ey, (z_list, c_list))\n\n        for hi in h:\n            ytu = F.log_softmax(self.joint(hi, y[0]), dim=0)\n            logp, pred = torch.max(ytu, dim=0)\n\n            if pred != self.blank:\n                hyp[""yseq""].append(int(pred))\n                hyp[""score""] += float(logp)\n\n                eys = torch.full((1, 1), hyp[""yseq""][-1], dtype=torch.long)\n                ey = self.dropout_emb(self.embed(eys))\n                att_c, att_w = self.att[0](\n                    h.unsqueeze(0), [h.size(0)], self.dropout_dec[0](z_list[0]), att_w\n                )\n                ey = torch.cat((ey[0], att_c), dim=1)\n\n                y, (z_list, c_list) = self.rnn_forward(ey, (z_list, c_list))\n\n        return [hyp]\n\n    def recognize_beam(self, h, recog_args, rnnlm=None):\n        """"""Beam search recognition.\n\n        Args:\n            h (torch.Tensor): encoder hidden state sequences (Tmax, Henc)\n            recog_args (Namespace): argument Namespace containing options\n            rnnlm (torch.nn.Module): language module\n\n        Results:\n            nbest_hyps (list of dicts): n-best decoding results\n\n        """"""\n        beam = recog_args.beam_size\n        k_range = min(beam, self.odim)\n        nbest = recog_args.nbest\n        normscore = recog_args.score_norm_transducer\n\n        self.att[0].reset()\n\n        z_list, c_list = self.zero_state(h.unsqueeze(0))\n        eys = torch.zeros((1, self.embed_dim))\n\n        att_c, att_w = self.att[0](\n            h.unsqueeze(0), [h.size(0)], self.dropout_dec[0](z_list[0]), None\n        )\n\n        ey = torch.cat((eys, att_c), dim=1)\n        _, (z_list, c_list) = self.rnn_forward(ey, None)\n\n        if rnnlm:\n            kept_hyps = [\n                {\n                    ""score"": 0.0,\n                    ""yseq"": [self.blank],\n                    ""z_prev"": z_list,\n                    ""c_prev"": c_list,\n                    ""a_prev"": None,\n                    ""lm_state"": None,\n                }\n            ]\n        else:\n            kept_hyps = [\n                {\n                    ""score"": 0.0,\n                    ""yseq"": [self.blank],\n                    ""z_prev"": z_list,\n                    ""c_prev"": c_list,\n                    ""a_prev"": None,\n                }\n            ]\n\n        for i, hi in enumerate(h):\n            hyps = kept_hyps\n            kept_hyps = []\n\n            while True:\n                new_hyp = max(hyps, key=lambda x: x[""score""])\n                hyps.remove(new_hyp)\n\n                vy = to_device(\n                    self, torch.full((1, 1), new_hyp[""yseq""][-1], dtype=torch.long)\n                )\n                ey = self.dropout_emb(self.embed(vy))\n\n                att_c, att_w = self.att[0](\n                    h.unsqueeze(0),\n                    [h.size(0)],\n                    self.dropout_dec[0](new_hyp[""z_prev""][0]),\n                    new_hyp[""a_prev""],\n                )\n\n                ey = torch.cat((ey[0], att_c), dim=1)\n                y, (z_list, c_list) = self.rnn_forward(\n                    ey, (new_hyp[""z_prev""], new_hyp[""c_prev""])\n                )\n                ytu = F.log_softmax(self.joint(hi, y[0]), dim=0)\n\n                if rnnlm:\n                    rnnlm_state, rnnlm_scores = rnnlm.predict(\n                        new_hyp[""lm_state""], vy[0]\n                    )\n\n                for k in six.moves.range(self.odim):\n                    beam_hyp = {\n                        ""score"": new_hyp[""score""] + float(ytu[k]),\n                        ""yseq"": new_hyp[""yseq""][:],\n                        ""z_prev"": new_hyp[""z_prev""],\n                        ""c_prev"": new_hyp[""c_prev""],\n                        ""a_prev"": new_hyp[""a_prev""],\n                    }\n                    if rnnlm:\n                        beam_hyp[""lm_state""] = new_hyp[""lm_state""]\n\n                    if k == self.blank:\n                        kept_hyps.append(beam_hyp)\n                    else:\n                        beam_hyp[""z_prev""] = z_list[:]\n                        beam_hyp[""c_prev""] = c_list[:]\n                        beam_hyp[""a_prev""] = att_w[:]\n                        beam_hyp[""yseq""].append(int(k))\n\n                        if rnnlm:\n                            beam_hyp[""lm_state""] = rnnlm_state\n                            beam_hyp[""score""] += (\n                                recog_args.lm_weight * rnnlm_scores[0][k]\n                            )\n\n                        hyps.append(beam_hyp)\n\n                if len(kept_hyps) >= k_range:\n                    break\n\n        if normscore:\n            nbest_hyps = sorted(\n                kept_hyps, key=lambda x: x[""score""] / len(x[""yseq""]), reverse=True\n            )[:nbest]\n        else:\n            nbest_hyps = sorted(kept_hyps, key=lambda x: x[""score""], reverse=True)[\n                :nbest\n            ]\n\n        return nbest_hyps\n\n    def calculate_all_attentions(self, hs_pad, hlens, ys_pad):\n        """"""Calculate all of attentions.\n\n        Args:\n            hs_pad (torch.Tensor): batch of padded hidden state sequences (B, Tmax, D)\n            hlens (torch.Tensor): batch of lengths of hidden state sequences (B)\n            ys_pad (torch.Tensor):\n                batch of padded character id sequence tensor (B, Lmax)\n\n        Returns:\n            att_ws (ndarray): attention weights with the following shape,\n                1) multi-head case => attention weights (B, H, Lmax, Tmax),\n                2) other case => attention weights (B, Lmax, Tmax).\n\n        """"""\n        ys = [y[y != self.ignore_id] for y in ys_pad]\n\n        hlens = list(map(int, hlens))\n\n        blank = ys[0].new([self.blank])\n\n        ys_in = [torch.cat([blank, y], dim=0) for y in ys]\n        ys_in_pad = pad_list(ys_in, self.blank)\n\n        olength = ys_in_pad.size(1)\n\n        att_w = None\n        att_ws = []\n        self.att[0].reset()\n\n        eys = self.dropout_emb(self.embed(ys_in_pad))\n        z_list, c_list = self.zero_state(eys)\n\n        for i in six.moves.range(olength):\n            att_c, att_w = self.att[0](\n                hs_pad, hlens, self.dropout_dec[0](z_list[0]), att_w\n            )\n            ey = torch.cat((eys[:, i, :], att_c), dim=1)\n            _, (z_list, c_list) = self.rnn_forward(ey, (z_list, c_list))\n\n            att_ws.append(att_w)\n\n        att_ws = att_to_numpy(att_ws, self.att[0])\n\n        return att_ws\n\n\ndef decoder_for(args, odim, att=None, blank=0):\n    """"""Transducer mode selector.""""""\n    if args.rnnt_mode == ""rnnt"":\n        return DecoderRNNT(\n            args.eprojs,\n            odim,\n            args.dtype,\n            args.dlayers,\n            args.dunits,\n            blank,\n            args.dec_embed_dim,\n            args.joint_dim,\n            args.dropout_rate_decoder,\n            args.dropout_rate_embed_decoder,\n        )\n    elif args.rnnt_mode == ""rnnt-att"":\n        return DecoderRNNTAtt(\n            args.eprojs,\n            odim,\n            args.dtype,\n            args.dlayers,\n            args.dunits,\n            blank,\n            att,\n            args.dec_embed_dim,\n            args.joint_dim,\n            args.dropout_rate_decoder,\n            args.dropout_rate_embed_decoder,\n        )\n'"
espnet/nets/pytorch_backend/transducer/transformer_decoder.py,40,"b'""""""Decoder definition for transformer-transducer models.""""""\n\nimport six\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import to_device\n\nfrom espnet.nets.pytorch_backend.transducer.transformer_decoder_layer import (\n    DecoderLayer,  # noqa: H301\n)\n\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.repeat import repeat\n\n\nclass Decoder(torch.nn.Module):\n    """"""Decoder module for transformer-transducer models.\n\n    Args:\n        odim (int): dimension of outputs\n        jdim (int): dimension of joint-space\n        attention_dim (int): dimension of attention\n        attention_heads (int): number of heads in multi-head attention\n        linear_units (int): number of units in position-wise feed forward\n        num_blocks (int): number of decoder blocks\n        dropout_rate (float): dropout rate for decoder\n        positional_dropout_rate (float): dropout rate for positional encoding\n        attention_dropout_rate (float): dropout rate for attention\n        input_layer (str or torch.nn.Module): input layer type\n        padding_idx (int): padding value for embedding\n        pos_enc_class (class): PositionalEncoding or ScaledPositionalEncoding\n        blank (int): blank symbol ID\n\n    """"""\n\n    def __init__(\n        self,\n        odim,\n        jdim,\n        attention_dim=512,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.0,\n        attention_dropout_rate=0.0,\n        input_layer=""embed"",\n        pos_enc_class=PositionalEncoding,\n        blank=0,\n    ):\n        """"""Construct a Decoder object for transformer-transducer models.""""""\n        torch.nn.Module.__init__(self)\n\n        if input_layer == ""embed"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(odim, attention_dim),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer == ""linear"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(odim, attention_dim),\n                torch.nn.LayerNorm(attention_dim),\n                torch.nn.Dropout(dropout_rate),\n                torch.nn.ReLU(),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif isinstance(input_layer, torch.nn.Module):\n            self.embed = torch.nn.Sequential(\n                input_layer, pos_enc_class(attention_dim, positional_dropout_rate)\n            )\n        else:\n            raise NotImplementedError(""only `embed` or torch.nn.Module is supported."")\n\n        self.decoders = repeat(\n            num_blocks,\n            lambda: DecoderLayer(\n                attention_dim,\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, attention_dropout_rate\n                ),\n                PositionwiseFeedForward(attention_dim, linear_units, dropout_rate),\n                dropout_rate,\n            ),\n        )\n\n        self.after_norm = LayerNorm(attention_dim)\n\n        self.lin_enc = torch.nn.Linear(attention_dim, jdim)\n        self.lin_dec = torch.nn.Linear(attention_dim, jdim, bias=False)\n        self.lin_out = torch.nn.Linear(jdim, odim)\n\n        self.attention_dim = attention_dim\n        self.odim = odim\n\n        self.blank = blank\n\n    def forward(self, tgt, tgt_mask, memory):\n        """"""Forward transformer-transducer decoder.\n\n        Args:\n            tgt (torch.Tensor): input token ids, int64 (batch, maxlen_out)\n                                if input_layer == ""embed""\n                                input tensor\n                                (batch, maxlen_out, #mels) in the other cases\n            tgt_mask (torch.Tensor): input token mask,  (batch, maxlen_out)\n                                     dtype=torch.uint8 in PyTorch 1.2-\n                                     dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n            memory (torch.Tensor): encoded memory, float32  (batch, maxlen_in, feat)\n\n        Return:\n            z (torch.Tensor): joint output (batch, maxlen_in, maxlen_out, odim)\n            tgt_mask (torch.Tensor): score mask before softmax (batch, maxlen_out)\n\n        """"""\n        tgt = self.embed(tgt)\n\n        tgt, tgt_mask = self.decoders(tgt, tgt_mask)\n        tgt = self.after_norm(tgt)\n\n        h_enc = memory.unsqueeze(2)\n        h_dec = tgt.unsqueeze(1)\n\n        z = self.joint(h_enc, h_dec)\n\n        return z, tgt_mask\n\n    def joint(self, h_enc, h_dec):\n        """"""Joint computation of z.\n\n        Args:\n            h_enc (torch.Tensor):\n                batch of expanded hidden state (batch, maxlen_in, 1, Henc)\n            h_dec (torch.Tensor):\n                batch of expanded hidden state (batch, 1, maxlen_out, Hdec)\n\n        Returns:\n            z (torch.Tensor): output (batch, maxlen_in, maxlen_out, odim)\n\n        """"""\n        z = torch.tanh(self.lin_enc(h_enc) + self.lin_dec(h_dec))\n        z = self.lin_out(z)\n\n        return z\n\n    def forward_one_step(self, tgt, tgt_mask, cache=None):\n        """"""Forward one step.\n\n        Args:\n            tgt (torch.Tensor): input token ids, int64 (batch, maxlen_out)\n                                if input_layer == ""embed""\n                                input tensor (batch, maxlen_out, #mels)\n                                in the other cases\n            tgt_mask (torch.Tensor): input token mask,  (batch, Tmax)\n                                     dtype=torch.uint8 in PyTorch 1.2-\n                                     dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n\n        """"""\n        tgt = self.embed(tgt)\n\n        if cache is None:\n            cache = self.init_state()\n        new_cache = []\n\n        for c, decoder in zip(cache, self.decoders):\n            tgt, tgt_mask = decoder(tgt, tgt_mask, c)\n            new_cache.append(tgt)\n\n        tgt = self.after_norm(tgt[:, -1])\n\n        return tgt, new_cache\n\n    def init_state(self, x=None):\n        """"""Get an initial state for decoding.""""""\n        return [None for i in range(len(self.decoders))]\n\n    def recognize(self, h, recog_args):\n        """"""Greedy search implementation for transformer-transducer.\n\n        Args:\n            h (torch.Tensor): encoder hidden state sequences (maxlen_in, Henc)\n            recog_args (Namespace): argument Namespace containing options\n\n        Returns:\n            hyp (list of dicts): 1-best decoding results\n\n        """"""\n        hyp = {""score"": 0.0, ""yseq"": [self.blank]}\n\n        ys = to_device(self, torch.tensor(hyp[""yseq""], dtype=torch.long)).unsqueeze(0)\n        ys_mask = to_device(self, subsequent_mask(1).unsqueeze(0))\n        y, c = self.forward_one_step(ys, ys_mask, None)\n\n        for i, hi in enumerate(h):\n            ytu = torch.log_softmax(self.joint(hi, y[0]), dim=0)\n            logp, pred = torch.max(ytu, dim=0)\n\n            if pred != self.blank:\n                hyp[""yseq""].append(int(pred))\n                hyp[""score""] += float(logp)\n\n                ys = to_device(self, torch.tensor(hyp[""yseq""]).unsqueeze(0))\n                ys_mask = to_device(\n                    self, subsequent_mask(len(hyp[""yseq""])).unsqueeze(0)\n                )\n\n                y, c = self.forward_one_step(ys, ys_mask, c)\n\n        return [hyp]\n\n    def recognize_beam(self, h, recog_args, rnnlm=None):\n        """"""Beam search implementation for transformer-transducer.\n\n        Args:\n            h (torch.Tensor): encoder hidden state sequences (maxlen_in, Henc)\n            recog_args (Namespace): argument Namespace containing options\n            rnnlm (torch.nn.Module): language model module\n\n        Returns:\n            nbest_hyps (list of dicts): n-best decoding results\n\n        """"""\n        beam = recog_args.beam_size\n        k_range = min(beam, self.odim)\n        nbest = recog_args.nbest\n        normscore = recog_args.score_norm_transducer\n\n        if rnnlm:\n            kept_hyps = [\n                {""score"": 0.0, ""yseq"": [self.blank], ""cache"": None, ""lm_state"": None}\n            ]\n        else:\n            kept_hyps = [{""score"": 0.0, ""yseq"": [self.blank], ""cache"": None}]\n\n        for i, hi in enumerate(h):\n            hyps = kept_hyps\n            kept_hyps = []\n\n            while True:\n                new_hyp = max(hyps, key=lambda x: x[""score""])\n                hyps.remove(new_hyp)\n\n                ys = to_device(self, torch.tensor(new_hyp[""yseq""]).unsqueeze(0))\n                ys_mask = to_device(\n                    self, subsequent_mask(len(new_hyp[""yseq""])).unsqueeze(0)\n                )\n                y, c = self.forward_one_step(ys, ys_mask, new_hyp[""cache""])\n\n                ytu = torch.log_softmax(self.joint(hi, y[0]), dim=0)\n\n                if rnnlm:\n                    rnnlm_state, rnnlm_scores = rnnlm.predict(\n                        new_hyp[""lm_state""], ys[:, -1]\n                    )\n\n                for k in six.moves.range(self.odim):\n                    beam_hyp = {\n                        ""score"": new_hyp[""score""] + float(ytu[k]),\n                        ""yseq"": new_hyp[""yseq""][:],\n                        ""cache"": new_hyp[""cache""],\n                    }\n\n                    if rnnlm:\n                        beam_hyp[""lm_state""] = new_hyp[""lm_state""]\n\n                    if k == self.blank:\n                        kept_hyps.append(beam_hyp)\n                    else:\n                        beam_hyp[""yseq""].append(int(k))\n                        beam_hyp[""cache""] = c\n\n                        if rnnlm:\n                            beam_hyp[""lm_state""] = rnnlm_state\n                            beam_hyp[""score""] += (\n                                recog_args.lm_weight * rnnlm_scores[0][k]\n                            )\n\n                        hyps.append(beam_hyp)\n\n                if len(kept_hyps) >= k_range:\n                    break\n\n        if normscore:\n            nbest_hyps = sorted(\n                kept_hyps, key=lambda x: x[""score""] / len(x[""yseq""]), reverse=True\n            )[:nbest]\n        else:\n            nbest_hyps = sorted(kept_hyps, key=lambda x: x[""score""], reverse=True)[\n                :nbest\n            ]\n\n        return nbest_hyps\n'"
espnet/nets/pytorch_backend/transducer/transformer_decoder_layer.py,5,"b'""""""Decoder layer definition for transformer-transducer models.""""""\n\nimport torch\nfrom torch import nn\n\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n\n\nclass DecoderLayer(nn.Module):\n    """"""Single decoder layer module for transformer-transducer models.\n\n    Args:\n        size (int): input dim\n        self_attn (MultiHeadedAttention): self attention module\n        feed_forward (PositionwiseFeedForward): feed forward layer module\n        dropout_rate (float): dropout rate\n        normalize_before (bool): whether to use layer_norm before the first block\n        concat_after (bool): whether to concat attention layer\'s input and output\n\n    """"""\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        """"""Construct an DecoderLayer object.""""""\n        super(DecoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n\n        self.norm1 = LayerNorm(size)\n        self.norm2 = LayerNorm(size)\n\n        self.dropout = nn.Dropout(dropout_rate)\n\n        self.size = size\n\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat = nn.Linear((size + size), size)\n\n    def forward(self, tgt, tgt_mask, cache=None):\n        """"""Compute decoded features.\n\n        Args:\n            x (torch.Tensor): decoded previous target features (B, Lmax, idim)\n            mask (torch.Tensor): mask for x (batch, Lmax)\n            cache (torch.Tensor): cached output (B, Lmax-1, idim)\n\n        """"""\n        residual = tgt\n        if self.normalize_before:\n            tgt = self.norm1(tgt)\n\n        if cache is None:\n            tgt_q = tgt\n        else:\n            assert cache.shape == (\n                tgt.shape[0],\n                tgt.shape[1] - 1,\n                self.size,\n            ), f""{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}""\n\n            tgt_q = tgt[:, -1, :]\n            residual = residual[:, -1, :]\n\n            if tgt_mask is not None:\n                tgt_mask = tgt_mask[:, -1:, :]\n\n        if self.concat_after:\n            tgt_concat = torch.cat(\n                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_mask)), dim=-1\n            )\n            tgt = residual + self.concat(tgt_concat)\n        else:\n            tgt = residual + self.dropout(self.self_attn(tgt_q, tgt, tgt, tgt_mask))\n        if not self.normalize_before:\n            tgt = self.norm1(tgt)\n\n        residual = tgt\n        if self.normalize_before:\n            tgt = self.norm2(tgt)\n\n        tgt = residual + self.dropout(self.feed_forward(tgt))\n\n        if not self.normalize_before:\n            tgt = self.norm2(tgt)\n\n        if cache is not None:\n            tgt = torch.cat([cache, tgt], dim=1)\n\n        return tgt, tgt_mask\n'"
espnet/nets/pytorch_backend/transducer/utils.py,11,"b'""""""Utility functions for transducer models.""""""\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import pad_list\n\n\ndef prepare_loss_inputs(ys_pad, hlens, blank_id=0, ignore_id=-1):\n    """"""Prepare tensors for transducer loss computation.\n\n    Args:\n        ys_pad (torch.Tensor): batch of padded target sequences (B, Lmax)\n        hlens (torch.Tensor): batch of hidden sequence lengthts (B)\n                              or batch of masks (B, 1, Tmax)\n        blank_id (int): index of blank label\n        ignore_id (int): index of initial padding\n\n    Returns:\n        ys_in_pad (torch.Tensor): batch of padded target sequences + blank (B, Lmax + 1)\n        target (torch.Tensor): batch of padded target sequences (B, Lmax)\n        pred_len (torch.Tensor): batch of hidden sequence lengths (B)\n        target_len (torch.Tensor): batch of output sequence lengths (B)\n\n    """"""\n    device = ys_pad.device\n\n    ys = [y[y != ignore_id] for y in ys_pad]\n\n    blank = ys[0].new([blank_id])\n\n    ys_in = [torch.cat([blank, y], dim=0) for y in ys]\n    ys_in_pad = pad_list(ys_in, blank_id)\n\n    target = pad_list(ys, blank_id).type(torch.int32)\n    target_len = torch.IntTensor([y.size(0) for y in ys])\n\n    if torch.is_tensor(hlens):\n        if hlens.dim() > 1:\n            hs = [h[h != 0] for h in hlens]\n            hlens = list(map(int, [h.size(0) for h in hs]))\n        else:\n            hlens = list(map(int, hlens))\n\n    pred_len = torch.IntTensor(hlens)\n\n    pred_len = pred_len.to(device)\n    target = target.to(device)\n    target_len = target_len.to(device)\n\n    return ys_in_pad, target, pred_len, target_len\n'"
espnet/nets/pytorch_backend/transducer/vgg.py,20,"b'""""""VGG2L definition for transformer-transducer.""""""\n\nimport torch\n\n\nclass VGG2L(torch.nn.Module):\n    """"""VGG2L module for transformer-transducer encoder.""""""\n\n    def __init__(self, idim, odim):\n        """"""Construct a VGG2L object.\n\n        Args:\n            idim (int): dimension of inputs\n            odim (int): dimension of outputs\n\n        """"""\n        super(VGG2L, self).__init__()\n\n        self.vgg2l = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 64, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(64, 64, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((3, 2)),\n            torch.nn.Conv2d(64, 128, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(128, 128, 3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d((2, 2)),\n        )\n\n        self.output = torch.nn.Linear(128 * ((idim // 2) // 2), odim)\n\n    def forward(self, x, x_mask):\n        """"""VGG2L forward for x.\n\n        Args:\n            x (torch.Tensor): input torch (B, T, idim)\n            x_mask (torch.Tensor): (B, 1, T)\n\n        Returns:\n            x (torch.Tensor): input torch (B, sub(T), attention_dim)\n            x_mask (torch.Tensor): (B, 1, sub(T))\n\n        """"""\n        x = x.unsqueeze(1)\n        x = self.vgg2l(x)\n\n        b, c, t, f = x.size()\n\n        x = self.output(x.transpose(1, 2).contiguous().view(b, t, c * f))\n\n        if x_mask is None:\n            return x, None\n        else:\n            x_mask = self.create_new_mask(x_mask, x)\n\n            return x, x_mask\n\n    def create_new_mask(self, x_mask, x):\n        """"""Create a subsampled version of x_mask.\n\n        Args:\n            x_mask (torch.Tensor): (B, 1, T)\n            x (torch.Tensor): (B, sub(T), attention_dim)\n\n        Returns:\n            x_mask (torch.Tensor): (B, 1, sub(T))\n\n        """"""\n        x_t1 = x_mask.size(2) - (x_mask.size(2) % 3)\n        x_mask = x_mask[:, :, :x_t1][:, :, ::3]\n\n        x_t2 = x_mask.size(2) - (x_mask.size(2) % 2)\n        x_mask = x_mask[:, :, :x_t2][:, :, ::2]\n\n        return x_mask\n'"
espnet/nets/pytorch_backend/transformer/__init__.py,0,"b'""""""Initialize sub package.""""""\n'"
espnet/nets/pytorch_backend/transformer/add_sos_eos.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Unility funcitons for Transformer.""""""\n\nimport torch\n\n\ndef add_sos_eos(ys_pad, sos, eos, ignore_id):\n    """"""Add <sos> and <eos> labels.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int sos: index of <sos>\n    :param int eos: index of <eeos>\n    :param int ignore_id: index of padding\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    :return: padded tensor (B, Lmax)\n    :rtype: torch.Tensor\n    """"""\n    from espnet.nets.pytorch_backend.nets_utils import pad_list\n\n    _sos = ys_pad.new([sos])\n    _eos = ys_pad.new([eos])\n    ys = [y[y != ignore_id] for y in ys_pad]  # parse padded ys\n    ys_in = [torch.cat([_sos, y], dim=0) for y in ys]\n    ys_out = [torch.cat([y, _eos], dim=0) for y in ys]\n    return pad_list(ys_in, eos), pad_list(ys_out, ignore_id)\n'"
espnet/nets/pytorch_backend/transformer/attention.py,11,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Multi-Head Attention layer definition.""""""\n\nimport math\n\nimport numpy\nimport torch\nfrom torch import nn\n\n\nclass MultiHeadedAttention(nn.Module):\n    """"""Multi-Head Attention layer.\n\n    :param int n_head: the number of head s\n    :param int n_feat: the number of features\n    :param float dropout_rate: dropout rate\n\n    """"""\n\n    def __init__(self, n_head, n_feat, dropout_rate):\n        """"""Construct an MultiHeadedAttention object.""""""\n        super(MultiHeadedAttention, self).__init__()\n        assert n_feat % n_head == 0\n        # We assume d_v always equals d_k\n        self.d_k = n_feat // n_head\n        self.h = n_head\n        self.linear_q = nn.Linear(n_feat, n_feat)\n        self.linear_k = nn.Linear(n_feat, n_feat)\n        self.linear_v = nn.Linear(n_feat, n_feat)\n        self.linear_out = nn.Linear(n_feat, n_feat)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout_rate)\n\n    def forward(self, query, key, value, mask):\n        """"""Compute \'Scaled Dot Product Attention\'.\n\n        :param torch.Tensor query: (batch, time1, size)\n        :param torch.Tensor key: (batch, time2, size)\n        :param torch.Tensor value: (batch, time2, size)\n        :param torch.Tensor mask: (batch, time1, time2)\n        :param torch.nn.Dropout dropout:\n        :return torch.Tensor: attentined and transformed `value` (batch, time1, d_model)\n             weighted by the query dot key attention (batch, head, time1, time2)\n        """"""\n        n_batch = query.size(0)\n        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n        q = q.transpose(1, 2)  # (batch, head, time1, d_k)\n        k = k.transpose(1, 2)  # (batch, head, time2, d_k)\n        v = v.transpose(1, 2)  # (batch, head, time2, d_k)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(\n            self.d_k\n        )  # (batch, head, time1, time2)\n        if mask is not None:\n            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, time1, time2)\n            min_value = float(\n                numpy.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min\n            )\n            scores = scores.masked_fill(mask, min_value)\n            self.attn = torch.softmax(scores, dim=-1).masked_fill(\n                mask, 0.0\n            )  # (batch, head, time1, time2)\n        else:\n            self.attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n\n        p_attn = self.dropout(self.attn)\n        x = torch.matmul(p_attn, v)  # (batch, head, time1, d_k)\n        x = (\n            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)\n        )  # (batch, time1, d_model)\n        return self.linear_out(x)  # (batch, time1, d_model)\n'"
espnet/nets/pytorch_backend/transformer/decoder.py,38,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Decoder definition.""""""\n\nfrom typing import Any\nfrom typing import List\nfrom typing import Tuple\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import rename_state_dict\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.decoder_layer import DecoderLayer\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.pytorch_backend.transformer.mask import subsequent_mask\nfrom espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.repeat import repeat\nfrom espnet.nets.scorer_interface import BatchScorerInterface\n\n\ndef _pre_hook(\n    state_dict,\n    prefix,\n    local_metadata,\n    strict,\n    missing_keys,\n    unexpected_keys,\n    error_msgs,\n):\n    # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n    rename_state_dict(prefix + ""output_norm."", prefix + ""after_norm."", state_dict)\n\n\nclass Decoder(BatchScorerInterface, torch.nn.Module):\n    """"""Transfomer decoder module.\n\n    :param int odim: output dim\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate for attention\n    :param str or torch.nn.Module input_layer: input layer type\n    :param bool use_output_layer: whether to use output layer\n    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer\'s input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    """"""\n\n    def __init__(\n        self,\n        odim,\n        attention_dim=256,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        self_attention_dropout_rate=0.0,\n        src_attention_dropout_rate=0.0,\n        input_layer=""embed"",\n        use_output_layer=True,\n        pos_enc_class=PositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        """"""Construct an Decoder object.""""""\n        torch.nn.Module.__init__(self)\n        self._register_load_state_dict_pre_hook(_pre_hook)\n        if input_layer == ""embed"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(odim, attention_dim),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer == ""linear"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(odim, attention_dim),\n                torch.nn.LayerNorm(attention_dim),\n                torch.nn.Dropout(dropout_rate),\n                torch.nn.ReLU(),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif isinstance(input_layer, torch.nn.Module):\n            self.embed = torch.nn.Sequential(\n                input_layer, pos_enc_class(attention_dim, positional_dropout_rate)\n            )\n        else:\n            raise NotImplementedError(""only `embed` or torch.nn.Module is supported."")\n        self.normalize_before = normalize_before\n        self.decoders = repeat(\n            num_blocks,\n            lambda: DecoderLayer(\n                attention_dim,\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, self_attention_dropout_rate\n                ),\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, src_attention_dropout_rate\n                ),\n                PositionwiseFeedForward(attention_dim, linear_units, dropout_rate),\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n        if use_output_layer:\n            self.output_layer = torch.nn.Linear(attention_dim, odim)\n        else:\n            self.output_layer = None\n\n    def forward(self, tgt, tgt_mask, memory, memory_mask):\n        """"""Forward decoder.\n\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n                                 if input_layer == ""embed""\n                                 input tensor (batch, maxlen_out, #mels)\n                                 in the other cases\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param torch.Tensor memory_mask: encoded memory mask,  (batch, maxlen_in)\n                                         dtype=torch.uint8 in PyTorch 1.2-\n                                         dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :return x: decoded token score before softmax (batch, maxlen_out, token)\n                   if use_output_layer is True,\n                   final block outputs (batch, maxlen_out, attention_dim)\n                   in the other cases\n        :rtype: torch.Tensor\n        :return tgt_mask: score mask before softmax (batch, maxlen_out)\n        :rtype: torch.Tensor\n        """"""\n        x = self.embed(tgt)\n        x, tgt_mask, memory, memory_mask = self.decoders(\n            x, tgt_mask, memory, memory_mask\n        )\n        if self.normalize_before:\n            x = self.after_norm(x)\n        if self.output_layer is not None:\n            x = self.output_layer(x)\n        return x, tgt_mask\n\n    def forward_one_step(self, tgt, tgt_mask, memory, cache=None):\n        """"""Forward one step.\n\n        :param torch.Tensor tgt: input token ids, int64 (batch, maxlen_out)\n        :param torch.Tensor tgt_mask: input token mask,  (batch, maxlen_out)\n                                      dtype=torch.uint8 in PyTorch 1.2-\n                                      dtype=torch.bool in PyTorch 1.2+ (include 1.2)\n        :param torch.Tensor memory: encoded memory, float32  (batch, maxlen_in, feat)\n        :param List[torch.Tensor] cache:\n            cached output list of (batch, max_time_out-1, size)\n        :return y, cache: NN output value and cache per `self.decoders`.\n            `y.shape` is (batch, maxlen_out, token)\n        :rtype: Tuple[torch.Tensor, List[torch.Tensor]]\n        """"""\n        x = self.embed(tgt)\n        if cache is None:\n            cache = [None] * len(self.decoders)\n        new_cache = []\n        for c, decoder in zip(cache, self.decoders):\n            x, tgt_mask, memory, memory_mask = decoder(\n                x, tgt_mask, memory, None, cache=c\n            )\n            new_cache.append(x)\n\n        if self.normalize_before:\n            y = self.after_norm(x[:, -1])\n        else:\n            y = x[:, -1]\n        if self.output_layer is not None:\n            y = torch.log_softmax(self.output_layer(y), dim=-1)\n\n        return y, new_cache\n\n    # beam search API (see ScorerInterface)\n    def score(self, ys, state, x):\n        """"""Score.""""""\n        ys_mask = subsequent_mask(len(ys), device=x.device).unsqueeze(0)\n        logp, state = self.forward_one_step(\n            ys.unsqueeze(0), ys_mask, x.unsqueeze(0), cache=state\n        )\n        return logp.squeeze(0), state\n\n    # batch beam search API (see BatchScorerInterface)\n    def batch_score(\n        self, ys: torch.Tensor, states: List[Any], xs: torch.Tensor\n    ) -> Tuple[torch.Tensor, List[Any]]:\n        """"""Score new token batch (required).\n\n        Args:\n            ys (torch.Tensor): torch.int64 prefix tokens (n_batch, ylen).\n            states (List[Any]): Scorer states for prefix tokens.\n            xs (torch.Tensor):\n                The encoder feature that generates ys (n_batch, xlen, n_feat).\n\n        Returns:\n            tuple[torch.Tensor, List[Any]]: Tuple of\n                batchfied scores for next token with shape of `(n_batch, n_vocab)`\n                and next state list for ys.\n\n        """"""\n        # merge states\n        n_batch = len(ys)\n        n_layers = len(self.decoders)\n        if states[0] is None:\n            batch_state = None\n        else:\n            # transpose state of [batch, layer] into [layer, batch]\n            batch_state = [\n                torch.stack([states[b][i] for b in range(n_batch)])\n                for i in range(n_layers)\n            ]\n\n        # batch decoding\n        ys_mask = subsequent_mask(ys.size(-1), device=xs.device).unsqueeze(0)\n        logp, states = self.forward_one_step(ys, ys_mask, xs, cache=batch_state)\n\n        # transpose state of [layer, batch] into [batch, layer]\n        state_list = [[states[i][b] for i in range(n_layers)] for b in range(n_batch)]\n        return logp, state_list\n'"
espnet/nets/pytorch_backend/transformer/decoder_layer.py,8,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Decoder self-attention layer definition.""""""\n\nimport torch\nfrom torch import nn\n\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n\n\nclass DecoderLayer(nn.Module):\n    """"""Single decoder layer module.\n\n    :param int size: input dim\n    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n        self_attn: self attention module\n    :param espnet.nets.pytorch_backend.transformer.attention.MultiHeadedAttention\n        src_attn: source attention module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n        PositionwiseFeedForward feed_forward: feed forward layer module\n    :param float dropout_rate: dropout rate\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer\'s input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n\n    """"""\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        src_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        """"""Construct an DecoderLayer object.""""""\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.norm1 = LayerNorm(size)\n        self.norm2 = LayerNorm(size)\n        self.norm3 = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear1 = nn.Linear(size + size, size)\n            self.concat_linear2 = nn.Linear(size + size, size)\n\n    def forward(self, tgt, tgt_mask, memory, memory_mask, cache=None):\n        """"""Compute decoded features.\n\n        Args:\n            tgt (torch.Tensor):\n                decoded previous target features (batch, max_time_out, size)\n            tgt_mask (torch.Tensor): mask for x (batch, max_time_out)\n            memory (torch.Tensor): encoded source features (batch, max_time_in, size)\n            memory_mask (torch.Tensor): mask for memory (batch, max_time_in)\n            cache (torch.Tensor): cached output (batch, max_time_out-1, size)\n\n        """"""\n        residual = tgt\n        if self.normalize_before:\n            tgt = self.norm1(tgt)\n\n        if cache is None:\n            tgt_q = tgt\n            tgt_q_mask = tgt_mask\n        else:\n            # compute only the last frame query keeping dim: max_time_out -> 1\n            assert cache.shape == (\n                tgt.shape[0],\n                tgt.shape[1] - 1,\n                self.size,\n            ), f""{cache.shape} == {(tgt.shape[0], tgt.shape[1] - 1, self.size)}""\n            tgt_q = tgt[:, -1:, :]\n            residual = residual[:, -1:, :]\n            tgt_q_mask = None\n            if tgt_mask is not None:\n                tgt_q_mask = tgt_mask[:, -1:, :]\n\n        if self.concat_after:\n            tgt_concat = torch.cat(\n                (tgt_q, self.self_attn(tgt_q, tgt, tgt, tgt_q_mask)), dim=-1\n            )\n            x = residual + self.concat_linear1(tgt_concat)\n        else:\n            x = residual + self.dropout(self.self_attn(tgt_q, tgt, tgt, tgt_q_mask))\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        if self.concat_after:\n            x_concat = torch.cat(\n                (x, self.src_attn(x, memory, memory, memory_mask)), dim=-1\n            )\n            x = residual + self.concat_linear2(x_concat)\n        else:\n            x = residual + self.dropout(self.src_attn(x, memory, memory, memory_mask))\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm3(x)\n        x = residual + self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm3(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        return x, tgt_mask, memory, memory_mask\n'"
espnet/nets/pytorch_backend/transformer/embedding.py,16,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Positonal Encoding Module.""""""\n\nimport math\n\nimport torch\n\n\ndef _pre_hook(\n    state_dict,\n    prefix,\n    local_metadata,\n    strict,\n    missing_keys,\n    unexpected_keys,\n    error_msgs,\n):\n    """"""Perform pre-hook in load_state_dict for backward compatibility.\n\n    Note:\n        We saved self.pe until v.0.5.2 but we have omitted it later.\n        Therefore, we remove the item ""pe"" from `state_dict` for backward compatibility.\n\n    """"""\n    k = prefix + ""pe""\n    if k in state_dict:\n        state_dict.pop(k)\n\n\nclass PositionalEncoding(torch.nn.Module):\n    """"""Positional encoding.\n\n    :param int d_model: embedding dim\n    :param float dropout_rate: dropout rate\n    :param int max_len: maximum input length\n\n    """"""\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        """"""Construct an PositionalEncoding object.""""""\n        super(PositionalEncoding, self).__init__()\n        self.d_model = d_model\n        self.xscale = math.sqrt(self.d_model)\n        self.dropout = torch.nn.Dropout(p=dropout_rate)\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, max_len))\n        self._register_load_state_dict_pre_hook(_pre_hook)\n\n    def extend_pe(self, x):\n        """"""Reset the positional encodings.""""""\n        if self.pe is not None:\n            if self.pe.size(1) >= x.size(1):\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        pe = torch.zeros(x.size(1), self.d_model)\n        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.float32)\n            * -(math.log(10000.0) / self.d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, x: torch.Tensor):\n        """"""Add positional encoding.\n\n        Args:\n            x (torch.Tensor): Input. Its shape is (batch, time, ...)\n\n        Returns:\n            torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)\n\n        """"""\n        self.extend_pe(x)\n        x = x * self.xscale + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n\n\nclass ScaledPositionalEncoding(PositionalEncoding):\n    """"""Scaled positional encoding module.\n\n    See also: Sec. 3.2  https://arxiv.org/pdf/1809.08895.pdf\n\n    """"""\n\n    def __init__(self, d_model, dropout_rate, max_len=5000):\n        """"""Initialize class.\n\n        :param int d_model: embedding dim\n        :param float dropout_rate: dropout rate\n        :param int max_len: maximum input length\n\n        """"""\n        super().__init__(d_model=d_model, dropout_rate=dropout_rate, max_len=max_len)\n        self.alpha = torch.nn.Parameter(torch.tensor(1.0))\n\n    def reset_parameters(self):\n        """"""Reset parameters.""""""\n        self.alpha.data = torch.tensor(1.0)\n\n    def forward(self, x):\n        """"""Add positional encoding.\n\n        Args:\n            x (torch.Tensor): Input. Its shape is (batch, time, ...)\n\n        Returns:\n            torch.Tensor: Encoded tensor. Its shape is (batch, time, ...)\n\n        """"""\n        self.extend_pe(x)\n        x = x + self.alpha * self.pe[:, : x.size(1)]\n        return self.dropout(x)\n'"
espnet/nets/pytorch_backend/transformer/encoder.py,19,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Encoder definition.""""""\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.nets_utils import rename_state_dict\nfrom espnet.nets.pytorch_backend.transducer.vgg import VGG2L\nfrom espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\nfrom espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\nfrom espnet.nets.pytorch_backend.transformer.multi_layer_conv import Conv1dLinear\nfrom espnet.nets.pytorch_backend.transformer.multi_layer_conv import MultiLayeredConv1d\nfrom espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import (\n    PositionwiseFeedForward,  # noqa: H301\n)\nfrom espnet.nets.pytorch_backend.transformer.repeat import repeat\nfrom espnet.nets.pytorch_backend.transformer.subsampling import Conv2dSubsampling\n\n\ndef _pre_hook(\n    state_dict,\n    prefix,\n    local_metadata,\n    strict,\n    missing_keys,\n    unexpected_keys,\n    error_msgs,\n):\n    # https://github.com/espnet/espnet/commit/21d70286c354c66c0350e65dc098d2ee236faccc#diff-bffb1396f038b317b2b64dd96e6d3563\n    rename_state_dict(prefix + ""input_layer."", prefix + ""embed."", state_dict)\n    # https://github.com/espnet/espnet/commit/3d422f6de8d4f03673b89e1caef698745ec749ea#diff-bffb1396f038b317b2b64dd96e6d3563\n    rename_state_dict(prefix + ""norm."", prefix + ""after_norm."", state_dict)\n\n\nclass Encoder(torch.nn.Module):\n    """"""Transformer encoder module.\n\n    :param int idim: input dim\n    :param int attention_dim: dimention of attention\n    :param int attention_heads: the number of heads of multi head attention\n    :param int linear_units: the number of units of position-wise feed forward\n    :param int num_blocks: the number of decoder blocks\n    :param float dropout_rate: dropout rate\n    :param float attention_dropout_rate: dropout rate in attention\n    :param float positional_dropout_rate: dropout rate after adding positional encoding\n    :param str or torch.nn.Module input_layer: input layer type\n    :param class pos_enc_class: PositionalEncoding or ScaledPositionalEncoding\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer\'s input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n    :param str positionwise_layer_type: linear of conv1d\n    :param int positionwise_conv_kernel_size: kernel size of positionwise conv1d layer\n    :param int padding_idx: padding_idx for input_layer=embed\n    """"""\n\n    def __init__(\n        self,\n        idim,\n        attention_dim=256,\n        attention_heads=4,\n        linear_units=2048,\n        num_blocks=6,\n        dropout_rate=0.1,\n        positional_dropout_rate=0.1,\n        attention_dropout_rate=0.0,\n        input_layer=""conv2d"",\n        pos_enc_class=PositionalEncoding,\n        normalize_before=True,\n        concat_after=False,\n        positionwise_layer_type=""linear"",\n        positionwise_conv_kernel_size=1,\n        padding_idx=-1,\n    ):\n        """"""Construct an Encoder object.""""""\n        super(Encoder, self).__init__()\n        self._register_load_state_dict_pre_hook(_pre_hook)\n\n        if input_layer == ""linear"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Linear(idim, attention_dim),\n                torch.nn.LayerNorm(attention_dim),\n                torch.nn.Dropout(dropout_rate),\n                torch.nn.ReLU(),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer == ""conv2d"":\n            self.embed = Conv2dSubsampling(idim, attention_dim, dropout_rate)\n        elif input_layer == ""vgg2l"":\n            self.embed = VGG2L(idim, attention_dim)\n        elif input_layer == ""embed"":\n            self.embed = torch.nn.Sequential(\n                torch.nn.Embedding(idim, attention_dim, padding_idx=padding_idx),\n                pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif isinstance(input_layer, torch.nn.Module):\n            self.embed = torch.nn.Sequential(\n                input_layer, pos_enc_class(attention_dim, positional_dropout_rate),\n            )\n        elif input_layer is None:\n            self.embed = torch.nn.Sequential(\n                pos_enc_class(attention_dim, positional_dropout_rate)\n            )\n        else:\n            raise ValueError(""unknown input_layer: "" + input_layer)\n        self.normalize_before = normalize_before\n        if positionwise_layer_type == ""linear"":\n            positionwise_layer = PositionwiseFeedForward\n            positionwise_layer_args = (attention_dim, linear_units, dropout_rate)\n        elif positionwise_layer_type == ""conv1d"":\n            positionwise_layer = MultiLayeredConv1d\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        elif positionwise_layer_type == ""conv1d-linear"":\n            positionwise_layer = Conv1dLinear\n            positionwise_layer_args = (\n                attention_dim,\n                linear_units,\n                positionwise_conv_kernel_size,\n                dropout_rate,\n            )\n        else:\n            raise NotImplementedError(""Support only linear or conv1d."")\n        self.encoders = repeat(\n            num_blocks,\n            lambda: EncoderLayer(\n                attention_dim,\n                MultiHeadedAttention(\n                    attention_heads, attention_dim, attention_dropout_rate\n                ),\n                positionwise_layer(*positionwise_layer_args),\n                dropout_rate,\n                normalize_before,\n                concat_after,\n            ),\n        )\n        if self.normalize_before:\n            self.after_norm = LayerNorm(attention_dim)\n\n    def forward(self, xs, masks):\n        """"""Encode input sequence.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :return: position embedded tensor and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]:\n        """"""\n        if isinstance(self.embed, (Conv2dSubsampling, VGG2L)):\n            xs, masks = self.embed(xs, masks)\n        else:\n            xs = self.embed(xs)\n        xs, masks = self.encoders(xs, masks)\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n        return xs, masks\n\n    def forward_one_step(self, xs, masks, cache=None):\n        """"""Encode input frame.\n\n        :param torch.Tensor xs: input tensor\n        :param torch.Tensor masks: input mask\n        :param List[torch.Tensor] cache: cache tensors\n        :return: position embedded tensor, mask and new cache\n        :rtype Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        """"""\n        if isinstance(self.embed, Conv2dSubsampling):\n            xs, masks = self.embed(xs, masks)\n        else:\n            xs = self.embed(xs)\n        if cache is None:\n            cache = [None for _ in range(len(self.encoders))]\n        new_cache = []\n        for c, e in zip(cache, self.encoders):\n            xs, masks = e(xs, masks, cache=c)\n            new_cache.append(xs)\n        if self.normalize_before:\n            xs = self.after_norm(xs)\n        return xs, masks, new_cache\n'"
espnet/nets/pytorch_backend/transformer/encoder_layer.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Encoder self-attention layer definition.""""""\n\nimport torch\n\nfrom torch import nn\n\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n\n\nclass EncoderLayer(nn.Module):\n    """"""Encoder layer module.\n\n    :param int size: input dim\n    :param espnet.nets.pytorch_backend.transformer.attention.\n        MultiHeadedAttention self_attn: self attention module\n    :param espnet.nets.pytorch_backend.transformer.positionwise_feed_forward.\n        PositionwiseFeedForward feed_forward:\n        feed forward module\n    :param float dropout_rate: dropout rate\n    :param bool normalize_before: whether to use layer_norm before the first block\n    :param bool concat_after: whether to concat attention layer\'s input and output\n        if True, additional linear will be applied.\n        i.e. x -> x + linear(concat(x, att(x)))\n        if False, no additional linear will be applied. i.e. x -> x + att(x)\n\n    """"""\n\n    def __init__(\n        self,\n        size,\n        self_attn,\n        feed_forward,\n        dropout_rate,\n        normalize_before=True,\n        concat_after=False,\n    ):\n        """"""Construct an EncoderLayer object.""""""\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.norm1 = LayerNorm(size)\n        self.norm2 = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.size = size\n        self.normalize_before = normalize_before\n        self.concat_after = concat_after\n        if self.concat_after:\n            self.concat_linear = nn.Linear(size + size, size)\n\n    def forward(self, x, mask, cache=None):\n        """"""Compute encoded features.\n\n        :param torch.Tensor x: encoded source features (batch, max_time_in, size)\n        :param torch.Tensor mask: mask for x (batch, max_time_in)\n        :param torch.Tensor cache: cache for x (batch, max_time_in - 1, size)\n        :rtype: Tuple[torch.Tensor, torch.Tensor]\n        """"""\n        residual = x\n        if self.normalize_before:\n            x = self.norm1(x)\n\n        if cache is None:\n            x_q = x\n        else:\n            assert cache.shape == (x.shape[0], x.shape[1] - 1, self.size)\n            x_q = x[:, -1:, :]\n            residual = residual[:, -1:, :]\n            mask = None if mask is None else mask[:, -1:, :]\n\n        if self.concat_after:\n            x_concat = torch.cat((x, self.self_attn(x_q, x, x, mask)), dim=-1)\n            x = residual + self.concat_linear(x_concat)\n        else:\n            x = residual + self.dropout(self.self_attn(x_q, x, x, mask))\n        if not self.normalize_before:\n            x = self.norm1(x)\n\n        residual = x\n        if self.normalize_before:\n            x = self.norm2(x)\n        x = residual + self.dropout(self.feed_forward(x))\n        if not self.normalize_before:\n            x = self.norm2(x)\n\n        if cache is not None:\n            x = torch.cat([cache, x], dim=1)\n\n        return x, mask\n'"
espnet/nets/pytorch_backend/transformer/initializer.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Parameter initialization.""""""\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n\n\ndef initialize(model, init_type=""pytorch""):\n    """"""Initialize Transformer module.\n\n    :param torch.nn.Module model: transformer instance\n    :param str init_type: initialization type\n    """"""\n    if init_type == ""pytorch"":\n        return\n\n    # weight init\n    for p in model.parameters():\n        if p.dim() > 1:\n            if init_type == ""xavier_uniform"":\n                torch.nn.init.xavier_uniform_(p.data)\n            elif init_type == ""xavier_normal"":\n                torch.nn.init.xavier_normal_(p.data)\n            elif init_type == ""kaiming_uniform"":\n                torch.nn.init.kaiming_uniform_(p.data, nonlinearity=""relu"")\n            elif init_type == ""kaiming_normal"":\n                torch.nn.init.kaiming_normal_(p.data, nonlinearity=""relu"")\n            else:\n                raise ValueError(""Unknown initialization: "" + init_type)\n    # bias init\n    for p in model.parameters():\n        if p.dim() == 1:\n            p.data.zero_()\n\n    # reset some modules with default init\n    for m in model.modules():\n        if isinstance(m, (torch.nn.Embedding, LayerNorm)):\n            m.reset_parameters()\n'"
espnet/nets/pytorch_backend/transformer/label_smoothing_loss.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Label smoothing module.""""""\n\nimport torch\nfrom torch import nn\n\n\nclass LabelSmoothingLoss(nn.Module):\n    """"""Label-smoothing loss.\n\n    :param int size: the number of class\n    :param int padding_idx: ignored class id\n    :param float smoothing: smoothing rate (0.0 means the conventional CE)\n    :param bool normalize_length: normalize loss by sequence length if True\n    :param torch.nn.Module criterion: loss function to be smoothed\n    """"""\n\n    def __init__(\n        self,\n        size,\n        padding_idx,\n        smoothing,\n        normalize_length=False,\n        criterion=nn.KLDivLoss(reduction=""none""),\n    ):\n        """"""Construct an LabelSmoothingLoss object.""""""\n        super(LabelSmoothingLoss, self).__init__()\n        self.criterion = criterion\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        self.normalize_length = normalize_length\n\n    def forward(self, x, target):\n        """"""Compute loss between x and target.\n\n        :param torch.Tensor x: prediction (batch, seqlen, class)\n        :param torch.Tensor target:\n            target signal masked with self.padding_id (batch, seqlen)\n        :return: scalar float value\n        :rtype torch.Tensor\n        """"""\n        assert x.size(2) == self.size\n        batch_size = x.size(0)\n        x = x.view(-1, self.size)\n        target = target.view(-1)\n        with torch.no_grad():\n            true_dist = x.clone()\n            true_dist.fill_(self.smoothing / (self.size - 1))\n            ignore = target == self.padding_idx  # (B,)\n            total = len(target) - ignore.sum().item()\n            target = target.masked_fill(ignore, 0)  # avoid -1 index\n            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n        kl = self.criterion(torch.log_softmax(x, dim=1), true_dist)\n        denom = total if self.normalize_length else batch_size\n        return kl.masked_fill(ignore.unsqueeze(1), 0).sum() / denom\n'"
espnet/nets/pytorch_backend/transformer/layer_norm.py,3,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Layer normalization module.""""""\n\nimport torch\n\n\nclass LayerNorm(torch.nn.LayerNorm):\n    """"""Layer normalization module.\n\n    :param int nout: output dim size\n    :param int dim: dimension to be normalized\n    """"""\n\n    def __init__(self, nout, dim=-1):\n        """"""Construct an LayerNorm object.""""""\n        super(LayerNorm, self).__init__(nout, eps=1e-12)\n        self.dim = dim\n\n    def forward(self, x):\n        """"""Apply layer normalization.\n\n        :param torch.Tensor x: input tensor\n        :return: layer normalized tensor\n        :rtype torch.Tensor\n        """"""\n        if self.dim == -1:\n            return super(LayerNorm, self).forward(x)\n        return super(LayerNorm, self).forward(x.transpose(1, -1)).transpose(1, -1)\n'"
espnet/nets/pytorch_backend/transformer/mask.py,15,"b'# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Mask module.""""""\n\nfrom distutils.version import LooseVersion\n\nimport torch\n\nis_torch_1_2_plus = LooseVersion(torch.__version__) >= LooseVersion(""1.2.0"")\n# LooseVersion(\'1.2.0\') == LooseVersion(torch.__version__) can\'t include e.g. 1.2.0+aaa\nis_torch_1_2 = (\n    LooseVersion(""1.3"") > LooseVersion(torch.__version__) >= LooseVersion(""1.2"")\n)\ndatatype = torch.bool if is_torch_1_2_plus else torch.uint8\n\n\ndef subsequent_mask(size, device=""cpu"", dtype=datatype):\n    """"""Create mask for subsequent steps (1, size, size).\n\n    :param int size: size of mask\n    :param str device: ""cpu"" or ""cuda"" or torch.Tensor.device\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    >>> subsequent_mask(3)\n    [[1, 0, 0],\n     [1, 1, 0],\n     [1, 1, 1]]\n    """"""\n    if is_torch_1_2 and dtype == torch.bool:\n        # torch=1.2 doesn\'t support tril for bool tensor\n        ret = torch.ones(size, size, device=device, dtype=torch.uint8)\n        return torch.tril(ret, out=ret).type(dtype)\n    else:\n        ret = torch.ones(size, size, device=device, dtype=dtype)\n        return torch.tril(ret, out=ret)\n\n\ndef target_mask(ys_in_pad, ignore_id):\n    """"""Create mask for decoder self-attention.\n\n    :param torch.Tensor ys_pad: batch of padded target sequences (B, Lmax)\n    :param int ignore_id: index of padding\n    :param torch.dtype dtype: result dtype\n    :rtype: torch.Tensor\n    """"""\n    ys_mask = ys_in_pad != ignore_id\n    m = subsequent_mask(ys_mask.size(-1), device=ys_mask.device).unsqueeze(0)\n    return ys_mask.unsqueeze(-2) & m\n'"
espnet/nets/pytorch_backend/transformer/multi_layer_conv.py,10,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Tomoki Hayashi\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Layer modules for FFT block in FastSpeech (Feed-forward Transformer).""""""\n\nimport torch\n\n\nclass MultiLayeredConv1d(torch.nn.Module):\n    """"""Multi-layered conv1d for Transformer block.\n\n    This is a module of multi-leyered conv1d designed\n    to replace positionwise feed-forward network\n    in Transforner block, which is introduced in\n    `FastSpeech: Fast, Robust and Controllable Text to Speech`_.\n\n    .. _`FastSpeech: Fast, Robust and Controllable Text to Speech`:\n        https://arxiv.org/pdf/1905.09263.pdf\n\n    """"""\n\n    def __init__(self, in_chans, hidden_chans, kernel_size, dropout_rate):\n        """"""Initialize MultiLayeredConv1d module.\n\n        Args:\n            in_chans (int): Number of input channels.\n            hidden_chans (int): Number of hidden channels.\n            kernel_size (int): Kernel size of conv1d.\n            dropout_rate (float): Dropout rate.\n\n        """"""\n        super(MultiLayeredConv1d, self).__init__()\n        self.w_1 = torch.nn.Conv1d(\n            in_chans,\n            hidden_chans,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n        )\n        self.w_2 = torch.nn.Conv1d(\n            hidden_chans,\n            in_chans,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n        )\n        self.dropout = torch.nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of input tensors (B, ..., in_chans).\n\n        Returns:\n            Tensor: Batch of output tensors (B, ..., hidden_chans).\n\n        """"""\n        x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)\n        return self.w_2(self.dropout(x).transpose(-1, 1)).transpose(-1, 1)\n\n\nclass Conv1dLinear(torch.nn.Module):\n    """"""Conv1D + Linear for Transformer block.\n\n    A variant of MultiLayeredConv1d, which replaces second conv-layer to linear.\n\n    """"""\n\n    def __init__(self, in_chans, hidden_chans, kernel_size, dropout_rate):\n        """"""Initialize Conv1dLinear module.\n\n        Args:\n            in_chans (int): Number of input channels.\n            hidden_chans (int): Number of hidden channels.\n            kernel_size (int): Kernel size of conv1d.\n            dropout_rate (float): Dropout rate.\n\n        """"""\n        super(Conv1dLinear, self).__init__()\n        self.w_1 = torch.nn.Conv1d(\n            in_chans,\n            hidden_chans,\n            kernel_size,\n            stride=1,\n            padding=(kernel_size - 1) // 2,\n        )\n        self.w_2 = torch.nn.Linear(hidden_chans, in_chans)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        """"""Calculate forward propagation.\n\n        Args:\n            x (Tensor): Batch of input tensors (B, ..., in_chans).\n\n        Returns:\n            Tensor: Batch of output tensors (B, ..., hidden_chans).\n\n        """"""\n        x = torch.relu(self.w_1(x.transpose(-1, 1))).transpose(-1, 1)\n        return self.w_2(self.dropout(x))\n'"
espnet/nets/pytorch_backend/transformer/optimizer.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Optimizer module.""""""\n\nimport torch\n\n\nclass NoamOpt(object):\n    """"""Optim wrapper that implements rate.""""""\n\n    def __init__(self, model_size, factor, warmup, optimizer):\n        """"""Construct an NoamOpt object.""""""\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup\n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n\n    @property\n    def param_groups(self):\n        """"""Return param_groups.""""""\n        return self.optimizer.param_groups\n\n    def step(self):\n        """"""Update parameters and rate.""""""\n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p[""lr""] = rate\n        self._rate = rate\n        self.optimizer.step()\n\n    def rate(self, step=None):\n        """"""Implement `lrate` above.""""""\n        if step is None:\n            step = self._step\n        return (\n            self.factor\n            * self.model_size ** (-0.5)\n            * min(step ** (-0.5), step * self.warmup ** (-1.5))\n        )\n\n    def zero_grad(self):\n        """"""Reset gradient.""""""\n        self.optimizer.zero_grad()\n\n    def state_dict(self):\n        """"""Return state_dict.""""""\n        return {\n            ""_step"": self._step,\n            ""warmup"": self.warmup,\n            ""factor"": self.factor,\n            ""model_size"": self.model_size,\n            ""_rate"": self._rate,\n            ""optimizer"": self.optimizer.state_dict(),\n        }\n\n    def load_state_dict(self, state_dict):\n        """"""Load state_dict.""""""\n        for key, value in state_dict.items():\n            if key == ""optimizer"":\n                self.optimizer.load_state_dict(state_dict[""optimizer""])\n            else:\n                setattr(self, key, value)\n\n\ndef get_std_opt(model, d_model, warmup, factor):\n    """"""Get standard NoamOpt.""""""\n    base = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n    return NoamOpt(d_model, factor, warmup, base)\n'"
espnet/nets/pytorch_backend/transformer/plot.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport logging\n\nimport matplotlib.pyplot as plt\nimport numpy\n\nfrom espnet.asr import asr_utils\n\n\ndef _plot_and_save_attention(att_w, filename, xtokens=None, ytokens=None):\n    # dynamically import matplotlib due to not found error\n    from matplotlib.ticker import MaxNLocator\n    import os\n\n    d = os.path.dirname(filename)\n    if not os.path.exists(d):\n        os.makedirs(d)\n    w, h = plt.figaspect(1.0 / len(att_w))\n    fig = plt.Figure(figsize=(w * 2, h * 2))\n    axes = fig.subplots(1, len(att_w))\n    if len(att_w) == 1:\n        axes = [axes]\n    for ax, aw in zip(axes, att_w):\n        # plt.subplot(1, len(att_w), h)\n        ax.imshow(aw.astype(numpy.float32), aspect=""auto"")\n        ax.set_xlabel(""Input"")\n        ax.set_ylabel(""Output"")\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n        # Labels for major ticks\n        if xtokens is not None:\n            ax.set_xticks(numpy.linspace(0, len(xtokens) - 1, len(xtokens)))\n            ax.set_xticks(numpy.linspace(0, len(xtokens) - 1, 1), minor=True)\n            ax.set_xticklabels(xtokens + [""""], rotation=40)\n        if ytokens is not None:\n            ax.set_yticks(numpy.linspace(0, len(ytokens) - 1, len(ytokens)))\n            ax.set_yticks(numpy.linspace(0, len(ytokens) - 1, 1), minor=True)\n            ax.set_yticklabels(ytokens + [""""])\n    fig.tight_layout()\n    return fig\n\n\ndef savefig(plot, filename):\n    plot.savefig(filename)\n    plt.clf()\n\n\ndef plot_multi_head_attention(\n    data,\n    attn_dict,\n    outdir,\n    suffix=""png"",\n    savefn=savefig,\n    ikey=""input"",\n    iaxis=0,\n    okey=""output"",\n    oaxis=0,\n):\n    """"""Plot multi head attentions.\n\n    :param dict data: utts info from json file\n    :param dict[str, torch.Tensor] attn_dict: multi head attention dict.\n        values should be torch.Tensor (head, input_length, output_length)\n    :param str outdir: dir to save fig\n    :param str suffix: filename suffix including image type (e.g., png)\n    :param savefn: function to save\n\n    """"""\n    for name, att_ws in attn_dict.items():\n        for idx, att_w in enumerate(att_ws):\n            filename = ""%s/%s.%s.%s"" % (outdir, data[idx][0], name, suffix)\n            dec_len = int(data[idx][1][okey][oaxis][""shape""][0])\n            enc_len = int(data[idx][1][ikey][iaxis][""shape""][0])\n            xtokens, ytokens = None, None\n            if ""encoder"" in name:\n                att_w = att_w[:, :enc_len, :enc_len]\n                # for MT\n                if ""token"" in data[idx][1][ikey][iaxis].keys():\n                    xtokens = data[idx][1][ikey][iaxis][""token""].split()\n                    ytokens = xtokens[:]\n            elif ""decoder"" in name:\n                if ""self"" in name:\n                    att_w = att_w[:, : dec_len + 1, : dec_len + 1]  # +1 for <sos>\n                else:\n                    att_w = att_w[:, : dec_len + 1, :enc_len]  # +1 for <sos>\n                    # for MT\n                    if ""token"" in data[idx][1][ikey][iaxis].keys():\n                        xtokens = data[idx][1][ikey][iaxis][""token""].split()\n                # for ASR/ST/MT\n                if ""token"" in data[idx][1][okey][oaxis].keys():\n                    ytokens = [""<sos>""] + data[idx][1][okey][oaxis][""token""].split()\n                    if ""self"" in name:\n                        xtokens = ytokens[:]\n            else:\n                logging.warning(""unknown name for shaping attention"")\n            fig = _plot_and_save_attention(att_w, filename, xtokens, ytokens)\n            savefn(fig, filename)\n\n\nclass PlotAttentionReport(asr_utils.PlotAttentionReport):\n    def plotfn(self, *args, **kwargs):\n        kwargs[""ikey""] = self.ikey\n        kwargs[""iaxis""] = self.iaxis\n        kwargs[""okey""] = self.okey\n        kwargs[""oaxis""] = self.oaxis\n        plot_multi_head_attention(*args, **kwargs)\n\n    def __call__(self, trainer):\n        attn_dict = self.get_attention_weights()\n        suffix = ""ep.{.updater.epoch}.png"".format(trainer)\n        self.plotfn(self.data, attn_dict, self.outdir, suffix, savefig)\n\n    def get_attention_weights(self):\n        batch = self.converter([self.transform(self.data)], self.device)\n        if isinstance(batch, tuple):\n            att_ws = self.att_vis_fn(*batch)\n        elif isinstance(batch, dict):\n            att_ws = self.att_vis_fn(**batch)\n        return att_ws\n\n    def log_attentions(self, logger, step):\n        def log_fig(plot, filename):\n            from os.path import basename\n\n            logger.add_figure(basename(filename), plot, step)\n            plt.clf()\n\n        attn_dict = self.get_attention_weights()\n        self.plotfn(self.data, attn_dict, self.outdir, """", log_fig)\n'"
espnet/nets/pytorch_backend/transformer/positionwise_feed_forward.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Positionwise feed forward layer definition.""""""\n\nimport torch\n\n\nclass PositionwiseFeedForward(torch.nn.Module):\n    """"""Positionwise feed forward layer.\n\n    :param int idim: input dimenstion\n    :param int hidden_units: number of hidden units\n    :param float dropout_rate: dropout rate\n\n    """"""\n\n    def __init__(self, idim, hidden_units, dropout_rate):\n        """"""Construct an PositionwiseFeedForward object.""""""\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = torch.nn.Linear(idim, hidden_units)\n        self.w_2 = torch.nn.Linear(hidden_units, idim)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        """"""Forward funciton.""""""\n        return self.w_2(self.dropout(torch.relu(self.w_1(x))))\n'"
espnet/nets/pytorch_backend/transformer/repeat.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Repeat the same layer definition.""""""\n\nimport torch\n\n\nclass MultiSequential(torch.nn.Sequential):\n    """"""Multi-input multi-output torch.nn.Sequential.""""""\n\n    def forward(self, *args):\n        """"""Repeat.""""""\n        for m in self:\n            args = m(*args)\n        return args\n\n\ndef repeat(N, fn):\n    """"""Repeat module N times.\n\n    :param int N: repeat time\n    :param function fn: function to generate module\n    :return: repeated modules\n    :rtype: MultiSequential\n    """"""\n    return MultiSequential(*[fn() for _ in range(N)])\n'"
espnet/nets/pytorch_backend/transformer/subsampling.py,11,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n# Copyright 2019 Shigeki Karita\n#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\n""""""Subsampling layer definition.""""""\n\nimport torch\n\nfrom espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n\n\nclass Conv2dSubsampling(torch.nn.Module):\n    """"""Convolutional 2D subsampling (to 1/4 length).\n\n    :param int idim: input dim\n    :param int odim: output dim\n    :param flaot dropout_rate: dropout rate\n\n    """"""\n\n    def __init__(self, idim, odim, dropout_rate):\n        """"""Construct an Conv2dSubsampling object.""""""\n        super(Conv2dSubsampling, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, odim, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(odim, odim, 3, 2),\n            torch.nn.ReLU(),\n        )\n        self.out = torch.nn.Sequential(\n            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),\n            PositionalEncoding(odim, dropout_rate),\n        )\n\n    def forward(self, x, x_mask):\n        """"""Subsample x.\n\n        :param torch.Tensor x: input tensor\n        :param torch.Tensor x_mask: input mask\n        :return: subsampled x and mask\n        :rtype Tuple[torch.Tensor, torch.Tensor]\n        """"""\n        x = x.unsqueeze(1)  # (b, c, t, f)\n        x = self.conv(x)\n        b, c, t, f = x.size()\n        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n        if x_mask is None:\n            return x, None\n        return x, x_mask[:, :, :-2:2][:, :, :-2:2]\n'"
test/espnet2/asr/decoder/__init__.py,0,b''
test/espnet2/asr/decoder/test_rnn_decoder.py,6,"b'import pytest\nimport torch\n\nfrom espnet2.asr.decoder.rnn_decoder import RNNDecoder\n\n\n@pytest.mark.parametrize(""context_residual"", [True, False])\n@pytest.mark.parametrize(""rnn_type"", [""lstm"", ""gru""])\ndef test_RNNDecoder_backward(context_residual, rnn_type):\n    decoder = RNNDecoder(10, 12, context_residual=context_residual, rnn_type=rnn_type)\n    x = torch.randn(2, 9, 12)\n    x_lens = torch.tensor([9, 7], dtype=torch.long)\n    t = torch.randint(0, 10, [2, 4], dtype=torch.long)\n    t_lens = torch.tensor([4, 3], dtype=torch.long)\n    z_all, ys_in_lens = decoder(x, x_lens, t, t_lens)\n    z_all.sum().backward()\n\n\n@pytest.mark.parametrize(""context_residual"", [True, False])\n@pytest.mark.parametrize(""rnn_type"", [""lstm"", ""gru""])\ndef test_RNNDecoder_init_state(context_residual, rnn_type):\n    decoder = RNNDecoder(10, 12, context_residual=context_residual, rnn_type=rnn_type)\n    x = torch.randn(9, 12)\n    state = decoder.init_state(x)\n    t = torch.randint(0, 10, [4], dtype=torch.long)\n    decoder.score(t, state, x)\n\n\ndef test_RNNDecoder_invalid_type():\n    with pytest.raises(ValueError):\n        RNNDecoder(10, 12, rnn_type=""foo"")\n'"
test/espnet2/asr/decoder/test_transformer_decoder.py,7,"b'import pytest\nimport torch\n\nfrom espnet2.asr.decoder.transformer_decoder import TransformerDecoder\n\n\n@pytest.mark.parametrize(""input_layer"", [""linear"", ""embed""])\n@pytest.mark.parametrize(""normalize_before"", [True, False])\n@pytest.mark.parametrize(""use_output_layer"", [True, False])\ndef test_TransformerDecoder_backward(input_layer, normalize_before, use_output_layer):\n    decoder = TransformerDecoder(\n        10,\n        12,\n        input_layer=input_layer,\n        normalize_before=normalize_before,\n        use_output_layer=use_output_layer,\n    )\n    x = torch.randn(2, 9, 12)\n    x_lens = torch.tensor([9, 7], dtype=torch.long)\n    if input_layer == ""embed"":\n        t = torch.randint(0, 10, [2, 4], dtype=torch.long)\n    else:\n        t = torch.randn(2, 4, 10)\n    t_lens = torch.tensor([4, 3], dtype=torch.long)\n    z_all, ys_in_lens = decoder(x, x_lens, t, t_lens)\n    z_all.sum().backward()\n\n\ndef test_TransformerDecoder_init_state():\n    decoder = TransformerDecoder(10, 12)\n    x = torch.randn(9, 12)\n    state = decoder.init_state(x)\n    t = torch.randint(0, 10, [4], dtype=torch.long)\n    decoder.score(t, state, x)\n\n\ndef test_TransformerDecoder_invalid_type():\n    with pytest.raises(ValueError):\n        TransformerDecoder(10, 12, input_layer=""foo"")\n'"
test/espnet2/asr/encoder/__init__.py,0,b''
test/espnet2/asr/encoder/test_rnn_encoder.py,2,"b'import pytest\nimport torch\n\nfrom espnet2.asr.encoder.rnn_encoder import RNNEncoder\n\n\n@pytest.mark.parametrize(""rnn_type"", [""lstm"", ""gru""])\n@pytest.mark.parametrize(""bidirectional"", [True, False])\n@pytest.mark.parametrize(""use_projection"", [True, False])\n@pytest.mark.parametrize(""subsample"", [None, (2, 2, 1, 1)])\ndef test_Encoder_forward_backward(rnn_type, bidirectional, use_projection, subsample):\n    encoder = RNNEncoder(\n        5,\n        rnn_type=rnn_type,\n        bidirectional=bidirectional,\n        use_projection=use_projection,\n        subsample=subsample,\n    )\n    x = torch.randn(2, 10, 5, requires_grad=True)\n    x_lens = torch.LongTensor([10, 8])\n    y, _, _ = encoder(x, x_lens)\n    y.sum().backward()\n\n\ndef test_Encoder_output_size():\n    encoder = RNNEncoder(5, output_size=10)\n    assert encoder.output_size() == 10\n\n\ndef test_Encoder_invalid_type():\n    with pytest.raises(ValueError):\n        RNNEncoder(5, rnn_type=""fff"")\n'"
test/espnet2/asr/encoder/test_transformer_encoder.py,4,"b'import pytest\nimport torch\n\nfrom espnet2.asr.encoder.transformer_encoder import TransformerEncoder\n\n\n@pytest.mark.parametrize(""input_layer"", [""linear"", ""conv2d"", ""embed"", None])\n@pytest.mark.parametrize(""positionwise_layer_type"", [""conv1d"", ""conv1d-linear""])\ndef test_Encoder_forward_backward(input_layer, positionwise_layer_type):\n    encoder = TransformerEncoder(\n        20,\n        output_size=40,\n        input_layer=input_layer,\n        positionwise_layer_type=positionwise_layer_type,\n    )\n    if input_layer == ""embed"":\n        x = torch.randint(0, 10, [2, 10])\n    elif input_layer is None:\n        x = torch.randn(2, 10, 40, requires_grad=True)\n    else:\n        x = torch.randn(2, 10, 20, requires_grad=True)\n    x_lens = torch.LongTensor([10, 8])\n    y, _, _ = encoder(x, x_lens)\n    y.sum().backward()\n\n\ndef test_Encoder_output_size():\n    encoder = TransformerEncoder(20, output_size=256)\n    assert encoder.output_size() == 256\n\n\ndef test_Encoder_invalid_type():\n    with pytest.raises(ValueError):\n        TransformerEncoder(20, input_layer=""fff"")\n'"
test/espnet2/asr/encoder/test_vgg_rnn_encoder.py,2,"b'import pytest\nimport torch\n\nfrom espnet2.asr.encoder.vgg_rnn_encoder import VGGRNNEncoder\n\n\n@pytest.mark.parametrize(""rnn_type"", [""lstm"", ""gru""])\n@pytest.mark.parametrize(""bidirectional"", [True, False])\n@pytest.mark.parametrize(""use_projection"", [True, False])\ndef test_Encoder_forward_backward(rnn_type, bidirectional, use_projection):\n    encoder = VGGRNNEncoder(\n        5, rnn_type=rnn_type, bidirectional=bidirectional, use_projection=use_projection\n    )\n    x = torch.randn(2, 10, 5, requires_grad=True)\n    x_lens = torch.LongTensor([10, 8])\n    y, _, _ = encoder(x, x_lens)\n    y.sum().backward()\n\n\ndef test_Encoder_output_size():\n    encoder = VGGRNNEncoder(5, output_size=10)\n    assert encoder.output_size() == 10\n\n\ndef test_Encoder_invalid_type():\n    with pytest.raises(ValueError):\n        VGGRNNEncoder(5, rnn_type=""fff"")\n'"
test/espnet2/asr/frontend/__init__.py,0,b''
test/espnet2/asr/frontend/test_frontend.py,4,"b'import pytest\nimport torch\n\nfrom espnet2.asr.frontend.default import DefaultFrontend\n\n\ndef test_frontend_repr():\n    frontend = DefaultFrontend(fs=""16k"")\n    print(frontend)\n\n\ndef test_frontend_output_size():\n    frontend = DefaultFrontend(fs=""16k"", n_mels=40)\n    assert frontend.output_size() == 40\n\n\ndef test_frontend_backward():\n    frontend = DefaultFrontend(fs=160, n_fft=128, win_length=32, frontend_conf=None)\n    x = torch.randn(2, 300, requires_grad=True)\n    x_lengths = torch.LongTensor([300, 89])\n    y, y_lengths = frontend(x, x_lengths)\n    y.sum().backward()\n\n\n@pytest.mark.parametrize(""use_wpe"", [True, False])\n@pytest.mark.parametrize(""use_beamformer"", [True, False])\n@pytest.mark.parametrize(""train"", [True, False])\ndef test_frontend_backward_multi_channel(train, use_wpe, use_beamformer):\n    frontend = DefaultFrontend(\n        fs=300,\n        n_fft=128,\n        win_length=128,\n        frontend_conf={""use_wpe"": use_wpe, ""use_beamformer"": use_beamformer},\n    )\n    if train:\n        frontend.train()\n    else:\n        frontend.eval()\n    x = torch.randn(2, 1000, 2, requires_grad=True)\n    x_lengths = torch.LongTensor([1000, 980])\n    y, y_lengths = frontend(x, x_lengths)\n    y.sum().backward()\n'"
test/espnet2/asr/specaug/test_specaug.py,1,"b'import pytest\nimport torch\n\nfrom espnet2.asr.specaug.specaug import SpecAug\n\n\n@pytest.mark.parametrize(""apply_time_warp"", [False, True])\n@pytest.mark.parametrize(""apply_freq_mask"", [False, True])\n@pytest.mark.parametrize(""apply_time_mask"", [False, True])\ndef test_SpecAuc(apply_time_warp, apply_freq_mask, apply_time_mask):\n    if not apply_time_warp and not apply_time_mask and not apply_freq_mask:\n        with pytest.raises(ValueError):\n            specaug = SpecAug(\n                apply_time_warp=apply_time_warp,\n                apply_freq_mask=apply_freq_mask,\n                apply_time_mask=apply_time_mask,\n            )\n    else:\n        specaug = SpecAug(\n            apply_time_warp=apply_time_warp,\n            apply_freq_mask=apply_freq_mask,\n            apply_time_mask=apply_time_mask,\n        )\n        x = torch.randn(2, 1000, 80)\n        specaug(x)\n\n\n@pytest.mark.parametrize(""apply_time_warp"", [False, True])\n@pytest.mark.parametrize(""apply_freq_mask"", [False, True])\n@pytest.mark.parametrize(""apply_time_mask"", [False, True])\ndef test_SpecAuc_repr(apply_time_warp, apply_freq_mask, apply_time_mask):\n    if not apply_time_warp and not apply_time_mask and not apply_freq_mask:\n        return\n    specaug = SpecAug(\n        apply_time_warp=apply_time_warp,\n        apply_freq_mask=apply_freq_mask,\n        apply_time_mask=apply_time_mask,\n    )\n    print(specaug)\n'"
test/espnet2/tts/feats_extract/__init__.py,0,b''
test/espnet2/tts/feats_extract/test_log_mel_fbank.py,6,"b'import torch\n\nfrom espnet2.tts.feats_extract.log_mel_fbank import LogMelFbank\n\n\ndef test_forward():\n    layer = LogMelFbank(n_fft=2, n_mels=2)\n    x = torch.randn(2, 4, 9)\n    y, _ = layer(x, torch.LongTensor([4, 3]))\n    assert y.shape == (2, 1, 9, 2)\n\n\ndef test_backward_leaf_in():\n    layer = LogMelFbank(n_fft=2, n_mels=2)\n    x = torch.randn(2, 4, 9, requires_grad=True)\n    y, _ = layer(x, torch.LongTensor([4, 3]))\n    y.sum().backward()\n\n\ndef test_backward_not_leaf_in():\n    layer = LogMelFbank(n_fft=2, n_mels=2)\n    x = torch.randn(2, 4, 9, requires_grad=True)\n    x = x + 2\n    y, _ = layer(x, torch.LongTensor([4, 3]))\n    y.sum().backward()\n\n\ndef test_output_size():\n    layer = LogMelFbank(n_fft=2, n_mels=2, fs=""16k"")\n    print(layer.output_size())\n\n\ndef test_get_parameters():\n    layer = LogMelFbank(n_fft=2, n_mels=2, fs=""16k"")\n    print(layer.get_parameters())\n'"
test/espnet2/tts/feats_extract/test_log_spectrogram.py,6,"b'import torch\n\nfrom espnet2.tts.feats_extract.log_spectrogram import LogSpectrogram\n\n\ndef test_forward():\n    layer = LogSpectrogram(n_fft=2)\n    x = torch.randn(2, 4, 9)\n    y, _ = layer(x, torch.LongTensor([4, 3]))\n    assert y.shape == (2, 1, 9, 2)\n\n\ndef test_backward_leaf_in():\n    layer = LogSpectrogram(n_fft=2)\n    x = torch.randn(2, 4, 9, requires_grad=True)\n    y, _ = layer(x, torch.LongTensor([4, 3]))\n    y.sum().backward()\n\n\ndef test_backward_not_leaf_in():\n    layer = LogSpectrogram(n_fft=2)\n    x = torch.randn(2, 4, 9, requires_grad=True)\n    x = x + 2\n    y, _ = layer(x, torch.LongTensor([4, 3]))\n    y.sum().backward()\n\n\ndef test_output_size():\n    layer = LogSpectrogram(n_fft=2)\n    print(layer.output_size())\n\n\ndef test_get_parameters():\n    layer = LogSpectrogram(n_fft=2)\n    print(layer.get_parameters())\n'"
egs2/TEMPLATE/asr1/pyscripts/audio/format_wav_scp.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport logging\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import Tuple, Optional\n\nimport kaldiio\nimport humanfriendly\nimport numpy as np\nimport resampy\nimport soundfile\nfrom tqdm import tqdm\n\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet2.utils.fileio import read_2column_text, SoundScpWriter\n\n\ndef humanfriendly_or_none(value: str):\n    if value in (""none"", ""None"", ""NONE""):\n        return None\n    return humanfriendly.parse_size(value)\n\n\ndef str2int_tuple(integers: str) -> Optional[Tuple[int, ...]]:\n    """"""\n\n    >>> str2int_tuple(\'3,4,5\')\n    (3, 4, 5)\n\n    """"""\n    assert check_argument_types()\n    if integers.strip() in (""none"", ""None"", ""NONE"", ""null"", ""Null"", ""NULL""):\n        return None\n    return tuple(map(int, integers.strip().split("","")))\n\n\ndef main():\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    logging.basicConfig(level=logging.INFO, format=logfmt)\n    logging.info(get_commandline_args())\n\n    parser = argparse.ArgumentParser(\n        description=\'Create waves list from ""wav.scp""\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""scp"")\n    parser.add_argument(""outdir"")\n    parser.add_argument(\n        ""--name"",\n        default=""wav"",\n        help=""Specify the prefix word of output file name "" \'such as ""wav.scp""\',\n    )\n    parser.add_argument(""--segments"", default=None)\n    parser.add_argument(\n        ""--fs"",\n        type=humanfriendly_or_none,\n        default=None,\n        help=""If the sampling rate specified, "" ""Change the sampling rate."",\n    )\n    parser.add_argument(""--audio-format"", default=""wav"")\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(""--ref-channels"", default=None, type=str2int_tuple)\n    group.add_argument(""--utt2ref-channels"", default=None, type=str)\n    args = parser.parse_args()\n\n    out_num_samples = Path(args.outdir) / f""utt2num_samples""\n\n    if args.ref_channels is not None:\n\n        def utt2ref_channels(x) -> Tuple[int, ...]:\n            return args.ref_channels\n\n    elif args.utt2ref_channels is not None:\n        utt2ref_channels_dict = read_2column_text(args.utt2ref_channels)\n\n        def utt2ref_channels(x, d=utt2ref_channels_dict) -> Tuple[int, ...]:\n            chs_str = d[x]\n            return tuple(map(int, chs_str.split()))\n\n    else:\n        utt2ref_channels = None\n\n    if args.segments is not None:\n        # Note: kaldiio supports only wav-pcm-int16le file.\n        loader = kaldiio.load_scp_sequential(args.scp, segments=args.segments)\n        with SoundScpWriter(\n            args.outdir,\n            Path(args.outdir) / f""{args.name}.scp"",\n            format=args.audio_format,\n        ) as writer, out_num_samples.open(""w"") as fnum_samples:\n            for uttid, (rate, wave) in tqdm(loader):\n                # wave: (Time,) or (Time, Nmic)\n                if wave.ndim == 2 and utt2ref_channels is not None:\n                    wave = wave[:, utt2ref_channels(uttid)]\n\n                if args.fs is not None and args.fs != rate:\n                    # FIXME(kamo): To use sox?\n                    wave = resampy.resample(\n                        wave.astype(np.float64), rate, args.fs, axis=0\n                    )\n                    wave = wave.astype(np.int16)\n                    rate = args.fs\n                writer[uttid] = rate, wave\n                fnum_samples.write(f""{uttid} {len(wave)}\\n"")\n    else:\n        wavdir = Path(args.outdir) / f""data_{args.name}""\n        wavdir.mkdir(parents=True, exist_ok=True)\n        out_wavscp = Path(args.outdir) / f""{args.name}.scp""\n\n        with Path(args.scp).open(""r"") as fscp, out_wavscp.open(\n            ""w""\n        ) as fout, out_num_samples.open(""w"") as fnum_samples:\n            for line in tqdm(fscp):\n                uttid, wavpath = line.strip().split(None, 1)\n\n                if wavpath.endswith(""|""):\n                    # Streaming input e.g. cat a.wav |\n                    with kaldiio.open_like_kaldi(wavpath, ""rb"") as f:\n                        with BytesIO(f.read()) as g:\n                            wave, rate = soundfile.read(g, dtype=np.int16)\n                            if wave.ndim == 2 and utt2ref_channels is not None:\n                                wave = wave[:, utt2ref_channels(uttid)]\n\n                        if args.fs is not None and args.fs != rate:\n                            # FIXME(kamo): To use sox?\n                            wave = resampy.resample(\n                                wave.astype(np.float64), rate, args.fs, axis=0\n                            )\n                            wave = wave.astype(np.int16)\n                            rate = args.fs\n\n                        owavpath = str(wavdir / f""{uttid}.{args.audio_format}"")\n                        soundfile.write(owavpath, wave, rate)\n                        fout.write(f""{uttid} {owavpath}\\n"")\n                else:\n                    wave, rate = soundfile.read(wavpath, dtype=np.int16)\n                    if wave.ndim == 2 and utt2ref_channels is not None:\n                        wave = wave[:, utt2ref_channels(uttid)]\n                        save_asis = False\n\n                    elif Path(wavpath).suffix == ""."" + args.audio_format and (\n                        args.fs is None or args.fs == rate\n                    ):\n                        save_asis = True\n\n                    else:\n                        save_asis = False\n\n                    if save_asis:\n                        # Neither --segments nor --fs are specified and\n                        # the line doesn\'t end with ""|"",\n                        # i.e. not using unix-pipe,\n                        # only in this case,\n                        # just using the original file as is.\n                        fout.write(f""{uttid} {wavpath}\\n"")\n                    else:\n                        if args.fs is not None and args.fs != rate:\n                            # FIXME(kamo): To use sox?\n                            wave = resampy.resample(\n                                wave.astype(np.float64), rate, args.fs, axis=0\n                            )\n                            wave = wave.astype(np.int16)\n                            rate = args.fs\n\n                        owavpath = str(wavdir / f""{uttid}.{args.audio_format}"")\n                        soundfile.write(owavpath, wave, rate)\n                        fout.write(f""{uttid} {owavpath}\\n"")\n                fnum_samples.write(f""{uttid} {len(wave)}\\n"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs2/TEMPLATE/asr1/pyscripts/feats/feat-to-shape.py,0,"b'#!/usr/bin/env python3\nimport argparse\nimport logging\nimport sys\n\nfrom espnet.transform.transformation import Transformation\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_utils import get_commandline_args\nfrom espnet.utils.cli_utils import is_scipy_wav_style\n\n\ndef get_parser():\n    parser = argparse.ArgumentParser(\n        description=""convert feature to its shape"",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(""--verbose"", ""-V"", default=0, type=int, help=""Verbose option"")\n    parser.add_argument(\n        ""--filetype"",\n        type=str,\n        default=""mat"",\n        choices=[""mat"", ""hdf5"", ""sound.hdf5"", ""sound""],\n        help=""Specify the file format for the rspecifier. ""\n        \'""mat"" is the matrix format in kaldi\',\n    )\n    parser.add_argument(\n        ""--preprocess-conf"",\n        type=str,\n        default=None,\n        help=""The configuration file for the pre-processing"",\n    )\n    parser.add_argument(\n        ""rspecifier"", type=str, help=""Read specifier for feats. e.g. ark:some.ark""\n    )\n    parser.add_argument(\n        ""out"",\n        nargs=""?"",\n        type=argparse.FileType(""w""),\n        default=sys.stdout,\n        help=""The output filename. "" ""If omitted, then output to sys.stdout"",\n    )\n    return parser\n\n\ndef main():\n    parser = get_parser()\n    args = parser.parse_args()\n\n    # logging info\n    logfmt = ""%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s""\n    if args.verbose > 0:\n        logging.basicConfig(level=logging.INFO, format=logfmt)\n    else:\n        logging.basicConfig(level=logging.WARN, format=logfmt)\n    logging.info(get_commandline_args())\n\n    if args.preprocess_conf is not None:\n        preprocessing = Transformation(args.preprocess_conf)\n        logging.info(""Apply preprocessing: {}"".format(preprocessing))\n    else:\n        preprocessing = None\n\n    # There are no necessary for matrix without preprocessing,\n    # so change to file_reader_helper to return shape.\n    # This make sense only with filetype=""hdf5"".\n    for utt, mat in file_reader_helper(\n        args.rspecifier, args.filetype, return_shape=preprocessing is None\n    ):\n        if preprocessing is not None:\n            if is_scipy_wav_style(mat):\n                # If data is sound file, then got as Tuple[int, ndarray]\n                rate, mat = mat\n            mat = preprocessing(mat, uttid_list=utt)\n            shape_str = "","".join(map(str, mat.shape))\n        else:\n            if len(mat) == 2 and isinstance(mat[1], tuple):\n                # If data is sound file, Tuple[int, Tuple[int, ...]]\n                rate, mat = mat\n            shape_str = "","".join(map(str, mat))\n        args.out.write(""{} {}\\n"".format(utt, shape_str))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
