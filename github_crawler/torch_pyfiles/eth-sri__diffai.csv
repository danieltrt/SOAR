file_path,api_count,code
__init__.py,0,"b'import sys\nimport os\n\nSCRIPT_DIR = os.path.dirname(os.path.realpath(os.path.join(os.getcwd(), os.path.expanduser(__file__))))\nprint(SCRIPT_DIR)\nsys.path.append(SCRIPT_DIR)\n\n\n'"
__main__.py,23,"b'import future\nimport builtins\nimport past\nimport six\nimport copy\n\nfrom timeit import default_timer as timer\nfrom datetime import datetime\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torch.utils.data import Dataset\nimport decimal\nimport torch.onnx\n\n\nimport inspect\nfrom inspect import getargspec\nimport os\nimport helpers as h\nfrom helpers import Timer\nimport copy\nimport random\n\nfrom components import *\nimport models\n\nimport goals\nimport scheduling\n\nfrom goals import *\nfrom scheduling import *\n\nimport math\n\nimport warnings\nfrom torch.serialization import SourceChangeWarning\n\nPOINT_DOMAINS = [m for m in h.getMethods(goals) if issubclass(m, goals.Point)]\nSYMETRIC_DOMAINS = [goals.Box] + POINT_DOMAINS\n\n\ndatasets.Imagenet12 = None\n\nclass Top(nn.Module):\n    def __init__(self, args, net, ty = Point):\n        super(Top, self).__init__()\n        self.net = net\n        self.ty = ty\n        self.w = args.width\n        self.global_num = 0\n        self.getSpec = getattr(self, args.spec)\n        self.sub_batch_size = args.sub_batch_size\n        self.curve_width = args.curve_width\n        self.regularize = args.regularize\n\n\n        self.speedCount = 0\n        self.speed = 0.0\n\n    def addSpeed(self, s):\n        self.speed = (s + self.speed * self.speedCount) / (self.speedCount + 1)\n        self.speedCount += 1\n\n    def forward(self, x):\n        return self.net(x)\n\n    def clip_norm(self):\n        self.net.clip_norm()\n\n    def boxSpec(self, x, target, **kargs):\n        return [(self.ty.box(x, w = self.w, model=self, target=target, untargeted=True, **kargs).to_dtype(), target)]\n\n    def curveSpec(self, x, target, **kargs):\n        if self.ty.__class__ in SYMETRIC_DOMAINS:\n            return self.boxSpec(x,target, **kargs)\n        \n\n        batch_size = x.size()[0]\n\n        newTargs = [ None for i in range(batch_size) ]\n        newSpecs = [ None for i in range(batch_size) ]\n        bestSpecs = [ None for i in range(batch_size) ]\n\n        for i in range(batch_size):\n            newTarg = target[i]\n            newTargs[i] = newTarg\n            newSpec = x[i]\n\n            best_x = newSpec\n            best_dist = float(""inf"")\n            for j in range(batch_size):\n                potTarg = target[j] \n                potSpec = x[j]\n                if (not newTarg.data.equal(potTarg.data)) or i == j:\n                    continue\n                curr_dist = (newSpec - potSpec).norm(1).item()  # must experiment with the type of norm here\n                if curr_dist <= best_dist:\n                    best_x = potSpec\n\n            newSpecs[i] = newSpec\n            bestSpecs[i] = best_x\n                \n        new_batch_size = self.sub_batch_size\n        batchedTargs = h.chunks(newTargs, new_batch_size)\n        batchedSpecs = h.chunks(newSpecs, new_batch_size)\n        batchedBest = h.chunks(bestSpecs, new_batch_size)\n\n        def batch(t,s,b):\n            t = h.lten(t)\n            s = torch.stack(s)\n            b = torch.stack(b)\n\n            if h.use_cuda:\n                t.cuda()\n                s.cuda()\n                b.cuda()\n\n            m = self.ty.line(s, b, w = self.curve_width, **kargs)\n            return (m , t)\n\n        return [batch(t,s,b) for t,s,b in zip(batchedTargs, batchedSpecs, batchedBest)]\n\n\n    def regLoss(self):\n        if self.regularize is None or self.regularize <= 0.0:\n            return 0\n        reg_loss = 0\n        r = self.net.regularize(2)\n        return self.regularize * r\n        \n    def aiLoss(self, dom, target, **args):\n        r = self(dom)\n        return self.regLoss() +  r.loss(target = target, **args)\n\n    def printNet(self, f):\n        self.net.printNet(f)\n\n        \n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch DiffAI Example\',  formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'--batch-size\', type=int, default=10, metavar=\'N\', help=\'input batch size for training\')\nparser.add_argument(\'--test-first\', type=h.str2bool, nargs=\'?\', const=True, default=True, help=\'test first\')\nparser.add_argument(\'--test-freq\', type=int, default=1, metavar=\'N\', help=\'number of epochs to skip before testing\')\nparser.add_argument(\'--test-batch-size\', type=int, default=10, metavar=\'N\', help=\'input batch size for testing\')\nparser.add_argument(\'--sub-batch-size\', type=int, default=3, metavar=\'N\', help=\'input batch size for curve specs\')\n\nparser.add_argument(\'--custom-schedule\', type=str, default="""", metavar=\'net\', help=\'Learning rate scheduling for lr-multistep.  Defaults to [200,250,300] for CIFAR10 and [15,25] for everything else.\')\n\nparser.add_argument(\'--test\', type=str, default=None, metavar=\'net\', help=\'Saved net to use, in addition to any other nets you specify with -n\')\nparser.add_argument(\'--update-test-net\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=""should update test net"")\n\nparser.add_argument(\'--sgd\',type=h.str2bool, nargs=\'?\', const=True, default=False, help=""use sgd instead of adam"")\nparser.add_argument(\'--onyx\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=""should output onyx"")\nparser.add_argument(\'--save-dot-net\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=""should output in .net"")\nparser.add_argument(\'--update-test-net-name\', type=str, choices = h.getMethodNames(models), default=None, help=""update test net name"")\n\nparser.add_argument(\'--normalize-layer\', type=h.str2bool, nargs=\'?\', const=True, default=True, help=""should include a training set specific normalization layer"")\nparser.add_argument(\'--clip-norm\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=""should clip the normal and use normal decomposition for weights"")\n\nparser.add_argument(\'--epochs\', type=int, default=1000, metavar=\'N\', help=\'number of epochs to train\')\nparser.add_argument(\'--log-freq\', type=int, default=10, metavar=\'N\', help=\'The frequency with which log statistics are printed\')\nparser.add_argument(\'--save-freq\', type=int, default=1, metavar=\'N\', help=\'The frequency with which nets and images are saved, in terms of number of test passes\')\nparser.add_argument(\'--number-save-images\', type=int, default=0, metavar=\'N\', help=\'The number of images to save. Should be smaller than test-size.\')\n\nparser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\', help=\'learning rate\')\nparser.add_argument(\'--lr-multistep\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=\'learning rate multistep scheduling\')\n\nparser.add_argument(\'--threshold\', type=float, default=-0.01, metavar=\'TH\', help=\'threshold for lr schedule\')\nparser.add_argument(\'--patience\', type=int, default=0, metavar=\'PT\', help=\'patience for lr schedule\')\nparser.add_argument(\'--factor\', type=float, default=0.5, metavar=\'R\', help=\'reduction multiplier for lr schedule\')\nparser.add_argument(\'--max-norm\', type=float, default=10000, metavar=\'MN\', help=\'the maximum norm allowed in weight distribution\')\n\n\nparser.add_argument(\'--curve-width\', type=float, default=None, metavar=\'CW\', help=\'the width of the curve spec\')\n\nparser.add_argument(\'--width\', type=float, default=0.01, metavar=\'CW\', help=\'the width of either the line or box\')\nparser.add_argument(\'--spec\', choices = [ x for x in dir(Top) if x[-4:] == ""Spec"" and len(getargspec(getattr(Top, x)).args) == 3]\n                    , default=""boxSpec"", help=\'picks which spec builder function to use for training\')\n\n\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\', help=\'random seed\')\nparser.add_argument(""--use-schedule"", type=h.str2bool, nargs=\'?\', \n                    const=True, default=False,\n                    help=""activate learning rate schedule"")\n\nparser.add_argument(\'-d\', \'--domain\', sub_choices = None, action = h.SubAct\n                    , default=[], help=\'picks which abstract goals to use for training\', required=True)\n\nparser.add_argument(\'-t\', \'--test-domain\', sub_choices = None, action = h.SubAct\n                    , default=[], help=\'picks which abstract goals to use for testing.  Examples include \' + str(goals), required=True)\n\nparser.add_argument(\'-n\', \'--net\', choices = h.getMethodNames(models), action = \'append\'\n                    , default=[], help=\'picks which net to use for training\')  # one net for now\n\nparser.add_argument(\'-D\', \'--dataset\', choices = [n for (n,k) in inspect.getmembers(datasets, inspect.isclass) if issubclass(k, Dataset)]\n                    , default=""MNIST"", help=\'picks which dataset to use.\')\n\nparser.add_argument(\'-o\', \'--out\', default=""out"", help=\'picks the folder to save the outputs\')\nparser.add_argument(\'--dont-write\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=\'dont write anywhere if this flag is on\')\nparser.add_argument(\'--write-first\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=\'write the initial net.  Useful for comparing algorithms, a pain for testing.\')\nparser.add_argument(\'--test-size\', type=int, default=2000, help=\'number of examples to test with\')\n\nparser.add_argument(\'-r\', \'--regularize\', type=float, default=None, help=\'use regularization\')\n\n\nargs = parser.parse_args()\n\nlargest_domain = max([len(h.catStrs(d)) for d in (args.domain)] )\nlargest_test_domain = max([len(h.catStrs(d)) for d in (args.test_domain)] )\n\nargs.log_interval = int(50000 / (args.batch_size * args.log_freq))\n\nh.max_c_for_norm = args.max_norm\n\nif h.use_cuda:\n    torch.cuda.manual_seed(1 + args.seed)    \nelse:\n    torch.manual_seed(args.seed)\n\ntrain_loader = h.loadDataset(args.dataset, args.batch_size, True, False)\ntest_loader = h.loadDataset(args.dataset, args.test_batch_size, False, False)\n\ninput_dims = train_loader.dataset[0][0].size()\nnum_classes = int(max(getattr(train_loader.dataset, \'train_labels\' if args.dataset != ""SVHN"" else \'labels\'))) + 1\n\nprint(""input_dims: "", input_dims)\nprint(""Num classes: "", num_classes)\n\nvargs = vars(args)\n\ntotal_batches_seen = 0\n\ndef train(epoch, models):\n    global total_batches_seen\n\n    for model in models:\n        model.train()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        total_batches_seen += 1\n        time = float(total_batches_seen) / len(train_loader)\n        if h.use_cuda:\n            data, target = data.cuda(), target.cuda()\n\n        for model in models:\n            model.global_num += data.size()[0]\n\n            timer = Timer(""train a sample from "" + model.name + "" with "" + model.ty.name, data.size()[0], False)\n            lossy = 0\n            with timer:\n                for s in model.getSpec(data.to_dtype(),target, time = time):\n                    model.optimizer.zero_grad()\n                    loss = model.aiLoss(*s, time = time, **vargs).mean(dim=0)\n                    lossy += loss.detach().item()\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                    for p in model.parameters():\n                        if p is not None and torch.isnan(p).any():\n                            print(""Such nan in vals"")\n                        if p is not None and p.grad is not None and torch.isnan(p.grad).any():\n                            print(""Such nan in postmagic"")\n                            stdv = 1 / math.sqrt(h.product(p.data.shape))\n                            p.grad = torch.where(torch.isnan(p.grad), torch.normal(mean=h.zeros(p.grad.shape), std=stdv), p.grad) \n\n                    model.optimizer.step()\n\n                    for p in model.parameters():\n                        if p is not None and torch.isnan(p).any():\n                            print(""Such nan in vals after grad"")\n                            stdv = 1 / math.sqrt(h.product(p.data.shape))\n                            p.data = torch.where(torch.isnan(p.data), torch.normal(mean=h.zeros(p.data.shape), std=stdv), p.data) \n                    \n                    if args.clip_norm:\n                        model.clip_norm()\n                    for p in model.parameters():\n                        if p is not None and torch.isnan(p).any():\n                            raise Exception(""Such nan in vals after clip"")\n                    \n            model.addSpeed(timer.getUnitTime())\n\n            if batch_idx % args.log_interval == 0:\n                print((\'Train Epoch {:12} {:\'+ str(largest_domain) +\'}: {:3} [{:7}/{} ({:.0f}%)] \\tAvg sec/ex {:1.8f}\\tLoss: {:.6f}\').format(\n                    model.name,  model.ty.name,\n                    epoch, \n                    batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n                    model.speed,\n                    lossy))\n\n    \nnum_tests = 0                \ndef test(models, epoch, f = None):\n    global num_tests\n    num_tests += 1\n    class MStat:\n        def __init__(self, model):\n            model.eval()\n            self.model = model\n            self.correct = 0\n            class Stat:\n                def __init__(self, d, dnm):\n                    self.domain = d\n                    self.name = dnm\n                    self.width = 0\n                    self.max_eps = None\n                    self.safe = 0\n                    self.proved = 0\n                    self.time = 0\n            self.domains = [ Stat(h.parseValues(d, goals), h.catStrs(d)) for d in args.test_domain ]\n    model_stats = [ MStat(m) for m in models ]\n        \n    num_its = 0\n    saved_data_target = []\n    for data, target in test_loader:\n        if num_its >= args.test_size:\n            break\n\n        if num_tests == 1:\n            saved_data_target += list(zip(list(data), list(target)))\n        \n        num_its += data.size()[0]\n        if h.use_cuda:\n            data, target = data.cuda().to_dtype(), target.cuda()\n\n        for m in model_stats:\n\n            with torch.no_grad():\n                pred = m.model(data).vanillaTensorPart().max(1, keepdim=True)[1] # get the index of the max log-probability\n                m.correct += pred.eq(target.data.view_as(pred)).sum()\n\n            for stat in m.domains:\n                timer = Timer(shouldPrint = False)\n                with timer:\n                    def calcData(data, target):\n                        box = stat.domain.box(data, w = m.model.w, model=m.model, untargeted = True, target=target).to_dtype()\n                        with torch.no_grad():\n                            bs = m.model(box)\n                            org = m.model(data).vanillaTensorPart().max(1,keepdim=True)[1]\n                            stat.width += bs.diameter().sum().item() # sum up batch loss\n                            stat.proved += bs.isSafe(org).sum().item()\n                            stat.safe += bs.isSafe(target).sum().item()\n                            # stat.max_eps += 0 # TODO: calculate max_eps\n\n                    if m.model.net.neuronCount() < 5000 or stat.domain.__class__ in SYMETRIC_DOMAINS:\n                        calcData(data, target)\n                    else:\n                        for d,t in zip(data, target):\n                            calcData(d.unsqueeze(0),t.unsqueeze(0))\n                stat.time += timer.getUnitTime()\n                \n    l = num_its # len(test_loader.dataset)\n    for m in model_stats:\n        if args.lr_multistep:\n            m.model.lrschedule.step()\n\n        pr_corr = float(m.correct) / float(l)\n        if args.use_schedule:\n            m.model.lrschedule.step(1 - pr_corr)\n        \n        h.printBoth((\'Test: {:12} trained with {:\'+ str(largest_domain) +\'} - Avg sec/ex {:1.12f}, Accuracy: {}/{} ({:3.1f}%)\').format(\n            m.model.name, m.model.ty.name,\n            m.model.speed,\n            m.correct, l, 100. * pr_corr), f = f)\n        \n        model_stat_rec = """"\n        for stat in m.domains:\n            pr_safe = stat.safe / l\n            pr_proved = stat.proved / l\n            pr_corr_given_proved = pr_safe / pr_proved if pr_proved > 0 else 0.0\n            h.printBoth((""\\t{:"" + str(largest_test_domain)+""} - Width: {:<36.16f} Pr[Proved]={:<1.3f}  Pr[Corr and Proved]={:<1.3f}  Pr[Corr|Proved]={:<1.3f} {}Time = {:<7.5f}"" ).format(\n                stat.name, \n                stat.width / l, \n                pr_proved, \n                pr_safe, pr_corr_given_proved, \n                ""AvgMaxEps: {:1.10f} "".format(stat.max_eps / l) if stat.max_eps is not None else """",\n                stat.time), f = f)\n            model_stat_rec += ""{}_{:1.3f}_{:1.3f}_{:1.3f}__"".format(stat.name, pr_proved, pr_safe, pr_corr_given_proved)\n        prepedname = m.model.ty.name.replace("" "", ""_"").replace("","", """").replace(""("", ""_"").replace("")"", ""_"").replace(""="", ""_"")\n        net_file = os.path.join(out_dir, m.model.name +""__"" +prepedname + ""_checkpoint_""+str(epoch)+""_with_{:1.3f}"".format(pr_corr))\n\n        h.printBoth(""\\tSaving netfile: {}\\n"".format(net_file + "".pynet""), f = f)\n\n        if (num_tests % args.save_freq == 1 or args.save_freq == 1) and not args.dont_write and (num_tests > 1 or args.write_first):\n            print(""Actually Saving"")\n            torch.save(m.model.net, net_file + "".pynet"")\n            if args.save_dot_net:\n                with h.mopen(args.dont_write, net_file + "".net"", ""w"") as f2:\n                    m.model.net.printNet(f2)\n                    f2.close()\n            if args.onyx:\n                nn = copy.deepcopy(m.model.net)\n                nn.remove_norm()\n                torch.onnx.export(nn, h.zeros([1] + list(input_dims)), net_file + "".onyx"", \n                                  verbose=False, input_names=[""actual_input""] + [""param""+str(i) for i in range(len(list(nn.parameters())))], output_names=[""output""])\n\n\n    if num_tests == 1 and not args.dont_write:\n        img_dir = os.path.join(out_dir, ""images"")\n        if not os.path.exists(img_dir):\n            os.makedirs(img_dir)\n        for img_num,(img,target) in zip(range(args.number_save_images), saved_data_target[:args.number_save_images]):\n            sz = """"\n            for s in img.size():\n                sz += str(s) + ""x""\n            sz = sz[:-1]\n\n            img_file = os.path.join(img_dir, args.dataset + ""_"" + sz + ""_""+ str(img_num))\n            if img_num == 0:\n                print(""Saving image to: "", img_file + "".img"")\n            with open(img_file + "".img"", ""w"") as imgfile:\n                flatimg = img.view(h.product(img.size()))\n                for t in flatimg.cpu():\n                    print(decimal.Decimal(float(t)).__format__(""f""), file=imgfile)\n            with open(img_file + "".class"" , ""w"") as imgfile:\n                print(int(target.item()), file=imgfile)\n\ndef createModel(net, domain, domain_name):\n    net_weights, net_create = net\n    domain.name = domain_name\n\n    net = net_create()\n    m = {}\n    for (k,v) in net_weights.state_dict().items():\n        m[k] = v.to_dtype()\n    net.load_state_dict(m)\n\n    model = Top(args, net, domain)\n    if args.clip_norm:\n        model.clip_norm()\n    if h.use_cuda:\n        model.cuda()\n    if args.sgd:\n        model.optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n    else:\n        model.optimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n    if args.lr_multistep:\n        model.lrschedule = optim.lr_scheduler.MultiStepLR(\n            model.optimizer,\n            gamma = 0.1,\n            milestones = eval(args.custom_schedule) if args.custom_schedule != """" else ([200, 250, 300] if args.dataset == ""CIFAR10"" else [15, 25]))\n    else:\n        model.lrschedule = optim.lr_scheduler.ReduceLROnPlateau(\n            model.optimizer,\n            \'min\',\n            patience=args.patience,\n            threshold= args.threshold,\n            min_lr=0.000001,\n            factor=args.factor,\n            verbose=True)\n\n    net.name = net_create.__name__\n    model.name = net_create.__name__\n\n    return model\n\nout_dir = os.path.join(args.out, args.dataset, str(args.net)[1:-1].replace("", "",""_"").replace(""\'"",""""),\n                       args.spec, ""width_""+str(args.width), h.file_timestamp() )\n\nprint(""Saving to:"", out_dir)\n\nif not os.path.exists(out_dir) and not args.dont_write:\n    os.makedirs(out_dir)\n\nprint(""Starting Training with:"")\nwith h.mopen(args.dont_write, os.path.join(out_dir, ""config.txt""), ""w"") as f:\n    for k in sorted(vars(args)):\n        h.printBoth(""\\t""+k+"": ""+str(getattr(args,k)), f = f)\nprint("""")\n\ndef buildNet(n):\n    n = n(num_classes)\n    if args.normalize_layer:\n        if args.dataset in [""MNIST""]:\n            n = Seq(Normalize([0.1307], [0.3081] ), n)\n        elif args.dataset in [""CIFAR10"", ""CIFAR100""]:\n            n = Seq(Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]), n)\n        elif args.dataset in [""SVHN""]:\n            n = Seq(Normalize([0.5,0.5,0.5], [0.2, 0.2, 0.2]), n)\n        elif args.dataset in [""Imagenet12""]:\n            n = Seq(Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]), n)\n    n = n.infer(input_dims)\n    if args.clip_norm:\n        n.clip_norm()\n    return n\n\nif not args.test is None:\n\n    test_name = None\n\n    def loadedNet():\n        if test_name is not None:\n            n = getattr(models,test_name)\n            n = buildNet(n)\n            if args.clip_norm:\n                n.clip_norm()\n            return n\n        else:\n            with warnings.catch_warnings():\n                warnings.simplefilter(""ignore"", SourceChangeWarning)\n                return torch.load(args.test)\n\n    net = loadedNet().double() if h.dtype == torch.float64 else loadedNet().float()\n    \n\n    if args.update_test_net_name is not None:\n        test_name = args.update_test_net_name\n    elif args.update_test_net and \'__name__\' in dir(net):\n        test_name = net.__name__\n\n    if test_name is not None:\n        loadedNet.__name__ = test_name\n\n    nets = [ (net, loadedNet) ]\n\nelif args.net == []:\n    raise Exception(""Need to specify at least one net with either -n or --test"")\nelse:\n    nets = []\n\nfor n in args.net:\n    m = getattr(models,n)\n    net_create = (lambda m: lambda: buildNet(m))(m) # why doesn\'t python do scoping right?  This is a thunk.  It is bad.\n    net_create.__name__ = n\n    net = buildNet(m)\n    net.__name__ = n\n    nets += [ (net, net_create) ]\n\n    print(""Name: "", net_create.__name__)\n    print(""Number of Neurons (relus): "", net.neuronCount())\n    print(""Number of Parameters: "", sum([h.product(s.size()) for s in net.parameters()]))\n    print(""Depth (relu layers): "", net.depth())\n    print()\n    net.showNet()\n    print()\n\n\nif args.domain == []:\n    models = [ createModel(net, goals.Box(args.width), ""Box"") for net in nets]\nelse:\n    models = h.flat([[createModel(net, h.parseValues(d, goals, scheduling), h.catStrs(d)) for net in nets] for d in args.domain])\n\n\nwith h.mopen(args.dont_write, os.path.join(out_dir, ""log.txt""), ""w"") as f:\n    startTime = timer()\n    for epoch in range(1, args.epochs + 1):\n        if f is not None:\n            f.flush()\n        if (epoch - 1) % args.test_freq == 0 and (epoch > 1 or args.test_first):\n            with Timer(""test all models before epoch ""+str(epoch), 1):\n                test(models, epoch, f)\n                if f is not None:\n                    f.flush()\n        h.printBoth(""Elapsed-Time: {:.2f}s\\n"".format(timer() - startTime), f = f)\n        if args.epochs <= args.test_freq:\n            break\n        with Timer(""train all models in epoch"", 1, f = f):\n            train(epoch, models)\n'"
ai.py,49,"b'import future\nimport builtins\nimport past\nimport six\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd\n\nfrom functools import reduce\n\ntry:\n    from . import helpers as h\nexcept:\n    import helpers as h\n\n\n\ndef catNonNullErrors(op, ref_errs=None): # the way of things is ugly\n    def doop(er1, er2):\n        erS, erL = (er1, er2)\n        sS, sL = (erS.size()[0], erL.size()[0])\n\n        if sS == sL: # TODO: here we know we used transformers on either side which didnt introduce new error terms (this is a hack for hybrid zonotopes and doesn\'t work with adaptive error term adding).\n            return op(erS,erL)\n\n        if ref_errs is not None:\n            sz = ref_errs.size()[0]\n        else:\n            sz = min(sS, sL)\n    \n        p1 = op(erS[:sz], erL[:sz])\n        erSrem = erS[sz:]\n        erLrem = erS[sz:]\n        p2 = op(erSrem, h.zeros(erSrem.shape))\n        p3 = op(h.zeros(erLrem.shape), erLrem)\n        return torch.cat((p1,p2,p3), dim=0)\n    return doop\n\ndef creluBoxy(dom):\n    if dom.errors is None:\n        if dom.beta is None:\n            return dom.new(F.relu(dom.head), None, None)\n        er = dom.beta \n        mx = F.relu(dom.head + er)\n        mn = F.relu(dom.head - er)\n        return dom.new((mn + mx) / 2, (mx - mn) / 2 , None)\n\n    aber = torch.abs(dom.errors)\n\n    sm = torch.sum(aber, 0) \n\n    if not dom.beta is None:\n        sm += dom.beta\n\n    mx = dom.head + sm\n    mn = dom.head - sm\n\n    should_box = mn.lt(0) * mx.gt(0)\n    gtz = dom.head.gt(0).to_dtype()\n    mx /= 2\n    newhead = h.ifThenElse(should_box, mx, gtz * dom.head)\n    newbeta = h.ifThenElse(should_box, mx, gtz * (dom.beta if not dom.beta is None else 0))\n    newerr = (1 - should_box.to_dtype()) * gtz * dom.errors\n\n    return dom.new(newhead, newbeta , newerr)\n\n\ndef creluBoxySound(dom):\n    if dom.errors is None:\n        if dom.beta is None:\n            return dom.new(F.relu(dom.head), None, None)\n        er = dom.beta \n        mx = F.relu(dom.head + er)\n        mn = F.relu(dom.head - er)\n        return dom.new((mn + mx) / 2, (mx - mn) / 2 + 2e-6 , None)\n\n    aber = torch.abs(dom.errors)\n\n    sm = torch.sum(aber, 0) \n\n    if not dom.beta is None:\n        sm += dom.beta\n\n    mx = dom.head + sm\n    mn = dom.head - sm\n\n    should_box = mn.lt(0) * mx.gt(0)\n    gtz = dom.head.gt(0).to_dtype()\n    mx /= 2\n    newhead = h.ifThenElse(should_box, mx, gtz * dom.head)\n    newbeta = h.ifThenElse(should_box, mx + 2e-6, gtz * (dom.beta if not dom.beta is None else 0))\n    newerr = (1 - should_box.to_dtype()) * gtz * dom.errors\n\n    return dom.new(newhead, newbeta, newerr)\n\n\ndef creluSwitch(dom):\n    if dom.errors is None:\n        if dom.beta is None:\n            return dom.new(F.relu(dom.head), None, None)\n        er = dom.beta \n        mx = F.relu(dom.head + er)\n        mn = F.relu(dom.head - er)\n        return dom.new((mn + mx) / 2, (mx - mn) / 2 , None)\n\n    aber = torch.abs(dom.errors)\n\n    sm = torch.sum(aber, 0) \n\n    if not dom.beta is None:\n        sm += dom.beta\n\n    mn = dom.head - sm\n    mx = sm\n    mx += dom.head\n\n    should_box = mn.lt(0) * mx.gt(0)\n    gtz = dom.head.gt(0)\n\n    mn.neg_()\n    should_boxer = mn.gt(mx)\n\n    mn /= 2\n    newhead = h.ifThenElse(should_box, h.ifThenElse(should_boxer, mx / 2, dom.head + mn ), gtz.to_dtype() * dom.head)\n    zbet =  dom.beta if not dom.beta is None else 0\n    newbeta = h.ifThenElse(should_box, h.ifThenElse(should_boxer, mx / 2, mn + zbet), gtz.to_dtype() * zbet)\n    newerr  = h.ifThenElseL(should_box, 1 - should_boxer, gtz).to_dtype() * dom.errors\n\n    return dom.new(newhead, newbeta , newerr)\n\ndef creluSmooth(dom):\n    if dom.errors is None:\n        if dom.beta is None:\n            return dom.new(F.relu(dom.head), None, None)\n        er = dom.beta \n        mx = F.relu(dom.head + er)\n        mn = F.relu(dom.head - er)\n        return dom.new((mn + mx) / 2, (mx - mn) / 2 , None)\n\n    aber = torch.abs(dom.errors)\n\n    sm = torch.sum(aber, 0) \n\n    if not dom.beta is None:\n        sm += dom.beta\n\n    mn = dom.head - sm\n    mx = sm\n    mx += dom.head\n\n\n    nmn = F.relu(-1 * mn)\n\n    zbet =  (dom.beta if not dom.beta is None else 0)\n    newheadS = dom.head + nmn / 2\n    newbetaS = zbet + nmn / 2\n    newerrS = dom.errors\n\n    mmx = F.relu(mx)\n\n    newheadB = mmx / 2\n    newbetaB = newheadB\n    newerrB = 0\n\n    eps = 0.0001\n    t = nmn / (mmx + nmn + eps) # mn.lt(0).to_dtype() * F.sigmoid(nmn - nmx)\n\n    shouldnt_zero = mx.gt(0).to_dtype()\n\n    newhead = shouldnt_zero * ( (1 - t) * newheadS + t * newheadB)\n    newbeta = shouldnt_zero * ( (1 - t) * newbetaS + t * newbetaB)\n    newerr =  shouldnt_zero * ( (1 - t) * newerrS  + t * newerrB)\n\n    return dom.new(newhead, newbeta , newerr)\n\n\ndef creluNIPS(dom):\n    if dom.errors is None:\n        if dom.beta is None:\n            return dom.new(F.relu(dom.head), None, None)\n        er = dom.beta \n        mx = F.relu(dom.head + er)\n        mn = F.relu(dom.head - er)\n        return dom.new((mn + mx) / 2, (mx - mn) / 2 , None)\n    \n    sm = torch.sum(torch.abs(dom.errors), 0) \n\n    if not dom.beta is None:\n        sm += dom.beta\n\n    mn = dom.head - sm\n    mx =  dom.head + sm\n\n    mngz = mn >= 0.0\n\n    zs = h.zeros(dom.head.shape)\n\n    diff = mx - mn\n\n    lam = torch.where((mx > 0) & (diff > 0.0), mx / diff, zs)\n    mu = lam * mn * (-0.5)\n\n    betaz = zs if dom.beta is None else dom.beta \n\n    newhead = torch.where(mngz, dom.head , lam * dom.head + mu)\n    mngz += diff <= 0.0\n    newbeta = torch.where(mngz, betaz    , lam * betaz + mu ) # mu is always positive on this side\n    newerr = torch.where(mngz, dom.errors, lam * dom.errors )\n    return dom.new(newhead, newbeta, newerr)\n\n\n\n\nclass MaxTypes:\n\n    @staticmethod\n    def ub(x):\n        return x.ub()\n\n    @staticmethod\n    def only_beta(x):\n        return x.beta if x.beta is not None else x.head * 0\n\n    @staticmethod\n    def head_beta(x):\n        return MaxTypes.only_beta(x) + x.head\n\nclass HybridZonotope:\n\n    def isSafe(self, target):\n        od,_ = torch.min(h.preDomRes(self,target).lb(), 1)\n        return od.gt(0.0).long()\n\n    def isPoint(self):\n        return False\n\n    def labels(self):\n        target = torch.max(self.ub(), 1)[1]\n        l = list(h.preDomRes(self,target).lb()[0])\n        return [target.item()] + [ i for i,v in zip(range(len(l)), l) if v <= 0]\n\n    def relu(self):\n        return self.customRelu(self)\n    \n    def __init__(self, head, beta, errors, customRelu = creluBoxy, **kargs):\n        self.head = head\n        self.errors = errors\n        self.beta = beta\n        self.customRelu = creluBoxy if customRelu is None else customRelu\n\n    def new(self, *args, customRelu = None, **kargs):\n        return self.__class__(*args, **kargs, customRelu = self.customRelu if customRelu is None else customRelu).checkSizes()\n\n    def zono_to_hybrid(self, *args, **kargs): # we are already a hybrid zono.\n        return self.new(self.head, self.beta, self.errors, **kargs)\n\n    def hybrid_to_zono(self, *args, correlate=True, customRelu = None, **kargs):\n        beta = self.beta\n        errors = self.errors\n        if correlate and beta is not None:\n            batches = beta.shape[0]\n            num_elem = h.product(beta.shape[1:])\n            ei = h.getEi(batches, num_elem)\n        \n            if len(beta.shape) > 2:\n                ei = ei.contiguous().view(num_elem, *beta.shape)\n            err = ei * beta\n            errors = torch.cat((err, errors), dim=0) if errors is not None else err\n            beta = None\n\n        return Zonotope(self.head, beta, errors if errors is not None else (self.beta * 0).unsqueeze(0) , customRelu = self.customRelu if customRelu is None else None)\n\n\n\n    def abstractApplyLeaf(self, foo, *args, **kargs):\n        return getattr(self, foo)(*args, **kargs)\n\n    def decorrelate(self, cc_indx_batch_err): # keep these errors\n        if self.errors is None:\n            return self\n\n        batch_size = self.head.shape[0]\n        num_error_terms = self.errors.shape[0]\n\n        \n\n        beta = h.zeros(self.head.shape).to_dtype() if self.beta is None  else self.beta\n        errors = h.zeros([0] + list(self.head.shape)).to_dtype() if self.errors is None else self.errors\n\n        inds_i = torch.arange(self.head.shape[0], device=h.device).unsqueeze(1).long()\n        errors = errors.to_dtype().permute(1,0, *list(range(len(self.errors.shape)))[2:])\n        \n        sm = errors.clone()\n        sm[inds_i, cc_indx_batch_err] = 0\n        \n        beta = beta.to_dtype() + sm.abs().sum(dim=1)\n\n        errors = errors[inds_i, cc_indx_batch_err]\n        errors = errors.permute(1,0, *list(range(len(self.errors.shape)))[2:]).contiguous()\n        return self.new(self.head, beta, errors)\n    \n    def dummyDecorrelate(self, num_decorrelate):\n        if num_decorrelate == 0 or self.errors is None:\n            return self\n        elif num_decorrelate >= self.errors.shape[0]:\n            beta = self.beta\n            if self.errors is not None:\n                errs = self.errors.abs().sum(dim=0)\n                if beta is None:\n                    beta = errs\n                else:\n                    beta += errs\n            return self.new(self.head, beta, None)\n        return None\n\n    def stochasticDecorrelate(self, num_decorrelate, choices = None, num_to_keep=False):\n        dummy = self.dummyDecorrelate(num_decorrelate)\n        if dummy is not None:\n            return dummy\n        num_error_terms = self.errors.shape[0]\n        batch_size = self.head.shape[0]\n\n        ucc_mask = h.ones([batch_size, self.errors.shape[0]]).long()\n        cc_indx_batch_err = h.cudify(torch.multinomial(ucc_mask.to_dtype(), num_decorrelate if num_to_keep else num_error_terms - num_decorrelate, replacement=False)) if choices is None else choices\n        return self.decorrelate(cc_indx_batch_err)\n\n    def decorrelateMin(self, num_decorrelate, num_to_keep=False):\n        dummy = self.dummyDecorrelate(num_decorrelate)\n        if dummy is not None:\n            return dummy\n\n        num_error_terms = self.errors.shape[0]\n        batch_size = self.head.shape[0]\n\n        error_sum_b_e = self.errors.abs().view(self.errors.shape[0], batch_size, -1).sum(dim=2).permute(1,0)\n        cc_indx_batch_err = error_sum_b_e.topk(num_decorrelate if num_to_keep else num_error_terms - num_decorrelate)[1]\n        return self.decorrelate(cc_indx_batch_err)\n      \n    def correlate(self, cc_indx_batch_beta): # given in terms of the flattened matrix.\n        num_correlate = h.product(cc_indx_batch_beta.shape[1:])\n        \n        beta = h.zeros(self.head.shape).to_dtype() if self.beta is None  else self.beta\n        errors = h.zeros([0] + list(self.head.shape)).to_dtype() if self.errors is None else self.errors\n\n        batch_size = beta.shape[0]\n        new_errors = h.zeros([num_correlate] + list(self.head.shape)).to_dtype()\n        \n        inds_i = torch.arange(batch_size, device=h.device).unsqueeze(1).long()\n\n        nc = torch.arange(num_correlate, device=h.device).unsqueeze(1).long()\n\n        new_errors = new_errors.permute(1,0, *list(range(len(new_errors.shape)))[2:]).contiguous().view(batch_size, num_correlate, -1)\n        new_errors[inds_i, nc.unsqueeze(0).expand([batch_size]+list(nc.shape)).squeeze(2), cc_indx_batch_beta] = beta.view(batch_size,-1)[inds_i, cc_indx_batch_beta]\n\n        new_errors = new_errors.permute(1,0, *list(range(len(new_errors.shape)))[2:]).contiguous().view(num_correlate, batch_size, *beta.shape[1:])\n        errors = torch.cat((errors, new_errors), dim=0)\n            \n        beta.view(batch_size, -1)[inds_i, cc_indx_batch_beta] = 0\n        \n        return self.new(self.head, beta, errors)\n\n    def stochasticCorrelate(self, num_correlate, choices = None):\n        if num_correlate == 0:\n            return self\n\n        domshape = self.head.shape\n        batch_size = domshape[0]\n        num_pixs = h.product(domshape[1:])\n        num_correlate = min(num_correlate, num_pixs)\n        ucc_mask = h.ones([batch_size, num_pixs ]).long()\n\n        cc_indx_batch_beta = h.cudify(torch.multinomial(ucc_mask.to_dtype(), num_correlate, replacement=False)) if choices is None else choices\n        return self.correlate(cc_indx_batch_beta)\n\n\n    def correlateMaxK(self, num_correlate):\n        if num_correlate == 0:\n            return self\n        \n        domshape = self.head.shape\n        batch_size = domshape[0]\n        num_pixs = h.product(domshape[1:])\n        num_correlate = min(num_correlate, num_pixs)\n\n        concrete_max_image = self.ub().view(batch_size, -1)\n\n        cc_indx_batch_beta = concrete_max_image.topk(num_correlate)[1]\n        return self.correlate(cc_indx_batch_beta)\n\n    def correlateMaxPool(self, *args, max_type = MaxTypes.ub , max_pool = F.max_pool2d, **kargs):\n        domshape = self.head.shape\n        batch_size = domshape[0]\n        num_pixs = h.product(domshape[1:])\n\n        concrete_max_image = max_type(self)\n\n        cc_indx_batch_beta = max_pool(concrete_max_image, *args, return_indices=True, **kargs)[1].view(batch_size, -1)\n\n        return self.correlate(cc_indx_batch_beta)\n\n    def checkSizes(self):\n        if not self.errors is None:\n            if not self.errors.size()[1:] == self.head.size():\n                raise Exception(""Such bad sizes on error:"", self.errors.shape, "" head:"", self.head.shape)\n            if torch.isnan(self.errors).any():\n                raise Exception(""Such nan in errors"")\n        if not self.beta is None:\n            if not self.beta.size() == self.head.size():\n                raise Exception(""Such bad sizes on beta"")\n\n            if torch.isnan(self.beta).any():\n                raise Exception(""Such nan in errors"")\n            if self.beta.lt(0.0).any():\n                self.beta = self.beta.abs()\n            \n        return self\n\n    def __mul__(self, flt):\n        return self.new(self.head * flt, None if self.beta is None else self.beta * abs(flt), None if self.errors is None else self.errors * flt)\n    \n    def __truediv__(self, flt):\n        flt = 1. / flt\n        return self.new(self.head * flt, None if self.beta is None else self.beta * abs(flt), None if self.errors is None else self.errors * flt)\n\n    def __add__(self, other):\n        if isinstance(other, HybridZonotope):\n            return self.new(self.head + other.head, h.msum(self.beta, other.beta, lambda a,b: a + b), h.msum(self.errors, other.errors, catNonNullErrors(lambda a,b: a + b)))\n        else:\n            # other has to be a standard variable or tensor\n            return self.new(self.head + other, self.beta, self.errors)\n\n    def addPar(self, a, b):\n        return self.new(a.head + b.head, h.msum(a.beta, b.beta, lambda a,b: a + b), h.msum(a.errors, b.errors, catNonNullErrors(lambda a,b: a + b, self.errors)))\n\n    def __sub__(self, other):\n        if isinstance(other, HybridZonotope):\n            return self.new(self.head - other.head\n                            , h.msum(self.beta, other.beta, lambda a,b: a + b)\n                            , h.msum(self.errors, None if other.errors is None else -other.errors, catNonNullErrors(lambda a,b: a + b)))\n        else:\n            # other has to be a standard variable or tensor\n            return self.new(self.head - other, self.beta, self.errors)\n\n    def bmm(self, other):\n        hd = self.head.bmm(other)\n        bet = None if self.beta is None else self.beta.bmm(other.abs())\n\n        if self.errors is None:\n            er = None\n        else:\n            er = self.errors.matmul(other)\n        return self.new(hd, bet, er)\n\n\n    def getBeta(self):\n        return self.head * 0 if self.beta is None else self.beta\n\n    def getErrors(self):\n        return (self.head * 0).unsqueeze(0) if self.beta is None else self.errors\n\n    def merge(self, other, ref = None): # the vast majority of the time ref should be none here.  Not for parallel computation with powerset\n        s_beta = self.getBeta() # so that beta is never none\n\n        sbox_u = self.head + s_beta\n        sbox_l = self.head - s_beta\n        o_u = other.ub()\n        o_l = other.lb()\n        o_in_s = (o_u <= sbox_u) & (o_l >= sbox_l)\n\n        s_err_mx = self.errors.abs().sum(dim=0)\n\n        if not isinstance(other, HybridZonotope):\n            new_head = (self.head + other.center()) / 2\n            new_beta = torch.max(sbox_u + s_err_mx,o_u) - new_head\n            return self.new(torch.where(o_in_s, self.head, new_head), torch.where(o_in_s, self.beta,new_beta), o_in_s.float() * self.errors)\n        \n        # TODO: could be more efficient if one of these doesn\'t have beta or errors but thats okay for now.\n        s_u = sbox_u + s_err_mx\n        s_l = sbox_l - s_err_mx\n\n        obox_u = o_u - other.head\n        obox_l = o_l + other.head\n\n        s_in_o = (s_u <= obox_u) & (s_l >= obox_l)\n        \n        # TODO: could theoretically still do something better when one is contained partially in the other\n        new_head = (self.head + other.center()) / 2\n        new_beta = torch.max(sbox_u + self.getErrors().abs().sum(dim=0),o_u) - new_head\n\n        return self.new(torch.where(o_in_s, self.head, torch.where(s_in_o, other.head, new_head))\n                        , torch.where(o_in_s, s_beta,torch.where(s_in_o, other.getBeta(), new_beta))\n                        , h.msum(o_in_s.float() * self.errors, s_in_o.float() * other.errors, catNonNullErrors(lambda a,b: a + b, ref_errs = ref.errors if ref is not None else ref))) # these are both zero otherwise\n    \n\n    def conv(self, conv, weight, bias = None, **kargs):\n        h = self.errors\n        inter = h if h is None else h.view(-1, *h.size()[2:])\n        hd = conv(self.head, weight, bias=bias, **kargs)\n        res = h if h is None else conv(inter, weight, bias=None, **kargs)\n\n        return self.new( hd\n                       , None if self.beta is None else conv(self.beta, weight.abs(), bias = None, **kargs)\n                       , h if h is None else res.view(h.size()[0], h.size()[1], *res.size()[1:]))\n            \n\n    def conv1d(self, *args, **kargs):\n        return self.conv(lambda x, *args, **kargs: x.conv1d(*args,**kargs), *args, **kargs)\n                   \n    def conv2d(self, *args, **kargs):\n        return self.conv(lambda x, *args, **kargs: x.conv2d(*args,**kargs), *args, **kargs)                   \n\n    def conv3d(self, *args, **kargs):\n        return self.conv(lambda x, *args, **kargs: x.conv3d(*args,**kargs), *args, **kargs)\n\n    def conv_transpose1d(self, *args, **kargs):\n        return self.conv(lambda x, *args, **kargs: x.conv_transpose1d(*args,**kargs), *args, **kargs)\n                   \n    def conv_transpose2d(self, *args, **kargs):\n        return self.conv(lambda x, *args, **kargs: x.conv_transpose2d(*args,**kargs), *args, **kargs)                   \n\n    def conv_transpose3d(self, *args, **kargs):\n        return self.conv(lambda x, *args, **kargs: x.conv_transpose3d(*args,**kargs), *args, **kargs)\n        \n    def matmul(self, other):\n        return self.new(self.head.matmul(other), None if self.beta is None else self.beta.matmul(other.abs()), None if self.errors is None else self.errors.matmul(other))\n\n    def unsqueeze(self, i):\n        return self.new(self.head.unsqueeze(i), None if self.beta is None else self.beta.unsqueeze(i), None if self.errors is None else self.errors.unsqueeze(i + 1))\n\n    def squeeze(self, dim):\n        return self.new(self.head.squeeze(dim),\n                        None if self.beta is None else self.beta.squeeze(dim),\n                        None if self.errors is None else self.errors.squeeze(dim + 1 if dim >= 0 else dim))    \n\n    def double(self):\n        return self.new(self.head.double(), self.beta.double()  if self.beta is not None else None, self.errors.double() if self.errors is not None else None) \n\n    def float(self):\n        return self.new(self.head.float(), self.beta.float()  if self.beta is not None else None, self.errors.float() if self.errors is not None else None) \n\n    def to_dtype(self):\n        return self.new(self.head.to_dtype(), self.beta.to_dtype()  if self.beta is not None else None, self.errors.to_dtype() if self.errors is not None else None) \n    \n    def sum(self, dim=1):\n        return self.new(torch.sum(self.head,dim=dim), None if self.beta is None else torch.sum(self.beta,dim=dim), None if self.errors is None else torch.sum(self.errors, dim= dim + 1 if dim >= 0 else dim))\n\n    def view(self,*newshape):\n        return self.new(self.head.view(*newshape), \n                        None if self.beta is None else self.beta.view(*newshape),\n                        None if self.errors is None else self.errors.view(self.errors.size()[0], *newshape))\n\n    def gather(self,dim, index):\n        return self.new(self.head.gather(dim, index), \n                        None if self.beta is None else self.beta.gather(dim, index),\n                        None if self.errors is None else self.errors.gather(dim + 1, index.expand([self.errors.size()[0]] + list(index.size()))))\n    \n    def concretize(self):\n        if self.errors is None:\n            return self\n\n        return self.new(self.head, torch.sum(self.concreteErrors().abs(),0), None) # maybe make a box?\n    \n    def cat(self,other, dim=0):\n        return self.new(self.head.cat(other.head, dim = dim), \n                        h.msum(other.beta, self.beta, lambda a,b: a.cat(b, dim = dim)),\n                        h.msum(self.errors, other.errors, catNonNullErrors(lambda a,b: a.cat(b, dim+1))))\n\n\n    def split(self, split_size, dim = 0):\n        heads = list(self.head.split(split_size, dim))\n        betas = list(self.beta.split(split_size, dim)) if not self.beta is None else None\n        errorss = list(self.errors.split(split_size, dim + 1)) if not self.errors is None else None\n        \n        def makeFromI(i):\n            return self.new( heads[i], \n                             None if betas is None else betas[i], \n                             None if errorss is None else errorss[i])\n        return tuple(makeFromI(i) for i in range(len(heads)))\n\n        \n    \n    def concreteErrors(self):\n        if self.beta is None and self.errors is None:\n            raise Exception(""shouldn\'t have both beta and errors be none"")\n        if self.errors is None:\n            return self.beta.unsqueeze(0)\n        if self.beta is None:\n            return self.errors\n        return torch.cat([self.beta.unsqueeze(0),self.errors], dim=0)\n\n\n    def applyMonotone(self, foo, *args, **kargs):\n        if self.beta is None and self.errors is None:\n            return self.new(foo(self.head), None , None)\n\n        beta = self.concreteErrors().abs().sum(dim=0)\n\n        tp = foo(self.head + beta, *args, **kargs)\n        bt = foo(self.head - beta, *args, **kargs)\n\n        new_hybrid = self.new((tp + bt) / 2, (tp - bt) / 2 , None)\n\n\n        if self.errors is not None:\n            return new_hybrid.correlateMaxK(self.errors.shape[0])\n        return new_hybrid\n\n    def avg_pool2d(self, *args, **kargs):\n        nhead = F.avg_pool2d(self.head, *args, **kargs)\n        return self.new(nhead, \n                        None if self.beta is None else F.avg_pool2d(self.beta, *args, **kargs), \n                        None if self.errors is None else F.avg_pool2d(self.errors.view(-1, *self.head.shape[1:]), *args, **kargs).view(-1,*nhead.shape)) \n\n    def adaptive_avg_pool2d(self, *args, **kargs):\n        nhead = F.adaptive_avg_pool2d(self.head, *args, **kargs)\n        return self.new(nhead, \n                        None if self.beta is None else F.adaptive_avg_pool2d(self.beta, *args, **kargs), \n                        None if self.errors is None else F.adaptive_avg_pool2d(self.errors.view(-1, *self.head.shape[1:]), *args, **kargs).view(-1,*nhead.shape)) \n\n    def elu(self):\n        return self.applyMonotone(F.elu)\n\n    def selu(self):\n        return self.applyMonotone(F.selu)\n\n    def sigm(self):\n        return self.applyMonotone(F.sigmoid)\n\n    def softplus(self):\n        if self.errors is None:\n            if self.beta is None:\n                return self.new(F.softplus(self.head), None , None)\n            tp = F.softplus(self.head + self.beta)\n            bt = F.softplus(self.head - self.beta)\n            return self.new((tp + bt) / 2, (tp - bt) / 2 , None)\n\n        errors = self.concreteErrors()\n        o = h.ones(self.head.size())\n\n        def sp(hd):\n            return F.softplus(hd) # torch.log(o + torch.exp(hd))  # not very stable\n        def spp(hd):\n            ehd = torch.exp(hd)\n            return ehd.div(ehd + o)\n        def sppp(hd):\n            ehd = torch.exp(hd)\n            md = ehd + o\n            return ehd.div(md.mul(md))\n\n        fa = sp(self.head)\n        fpa = spp(self.head)\n\n        a = self.head\n\n        k = torch.sum(errors.abs(), 0) \n\n        def evalG(r):\n            return r.mul(r).mul(sppp(a + r))\n\n        m = torch.max(evalG(h.zeros(k.size())), torch.max(evalG(k), evalG(-k)))\n        m = h.ifThenElse( a.abs().lt(k), torch.max(m, torch.max(evalG(a), evalG(-a))), m)\n        m /= 2\n        \n        return self.new(fa, m if self.beta is None else m + self.beta.mul(fpa), None if self.errors is None else self.errors.mul(fpa))\n\n    def center(self):\n        return self.head\n\n    def vanillaTensorPart(self):\n        return self.head\n\n    def lb(self):\n        return self.head - self.concreteErrors().abs().sum(dim=0)\n\n    def ub(self):\n        return self.head + self.concreteErrors().abs().sum(dim=0)\n\n    def size(self):\n        return self.head.size()\n\n    def diameter(self):\n        abal = torch.abs(self.concreteErrors()).transpose(0,1)\n        return abal.sum(1).sum(1) # perimeter\n\n    def loss(self, target, **args):\n        r = -h.preDomRes(self, target).lb()\n        return F.softplus(r.max(1)[0])\n\n    def deep_loss(self, act = F.relu, *args, **kargs):\n        batch_size = self.head.shape[0]\n        inds = torch.arange(batch_size, device=h.device).unsqueeze(1).long()\n\n        def dl(l,u):\n            ls, lsi = torch.sort(l, dim=1)\n            ls_u = u[inds, lsi]\n\n            def slidingMax(a): # using maxpool\n                k = a.shape[1]\n                ml = a.min(dim=1)[0].unsqueeze(1)\n\n                inp = torch.cat((h.zeros([batch_size, k]), a - ml), dim=1)\n                mpl = F.max_pool1d(inp.unsqueeze(1) ,  kernel_size = k, stride=1, padding = 0, return_indices=False).squeeze(1)\n                return mpl[:,:-1] + ml\n            \n            return act(slidingMax(ls_u) - ls).sum(dim=1)\n\n        l = self.lb().view(batch_size, -1)\n        u = self.ub().view(batch_size, -1)\n        return ( dl(l,u) + dl(-u,-l) ) / (2 * l.shape[1]) # make it easier to regularize against\n\n\n\nclass Zonotope(HybridZonotope):\n    def applySuper(self, ret):\n        batches = ret.head.size()[0]\n        num_elem = h.product(ret.head.size()[1:])\n        ei = h.getEi(batches, num_elem)\n\n        if len(ret.head.size()) > 2:\n            ei = ei.contiguous().view(num_elem, *ret.head.size())\n\n        ret.errors = torch.cat( (ret.errors, ei * ret.beta) ) if not ret.beta is None else ret.errors\n        ret.beta = None\n        return ret.checkSizes()\n\n    def zono_to_hybrid(self, *args, customRelu = None, **kargs): # we are already a hybrid zono.\n        return HybridZonotope(self.head, self.beta, self.errors, customRelu = self.customRelu if customRelu is None else customRelu)\n\n    def hybrid_to_zono(self, *args, **kargs):\n        return self.new(self.head, self.beta, self.errors, **kargs)\n\n    def applyMonotone(self, *args, **kargs):\n        return self.applySuper(super(Zonotope,self).applyMonotone(*args, **kargs))\n\n    def softplus(self):\n        return self.applySuper(super(Zonotope,self).softplus())\n\n    def relu(self):\n        return self.applySuper(super(Zonotope,self).relu())\n\n    def splitRelu(self, *args, **kargs):\n        return [self.applySuper(a) for a in super(Zonotope, self).splitRelu(*args, **kargs)]\n\n\ndef mysign(x):\n    e = x.eq(0).to_dtype()\n    r = x.sign().to_dtype()\n    return r + e\n\ndef mulIfEq(grad,out,target):\n    pred = out.max(1, keepdim=True)[1]\n    is_eq = pred.eq(target.view_as(pred)).to_dtype()\n    is_eq = is_eq.view([-1] + [1 for _ in grad.size()[1:]]).expand_as(grad)\n    return is_eq\n    \n\ndef stdLoss(out, target):\n    if torch.__version__[0] == ""0"":\n        return F.cross_entropy(out, target, reduce = False)\n    else:\n        return F.cross_entropy(out, target, reduction=\'none\')\n\n\n\nclass ListDomain(object):\n\n    def __init__(self, al, *args, **kargs):\n        self.al = list(al)\n\n    def new(self, *args, **kargs):\n        return self.__class__(*args, **kargs)\n\n    def isSafe(self,*args,**kargs):\n        raise ""Domain Not Suitable For Testing""\n\n    def labels(self):\n        raise ""Domain Not Suitable For Testing""\n\n    def isPoint(self):\n        return all(a.isPoint() for a in self.al)\n\n    def __mul__(self, flt):\n        return self.new(a.__mul__(flt) for a in self.al)\n\n    def __truediv__(self, flt):\n        return self.new(a.__truediv__(flt) for a in self.al)\n\n    def __add__(self, other):\n        if isinstance(other, ListDomain):\n            return self.new(a.__add__(o) for a,o in zip(self.al, other.al))\n        else:\n            return self.new(a.__add__(other) for a in self.al)\n\n    def merge(self, other, ref = None):\n        if ref is None:\n            return self.new(a.merge(o) for a,o in zip(self.al,other.al) )\n        return self.new(a.merge(o, ref = r) for a,o,r in zip(self.al,other.al, ref.al))\n\n    def addPar(self, a, b):\n        return self.new(s.addPar(av,bv) for s,av,bv in zip(self.al, a.al, b.al))\n\n    def __sub__(self, other):\n        if isinstance(other, ListDomain):\n            return self.new(a.__sub__(o) for a,o in zip(self.al, other.al))\n        else:\n            return self.new(a.__sub__(other) for a in self.al)\n\n    def abstractApplyLeaf(self, *args, **kargs):\n        return self.new(a.abstractApplyLeaf(*args, **kargs) for a in self.al)\n\n    def bmm(self, other):\n        return self.new(a.bmm(other) for a in self.al)\n\n    def matmul(self, other):\n        return self.new(a.matmul(other) for a in self.al)\n\n    def conv(self, *args, **kargs):\n        return self.new(a.conv(*args, **kargs) for a in self.al)\n\n    def conv1d(self, *args, **kargs):\n        return self.new(a.conv1d(*args, **kargs) for a in self.al)\n\n    def conv2d(self, *args, **kargs):\n        return self.new(a.conv2d(*args, **kargs) for a in self.al)\n\n    def conv3d(self, *args, **kargs):\n        return self.new(a.conv3d(*args, **kargs) for a in self.al)\n\n    def max_pool2d(self, *args, **kargs):\n        return self.new(a.max_pool2d(*args, **kargs) for a in self.al)\n\n    def avg_pool2d(self, *args, **kargs):\n        return self.new(a.avg_pool2d(*args, **kargs) for a in self.al)\n\n    def adaptive_avg_pool2d(self, *args, **kargs):\n        return self.new(a.adaptive_avg_pool2d(*args, **kargs) for a in self.al)\n\n    def unsqueeze(self, *args, **kargs):\n        return self.new(a.unsqueeze(*args, **kargs) for a in self.al)\n\n    def squeeze(self, *args, **kargs):\n        return self.new(a.squeeze(*args, **kargs) for a in self.al)\n\n    def view(self, *args, **kargs):\n        return self.new(a.view(*args, **kargs) for a in self.al)\n\n    def gather(self, *args, **kargs):\n        return self.new(a.gather(*args, **kargs) for a in self.al)\n\n    def sum(self, *args, **kargs):\n        return self.new(a.sum(*args,**kargs) for a in self.al)\n\n    def double(self):\n        return self.new(a.double() for a in self.al)\n\n    def float(self):\n        return self.new(a.float() for a in self.al)\n\n    def to_dtype(self):\n        return self.new(a.to_dtype() for a in self.al)\n\n    def vanillaTensorPart(self):\n        return self.al[0].vanillaTensorPart()\n\n    def center(self):\n        return self.new(a.center() for a in self.al)\n\n    def ub(self):\n        return self.new(a.ub() for a in self.al)\n\n    def lb(self):\n        return self.new(a.lb() for a in self.al)\n\n    def relu(self):\n        return self.new(a.relu() for a in self.al)\n\n    def splitRelu(self, *args, **kargs):\n        return self.new(a.splitRelu(*args, **kargs) for a in self.al)\n\n    def softplus(self):\n        return self.new(a.softplus() for a in self.al)\n\n    def elu(self):\n        return self.new(a.elu() for a in self.al)\n\n    def selu(self):\n        return self.new(a.selu() for a in self.al)\n\n    def sigm(self):\n        return self.new(a.sigm() for a in self.al)\n\n    def cat(self, other, *args, **kargs):\n        return self.new(a.cat(o, *args, **kargs) for a,o in zip(self.al, other.al))\n\n\n    def split(self, *args, **kargs):\n        return [self.new(*z) for z in zip(a.split(*args, **kargs) for a in self.al)]\n\n    def size(self):\n        return self.al[0].size()\n\n    def loss(self, *args, **kargs):\n        return sum(a.loss(*args, **kargs) for a in self.al)\n\n    def deep_loss(self, *args, **kargs):\n        return sum(a.deep_loss(*args, **kargs) for a in self.al)\n\n    def checkSizes(self):\n        for a in self.al:\n            a.checkSizes()\n        return self\n\n\nclass TaggedDomain(object):\n\n\n    def __init__(self, a, tag = None):\n        self.tag = tag\n        self.a = a\n\n    def isSafe(self,*args,**kargs):\n        return self.a.isSafe(*args, **kargs)\n\n    def isPoint(self):\n        return self.a.isPoint()\n\n    def labels(self):\n        raise ""Domain Not Suitable For Testing""\n\n    def __mul__(self, flt):\n        return TaggedDomain(self.a.__mul__(flt), self.tag)\n\n    def __truediv__(self, flt):\n        return TaggedDomain(self.a.__truediv__(flt), self.tag)\n\n    def __add__(self, other):\n        if isinstance(other, TaggedDomain):\n            return TaggedDomain(self.a.__add__(other.a), self.tag)\n        else:\n            return TaggedDomain(self.a.__add__(other), self.tag)\n\n    def addPar(self, a,b):\n        return TaggedDomain(self.a.addPar(a.a, b.a), self.tag)\n\n    def __sub__(self, other):\n        if isinstance(other, TaggedDomain):\n            return TaggedDomain(self.a.__sub__(other.a), self.tag)\n        else:\n            return TaggedDomain(self.a.__sub__(other), self.tag)\n\n    def bmm(self, other):\n        return TaggedDomain(self.a.bmm(other), self.tag)\n\n    def matmul(self, other):\n        return TaggedDomain(self.a.matmul(other), self.tag)\n\n    def conv(self, *args, **kargs):\n        return TaggedDomain(self.a.conv(*args, **kargs) , self.tag)\n\n    def conv1d(self, *args, **kargs):\n        return TaggedDomain(self.a.conv1d(*args, **kargs), self.tag)\n\n    def conv2d(self, *args, **kargs):\n        return TaggedDomain(self.a.conv2d(*args, **kargs), self.tag)\n\n    def conv3d(self, *args, **kargs):\n        return TaggedDomain(self.a.conv3d(*args, **kargs), self.tag)\n\n    def max_pool2d(self, *args, **kargs):\n        return TaggedDomain(self.a.max_pool2d(*args, **kargs), self.tag)\n\n    def avg_pool2d(self, *args, **kargs):\n        return TaggedDomain(self.a.avg_pool2d(*args, **kargs), self.tag)\n\n    def adaptive_avg_pool2d(self, *args, **kargs):\n        return TaggedDomain(self.a.adaptive_avg_pool2d(*args, **kargs), self.tag)\n\n\n    def unsqueeze(self, *args, **kargs):\n        return TaggedDomain(self.a.unsqueeze(*args, **kargs), self.tag)\n\n    def squeeze(self, *args, **kargs):\n        return TaggedDomain(self.a.squeeze(*args, **kargs), self.tag)\n\n    def abstractApplyLeaf(self, *args, **kargs):\n        return TaggedDomain(self.a.abstractApplyLeaf(*args, **kargs), self.tag)\n\n    def view(self, *args, **kargs):\n        return TaggedDomain(self.a.view(*args, **kargs), self.tag)\n\n    def gather(self, *args, **kargs):\n        return TaggedDomain(self.a.gather(*args, **kargs), self.tag)\n\n    def sum(self, *args, **kargs):\n        return TaggedDomain(self.a.sum(*args,**kargs), self.tag)\n\n    def double(self):\n        return TaggedDomain(self.a.double(), self.tag)\n\n    def float(self):\n        return TaggedDomain(self.a.float(), self.tag)\n\n    def to_dtype(self):\n        return TaggedDomain(self.a.to_dtype(), self.tag)\n\n    def vanillaTensorPart(self):\n        return self.a.vanillaTensorPart()\n\n    def center(self):\n        return TaggedDomain(self.a.center(), self.tag)\n\n    def ub(self):\n        return TaggedDomain(self.a.ub(), self.tag)\n\n    def lb(self):\n        return TaggedDomain(self.a.lb(), self.tag)\n\n    def relu(self):\n        return TaggedDomain(self.a.relu(), self.tag)\n\n    def splitRelu(self, *args, **kargs):\n        return TaggedDomain(self.a.splitRelu(*args, **kargs), self.tag)\n\n    def diameter(self):\n        return self.a.diameter()\n\n    def softplus(self):\n        return TaggedDomain(self.a.softplus(), self.tag)\n\n    def elu(self):\n        return TaggedDomain(self.a.elu(), self.tag)\n\n    def selu(self):\n        return TaggedDomain(self.a.selu(), self.tag)\n\n    def sigm(self):\n        return TaggedDomain(self.a.sigm(), self.tag)\n\n\n    def cat(self, other, *args, **kargs):\n        return TaggedDomain(self.a.cat(other.a, *args, **kargs), self.tag)\n\n    def split(self, *args, **kargs):\n        return [TaggedDomain(z, self.tag) for z in self.a.split(*args, **kargs)]\n\n    def size(self):\n        \n        return self.a.size()\n\n    def loss(self, *args, **kargs):\n        return self.tag.loss(self.a, *args, **kargs)\n\n    def deep_loss(self, *args, **kargs):\n        return self.a.deep_loss(*args, **kargs)\n\n    def checkSizes(self):\n        self.a.checkSizes()\n        return self\n\n    def merge(self, other, ref = None):\n        return TaggedDomain(self.a.merge(other.a, ref = None if ref is None else ref.a), self.tag)\n'"
components.py,27,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.distributions import multinomial, categorical\nimport torch.optim as optim\n\nimport math\n\ntry:\n    from . import helpers as h\n    from . import ai\n    from . import scheduling as S\nexcept:\n    import helpers as h\n    import ai\n    import scheduling as S\n\nimport math\nimport abc\n\nfrom torch.nn.modules.conv import _ConvNd\nfrom enum import Enum\n\n\nclass InferModule(nn.Module):\n    def __init__(self, *args, normal = False, ibp_init = False, **kwargs):\n        self.args = args\n        self.kwargs = kwargs\n        self.infered = False\n        self.normal = normal\n        self.ibp_init = ibp_init\n\n    def infer(self, in_shape, global_args = None):\n        """""" this is really actually stateful. """"""\n\n        if self.infered:\n            return self\n        self.infered = True\n\n        super(InferModule, self).__init__()\n        self.inShape = list(in_shape)\n        self.outShape = list(self.init(list(in_shape), *self.args, global_args = global_args, **self.kwargs))\n        if self.outShape is None:\n            raise ""init should set the out_shape""\n        \n        self.reset_parameters()\n        return self\n    \n    def reset_parameters(self):\n        if not hasattr(self,\'weight\') or self.weight is None:\n            return\n        n = h.product(self.weight.size()) / self.outShape[0]\n        stdv = 1 / math.sqrt(n)\n        \n        if self.ibp_init:\n            torch.nn.init.orthogonal_(self.weight.data)\n        elif self.normal:\n            self.weight.data.normal_(0, stdv)\n            self.weight.data.clamp_(-1, 1)\n        else:\n            self.weight.data.uniform_(-stdv, stdv)\n\n        if self.bias is not None:\n            if self.ibp_init:\n                self.bias.data.zero_()\n            elif self.normal:\n                self.bias.data.normal_(0, stdv)\n                self.bias.data.clamp_(-1, 1)\n            else:\n                self.bias.data.uniform_(-stdv, stdv)\n\n    def clip_norm(self):\n        if not hasattr(self, ""weight""):\n            return\n        if not hasattr(self,""weight_g""):\n            if torch.__version__[0] == ""0"":\n                nn.utils.weight_norm(self, dim=None)\n            else:\n                nn.utils.weight_norm(self)\n                \n        self.weight_g.data.clamp_(-h.max_c_for_norm, h.max_c_for_norm)\n\n        if torch.__version__[0] != ""0"":\n            self.weight_v.data.clamp_(-h.max_c_for_norm * 10000,h.max_c_for_norm * 10000)\n            if hasattr(self, ""bias""):\n                self.bias.data.clamp_(-h.max_c_for_norm * 10000, h.max_c_for_norm * 10000)\n\n    def regularize(self, p):\n        reg = 0 \n        if torch.__version__[0] == ""0"":\n            for param in self.parameters():\n                reg += param.norm(p)\n        else:\n            if hasattr(self, ""weight_g""):\n                reg += self.weight_g.norm().sum()\n                reg += self.weight_v.norm().sum()\n            elif hasattr(self, ""weight""):\n                reg += self.weight.norm().sum()\n\n            if hasattr(self, ""bias""):\n                reg += self.bias.view(-1).norm(p=p).sum()\n\n        return reg\n\n    def remove_norm(self):\n        if hasattr(self,""weight_g""):\n            torch.nn.utils.remove_weight_norm(self)\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__)\n\n    def printNet(self, f):\n        print(self.__class__.__name__, file=f)\n\n    @abc.abstractmethod        \n    def forward(self, *args, **kargs):\n        pass\n\n    def __call__(self, *args, onyx=False, **kargs):\n        if onyx:\n            return self.forward(*args, onyx=onyx, **kargs)\n        else:\n            return super(InferModule, self).__call__(*args, **kargs)\n    \n    @abc.abstractmethod\n    def neuronCount(self):\n        pass\n\n    def depth(self):\n        return 0\n\ndef getShapeConv(in_shape, conv_shape, stride = 1, padding = 0):\n    inChan, inH, inW = in_shape\n    outChan, kH, kW = conv_shape[:3]\n\n    outH = 1 + int((2 * padding + inH - kH) / stride)\n    outW = 1 + int((2 * padding + inW - kW) / stride)\n    return (outChan, outH, outW)\n\ndef getShapeConvTranspose(in_shape, conv_shape, stride = 1, padding = 0, out_padding=0):\n    inChan, inH, inW = in_shape\n    outChan, kH, kW = conv_shape[:3]\n\n    outH = (inH - 1 ) * stride - 2 * padding + kH + out_padding\n    outW = (inW - 1 ) * stride - 2 * padding + kW + out_padding\n    return (outChan, outH, outW)\n\n\n\nclass Linear(InferModule):\n    def init(self, in_shape, out_shape, **kargs):\n        self.in_neurons = h.product(in_shape)\n        if isinstance(out_shape, int):\n            out_shape = [out_shape]\n        self.out_neurons = h.product(out_shape) \n        \n        self.weight = torch.nn.Parameter(torch.Tensor(self.in_neurons, self.out_neurons))\n        self.bias = torch.nn.Parameter(torch.Tensor(self.out_neurons))\n\n        return out_shape\n\n    def forward(self, x, **kargs):\n        s = x.size()\n        x = x.view(s[0], h.product(s[1:]))\n        return (x.matmul(self.weight) + self.bias).view(s[0], *self.outShape)\n\n    def neuronCount(self):\n        return 0\n\n    def showNet(self, t = """"):\n        print(t + ""Linear out="" + str(self.out_neurons))\n\n    def printNet(self, f):\n        print(""Linear("" + str(self.out_neurons) + "")"" )\n\n        print(h.printListsNumpy(list(self.weight.transpose(1,0).data)), file= f)\n        print(h.printNumpy(self.bias), file= f)\n\nclass Activation(InferModule):\n    def init(self, in_shape, global_args = None, activation = ""ReLU"", **kargs):\n        self.activation = [ ""ReLU"",""Sigmoid"", ""Tanh"", ""Softplus"", ""ELU"", ""SELU""].index(activation)\n        self.activation_name = activation\n        return in_shape\n\n    def regularize(self, p):\n        return 0\n\n    def forward(self, x, **kargs):\n        return [lambda x:x.relu(), lambda x:x.sigmoid(), lambda x:x.tanh(), lambda x:x.softplus(), lambda x:x.elu(), lambda x:x.selu()][self.activation](x)\n\n    def neuronCount(self):\n        return h.product(self.outShape)\n\n    def depth(self):\n        return 1\n\n    def showNet(self, t = """"):\n        print(t + self.activation_name)\n\n    def printNet(self, f):\n        pass\n\nclass ReLU(Activation):\n    pass\n\ndef activation(*args, batch_norm = False, **kargs):\n    a = Activation(*args, **kargs)\n    return Seq(BatchNorm(), a) if batch_norm else a\n\nclass Identity(InferModule): # for feigning model equivelence when removing an op\n    def init(self, in_shape, global_args = None, **kargs):\n        return in_shape\n\n    def forward(self, x, **kargs):\n        return x\n\n    def neuronCount(self):\n        return 0\n\n    def printNet(self, f):\n        pass\n\n    def regularize(self, p):\n        return 0\n\n    def showNet(self, *args, **kargs):\n        pass\n\nclass Dropout(InferModule):\n    def init(self, in_shape, p=0.5, use_2d = False, alpha_dropout = False, **kargs):\n        self.p = S.Const.initConst(p)\n        self.use_2d = use_2d\n        self.alpha_dropout = alpha_dropout\n        return in_shape\n\n    def forward(self, x, time = 0, **kargs):\n        if self.training:\n            with torch.no_grad():\n                p = self.p.getVal(time = time)\n                mask = (F.dropout2d if self.use_2d else F.dropout)(h.ones(x.size()),p=p, training=True) \n            if self.alpha_dropout:\n                with torch.no_grad():\n                    keep_prob = 1 - p\n                    alpha = -1.7580993408473766\n                    a = math.pow(keep_prob + alpha * alpha * keep_prob * (1 - keep_prob), -0.5)\n                    b = -a * alpha * (1 - keep_prob)\n                    mask = mask * a\n                return x * mask + b\n            else:\n                return x * mask\n        else:\n            return x\n\n    def neuronCount(self):\n        return 0\n\n    def showNet(self, t = """"):\n        print(t + ""Dropout p="" + str(self.p))\n\n    def printNet(self, f):\n        print(""Dropout("" + str(self.p) + "")"" )\n\nclass PrintActivation(Identity):\n    def init(self, in_shape, global_args = None, activation = ""ReLU"", **kargs):\n        self.activation = activation\n        return in_shape\n\n    def printNet(self, f):\n        print(self.activation, file = f)\n\nclass PrintReLU(PrintActivation):\n    pass\n\nclass Conv2D(InferModule):\n\n    def init(self, in_shape, out_channels, kernel_size, stride = 1, global_args = None, bias=True, padding = 0, activation = ""ReLU"", **kargs):\n        self.prev = in_shape\n        self.in_channels = in_shape[0]\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.activation = activation\n        self.use_softplus = h.default(global_args, \'use_softplus\', False)\n        \n        weights_shape = (self.out_channels, self.in_channels, kernel_size, kernel_size)        \n        self.weight = torch.nn.Parameter(torch.Tensor(*weights_shape))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.Tensor(weights_shape[0]))\n        else:\n            self.bias = None # h.zeros(weights_shape[0])\n            \n        outshape = getShapeConv(in_shape, (out_channels, kernel_size, kernel_size), stride, padding)\n        return outshape\n        \n    def forward(self, input, **kargs):\n        return input.conv2d(self.weight, bias=self.bias, stride=self.stride, padding = self.padding )\n    \n    def printNet(self, f): # only complete if we\'ve forwardt stride=1\n        print(""Conv2D"", file = f)\n        sz = list(self.prev)\n        print(self.activation + "", filters={}, kernel_size={}, input_shape={}, stride={}, padding={}"".format(self.out_channels, [self.kernel_size, self.kernel_size], list(reversed(sz)), [self.stride, self.stride], self.padding ), file = f)\n        print(h.printListsNumpy([[list(p) for p in l ] for l in self.weight.permute(2,3,1,0).data]) , file= f)\n        print(h.printNumpy(self.bias if self.bias is not None else h.dten(self.out_channels)), file= f)\n\n    def showNet(self, t = """"):\n        sz = list(self.prev)\n        print(t + ""Conv2D, filters={}, kernel_size={}, input_shape={}, stride={}, padding={}"".format(self.out_channels, [self.kernel_size, self.kernel_size], list(reversed(sz)), [self.stride, self.stride], self.padding ))\n\n    def neuronCount(self):\n        return 0\n\n\nclass ConvTranspose2D(InferModule):\n\n    def init(self, in_shape, out_channels, kernel_size, stride = 1, global_args = None, bias=True, padding = 0, out_padding=0, activation = ""ReLU"", **kargs):\n        self.prev = in_shape\n        self.in_channels = in_shape[0]\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.out_padding = out_padding\n        self.activation = activation\n        self.use_softplus = h.default(global_args, \'use_softplus\', False)\n        \n        weights_shape = (self.in_channels, self.out_channels, kernel_size, kernel_size)        \n        self.weight = torch.nn.Parameter(torch.Tensor(*weights_shape))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.Tensor(weights_shape[0]))\n        else:\n            self.bias = None # h.zeros(weights_shape[0])\n            \n        outshape = getShapeConvTranspose(in_shape, (out_channels, kernel_size, kernel_size), stride, padding, out_padding)\n        return outshape\n\n    def forward(self, input, **kargs):\n        return input.conv_transpose2d(self.weight, bias=self.bias, stride=self.stride, padding = self.padding, output_padding=self.out_padding)\n    \n    def printNet(self, f): # only complete if we\'ve forwardt stride=1\n        print(""ConvTranspose2D"", file = f)\n        print(self.activation + "", filters={}, kernel_size={}, input_shape={}"".format(self.out_channels, list(self.kernel_size), list(self.prev) ), file = f)\n        print(h.printListsNumpy([[list(p) for p in l ] for l in self.weight.permute(2,3,1,0).data]) , file= f)\n        print(h.printNumpy(self.bias), file= f)\n\n    def neuronCount(self):\n        return 0\n\n\n\nclass MaxPool2D(InferModule):\n    def init(self, in_shape, kernel_size, stride = None, **kargs):\n        self.prev = in_shape\n        self.kernel_size = kernel_size\n        self.stride = kernel_size if stride is None else stride\n        return getShapeConv(in_shape, (in_shape[0], kernel_size, kernel_size), stride)\n\n    def forward(self, x, **kargs):\n        return x.max_pool2d(self.kernel_size, self.stride)\n    \n    def printNet(self, f):\n        print(""MaxPool2D stride={}, kernel_size={}, input_shape={}"".format(list(self.stride), list(self.shape[2:]), list(self.prev[1:]+self.prev[:1]) ), file = f)\n        \n    def neuronCount(self):\n        return h.product(self.outShape)\n\nclass AvgPool2D(InferModule):\n    def init(self, in_shape, kernel_size, stride = None, **kargs):\n        self.prev = in_shape\n        self.kernel_size = kernel_size\n        self.stride = kernel_size if stride is None else stride\n        out_size = getShapeConv(in_shape, (in_shape[0], kernel_size, kernel_size), self.stride, padding = 1)\n        return out_size \n\n    def forward(self, x, **kargs):\n        if h.product(x.size()[2:]) == 1:\n            return x\n        return x.avg_pool2d(kernel_size = self.kernel_size, stride = self.stride, padding = 1)\n    \n    def printNet(self, f):\n        print(""AvgPool2D stride={}, kernel_size={}, input_shape={}"".format(list(self.stride), list(self.shape[2:]), list(self.prev[1:]+self.prev[:1]) ), file = f)\n        \n    def neuronCount(self):\n        return h.product(self.outShape)\n\nclass AdaptiveAvgPool2D(InferModule):\n    def init(self, in_shape, out_shape, **kargs):\n        self.prev = in_shape\n        self.out_shape = list(out_shape)\n        return [in_shape[0]] + self.out_shape\n\n    def forward(self, x, **kargs):\n        return x.adaptive_avg_pool2d(self.out_shape)\n    \n    def printNet(self, f):\n        print(""AdaptiveAvgPool2D out_Shape={} input_shape={}"".format(list(self.out_shape), list(self.prev[1:]+self.prev[:1]) ), file = f)\n        \n    def neuronCount(self):\n        return h.product(self.outShape)\n\nclass Normalize(InferModule):\n    def init(self, in_shape, mean, std, **kargs):\n        self.mean_v = mean\n        self.std_v = std\n        self.mean = h.dten(mean)\n        self.std = 1 / h.dten(std)\n        return in_shape\n\n    def forward(self, x, **kargs):\n        mean_ex = self.mean.view(self.mean.shape[0],1,1).expand(*x.size()[1:])\n        std_ex = self.std.view(self.std.shape[0],1,1).expand(*x.size()[1:])\n        return (x - mean_ex) * std_ex\n\n    def neuronCount(self):\n        return 0\n\n    def printNet(self, f):\n        print(""Normalize mean={} std={}"".format(self.mean_v, self.std_v), file = f)\n\n    def showNet(self, t = """"):\n        print(t + ""Normalize mean={} std={}"".format(self.mean_v, self.std_v))\n\nclass Flatten(InferModule):\n    def init(self, in_shape, **kargs):\n        return h.product(in_shape)\n        \n    def forward(self, x, **kargs):\n        s = x.size()\n        return x.view(s[0], h.product(s[1:]))\n\n    def neuronCount(self):\n        return 0\n\nclass BatchNorm(InferModule):\n    def init(self, in_shape, track_running_stats = True, momentum = 0.1, eps=1e-5, **kargs):\n        self.gamma = torch.nn.Parameter(torch.Tensor(*in_shape))\n        self.beta = torch.nn.Parameter(torch.Tensor(*in_shape))\n        self.eps = eps\n        self.track_running_stats = track_running_stats\n        self.momentum = momentum\n\n        self.running_mean = None\n        self.running_var =  None\n\n        self.num_batches_tracked = 0\n        return in_shape\n\n    def reset_parameters(self):\n        self.gamma.data.fill_(1)\n        self.beta.data.zero_()\n\n    def forward(self, x, **kargs):\n        exponential_average_factor = 0.0\n        if self.training and self.track_running_stats:\n            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n            if self.num_batches_tracked is not None:\n                self.num_batches_tracked += 1\n                if self.momentum is None:  # use cumulative moving average\n                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n                else:  # use exponential moving average\n                    exponential_average_factor = self.momentum\n\n        new_mean = x.vanillaTensorPart().detach().mean(dim=0)\n        new_var = x.vanillaTensorPart().detach().var(dim=0, unbiased=False)\n        if torch.isnan(new_var * 0).any(): \n            return x\n        if self.training:\n            self.running_mean = (1 - exponential_average_factor) * self.running_mean + exponential_average_factor * new_mean  if self.running_mean is not None else new_mean\n            if self.running_var is None:\n                self.running_var = new_var\n            else:\n                q = (1 - exponential_average_factor) * self.running_var  \n                r = exponential_average_factor * new_var\n                self.running_var = q + r \n\n        if self.track_running_stats and self.running_mean is not None and self.running_var is not None:\n            new_mean = self.running_mean \n            new_var = self.running_var\n        \n        diver = 1 / (new_var + self.eps).sqrt()\n\n        if torch.isnan(diver).any():\n            print(""Really shouldn\'t happen ever"")\n            return x\n        else:\n            out = (x - new_mean) * diver * self.gamma + self.beta\n            return  out\n    \n    def neuronCount(self):\n        return 0\n\nclass Unflatten2d(InferModule):\n    def init(self, in_shape, w, **kargs):\n        self.w = w\n        self.outChan = int(h.product(in_shape) / (w * w))\n        \n        return (self.outChan, self.w, self.w)\n        \n    def forward(self, x, **kargs):\n        s = x.size()\n        return x.view(s[0], self.outChan, self.w, self.w)\n\n    def neuronCount(self):\n        return 0\n\n\nclass View(InferModule):\n    def init(self, in_shape, out_shape, **kargs):\n        assert(h.product(in_shape) == h.product(out_shape))\n        return out_shape\n        \n    def forward(self, x, **kargs):\n        s = x.size()\n        return x.view(s[0], *self.outShape)\n\n    def neuronCount(self):\n        return 0\n    \nclass Seq(InferModule):\n    def init(self, in_shape, *layers, **kargs):\n        self.layers = layers\n        self.net = nn.Sequential(*layers)\n        self.prev = in_shape\n        for s in layers:\n            in_shape = s.infer(in_shape, **kargs).outShape\n        return in_shape\n    \n    def forward(self, x, **kargs):\n        \n        for l in self.layers:\n            x = l(x, **kargs)\n        return x\n\n    def clip_norm(self):\n        for l in self.layers:\n            l.clip_norm()\n\n    def regularize(self, p):\n        return sum(n.regularize(p) for n in self.layers)\n\n    def remove_norm(self):\n        for l in self.layers:\n            l.remove_norm()\n\n    def printNet(self, f):\n        for l in self.layers:\n            l.printNet(f)\n\n    def showNet(self, *args, **kargs):\n        for l in self.layers:\n            l.showNet(*args, **kargs)\n\n    def neuronCount(self):\n        return sum([l.neuronCount() for l in self.layers ]) \n\n    def depth(self):\n        return sum([l.depth() for l in self.layers ]) \n    \ndef FFNN(layers, last_lin = False, last_zono = False, **kargs):\n    starts = layers\n    ends = []\n    if last_lin:\n        ends = ([CorrelateAll(only_train=False)] if last_zono else []) + [PrintActivation(activation = ""Affine""), Linear(layers[-1],**kargs)]\n        starts = layers[:-1]\n    \n    return Seq(*([ Seq(PrintActivation(**kargs), Linear(s, **kargs), activation(**kargs)) for s in starts] + ends))\n\ndef Conv(*args, **kargs):\n    return Seq(Conv2D(*args, **kargs), activation(**kargs))\n\ndef ConvTranspose(*args, **kargs):\n    return Seq(ConvTranspose2D(*args, **kargs), activation(**kargs))\n\nMP = MaxPool2D \n\ndef LeNet(conv_layers, ly = [], bias = True, normal=False, **kargs):\n    def transfer(tp):\n        if isinstance(tp, InferModule):\n            return tp\n        if isinstance(tp[0], str):\n            return MaxPool2D(*tp[1:])\n        return Conv(out_channels = tp[0], kernel_size = tp[1], stride = tp[-1] if len(tp) == 4 else 1, bias=bias, normal=normal, **kargs)\n    conv = [transfer(s) for s in conv_layers]\n    return Seq(*conv, FFNN(ly, **kargs, bias=bias)) if len(ly) > 0 else Seq(*conv)\n\ndef InvLeNet(ly, w, conv_layers, bias = True, normal=False, **kargs):\n    def transfer(tp):\n        return ConvTranspose(out_channels = tp[0], kernel_size = tp[1], stride = tp[2], padding = tp[3], out_padding = tp[4], bias=False, normal=normal)\n                      \n    return Seq(FFNN(ly, bias=bias), Unflatten2d(w),  *[transfer(s) for s in conv_layers])\n\nclass FromByteImg(InferModule):\n    def init(self, in_shape, **kargs):\n        return in_shape\n    \n    def forward(self, x, **kargs):\n        return x.to_dtype()/ 256.\n\n    def neuronCount(self):\n        return 0\n        \nclass Skip(InferModule):\n    def init(self, in_shape, net1, net2, **kargs):\n        self.net1 = net1.infer(in_shape, **kargs)\n        self.net2 = net2.infer(in_shape, **kargs)\n        assert(net1.outShape[1:] == net2.outShape[1:])\n        return [ net1.outShape[0] + net2.outShape[0] ] + net1.outShape[1:]\n    \n    def forward(self, x, **kargs):\n        r1 = self.net1(x, **kargs)\n        r2 = self.net2(x, **kargs)\n        return r1.cat(r2, dim=1)\n\n    def regularize(self, p):\n        return self.net1.regularize(p) + self.net2.regularize(p)\n\n    def clip_norm(self):\n        self.net1.clip_norm()\n        self.net2.clip_norm()\n\n    def remove_norm(self):\n        self.net1.remove_norm()\n        self.net2.remove_norm()\n\n    def neuronCount(self):\n        return self.net1.neuronCount() + self.net2.neuronCount()\n\n    def printNet(self, f):\n        print(""SkipNet1"", file=f)\n        self.net1.printNet(f)\n        print(""SkipNet2"", file=f)\n        self.net2.printNet(f)\n        print(""SkipCat dim=1"", file=f)\n\n    def showNet(self, t = """"):\n        print(t+""SkipNet1"")\n        self.net1.showNet(""    ""+t)\n        print(t+""SkipNet2"")\n        self.net2.showNet(""    ""+t)\n        print(t+""SkipCat dim=1"")\n\nclass ParSum(InferModule):\n    def init(self, in_shape, net1, net2, **kargs):\n        self.net1 = net1.infer(in_shape, **kargs)\n        self.net2 = net2.infer(in_shape, **kargs)\n        assert(net1.outShape == net2.outShape)\n        return net1.outShape\n    \n\n\n    def forward(self, x, **kargs):\n        \n        r1 = self.net1(x, **kargs)\n        r2 = self.net2(x, **kargs)\n        return x.addPar(r1,r2)\n\n    def clip_norm(self):\n        self.net1.clip_norm()\n        self.net2.clip_norm()\n\n    def remove_norm(self):\n        self.net1.remove_norm()\n        self.net2.remove_norm()\n\n    def neuronCount(self):\n        return self.net1.neuronCount() + self.net2.neuronCount()\n    \n    def depth(self):\n        return max(self.net1.depth(), self.net2.depth())\n\n    def printNet(self, f):\n        print(""ParNet1"", file=f)\n        self.net1.printNet(f)\n        print(""ParNet2"", file=f)\n        self.net2.printNet(f)\n        print(""ParCat dim=1"", file=f)\n\n    def showNet(self, t = """"):\n        print(t + ""ParNet1"")\n        self.net1.showNet(""    ""+t)\n        print(t + ""ParNet2"")\n        self.net2.showNet(""    ""+t)\n        print(t + ""ParSum"")\n\nclass ToZono(Identity):\n    def init(self, in_shape, customRelu = None, only_train = False, **kargs):\n        self.customRelu = customRelu\n        self.only_train = only_train\n        return in_shape\n\n    def forward(self, x, **kargs):\n        return self.abstract_forward(x, **kargs) if self.training or not self.only_train else x\n\n    def abstract_forward(self, x, **kargs):\n        return x.abstractApplyLeaf(\'hybrid_to_zono\', customRelu = self.customRelu)\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__ + "" only_train="" + str(self.only_train))\n\nclass CorrelateAll(ToZono):\n    def abstract_forward(self, x, **kargs):\n        return x.abstractApplyLeaf(\'hybrid_to_zono\',correlate=True, customRelu = self.customRelu)\n\nclass ToHZono(ToZono):\n    def abstract_forward(self, x, **kargs):\n        return x.abstractApplyLeaf(\'zono_to_hybrid\',customRelu = self.customRelu)\n\nclass Concretize(ToZono):\n    def init(self, in_shape, only_train = True, **kargs):\n        self.only_train = only_train\n        return in_shape\n\n    def abstract_forward(self, x, **kargs):\n        return x.abstractApplyLeaf(\'concretize\')\n\n# stochastic correlation\nclass CorrRand(Concretize):\n    def init(self, in_shape, num_correlate, only_train = True, **kargs):\n        self.only_train = only_train\n        self.num_correlate = num_correlate\n        return in_shape\n        \n    def abstract_forward(self, x):\n        return x.abstractApplyLeaf(""stochasticCorrelate"", self.num_correlate)\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__ + "" only_train="" + str(self.only_train) + "" num_correlate=""+ str(self.num_correlate))\n\nclass CorrMaxK(CorrRand):\n    def abstract_forward(self, x):\n        return x.abstractApplyLeaf(""correlateMaxK"", self.num_correlate)\n\n\nclass CorrMaxPool2D(Concretize):\n    def init(self,in_shape, kernel_size, only_train = True, max_type = ai.MaxTypes.head_beta, **kargs):\n        self.only_train = only_train\n        self.kernel_size = kernel_size\n        self.max_type = max_type\n        return in_shape\n        \n    def abstract_forward(self, x):\n        return x.abstractApplyLeaf(""correlateMaxPool"", kernel_size = self.kernel_size, stride = self.kernel_size, max_type = self.max_type)\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__ + "" only_train="" + str(self.only_train) + "" kernel_size=""+ str(self.kernel_size) + "" max_type="" +str(self.max_type))\n\nclass CorrMaxPool3D(Concretize):\n    def init(self,in_shape, kernel_size, only_train = True, max_type = ai.MaxTypes.only_beta, **kargs):\n        self.only_train = only_train\n        self.kernel_size = kernel_size\n        self.max_type = max_type\n        return in_shape\n        \n    def abstract_forward(self, x):\n        return x.abstractApplyLeaf(""correlateMaxPool"", kernel_size = self.kernel_size, stride = self.kernel_size, max_type = self.max_type, max_pool = F.max_pool3d)\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__ + "" only_train="" + str(self.only_train) + "" kernel_size=""+ str(self.kernel_size) + "" max_type="" +self.max_type)\n\nclass CorrFix(Concretize):\n    def init(self,in_shape, k, only_train = True, **kargs):\n        self.k = k\n        self.only_train = only_train\n        return in_shape\n        \n    def abstract_forward(self, x):\n        sz = x.size()\n        """"""\n        # for more control in the future\n        indxs_1 = torch.arange(start = 0, end = sz[1], step = math.ceil(sz[1] / self.dims[1]) )\n        indxs_2 = torch.arange(start = 0, end = sz[2], step = math.ceil(sz[2] / self.dims[2]) )\n        indxs_3 = torch.arange(start = 0, end = sz[3], step = math.ceil(sz[3] / self.dims[3]) )\n\n        indxs = torch.stack(torch.meshgrid((indxs_1,indxs_2,indxs_3)), dim=3).view(-1,3)\n        """"""\n        szm = h.product(sz[1:])\n        indxs = torch.arange(start = 0, end = szm, step = math.ceil(szm / self.k))\n        indxs = indxs.unsqueeze(0).expand(sz[0], indxs.size()[0])\n\n        \n        return x.abstractApplyLeaf(""correlate"", indxs)\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__ + "" only_train="" + str(self.only_train) + "" k=""+ str(self.k))\n\n\nclass DecorrRand(Concretize):\n    def init(self, in_shape, num_decorrelate, only_train = True, **kargs):\n        self.only_train = only_train\n        self.num_decorrelate = num_decorrelate\n        return in_shape\n        \n    def abstract_forward(self, x):\n        return x.abstractApplyLeaf(""stochasticDecorrelate"", self.num_decorrelate)\n\nclass DecorrMin(Concretize):\n    def init(self, in_shape, num_decorrelate, only_train = True, num_to_keep = False, **kargs):\n        self.only_train = only_train\n        self.num_decorrelate = num_decorrelate\n        self.num_to_keep = num_to_keep\n        return in_shape\n        \n    def abstract_forward(self, x):\n        return x.abstractApplyLeaf(""decorrelateMin"", self.num_decorrelate, num_to_keep = self.num_to_keep)\n\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__ + "" only_train="" + str(self.only_train) + "" k=""+ str(self.num_decorrelate) + "" num_to_keep="" + str(self.num_to_keep) )\n\nclass DeepLoss(ToZono):\n    def init(self, in_shape, bw = 0.01, act = F.relu, **kargs): # weight must be between 0 and 1\n        self.only_train = True\n        self.bw = S.Const.initConst(bw)\n        self.act = act\n        return in_shape\n\n    def abstract_forward(self, x, **kargs):\n        if x.isPoint():\n            return x \n        return ai.TaggedDomain(x, self.MLoss(self, x))\n\n    class MLoss():\n        def __init__(self, obj, x):\n            self.obj = obj\n            self.x = x\n\n        def loss(self, a, *args, lr = 1, time = 0, **kargs):\n            bw = self.obj.bw.getVal(time = time)\n            pre_loss = a.loss(*args, time = time, **kargs, lr = lr * (1 - bw))\n            if bw <= 0.0:\n                return pre_loss\n            return (1 - bw) * pre_loss + bw * self.x.deep_loss(act = self.obj.act)\n\n    def showNet(self, t = """"):\n        print(t + self.__class__.__name__ + "" only_train="" + str(self.only_train) + "" bw=""+ str(self.bw) + "" act="" + str(self.act) )\n\nclass IdentLoss(DeepLoss):\n    def abstract_forward(self, x, **kargs):\n        return x\n    \ndef SkipNet(net1, net2, ffnn, **kargs):\n    return Seq(Skip(net1,net2), FFNN(ffnn, **kargs))\n\ndef WideBlock(out_filters, downsample=False, k=3, bias=False, **kargs):\n    if not downsample:\n        k_first = 3\n        skip_stride = 1\n        k_skip = 1\n    else: \n        k_first = 4\n        skip_stride = 2\n        k_skip = 2\n\n    # conv2d280(input)\n    blockA = Conv2D(out_filters, kernel_size=k_skip, stride=skip_stride, padding=0, bias=bias, normal=True, **kargs) \n\n    # conv2d282(relu(conv2d278(input)))\n    blockB = Seq( Conv(out_filters, kernel_size = k_first, stride = skip_stride, padding = 1, bias=bias, normal=True, **kargs)\n                , Conv2D(out_filters, kernel_size = k, stride = 1, padding = 1, bias=bias, normal=True, **kargs))\n    return Seq(ParSum(blockA, blockB), activation(**kargs)) \n\n\n\ndef BasicBlock(in_planes, planes, stride=1, bias = False, skip_net = False, **kargs):\n    block = Seq( Conv(planes, kernel_size = 3, stride = stride, padding = 1, bias=bias, normal=True, **kargs)\n               , Conv2D(planes, kernel_size = 3, stride = 1, padding = 1, bias=bias, normal=True, **kargs))\n\n    if stride != 1 or in_planes != planes:\n        block = ParSum(block, Conv2D(planes, kernel_size=1, stride=stride, bias=bias, normal=True, **kargs))\n    elif not skip_net:\n        block = ParSum(block, Identity())\n    return Seq(block, activation(**kargs))\n\n# https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\ndef ResNet(blocksList, extra = [], bias = False, **kargs):\n\n    layers = []\n    in_planes = 64\n    planes = 64\n    stride = 0\n    for num_blocks in blocksList:\n        if stride < 2:\n            stride += 1\n\n        strides = [stride] + [1]*(num_blocks-1)\n        for stride in strides:\n            layers.append(BasicBlock(in_planes, planes, stride, bias = bias, **kargs))\n            in_planes = planes\n        planes *= 2\n\n    print(""RESlayers: "", len(layers))\n    for e,l in extra:\n        layers[l] = Seq(layers[l], e)\n    \n    return Seq(Conv(64, kernel_size=3, stride=1, padding = 1, bias=bias, normal=True, printShape=True), \n               *layers)\n\n\n\ndef DenseNet(growthRate, depth, reduction, num_classes, bottleneck = True):\n\n    def Bottleneck(growthRate):\n        interChannels = 4*growthRate\n\n        n = Seq( ReLU(), \n                 Conv2D(interChannels, kernel_size=1, bias=True, ibp_init = True),\n                 ReLU(),\n                 Conv2D(growthRate, kernel_size=3, padding=1, bias=True, ibp_init = True)\n                 ) \n\n        return Skip(Identity(), n)\n\n    def SingleLayer(growthRate):\n        n = Seq( ReLU(), \n                 Conv2D(growthRate, kernel_size=3, padding=1, bias=True, ibp_init = True))\n        return Skip(Identity(), n)\n\n    def Transition(nOutChannels):\n        return Seq( ReLU(),\n                    Conv2D(nOutChannels, kernel_size = 1, bias = True, ibp_init = True),\n                    AvgPool2D(kernel_size=2))\n\n    def make_dense(growthRate, nDenseBlocks, bottleneck):\n        return Seq(*[Bottleneck(growthRate) if bottleneck else SingleLayer(growthRate) for i in range(nDenseBlocks)])\n\n    nDenseBlocks = (depth-4) // 3\n    if bottleneck:\n        nDenseBlocks //= 2\n\n    nChannels = 2*growthRate\n    conv1 = Conv2D(nChannels, kernel_size=3, padding=1, bias=True, ibp_init = True)\n    dense1 = make_dense(growthRate, nDenseBlocks, bottleneck)\n    nChannels += nDenseBlocks * growthRate\n    nOutChannels = int(math.floor(nChannels*reduction))\n    trans1 = Transition(nOutChannels)\n\n    nChannels = nOutChannels\n    dense2 = make_dense(growthRate, nDenseBlocks, bottleneck)\n    nChannels += nDenseBlocks*growthRate\n    nOutChannels = int(math.floor(nChannels*reduction))\n    trans2 = Transition(nOutChannels)\n    \n    nChannels = nOutChannels\n    dense3 = make_dense(growthRate, nDenseBlocks, bottleneck)\n\n    return Seq(conv1, dense1, trans1, dense2, trans2, dense3,\n               ReLU(),\n               AvgPool2D(kernel_size=8),\n               CorrelateAll(only_train=False, ignore_point = True),\n               Linear(num_classes, ibp_init = True))\n\n'"
convert.py,9,"b'import future\nimport builtins\nimport past\nimport six\n\nfrom timeit import default_timer as timer\nfrom datetime import datetime\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, utils\nfrom torch.utils.data import Dataset\n\nimport inspect\nfrom inspect import getargspec\nimport os\nimport helpers as h\nfrom helpers import Timer\nimport copy\nimport random\nfrom itertools import count\n\nfrom components import *\nimport models\n\nimport goals\nfrom goals import *\nimport math\n\nfrom torch.serialization import SourceChangeWarning\nimport warnings\n\n\nparser = argparse.ArgumentParser(description=\'Convert a pickled PyTorch DiffAI net to an abstract onyx net which returns the interval concretization around the final logits.  The first dimension of the output is the natural center, the second dimension is the lb, the third is the ub\',  formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'-n\', \'--net\', type=str, default=None, metavar=\'N\', help=\'Saved and pickled net to use, in pynet format\', required=True)\nparser.add_argument(\'-d\', \'--domain\', type=str, default=""Point()"", help=\'picks which abstract goals to use for testing.  Uses box.  Doesn\\\'t use time, so don\\\'t use Lin.  Unless point, should specify a width w.\')\nparser.add_argument(\'-b\', \'--batch-size\', type=int, default=1, help=\'The batch size to export.  Not sure this matters.\')\n\nparser.add_argument(\'-o\', \'--out\', type=str, default=""convert_out/"", metavar=\'F\', help=\'Where to save the net.\')\n\nparser.add_argument(\'--update-net\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=""should update test net"")\nparser.add_argument(\'--net-name\', type=str, choices = h.getMethodNames(models), default=None, help=""update test net name"")\n\nparser.add_argument(\'--save-name\', type=str, default=None, help=""name to save the net with.  Defaults to <domain>___<netfile-.pynet>.onyx"")\n\nparser.add_argument(\'-D\', \'--dataset\', choices = [n for (n,k) in inspect.getmembers(datasets, inspect.isclass) if issubclass(k, Dataset)]\n                    , default=""MNIST"", help=\'picks which dataset to use.\')\n\nparser.add_argument(\'--map-to-cpu\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=""map cuda operations in save back to cpu; enables to run on a computer without a GPU"")\n\nparser.add_argument(\'--tf-input\', type=h.str2bool, nargs=\'?\', const=True, default=False, help=""change the shape of the input data from batch-channels-height-width (standard in pytroch) to batch-height-width-channels (standard in tf)"")\n\nargs = parser.parse_args()\n\nout_dir = args.out\n\nif not os.path.exists(out_dir):\n    os.makedirs(out_dir)\n\nwith warnings.catch_warnings(record=True) as w:\n    warnings.simplefilter(""always"", SourceChangeWarning)\n    if args.map_to_cpu:\n        net = torch.load(args.net, map_location=\'cpu\')\n    else:\n        net = torch.load(args.net)\n\nnet_name = None\n\nif args.net_name is not None:\n    net_name = args.net_name\nelif args.update_net and \'name\' in dir(net):\n    net_name = net.name\n    \n\ndef buildNet(n, input_dims, num_classes):\n    n = n(num_classes)\n    if args.dataset in [""MNIST""]:\n        n = Seq(Normalize([0.1307], [0.3081] ), n)\n    elif args.dataset in [""CIFAR10"", ""CIFAR100""]:\n        n = Seq(Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]), n)\n    elif dataset in [""SVHN""]:\n        n = Seq(Normalize([0.5,0.5,0.5], [0.2, 0.2, 0.2]), n)\n    elif dataset in [""Imagenet12""]:\n        n = Seq(Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]), n)\n\n    n = n.infer(input_dims)\n    n.clip_norm()\n    return n\n\n\nif net_name is not None:\n    n = getattr(models,net_name)\n    n = buildNet(n, net.inShape, net.outShape)\n    n.load_state_dict(net.state_dict())\n    net = n\n\nnet = net.to(h.device)\nnet.remove_norm()\n\ndomain = eval(args.domain)\n\nif args.save_name is None:\n    save_name = h.prepareDomainNameForFile(args.domain) + ""___"" + os.path.basename(args.net)[:-6] + "".onyx""  \nelse:\n    save_name = args.save_name\n\ndef abstractNet(inpt):\n    if args.tf_input:\n        inpt = inpt.permute(0, 3, 1, 2)\n    dom = domain.box(inpt, w = None)\n    o = net(dom, onyx=True).unsqueeze(1)\n\n    out = torch.cat([o.vanillaTensorPart(), o.lb().vanillaTensorPart(), o.ub().vanillaTensorPart()], dim=1)\n    return out\n\ninput_shape = [args.batch_size] + list(net.inShape)\nif args.tf_input:\n    input_shape = [args.batch_size] + list(net.inShape)[1:]  + [net.inShape[0]]\ndummy = h.zeros(input_shape)\n\nabstractNet(dummy)\n\nclass AbstractNet(nn.Module):\n    def __init__(self, domain, net, abstractNet):\n        super(AbstractNet, self).__init__()\n        self.net = net\n        self.abstractNet = abstractNet\n        if hasattr(domain, ""net"") and domain.net is not None:\n            self.netDom = domain.net\n\n    def forward(self, inpt):\n        return self.abstractNet(inpt)\n\nabsNet = AbstractNet(domain, net, abstractNet)\n\nout_path = os.path.join(out_dir,  save_name)\nprint(""Saving:"", out_path)\n\nparam_list = [""param""+str(i) for i in range(len(list(absNet.parameters())))]\n\ntorch.onnx.export(absNet, dummy, out_path, verbose=False, input_names=[""actual_input""] + param_list, output_names=[""output""])\n\n'"
goals.py,17,"b'import future\nimport builtins\nimport past\nimport six\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd\nimport components as comp\nfrom torch.distributions import multinomial, categorical\n\nimport math\nimport numpy as np\n\ntry:\n    from . import helpers as h\n    from . import ai\n    from . import scheduling as S\nexcept:\n    import helpers as h\n    import ai\n    import scheduling as S\n\n\n\nclass WrapDom(object):\n    def __init__(self, a):\n        self.a = eval(a) if type(a) is str else a\n\n    def box(self, *args, **kargs):\n        return self.Domain(self.a.box(*args, **kargs))\n\n    def boxBetween(self, *args, **kargs):\n        return self.Domain(self.a.boxBetween(*args, **kargs))\n\n    def line(self, *args, **kargs):\n        return self.Domain(self.a.line(*args, **kargs))\n\nclass DList(object):\n    Domain = ai.ListDomain\n    class MLoss():\n        def __init__(self, aw):\n            self.aw = aw\n        def loss(self, dom, *args, lr = 1, **kargs):\n            if self.aw <= 0.0:\n                return 0\n            return self.aw * dom.loss(*args, lr = lr * self.aw, **kargs)\n\n    def __init__(self, *al):\n        if len(al) == 0:\n            al = [(""Point()"", 1.0), (""Box()"", 0.1)]\n\n        self.al = [(eval(a) if type(a) is str else a, S.Const.initConst(aw)) for a,aw in al]\n\n    def getDiv(self, **kargs):\n        return 1.0 / sum(aw.getVal(**kargs) for _,aw in self.al)\n\n    def box(self, *args, **kargs):\n        m = self.getDiv(**kargs)\n        return self.Domain(ai.TaggedDomain(a.box(*args, **kargs), DList.MLoss(aw.getVal(**kargs) * m)) for a,aw in self.al)\n\n    def boxBetween(self, *args, **kargs):\n        \n        m = self.getDiv(**kargs)\n        return self.Domain(ai.TaggedDomain(a.boxBetween(*args, **kargs), DList.MLoss(aw.getVal(**kargs) * m)) for a,aw in self.al)\n\n    def line(self, *args, **kargs):\n        m = self.getDiv(**kargs)\n        return self.Domain(ai.TaggedDomain(a.line(*args, **kargs), DList.MLoss(aw.getVal(**kargs) * m)) for a,aw in self.al)\n        \n    def __str__(self):\n        return ""DList(%s)"" % h.sumStr(""(""+str(a)+"",""+str(w)+"")"" for a,w in self.al)\n\nclass Mix(DList):\n    def __init__(self, a=""Point()"", b=""Box()"", aw = 1.0, bw = 0.1):\n        super(Mix, self).__init__((a,aw), (b,bw))\n\nclass LinMix(DList):\n    def __init__(self, a=""Point()"", b=""Box()"", bw = 0.1):\n        super(LinMix, self).__init__((a,S.Complement(bw)), (b,bw))\n\nclass DProb(object):\n    def __init__(self, *doms):\n        if len(doms) == 0:\n            doms = [(""Point()"", 0.8), (""Box()"", 0.2)]\n        div = 1.0 / sum(float(aw) for _,aw in doms)\n        self.domains = [eval(a) if type(a) is str else a for a,_ in doms]\n        self.probs = [ div * float(aw)  for _,aw in doms]\n\n    def chooseDom(self):\n        return self.domains[np.random.choice(len(self.domains), p = self.probs)] if len(self.domains) > 1 else self.domains[0]\n\n    def box(self, *args, **kargs):\n        domain = self.chooseDom()\n        return domain.box(*args, **kargs)\n\n    def line(self, *args, **kargs):\n        domain = self.chooseDom()\n        return domain.line(*args, **kargs)\n\n    def __str__(self):\n        return ""DProb(%s)"" % h.sumStr(""(""+str(a)+"",""+str(w)+"")"" for a,w in zip(self.domains, self.probs))\n\nclass Coin(DProb):\n    def __init__(self, a=""Point()"", b=""Box()"", ap = 0.8, bp = 0.2):\n        super(Coin, self).__init__((a,ap), (b,bp))\n\nclass Point(object):\n    Domain = h.dten\n    def __init__(self, **kargs):\n        pass\n\n    def box(self, original, *args, **kargs):\n        return original\n\n    def line(self, original, other, *args, **kargs): \n        return (original + other) / 2\n\n    def boxBetween(self, o1, o2, *args, **kargs):\n        return (o1 + o2) / 2\n\n    def __str__(self):\n        return ""Point()""\n\nclass PointA(Point):\n    def boxBetween(self, o1, o2, *args, **kargs):\n        return o1\n\n    def __str__(self):\n        return ""PointA()""\n\nclass PointB(Point):\n    def boxBetween(self, o1, o2, *args, **kargs):\n        return o2\n\n    def __str__(self):\n        return ""PointB()""\n\n\nclass NormalPoint(Point):\n    def __init__(self, w = None, **kargs):\n        self.epsilon = w\n        \n    def box(self, original, w, *args, **kargs):\n        """""" original = mu = mean, epsilon = variance""""""\n        if not self.epsilon is None:\n            w = self.epsilon\n\n        inter = torch.randn_like(original, device = h.device) * w\n        return original + inter\n\n    def __str__(self):\n        return ""NormalPoint(%s)"" % ("""" if self.epsilon is None else str(self.epsilon))\n\n\n\nclass MI_FGSM(Point):\n\n    def __init__(self, w = None, r = 20.0, k = 100, mu = 0.8, should_end = True, restart = None, searchable=False,**kargs):\n        self.epsilon = S.Const.initConst(w)\n        self.k = k\n        self.mu = mu\n        self.r = float(r)\n        self.should_end = should_end\n        self.restart = restart\n        self.searchable = searchable\n\n    def box(self, original, model, target = None, untargeted = False, **kargs):\n        if target is None:\n            untargeted = True\n            with torch.no_grad():\n                target = model(original).max(1)[1]\n        return self.attack(model, original, untargeted, target, **kargs)\n\n    def boxBetween(self, o1, o2, model, target = None, *args, **kargs):\n        return self.attack(model, (o1 - o2).abs() / 2, (o1 + o2) / 2, target, **kargs)\n\n\n    def attack(self, model, xo, untargeted, target, w, loss_function=ai.stdLoss, **kargs):\n        w = self.epsilon.getVal(c = w, **kargs)\n\n        x = nn.Parameter(xo.clone(), requires_grad=True)\n        gradorg = h.zeros(x.shape)\n        is_eq = 1\n\n        w = h.ones(x.shape) * w\n        for i in range(self.k):\n            if self.restart is not None and i % int(self.k / self.restart) == 0:\n                x = is_eq * (torch.rand_like(xo) * w + xo) + (1 - is_eq) * x\n                x = nn.Parameter(x, requires_grad = True)\n\n            model.optimizer.zero_grad()\n\n            out = model(x).vanillaTensorPart()\n            loss = loss_function(out, target)\n\n            loss.sum().backward(retain_graph=True)\n            with torch.no_grad():\n                oth = x.grad / torch.norm(x.grad, p=1)\n                gradorg *= self.mu \n                gradorg += oth\n                grad = (self.r * w / self.k) * ai.mysign(gradorg)\n                if self.should_end:\n                    is_eq = ai.mulIfEq(grad, out, target)\n                x = (x + grad * is_eq) if untargeted else (x - grad * is_eq)\n\n                x = xo + torch.min(torch.max(x - xo, -w),w)\n                x.requires_grad_()\n\n        model.optimizer.zero_grad()\n\n        return x\n\n    def boxBetween(self, o1, o2, model, target, *args, **kargs):\n        raise ""Not boxBetween is not yet supported by MI_FGSM""\n\n    def __str__(self):\n        return ""MI_FGSM(%s)"" % (("""" if self.epsilon is None else ""w=""+str(self.epsilon)+"","")\n                                + ("""" if self.k == 5 else ""k=""+str(self.k)+"","")\n                                + ("""" if self.r == 5.0 else ""r=""+str(self.r)+"","")\n                                + ("""" if self.mu == 0.8 else ""r=""+str(self.mu)+"","")\n                                + ("""" if self.should_end else ""should_end=False""))\n\n\nclass PGD(MI_FGSM):\n    def __init__(self, r = 5.0, k = 5, **kargs):\n        super(PGD,self).__init__(r=r, k = k, mu = 0, **kargs)\n\n    def __str__(self):\n        return ""PGD(%s)"" % (("""" if self.epsilon is None else ""w=""+str(self.epsilon)+"","")\n                            + ("""" if self.k == 5 else ""k=""+str(self.k)+"","")\n                            + ("""" if self.r == 5.0 else ""r=""+str(self.r)+"","")\n                            + ("""" if self.should_end else ""should_end=False""))\n\nclass IFGSM(PGD):\n\n    def __init__(self, k = 5, **kargs):\n        super(IFGSM, self).__init__(r = 1, k=k, **kargs)\n\n    def __str__(self):\n        return ""IFGSM(%s)"" % (("""" if self.epsilon is None else ""w=""+str(self.epsilon)+"","")\n                              + ("""" if self.k == 5 else ""k=""+str(self.k)+"","")\n                              + ("""" if self.should_end else ""should_end=False""))\n\nclass NormalAdv(Point): \n    def __init__(self, a=""IFGSM()"", w = None):\n        self.a = (eval(a) if type(a) is str else a)\n        self.epsilon = S.Const.initConst(w)\n\n    def box(self, original, w, *args, **kargs):\n        epsilon = self.epsilon.getVal(c = w, shape = original.shape[:1], **kargs)\n        assert (0 <= h.dten(epsilon)).all()\n        epsilon = torch.randn(original.size()[0:1], device = h.device)[0] * epsilon\n        return self.a.box(original, w = epsilon, *args, **kargs)\n\n    def __str__(self):\n        return ""NormalAdv(%s)"" % ( str(self.a) + ("""" if self.epsilon is None else "",w=""+str(self.epsilon)))\n\n\nclass InclusionSample(Point):\n    def __init__(self, sub, a=""Box()"", normal = False, w = None, **kargs):\n        self.sub = S.Const.initConst(sub)  # sub is the fraction of w to use.\n        self.w = S.Const.initConst(w)\n        self.normal = normal\n        self.a = (eval(a) if type(a) is str else a)\n\n    def box(self, original, w, *args, **kargs):\n        w = self.w.getVal(c = w, shape = original.shape[:1], **kargs)\n        sub = self.sub.getVal(c = 1, shape = original.shape[:1], **kargs)\n\n        assert (0 <= h.dten(w)).all()\n        assert (h.dten(sub) <= 1).all()\n        assert (0 <= h.dten(sub)).all() \n        if self.normal:\n            inter = torch.randn_like(original, device = h.device)\n        else:\n            inter = (torch.rand_like(original, device = h.device) * 2 - 1) \n\n        inter = inter * w * (1 - sub)\n        \n        return self.a.box(original + inter, w = w * sub, *args, **kargs)\n\n    def boxBetween(self, o1, o2, *args, **kargs):\n        w = (o2 - o1).abs()\n        return self.box( (o2 + o1)/2 , w = w, *args, **kargs)\n\n    def __str__(self):\n        return ""InclusionSample(%s, %s)"" % (str(self.sub), str(self.a) + ("""" if self.epsilon is None else "",w=""+str(self.epsilon)))\n\nInSamp = InclusionSample\n\n\nclass AdvInclusion(InclusionSample):\n    def __init__(self, sub, a=""IFGSM()"", b=""Box()"", w = None, **kargs):\n        self.sub = S.Const.initConst(sub)  # sub is the fraction of w to use.\n        self.w = S.Const.initConst(w)\n        self.a = (eval(a) if type(a) is str else a)\n        self.b = (eval(b) if type(b) is str else b)\n\n    def box(self, original, w, *args, **kargs):\n        w = self.w.getVal(c = w, shape = original.shape, **kargs)\n        sub = self.sub.getVal(c = 1, shape = original.shape, **kargs)\n\n        assert (0 <= h.dten(w)).all()\n        assert (h.dten(sub) <= 1).all()\n        assert (0 <= h.dten(sub)).all() \n\n        if h.dten(w).sum().item() <= 0.0:\n            inter = original\n        else:\n            inter = self.a.box(original, w = w * (1 - sub), *args, **kargs)\n\n        return self.b.box(inter, w = w * sub, *args, **kargs)\n\n    def __str__(self):\n        return ""AdvInclusion(%s, %s, %s)"" % (str(self.sub), str(self.a), str(self.b) + ("""" if self.epsilon is None else "",w=""+str(self.epsilon)))\n\n\nclass AdvDom(Point):\n    def __init__(self, a=""IFGSM()"", b=""Box()""):\n        self.a = (eval(a) if type(a) is str else a)\n        self.b = (eval(b) if type(b) is str else b)\n\n    def box(self, original,*args, **kargs):\n        adv = self.a.box(original, *args, **kargs)\n        return self.b.boxBetween(original, adv.ub(), *args, **kargs)\n\n    def boxBetween(self, o1, o2, *args, **kargs):\n        original = (o1 + o2) / 2\n        adv = self.a.boxBetween(o1, o2, *args, **kargs)\n        return self.b.boxBetween(original, adv.ub(), *args, **kargs)\n\n    def __str__(self):\n        return ""AdvDom(%s)"" % (("""" if self.width is None else ""width=""+str(self.width)+"","")\n                               + str(self.a) + "","" + str(self.b))\n\n\n\nclass BiAdv(AdvDom):\n    def box(self, original, **kargs):\n        adv = self.a.box(original, **kargs)\n        extreme = (adv.ub() - original).abs()\n        return self.b.boxBetween(original - extreme, original + extreme, **kargs)\n    \n    def boxBetween(self, o1, o2, *args, **kargs):\n        original = (o1 + o2) / 2\n        adv = self.a.boxBetween(o1, o2, *args, **kargs)\n        extreme = (adv.ub() - original).abs()\n        return self.b.boxBetween(original - extreme, original + extreme, *args, **kargs)\n\n    def __str__(self):\n        return ""BiAdv"" + AdvDom.__str__(self)[6:]\n\n\nclass HBox(object):\n    Domain = ai.HybridZonotope\n\n    def domain(self, *args, **kargs):\n        return ai.TaggedDomain(self.Domain(*args, **kargs), self)\n\n    def __init__(self, w = None, tot_weight = 1, width_weight = 0, pow_loss = None, log_loss = False, searchable = True, cross_loss = True, **kargs):\n        self.w = S.Const.initConst(w)\n        self.tot_weight = S.Const.initConst(tot_weight)\n        self.width_weight = S.Const.initConst(width_weight)\n        self.pow_loss = pow_loss\n        self.searchable = searchable\n        self.log_loss = log_loss\n        self.cross_loss = cross_loss\n\n    def __str__(self):\n        return ""HBox(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n\n    def boxBetween(self, o1, o2,  *args, **kargs):\n        batches = o1.size()[0]\n        num_elem = h.product(o1.size()[1:])\n        ei = h.getEi(batches, num_elem)\n        \n        if len(o1.size()) > 2:\n            ei = ei.contiguous().view(num_elem, *o1.size())\n\n        return self.domain((o1 + o2) / 2, None, ei * (o2 - o1).abs() / 2).checkSizes()\n\n    def box(self, original, w, **kargs):\n        """"""\n        This version of it is slow, but keeps correlation down the line.\n        """"""\n        radius = self.w.getVal(c = w, **kargs)\n\n        batches = original.size()[0]\n        num_elem = h.product(original.size()[1:])\n        ei = h.getEi(batches,num_elem)\n        \n        if len(original.size()) > 2:\n            ei = ei.contiguous().view(num_elem, *original.size())\n\n        return self.domain(original, None, ei * radius).checkSizes()\n\n    def line(self, o1, o2, **kargs):\n        w = self.w.getVal(c = 0, **kargs)\n\n        ln = ((o2 - o1) / 2).unsqueeze(0)\n        if not w is None and w > 0.0:\n            batches = o1.size()[0]\n            num_elem = h.product(o1.size()[1:])\n            ei = h.getEi(batches,num_elem)\n            if len(o1.size()) > 2:\n                ei = ei.contiguous().view(num_elem, *o1.size())\n            ln = torch.cat([ln, ei * w])\n        return self.domain((o1 + o2) / 2, None, ln ).checkSizes()\n\n    def loss(self, dom, target, *args, **kargs):\n        width_weight = self.width_weight.getVal(**kargs)\n        tot_weight = self.tot_weight.getVal(**kargs)\n        \n        if self.cross_loss:\n            r = dom.ub()\n            inds = torch.arange(r.shape[0], device=h.device, dtype=h.ltype)\n            r[inds,target] = dom.lb()[inds,target]\n            tot = r.loss(target, *args, **kargs)\n        else:\n            tot = dom.loss(target, *args, **kargs)\n\n        if self.log_loss:\n            tot = (tot + 1).log()\n        if self.pow_loss is not None and self.pow_loss > 0 and self.pow_loss != 1:\n            tot = tot.pow(self.pow_loss)\n\n        ls = tot * tot_weight\n        if width_weight > 0:\n            ls += dom.diameter() * width_weight\n\n        return ls / (width_weight + tot_weight)\n\nclass Box(HBox):\n    def __str__(self):\n        return ""Box(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n\n    def box(self, original, w, **kargs):  \n        """"""\n        This version of it takes advantage of betas being uncorrelated.  \n        Unfortunately they stay uncorrelated forever.  \n        Counterintuitively, tests show more accuracy - this is because the other box\n        creates lots of 0 errors which get accounted for by the calcultion of the newhead in relu \n        which is apparently worse than not accounting for errors.\n        """"""\n        radius = self.w.getVal(c = w, **kargs)\n        return self.domain(original, h.ones(original.size()) * radius, None).checkSizes()\n    \n    def line(self, o1, o2, **kargs):\n        w = self.w.getVal(c = 0, **kargs)\n        return self.domain((o1 + o2) / 2, ((o2 - o1) / 2).abs() + h.ones(o2.size()) * w, None).checkSizes()\n\n    def boxBetween(self, o1, o2, *args, **kargs):\n        return self.line(o1, o2, **kargs)\n\nclass ZBox(HBox):\n\n    def __str__(self):\n        return ""ZBox(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n\n    def Domain(self, *args, **kargs):\n        return ai.Zonotope(*args, **kargs)\n\nclass HSwitch(HBox):\n    def __str__(self):\n        return ""HSwitch(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n    \n    def Domain(self, *args, **kargs):\n        return ai.HybridZonotope(*args, customRelu = ai.creluSwitch, **kargs)\n    \nclass ZSwitch(ZBox):\n\n    def __str__(self):\n        return ""ZSwitch(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n    def Domain(self, *args, **kargs):\n        return ai.Zonotope(*args, customRelu = ai.creluSwitch, **kargs)\n\n\nclass ZNIPS(ZBox):\n\n    def __str__(self):\n        return ""ZSwitch(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n\n    def Domain(self, *args, **kargs):\n        return ai.Zonotope(*args, customRelu = ai.creluNIPS, **kargs)\n    \nclass HSmooth(HBox):\n    def __str__(self):\n        return ""HSmooth(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n\n    def Domain(self, *args, **kargs):\n        return ai.HybridZonotope(*args, customRelu = ai.creluSmooth, **kargs)\n    \nclass HNIPS(HBox):\n    def __str__(self):\n        return ""HSmooth(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n\n    def Domain(self, *args, **kargs):\n        return ai.HybridZonotope(*args, customRelu = ai.creluNIPS, **kargs)\n\nclass ZSmooth(ZBox):\n    def __str__(self):\n        return ""ZSmooth(%s)"" % ("""" if self.w is None else ""w=""+str(self.w))\n\n    def Domain(self, *args, **kargs):\n        return ai.Zonotope(*args, customRelu = ai.creluSmooth, **kargs)\n\n\n\n\n\n# stochastic correlation\nclass HRand(WrapDom):\n    # domain must be an ai style domain like hybrid zonotope.\n    def __init__(self, num_correlated, a = ""HSwitch()"", **kargs):\n        super(HRand, self).__init__(Box())\n        self.num_correlated = num_correlated\n        self.dom = eval(a) if type(a) is str else a\n        \n    def Domain(self, d):\n        with torch.no_grad():\n            out = d.abstractApplyLeaf(\'stochasticCorrelate\', self.num_correlated)\n            out = self.dom.Domain(out.head, out.beta, out.errors)\n        return out\n\n    def __str__(self):\n        return ""HRand(%s, domain = %s)"" % (str(self.num_correlated), str(self.a))\n'"
helpers.py,40,"b'import future\nimport builtins\nimport past\nimport six\nimport inspect\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport argparse\nimport decimal\nimport PIL\nfrom torchvision import datasets, transforms\nfrom datetime import datetime\n\nfrom forbiddenfruit import curse\n#from torch.autograd import Variable\n\nfrom timeit import default_timer as timer\n\nclass Timer:\n    def __init__(self, activity = None, units = 1, shouldPrint = True, f = None):\n        self.activity = activity\n        self.units = units\n        self.shouldPrint = shouldPrint\n        self.f = f\n    def __enter__(self):\n        self.start = timer()\n        return self\n    def getUnitTime(self):\n        return (self.end - self.start) / self.units\n\n    def __str__(self):\n        return ""Avg time to "" + self.activity + "": ""+str(self.getUnitTime())\n\n    def __exit__(self, *args):\n        self.end = timer()\n        if self.shouldPrint:\n            printBoth(self, f = self.f)\n            \ndef cudify(x):\n    if use_cuda:\n        return x.cuda(async=True)\n    return x\n\ndef pyval(a, **kargs):\n    return dten([a], **kargs)\n\ndef ifThenElse(cond, a, b):\n    cond = cond.to_dtype()\n    return cond * a + (1 - cond) * b\n\ndef ifThenElseL(cond, a, b):\n    return cond * a + (1 - cond) * b\n\ndef product(it):\n    if isinstance(it,int):\n        return it\n    product = 1\n    for x in it:\n        if x >= 0:\n            product *= x\n    return product\n\ndef getEi(batches, num_elem):\n    return eye(num_elem).expand(batches, num_elem,num_elem).permute(1,0,2)\n\ndef one_hot(batch,d):\n    bs = batch.size()[0]\n    indexes = [ list(range(bs)), batch]\n    values = [ 1 for _ in range(bs) ]\n    return cudify(torch.sparse.FloatTensor(ltenCPU(indexes), ftenCPU(values), torch.Size([bs,d])))\n\ndef seye(n, m = None): \n    if m is None:\n        m = n\n    mn = n if n < m else m\n    indexes = [[ i for i in range(mn) ], [ i  for i in range(mn) ] ]\n    values = [1 for i in range(mn) ]\n    return cudify(torch.sparse.ByteTensor(ltenCPU(indexes), dtenCPU(values), torch.Size([n,m])))\n\ndtype = torch.float32\nftype = torch.float32\nltype = torch.int64\nbtype = torch.uint8\n\ntorch.set_default_dtype(dtype)\n\ncpu = torch.device(""cpu"")\n\ncuda_async = True\n\nftenCPU = lambda *args, **kargs: torch.tensor(*args, dtype=ftype, device=cpu, **kargs)\ndtenCPU = lambda *args, **kargs: torch.tensor(*args, dtype=dtype, device=cpu, **kargs)\nltenCPU = lambda *args, **kargs: torch.tensor(*args, dtype=ltype, device=cpu, **kargs)\nbtenCPU = lambda *args, **kargs: torch.tensor(*args, dtype=btype, device=cpu, **kargs)\n\nif torch.cuda.is_available() and not \'NOCUDA\' in os.environ:\n    print(""using cuda"")\n    device = torch.device(""cuda"")\n    ften = lambda *args, **kargs: torch.tensor(*args, dtype=ftype, device=device, **kargs).cuda(non_blocking=cuda_async)\n    dten = lambda *args, **kargs: torch.tensor(*args, dtype=dtype, device=device, **kargs).cuda(non_blocking=cuda_async)\n    lten = lambda *args, **kargs: torch.tensor(*args, dtype=ltype, device=device, **kargs).cuda(non_blocking=cuda_async)\n    bten = lambda *args, **kargs: torch.tensor(*args, dtype=btype, device=device, **kargs).cuda(non_blocking=cuda_async)\n    ones = lambda *args, **cargs: torch.ones(*args, **cargs).cuda(non_blocking=cuda_async)\n    zeros = lambda *args, **cargs: torch.zeros(*args, **cargs).cuda(non_blocking=cuda_async)\n    eye = lambda *args, **cargs: torch.eye(*args, **cargs).cuda(non_blocking=cuda_async)\n    use_cuda = True\n    print(""set up cuda"")\nelse:\n    print(""not using cuda"")\n    ften = ftenCPU\n    dten = dtenCPU\n    lten = ltenCPU\n    bten = btenCPU\n    ones = torch.ones\n    zeros = torch.zeros\n    eye = torch.eye\n    use_cuda = False\n    device = cpu\n\ndef smoothmax(x, alpha, dim = 0):\n    return x.mul(F.softmax(x * alpha, dim)).sum(dim + 1)\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\n\ndef flat(lst):\n    lst_ = []\n    for l in lst:\n        lst_ += l\n    return lst_\n\n\ndef printBoth(*st, f = None):\n    print(*st)\n    if not f is None:\n        print(*st, file=f)\n\n\ndef hasMethod(cl, mt):\n    return callable(getattr(cl, mt, None))\n\ndef getMethodNames(Foo): \n    return [func for func in dir(Foo) if callable(getattr(Foo, func)) and not func.startswith(""__"")]\n\ndef getMethods(Foo): \n    return [getattr(Foo, m) for m in getMethodNames(Foo)]\n\nmax_c_for_norm = 10000\n\ndef numel(arr):\n    return product(arr.size())\n\ndef chunks(l, n):\n    """"""Yield successive n-sized chunks from l.""""""\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\n\ndef loadDataset(dataset, batch_size, train, transform = True):\n    oargs = {}\n    if dataset in [""MNIST"", ""CIFAR10"", ""CIFAR100"", ""FashionMNIST"", ""PhotoTour""]:\n        oargs[\'train\'] = train\n    elif dataset in [""STL10"", ""SVHN""] :\n        oargs[\'split\'] = \'train\' if train else \'test\'\n    elif dataset in [""LSUN""]:\n        oargs[\'classes\'] = \'train\' if train else \'test\'\n    elif dataset in [""Imagenet12""]:\n        pass\n    else:\n        raise Exception(dataset + "" is not yet supported"")\n\n    if dataset in [""MNIST""]:\n        transformer = transforms.Compose([ transforms.ToTensor()]\n                                         + ([transforms.Normalize((0.1307,), (0.3081,))] if transform else []))\n    elif dataset in [""CIFAR10"", ""CIFAR100""]:\n        transformer = transforms.Compose(([ #transforms.RandomCrop(32, padding=4), \n                                            transforms.RandomAffine(0, (0.125, 0.125), resample=PIL.Image.BICUBIC) ,\n                                            transforms.RandomHorizontalFlip(), \n                                            #transforms.RandomRotation(15, resample = PIL.Image.BILINEAR) \n                                          ] if train else [])\n                                         + [transforms.ToTensor()] \n                                         + ([transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))] if transform else []))\n    elif dataset in [""SVHN""]:\n        transformer = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5,0.5,0.5), (0.2,0.2,0.2))])\n    else:\n        transformer = transforms.ToTensor()\n\n    if dataset in [""Imagenet12""]:\n        # https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset\n        train_set = datasets.ImageFolder(\n            \'../data/Imagenet12/train\' if train else \'../data/Imagenet12/val\',\n            transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(),\n                normalize,\n            ]))\n    else:\n        train_set = getattr(datasets, dataset)(\'../data\', download=True, transform=transformer, **oargs)\n    return torch.utils.data.DataLoader(\n        train_set\n        , batch_size=batch_size\n        , shuffle=True, \n        **({\'num_workers\': 1, \'pin_memory\': True} if use_cuda else {}))\n\n\ndef variable(Pt):\n    class Point:\n        def isSafe(self,target):\n            pred = self.max(1, keepdim=True)[1] # get the index of the max log-probability\n            return pred.eq(target.data.view_as(pred))\n\n        def isPoint(self):\n            return True\n\n        def labels(self):\n            return [self[0].max(1)[1]] # get the index of the max log-probability\n            \n        def softplus(self): \n            return F.softplus(self)\n\n        def elu(self): \n            return F.elu(self)\n\n        def selu(self): \n            return F.selu(self)\n\n        def sigm(self): \n            return F.sigmoid(self)\n        \n        def conv3d(self, *args, **kargs): \n            return F.conv3d(self, *args, **kargs)\n        def conv2d(self, *args, **kargs): \n            return F.conv2d(self, *args, **kargs)\n        def conv1d(self, *args, **kargs): \n            return F.conv1d(self, *args, **kargs)\n\n        def conv_transpose3d(self, *args, **kargs): \n            return F.conv_transpose3d(self, *args, **kargs)\n        def conv_transpose2d(self, *args, **kargs): \n            return F.conv_transpose2d(self, *args, **kargs)\n        def conv_transpose1d(self, *args, **kargs): \n            return F.conv_transpose1d(self, *args, **kargs)\n\n        def max_pool2d(self, *args, **kargs): \n            return F.max_pool2d(self, *args, **kargs)\n\n        def avg_pool2d(self, *args, **kargs): \n            return F.avg_pool2d(self, *args, **kargs)\n\n        def adaptive_avg_pool2d(self, *args, **kargs): \n            return F.adaptive_avg_pool2d(self, *args, **kargs)\n\n\n        def cat(self, other, dim = 0, **kargs): \n            return torch.cat((self, other), dim = dim, **kargs)\n\n        def addPar(self, a, b):\n            return a + b\n\n        def abstractApplyLeaf(self, foo, *args, **kargs):\n            return self\n        \n        def diameter(self): \n            return pyval(0)\n\n        def to_dtype(self): \n            return self.type(dtype=dtype, non_blocking=cuda_async)\n\n        def loss(self, target, **kargs):\n            if torch.__version__[0] == ""0"":\n                return F.cross_entropy(self, target, reduce = False)\n            else:\n                return F.cross_entropy(self, target, reduction=\'none\')\n\n        def deep_loss(self, *args, **kargs):\n            return 0\n\n        def merge(self, *args, **kargs):\n            return self\n\n        def splitRelu(self, *args, **kargs):\n            return self\n\n        def lb(self): \n            return self\n        def vanillaTensorPart(self):\n            return self\n        def center(self): \n            return self\n        def ub(self): \n            return self\n\n        def cudify(self, cuda_async = True):\n            return self.cuda(non_blocking=cuda_async) if use_cuda else self\n    \n    def log_softmax(self, *args, dim = 1, **kargs): \n        return F.log_softmax(self, *args,dim = dim, **kargs)       \n\n    if torch.__version__[0] == ""0"" and torch.__version__ != ""0.4.1"":\n        Point.log_softmax = log_softmax\n\n\n    def log_softmax(self, *args, dim = 1, **kargs): \n        return F.log_softmax(self, *args,dim = dim, **kargs)       \n\n    if torch.__version__[0] == ""0"" and torch.__version__ != ""0.4.1"":\n        Point.log_softmax = log_softmax\n\n    for nm in getMethodNames(Point):\n        curse(Pt, nm, getattr(Point, nm))\n\nvariable(torch.autograd.Variable)\nvariable(torch.cuda.DoubleTensor)\nvariable(torch.DoubleTensor)\nvariable(torch.cuda.FloatTensor)\nvariable(torch.FloatTensor)\nvariable(torch.ByteTensor)\nvariable(torch.Tensor)\n\n\ndef default(dic, nm, d):\n    if dic is not None and nm in dic:\n        return dic[nm]\n    return d\n\n\n\n\ndef softmaxBatchNP(x, epsilon, subtract = False):\n    """"""Compute softmax values for each sets of scores in x.""""""\n    x = x.astype(np.float64)\n    ex = x / epsilon if epsilon is not None else x\n    if subtract:\n        ex -= ex.max(axis=1)[:,np.newaxis]    \n    e_x = np.exp(ex)\n    sm = (e_x / e_x.sum(axis=1)[:,np.newaxis])\n    am = np.argmax(x, axis=1)\n    bads = np.logical_not(np.isfinite(sm.sum(axis = 1)))\n\n    if epsilon is None:\n        sm[bads] = 0\n        sm[bads, am[bads]] = 1\n    else:\n        epsilon *= (x.shape[1] - 1) / x.shape[1]\n        sm[bads] = epsilon / (x.shape[1] - 1)\n        sm[bads, am[bads]] = 1 - epsilon\n\n    sm /= sm.sum(axis=1)[:,np.newaxis]\n    return sm\n\n\ndef cadd(a,b):\n    both = a.cat(b)\n    a, b = both.split(a.size()[0])\n    return a + b\n\ndef msum(a,b, l):\n    if a is None:\n        return b\n    if b is None:\n        return a\n    return l(a,b)\n\nclass SubAct(argparse.Action):\n    def __init__(self, sub_choices, *args, **kargs):\n        super(SubAct,self).__init__(*args, nargs=\'+\', **kargs)\n        self.sub_choices = sub_choices\n        self.sub_choices_names = None if sub_choices is None else getMethodNames(sub_choices)\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if self.sub_choices_names is not None and not values[0] in self.sub_choices_names:\n            msg = \'invalid choice: %r (choose from %s)\' % (values[0], self.sub_choices_names)\n            raise argparse.ArgumentError(self, msg)\n\n        prev = getattr(namespace, self.dest)\n        setattr(namespace, self.dest, prev + [values])\n\ndef catLists(val):\n    if isinstance(val, list):\n        v = []\n        for i in val:\n            v += catLists(i)\n        return v\n    return [val]\n\ndef sumStr(val):\n    s = """"\n    for v in val:\n        s += v\n    return s\n\ndef catStrs(val):\n    s = val[0]\n    if len(val) > 1:\n        s += ""(""\n    for v in val[1:2]:\n        s += v\n    for v in val[2:]:\n        s += "", ""+v\n    if len(val) > 1:\n        s += "")""\n    return s\n\ndef printNumpy(x):\n    return ""["" + sumStr([decimal.Decimal(float(v)).__format__(""f"") + "", "" for v in x.data.cpu().numpy()])[:-2]+""]""\n\ndef printStrList(x):\n    return ""["" + sumStr(v + "", "" for v in x)[:-2]+""]""\n\ndef printListsNumpy(val):\n    if isinstance(val, list):\n        return printStrList(printListsNumpy(v) for v in val)\n    return printNumpy(val)\n\ndef parseValues(values, methods, *others):\n    if len(values) == 1 and values[0]:\n        x = eval(values[0], dict(pair for l in ([methods] + list(others)) for pair in l.__dict__.items()) )\n\n        return x() if inspect.isclass(x) else x\n    args = []\n    kargs = {}\n    for arg in values[1:]:\n        if \'=\' in arg:\n            k = arg.split(\'=\')[0]\n            v = arg[len(k)+1:]\n            try:\n                kargs[k] = eval(v)\n            except:\n                kargs[k] = v\n        else:\n            args += [eval(arg)]\n    return getattr(methods, values[0])(*args, **kargs)\n\ndef preDomRes(outDom, target): # TODO: make faster again by keeping sparse tensors sparse\n    t = one_hot(target.long(), outDom.size()[1]).to_dense().to_dtype()\n    tmat = t.unsqueeze(2).matmul(t.unsqueeze(1))\n\n    tl = t.unsqueeze(2).expand(-1, -1, tmat.size()[1])\n\n    inv_t = eye(tmat.size()[1]).expand(tmat.size()[0], -1, -1)\n    inv_t = inv_t - tmat\n\n    tl = tl.bmm(inv_t)\n\n    fst = outDom.unsqueeze(1).matmul(tl).squeeze(1)\n    snd = outDom.unsqueeze(1).matmul(inv_t).squeeze(1)\n        \n    return (fst - snd) + t\n\ndef mopen(shouldnt, *args, **kargs):\n    if shouldnt:\n        import contextlib\n        return contextlib.suppress()\n    return open(*args, **kargs)\n        \ndef file_timestamp():\n    return str(datetime.now()).replace("":"","""").replace("" "", """")\n\ndef prepareDomainNameForFile(s):\n    return s.replace("" "", ""_"").replace("","", """").replace(""("", ""_"").replace("")"", ""_"").replace(""="", ""_"")\n\n# delimited only\ndef callCC(foo):\n    class RV(BaseException):\n        def __init__(self, v):\n            self.v = v\n\n    def cc(x):\n        raise RV(x)\n\n    try:\n        return foo(cc)\n    except RV as rv:\n        return rv.v\n'"
losses.py,5,"b'# This source file is part of DiffAI\n# Copyright (c) 2018 Secure, Reliable, and Intelligent Systems Lab (SRI), ETH Zurich\n# This software is distributed under the MIT License: https://opensource.org/licenses/MIT\n# SPDX-License-Identifier: MIT\n# For more information see https://github.com/eth-sri/diffai\n\n# THE SOFTWARE IS PROVIDED ""AS-IS"" WITHOUT ANY WARRANTY OF ANY KIND, EITHER\n# EXPRESS, IMPLIED OR STATUTORY, INCLUDING BUT NOT LIMITED TO ANY WARRANTY\n# THAT THE SOFTWARE WILL CONFORM TO SPECIFICATIONS OR BE ERROR-FREE AND ANY\n# IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE,\n# TITLE, OR NON-INFRINGEMENT.  IN NO EVENT SHALL ETH ZURICH BE LIABLE FOR ANY     \n#  DAMAGES, INCLUDING BUT NOT LIMITED TO DIRECT, INDIRECT,\n# SPECIAL OR CONSEQUENTIAL DAMAGES, ARISING OUT OF, RESULTING FROM, OR IN\n# ANY WAY CONNECTED WITH THIS SOFTWARE (WHETHER OR NOT BASED UPON WARRANTY,\n# CONTRACT, TORT OR OTHERWISE).\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport helpers as h\nimport domains\nfrom domains import *\nimport math\n\n\nPOINT_DOMAINS = [m for m in h.getMethods(domains) if h.hasMethod(m, ""attack"")] + [ torch.FloatTensor, torch.Tensor, torch.cuda.FloatTensor ] \nSYMETRIC_DOMAINS = [domains.Box] + POINT_DOMAINS\n\ndef domRes(outDom, target, **args): # TODO: make faster again by keeping sparse tensors sparse\n    t = h.one_hot(target.data.long(), outDom.size()[1]).to_dense()\n    tmat = t.unsqueeze(2).matmul(t.unsqueeze(1))\n    \n    tl = t.unsqueeze(2).expand(-1, -1, tmat.size()[1])\n    \n    inv_t = h.eye(tmat.size()[1]).expand(tmat.size()[0], -1, -1)\n    inv_t = inv_t - tmat\n    \n    tl = tl.bmm(inv_t)\n    \n    fst = outDom.bmm(tl)\n    snd = outDom.bmm(inv_t)\n    diff = fst - snd\n    return diff.lb() + t\n\ndef isSafeDom(outDom, target, **args):\n    od,_ = torch.min(domRes(outDom, target, **args), 1)\n    return od.gt(0.0).long().item()\n\n\ndef isSafeBox(target, net, inp, eps, dom):\n    atarg = target.argmax(1)[0].unsqueeze(0)\n    if hasattr(dom, ""attack""):\n        x = dom.attack(net, eps, inp, target)\n        pred = net(x).argmax(1)[0].unsqueeze(0) # get the index of the max log-probability\n        return pred.item() == atarg.item()\n    else:\n        outDom = net(dom.box(inp, eps))\n        return isSafeDom(outDom, atarg)\n'"
models.py,0,"b'try:\n    from . import components as n\n    from . import ai\n    from . import scheduling as S\nexcept:\n    import components as n\n    import scheduling as S\n    import ai\n\n############# Previously Known Models.  Not guaranteed to have the same performance as previous papers.\n\ndef FFNN(c, **kargs):\n    return n.FFNN([100, 100, 100, 100, 100,c], last_lin = True, last_zono = True, **kargs)\n\ndef ConvSmall(c, **kargs):\n    return n.LeNet([ (16,4,4,2), (32,4,4,2) ], [100,c], last_lin = True, last_zono = True, **kargs)\n\ndef ConvMed(c, **kargs):\n    return n.LeNet([ (16,4,4,2), (32,4,4,2) ], [100,c], padding = 1, last_lin = True, last_zono = True, **kargs)\n\ndef ConvBig(c, **kargs):\n    return n.LeNet([ (32,3,3,1), (32,4,4,2) , (64,3,3,1), (64,4,4,2)], [512, 512,c], padding = 1, last_lin = True, last_zono = True, **kargs)\n\ndef ConvLargeIBP(c, **kargs):\n    return n.LeNet([ (64, 3, 3, 1), (64,3,3,1), (128,3,3,2), (128,3,3,1), (128,3,3,1)], [200,c], padding=1, ibp_init = True, bias = True, last_lin = True, last_zono = True, **kargs)\n\ndef ResNetWong(c, **kargs):\n    return n.Seq(n.Conv(16, 3, padding=1, bias=False), n.WideBlock(16), n.WideBlock(16), n.WideBlock(32, True), n.WideBlock(64, True), n.FFNN([1000, c], ibp_init = True, bias=True, last_lin=True, last_zono = True, **kargs))\n\ndef TruncatedVGG(c, **kargs):\n    return n.LeNet([ (64, 3, 3, 1), (64,3,3,1), (128,3,3,2), (128,3,3,1)], [512,c], padding=1, ibp_init = True, bias = True, last_lin = True, last_zono = True, **kargs)\n\n\n############# New Models\n\ndef ResNetTiny(c, **kargs): # resnetWide also used by mixtrain and scaling provable adversarial defenses\n    def wb(c, bias = True, **kargs):\n        return n.WideBlock(c, False, bias=bias, ibp_init=True, batch_norm = False, **kargs)\n    return n.Seq(n.Conv(16, 3, padding=1, bias=True, ibp_init = True), \n                 wb(16), \n                 wb(32), \n                 wb(32), \n                 wb(32), \n                 wb(32), \n                 n.FFNN([500, c], bias=True, last_lin=True, ibp_init = True, last_zono = True, **kargs))\n\ndef ResNetTiny_FewCombo(c, **kargs): # resnetWide also used by mixtrain and scaling provable adversarial defenses\n    def wb(c, bias = True, **kargs):\n        return n.WideBlock(c, False, bias=bias, ibp_init=True, batch_norm = False, **kargs)\n    dl = n.DeepLoss\n    cmk = n.CorrMaxK\n    cm2d = n.CorrMaxPool2D\n    cm3d = n.CorrMaxPool3D\n    dec = lambda x: n.DecorrMin(x, num_to_keep = True)\n    return n.Seq(cmk(32), \n                 n.Conv(16, 3, padding=1, bias=True, ibp_init = True), dec(8), \n                 wb(16), dec(4), \n                 wb(32), n.Concretize(), \n                 wb(32), \n                 wb(32), \n                 wb(32), cmk(10), \n                 n.FFNN([500, c], bias=True, last_lin=True, ibp_init = True, last_zono = True, **kargs))\n\n\ndef ResNetTiny_ManyFixed(c, **kargs): # resnetWide also used by mixtrain and scaling provable adversarial defenses\n    def wb(c, bias = True, **kargs):\n        return n.WideBlock(c, False, bias=bias, ibp_init=True, batch_norm = False, **kargs)\n    cmk = n.CorrFix\n    dec = lambda x: n.DecorrMin(x, num_to_keep = True)\n    return n.Seq(n.CorrMaxK(32), \n                 n.Conv(16, 3, padding=1, bias=True, ibp_init = True), cmk(16), dec(16), \n                 wb(16), cmk(8), dec(8), \n                 wb(32), cmk(8), dec(8), \n                 wb(32), cmk(4), dec(4), \n                 wb(32), n.Concretize(),\n                 wb(32), \n                 n.FFNN([500, c], bias=True, last_lin=True, ibp_init = True, last_zono = True, **kargs))\n\ndef SkipNet18(c, **kargs):\n    return n.Seq(n.ResNet([2,2,2,2], bias = True, ibp_init = True, skip_net = True), n.FFNN([512, 512, c], bias=True, last_lin=True, last_zono = True, ibp_init = True, **kargs))\n\ndef SkipNet18_Combo(c, **kargs):\n    dl = n.DeepLoss\n    cmk = n.CorrFix\n    dec = lambda x: n.DecorrMin(x, num_to_keep = True)\n    return n.Seq(n.ResNet([2,2,2,2], extra = [ (cmk(20),2),(dec(10),2)\n                                              ,(cmk(10),3),(dec(5),3),(dl(S.Until(90, S.Lin(0, 0.2, 50, 40), 0)), 3)\n                                              ,(cmk(5),4),(dec(2),4)], bias = True, ibp_init=True, skip_net = True), n.FFNN([512, 512, c], bias=True, last_lin=True, last_zono = True, ibp_init=True, **kargs))\n\ndef ResNet18(c, **kargs):\n    return n.Seq(n.ResNet([2,2,2,2], bias = True, ibp_init = True), n.FFNN([512, 512, c], bias=True, last_lin=True, last_zono = True, ibp_init = True, **kargs))\n\n\ndef ResNetLarge_LargeCombo(c, **kargs): # resnetWide also used by mixtrain and scaling provable adversarial defenses\n    def wb(c, bias = True, **kargs):\n        return n.WideBlock(c, False, bias=bias, ibp_init=True, batch_norm = False, **kargs)\n    dl = n.DeepLoss\n    cmk = n.CorrMaxK\n    cm2d = n.CorrMaxPool2D\n    cm3d = n.CorrMaxPool3D\n    dec = lambda x: n.DecorrMin(x, num_to_keep = True)\n    return n.Seq(n.Conv(16, 3, padding=1, bias=True, ibp_init = True), cmk(4),\n                 wb(16), cmk(4), dec(4),\n                 wb(32), cmk(4), dec(4),\n                 wb(32), dl(S.Until(1, 0, S.Lin(0.5, 0, 50, 3))), \n                 wb(32), cmk(4), dec(4),\n                 wb(64), cmk(4), dec(2),\n                 wb(64), dl(S.Until(24, S.Lin(0, 0.1, 20, 4), S.Lin(0.1, 0, 50))), \n                 wb(64), \n                 n.FFNN([1000, c], bias=True, last_lin=True, ibp_init = True, **kargs))\n\n\n\ndef ResNet34(c, **kargs):\n    return n.Seq(n.ResNet([3,4,6,3], bias = True, ibp_init = True), n.FFNN([512, 512, c], bias=True, last_lin=True, last_zono = True, ibp_init = True, **kargs))\n\n\ndef DenseNet100(c, **kwargs):\n    return n.DenseNet(growthRate=12, depth=100, reduction=0.5,\n                      bottleneck=True, num_classes = c)\n'"
scheduling.py,2,"b'import torch\nimport torch.nn as nn\nimport math\n\ntry:\n    from . import helpers as h\nexcept:\n    import helpers as h\n\n\n\nclass Const():\n    def __init__(self, c):\n        self.c = c if c is None else float(c)\n\n    def getVal(self, c = None, **kargs):\n        return self.c if self.c is not None else c\n\n    def __str__(self):\n        return str(self.c)\n\n    def initConst(x):\n        return x if isinstance(x, Const) else Const(x)\n\nclass Lin(Const):\n    def __init__(self, start, end, steps, initial = 0, quant = False):\n        self.start = float(start)\n        self.end = float(end)\n        self.steps = float(steps)\n        self.initial = float(initial)\n        self.quant = quant\n\n    def getVal(self, time = 0, **kargs):\n        if self.quant:\n            time = math.floor(time)\n        return (self.end - self.start) * max(0,min(1, float(time - self.initial) / self.steps)) + self.start\n\n    def __str__(self):\n        return ""Lin(%s,%s,%s,%s, quant=%s)"".format(str(self.start), str(self.end), str(self.steps), str(self.initial), str(self.quant))\n\nclass Until(Const):\n    def __init__(self, thresh, a, b):\n        self.a = Const.initConst(a)\n        self.b = Const.initConst(b)\n        self.thresh = thresh\n\n    def getVal(self, *args, time = 0, **kargs):\n        return self.a.getVal(*args, time = time, **kargs) if time < self.thresh else self.b.getVal(*args, time = time - self.thresh, **kargs)\n\n    def __str__(self):\n        return ""Until(%s, %s, %s)"" % (str(self.thresh), str(self.a), str(self.b))\n\nclass Scale(Const): # use with mix when aw = 1, and 0 <= c < 1\n    def __init__(self, c):\n        self.c = Const.initConst(c)\n\n    def getVal(self, *args, **kargs):\n        c = self.c.getVal(*args, **kargs)\n        if c == 0:\n            return 0\n        assert c >= 0\n        assert c < 1\n        return c / (1 - c)\n\n    def __str__(self):\n        return ""Scale(%s)"" % str(self.c)\n\ndef MixLin(*args, **kargs):\n    return Scale(Lin(*args, **kargs))\n\nclass Normal(Const):\n    def __init__(self, c):\n        self.c = Const.initConst(c)\n\n    def getVal(self, *args, shape = [1], **kargs):\n        c = self.c.getVal(*args, shape = shape, **kargs)\n        return torch.randn(shape, device = h.device).abs() * c\n\n    def __str__(self):\n        return ""Normal(%s)"" % str(self.c)\n\nclass Clip(Const):\n    def __init__(self, c, l, u):\n        self.c = Const.initConst(c)\n        self.l = Const.initConst(l)\n        self.u = Const.initConst(u)\n\n    def getVal(self, *args, **kargs):\n        c = self.c.getVal(*args, **kargs)\n        l = self.l.getVal(*args, **kargs)\n        u = self.u.getVal(*args, **kargs)\n        if isinstance(c, float):\n            return min(max(c,l),u)\n        else:\n            return c.clamp(l,u)\n\n    def __str__(self):\n        return ""Clip(%s, %s, %s)"" % (str(self.c), str(self.l), str(self.u))\n\nclass Fun(Const):\n    def __init__(self, foo):\n        self.foo = foo\n    def getVal(self, *args, **kargs):\n        return self.foo(*args, **kargs)\n    \n    def __str__(self):\n        return ""Fun(...)""\n\nclass Complement(Const): # use with mix when aw = 1, and 0 <= c < 1\n    def __init__(self, c):\n        self.c = Const.initConst(c)\n\n    def getVal(self, *args, **kargs):\n        c = self.c.getVal(*args, **kargs)\n        assert c >= 0\n        assert c <= 1\n        return 1 - c\n\n    def __str__(self):\n        return ""Complement(%s)"" % str(self.c)\n'"
