file_path,api_count,code
vae.py,18,"b'import torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import transforms\nimport torch.optim as optim\nfrom torch import nn\nimport matplotlib.pyplot as plt\n\n\nclass Normal(object):\n    def __init__(self, mu, sigma, log_sigma, v=None, r=None):\n        self.mu = mu\n        self.sigma = sigma  # either stdev diagonal itself, or stdev diagonal from decomposition\n        self.logsigma = log_sigma\n        dim = mu.get_shape()\n        if v is None:\n            v = torch.FloatTensor(*dim)\n        if r is None:\n            r = torch.FloatTensor(*dim)\n        self.v = v\n        self.r = r\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Encoder, self).__init__()\n        self.linear1 = torch.nn.Linear(D_in, H)\n        self.linear2 = torch.nn.Linear(H, D_out)\n\n    def forward(self, x):\n        x = F.relu(self.linear1(x))\n        return F.relu(self.linear2(x))\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Decoder, self).__init__()\n        self.linear1 = torch.nn.Linear(D_in, H)\n        self.linear2 = torch.nn.Linear(H, D_out)\n\n    def forward(self, x):\n        x = F.relu(self.linear1(x))\n        return F.relu(self.linear2(x))\n\n\nclass VAE(torch.nn.Module):\n    latent_dim = 8\n\n    def __init__(self, encoder, decoder):\n        super(VAE, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self._enc_mu = torch.nn.Linear(100, 8)\n        self._enc_log_sigma = torch.nn.Linear(100, 8)\n\n    def _sample_latent(self, h_enc):\n        """"""\n        Return the latent normal sample z ~ N(mu, sigma^2)\n        """"""\n        mu = self._enc_mu(h_enc)\n        log_sigma = self._enc_log_sigma(h_enc)\n        sigma = torch.exp(log_sigma)\n        std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n\n        self.z_mean = mu\n        self.z_sigma = sigma\n\n        return mu + sigma * Variable(std_z, requires_grad=False)  # Reparameterization trick\n\n    def forward(self, state):\n        h_enc = self.encoder(state)\n        z = self._sample_latent(h_enc)\n        return self.decoder(z)\n\n\ndef latent_loss(z_mean, z_stddev):\n    mean_sq = z_mean * z_mean\n    stddev_sq = z_stddev * z_stddev\n    return 0.5 * torch.mean(mean_sq + stddev_sq - torch.log(stddev_sq) - 1)\n\n\nif __name__ == \'__main__\':\n\n    input_dim = 28 * 28\n    batch_size = 32\n\n    transform = transforms.Compose(\n        [transforms.ToTensor()])\n    mnist = torchvision.datasets.MNIST(\'./\', download=True, transform=transform)\n\n    dataloader = torch.utils.data.DataLoader(mnist, batch_size=batch_size,\n                                             shuffle=True, num_workers=2)\n\n    print(\'Number of samples: \', len(mnist))\n\n    encoder = Encoder(input_dim, 100, 100)\n    decoder = Decoder(8, 100, input_dim)\n    vae = VAE(encoder, decoder)\n\n    criterion = nn.MSELoss()\n\n    optimizer = optim.Adam(vae.parameters(), lr=0.0001)\n    l = None\n    for epoch in range(100):\n        for i, data in enumerate(dataloader, 0):\n            inputs, classes = data\n            inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes)\n            optimizer.zero_grad()\n            dec = vae(inputs)\n            ll = latent_loss(vae.z_mean, vae.z_sigma)\n            loss = criterion(dec, inputs) + ll\n            loss.backward()\n            optimizer.step()\n            l = loss.data[0]\n        print(epoch, l)\n\n    plt.imshow(vae(inputs).data[0].numpy().reshape(28, 28), cmap=\'gray\')\n    plt.show(block=True)\n'"
