file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\n\ndef setup_package():\n    setup(name=""spacy-transformers"", packages=find_packages())\n\n\nif __name__ == ""__main__"":\n    setup_package()\n'"
examples/init_model.py,0,"b'#!/usr/bin/env python\nimport plac\nfrom wasabi import Printer\nfrom spacy_transformers import TransformersLanguage, TransformersWordPiecer\nfrom spacy_transformers import TransformersTok2Vec\n\n\n@plac.annotations(\n    path=(""Output path"", ""positional"", None, str),\n    name=(""Name of pre-trained model"", ""option"", ""n"", str),\n    lang=(""Language code to use"", ""option"", ""l"", str),\n)\ndef main(path, name=""bert-base-uncased"", lang=""en""):\n    msg = Printer()\n    msg.info(f""Creating model for \'{name}\' ({lang})"")\n    with msg.loading(f""Setting up the pipeline...""):\n        nlp = TransformersLanguage(trf_name=name, meta={""lang"": lang})\n        nlp.add_pipe(nlp.create_pipe(""sentencizer""))\n        nlp.add_pipe(TransformersWordPiecer.from_pretrained(nlp.vocab, name))\n        nlp.add_pipe(TransformersTok2Vec.from_pretrained(nlp.vocab, name))\n    msg.good(""Initialized the model pipeline"")\n    nlp.to_disk(path)\n    msg.good(f""Saved \'{name}\' ({lang})"")\n    msg.text(f""Pipeline: {nlp.pipe_names}"")\n    msg.text(f""Location: {path}"")\n    with msg.loading(""Verifying model loads...""):\n        nlp.from_disk(path)\n    msg.good(""Model loads!"")\n\n\nif __name__ == ""__main__"":\n    plac.call(main)\n'"
examples/test_wordpiece_alignment.py,0,"b'#!/usr/bin/env python\nimport plac\nfrom spacy_transformers import TransformersWordPiecer\nfrom spacy.util import get_lang_class\nimport thinc.extra.datasets\nfrom wasabi import Printer, color\nimport difflib\nimport sys\nimport tqdm\n\nmsg = Printer()\n\n\n@plac.annotations(\n    name=(""Pretrained model name, e.g. \'bert-base-uncased\'"", ""positional"", None, str),\n    n_texts=(""Number of texts to load (0 for all)"", ""option"", ""n"", int),\n    lang=(""spaCy language to use for tokenization"", ""option"", ""l"", str),\n    skip=(""Don\'t stop processing on errors, only print"", ""flag"", ""s"", bool),\n    retry=(""Enable retrying alignment by aggressive replacement"", ""flag"", ""r"", bool),\n    force=(""Enable forced alignment via tokens"", ""flag"", ""f"", bool),\n)\ndef main(\n    name=""bert-base-uncased"",\n    n_texts=1000,\n    lang=""en"",\n    skip=False,\n    retry=False,\n    force=False,\n):\n    """"""Test the wordpiecer on a large dataset to find misalignments. If both the\n    retry and force flag are set (which is the default runtime configuration),\n    this script should always pass.\n\n    * retry: If alignment fails after cleaning and normalizing both sets of\n        tokens, try again with a more aggressive strategy that strips out all\n        characters that are not uppercase/lowercase letters.\n    * force: If alignment still fails, run the word-piece tokenizer on the\n        individual spaCy tokens, so that alignment is trivial. This should\n        always work.\n    """"""\n    cfg = {""retry_alignment"": retry, ""force_alignment"": force}\n    nlp = get_lang_class(lang)()\n    nlp.add_pipe(nlp.create_pipe(""sentencizer""))\n    wp = TransformersWordPiecer.from_pretrained(nlp.vocab, trf_name=name, **cfg)\n    msg.good(f""Loaded WordPiecer for model \'{name}\'"")\n    with msg.loading(""Loading IMDB data...""):\n        data, _ = thinc.extra.datasets.imdb(limit=n_texts)\n    texts, _ = zip(*data)\n    msg.good(f""Using {len(texts)} texts from IMDB data"")\n    msg.info(""Processing texts..."")\n    sent_counts = 0\n    for doc in tqdm.tqdm(nlp.pipe(texts), total=len(texts)):\n        try:\n            doc = wp(doc)\n            sent_counts += len(list(doc.sents))\n        except AssertionError as e:\n            if len(e.args) and isinstance(e.args[0], tuple):  # Misaligned error\n                a, b = e.args[0]\n                msg.fail(""Misaligned tokens"")\n                print(diff_strings(a, b))\n                if not skip:\n                    sys.exit(1)\n            elif len(e.args):\n                msg.fail(f""Error: {e.args[0]}"", exits=None if skip else 1)\n            else:\n                if skip:\n                    print(e)\n                else:\n                    raise e\n    msg.good(f""Processed {len(texts)} documents ({sent_counts} sentences)"")\n\n\ndef diff_strings(a, b):\n    output = []\n    matcher = difflib.SequenceMatcher(None, a, b)\n    for opcode, a0, a1, b0, b1 in matcher.get_opcodes():\n        if opcode == ""equal"":\n            output.append(a[a0:a1])\n        elif opcode == ""insert"":\n            output.append(color(b[b0:b1], fg=16, bg=""green""))\n        elif opcode == ""delete"":\n            output.append(color(a[a0:a1], fg=16, bg=""red""))\n        elif opcode == ""replace"":\n            output.append(color(b[b0:b1], fg=16, bg=""green""))\n            output.append(color(a[a0:a1], fg=16, bg=""red""))\n    return """".join(output)\n\n\nif __name__ == ""__main__"":\n    plac.call(main)\n'"
examples/train_textcat.py,1,"b'#!/usr/bin/env python\nimport plac\nimport re\nimport random\nimport json\nfrom pathlib import Path\nfrom collections import Counter\nimport thinc.extra.datasets\nimport spacy\nimport torch\nfrom spacy.util import minibatch\nimport tqdm\nimport unicodedata\nimport wasabi\nfrom spacy_transformers.util import cyclic_triangular_rate\n\n\n@plac.annotations(\n    model=(""Model name"", ""positional"", None, str),\n    input_dir=(""Optional input directory"", ""option"", ""i"", Path),\n    output_dir=(""Optional output directory"", ""option"", ""o"", Path),\n    use_test=(""Whether to use the actual test set"", ""flag"", ""E""),\n    batch_size=(""Number of docs per batch"", ""option"", ""bs"", int),\n    learn_rate=(""Learning rate"", ""option"", ""lr"", float),\n    max_wpb=(""Max words per sub-batch"", ""option"", ""wpb"", int),\n    n_texts=(""Number of texts to train from"", ""option"", ""t"", int),\n    n_iter=(""Number of training epochs"", ""option"", ""n"", int),\n    pos_label=(""Positive label for evaluation"", ""option"", ""pl"", str),\n)\ndef main(\n    model,\n    input_dir=None,\n    output_dir=None,\n    n_iter=5,\n    n_texts=100,\n    batch_size=8,\n    learn_rate=2e-5,\n    max_wpb=1000,\n    use_test=False,\n    pos_label=None,\n):\n    spacy.util.fix_random_seed(0)\n    is_using_gpu = spacy.prefer_gpu()\n    if is_using_gpu:\n        torch.set_default_tensor_type(""torch.cuda.FloatTensor"")\n    if output_dir is not None:\n        output_dir = Path(output_dir)\n        if not output_dir.exists():\n            output_dir.mkdir()\n\n    nlp = spacy.load(model)\n    print(nlp.pipe_names)\n    print(f""Loaded model \'{model}\'"")\n    textcat = nlp.create_pipe(\n        ""trf_textcat"",\n        config={""architecture"": ""softmax_last_hidden"", ""words_per_batch"": max_wpb},\n    )\n    if input_dir is not None:\n        train_texts, train_cats = read_inputs(input_dir / ""training.jsonl"")\n        eval_texts, eval_cats = read_inputs(input_dir / ""evaluation.jsonl"")\n        labels = set()\n        for cats in train_cats + eval_cats:\n            labels.update(cats)\n        # use the first label in the set as the positive label if one isn\'t\n        # provided\n        for label in sorted(labels):\n            if not pos_label:\n                pos_label = label\n            textcat.add_label(label)\n    else:\n        # add label to text classifier\n        textcat.add_label(""POSITIVE"")\n        textcat.add_label(""NEGATIVE"")\n        if not pos_label:\n            pos_label = ""POSITIVE""\n        # load the IMDB dataset\n        print(""Loading IMDB data..."")\n        if use_test:\n            (train_texts, train_cats), (\n                eval_texts,\n                eval_cats,\n            ) = load_data_for_final_test(limit=n_texts)\n        else:\n            (train_texts, train_cats), (eval_texts, eval_cats) = load_data(\n                limit=n_texts\n            )\n\n    print(""Labels:"", textcat.labels)\n    print(""Positive label for evaluation:"", pos_label)\n    nlp.add_pipe(textcat, last=True)\n    print(f""Using {len(train_texts)} training docs, {len(eval_texts)} evaluation"")\n    split_training_by_sentence = False\n    if split_training_by_sentence:\n        # If we\'re using a model that averages over sentence predictions (we are),\n        # there are some advantages to just labelling each sentence as an example.\n        # It means we can mix the sentences into different batches, so we can make\n        # more frequent updates. It also changes the loss somewhat, in a way that\'s\n        # not obviously better -- but it does seem to work well.\n        train_texts, train_cats = make_sentence_examples(nlp, train_texts, train_cats)\n        print(f""Extracted {len(train_texts)} training sents"")\n    # total_words = sum(len(text.split()) for text in train_texts)\n    train_data = list(zip(train_texts, [{""cats"": cats} for cats in train_cats]))\n    # Initialize the TextCategorizer, and create an optimizer.\n    optimizer = nlp.resume_training()\n    optimizer.alpha = 0.001\n    optimizer.trf_weight_decay = 0.005\n    optimizer.L2 = 0.0\n    learn_rates = cyclic_triangular_rate(\n        learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size\n    )\n    print(""Training the model..."")\n    print(""{:^5}\\t{:^5}\\t{:^5}\\t{:^5}"".format(""LOSS"", ""P"", ""R"", ""F""))\n\n    pbar = tqdm.tqdm(total=100, leave=False)\n    results = []\n    epoch = 0\n    step = 0\n    eval_every = 100\n    patience = 3\n    while True:\n        # Train and evaluate\n        losses = Counter()\n        random.shuffle(train_data)\n        batches = minibatch(train_data, size=batch_size)\n        for batch in batches:\n            optimizer.trf_lr = next(learn_rates)\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.1, losses=losses)\n            pbar.update(1)\n            if step and (step % eval_every) == 0:\n                pbar.close()\n                with nlp.use_params(optimizer.averages):\n                    scores = evaluate(nlp, eval_texts, eval_cats, pos_label)\n                results.append((scores[""textcat_f""], step, epoch))\n                print(\n                    ""{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}"".format(\n                        losses[""trf_textcat""],\n                        scores[""textcat_p""],\n                        scores[""textcat_r""],\n                        scores[""textcat_f""],\n                    )\n                )\n                pbar = tqdm.tqdm(total=eval_every, leave=False)\n            step += 1\n        epoch += 1\n        # Stop if no improvement in HP.patience checkpoints\n        if results:\n            best_score, best_step, best_epoch = max(results)\n            if ((step - best_step) // eval_every) >= patience:\n                break\n\n    msg = wasabi.Printer()\n    table_widths = [2, 4, 6]\n    msg.info(f""Best scoring checkpoints"")\n    msg.row([""Epoch"", ""Step"", ""Score""], widths=table_widths)\n    msg.row([""-"" * width for width in table_widths])\n    for score, step, epoch in sorted(results, reverse=True)[:10]:\n        msg.row([epoch, step, ""%.2f"" % (score * 100)], widths=table_widths)\n\n    # Test the trained model\n    test_text = eval_texts[0]\n    doc = nlp(test_text)\n    print(test_text, doc.cats)\n\n    if output_dir is not None:\n        nlp.to_disk(output_dir)\n        print(""Saved model to"", output_dir)\n        # test the saved model\n        print(""Loading from"", output_dir)\n        nlp2 = spacy.load(output_dir)\n        doc2 = nlp2(test_text)\n        print(test_text, doc2.cats)\n\n\ndef read_inputs(input_path):\n    texts = []\n    cats = []\n    with input_path.open(mode=""r"") as file_:\n        for line in file_:\n            text, gold = json.loads(line)\n            text = preprocess_text(text)\n            texts.append(text)\n            cats.append(gold[""cats""])\n    return texts, cats\n\n\ndef make_sentence_examples(nlp, texts, labels):\n    """"""Treat each sentence of the document as an instance, using the doc labels.""""""\n    sents = []\n    sent_cats = []\n    for text, cats in zip(texts, labels):\n        doc = nlp.make_doc(text)\n        doc = nlp.get_pipe(""sentencizer"")(doc)\n        for sent in doc.sents:\n            sents.append(sent.text)\n            sent_cats.append(cats)\n    return sents, sent_cats\n\n\nwhite_re = re.compile(r""\\s\\s+"")\n\n\ndef preprocess_text(text):\n    text = text.replace(""<s>"", ""<open-s-tag>"")\n    text = text.replace(""</s>"", ""<close-s-tag>"")\n    text = white_re.sub("" "", text).strip()\n    return """".join(\n        c for c in unicodedata.normalize(""NFD"", text) if unicodedata.category(c) != ""Mn""\n    )\n\n\ndef load_data(*, limit=0, dev_size=2000):\n    """"""Load data from the IMDB dataset, splitting off a held-out set.""""""\n    if limit != 0:\n        limit += dev_size\n    assert dev_size != 0\n    train_data, _ = thinc.extra.datasets.imdb(limit=limit)\n    assert len(train_data) > dev_size\n    random.shuffle(train_data)\n    dev_data = train_data[:dev_size]\n    train_data = train_data[dev_size:]\n    # (temporary?) modification: run preprocessing for IMDB corpus to avoid\n    # empty sentences for longer whitespace sequences, in particular trailing\n    # whitespace (the dataset loader converts <br/><br/> to ""\\n\\n\\n\\n"")\n    #\n    # explanation: with the correct attention masks, empty sentences-as-docs\n    # are passed to spaCy\'s TextCategorizer, which can\'t currently handle empty\n    # docs (spaCy v2.2.4, thinc v7.4.0)\n    train_texts, train_labels = _prepare_partition(train_data, preprocess=True)\n    dev_texts, dev_labels = _prepare_partition(dev_data, preprocess=True)\n    return (train_texts, train_labels), (dev_texts, dev_labels)\n\n\ndef load_data_for_final_test(*, limit=0):\n    print(\n        ""Warning: Using test data. You should use development data for most experiments.""\n    )\n    train_data, test_data = thinc.extra.datasets.imdb()\n    random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    train_texts, train_labels = _prepare_partition(train_data)\n    test_texts, test_labels = _prepare_partition(test_data)\n    return (train_texts, train_labels), (test_texts, test_labels)\n\n\ndef _prepare_partition(text_label_tuples, *, preprocess=False):\n    texts, labels = zip(*text_label_tuples)\n    if preprocess:\n        # Preprocessing can mask errors in our handling of noisy text, so\n        # we don\'t want to do it by default\n        texts = [preprocess_text(text) for text in texts]\n    cats = [{""POSITIVE"": bool(y), ""NEGATIVE"": not bool(y)} for y in labels]\n    return texts, cats\n\n\ndef evaluate(nlp, texts, cats, pos_label):\n    tp = 0.0  # True positives\n    fp = 0.0  # False positives\n    fn = 0.0  # False negatives\n    tn = 0.0  # True negatives\n    total_words = sum(len(text.split()) for text in texts)\n    with tqdm.tqdm(total=total_words, leave=False) as pbar:\n        for i, doc in enumerate(nlp.pipe(texts, batch_size=8)):\n            gold = cats[i]\n            for label, score in doc.cats.items():\n                if label not in gold:\n                    continue\n                if label != pos_label:\n                    continue\n                if score >= 0.5 and gold[label] >= 0.5:\n                    tp += 1.0\n                elif score >= 0.5 and gold[label] < 0.5:\n                    fp += 1.0\n                elif score < 0.5 and gold[label] < 0.5:\n                    tn += 1\n                elif score < 0.5 and gold[label] >= 0.5:\n                    fn += 1\n            pbar.update(len(doc.text.split()))\n    precision = tp / (tp + fp + 1e-8)\n    recall = tp / (tp + fn + 1e-8)\n    if (precision + recall) == 0:\n        f_score = 0.0\n    else:\n        f_score = 2 * (precision * recall) / (precision + recall)\n    return {""textcat_p"": precision, ""textcat_r"": recall, ""textcat_f"": f_score}\n\n\nif __name__ == ""__main__"":\n    plac.call(main)\n'"
spacy_transformers/__init__.py,0,"b'from .language import TransformersLanguage\nfrom .pipeline.tok2vec import TransformersTok2Vec  # noqa\nfrom .pipeline.textcat import TransformersTextCategorizer  # noqa\nfrom .pipeline.wordpiecer import TransformersWordPiecer  # noqa\nfrom .model_registry import register_model, get_model_function  # noqa\nfrom .util import pkg_meta\n\n__version__ = pkg_meta[""version""]\nTransformersLanguage.install_extensions()\n'"
spacy_transformers/_tokenizers.py,0,"b'""""""Adjust the initialization and serialization of the Transformers\ntokenizers, so that they work more nicely with spaCy. Specifically, the\nTransformers classes take file paths as arguments to their __init__, which means\nwe can\'t easily use to_bytes() and from_bytes() with them.\n\nAdditionally, provide a .clean_text() method that is used to match the preprocessing,\nso that we can get the alignment working.\n""""""\nfrom collections import OrderedDict\nimport spacy\nimport ftfy\nimport srsly\nimport regex\nimport sentencepiece\nfrom pathlib import Path\nimport unicodedata\nimport re\n\nimport transformers\nfrom transformers.tokenization_gpt2 import bytes_to_unicode\nfrom transformers.tokenization_bert import BasicTokenizer, WordpieceTokenizer\n\n\nBASE_CLASS_FIELDS = [\n    ""_bos_token"",\n    ""_eos_token"",\n    ""_unk_token"",\n    ""_sep_token"",\n    ""_pad_token"",\n    ""_cls_token"",\n    ""_mask_token"",\n    ""_additional_special_tokens"",\n    ""max_len"",\n    ""added_tokens_encoder"",\n    ""added_tokens_decoder"",\n    ""init_kwargs"",\n    ""unique_added_tokens_encoder_list"",\n]\n\n\nclass SerializationMixin:\n    """"""Provide generic serialization methods, for compatibility with spaCy.\n    These expect the tokenizer subclass to provide the following:\n\n    * serialization_fields (List[str]): List of attributes to serialize. All\n        attributes should have json-serializable values.\n    * finish_deserializing(): A function to be called after from_bytes(),\n        to finish setting up the instance.\n    """"""\n\n    def prepare_for_serialization(self):\n        self.unique_added_tokens_encoder_list = list(self.unique_added_tokens_encoder)\n\n    def finish_deserializing(self):\n        self.unique_added_tokens_encoder = set(self.unique_added_tokens_encoder_list)\n\n    def from_bytes(self, bytes_data, exclude=tuple(), **kwargs):\n        msg = srsly.msgpack_loads(bytes_data)\n        for field in self.serialization_fields:\n            setattr(self, field, msg[field])\n        self.finish_deserializing()\n        return self\n\n    def to_bytes(self, exclude=tuple(), **kwargs):\n        self.prepare_for_serialization()\n        msg = OrderedDict()\n        for field in self.serialization_fields:\n            msg[field] = getattr(self, field, None)\n        return srsly.msgpack_dumps(msg)\n\n    def from_disk(self, path, exclude=tuple(), **kwargs):\n        with (path / ""transformers_tokenizer.msg"").open(""rb"") as file_:\n            data = file_.read()\n        return self.from_bytes(data, **kwargs)\n\n    def to_disk(self, path, exclude=tuple(), **kwargs):\n        data = self.to_bytes(**kwargs)\n        with (path / ""transformers_tokenizer.msg"").open(""wb"") as file_:\n            file_.write(data)\n\n\nclass SerializableBertTokenizer(transformers.BertTokenizer, SerializationMixin):\n    serialization_fields = list(BASE_CLASS_FIELDS) + [\n        ""vocab"",\n        ""do_basic_tokenize"",\n        ""do_lower_case"",\n        ""never_split"",\n        ""tokenize_chinese_chars"",\n    ]\n\n    @classmethod\n    def blank(cls):\n        self = cls.__new__(cls)\n        for field in self.serialization_fields:\n            setattr(self, field, None)\n        self.ids_to_tokens = None\n        self.basic_tokenizer = None\n        self.wordpiece_tokenizer = None\n        return self\n\n    def prepare_for_serialization(self):\n        if self.basic_tokenizer is not None:\n            self.do_lower_case = self.basic_tokenizer.do_lower_case\n            self.never_split = self.basic_tokenizer.never_split\n            self.tokenize_chinese_chars = self.basic_tokenizer.tokenize_chinese_chars\n        super().prepare_for_serialization()\n\n    def finish_deserializing(self):\n        self.ids_to_tokens = OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()]\n        )\n        if self.do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(\n                do_lower_case=self.do_lower_case,\n                never_split=self.never_split,\n                tokenize_chinese_chars=self.tokenize_chinese_chars,\n            )\n        self.wordpiece_tokenizer = WordpieceTokenizer(\n            vocab=self.vocab, unk_token=self.unk_token\n        )\n        super().finish_deserializing()\n\n    def clean_token(self, text):\n        if self.do_basic_tokenize:\n            text = self.basic_tokenizer._clean_text(text)\n        text = text.strip()\n        return clean_accents(text)\n\n    def clean_wp_token(self, token):\n        return token.replace(""##"", """", 1).strip()\n\n    def add_special_tokens(self, segments):\n        output = []\n        for segment in segments:\n            output.extend(segment)\n            if segment:\n                output.append(self.sep_token)\n        if output:\n            # If we otherwise would have an empty output, don\'t add cls\n            output.insert(0, self.cls_token)\n        return output\n\n    def fix_alignment(self, segments):\n        """"""Turn a nested segment alignment into an alignment for the whole input,\n        by offsetting and accounting for special tokens.""""""\n        offset = 0\n        output = []\n        for segment in segments:\n            if segment:\n                offset += 1\n            seen = set()\n            for idx_group in segment:\n                output.append([idx + offset for idx in idx_group])\n                seen.update({idx for idx in idx_group})\n            offset += len(seen)\n        return output\n\n\nclass SerializableDistilBertTokenizer(\n    transformers.DistilBertTokenizer, SerializationMixin\n):\n    serialization_fields = list(BASE_CLASS_FIELDS) + [\n        ""vocab"",\n        ""do_basic_tokenize"",\n        ""do_lower_case"",\n        ""never_split"",\n        ""tokenize_chinese_chars"",\n    ]\n\n    @classmethod\n    def blank(cls):\n        self = cls.__new__(cls)\n        for field in self.serialization_fields:\n            setattr(self, field, None)\n        self.ids_to_tokens = None\n        self.basic_tokenizer = None\n        self.wordpiece_tokenizer = None\n        return self\n\n    def prepare_for_serialization(self):\n        if self.basic_tokenizer is not None:\n            self.do_lower_case = self.basic_tokenizer.do_lower_case\n            self.never_split = self.basic_tokenizer.never_split\n            self.tokenize_chinese_chars = self.basic_tokenizer.tokenize_chinese_chars\n        super().prepare_for_serialization()\n\n    def finish_deserializing(self):\n        self.ids_to_tokens = OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()]\n        )\n        if self.do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(\n                do_lower_case=self.do_lower_case,\n                never_split=self.never_split,\n                tokenize_chinese_chars=self.tokenize_chinese_chars,\n            )\n        self.wordpiece_tokenizer = WordpieceTokenizer(\n            vocab=self.vocab, unk_token=self.unk_token\n        )\n        super().finish_deserializing()\n\n    def clean_token(self, text):\n        if self.do_basic_tokenize:\n            text = self.basic_tokenizer._clean_text(text)\n        text = text.strip()\n        return clean_accents(text)\n\n    def clean_wp_token(self, token):\n        return token.replace(""##"", """", 1).strip()\n\n    def add_special_tokens(self, segments):\n        output = []\n        for segment in segments:\n            output.extend(segment)\n            if segment:\n                output.append(self.sep_token)\n        if output:\n            # If we otherwise would have an empty output, don\'t add cls\n            output.insert(0, self.cls_token)\n        return output\n\n    def fix_alignment(self, segments):\n        """"""Turn a nested segment alignment into an alignment for the whole input,\n        by offsetting and accounting for special tokens.""""""\n        offset = 0\n        output = []\n        for segment in segments:\n            if segment:\n                offset += 1\n            seen = set()\n            for idx_group in segment:\n                output.append([idx + offset for idx in idx_group])\n                seen.update({idx for idx in idx_group})\n            offset += len(seen)\n        return output\n\n\nclass SerializableGPT2Tokenizer(transformers.GPT2Tokenizer, SerializationMixin):\n    serialization_fields = list(BASE_CLASS_FIELDS) + [\n        ""encoder"",\n        ""_bpe_ranks"",\n        ""errors"",\n        ""_regex_pattern"",\n    ]\n\n    @classmethod\n    def blank(cls):\n        self = cls.__new__(cls)\n        for field in self.serialization_fields:\n            setattr(self, field, None)\n        self.byte_encoder = None\n        self.byte_decoder = None\n        self.bpe_ranks = {}\n        self.cache = None\n        self.pat = None\n        return self\n\n    def finish_deserializing(self):\n        self.bpe_ranks = deserialize_bpe_ranks(self._bpe_ranks)\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        self.cache = {}\n        self.pat = regex.compile(self._regex_pattern, flags=regex.V0)\n        super().finish_deserializing()\n\n    def prepare_for_serialization(self):\n        self._regex_pattern = self.pat.pattern\n        self._bpe_ranks = serialize_bpe_ranks(self.bpe_ranks)\n        super().prepare_for_serialization()\n\n    def clean_token(self, text):\n        text = clean_extended_unicode(text)\n        return text.strip()\n\n    def clean_wp_token(self, text):\n        text = text.replace(""\\u0120"", """", 1)\n        text = text.replace(""\\u010a"", """", 1)\n        text = ftfy.fix_text(text)\n        text = clean_extended_unicode(text)\n        return text.strip()\n\n    def add_special_tokens(self, segments):\n        output = []\n        for segment in segments:\n            if segment:\n                output.append(self.bos_token)\n            output.extend(segment)\n            if segment:\n                output.append(self.eos_token)\n        return output\n\n    def fix_alignment(self, segments):\n        """"""Turn a nested segment alignment into an alignment for the whole input,\n        by offsetting and accounting for special tokens.""""""\n        offset = 0\n        output = []\n        for segment in segments:\n            if segment:\n                offset += 1\n            seen = set()\n            for idx_group in segment:\n                output.append([idx + offset for idx in idx_group])\n                seen.update({idx for idx in idx_group})\n            offset += len(seen)\n            if segment:\n                offset += 1\n        return output\n\n\nclass SerializableXLMTokenizer(transformers.XLMTokenizer, SerializationMixin):\n    _replace_re = re.compile(r""[\\s\\.\\-`\'\\"";]+"")\n    serialization_fields = list(BASE_CLASS_FIELDS) + [""encoder"", ""_bpe_ranks""]\n\n    @classmethod\n    def blank(cls):\n        self = cls.__new__(cls)\n        for field in self.serialization_fields:\n            setattr(self, field, None)\n        self.nlp = None\n        self.fix_text = None\n        self.cache = None\n        self.decoder = {}\n        self.bpe_ranks = {}\n        return self\n\n    def finish_deserializing(self):\n        self.bpe_ranks = deserialize_bpe_ranks(self._bpe_ranks)\n        self.nlp = spacy.blank(""en"")\n        self.fix_text = ftfy.fix_text\n        self.cache = {}\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        super().finish_deserializing()\n\n    def prepare_for_serialization(self):\n        self._bpe_ranks = serialize_bpe_ranks(self.bpe_ranks)\n        super().prepare_for_serialization()\n\n    def clean_token(self, text):\n        # Model seems to just strip out all unicode so we need to do this, too,\n        # instead of calling clean_accents etc.\n        text = ftfy.fix_text(text)\n        text = re.sub(r""&(#\\d+|#x[a-f\\d]+)"", """", text)  # malformed HTML entities\n        text = clean_extended_unicode(text)\n        text = self._replace_re.sub("""", text)\n        return text.strip()\n\n    def clean_wp_token(self, text):\n        text = ftfy.fix_text(text)\n        text = clean_extended_unicode(text)\n        text = self._replace_re.sub("""", text)\n        return text.replace(""</w>"", """").strip()\n\n    def add_special_tokens(self, segments):\n        # See https://github.com/facebookresearch/XLM/issues/113\n        output = []\n        for segment in segments:\n            if segment:\n                output.append(self.bos_token)\n            output.extend(segment)\n            if segment:\n                output.append(self.eos_token)\n        return output\n\n    def fix_alignment(self, segments):\n        """"""Turn a nested segment alignment into an alignment for the whole input,\n        by offsetting and accounting for special tokens.""""""\n        offset = 0\n        output = []\n        for segment in segments:\n            if segment:\n                offset += 1\n            seen = set()\n            for idx_group in segment:\n                output.append([idx + offset for idx in idx_group])\n                seen.update({idx for idx in idx_group})\n            offset += len(seen)\n            if segment:\n                offset += 1\n        return output\n\n\nclass SerializableXLNetTokenizer(transformers.XLNetTokenizer, SerializationMixin):\n    _replace_re = re.compile(r""[\\s\'\\"";]+"")\n    _replacements = [(""\xc2\xba"", ""o""), *zip(""\xe2\x81\xb0\xc2\xb9\xc2\xb2\xc2\xb3\xe2\x81\xb4\xe2\x81\xb5\xe2\x81\xb6\xe2\x81\xb7\xe2\x81\xb8\xe2\x81\xb9"", ""0123456789"")]\n    serialization_fields = list(BASE_CLASS_FIELDS) + [\n        ""do_lower_case"",\n        ""remove_space"",\n        ""keep_accents"",\n        ""vocab_bytes"",\n    ]\n\n    @classmethod\n    def blank(cls):\n        self = cls.__new__(cls)\n        for field in self.serialization_fields:\n            setattr(self, field, None)\n        self.sp_model = None\n        return self\n\n    def prepare_for_serialization(self):\n        if hasattr(self, ""vocab_file""):\n            vocab_path = Path(self.vocab_file)\n            with vocab_path.open(""rb"") as f:\n                self.vocab_bytes = f.read()\n        super().prepare_for_serialization()\n\n    def finish_deserializing(self):\n        self.sp_model = sentencepiece.SentencePieceProcessor()\n        self.sp_model.LoadFromSerializedProto(self.vocab_bytes)\n        super().finish_deserializing()\n\n    def clean_token(self, text):\n        text = clean_fractions(text)\n        text = clean_accents(text)\n        for a, b in self._replacements:\n            text = text.replace(a, b)\n        text = clean_extended_unicode(text)\n        text = self._replace_re.sub("""", text)\n        return text.strip()\n\n    def clean_wp_token(self, text):\n        # Note: The special control character is \\u2581\n        text = clean_accents(text)\n        for a, b in self._replacements:\n            text = text.replace(a, b)\n        text = clean_extended_unicode(text)\n        text = self._replace_re.sub("""", text)\n        return text.strip()\n\n    def add_special_tokens(self, segments):\n        output = []\n        for segment in segments:\n            output.extend(segment)\n            if segment:\n                output.append(self.eos_token)\n        if output:\n            output.append(self.cls_token)\n        return output\n\n    def fix_alignment(self, segments):\n        """"""Turn a nested segment alignment into an alignment for the whole input,\n        by offsetting and accounting for special tokens.""""""\n        offset = 0\n        output = []\n        for segment in segments:\n            seen = set()\n            for idx_group in segment:\n                output.append([idx + offset for idx in idx_group])\n                seen.update({idx for idx in idx_group})\n            offset += len(seen)\n            if segment:\n                offset += 1\n        return output\n\n\nclass SerializableRobertaTokenizer(transformers.RobertaTokenizer, SerializationMixin):\n    serialization_fields = list(BASE_CLASS_FIELDS) + [\n        ""encoder"",\n        ""_bpe_ranks"",\n        ""errors"",\n        ""_regex_pattern"",\n    ]\n\n    @classmethod\n    def blank(cls):\n        self = cls.__new__(cls)\n        for field in self.serialization_fields:\n            setattr(self, field, None)\n        self.byte_encoder = None\n        self.byte_decoder = None\n        self.bpe_ranks = {}\n        self.cache = None\n        self.pat = None\n        return self\n\n    def finish_deserializing(self):\n        self.bpe_ranks = deserialize_bpe_ranks(self._bpe_ranks)\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        self.cache = {}\n        self.pat = regex.compile(self._regex_pattern, flags=regex.V0)\n        super().finish_deserializing()\n\n    def prepare_for_serialization(self):\n        self._regex_pattern = self.pat.pattern\n        self._bpe_ranks = serialize_bpe_ranks(self.bpe_ranks)\n        super().prepare_for_serialization()\n\n    def clean_token(self, text):\n        text = clean_extended_unicode(text)\n        return text.strip()\n\n    def clean_wp_token(self, text):\n        text = text.replace(""\\u0120"", """", 1)\n        text = text.replace(""\\u010a"", """", 1)\n        text = ftfy.fix_text(text)\n        text = clean_extended_unicode(text)\n        return text.strip()\n\n    def add_special_tokens(self, segments):\n        # A RoBERTa sequence pair has the following format: [CLS] A [SEP][SEP] B [SEP]\n        output = []\n        for segment in segments:\n            if output:\n                output.append(self.sep_token)\n            output.extend(segment)\n            if segment:\n                output.append(self.sep_token)\n        if output:\n            # If we otherwise would have an empty output, don\'t add cls\n            output.insert(0, self.cls_token)\n        return output\n\n    def fix_alignment(self, segments):\n        """"""Turn a nested segment alignment into an alignment for the whole input,\n        by offsetting and accounting for special tokens.""""""\n        offset = 0\n        output = []\n        for segment in segments:\n            if segment:\n                offset += 1\n            seen = set()\n            for idx_group in segment:\n                output.append([idx + offset for idx in idx_group])\n                seen.update({idx for idx in idx_group})\n            offset += len(seen)\n            if segment:\n                offset += 1\n        return output\n\n\ndef clean_accents(text):\n    return """".join(\n        c for c in unicodedata.normalize(""NFD"", text) if unicodedata.category(c) != ""Mn""\n    )\n\n\ndef clean_fractions(text):\n    chars = []\n    for c in text:\n        try:\n            name = unicodedata.name(c)\n        except ValueError:\n            chars.append(c)\n            continue\n        name = unicodedata.name(c)\n        if name.startswith(""VULGAR FRACTION""):\n            chars.append(unicodedata.normalize(""NFKC"", c))\n        else:\n            chars.append(c)\n    return """".join(chars)\n\n\ndef clean_extended_unicode(text):\n    return """".join(i for i in text if 31 < ord(i) < 127)\n\n\ndef serialize_bpe_ranks(data):\n    return [{""key"": list(key), ""value"": value} for key, value in data.items()]\n\n\ndef deserialize_bpe_ranks(data):\n    return {tuple(item[""key""]): item[""value""] for item in data}\n'"
spacy_transformers/_train.py,0,"b'import random\nfrom spacy.util import minibatch\nfrom .util import cyclic_triangular_rate\n\n\ndef train_while_improving(\n    nlp,\n    train_data,\n    evaluate,\n    *,\n    learning_rate: float,\n    batch_size: int,\n    weight_decay: float,\n    classifier_lr: float,\n    dropout: float,\n    lr_range: int,\n    lr_period: int,\n    steps_per_batch: int,\n    patience: int,\n    eval_every: int\n):\n    """"""Train until an evaluation stops improving. Works as a generator,\n    with each iteration yielding a tuple `(batch, info, is_best_checkpoint)`,\n    where info is a dict, and is_best_checkpoint is in [True, False, None] --\n    None indicating that the iteration was not evaluated as a checkpoint.\n    The evaluation is conducted by calling the evaluate callback, which should\n\n    Positional arguments:\n        nlp: The spaCy pipeline to evaluate.\n        train_data (Sized[Tuple[Union[unicode, Doc], Union[GoldParse, dict]]]):\n            The training data.\n        evaluate (Callable[[], Tuple[float, Any]]): A callback to perform evaluation.\n            The callback should take no arguments and return a tuple\n            `(main_score, other_scores)`. The main_score should be a float where\n            higher is better. other_scores can be any object.\n\n    The following hyper-parameters (passed as keyword arguments) may need to be\n    adjusted for your problem:\n\n        learning_rate (float): Central learning rate to cycle around. 1e-5 is\n            often good, but it can depend on the batch size.\n        batch_size (int): The number of examples per iteration. Try 32 and 128.\n            Larger batch size makes the gradient estimation more accurate, but\n            means fewer updates to the model are made per pass over the data.\n            With more passes over the data, the updates are less noisy, which\n            can lead to inferior generalisation and longer training times.\n            The batch size is expressed in number of examples, so the best\n            value will depend on the number of words per example in your data.\n            If your documents are long, you can use a lower batch size (because\n            you\'re using more information to estimate the gradients). With a\n            large dataset and short examples, try a batch size of 128, possibly\n            setting a higher value for `steps_per_batch` if you run out of memory\n            (see below). Also try a smaller batch size like 32.\n\n    The following hyper-parameters can affect accuracy, but generalize fairly\n    well and probably don\'t need to be tuned:\n\n        weight_decay (float): The weight decay for the AdamW optimizer. 0.005\n            is a good value.\n        classifier_lr (float): The learning rate for the classifier parameters,\n            which must be trained from scatch on each new problem. A value of\n            0.001 is good -- it\'s best for the classifier to learn much faster\n            than the rest of the network, which is initialised from the language\n            model.\n        lr_range (int): The range to vary the learning rate over during training.\n            The learning rate will cycle between learning_rate / lr_range and\n            learning_rate * lr_range. 2 is good.\n        lr_period (int): How many epochs per min-to-max period in the cycle.\n            2 is good. Definitely don\'t set patience < lr_period * 2 --- you\n            want at least one full cycle before you give up.\n\n    The following hyper-parameters impact compute budgets.\n\n        patience (int): How many evaluations to allow without improvement\n            before giving up. e.g. if patience is 5 and the best evaluation\n            was the tenth one, the loop may halt from evaluation 15 onward.\n            The loop only halts after a full epoch though, so depending on the\n            evaluation frequency, more than `patience` checkpoints may occur\n            after the best one. 10 is good.\n        eval_every (int): How often to evaluate, in number of iterations.\n            max(100, (len(train_data) // batch_size) // 10) is good -- i.e.\n            either 100, or about every 10% of a full epoch. For small training\n        steps_per_batch (int): Accumulate gradients over a number of steps for\n            each batch. This allows you to use a higher batch size with less memory,\n            at the expense of potentially slower compute costs. If you don\'t need\n            it, just set it to 1.\n\n    Every iteration, the function yields out a tuple with:\n\n    * batch: A zipped sequence of Tuple[Doc, GoldParse] pairs.\n    * info: A dict with various information about the last update (see below).\n    * is_best_checkpoint: A value in None, False, True, indicating whether this\n        was the best evaluation so far. You should use this to save the model\n        checkpoints during training. If None, evaluation was not conducted on\n        that iteration. False means evaluation was conducted, but a previous\n        evaluation was better.\n\n    The info dict provides the following information:\n\n        epoch (int): How many passes over the data have been completed.\n        step (int): How many steps have been completed.\n        score (float): The main score form the last evaluation.\n        other_scores: : The other scores from the last evaluation.\n        loss: The accumulated losses throughout training.\n        checkpoints: A list of previous results, where each result is a (score, step, epoch) tuple.\n    """"""\n    nr_eg = len(train_data)\n    nr_batch = nr_eg // batch_size\n    steps_per_epoch = nr_batch * steps_per_batch\n    optimizer = nlp.resume_training()\n    learn_rates = cyclic_triangular_rate(\n        learning_rate / lr_range, learning_rate * lr_range, steps_per_epoch\n    )\n    optimizer.trf_lr = next(learn_rates)\n    optimizer.trf_weight_decay = HP.weight_decay\n    # This sets the learning rate for the Thinc layers, i.e. just the final\n    # softmax. By keeping this LR high, we avoid a problem where the model\n    # spends too long flat, which harms the transfer learning.\n    optimizer.alpha = HP.classifier_lr\n    epoch = 0\n    step = 0\n    results = []\n    while True:\n        random.shuffle(train_data)\n        for batch in minibatch(train_data, size=(batch_size // steps_per_batch)):\n            optimizer.trf_lr = next(learn_rates)\n            docs, golds = zip(*batch)\n            losses = {}\n            nlp.update(\n                docs,\n                golds,\n                drop=HP.dropout,\n                losses=losses,\n                sgd=(optimizer if (step % steps_per_batch == 0) else None),\n            )\n            if step != 0 and not (step % (eval_every * steps_per_batch)):\n                with nlp.use_params(optimizer.averages):\n                    score, other_scores = evaluate()\n                results.append((score, step, epoch))\n                is_best_checkpoint = score == max(results)[0]\n            else:\n                score, other_scores = (None, None)\n                is_best_checkpoint = None\n            info = {\n                ""epoch"": epoch,\n                ""step"": step,\n                ""score"": score,\n                ""other_scores"": other_scores,\n                ""loss"": losses,\n                ""checkpoints"": results,\n            }\n            yield batch, info, is_best_checkpoint\n            step += 1\n        epoch += 1\n        # Stop if no improvement in HP.patience checkpoints\n        if results:\n            best_score, best_step, best_epoch = max(results)\n            if ((step - best_step) // HP.eval_every) >= HP.patience:\n                break\n'"
spacy_transformers/activations.py,0,"b'from thinc.neural.util import get_array_module\nfrom dataclasses import dataclass\nimport numpy\n\nfrom .util import List, Array\nfrom .util import lengths2mask\n\n\n@dataclass\nclass RaggedArray:\n    data: Array\n    lengths: List[int]\n\n    @property\n    def size(self) -> int:\n        return self.data.size\n\n    @property\n    def xp(self):\n        return get_array_module(self.data)\n\n    @property\n    def dtype(self):\n        return self.data.dtype\n\n    @classmethod\n    def blank(cls, xp=numpy) -> ""RaggedArray"":\n        return RaggedArray(xp.zeros((0,), dtype=""f""), [])\n\n    @classmethod\n    def from_truncated(cls, square: Array, lengths: List[int]) -> ""RaggedArray"":\n        if len(lengths) != square.shape[0]:\n            raise ValueError(""Truncated array must have shape[0] == len(lengths)"")\n        width = square.shape[1]\n        max_len = max(lengths, default=0)\n        extra_dims = square.shape[2:]\n        if width == max_len:\n            return RaggedArray(square, lengths)\n        elif width > max_len:\n            raise ValueError(""Expected width < max_len. Got {width} > {max_len}"")\n        xp = get_array_module(square)\n        expanded = xp.zeros((sum(lengths),) + extra_dims, dtype=square.dtype)\n        # TODO: I know there\'s a way to do this without the loop :(. Escapes\n        # me currently.\n        start = 0\n        for i, length in enumerate(lengths):\n            # We could have a row that\'s actually shorter than the width,\n            # if the array was padded. Make sure we don\'t get junk values.\n            row_width = min(width, length)\n            expanded[start : start + row_width] = square[i, :row_width]\n            start += length\n        return cls(expanded, lengths)\n\n    @classmethod\n    def from_padded(cls, padded: Array, lengths: List[int]) -> ""RaggedArray"":\n        if max(lengths, default=0) > padded.shape[1]:\n            return cls.from_truncated(padded, lengths)\n        mask = lengths2mask(lengths)\n        assert sum(mask) == sum(lengths)\n        all_rows = padded.reshape((-1,) + padded.shape[2:])\n        xp = get_array_module(all_rows)\n        data = xp.ascontiguousarray(all_rows[mask])\n        assert data.shape[0] == sum(lengths)\n        return cls(data, lengths)\n\n    def to_padded(self, *, value=0, to: int = -1) -> Array:\n        assert sum(self.lengths) == self.data.shape[0]\n        max_len = max(self.lengths, default=0)\n        if to >= 1 and to < max_len:\n            raise ValueError(f""Cannot pad to {to}: Less than max length {max_len}"")\n        to = max(to, max_len)\n        # Slightly convoluted implementation here, to do the operation in one\n        # and avoid the loop\n        shape = (len(self.lengths), to) + self.data.shape[1:]\n        values = self.xp.zeros(shape, dtype=self.dtype)\n        if value != 0:\n            values.fill(value)\n        if self.data.size == 0:\n            return values\n        mask = lengths2mask(self.lengths)\n        values = values.reshape((len(self.lengths) * to,) + self.data.shape[1:])\n        values[mask] = self.data\n        values = values.reshape(shape)\n        return values\n\n    def get(self, i: int) -> Array:\n        start = sum(self.lengths[:i])\n        end = start + self.lengths[i]\n        return self.data[start:end]\n\n\n@dataclass\nclass Activations:\n    lh: RaggedArray\n    po: RaggedArray\n\n    @classmethod\n    def blank(cls, *, xp=numpy):\n        return cls(RaggedArray.blank(xp=xp), RaggedArray.blank(xp=xp))\n\n    @property\n    def xp(self):\n        return self.lh.xp\n\n    @property\n    def has_lh(self) -> bool:\n        return bool(self.lh.data.size)\n\n    @property\n    def has_po(self) -> bool:\n        return bool(self.po.data.size)\n'"
spacy_transformers/hyper_params.py,0,"b'from configparser import ConfigParser\n\n\nFIELDS = dict(\n    max_seq_length=int,\n    dropout=float,\n    batch_size=int,\n    eval_batch_size=int,\n    learning_rate=float,\n    lr_range=float,\n    lr_period=int,\n    weight_decay=float,\n    adam_epsilon=float,\n    max_grad_norm=float,\n    num_train_epochs=int,\n    max_steps=int,\n    warmup_steps=int,\n    seed=int,\n    textcat_arch=str,\n    eval_every=int,\n    use_learn_rate_schedule=int,\n    use_swa=int,\n    patience=int,\n)\n\n\nclass HyperParams:\n    pass\n\n\ndef get_hyper_params(path):\n    cfg = ConfigParser()\n    with path.open(""r"", encoding=""utf8"") as file_:\n        cfg.read_string(file_.read())\n    values = cfg[""defaults""]\n    HP = HyperParams()\n    for field, type_ in FIELDS.items():\n        setattr(HP, field, type_(values[field]))\n    return HP\n'"
spacy_transformers/language.py,0,"b'from spacy.language import Language\nfrom spacy.tokens import Doc, Span, Token\nfrom spacy.util import get_lang_class\nfrom spacy.gold import GoldParse\nfrom .util import is_special_token, pkg_meta, ATTRS, PIPES, LANG_FACTORY\n\n\nclass TransformersLanguage(Language):\n    """"""A subclass of spacy.Language that holds a Transformer pipeline.\n\n    Transformer pipelines work only slightly differently from spaCy\'s default\n    pipelines. Specifically, we introduce a new pipeline component at the start\n    of the pipeline, TransformerTok2Vec. We then modify the nlp.update()\n    function to run the TransformerTok2Vec before the other pipeline components,\n    and backprop it after the other components are done.\n    """"""\n\n    lang_factory_name = LANG_FACTORY\n\n    @staticmethod\n    def install_extensions():\n        tok2vec_attrs = [\n            ATTRS.last_hidden_state,\n            ATTRS.pooler_output,\n            ATTRS.all_hidden_states,\n            ATTRS.all_attentions,\n            ATTRS.d_last_hidden_state,\n            ATTRS.d_pooler_output,\n            ATTRS.d_all_hidden_states,\n            ATTRS.d_all_attentions,\n        ]\n        for attr in tok2vec_attrs:\n            Doc.set_extension(attr, default=None)\n            Span.set_extension(attr, getter=get_span_tok2vec_getter(attr))\n            Token.set_extension(attr, getter=get_token_tok2vec_getter(attr))\n        wp_attrs = [ATTRS.alignment, ATTRS.word_pieces, ATTRS.word_pieces_]\n        for attr in wp_attrs:\n            Doc.set_extension(attr, default=None)\n            Span.set_extension(attr, getter=get_span_wp_getter(attr))\n            Token.set_extension(attr, getter=get_token_wp_getter(attr))\n        Doc.set_extension(ATTRS.separator, default=None)\n        Span.set_extension(\n            ATTRS.separator, getter=lambda span: span.doc._.get(ATTRS.separator)\n        )\n        Token.set_extension(\n            ATTRS.separator, getter=lambda token: token.doc._.get(ATTRS.separator)\n        )\n        Doc.set_extension(ATTRS.segments, getter=get_segments)\n        Span.set_extension(ATTRS.segments, getter=get_segments)\n        for cls in [Token, Span, Doc]:\n            cls.set_extension(ATTRS.start, getter=get_wp_start)\n            cls.set_extension(ATTRS.end, getter=get_wp_end)\n\n    def __init__(\n        self, vocab=True, make_doc=True, max_length=10 ** 6, meta={}, **kwargs\n    ):\n        """"""Initialize the language class. Expects either a trf_name setting in\n        the meta or as a keyword argument, specifying the pre-trained model\n        name. This is used to set up the model-specific tokenizer.\n        """"""\n        meta = dict(meta)\n        meta[""lang_factory""] = self.lang_factory_name\n        # Add this package to requirements to it will be included in the\n        # install_requires of any model using this language class\n        package = f""{pkg_meta[\'title\']}>={pkg_meta[\'version\']}""\n        meta.setdefault(""requirements"", []).append(package)\n        self.lang = meta.get(""lang"", ""xx"")\n        self.Defaults = get_defaults(self.lang)\n        super().__init__(vocab, make_doc, max_length, meta=meta, **kwargs)\n\n    def update(self, docs, golds, drop=0.0, sgd=None, losses=None, component_cfg={}):\n        component_cfg = dict(component_cfg)\n        if self.has_pipe(""sentencizer""):\n            sentencizer = self.get_pipe(""sentencizer"")\n        else:\n            sentencizer = lambda doc: doc\n        if self.has_pipe(PIPES.wordpiecer):\n            wp = self.get_pipe(PIPES.wordpiecer)\n        else:\n            wp = lambda doc: doc\n        tok2vec = self.get_pipe(PIPES.tok2vec)\n        new_docs = []\n        new_golds = []\n        for doc, gold in zip(docs, golds):\n            if isinstance(doc, str):\n                doc = self.make_doc(doc)\n            doc = sentencizer(doc)\n            if doc._.get(ATTRS.word_pieces) is None:\n                doc = wp(doc)\n            if not isinstance(gold, GoldParse):\n                gold = GoldParse(doc, **gold)\n            new_docs.append(doc)\n            new_golds.append(gold)\n        docs = new_docs\n        golds = new_golds\n        outputs, backprop_tok2vec = tok2vec.begin_update(\n            docs, drop=drop, **component_cfg.get(PIPES.tok2vec, {})\n        )\n        tok2vec.set_annotations(docs, outputs)\n        for doc in docs:\n            assert doc._.get(ATTRS.last_hidden_state) is not None\n        with self.disable_pipes(PIPES.tok2vec):\n            super().update(\n                docs,\n                golds,\n                drop=0.1,\n                sgd=sgd,\n                losses=losses,\n                component_cfg=component_cfg,\n            )\n        backprop_tok2vec(docs, sgd=sgd)\n\n    def resume_training(self, sgd=None, component_cfg=None, **kwargs):\n        """"""Continue training a pre-trained model.\n\n        Before running the normal Language.resume_training method, we do the\n        following:\n\n        * Look for a tok2vec pipeline component. The component name can be\n            changed with the tok2vec_name keyword\n            argument. If no component is found, a ValueError is raised.\n        * If any other components have `component.model == True` and a\n            `.begin_training()` method, we call the `.begin_training()` method.\n            Configuration can be passed in using the component_cfg keyword\n            argument. If unset, we also pass in a value for token_vector_width,\n            which we read from the tok2vec component.\n        """"""\n        if component_cfg is None:\n            component_cfg = {}\n        tok2vec_name = kwargs.get(""tok2vec_name"", PIPES.tok2vec)\n        tok2vec = self.get_pipe(tok2vec_name)\n        token_vector_width = tok2vec.token_vector_width\n        for name, component in self.pipeline:\n            if name == tok2vec_name:\n                continue\n            elif getattr(component, ""model"", None) is not True:\n                continue\n            elif not hasattr(component, ""begin_training""):\n                continue\n            cfg = component_cfg.get(name, {})\n            if ""tok2vec_name"" not in component_cfg:\n                cfg[""tok2vec_name""] = tok2vec_name\n            if ""token_vector_width"" not in component_cfg:\n                cfg[""token_vector_width""] = token_vector_width\n            component.cfg.update(cfg)\n            component.begin_training(pipeline=self.pipeline, sgd=False, **cfg)\n            assert component.model is not True\n        optimizer = super().resume_training(sgd=sgd, **kwargs)\n        optimizer.L2 = 0.0\n        return optimizer\n\n\ndef get_defaults(lang):\n    """"""Get the language-specific defaults, if available in spaCy.""""""\n    try:\n        lang_cls = get_lang_class(lang)\n        return lang_cls.Defaults\n    except ImportError:\n        return Language.Defaults\n\n\ndef get_wp_start(span):\n    if isinstance(span, Token):\n        span = span.doc[span.i : span.i + 1]\n    for token in span:\n        if token._.get(ATTRS.alignment):\n            wp_start = token._.get(ATTRS.alignment)[0]\n            break\n    else:\n        return None\n    wordpieces = span.doc._.get(ATTRS.word_pieces_)\n    # This is a messy way to check for the XLNet-style pattern, where we can\n    # have <sep> <cls>. In the BERT-style pattern, we have [cls] at start.\n    if is_special_token(wordpieces[0]):\n        if wp_start >= 1 and is_special_token(wordpieces[wp_start - 1]):\n            return wp_start - 1\n    return wp_start\n\n\ndef get_wp_end(span):\n    if isinstance(span, Token):\n        span = span.doc[span.i : span.i + 1]\n    for token in reversed(span):\n        if token._.get(ATTRS.alignment):\n            wp_end = token._.get(ATTRS.alignment)[-1]\n            break\n    else:\n        return None\n    wordpieces = span.doc._.get(ATTRS.word_pieces_)\n    if (wp_end + 1) < len(wordpieces) and is_special_token(wordpieces[wp_end + 1]):\n        wp_end += 1\n    # This is a messy way to check for the XLNet-style pattern, where we can\n    # have <sep> <cls>. In the BERT-style pattern, we have [cls] at start.\n    if not is_special_token(wordpieces[0]):\n        if (wp_end + 1) < len(wordpieces) and is_special_token(wordpieces[wp_end + 1]):\n            wp_end += 1\n    return wp_end\n\n\ndef get_span_wp_getter(attr):\n    def span_alignment_getter(span):\n        return [token._.get(attr) for token in span]\n\n    def span_getter(span):\n        start = span._.get(ATTRS.start)\n        end = span._.get(ATTRS.end)\n        if start is None and end is None:\n            return []\n        doc_values = span.doc._.get(attr)\n        start = start if start is not None else 0\n        if end is None:\n            return doc_values[start:]\n        return doc_values[start : end + 1]\n\n    if attr == ATTRS.alignment:\n        return span_alignment_getter\n    else:\n        return span_getter\n\n\ndef get_token_wp_getter(attr):\n    def token_alignment_getter(token):\n        doc_values = token.doc._.get(attr)\n        return doc_values[token.i] if doc_values is not None else None\n\n    def token_wordpiece_getter(token):\n        doc_values = token.doc._.get(attr)\n        start = token._.get(ATTRS.start)\n        end = token._.get(ATTRS.end)\n        if start is None and end is None:\n            return []\n        return [doc_values[i] for i in range(start, end + 1)]\n\n    if attr == ATTRS.alignment:\n        return token_alignment_getter\n    else:\n        return token_wordpiece_getter\n\n\ndef get_span_tok2vec_getter(attr):\n    def span_getter(span):\n        doc_activations = span.doc._.get(attr)\n        if doc_activations is None:\n            return None\n        wp_start = span[0]._.get(ATTRS.start)\n        wp_end = span[-1]._.get(ATTRS.end)\n        if wp_start is not None and wp_end is not None:\n            return doc_activations[wp_start : wp_end + 1]\n        else:\n            # Return empty slice.\n            return doc_activations[0:0]\n\n    return span_getter\n\n\ndef get_token_tok2vec_getter(attr):\n    def token_getter(token):\n        # Delegate through span, so get a span with just the token.\n        span = token.doc[token.i : token.i + 1]\n        return span._.get(attr)\n\n    return token_getter\n\n\ndef get_segments(doc):\n    separator = doc._.get(ATTRS.separator)\n    if separator is not None:\n        start = 0\n        for token in doc:\n            if token.text == separator:\n                yield doc[start : token.i + 1]\n                start = token.i + 1\n        yield doc[start:]\n    else:\n        yield doc[:]\n'"
spacy_transformers/model_registry.py,0,"b'from typing import Tuple, Callable, List, Optional\nfrom thinc.api import wrap, layerize, chain, flatten_add_lengths, with_getitem\nfrom thinc.t2v import Pooling, mean_pool\nfrom thinc.v2v import Softmax, Affine, Model\nfrom thinc.neural.util import get_array_module\nfrom spacy.tokens import Span, Doc\nfrom spacy._ml import PrecomputableAffine, flatten\nimport numpy\n\nfrom .wrapper import TransformersWrapper\nfrom .util import Array, Dropout, Optimizer\nfrom .util import batch_by_length, flatten_list, is_class_token\nfrom .util import get_segment_ids, is_special_token, ATTRS\nfrom .activations import Activations as Acts\nfrom .activations import RaggedArray\n\n\nREGISTRY = {}\n\n\ndef register_model(name: str, model=None):\n    """"""Decorator to register a model.""""""\n    global REGISTRY\n    if model is not None:\n        REGISTRY[name] = model\n        return model\n\n    def do_registration(model):\n        REGISTRY[name] = model\n        return model\n\n    return do_registration\n\n\ndef get_model_function(name: str):\n    """"""Get a model creation function from the registry by name.""""""\n    if name not in REGISTRY:\n        names = "", "".join(sorted(REGISTRY.keys()))\n        raise KeyError(f""Model {name} not found in registry. Available names: {names}"")\n    return REGISTRY[name]\n\n\n@register_model(""tok2vec_per_sentence"")\ndef tok2vec_per_sentence(model_name, cfg):\n    max_words = cfg.get(""words_per_batch"", 1000)\n    name = cfg[""trf_name""]\n\n    model = foreach_sentence(\n        chain(get_word_pieces(name), with_length_batching(model_name, max_words))\n    )\n    return model\n\n\n@register_model(""softmax_tanh_class_vector"")\ndef softmax_tanh_class_vector(nr_class, *, exclusive_classes=True, **cfg):\n    """"""Select features from the class-vectors from the last hidden state,\n    mean-pool them, apply a tanh-activated hidden layer, and then softmax-activated\n    output layer to produce one vector per document.\n    The gradients of the class vectors are incremented in the backward pass,\n    to allow fine-tuning.\n    """"""\n    width = cfg[""token_vector_width""]\n    return chain(\n        get_class_tokens,\n        flatten_add_lengths,\n        with_getitem(0, chain(Affine(width, width), tanh)),\n        Pooling(mean_pool),\n        Softmax(nr_class, width),\n    )\n\n\n@register_model(""softmax_class_vector"")\ndef softmax_class_vector(nr_class, *, exclusive_classes=True, **cfg):\n    """"""Select features from the class-vectors from the last hidden state,\n    mean-pool them, and apply a softmax-activated hidden layer to produce one\n    vector per document. The gradients of the class vectors are incremented\n    in the backward pass, to allow fine-tuning.\n    """"""\n    width = cfg[""token_vector_width""]\n    return chain(\n        get_class_tokens,\n        flatten_add_lengths,\n        Pooling(mean_pool),\n        Softmax(nr_class, width),\n    )\n\n\n@register_model(""softmax_last_hidden"")\ndef softmax_last_hidden(nr_class, *, exclusive_classes=True, **cfg):\n    """"""Select features from the class-vectors from the last hidden state,\n    mean-pool them, and softmax to produce one vector per document.\n    The gradients of the class vectors are incremented in the backward pass,\n    to allow fine-tuning.\n    """"""\n    width = cfg[""token_vector_width""]\n    return chain(\n        get_last_hidden,\n        flatten_add_lengths,\n        Pooling(mean_pool),\n        Softmax(nr_class, width),\n    )\n\n\n@register_model(""softmax_pooler_output"")\ndef softmax_pooler_output(nr_class, *, exclusive_classes=True, **cfg):\n    """"""Select features from the pooler output, (if necessary) mean-pool them\n    to produce one vector per item, and then softmax them.\n    The gradients of the class vectors are incremented in the backward pass,\n    to allow fine-tuning.\n    """"""\n    return chain(\n        get_pooler_output,\n        flatten_add_lengths,\n        with_getitem(0, Softmax(nr_class, cfg[""token_vector_width""])),\n        Pooling(mean_pool),\n    )\n\n@register_model(""tensor_affine_tok2vec"")\ndef tensor_affine_tok2vec(output_size, tensor_size, **cfg):\n    return chain(\n        get_tensors,\n        flatten,\n        Affine(output_size, tensor_size)\n    )\n\n\n@register_model(""precomputable_maxout"")\ndef precompute_hiddens(nO, nI, nF, nP, **cfg):\n    return PrecomputableAffine(hidden_width, nF=nr_feature,\n        nI=token_vector_width, nP=parser_maxout_pieces)\n \n\n@register_model(""affine_output"")\ndef affine_output(nO, nI, drop_factor, **cfg):\n    return Affine(nO, nI, drop_factor=drop_factor)\n \n\n@layerize\ndef get_tensors(docs, drop=0.0):\n    """"""Output a List[array], where the array is the tensor of each document.""""""\n    tensors = [doc.tensor for doc in docs]\n\n    def backprop_tensors(d_tensors, sgd=None):\n        for doc, d_t in zip(docs, d_tensor):\n            # Count how often each word-piece token is represented. This allows\n            # a weighted sum, so that we can make sure doc.tensor.sum()\n            # equals wp_tensor.sum(). Do this with sensitivity to boundary tokens\n            wp_rows, align_sizes = _get_boundary_sensitive_alignment(doc)\n            d_lh = _get_or_set_d_last_hidden_state(doc)\n            for i, word_piece_slice in enumerate(wp_rows):\n                for j in word_piece_slice:\n                    d_lh[j] += d_tensor[i]\n            xp = get_array_module(d_lh)\n            d_lh /= xp.array(align_sizes, dtype=""f"").reshape(-1, 1)\n            return None\n\n    return tensors, backprop_tensors\n\n\ndef _get_or_set_d_last_hidden_state(doc):\n    xp = get_array_model(doc._.get(ATTRS.last_hidden_state))\n    if doc._.get(ATTRS.d_last_hidden_state).size == 0:\n        shape = doc._.get(ATTRS.last_hidden_state).shape\n        dtype = doc._.get(ATTRS.last_hidden_state).dtype\n        doc._.set(ATTRS.d_last_hidden_state, xp.zeros(shape, dtype=dtype))\n    return doc._.get(ATTRS.d_last_hidden_state)\n\n\n@layerize\ndef get_class_tokens(docs, drop=0.0):\n    """"""Output a List[array], where the array is the class vector\n    for each sentence in the document. To backprop, we increment the values\n    in the Doc\'s d_last_hidden_state array.\n    """"""\n    xp = get_array_module(docs[0]._.get(ATTRS.last_hidden_state))\n    outputs = []\n    doc_class_tokens = []\n    for doc in docs:\n        class_tokens = []\n        for i, wp in enumerate(doc._.get(ATTRS.word_pieces_)):\n            if is_class_token(wp):\n                class_tokens.append(i)\n        doc_class_tokens.append(xp.array(class_tokens, dtype=""i""))\n        wp_tensor = doc._.get(ATTRS.last_hidden_state)\n        outputs.append(wp_tensor[doc_class_tokens[-1]])\n\n    def backprop_class_tokens(d_outputs, sgd=None):\n        for doc, class_tokens, dY in zip(docs, doc_class_tokens, d_outputs):\n            if doc._.get(ATTRS.d_last_hidden_state).size == 0:\n                xp = get_array_module(doc._.get(ATTRS.last_hidden_state))\n                grads = xp.zeros(doc._.get(ATTRS.last_hidden_state).shape, dtype=""f"")\n                doc._.set(ATTRS.d_last_hidden_state, grads)\n            doc._.get(ATTRS.d_last_hidden_state)[class_tokens] += dY\n        return None\n\n    return outputs, backprop_class_tokens\n\n\nget_class_tokens.name = ""get_class_tokens""\n\n\n@layerize\ndef get_pooler_output(docs, drop=0.0):\n    """"""Output a List[array], where the array is the class vector\n    for each sentence in the document. To backprop, we increment the values\n    in the Doc\'s d_last_hidden_state array.\n    """"""\n    for doc in docs:\n        if doc._.get(ATTRS.pooler_output) is None:\n            raise ValueError(\n                ""Pooler output unset. Perhaps you\'re using the wrong architecture? ""\n                ""The BERT model provides a pooler output, but XLNet doesn\'t. ""\n                ""You might need to set \'architecture\': \'softmax_class_vector\' ""\n                ""instead.""\n            )\n    outputs = [doc._.get(ATTRS.pooler_output) for doc in docs]\n\n    def backprop_pooler_output(d_outputs, sgd=None):\n        for doc, dY in zip(docs, d_outputs):\n            if doc._.get(ATTRS.d_pooler_output).size == 0:\n                xp = get_array_module(doc._.get(ATTRS.pooler_output))\n                grads = xp.zeros(doc._.get(ATTRS.pooler_output).shape, dtype=""f"")\n                doc._.set(ATTRS.d_pooler_output, grads)\n            doc._.set(ATTRS.d_pooler_output, doc._.get(ATTRS.d_pooler_output) + dY)\n        return None\n\n    return outputs, backprop_pooler_output\n\n\nget_pooler_output.name = ""get_pooler_output""\n\n\n@layerize\ndef get_last_hidden(docs, drop=0.0):\n    """"""Output a List[array], where the array is the last hidden vector vector\n    for each document. To backprop, we increment the values\n    in the Doc\'s d_last_hidden_state array.\n    """"""\n    outputs = [doc._.get(ATTRS.last_hidden_state) for doc in docs]\n    for out in outputs:\n        assert out is not None\n        assert out.size != 0\n\n    def backprop_last_hidden(d_outputs, sgd=None):\n        for doc, d_lh in zip(docs, d_outputs):\n            xp = get_array_module(d_lh)\n            shape = d_lh.shape\n            dtype = d_lh.dtype\n            if doc._.get(ATTRS.d_last_hidden_state).size == 0:\n                doc._.set(ATTRS.d_last_hidden_state, xp.zeros(shape, dtype=dtype))\n            doc._.set(\n                ATTRS.d_last_hidden_state, doc._.get(ATTRS.d_last_hidden_state) + d_lh\n            )\n        return None\n\n    return outputs, backprop_last_hidden\n\n\nget_last_hidden.name = ""get_last_hidden""\n\n\n@layerize\ndef softmax(X, drop=0.0):\n    ops = Model.ops\n    Y = ops.softmax(X)\n\n    def backprop_softmax(dY, sgd=None):\n        dX = ops.backprop_softmax(Y, dY)\n        return dX\n\n    return Y, backprop_softmax\n\n\n@layerize\ndef tanh(X, drop=0.0):\n    xp = get_array_module(X)\n    Y = xp.tanh(X)\n\n    def backprop_tanh(dY, sgd=None):\n        one = Y.dtype.type(1)\n        dX = dY * (one - Y * Y)\n        return dX\n\n    return Y, backprop_tanh\n\n\ndef get_word_pieces(transformers_name):\n    def get_features_forward(sents, drop=0.0):\n        assert isinstance(sents[0], Span)\n        ids = []\n        segment_ids = []\n        lengths = []\n        for sent in sents:\n            wordpieces = sent._.get(ATTRS.word_pieces)\n            # This is a bit convoluted, but we need the lengths without any\n            # separator tokens or class tokens. `segments` gives Span objects.\n            seg_lengths = [\n                len(\n                    [\n                        w\n                        for w in seg._.get(ATTRS.word_pieces_)\n                        if not is_special_token(w)\n                    ]\n                )\n                for seg in sent._.get(ATTRS.segments)\n            ]\n            if wordpieces:\n                ids.extend(wordpieces)\n                lengths.append(len(wordpieces))\n                sent_seg_ids = get_segment_ids(transformers_name, *seg_lengths)\n                segment_ids.extend(sent_seg_ids)\n                assert len(wordpieces) == len(sent_seg_ids), (\n                    sent._.get(ATTRS.word_pieces_),\n                    seg_lengths,\n                    len(wordpieces),\n                    len(sent_seg_ids),\n                )\n            else:\n                lengths.append(0)\n        assert len(ids) == len(segment_ids), (len(ids), len(segment_ids))\n        features = numpy.array(list(zip(ids, segment_ids)), dtype=numpy.int_)\n        assert features.shape[0] == sum(lengths), (features.shape, sum(lengths))\n        return RaggedArray(features, lengths), None\n\n    return layerize(get_features_forward, name=""get_features_forward"")\n\n\ndef with_length_batching(\n    model: TransformersWrapper, max_words: int\n) -> TransformersWrapper:\n    ops = model.ops\n\n    def apply_model_to_batches(\n        inputs: RaggedArray, drop: Dropout = 0.0\n    ) -> Tuple[Acts, Callable]:\n        if max_words == 0 or inputs.data.shape[0] < max_words:\n            return model.begin_update(inputs, drop=drop)\n        Xs: List[Array] = ops.unflatten(inputs.data, inputs.lengths)\n        outputs = None\n        backprops = []\n        index2rows = {}\n        start = 0\n        # Map each index to the slice of rows in the flattened data it refers to.\n        for i, length in enumerate(inputs.lengths):\n            index2rows[i] = [start + j for j in range(length)]\n            start += length\n        total_rows = sum(inputs.lengths)\n        for indices in batch_by_length(Xs, max_words):\n            X: Array = inputs.xp.concatenate([Xs[i] for i in indices])\n            lengths = [inputs.lengths[i] for i in indices]\n            Y, get_dX = model.begin_update(RaggedArray(X, lengths), drop=drop)\n            if outputs is None:\n                lh_shape = (total_rows, Y.lh.data.shape[-1])\n                po_shape = (len(inputs.lengths), Y.po.data.shape[-1])\n                outputs = Acts(\n                    RaggedArray(Y.lh.xp.zeros(lh_shape, dtype=""f""), inputs.lengths),\n                    RaggedArray(\n                        Y.po.xp.zeros(po_shape, dtype=""f""), [1 for _ in inputs.lengths]\n                    ),\n                )\n            lh_rows = []\n            po_rows = []\n            for index in indices:\n                lh_rows.extend(index2rows[index])\n                po_rows.append(index)\n            lh_rows = outputs.xp.array(lh_rows, dtype=""i"")\n            po_rows = outputs.xp.array(po_rows, dtype=""i"")\n            outputs.lh.data[lh_rows] = Y.lh.data\n            if outputs.has_po and po_rows.size:\n                outputs.po.data[po_rows] = Y.po.data\n            backprops.append((get_dX, lh_rows, po_rows, lengths))\n\n        def backprop_batched(d_outputs: Acts, sgd: Optimizer = None):\n            for get_dX, lh_rows, po_rows, lengths in backprops:\n                if d_outputs.has_lh:\n                    d_lh = d_outputs.lh.data[lh_rows]\n                    lh_lengths = lengths\n                else:\n                    d_lh = d_outputs.lh.data\n                    lh_lengths = []\n                if d_outputs.has_po:\n                    d_po = d_outputs.po.data[po_rows]\n                    po_lengths = [1 for _ in lengths]\n                else:\n                    d_po = d_outputs.po.data\n                    po_lengths = []\n                dY = Acts(RaggedArray(d_lh, lh_lengths), RaggedArray(d_po, po_lengths))\n                dX = get_dX(dY, sgd=sgd)\n                assert dX is None\n            return None\n\n        return outputs, backprop_batched\n\n    return wrap(apply_model_to_batches, model)\n\n\ndef foreach_sentence(layer: Model, drop_factor: float = 1.0) -> Model:\n    """"""Map a layer across sentences (assumes spaCy-esque .sents interface)""""""\n\n    def sentence_fwd(docs: List[Doc], drop: Dropout = 0.0) -> Tuple[Acts, Callable]:\n        if not all(doc.is_sentenced for doc in docs):\n            return layer.begin_update([d[:] for d in docs], drop=drop)\n        sents = flatten_list([list(doc.sents) for doc in docs])\n        words_per_doc = [len(d._.get(ATTRS.word_pieces)) for d in docs]\n        words_per_sent = [len(s._.get(ATTRS.word_pieces)) for s in sents]\n        sents_per_doc = [len(list(d.sents)) for d in docs]\n        assert sum(words_per_doc) == sum(words_per_sent)\n        acts, bp_acts = layer.begin_update(sents, drop=drop)\n        # To go from ""per sentence"" activations to ""per doc"" activations, we\n        # just have to tell it where the sequences end.\n        acts.lh.lengths = words_per_doc\n        acts.po.lengths = sents_per_doc\n\n        def sentence_bwd(d_acts: Acts, sgd: Optional[Optimizer] = None) -> None:\n            assert isinstance(d_acts, Acts)\n            # Translate back to the per-sentence activations\n            if d_acts.has_lh:\n                assert d_acts.lh.data.shape[0] == sum(d_acts.lh.lengths)\n                assert d_acts.lh.lengths == words_per_doc\n            d_acts.lh.lengths = words_per_sent\n            d_acts.po.lengths = [1 for _ in words_per_sent]\n            d_ids = bp_acts(d_acts, sgd=sgd)\n            if not (d_ids is None or all(ds is None for ds in d_ids)):\n                raise ValueError(""Expected gradient of sentence to be None"")\n            return d_ids\n\n        return acts, sentence_bwd\n\n    return wrap(sentence_fwd, layer)\n'"
spacy_transformers/util.py,0,"b'from typing import Union, List, Sequence, Callable, Any, Optional\nimport transformers\nimport numpy\nfrom spacy.tokens import Doc, Span\n\nfrom . import _tokenizers\n\ntry:\n    # This allows us to use cupy with mypy, for type checking\n    import cupy  # noqa\nexcept ImportError:\n    pass\n\ntry:  # Python 3.8\n    import importlib.metadata as importlib_metadata\nexcept ImportError:\n    import importlib_metadata  # noqa: F401\n\n\npkg_meta = importlib_metadata.metadata(__name__.split(""."")[0])\n\n\nArray = Union[""numpy.ndarray"", ""cupy.ndarray""]\nOptimizer = Callable[[Array, Array, Optional[int]], None]\nDropout = Optional[float]\n\n\nSPECIAL_TOKENS: Sequence[str] = (\n    ""[CLS]"",\n    ""[BOS]"",\n    ""[SEP]"",\n    ""<cls>"",\n    ""<sep>"",\n    ""<|endoftext|>"",\n    ""<s>"",\n    ""</s>"",\n)\n\n\nclass ATTRS(object):\n    alignment = ""trf_alignment""\n    word_pieces = ""trf_word_pieces""\n    word_pieces_ = ""trf_word_pieces_""\n    separator = ""trf_separator""\n    segments = ""trf_segments""\n    start = ""trf_start""\n    end = ""trf_end""\n    last_hidden_state = ""trf_last_hidden_state""\n    pooler_output = ""trf_pooler_output""\n    all_hidden_states = ""trf_all_hidden_states""\n    all_attentions = ""trf_all_attentions""\n    d_last_hidden_state = ""trf_d_last_hidden_state""\n    d_pooler_output = ""trf_d_pooler_output""\n    d_all_hidden_states = ""trf_d_all_hidden_states""\n    d_all_attentions = ""trf_d_all_attentions""\n\n\nclass PIPES(object):\n    wordpiecer = ""trf_wordpiecer""\n    tok2vec = ""trf_tok2vec""\n    textcat = ""trf_textcat""\n    ner = ""trf_ner""\n\n\nLANG_FACTORY = ""trf""\n\n\ndef get_config(name):\n    """"""Map a name to the appropriate transformers.*Config class.""""""\n    name = get_config_name(name)\n    if name.startswith(""roberta""):\n        return transformers.RobertaConfig\n    elif name.startswith(""distilbert""):\n        return transformers.DistilBertConfig\n    elif name.startswith(""bert""):\n        return transformers.BertConfig\n    elif name.startswith(""xlnet""):\n        return transformers.XLNetConfig\n    elif name.startswith(""gpt2""):\n        return transformers.GPT2Config\n    elif name.startswith(""xlm""):\n        return transformers.XLMConfig\n    else:\n        raise ValueError(f""Unsupported transformers config name: \'{name}\'"")\n\n\ndef get_model(name):\n    """"""Map a name to the appropriate transformers.*Model class.""""""\n    name = get_config_name(name)\n    if name.startswith(""roberta""):\n        return transformers.RobertaModel\n    elif name.startswith(""distilbert""):\n        return transformers.DistilBertModel\n    elif name.startswith(""bert""):\n        return transformers.BertModel\n    elif name.startswith(""xlnet""):\n        return transformers.XLNetModel\n    elif name.startswith(""gpt2""):\n        return transformers.GPT2Model\n    elif name.startswith(""xlm""):\n        return transformers.XLMModel\n    else:\n        raise ValueError(f""Unsupported transformers config name: \'{name}\'"")\n\n\ndef get_tokenizer(name):\n    """"""Get a transformers.*Tokenizer class from a name.""""""\n    name = get_config_name(name)\n    if name.startswith(""roberta""):\n        return _tokenizers.SerializableRobertaTokenizer\n    elif name.startswith(""distilbert""):\n        return _tokenizers.SerializableDistilBertTokenizer\n    elif name.startswith(""bert""):\n        return _tokenizers.SerializableBertTokenizer\n    elif name.startswith(""xlnet""):\n        return _tokenizers.SerializableXLNetTokenizer\n    elif name.startswith(""gpt2""):\n        return _tokenizers.SerializableGPT2Tokenizer\n    elif name.startswith(""xlm""):\n        return _tokenizers.SerializableXLMTokenizer\n    else:\n        raise ValueError(f""Unsupported transformers config name: \'{name}\'"")\n\n\ndef get_config_name(name):\n    try:\n        name = transformers.AutoConfig.from_pretrained(name).model_type\n    except EnvironmentError:\n        name = name.lower()\n        name = name.split(""/"")[-1]\n    return name\n\n\ndef pad_batch(\n    batch: List[Array], *, axis: int = 0, xp=numpy, to: int = 0, value: int = -1\n) -> Array:\n    """"""Pad a batch of arrays with zeros so that sequences are the same\n    length, and form them into a single array.\n    """"""\n    if not batch:\n        return xp.zeros((0, to))\n\n    if isinstance(batch[0], list):\n        batch = [xp.array(x, dtype=numpy.int_) for x in batch]\n\n    max_len = max((seq.shape[axis] for seq in batch), default=0)\n    if to < 1:\n        to = max_len\n    elif max_len > to:\n        raise ValueError(f""Cannot pad_batch with max len {max_len} to {to}."")\n\n    if isinstance(batch[0], list) or len(batch[0].shape) == 1:\n        return _pad_batch_1d(batch, xp=xp, to=to, value=value)\n    else:\n        return _pad_batch_nd(batch, axis=axis, xp=xp, to=to, value=value)\n\n\ndef _pad_batch_1d(batch: List[Array], *, xp=numpy, to: int, value) -> Array:\n    """"""Pad a batch of lists or 1d arrays with zeros so that sequences are the same\n    length, and form them into a single array.\n    """"""\n    padded: List[Array] = []\n    seq: Array\n    values = (0, value)\n    pad_desc = [[0, 0]]\n    for seq in batch:\n        pad_desc[0][1] = to - len(seq)\n        padded.append(xp.pad(seq, pad_desc, mode=""constant"", constant_values=values))\n    output = xp.vstack(padded)\n    assert output.shape == (len(batch), to), output.shape\n    return output\n\n\ndef _pad_batch_nd(\n    batch: List[Array], axis: int, *, xp=numpy, to: int = 0, value=-1\n) -> Array:\n    padded: List[Array] = []\n    seq: Array\n    values = (0, value)\n    pad_desc = [[0, 0] for _ in batch[0].shape]\n    for seq in batch:\n        # Ugh, numpy.pad sucks.\n        pad_desc[axis][1] = to - seq.shape[axis]\n        arr = xp.pad(seq, pad_desc, mode=""constant"", constant_values=values)\n        if len(arr.shape) == 2:\n            # This prevents us concatenating on the sequence dimension, when what\n            # we want is to have a new batch dimension.\n            arr = arr.reshape((1, arr.shape[0], arr.shape[1]))\n        padded.append(arr)\n    output = xp.vstack(padded)\n    return output\n\n\ndef batch_by_length(seqs: Union[List[Array]], max_words: int) -> List[List[int]]:\n    """"""Given a list of sequences, return a batched list of indices into the\n    list, where the batches are grouped by length, in descending order. Batches\n    may be at most max_words in size, defined as max sequence length * size.\n    """"""\n    # Use negative index so we can get sort by position ascending.\n    lengths_indices = [(len(seq), i) for i, seq in enumerate(seqs)]\n    lengths_indices.sort()\n    batches: List[List[int]] = []\n    batch: List[int] = []\n    for length, i in lengths_indices:\n        # i = -neg_i\n        if not batch:\n            batch.append(i)\n        elif length * (len(batch) + 1) <= max_words:\n            batch.append(i)\n        else:\n            batches.append(batch)\n            batch = [i]\n    if batch:\n        batches.append(batch)\n    # Check lengths match\n    assert sum(len(b) for b in batches) == len(seqs)\n    # Check no duplicates\n    seen = set()\n    for b in batches:\n        seen.update(b)\n    assert len(seen) == len(seqs)\n    batches = [list(sorted(batch)) for batch in batches]\n    batches.reverse()\n    return batches\n\n\ndef ensure3d(arr: Array, *, axis: int = 1) -> Array:\n    """"""Make sure an array is 3d, inserting a dimension at axis if not.""""""\n    if arr.size == 0:\n        return arr.reshape((0, 0, 0))\n    elif len(arr.shape) == 3:\n        return arr\n    elif len(arr.shape) == 2:\n        return arr.reshape((arr.shape[0], 1, arr.shape[1]))\n    else:\n        raise ValueError(f""Cannot make array 3d. Shape: {arr.shape}"")\n\n\ndef unflatten_list(flat: List[Any], lengths: List[int]) -> List[List[Any]]:\n    """"""Unflatten a list into nested sublists, where each sublist i should have\n    length lengths[i].""""""\n    nested: List[List[Any]] = []\n    offset = 0\n    for length in lengths:\n        nested.append(flat[offset : offset + length])\n        offset += length\n    return nested\n\n\ndef flatten_list(nested: List[List[Any]]) -> List[Any]:\n    """"""Flatten a nested list.""""""\n    flat = []\n    for x in nested:\n        flat.extend(x)\n    return flat\n\n\ndef lengths2mask(lengths):\n    """"""Get a boolean mask of which entries in a padded batch are valid, given\n    a list of lengths.""""""\n    padded = pad_batch([numpy.ones((L,), dtype=""i"") for L in lengths])\n    padded[padded < 0] = 0\n    return padded.reshape((-1,)) >= 1\n\n\ndef is_special_token(text: str) -> bool:\n    return text in SPECIAL_TOKENS\n\n\ndef is_class_token(text: str) -> bool:\n    return text == ""[CLS]"" or text == ""<cls>""\n\n\ndef get_segment_ids(name: str, *lengths) -> List[int]:\n    if len(lengths) == 1:\n        length1 = lengths[0]\n        length2 = 0\n    elif len(lengths) == 2:\n        length1, length2 = lengths\n    else:\n        msg = f""Expected 1 or 2 segments. Got {len(lengths)}""\n        raise ValueError(msg)\n    name = get_config_name(name)\n    if name.startswith(""bert""):\n        return get_bert_segment_ids(length1, length2)\n    elif name.startswith(""distilbert""):\n        return get_bert_segment_ids(length1, length2)\n    elif name.startswith(""xlnet""):\n        return get_xlnet_segment_ids(length1, length2)\n    elif name.startswith(""xlm""):\n        return get_xlm_segment_ids(length1, length2)\n    elif name.startswith(""gpt2""):\n        return get_gpt2_segment_ids(length1, length2)\n    elif name.startswith(""roberta""):\n        return get_roberta_segment_ids(length1, length2)\n\n    else:\n        raise ValueError(f""Unexpected model name: {name}"")\n\n\ndef get_bert_segment_ids(length1: int, length2: int) -> List[int]:\n    """"""Get an array of segment IDs in BERT\'s format, for an input with one or\n    two segments (set length2=0 for one segment). The lengths should be just the\n    wordpiece lengths, not including the SEP and CLS tokens.\n\n    According to the HF glue_utils.py module, the convention for BERT is:\n\n    (a) For sequence pairs:\n        tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        type_ids:   0   0  0    0    0     0     0   0   1  1  1  1   1   1\n\n    (b) For single sequences:\n        tokens:   [CLS] the dog is hairy . [SEP]\n        type_ids:   0   0   0   0  0     0   0\n    """"""\n    if length2:\n        return [0] * length1 + [0] + [0] + [1] * length2 + [1]\n    else:\n        return [0] * length1 + [0] + [0]\n\n\ndef get_xlnet_segment_ids(length1: int, length2: int) -> List[int]:\n    """"""Get an array of segment IDs in XLNet\'s format, for an input with one or\n    two segments (set length2=0 for one segment). The lengths should be just the\n    wordpiece lengths, not including the SEP and CLS tokens.\n\n    According to the XLNet code classifer_utils.py module, the convention is:\n\n    (a) For sequence pairs:\n        tokens:    is this jack ##son ##ville ? <sep> no it is not . <sep> <cls>\n        type_ids:   0   0  0    0      0      0  0    1   1  1  1  1   1   2\n\n    (b) For single sequences:\n        tokens:   the dog is hairy . <sep> <cls>\n        type_ids:   0   0 0   0    0   0   2\n    """"""\n    if length2:\n        return [0] * length1 + [0] + [1] * length2 + [1, 2]\n    else:\n        return [0] * length1 + [0, 2]\n\n\ndef get_xlm_segment_ids(length1: int, length2: int) -> List[int]:\n    """"""Get an array of segment IDs in XLNet\'s format, for an input with one or\n    two segments (set length2=0 for one segment). The lengths should be just the\n    wordpiece lengths, not including the SEP and CLS tokens.\n\n    According to the HF glue_utils.py script, the convention is:\n\n    (a) For sequence pairs:\n        tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        type_ids:   0   0  0    0    0     0     0   0   1  1  1  1   1   1\n\n    (b) For single sequences:\n        tokens:   [CLS] the dog is hairy . [SEP]\n        type_ids:   0   0   0   0  0     0   0\n    """"""\n    if length2:\n        return [0] * length1 + [0] + [0] + [1] * length2 + [1]\n    else:\n        [0] * length1 + [0] + [0]\n\n\ndef get_gpt2_segment_ids(length1: int, length2: int) -> List[int]:\n    """"""Get an array of segment IDs in GPT2\'s format, for an input with one or\n    two segments (set length2=0 for one segment). The lengths should be just the\n    wordpiece lengths, not including the SEP and CLS tokens.\n\n    I\'m really not sure how this should look? We currently require segment\n    boundaries, so we\'re just using the <|endoftext|> markers in their vocab?\n\n    (a) For sequence pairs:\n        tokens:   <|eot|> is this jack ##son ##ville ? <|eot|> no it is not . <|eot|>\n        type_ids:   0     0  0    0    0     0       0  0      1  1  1  1   1 1\n\n    (b) For single sequences:\n        tokens:   <|eot|> the dog is hairy . <|eot|>\n        type_ids:   0      0   0   0   0   0 0\n    """"""\n    if not length2:\n        return [0] + [0] * length1 + [0]\n    else:\n        return [0] + [0] * length1 + [0] + [1] * length2 + [1]\n\n\ndef get_roberta_segment_ids(length1: int, length2: int) -> List[int]:\n    # Roberta doesn\'t use Segment IDs\n    total = 1 + length1 + 1 + (1 + length2 + 1) * bool(length2)\n    return [0] * total\n\n\ndef get_sents(doc: Union[Span, Doc]) -> List[Span]:\n    if doc.is_sentenced:\n        return list(doc.sents)\n    else:\n        return [doc[:]]\n\n\ndef warmup_linear_rates(initial_rate, warmup_steps, total_steps):\n    """"""Generate a series, starting from an initial rate, and then with a warmup\n    period, and then a linear decline. Used for learning rates.\n    """"""\n    step = 0\n    while True:\n        if step < warmup_steps:\n            factor = step / max(1, warmup_steps)\n        else:\n            factor = max(\n                0.0, (total_steps - step) / max(1.0, total_steps - warmup_steps)\n            )\n        yield factor * initial_rate\n        step += 1\n\n\ndef cyclic_triangular_rate(min_lr, max_lr, period):\n    it = 1\n    while True:\n        # https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n        cycle = numpy.floor(1 + it / (2 * period))\n        x = numpy.abs(it / period - 2 * cycle + 1)\n        relative = max(0, 1 - x)\n        yield min_lr + (max_lr - min_lr) * relative\n        it += 1\n\n\nfrom .activations import Activations  # noqa\n'"
spacy_transformers/wrapper.py,12,"b'from thinc.extra.wrappers import PyTorchWrapper, xp2torch, torch2xp\nfrom transformers.optimization import AdamW\nimport transformers\nimport torch.autograd\nimport torch.nn.utils.clip_grad\nimport torch\nfrom typing import Tuple, Callable, Any\nfrom thinc.neural.optimizers import Optimizer\nimport numpy\nimport contextlib\nfrom thinc.compat import BytesIO\n\nfrom .util import get_model, Dropout\nfrom .activations import RaggedArray, Activations\n\n\nFINE_TUNE = True\nCONFIG = {""output_hidden_states"": True, ""output_attentions"": True}\n\n\nclass TransformersWrapper(PyTorchWrapper):\n    """"""Wrap a Transformers model for use in Thinc.\n\n    The model will take as input a spacy_transformers.util.RaggedArray\n    object that will specify the input IDs and optionally the segment IDs. The\n    RaggedArray is basically a tuple (ids, lengths), where ids is concatenated\n    for a whole batch (this format allows the data to be contiguous even if\n    the sequences are different lengths). The segment IDs should be coded as\n    the different models expect them -- see\n    https://github.com/huggingface/transformers/blob/master/examples/utils_glue.py\n    """"""\n\n    _model: Any\n    _optimizer: Any\n    cfg: dict\n\n    @classmethod\n    def from_pretrained(cls, name):\n        model_cls = get_model(name)\n        model = model_cls.from_pretrained(name, **CONFIG)\n        self = cls(name, model.config.to_dict(), model)\n        self.cfg.update(self.transformers_model.config.to_dict())\n        return self\n\n    def __init__(self, name, config, model):\n        PyTorchWrapper.__init__(self, model)\n        self.cfg = dict(config)\n\n    @property\n    def nO(self):\n        if ""hidden_size"" in self.cfg:\n            # BERT\n            return self.cfg[""hidden_size""]\n        elif ""hidden_dim"" in self.cfg:\n            # DistilBERT\n            return self.cfg[""hidden_dim""] // 4\n        elif ""n_embd"" in self.cfg:\n            # GPT2\n            return self.cfg[""n_embd""]\n        elif ""d_model"" in self.cfg:\n            # XLNet\n            return self.cfg[""d_model""]\n        elif hasattr(self.transformers_model, ""dim""):\n            # XLM\n            return self.transformers_model.dim\n        else:\n            keys = "", "".join(self.cfg.keys())\n            raise ValueError(f""Unexpected config. Keys: {keys}"")\n\n    @property\n    def transformers_model(self):\n        return self._model\n\n    @property\n    def max_length(self):\n        # `n_positions` in GPT2 config\n        return self.cfg.get(""max_position_embeddings"", self.cfg.get(""n_positions"", 128))\n\n    def predict(self, inputs: RaggedArray):\n        self._model.eval()\n        model_kwargs = self.get_model_kwargs(inputs)\n        with torch.no_grad():\n            if hasattr(self._optimizer, ""swap_swa_sgd""):\n                self._optimizer.swap_swa_sgd()\n            y_var = self._model(**model_kwargs)\n            if hasattr(self._optimizer, ""swap_swa_sgd""):\n                self._optimizer.swap_swa_sgd()\n        return self.make_activations(y_var, inputs.lengths)\n\n    def begin_update(\n        self, inputs: RaggedArray, drop: Dropout = 0.0\n    ) -> Tuple[Activations, Callable[..., None]]:\n        if drop is None:\n            # ""drop is None"" indicates prediction. It\'s one of the parts of\n            # Thinc\'s API I\'m least happy with...\n            return self.predict(inputs), lambda dY, sgd=None: None\n        max_original = max(inputs.lengths, default=0)\n        model_kwargs = self.get_model_kwargs(inputs)\n        self._model.train()\n        # Prepare all the model arguments, including the attention mask\n        y_var = self._model(**model_kwargs)\n        output = self.make_activations(y_var, inputs.lengths)\n        assert output.lh.data.shape[0] == inputs.data.shape[0], (\n            output.lh.data.shape,\n            inputs.data.shape,\n        )\n\n        def backward_pytorch(d_output: Activations, sgd: Optimizer = None) -> None:\n            y_for_bwd = []\n            dy_for_bwd = []\n            if d_output.has_lh:\n                assert d_output.lh.data.shape[0] == sum(d_output.lh.lengths)\n                d_lh = d_output.lh.to_padded(to=max_original)\n                if self.max_length and d_lh.shape[1] >= self.max_length:\n                    d_lh = d_lh[:, : self.max_length]\n                dy_for_bwd.append(xp2torch(d_lh))\n                y_for_bwd.append(y_var[0])\n            if d_output.has_po:\n                dy_for_bwd.append(xp2torch(d_output.po.data))\n                y_for_bwd.append(y_var[1])\n            if FINE_TUNE:\n                torch.autograd.backward(y_for_bwd, grad_tensors=dy_for_bwd)\n                if sgd is not None:\n                    if self._optimizer is None:\n                        self._optimizer = self._create_optimizer(sgd)\n                    if sgd.max_grad_norm:\n                        torch.nn.utils.clip_grad.clip_grad_norm_(\n                            self._model.parameters(), sgd.max_grad_norm\n                        )\n                    optimizer = self._optimizer\n                    for group in optimizer.param_groups:\n                        group[""lr""] = getattr(sgd, ""trf_lr"", sgd.alpha)\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    self._update_pytorch_averages(sgd)\n            return None\n\n        self._model.eval()\n        return output, backward_pytorch\n\n    @contextlib.contextmanager\n    def use_params(self, params):\n        key_prefix = f""pytorch_{self.id}_""\n        state_dict = {}\n        for k, v in params.items():\n            if hasattr(k, ""startswith"") and k.startswith(key_prefix):\n                state_dict[k.replace(key_prefix, """")] = xp2torch(v)\n        if state_dict:\n            backup = {k: v.clone() for k, v in self._model.state_dict().items()}\n            self._model.load_state_dict(state_dict)\n            yield\n            self._model.load_state_dict(backup)\n        else:\n            yield\n\n    def make_activations(self, fields, lengths) -> Activations:\n        """"""Create Activations from the output tuples produced by PyTorch Transformers.\n        Includes converting torch tensors to xp, and handling missing values.\n        """"""\n        fields = list(fields)\n        fields[0] = torch2xp(fields[0])\n        fields[0] = RaggedArray.from_padded(fields[0], lengths)\n        assert fields[0].data.shape[0] == sum(lengths)\n        # lh: last hidden\n        # po: pooler_output\n        # ah: all_hidden\n        # aa: all_attention\n        if len(fields) != 4:\n            lh = fields[0]\n            po = RaggedArray.blank()\n        else:\n            if isinstance(fields[1], tuple):\n                fields[1] = RaggedArray.blank()\n            else:\n                fields[1] = RaggedArray(torch2xp(fields[1]), [1] * len(lengths))\n            lh, po, _, _2 = fields\n        # Convert last_hidden_state to xp\n        return Activations(lh, po)\n\n    def get_model_kwargs(self, inputs):\n        padded = inputs.to_padded(value=-1)\n        if padded.ndim == 2:\n            padded = padded.reshape(padded.shape + (1,))\n        ids = padded[:, :, 0]\n        neg_idx = ids < 0\n        ids[neg_idx] = 0\n        ids = torch.as_tensor(ids, dtype=torch.int64)\n        if padded.shape[2] == 2:\n            segment_ids = padded[:, :, 1]\n            numpy.place(segment_ids, segment_ids<0, 0)\n            segment_ids = torch.as_tensor(segment_ids, dtype=torch.int64)\n        else:\n            segment_ids = torch.zeros_like(ids)\n        # Calculate ""attention mask"" for BERT and  XLNet, but not GPT2 (sigh)\n        if isinstance(self._model, (transformers.BertModel, transformers.XLNetModel)):\n            mask = self.ops.xp.ones(ids.shape, dtype=numpy.int_)\n            mask[neg_idx] = 0\n            mask = xp2torch(mask)\n            return {\n                ""input_ids"": ids,\n                ""attention_mask"": mask,\n                ""token_type_ids"": segment_ids,\n            }\n        elif isinstance(self._model, (transformers.DistilBertModel)):\n            # Mask, but no token type IDs for DistilBert (sigh again...)\n            mask = self.ops.xp.ones(ids.shape, dtype=numpy.int_)\n            mask[neg_idx] = 0\n            mask = xp2torch(mask)\n            return {""input_ids"": ids, ""attention_mask"": mask}\n        else:\n            return {""input_ids"": ids, ""token_type_ids"": segment_ids}\n\n    def _create_optimizer(self, sgd):\n        optimizer = AdamW(\n            self._model.parameters(),\n            lr=getattr(sgd, ""trf_lr"", sgd.alpha),\n            eps=sgd.eps,\n            betas=(sgd.b1, sgd.b2),\n            weight_decay=getattr(sgd, ""trf_weight_decay"", 0.0),\n        )\n        optimizer.zero_grad()\n        return optimizer\n\n    def _update_pytorch_averages(self, sgd, *, init_steps=1):\n        if sgd.averages is None:\n            return\n        # Collect parameters if we don\'t have them\n        for name, param in self._model.state_dict().items():\n            key = f""pytorch_{self.id}_{name}""\n            sgd.nr_update[key] += 1\n            xp_param = torch2xp(param)\n            if key in sgd.averages:\n                self.ops.update_averages(\n                    sgd.averages[key], xp_param, sgd.nr_update[key]\n                )\n            else:\n                sgd.averages[key] = xp_param.copy()\n                sgd.nr_update[key] = init_steps\n\n    def to_disk(self, path):\n        torch.save(self._model.state_dict(), str(path))\n\n    def from_disk(self, path):\n        if self.ops.device == ""cpu"":\n            map_location = ""cpu""\n        else:\n            map_location = ""cuda:0""\n        self._model.load_state_dict(torch.load(path, map_location=map_location))\n        self._model.to(map_location)\n\n    def to_bytes(self):\n        filelike = BytesIO()\n        torch.save(self._model.state_dict(), filelike)\n        filelike.seek(0)\n        return filelike.getvalue()\n\n    def from_bytes(self, data):\n        filelike = BytesIO(data)\n        filelike.seek(0)\n        if self.ops.device == ""cpu"":\n            map_location = ""cpu""\n        else:\n            map_location = ""cuda:0""\n        self._model.load_state_dict(torch.load(filelike, map_location=map_location))\n        self._model.to(map_location)\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b'import pytest\nfrom spacy_transformers import TransformersLanguage, TransformersWordPiecer\nfrom spacy_transformers import TransformersTok2Vec\n\nMODEL_NAMES = [""bert-base-uncased"", ""gpt2"", ""xlnet-base-cased""]\n\n\n@pytest.fixture(scope=""session"", params=MODEL_NAMES)\ndef name(request):\n    return request.param\n\n\n@pytest.fixture(scope=""session"")\ndef nlp(name):\n    p_nlp = TransformersLanguage(trf_name=name)\n    p_nlp.add_pipe(p_nlp.create_pipe(""sentencizer""))\n    p_nlp.add_pipe(TransformersWordPiecer.from_pretrained(p_nlp.vocab, trf_name=name))\n    p_nlp.add_pipe(TransformersTok2Vec.from_pretrained(p_nlp.vocab, name=name))\n    return p_nlp\n'"
tests/test_activations.py,0,"b'import pytest\nimport numpy\nfrom spacy_transformers.activations import Activations, RaggedArray\n\n\ndef test_act_blank():\n    acts = Activations.blank()\n    assert acts.lh.data.size == 0\n    assert acts.lh.lengths == []\n    assert acts.po.data.size == 0\n    assert acts.po.lengths == []\n    assert not acts.has_lh\n    assert not acts.has_po\n\n\n@pytest.mark.parametrize(\n    ""extra_dims,lengths,pad_to,expected_shape"",\n    [\n        ((), [1], 1, (1, 1)),\n        ((), [1, 2], -1, (2, 2)),\n        ((3,), [1, 2], -1, (2, 2, 3)),\n        ((3,), [1, 2, 5], -1, (3, 5, 3)),\n        ((3,), [1, 2, 5], 4, (3, 5, 3)),\n    ],\n)\ndef test_ragged_to_padded(extra_dims, lengths, pad_to, expected_shape):\n    arr = RaggedArray(numpy.ones((sum(lengths),) + extra_dims), lengths)\n    if pad_to > 1 and pad_to < max(lengths):\n        with pytest.raises(ValueError):\n            padded = arr.to_padded(to=pad_to)\n    else:\n        padded = arr.to_padded(to=pad_to)\n        assert padded.shape == expected_shape\n\n\n@pytest.mark.parametrize(\n    ""shape,lengths,expected_shape"",\n    [\n        ((1, 2), [3], (3,)),\n        ((2, 3), [3, 2], (5,)),\n        ((2, 4, 4), [4, 2], (6, 4)),\n        ((4, 5, 2), [5, 2, 1, 3], (11, 2)),\n    ],\n)\ndef test_ragged_from_padded(shape, lengths, expected_shape):\n    padded = numpy.ones(shape, dtype=""i"")\n    for i, length in enumerate(lengths):\n        padded[i, length:] = 0\n    ragged = RaggedArray.from_padded(padded, lengths)\n    assert ragged.data.shape == expected_shape\n    assert ragged.data.sum() == padded.sum()\n\n\n@pytest.mark.parametrize(\n    ""shape,lengths,expected_shape"",\n    [\n        ((1, 2), [3], (3,)),\n        ((2, 2), [3, 2], (5,)),\n        ((2, 2, 4), [3, 2], (5, 4)),\n        ((4, 2, 2), [5, 2, 1, 3], (11, 2)),\n    ],\n)\ndef test_ragged_from_truncated(shape, lengths, expected_shape):\n    truncated = numpy.ones(shape, dtype=""i"")\n    for i, length in enumerate(lengths):\n        truncated[i, length:] = 0\n    ragged = RaggedArray.from_truncated(truncated, lengths)\n    assert ragged.data.shape == expected_shape\n    assert ragged.data.sum() == truncated.sum()\n'"
tests/test_extensions.py,0,"b'import pytest\nfrom spacy_transformers import TransformersWordPiecer\nfrom spacy_transformers.util import ATTRS\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\n\n\n@pytest.fixture\ndef vocab():\n    return Vocab()\n\n\n@pytest.fixture\ndef wordpiecer(name):\n    return TransformersWordPiecer.from_pretrained(vocab, trf_name=name)\n\n\ndef test_alignment_extension_attr(vocab):\n    doc = Doc(vocab, words=[""hello"", ""world"", ""test""])\n    doc._.set(ATTRS.alignment, [[1, 2], [3, 4], [5, 6]])\n    assert doc[0]._.get(ATTRS.alignment) == [1, 2]\n    assert doc[1]._.get(ATTRS.alignment) == [3, 4]\n    assert doc[2]._.get(ATTRS.alignment) == [5, 6]\n    assert doc[0:2]._.get(ATTRS.alignment) == [[1, 2], [3, 4]]\n    assert doc[1:3]._.get(ATTRS.alignment) == [[3, 4], [5, 6]]\n\n\ndef test_wp_span_extension_attr(name, vocab, wordpiecer):\n    if name == ""gpt2"":\n        return\n    doc = Doc(vocab, words=[""hello"", ""world""])\n    for w in doc[1:]:\n        w.is_sent_start = False\n    doc = wordpiecer(doc)\n    assert len(doc._.get(ATTRS.word_pieces)) == 4\n    assert doc[0]._.get(ATTRS.start) == 0\n    assert doc[-1]._.get(ATTRS.end) == 3\n    assert len(doc[:]._.get(ATTRS.word_pieces)) == 4\n'"
tests/test_language.py,0,"b'import pytest\nfrom numpy.testing import assert_equal\nfrom spacy_transformers import TransformersLanguage, TransformersWordPiecer\nfrom spacy_transformers import TransformersTok2Vec, pkg_meta\nfrom spacy_transformers.util import ATTRS\nfrom spacy.attrs import LANG\n\nfrom .util import make_tempdir, is_valid_tensor\n\n\ndef test_language_init(name):\n    meta = {""lang"": ""en"", ""name"": ""test"", ""pipeline"": []}\n    nlp = TransformersLanguage(meta=meta, trf_name=name)\n    assert nlp.lang == ""en""\n    assert nlp.meta[""lang""] == ""en""\n    assert nlp.meta[""lang_factory""] == TransformersLanguage.lang_factory_name\n    assert nlp.vocab.lang == ""en""\n    # Make sure we really have the EnglishDefaults here\n    assert nlp.Defaults.lex_attr_getters[LANG](None) == ""en""\n    # Test requirements\n    package = f""{pkg_meta[\'title\']}>={pkg_meta[\'version\']}""\n    assert package in nlp.meta[""requirements""]\n\n\ndef test_language_run(nlp):\n    doc = nlp(""hello world"")\n    assert is_valid_tensor(doc.tensor)\n\n\ndef test_language_wordpiece_to_from_bytes(name):\n    nlp = TransformersLanguage()\n    nlp.add_pipe(nlp.create_pipe(""sentencizer""))\n    wordpiecer = TransformersWordPiecer.from_pretrained(nlp.vocab, trf_name=name)\n    nlp.add_pipe(wordpiecer)\n    doc = nlp(""hello world"")\n    assert doc._.get(ATTRS.word_pieces) is not None\n    nlp2 = TransformersLanguage()\n    nlp2.add_pipe(nlp.create_pipe(""sentencizer""))\n    nlp2.add_pipe(TransformersWordPiecer(nlp2.vocab))\n    with pytest.raises(ValueError):\n        new_doc = nlp2(""hello world"")\n    nlp2.from_bytes(nlp.to_bytes())\n    new_doc = nlp2(""hello world"")\n    assert new_doc._.get(ATTRS.word_pieces) is not None\n\n\ndef test_language_wordpiece_tok2vec_to_from_bytes(nlp, name):\n    doc = nlp(""hello world"")\n    assert is_valid_tensor(doc.tensor)\n    nlp2 = TransformersLanguage()\n    nlp2.add_pipe(nlp2.create_pipe(""sentencizer""))\n    nlp2.add_pipe(TransformersWordPiecer(nlp.vocab))\n    nlp2.add_pipe(TransformersTok2Vec(nlp.vocab))\n    with pytest.raises(ValueError):\n        new_doc = nlp2(""hello world"")\n    nlp2.from_bytes(nlp.to_bytes())\n    new_doc = nlp2(""hello world"")\n    assert is_valid_tensor(new_doc.tensor)\n    assert new_doc._.get(ATTRS.word_pieces) is not None\n\n\ndef test_language_to_from_disk(nlp, name):\n    doc = nlp(""hello world"")\n    assert is_valid_tensor(doc.tensor)\n    with make_tempdir() as tempdir:\n        nlp.to_disk(tempdir)\n        new_nlp = TransformersLanguage()\n        new_nlp.add_pipe(new_nlp.create_pipe(""sentencizer""))\n        wordpiecer = TransformersWordPiecer(new_nlp.vocab, trf_name=name)\n        tok2vec = TransformersTok2Vec(new_nlp.vocab, trf_name=name)\n        new_nlp.add_pipe(wordpiecer)\n        new_nlp.add_pipe(tok2vec)\n        new_nlp.from_disk(tempdir)\n    assert new_nlp.pipe_names == nlp.pipe_names\n    new_doc = new_nlp(""hello world"")\n    assert is_valid_tensor(new_doc.tensor)\n    assert_equal(doc.tensor, new_doc.tensor)\n'"
tests/test_model_registry.py,0,"b'import pytest\nimport numpy\nfrom thinc.api import layerize\nfrom spacy_transformers.model_registry import with_length_batching\nfrom spacy_transformers.activations import Activations, RaggedArray\n\n\ndef make_activations(inputs, width):\n    """"""Make an Activations object with the right sizing for an input.""""""\n    lh = numpy.ones((inputs.data.shape[0], width), dtype=""f"")\n    po = numpy.ones((len(inputs.lengths), width), dtype=""f"")\n    return Activations(\n        RaggedArray(lh, inputs.lengths), RaggedArray(po, [1 for _ in inputs.lengths])\n    )\n\n\ndef create_dummy_model(width):\n    record_fwd = []\n    record_bwd = []\n\n    @layerize\n    def dummy_model(inputs, drop=0.0):\n        def dummy_model_backward(d_outputs, sgd=None):\n            record_bwd.append(d_outputs)\n            return None\n\n        record_fwd.append(inputs)\n        return make_activations(inputs, width), dummy_model_backward\n\n    return dummy_model, record_fwd, record_bwd\n\n\n@pytest.mark.parametrize(\n    ""max_words,lengths,expected_batches"",\n    [(10, [4, 2, 2, 8, 2, 10], [[10], [8], [4], [2, 2, 2]]), (500, [9, 5], [[9, 5]])],\n)\ndef test_with_length_batching(max_words, lengths, expected_batches, width=12):\n    inputs = RaggedArray(numpy.arange(sum(lengths), dtype=""i""), lengths)\n    # The record_fwd and record_bwd variables will record the calls into the\n    # forward and backward passes, respectively. We can use this to check that\n    # the batching was done correctly.\n    dummy, record_fwd, record_bwd = create_dummy_model(width)\n    batched_dummy = with_length_batching(dummy, max_words)\n    outputs, backprop = batched_dummy.begin_update(inputs)\n    assert len(record_fwd) == len(expected_batches)\n    assert [b.lengths for b in record_fwd] == expected_batches\n    assert outputs.lh.data.shape == (inputs.data.shape[0], width)\n    assert outputs.po.data.shape == (len(inputs.lengths), width)\n    none = backprop(outputs)\n    assert none is None\n    assert len(record_bwd) == len(expected_batches)\n'"
tests/test_ner.py,0,"b'import pytest\nfrom spacy_transformers.pipeline import TransformersTextCategorizer\nfrom spacy_transformers.util import PIPES\nfrom spacy.gold import GoldParse\n\n\ndef textcat(name, nlp, request):\n    arch = request.param\n    tensor_size = nlp.get_pipe(PIPES.tok2vec).model.nO\n    ner = TransformersEntityRecognizer(nlp.vocab)\n    ner.add_label(""PERSON"")\n    config = {""trf_name"": name, ""tensor_size"": tensor_size}\n    ner.begin_training(**config)\n    return ner\n\n@pytest.mark.xfail\ndef test_ner_init(nlp):\n    ner = TransformersEntityRecognizer(nlp.vocab)\n    assert textcat.labels == ()\n    textcat.add_label(""PERSON"")\n    assert textcat.labels == (""PERSON"",)\n\n\n@pytest.mark.xfail\ndef test_ner_call(ner, nlp):\n    doc = nlp(""hello world"")\n\n\n@pytest.mark.xfail\ndef test_ner_update(ner, nlp):\n    doc = nlp(""hello matt"")\n    optimizer = nlp.resume_training()\n    ents = [""O"", ""U-PERSON""]\n    losses = {}\n    ner.update([doc], [GoldParse(doc, entities=ents)], sgd=optimizer, losses=losses)\n    assert PIPES.ner in losses\n\n\n@pytest.mark.xfail\ndef test_ner_update_multi_sentence(ner, nlp):\n    doc = nlp(""Hello matt. This is ian."")\n    assert len(list(doc.sents)) == 2\n    optimizer = nlp.resume_training()\n    ents = [""O"", ""U-PERSON"", ""O"", ""O"", ""O"", ""U-PERSON"", ""O""]\n    losses = {}\n    ner.update([doc], [GoldParse(doc, entities=ents)], sgd=optimizer, losses=losses)\n    assert PIPES.ner in losses\n\n\n@pytest.mark.xfail\ndef test_ner_update_batch(ner, nlp):\n    doc1 = nlp(""Hello world. This is sentence 2."")\n    doc2 = nlp(""Hi again. This is sentence 4."")\n    ents1 = [""O""] * len(doc1)\n    ents2 = [""O""] * len(doc2)\n    assert len(list(doc1.sents)) == 2\n    assert len(list(doc2.sents)) == 2\n    optimizer = nlp.resume_training()\n    golds = [GoldParse(doc1, entities=ents1), GoldParse(doc2, entities=ents2)]\n    losses = {}\n    ner.update([doc1, doc2], golds, sgd=optimizer, losses=losses)\n    assert PIPES.ner in losses\n'"
tests/test_textcat.py,0,"b'import pytest\nfrom spacy_transformers.pipeline import TransformersTextCategorizer\nfrom spacy_transformers.util import PIPES\nfrom spacy.gold import GoldParse\n\n\n@pytest.fixture(\n    params=[""softmax_last_hidden"", ""softmax_pooler_output"", ""softmax_class_vector""]\n)\ndef textcat(name, nlp, request):\n    arch = request.param\n    width = nlp.get_pipe(PIPES.tok2vec).model.nO\n    textcat = TransformersTextCategorizer(nlp.vocab, token_vector_width=width)\n    textcat.add_label(""Hello"")\n    config = {""architecture"": arch, ""trf_name"": name}\n    if name.startswith(""gpt2"") and arch in (\n        ""softmax_pooler_output"",\n        ""softmax_class_vector"",\n    ):\n        with pytest.raises(ValueError):\n            textcat.begin_training(**config)\n        textcat.begin_training(trf_name=name, architecture=""softmax_last_hidden"")\n    elif name.startswith(""xlnet"") and arch == ""softmax_pooler_output"":\n        with pytest.raises(ValueError):\n            textcat.begin_training(**config)\n        textcat.begin_training(trf_name=name, architecture=""softmax_last_hidden"")\n    else:\n        textcat.begin_training(**config)\n    return textcat\n\n\ndef test_textcat_init(nlp):\n    textcat = TransformersTextCategorizer(nlp.vocab)\n    assert textcat.labels == ()\n    textcat.add_label(""Hello"")\n    assert textcat.labels == (""Hello"",)\n\n\ndef test_textcat_call(textcat, nlp):\n    doc = nlp(""hello world"")\n    for label in textcat.labels:\n        assert label not in doc.cats\n    doc = textcat(doc)\n    for label in textcat.labels:\n        assert label in doc.cats\n\n\ndef test_textcat_update(textcat, nlp):\n    doc = nlp(""hello world"")\n    optimizer = nlp.resume_training()\n    cats = {""Hello"": 1.0}\n    losses = {}\n    textcat.update([doc], [GoldParse(doc, cats=cats)], sgd=optimizer, losses=losses)\n    assert PIPES.textcat in losses\n\n\ndef test_textcat_update_multi_sentence(textcat, nlp):\n    doc = nlp(""Hello world. This is sentence 2."")\n    assert len(list(doc.sents)) == 2\n    optimizer = nlp.resume_training()\n    cats = {""Hello"": 1.0}\n    losses = {}\n    textcat.update([doc], [GoldParse(doc, cats=cats)], sgd=optimizer, losses=losses)\n    assert PIPES.textcat in losses\n\n\ndef test_textcat_update_batch(textcat, nlp):\n    doc1 = nlp(""Hello world. This is sentence 2."")\n    doc2 = nlp(""Hi again. This is sentence 4."")\n    assert len(list(doc1.sents)) == 2\n    assert len(list(doc2.sents)) == 2\n    optimizer = nlp.resume_training()\n    golds = [GoldParse(doc1, cats={""Hello"": 1.0}), GoldParse(doc2, cats={""Hello"": 0.0})]\n    losses = {}\n    textcat.update([doc1, doc2], golds, sgd=optimizer, losses=losses)\n    assert PIPES.textcat in losses\n'"
tests/test_tok2vec.py,0,"b'import pytest\nfrom numpy.testing import assert_equal\nfrom spacy_transformers import TransformersTok2Vec\nfrom spacy_transformers.util import PIPES, ATTRS\nfrom spacy.vocab import Vocab\nimport pickle\n\nfrom .util import make_tempdir, is_valid_tensor\n\n\n@pytest.fixture\ndef docs(nlp):\n    texts = [""the cat sat on the mat."", ""hello world.""]\n    return [nlp(text) for text in texts]\n\n\n@pytest.fixture(scope=""session"")\ndef tok2vec(name, nlp):\n    return nlp.get_pipe(PIPES.tok2vec)\n\n\ndef test_from_pretrained(tok2vec, docs):\n    docs_out = list(tok2vec.pipe(docs))\n    assert len(docs_out) == len(docs)\n    for doc in docs_out:\n        diff = doc.tensor.sum() - doc._.get(ATTRS.last_hidden_state).sum()\n        assert abs(diff) <= 1e-2\n\n\ndef test_set_annotations(name, tok2vec, docs):\n    scores = tok2vec.predict(docs)\n    tok2vec.set_annotations(docs, scores)\n    for doc in docs:\n        assert doc._.get(ATTRS.last_hidden_state) is not None\n        if name.startswith(""bert""):\n            assert doc._.get(ATTRS.pooler_output) is not None\n        assert doc._.get(ATTRS.d_last_hidden_state) is not None\n        assert doc._.get(ATTRS.d_last_hidden_state).ndim == 2\n        assert doc._.get(ATTRS.d_pooler_output).ndim == 2\n\n\ndef test_begin_finish_update(tok2vec, docs):\n    scores, finish_update = tok2vec.begin_update(docs)\n    finish_update(docs)\n\n\n@pytest.mark.parametrize(\n    ""text1,text2,is_similar,threshold"",\n    [\n        (""The dog barked."", ""The puppy barked."", True, 0.5),\n        (""rats are cute"", ""cats please me"", True, 0.6),\n    ],\n)\ndef test_similarity(nlp, text1, text2, is_similar, threshold):\n    doc1 = nlp(text1)\n    doc2 = nlp(text2)\n    similarity = doc1.similarity(doc2)\n    if is_similar:\n        assert similarity >= threshold\n    else:\n        assert similarity < threshold\n\n\ndef test_tok2vec_to_from_bytes(tok2vec, docs):\n    doc = tok2vec(docs[0])\n    assert is_valid_tensor(doc.tensor)\n    bytes_data = tok2vec.to_bytes()\n    new_tok2vec = TransformersTok2Vec(Vocab(), **tok2vec.cfg)\n    with pytest.raises(ValueError):\n        new_doc = new_tok2vec(docs[0])\n    new_tok2vec.from_bytes(bytes_data)\n    new_doc = new_tok2vec(docs[0])\n    assert is_valid_tensor(new_doc.tensor)\n    assert_equal(doc.tensor, new_doc.tensor)\n\n\ndef test_tok2vec_to_from_disk(tok2vec, docs):\n    doc = tok2vec(docs[0])\n    assert is_valid_tensor(doc.tensor)\n    with make_tempdir() as tempdir:\n        file_path = tempdir / ""tok2vec""\n        tok2vec.to_disk(file_path)\n        new_tok2vec = TransformersTok2Vec(Vocab())\n        new_tok2vec.from_disk(file_path)\n    new_doc = new_tok2vec(docs[0])\n    assert is_valid_tensor(new_doc.tensor)\n    assert_equal(doc.tensor, new_doc.tensor)\n\n\n# https://github.com/cloudpipe/cloudpickle/pull/299\n@pytest.mark.skip(reason=""Fails on 3.7 due to type annotations"")\ndef test_tok2vec_pickle_dumps_loads(tok2vec, docs):\n    doc = tok2vec(docs[0])\n    assert is_valid_tensor(doc.tensor)\n    pkl_data = pickle.dumps(tok2vec)\n    new_tok2vec = pickle.loads(pkl_data)\n    new_doc = new_tok2vec(docs[0])\n    assert is_valid_tensor(new_doc.tensor)\n    assert_equal(doc.tensor, new_doc.tensor)\n'"
tests/test_util.py,0,"b'import pytest\nimport numpy\nfrom numpy.testing import assert_equal\nfrom spacy_transformers.pipeline.wordpiecer import align_word_pieces\nfrom spacy_transformers.util import pad_batch, batch_by_length, lengths2mask\n\n\n@pytest.mark.parametrize(\n    ""spacy_tokens,wp_tokens,expected_alignment"",\n    [\n        ([""a""], [""a""], [[0]]),\n        ([""a"", ""b"", ""c""], [""a"", ""b"", ""c""], [[0], [1], [2]]),\n        ([""ab"", ""c""], [""a"", ""b"", ""c""], [[0, 1], [2]]),\n        ([""a"", ""b"", ""c""], [""ab"", ""c""], [[0], [0], [1]]),\n        ([""ab"", ""cd""], [""a"", ""bc"", ""d""], [[0, 1], [1, 2]]),\n        ([""abcd""], [""ab"", ""cd""], [[0, 1]]),\n        ([], [], []),\n        (\n            [""it"", ""realllllllly"", ""sucks"", "".""],\n            [""it"", ""real"", ""ll"", ""ll"", ""ll"", ""ly"", ""suck"", ""s"", "".""],\n            [[0], [1, 2, 3, 4, 5], [6, 7], [8]],\n        ),\n        ([""Well"", "","", ""i""], [""Well"", "","", """", ""i""], [[0], [1, 2], [2, 3]]),\n    ],\n)\ndef test_align_word_pieces(spacy_tokens, wp_tokens, expected_alignment):\n    output = align_word_pieces(spacy_tokens, wp_tokens)\n    assert output == expected_alignment\n\n\n@pytest.mark.parametrize(\n    ""lengths,max_words,expected"",\n    [\n        ([1, 2, 2, 4], 4, [[3], [2], [0, 1]]),\n        ([1, 2, 2, 4], 3, [[3], [2], [1], [0]]),\n        ([4, 2, 2, 1], 4, [[0], [2], [1, 3]]),\n        ([4, 4, 2, 2, 1], 6, [[1], [0], [2, 3, 4]]),\n        ([10, 7, 2, 2, 1], 10, [[0], [1], [2, 3, 4]]),\n    ],\n)\ndef test_batch_by_length(lengths, max_words, expected):\n    seqs = [""a"" * length for length in lengths]\n    batches = batch_by_length(seqs, max_words)\n    assert batches == expected\n\n\n@pytest.mark.parametrize(\n    ""lengths,expected_shape"",\n    [\n        ([1, 2], (2, 2)),\n        ([1, 2, 3], (3, 3)),\n        ([0, 1], (2, 1)),\n        ([1], (1, 1)),\n        ([1, 5, 2, 4], (4, 5)),\n    ],\n)\ndef test_pad_batch(lengths, expected_shape):\n    seqs = [numpy.ones((length,), dtype=""f"") for length in lengths]\n    padded = pad_batch(seqs)\n    for i, seq in enumerate(seqs):\n        padded[i][padded[i] < 0] = 0\n        assert padded[i].sum() == seq.sum()\n        assert_equal(padded[i, : len(seq)], seq)\n    assert padded.shape == expected_shape\n\n\n@pytest.mark.parametrize(\n    ""lengths,width,expected_shape"",\n    [\n        ([1, 2], 5, (2, 2, 5)),\n        ([1, 2, 3], 2, (3, 3, 2)),\n        ([0, 1], 3, (2, 1, 3)),\n        ([1], 4, (1, 1, 4)),\n        ([1, 5, 2, 4], 6, (4, 5, 6)),\n    ],\n)\ndef test_pad_batch_2d(lengths, width, expected_shape):\n    seqs = [numpy.ones((length, width), dtype=""f"") for length in lengths]\n    padded = pad_batch(seqs)\n    for i, seq in enumerate(seqs):\n        padded[i][padded[i] < 0] = 0\n        assert padded[i].sum() == seq.sum()\n        assert_equal(padded[i, : len(seq)], seq)\n    assert padded.shape == expected_shape\n\n\n@pytest.mark.parametrize(""lengths"", [[1], [1, 2], [5, 3, 1]])\ndef test_lengths2mask(lengths):\n    mask = lengths2mask(lengths)\n    assert mask.ndim == 1\n    assert mask.sum() == sum(lengths)\n    assert mask.size == max(lengths) * len(lengths)\n'"
tests/test_wordpiecer.py,0,"b'import pytest\nfrom spacy_transformers import TransformersWordPiecer\nfrom spacy_transformers.util import is_special_token, get_tokenizer, ATTRS\nfrom spacy.vocab import Vocab\nfrom spacy.tokens import Doc\nfrom spacy.pipeline import Sentencizer\n\n\n@pytest.fixture(scope=""session"")\ndef wp(name):\n    return TransformersWordPiecer.from_pretrained(Vocab(), trf_name=name)\n\n\n@pytest.fixture(scope=""session"")\ndef sentencizer():\n    return Sentencizer()\n\n\ndef test_wordpiecer(wp):\n    words = [""hello"", ""world"", ""this"", ""is"", ""a"", ""test""]\n    doc = Doc(wp.vocab, words=words)\n    doc[0].is_sent_start = True\n    doc[1].is_sent_start = False\n    doc = wp(doc)\n    cleaned_words = [wp.model.clean_wp_token(t) for t in doc._.get(ATTRS.word_pieces_)]\n    cleaned_words = [w for w in cleaned_words if not is_special_token(w)]\n    assert """".join(cleaned_words) == """".join(words)\n\n\n@pytest.mark.parametrize(\n    ""words,target_name,expected_align"",\n    [\n        (\n            [""hello"", ""world"", ""this"", ""is"", ""a"", ""teest""],\n            ""bert-base-uncased"",\n            [[1], [2], [3], [4], [5], [6, 7]],\n        ),\n        (\n            [""hello"", ""world"", ""this"", ""is"", ""a"", ""teest""],\n            ""xlnet-base-cased"",\n            [[0], [1], [2], [3], [4], [5, 6]],\n        ),\n        ([""\xc3\xa5\\taa"", ""."", ""\xe3\x81\x8c\\n\xcf\x80""], ""bert-base-uncased"", [[1, 2], [3], [6, 7]]),\n        ([""\xc3\xa5\\taa"", ""."", ""\xe3\x81\x8c\\n\xcf\x80""], ""xlnet-base-cased"", [[0, 1, 2], [4], [7, 8, 10]]),\n        ([""\\u3099""], ""bert-base-uncased"", [[]]),\n        ([""I.\\n\\n\\n\\n\\n""], ""bert-base-uncased"", [[1, 2]]),\n    ],\n)\ndef test_align(wp, sentencizer, name, words, target_name, expected_align):\n    if name != target_name:\n        pytest.skip()\n    doc = Doc(wp.vocab, words=words)\n    doc = sentencizer(doc)\n    doc = wp(doc)\n    assert doc._.get(ATTRS.alignment) == expected_align\n\n\ndef test_xlnet_weird_align(name, wp):\n    if ""xlnet"" not in name.lower():\n        pytest.skip()\n    text = ""Well, i rented this movie and found out it realllllllly sucks.""\n    spacy_tokens = [\n        ""Well"",\n        "","",\n        ""i"",\n        ""rented"",\n        ""this"",\n        ""movie"",\n        ""and"",\n        ""found"",\n        ""out"",\n        ""it"",\n        ""realllllllly"",\n        ""sucks"",\n        ""."",\n    ]\n    spaces = [True] * len(spacy_tokens)\n    spaces[0] = False\n    spaces[-2] = False\n    spaces[-1] = False\n    doc = Doc(wp.vocab, words=spacy_tokens, spaces=spaces)\n    doc[1].is_sent_start = False\n    assert doc.text == text\n    doc = wp(doc)\n    assert doc._.get(ATTRS.word_pieces_)[-2] == ""</s>""\n    assert doc._.get(ATTRS.word_pieces_)[-1] == ""<cls>""\n\n\ndef test_tokenizers_to_from_bytes(name):\n    text = ""hello world""\n    tokenizer_cls = get_tokenizer(name)\n    tokenizer = tokenizer_cls.from_pretrained(name)\n    doc = tokenizer.tokenize(text)\n    assert isinstance(doc, list) and len(doc)\n    bytes_data = tokenizer.to_bytes(name)\n    new_tokenizer = tokenizer_cls.blank().from_bytes(bytes_data)\n    new_doc = new_tokenizer.tokenize(text)\n    assert isinstance(new_doc, list) and len(new_doc)\n    assert doc == new_doc\n'"
tests/test_wrapper.py,0,"b'import numpy\nimport pytest\nfrom thinc.neural.optimizers import Adam\nfrom spacy_transformers.activations import Activations, RaggedArray\nfrom spacy_transformers.util import get_tokenizer, PIPES\n\n\n@pytest.fixture\ndef tokenizer(name):\n    return get_tokenizer(name).from_pretrained(name)\n\n\n@pytest.fixture\ndef ids(tokenizer):\n    text = ""the cat sat on the mat""\n    return numpy.array(tokenizer.encode(text), dtype=numpy.int_)\n\n\n@pytest.fixture\ndef inputs(ids):\n    return RaggedArray(ids, [len(ids)])\n\n\n@pytest.fixture\ndef model(nlp):\n    return nlp.get_pipe(PIPES.tok2vec).model._model\n\n\ndef test_wrapper_from_pretrained(name, model, inputs):\n    outputs, backprop = model.begin_update(inputs)\n    assert outputs.has_lh\n    optimizer = Adam(model.ops, 0.001)\n    d_outputs = Activations(outputs.lh, RaggedArray.blank())\n    backprop(d_outputs, sgd=optimizer)\n'"
tests/util.py,0,b'from contextlib import contextmanager\nfrom pathlib import Path\nimport tempfile\nimport shutil\nimport numpy\n\n\n@contextmanager\ndef make_tempdir():\n    d = Path(tempfile.mkdtemp())\n    yield d\n    shutil.rmtree(str(d))\n\n\ndef is_valid_tensor(tensor):\n    return tensor is not None and numpy.nonzero(tensor) and tensor.size != 0\n'
examples/tasks/glue_util.py,0,"b'# coding: utf8\n""""""\nUtilities to work with the GLUE shared task data.\n\nAdapted from Huggingface\'s transformers.\n""""""\nimport csv\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Iterator, List, Tuple\n\n\n@dataclass\nclass InputExample:\n    """"""A single training/test example for simple sequence classification.\n\n    Args:\n        guid: Unique id for the example.\n        text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n        text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n        label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    """"""\n\n    guid: int\n    text_a: str\n    text_b: str = """"\n    label: str = """"\n\n\ndef read_train_data(data_dir: Path, task: str) -> List[InputExample]:\n    return PROCESSORS[task]().get_train_examples(data_dir)\n\n\ndef read_dev_data(data_dir: Path, task: str) -> List[InputExample]:\n    return PROCESSORS[task]().get_dev_examples(data_dir)\n\n\ndef describe_task(task: str) -> dict:\n    T = PROCESSORS[task]()\n    return dict(task_name=T.name, task_type=T.task, labels=T.labels)\n\n\nclass DataProcessor:\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    name: str\n    task: str\n    labels: Tuple[str, ...]\n    train_filename: str\n    dev_filename: str\n    subdir: str\n\n    @property\n    def train_name(self) -> str:\n        """"""The partition-name used for the training data. Usually matches\n        train_filename but without the extension.\n        """"""\n        return self.train_filename.rsplit(""."", 1)[0]\n\n    @property\n    def dev_name(self) -> str:\n        """"""The partition-name used for the dev data. Usually matches\n        dev_filename but without the extension.\n        """"""\n        return self.dev_filename.rsplit(""."", 1)[0]\n\n    def get_train_examples(self, data_dir: Path) -> List[InputExample]:\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        filename = data_dir / self.subdir / self.train_filename\n        return list(self._read_examples(filename, self.train_name))\n\n    def get_dev_examples(self, data_dir) -> List[InputExample]:\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        filename = data_dir / self.subdir / self.dev_filename\n        return list(self._read_examples(filename, self.dev_name))\n\n    def _read_examples(\n        self, path: Path, set_type: str, quote=None\n    ) -> Iterator[InputExample]:\n        """"""Creates examples for the training and dev sets.""""""\n        with path.open(""r"", encoding=""utf-8-sig"") as file_:\n            reader = csv.reader(file_, delimiter=""\\t"", quotechar=quote)\n            for i, line in enumerate(reader):\n                if i == 0:\n                    continue\n                example = self.create_example(i, set_type, line)\n                if example is not None:\n                    yield example\n\n    def create_example(self, i: int, set_type: str, line: List[str]) -> InputExample:\n        raise NotImplementedError\n\n\nclass MrpcProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    name = ""mrpc""\n    subdir = ""MRPC""\n    task = ""classification""\n\n    labels = (""0"", ""1"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = f""{i}-{set_type}""\n        return InputExample(guid, line[3], line[4], line[0])\n\n\nclass MnliProcessor(DataProcessor):\n    """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n    name = ""mnli""\n    subdir = ""MNLI""\n    task = ""classification""\n    labels = (""contradiction"", ""entailment"", ""neutral"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev_matched.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = f""{set_type}-{line[0]}""\n        return InputExample(guid, line[8], line[9], line[-1])\n\n\nclass MnliMismatchedProcessor(DataProcessor):\n    """"""Processor for the MultiNLI Mismatched data set (GLUE version).""""""\n\n    name = ""mnli-mm""\n    subdir = ""MNLI""\n    task = ""classification""\n\n    labels = (""contradiction"", ""entailment"", ""neutral"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev_matched.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = f""{set_type}-{line[0]}""\n        return InputExample(guid, line[8], line[9], line[-1])\n\n\nclass ColaProcessor(DataProcessor):\n    name = ""cola""\n    subdir = ""CoLA""\n    task = ""classification""\n\n    labels = (""0"", ""1"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = f""{set_type}-{i}""\n        return InputExample(guid, line[3], """", line[1])\n\n\nclass Sst2Processor(DataProcessor):\n    """"""Processor for the SST-2 data set (GLUE version).""""""\n\n    name = ""sst2""\n    subdir = ""SST-2""\n    task = ""classification""\n\n    labels = (""0"", ""1"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = f""{set_type}-{i}""\n        return InputExample(guid, line[0], """", line[1])\n\n\nclass StsbProcessor(DataProcessor):\n    """"""Processor for the STS-B data set (GLUE version).""""""\n\n    name = ""sts-b""\n    subdir = ""STS-B""\n    task = ""regression""\n\n    labels = ("""",)\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = f""{set_type}-{line[0]}""\n        return InputExample(guid, line[7], line[8], line[-1])\n\n\nclass QqpProcessor(DataProcessor):\n    """"""Processor for the QQP data set (GLUE version).""""""\n\n    name = ""qqp""\n    subdir = ""QQP""\n    task = ""classification""\n\n    labels = (""0"", ""1"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = f""{set_type}-{line[0]}""\n        try:\n            text_a = line[3]\n            text_b = line[4]\n            label = line[5]\n        except IndexError:\n            return None\n        return InputExample(guid, text_a, text_b, label)\n\n\nclass QnliProcessor(DataProcessor):\n    """"""Processor for the QNLI data set (GLUE version).""""""\n\n    name = ""qnli""\n    subdir = ""QNLI""\n    task = ""classification""\n\n    labels = (""entailment"", ""not_entailment"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    @property\n    def dev_name(self):\n        return ""dev_matched""\n\n    def create_example(self, i, set_type, line):\n        """"""Creates examples for the training and dev sets.""""""\n        guid = f""{set_type}-{line[0]}""\n        return InputExample(guid, line[1], line[2], line[-1])\n\n\nclass RteProcessor(DataProcessor):\n    """"""Processor for the RTE data set (GLUE version).""""""\n\n    name = ""rte""\n    subdir = ""RTE""\n    task = ""classification""\n    labels = (""entailment"", ""not_entailment"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    def create_example(self, i, set_type, line):\n        """"""Creates examples for the training and dev sets.""""""\n        guid = f""{set_type}-{line[0]}""\n        return InputExample(guid, line[1], line[2], line[-1])\n\n\nclass WnliProcessor(DataProcessor):\n    """"""Processor for the WNLI data set (GLUE version).""""""\n\n    name = ""wnli""\n    subdir = ""WNLI""\n    task = ""classification""\n    labels = (""0"", ""1"")\n    train_filename = ""train.tsv""\n    dev_filename = ""dev.tsv""\n\n    def create_example(self, i, set_type, line):\n        guid = (f""{set_type}-{line[0]}"",)\n        return InputExample(guid, line[1], line[2], line[-1])\n\n\nPROCESSORS_LIST = [\n    ColaProcessor,\n    MnliProcessor,\n    MnliMismatchedProcessor,\n    MrpcProcessor,\n    Sst2Processor,\n    StsbProcessor,\n    QqpProcessor,\n    QnliProcessor,\n    RteProcessor,\n    WnliProcessor,\n]\n\nPROCESSORS = {proc.name: proc for proc in PROCESSORS_LIST}\n'"
examples/tasks/metrics.py,0,"b'from scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\n\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean()\n\n\ndef acc_and_f1(preds, labels):\n    acc = simple_accuracy(preds, labels)\n    f1 = f1_score(y_true=labels, y_pred=preds)\n    return {""acc"": acc, ""f1"": f1, ""acc_and_f1"": (acc + f1) / 2}\n\n\ndef pearson_and_spearman(preds, labels):\n    pearson_corr = pearsonr(preds, labels)[0]\n    spearman_corr = spearmanr(preds, labels)[0]\n    return {\n        ""pearson"": pearson_corr,\n        ""spearmanr"": spearman_corr,\n        ""corr"": (pearson_corr + spearman_corr) / 2,\n    }\n\n\ndef compute_metrics(task_name, preds, labels):\n    assert len(preds) == len(labels)\n    if task_name == ""cola"":\n        return ""mcc"", {""mcc"": matthews_corrcoef(labels, preds)}\n    elif task_name == ""sst2"":\n        return ""acc"", {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""mrpc"":\n        return ""acc_and_f1"", acc_and_f1(preds, labels)\n    elif task_name == ""sts-b"":\n        return ""corr"", pearson_and_spearman(preds, labels)\n    elif task_name == ""qqp"":\n        return ""acc_and_f1"", acc_and_f1(preds, labels)\n    elif task_name == ""mnli"":\n        return ""acc"", {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""mnli-mm"":\n        return ""acc"", {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""qnli"":\n        return ""acc"", {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""rte"":\n        return ""acc"", {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""wnli"":\n        return ""acc"", {""acc"": simple_accuracy(preds, labels)}\n    else:\n        raise KeyError(task_name)\n'"
examples/tasks/run_glue.py,2,"b'import plac\nimport random\nimport torch\nimport spacy\nimport spacy.util\nimport tqdm\nimport numpy\nimport wasabi\nfrom spacy.gold import GoldParse\n\nfrom collections import Counter\nfrom pathlib import Path\nfrom spacy.util import minibatch\n\nfrom spacy_transformers.util import cyclic_triangular_rate, PIPES, ATTRS\nfrom spacy_transformers.hyper_params import get_hyper_params\n\nfrom glue_util import read_train_data, read_dev_data\nfrom glue_util import describe_task\nfrom metrics import compute_metrics\n\n\ndef create_model(model_name, *, task_type, task_name, labels):\n    nlp = spacy.load(model_name)\n    textcat = nlp.create_pipe(PIPES.textcat, config={""architecture"": HP.textcat_arch})\n    for label in labels:\n        textcat.add_label(label)\n    nlp.add_pipe(textcat)\n    optimizer = nlp.resume_training()\n    optimizer.alpha = HP.learning_rate\n    optimizer.max_grad_norm = HP.max_grad_norm\n    optimizer.eps = HP.adam_epsilon\n    optimizer.L2 = 0.0\n    return nlp, optimizer\n\n\ndef train_epoch(nlp, optimizer, train_data):\n    # This isn\'t the recommended code -- but it\'s the easiest way to do the\n    # experiment for now.\n    global HP\n    random.shuffle(train_data)\n    batches = minibatch(train_data, size=HP.batch_size)\n    tok2vec = nlp.get_pipe(PIPES.tok2vec)\n    textcat = nlp.get_pipe(PIPES.textcat)\n    for batch in batches:\n        docs, golds = zip(*batch)\n        tokvecs, backprop_tok2vec = tok2vec.begin_update(docs, drop=HP.dropout)\n        losses = {}\n        tok2vec.set_annotations(docs, tokvecs)\n        textcat.update(docs, golds, drop=HP.dropout, sgd=optimizer, losses=losses)\n        backprop_tok2vec(docs, sgd=optimizer)\n        yield batch, losses\n        for doc in docs:\n            free_tensors(doc)\n\n\ndef evaluate(nlp, task, docs_golds):\n    tok2vec = nlp.get_pipe(PIPES.tok2vec)\n    textcat = nlp.get_pipe(PIPES.textcat)\n    right = 0\n    total = 0\n    guesses = []\n    truths = []\n    labels = textcat.labels\n    for batch in minibatch(docs_golds, size=HP.eval_batch_size):\n        docs, golds = zip(*batch)\n        docs = list(textcat.pipe(tok2vec.pipe(docs)))\n        for doc, gold in zip(docs, golds):\n            guess, _ = max(doc.cats.items(), key=lambda it: it[1])\n            truth, _ = max(gold.cats.items(), key=lambda it: it[1])\n            if guess not in labels:\n                msg = (\n                    f""Unexpected label {guess} predicted. ""\n                    f""Expectded labels: {\', \'.join(labels)}""\n                )\n                raise ValueError(msg)\n            if truth not in labels:\n                msg = (\n                    f""Unexpected label {truth} predicted. ""\n                    f""Expectded labels: {\', \'.join(labels)}""\n                )\n                raise ValueError(msg)\n            guesses.append(labels.index(guess))\n            truths.append(labels.index(truth))\n            right += guess == truth\n            total += 1\n            free_tensors(doc)\n    main_name, metrics = compute_metrics(\n        task, numpy.array(guesses), numpy.array(truths)\n    )\n    metrics[""_accuracy""] = right / total\n    metrics[""_right""] = right\n    metrics[""_total""] = total\n    metrics[""_main""] = metrics[main_name]\n    return metrics[main_name], metrics\n\n\ndef process_data(nlp, task, examples):\n    """"""Set-up Doc and GoldParse objects from the examples. This makes it easier\n    to set up text-pair tasks, and also easy to handle datasets with non-real\n    tokenization.""""""\n    wordpiecer = nlp.get_pipe(PIPES.wordpiecer)\n    textcat = nlp.get_pipe(PIPES.textcat)\n    docs = []\n    golds = []\n    for eg in examples:\n        if eg.text_b:\n            assert ""\\n"" not in eg.text_a\n            assert ""\\n"" not in eg.text_b\n            doc = nlp.make_doc(eg.text_a + ""\\n"" + eg.text_b)\n            doc._.set(ATTRS.separator, ""\\n"")\n        else:\n            doc = nlp.make_doc(eg.text_a)\n        doc = wordpiecer(doc)\n        cats = {label: 0.0 for label in textcat.labels}\n        cats[eg.label] = 1.0\n        gold = GoldParse(doc, cats=cats)\n        docs.append(doc)\n        golds.append(gold)\n    return list(zip(docs, golds))\n\n\ndef free_tensors(doc):\n    doc.tensor = None\n    doc._.set(ATTRS.last_hidden_state, None)\n    doc._.set(ATTRS.pooler_output, None)\n    doc._.set(ATTRS.all_hidden_states, [])\n    doc._.set(ATTRS.all_attentions, [])\n    doc._.set(ATTRS.d_last_hidden_state, None)\n    doc._.set(ATTRS.d_pooler_output, None)\n    doc._.set(ATTRS.d_all_hidden_states, [])\n    doc._.set(ATTRS.d_all_attentions, [])\n\n\ndef main(\n    model_name: (""Model package"", ""positional"", None, str),\n    task: (""Task name"", ""positional"", None, str),\n    data_dir: (""Path to data directory"", ""positional"", None, Path),\n    output_dir: (""Path to output directory"", ""positional"", None, Path),\n    hyper_params: (""Path to hyper params file"", ""positional"", None, Path) = None,\n    dont_gpu: (""Dont use GPU, even if available"", ""flag"", ""G"") = False,\n):\n    global HP\n    HP = get_hyper_params(hyper_params)\n    msg = wasabi.Printer()\n\n    spacy.util.fix_random_seed(HP.seed)\n    torch.manual_seed(HP.seed)\n    if not dont_gpu:\n        is_using_gpu = spacy.prefer_gpu()\n        msg.info(f""Use gpu? {is_using_gpu}"")\n        if is_using_gpu:\n            torch.set_default_tensor_type(""torch.cuda.FloatTensor"")\n\n    with msg.loading(""Reading corpus""):\n        raw_train_data = read_train_data(data_dir, task)\n        raw_dev_data = read_dev_data(data_dir, task)\n    with msg.loading(""Loading model""):\n        nlp, optimizer = create_model(model_name, **describe_task(task))\n    with msg.loading(""Tokenizing corpus""):\n        train_data = process_data(nlp, task, raw_train_data)\n        dev_data = process_data(nlp, task, raw_dev_data)\n\n    nr_batch = len(train_data) // HP.batch_size\n    # Set up printing\n    table_widths = [2, 4, 4]\n    msg.info(f""Training. Initial learn rate: {optimizer.alpha}"")\n    msg.row([""#"", ""Loss"", ""Score""], widths=table_widths)\n    msg.row([""-"" * width for width in table_widths])\n    # Set up learning rate schedule\n    learn_rates = cyclic_triangular_rate(\n        HP.learning_rate / HP.lr_range,\n        HP.learning_rate * HP.lr_range,\n        nr_batch * HP.lr_period,\n    )\n    optimizer.trf_lr = next(learn_rates)\n    optimizer.trf_weight_decay = HP.weight_decay\n    optimizer.trf_use_swa = HP.use_swa\n    # This sets the learning rate for the Thinc layers, i.e. just the final\n    # softmax. By keeping this LR high, we avoid a problem where the model\n    # spends too long flat, which harms the transfer learning.\n    optimizer.alpha = 0.001\n    step = 0\n    if HP.eval_every < 1:\n        HP.eval_every = nr_batch\n    pbar = tqdm.tqdm(total=HP.eval_every, leave=False)\n    results = []\n    epoch = 0\n    while True:\n        # Train and evaluate\n        losses = Counter()\n        for batch, loss in train_epoch(nlp, optimizer, train_data):\n            pbar.update(1)\n            losses.update(loss)\n\n            optimizer.trf_lr = next(learn_rates)\n            if step and (step % HP.eval_every) == 0:\n                with nlp.use_params(optimizer.averages):\n                    main_score, accuracies = evaluate(nlp, task, dev_data)\n                results.append((main_score, step, epoch))\n                msg.row(\n                    [str(step), ""%.2f"" % losses[PIPES.textcat], main_score],\n                    widths=table_widths,\n                )\n                pbar.close()\n                pbar = tqdm.tqdm(total=HP.eval_every, leave=False)\n            step += 1\n        epoch += 1\n        # Stop if no improvement in HP.patience checkpoints\n        if results:\n            best_score, best_step, best_epoch = max(results)\n            if ((step - best_step) // HP.eval_every) >= HP.patience:\n                break\n\n    table_widths = [2, 4, 6]\n    msg.info(f""Best scoring checkpoints"")\n    msg.row([""Epoch"", ""Step"", ""Score""], widths=table_widths)\n    msg.row([""-"" * width for width in table_widths])\n    for score, step, epoch in sorted(results, reverse=True)[:10]:\n        msg.row([epoch, step, ""%.2f"" % (score * 100)], widths=table_widths)\n\n\nif __name__ == ""__main__"":\n    plac.call(main)\n'"
spacy_transformers/pipeline/__init__.py,0,b'from .textcat import TransformersTextCategorizer  # noqa\nfrom .tok2vec import TransformersTok2Vec  # noqa\nfrom .wordpiecer import TransformersWordPiecer  # noqa\n'
spacy_transformers/pipeline/ner.py,0,"b'import spacy.pipeline\nfrom ..model_registry import get_model_function\nfrom ..util import PIPES\n\nDEBUG_LOSS = False\n\n\nclass TransformersEntityRecognizer(spacy.pipeline.EntityRecognizer):\n    """"""Subclass of spaCy\'s built-in EntityRecognizer component that supports\n    using the features assigned by the PyTorch-Transformers models via the token\n    vector encoder. It requires the TransformerTokenVectorEncoder to run before\n    it in the pipeline.\n    """"""\n\n    name = PIPES.ner\n    factory = PIPES.ner\n\n    @classmethod\n    def from_nlp(cls, nlp, **cfg):\n        """"""Factory to add to Language.factories via entry point.""""""\n        return cls(nlp.vocab, **cfg)\n\n    @classmethod\n    def Model(cls, nr_class, **cfg):\n        hidden_width = cfg.get(""hidden_width"", 64)\n        token_vector_width = cfg.get(""token_vector_width"", 96)\n        nr_feature = cls.nr_feature\n        maxout_pieces = cfg.get(""maxout_pieces"", 3)\n        tensor_size = cfg.get(""tensor_size"", 768)\n\n        configs = {\n            ""tok2vec"": {\n                ""arch"": ""tensor_affine_tok2vec"",\n                ""config"": {\n                    ""output_size"": token_vector_width,\n                    ""tensor_size"": tensor_size\n                }\n            },\n            ""lower"": {\n                ""arch"": ""precomputable_maxout"",\n                ""config"": {\n                    ""output_size"": hidden_width,\n                    ""input_size"": token_vector_width,\n                    ""number_features"": nr_feature,\n                    ""maxout_pieces"": maxout_pieces\n                }\n            },\n            ""upper"": {\n                ""arch"": ""affine_output"",\n                ""config"": {\n                    ""output_size"": nr_class,\n                    ""input_size"": hidden_width,\n                    ""drop_factor"": 0.0\n                }\n            }`\n        }\n        tok2vec_arch = get_model_function(configs[""tok2vec""][""arch""])\n        lower_arch = get_model_function(configs[""lower""][""arch""])\n        upper_arch = get_model_function(configs[""upper""][""arch""])\n        tok2vec = tok2vec_arch(**configs[""tok2vec""][""config""])\n        lower = lower_arch(**configs[""lower""][""config""])\n\n        with Model.use_device(\'cpu\'):\n            upper = upper_arch(**configs[""upper""][""config""])\n        upper.W *= 0\n        return ParserModel(tok2vec, lower, upper), configs\n'"
spacy_transformers/pipeline/textcat.py,0,"b'import spacy.pipeline\nfrom ..model_registry import get_model_function\nfrom ..util import PIPES\n\nDEBUG_LOSS = False\n\n\nclass TransformersTextCategorizer(spacy.pipeline.TextCategorizer):\n    """"""Subclass of spaCy\'s built-in TextCategorizer component that supports\n    using the features assigned by the transformer models via the token\n    vector encoder. It requires the TransformerTokenVectorEncoder to run before\n    it in the pipeline.\n    """"""\n\n    name = PIPES.textcat\n    factory = PIPES.textcat\n\n    @classmethod\n    def from_nlp(cls, nlp, **cfg):\n        """"""Factory to add to Language.factories via entry point.""""""\n        return cls(nlp.vocab, **cfg)\n\n    @classmethod\n    def Model(cls, nr_class=1, exclusive_classes=False, **cfg):\n        """"""Create a text classification model using a transformer model\n        for token vector encoding.\n\n        nr_class (int): Number of classes.\n        width (int): The width of the tensors being assigned.\n        exclusive_classes (bool): Make categories mutually exclusive.\n        **cfg: Optional config parameters.\n        RETURNS (thinc.neural.Model): The model.\n        """"""\n        arch = cfg.get(""architecture"", ""softmax_class_vector"")\n        # This is optional -- but if it\'s set, we can debug config errors.\n        transformers_name = cfg.get(""trf_name"", """")\n        is_gpt2 = ""gpt2"" in transformers_name\n        is_xlnet = ""xlnet"" in transformers_name\n        msg = (\n            f""TransformerTextCategorizer model architecture set to \'{arch}\' ""\n            f""with {transformers_name} transformer. This ""\n            f""combination is incompatible, as the transformer does not ""\n            f""provide that output feature.""\n        )\n        if is_gpt2 and arch in (""softmax_class_vector"", ""softmax_pooler_output""):\n            raise ValueError(msg)\n        elif is_xlnet and arch == ""softmax_pooler_output"":\n            raise ValueError(msg)\n        make_model = get_model_function(arch)\n        return make_model(nr_class, exclusive_classes=exclusive_classes, **cfg)\n\n    def get_loss(self, docs, golds, scores):\n        # This is a useful diagnostic while figuring out whether your model is\n        # learning anything. We print the loss each batch, and also the\n        # mean and variance for the score of the first class.\n        # If the model is learning things, we want to see the mean score stay\n        # close the the class distribution (e.g. 0.5 for balanced classes),\n        # while the variance in scores should increase (i.e. different examples\n        # should get different scores).\n        loss, d_scores = super().get_loss(docs, golds, scores)\n        mean_score = scores.mean(axis=0)[0]\n        var_score = scores.var(axis=0)[0]\n        if DEBUG_LOSS:\n            print(""L"", ""%.4f"" % loss, ""m"", ""%.3f"" % mean_score, ""v"", ""%.6f"" % var_score)\n        return loss, d_scores\n\n    def begin_training(\n        self, get_gold_tuples=lambda: [], pipeline=None, sgd=None, **kwargs\n    ):\n        if self.model is True:\n            self.cfg.update(kwargs)\n            self.require_labels()\n            self.model = self.Model(len(self.labels), **self.cfg)\n        if sgd is None:\n            sgd = self.create_optimizer()\n        return sgd\n'"
spacy_transformers/pipeline/tok2vec.py,0,"b'from typing import Any, List\nfrom thinc.neural.ops import get_array_module\nfrom spacy.pipeline import Pipe\nfrom spacy.tokens import Doc\nfrom spacy.vocab import Vocab\nfrom spacy.util import minibatch\n\nfrom ..wrapper import TransformersWrapper\nfrom ..model_registry import get_model_function\nfrom ..activations import Activations, RaggedArray\nfrom ..util import get_config, get_model, get_sents, PIPES, ATTRS\n\n\nclass TransformersTok2Vec(Pipe):\n    """"""spaCy pipeline component to use transformer models.\n\n    The component assigns the output of the transformer to the Doc\'s\n    extension attributes. We also calculate an alignment between the word-piece\n    tokens and the spaCy tokenization, so that we can use the last hidden states\n    to set the doc.tensor attribute. When multiple word-piece tokens align to\n    the same spaCy token, the spaCy token receives the sum of their values.\n    """"""\n\n    name = PIPES.tok2vec\n    factory = PIPES.tok2vec\n\n    @classmethod\n    def from_nlp(cls, nlp, **cfg):\n        """"""Factory to add to Language.factories via entry point.""""""\n        return cls(nlp.vocab, **cfg)\n\n    @classmethod\n    def from_pretrained(cls, vocab: Vocab, name: str, **cfg):\n        """"""Create a TransformersTok2Vec instance using pre-trained weights\n        from a transformer model, even if it\'s not installed as a\n        spaCy package.\n\n        vocab (spacy.vocab.Vocab): The spaCy vocab to use.\n        name (unicode): Name of pre-trained model, e.g. \'bert-base-uncased\'.\n        RETURNS (TransformersTok2Vec): The token vector encoder.\n        """"""\n        cfg[""trf_name""] = name\n        model = cls.Model(from_pretrained=True, **cfg)\n        cfg[""trf_config""] = dict(model._model.transformers_model.config.to_dict())\n        self = cls(vocab, model=model, **cfg)\n        return self\n\n    @classmethod\n    def Model(cls, **cfg) -> Any:\n        """"""Create an instance of `TransformersWrapper`, which holds the\n        Transformers model.\n\n        **cfg: Optional config parameters.\n        RETURNS (thinc.neural.Model): The wrapped model.\n        """"""\n        name = cfg.get(""trf_name"")\n        if not name:\n            raise ValueError(f""Need trf_name argument, e.g. \'bert-base-uncased\'"")\n        if cfg.get(""from_pretrained""):\n            wrap = TransformersWrapper.from_pretrained(name)\n        else:\n            config = cfg[""trf_config""]\n            # Work around floating point limitation in ujson:\n            # If we have the setting ""layer_norm_eps"" as 0,\n            # that\'s because of misprecision in serializing. Fix that.\n            config[""layer_norm_eps""] = 1e-12\n            config_cls = get_config(name)\n            model_cls = get_model(name)\n            # Need to match the name their constructor expects.\n            if ""vocab_size"" in cfg[""trf_config""]:\n                vocab_size = cfg[""trf_config""][""vocab_size""]\n                cfg[""trf_config""][""vocab_size_or_config_json_file""] = vocab_size\n            wrap = TransformersWrapper(name, config, model_cls(config_cls(**config)))\n        make_model = get_model_function(cfg.get(""architecture"", ""tok2vec_per_sentence""))\n        model = make_model(wrap, cfg)\n        setattr(model, ""nO"", wrap.nO)\n        setattr(model, ""_model"", wrap)\n        return model\n\n    def __init__(self, vocab, model=True, **cfg):\n        """"""Initialize the component.\n\n        model (thinc.neural.Model / True): The component\'s model or `True` if\n            not initialized yet.\n        **cfg: Optional config parameters.\n        """"""\n        self.vocab = vocab\n        self.model = model\n        self.cfg = cfg\n\n    @property\n    def token_vector_width(self):\n        return self.model._model.nO\n\n    @property\n    def transformers_model(self):\n        return self.model._model.transformers_model\n\n    def __call__(self, doc):\n        """"""Process a Doc and assign the extracted features.\n\n        doc (spacy.tokens.Doc): The Doc to process.\n        RETURNS (spacy.tokens.Doc): The processed Doc.\n        """"""\n        self.require_model()\n        outputs = self.predict([doc])\n        self.set_annotations([doc], outputs)\n        return doc\n\n    def pipe(self, stream, batch_size=128):\n        """"""Process Doc objects as a stream and assign the extracted features.\n\n        stream (iterable): A stream of Doc objects.\n        batch_size (int): The number of texts to buffer.\n        YIELDS (spacy.tokens.Doc): Processed Docs in order.\n        """"""\n        for docs in minibatch(stream, size=batch_size):\n            docs = list(docs)\n            outputs = self.predict(docs)\n            self.set_annotations(docs, outputs)\n            for doc in docs:\n                yield doc\n\n    def begin_update(self, docs, drop=None, **cfg):\n        """"""Get the predictions and a callback to complete the gradient update.\n        This is only used internally within TransformersLanguage.update.\n        """"""\n        outputs, backprop = self.model.begin_update(docs, drop=drop)\n\n        def finish_update(docs, sgd=None):\n            assert len(docs)\n            d_lh = []\n            d_po = []\n            lh_lengths = []\n            po_lengths = []\n            for doc in docs:\n                d_lh.append(doc._.get(ATTRS.d_last_hidden_state))\n                d_po.append(doc._.get(ATTRS.d_pooler_output))\n                lh_lengths.append(doc._.get(ATTRS.d_last_hidden_state).shape[0])\n                po_lengths.append(doc._.get(ATTRS.d_pooler_output).shape[0])\n            xp = self.model.ops.xp\n            gradients = Activations(\n                RaggedArray(xp.vstack(d_lh), lh_lengths),\n                RaggedArray(xp.vstack(d_po), po_lengths),\n            )\n            backprop(gradients, sgd=sgd)\n            for doc in docs:\n                doc._.get(ATTRS.d_last_hidden_state).fill(0)\n                doc._.get(ATTRS.d_last_hidden_state).fill(0)\n            return None\n\n        return outputs, finish_update\n\n    def predict(self, docs):\n        """"""Run the transformer model on a batch of docs and return the\n        extracted features.\n\n        docs (iterable): A batch of Docs to process.\n        RETURNS (list): A list of Activations objects, one per doc.\n        """"""\n        return self.model.predict(docs)\n\n    def set_annotations(self, docs: List[Doc], activations: Activations):\n        """"""Assign the extracted features to the Doc objects and overwrite the\n        vector and similarity hooks.\n\n        docs (iterable): A batch of `Doc` objects.\n        activations (iterable): A batch of activations.\n        """"""\n        xp = activations.xp\n        for i, doc in enumerate(docs):\n            # Make it 2d -- acts are always 3d, to represent batch size.\n            wp_tensor = activations.lh.get(i)\n            doc.tensor = self.model.ops.allocate((len(doc), self.model.nO))\n            doc._.set(ATTRS.last_hidden_state, wp_tensor)\n            if activations.has_po:\n                pooler_output = activations.po.get(i)\n                doc._.set(ATTRS.pooler_output, pooler_output)\n            doc._.set(\n                ATTRS.d_last_hidden_state, xp.zeros((0, 0), dtype=wp_tensor.dtype)\n            )\n            doc._.set(ATTRS.d_pooler_output, xp.zeros((0, 0), dtype=wp_tensor.dtype))\n            doc._.set(ATTRS.d_all_hidden_states, [])\n            doc._.set(ATTRS.d_all_attentions, [])\n            if wp_tensor.shape != (len(doc._.get(ATTRS.word_pieces)), self.model.nO):\n                raise ValueError(\n                    ""Mismatch between tensor shape and word pieces. This usually ""\n                    ""means we did something wrong in the sentence reshaping, ""\n                    ""or possibly finding the separator tokens.""\n                )\n            # Count how often each word-piece token is represented. This allows\n            # a weighted sum, so that we can make sure doc.tensor.sum()\n            # equals wp_tensor.sum(). Do this with sensitivity to boundary tokens\n            wp_rows, align_sizes = _get_boundary_sensitive_alignment(doc)\n            wp_weighted = wp_tensor / xp.array(align_sizes, dtype=""f"").reshape((-1, 1))\n            # TODO: Obviously incrementing the rows individually is bad. How\n            # to do in one shot without blowing up the memory?\n            for i, word_piece_slice in enumerate(wp_rows):\n                doc.tensor[i] = wp_weighted[word_piece_slice,].sum(0)\n            doc.user_hooks[""vector""] = get_doc_vector_via_tensor\n            doc.user_span_hooks[""vector""] = get_span_vector_via_tensor\n            doc.user_token_hooks[""vector""] = get_token_vector_via_tensor\n            doc.user_hooks[""similarity""] = get_similarity_via_tensor\n            doc.user_span_hooks[""similarity""] = get_similarity_via_tensor\n            doc.user_token_hooks[""similarity""] = get_similarity_via_tensor\n\n\ndef _get_boundary_sensitive_alignment(doc):\n    align_sizes = [0 for _ in range(len(doc._.get(ATTRS.word_pieces)))]\n    wp_rows = []\n    for word_piece_slice in doc._.get(ATTRS.alignment):\n        wp_rows.append(list(word_piece_slice))\n        for i in word_piece_slice:\n            align_sizes[i] += 1\n    # To make this weighting work, we ""align"" the boundary tokens against\n    # every token in their sentence. The boundary tokens are otherwise\n    # unaligned, which is how we identify them.\n    for sent in get_sents(doc):\n        offset = sent._.get(ATTRS.start)\n        for i in range(len(sent._.get(ATTRS.word_pieces))):\n            if align_sizes[offset + i] == 0:\n                align_sizes[offset + i] = len(sent)\n                for tok in sent:\n                    wp_rows[tok.i].append(offset + i)\n    return wp_rows, align_sizes\n\n\ndef get_doc_vector_via_tensor(doc):\n    return doc.tensor.sum(axis=0)\n\n\ndef get_span_vector_via_tensor(span):\n    return span.doc.tensor[span.start : span.end].sum(axis=0)\n\n\ndef get_token_vector_via_tensor(token):\n    return token.doc.tensor[token.i]\n\n\ndef get_similarity_via_tensor(doc1, doc2):\n    v1 = doc1.vector\n    v2 = doc2.vector\n    xp = get_array_module(v1)\n    return xp.dot(v1, v2) / (doc1.vector_norm * doc2.vector_norm)\n'"
spacy_transformers/pipeline/wordpiecer.py,0,"b'from spacy.pipeline import Pipe\nfrom spacy.util import minibatch\nfrom tokenizations import get_alignments\nimport re\nimport numpy\n\nfrom ..util import get_tokenizer, flatten_list, get_sents, PIPES, ATTRS\n\n\nclass TransformersWordPiecer(Pipe):\n    """"""spaCy pipeline component to assign transformer word-piece\n    tokenization to the Doc, which can then be used by the token vector\n    encoder. Note that this component doesn\'t modify spaCy\'s tokenization. It\n    only sets extension attributes and aligns the tokens.""""""\n\n    name = PIPES.wordpiecer\n    factory = PIPES.wordpiecer\n\n    @classmethod\n    def from_nlp(cls, nlp, **cfg):\n        """"""Factory to add to Language.factories via entry point.""""""\n        return cls(nlp.vocab, **cfg)\n\n    @classmethod\n    def from_pretrained(cls, vocab, trf_name, **cfg):\n        model = get_tokenizer(trf_name).from_pretrained(trf_name)\n        return cls(vocab, model=model, trf_name=trf_name, **cfg)\n\n    @classmethod\n    def Model(cls, trf_name, **kwargs):\n        return get_tokenizer(trf_name).blank()\n\n    def __init__(self, vocab, model=True, **cfg):\n        """"""Initialize the component.\n\n        vocab (spacy.vocab.Vocab): The spaCy vocab to use.\n        model: Not used here.\n        **cfg: Optional config parameters.\n        """"""\n        self.vocab = vocab\n        self.cfg = cfg\n        self.model = model\n        self._num_added_tokens = None\n\n    def __call__(self, doc):\n        """"""Apply the pipe to one document. The document is\n        modified in-place, and returned.\n\n        Both __call__ and pipe should delegate to the `predict()`\n        and `set_annotations()` methods.\n        """"""\n        self.require_model()\n        pieces = self.predict([doc])\n        self.set_annotations([doc], pieces)\n        return doc\n\n    @property\n    def alignment_strategy(self):\n        """"""Mostly used for debugging to make the WordPiecer raise error on\n        misaligned tokens and prevent forcing alignment.\n        """"""\n        retry_alignment = self.cfg.get(""retry_alignment"", False)\n        force_alignment = self.cfg.get(""force_alignment"", True)\n        return (retry_alignment, force_alignment)\n\n    def pipe(self, stream, batch_size=128):\n        """"""Process Doc objects as a stream and assign the extracted features.\n\n        stream (iterable): A stream of Doc objects.\n        batch_size (int): The number of texts to buffer.\n        YIELDS (spacy.tokens.Doc): Processed Docs in order.\n        """"""\n        for docs in minibatch(stream, size=batch_size):\n            docs = list(docs)\n            outputs = self.predict(docs)\n            self.set_annotations(docs, outputs)\n            for doc in docs:\n                yield doc\n\n    def predict(self, docs):\n        """"""Run the word-piece tokenizer on a batch of docs and return the\n        extracted strings.\n\n        docs (iterable): A batch of Docs to process.\n        RETURNS (tuple): A (strings, None) tuple.\n        """"""\n        output = []\n        max_seq_length = self.model.max_len - self.num_added_tokens()\n        for doc in docs:\n            doc_words = []\n            doc_align = []\n            offset = 0\n            for sent in get_sents(doc):\n                sent_words = []\n                sent_align = []\n                for segment in sent._.get(ATTRS.segments):\n                    seg_words = self.model.tokenize(segment.text)\n                    seg_words, seg_align = self._align(\n                        segment, seg_words, offset=offset\n                    )\n                    seg_words = seg_words[:max_seq_length]\n                    for i, align in enumerate(seg_align):\n                        if len(align) >= 1 and align[-1] < max_seq_length:\n                            continue\n                        seg_align[i] = [x for x in align if x < max_seq_length]\n                    assert len(segment) == len(seg_align)\n                    sent_words.append(seg_words)\n                    sent_align.append(seg_align)\n                sw_flat = self.model.add_special_tokens(sent_words)\n                sa_flat = self.model.fix_alignment(sent_align)\n                doc_words.extend(sw_flat)\n                doc_align.extend(sa_flat)\n                offset += len(sw_flat)\n            output.append((doc_words, doc_align))\n        return output\n\n    def _align(self, segment, wp_tokens, *, offset=0):\n        spacy_tokens = [w.text for w in segment]\n        a2b, b2a = get_alignments(spacy_tokens, wp_tokens)\n\n        # a2b must contain the boundary of `segment` (head and last token index)\n        # so insert them when they are missed.\n        if a2b and b2a:\n            if len(b2a[0]) == 0:\n                a2b[0].insert(0, 0)\n            if len(b2a[-1]) == 0:\n                a2b[-1].append(len(b2a) - 1)\n        a2b = [[i + offset for i in a] for a in a2b]\n        return wp_tokens, a2b\n\n    def num_added_tokens(self):\n        # GPT2 returns 0 for `tokenizer.num_added_tokens()` but\n        # `tokenizer.add_special_tokens()` adds two \'<|endoftext|>\' tokens,\n        # so determine this once initially in practice\n        if self._num_added_tokens is None:\n            words = [""a"", ""b"", ""c""]\n            words_with_added = self.model.add_special_tokens([words])\n            self._num_added_tokens = len(words_with_added) - len(words)\n        return self._num_added_tokens\n\n    def set_annotations(self, docs, outputs, tensors=None):\n        """"""Assign the extracted tokens and IDs to the Doc objects.\n\n        docs (iterable): A batch of `Doc` objects.\n        outputs (iterable): A batch of outputs.\n        """"""\n        # Set model.max_len to some high value, to avoid annoying prints.\n        max_len = self.model.max_len\n        self.model.max_len = 1e12\n        for doc, (wordpieces, alignment) in zip(docs, outputs):\n            doc._.set(ATTRS.word_pieces_, wordpieces)\n            doc._.set(ATTRS.word_pieces, self.model.convert_tokens_to_ids(wordpieces))\n            doc._.set(ATTRS.alignment, alignment)\n            nr_word = len(doc._.get(ATTRS.word_pieces))\n            words_per_sent = sum(\n                len(sent._.get(ATTRS.word_pieces)) for sent in get_sents(doc)\n            )\n            if nr_word != words_per_sent:\n                print([repr(w.text) for w in doc])\n                for sent in get_sents(doc):\n                    print(sent._.get(ATTRS.word_pieces_))\n                    for w in sent:\n                        print(w.text, w._.get(ATTRS.alignment))\n                print(doc._.get(ATTRS.word_pieces_))\n                self.model.max_len = max_len\n                raise ValueError(\n                    f""Error calculating word pieces for sentences. Total number ""\n                    f""of wordpieces in the doc was {nr_word}, but adding up the ""\n                    f""wordpieces for its sentences we get {words_per_sent}. This ""\n                    f""means there\'s a bug in the extension attributes or ""\n                    f""the tokenizer.add_special_tokens() logic, often when ""\n                    f""a spaCy sentence aligns against 0 wordpieces.""\n                )\n        self.model.max_len = max_len\n\n    def use_params(self, params):\n        yield\n\n\nalpha_re = re.compile(r""[^A-Za-z]+"")\n\n\ndef align_word_pieces(spacy_tokens, wp_tokens, retry=True):\n    """"""Align tokens against word-piece tokens. The alignment is returned as a\n    list of lists. If alignment[3] == [4, 5, 6], that means that spacy_tokens[3]\n    aligns against 3 tokens: wp_tokens[4], wp_tokens[5] and wp_tokens[6].\n    All spaCy tokens must align against at least one element of wp_tokens.\n    """"""\n    spacy_tokens = list(spacy_tokens)\n    wp_tokens = list(wp_tokens)\n    if not wp_tokens:\n        return [[] for _ in spacy_tokens]\n    elif not spacy_tokens:\n        return []\n    # Check alignment\n    spacy_string = """".join(spacy_tokens).lower()\n    wp_string = """".join(wp_tokens).lower()\n    if not spacy_string and not wp_string:\n        return None\n    if spacy_string != wp_string:\n        if retry:\n            # Flag to control whether to apply a fallback strategy when we\n            # don\'t align, of making more aggressive replacements. It\'s not\n            # clear whether this will lead to better or worse results than the\n            # ultimate fallback strategy, of calling the sub-tokenizer on the\n            # spaCy tokens. Probably trying harder to get alignment is good:\n            # the ultimate fallback actually *changes what wordpieces we\n            # return*, so we get (potentially) different results out of the\n            # transformer. The more aggressive alignment can only change how we\n            # map those transformer features to tokens.\n            spacy_tokens = [alpha_re.sub("""", t) for t in spacy_tokens]\n            wp_tokens = [alpha_re.sub("""", t) for t in wp_tokens]\n            spacy_string = """".join(spacy_tokens).lower()\n            wp_string = """".join(wp_tokens).lower()\n            if spacy_string == wp_string:\n                return _align(spacy_tokens, wp_tokens)\n        # If either we\'re not trying the fallback alignment, or the fallback\n        # fails, we return None. This tells the wordpiecer to align by\n        # calling the sub-tokenizer on the spaCy tokens.\n        return None\n    output = _align(spacy_tokens, wp_tokens)\n    if len(set(flatten_list(output))) != len(wp_tokens):\n        return None\n    return output\n\n\ndef _align(seq1, seq2):\n    # Map character positions to tokens\n    map1 = _get_char_map(seq1)\n    map2 = _get_char_map(seq2)\n    # For each token in seq1, get the set of tokens in seq2\n    # that share at least one character with that token.\n    alignment = [set() for _ in seq1]\n    unaligned = set(range(len(seq2)))\n    for char_position in range(map1.shape[0]):\n        i = map1[char_position]\n        j = map2[char_position]\n        alignment[i].add(j)\n        if j in unaligned:\n            unaligned.remove(j)\n    # Sort, make list\n    output = [sorted(list(s)) for s in alignment]\n    # Expand alignment to adjacent unaligned tokens of seq2\n    for indices in output:\n        if indices:\n            while indices[0] >= 1 and indices[0] - 1 in unaligned:\n                indices.insert(0, indices[0] - 1)\n            last = len(seq2) - 1\n            while indices[-1] < last and indices[-1] + 1 in unaligned:\n                indices.append(indices[-1] + 1)\n    return output\n\n\ndef _get_char_map(seq):\n    char_map = numpy.zeros((sum(len(token) for token in seq),), dtype=""i"")\n    offset = 0\n    for i, token in enumerate(seq):\n        for j in range(len(token)):\n            char_map[offset + j] = i\n        offset += len(token)\n    return char_map\n\n\ndef _tokenize_individual_tokens(model, sent):\n    # As a last-chance strategy, run the wordpiece tokenizer on the\n    # individual tokens, so that alignment is trivial.\n    wp_tokens = []\n    sent_align = []\n    offset = 0\n    for token in sent:\n        if token.text.strip():\n            subtokens = model.tokenize(token.text)\n            wp_tokens.extend(subtokens)\n            sent_align.append([i + offset for i in range(len(subtokens))])\n            offset += len(subtokens)\n        else:\n            sent_align.append([])\n    return wp_tokens, sent_align\n'"
