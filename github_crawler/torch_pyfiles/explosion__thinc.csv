file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nimport sys\nimport distutils.util\nfrom distutils.command.build_ext import build_ext\nfrom distutils.sysconfig import get_python_inc\nfrom setuptools import Extension, setup, find_packages\nfrom pathlib import Path\nimport numpy\nfrom Cython.Build import cythonize\nfrom Cython.Compiler import Options\n\n\n# Preserve `__doc__` on functions and classes\n# http://docs.cython.org/en/latest/src/userguide/source_files_and_compilation.html#compiler-options\nOptions.docstrings = True\n\n\nPACKAGES = find_packages()\nMOD_NAMES = [\n    ""thinc.backends.linalg"",\n    ""thinc.backends.numpy_ops"",\n    ""thinc.extra.search"",\n    ""thinc.layers.sparselinear"",\n]\nCOMPILE_OPTIONS = {\n    ""msvc"": [""/Ox"", ""/EHsc""],\n    ""other"": [""-O3"", ""-Wno-strict-prototypes"", ""-Wno-unused-function""],\n}\nCOMPILER_DIRECTIVES = {\n    ""language_level"": -3,\n    ""embedsignature"": True,\n    ""annotation_typing"": False,\n}\nLINK_OPTIONS = {""msvc"": [], ""other"": []}\n\n\ndef is_new_osx():\n    """"""Check whether we\'re on OSX >= 10.10""""""\n    name = distutils.util.get_platform()\n    if sys.platform != ""darwin"":\n        return False\n    elif name.startswith(""macosx-10""):\n        minor_version = int(name.split(""-"")[1].split(""."")[1])\n        if minor_version >= 7:\n            return True\n        else:\n            return False\n    else:\n        return False\n\n\nif is_new_osx():\n    # On Mac, use libc++ because Apple deprecated use of libstdc\n    COMPILE_OPTIONS[""other""].append(""-stdlib=libc++"")\n    LINK_OPTIONS[""other""].append(""-lc++"")\n    # g++ (used by unix compiler on mac) links to libstdc++ as a default lib.\n    # See: https://stackoverflow.com/questions/1653047/avoid-linking-to-libstdc\n    LINK_OPTIONS[""other""].append(""-nodefaultlibs"")\n\n\n# By subclassing build_extensions we have the actual compiler that will be used\n# which is really known only after finalize_options\n# http://stackoverflow.com/questions/724664/python-distutils-how-to-get-a-compiler-that-is-going-to-be-used\nclass build_ext_options:\n    def build_options(self):\n        if hasattr(self.compiler, ""initialize""):\n            self.compiler.initialize()\n        self.compiler.platform = sys.platform[:6]\n        for e in self.extensions:\n            e.extra_compile_args = COMPILE_OPTIONS.get(\n                self.compiler.compiler_type, COMPILE_OPTIONS[""other""]\n            )\n            e.extra_link_args = LINK_OPTIONS.get(\n                self.compiler.compiler_type, LINK_OPTIONS[""other""]\n            )\n\n\nclass build_ext_subclass(build_ext, build_ext_options):\n    def build_extensions(self):\n        build_ext_options.build_options(self)\n        build_ext.build_extensions(self)\n\n\ndef clean(path):\n    for path in path.glob(""**/*""):\n        if path.is_file() and path.suffix in ("".so"", "".cpp""):\n            print(f""Deleting {path.name}"")\n            path.unlink()\n\n\ndef setup_package():\n    root = Path(__file__).parent\n\n    if len(sys.argv) > 1 and sys.argv[1] == ""clean"":\n        return clean(root / ""thinc"")\n\n    with (root / ""thinc"" / ""about.py"").open(""r"") as f:\n        about = {}\n        exec(f.read(), about)\n\n    include_dirs = [get_python_inc(plat_specific=True), numpy.get_include()]\n    ext_modules = []\n    for name in MOD_NAMES:\n        mod_path = name.replace(""."", ""/"") + "".pyx""\n        ext = Extension(name, [mod_path], language=""c++"", include_dirs=include_dirs)\n        ext_modules.append(ext)\n    print(""Cythonizing sources"")\n    ext_modules = cythonize(ext_modules, compiler_directives=COMPILER_DIRECTIVES)\n\n    setup(\n        name=""thinc"",\n        packages=PACKAGES,\n        version=about[""__version__""],\n        ext_modules=ext_modules,\n        cmdclass={""build_ext"": build_ext_subclass},\n        package_data={"""": [""*.pyx"", ""*.pxd"", ""*.pxi"", ""*.cpp"", ""*.cu""]},\n    )\n\n\nif __name__ == ""__main__"":\n    setup_package()\n'"
examples/mnist.py,0,"b'""""""\nPyTorch version: https://github.com/pytorch/examples/blob/master/mnist/main.py\nTensorFlow version: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py\n""""""\n# pip install thinc ml_datasets typer\nfrom thinc.api import Model, chain, Relu, Softmax, Adam\nimport ml_datasets\nfrom wasabi import msg\nfrom tqdm import tqdm\nimport typer\n\n\ndef main(\n    n_hidden: int = 256, dropout: float = 0.2, n_iter: int = 10, batch_size: int = 128\n):\n    # Define the model\n    model: Model = chain(\n        Relu(nO=n_hidden, dropout=dropout),\n        Relu(nO=n_hidden, dropout=dropout),\n        Softmax(),\n    )\n    # Load the data\n    (train_X, train_Y), (dev_X, dev_Y) = ml_datasets.mnist()\n    # Set any missing shapes for the model.\n    model.initialize(X=train_X[:5], Y=train_Y[:5])\n    train_data = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n    dev_data = model.ops.multibatch(batch_size, dev_X, dev_Y)\n    # Create the optimizer.\n    optimizer = Adam(0.001)\n    for i in range(n_iter):\n        for X, Y in tqdm(train_data, leave=False):\n            Yh, backprop = model.begin_update(X)\n            backprop(Yh - Y)\n            model.finish_update(optimizer)\n        # Evaluate and print progress\n        correct = 0\n        total = 0\n        for X, Y in dev_data:\n            Yh = model.predict(X)\n            correct += (Yh.argmax(axis=1) == Y.argmax(axis=1)).sum()\n            total += Yh.shape[0]\n        score = correct / total\n        msg.row((i, f""{score:.3f}""), widths=(3, 5))\n\n\nif __name__ == ""__main__"":\n    typer.run(main)\n'"
examples/transformers_tagger.py,7,"b'""""""Train a transformer tagging model, using Huggingface\'s Transformers.""""""\n# pip install thinc ml_datasets typer tqdm transformers torch\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Callable\nimport torch\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModel\nimport thinc\nfrom thinc.api import PyTorchWrapper, Softmax, chain, with_array, Model, Config\nfrom thinc.api import torch2xp, xp2torch, SequenceCategoricalCrossentropy\nfrom thinc.api import prefer_gpu, use_pytorch_for_gpu_memory\nfrom thinc.types import Floats2d, ArgsKwargs\nimport ml_datasets\nimport tqdm\nimport typer\n\n\nCONFIG = """"""\n[model]\n@layers = ""TransformersTagger.v1""\nstarter = ""bert-base-multilingual-cased""\n\n[optimizer]\n@optimizers = ""RAdam.v1""\nweight_decay = 1e-8\n\n[optimizer.learn_rate]\n@schedules = ""warmup_linear.v1""\ninitial_rate = 0.01\nwarmup_steps = 3000\ntotal_steps = 6000\n\n[training]\nbatch_size = 128\nwords_per_subbatch = 2000\nn_epoch = 10\n""""""\n\n\ndef main(path: Optional[Path] = None, out_dir: Optional[Path] = None):\n    if prefer_gpu():\n        print(""Using gpu!"")\n        use_pytorch_for_gpu_memory()\n    # You can edit the CONFIG string within the file, or copy it out to\n    # a separate file and pass in the path.\n    if path is None:\n        config = Config().from_str(CONFIG)\n    else:\n        config = Config().from_disk(path)\n    # make_from_config constructs objects whenever you have blocks with an @ key.\n    # In the optimizer block we write @optimizers = ""Adam.v1"". This tells Thinc\n    # to use registry.optimizers to fetch the ""Adam.v1"" function. You can\n    # register your own functions as well and build up trees of objects.\n    C = thinc.registry.make_from_config(config)\n\n    words_per_subbatch = C[""training""][""words_per_subbatch""]\n    n_epoch = C[""training""][""n_epoch""]\n    batch_size = C[""training""][""batch_size""]\n    model = C[""model""]\n    optimizer = C[""optimizer""]\n    calculate_loss = SequenceCategoricalCrossentropy()\n\n    (train_X, train_Y), (dev_X, dev_Y) = ml_datasets.ud_ancora_pos_tags()\n    # Convert the outputs to cupy (if we\'re using that)\n    train_Y = list(map(model.ops.asarray, train_Y))\n    dev_Y = list(map(model.ops.asarray, dev_Y))\n    # Pass in a small batch of data, to fill in missing shapes\n    model.initialize(X=train_X[:5], Y=train_Y[:5])\n    for epoch in range(n_epoch):\n        # Transformers often learn best with large batch sizes -- larger than\n        # fits in GPU memory. But you don\'t have to backprop the whole batch\n        # at once. Here we consider the ""logical"" batch size (number of examples\n        # per update) separately from the physical batch size.\n        batches = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n        for outer_batch in tqdm.tqdm(batches, leave=False):\n            # For the physical batch size, what we care about is the number\n            # of words (considering padding too). We also want to sort by\n            # length, for efficiency.\n            for batch in minibatch_by_words(outer_batch, words_per_subbatch):\n                inputs, truths = zip(*batch)\n                guesses, backprop = model(inputs, is_train=True)\n                backprop(calculate_loss.get_grad(guesses, truths))\n            # At the end of the batch, we call the optimizer with the accumulated\n            # gradients, and advance the learning rate schedules.\n            model.finish_update(optimizer)\n            optimizer.step_schedules()\n        # You might want to evaluate more often than once per epoch; that\'s up\n        # to you.\n        score = evaluate_sequences(model, dev_X, dev_Y, 128)\n        print(epoch, f""{score:.3f}"")\n        if out_dir:\n            model.to_disk(out_dir / f""{epoch}.bin"")\n\n\n@dataclass\nclass TokensPlus:\n    """"""Dataclass to hold the output of the Huggingface \'batch_encode_plus\' method.""""""\n\n    input_ids: torch.Tensor\n    token_type_ids: torch.Tensor\n    attention_mask: torch.Tensor\n    input_len: List[int]\n    overflowing_tokens: Optional[torch.Tensor] = None\n    num_truncated_tokens: Optional[torch.Tensor] = None\n    special_tokens_mask: Optional[torch.Tensor] = None\n\n\n@thinc.registry.layers(""TransformersTagger.v1"")\ndef TransformersTagger(\n    starter: str, n_tags: int = 17\n) -> Model[List[List[str]], List[Floats2d]]:\n    return chain(\n        TransformersTokenizer(starter),\n        Transformer(starter),\n        with_array(Softmax(nO=n_tags)),\n    )\n\n\n@thinc.registry.layers(""transformers_tokenizer.v1"")\ndef TransformersTokenizer(name: str) -> Model[List[List[str]], TokensPlus]:\n    def forward(\n        model, texts: List[List[str]], is_train: bool\n    ) -> Tuple[TokensPlus, Callable]:\n        tokenizer = model.attrs[""tokenizer""]\n        token_data = tokenizer.batch_encode_plus(\n            [(text, None) for text in texts],\n            add_special_tokens=True,\n            return_token_type_ids=True,\n            return_attention_masks=True,\n            return_input_lengths=True,\n            return_tensors=""pt"",\n        )\n        return TokensPlus(**token_data), lambda d_tokens: []\n\n    return Model(\n        ""tokenizer"", forward, attrs={""tokenizer"": AutoTokenizer.from_pretrained(name)},\n    )\n\n\n@thinc.registry.layers(""transformers_model.v1"")\ndef Transformer(name: str) -> Model[TokensPlus, List[Floats2d]]:\n    return PyTorchWrapper(\n        AutoModel.from_pretrained(name),\n        convert_inputs=convert_transformer_inputs,\n        convert_outputs=convert_transformer_outputs,\n    )\n\n\ndef convert_transformer_inputs(model, tokens: TokensPlus, is_train):\n    kwargs = {\n        ""input_ids"": tokens.input_ids,\n        ""attention_mask"": tokens.attention_mask,\n        ""token_type_ids"": tokens.token_type_ids,\n    }\n    return ArgsKwargs(args=(), kwargs=kwargs), lambda dX: []\n\n\ndef convert_transformer_outputs(model, inputs_outputs, is_train):\n    layer_inputs, torch_outputs = inputs_outputs\n    torch_tokvecs: torch.Tensor = torch_outputs[0]\n    # Free the memory as soon as we can\n    torch_outputs = None\n    lengths = list(layer_inputs.input_len)\n    tokvecs: List[Floats2d] = model.ops.unpad(torch2xp(torch_tokvecs), lengths)\n    # Remove the BOS and EOS markers.\n    tokvecs = [arr[1:-1] for arr in tokvecs]\n\n    def backprop(d_tokvecs: List[Floats2d]) -> ArgsKwargs:\n        # Restore entries for bos and eos markers.\n        row = model.ops.alloc2f(1, d_tokvecs[0].shape[1])\n        d_tokvecs = [model.ops.xp.vstack((row, arr, row)) for arr in d_tokvecs]\n        return ArgsKwargs(\n            args=(torch_tokvecs,),\n            kwargs={""grad_tensors"": xp2torch(model.ops.pad(d_tokvecs))},\n        )\n\n    return tokvecs, backprop\n\n\ndef evaluate_sequences(\n    model, Xs: List[Floats2d], Ys: List[Floats2d], batch_size: int\n) -> float:\n    correct = 0.0\n    total = 0.0\n    for X, Y in model.ops.multibatch(batch_size, Xs, Ys):\n        Yh = model.predict(X)\n        for yh, y in zip(Yh, Y):\n            correct += (y.argmax(axis=1) == yh.argmax(axis=1)).sum()\n            total += y.shape[0]\n    return float(correct / total)\n\n\ndef minibatch_by_words(pairs, max_words):\n    """"""Group pairs of sequences into minibatches under max_words in size,\n    considering padding. The size of a padded batch is the length of its\n    longest sequence multiplied by the number of elements in the batch.\n    """"""\n    pairs = list(zip(*pairs))\n    pairs.sort(key=lambda xy: len(xy[0]), reverse=True)\n    batch = []\n    for X, Y in pairs:\n        batch.append((X, Y))\n        n_words = max(len(xy[0]) for xy in batch) * len(batch)\n        if n_words >= max_words:\n            # We went *over* the cap, so don\'t emit the batch with this\n            # example -- move that example into the next one.\n            yield batch[:-1]\n            batch = [(X, Y)]\n    if batch:\n        yield batch\n\n\nif __name__ == ""__main__"":\n    typer.run(main)\n'"
examples/type_checking.py,0,"b'from typing import List\n\nfrom thinc.layers import Relu, Softmax, chain, reduce_max, concatenate\nfrom thinc.model import Model\n\n# Define Custom X/Y types\nMyModelX = List[List[float]]\nMyModelY = List[List[float]]\nmodel: Model[MyModelX, MyModelY] = chain(\n    Relu(12), Relu(12, dropout=0.2), Softmax(),\n)\n# ERROR: incompatible type ""bool"", expected ""List[List[float]]""\nmodel(False)\n# ERROR: List item 0 has incompatible type ""str""; expected ""float""\nmodel.begin_update([[""0""]])\n# ERROR: incompatible type ""bool"", expected ""List[List[float]]""\nmodel.predict(True)\n\n\n# This example should be run with mypy. This is an example of type-level checking\n# for network validity.\n#\n# We first define an invalid network.\n# It\'s invalid because reduce_max expects Array3d as input, while Relu produces\n# Array2d as output. chain has type-logic to verify input and output types\n# line up.\n#\n# You should see the error an error,\n# examples/howto/type_chain.py:10: error: Cannot infer type argument 2 of ""chain""\nbad_model = chain(Relu(10), reduce_max(), Softmax())\n\nconcate_model = concatenate(Relu(10), reduce_max(), Relu(10), Relu(10)), reduce_max()\n\nconcate_chain_model = chain(\n    concatenate(Relu(10), reduce_max(), Relu(10), Relu(10)), reduce_max()\n)\n\n# Now let\'s try it with a network that does work, just to be sure.\ngood_model = chain(Relu(10), Relu(10), Softmax())\n\n# Finally we can reveal_type on the good model, to see what it thinks.\nreveal_type(good_model)\n'"
thinc/__init__.py,0,b'# Necessary for some side-effects in Cython. Not sure I understand.\nimport numpy\n\nfrom .about import __version__\nfrom .config import registry\n'
thinc/about.py,0,"b'__version__ = ""8.0.0a10""\n__release__ = True\n'"
thinc/api.py,0,"b'from .config import Config, registry, ConfigValidationError\nfrom .initializers import normal_init, uniform_init, glorot_uniform_init, zero_init\nfrom .initializers import configure_normal_init\nfrom .loss import CategoricalCrossentropy, L2Distance, CosineDistance\nfrom .loss import SequenceCategoricalCrossentropy\nfrom .model import Model, serialize_attr, deserialize_attr\nfrom .model import set_dropout_rate, change_attr_values\nfrom .shims import Shim, PyTorchShim, TensorFlowShim, keras_model_fns, MXNetShim\nfrom .shims import maybe_handshake_model\nfrom .optimizers import Adam, RAdam, SGD, Optimizer\nfrom .schedules import cyclic_triangular, warmup_linear, constant, constant_then\nfrom .schedules import decaying, slanted_triangular, compounding\nfrom .types import Ragged, Padded, ArgsKwargs, Unserializable\nfrom .util import fix_random_seed, is_cupy_array, set_active_gpu\nfrom .util import prefer_gpu, require_gpu, DataValidationError\nfrom .util import to_categorical, get_width, get_array_module, to_numpy\nfrom .util import torch2xp, xp2torch, tensorflow2xp, xp2tensorflow, mxnet2xp, xp2mxnet\nfrom .backends import get_ops, set_current_ops, get_current_ops, use_ops\nfrom .backends import Ops, CupyOps, NumpyOps, JaxOps, has_cupy, has_jax\nfrom .backends import use_pytorch_for_gpu_memory, use_tensorflow_for_gpu_memory\n\nfrom .layers import Dropout, Embed, expand_window, HashEmbed, LayerNorm, Linear\nfrom .layers import Maxout, Mish, MultiSoftmax, Relu, softmax_activation, Softmax, LSTM\nfrom .layers import CauchySimilarity, ParametricAttention, Logistic\nfrom .layers import SparseLinear, StaticVectors, FeatureExtractor\nfrom .layers import PyTorchWrapper, PyTorchRNNWrapper, PyTorchLSTM\nfrom .layers import TensorFlowWrapper, keras_subclass, MXNetWrapper\n\nfrom .layers import add, bidirectional, chain, clone, concatenate, noop\nfrom .layers import residual, uniqued, siamese, list2ragged, ragged2list\nfrom .layers import with_array, with_padded, with_list, with_ragged, with_flatten\nfrom .layers import with_reshape, with_getitem, strings2arrays, list2array\nfrom .layers import list2ragged, ragged2list, list2padded, padded2list, remap_ids\nfrom .layers import array_getitem, with_cpu, with_debug\n\nfrom .layers import reduce_max, reduce_mean, reduce_sum\n\n\n__all__ = list(locals().keys())\n'"
thinc/config.py,0,"b'from typing import Union, Dict, Any, Optional, List, Tuple, Callable, Type, Sequence\nfrom types import GeneratorType\nfrom configparser import ConfigParser, ExtendedInterpolation\nfrom pathlib import Path\nfrom pydantic import BaseModel, create_model, ValidationError\nfrom pydantic.main import ModelMetaclass\nfrom wasabi import table\nimport srsly\nimport catalogue\nimport inspect\nimport io\nimport numpy\n\nfrom .types import Decorator\n\n\ndef get_configparser():\n    config = ConfigParser(interpolation=ExtendedInterpolation())\n    # Preserve case of keys: https://stackoverflow.com/a/1611877/6400719\n    config.optionxform = str\n    return config\n\n\nclass Config(dict):\n    """"""This class holds the model and training configuration and can load and\n    save the TOML-style configuration format from/to a string, file or bytes.\n    The Config class is a subclass of dict and uses Python\'s ConfigParser\n    under the hood.\n    """"""\n\n    def __init__(\n        self, data: Optional[Union[Dict[str, Any], ""ConfigParser"", ""Config""]] = None\n    ) -> None:\n        """"""Initialize a new Config object with optional data.""""""\n        dict.__init__(self)\n        if data is None:\n            data = {}\n        self.update(data)\n\n    def interpret_config(self, config: Union[Dict[str, Any], ""ConfigParser""]):\n        """"""Interpret a config, parse nested sections and parse the values\n        as JSON. Mostly used internally and modifies the config in place.\n        """"""\n        for section, values in config.items():\n            if section == ""DEFAULT"":\n                # Skip [DEFAULT] section for now since it causes validation\n                # errors and we don\'t want to use it\n                continue\n            parts = section.split(""."")\n            node = self\n            for part in parts:\n                node = node.setdefault(part, {})\n            if not isinstance(node, dict):\n                # Happens if both value *and* subsection were defined for a key\n                err = [{""loc"": parts, ""msg"": ""found conflicting values""}]\n                raise ConfigValidationError(f""{self}\\n{({part: dict(values)})}"", err)\n            for key, value in values.items():\n                node[key] = srsly.json_loads(config.get(section, key))\n\n    def from_str(self, text: str) -> ""Config"":\n        ""Load the config from a string.""\n        config = get_configparser()\n        config.read_string(text)\n        for key in list(self.keys()):\n            self.pop(key)\n        self.interpret_config(config)\n        return self\n\n    def to_str(self) -> str:\n        """"""Write the config to a string.""""""\n        flattened = get_configparser()\n        queue: List[Tuple[tuple, ""Config""]] = [(tuple(), self)]\n        for path, node in queue:\n            for key, value in node.items():\n                if hasattr(value, ""items""):\n                    queue.append((path + (key,), value))\n                else:\n                    assert path\n                    section_name = ""."".join(path)\n                    if not flattened.has_section(section_name):\n                        flattened.add_section(section_name)\n                    flattened.set(section_name, key, srsly.json_dumps(value))\n        string_io = io.StringIO()\n        flattened.write(string_io)\n        return string_io.getvalue().strip()\n\n    def to_bytes(self) -> bytes:\n        """"""Serialize the config to a byte string.""""""\n        return self.to_str().encode(""utf8"")\n\n    def from_bytes(self, bytes_data: bytes) -> ""Config"":\n        """"""Load the config from a byte string.""""""\n        return self.from_str(bytes_data.decode(""utf8""))\n\n    def to_disk(self, path: Union[str, Path]):\n        """"""Serialize the config to a file.""""""\n        path = Path(path)\n        with path.open(""w"", encoding=""utf8"") as file_:\n            file_.write(self.to_str())\n\n    def from_disk(self, path: Union[str, Path]) -> ""Config"":\n        """"""Load config from a file.""""""\n        with Path(path).open(""r"", encoding=""utf8"") as file_:\n            text = file_.read()\n        return self.from_str(text)\n\n\nclass ConfigValidationError(ValueError):\n    def __init__(\n        self,\n        config: Union[Config, Dict[str, Dict[str, Any]]],\n        errors: List[Dict[str, Any]],\n        message: str = ""Config validation error"",\n        element: str = """",\n    ) -> None:\n        """"""Custom error for validating configs.""""""\n        data = []\n        for error in errors:\n            err_loc = "" -> "".join([str(p) for p in error.get(""loc"", [])])\n            if element:\n                err_loc = f""{element} -> {err_loc}""\n            data.append((err_loc, error.get(""msg"")))\n        result = [message, table(data), f""{config}""]\n        ValueError.__init__(self, ""\\n\\n"" + ""\\n"".join(result))\n\n\nARGS_FIELD = ""*""\nARGS_FIELD_ALIAS = ""VARIABLE_POSITIONAL_ARGS""  # user is unlikely going to use this\n\n\nclass EmptySchema(BaseModel):\n    class Config:\n        extra = ""allow""\n        arbitrary_types_allowed = True\n\n\nclass _PromiseSchemaConfig:\n    extra = ""forbid""\n    arbitrary_types_allowed = True\n    # Underscore fields are not allowed in model, so use alias\n    fields = {ARGS_FIELD_ALIAS: {""alias"": ARGS_FIELD}}\n\n\nclass registry(object):\n    # fmt: off\n    optimizers: Decorator = catalogue.create(""thinc"", ""optimizers"", entry_points=True)\n    schedules: Decorator = catalogue.create(""thinc"", ""schedules"", entry_points=True)\n    layers: Decorator = catalogue.create(""thinc"", ""layers"", entry_points=True)\n    losses: Decorator = catalogue.create(""thinc"", ""losses"", entry_points=True)\n    initializers: Decorator = catalogue.create(""thinc"", ""initializers"", entry_points=True)\n    datasets: Decorator = catalogue.create(""thinc"", ""datasets"", entry_points=True)\n    # fmt: on\n\n    @classmethod\n    def create(cls, registry_name: str, entry_points: bool = False) -> None:\n        """"""Create a new custom registry.""""""\n        if hasattr(cls, registry_name):\n            raise ValueError(f""Registry \'{registry_name}\' already exists"")\n        reg: Decorator = catalogue.create(\n            ""thinc"", registry_name, entry_points=entry_points\n        )\n        setattr(cls, registry_name, reg)\n\n    @classmethod\n    def get(cls, registry_name: str, func_name: str) -> Callable:\n        """"""Get a registered function from a given registry.""""""\n        if not hasattr(cls, registry_name):\n            raise ValueError(f""Unknown registry: \'{registry_name}\'"")\n        reg = getattr(cls, registry_name)\n        func = reg.get(func_name)\n        if func is None:\n            raise ValueError(f""Could not find \'{func_name}\' in \'{registry_name}\'"")\n        return func\n\n    @classmethod\n    def make_from_config(\n        cls,\n        config: Union[Config, Dict[str, Dict[str, Any]]],\n        *,\n        schema: Type[BaseModel] = EmptySchema,\n        validate: bool = True,\n    ) -> Config:\n        """"""Unpack a config dictionary, creating objects from the registry\n        recursively. If validate=True, the config will be validated against the\n        type annotations of the registered functions referenced in the config\n        (if available) and/or the schema (if available).\n        """"""\n        # Valid: {""optimizer"": {""@optimizers"": ""my_cool_optimizer"", ""rate"": 1.0}}\n        # Invalid: {""@optimizers"": ""my_cool_optimizer"", ""rate"": 1.0}\n        if cls.is_promise(config):\n            err_msg = ""The top-level config object can\'t be a reference to a registered function.""\n            raise ConfigValidationError(config, [{""msg"": err_msg}])\n        _, _, resolved = cls._fill(config, schema, validate)\n        return resolved\n\n    @classmethod\n    def fill_config(\n        cls,\n        config: Union[Config, Dict[str, Dict[str, Any]]],\n        *,\n        schema: Type[BaseModel] = EmptySchema,\n        validate: bool = True,\n    ) -> Config:\n        """"""Unpack a config dictionary, leave all references to registry\n        functions intact and don\'t resolve them, but fill in all values and\n        defaults based on the type annotations. If validate=True, the config\n        will be validated against the type annotations of the registered\n        functions referenced in the config (if available) and/or the schema\n        (if available).\n        """"""\n        # Valid: {""optimizer"": {""@optimizers"": ""my_cool_optimizer"", ""rate"": 1.0}}\n        # Invalid: {""@optimizers"": ""my_cool_optimizer"", ""rate"": 1.0}\n        if cls.is_promise(config):\n            err_msg = ""The top-level config object can\'t be a reference to a registered function.""\n            raise ConfigValidationError(config, [{""msg"": err_msg}])\n        filled, _, _ = cls._fill(config, schema, validate)\n        return filled\n\n    @classmethod\n    def _fill(\n        cls,\n        config: Union[Config, Dict[str, Dict[str, Any]]],\n        schema: Type[BaseModel] = EmptySchema,\n        validate: bool = True,\n        parent: str = """",\n    ) -> Tuple[Config, Config, Config]:\n        """"""Build three representations of the config:\n        1. All promises are preserved (just like config user would provide).\n        2. Promises are replaced by their return values. This is the validation\n           copy and will be parsed by pydantic. It lets us include hacks to\n           work around problems (e.g. handling of generators).\n        3. Final copy with promises replaced by their return values. This is\n           what registry.make_from_config returns.\n        """"""\n        filled: Dict[str, Any] = {}\n        validation: Dict[str, Any] = {}\n        final: Dict[str, Any] = {}\n        for key, value in config.items():\n            key_parent = f""{parent}.{key}"".strip(""."")\n            if cls.is_promise(value):\n                promise_schema = cls.make_promise_schema(value)\n                filled[key], validation[key], final[key] = cls._fill(\n                    value, promise_schema, validate, parent=key_parent\n                )\n                # Call the function and populate the field value. We can\'t just\n                # create an instance of the type here, since this wouldn\'t work\n                # for generics / more complex custom types\n                getter = cls.get_constructor(final[key])\n                args, kwargs = cls.parse_args(final[key])\n                try:\n                    getter_result = getter(*args, **kwargs)\n                except Exception as err:\n                    err_msg = ""Can\'t construct config: calling registry function failed""\n                    raise ConfigValidationError(\n                        {key: value}, [{""msg"": err, ""loc"": [getter.__name__]}], err_msg\n                    )\n                validation[key] = getter_result\n                final[key] = getter_result\n                if isinstance(validation[key], GeneratorType):\n                    # If value is a generator we can\'t validate type without\n                    # consuming it (which doesn\'t work if it\'s infinite \xe2\x80\x93 see\n                    # schedule for examples). So we skip it.\n                    validation[key] = []\n            elif hasattr(value, ""items""):\n                field_type = EmptySchema\n                if key in schema.__fields__:\n                    field = schema.__fields__[key]\n                    field_type = field.type_\n                    if not isinstance(field.type_, ModelMetaclass):\n                        # If we don\'t have a pydantic schema and just a type\n                        field_type = EmptySchema\n                filled[key], validation[key], final[key] = cls._fill(\n                    value, field_type, validate, parent=key_parent\n                )\n                if key == ARGS_FIELD and isinstance(validation[key], dict):\n                    # If the value of variable positional args is a dict (e.g.\n                    # created via config blocks), only use its values\n                    validation[key] = list(validation[key].values())\n                    final[key] = list(final[key].values())\n            else:\n                filled[key] = value\n                # Prevent pydantic from consuming generator if part of a union\n                validation[key] = value if not isinstance(value, GeneratorType) else []\n                final[key] = value\n        # Now that we\'ve filled in all of the promises, update with defaults\n        # from schema, and validate if validation is enabled\n        if validate:\n            try:\n                result = schema.parse_obj(validation)\n            except ValidationError as e:\n                raise ConfigValidationError(config, e.errors(), element=parent)\n        else:\n            # Same as parse_obj, but without validation\n            result = schema.construct(**validation)\n        validation.update(result.dict(exclude={ARGS_FIELD_ALIAS}))\n        filled, final = cls._update_from_parsed(validation, filled, final)\n        return Config(filled), Config(validation), Config(final)\n\n    @classmethod\n    def _update_from_parsed(\n        cls, validation: Dict[str, Any], filled: Dict[str, Any], final: Dict[str, Any]\n    ):\n        """"""Update the final result with the parsed config like converted\n        values recursively.\n        """"""\n        for key, value in validation.items():\n            if key not in filled:\n                filled[key] = value\n            if key not in final:\n                final[key] = value\n            if isinstance(value, dict):\n                filled[key], final[key] = cls._update_from_parsed(\n                    value, filled[key], final[key]\n                )\n            # Update final config with parsed value if they\'re not equal (in\n            # value and in type) but not if it\'s a generator because we had to\n            # replace that to validate it correctly\n            elif key == ARGS_FIELD:\n                continue  # don\'t substitute if list of positional args\n            elif isinstance(value, numpy.ndarray):  # check numpy first, just in case\n                final[key] = value\n            elif (\n                value != final[key] or not isinstance(type(value), type(final[key]))\n            ) and not isinstance(final[key], GeneratorType):\n                final[key] = value\n        return filled, final\n\n    @classmethod\n    def is_promise(cls, obj: Any) -> bool:\n        """"""Check whether an object is a ""promise"", i.e. contains a reference\n        to a registered function (via a key starting with `""@""`.\n        """"""\n        if not hasattr(obj, ""keys""):\n            return False\n        id_keys = [k for k in obj.keys() if k.startswith(""@"")]\n        if len(id_keys):\n            return True\n        return False\n\n    @classmethod\n    def get_constructor(cls, obj: Dict[str, Any]) -> Callable:\n        id_keys = [k for k in obj.keys() if k.startswith(""@"")]\n        if len(id_keys) != 1:\n            err_msg = f""A block can only contain one function registry reference. Got: {id_keys}""\n            raise ConfigValidationError(obj, [{""msg"": err_msg}])\n        else:\n            key = id_keys[0]\n            value = obj[key]\n            return cls.get(key[1:], value)\n\n    @classmethod\n    def parse_args(cls, obj: Dict[str, Any]) -> Tuple[List[Any], Dict[str, Any]]:\n        args = []\n        kwargs = {}\n        for key, value in obj.items():\n            if not key.startswith(""@""):\n                if key == ARGS_FIELD:\n                    args = value\n                else:\n                    kwargs[key] = value\n        return args, kwargs\n\n    @classmethod\n    def make_promise_schema(cls, obj: Dict[str, Any]) -> Type[BaseModel]:\n        """"""Create a schema for a promise dict (referencing a registry function)\n        by inspecting the function signature.\n        """"""\n        func = cls.get_constructor(obj)\n        # Read the argument annotations and defaults from the function signature\n        id_keys = [k for k in obj.keys() if k.startswith(""@"")]\n        sig_args: Dict[str, Any] = {id_keys[0]: (str, ...)}\n        for param in inspect.signature(func).parameters.values():\n            # If no annotation is specified assume it\'s anything\n            annotation = param.annotation if param.annotation != param.empty else Any\n            # If no default value is specified assume that it\'s required\n            default = param.default if param.default != param.empty else ...\n            # Handle spread arguments and use their annotation as Sequence[whatever]\n            if param.kind == param.VAR_POSITIONAL:\n                spread_annot = Sequence[annotation]  # type: ignore\n                sig_args[ARGS_FIELD_ALIAS] = (spread_annot, default)\n            else:\n                sig_args[param.name] = (annotation, default)\n        sig_args[""__config__""] = _PromiseSchemaConfig\n        return create_model(""ArgModel"", **sig_args)\n\n\n__all__ = [""Config"", ""registry"", ""ConfigValidationError""]\n'"
thinc/initializers.py,0,"b'from typing import Callable\nimport numpy\n\nfrom .backends import Ops\nfrom .config import registry\nfrom .types import FloatsXd, Shape\nfrom .util import partial\n\n# TODO: Harmonize naming with Keras, and fill in missing entries\n# https://keras.io/initializers/ We should also have He normal/uniform\n# and probably lecun normal/uniform.\n\n# Initialize via numpy, before copying to ops. This makes it easier to work with\n# the different backends, because the backend won\'t affect the randomization.\n# It\'s especially helpful for JAX, which has a pretty intrincate PRNG scheme I\n# haven\'t figured out yet.\n\n\ndef lecun_normal_init(ops: Ops, shape: Shape) -> FloatsXd:\n    scale = numpy.sqrt(1.0 / shape[1])\n    return ops.asarray_f(numpy.random.normal(0, scale, shape))\n\n\n@registry.initializers(""lecun_normal_init.v1"")\ndef configure_lecun_normal_init() -> Callable[[Shape], FloatsXd]:\n    return partial(lecun_normal_init)\n\n\ndef he_normal_init(ops: Ops, shape: Shape) -> FloatsXd:\n    scale = numpy.sqrt(2.0 / shape[1])\n    return ops.asarray_f(numpy.random.normal(0, scale, shape))\n\n\n@registry.initializers(""he_normal_init.v1"")\ndef configure_he_normal_init() -> Callable[[Shape], FloatsXd]:\n    return partial(he_normal_init)\n\n\ndef glorot_normal_init(ops: Ops, shape: Shape) -> FloatsXd:\n    scale = numpy.sqrt(2.0 / (shape[1] + shape[0]))\n    return ops.asarray_f(numpy.random.normal(0, scale, shape))\n\n\n@registry.initializers(""glorot_normal_init.v1"")\ndef configure_glorot_normal_init() -> Callable[[Shape], FloatsXd]:\n    return partial(glorot_normal_init)\n\n\ndef he_uniform_init(ops: Ops, shape: Shape) -> FloatsXd:\n    scale = numpy.sqrt(6.0 / shape[1])\n    return ops.asarray_f(numpy.random.uniform(-scale, scale, shape))\n\n\n@registry.initializers(""he_uniform_init.v1"")\ndef configure_he_uniform_init() -> Callable[[Shape], FloatsXd]:\n    return partial(he_uniform_init)\n\n\ndef lecun_uniform_init(ops: Ops, shape: Shape) -> FloatsXd:\n    scale = numpy.sqrt(3.0 / shape[1])\n    return ops.asarray_f(numpy.random.uniform(-scale, scale, shape))\n\n\n@registry.initializers(""lecun_uniform_init.v1"")\ndef configure_lecun_uniform_init() -> Callable[[Shape], FloatsXd]:\n    return partial(lecun_uniform_init)\n\n\ndef glorot_uniform_init(ops: Ops, shape: Shape) -> FloatsXd:\n    scale = numpy.sqrt(6.0 / (shape[0] + shape[1]))\n    return ops.asarray_f(numpy.random.uniform(-scale, scale, shape))\n\n\n@registry.initializers(""glorot_uniform_init.v1"")\ndef configure_glorot_uniform_init() -> Callable[[Shape], FloatsXd]:\n    return partial(glorot_uniform_init)\n\n\ndef zero_init(ops: Ops, shape: Shape) -> FloatsXd:\n    return ops.alloc(shape)\n\n\n@registry.initializers(""zero_init.v1"")\ndef configure_zero_init() -> Callable[[FloatsXd], FloatsXd]:\n    return partial(zero_init)\n\n\ndef uniform_init(\n    ops: Ops, shape: Shape, *, lo: float = -0.1, hi: float = 0.1\n) -> FloatsXd:\n    values = numpy.random.uniform(lo, hi, shape)\n    return ops.asarray_f(values.astype(""float32""))\n\n\n@registry.initializers(""uniform_init.v1"")\ndef configure_uniform_init(\n    *, lo: float = -0.1, hi: float = 0.1\n) -> Callable[[FloatsXd], FloatsXd]:\n    return partial(uniform_init, lo=lo, hi=hi)\n\n\ndef normal_init(ops: Ops, shape: Shape, *, mean: int = 0) -> FloatsXd:\n    size = int(ops.xp.prod(ops.xp.asarray(shape)))\n    inits = numpy.random.normal(scale=mean, size=size).astype(""float32"")\n    inits = ops.reshape_f(inits, shape)\n    return ops.asarray_f(inits)\n\n\n@registry.initializers(""normal_init.v1"")\ndef configure_normal_init(*, mean: float = 0) -> Callable[[FloatsXd], FloatsXd]:\n    return partial(normal_init, mean=mean)\n\n\n__all__ = [\n    ""normal_init"",\n    ""uniform_init"",\n    ""glorot_uniform_init"",\n    ""zero_init"",\n    ""lecun_uniform_init"",\n    ""he_uniform_init"",\n    ""glorot_normal_init"",\n    ""he_normal_init"",\n    ""lecun_normal_init"",\n]\n'"
thinc/loss.py,0,"b'from typing import Tuple, List, cast, TypeVar, Generic, Any, Union, Optional\nfrom typing import Dict\n\nfrom .types import Floats2d, Ints1d\nfrom .util import get_array_module, to_categorical\nfrom .config import registry\n\n\nLossT = TypeVar(""LossT"")\nGradT = TypeVar(""GradT"")\nGuessT = TypeVar(""GuessT"")\nTruthT = TypeVar(""TruthT"")\nIntsOrFloats = Union[Ints1d, Floats2d]\nIntsOrFloatsOrStrs = Union[Ints1d, Floats2d, List[int], List[str]]\n\n\nclass Loss(Generic[GuessT, TruthT, GradT, LossT]):  # pragma: no cover\n    """"""Base class for classes computing the loss / gradient. The class can\n    be initialized with settings if needed. It provides get_loss and\n    get_grad as separate methods to allow calculating them separately. It\n    also provides a __call__ method that returns a tuple of both.\n    """"""\n\n    def __init__(self, **kwargs: Any) -> None:\n        ...\n\n    def __call__(self, guesses: GuessT, truths: TruthT) -> Tuple[GradT, LossT]:\n        return self.get_grad(guesses, truths), self.get_loss(guesses, truths)\n\n    def get_grad(self, guesses: GuessT, truths: TruthT) -> GradT:\n        ...\n\n    def get_loss(self, guesses: GuessT, truths: TruthT) -> LossT:\n        ...\n\n\nclass CategoricalCrossentropy(Loss):\n    names: Optional[List[str]]\n    missing_value: Optional[Union[str, int]]\n    _name_to_i: Dict[str, int]\n\n    def __init__(\n        self,\n        *,\n        normalize: bool = True,\n        names: Optional[List[str]] = None,\n        missing_value: Optional[Union[str, int]] = None,\n    ):\n        self.normalize = normalize\n        self.names = names\n        self.missing_value = missing_value\n        if names is not None:\n            self._name_to_i = {name: i for i, name in enumerate(names)}\n        else:\n            self._name_to_i = {}\n\n    def convert_truths(self, truths, guesses: Floats2d) -> Tuple[Floats2d, Floats2d]:\n        xp = get_array_module(guesses)\n        missing = []\n        missing_value = self.missing_value\n        # Convert list of ints or list of strings\n        if isinstance(truths, list):\n            truths = list(truths)\n            if len(truths) and not isinstance(truths[0], int):\n                if self.names is None:\n                    msg = (\n                        ""Cannot calculate loss from list of strings without names. ""\n                        ""You can pass the names as a keyword argument when you ""\n                        ""create the loss object, ""\n                        ""e.g. CategoricalCrossentropy(names=[\'dog\', \'cat\'])""\n                    )\n                    raise ValueError(msg)\n                for i, value in enumerate(truths):\n                    if value == missing_value:\n                        truths[i] = self.names[0]\n                        missing.append(i)\n                truths = [self._name_to_i[name] for name in truths]\n            truths = xp.asarray(truths, dtype=""i"")\n        else:\n            missing = []\n        if truths.ndim != guesses.ndim:\n            # transform categorical values to one-hot encoding\n            truths = to_categorical(cast(Ints1d, truths), n_classes=guesses.shape[-1])\n        mask = _make_mask(missing, guesses)\n        return truths, mask\n\n    def __call__(\n        self, guesses: Floats2d, truths: IntsOrFloatsOrStrs,\n    ) -> Tuple[Floats2d, float]:\n        d_truth = self.get_grad(guesses, truths)\n        return (d_truth, self._get_loss_from_grad(d_truth))\n\n    def get_grad(self, guesses: Floats2d, truths: IntsOrFloatsOrStrs,) -> Floats2d:\n        target, mask = self.convert_truths(truths, guesses)\n        if guesses.shape != target.shape:  # pragma: no cover\n            err = f""Cannot calculate CategoricalCrossentropy loss: mismatched shapes: {guesses.shape} vs {target.shape}.""\n            raise ValueError(err)\n        if guesses.any() > 1 or guesses.any() < 0:  # pragma: no cover\n            err = f""Cannot calculate CategoricalCrossentropy loss with guesses outside the [0,1] interval.""\n            raise ValueError(err)\n        if target.any() > 1 or target.any() < 0:  # pragma: no cover\n            err = f""Cannot calculate CategoricalCrossentropy loss with truth values outside the [0,1] interval.""\n            raise ValueError(err)\n        difference = guesses - target\n        difference *= mask\n        if self.normalize:\n            difference = difference / guesses.shape[0]\n        return difference\n\n    def get_loss(self, guesses: Floats2d, truths: IntsOrFloats,) -> float:\n        d_truth = self.get_grad(guesses, truths)\n        return self._get_loss_from_grad(d_truth)\n\n    def _get_loss_from_grad(self, d_truth: Floats2d) -> float:\n        # TODO: Add overload for axis=None case to sum\n        return (d_truth ** 2).sum()  # type: ignore\n\n\n@registry.losses(""CategoricalCrossentropy.v1"")\ndef configure_CategoricalCrossentropy(\n    *,\n    normalize: bool = True,\n    names: Optional[List[str]] = None,\n    missing_value: Optional[Union[str, int]] = None,\n) -> CategoricalCrossentropy:\n    return CategoricalCrossentropy(\n        normalize=normalize, names=names, missing_value=missing_value\n    )\n\n\nclass SequenceCategoricalCrossentropy(Loss):\n    def __init__(\n        self,\n        *,\n        normalize: bool = True,\n        names: Optional[List[str]] = None,\n        missing_value: Optional[Union[str, int]] = None,\n    ):\n        self.cc = CategoricalCrossentropy(\n            normalize=False, names=names, missing_value=missing_value\n        )\n        self.normalize = normalize\n\n    def __call__(\n        self, guesses: List[Floats2d], truths: List[Union[Ints1d, Floats2d]]\n    ) -> Tuple[List[Floats2d], float]:\n        grads = self.get_grad(guesses, truths)\n        loss = self._get_loss_from_grad(grads)\n        return grads, loss\n\n    def get_grad(\n        self, guesses: List[Floats2d], truths: List[Union[Ints1d, Floats2d]]\n    ) -> List[Floats2d]:\n        err = ""Cannot calculate SequenceCategoricalCrossentropy loss: guesses and truths must be same length""\n        if len(guesses) != len(truths):  # pragma: no cover\n            raise ValueError(err)\n        n = sum(yh.shape[0] for yh in guesses)\n        d_scores = []\n        for yh, y in zip(guesses, truths):\n            d_yh = self.cc.get_grad(yh, y)\n            if self.normalize:\n                d_yh /= n\n            d_scores.append(d_yh)\n        return d_scores\n\n    def get_loss(\n        self, guesses: List[Floats2d], truths: List[Union[Ints1d, Floats2d]]\n    ) -> float:\n        return self._get_loss_from_grad(self.get_grad(guesses, truths))\n\n    def _get_loss_from_grad(self, grads: List[Floats2d]) -> float:\n        loss = 0.0\n        for grad in grads:\n            loss += self.cc._get_loss_from_grad(grad)\n        return loss\n\n\n@registry.losses(""SequenceCategoricalCrossentropy.v1"")\ndef configure_SequenceCategoricalCrossentropy(\n    *, normalize: bool = True, names: Optional[List[str]] = None\n) -> SequenceCategoricalCrossentropy:\n    return SequenceCategoricalCrossentropy(normalize=normalize, names=names)\n\n\nclass L2Distance(Loss):\n    def __init__(self, *, normalize: bool = True):\n        self.normalize = normalize\n\n    def __call__(self, guesses: Floats2d, truths: Floats2d) -> Tuple[Floats2d, float]:\n        return self.get_grad(guesses, truths), self.get_loss(guesses, truths)\n\n    def get_grad(self, guesses: Floats2d, truths: Floats2d) -> Floats2d:\n        if guesses.shape != truths.shape:  # pragma: no cover\n            err = f""Cannot calculate L2 distance: mismatched shapes: {guesses.shape} vs {truths.shape}.""\n            raise ValueError(err)\n        difference = guesses - truths\n        if self.normalize:\n            difference = difference / guesses.shape[0]\n        return difference\n\n    def get_loss(self, guesses: Floats2d, truths: Floats2d) -> float:\n        if guesses.shape != truths.shape:  # pragma: no cover\n            err = f""Cannot calculate L2 distance: mismatched shapes: {guesses.shape} vs {truths.shape}.""\n            raise ValueError(err)\n        d_truth = self.get_grad(guesses, truths)\n        # TODO: Add overload for axis=None case to sum\n        return (d_truth ** 2).sum()  # type: ignore\n\n\n@registry.losses(""L2Distance.v1"")\ndef configure_L2Distance(*, normalize: bool = True) -> L2Distance:\n    return L2Distance(normalize=normalize)\n\n\nclass CosineDistance(Loss):\n    def __init__(self, *, normalize: bool = True, ignore_zeros: bool = False):\n        self.normalize = normalize\n        self.ignore_zeros = ignore_zeros\n\n    def __call__(self, guesses: Floats2d, truths: Floats2d) -> Tuple[Floats2d, float]:\n        return self.get_grad(guesses, truths), self.get_loss(guesses, truths)\n\n    def get_similarity(self, guesses: Floats2d, truths: Floats2d) -> float:\n        if guesses.shape != truths.shape:  # pragma: no cover\n            err = f""Cannot calculate cosine similarity: mismatched shapes: {guesses.shape} vs {truths.shape}.""\n            raise ValueError(err)\n\n        xp = get_array_module(guesses)\n        # Add a small constant to avoid 0 vectors\n        yh = guesses + 1e-8\n        y = truths + 1e-8\n        norm_yh = xp.linalg.norm(yh, axis=1, keepdims=True)\n        norm_y = xp.linalg.norm(y, axis=1, keepdims=True)\n        mul_norms = norm_yh * norm_y\n        cosine = (yh * y).sum(axis=1, keepdims=True) / mul_norms\n        return cosine\n\n    def get_grad(self, guesses: Floats2d, truths: Floats2d) -> Floats2d:\n        if guesses.shape != truths.shape:  # pragma: no cover\n            err = f""Cannot calculate cosine similarity: mismatched shapes: {guesses.shape} vs {truths.shape}.""\n            raise ValueError(err)\n\n        # Note: not using get_distance() here to avoid duplicating certain calculations\n        xp = get_array_module(guesses)\n        # Find the zero vectors\n        if self.ignore_zeros:\n            zero_indices = xp.abs(truths).sum(axis=1) == 0\n        # Add a small constant to avoid 0 vectors\n        yh = guesses + 1e-8\n        y = truths + 1e-8\n        # https://math.stackexchange.com/questions/1923613/partial-derivative-of-cosinesimilarity\n        norm_yh = xp.linalg.norm(yh, axis=1, keepdims=True)\n        norm_y = xp.linalg.norm(y, axis=1, keepdims=True)\n        mul_norms = norm_yh * norm_y\n        cosine = (yh * y).sum(axis=1, keepdims=True) / mul_norms\n        d_yh = (y / mul_norms) - (cosine * (yh / norm_yh ** 2))\n        if self.ignore_zeros:\n            # If the target was a zero vector, don\'t count it in the loss.\n            d_yh[zero_indices] = 0\n        if self.normalize:\n            d_yh = d_yh / guesses.shape[0]\n        return -d_yh\n\n    def get_loss(self, guesses: Floats2d, truths: Floats2d) -> float:\n        if guesses.shape != truths.shape:  # pragma: no cover\n            err = f""Cannot calculate cosine similarity: mismatched shapes: {guesses.shape} vs {truths.shape}.""\n            raise ValueError(err)\n\n        xp = get_array_module(guesses)\n        cosine = self.get_similarity(guesses, truths)\n        losses = xp.abs(cosine - 1)\n        if self.ignore_zeros:\n            # If the target was a zero vector, don\'t count it in the loss.\n            zero_indices = xp.abs(truths).sum(axis=1) == 0\n            losses[zero_indices] = 0\n        if self.normalize:\n            losses = losses / guesses.shape[0]\n        loss = losses.sum()\n        return loss\n\n\n@registry.losses(""CosineDistance.v1"")\ndef configure_CosineDistance(\n    *, normalize: bool = True, ignore_zeros: bool = False\n) -> CosineDistance:\n    return CosineDistance(normalize=normalize, ignore_zeros=ignore_zeros)\n\n\ndef _make_mask(missing, guesses) -> Floats2d:\n    xp = get_array_module(guesses)\n    mask = xp.ones(guesses.shape, dtype=""f"")\n    mask[missing] = 0\n    return mask\n\n\n__all__ = [\n    ""SequenceCategoricalCrossentropy"",\n    ""CategoricalCrossentropy"",\n    ""L2Distance"",\n    ""CosineDistance"",\n]\n'"
thinc/model.py,0,"b'from typing import Dict, List, Callable, Optional, Any, Union, Iterable, Set, cast\nfrom typing import Generic, Sequence, Tuple, TypeVar\nimport contextlib\nfrom contextvars import ContextVar\nimport srsly\nfrom pathlib import Path\nimport copy\nimport functools\nimport threading\n\nfrom .backends import ParamServer, Ops, NumpyOps, CupyOps, get_current_ops\nfrom .optimizers import Optimizer  # noqa: F401\nfrom .shims import Shim\nfrom .util import convert_recursive, is_xp_array, get_array_module\nfrom .util import partial, validate_fwd_input_output\nfrom .types import FloatsXd, Floats1d\n\n\nInT = TypeVar(""InT"")\nOutT = TypeVar(""OutT"")\nSelfT = TypeVar(""SelfT"", bound=""Model"")\n\ncontext_operators: ContextVar[dict] = ContextVar(""context_operators"", default={})\nDATA_VALIDATION: ContextVar[bool] = ContextVar(""DATA_VALIDATION"", default=True)\n\n\ndef empty_init(model: ""Model"", *args, **kwargs) -> ""Model"":\n    return model\n\n\nclass Model(Generic[InT, OutT]):\n    """"""Class for implementing Thinc models and layers.""""""\n\n    global_id: int = 0\n    global_id_lock: threading.Lock = threading.Lock()\n    _context_operators = context_operators\n\n    name: str\n    ops: Ops\n    id: int\n    _func: Callable\n    init: Callable\n    _params: ParamServer\n    _dims: Dict[str, Optional[int]]\n    _layers: List[""Model""]\n    _shims: List[Shim]\n    _attrs: Dict[str, Any]\n    _has_params: Dict[str, Optional[bool]]\n\n    # This ""locks"" the class, so we get an error if you try to assign to\n    # an unexpected variable.\n    __slots__ = [\n        ""name"",\n        ""id"",\n        ""ops"",\n        ""_func"",\n        ""init"",\n        ""_params"",\n        ""_dims"",\n        ""_attrs"",\n        ""_refs"",\n        ""_layers"",\n        ""_shims"",\n        ""_has_params"",\n    ]\n\n    def __init__(\n        self,\n        name: str,\n        forward: Callable,\n        *,\n        init: Optional[Callable] = None,\n        dims: Dict[str, Optional[int]] = {},\n        params: Dict[str, Optional[FloatsXd]] = {},\n        layers: Sequence[""Model""] = [],\n        shims: List[Shim] = [],\n        attrs: Dict[str, Any] = {},\n        refs: Dict[str, Optional[""Model""]] = {},\n        ops: Optional[Union[NumpyOps, CupyOps]] = None,\n    ):\n        """"""Initialize a new model.""""""\n        self.name = name\n        if init is None:\n            init = partial(empty_init, self)\n        # Assign to callable attrs: https://github.com/python/mypy/issues/2427\n        setattr(self, ""_func"", forward)\n        setattr(self, ""init"", init)\n        self.ops = ops if ops is not None else get_current_ops()\n        self._params = ParamServer()\n        self._dims = dict(dims)\n        self._attrs = dict(attrs)\n        self._refs = dict(refs)\n        self._layers = list(layers)\n        self._shims = list(shims)\n        # Take care to increment the base class here! It needs to be unique\n        # across all models.\n        with Model.global_id_lock:\n            Model.global_id += 1\n        self.id = Model.global_id\n        self._has_params = {}\n        for name, value in params.items():\n            self._has_params[name] = None\n            if value is not None:\n                self.set_param(name, value)\n\n    @property\n    def layers(self) -> List[""Model""]:\n        """"""A list of child layers of the model. You can append to it to add\n        layers but not reassign it.\n        """"""\n        return self._layers\n\n    @property\n    def shims(self) -> List[Shim]:\n        return self._shims\n\n    @property\n    def attrs(self) -> Dict[str, Any]:\n        """"""A dict of the model\'s attrs. You can write to it to update attrs but\n        not reassign it.\n        """"""\n        return self._attrs\n\n    @property\n    def param_names(self) -> Tuple[str, ...]:\n        """"""Get the names of registered parameter (including unset).""""""\n        return tuple(self._has_params.keys())\n\n    @property\n    def grad_names(self) -> Tuple[str, ...]:\n        """"""Get the names of parameters with registered gradients (including unset).""""""\n        return tuple([name for name in self.param_names if self.has_grad(name)])\n\n    @property\n    def dim_names(self) -> Tuple[str, ...]:\n        """"""Get the names of registered dimensions (including unset).""""""\n        return tuple(self._dims.keys())\n\n    @property\n    def ref_names(self) -> Tuple[str, ...]:\n        """"""Get the names of registered node references (including unset).""""""\n        return tuple(self._refs.keys())\n\n    @classmethod\n    @contextlib.contextmanager\n    def define_operators(cls, operators: Dict[str, Callable]):\n        """"""Bind arbitrary binary functions to Python operators, for use in any\n        `Model` instance. Can (and should) be used as a contextmanager.\n\n        EXAMPLE:\n            with Model.define_operators({"">>"": chain}):\n                model = Relu(512) >> Relu(512) >> Softmax()\n        """"""\n        token = cls._context_operators.set(dict(operators))\n        yield\n        cls._context_operators.reset(token)\n\n    def has_dim(self, name: str) -> Optional[bool]:\n        """"""Check whether the model has a dimension of a given name. If the\n        dimension is registered but the value is unset, returns None.\n        """"""\n        if name not in self._dims:\n            return False\n        elif self._dims[name] is not None:\n            return True\n        else:\n            return None\n\n    def get_dim(self, name: str) -> int:\n        """"""Retrieve the value of a dimension of the given name.""""""\n        if name not in self._dims:\n            raise KeyError(f""Cannot get dimension \'{name}\' for model \'{self.name}\'"")\n        value = self._dims[name]\n        if value is None:\n            err = f""Cannot get dimension \'{name}\' for model \'{self.name}\': value unset""\n            raise ValueError(err)\n        else:\n            return value\n\n    def set_dim(self, name: str, value: int) -> None:\n        """"""Set a value for a dimension.""""""\n        if name not in self._dims:\n            raise KeyError(\n                f""Cannot set unknown dimension \'{name}\' for model \'{self.name}\'.""\n            )\n        old_value = self._dims[name]\n        if old_value is not None and old_value != value:\n            err = f""Attempt to change dimension \'{name}\' for model \'{self.name}\' from {old_value} to {value}""\n            raise ValueError(err)\n        self._dims[name] = value\n\n    def has_param(self, name: str) -> Optional[bool]:\n        """"""Check whether the model has a weights parameter of the given name.\n\n        Returns None if the parameter is registered but currently unset.\n        """"""\n        if name not in self._has_params:\n            return False\n        elif self._has_params[name] is not None:\n            return True\n        else:\n            return None\n\n    def get_param(self, name: str) -> FloatsXd:\n        """"""Retrieve a weights parameter by name.""""""\n        if name not in self._has_params:\n            raise KeyError(f""Unknown param: \'{name}\' for model \'{self.name}\'."")\n        if not self._params.has_param(self.id, name):\n            raise KeyError(\n                f""Parameter \'{name}\' for model \'{self.name}\' has not been allocated yet.""\n            )\n        return self._params.get_param(self.id, name)\n\n    def set_param(self, name: str, value: Optional[FloatsXd]) -> None:\n        """"""Set a weights parameter\'s value.""""""\n        if value is None:\n            self._has_params[name] = None\n        else:\n            self._params.set_param(self.id, name, value)\n            self._has_params[name] = True\n\n    def has_grad(self, name: str) -> bool:\n        """"""Check whether the model has a non-zero gradient for a parameter.\n        """"""\n        return self._params.has_grad(self.id, name)\n\n    def get_grad(self, name: str) -> FloatsXd:\n        """"""Get a gradient from the model.""""""\n        return self._params.get_grad(self.id, name)\n\n    def set_grad(self, name: str, value: FloatsXd) -> None:\n        """"""Set a gradient value for the model.""""""\n        self._params.set_grad(self.id, name, value)\n\n    def inc_grad(self, name: str, value: FloatsXd) -> None:\n        """"""Increment the gradient of a parameter by a value.""""""\n        self._params.inc_grad(self.id, name, value)\n\n    def has_ref(self, name: str) -> Optional[bool]:\n        """"""Check whether the model has a reference of a given name. If the\n        reference is registered but the value is unset, returns None.\n        """"""\n        if name not in self._refs:\n            return False\n        elif self._refs[name] is not None:\n            return True\n        else:\n            return None\n\n    def get_ref(self, name: str) -> ""Model"":\n        """"""Retrieve the value of a reference of the given name.""""""\n        if name not in self._refs:\n            raise KeyError(f""Cannot get reference \'{name}\' for model \'{self.name}\'."")\n        value = self._refs[name]\n        if value is None:\n            err = f""Cannot get reference \'{name}\' for model \'{self.name}\': value unset.""\n            raise ValueError(err)\n        else:\n            return value\n\n    def set_ref(self, name: str, value: Optional[""Model""]) -> None:\n        """"""Set a value for a reference.""""""\n        if value is None:\n            self._refs[name] = value\n        elif value in self.walk():\n            self._refs[name] = value\n        else:\n            raise ValueError(""Cannot add reference to node not in tree."")\n\n    def __call__(self, X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n        """"""Call the model\'s `forward` function, returning the output and a\n        callback to compute the gradients via backpropagation.""""""\n        return self._func(self, X, is_train=is_train)\n\n    def initialize(self, X: Optional[InT] = None, Y: Optional[OutT] = None) -> ""Model"":\n        """"""Finish initialization of the model, optionally providing a batch of\n        example input and output data to perform shape inference.""""""\n        if DATA_VALIDATION.get():\n            validate_fwd_input_output(self.name, self._func, X, Y)\n        if self.init is not None:\n            self.init(self, X=X, Y=Y)\n        return self\n\n    def begin_update(self, X: InT) -> Tuple[OutT, Callable[[InT], OutT]]:\n        """"""Run the model over a batch of data, returning the output and a\n        callback to complete the backward pass. A tuple (Y, finish_update),\n        where Y is a batch of output data, and finish_update is a callback that\n        takes the gradient with respect to the output and an optimizer function,\n        and returns the gradient with respect to the input.\n        """"""\n        return self._func(self, X, is_train=True)\n\n    def predict(self, X: InT) -> OutT:\n        """"""Call the model\'s `forward` function with `is_train=False`, and return\n        only the output, instead of the `(output, callback)` tuple.\n        """"""\n        return self._func(self, X, is_train=False)[0]\n\n    def finish_update(self, optimizer: Optimizer) -> None:\n        """"""Update parameters with current gradients. The optimizer is called\n        with each parameter and gradient of the model.\n        """"""\n        params = []\n        grads = []\n        shapes = []\n        for node in self.walk():\n            for shim in node.shims:\n                shim.finish_update(optimizer)\n        for node in self.walk():\n            for name in node.param_names:\n                param = node.get_param(name)\n                if node.has_grad(name):\n                    grad = node.get_grad(name)\n                else:\n                    grad = node.ops.xp.zeros_like(param)\n\n                params.append(self.ops.asarray(param.ravel()))\n                grads.append(self.ops.asarray(grad.ravel()))\n                shapes.append((param.size, param.shape))\n        if not params:\n            return\n        flat_params, flat_grads = optimizer(\n            (self.id, self.name),\n            self.ops.xp.concatenate(params),\n            self.ops.xp.concatenate(grads),\n        )\n        params = []\n        grads = []\n        start = 0\n        for node in self.walk():\n            for name in node.param_names:\n                size, shape = shapes.pop(0)\n                param = flat_params[start : start + size]  # type: ignore\n                grad = flat_grads[start : start + size]  # type: ignore\n                param = node.ops.asarray(param.reshape(shape))  # type: ignore\n                grad = node.ops.asarray(grad.reshape(shape))  # type: ignore\n                node.set_param(name, param)\n                node.set_grad(name, grad)\n                start += size\n\n    @contextlib.contextmanager\n    def use_params(self, params: Dict[Tuple[int, str], FloatsXd]):\n        """"""Context manager to temporarily set the model\'s parameters to\n        specified values. The params are a dictionary keyed by model IDs, whose\n        values are arrays of weight values.\n        """"""\n        backup = {}\n        for name in self.param_names:\n            key = (self.id, name)\n            if key in params:\n                backup[name] = self.get_param(name)\n                self.set_param(name, params[key])\n\n        with contextlib.ExitStack() as stack:\n            for layer in self.layers:\n                stack.enter_context(layer.use_params(params))\n            for shim in self.shims:\n                stack.enter_context(shim.use_params(params))\n            yield\n        if backup:\n            for name, param in backup.items():\n                self.set_param(name, param)\n\n    def walk(self) -> Iterable[""Model""]:\n        """"""Iterate out layers of the model, breadth-first.""""""\n        queue = [self]\n        seen: Set[int] = set()\n        for node in queue:\n            if id(node) in seen:\n                continue\n            seen.add(id(node))\n            yield node\n            queue.extend(node.layers)\n\n    def remove_node(self, node: ""Model"") -> None:\n        """"""Remove a node from all layers lists, and then update references.\n        References that no longer point to a node within the tree will be set\n        to `None`. For instance, let\'s say a node has its grandchild as a reference.\n        If the child is removed, the grandchild reference will be left dangling,\n        so will be set to None.\n        """"""\n        for child in list(self.walk()):\n            while node in child.layers:\n                child.layers.remove(node)\n        tree = set(self.walk())\n        for node in tree:\n            for name in node.ref_names:\n                ref = node.get_ref(name)\n                if ref is not None and ref not in tree:\n                    node.set_ref(name, None)\n\n    def get_gradients(self) -> Dict[Tuple[int, str], Tuple[FloatsXd, FloatsXd]]:\n        """"""Get non-zero gradients of the model\'s parameters, as a dictionary\n        keyed by the parameter ID. The values are (weights, gradients) tuples.\n        """"""\n        gradients = {}\n        for node in self.walk():\n            for name in node.grad_names:\n                param = node.get_param(name)\n                grad = node.get_grad(name)\n                gradients[(node.id, name)] = (param, grad)\n        return gradients\n\n    def copy(self: SelfT) -> SelfT:\n        """"""\n        Create a copy of the model, its attributes, and its parameters. Any child\n        layers will also be deep-copied. The copy will receive a distinct `model.id`\n        value.\n        """"""\n        params = {}\n        grads = {}\n        for name in self.param_names:\n            params[name] = self.get_param(name) if self.has_param(name) else None\n        for name in self.grad_names:\n            grads[name] = self.get_grad(name)\n\n        copied: Model[InT, OutT] = Model(\n            self.name,\n            self._func,\n            init=self.init,\n            params=copy.deepcopy(params),\n            dims=copy.deepcopy(self._dims),\n            attrs=copy.deepcopy(self._attrs),\n            layers=[layer.copy() for layer in self.layers],\n            shims=[shim.copy() for shim in self.shims],\n        )\n        for name in self.grad_names:\n            copied.set_grad(name, self.get_grad(name).copy())\n        return cast(SelfT, copied)\n\n    def to_gpu(self, gpu_id: int) -> None:  # pragma: no cover\n        """"""Transfer the model to a given GPU device.""""""\n        import cupy.cuda.device\n\n        with cupy.cuda.device.Device(gpu_id):\n            self._to_ops(CupyOps())\n\n    def to_cpu(self) -> None:  # pragma: no cover\n        """"""Transfer the model to CPU.""""""\n        self._to_ops(NumpyOps())\n\n    def _to_ops(self, ops: Ops) -> None:  # pragma: no cover\n        """"""Common method for to_cpu/to_gpu.""""""\n        for node in self.walk():\n            node.ops = ops\n            for name in node.param_names:\n                if node.has_param(name):\n                    node.set_param(name, ops.asarray_f(node.get_param(name)))\n                if node.has_grad(name):\n                    node.set_grad(name, ops.asarray_f(node.get_grad(name)))\n            for shim in node.shims:\n                shim.to_device(ops.device_type, ops.device_id)\n\n    def to_bytes(self) -> bytes:\n        """"""Serialize the model to a bytes representation. Models are usually\n        serialized using msgpack, so you should be able to call msgpack.loads()\n        on the data and get back a dictionary with the contents.\n\n        Serialization should round-trip identically, i.e. the same bytes should\n        result from loading and serializing a model.\n        """"""\n        msg = self.to_dict()\n        msg = convert_recursive(is_xp_array, self.ops.to_numpy, msg)\n        return srsly.msgpack_dumps(msg)\n\n    def to_disk(self, path: Union[Path, str]) -> None:\n        """"""Serialize the model to disk. Most models will serialize to a single\n        file, which should just be the bytes contents of model.to_bytes().\n        """"""\n        path = Path(path)\n        with path.open(""wb"") as file_:\n            file_.write(self.to_bytes())\n\n    def to_dict(self) -> Dict:\n        """"""Serialize the model to a dict representation.\n\n        Serialization should round-trip identically, i.e. the same dict should\n        result from loading and serializing a model.\n        """"""\n        # We separate out like this to make it easier to read the data in chunks.\n        # The shims might have large weights, while the nodes data will be\n        # small. The attrs are probably not very large, but could be.\n        # The lists are aligned, and refer to the order of self.walk().\n        msg: Dict[str, List] = {""nodes"": [], ""attrs"": [], ""params"": [], ""shims"": []}\n        nodes = list(self.walk())\n        # Serialize references by their index into the flattened tree.\n        # This is the main reason we can\'t accept out-of-tree references:\n        # we\'d have no way to serialize/deserialize them.\n        node_to_i: Dict[int, Optional[int]]\n        node_to_i = {node.id: i for i, node in enumerate(nodes)}\n        for i, node in enumerate(nodes):\n            refs: Dict[str, Optional[int]] = {}\n            invalid_refs: List[str] = []\n            for name in node.ref_names:\n                if not node.has_ref(name):\n                    refs[name] = None\n                else:\n                    ref = node.get_ref(name)\n                    if ref.id in node_to_i:\n                        refs[name] = node_to_i[ref.id]\n                    else:\n                        invalid_refs.append(name)\n            if invalid_refs:\n                raise ValueError(f""Cannot get references: {invalid_refs}"")\n            dims = {}\n            for dim in node.dim_names:\n                dims[dim] = node.get_dim(dim) if node.has_dim(dim) else None\n            msg[""nodes""].append(\n                {""index"": i, ""name"": node.name, ""dims"": dims, ""refs"": refs}\n            )\n        for node in nodes:\n            attrs = {}\n            for name, value in node.attrs.items():\n                try:\n                    attrs[name] = serialize_attr(value, value, name, node)\n                except TypeError:\n                    continue\n            msg[""attrs""].append(attrs)\n        for node in nodes:\n            msg[""shims""].append([shim.to_bytes() for shim in node.shims])\n        for node in nodes:\n            params: Dict[str, Optional[FloatsXd]] = {}\n            for name in node.param_names:\n                if node.has_param(name):\n                    params[name] = cast(Optional[FloatsXd], node.get_param(name))\n                else:\n                    params[name] = None\n            msg[""params""].append(params)\n        return msg\n\n    def from_bytes(self, bytes_data: bytes) -> ""Model"":\n        """"""Deserialize the model from a bytes representation. Models are usually\n        serialized using msgpack, so you should be able to call msgpack.loads()\n        on the data and get back a dictionary with the contents.\n\n        Serialization should round-trip identically, i.e. the same bytes should\n        result from loading and serializing a model.\n        """"""\n        msg = srsly.msgpack_loads(bytes_data)\n        msg = convert_recursive(is_xp_array, self.ops.asarray, msg)\n        return self.from_dict(msg)\n\n    def from_disk(self, path: Union[Path, str]) -> ""Model"":\n        """"""Deserialize the model from disk. Most models will serialize to a single\n        file, which should just be the bytes contents of model.to_bytes().\n        """"""\n        path = Path(path)\n        with path.open(""rb"") as file_:\n            bytes_data = file_.read()\n        return self.from_bytes(bytes_data)\n\n    def from_dict(self, msg: Dict) -> ""Model"":\n        if ""nodes"" not in msg.keys():  # pragma: no cover\n            err = ""Trying to read a Model that was created with an incompatible version of Thinc""\n            raise ValueError(err)\n        nodes = list(self.walk())\n        if len(msg[""nodes""]) != len(nodes):\n            raise ValueError(""Cannot deserialize model: mismatched structure"")\n        for i, node in enumerate(nodes):\n            info = msg[""nodes""][i]\n            node.name = info[""name""]\n            for dim, value in info[""dims""].items():\n                if value is not None:\n                    node.set_dim(dim, value)\n            for ref, ref_index in info[""refs""].items():\n                if ref_index is None:\n                    node.set_ref(ref, None)\n                else:\n                    node.set_ref(ref, nodes[ref_index])\n            for attr, value in msg[""attrs""][i].items():\n                default_value = node.attrs.get(attr)\n                loaded_value = deserialize_attr(default_value, value, attr, node)\n                node.attrs[attr] = loaded_value\n            for param_name, value in msg[""params""][i].items():\n                if value is not None:\n                    value = node.ops.asarray(value)\n                node.set_param(param_name, value)\n            for i, shim_bytes in enumerate(msg[""shims""][i]):\n                node.shims[i].from_bytes(shim_bytes)\n        return self\n\n    def __add__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'+\' operator.""""""\n        if ""+"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: +"")\n        return self._context_operators.get()[""+""](self, other)\n\n    def __sub__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'-\' operator.""""""\n        if ""-"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: -"")\n        return self._context_operators.get()[""-""](self, other)\n\n    def __mul__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'*\' operator.""""""\n        if ""*"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: *"")\n        return self._context_operators.get()[""*""](self, other)\n\n    def __matmul__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'@\' operator.""""""\n        if ""@"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: @"")\n        return self._context_operators.get()[""@""](self, other)\n\n    def __div__(self, other: Any) -> ""Model"":  # pragma: no cover\n        """"""Apply the function bound to the \'/\' operator.""""""\n        if ""/"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: /"")\n        return self._context_operators.get()[""/""](self, other)\n\n    def __truediv__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'/\' operator.""""""\n        if ""/"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: /"")\n        return self._context_operators.get()[""/""](self, other)\n\n    def __floordiv__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'//\' operator.""""""\n        if ""//"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: //"")\n        return self._context_operators.get()[""//""](self, other)\n\n    def __mod__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'%\' operator.""""""\n        if ""%"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: %"")\n        return self._context_operators.get()[""%""](self, other)\n\n    def __pow__(self, other: Any, **kwargs) -> ""Model"":\n        """"""Apply the function bound to the \'**\' operator.""""""\n        if ""**"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: **"")\n        return self._context_operators.get()[""**""](self, other)\n\n    def __lshift__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'<<\' operator.""""""\n        if ""<<"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: <<"")\n        return self._context_operators.get()[""<<""](self, other)\n\n    def __rshift__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'>>\' operator.""""""\n        if "">>"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: >>"")\n        return self._context_operators.get()["">>""](self, other)\n\n    def __and__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'&\' operator.""""""\n        if ""&"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: &"")\n        return self._context_operators.get()[""&""](self, other)\n\n    def __xor__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'^\' operator.""""""\n        if ""^"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: ^"")\n        return self._context_operators.get()[""^""](self, other)\n\n    def __or__(self, other: Any) -> ""Model"":\n        """"""Apply the function bound to the \'|\' operator.""""""\n        if ""|"" not in self._context_operators.get():\n            raise TypeError(""Undefined operator: |"")\n        return self._context_operators.get()[""|""](self, other)\n\n\n@functools.singledispatch\ndef serialize_attr(_: Any, value: Any, name: str, model: Model) -> bytes:\n    """"""Serialize an attribute value (defaults to msgpack). You can register\n    custom serializers using the @serialize_attr.register decorator with the\n    type to serialize, e.g.: @serialize_attr.register(MyCustomObject).\n    """"""\n    return srsly.msgpack_dumps(value)\n\n\n@functools.singledispatch\ndef deserialize_attr(_: Any, value: Any, name: str, model: Model) -> Any:\n    """"""Deserialize an attribute value (defaults to msgpack). You can register\n    custom deserializers using the @deserialize_attr.register decorator with the\n    type to deserialize, e.g.: @deserialize_attr.register(MyCustomObject).\n    """"""\n    return srsly.msgpack_loads(value)\n\n\ndef _jax_flatten_model(model):  # pragma: ignore\n    """"""A Jax flattener for Thinc models. Registering this (and the paired\n    unflatten function) allows Thinc models to be passed into Jax JIT-ed functions.\n\n    The model must have an attr ""registered_constructor"" that can rebuild the\n    model object via the layers registry, with no arguments. You should use a\n    constructor that doesn\'t allocate any parameters and works reasonably quickly.\n    """"""\n    registry_name = model.attrs[""registry_name""]\n    msg = model.to_dict()\n    params = msg.pop(""params"")\n    param_values = []\n    param_keys = []\n    for i, param_info in enumerate(params):\n        for name, value in param_info.items():\n            param_values.append(value)\n            param_keys.append((i, name))\n    # param_values needs to be a flat list of leaf types, e.g. arrays. Notably,\n    # strings are not leaves!\n    # The aux data can be anything, but I think it shouldn\'t be variables?\n    return param_values, (registry_name, param_keys, msg)\n\n\ndef _jax_unflatten_model(info, param_values):  # pragma: ignore\n    """"""The Jax unflattener, paired with jax_flatten_model""""""\n    # This is pretty ugly. But I don\'t know where I can put this function\n    # that has access to the registry object without causing import circles?\n    from .config import registry\n\n    registry_name, param_keys, msg = info\n    model = registry.layers.get(registry_name)()\n    msg[""params""] = [{} for _ in range(len(msg[""nodes""]))]\n    for (i, name), value in zip(param_keys, param_values):\n        msg[""params""][i][name] = value\n    return model.from_dict(msg)\n\n\ntry:  # pragma: no cover\n    import jax.tree_util\n\n    jax.tree_util.register_pytree_node(Model, _jax_flatten_model, _jax_unflatten_model)\nexcept ImportError:  # pragma: no cover\n    pass\n\n\n_ModelT = TypeVar(""_ModelT"", bound=Model)\n\n\ndef change_attr_values(model: _ModelT, mapping: Dict[str, Dict[str, Any]]) -> _ModelT:\n    """"""Walk over the model\'s nodes, changing the value of attributes using the\n    provided mapping, which maps node names to attr names to attr values.\n    """"""\n    for node in model.walk():\n        if node.name in mapping:\n            attrs = mapping[node.name]\n            for attr, value in attrs.items():\n                if attr in node.attrs:\n                    node.attrs[attr] = value\n    return model\n\n\ndef set_dropout_rate(model: _ModelT, drop: float, attrs=[""dropout_rate""]) -> _ModelT:\n    """"""Walk over the model\'s nodes, setting the dropout rate. You can specify\n    one or more attribute names, by default it looks for [""dropout_rate""].\n    """"""\n    for node in model.walk():\n        for attr in attrs:\n            if attr in node.attrs:\n                node.attrs[attr] = drop\n    return model\n\n\n__all__ = [\n    ""Model"",\n    ""serialize_attr"",\n    ""deserialize_attr"",\n    ""change_attr_values"",\n    ""set_dropout_rate"",\n]\n'"
thinc/mypy.py,0,"b'from typing import Dict, List\nimport itertools\nfrom mypy.errors import Errors\nfrom mypy.errorcodes import ErrorCode\nfrom mypy.options import Options\nfrom mypy.plugin import FunctionContext, Plugin, CheckerPluginInterface\nfrom mypy.types import Instance, Type, CallableType, TypeVarType\nfrom mypy.nodes import Expression, CallExpr, NameExpr, FuncDef, Decorator, MypyFile\nfrom mypy.checker import TypeChecker\nfrom mypy.subtypes import is_subtype\n\nthinc_model_fullname = ""thinc.model.Model""\nchained_out_fullname = ""thinc.types.XY_YZ_OutT""\nintoin_outtoout_out_fullname = ""thinc.types.XY_XY_OutT""\n\n\ndef plugin(version: str):\n    return ThincPlugin\n\n\nclass ThincPlugin(Plugin):\n    def __init__(self, options: Options) -> None:\n        super().__init__(options)\n\n    def get_function_hook(self, fullname: str):\n        return function_hook\n\n\ndef function_hook(ctx: FunctionContext) -> Type:\n    try:\n        return get_reducers_type(ctx)\n    except AssertionError:\n        # Add more function callbacks here\n        return ctx.default_return_type\n\n\ndef get_reducers_type(ctx: FunctionContext) -> Type:\n    assert isinstance(ctx.context, CallExpr)\n    assert isinstance(ctx.api, TypeChecker)\n    assert isinstance(ctx.default_return_type, Instance)\n    assert isinstance(ctx.context.callee, NameExpr)\n    assert isinstance(ctx.context.callee.node, (FuncDef, Decorator))\n    assert isinstance(ctx.context.callee.node.type, CallableType)\n    assert isinstance(ctx.context.callee.node.type.ret_type, Instance)\n    assert ctx.context.callee.node.type.ret_type.args\n    assert len(ctx.context.callee.node.type.ret_type.args) == 2\n    out_type = ctx.context.callee.node.type.ret_type.args[1]\n    assert isinstance(out_type, TypeVarType)\n    assert out_type.fullname\n    if out_type.fullname not in {intoin_outtoout_out_fullname, chained_out_fullname}:\n        return ctx.default_return_type\n    args = list(itertools.chain(*ctx.args))\n    arg_types = []\n    for arg_type in itertools.chain(*ctx.arg_types):\n        assert isinstance(arg_type, Instance)\n        arg_types.append(arg_type)\n    arg_pairs = list(zip(args[:-1], args[1:]))\n    arg_types_pairs = list(zip(arg_types[:-1], arg_types[1:]))\n    if out_type.fullname == chained_out_fullname:\n        for (arg1, arg2), (type1, type2) in zip(arg_pairs, arg_types_pairs):\n            assert isinstance(type1, Instance)\n            assert isinstance(type2, Instance)\n            assert type1.type.fullname == thinc_model_fullname\n            assert type2.type.fullname == thinc_model_fullname\n            check_chained(\n                l1_arg=arg1, l1_type=type1, l2_arg=arg2, l2_type=type2, api=ctx.api\n            )\n        return Instance(\n            ctx.default_return_type.type, [arg_types[0].args[0], arg_types[-1].args[1]]\n        )\n    elif out_type.fullname == intoin_outtoout_out_fullname:\n        for (arg1, arg2), (type1, type2) in zip(arg_pairs, arg_types_pairs):\n            assert isinstance(type1, Instance)\n            assert isinstance(type2, Instance)\n            assert type1.type.fullname == thinc_model_fullname\n            assert type2.type.fullname == thinc_model_fullname\n            check_intoin_outtoout(\n                l1_arg=arg1, l1_type=type1, l2_arg=arg2, l2_type=type2, api=ctx.api\n            )\n        return Instance(\n            ctx.default_return_type.type, [arg_types[0].args[0], arg_types[0].args[1]]\n        )\n    assert False, ""Thinc mypy plugin error: it should return before this point""\n\n\ndef check_chained(\n    *,\n    l1_arg: Expression,\n    l1_type: Instance,\n    l2_arg: Expression,\n    l2_type: Instance,\n    api: CheckerPluginInterface,\n):\n    if not is_subtype(l1_type.args[1], l2_type.args[0]):\n        api.fail(\n            f""Layer outputs type ({l1_type.args[1]}) but the next layer expects ({l2_type.args[0]}) as an input"",\n            l1_arg,\n            code=error_layer_output,\n        )\n        api.fail(\n            f""Layer input type ({l2_type.args[0]}) is not compatible with output ({l1_type.args[1]}) from previous layer"",\n            l2_arg,\n            code=error_layer_input,\n        )\n\n\ndef check_intoin_outtoout(\n    *,\n    l1_arg: Expression,\n    l1_type: Instance,\n    l2_arg: Expression,\n    l2_type: Instance,\n    api: CheckerPluginInterface,\n):\n    if l1_type.args[0] != l2_type.args[0]:\n        api.fail(\n            f""Layer input ({l1_type.args[0]}) not compatible with next layer input ({l2_type.args[0]})"",\n            l1_arg,\n            code=error_layer_input,\n        )\n        api.fail(\n            f""Layer input ({l2_type.args[0]}) not compatible with previous layer input ({l1_type.args[0]})"",\n            l2_arg,\n            code=error_layer_input,\n        )\n    if l1_type.args[1] != l2_type.args[1]:\n        api.fail(\n            f""Layer output ({l1_type.args[1]}) not compatible with next layer output ({l2_type.args[1]})"",\n            l1_arg,\n            code=error_layer_output,\n        )\n        api.fail(\n            f""Layer output ({l2_type.args[1]}) not compatible with previous layer output ({l1_type.args[1]})"",\n            l2_arg,\n            code=error_layer_output,\n        )\n\n\nerror_layer_input = ErrorCode(""layer-mismatch-input"", ""Invalid layer input"", ""Thinc"")\nerror_layer_output = ErrorCode(""layer-mismatch-output"", ""Invalid layer output"", ""Thinc"")\n\n\nclass IntrospectChecker(TypeChecker):\n    def __init__(\n        self,\n        errors: Errors,\n        modules: Dict[str, MypyFile],\n        options: Options,\n        tree: MypyFile,\n        path: str,\n        plugin: Plugin,\n    ):\n        self._error_messages: List[str] = []\n        super().__init__(errors, modules, options, tree, path, plugin)\n'"
thinc/optimizers.py,0,"b'import math\n\nfrom typing import Dict, Optional, Union, Tuple, List, cast\nfrom collections import defaultdict\n\nfrom .backends import get_array_ops\nfrom .types import Generator, FloatsXd\nfrom .config import registry\n\n\nKeyT = Tuple[int, str]\nFloatOrSeq = Union[float, List[float], Generator]\nIntOrSeq = Union[int, List[int], Generator]\n\nSGD_DEFAULTS: Dict[str, Union[float, bool, int]] = {\n    ""L2"": 0.0,\n    ""L2_is_weight_decay"": True,\n    ""grad_clip"": 1.0,\n}\n\n\nADAM_DEFAULTS: Dict[str, Union[float, bool, int]] = {\n    ""learn_rate"": 0.001,\n    ""beta1"": 0.9,\n    ""beta2"": 0.999,\n    ""eps"": 1e-08,\n    ""L2"": SGD_DEFAULTS[""L2""],\n    ""grad_clip"": SGD_DEFAULTS[""grad_clip""],\n    ""L2_is_weight_decay"": True,\n}\n\n\n@registry.optimizers(""RAdam.v1"")\ndef RAdam(\n    learn_rate: FloatOrSeq = ADAM_DEFAULTS[""learn_rate""],\n    *,\n    beta1: FloatOrSeq = ADAM_DEFAULTS[""beta1""],\n    beta2: FloatOrSeq = ADAM_DEFAULTS[""beta2""],\n    eps: FloatOrSeq = ADAM_DEFAULTS[""eps""],\n    L2: FloatOrSeq = ADAM_DEFAULTS[""L2""],\n    L2_is_weight_decay: bool = cast(bool, ADAM_DEFAULTS[""L2_is_weight_decay""]),\n    grad_clip: FloatOrSeq = ADAM_DEFAULTS[""grad_clip""],\n    use_averages: bool = True,\n):\n    return Optimizer(\n        learn_rate,\n        beta1=beta1,\n        beta2=beta2,\n        eps=eps,\n        grad_clip=grad_clip,\n        L2_is_weight_decay=L2_is_weight_decay,\n        L2=L2,\n        use_averages=use_averages,\n        use_radam=True,\n    )\n\n\n@registry.optimizers(""Adam.v1"")\ndef Adam(\n    learn_rate: FloatOrSeq = ADAM_DEFAULTS[""learn_rate""],\n    *,\n    L2: FloatOrSeq = ADAM_DEFAULTS[""L2""],\n    beta1: FloatOrSeq = ADAM_DEFAULTS[""beta1""],\n    beta2: FloatOrSeq = ADAM_DEFAULTS[""beta2""],\n    eps: FloatOrSeq = ADAM_DEFAULTS[""eps""],\n    grad_clip: FloatOrSeq = ADAM_DEFAULTS[""grad_clip""],\n    L2_is_weight_decay: bool = cast(bool, ADAM_DEFAULTS[""L2_is_weight_decay""]),\n    use_averages: bool = True,\n):\n    return Optimizer(\n        learn_rate,\n        L2=L2,\n        beta1=beta1,\n        beta2=beta2,\n        eps=eps,\n        grad_clip=grad_clip,\n        L2_is_weight_decay=L2_is_weight_decay,\n        use_averages=use_averages,\n        use_radam=False,\n    )\n\n\n@registry.optimizers(""SGD.v1"")\ndef SGD(\n    learn_rate: FloatOrSeq,\n    *,\n    L2: FloatOrSeq = SGD_DEFAULTS[""L2""],\n    grad_clip: FloatOrSeq = SGD_DEFAULTS[""grad_clip""],\n    L2_is_weight_decay: bool = cast(bool, SGD_DEFAULTS[""L2_is_weight_decay""]),\n    use_averages: bool = True,\n):\n    return Optimizer(\n        learn_rate,\n        L2=L2,\n        grad_clip=grad_clip,\n        L2_is_weight_decay=L2_is_weight_decay,\n        beta1=0.0,\n        beta2=0.0,\n        use_averages=use_averages,\n    )\n\n\nclass Optimizer(object):\n    """"""Do various flavours of stochastic gradient descent, with first and\n    second order momentum. Currently support \'vanilla\' SGD, Adam, and RAdam.\n    """"""\n\n    mom1: Dict[KeyT, FloatsXd]\n    mom2: Dict[KeyT, FloatsXd]\n    averages: Optional[Dict[KeyT, FloatsXd]]\n    schedules: Dict[str, Generator]\n    nr_update: Dict[KeyT, int]\n    last_seen: Dict[KeyT, int]\n    grad_clip: float\n    learn_rate: float\n    b1: float\n    b2: float\n    eps: float\n    L2: float\n    use_radam: bool\n    L2_is_weight_decay: bool\n    _radam_buffer: List[List[Optional[FloatsXd]]]\n\n    # This ""locks"" the class, so we get an error if you try to assign to\n    # an unexpected variable.\n    __slots__ = [\n        ""mom1"",\n        ""mom2"",\n        ""averages"",\n        ""schedules"",\n        ""nr_update"",\n        ""last_seen"",\n        ""grad_clip"",\n        ""learn_rate"",\n        ""b1"",\n        ""b2"",\n        ""eps"",\n        ""L2"",\n        ""use_radam"",\n        ""L2_is_weight_decay"",\n        ""_radam_buffer"",\n    ]\n\n    def __init__(\n        self,\n        learn_rate: FloatOrSeq,\n        *,\n        L2: FloatOrSeq = ADAM_DEFAULTS[""L2""],\n        beta1: FloatOrSeq = ADAM_DEFAULTS[""beta1""],\n        beta2: FloatOrSeq = ADAM_DEFAULTS[""beta2""],\n        eps: FloatOrSeq = ADAM_DEFAULTS[""eps""],\n        grad_clip: FloatOrSeq = ADAM_DEFAULTS[""grad_clip""],\n        use_averages: bool = True,\n        use_radam: bool = False,\n        L2_is_weight_decay: bool = True,\n    ):\n        """"""\n        Initialize an optimizer.\n\n        learn_rate (float): The initial learning rate.\n        L2 (float): The L2 regularization term.\n        beta1 (float): First-order momentum.\n        beta2 (float): Second-order momentum.\n        eps (float): Epsilon term for Adam etc.\n        grad_clip (float): Gradient clipping.\n        use_averages (bool): Whether to track moving averages of the parameters.\n        use_radam (bool): Whether to use the RAdam optimizer.\n        L2_is_weight_decay (bool): Whether to interpret the L2 parameter as a\n            weight decay term, in the style of the AdamW optimizer.\n        """"""\n        self.mom1 = {}\n        self.mom2 = {}\n        if use_averages:\n            self.averages = {}\n        else:\n            self.averages = None\n        self.schedules = {}\n        self.nr_update = defaultdict(int)\n        self.last_seen = defaultdict(int)\n        self._set_attr_or_schedule(""grad_clip"", grad_clip)\n        self._set_attr_or_schedule(""learn_rate"", learn_rate)\n        self._set_attr_or_schedule(""b1"", beta1)\n        self._set_attr_or_schedule(""b2"", beta2)\n        self._set_attr_or_schedule(""eps"", eps)\n        self._set_attr_or_schedule(""L2"", L2)\n        self.use_radam = use_radam\n        self.L2_is_weight_decay = L2_is_weight_decay\n        self._radam_buffer = [[None, None, None] for _ in range(10)]\n\n    def _set_attr_or_schedule(self, name, value):\n        if isinstance(value, (float, bool, int)):\n            setattr(self, name, value)\n        else:\n            if isinstance(value, list):\n                value = iter(value)\n            self.schedules[name] = value\n            try:\n                setattr(self, name, next(value))\n            except (StopIteration, TypeError) as e:\n                err = f""Invalid schedule for \'{name}\' ({type(value)})\\n{e}""\n                raise ValueError(err)\n\n    def step_schedules(self):\n        for key, schedule in self.schedules.items():\n            try:\n                value = next(schedule)\n            except StopIteration:  # schedule exhausted, use last value\n                value = getattr(self, key)\n            setattr(self, key, value)\n\n    def __call__(\n        self,\n        key: Tuple[int, str],\n        weights: FloatsXd,\n        gradient: FloatsXd,\n        *,\n        lr_scale: float = 1.0,\n    ):\n        """"""Call the optimizer with weights and a gradient. The key is the\n        identifier for the parameter, usually the node ID and parameter name.\n        """"""\n        if len(gradient) < 1:\n            return weights, gradient\n        ops = get_array_ops(weights)\n        self.nr_update[key] += 1\n        nr_upd = self.nr_update[key]\n        if self.L2 != 0 and not self.L2_is_weight_decay:\n            gradient += self.L2 * weights\n        if self.grad_clip:\n            gradient = ops.clip_gradient(gradient, self.grad_clip)\n        if self.use_radam:\n            weights, gradient = self._radam(\n                ops, weights, gradient, lr_scale, key, nr_upd\n            )\n        elif self.b1 > 0.0 and self.b2 > 0.0:\n            weights, gradient = self._adam(\n                ops, weights, gradient, lr_scale, key, nr_upd\n            )\n        elif self.b2 > 0.0:  # pragma: no cover\n            raise NotImplementedError  # TODO: error message\n        else:\n            weights -= lr_scale * self.learn_rate * gradient\n        gradient = gradient * 0.0\n        if self.L2 != 0 and self.L2_is_weight_decay:\n            weights -= lr_scale * self.learn_rate * self.L2 * weights\n        if self.averages is not None:\n            if key not in self.averages:\n                self.averages[key] = ops.alloc(weights.shape, dtype=""float32"")\n            ops.update_averages(self.averages[key], weights, nr_upd)\n        return weights, gradient\n\n    def _radam(self, ops, weights, grad, lr_scale, key, nr_upd):\n        if key not in self.mom1:\n            self.mom1[key] = ops.alloc1f(weights.size)\n        if key not in self.mom2:\n            self.mom2[key] = ops.alloc1f(weights.size)\n\n        # While we port from the pytorch implementation, keep some of the same\n        # naming\n        state = {\n            ""step"": self.nr_update[key],\n            ""exp_avg"": self.mom1[key],\n            ""exp_avg_sq"": self.mom2[key],\n        }\n        group = {\n            ""lr"": self.learn_rate,\n            ""betas"": [self.b1, self.b2],\n            ""eps"": self.eps,\n            ""weight_decay"": 0.0,\n            ""buffer"": self._radam_buffer,\n        }\n        degenerated_to_sgd = True\n\n        exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n        beta1, beta2 = group[""betas""]\n\n        # exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n        exp_avg_sq *= beta2\n        exp_avg_sq += (1 - beta2) * (grad ** 2)\n        # exp_avg.mul_(beta1).add_(1 - beta1, grad)\n        exp_avg *= beta1\n        exp_avg += (1 - beta1) * grad\n\n        state[""step""] += 1\n        buffered = group[""buffer""][int(state[""step""] % 10)]\n        if state[""step""] == buffered[0]:\n            N_sma, step_size = buffered[1], buffered[2]\n        else:\n            buffered[0] = state[""step""]\n            beta2_t = beta2 ** state[""step""]\n            N_sma_max = 2 / (1 - beta2) - 1\n            N_sma = N_sma_max - 2 * state[""step""] * beta2_t / (1 - beta2_t)\n            buffered[1] = N_sma\n\n            # more conservative since it\'s an approximated value\n            if N_sma >= 5:\n                step_size = math.sqrt(\n                    (1 - beta2_t)\n                    * (N_sma - 4)\n                    / (N_sma_max - 4)\n                    * (N_sma - 2)\n                    / N_sma\n                    * N_sma_max\n                    / (N_sma_max - 2)\n                ) / (1 - beta1 ** state[""step""])\n            elif degenerated_to_sgd:\n                step_size = 1.0 / (1 - beta1 ** state[""step""])\n            else:\n                step_size = -1\n            buffered[2] = step_size\n\n        # more conservative since it\'s an approximated value\n        if N_sma >= 5:\n            if group[""weight_decay""] != 0:\n                weights += -group[""weight_decay""] * group[""lr""] * weights\n            denom = ops.xp.sqrt(exp_avg_sq) + group[""eps""]\n            weights += -step_size * group[""lr""] * (exp_avg / denom)\n        elif step_size > 0:\n            if group[""weight_decay""] != 0:\n                weights += -group[""weight_decay""] * group[""lr""] * weights\n            weights += -step_size * group[""lr""] * exp_avg\n        return weights, grad\n\n    def _adam(self, ops, weights, gradient, lr_scale, key, nr_upd):\n        weights_1D = ops.reshape1f(weights, weights.size)\n        gradient_1D = ops.reshape1f(gradient, gradient.size)\n        if key not in self.mom1:\n            self.mom1[key] = ops.alloc1f(weights.size)\n        if key not in self.mom2:\n            self.mom2[key] = ops.alloc1f(weights.size)\n        mom1 = self.mom1[key]\n        mom2 = self.mom2[key]\n        b1 = self.b1\n        b2 = self.b2\n        fix1 = 1.0 - (b1 ** nr_upd)\n        fix2 = 1.0 - (b2 ** nr_upd)\n        lr = self.learn_rate * fix2 ** 0.5 / fix1\n        eps = self.eps\n        # needs to be 1D going into the adam function\n        weights_1D, gradient_1D, mom1, mom2 = ops.adam(\n            weights_1D, gradient_1D, mom1, mom2, b1, b2, eps, lr * lr_scale\n        )\n        self.mom1[key] = mom1\n        self.mom2[key] = mom2\n        return (\n            ops.reshape_f(weights_1D, weights.shape),\n            ops.reshape_f(gradient_1D, gradient.shape),\n        )\n\n\n__all__ = [""Adam"", ""RAdam"", ""SGD"", ""Optimizer"", ""ADAM_DEFAULTS"", ""SGD_DEFAULTS""]\n'"
thinc/schedules.py,0,"b'""""""Generators that provide different rates, schedules, decays or series.""""""\nfrom typing import Iterable\nimport numpy\n\nfrom .config import registry\n\n\n@registry.schedules(""constant_then.v1"")\ndef constant_then(\n    rate: float, steps: int, schedule: Iterable[float]\n) -> Iterable[float]:\n    """"""Yield a constant rate for N steps, before starting a schedule.""""""\n    for i in range(steps):\n        yield rate\n    for value in schedule:\n        yield value\n\n\n@registry.schedules(""constant.v1"")\ndef constant(rate: float) -> Iterable[float]:\n    """"""Yield a constant rate.""""""\n    while True:\n        yield rate\n\n\n@registry.schedules(""decaying.v1"")\ndef decaying(base_rate: float, decay: float, *, t: int = 0) -> Iterable[float]:\n    """"""Yield an infinite series of linearly decaying values,\n    following the schedule: base_rate * 1 / (1 + decay * t)\n\n    EXAMPLE:\n        >>> learn_rates = decaying(0.001, 1e-4)\n        >>> next(learn_rates)\n        0.001\n        >>> next(learn_rates)\n        0.00999\n    """"""\n    while True:\n        yield base_rate * (1.0 / (1.0 + decay * t))\n        t += 1\n\n\n@registry.schedules(""compounding.v1"")\ndef compounding(\n    start: float, stop: float, compound: float, *, t: float = 0.0\n) -> Iterable[float]:\n    """"""Yield an infinite series of compounding values. Each time the\n    generator is called, a value is produced by multiplying the previous\n    value by the compound rate.\n\n    EXAMPLE:\n        >>> sizes = compounding(1.0, 10.0, 1.5)\n        >>> assert next(sizes) == 1.\n        >>> assert next(sizes) == 1 * 1.5\n        >>> assert next(sizes) == 1.5 * 1.5\n    """"""\n    curr = float(start)\n    while True:\n        yield _clip(curr, start, stop)\n        curr *= compound\n\n\ndef _clip(value: float, start: float, stop: float) -> float:\n    return max(value, stop) if (start > stop) else min(value, stop)\n\n\n@registry.schedules(""slanted_triangular.v1"")\ndef slanted_triangular(\n    max_rate: float,\n    num_steps: int,\n    *,\n    cut_frac: float = 0.1,\n    ratio: int = 32,\n    decay: float = 1.0,\n    t: float = 0.0,\n) -> Iterable[float]:\n    """"""Yield an infinite series of values according to Howard and Ruder\'s\n    ""slanted triangular learning rate"" schedule.\n    """"""\n    cut = int(num_steps * cut_frac)\n    while True:\n        t += 1\n        if t < cut:\n            p = t / cut\n        else:\n            p = 1 - ((t - cut) / (cut * (1 / cut_frac - 1)))\n        learn_rate = max_rate * (1 + p * (ratio - 1)) * (1 / ratio)\n        yield learn_rate\n\n\n@registry.schedules(""warmup_linear.v1"")\ndef warmup_linear(\n    initial_rate: float, warmup_steps: int, total_steps: int\n) -> Iterable[float]:\n    """"""Generate a series, starting from an initial rate, and then with a warmup\n    period, and then a linear decline. Used for learning rates.\n    """"""\n    step = 0\n    while True:\n        if step < warmup_steps:\n            factor = step / max(1, warmup_steps)\n        else:\n            factor = max(\n                0.0, (total_steps - step) / max(1.0, total_steps - warmup_steps)\n            )\n        yield factor * initial_rate\n        step += 1\n\n\n@registry.schedules(""cyclic_triangular.v1"")\ndef cyclic_triangular(min_lr: float, max_lr: float, period: int) -> Iterable[float]:\n    it = 1\n    while True:\n        # https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n        cycle = numpy.floor(1 + it / (2 * period))\n        x = numpy.abs(it / period - 2 * cycle + 1)\n        relative = max(0, 1 - x)\n        yield min_lr + (max_lr - min_lr) * relative\n        it += 1\n\n\n__all__ = [\n    ""cyclic_triangular"",\n    ""warmup_linear"",\n    ""constant"",\n    ""constant_then"",\n    ""decaying"",\n    ""warmup_linear"",\n    ""slanted_triangular"",\n    ""compounding"",\n]\n'"
thinc/types.py,0,"b'from typing import Union, Tuple, Sized, Container, Any, TypeVar, Callable\nfrom typing import Iterable, Iterator, Sequence, Dict, Generic, cast\nfrom typing import Optional, List, overload\nfrom dataclasses import dataclass\nimport numpy\nimport sys\n\ntry:\n    import cupy\n\n    get_array_module = cupy.get_array_module\nexcept ImportError:\n    get_array_module = lambda obj: numpy\n\n# Use typing_extensions for Python versions < 3.8\nif sys.version_info < (3, 8):\n    from typing_extensions import Protocol, Literal\nelse:\n    from typing import Protocol, Literal  # noqa: F401\n\n\n# fmt: off\nXY_YZ_OutT = TypeVar(""XY_YZ_OutT"")\nXY_XY_OutT = TypeVar(""XY_XY_OutT"")\n\nOpsNames = Literal[""numpy"", ""cupy"", ""jax""]\nDeviceTypes = Literal[""cpu"", ""gpu"", ""tpu""]\nBatchable = Union[""Pairs"", ""Ragged"", ""Padded"", ""ArrayXd"", List, Tuple]\nXp = Union[""numpy"", ""cupy""]  # type: ignore\nShape = Tuple[int, ...]\nDTypes = Literal[""f"", ""i"", ""float16"", ""float32"", ""float64"", ""int32"", ""int64"", ""uint32"", ""uint64""]\nDTypesFloat = Literal[""f"", ""float32"", ""float16"", ""float64""]\nDTypesInt = Literal[""i"", ""int32"", ""int64"", ""uint32"", ""uint64""]\n\nArray1d = Union[""Floats1d"", ""Ints1d""]\nArray2d = Union[""Floats2d"", ""Ints2d""]\nArray3d = Union[""Floats3d"", ""Ints3d""]\nArray4d = Union[""Floats4d"", ""Ints4d""]\nFloatsXd = Union[""Floats1d"", ""Floats2d"", ""Floats3d"", ""Floats4d""]\nIntsXd = Union[""Ints1d"", ""Ints2d"", ""Ints3d"", ""Ints4d""]\nArrayXd = Union[FloatsXd, IntsXd]\nList1d = Union[List[""Floats1d""], List[""Ints1d""]]\nList2d = Union[List[""Floats2d""], List[""Ints2d""]]\nList3d = Union[List[""Floats3d""], List[""Ints3d""]]\nList4d = Union[List[""Floats4d""], List[""Ints4d""]]\nListXd = Union[List[""FloatsXd""], List[""IntsXd""]]\n\nArrayT = TypeVar(""ArrayT"")\nSelfT = TypeVar(""SelfT"")\nArray1dT = TypeVar(""Array1dT"", bound=""Array1d"")\n\n# These all behave the same as far as indexing is concerned\nSlicish = Union[slice, List[int], ""ArrayXd""]\n_1_KeyScalar = int\n_1_Key1d = Slicish\n_1_AllKeys = Union[_1_KeyScalar, _1_Key1d]\n_F1_AllReturns = Union[float, ""Floats1d""]\n_I1_AllReturns = Union[int, ""Ints1d""]\n\n_2_KeyScalar = Tuple[int, int]\n_2_Key1d = Union[int, Tuple[Slicish, int], Tuple[int, Slicish]]\n_2_Key2d = Union[Tuple[Slicish, Slicish], Slicish]\n_2_AllKeys = Union[_2_KeyScalar, _2_Key1d, _2_Key2d]\n_F2_AllReturns = Union[float, ""Floats1d"", ""Floats2d""]\n_I2_AllReturns = Union[int, ""Ints1d"", ""Ints2d""]\n\n_3_KeyScalar = Tuple[int, int, int]\n_3_Key1d = Union[Tuple[int, int], Tuple[int, int, Slicish], Tuple[int, Slicish, int], Tuple[Slicish, int, int]]\n_3_Key2d = Union[int, Tuple[int, Slicish], Tuple[Slicish, int], Tuple[int, Slicish, Slicish], Tuple[Slicish, int, Slicish], Tuple[Slicish, Slicish, int]]\n_3_Key3d = Union[Slicish, Tuple[Slicish, Slicish], Tuple[Slicish, Slicish, Slicish]]\n_3_AllKeys = Union[_3_KeyScalar, _3_Key1d, _3_Key2d, _3_Key3d]\n_F3_AllReturns = Union[float, ""Floats1d"", ""Floats2d"", ""Floats3d""]\n_I3_AllReturns = Union[int, ""Ints1d"", ""Ints2d"", ""Ints3d""]\n\n# Typedefs for the reduction methods.\nTru = Literal[True]\nFal = Literal[False]\nOneAx = Union[int, Tuple[int]]\nTwoAx = Tuple[int, int]\nThreeAx = Tuple[int, int, int]\nFourAx = Tuple[int, int, int, int]\n_1_AllAx = Optional[OneAx]\n_2_AllAx = Union[Optional[TwoAx], OneAx]\n_3_AllAx = Union[Optional[ThreeAx], TwoAx, OneAx]\n_4_AllAx = Union[Optional[FourAx], ThreeAx, TwoAx, OneAx]\n_1F_ReduceResults = Union[float, ""Floats1d""]\n_2F_ReduceResults = Union[float, ""Floats1d"", ""Floats2d""]\n_3F_ReduceResults = Union[float, ""Floats1d"", ""Floats2d"", ""Floats3d""]\n_4F_ReduceResults = Union[float, ""Floats1d"", ""Floats2d"", ""Floats3d"", ""Floats4d""]\n_1I_ReduceResults = Union[int, ""Ints1d""]\n_2I_ReduceResults = Union[int, ""Ints1d"", ""Ints2d""]\n_3I_ReduceResults = Union[int, ""Ints1d"", ""Ints2d"", ""Ints3d""]\n_4I_ReduceResults = Union[int, ""Ints1d"", ""Ints2d"", ""Ints3d"", ""Ints4d""]\n\n# TODO:\n# We need to get correct overloads in for the following reduction methods.\n# The \'sum\' reduction is correct --- the others need to be just the same,\n# but with a different name.\n\n# max, min, prod, round, var, mean, ptp, std\n\n# There\'s also one *slightly* different function, cumsum. This doesn\'t\n# have a scalar version -- it always makes an array.\n\n\nclass _Array(Sized, Container):\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v)\n\n    @property\n    def dtype(self) -> DTypes: ...\n    @property\n    def data(self) -> memoryview: ...\n    @property\n    def flags(self) -> Any: ...\n    @property\n    def size(self) -> int: ...\n    @property\n    def itemsize(self) -> int: ...\n    @property\n    def nbytes(self) -> int: ...\n    @property\n    def ndim(self) -> int: ...\n    @property\n    def shape(self) -> Shape: ...\n    @property\n    def strides(self) -> Tuple[int, ...]: ...\n\n    # TODO: Is ArrayT right?\n    def astype(self: ArrayT, dtype: DTypes, order: str = ..., casting: str = ..., subok: bool = ..., copy: bool = ...) -> ArrayT: ...\n    def copy(self: ArrayT, order: str = ...) -> ArrayT: ...\n    def fill(self, value: Any) -> None: ...\n    # Shape manipulation\n    def reshape(self: ArrayT, shape: Shape, *, order: str = ...) -> ArrayT: ...\n    def transpose(self: ArrayT, axes: Shape) -> ArrayT: ...\n    # TODO: is this right? It returns 1d\n    def flatten(self, order: str = ...): ...\n    # TODO: is this right? It returns 1d\n    def ravel(self, order: str = ...): ...\n    def squeeze(self, axis: Union[int, Shape] = ...): ...\n    def __len__(self) -> int: ...\n    def __setitem__(self, key, value): ...\n    def __iter__(self) -> Iterator[Any]: ...\n    def __contains__(self, key) -> bool: ...\n    def __index__(self) -> int: ...\n    def __int__(self) -> int: ...\n    def __float__(self) -> float: ...\n    def __complex__(self) -> complex: ...\n    def __bool__(self) -> bool: ...\n    def __bytes__(self) -> bytes: ...\n    def __str__(self) -> str: ...\n    def __repr__(self) -> str: ...\n    def __copy__(self, order: str = ...): ...\n    def __deepcopy__(self, memo: dict) -> ArrayT: ...\n    def __lt__(self, other): ...\n    def __le__(self, other): ...\n    def __eq__(self, other): ...\n    def __ne__(self, other): ...\n    def __gt__(self, other): ...\n    def __ge__(self, other): ...\n    def __add__(self, other): ...\n    def __radd__(self, other): ...\n    def __iadd__(self, other): ...\n    def __sub__(self, other): ...\n    def __rsub__(self, other): ...\n    def __isub__(self, other): ...\n    def __mul__(self, other): ...\n    def __rmul__(self, other): ...\n    def __imul__(self, other): ...\n    def __truediv__(self, other): ...\n    def __rtruediv__(self, other): ...\n    def __itruediv__(self, other): ...\n    def __floordiv__(self, other): ...\n    def __rfloordiv__(self, other): ...\n    def __ifloordiv__(self, other): ...\n    def __mod__(self, other): ...\n    def __rmod__(self, other): ...\n    def __imod__(self, other): ...\n    def __divmod__(self, other): ...\n    def __rdivmod__(self, other): ...\n    # NumPy\'s __pow__ doesn\'t handle a third argument\n    def __pow__(self, other): ...\n    def __rpow__(self, other): ...\n    def __ipow__(self, other): ...\n    def __lshift__(self, other): ...\n    def __rlshift__(self, other): ...\n    def __ilshift__(self, other): ...\n    def __rshift__(self, other): ...\n    def __rrshift__(self, other): ...\n    def __irshift__(self, other): ...\n    def __and__(self, other): ...\n    def __rand__(self, other): ...\n    def __iand__(self, other): ...\n    def __xor__(self, other): ...\n    def __rxor__(self, other): ...\n    def __ixor__(self, other): ...\n    def __or__(self, other): ...\n    def __ror__(self, other): ...\n    def __ior__(self, other): ...\n    def __matmul__(self, other): ...\n    def __rmatmul__(self, other): ...\n    def __neg__(self: ArrayT) -> ArrayT: ...\n    def __pos__(self: ArrayT) -> ArrayT: ...\n    def __abs__(self: ArrayT) -> ArrayT: ...\n    def __invert__(self: ArrayT) -> ArrayT: ...\n    def get(self: ArrayT) -> ArrayT: ...\n    def all(self, axis: int = -1, out: Optional[ArrayT] = None, keepdims: bool = False) -> ArrayT: ...\n    def any(self, axis: int = -1, out: Optional[ArrayT] = None, keepdims: bool = False) -> ArrayT: ...\n    # def argmax(self, axis: int = -1, out: Optional[""Array""] = None, keepdims: Union[Tru, Fal]=False) -> Union[int, ""Ints1d""]: ...\n    def argmin(self, axis: int = -1, out: Optional[ArrayT] = None) -> ArrayT: ...\n    def clip(self, a_min: Any, a_max: Any, out: Optional[ArrayT]) -> ArrayT: ...\n    #def cumsum( self: ArrayT, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[ArrayT] = None) -> ArrayT: ...\n    def max(self, axis: int = -1, out: Optional[ArrayT] = None) -> ArrayT: ...\n    # def mean(self, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[SelfT] = None, keepdims: bool = False) -> ""Array"": ...\n    def min(self, axis: int = -1, out: Optional[ArrayT] = None) -> ArrayT: ...\n    def nonzero(self) -> ArrayT: ...\n    def prod(self, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[ArrayT] = None, keepdims: bool = False) -> ArrayT: ...\n    def round(self, decimals: int = 0, out: Optional[ArrayT] = None) -> ArrayT: ...\n    # def sum(self, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[ArrayT] = None, keepdims: bool = False) -> ArrayT: ...\n    def tobytes(self, order: str = ""C"") -> bytes: ...\n    def tolist(self) -> List[Any]: ...\n    def var(self: SelfT, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[ArrayT] = None, ddof: int = 0, keepdims: bool = False) -> SelfT: ...\n\n\nclass _Floats(_Array):\n    @property\n    def dtype(self) -> DTypesFloat: ...\n\n    def fill(self, value: float) -> None: ...\n    def reshape(self, shape: Shape, *, order: str = ...) -> ""_Floats"": ...\n\n\nclass _Ints(_Array):\n    @property\n    def dtype(self) -> DTypesInt: ...\n\n    def fill(self, value: int) -> None: ...\n    def reshape(self, shape: Shape, *, order: str = ...) -> ""_Ints"": ...\n\n\n""""""\nExtensive overloads to represent __getitem__ behaviour.\n\nIn an N+1 dimensional array, there will be N possible return types. For instance,\nif you have a 2d array, you could get back a float (array[i, j]), a floats1d\n(array[i]) or a floats2d (array[:i, :j]). You\'ll get the scalar if you have N\nints in the index, a 1d array if you have N-1 ints, etc.\n\nSo the trick here is to make a union with the various combinations that produce\neach result type, and then only have one overload per result. If we overloaded\non each *key* type, that would get crazy, because there\'s tonnes of combinations.\n\nIn each rank, we can use the same key-types for float and int, but we need a\ndifferent return-type union.\n""""""\n\n\nclass _Array1d(_Array):\n    """"""1-dimensional array.""""""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=1)\n\n    @property\n    def ndim(self) -> Literal[1]: ...\n    @property\n    def shape(self) -> Tuple[int]: ...\n\n    def __iter__(self) -> Iterator[Union[float, int]]: ...\n    def astype(self, dtype: DTypes, order: str = ..., casting: str = ..., subok: bool = ..., copy: bool = ...) -> ""_Array1d"": ...\n    def flatten(self: SelfT, order: str = ...) -> SelfT: ...\n    def ravel(self: SelfT, order: str = ...) -> SelfT: ...\n    # These is actually a bit too strict: It\'s legal to say \'array1d + array2d\'\n    # That\'s kind of bad code though; it\'s better to write array2d + array1d.\n    # We could relax this, but let\'s try the strict version.\n    def __add__(self: SelfT, other: Union[float, int, ""Array1d""]) -> SelfT: ...\n    def __sub__(self: SelfT, other: Union[float, int, ""Array1d""]) -> SelfT: ...\n    def __mul__(self: SelfT, other: Union[float, int, ""Array1d""]) -> SelfT: ...\n    def __pow__(self: SelfT, other: Union[float, int, ""Array1d""]) -> SelfT: ...\n    def __matmul__(self: SelfT, other: Union[float, int, ""Array1d""]) -> SelfT: ...\n    # These are not too strict though: you can\'t do += with higher dimensional.\n    def __iadd__(self, other: Union[float, int, ""Array1d""]): ...\n    def __isub__(self, other: Union[float, int, ""Array1d""]): ...\n    def __imul__(self, other: Union[float, int, ""Array1d""]): ...\n    def __ipow__(self, other: Union[float, int, ""Array1d""]): ...\n\n    @overload\n    def argmax(self, keepdims: Fal = False, axis: int = -1, out: Optional[_Array] = None) -> int: ...\n    @overload\n    def argmax(self, keepdims: Tru, axis: int = -1, out: Optional[_Array] = None) -> ""Ints1d"": ...\n    def argmax(self, keepdims: bool = False, axis: int = -1, out: Optional[_Array] = None) -> Union[int, ""Ints1d""]: ...\n\n    @overload\n    def mean(self, keepdims: Tru, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[""Floats1d""] = None) -> ""Floats1d"": ...\n    @overload\n    def mean(self, keepdims: Fal = False, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[""Floats1d""] = None) -> float: ...\n    def mean(self, keepdims: bool = False, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[""Floats1d""] = None) -> Union[""Floats1d"", float]: ...\n\n\nclass Floats1d(_Array1d, _Floats):\n    """"""1-dimensional array of floats.""""""\n\n    T: ""Floats1d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtine validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=1, dtype=""f"")\n\n    def __iter__(self) -> Iterator[float]: ...\n\n    @overload\n    def __getitem__(self, key: _1_KeyScalar) -> float: ...\n    @overload\n    def __getitem__(self, key: _1_Key1d) -> ""Floats1d"": ...\n    def __getitem__(self, key: _1_AllKeys) -> _F1_AllReturns: ...\n\n    @overload\n    def __setitem__(self, key: _1_KeyScalar, value: float) -> None: ...\n    @overload\n    def __setitem__(self, key: _1_Key1d, value: ""Floats1d"") -> None: ...\n    def __setitem__(self, key: _1_AllKeys, _F1_AllReturns) -> None: ...\n\n    @overload\n    def cumsum(self, *, keepdims: Tru, axis: Optional[OneAx] = None, out: Optional[""Floats1d""] = None) -> ""Floats1d"": ...\n    @overload # Cumsum is unusual in this\n    def cumsum(self, *, keepdims: Fal, axis: Optional[OneAx] = None, out: Optional[""Floats1d""] = None) -> ""Floats1d"": ...\n    def cumsum(self, *, keepdims: bool = False, axis: _1_AllAx = None, out: Optional[""Floats1d""] = None) -> ""Floats1d"": ...\n\n    @overload\n    def sum(self, *, keepdims: Tru, axis: Optional[OneAx] = None, out: Optional[""Floats1d""] = None) -> ""Floats1d"": ...\n    @overload\n    def sum(self, *, keepdims: Fal, axis: Optional[OneAx] = None, out = None) -> float: ...\n    def sum(self, *, keepdims: bool = False, axis: _1_AllAx = None, out: Optional[""Floats1d""] = None) -> _1F_ReduceResults: ...\n\n\nclass Ints1d(_Array1d, _Ints):\n    """"""1-dimensional array of ints.""""""\n\n    T: ""Ints1d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=1, dtype=""i"")\n\n    def __iter__(self) -> Iterator[int]: ...\n\n    @overload\n    def __getitem__(self, key: _1_KeyScalar) -> int: ...\n    @overload\n    def __getitem__(self, key: _1_Key1d) -> ""Ints1d"": ...\n    def __getitem__(self, key: _1_AllKeys) -> _I1_AllReturns: ...\n\n    @overload\n    def __setitem__(self, key: _1_KeyScalar, value: int) -> None: ...\n    @overload\n    def __setitem__(self, key: _1_Key1d, value: Union[int, ""Ints1d""]) -> None: ...\n    def __setitem__(self, key: _1_AllKeys, _I1_AllReturns) -> None: ...\n\n    @overload\n    def cumsum(self, *, keepdims: Tru, axis: Optional[OneAx] = None, out: Optional[""Ints1d""] = None) -> ""Ints1d"": ...\n    @overload\n    def cumsum(self, *, keepdims: Fal = False, axis: Optional[OneAx] = None, out: Optional[""Ints1d""] = None) -> ""Ints1d"": ...\n    def cumsum(self, *, keepdims: bool = False, axis: _1_AllAx = None, out: Optional[""Ints1d""] = None) -> ""Ints1d"": ...\n\n    @overload\n    def sum(self, *, keepdims: Tru, axis: Optional[OneAx] = None, out: Optional[""Ints1d""] = None) -> ""Ints1d"": ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: Optional[OneAx] = None, out = None) -> int: ...\n    def sum(self, *, keepdims: bool = False, axis: _1_AllAx = None, out: Optional[""Ints1d""] = None) -> _1I_ReduceResults: ...\n\n\n\nclass _Array2d(_Array):\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=2)\n\n    @property\n    def ndim(self) -> Literal[2]: ...\n    @property\n    def shape(self) -> Tuple[int, int]: ...\n\n    def __iter__(self) -> Iterator[Array1d]: ...\n    def astype(self, dtype: DTypes, order: str = ..., casting: str = ..., subok: bool = ..., copy: bool = ...) -> ""Array2d"": ...\n    # These is actually a bit too strict: It\'s legal to say \'array2d + array3d\'\n    # That\'s kind of bad code though; it\'s better to write array3d + array2d.\n    # We could relax this, but let\'s try the strict version.\n    def __add__(self: ArrayT, other: Union[float, int, Array1d, ""Array2d""]) -> ArrayT: ...\n    def __sub__(self: ArrayT, other: Union[float, int, Array1d, ""Array2d""]) -> ArrayT: ...\n    def __mul__(self: ArrayT, other: Union[float, int, Array1d, ""Array2d""]) -> ArrayT: ...\n    def __pow__(self: ArrayT, other: Union[float, int, Array1d, ""Array2d""]) -> ArrayT: ...\n    def __matmul__(self: ArrayT, other: Union[float, int, Array1d, ""Array2d""]) -> ArrayT: ...\n    # These are not too strict though: you can\'t do += with higher dimensional.\n    def __iadd__(self, other: Union[float, int, Array1d, ""Array2d""]): ...\n    def __isub__(self, other: Union[float, int, Array1d, ""Array2d""]): ...\n    def __imul__(self, other: Union[float, int, Array1d, ""Array2d""]): ...\n    def __ipow__(self, other: Union[float, int, Array1d, ""Array2d""]): ...\n\n    @overload\n    def argmax(self, keepdims: Fal = False, axis: int = -1, out: Optional[_Array] = None) -> Ints1d: ...\n    @overload\n    def argmax(self, keepdims: Tru, axis: int = -1, out: Optional[_Array] = None) -> ""Ints2d"": ...\n    def argmax(self, keepdims: bool = False, axis: int = -1, out: Optional[_Array] = None) -> Union[Ints1d, ""Ints2d""]: ...\n\n    @overload\n    def mean(self, keepdims: Fal = False, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[""Floats2d""] = None) -> Floats1d: ...\n    @overload\n    def mean(self, keepdims: Tru, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[""Floats2d""] = None) -> ""Floats2d"": ...\n    def mean(self, keepdims: bool = False, axis: int = -1, dtype: Optional[DTypes] = None, out: Optional[""Floats2d""] = None) -> Union[""Floats2d"", Floats1d]: ...\n\n\nclass Floats2d(_Array2d, _Floats):\n    """"""2-dimensional array of floats""""""\n\n    T: ""Floats2d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=2, dtype=""f"")\n\n    def __iter__(self) -> Iterator[Floats1d]: ...\n\n    @overload\n    def __getitem__(self, key: _2_KeyScalar) -> float: ...\n    @overload\n    def __getitem__(self, key: _2_Key1d) -> Floats1d: ...\n    @overload\n    def __getitem__(self, key: _2_Key2d) -> ""Floats2d"": ...\n    def __getitem__(self, key: _2_AllKeys) -> _F2_AllReturns: ...\n\n    @overload\n    def __setitem__(self, key: _2_KeyScalar, value: float) -> None: ...\n    @overload\n    def __setitem__(self, key: _2_Key1d, value: Union[float, Floats1d]) -> None: ...\n    @overload\n    def __setitem__(self, key: _2_Key2d, value: _F2_AllReturns) -> None: ...\n    def __setitem__(self, key: _2_AllKeys, value: _F2_AllReturns) -> None: ...\n\n    @overload\n    def sum(self, *, keepdims: Tru, axis: _2_AllAx = None, out: Optional[""Floats2d""] = None) -> ""Floats2d"": ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: OneAx, out: Optional[Floats1d] = None) -> Floats1d: ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: TwoAx, out = None) -> float: ...\n    def sum(self, *, keepdims: bool = False, axis: _2_AllAx = None, out: Union[None, ""Floats1d"", ""Floats2d""] = None) -> _2F_ReduceResults: ...\n\n\n\nclass Ints2d(_Array2d, _Ints):\n    """"""2-dimensional array of ints.""""""\n\n    T: ""Ints2d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=2, dtype=""i"")\n\n    def __iter__(self) -> Iterator[Ints1d]: ...\n\n    @overload\n    def __getitem__(self, key: _2_KeyScalar) -> int: ...\n    @overload\n    def __getitem__(self, key: _2_Key1d) -> Ints1d: ...\n    @overload\n    def __getitem__(self, key: _2_Key2d) -> ""Ints2d"": ...\n    def __getitem__(self, key: _2_AllKeys) -> _I2_AllReturns: ...\n\n    @overload\n    def __setitem__(self, key: _2_KeyScalar, value: int) -> None: ...\n    @overload\n    def __setitem__(self, key: _2_Key1d, value: Ints1d) -> None: ...\n    @overload\n    def __setitem__(self, key: _2_Key2d, value: ""Ints2d"") -> None: ...\n    def __setitem__(self, key: _2_AllKeys, value: _I2_AllReturns) -> None: ...\n\n    @overload\n    def sum(self, keepdims: Fal = False, axis: int = -1, out: Optional[""Ints1d""] = None) -> Ints1d: ...\n    @overload\n    def sum(self, keepdims: Tru, axis: int = -1, out: Optional[""Ints2d""] = None) -> ""Ints2d"": ...\n    def sum(self, keepdims: bool = False, axis: int = -1, out: Optional[Union[""Ints1d"", ""Ints2d""]] = None) -> Union[""Ints2d"", Ints1d]: ...\n\n\nclass _Array3d(_Array):\n    """"""3-dimensional array of floats""""""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=3)\n\n    @property\n    def ndim(self) -> Literal[3]: ...\n    @property\n    def shape(self) -> Tuple[int, int, int]: ...\n\n    def __iter__(self) -> Iterator[Array2d]: ...\n    def astype(self, dtype: DTypes, order: str = ..., casting: str = ..., subok: bool = ..., copy: bool = ...) -> ""Array3d"": ...\n    # These is actually a bit too strict: It\'s legal to say \'array2d + array3d\'\n    # That\'s kind of bad code though; it\'s better to write array3d + array2d.\n    # We could relax this, but let\'s try the strict version.\n    def __add__(self: SelfT, other: Union[float, int, Array1d, Array2d, ""Array3d""]) -> SelfT: ...\n    def __sub__(self: SelfT, other: Union[float, int, Array1d, Array2d, ""Array3d""]) -> SelfT: ...\n    def __mul__(self: SelfT, other: Union[float, int, Array1d, Array2d, ""Array3d""]) -> SelfT: ...\n    def __pow__(self: SelfT, other: Union[float, int, Array1d, Array2d, ""Array3d""]) -> SelfT: ...\n    def __matmul__(self: SelfT, other: Union[float, int, Array1d, Array2d, ""Array3d""]) -> SelfT: ...\n    # These are not too strict though: you can\'t do += with higher dimensional.\n    def __iadd__(self, other: Union[float, int, Array1d, Array2d, ""Array3d""]): ...\n    def __isub__(self, other: Union[float, int, Array1d, Array2d, ""Array3d""]): ...\n    def __imul__(self, other: Union[float, int, Array1d, Array2d, ""Array3d""]): ...\n    def __ipow__(self, other: Union[float, int, Array1d, Array2d, ""Array3d""]): ...\n\n    @overload\n    def argmax(self, keepdims: Fal = False, axis: int = -1, out: Optional[_Array] = None) -> Ints2d: ...\n    @overload\n    def argmax(self, keepdims: Tru, axis: int = -1, out: Optional[_Array] = None) -> ""Ints3d"": ...\n    def argmax(self, keepdims: bool = False, axis: int = -1, out: Optional[_Array] = None) -> Union[Ints2d, ""Ints3d""]: ...\n\n\nclass Floats3d(_Array3d, _Floats):\n    """"""3-dimensional array of floats""""""\n\n    T: ""Floats3d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=3, dtype=""f"")\n\n    def __iter__(self) -> Iterator[Floats2d]: ...\n\n    @overload\n    def __getitem__(self, key: _3_KeyScalar) -> float: ...\n    @overload\n    def __getitem__(self, key: _3_Key1d) -> Floats1d: ...\n    @overload\n    def __getitem__(self, key: _3_Key2d) -> Floats2d: ...\n    @overload\n    def __getitem__(self, key: _3_Key3d) -> ""Floats3d"": ...\n    def __getitem__(self, key: _3_AllKeys) -> _F3_AllReturns: ...\n\n    @overload\n    def __setitem__(self, key: _3_KeyScalar, value: float) -> None: ...\n    @overload\n    def __setitem__(self, key: _3_Key1d, value: Floats1d) -> None: ...\n    @overload\n    def __setitem__(self, key: _3_Key2d, value: Floats2d) -> None: ...\n    @overload\n    def __setitem__(self, key: _3_Key3d, value: ""Floats3d"") -> None: ...\n    def __setitem__(self, key: _3_AllKeys, value: _F3_AllReturns) -> None: ...\n\n    @overload\n    def sum(self, *, keepdims: Tru, axis: _3_AllAx = None, out: Optional[""Floats3d""] = None) -> ""Floats3d"": ...\n    @overload\n    def sum(self, *, keepdims: Fal, axis: OneAx, out: Optional[Floats2d] = None) -> Floats2d: ...\n    @overload\n    def sum(self, *, keepdims: Fal, axis: TwoAx, out: Optional[Floats1d] = None) -> Floats1d: ...\n    @overload\n    def sum(self, *, keepdims: Fal, axis: Optional[ThreeAx], out = None) -> float: ...\n    def sum(self, *, keepdims: bool = False, axis: _3_AllAx = None, out: Union[None, Floats1d, Floats2d, ""Floats3d""] = None) -> _3F_ReduceResults: ...\n\n\nclass Ints3d(_Array3d, _Ints):\n    """"""3-dimensional array of ints.""""""\n\n    T: ""Ints3d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=3, dtype=""i"")\n\n    def __iter__(self) -> Iterator[Ints2d]: ...\n\n    @overload\n    def __getitem__(self, key: _3_KeyScalar) -> int: ...\n    @overload\n    def __getitem__(self, key: _3_Key1d) -> Ints1d: ...\n    @overload\n    def __getitem__(self, key: _3_Key2d) -> Ints2d: ...\n    @overload\n    def __getitem__(self, key: _3_Key3d) -> ""Ints3d"": ...\n    def __getitem__(self, key: _3_AllKeys) -> _I3_AllReturns: ...\n\n    @overload\n    def __setitem__(self, key: _3_KeyScalar, value: int) -> None: ...\n    @overload\n    def __setitem__(self, key: _3_Key1d, value: Ints1d) -> None: ...\n    @overload\n    def __setitem__(self, key: _3_Key2d, value: Ints2d) -> None: ...\n    @overload\n    def __setitem__(self, key: _3_Key3d, value: ""Ints3d"") -> None: ...\n    def __setitem__(self, key: _3_AllKeys, value: _I3_AllReturns) -> None: ...\n\n    @overload\n    def sum(self, *, keepdims: Tru, axis: _3_AllAx = None, out: Optional[""Ints3d""] = None) -> ""Ints3d"": ...\n    @overload\n    def sum(self, *, keepdims: Fal, axis: OneAx, out: Optional[Ints2d] = None) -> Ints2d: ...\n    @overload\n    def sum(self, *, keepdims: Fal, axis: TwoAx, out: Optional[Ints1d] = None) -> Ints1d: ...\n    @overload\n    def sum(self, *, keepdims: Fal, axis: Optional[ThreeAx], out = None) -> int: ...\n    def sum(self, *, keepdims: bool = False, axis: _3_AllAx = None, out: Union[None, Ints1d, Ints2d, ""Ints3d""] = None) -> _3I_ReduceResults: ...\n\n\nclass _Array4d(_Array):\n    """"""4-dimensional array.""""""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=4)\n\n    @property\n    def ndim(self) -> Literal[4]: ...\n    @property\n    def shape(self) -> Tuple[int, int, int, int]: ...\n\n    def __iter__(self) -> Iterator[Array3d]: ...\n    def astype(self, dtype: DTypes, order: str = ..., casting: str = ..., subok: bool = ..., copy: bool = ...) -> ""_Array4d"": ...\n    # These is actually a bit too strict: It\'s legal to say \'array4d + array5d\'\n    # That\'s kind of bad code though; it\'s better to write array5d + array4d.\n    # We could relax this, but let\'s try the strict version.\n    def __add__(self: SelfT, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]) -> SelfT: ...\n    def __sub__(self: SelfT, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]) -> SelfT: ...\n    def __mul__(self: SelfT, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]) -> SelfT: ...\n    def __pow__(self: SelfT, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]) -> SelfT: ...\n    def __matmul__(self: SelfT, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]) -> SelfT: ...\n    # These are not too strict though: you can\'t do += with higher dimensional.\n    def __iadd__(self, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]): ...\n    def __isub__(self, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]): ...\n    def __imul__(self, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]): ...\n    def __ipow__(self, other: Union[float, int, Array1d, Array2d, Array3d, ""Array4d""]): ...\n\n\nclass Floats4d(_Array4d, _Floats):\n    """"""4-dimensional array of floats.""""""\n\n    T: ""Floats4d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=4, dtype=""f"")\n\n    def __iter__(self) -> Iterator[Floats3d]: ...\n    # def __getitem__(self, key: int) -> Floats3d: ...\n\n    @overload\n    def sum(self, *, keepdims: Tru, axis: _4_AllAx = None, out: Optional[""Floats4d""] = None) -> ""Floats4d"": ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: OneAx, out: Optional[Floats3d] = None) -> Floats3d: ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: TwoAx, out: Optional[Floats2d] = None) -> Floats2d: ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: ThreeAx, out: Optional[Floats1d] = None) -> Floats1d: ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: Optional[FourAx], out = None) -> float: ...\n    def sum(self, *, keepdims: bool = False, axis: _4_AllAx = None, out: Union[None, Floats1d, Floats2d, Floats3d, ""Floats4d""] = None) -> _4F_ReduceResults: ...\n\n\n\nclass Ints4d(_Array4d, _Ints):\n    """"""4-dimensional array of ints.""""""\n\n    T: ""Ints4d""\n\n    @classmethod\n    def __get_validators__(cls):\n        """"""Runtime validation for pydantic.""""""\n        yield lambda v: validate_array(v, ndim=4, dtype=""i"")\n\n    def __iter__(self) -> Iterator[Ints3d]: ...\n    # def __getitem__(self, key: int) -> Ints3d: ...\n\n    @overload\n    def sum(self, *, keepdims: Tru, axis: _4_AllAx = None, out: Optional[""Ints4d""] = None) -> ""Ints4d"": ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: OneAx, out: Optional[Ints3d] = None) -> Ints3d: ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: TwoAx, out: Optional[Ints2d] = None) -> Ints2d: ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: ThreeAx, out: Optional[Ints1d] = None) -> Ints1d: ...\n    @overload\n    def sum(self, *, keepdims: Fal = False, axis: Optional[FourAx] = None, out = None) -> int: ...\n    def sum(self, *, keepdims: bool = False, axis: _4_AllAx = None, out: Optional[Union[Ints1d, Ints2d, Ints3d, ""Ints4d""]] = None) -> _4I_ReduceResults: ...\n\n\n\n_DIn = TypeVar(""_DIn"")\n\n\nclass Decorator(Protocol):\n    """"""Protocol to mark a function as returning its child with identical signature.""""""\n\n    def __call__(self, name: str) -> Callable[[_DIn], _DIn]: ...\n\n\nclass Doc(Sized, Container):\n    """"""Type for spaCy Doc objects.""""""\n\n    T: ""Doc""\n    base: Optional[""Doc""]\n\n    @property\n    def doc(self) -> ""Doc"": ...\n    @property\n    def start(self) -> int: ...\n    @property\n    def end(self) -> int: ...\n\n    def to_array(self, attr_ids: Union[str, int, List[Union[str, int]]]) -> Ints2d: ...\n\n\n# fmt: on\n\n\nclass Generator(Iterator):\n    """"""Custom generator type. Used to annotate function arguments that accept\n    generators so they can be validated by pydantic (which doesn\'t support\n    iterators/iterables otherwise).\n    """"""\n\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):\n        if not hasattr(v, ""__iter__"") and not hasattr(v, ""__next__""):\n            raise TypeError(""not a valid iterator"")\n        return v\n\n\n@dataclass\nclass SizedGenerator:\n    """"""A generator that has a __len__ and can repeatedly call the generator\n    function.\n    """"""\n\n    get_items: Callable[[], Generator]\n    length: int\n\n    def __len__(self):\n        return self.length\n\n    def __iter__(self):\n        yield from self.get_items()\n\n\n@dataclass\nclass Padded:\n    """"""A batch of padded sequences, sorted by decreasing length. The data array\n    is of shape (step, batch, ...). The auxiliary array size_at_t indicates the\n    length of the batch at each timestep, so you can do data[:, :size_at_t[t]] to\n    shrink the batch. The lengths array indicates the length of each row b,\n    and the indices indicates the original ordering.\n    """"""\n\n    data: Floats3d\n    size_at_t: Ints1d\n    lengths: Ints1d\n    indices: Ints1d\n\n    def __len__(self) -> int:\n        return self.lengths.shape[0]\n\n    def __getitem__(self, index: Union[int, slice, Ints1d]) -> ""Padded"":\n        if isinstance(index, int):\n            # Slice to keep the dimensionality\n            return Padded(\n                self.data[:, index : index + 1],\n                self.lengths[index : index + 1],\n                self.lengths[index : index + 1],\n                self.indices[index : index + 1],\n            )\n        elif isinstance(index, slice):\n            return Padded(\n                self.data[:, index],\n                self.lengths[index],\n                self.lengths[index],\n                self.indices[index],\n            )\n        else:\n            # If we get a sequence of indices, we need to be careful that\n            # we maintain the length-sorting, while also keeping the mapping\n            # back to the original order correct.\n            sorted_index = list(sorted(index))\n            return Padded(\n                self.data[sorted_index],\n                self.size_at_t[sorted_index],\n                self.lengths[sorted_index],\n                self.indices[index],  # Use original, to maintain order.\n            )\n\n\n@dataclass\nclass Ragged:\n    """"""A batch of concatenated sequences, that vary in the size of their\n    first dimension. Ragged allows variable-length sequence data to be contiguous\n    in memory, without padding.\n\n    Indexing into Ragged is just like indexing into the *lengths* array, except\n    it returns a Ragged object with the accompanying sequence data. For instance,\n    you can write ragged[1:4] to get a Ragged object with sequences 1, 2 and 3.\n    """"""\n\n    data: Array2d\n    lengths: Ints1d\n    data_shape: Tuple[int, ...]\n    _cumsums: Optional[Ints1d] = None\n\n    def __init__(self, data: _Array, lengths: Ints1d):\n        self.lengths = lengths\n        # Frustratingly, the -1 dimension doesn\'t work with 0 size...\n        if data.size:\n            self.data = cast(Array2d, data.reshape((data.shape[0], -1)))\n        else:\n            self.data = cast(Array2d, data.reshape((0, 0)))\n        self.data_shape = (-1,) + data.shape[1:]\n\n    @property\n    def dataXd(self) -> ArrayXd:\n        if self.data.size:\n            reshaped = self.data.reshape(self.data_shape)\n        else:\n            reshaped = self.data.reshape((self.data.shape[0],) + self.data_shape[1:])\n        return cast(ArrayXd, reshaped)\n\n    def __len__(self) -> int:\n        return self.lengths.shape[0]\n\n    def __getitem__(self, index: Union[int, slice, Array1d]) -> ""Ragged"":\n        if isinstance(index, tuple):\n            raise IndexError(""Ragged arrays do not support 2d indexing."")\n        starts = self._get_starts()\n        ends = self._get_ends()\n        if isinstance(index, int):\n            s = starts[index]\n            e = ends[index]\n            return Ragged(self.data[s:e], self.lengths[index : index + 1])\n        elif isinstance(index, slice):\n            lengths = self.lengths[index]\n            cumsums = self._get_cumsums()\n            start = cumsums[index.start - 1] if index.start >= 1 else 0\n            end = start + lengths.sum()\n            return Ragged(self.data[start:end].reshape(self.data_shape), lengths)\n        else:\n            # There must be a way to do this ""properly"" :(. Sigh, hate numpy.\n            xp = get_array_module(self.data)\n            data = xp.vstack([self[int(i)].data for i in index])\n            return Ragged(data.reshape(self.data_shape), self.lengths[index])\n\n    def _get_cumsums(self) -> Ints1d:\n        if self._cumsums is None:\n            self._cumsums = self.lengths.cumsum()\n        return self._cumsums\n\n    def _get_starts(self) -> Ints1d:\n        cumsums = self._get_cumsums()\n        xp = get_array_module(cumsums)\n        zero = xp.array([0], dtype=""i"")\n        return xp.concatenate((zero, cumsums[:-1]))\n\n    def _get_ends(self) -> Ints1d:\n        return self._get_cumsums()\n\n\n_P = TypeVar(""_P"", bound=Sequence)\n\n\n@dataclass\nclass Pairs(Generic[_P]):\n    """"""Dataclass for pairs of sequences that allows indexing into the sequences\n    while keeping them aligned.\n    """"""\n\n    one: _P\n    two: _P\n\n    def __getitem__(self, index) -> ""Pairs[_P]"":\n        return Pairs(self.one[index], self.two[index])\n\n    def __len__(self) -> int:\n        return len(self.one)\n\n\n@dataclass\nclass ArgsKwargs:\n    """"""A tuple of (args, kwargs) that can be spread into some function f:\n\n        f(*args, **kwargs)\n    """"""\n\n    args: Tuple[Any, ...]\n    kwargs: Dict[str, Any]\n\n    @classmethod\n    def from_items(cls, items: Sequence[Tuple[Union[int, str], Any]]) -> ""ArgsKwargs"":\n        """"""Create an ArgsKwargs object from a sequence of (key, value) tuples,\n        such as produced by argskwargs.items(). Each key should be either a string\n        or an integer. Items with int keys are added to the args list, and\n        items with string keys are added to the kwargs list. The args list is\n        determined by sequence order, not the value of the integer.\n        """"""\n        args = []\n        kwargs = {}\n        for key, value in items:\n            if isinstance(key, int):\n                args.append(value)\n            else:\n                kwargs[key] = value\n        return cls(args=tuple(args), kwargs=kwargs)\n\n    def keys(self) -> Iterable[Union[int, str]]:\n        """"""Yield indices from self.args, followed by keys from self.kwargs.""""""\n        yield from range(len(self.args))\n        yield from self.kwargs.keys()\n\n    def values(self) -> Iterable[Any]:\n        """"""Yield elements of from self.args, followed by values from self.kwargs.""""""\n        yield from self.args\n        yield from self.kwargs.values()\n\n    def items(self) -> Iterable[Tuple[Union[int, str], Any]]:\n        """"""Yield enumerate(self.args), followed by self.kwargs.items()""""""\n        yield from enumerate(self.args)\n        yield from self.kwargs.items()\n\n\n@dataclass\nclass Unserializable:\n    """"""Wrap a value to prevent it from being serialized by msgpack.""""""\n\n    obj: Any\n\n\ndef validate_array(obj, ndim=None, dtype=None):\n    """"""Runtime validator for pydantic to validate array types.""""""\n    xp = get_array_module(obj)\n    if not isinstance(obj, xp.ndarray):\n        raise TypeError(""not a valid numpy or cupy array"")\n    errors = []\n    if ndim is not None and obj.ndim != ndim:\n        errors.append(f""wrong array dimensions (expected {ndim}, got {obj.ndim})"")\n    if dtype is not None:\n        dtype_mapping = {""f"": [""float32""], ""i"": [""int32"", ""int64"", ""uint32"", ""uint64""]}\n        expected_types = dtype_mapping.get(dtype, [])\n        if obj.dtype not in expected_types:\n            expected = ""/"".join(expected_types)\n            err = f""wrong array data type (expected {expected}, got {obj.dtype})""\n            errors.append(err)\n    if errors:\n        raise ValueError("", "".join(errors))\n    return obj\n'"
thinc/util.py,11,"b'from typing import Any, Union, Sequence, cast, Dict, Optional, Callable, TypeVar\nfrom typing import List\nimport numpy\nimport random\nimport functools\nfrom wasabi import table\nfrom pydantic import create_model, ValidationError\nimport inspect\nimport os\nimport tempfile\nimport contextlib\n\ntry:  # pragma: no cover\n    import cupy\n\n    has_cupy = True\nexcept (ImportError, AttributeError):\n    cupy = None\n    has_cupy = False\n\ntry:  # pragma: no cover\n    import jax\n    import jax.numpy\n\n    has_jax = True\nexcept ImportError:  # pragma: no cover\n    jax = None\n    has_jax = False\n\ntry:  # pragma: no cover\n    import torch\n    import torch.tensor\n    import torch.utils.dlpack\n\n    has_torch = True\nexcept ImportError:  # pragma: no cover\n    has_torch = False\n\ntry:  # pragma: no cover\n    import tensorflow as tf\n\n    has_tensorflow = True\nexcept ImportError:  # pragma: no cover\n    has_tensorflow = False\n\n\ntry:  # pragma: no cover\n    import mxnet as mx\n\n    has_mxnet = True\nexcept ImportError:  # pragma: no cover\n    has_mxnet = False\n\nfrom .types import ArrayXd, ArgsKwargs, Ragged, Padded, FloatsXd, IntsXd\n\n\ndef get_array_module(arr):  # pragma: no cover\n    if is_cupy_array(arr):\n        return cupy\n    elif is_jax_array(arr):\n        return jax.numpy\n    else:\n        return numpy\n\n\ndef fix_random_seed(seed: int = 0) -> None:  # pragma: no cover\n    """"""Set the random seed across random, numpy.random and cupy.random.""""""\n    random.seed(seed)\n    numpy.random.seed(seed)\n    if cupy is not None:\n        cupy.random.seed(seed)\n\n\ndef is_xp_array(obj: Any) -> bool:\n    """"""Check whether an object is a numpy or cupy array.""""""\n    return is_numpy_array(obj) or is_cupy_array(obj) or is_jax_array(obj)\n\n\ndef is_cupy_array(obj: Any) -> bool:  # pragma: no cover\n    """"""Check whether an object is a cupy array""""""\n    if not has_cupy:\n        return False\n    elif isinstance(obj, cupy.ndarray):\n        return True\n    else:\n        return False\n\n\ndef is_jax_array(obj: Any) -> bool:  # pragma: no cover\n    """"""Check whether an object is a jax.numpy array""""""\n    if not has_jax:\n        return False\n    elif isinstance(obj, numpy.ndarray):\n        # Numpy arrays evaluate as True for instance of jax.numpy.ndarray :(\n        return False\n    elif isinstance(obj, jax.numpy.ndarray):\n        return True\n    else:\n        return False\n\n\ndef is_numpy_array(obj: Any) -> bool:\n    """"""Check whether an object is a numpy array""""""\n    if isinstance(obj, numpy.ndarray):\n        return True\n    else:\n        return False\n\n\ndef is_torch_array(obj: Any) -> bool:  # pragma: no cover\n    if torch is None:\n        return False\n    elif isinstance(obj, torch.Tensor):\n        return True\n    else:\n        return False\n\n\ndef is_tensorflow_array(obj: Any) -> bool:  # pragma: no cover\n    if not has_tensorflow:\n        return False\n    elif isinstance(obj, tf.Tensor):\n        return True\n    else:\n        return False\n\n\ndef is_mxnet_array(obj: Any) -> bool:  # pragma: no cover\n    if not has_mxnet:\n        return False\n    elif isinstance(obj, mx.nd.NDArray):\n        return True\n    else:\n        return False\n\n\ndef to_numpy(data):  # pragma: no cover\n    if isinstance(data, numpy.ndarray):\n        return data\n    elif has_cupy and isinstance(data, cupy.ndarray):\n        return data.get()\n    elif has_jax and isinstance(data, jax.numpy.ndarray):\n        return jax.device_get(data)\n    else:\n        return numpy.array(data)\n\n\ndef set_active_gpu(gpu_id: int) -> ""cupy.cuda.Device"":  # pragma: no cover\n    """"""Set the current GPU device for cupy and torch (if available).""""""\n    import cupy.cuda.device\n\n    device = cupy.cuda.device.Device(gpu_id)\n    device.use()\n    try:\n        import torch\n\n        torch.cuda.set_device(gpu_id)\n        torch.set_default_tensor_type(""torch.cuda.FloatTensor"")\n    except ImportError:\n        pass\n    return device\n\n\ndef prefer_gpu(gpu_id: int = 0) -> bool:  # pragma: no cover\n    """"""Use GPU if it\'s available. Returns True if so, False otherwise.""""""\n    from .backends.cupy_ops import CupyOps\n\n    if CupyOps.xp is None:\n        return False\n    else:\n        require_gpu(gpu_id=gpu_id)\n        return True\n\n\ndef require_gpu(gpu_id: int = 0) -> bool:  # pragma: no cover\n    from .backends import set_current_ops, CupyOps\n\n    if CupyOps.xp is None:\n        raise ValueError(""GPU is not accessible. Was the library installed correctly?"")\n\n    set_current_ops(CupyOps())\n    set_active_gpu(gpu_id)\n    return True\n\n\ndef copy_array(dst: ArrayXd, src: ArrayXd) -> None:  # pragma: no cover\n    if isinstance(dst, numpy.ndarray) and isinstance(src, numpy.ndarray):\n        dst[:] = src\n    elif is_cupy_array(dst):\n        src = cupy.array(src, copy=False)\n        cupy.copyto(dst, src)\n    else:\n        numpy.copyto(dst, src)\n\n\ndef to_categorical(Y: IntsXd, n_classes: Optional[int] = None) -> FloatsXd:\n    # From keras\n    xp = get_array_module(Y)\n    if xp is cupy:  # pragma: no cover\n        Y = Y.get()\n    keep_shapes: List[int] = list(Y.shape)\n    Y = numpy.array(Y, dtype=""int"").ravel()\n    if n_classes is None:\n        n_classes = int(numpy.max(Y) + 1)\n    keep_shapes.append(n_classes)\n    n = Y.shape[0]\n    categorical = numpy.zeros((n, n_classes), dtype=""float32"")\n    categorical[numpy.arange(n), Y] = 1\n    return xp.asarray(categorical).reshape(keep_shapes)\n\n\ndef get_width(\n    X: Union[ArrayXd, Ragged, Padded, Sequence[ArrayXd]], *, dim: int = -1\n) -> int:\n    """"""Infer the \'width\' of a batch of data, which could be any of: Array,\n    Ragged, Padded or Sequence of Arrays.\n    """"""\n    if isinstance(X, Ragged):\n        return get_width(X.data, dim=dim)\n    elif isinstance(X, Padded):\n        return get_width(X.data, dim=dim)\n    elif hasattr(X, ""shape"") and hasattr(X, ""ndim""):\n        X = cast(ArrayXd, X)\n        if len(X.shape) == 0:\n            return 0\n        elif len(X.shape) == 1:\n            return int(X.max()) + 1\n        else:\n            return X.shape[dim]\n    elif isinstance(X, (list, tuple)):\n        if len(X) == 0:\n            return 0\n        else:\n            return get_width(X[0], dim=dim)\n    else:\n        err = ""Cannot get width of object: has neither shape nor __getitem__""\n        raise ValueError(err)\n\n\ndef assert_tensorflow_installed() -> None:  # pragma: no cover\n    """"""Raise an ImportError if TensorFlow is not installed.""""""\n    template = ""TensorFlow support requires {pkg}: pip install thinc[tensorflow]""\n    if not has_tensorflow:\n        raise ImportError(template.format(pkg=""tensorflow>=2.0.0""))\n\n\ndef assert_mxnet_installed() -> None:  # pragma: no cover\n    """"""Raise an ImportError if MXNet is not installed.""""""\n    if not has_mxnet:\n        raise ImportError(""MXNet support requires mxnet: pip install thinc[mxnet]"")\n\n\ndef assert_pytorch_installed() -> None:  # pragma: no cover\n    """"""Raise an ImportError if PyTorch is not installed.""""""\n    if not has_torch:\n        raise ImportError(""PyTorch support requires torch: pip install thinc[torch]"")\n\n\ndef convert_recursive(\n    is_match: Callable[[Any], bool], convert_item: Callable[[Any], Any], obj: Any\n) -> Any:\n    """"""Either convert a single value if it matches a given function, or\n    recursively walk over potentially nested lists, tuples and dicts applying\n    the conversion, and returns the same type. Also supports the ArgsKwargs\n    dataclass.\n    """"""\n    if is_match(obj):\n        return convert_item(obj)\n    elif isinstance(obj, ArgsKwargs):\n        converted = convert_recursive(is_match, convert_item, list(obj.items()))\n        return ArgsKwargs.from_items(converted)\n    elif isinstance(obj, dict):\n        converted = {}\n        for key, value in obj.items():\n            key = convert_recursive(is_match, convert_item, key)\n            value = convert_recursive(is_match, convert_item, value)\n            converted[key] = value\n        return converted\n    elif isinstance(obj, list):\n        return [convert_recursive(is_match, convert_item, item) for item in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_recursive(is_match, convert_item, item) for item in obj)\n    else:\n        return obj\n\n\ndef xp2torch(\n    xp_tensor: ArrayXd, requires_grad: bool = False\n) -> ""torch.Tensor"":  # pragma: no cover\n    """"""Convert a numpy or cupy tensor to a PyTorch tensor.""""""\n    if hasattr(xp_tensor, ""toDlpack""):\n        dlpack_tensor = xp_tensor.toDlpack()  # type: ignore\n        torch_tensor = torch.utils.dlpack.from_dlpack(dlpack_tensor)\n    else:\n        torch_tensor = torch.from_numpy(xp_tensor)\n    if requires_grad:\n        torch_tensor.requires_grad_()\n    return torch_tensor\n\n\ndef torch2xp(torch_tensor: ""torch.Tensor"") -> ArrayXd:  # pragma: no cover\n    """"""Convert a torch tensor to a numpy or cupy tensor.""""""\n    if torch_tensor.is_cuda:\n        return cupy.fromDlpack(torch.utils.dlpack.to_dlpack(torch_tensor))\n    else:\n        return torch_tensor.detach().numpy()\n\n\ndef xp2tensorflow(\n    xp_tensor: ArrayXd, requires_grad: bool = False, as_variable: bool = False\n) -> ""tf.Tensor"":  # pragma: no cover\n    """"""Convert a numpy or cupy tensor to a TensorFlow Tensor or Variable""""""\n    assert_tensorflow_installed()\n    tensorflow_tensor = tf.convert_to_tensor(xp_tensor)\n    if as_variable:\n        # tf.Variable() automatically puts in GPU if available.\n        # So we need to control it using the context manager\n        with tf.device(tensorflow_tensor.device):\n            tensorflow_tensor = tf.Variable(tensorflow_tensor, trainable=requires_grad)\n    if requires_grad is False and as_variable is False:\n        # tf.stop_gradient() automatically puts in GPU if available.\n        # So we need to control it using the context manager\n        with tf.device(tensorflow_tensor.device):\n            tensorflow_tensor = tf.stop_gradient(tensorflow_tensor)\n    return tensorflow_tensor\n\n\ndef tensorflow2xp(tensorflow_tensor: ""tf.Tensor"") -> ArrayXd:  # pragma: no cover\n    """"""Convert a Tensorflow tensor to numpy or cupy tensor.""""""\n    assert_tensorflow_installed()\n    return tensorflow_tensor.numpy()\n\n\ndef xp2mxnet(\n    xp_tensor: ArrayXd, requires_grad: bool = False\n) -> ""torch.Tensor"":  # pragma: no cover\n    """"""Convert a numpy or cupy tensor to a MXNet tensor.""""""\n    if hasattr(xp_tensor, ""toDlpack""):\n        dlpack_tensor = xp_tensor.toDlpack()  # type: ignore\n        mx_tensor = mx.nd.from_dlpack(dlpack_tensor)\n    else:\n        mx_tensor = mx.nd.from_numpy(xp_tensor)\n    if requires_grad:\n        mx_tensor.attach_grad()\n    return mx_tensor\n\n\ndef mxnet2xp(mx_tensor: ""mx.nd.NDArray"") -> ArrayXd:  # pragma: no cover\n    """"""Convert a MXNet tensor to a numpy or cupy tensor.""""""\n    if mx_tensor.context.device_type != ""cpu"":\n        return cupy.fromDlpack(mx_tensor.to_dlpack_for_write())\n    else:\n        return mx_tensor.detach().asnumpy()\n\n\n# This is how functools.partials seems to do it, too, to retain the return type\nPartialT = TypeVar(""PartialT"")\n\n\ndef partial(\n    func: Callable[..., PartialT], *args: Any, **kwargs: Any\n) -> Callable[..., PartialT]:\n    """"""Wrapper around functools.partial that retains docstrings and can include\n    other workarounds if needed.\n    """"""\n    partial_func = functools.partial(func, *args, **kwargs)\n    partial_func.__doc__ = func.__doc__\n    return partial_func\n\n\nclass DataValidationError(ValueError):\n    def __init__(\n        self, name: str, X: Any, Y: Any, errors: List[Dict[str, Any]] = []\n    ) -> None:\n        """"""Custom error for validating inputs / outputs at runtime.""""""\n        message = f""Data validation error in \'{name}\'""\n        type_info = f""X: {type(X)} Y: {type(Y)}""\n        data = []\n        for error in errors:\n            err_loc = "" -> "".join([str(p) for p in error.get(""loc"", [])])\n            data.append((err_loc, error.get(""msg"")))\n        result = [message, type_info, table(data)]\n        ValueError.__init__(self, ""\\n\\n"" + ""\\n"".join(result))\n\n\nclass _ArgModelConfig:\n    extra = ""forbid""\n    arbitrary_types_allowed = True\n\n\ndef validate_fwd_input_output(\n    name: str, func: Callable[[Any, Any, bool], Any], X: Any, Y: Any\n) -> None:\n    """"""Validate the input and output of a forward function against the type\n    annotations, if available. Used in Model.initialize with the input and\n    output samples as they pass through the network.\n    """"""\n    sig = inspect.signature(func)\n    empty = inspect.Signature.empty\n    params = list(sig.parameters.values())\n    if len(params) != 3:\n        bad_params = f""{len(params)} ({\', \'.join([p.name for p in params])})""\n        err = f""Invalid forward function. Expected 3 arguments (model, X , is_train), got {bad_params}""\n        raise DataValidationError(name, X, Y, [{""msg"": err}])\n    annot_x = params[1].annotation\n    annot_y = sig.return_annotation\n    sig_args: Dict[str, Any] = {""__config__"": _ArgModelConfig}\n    args = {}\n    if X is not None and annot_x != empty:\n        if isinstance(X, list) and len(X) > 5:\n            X = X[:5]\n        sig_args[""X""] = (annot_x, ...)\n        args[""X""] = X\n    if Y is not None and annot_y != empty:\n        if isinstance(Y, list) and len(Y) > 5:\n            Y = Y[:5]\n        sig_args[""Y""] = (annot_y, ...)\n        args[""Y""] = (Y, lambda x: x)\n    ArgModel = create_model(""ArgModel"", **sig_args)\n    try:\n        ArgModel.parse_obj(args)\n    except ValidationError as e:\n        raise DataValidationError(name, X, Y, e.errors())\n\n\n@contextlib.contextmanager\ndef make_tempfile(mode=""r""):\n    f = tempfile.NamedTemporaryFile(mode=mode, delete=False)\n    yield f\n    f.close()\n    os.remove(f.name)\n\n\n__all__ = [\n    ""get_array_module"",\n    ""fix_random_seed"",\n    ""is_cupy_array"",\n    ""is_numpy_array"",\n    ""set_active_gpu"",\n    ""prefer_gpu"",\n    ""require_gpu"",\n    ""copy_array"",\n    ""to_categorical"",\n    ""get_width"",\n    ""xp2torch"",\n    ""torch2xp"",\n    ""tensorflow2xp"",\n    ""xp2tensorflow"",\n    ""validate_fwd_input_output"",\n    ""DataValidationError"",\n    ""make_tempfile"",\n]\n'"
examples/benchmarks/lstm_tagger.py,0,"b'""""""\nCompare tagging speed for LSTM, using dummy data.\n\nResults on CPU laptop:\n\nPyTorchLSTM.v1:\nPredicted 39017 4.033804399892688 Ys[0] 0.05000001 5.551115e-17\n\nLSTM (NumpyOps):\nPredicted 39018 13.174870599992573 Ys[0] 0.05000001 5.551115e-17\n\nSo PyTorch is 3x faster currently.\n""""""\nfrom typing import List\nimport typer\nimport numpy.random\nfrom timeit import default_timer as timer\nfrom thinc.api import Model, Config, registry, chain, list2padded, with_array\nfrom thinc.api import to_categorical, set_current_ops, JaxOps\nfrom thinc.api import NumpyOps, CupyOps, fix_random_seed, require_gpu\nfrom thinc.types import Array2d, Padded\nimport jax.tree_util\n\nCONFIG = """"""\n[data]\nn_samples = 100000\nn_tags = 20\nn_vocab = 10000\nlength_mean = 40\nlength_variance = 1\n\n[common]\nwidth = 300\n\n[model]\n@layers = ""LSTMTagger.v1""\n\n[model.embed]\n@layers = ""Embed.v1""\nnO = ${common:width}\nnV = ${data:n_vocab}\n\n[model.encode]\n@layers = ""LSTM.v1""\nnO = ${common:width}\nnI = ${common:width}\ndepth = 1\n\n[model.predict]\n@layers = ""Softmax.v1""\nnO = ${data:n_tags}\n""""""\n\n\n@registry.layers(""LSTMTagger.v1"")\ndef build_tagger(\n    embed: Model[Array2d, Array2d],\n    encode: Model[Padded, Padded],\n    predict: Model[Array2d, Array2d],\n) -> Model[List[Array2d], Padded]:\n    model = chain(\n        list2padded(),\n        with_array(embed),\n        encode,\n        # with_array(predict),\n    )\n    model.set_ref(""embed"", embed)\n    model.set_ref(""encode"", encode)\n    model.set_ref(""predict"", model.layers[-1])\n    return model\n\n\ndef get_dummy_data(n_samples, n_tags, n_vocab, length_mean, length_variance):\n    Xs = []\n    Ys = []\n    for _ in range(n_samples):\n        # length = numpy.random.normal(size=1, scale=length_variance) + length_mean\n        length = length_mean\n        shape = (max(1, int(length)),)\n        X = numpy.random.uniform(0, n_vocab - 1, shape)\n        Y = numpy.random.uniform(0, n_tags - 1, shape)\n        assert X.size, length\n        assert Y.size, length\n        Xs.append(X.reshape((-1, 1)).astype(""i""))\n        Ys.append(to_categorical(Y.astype(""i"")))\n    return Xs, Ys\n\n\njax.tree_util.register_pytree_node(\n    Padded,\n    lambda pad: ((pad.data, pad.size_at_t, pad.lengths, pad.indices), None),\n    lambda info, values: Padded(*values),\n)\n\n\ndef run_forward(model, Xs):\n    total = 0.0\n    for batch in Xs:\n        Y = model.predict(batch)\n        total += Y.data.sum()\n    return float(total)\n\n\ndef set_backend(name, gpu_id):\n    global CONFIG\n    if name == ""jax"":\n        set_current_ops(JaxOps())\n        CONFIG = CONFIG.replace(""PyTorch"", """")\n    else:\n        if gpu_id == -1:\n            set_current_ops(NumpyOps())\n        else:\n            set_current_ops(CupyOps())\n        CONFIG = CONFIG.replace(""LSTM.v1"", ""PyTorchLSTM.v1"")\n\n\ndef main(jax: bool = False, pytorch: bool = False, gpu_id: int = -1):\n    global CONFIG\n    fix_random_seed(0)\n    if gpu_id >= 0:\n        require_gpu(gpu_id)\n        print(""Set GPU"", gpu_id)\n    backends = {""jax"": jax, ""pytorch"": pytorch}\n    for name, use_backend in backends.items():\n        if not use_backend:\n            print(f""Skipping {name}"")\n            continue\n        set_backend(name, gpu_id)\n        C = registry.make_from_config(Config().from_str(CONFIG))\n        model = C[""model""]\n        X, Y = get_dummy_data(**C[""data""])\n        print(""Copy to device"")\n        X = [model.ops.asarray(x) for x in X]\n        Y = [model.ops.asarray(y) for y in Y]\n        print(""Begin init"", len(X))\n        model.initialize(X=X[:5])\n        print(""Pre-batch"")\n        n_words = sum(len(x) for x in X)\n        X = [model.layers[0].predict(batch) for batch in model.ops.minibatch(16, X)]\n        model.layers.pop(0)\n        print(""Start"")\n        start_time = timer()\n        end_time = timer()\n        print(name, n_words, end_time - start_time)\n\n\nif __name__ == ""__main__"":\n    typer.run(main)\n'"
thinc/backends/__init__.py,0,"b'import contextlib\nfrom typing import Type\n\nfrom contextvars import ContextVar\n\nfrom .ops import Ops\nfrom .cupy_ops import CupyOps, has_cupy\nfrom .numpy_ops import NumpyOps\nfrom .jax_ops import JaxOps, has_jax, jax_jit\nfrom ._cupy_allocators import cupy_tensorflow_allocator, cupy_pytorch_allocator\nfrom ._param_server import ParamServer\nfrom ..util import assert_tensorflow_installed, assert_pytorch_installed\nfrom ..util import is_cupy_array, is_jax_array\nfrom ..types import OpsNames\n\n\ncontext_ops: ContextVar[NumpyOps] = ContextVar(""context_ops"", default=NumpyOps())\ncontext_Ops: ContextVar[Type[NumpyOps]] = ContextVar(""context_Ops"", default=NumpyOps)\n\n\ndef use_pytorch_for_gpu_memory() -> None:  # pragma: no cover\n    """"""Route GPU memory allocation via PyTorch.\n\n    This is recommended for using PyTorch and cupy together, as otherwise\n    OOM errors can occur when there\'s available memory sitting in the other\n    library\'s pool.\n\n    We\'d like to support routing Tensorflow memory allocation via PyTorch as well\n    (or vice versa), but do not currently have an implementation for it.\n    """"""\n    import cupy.cuda\n\n    assert_pytorch_installed()\n    cupy.cuda.set_allocator(cupy_pytorch_allocator)\n\n\ndef use_tensorflow_for_gpu_memory() -> None:  # pragma: no cover\n    """"""Route GPU memory allocation via TensorFlow.\n\n    This is recommended for using TensorFlow and cupy together, as otherwise\n    OOM errors can occur when there\'s available memory sitting in the other\n    library\'s pool.\n\n    We\'d like to support routing PyTorch memory allocation via Tensorflow as\n    well (or vice versa), but do not currently have an implementation for it.\n    """"""\n    import cupy.cuda\n\n    assert_tensorflow_installed()\n    cupy.cuda.set_allocator(cupy_tensorflow_allocator)\n\n\ndef get_ops(name: OpsNames, **kwargs) -> Ops:\n    """"""Get a backend object.""""""\n    ops = {""numpy"": NumpyOps, ""cupy"": CupyOps, ""jax"": JaxOps}\n    if name not in ops:\n        raise ValueError(f""Invalid backend: {name}"")\n    cls = ops[name]\n    return cls(**kwargs)\n\n\ndef get_array_ops(arr):\n    """"""Return an Ops object to match the array\'s device and backend.""""""\n    if is_cupy_array(arr):\n        return CupyOps()\n    elif is_jax_array(arr):\n        return JaxOps()\n    else:\n        return NumpyOps()\n\n\n@contextlib.contextmanager\ndef use_ops(name: OpsNames, **kwargs):\n    """"""Change the backend to execute on for the scope of the block.""""""\n    current_ops = get_current_ops()\n    set_current_ops(get_ops(name, **kwargs))\n    yield\n    set_current_ops(current_ops)\n\n\ndef get_current_ops() -> Ops:\n    """"""Get the current backend object.""""""\n    return context_ops.get()\n\n\ndef set_current_ops(ops: Ops) -> None:\n    """"""Change the current backend object.""""""\n    context_ops.set(ops)\n\n\n__all__ = [\n    ""set_current_ops"",\n    ""get_current_ops"",\n    ""use_ops"",\n    ""jax_jit"",\n    ""ParamServer"",\n    ""Ops"",\n    ""CupyOps"",\n    ""NumpyOps"",\n    ""JaxOps"",\n    ""has_jax"",\n    ""has_cupy"",\n]\n'"
thinc/backends/_cupy_allocators.py,1,"b'from typing import cast\n\nfrom ..types import ArrayXd\nfrom ..util import tensorflow2xp\n\ntry:\n    import tensorflow\nexcept ImportError:\n    pass\n\ntry:\n    import torch\nexcept ImportError:\n    pass\n\ntry:\n    from cupy.cuda.memory import MemoryPointer\n    from cupy.cuda.memory import UnownedMemory\nexcept ImportError:\n    pass\n\n\ndef cupy_tensorflow_allocator(size_in_bytes: int):\n    """"""Function that can be passed into cupy.cuda.set_allocator, to have cupy\n    allocate memory via TensorFlow. This is important when using the two libraries\n    together, as otherwise OOM errors can occur when there\'s available memory\n    sitting in the other library\'s pool.\n    """"""\n    size_in_bytes = max(1024, size_in_bytes)\n    tensor = tensorflow.zeros((size_in_bytes // 4,), dtype=tensorflow.dtypes.float32)\n    # We convert to cupy via dlpack, so that we can get a memory pointer.\n    cupy_array = cast(ArrayXd, tensorflow2xp(tensor))\n    address = int(cupy_array.data)\n    # cupy has a neat class to help us here. Otherwise it will try to free.\n    memory = UnownedMemory(address, size_in_bytes, cupy_array)\n    # Now return a new memory pointer.\n    return MemoryPointer(memory, 0)\n\n\ndef cupy_pytorch_allocator(size_in_bytes: int):\n    """"""Function that can be passed into cupy.cuda.set_allocator, to have cupy\n    allocate memory via PyTorch. This is important when using the two libraries\n    together, as otherwise OOM errors can occur when there\'s available memory\n    sitting in the other library\'s pool.\n    """"""\n    # Cupy was having trouble with very small allocations?\n    size_in_bytes = max(1024, size_in_bytes)\n    # We use pytorch\'s underlying FloatStorage type to avoid overhead from\n    # creating a whole Tensor.\n    # This turns out to be way faster than making FloatStorage? Maybe\n    # a Python vs C++ thing I guess?\n    torch_tensor = torch.zeros((size_in_bytes // 4,), requires_grad=False)\n    # cupy has a neat class to help us here. Otherwise it will try to free.\n    # I think this is a private API? It\'s not in the types.\n    address = torch_tensor.data_ptr()  # type: ignore\n    memory = UnownedMemory(address, size_in_bytes, torch_tensor)\n    # Now return a new memory pointer.\n    return MemoryPointer(memory, 0)\n'"
thinc/backends/_custom_kernels.py,0,"b'import re\nfrom pathlib import Path\nfrom collections import defaultdict\n\ntry:\n    import cupy\nexcept ImportError:\n    cupy = None\n\n\nkernel_re = re.compile(r""extern \\""C\\"" __global__.+?(?=extern|$)"", re.DOTALL)\nname_re = re.compile(r""(?<=void )\\w+(?=\\()"")\n\n\ndef parse_kernels(src):\n    kernels = {}\n    for kernel in kernel_re.findall(src):\n        name = name_re.search(kernel).group()\n        kernels[name] = kernel\n    return kernels\n\n\ndef compile_kernels(src):\n    if cupy is None:\n        return defaultdict(lambda: None)\n    kernels = parse_kernels(src)\n    return {name: cupy.RawKernel(src, name) for name, src in kernels.items()}\n\n\ndef compile_mmh(src):\n    if cupy is None:\n        return None\n    return cupy.RawKernel(src, ""hash_data"")\n\n\nPWD = Path(__file__).parent\nSRC = (PWD / ""_custom_kernels.cu"").open(""r"", encoding=""utf8"").read()\nKERNELS = compile_kernels(SRC)\n\nMMH_SRC = (PWD / ""_murmur3.cu"").open(""r"", encoding=""utf8"").read()\nKERNELS[""hash""] = compile_mmh(MMH_SRC)\n\nseq2col_kernel = KERNELS[""seq2col""]\nmaxout_kernel = KERNELS[""maxout""]\nmish_kernel = KERNELS[""mish""]\nreduce_sum_kernel = KERNELS[""reduce_sum""]\nreduce_max_kernel = KERNELS[""reduce_max""]\n\nbackprop_seq2col_kernel = KERNELS[""backprop_seq2col""]\nbackprop_maxout_kernel = KERNELS[""backprop_maxout""]\nbackprop_mish_kernel = KERNELS[""backprop_mish""]\nbackprop_reduce_sum_kernel = KERNELS[""backprop_reduce_sum""]\nbackprop_reduce_mean_kernel = KERNELS[""backprop_reduce_mean""]\nbackprop_reduce_max_kernel = KERNELS[""backprop_reduce_max""]\nhash_data_kernel = compile_mmh(MMH_SRC)\n\n\ndef seq2col(X, nW, out=None, threads_per_block=128, num_blocks=128):\n    if out is None:\n        out = cupy.zeros((X.shape[0], X.shape[1] * ((nW * 2) + 1)), dtype=""f"")\n    B = X.shape[0]\n    I = X.shape[1]\n    seq2col_kernel((num_blocks,), (threads_per_block,), (out, X, nW, B, I))\n    return out\n\n\ndef maxout(X, out=None, threads_per_block=128, num_blocks=128):\n    B, I, P = X.shape\n    if out is None:\n        best = cupy.zeros((B, I), dtype=""f"")\n        which = cupy.zeros((B, I), dtype=""i"")\n    else:\n        best, which = None\n    maxout_kernel((num_blocks,), (threads_per_block,), (best, which, X, B, I, P))\n    return best, which\n\n\ndef mish(X, out=None, threshold=5, threads_per_block=128, num_blocks=128):\n    N = X.size\n    if out is None:\n        out = cupy.zeros(X.shape, dtype=""f"")\n    mish_kernel((num_blocks,), (threads_per_block,), (out, X, threshold, N))\n    return out\n\n\ndef reduce_sum(X, lengths, out=None, threads_per_block=128, num_blocks=128):\n    if out is None:\n        out = cupy.zeros((len(lengths), X.shape[1]), dtype=""f"")\n    B = len(lengths)\n    T = X.shape[0]\n    O = X.shape[1]\n    reduce_sum_kernel((num_blocks,), (threads_per_block,), (out, X, lengths, B, T, O))\n    return out\n\n\ndef reduce_mean(X, lengths, out=None, threads_per_block=128, num_blocks=128):\n    if out is None:\n        out = cupy.zeros((len(lengths), X.shape[1]), dtype=""f"")\n    B = len(lengths)\n    T = X.shape[0]\n    O = X.shape[1]\n    reduce_sum_kernel((num_blocks,), (threads_per_block,), (out, X, lengths, B, T, O))\n    # Avoid divide by zero\n    out /= lengths.reshape((-1, 1)) + 1e-10\n    return out\n\n\ndef reduce_max(X, lengths, out=None, threads_per_block=128, num_blocks=128):\n    if out is None:\n        maxes = cupy.zeros((len(lengths), X.shape[1]), dtype=""f"")\n        which = cupy.zeros((len(lengths), X.shape[1]), dtype=""i"")\n    else:\n        maxes, which = out\n    B = len(lengths)\n    T = X.shape[0]\n    O = X.shape[1]\n    reduce_max_kernel(\n        (num_blocks,), (threads_per_block,), (maxes, which, X, lengths, B, T, O)\n    )\n    return maxes, which\n\n\ndef backprop_seq2col(dY, nW, out=None, threads_per_block=128, num_blocks=128):\n    B = dY.shape[0]\n    nF = nW * 2 + 1\n    I = dY.shape[1] // nF\n    if out is None:\n        out = cupy.zeros((B, I), dtype=""f"")\n    backprop_seq2col_kernel((num_blocks,), (threads_per_block,), (out, dY, nW, B, I))\n    return out\n\n\ndef backprop_maxout(dY, which, P, out=None, threads_per_block=128, num_blocks=128):\n    B = dY.shape[0]\n    I = dY.shape[1]\n    if out is None:\n        out = cupy.zeros((B, I, P), dtype=""f"")\n    backprop_maxout_kernel(\n        (num_blocks,), (threads_per_block,), (out, dY, which, B, I, P)\n    )\n    return out\n\n\ndef backprop_mish(dY, X, out=None, threshold=5, threads_per_block=128, num_blocks=128):\n    B = dY.shape[0]\n    I = dY.shape[1]\n    if out is None:\n        out = cupy.zeros((B, I), dtype=""f"")\n    backprop_mish_kernel(\n        (num_blocks,), (threads_per_block,), (out, dY, X, threshold, B * I)\n    )\n    return out\n\n\ndef backprop_reduce_sum(\n    d_sum, lengths, out=None, threads_per_block=128, num_blocks=128\n):\n    B = len(lengths)\n    T = int(lengths.sum())\n    O = d_sum.shape[1]\n    if out is None:\n        out = cupy.zeros((T, O), dtype=""f"")\n\n    backprop_reduce_sum_kernel(\n        (num_blocks,), (threads_per_block,), (out, d_sum, lengths, B, T, O)\n    )\n    return out\n\n\ndef backprop_reduce_mean(\n    d_mean, lengths, out=None, threads_per_block=128, num_blocks=128\n):\n    B = len(lengths)\n    T = int(lengths.sum())\n    O = d_mean.shape[1]\n    if out is None:\n        out = cupy.zeros((T, O), dtype=""f"")\n\n    backprop_reduce_mean_kernel(\n        (num_blocks,), (threads_per_block,), (out, d_mean, lengths, B, T, O)\n    )\n    return out\n\n\ndef backprop_reduce_max(\n    d_maxes, which, lengths, out=None, threads_per_block=128, num_blocks=128\n):\n    B = len(lengths)\n    T = int(lengths.sum())\n    O = d_maxes.shape[1]\n    if out is None:\n        out = cupy.zeros((T, O), dtype=""f"")\n\n    backprop_reduce_max_kernel(\n        (num_blocks,), (threads_per_block,), (out, d_maxes, which, lengths, B, T, O)\n    )\n    return out\n\n\ndef hash(ids, seed, out=None, threads_per_block=128, num_blocks=128):\n    if out is None:\n        out = cupy.zeros((ids.shape[0], 4), dtype=""uint32"")\n    # sizeof(uint32_t) * 4\n    out_size = 4 * 4\n    in_size = 8  # sizeof(uint64_t)\n    # T = ids.shape[0]\n    hash_data_kernel(\n        (num_blocks,),\n        (threads_per_block,),\n        (out, ids, out_size, in_size, ids.shape[0], seed),\n    )\n    return out\n'"
thinc/backends/_param_server.py,0,"b'from typing import Dict, Tuple\n\nfrom ..types import FloatsXd\n\n\nKeyT = Tuple[int, str]\n\n\nclass ParamServer:\n    """"""Serve parameters for a single process.""""""\n\n    _params: Dict[KeyT, FloatsXd] = {}\n    _grads: Dict[KeyT, FloatsXd] = {}\n\n    def __init__(\n        self, params: Dict[KeyT, FloatsXd] = {}, grads: Dict[KeyT, FloatsXd] = {}\n    ):\n        self._params = dict(params)\n        self._grads = dict(grads)\n\n    @property\n    def param_keys(self) -> Tuple[KeyT, ...]:\n        """"""Get the names of registered parameter (including unset).""""""\n        return tuple(self._params.keys())\n\n    @property\n    def grad_keys(self) -> Tuple[KeyT, ...]:\n        return tuple([key for key in self.param_keys if self.has_grad(*key)])\n\n    def has_param(self, model_id: int, name: str) -> bool:\n        return (model_id, name) in self._params\n\n    def has_grad(self, model_id: int, name: str) -> bool:\n        return (model_id, name) in self._grads\n\n    def get_param(self, model_id: int, name: str) -> FloatsXd:\n        return self._params[(model_id, name)]\n\n    def get_grad(self, model_id: int, name: str) -> FloatsXd:\n        return self._grads[(model_id, name)]\n\n    def set_param(self, model_id: int, name: str, value: FloatsXd) -> None:\n        self._params[(model_id, name)] = value\n\n    def set_grad(self, model_id: int, name: str, value: FloatsXd) -> None:\n        self._grads[(model_id, name)] = value\n\n    def inc_grad(self, model_id: int, param_name: str, value: FloatsXd) -> None:\n        if not self.has_grad(model_id, param_name):  # pragma: no cover\n            # Adjustment for Jax\n            if hasattr(value, ""copy""):\n                self._grads[(model_id, param_name)] = value.copy()\n            else:\n                self._grads[(model_id, param_name)] = value\n        else:\n            self._grads[(model_id, param_name)] += value\n'"
thinc/backends/cupy_ops.py,0,"b'import numpy\n\ntry:\n    import cupy\n    import cupyx\n    import cupy.cuda\n    from cupy.cuda.compiler import compile_with_cache  # noqa: F401\n\n    has_cupy = True\n\n    # We no longer have to set up the memory pool, fortunately.\nexcept ImportError:\n    cupy = None\n    cupyx = None\n    has_cupy = False\n\nfrom .ops import Ops\nfrom .numpy_ops import NumpyOps\nfrom . import _custom_kernels\nfrom ..util import get_array_module\nfrom ..types import DeviceTypes\n\n\nclass CupyOps(Ops):\n    name = ""cupy""\n    xp = cupy\n    _xp2 = cupyx\n\n    def __init__(\n        self, device_type: DeviceTypes = ""gpu"", device_id: int = 0, **kwargs\n    ) -> None:\n        self.device_type = device_type\n        self.device_id = device_id\n\n    def to_numpy(self, data):\n        if isinstance(data, numpy.ndarray):\n            return data\n        else:\n            return data.get()\n\n    def gemm(self, x, y, out=None, trans1=False, trans2=False):\n        if isinstance(x, numpy.ndarray) or isinstance(y, numpy.ndarray):\n            raise ValueError(\n                ""Encountered a numpy array when processing with cupy. ""\n                ""Did you call model.ops.asarray on your data?""\n            )\n        if trans1:\n            x = x.T\n        if trans2:\n            y = y.T\n        if out is None:\n            return self.xp.dot(x, y)\n        else:\n            self.xp.dot(x, y, out=out)\n            return out\n\n    def asarray(self, data, dtype=None):\n        # This is sort of frustrating, but we can\'t easily otherwise pass\n        # forward ""unset"".\n        dtype = {""dtype"": dtype} if dtype is not None else {}\n        if isinstance(data, cupy.ndarray):\n            return self.xp.asarray(data, **dtype)\n        elif hasattr(data, ""data_ptr""):\n            # Handles PyTorch Tensors\n            pointer = cupy.cuda.MemoryPointer(data.data_ptr())\n            shape = data.stride()\n            array = self.xp.ndarray(shape, memptr=pointer, **dtype)\n            return array\n        else:\n            result = self.xp.array(data, **dtype)\n            return result\n\n    def maxout(self, X):\n        return _custom_kernels.maxout(X)\n\n    def backprop_maxout(self, dY, which, P):\n        return _custom_kernels.backprop_maxout(dY, which, P)\n\n    def relu(self, X, inplace=False):\n        if not inplace:\n            return X * (X > 0)\n        else:\n            X *= X > 0\n            return X\n\n    def backprop_relu(self, dY, Y, inplace=False):\n        if not inplace:\n            return dY * (Y > 0)\n        dY *= Y > 0\n        return dY\n\n    def mish(self, X, threshold=20.0):\n        return _custom_kernels.mish(X, threshold=threshold, out=None)\n\n    def backprop_mish(self, dY, X, threshold=20.0, out=None):\n        return _custom_kernels.backprop_mish(dY, X, threshold=threshold, out=out)\n\n    def clip_gradient(self, gradient, threshold):\n        grad_norm = cupy.maximum(cupy.linalg.norm(gradient), 1e-12)\n        gradient *= cupy.minimum(threshold, grad_norm) / grad_norm\n        return gradient\n    \n    def seq2col(self, seq, nW):\n        """"""Given an (M, N) sequence of vectors, return an (M, N*(nW*2+1)) sequence.\n        The new sequence is constructed by concatenating nW preceding and succeeding\n        vectors onto each column in the sequence, to extract a window of features.\n        """"""\n        return _custom_kernels.seq2col(seq, nW)\n\n    def backprop_seq2col(self, dY, nW):\n        return _custom_kernels.backprop_seq2col(dY, nW)\n\n    def reduce_mean(self, X, lengths):\n        return _custom_kernels.reduce_mean(X, lengths)\n\n    def backprop_reduce_mean(self, d_means, lengths):\n        return _custom_kernels.backprop_reduce_mean(d_means, lengths)\n\n    def reduce_max(self, X, lengths):\n        return _custom_kernels.reduce_max(X, lengths)\n\n    def backprop_reduce_max(self, d_maxes, which, lengths):\n        return _custom_kernels.backprop_reduce_max(d_maxes, which, lengths)\n\n    def reduce_sum(self, X, lengths):\n        return _custom_kernels.reduce_sum(X, lengths)\n\n    def backprop_reduce_sum(self, d_sums, lengths):\n        return _custom_kernels.backprop_reduce_sum(d_sums, lengths)\n\n    def hash(self, ids, seed):\n        return _custom_kernels.hash(ids, seed)\n\n    def scatter_add(self, table, indices, values):\n        self._xp2.scatter_add(table, indices, values)\n\n    def adam(\n        self, weights, gradient, mom1, mom2, beta1, beta2, eps, learn_rate, mod_rate=1.0\n    ):\n        adam_kernel(\n            gradient,\n            learn_rate,\n            1 - beta1,\n            1 - beta2,\n            eps,\n            weights,\n            mom1,\n            mom2\n        )\n        gradient.fill(0)\n        return weights, gradient, mom1, mom2\n\n    def position_encode(self, N, D, period=10000, out=None):\n        positions = NumpyOps().position_encode(N, D, period=period, out=out)\n        return self.asarray(positions)\n\n\nif cupy is not None:\n    adam_kernel = cupy.ElementwiseKernel(\n        ""T grad, T lr, T one_minus_beta1, T one_minus_beta2, T eps"",\n        ""T param, T m, T v"",\n        """"""m += one_minus_beta1 * (grad - m);\n        v += one_minus_beta2 * (grad * grad - v);\n        param -= lr * m / (sqrt(v) + eps);"""""",\n        ""adam"",\n    )\nelse:\n    adam_kernel = None\n\n'"
thinc/backends/jax_ops.py,0,"b'from typing import Sequence, Optional, List, Tuple, Callable, cast, TypeVar, Union\nfrom typing import overload\nimport numpy\n\nfrom .ops import Ops\nfrom ..types import Floats1d, Floats2d, Floats3d, Ints1d, Ints2d, Ints3d\nfrom ..types import ArrayXd, DTypes, Array3d, DeviceTypes, Padded, List2d, _Floats\n\n\ntry:  # pragma: no cover\n    import jax\n    import jax.ops\n    import jax.random\n    import jax.tree_util\n    from jax.ops import index_update, index\n\n    has_jax = True\nexcept ImportError:  # pragma: no cover\n    has_jax = False\n\n\nArrayT = TypeVar(""ArrayT"", bound=ArrayXd)\nFloatsT = TypeVar(""FloatsT"", bound=_Floats)\n_W = TypeVar(""_W"")\nWrapper = Callable[[_W], _W]\n\n\nclass JaxOps(Ops):\n    name = ""jax""\n    xp = jax.numpy if has_jax else None\n\n    def __init__(\n        self, device_type: DeviceTypes = ""gpu"", device_id: int = 0, **kwargs\n    ) -> None:\n        self.device_type = device_type\n        self.device_id = device_id\n\n    def as_contig(self, data: ArrayT, dtype: Optional[DTypes] = None) -> ArrayT:\n        arr = data if dtype is None else data.astype(dtype)\n        return cast(ArrayT, arr)\n\n    def to_numpy(self, data):  # pragma: no cover\n        if isinstance(data, numpy.ndarray):\n            return data\n        else:\n            return jax.device_get(data)\n\n    def seq2col(self, seq: Floats2d, nW: int) -> Floats2d:\n        """"""Given an (M, N) sequence of vectors, return an (M, N*(nW*2+1))\n        sequence. The new sequence is constructed by concatenating nW preceding\n        and succeeding vectors onto each column in the sequence, to extract a\n        window of features.\n        """"""\n        if nW == 1:\n            return seq2col_one(seq)\n        else:  # pragma: no cover\n            raise ValueError(""Currently only nW=1 supported."")\n\n    def backprop_seq2col(self, dY: Floats2d, nW: int) -> Floats2d:\n        if nW == 1:\n            return backprop_seq2col_one(dY)\n        else:  # pragma: no cover\n            raise ValueError(""Currently only nW=1 supported."")\n\n    def gemm(\n        self,\n        x: Floats2d,\n        y: Floats2d,\n        out: Optional[Floats2d] = None,\n        trans1: bool = False,\n        trans2: bool = False,\n    ) -> Floats2d:\n        if trans1:\n            x = x.T\n        if trans2:\n            y = y.T\n        return self.xp.dot(x, y)\n\n    def affine(self, X: Floats2d, W: Floats2d, b: Floats1d) -> Floats2d:\n        return affine(X, W, b)\n\n    def flatten(\n        self,\n        X: Sequence[ArrayT],\n        dtype: Optional[DTypes] = None,\n        pad: int = 0,\n        ndim_if_empty: int = 2,\n    ) -> ArrayT:\n        if X is None or len(X) == 0:\n            return self.alloc((0,) * ndim_if_empty, dtype=dtype or ""f"")\n        X = [x for x in X if x.size != 0]\n        if int(pad) >= 1:\n            return flatten_with_padding(X, pad)\n        else:\n            result = self.xp.concatenate(X)\n\n        result = self.xp.concatenate(X)\n        if dtype is not None:\n            result = self.xp.asarray(result, dtype=dtype)\n        return result\n\n    def unflatten(self, X: Floats2d, lengths: Ints1d, pad: int = 0) -> List[Floats2d]:\n        if not len(lengths):\n            return []\n        elif not X.size:\n            empty_shape = (0,) + tuple(X.shape[1:])\n            return [self.alloc(empty_shape) for _ in lengths]\n        elif pad == 0:\n            return unflatten_no_padding(X, self.asarray(lengths))\n        else:\n            return unflatten_with_padding(X, self.asarray(lengths), pad)\n\n    def maxout(self, X):\n        return maxout(X)\n\n    def backprop_maxout(self, dY, which, P):\n        return backprop_maxout(dY, which, P)\n\n    def mish(self, X: Floats2d, threshold: float = 20.0) -> Floats2d:\n        return mish(X, threshold)\n\n    def backprop_mish(\n        self,\n        dY: Floats2d,\n        X: Floats2d,\n        threshold: float = 20.0,\n        out: Optional[Floats2d] = None,\n    ) -> Floats2d:\n        return backprop_mish(dY, X, threshold)\n\n    def relu(self, X: Floats2d, inplace: bool = False) -> Floats2d:\n        return relu(X)\n\n    def backprop_relu(\n        self, dY: Floats2d, Y: Floats2d, inplace: bool = False\n    ) -> Floats2d:\n        return backprop_relu(dY, Y)\n\n    def update_averages(\n        self, ema: FloatsT, weights: FloatsT, t: int, max_decay: float = 0.9999\n    ) -> None:\n        decay = (1.0 + t) / (10.0 + t)\n        if decay > max_decay:\n            decay = max_decay\n        return update_averages(ema, weights, decay)\n\n    def adam(\n        self,\n        weights: Floats1d,\n        gradient: Floats1d,\n        mom1: Floats1d,\n        mom2: Floats1d,\n        beta1: float,\n        beta2: float,\n        eps: float,\n        learn_rate: float,\n        mod_rate: float = 1.0,\n    ) -> Tuple[Floats1d, Floats1d, Floats1d, Floats1d]:\n        return adam(\n            weights, gradient, mom1, mom2, beta1, beta2, eps, learn_rate * mod_rate\n        )\n\n    def clip_gradient(self, gradient: FloatsT, threshold: float) -> FloatsT:\n        xp = self.xp\n        grad_norm = xp.linalg.norm(gradient)\n        if grad_norm >= threshold:\n            gradient = gradient * (threshold / grad_norm)\n        return gradient\n\n    def logloss(self, y_true: FloatsT, y_pred: FloatsT):\n        return logloss\n\n    def reduce_sum(self, X: Floats2d, lengths: Ints1d) -> Floats2d:\n        return reduce_sum(X, lengths)\n\n    def reduce_mean(self, X: Floats2d, lengths: Ints1d) -> Floats2d:\n        return reduce_mean(X, lengths)\n\n    def reduce_max(self, X: Floats2d, lengths: Ints1d) -> Tuple[Floats2d, Ints2d]:\n        return reduce_max(X, lengths)\n\n    def backprop_reduce_sum(self, d_sums: Floats2d, lengths: Ints1d) -> Floats2d:\n        return backprop_reduce_sum(d_sums, lengths)\n\n    def backprop_reduce_mean(self, d_means: Floats2d, lengths: Ints1d) -> Floats2d:\n        return backprop_reduce_mean(d_means, lengths)\n\n    def backprop_reduce_max(\n        self, d_maxes: Floats2d, which: Ints2d, lengths: Ints1d\n    ) -> Floats2d:\n        return backprop_reduce_max(d_maxes, which, lengths)\n\n    @overload\n    def pad(self, seqs: List[Ints2d], round_to=1) -> Ints3d:\n        ...\n\n    @overload  # noqa: F811\n    def pad(self, seqs: List[Floats2d], round_to=1) -> Floats3d:\n        ...\n\n    def pad(  # noqa: F811\n        self, seqs: Union[List[Ints2d], List[Floats2d]], round_to=1\n    ) -> Array3d:\n        if not seqs:\n            raise ValueError(""Cannot pad empty sequence"")\n        if len(set(seq.ndim for seq in seqs)) != 1:\n            raise ValueError(""Cannot pad sequences with different ndims"")\n        if len(set(seq.dtype for seq in seqs)) != 1:\n            raise ValueError(""Cannot pad sequences with different dtypes"")\n        if len(set(seq.shape[1:] for seq in seqs)) != 1:\n            raise ValueError(""Cannot pad sequences that differ on other dimensions"")\n        # Find the maximum dimension along each axis. That\'s what we\'ll pad to.\n        length = max(len(seq) for seq in seqs)\n        # Round the length\n        length = (length + (round_to - 1)) // round_to * round_to\n        final_shape = (len(seqs), length) + seqs[0].shape[1:]\n        output: Array3d = self.alloc(final_shape, dtype=seqs[0].dtype)\n        for i, arr in enumerate(seqs):\n            output[i, : arr.shape[0]] = arr  # type: ignore\n        return output\n\n    def list2padded(self, seqs: List[Floats2d]) -> Padded:\n        """"""Pack a sequence of 2d arrays into a Padded datatype.""""""\n        # I don\'t know why this is so slow, but it\'s *terrible*. Try going\n        # via numpy?\n        from .numpy_ops import NumpyOps\n\n        numpy_ops = NumpyOps()\n        numpy_seqs = [numpy_ops.asarray(seq) for seq in seqs]\n        numpy_padded = numpy_ops.list2padded(numpy_seqs)\n        return Padded(\n            numpy_padded.data,\n            self.asarray1i(numpy_padded.size_at_t),\n            self.asarray1i(numpy_padded.lengths),\n            self.asarray1i(numpy_padded.indices),\n        )\n\n    def padded2list(self, padded: Padded) -> List2d:\n        indices = padded.indices\n        data = padded.data\n        lengths = padded.lengths\n        unpadded = [None] * len(lengths)\n        data = self.as_contig(data.transpose((1, 0, 2)))\n        for i in range(data.shape[0]):\n            index_update(unpadded, index[indices[i]], data[i, : lengths[i]])\n        return cast(List2d, unpadded)\n\n    def sigmoid(self, X: FloatsT, *, inplace: bool = False) -> FloatsT:\n        return sigmoid(X)\n\n    def dsigmoid(self, Y: FloatsT, *, inplace: bool = False) -> FloatsT:\n        return Y * (1.0 - Y)\n\n    def dtanh(self, Y: FloatsT, *, inplace: bool = False) -> FloatsT:\n        if inplace:\n            Y **= 2\n            Y *= -1.0\n            Y += 1.0\n            return Y\n        else:\n            return 1 - Y ** 2\n\n    def softmax(self, x: FloatsT, *, inplace: bool = False, axis: int = -1) -> FloatsT:\n        maxes = self.xp.max(x, axis=axis, keepdims=True)\n        shifted = x - maxes\n        new_x = self.xp.exp(shifted)\n        new_x /= new_x.sum(axis=axis, keepdims=True)\n        return new_x\n\n    def softmax_sequences(\n        self, Xs: Floats2d, lengths: Ints1d, *, inplace: bool = False, axis: int = -1\n    ) -> Floats2d:\n        if Xs.ndim >= 3:\n            err = f""Softmax currently only supports 2d. Got: {Xs.ndim}""\n            raise NotImplementedError(err)\n        # This loses almost no fidelity, and helps the numerical stability.\n        Xs = self.xp.clip(Xs, -20.0, 20.0)\n        new_x = self.xp.exp(Xs)\n        summed = self.backprop_reduce_sum(self.reduce_sum(new_x, lengths), lengths)\n        new_x /= summed\n        return new_x\n\n    def backprop_softmax(self, Y: FloatsT, dY: FloatsT, *, axis: int = -1) -> FloatsT:\n        dX = Y * dY\n        dX -= Y * dX.sum(axis=axis, keepdims=True)\n        return dX\n\n    def backprop_softmax_sequences(\n        self, dY: Floats2d, Y: Floats2d, lengths: Ints1d\n    ) -> Floats2d:\n        dX = Y * dY\n        sum_dX = self.backprop_reduce_sum(self.reduce_sum(dX, lengths), lengths)\n        dX -= Y * sum_dX\n        return dX\n\n    def recurrent_lstm(\n        self,\n        W: Floats2d,\n        b: Floats1d,\n        h_init: Floats1d,\n        c_init: Floats1d,\n        inputs: Floats3d,\n        is_train: bool = True,\n    ) -> Tuple[Floats3d, Tuple[Floats3d, Floats3d, Floats3d]]:\n        Y, (G, C, S) = recurrent_lstm_forward(W, b, h_init, c_init, inputs, is_train)\n        return Y, (G, C, S)\n\n    def backprop_recurrent_lstm(\n        self,\n        dY: Floats3d,\n        fwd_state: Tuple[Floats3d, Floats3d, Floats3d],\n        params: Tuple[Floats2d, Floats1d],\n    ) -> Tuple[Floats3d, Tuple[Floats2d, Floats1d, Floats1d, Floats1d]]:\n        dCt = self.alloc2f(dY.shape[1], dY.shape[2])\n        dW, db, dX, dY, dC0 = backprop_recurrent_lstm(dY, dCt, (fwd_state, params))\n        return dX, (dW, db, dY[0].sum(axis=0), dC0.sum(axis=0))\n\n    def insert_into(self, shape, Xs):\n        output = self.alloc(shape, dtype=Xs[0].dtype)\n        for i, x in enumerate(Xs):\n            output = index_update(output, index[i, : x.shape[0]], x)\n        return output\n\n\nclass JaxRandom:\n    """"""Perform randomization functions for Jax.""""""\n\n    def shuffle(self, array):\n        key = jax.random.PRNGKey(0)\n        return jax.random.shuffle(key, array)\n\n    def uniform(self, minval, maxval, shape):\n        key = jax.random.PRNGKey(0)\n        return jax.random.uniform(key, minval=0.0, maxval=1.0, shape=shape, dtype=""f"")\n\n    def normal(self, scale, size):\n        key = jax.random.PRNGKey(0)\n        return jax.random.normal(key, shape=(size,)).astype(""float32"")\n\n\ndef jax_jit(*static_args: int) -> Wrapper:\n    """"""Apply jax.jit to the decorated function, if Jax is installed. Otherwise,\n    do nothing. The decorator takes a variable-length sequence of positional\n    arguments, which are passed as a tuple to jax.jit as the \'static_argnums\'\n    keyword argument.\n    """"""\n\n    def wrapper(func: Callable) -> Callable:\n        return jax.jit(func, static_argnums=static_args) if has_jax else func\n\n    return wrapper\n\n\n@jax_jit()\ndef seq2col_one(seq):\n    # This is a test implementation that only supports nW=1\n    nW = 1\n    B = seq.shape[0]\n    I = seq.shape[1]\n    cols: Array3d = jax.numpy.zeros((B, (nW * 2 + 1), I))\n    # Copy left contexts. The last words aren\'t the left-context for anything.\n    cols = index_update(cols, index[nW:, :nW], seq[:-nW].reshape((-1, nW, I)))\n    cols = index_update(cols, index[:, nW], seq)\n    cols = index_update(cols, index[:-nW, nW + 1 :], seq[nW:].reshape((-1, nW, I)))\n    return cols.reshape((B, I * (2 * nW + 1)))\n\n\n@jax_jit()\ndef backprop_seq2col_one(dY):\n    xp = jax.numpy\n    nW = 1\n    nF = nW * 2 + 1\n    B = dY.shape[0]\n    I = dY.shape[1] // nF\n    dX = xp.zeros((B, I), dtype=""f"")\n    dY = dY.reshape((B, nF, I))\n    dX = index_update(dX, index[:-nW], dX[:-nW] + dY[nW:, :nW].reshape((-1, I)))\n    dX += dY[:, nW]\n    dX = index_update(dX, index[nW:], dX[nW:] + dY[:-nW, nW + 1 :].reshape((-1, I)))\n    return dX\n\n\n@jax_jit()\ndef affine(X, W, b):\n    return X @ W.T + b\n\n\n@jax_jit()\ndef relu(X):\n    return X * (X > 0)\n\n\n@jax_jit()\ndef backprop_relu(delta, signal_out):\n    return delta * (signal_out > 0)\n\n\n@jax_jit(1)\ndef flatten_with_padding(X, pad):\n    xp = jax.numpy\n    padded = []\n    for x in X:\n        padded.append(xp.zeros((pad,) + x.shape[1:], dtype=x.dtype))\n        padded.append(x)\n    padded.append(xp.zeros((pad,) + x.shape[1:], dtype=x.dtype))\n    return xp.concatenate(padded)\n\n\ndef unflatten_no_padding(X, lengths):\n    # Couldn\'t get the JIT version right here yet.\n    start = 0\n    unflat = []\n    for length in lengths:\n        unflat.append(X[start : start + length])\n        start += length\n    return unflat\n\n\ndef unflatten_with_padding(X, lengths, pad):\n    # Couldn\'t get the JIT version right here yet.\n    unflat = []\n    for length in lengths:\n        X = X[pad:]\n        unflat.append(X[:length])\n        X = X[length:]\n    X = X[pad:]\n    return unflat\n\n\n@jax_jit()\ndef maxout(X):\n    which = X.argmax(axis=-1)\n    return X.max(axis=-1), which\n\n\n@jax_jit(2)\ndef backprop_maxout(dY, which, P):\n    dX = jax.numpy.zeros((dY.shape[0], dY.shape[1], P), dtype=""float32"")\n    for b in range(dY.shape[0]):\n        for o in range(dY.shape[1]):\n            dX = index_update(dX, index[b, o, which[b, o]], dY[b, o])\n    return dX\n\n\n@jax_jit()\ndef adam(\n    weights: Floats1d,\n    gradient: Floats1d,\n    mom1: Floats1d,\n    mom2: Floats1d,\n    beta1: float,\n    beta2: float,\n    eps: float,\n    learn_rate: float,\n) -> Tuple[Floats1d, Floats1d, Floats1d, Floats1d]:\n    mom1 *= beta1\n    mom2 *= beta2\n    mom1 += gradient * (1.0 - beta1)\n    mom2 += gradient * gradient * (1.0 - beta2)\n    # Here we assume learn rate is calculated by the caller.\n    # cdef weight_t a_t = learn_rate * sqrt(1-beta2**hp.t) / (1-beta1**hp.t);\n    weights -= learn_rate * mom1 / (1.0 + eps)\n    return weights, gradient, mom1, mom2\n\n\n@jax_jit()\ndef update_averages(ema, weights, decay):\n    return ema - (1 - decay) * (ema - weights)\n\n\n@jax_jit()\ndef logloss(y_true: ArrayXd, y_pred: ArrayXd):\n    log_yp = jax.numpy.log(y_pred + 1e-8)\n    loss = (y_true * log_yp) + (1 - y_true) * jax.numpy.log((1 - y_pred) + 1e-8)\n    return -loss\n\n\n@jax_jit()\ndef reduce_sum(X: Floats2d, lengths: Ints1d) -> Floats2d:\n    Y = jax.numpy.zeros((lengths.shape[0], X.shape[1]), dtype=""f"")\n    start = 0\n    for i, length in enumerate(lengths):\n        Y = jax.ops.index_update(\n            Y, jax.ops.index[i], X[start : start + length].sum(axis=0)\n        )\n        start += length\n    return Y\n\n\n@jax_jit()\ndef reduce_mean(X: Floats2d, lengths: Ints1d) -> Floats2d:\n    Y = jax.numpy.zeros((lengths.shape[0], X.shape[1]), dtype=""f"")\n    start = 0\n    for i, length in enumerate(lengths):\n        Y = jax.ops.index_update(\n            Y, jax.ops.index[i], X[start : start + length].mean(axis=0)\n        )\n        start += length\n    return Y\n\n\n@jax_jit()\ndef reduce_max(self, X: Floats2d, lengths: Ints1d) -> Floats2d:\n    Y = jax.numpy.zeros((lengths.shape[0], X.shape[1]), dtype=""f"")\n    start = 0\n    for i, length in enumerate(lengths):\n        Y = jax.ops.index_update(\n            Y, jax.ops.index[i], X[start : start + length].max(axis=0)\n        )\n        start += length\n    return Y\n\n\n@jax_jit()\ndef backprop_reduce_sum(self, d_sums: Floats2d, lengths: Ints1d) -> Floats2d:\n    dX = self.alloc2f(lengths.sum(), d_sums.shape[1])\n    start = 0\n    for i, length in enumerate(lengths):\n        dX[start : start + length] = d_sums[i]\n        start += length\n    return dX\n\n\n@jax_jit()\ndef backprop_reduce_mean(self, d_means: Floats2d, lengths: Ints1d) -> Floats2d:\n    dX = self.alloc2f(lengths.sum(), d_means.shape[1])\n    start = 0\n    for i, length in enumerate(lengths):\n        dX[start : start + length] = d_means[i] / length\n        start += length\n    return dX\n\n\n@jax_jit()\ndef backprop_reduce_max(d_maxes: Floats2d, which: Ints2d, lengths: Ints1d) -> Floats2d:\n    dX = numpy.jax.zeros((lengths.sum(), d_maxes.shape[1]))\n    start = 0\n    for i, length in enumerate(lengths):\n        dX = index_update(dX, index[start : start + length, which[i]], d_maxes[i])\n        start += length\n    return dX\n\n\n@jax_jit(1)\ndef mish(X: Floats2d, threshold: float = 20.0) -> Floats2d:\n    Y = X * jax.numpy.tanh(jax.numpy.log(1.0 + jax.numpy.exp(X)))\n    return jax.numpy.where(X >= threshold, X, Y)\n\n\n@jax_jit(2)\ndef backprop_mish(X: Floats2d, dY: Floats2d, threshold: float = 20.0) -> Floats2d:\n    xp = jax.numpy\n    exp_x = xp.exp(X)\n    exp_2x = xp.exp(2 * X)\n    exp_3x = xp.exp(3 * X)\n    omega = (4.0 * (X + 1)) + (4 * exp_2x) + exp_3x + exp_x * (4.0 * X + 6)\n    delta = 2.0 * exp_x + exp_2x + 2.0\n    dX = dY * ((exp_x * omega) / (delta * delta))\n    # Gradient when above threshold will ignore softplus.\n    return jax.numpy.where(X >= threshold, dY, dX)\n\n\n@jax_jit()\ndef sigmoid(X: ArrayT) -> ArrayT:\n    return 1.0 / (1.0 + jax.numpy.exp(-X))\n\n\n@jax_jit()\ndef dsigmoid(Y: ArrayT) -> ArrayT:\n    return Y * (1.0 - Y)\n\n\n@jax_jit()\ndef dtanh(Y: ArrayT) -> ArrayT:\n    return 1 - Y ** 2\n\n\n@jax_jit(1)\ndef softmax(X: ArrayT, axis: int) -> ArrayT:\n    xp = jax.numpy\n    maxes = xp.max(X, axis=axis, keepdims=True)\n    shifted = X - maxes\n    new_x = xp.exp(shifted)\n    new_x /= new_x.sum(axis=axis, keepdims=True)\n    return new_x\n\n\n@jax_jit(2)\ndef softmax_sequences(Xs: Floats2d, lengths: Ints1d, axis: int) -> Floats2d:\n    xp = jax.numpy\n    # This loses almost no fidelity, and helps the numerical stability.\n    Xs = xp.clip(Xs, -20.0, 20.0)\n    new_x = xp.exp(Xs)\n    summed = backprop_reduce_sum(reduce_sum(new_x, lengths), lengths)\n    new_x /= summed\n    return new_x\n\n\n@jax_jit(2)\ndef backprop_softmax(Y: ArrayXd, dY: ArrayXd, axis: int) -> ArrayXd:\n    dX = Y * dY\n    dX -= Y * dX.sum(axis=axis, keepdims=True)\n    return dX\n\n\n@jax_jit(2)\ndef backprop_softmax_sequences(dY: Floats2d, Y: Floats2d, lengths: Ints1d) -> Floats2d:\n    dX = Y * dY\n    sum_dX = backprop_reduce_sum(reduce_sum(dX, lengths), lengths)\n    dX -= Y * sum_dX\n    return dX\n\n\n""""""\nLSTM Notation (kind of involved, but made it a lot easier to write)\n\nX: Inputs\nY: Outputs (aka hiddens)\nC: Cells\nG: Gates (Output of non-linearity, i.e. lstm_gates(X @ W.T)\nA: Activations (X @ W.T, before non-linearity)\n\nImagine we have the input:\nbatch = [\n    [""apple"", ""banana"", ""cantaloupe"", ""date"", ""elderberry""],\n    [""aardvark"", ""bat"", ""capybara"", ""dingo"", ""elephant""]\n]\n\nThe input variable X will have one vector per word, so X[0, 1] will be banana\'s\nvector, X[0, 1, 0] will be a float, the first element of that vector.\n\nWe\'re computing an output variable Y of shape (nL, nB, nO), so that Y[0, 1] is\nthe output variable of banana.\n\nA problem with variables for RNNs is keeping the timesteps straight. It\'s hard\nto distinguish the current, previous, and next timesteps. To solve this problem,\nwe follow the convention that **we are at timestep 3**.\n\nAdditionally, the variables for Y and C are offset by one, as the 0th elements\nhave the initial hiddens and initial cells. So:\n\n    t=3\n    Xt3: The input vectors for \'dingo\' and \'date\', i.e. X[t]\n    Yt3: The output vectors for \'dingo\' and \'date\', i.e. Y[t+1] (Y is offset.)\n    Ct2: The cells calculated at \'c...\', that are the input for \'d...\'\n    Ct3: The cells calculated at \'d...\', that are the input for \'e...\'\n    At3: The activations at \'d...\'\n    Gt3: The gates at \'d...\'\n""""""\n\n\n@jax_jit(5)\ndef recurrent_lstm_forward(W, b, c_init, h_init, X, is_train):\n    xp = jax.numpy\n    nL, nB, nI = X.shape\n    nO = h_init.shape[0]\n    # Preallocate these so we can pass them through for loop.\n    Y = xp.zeros((nL + 1, nB, nO), dtype=""f"")\n    if is_train:\n        G = xp.zeros((nL, nB, nO * 4), dtype=""f"")\n        C = xp.zeros((nL + 1, nB, nO), dtype=""f"")\n        # Set initial hidden and cell states. The Y and C will be shifted 1,\n        # so that we can have fewer arrays.\n        Y = index_update(Y, index[0], h_init)\n        C = index_update(C, index[0], c_init)\n    else:\n        G = xp.zeros((nB, nO * 4), dtype=""f"")\n        C = xp.zeros((nB, nO), dtype=""f"")\n        C += c_init\n    state = ((W, b, X), (Y, C, G))\n    state = jax.lax.fori_loop(0, X.shape[0], lstm_stepper_forward, state)\n    (W, b, X), (Y, C, G) = state\n    # Recall that Y and C are both offset by 1. Y[1] is the output for\n    # X[1], while Y[0] was used as an input for Y[1]. We use\n    # the S values to backprop the weights, so we need X the previous Ys.\n    if is_train:\n        S = xp.concatenate((X, Y[:-1]), axis=-1)\n        return Y[1:], (G, C, S)\n    else:\n        null = xp.zeros((0, 0, 0))\n        return Y, (null, null, null)\n\n\n@jax_jit()\ndef lstm_stepper_forward(t, state):\n    (W, b, X), (Y, C, G) = state\n    is_train = G.ndim >= 3\n    # Get the activations for this timestep.\n    At3 = lstm_weights_forward(X[t], Y[t], W, b)\n    # The offsets here are a bit unintuitive, because Y and C are 1-offset.\n    Ct2 = C[t] if is_train else C\n    Yt3, Ct3, Gt3 = lstm_gates_forward(At3, Ct2)\n    Y = index_update(Y, index[t + 1], Yt3)\n    if is_train:\n        C = index_update(C, index[t + 1], Ct3)\n        G = index_update(G, index[t], Gt3)\n        return (W, b, X), (Y, C, G)\n    else:\n        return (W, b, X), (Y, Ct3, Gt3)\n\n\n@jax_jit()\ndef backprop_recurrent_lstm(dY, dCt, fwd_vars):\n    (G, C, S), (W, b) = fwd_vars\n    xp = jax.numpy\n    nL = S.shape[0]\n    nB = dY.shape[1]\n    nI = S.shape[2] - dY.shape[2]\n    # Preallocate these so we can pass them through for loop.\n    dX = xp.zeros((nL, nB, nI), dtype=""f"")\n    dW = xp.zeros(W.shape, dtype=""f"")\n    db = xp.zeros(b.shape, dtype=""f"")\n    state = (\n        (dW, db, dX),  # The gradi-outs (Write-only)\n        (dY, dCt),  # The gradi-ins  (Read and write)\n        (G, C, S),  # Forward state  (Read-only)\n        (W, b),  # Params         (Read-only)\n    )\n    state = jax.lax.fori_loop(0, nL, backprop_lstm_stepper, state)\n    (dW, db, dX), (dY, dCt), (G, C, S), (W, b) = state\n    return dW, db, dX, dY, dCt\n\n\n@jax_jit()\ndef backprop_lstm_stepper(i, state):\n    (dW, db, dX), (dY, dCt3), (G, C, S), (W, b) = state\n    t = S.shape[0] - i\n    # Recall, we\'re at step 3, Y and C are offset by 1. See above.\n    dYt3 = dY[t + 1]\n    Ct3 = C[t + 1]\n    St3 = S[t]\n    Gt3 = G[t]\n    Ct2 = C[t]\n    dAt3, dCt2 = backprop_lstm_gates(dCt3, dYt3, Gt3, Ct3, Ct2)\n    dXt3, dYt2, dW3, db3 = backprop_lstm_weights(dAt3, (St3, W, b))\n    dX = index_update(dX, index[t], dXt3)\n    dY = index_update(dY, index[t], dYt2)\n    return (dW + dW3, db + db3, dX), (dY, dCt2), (G, C, S), (W, b)\n\n\n@jax_jit()\ndef lstm_weights_forward(Xt3, Yt2, W, b):\n    xp = jax.numpy\n    St3 = xp.hstack((Xt3, Yt2))\n    At3 = St3 @ W.T + b\n    return At3\n\n\n@jax_jit()\ndef backprop_lstm_weights(dAt3, fwd_state):\n    St3, W, b = fwd_state\n    dW = dAt3.T @ St3\n    db = dAt3.sum(axis=0)\n    dSt3 = dAt3 @ W\n    nO = W.shape[0] // 4\n    nI = St3.shape[1] - nO\n    dXt3 = dSt3[:, :nI]\n    dYt2 = dSt3[:, nI:]\n    return dXt3, dYt2, dW, db\n\n\n@jax_jit()\ndef lstm_gates_forward(At3, Ct2):\n    xp = jax.numpy\n    # hf, hi, ho, hc: Forget, input, output, cell gates.\n    At3_hf, At3_hi, At3_ho, At3_hc = xp.split(At3, 4, axis=-1)\n    # Number the steps here, to refer back for backward pass.\n    # 1. Activations\n    hf = sigmoid(At3_hf)  # 1a\n    hi = sigmoid(At3_hi)  # 1b\n    ho = sigmoid(At3_ho)  # 1c\n    hc = xp.tanh(At3_hc)  # 1d\n\n    Ct3 = hf * Ct2  # 2a\n    Ct3 += hi * hc  # 2b\n    tanhCt3 = xp.tanh(Ct3)  # 3a\n    Yt3 = tanhCt3 * ho  # 3b\n    # We don\'t need the gradient for this, it\'s just for backprop calculation.\n    Gt3 = xp.concatenate((hf, hi, ho, hc), axis=-1)\n    return Yt3, Ct3, Gt3\n\n\n@jax_jit()\ndef backprop_lstm_gates(\n    dYt3: Floats2d, dCt3: Floats2d, Gt3: Floats2d, Ct3: Floats2d, Ct2: Floats2d\n) -> Tuple[Floats3d, Floats2d]:\n    # See above for notation. Step numbering refers to forward_lstm_gates\n    xp = jax.numpy\n    hf, hi, ho, hc = xp.split(Gt3, 4, axis=-1)\n    tanhCt3 = xp.tanh(Ct3)\n    # 3b: Yt3 = tanhCt3 * ho\n    d_ho = dYt3 * tanhCt3\n    d_tanhCt3 = dYt3 * ho\n    # 3a: tanhCt3 = tanh(Ct3)\n    dCt3 += d_tanhCt3 * dtanh(tanhCt3)\n    # 2b: Ct3 += hi * hc\n    d_hi = dCt3 * hc\n    d_hc = dCt3 * hi\n    # 2a: Ct3 = hf * Ct2\n    d_hf = dCt3 * Ct2\n    dCt2 = dCt3 * hf\n    d_At3_hc = d_hc * dtanh(hc)  # 1d\n    d_At3_ho = d_ho * dsigmoid(ho)  # 1c\n    d_At3_hi = d_hi * dsigmoid(hi)  # 1b\n    d_At3_hf = d_hf * dsigmoid(hf)  # 1a\n    dAt3 = xp.concatenate((d_At3_hf, d_At3_hi, d_At3_ho, d_At3_hc), axis=-1)\n    return dAt3, dCt2\n\n\nif has_jax:\n    JaxOps.xp.random = JaxRandom()\n    JaxOps.xp.testing = numpy.testing\n    jax.tree_util.register_pytree_node(\n        JaxOps, lambda ops: ([], None), lambda info, values: JaxOps()\n    )\n\n__all__ = [""JaxOps"", ""has_jax"", ""jax_jit""]\n'"
thinc/backends/ops.py,0,"b'from typing import Optional, List, Tuple, Sequence, Union, cast, TypeVar\nfrom typing import Iterator, overload\nimport numpy\nimport itertools\n\nfrom ..types import Xp, Shape, DTypes, DTypesInt, DTypesFloat, List2d, ArrayXd\nfrom ..types import Array2d, Array3d, Floats1d, Floats2d, Floats3d, Floats4d\nfrom ..types import FloatsXd, Ints1d, Ints2d, Ints3d, Ints4d, IntsXd, _Floats\nfrom ..types import DeviceTypes, Generator, Padded, Batchable, SizedGenerator\nfrom ..util import get_array_module, is_xp_array, to_numpy\n\n\nArrayT = TypeVar(""ArrayT"", bound=ArrayXd)\nFloatsT = TypeVar(""FloatsT"", bound=_Floats)\n\n\nclass Ops:\n    name: str = ""base""\n    xp: Xp = numpy\n\n    def __init__(\n        self, device_type: DeviceTypes = ""cpu"", device_id: int = -1, **kwargs\n    ) -> None:\n        self.device_type = device_type\n        self.device_id = device_id\n\n    def to_numpy(self, data):  # pragma: no cover\n        if isinstance(data, numpy.ndarray):\n            return data\n        else:\n            raise ValueError(""Cannot convert non-numpy from base Ops class"")\n\n    def minibatch(\n        self,\n        size: Union[int, Generator],\n        sequence: Batchable,\n        *,\n        shuffle: bool = False,\n        buffer: int = 1,\n    ) -> SizedGenerator:\n        """"""Iterate slices from a sequence, optionally shuffled. Slices\n        may be either views or copies of the underlying data.\n\n        The `size` argument may be either an integer, or a sequence of integers.\n        If a sequence, a new size is drawn before every output.\n\n        If shuffle is True, shuffled batches are produced by first generating\n        an index array, shuffling it, and then using it to slice into the\n        sequence.\n\n        An internal queue of `buffer` items is accumulated before being each\n        output. Buffering is useful for some devices, to allow the\n        network to run asynchronously without blocking on every batch.\n        """"""\n        if not hasattr(sequence, ""__len__""):\n            err = f""Can\'t minibatch data. Expected sequence, got {type(sequence)}""\n            raise ValueError(err)\n        sizes = self._get_batch_sizes(\n            len(sequence), itertools.repeat(size) if isinstance(size, int) else size\n        )\n        indices = numpy.arange(len(sequence))\n\n        # This is a bit convoluted, but it\'s a time where convenience makes\n        # trickery worthwhile: instead of being an actual generator, we\n        # return our SizedGenerator object, which provides a __len__.\n        def _iter_items():\n            if shuffle:\n                numpy.random.shuffle(indices)\n            queue = []\n            i = 0\n            for size in sizes:\n                queue.append(self._get_batch(sequence, indices[i : i + size]))\n                if len(queue) >= buffer:\n                    yield from queue\n                    queue = []\n                i += size\n            yield from queue\n\n        return SizedGenerator(_iter_items, len(sizes))\n\n    def multibatch(\n        self,\n        size: Union[int, Generator],\n        sequence: Batchable,\n        *others: Batchable,\n        shuffle: bool = False,\n        buffer: int = 1,\n    ) -> SizedGenerator:\n        """"""Minibatch one or more sequences of data, and yield\n        lists with one batch per sequence. See ops.minibatch.\n        """"""\n        # You\'d think we could just do this by calling into minibatch and zip...\n        # But the shuffling makes it really hard.\n        sequences = (sequence,) + tuple(others)\n        if not all(hasattr(seq, ""__len__"") for seq in sequences):\n            values = "", "".join([f""{type(seq)}"" for seq in sequences])\n            err = f""Can\'t multibatch data. Expected sequences, got {values}""\n            raise ValueError(err)\n        sizes = self._get_batch_sizes(\n            len(sequence), itertools.repeat(size) if isinstance(size, int) else size\n        )\n        indices = numpy.arange(len(sequence))\n\n        def _iter_items():\n            if shuffle:\n                numpy.random.shuffle(indices)\n            queue = []\n            i = 0\n            for size in sizes:\n                idx_batch = indices[i : i + size]\n                queue.append([])\n                for sequence in sequences:\n                    queue[-1].append(self._get_batch(sequence, idx_batch))\n                if len(queue) >= buffer:\n                    yield from queue\n                    queue = []\n                i += size\n            yield from queue\n\n        return SizedGenerator(_iter_items, len(sizes))\n\n    def _get_batch(self, sequence, indices):\n        if isinstance(sequence, list):\n            subseq = [sequence[i] for i in indices]\n        elif isinstance(sequence, tuple):\n            subseq = tuple(sequence[i] for i in indices)  # type: ignore\n        else:\n            subseq = sequence[indices]  # type: ignore\n        if is_xp_array(subseq):\n            subseq = self.as_contig(\n                cast(ArrayXd, self.xp.asarray(subseq))\n            )  # type: ignore\n        return subseq\n\n    def _get_batch_sizes(self, length: int, sizes: Iterator[int]):\n        output = []\n        i = 0\n        while i < length:\n            output.append(next(sizes))\n            i += output[-1]\n        return output\n\n    def seq2col(self, seq: Floats2d, nW: int) -> Floats2d:\n        """"""Given an (M, N) sequence of vectors, return an (M, N*(nW*2+1))\n        sequence. The new sequence is constructed by concatenating nW preceding\n        and succeeding vectors onto each column in the sequence, to extract a\n        window of features.\n        """"""\n        # This is a test implementation that only supports nW=1\n        assert nW == 1\n        B = seq.shape[0]\n        I = seq.shape[1]\n        cols = self.alloc3f(B, (nW * 2 + 1), I)\n        # Copy left contexts. The last words aren\'t the left-context for anything.\n        cols[nW:, :nW] = self.reshape3f(seq[:-nW], -1, nW, I)\n        cols[:, nW] = seq\n        cols[:-nW, nW + 1 :] = self.reshape3f(seq[nW:], -1, nW, I)\n        return self.reshape2f(cols, B, I * (2 * nW + 1))\n\n    def backprop_seq2col(self, dY: Floats2d, nW: int) -> Floats2d:\n        """"""The reverse/backward operation of the `seq2col` function: calculate\n        the gradient of the original `(M, N)` sequence, as a function of the\n        gradient of the output `(M, N*(nW*2+1))` sequence.\n        """"""\n        # This is a test implementation that only supports nW=1\n        assert nW == 1\n        nF = nW * 2 + 1\n        B = dY.shape[0]\n        I = dY.shape[1] // nF\n        # Having trouble getting the kernel to work...\n        dX = self.alloc2f(B, I)\n        dY3d = self.reshape3f(dY, B, nF, I)\n        dX[:-nW] += self.reshape2f(dY3d[nW:, :nW], -1, I)\n        dX += dY3d[:, nW]\n        dX[nW:] += self.reshape2f(dY3d[:-nW, nW + 1 :], -1, I)\n        return dX\n\n    def gemm(\n        self,\n        x: Floats2d,\n        y: Floats2d,\n        out: Optional[Floats2d] = None,\n        trans1: bool = False,\n        trans2: bool = False,\n    ) -> Floats2d:\n        """"""Perform General Matrix Multiplication (GeMM) and optionally store\n        the result in the specified output variable.\n        """"""\n        if trans1:\n            x = x.T\n        if trans2:\n            y = y.T\n        if out is None:\n            return self.xp.dot(x, y)\n        else:\n            self.xp.dot(x, y, out=out)\n            return out\n\n    def affine(self, X: Floats2d, W: Floats2d, b: Floats1d) -> Floats2d:\n        """"""Apply a weights layer and a bias to some inputs, i.e.\n        Y = X @ W.T + b\n        """"""\n        Y = self.gemm(X, W, trans2=True)\n        Y += b\n        return Y\n\n    def flatten(\n        self,\n        X: Sequence[ArrayT],\n        dtype: Optional[DTypes] = None,\n        pad: int = 0,\n        ndim_if_empty: int = 2,\n    ) -> ArrayT:\n        """"""Flatten a list of arrays into one large array.""""""\n        if X is None or len(X) == 0:\n            return self.alloc((0,) * ndim_if_empty, dtype=dtype or ""f"")\n        xp = get_array_module(X[0])\n        X = [x for x in X if x.size != 0]\n        if int(pad) >= 1:\n            padded = []\n            for x in X:\n                padded.append(xp.zeros((pad,) + x.shape[1:], dtype=x.dtype))\n                padded.append(x)\n            padded.append(xp.zeros((pad,) + x.shape[1:], dtype=x.dtype))\n            X = padded\n        result = xp.concatenate(X)\n        if dtype is not None:\n            result = xp.asarray(result, dtype=dtype)\n        return result\n\n    def unflatten(self, X: Floats2d, lengths: Ints1d, pad: int = 0) -> List[Floats2d]:\n        """"""The reverse/backward operation of the `flatten` function: unflatten\n        a large array into a list of arrays according to the given lengths.\n        """"""\n        unflat = []\n        pad = int(pad)\n        for length in lengths:\n            length = int(length)\n            if pad >= 1 and length != 0:\n                X = X[pad:]\n            unflat.append(X[:length])\n            X = X[length:]\n        if pad >= 1:\n            X = X[pad:]\n        assert len(X) == 0\n        assert len(unflat) == len(lengths)\n        return unflat\n\n    @overload\n    def pad(self, seqs: List[Ints2d], round_to=1) -> Ints3d:\n        ...\n\n    @overload  # noqa: F811\n    def pad(self, seqs: List[Floats2d], round_to=1) -> Floats3d:\n        ...\n\n    def pad(  # noqa: F811\n        self, seqs: Union[List[Ints2d], List[Floats2d]], round_to=1\n    ) -> Array3d:\n        """"""Perform padding on a list of arrays so that they each have the same\n        length, by taking the maximum dimension across each axis. This only\n        works on non-empty sequences with the same `ndim` and `dtype`.\n        """"""\n        # TODO: This should be generalized to handle different ranks\n        if not seqs:\n            raise ValueError(""Cannot pad empty sequence"")\n        if len(set(seq.ndim for seq in seqs)) != 1:\n            raise ValueError(""Cannot pad sequences with different ndims"")\n        if len(set(seq.dtype for seq in seqs)) != 1:\n            raise ValueError(""Cannot pad sequences with different dtypes"")\n        if len(set(seq.shape[1:] for seq in seqs)) != 1:\n            raise ValueError(""Cannot pad sequences that differ on other dimensions"")\n        # Find the maximum dimension along each axis. That\'s what we\'ll pad to.\n        length = max(len(seq) for seq in seqs)\n        # Round the length to nearest bucket -- helps on GPU, to make similar\n        # array sizes.\n        length = (length + (round_to - 1)) // round_to * round_to\n        final_shape = (len(seqs), length) + seqs[0].shape[1:]\n        output: Array3d = self.alloc(final_shape, dtype=seqs[0].dtype)\n        for i, arr in enumerate(seqs):\n            # It\'s difficult to convince this that the dtypes will match.\n            output[i, : arr.shape[0]] = arr  # type: ignore\n        return output\n\n    def unpad(self, padded: Array3d, lengths: List[int]) -> List2d:\n        """"""The reverse/backward operation of the `pad` function: transform an\n        array back into a list of arrays, each with their original length.\n        """"""\n        output = []\n        for i, length in enumerate(lengths):\n            output.append(padded[i, :length])\n        return cast(List2d, output)\n\n    def list2padded(self, seqs: List[Floats2d]) -> Padded:\n        """"""Pack a sequence of 2d arrays into a Padded datatype.""""""\n        if not seqs:\n            return Padded(\n                self.alloc3f(0, 0, 0), self.alloc1i(0), self.alloc1i(0), self.alloc1i(0)\n            )\n        elif len(seqs) == 1:\n            data = self.reshape3f(seqs[0], seqs[0].shape[0], 1, seqs[0].shape[1])\n            size_at_t = self.asarray1i([1] * data.shape[0])\n            lengths = self.asarray1i([data.shape[0]])\n            indices = self.asarray1i([0])\n            return Padded(data, size_at_t, lengths, indices)\n        lengths_indices = [(len(seq), i) for i, seq in enumerate(seqs)]\n        lengths_indices.sort(reverse=True)\n        indices_ = [i for length, i in lengths_indices]\n        lengths_ = [length for length, i in lengths_indices]\n        nS = max([len(seq) for seq in seqs])\n        # Reorder the sequences, by length. This looks the same in either\n        # direction: you\'re swapping elements between their original and sorted\n        # position.\n        seqs = [seqs[x] for x in indices_]\n        arr: Floats3d = self.pad(seqs)\n        arr = self.as_contig(arr.transpose((1, 0, 2)))\n        # Build a lookup table so we can find how big the batch is at point t.\n        batch_size_at_t_ = self.alloc1i(nS)\n        batch_size_at_t_ += 1\n        i = len(lengths_)\n        for t in range(nS):\n            if t == lengths_[i - 1]:\n                i -= 1\n                if i == 0:\n                    break\n            batch_size_at_t_[t] = i\n        return Padded(\n            cast(Floats3d, arr),\n            self.asarray1i(batch_size_at_t_),\n            self.asarray1i(lengths_),\n            self.asarray1i(indices_),\n        )\n\n    def padded2list(self, padded: Padded) -> List2d:\n        """"""Unpack a Padded datatype to a list of 2-dimensional arrays.""""""\n        data = padded.data\n        indices = to_numpy(padded.indices)\n        lengths = to_numpy(padded.lengths)\n        unpadded: List[Optional[Floats2d]] = [None] * len(lengths)\n        data = self.as_contig(data.transpose((1, 0, 2)))\n        for i in range(data.shape[0]):\n            unpadded[indices[i]] = data[i, : int(lengths[i])]\n        return cast(List2d, unpadded)\n\n    def get_dropout_mask(self, shape: Shape, drop: Optional[float]) -> FloatsXd:\n        """"""Create a random mask for applying dropout, with a certain percent of\n        the mask (defined by `drop`) will contain zeros. The neurons at those\n        positions will be deactivated during training, resulting in a more\n        robust network and less overfitting.\n        """"""\n        if drop is None or drop <= 0:\n            return self.xp.ones(shape, dtype=""f"")\n        elif drop >= 1.0:\n            return self.alloc(shape)\n        coinflips = self.xp.random.uniform(0.0, 1.0, shape)\n        mask = (coinflips >= drop) / (1.0 - drop)\n        return cast(FloatsXd, self.asarray(mask, dtype=""float32""))\n\n    def alloc1f(self, d0: int, *, dtype: Optional[DTypesFloat] = ""float32"") -> Floats1d:\n        return self.alloc((d0,), dtype=dtype)\n\n    def alloc2f(\n        self, d0: int, d1: int, *, dtype: Optional[DTypesFloat] = ""float32""\n    ) -> Floats2d:\n        return self.alloc((d0, d1), dtype=dtype)\n\n    def alloc3f(\n        self, d0: int, d1: int, d2: int, *, dtype: Optional[DTypesFloat] = ""float32""\n    ) -> Floats3d:\n        return self.alloc((d0, d1, d2), dtype=dtype)\n\n    def alloc4f(\n        self,\n        d0: int,\n        d1: int,\n        d2: int,\n        d3: int,\n        *,\n        dtype: Optional[DTypesFloat] = ""float32"",\n    ) -> Floats4d:\n        return self.alloc((d0, d1, d2, d3), dtype=dtype)\n\n    def alloc_f(\n        self, shape: Shape, *, dtype: Optional[DTypesFloat] = ""float32""\n    ) -> FloatsXd:\n        return self.alloc(shape, dtype=dtype)\n\n    def alloc1i(self, d0: int, *, dtype: Optional[DTypesInt] = ""int32"") -> Ints1d:\n        return self.alloc((d0,), dtype=dtype)\n\n    def alloc2i(\n        self, d0: int, d1: int, *, dtype: Optional[DTypesInt] = ""int32""\n    ) -> Ints2d:\n        return self.alloc((d0, d1), dtype=dtype)\n\n    def alloc3i(\n        self, d0: int, d1: int, d2: int, *, dtype: Optional[DTypesInt] = ""int32""\n    ) -> Ints3d:\n        return self.alloc((d0, d1, d2), dtype=dtype)\n\n    def alloc4i(\n        self,\n        d0: int,\n        d1: int,\n        d2: int,\n        d3: int,\n        *,\n        dtype: Optional[DTypesInt] = ""int32"",\n    ) -> Ints4d:\n        return self.alloc((d0, d1, d2, d3), dtype=dtype)\n\n    def alloc_i(self, shape: Shape, *, dtype: Optional[DTypesInt] = ""int32"") -> IntsXd:\n        return self.alloc(shape, dtype=dtype)\n\n    def alloc(self, shape: Shape, *, dtype: Optional[DTypes] = ""float32"") -> ArrayT:\n        """"""Allocate an array of a certain shape.""""""\n        if isinstance(shape, int):\n            shape = (shape,)\n        return self.xp.zeros(shape, dtype=dtype)\n\n    def reshape1f(self, array: FloatsXd, d0: int) -> Floats1d:\n        return cast(Floats1d, self.reshape(array, (d0,)))\n\n    def reshape2f(self, array: FloatsXd, d0: int, d1: int) -> Floats2d:\n        return cast(Floats2d, self.reshape(array, (d0, d1)))\n\n    def reshape3f(self, array: FloatsXd, d0: int, d1: int, d2: int) -> Floats3d:\n        return cast(Floats3d, self.reshape(array, (d0, d1, d2)))\n\n    def reshape4f(\n        self, array: FloatsXd, d0: int, d1: int, d2: int, d3: int\n    ) -> Floats4d:\n        return cast(Floats4d, self.reshape(array, (d0, d1, d2, d3)))\n\n    def reshape_f(self, array: FloatsXd, shape: Shape) -> FloatsXd:\n        return self.reshape(array, shape)\n\n    def reshape1i(self, array: IntsXd, d0: int) -> Ints1d:\n        return cast(Ints1d, self.reshape(array, (d0,)))\n\n    def reshape2i(self, array: IntsXd, d0: int, d1: int) -> Ints2d:\n        return cast(Ints2d, self.reshape(array, (d0, d1)))\n\n    def reshape3i(self, array: IntsXd, d0: int, d1: int, d2: int) -> Ints3d:\n        return cast(Ints3d, self.reshape(array, (d0, d1, d2)))\n\n    def reshape4i(self, array: IntsXd, d0: int, d1: int, d2: int, d3: int) -> Ints4d:\n        return cast(Ints4d, self.reshape(array, (d0, d1, d2, d3)))\n\n    def reshape_i(self, array: IntsXd, shape: Shape) -> IntsXd:\n        return self.reshape(array, shape)\n\n    def reshape(self, array: ArrayT, shape: Shape) -> ArrayT:\n        """"""Reshape an array.""""""\n        if isinstance(shape, int):\n            shape = (shape,)\n        return cast(ArrayT, array.reshape(shape))\n\n    def asarray4f(\n        self,\n        data: Union[Floats4d, Sequence[int]],\n        *,\n        dtype: Optional[DTypes] = ""float32"",\n    ) -> Floats4d:\n        return cast(Floats4d, self.asarray(data, dtype=dtype))\n\n    def asarray3f(\n        self,\n        data: Union[Floats3d, Sequence[int]],\n        *,\n        dtype: Optional[DTypes] = ""float32"",\n    ) -> Floats3d:\n        return cast(Floats3d, self.asarray(data, dtype=dtype))\n\n    def asarray2f(\n        self,\n        data: Union[Floats2d, Sequence[int]],\n        *,\n        dtype: Optional[DTypes] = ""float32"",\n    ) -> Floats2d:\n        return cast(Floats2d, self.asarray(data, dtype=dtype))\n\n    def asarray1f(\n        self,\n        data: Union[Floats1d, Sequence[int]],\n        *,\n        dtype: Optional[DTypes] = ""float32"",\n    ) -> Floats1d:\n        return cast(Floats1d, self.asarray(data, dtype=dtype))\n\n    def asarray_f(\n        self,\n        data: Union[FloatsXd, Sequence[float]],\n        *,\n        dtype: Optional[DTypes] = ""float32"",\n    ) -> FloatsXd:\n        return cast(FloatsXd, self.asarray(data, dtype=dtype))\n\n    def asarray1i(\n        self, data: Union[Ints1d, Sequence[int]], *, dtype: Optional[DTypes] = ""int32""\n    ) -> Ints1d:\n        return cast(Ints1d, self.asarray(data, dtype=dtype))\n\n    def asarray2i(\n        self, data: Union[Ints2d, Sequence[int]], *, dtype: Optional[DTypes] = ""int32""\n    ) -> Ints2d:\n        return cast(Ints2d, self.asarray(data, dtype=dtype))\n\n    def asarray3i(\n        self, data: Union[Ints3d, Sequence[int]], *, dtype: Optional[DTypes] = ""int32""\n    ) -> Ints3d:\n        return cast(Ints3d, self.asarray(data, dtype=dtype))\n\n    def asarray4i(\n        self, data: Union[Ints4d, Sequence[int]], *, dtype: Optional[DTypes] = ""int32""\n    ) -> Ints4d:\n        return cast(Ints4d, self.asarray(data, dtype=dtype))\n\n    def asarray_i(\n        self, data: Union[IntsXd, Sequence[int]], *, dtype: Optional[DTypes] = ""int32""\n    ) -> IntsXd:\n        return cast(IntsXd, self.asarray(data, dtype=dtype))\n\n    def asarray(\n        self,\n        data: Union[ArrayXd, Sequence[ArrayXd], Sequence[float], Sequence[int]],\n        *,\n        dtype: Optional[DTypes] = None,\n    ) -> ArrayXd:\n        """"""Ensure a given array is of the correct type.""""""\n        if isinstance(data, self.xp.ndarray):\n            if dtype is not None:\n                return self.xp.asarray(data, dtype=dtype)\n            else:\n                return self.xp.asarray(data)\n        elif hasattr(data, ""numpy""):\n            # Handles PyTorch Tensor\n            return data.numpy()  # type: ignore\n        elif dtype is not None:\n            return self.xp.array(data, dtype=dtype)\n        else:\n            return self.xp.array(data)\n\n    def as_contig(self, data: ArrayT, dtype: Optional[DTypes] = None) -> ArrayT:\n        """"""Allow the backend to make a contiguous copy of an array.\n        Implementations of `Ops` do not have to make a copy or make it\n        contiguous if that would not improve efficiency for the execution engine.\n        """"""\n        kwargs = {""dtype"": dtype} if dtype is not None else {}\n        return self.xp.ascontiguousarray(data, **kwargs)\n\n    def sigmoid(self, X: FloatsT, *, inplace: bool = False) -> FloatsT:\n        if inplace:\n            self.xp.exp(-X, out=X)\n            X += 1.0\n            X **= -1.0\n            return X\n        else:\n            return 1.0 / (1.0 + self.xp.exp(-X))\n\n    def dsigmoid(self, Y: FloatsT, *, inplace: bool = False) -> FloatsT:\n        if inplace:\n            Y *= 1 - Y\n            return Y\n        else:\n            return Y * (1.0 - Y)\n\n    def dtanh(self, Y: FloatsT, *, inplace: bool = False) -> FloatsT:\n        if inplace:\n            Y **= 2\n            Y *= -1.0\n            Y += 1.0\n            return Y\n        else:\n            return 1 - Y ** 2\n\n    def softmax(self, x: FloatsT, *, inplace: bool = False, axis: int = -1) -> FloatsT:\n        maxes = self.xp.max(x, axis=axis, keepdims=True)\n        shifted = x - maxes\n        new_x = self.xp.exp(shifted)\n        new_x /= new_x.sum(axis=axis, keepdims=True)\n        return new_x\n\n    def softmax_sequences(\n        self, Xs: Floats2d, lengths: Ints1d, *, inplace: bool = False, axis: int = -1\n    ) -> Floats2d:\n        if Xs.ndim >= 3:\n            err = f""Softmax currently only supports 2d. Got: {Xs.ndim}""\n            raise NotImplementedError(err)\n        # This loses almost no fidelity, and helps the numerical stability.\n        Xs = self.xp.clip(Xs, -20.0, 20.0)\n        new_x = self.xp.exp(Xs)\n        summed = self.backprop_reduce_sum(self.reduce_sum(new_x, lengths), lengths)\n        new_x /= summed\n        return new_x\n\n    def backprop_softmax(self, Y: FloatsT, dY: FloatsT, *, axis: int = -1) -> FloatsT:\n        dX = Y * dY\n        dX -= Y * dX.sum(axis=axis, keepdims=True)\n        return dX\n\n    def backprop_softmax_sequences(\n        self, dY: Floats2d, Y: Floats2d, lengths: Ints1d\n    ) -> Floats2d:\n        dX = Y * dY\n        sum_dX = self.backprop_reduce_sum(self.reduce_sum(dX, lengths), lengths)\n        dX -= Y * sum_dX\n        return dX\n\n    def recurrent_lstm(\n        self,\n        W: Floats2d,\n        b: Floats1d,\n        h_init: Floats1d,\n        c_init: Floats1d,\n        inputs: Floats3d,\n        is_train: bool = True,\n    ) -> Tuple[Floats3d, Tuple[Floats3d, Floats3d, Floats3d]]:\n        Y, (G, C, S) = recurrent_lstm_forward(W, b, h_init, c_init, inputs)\n        return Y, (G, C, S)\n\n    def backprop_recurrent_lstm(\n        self,\n        dY: Floats3d,\n        fwd_state: Tuple[Floats3d, Floats3d, Floats3d],\n        params: Tuple[Floats2d, Floats1d],\n    ) -> Tuple[Floats3d, Tuple[Floats2d, Floats1d, Floats1d, Floats1d]]:\n        dCt = self.alloc2f(dY.shape[1], dY.shape[2])\n        empty_row = self.alloc3f(1, dY.shape[1], dY.shape[2])\n        # Offset dY by 1\n        dY = self.xp.vstack((empty_row, dY))\n        dW, db, dX, dY, dC0 = backprop_recurrent_lstm(dY, dCt, (fwd_state, params))\n        return dX, (dW, db, dY[0].sum(axis=0), dC0.sum(axis=0))\n\n    def maxout(self, X: Floats3d) -> Tuple[Floats2d, Ints2d]:\n        which = X.argmax(axis=-1, keepdims=False)\n        return X.max(axis=-1), which\n\n    def backprop_maxout(self, dY: Floats2d, which: Ints2d, P: int) -> Floats3d:\n        dX = self.alloc3f(dY.shape[0], dY.shape[1], P)\n        for b in range(dY.shape[0]):\n            for o in range(dY.shape[1]):\n                dX[b, o, which[b, o]] = dY[b, o]\n        return dX\n\n    def relu(self, X: Floats2d, inplace: bool = False) -> Floats2d:\n        if not inplace:\n            return X * (X > 0)\n        else:\n            X *= X > 0\n            return X\n\n    def backprop_relu(\n        self, dY: Floats2d, Y: Floats2d, inplace: bool = False\n    ) -> Floats2d:\n        if not inplace:\n            return dY * (Y > 0)\n        dY *= Y > 0\n        return dY\n\n    def mish(self, X: Floats2d, threshold: float = 20.0) -> Floats2d:\n        Y = self.alloc2f(*X.shape, dtype=X.dtype)\n        tmp = X * self.xp.tanh(self.xp.log(1.0 + self.xp.exp(X)))\n        for i in range(X.shape[0]):\n            for j in range(X.shape[1]):\n                if X[i, j] >= threshold:\n                    Y[i, j] = X[i, j]\n                else:\n                    Y[i, j] = tmp[i, j]\n        return Y\n\n    def backprop_mish(\n        self,\n        dY: Floats2d,\n        X: Floats2d,\n        threshold: float = 20.0,\n        out: Optional[Floats2d] = None,\n    ) -> Floats2d:\n        xp = get_array_module(X)\n        indices = X < threshold\n        Xsub = X[indices]\n        dYsub = dY[indices]\n        omega = 4.0 * (Xsub + 1.0)\n        omega += 4.0 * xp.exp(2.0 * Xsub)\n        omega += xp.exp(Xsub) * ((4.0 * Xsub) + 6.0)\n        delta = 2.0 * xp.exp(Xsub)\n        delta += xp.exp(2.0 * Xsub)\n        delta += 2.0\n        dXsub = dYsub * ((xp.exp(Xsub) * omega) / (delta ** 2))\n        if out is None:\n            out = xp.zeros(dY.shape, dtype=""f"")\n        # Gradient when above threshold will ignore softplus.\n        out[:] = dY + dY * self.dtanh(X)\n        out[indices] = dXsub\n        return out\n\n    def update_averages(\n        self, ema: FloatsT, weights: FloatsT, t: int, max_decay: float = 0.9999\n    ) -> None:\n        # Internals for optimizer\n        decay = (1.0 + t) / (10.0 + t)\n        if decay > max_decay:\n            decay = max_decay\n        ema -= (1 - decay) * (ema - weights)\n\n    def adam(\n        self,\n        weights: Floats1d,\n        gradient: Floats1d,\n        mom1: Floats1d,\n        mom2: Floats1d,\n        beta1: float,\n        beta2: float,\n        eps: float,\n        learn_rate: float,\n        mod_rate: float = 1.0,\n    ) -> Tuple[Floats1d, Floats1d, Floats1d, Floats1d]:\n        # Internals for optimizer\n        mom1 *= beta1\n        mom2 *= beta2\n        mom1 += gradient * (1.0 - beta1)\n        mom2 += gradient * gradient * (1.0 - beta2)\n        # Here we assume learn rate is calculated by the caller.\n        # cdef weight_t a_t = learn_rate * sqrt(1-beta2**hp.t) / (1-beta1**hp.t);\n        weights -= learn_rate * (mom1 / (mod_rate * self.xp.sqrt(mom2) + eps))\n        return weights, gradient, mom1, mom2\n\n    def clip_gradient(self, gradient: FloatsT, threshold: float) -> FloatsT:\n        # Internals for optimizer\n        xp = get_array_module(gradient)\n        grad_norm = xp.linalg.norm(gradient)\n        if grad_norm >= threshold:\n            gradient *= threshold / grad_norm\n        return gradient\n\n    def logloss(self, y_true: FloatsT, y_pred: FloatsT) -> float:\n        # Currently not used\n        log_yp = self.xp.log(y_pred + 1e-8)\n        loss = (y_true * log_yp) + (1 - y_true) * self.xp.log((1 - y_pred) + 1e-8)\n        return -loss\n\n    def reduce_sum(self, X: Floats2d, lengths: Ints1d) -> Floats2d:\n        Y = self.alloc2f(lengths.shape[0], X.shape[1])\n        start = 0\n        for i, length in enumerate(lengths):\n            Y[i] = X[start : start + length].sum(axis=0)\n            start += length\n        return Y\n\n    def reduce_mean(self, X: Floats2d, lengths: Ints1d) -> Floats2d:\n        Y = self.alloc2f(lengths.shape[0], X.shape[1])\n        start = 0\n        for i, length in enumerate(lengths):\n            if length:\n                Y[i] = X[start : start + length].mean(axis=0)\n            start += length\n        return Y\n\n    def reduce_max(self, X: Floats2d, lengths: Ints1d) -> Tuple[Floats2d, Ints2d]:\n        Y = self.alloc2f(lengths.shape[0], X.shape[1])\n        which = self.alloc2i(lengths.shape[0], X.shape[1])\n        start = 0\n        for i, length in enumerate(lengths):\n            if length:\n                which[i] = X[start : start + length].argmax(axis=0)\n                Y[i] = X[start : start + length].max(axis=0)\n            start += length\n        return Y, which\n\n    def backprop_reduce_sum(self, d_sums: Floats2d, lengths: Ints1d) -> Floats2d:\n        dX = self.alloc2f(lengths.sum(), d_sums.shape[1])\n        start = 0\n        for i, length in enumerate(lengths):\n            dX[start : start + length] = d_sums[i]\n            start += length\n        return dX\n\n    def backprop_reduce_mean(self, d_means: Floats2d, lengths: Ints1d) -> Floats2d:\n        dX = self.alloc2f(lengths.sum(), d_means.shape[1])\n        start = 0\n        for i, length in enumerate(lengths):\n            dX[start : start + length] = d_means[i] / length\n            start += length\n        return dX\n\n    def backprop_reduce_max(\n        self, d_maxes: Floats2d, which: Ints2d, lengths: Ints1d\n    ) -> Floats2d:\n        dX = self.alloc2f(lengths.sum(), d_maxes.shape[1])\n        start = 0\n        for i, length in enumerate(lengths):\n            dX[start : start + length, which[i]] = d_maxes[i]\n            start += length\n        return dX\n\n    def hash(self, ids: Ints1d, seed: int) -> Ints2d:\n        """"""Hash a sequence of 64-bit keys into a table with 4 32-bit keys, using\n        murmurhash3.\n        """"""\n        from .numpy_ops import NumpyOps\n\n        numpy_ops = NumpyOps()\n        return self.asarray2i(\n            numpy_ops.hash(numpy_ops.asarray(ids, dtype=""uint64""), seed)\n        )\n\n    def ngrams(self, n: int, keys: Ints1d) -> Ints1d:\n        from .numpy_ops import NumpyOps\n\n        numpy_ops = NumpyOps()\n        return self.asarray1i(\n            numpy_ops.ngrams(n, numpy_ops.asarray(keys, dtype=""uint64""))\n        )\n\n    def position_encode(\n        self, N: int, D: int, period: int = 10000, out: Optional[Floats2d] = None\n    ) -> Floats2d:\n        # Currently internals only\n        from .numpy_ops import NumpyOps\n\n        numpy_ops = NumpyOps()\n        return self.asarray2f(numpy_ops.position_encode(N, D, period, out))\n\n    def scatter_add(\n        self, table: FloatsXd, indices: IntsXd, values: FloatsXd\n    ) -> FloatsXd:\n        return self.xp.add.at(table, indices, values)\n\n    def insert_into(self, shape, Xs):\n        """"""Maybe don\'t need this? Just a quicky to get Jax working.""""""\n        output = self.alloc(shape, dtype=Xs[0].dtype)\n        for i, x in enumerate(Xs):\n            output[i, : x.shape[0]] = x\n        return output\n\n\n# This code is intentionally almost-duplicate with the Jax one. It\'s kind\n# of hard to condition on jax vs not jax without messing up the jax JIT,\n# and we\'ll want to have a more specialised implementation for non-Jax\n# versions. But for now this has been tested and works, so we\'ll just leave\n# it as a reference implementation.\n""""""\nLSTM Notation (kind of involved, but made it a lot easier to write)\n\nX: Inputs\nY: Outputs (aka hiddens)\nC: Cells\nG: Gates (Output of non-linearity, i.e. lstm_gates(X @ W.T)\nA: Activations (X @ W.T, before non-linearity)\n\nImagine we have the input:\nbatch = [\n    [""apple"", ""banana"", ""cantaloupe"", ""date"", ""elderberry""],\n    [""aardvark"", ""bat"", ""capybara"", ""dingo"", ""elephant""]\n]\n\nThe input variable X will have one vector per word, so X[0, 1] will be banana\'s\nvector, X[0, 1, 0] will be a float, the first element of that vector.\n\nWe\'re computing an output variable Y of shape (nL, nB, nO), so that Y[0, 1] is\nthe output variable of banana.\n\nA problem with variables for RNNs is keeping the timesteps straight. It\'s hard\nto distinguish the current, previous, and next timesteps. To solve this problem,\nwe follow the convention that **we are at timestep 3**.\n\nAdditionally, the variables for Y and C are offset by one, as the 0th elements\nhave the initial hiddens and initial cells. So:\n\n    t=3\n    Xt3: The input vectors for \'dingo\' and \'date\', i.e. X[t]\n    Yt3: The output vectors for \'dingo\' and \'date\', i.e. Y[t+1] (Y is offset.)\n    Ct2: The cells calculated at \'c...\', that are the input for \'d...\'\n    Ct3: The cells calculated at \'d...\', that are the input for \'e...\'\n    At3: The activations at \'d...\'\n    Gt3: The gates at \'d...\'\n""""""\n\n\ndef recurrent_lstm_forward(W, b, c_init, h_init, X):\n    xp = get_array_module(W)\n    nL, nB, nI = X.shape\n    nO = h_init.shape[0]\n    # Preallocate these so we can pass them through for loop.\n    Y = xp.zeros((nL + 1, nB, nO), dtype=""f"")\n    G = xp.zeros((nL, nB, nO * 4), dtype=""f"")\n    C = xp.zeros((nL + 1, nB, nO), dtype=""f"")\n    # Set initial hidden and cell states. The Y and C will be shifted 1,\n    # so that we can have fewer arrays.\n    Y[0] = h_init\n    C[0] = c_init\n    state = ((W, b, X), (Y, C, G))\n    for i in range(X.shape[0]):\n        state = lstm_stepper_forward(i, state)\n    (W, b, X), (Y, C, G) = state\n    # Recall that Y and C are both offset by 1. Y[1] is the output for\n    # X[1], while Y[0] was used as an input for Y[1]. We use\n    # the S values to backprop the weights, so we need X the previous Ys.\n    S = xp.concatenate((X, Y[:-1]), axis=-1)\n    return Y[1:], (G, C, S)\n\n\ndef lstm_stepper_forward(t, state):\n    (W, b, X), (Y, C, G) = state\n    # Get the activations for this timestep.\n    At3 = lstm_weights_forward(X[t], Y[t], W, b)\n    # The offsets here are a bit unintuitive, because Y and C are 1-offset.\n    Ct2 = C[t]\n    Yt3, Ct3, Gt3 = lstm_gates_forward(At3, Ct2)\n    Y[t + 1] = Yt3\n    C[t + 1] = Yt3\n    G[t] = Gt3\n    return (W, b, X), (Y, C, G)\n\n\ndef backprop_recurrent_lstm(dY, dCt, fwd_vars):\n    xp = get_array_module(dY)\n    (G, C, S), (W, b) = fwd_vars\n    nL = S.shape[0]\n    nB = dY.shape[1]\n    nI = S.shape[2] - dY.shape[2]\n    # Preallocate these so we can pass them through for loop.\n    dX = xp.zeros((nL, nB, nI), dtype=""f"")\n    dW = xp.zeros(W.shape, dtype=""f"")\n    db = xp.zeros(b.shape, dtype=""f"")\n    state = (\n        (dW, db, dX),  # The gradi-outs (Write-only)\n        (dY, dCt),  # The gradi-ins  (Read and write)\n        (G, C, S),  # Forward state  (Read-only)\n        (W, b),  # Params         (Read-only)\n    )\n    for t in range(nL - 1, -1, -1):\n        state = backprop_lstm_stepper(t, state)\n    (dW, db, dX), (dY, dCt), (G, C, S), (W, b) = state\n    return dW, db, dX, dY, dCt\n\n\ndef backprop_lstm_stepper(t, state):\n    (dW, db, dX), (dY, dCt3), (G, C, S), (W, b) = state\n    # Recall, we\'re at step 3, Y and C are offset by 1. See above.\n    dYt3 = dY[t + 1]\n    Ct3 = C[t + 1]\n    St3 = S[t]\n    Gt3 = G[t]\n    Ct2 = C[t]\n    dAt3, dCt2 = backprop_lstm_gates(dCt3, dYt3, Gt3, Ct3, Ct2)\n    dXt3, dYt2, dW3, db3 = backprop_lstm_weights(dAt3, (St3, W, b))\n    dX[t] = dXt3\n    dY[t] = dYt2\n    return (dW + dW3, db + db3, dX), (dY, dCt2), (G, C, S), (W, b)\n\n\ndef lstm_weights_forward(Xt3, Yt2, W, b):\n    xp = get_array_module(Yt2)\n    St3 = xp.concatenate((Xt3, Yt2), axis=-1)\n    At3 = St3 @ W.T + b\n    return At3\n\n\ndef backprop_lstm_weights(dAt3, fwd_state):\n    St3, W, b = fwd_state\n    dW = dAt3.T @ St3\n    db = dAt3.sum(axis=0)\n    dSt3 = dAt3 @ W\n    nO = W.shape[0] // 4\n    nI = St3.shape[1] - nO\n    dXt3 = dSt3[:, :nI]\n    dYt2 = dSt3[:, nI:]\n    return dXt3, dYt2, dW, db\n\n\ndef lstm_gates_forward(At3, Ct2):\n    xp = get_array_module(At3)\n    # hf, hi, ho, hc: Forget, input, output, cell gates.\n    At3_hf, At3_hi, At3_ho, At3_hc = xp.split(At3, 4, axis=-1)\n    # Number the steps here, to refer back for backward pass.\n    # 1. Activations\n    hf = sigmoid(At3_hf)  # 1a\n    hi = sigmoid(At3_hi)  # 1b\n    ho = sigmoid(At3_ho)  # 1c\n    hc = xp.tanh(At3_hc)  # 1d\n\n    Ct3 = hf * Ct2  # 2a\n    Ct3 += hi * hc  # 2b\n    tanhCt3 = xp.tanh(Ct3)  # 3a\n    Yt3 = tanhCt3 * ho  # 3b\n    # We don\'t need the gradient for this, it\'s just for backprop calculation.\n    Gt3 = xp.concatenate((hf, hi, ho, hc), axis=-1)\n    return Yt3, Ct3, Gt3\n\n\ndef backprop_lstm_gates(\n    dYt3: Array2d, dCt3: Array2d, Gt3: Array2d, Ct3: Array2d, Ct2: Array2d\n) -> Tuple[Array3d, Array2d]:\n    # See above for notation. Step numbering refers to forward_lstm_gates\n    xp = get_array_module(dYt3)\n    hf, hi, ho, hc = xp.split(Gt3, 4, axis=-1)\n    tanhCt3 = xp.tanh(Ct3)\n    # 3b: Yt3 = tanhCt3 * ho\n    d_ho = dYt3 * tanhCt3\n    d_tanhCt3 = dYt3 * ho\n    # 3a: tanhCt3 = tanh(Ct3)\n    dCt3 += d_tanhCt3 * dtanh(tanhCt3)\n    # 2b: Ct3 += hi * hc\n    d_hi = dCt3 * hc\n    d_hc = dCt3 * hi\n    # 2a: Ct3 = hf * Ct2\n    d_hf = dCt3 * Ct2\n    dCt2 = dCt3 * hf\n    d_At3_hc = d_hc * dtanh(hc)  # 1d\n    d_At3_ho = d_ho * dsigmoid(ho)  # 1c\n    d_At3_hi = d_hi * dsigmoid(hi)  # 1b\n    d_At3_hf = d_hf * dsigmoid(hf)  # 1a\n    dAt3 = xp.concatenate((d_At3_hf, d_At3_hi, d_At3_ho, d_At3_hc), axis=-1)\n    return dAt3, dCt2\n\n\ndef sigmoid(X):\n    xp = get_array_module(X)\n    return 1.0 / (1.0 + xp.exp(-X))\n\n\ndef dsigmoid(Y: ArrayT) -> ArrayT:\n    return Y * (1.0 - Y)\n\n\ndef dtanh(Y: ArrayT) -> ArrayT:\n    return 1 - Y ** 2\n'"
thinc/extra/__init__.py,0,b''
thinc/layers/__init__.py,0,"b'# Weights layers\nfrom .cauchysimilarity import CauchySimilarity\nfrom .dropout import Dropout\nfrom .embed import Embed\nfrom .expand_window import expand_window\nfrom .featureextractor import FeatureExtractor\nfrom .hashembed import HashEmbed\nfrom .layernorm import LayerNorm\nfrom .linear import Linear\nfrom .logistic import Logistic\nfrom .maxout import Maxout\nfrom .mish import Mish\nfrom .multisoftmax import MultiSoftmax\nfrom .parametricattention import ParametricAttention\nfrom .pytorchwrapper import PyTorchWrapper, PyTorchRNNWrapper\nfrom .relu import Relu\nfrom .softmax_activation import softmax_activation\nfrom .softmax import Softmax\nfrom .sparselinear import SparseLinear\nfrom .staticvectors import StaticVectors\nfrom .lstm import LSTM, PyTorchLSTM\nfrom .tensorflowwrapper import TensorFlowWrapper, keras_subclass\nfrom .mxnetwrapper import MXNetWrapper\n\n# Combinators\nfrom .add import add\nfrom .bidirectional import bidirectional\nfrom .chain import chain\nfrom .clone import clone\nfrom .concatenate import concatenate\nfrom .noop import noop\nfrom .residual import residual\nfrom .uniqued import uniqued\nfrom .siamese import siamese\n\n# Pooling\nfrom .reduce_max import reduce_max\nfrom .reduce_mean import reduce_mean\nfrom .reduce_sum import reduce_sum\n\n# Array manipulation\nfrom .array_getitem import array_getitem\n\n# Data-type transfers\nfrom .list2array import list2array\nfrom .list2ragged import list2ragged\nfrom .list2padded import list2padded\nfrom .ragged2list import ragged2list\nfrom .padded2list import padded2list\nfrom .remap_ids import remap_ids\nfrom .strings2arrays import strings2arrays\nfrom .with_array import with_array\nfrom .with_cpu import with_cpu\nfrom .with_flatten import with_flatten\nfrom .with_padded import with_padded\nfrom .with_list import with_list\nfrom .with_ragged import with_ragged\nfrom .with_reshape import with_reshape\nfrom .with_getitem import with_getitem\nfrom .with_debug import with_debug\n\n\n__all__ = [\n    ""CauchySimilarity"",\n    ""Linear"",\n    ""Dropout"",\n    ""Embed"",\n    ""expand_window"",\n    ""HashEmbed"",\n    ""LayerNorm"",\n    ""Maxout"",\n    ""Mish"",\n    ""MultiSoftmax"",\n    ""ParametricAttention"",\n    ""PyTorchWrapper"",\n    ""PyTorchRNNWrapper"",\n    ""Relu"",\n    ""softmax_activation"",\n    ""Softmax"",\n    ""SparseLinear"",\n    ""StaticVectors"",\n    ""LSTM"",\n    ""PyTorchLSTM"",\n    ""TensorFlowWrapper"",\n    ""add"",\n    ""bidirectional"",\n    ""chain"",\n    ""clone"",\n    ""concatenate"",\n    ""noop"",\n    ""residual"",\n    ""uniqued"",\n    ""siamese"",\n    ""reduce_max"",\n    ""reduce_mean"",\n    ""reduce_sum"",\n    ""list2array"",\n    ""list2ragged"",\n    ""list2padded"",\n    ""ragged2list"",\n    ""padded2list"",\n    ""with_reshape"",\n    ""with_getitem"",\n    ""with_array"",\n    ""with_cpu"",\n    ""with_list"",\n    ""with_ragged"",\n    ""with_padded"",\n    ""with_flatten"",\n    ""with_debug"",\n    ""remap_ids"",\n]\n'"
thinc/layers/add.py,0,"b'from typing import Tuple, Callable, Optional, TypeVar, Dict\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import ArrayXd, XY_XY_OutT\nfrom ..util import get_width\n\n\nInT = TypeVar(""InT"", bound=ArrayXd)\nOutT = TypeVar(""OutT"", bound=ArrayXd)\n\n\n@registry.layers(""add.v1"")\ndef add(\n    layer1: Model[InT, OutT], layer2: Model[InT, OutT], *layers: Model\n) -> Model[InT, XY_XY_OutT]:\n    """"""Compose two or more models `f`, `g`, etc, such that their outputs are\n    added, i.e. `add(f, g)(x)` computes `f(x) + g(x)`.\n    """"""\n    layers = (layer1, layer2) + layers\n    if layers[0].name == ""add"":\n        layers[0].layers.extend(layers[1:])\n        return layers[0]\n\n    # only add an nI dimension if each sub-layer has one\n    dims: Dict[str, Optional[int]] = {""nO"": None}\n    if all(node.has_dim(""nI"") in [True, None] for node in layers):\n        dims = {""nO"": None, ""nI"": None}\n\n    return Model(""add"", forward, init=init, dims=dims, layers=layers)\n\n\ndef forward(model: Model[InT, InT], X: InT, is_train: bool) -> Tuple[InT, Callable]:\n    if not model.layers:\n        return X, lambda dY: dY\n    Y, first_callback = model.layers[0](X, is_train=is_train)\n    callbacks = []\n    for layer in model.layers[1:]:\n        layer_Y, layer_callback = layer(X, is_train=is_train)\n        Y += layer_Y\n        callbacks.append(layer_callback)\n\n    def backprop(dY: InT) -> InT:\n        dX = first_callback(dY)\n        for callback in callbacks:\n            dX += callback(dY)\n        return dX\n\n    return Y, backprop\n\n\ndef init(\n    model: Model[InT, InT], X: Optional[InT] = None, Y: Optional[InT] = None\n) -> Model[InT, InT]:\n    if X is not None:\n        if model.has_dim(""nI"") is not False:\n            model.set_dim(""nI"", get_width(X))\n        for layer in model.layers:\n            if layer.has_dim(""nI"") is not False:\n                layer.set_dim(""nI"", get_width(X))\n    for layer in model.layers:\n        layer.initialize(X=X, Y=Y)\n    model.set_dim(""nO"", model.layers[0].get_dim(""nO""))\n    return model\n'"
thinc/layers/array_getitem.py,0,"b'from typing import Union, Sequence, Tuple\nfrom ..types import ArrayXd, FloatsXd, IntsXd\nfrom ..model import Model\n\n\nAxisIndex = Union[int, slice, Sequence[int]]\nIndex = Union[AxisIndex, Tuple[AxisIndex, ...]]\n\n\ndef array_getitem(index: Index) -> Model[ArrayXd, ArrayXd]:\n    """"""Index into input arrays, and return the subarrays.\n\n    index:\n        A valid numpy-style index. Multi-dimensional indexing can be performed\n        by passing in a tuple, and slicing can be performed using the slice object.\n        For instance, X[:, :-1] would be (slice(None, None), slice(None, -1)).\n    """"""\n    return Model(""array-getitem"", forward, attrs={""index"": index})\n\n\ndef floats_getitem(index: Index) -> Model[FloatsXd, FloatsXd]:\n    """"""Index into input arrays, and return the subarrays.\n\n    This delegates to `array_getitem`, but allows type declarations.\n    """"""\n    return Model(""floats-getitem"", forward, attrs={""index"": index})\n\n\ndef ints_getitem(index: Index) -> Model[IntsXd, IntsXd]:\n    """"""Index into input arrays, and return the subarrays.\n\n    This delegates to `array_getitem`, but allows type declarations.\n    """"""\n    return Model(""ints-getitem"", forward, attrs={""index"": index})\n\n\ndef forward(model, X, is_train):\n    index = model.attrs[""index""]\n    shape = X.shape\n    dtype = X.dtype\n\n    def backprop_get_column(dY):\n        dX = model.ops.alloc(shape, dtype=dtype)\n        dX[index] = dY\n        return dX\n\n    Y = X[index]\n    return Y, backprop_get_column\n'"
thinc/layers/bidirectional.py,0,"b'from typing import Optional, Tuple, Callable, cast\n\nfrom ..backends import Ops\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Padded\n\n\nInT = Padded\nOutT = Padded\n\n\n@registry.layers(""bidirectional.v1"")\ndef bidirectional(\n    l2r: Model[InT, OutT], r2l: Optional[Model[InT, OutT]] = None\n) -> Model[InT, OutT]:\n    """"""Stitch two RNN models into a bidirectional layer. Expects squared sequences.""""""\n    if r2l is None:\n        r2l = l2r.copy()\n    return Model(f""bi{l2r.name}"", forward, layers=[l2r, r2l], init=init)\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    l2r, r2l = model.layers\n    X_rev = _reverse(model.ops, X)\n    l2r_Z, bp_l2r_Z = l2r(X, is_train)\n    r2l_Z, bp_r2l_Z = r2l(X_rev, is_train)\n    Z = _concatenate(model.ops, l2r_Z, r2l_Z)\n\n    def backprop(dZ: OutT) -> InT:\n        d_l2r_Z, d_r2l_Z = _split(model.ops, dZ)\n        dX_l2r = bp_l2r_Z(d_l2r_Z)\n        dX_r2l = bp_r2l_Z(d_r2l_Z)\n        return _sum(dX_l2r, dX_r2l)\n\n    return Z, backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    (Y1, Y2) = _split(model.ops, Y) if Y is not None else (None, None)\n    model.layers[0].initialize(X=X, Y=Y1)\n    model.layers[1].initialize(X=X, Y=Y2)\n    return model\n\n\ndef _reverse(ops: Ops, Xp: Padded) -> Padded:\n    return Padded(Xp.data[::1], Xp.size_at_t, Xp.lengths, Xp.indices)\n\n\ndef _concatenate(ops: Ops, l2r: Padded, r2l: Padded) -> Padded:\n    return Padded(\n        ops.xp.concatenate((l2r.data, r2l.data), axis=-1),\n        l2r.size_at_t,\n        l2r.lengths,\n        l2r.indices,\n    )\n\n\ndef _split(ops: Ops, Xp: Padded) -> Tuple[Padded, Padded]:\n    half = Xp.data.shape[-1] // 2\n    # I don\'t know how to write these ellipsis in the overloads :(\n    X_l2r = Xp.data[cast(Tuple[slice, slice], (..., slice(None, half)))]\n    X_r2l = Xp.data[cast(Tuple[slice, slice], (..., slice(half)))]\n    return (\n        Padded(X_l2r, Xp.size_at_t, Xp.lengths, Xp.indices),\n        Padded(X_r2l, Xp.size_at_t, Xp.lengths, Xp.indices),\n    )\n\n\ndef _sum(Xp: Padded, Yp: Padded) -> Padded:\n    return Padded(Xp.data + Yp.data, Xp.size_at_t, Xp.lengths, Xp.indices)\n'"
thinc/layers/cauchysimilarity.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats1d, Floats2d\nfrom ..util import get_width\n\n\nInT = Tuple[Floats2d, Floats2d]\nOutT = Floats1d\n\n\n@registry.layers(""CauchySimilarity.v1"")\ndef CauchySimilarity(nI: Optional[int] = None) -> Model[InT, OutT]:\n    """"""Compare input vectors according to the Cauchy similarity function proposed by\n    Chen (2013). Primarily used within Siamese neural networks.\n    """"""\n    return Model(\n        ""cauchy_similarity"",\n        forward,\n        init=init,\n        dims={""nI"": nI, ""nO"": 1},\n        params={""W"": None},\n    )\n\n\ndef forward(\n    model: Model[InT, OutT], X1_X2: InT, is_train: bool\n) -> Tuple[OutT, Callable]:\n    X1, X2 = X1_X2\n    W = cast(Floats2d, model.get_param(""W""))\n    diff = X1 - X2\n    square_diff = diff ** 2\n    total = (W * square_diff).sum(axis=1)  # type: ignore\n    sim, bp_sim = inverse(total)\n\n    def backprop(d_sim: OutT) -> InT:\n        d_total = bp_sim(d_sim)\n        d_total = model.ops.reshape2f(d_total, -1, 1)\n        model.inc_grad(""W"", (d_total * square_diff).sum(axis=0))\n        d_square_diff = W * d_total\n        d_diff = 2 * d_square_diff * diff\n        return (d_diff, -d_diff)\n\n    return sim, backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X[0]))\n    # Initialize weights to 1\n    W = model.ops.alloc1f(model.get_dim(""nI""))\n    W += 1\n    model.set_param(""W"", W)\n    return model\n\n\ndef inverse(total: OutT) -> Tuple[OutT, Callable]:\n    inv = 1.0 / (1 + total)\n\n    def backward(d_inverse: OutT) -> OutT:\n        return d_inverse * (-1 / (total + 1) ** 2)\n\n    return inv, backward\n'"
thinc/layers/chain.py,0,"b'from typing import Tuple, Callable, Optional, TypeVar, Any, Dict\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..util import get_width\nfrom ..types import XY_YZ_OutT\n\n\nInT = TypeVar(""InT"")\nOutT = TypeVar(""OutT"")\nMidT = TypeVar(""MidT"")\n\n\n# Keep this function so we can provide variable arguments via the config\n@registry.layers(""chain.v1"")\ndef chain_no_types(*layer: Model) -> Model:\n    return chain(*layer)\n\n\ndef chain(\n    layer1: Model[InT, MidT], layer2: Model[MidT, OutT], *layers: Model\n) -> Model[InT, XY_YZ_OutT]:\n    """"""Compose two models `f` and `g` such that they become layers of a single\n    feed-forward model that computes `g(f(x))`.\n    Also supports chaining more than 2 layers.\n    """"""\n    layers = (layer1, layer2) + layers\n    dims: Dict[str, Optional[int]] = {""nO"": None}\n    # set input dimension only if first layer has one - should be ""False"" otherwise\n    if layers[0].has_dim(""nI"") is True:\n        dims[""nI""] = layers[0].get_dim(""nI"")\n    if layers[0].has_dim(""nI"") is None:\n        dims[""nI""] = None\n    # set output dimension according to last layer\n    if layers[-1].has_dim(""nO"") is True:\n        dims[""nO""] = layers[-1].get_dim(""nO"")\n\n    model: Model[InT, Any] = Model(\n        "">>"".join(layer.name for layer in layers),\n        forward,\n        init=init,\n        dims=dims,\n        layers=layers,\n    )\n    return model\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    """"""Apply the layers of `model` in sequence, feeding the output from one\n    layer into the next.\n    """"""\n    callbacks = []\n    for layer in model.layers:\n        Y, inc_layer_grad = layer(X, is_train=is_train)\n        callbacks.append(inc_layer_grad)\n        X = Y\n\n    def backprop(dY: OutT) -> InT:\n        for callback in reversed(callbacks):\n            dX = callback(dY)\n            dY = dX\n        return dX\n\n    return Y, backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    if X is None and Y is None:\n        for layer in model.layers:\n            layer.initialize()\n        if model.layers[0].has_dim(""nI""):\n            model.set_dim(""nI"", model.layers[0].get_dim(""nI""))\n        if model.layers[-1].has_dim(""nO""):\n            model.set_dim(""nO"", model.layers[-1].get_dim(""nO""))\n        return model\n\n    # Try to set nO on each layer, where available.\n    # Shape inference is tricky, especially for the output. The policy is:\n    # if a layer has an unset nO, we use the final Y (if provided). For other\n    # layers, Y=None.\n    curr_input = X\n    for layer in model.layers:\n        if layer.has_dim(""nO"") is None:\n            layer.initialize(X=curr_input, Y=Y)\n        else:\n            layer.initialize(X=curr_input)\n        if curr_input is not None:\n            curr_input = layer.predict(curr_input)\n\n    if model.layers[0].has_dim(""nI""):\n        model.set_dim(""nI"", model.layers[0].get_dim(""nI""))\n    if model.has_dim(""nO"") is None:\n        try:\n            nO = get_width(curr_input)  # type: ignore\n        except ValueError:\n            if model.layers[-1].has_dim(""nO""):\n                nO = model.layers[-1].get_dim(""nO"")\n            else:\n                nO = None  # type: ignore\n        model.set_dim(""nO"", nO)\n    return model\n'"
thinc/layers/clone.py,0,"b'from typing import TypeVar, cast, List\n\nfrom .noop import noop\nfrom .chain import chain\nfrom ..model import Model\nfrom ..config import registry\n\n\nInT = TypeVar(""InT"")\nOutT = TypeVar(""OutT"")\n\n\n@registry.layers(""clone.v1"")\ndef clone(orig: Model[InT, OutT], n: int) -> Model[InT, OutT]:\n    """"""Construct `n` copies of a layer, with distinct weights.  i.e.\n    `clone(f, 3)(x)` computes f(f\'(f\'\'(x))).\n    """"""\n    if n == 0:\n        return cast(Model[InT, OutT], noop())\n    elif n == 1:\n        return orig\n    layers: List[Model] = [orig]\n    for i in range(n - 1):\n        layers.append(orig.copy())\n    return cast(Model[InT, OutT], chain(*layers))\n'"
thinc/layers/concatenate.py,0,"b'from typing import Tuple, Callable, Optional, TypeVar, cast, Dict\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Array2d\nfrom ..util import get_width\nfrom .noop import noop\nfrom ..types import XY_XY_OutT\n\n\nInT = TypeVar(""InT"", bound=Array2d)\nOutT = TypeVar(""OutT"", bound=Array2d)\n\n\n@registry.layers(""concatenate.v1"")\ndef concatenate(*layers: Model) -> Model[InT, XY_XY_OutT]:\n    """"""Compose two or more models `f`, `g`, etc, such that their outputs are\n    concatenated, i.e. `concatenate(f, g)(x)` computes `hstack(f(x), g(x))`.\n    Also supports chaining more than 2 layers.\n    """"""\n    if not layers:\n        return cast(Model[InT, XY_XY_OutT], noop())\n    elif len(layers) == 1:\n        return layers[0]\n    elif layers[0]._func is forward:\n        layers[0].layers.extend(layers[1:])\n        return layers[0]\n\n    # only add an nI dimension if each sub-layer has one\n    dims: Dict[str, Optional[int]] = {""nO"": None}\n    if all(node.has_dim(""nI"") in [True, None] for node in layers):\n        dims = {""nO"": None, ""nI"": None}\n\n    return Model(\n        ""|"".join(layer.name for layer in layers),\n        forward,\n        init=init,\n        dims=dims,\n        layers=layers,\n    )\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Ys, callbacks = zip(*[layer(X, is_train=is_train) for layer in model.layers])\n    if isinstance(Ys[0], list):\n        return _list_forward(model, X, Ys, callbacks, is_train)\n    else:\n        return _array_forward(model, X, Ys, callbacks, is_train)\n\n\ndef _array_forward(\n    model: Model[InT, OutT], X, Ys, callbacks, is_train: bool\n) -> Tuple[OutT, Callable]:\n    widths = [Y.shape[1] for Y in Ys]\n    output = model.ops.xp.hstack(Ys)\n\n    def backprop(d_output: OutT) -> InT:\n        dY = model.ops.xp.ascontiguousarray(d_output[:, : widths[0]])\n        dX = callbacks[0](dY)\n        start = widths[0]\n        for bwd, width in zip(callbacks[1:], widths[1:]):\n            dY = model.ops.xp.ascontiguousarray(d_output[:, start : start + width])\n            dX += bwd(dY)\n            start += width\n        return dX\n\n    return output, backprop\n\n\ndef _list_forward(\n    model: Model[InT, OutT], X, Ys, callbacks, is_train: bool\n) -> Tuple[OutT, Callable]:\n    lengths = model.ops.asarray1i([len(x) for x in X])\n    Ys = [model.ops.xp.concatenate(Y, axis=0) for Y in Ys]\n    widths = [Y.shape[1] for Y in Ys]\n    output = model.ops.xp.hstack(Ys)\n    output = model.ops.unflatten(output, lengths)\n\n    def backprop(d_output: OutT) -> InT:\n        d_output = model.ops.xp.concatenate(d_output, axis=0)\n        dY = model.ops.as_contig(d_output[:, : widths[0]])\n        # We want to generalize unflatten later.\n        dY = model.ops.asarray(model.ops.unflatten(dY, lengths))  # type: ignore\n        dX = callbacks[0](dY)\n        start = widths[0]\n        for bwd, width in zip(callbacks[1:], widths[1:]):\n            dY = model.ops.as_contig(d_output[:, start : start + width])\n            dY = model.ops.asarray(model.ops.unflatten(dY, lengths))  # type: ignore\n            dX += bwd(dY)\n            start += width\n        return dX\n\n    return output, backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    if X is not None:\n        if model.has_dim(""nI"") is not False:\n            model.set_dim(""nI"", get_width(X))\n        for layer in model.layers:\n            if layer.has_dim(""nI"") is not False:\n                layer.set_dim(""nI"", get_width(X))\n    for layer in model.layers:\n        layer.initialize(X=X, Y=Y)\n    if all([layer.has_dim(""nO"") for layer in model.layers]):\n        model.set_dim(""nO"", sum(layer.get_dim(""nO"") for layer in model.layers))\n    return model\n'"
thinc/layers/dropout.py,0,"b'from typing import Tuple, Callable, List, TypeVar, Any\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import ArrayXd, Ragged, Padded\n\n\nInT = TypeVar(""InT"")\nArrayT = TypeVar(""ArrayT"", bound=ArrayXd)\n\n\n@registry.layers(""Dropout.v1"")\ndef Dropout(rate: float = 0.0) -> Model[InT, InT]:\n    """"""Help prevent overfitting by adding a random distortion to the input data\n    during training.  Specifically, cells of the input are zeroed with\n    probability determined by the `rate` argument.\n    """"""\n    return Model(""dropout"", forward, attrs={""dropout_rate"": rate, ""is_enabled"": True})\n\n\n# We\'re getting type hell here, I think because of the instance checks?\n# It\'s sort of painful, because I think this confused the types of other\n# layers that are trying to use dropout.\n# I\'ve relaxed the types for now, but it\'d be good to understand what\'s wrong\n# here.\ndef forward(model: Model, X, is_train: bool) -> Tuple[Any, Callable]:\n    rate = model.attrs[""dropout_rate""]\n    is_enabled = model.attrs[""is_enabled""]\n    if rate == 0 or not is_enabled:\n        return X, lambda dY: dY\n    elif isinstance(X, Ragged):\n        return _dropout_ragged(model, X, is_train)\n    elif isinstance(X, Padded):\n        return _dropout_padded(model, X, is_train)\n    elif isinstance(X, list):\n        return _dropout_lists(model, X, is_train)\n    else:\n        return _dropout_array(model, X, is_train)\n\n\ndef _dropout_array(\n    model: Model[ArrayT, ArrayT], X: ArrayT, is_train: bool\n) -> Tuple[ArrayT, Callable]:\n    rate = model.attrs[""dropout_rate""]\n    mask = model.ops.get_dropout_mask(X.shape, rate)\n\n    def backprop(dY: ArrayT) -> ArrayT:\n        return dY * mask\n\n    return X * mask, backprop\n\n\ndef _dropout_padded(\n    model: Model, Xp: Padded, is_train: bool\n) -> Tuple[Padded, Callable]:\n    X = Xp.data\n    mask = model.ops.get_dropout_mask(X.shape, model.attrs[""dropout_rate""])\n    Y = X * mask\n\n    def backprop(dYp: Padded) -> Padded:\n        return Padded(dYp.data * mask, dYp.size_at_t, dYp.lengths, dYp.indices)\n\n    return Padded(Y, Xp.size_at_t, Xp.lengths, Xp.indices), backprop\n\n\ndef _dropout_ragged(\n    model: Model, Xr: Ragged, is_train: bool\n) -> Tuple[Ragged, Callable]:\n    X = Xr.data\n    lengths = Xr.lengths\n    mask = model.ops.get_dropout_mask(X.shape, model.attrs[""dropout_rate""])\n    Y = X * mask\n\n    def backprop(dYr: Ragged) -> Ragged:\n        return Ragged(dYr.data * mask, dYr.lengths)\n\n    return Ragged(Y, lengths), backprop\n\n\ndef _dropout_lists(\n    model: Model[ArrayT, ArrayT], Xs: List[ArrayT], is_train: bool\n) -> Tuple[List[ArrayT], Callable]:\n    rate = model.attrs[""dropout_rate""]\n    masks = [model.ops.get_dropout_mask(X.shape, rate) for X in Xs]\n    Ys = [X * mask for X, mask in zip(Xs, masks)]\n\n    def backprop(dYs: List[ArrayT]) -> List[ArrayT]:\n        return [dY * mask for dY, mask in zip(dYs, masks)]\n\n    return Ys, backprop\n'"
thinc/layers/embed.py,0,"b'from typing import Dict, Callable, Tuple, Optional, Union, cast\n\nfrom .chain import chain\nfrom .array_getitem import ints_getitem\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Ints1d, Ints2d, Floats1d, Floats2d\nfrom ..initializers import uniform_init\nfrom ..util import get_width, partial\n\n\nInT = Union[Ints1d, Ints2d]\nOutT = Floats2d\n\n\n@registry.layers(""Embed.v1"")\ndef Embed(\n    nO: Optional[int] = None,\n    nV: Optional[int] = None,\n    *,\n    column: Optional[int] = None,\n    initializer: Callable = uniform_init,\n    dropout: Optional[float] = None\n) -> Model[InT, OutT]:\n    """"""Map integers to vectors, using a fixed-size lookup table.""""""\n    attrs: Dict[str, Union[None, int, float]] = {}\n    if dropout is not None:\n        attrs[""dropout_rate""] = dropout\n    model = Model(  # type: ignore\n        ""embed"",\n        forward,\n        init=partial(init, initializer),\n        attrs=attrs,\n        dims={""nO"": nO, ""nV"": nV},\n        params={""E"": None},\n    )\n    if column is not None:\n        # This is equivalent to array[:, column]. What you\'re actually doing\n        # there is passing in a tuple: array[(:, column)], except in the context\n        # of array indexing, the "":"" creates an object slice(0, None).\n        # So array[:, column] is array.__getitem__(slice(0), column).\n        model = chain(ints_getitem((slice(0, None), column)), model)\n    model.attrs[""column""] = column\n    return cast(Model[InT, OutT], model)\n\n\ndef forward(\n    model: Model[InT, OutT], ids: Ints1d, is_train: bool\n) -> Tuple[OutT, Callable]:\n    vectors = cast(Floats2d, model.get_param(""E""))\n    nO = vectors.shape[1]\n    nN = ids.shape[0]\n    dropout: Optional[float] = model.attrs.get(""dropout_rate"")\n    output = vectors[ids]\n    drop_mask = cast(Floats1d, model.ops.get_dropout_mask((nO,), dropout))\n    output *= drop_mask\n\n    def backprop(d_output: OutT) -> Ints1d:\n        d_output *= drop_mask\n        d_vectors = model.ops.alloc2f(*vectors.shape)\n        model.ops.scatter_add(d_vectors, ids, d_output)\n        model.inc_grad(""E"", d_vectors)\n        dX = model.ops.alloc1i(nN)\n        return dX\n\n    return output, backprop\n\n\ndef init(\n    initializer: Callable,\n    model: Model[InT, OutT],\n    X: Optional[Ints1d] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y))\n    shape = (model.get_dim(""nV""), model.get_dim(""nO""))\n    model.set_param(""E"", initializer(model.ops, shape))\n    return model\n'"
thinc/layers/expand_window.py,0,"b'from typing import Tuple, Callable\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats2d\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""expand_window.v1"")\ndef expand_window(window_size: int = 1) -> Model[InT, OutT]:\n    """"""For each vector in an input, construct an output vector that contains the\n    input and a window of surrounding vectors. This is one step in a convolution.\n    """"""\n    return Model(""expand_window"", forward, attrs={""window_size"": window_size})\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    nW = model.attrs[""window_size""]\n    Y = model.ops.seq2col(X, nW)\n\n    def backprop(dY: OutT) -> InT:\n        return model.ops.backprop_seq2col(dY, nW)\n\n    return Y, backprop\n'"
thinc/layers/featureextractor.py,0,"b'from typing import List, Union, Callable, Tuple\n\nfrom ..types import Ints2d, Doc\nfrom ..model import Model\nfrom ..config import registry\n\n\nInT = List[Doc]\nOutT = List[Ints2d]\n\n\n@registry.layers(""FeatureExtractor.v1"")\ndef FeatureExtractor(columns: List[Union[int, str]]) -> Model[InT, OutT]:\n    return Model(""extract_features"", forward, attrs={""columns"": columns})\n\n\ndef forward(model: Model[InT, OutT], docs, is_train: bool) -> Tuple[OutT, Callable]:\n    columns = model.attrs[""columns""]\n    features: OutT = []\n    for doc in docs:\n        if hasattr(doc, ""to_array""):\n            attrs = doc.to_array(columns)\n        else:\n            attrs = doc.doc.to_array(columns)[doc.start : doc.end]\n        features.append(model.ops.asarray2i(attrs, dtype=""uint64""))\n\n    backprop: Callable[[OutT], List] = lambda d_features: []\n    return features, backprop\n'"
thinc/layers/hashembed.py,0,"b'from typing import Callable, Dict, Tuple, Optional, Any, Union, cast\n\nfrom .chain import chain\nfrom .array_getitem import ints_getitem\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats1d, Floats2d, Ints2d, Ints1d\nfrom ..initializers import uniform_init\nfrom ..util import partial\n\n\nInT = Union[Ints2d, Ints1d]\nOutT = Floats2d\n\n\n@registry.layers(""HashEmbed.v1"")\ndef HashEmbed(\n    nO: int,\n    nV: int,\n    *,\n    seed: Optional[int] = None,\n    column: Optional[int] = None,\n    initializer: Callable = uniform_init,\n    dropout: Optional[float] = None\n) -> Model[InT, OutT]:\n    attrs: Dict[str, Any] = {""column"": column, ""seed"": seed}\n    if dropout is not None:\n        attrs[""dropout_rate""] = dropout\n    model = Model(  # type: ignore\n        ""hashembed"",\n        forward,\n        init=partial(init, initializer),\n        params={""E"": None},\n        dims={""nO"": nO, ""nV"": nV, ""nI"": None},\n        attrs=attrs,\n    )\n    if seed is None:\n        model.attrs[""seed""] = model.id\n    if column is not None:\n        # This is equivalent to array[:, column]. What you\'re actually doing\n        # there is passing in a tuple: array[(:, column)], except in the context\n        # of array indexing, the "":"" creates an object slice(0, None).\n        # So array[:, column] is array.__getitem__(slice(0), column).\n        model = chain(ints_getitem((slice(0, None), column)), model)\n    model.attrs[""column""] = column\n    return cast(Model[InT, OutT], model)\n\n\ndef forward(\n    model: Model[InT, OutT], ids: Ints1d, is_train: bool\n) -> Tuple[OutT, Callable]:\n    ids = model.ops.as_contig(ids, dtype=""uint64"")  # type: ignore\n    vectors = cast(Floats2d, model.get_param(""E""))\n    nV = vectors.shape[0]\n    nO = vectors.shape[1]\n    nN = ids.shape[0]\n    seed: int = model.attrs[""seed""]\n    keys = model.ops.hash(ids, seed) % nV\n    dropout: Optional[float] = model.attrs.get(""dropout_rate"")\n    drop_mask = cast(Floats1d, model.ops.get_dropout_mask((nO,), dropout))\n    output = vectors[keys].sum(axis=1)\n    output *= drop_mask\n\n    def backprop(d_vectors: OutT) -> Ints1d:\n        d_vectors *= drop_mask\n        dE = model.ops.alloc2f(*vectors.shape)\n        keysT = model.ops.as_contig(keys.T, dtype=""i"")\n        for i in range(keysT.shape[0]):\n            model.ops.scatter_add(dE, keysT[i], d_vectors)\n        model.inc_grad(""E"", dE)\n        dX = model.ops.alloc1i(nN)\n        return dX\n\n    return output, backprop\n\n\ndef init(\n    initializer: Callable,\n    model: Model[InT, OutT],\n    X: Optional[Ints1d] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    E = initializer(model.ops, (model.get_dim(""nV"") + 1, model.get_dim(""nO"")))\n    model.set_param(""E"", E)\n    return model\n'"
thinc/layers/layernorm.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats2d\nfrom ..backends import Ops\nfrom ..util import get_width\n\n\nInT = Floats2d\n\n\n@registry.layers(""LayerNorm.v1"")\ndef LayerNorm(nI: Optional[int] = None) -> Model[InT, InT]:\n    return Model(\n        ""layernorm"", forward, init=init, dims={""nI"": nI}, params={""G"": None, ""b"": None}\n    )\n\n\ndef forward(model: Model[InT, InT], X: InT, is_train: bool) -> Tuple[InT, Callable]:\n    N, mu, var = _get_moments(model.ops, X)\n    Xhat = (X - mu) * var ** (-1.0 / 2.0)\n    Y, backprop_rescale = _begin_update_scale_shift(model, Xhat)\n\n    def backprop(dY: InT) -> InT:\n        dY = backprop_rescale(dY)\n        dist, sum_dy, sum_dy_dist = _get_d_moments(model.ops, dY, X, mu)\n        d_xhat = N * dY - sum_dy - dist * var ** (-1.0) * sum_dy_dist\n        d_xhat *= var ** (-1.0 / 2)\n        d_xhat /= N\n        return d_xhat\n\n    return Y, backprop\n\n\ndef init(\n    model: Model[InT, InT], X: Optional[InT] = None, Y: Optional[InT] = None\n) -> Model[InT, InT]:\n    if X is not None:\n        X_width = get_width(X)\n        model.set_dim(""nI"", X_width)\n    if Y is not None:\n        Y_width = get_width(Y)\n        model.set_dim(""nI"", Y_width)\n    nI = model.get_dim(""nI"")\n    model.set_param(""G"", model.ops.alloc1f(nI) + 1)\n    model.set_param(""b"", model.ops.alloc1f(nI))\n    return model\n\n\ndef _begin_update_scale_shift(model: Model[InT, InT], X: InT) -> Tuple[InT, Callable]:\n    G = model.get_param(""G"")\n    b = model.get_param(""b"")\n    Y = X * G\n    Y += b\n\n    def finish_update_scale_shift(dY: InT) -> InT:\n        model.inc_grad(""b"", dY.sum(axis=0))\n        model.inc_grad(""G"", (dY * X).sum(axis=0))\n        return dY * G\n\n    return Y, finish_update_scale_shift\n\n\ndef _get_moments(ops: Ops, X: Floats2d) -> Tuple[Floats2d, Floats2d, Floats2d]:\n    # TODO: Do mean methods\n    mu: Floats2d = X.mean(axis=1, keepdims=True)\n    var: Floats2d = X.var(axis=1, keepdims=True) + 1e-08\n    return cast(Floats2d, ops.asarray_f([X.shape[1]])), mu, var\n\n\ndef _get_d_moments(\n    ops: Ops, dy: Floats2d, X: Floats2d, mu: Floats2d\n) -> Tuple[Floats2d, Floats2d, Floats2d]:\n    dist = X - mu\n    return (\n        dist,\n        ops.xp.sum(dy, axis=1, keepdims=True),\n        ops.xp.sum(dy * dist, axis=1, keepdims=True),\n    )\n'"
thinc/layers/linear.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats1d, Floats2d\nfrom ..initializers import glorot_uniform_init, zero_init\nfrom ..util import get_width, partial\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""Linear.v1"")\ndef Linear(\n    nO: Optional[int] = None,\n    nI: Optional[int] = None,\n    *,\n    init_W: Callable = glorot_uniform_init,\n    init_b: Callable = zero_init,\n) -> Model[InT, OutT]:\n    """"""Multiply inputs by a weights matrix and adds a bias vector.""""""\n    return Model(\n        ""linear"",\n        forward,\n        init=partial(init, init_W, init_b),\n        dims={""nO"": nO, ""nI"": nI},\n        params={""W"": None, ""b"": None},\n    )\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    W = cast(Floats2d, model.get_param(""W""))\n    b = cast(Floats1d, model.get_param(""b""))\n    Y = model.ops.gemm(X, W, trans2=True)\n    Y += b\n\n    def backprop(dY: OutT) -> InT:\n        model.inc_grad(""b"", dY.sum(axis=0))\n        model.inc_grad(""W"", model.ops.gemm(dY, X, trans1=True))\n        return model.ops.gemm(dY, W)\n\n    return Y, backprop\n\n\ndef init(\n    init_W: Callable,\n    init_b: Callable,\n    model: Model[InT, OutT],\n    X: Optional[InT] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X))\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y))\n    model.set_param(""W"", init_W(model.ops, (model.get_dim(""nO""), model.get_dim(""nI""))))\n    model.set_param(""b"", init_b(model.ops, (model.get_dim(""nO""),)))\n    return model\n'"
thinc/layers/list2array.py,0,"b'from typing import Tuple, Callable\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Array2d, List2d\n\n\nInT = List2d\nOutT = Array2d\n\n\n@registry.layers(""list2array.v1"")\ndef list2array() -> Model[InT, OutT]:\n    """"""Transform sequences to ragged arrays if necessary and return the data\n    from the ragged array. If sequences are already ragged, do nothing. A\n    ragged array is a tuple (data, lengths), where data is the concatenated data.\n    """"""\n    return Model(""list2array"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xs: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    lengths = model.ops.asarray1i([len(x) for x in Xs])\n\n    def backprop(dY: OutT) -> InT:\n        return model.ops.unflatten(dY, lengths)  # type: ignore\n\n    return model.ops.flatten(Xs), backprop  # type: ignore\n'"
thinc/layers/list2padded.py,0,"b'from typing import Tuple, Callable\n\nfrom ..types import Padded, List2d\nfrom ..model import Model\nfrom ..config import registry\n\n\nInT = List2d\nOutT = Padded\n\n\n@registry.layers(""list2padded.v1"")\ndef list2padded() -> Model[InT, OutT]:\n    """"""Create a layer to convert a list of array inputs into Padded.""""""\n    return Model(f""list2padded"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xs: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Yp = model.ops.list2padded(Xs)  # type: ignore\n\n    def backprop(dYp: OutT) -> InT:\n        return model.ops.padded2list(dYp)  # type: ignore\n\n    return Yp, backprop\n'"
thinc/layers/list2ragged.py,0,"b'from typing import Tuple, List, Callable, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats2d, Ragged\n\n\nInT = List[Floats2d]\nOutT = Ragged\n\n\n@registry.layers(""list2ragged.v1"")\ndef list2ragged() -> Model[InT, OutT]:\n    """"""Transform sequences to ragged arrays if necessary and return the ragged\n    array. If sequences are already ragged, do nothing. A ragged array is a\n    tuple (data, lengths), where data is the concatenated data.\n    """"""\n    return Model(""list2ragged"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xs: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    def backprop(dYr: OutT) -> InT:\n        # TODO: Unhack\n        return model.ops.unflatten(cast(Floats2d, dYr.data), dYr.lengths)\n\n    lengths = model.ops.asarray1i([len(x) for x in Xs])\n    return Ragged(model.ops.flatten(Xs), lengths), backprop\n'"
thinc/layers/logistic.py,0,"b'from typing import Tuple, Callable\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats2d\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""Logistic.v1"")\ndef Logistic() -> Model[InT, OutT]:\n    return Model(""logistic"", forward)\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Y = model.ops.sigmoid(X, inplace=False)\n\n    def backprop(dY: OutT) -> InT:\n        return dY * model.ops.dsigmoid(Y, inplace=False)\n\n    return Y, backprop\n'"
thinc/layers/lstm.py,3,"b'from typing import Optional, Tuple, Callable, cast\nfrom functools import partial\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..util import get_width\nfrom ..types import Floats1d, Floats2d, Floats3d, Padded\nfrom .bidirectional import bidirectional\nfrom .clone import clone\nfrom .noop import noop\nfrom ..initializers import glorot_uniform_init, zero_init\n\n\n@registry.layers(""LSTM.v1"")\ndef LSTM(\n    nO: Optional[int] = None,\n    nI: Optional[int] = None,\n    *,\n    bi: bool = False,\n    depth: int = 1,\n    dropout: float = 0.0,\n    init_W=glorot_uniform_init,\n    init_b=zero_init\n) -> Model[Padded, Padded]:\n    if dropout != 0.0:\n        msg = (\n            ""LSTM dropout not implemented yet. In the meantime, use the ""\n            ""PyTorchWrapper and the torch.LSTM class.""\n        )\n        raise NotImplementedError(msg)\n\n    if bi and nO is not None:\n        nO //= 2\n    model: Model[Padded, Padded] = Model(\n        ""lstm"",\n        forward,\n        dims={""nO"": nO, ""nI"": nI},\n        attrs={""registry_name"": ""LSTM.v1""},\n        params={""W"": None, ""b"": None, ""c"": None, ""h"": None},\n        init=partial(init, init_W, init_b),\n    )\n\n    if bi:\n        model = bidirectional(model)\n    return clone(model, depth)\n\n\n@registry.layers(""PyTorchLSTM.v1"")\ndef PyTorchLSTM(\n    nO: int, nI: int, *, bi: bool = False, depth: int = 1, dropout: float = 0.0\n) -> Model[Padded, Padded]:\n    import torch.nn\n    from .with_padded import with_padded\n    from .pytorchwrapper import PyTorchRNNWrapper\n\n    if depth == 0:\n        return noop()  # type: ignore\n    if bi:\n        nO = nO // 2\n    return with_padded(\n        PyTorchRNNWrapper(\n            torch.nn.LSTM(nI, nO, depth, bidirectional=bi, dropout=dropout)\n        )\n    )\n\n\ndef init(\n    init_W: Callable,\n    init_b: Callable,\n    model: Model,\n    X: Optional[Padded] = None,\n    Y: Optional[Padded] = None,\n) -> None:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X))\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y))\n    nO = model.get_dim(""nO"")\n    nI = model.get_dim(""nI"")\n    model.set_param(""W"", init_W(model.ops, (nO * 4, nO + nI)))\n    model.set_param(""b"", init_b(model.ops, (nO * 4,)))\n    model.set_param(""h"", zero_init(model.ops, (nO,)))\n    model.set_param(""c"", zero_init(model.ops, (nO,)))\n\n\ndef forward(\n    model: Model[Floats3d, Floats3d], Xp: Padded, is_train: bool\n) -> Tuple[Padded, Callable]:\n    X = Xp.data\n    W = cast(Floats2d, model.get_param(""W""))\n    b = cast(Floats1d, model.get_param(""b""))\n    h = cast(Floats1d, model.get_param(""h""))\n    c = cast(Floats1d, model.get_param(""c""))\n    Y, fwd_state = model.ops.recurrent_lstm(W, b, h, c, X, is_train)\n    Yp = Padded(Y, Xp.size_at_t, Xp.lengths, Xp.indices)\n\n    def backprop(dYp: Padded) -> Padded:\n        dX, (dW, db, d_h, d_c) = model.ops.backprop_recurrent_lstm(\n            dYp.data, fwd_state, (W, b)\n        )\n        model.inc_grad(""W"", dW)\n        model.inc_grad(""b"", db)\n        model.inc_grad(""h"", d_h)\n        model.inc_grad(""c"", d_c)\n        return Padded(X, dYp.size_at_t, dYp.lengths, dYp.indices)\n\n    return Yp, backprop\n'"
thinc/layers/maxout.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..initializers import glorot_uniform_init, zero_init\nfrom ..types import Floats2d\nfrom ..util import get_width, partial\nfrom .dropout import Dropout\nfrom .layernorm import LayerNorm\nfrom .chain import chain\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""Maxout.v1"")\ndef Maxout(\n    nO: Optional[int] = None,\n    nI: Optional[int] = None,\n    nP: Optional[int] = 3,\n    *,\n    init_W: Callable = glorot_uniform_init,\n    init_b: Callable = zero_init,\n    dropout: Optional[float] = None,\n    normalize: bool = False,\n) -> Model[InT, OutT]:\n    model: Model[InT, OutT] = Model(\n        ""maxout"",\n        forward,\n        init=partial(init, init_W, init_b),\n        dims={""nO"": nO, ""nI"": nI, ""nP"": nP},\n        params={""W"": None, ""b"": None},\n    )\n    if normalize:\n        model = chain(model, LayerNorm(nI=nO))\n    if dropout is not None:\n        model = chain(model, cast(Model[InT, OutT], Dropout(dropout)))\n    return model\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    nO = model.get_dim(""nO"")\n    nP = model.get_dim(""nP"")\n    nI = model.get_dim(""nI"")\n    b = model.get_param(""b"")\n    W = model.get_param(""W"")\n    W = model.ops.reshape2f(W, nO * nP, nI)\n    Y = model.ops.gemm(X, W, trans2=True)\n    Y += model.ops.reshape1f(b, nO * nP)\n    Z = model.ops.reshape3f(Y, Y.shape[0], nO, nP)\n    best, which = model.ops.maxout(Z)\n\n    def backprop(d_best: OutT) -> InT:\n        dZ = model.ops.backprop_maxout(d_best, which, nP)\n        # TODO: Add sum methods for Floats3d\n        model.inc_grad(""b"", dZ.sum(axis=0))  # type: ignore\n        dY = model.ops.reshape2f(dZ, dZ.shape[0], nO * nP)\n        dX = model.ops.reshape3f(model.ops.gemm(dY, X, trans1=True), nO, nP, nI)\n        model.inc_grad(""W"", dX)\n        return model.ops.gemm(dY, model.ops.reshape2f(W, nO * nP, nI))\n\n    return best, backprop\n\n\ndef init(\n    init_W: Callable,\n    init_b: Callable,\n    model: Model[InT, OutT],\n    X: Optional[InT] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X))\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y))\n    W_shape = (model.get_dim(""nO""), model.get_dim(""nP""), model.get_dim(""nI""))\n    model.set_param(""W"", init_W(model.ops, W_shape))\n    model.set_param(""b"", init_b(model.ops, (model.get_dim(""nO""), model.get_dim(""nP""))))\n    return model\n'"
thinc/layers/mish.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..initializers import glorot_uniform_init, zero_init\nfrom ..config import registry\nfrom ..types import Floats1d, Floats2d\nfrom ..util import get_width, partial\nfrom .chain import chain\nfrom .layernorm import LayerNorm\nfrom .dropout import Dropout\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""Mish.v1"")\ndef Mish(\n    nO: Optional[int] = None,\n    nI: Optional[int] = None,\n    *,\n    init_W: Callable = glorot_uniform_init,\n    init_b: Callable = zero_init,\n    dropout: Optional[float] = None,\n    normalize: bool = False,\n) -> Model[InT, OutT]:\n    """"""Dense layer with mish activation.\n    https://arxiv.org/pdf/1908.08681.pdf\n    """"""\n    model: Model[InT, OutT] = Model(\n        ""mish"",\n        forward,\n        init=partial(init, init_W, init_b),\n        dims={""nO"": nO, ""nI"": nI},\n        params={""W"": None, ""b"": None},\n    )\n    if normalize:\n        model = chain(model, cast(Model[InT, OutT], LayerNorm(nI=nO)))\n    if dropout is not None:\n        model = chain(model, cast(Model[InT, OutT], Dropout(dropout)))\n    return model\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    W = cast(Floats2d, model.get_param(""W""))\n    b = cast(Floats1d, model.get_param(""b""))\n    Y_pre_mish = model.ops.gemm(X, W, trans2=True)\n    Y_pre_mish += b\n    Y = model.ops.mish(Y_pre_mish)\n\n    def backprop(dY: OutT) -> InT:\n        dY_pre_mish = model.ops.backprop_mish(dY, Y_pre_mish)\n        model.inc_grad(""W"", model.ops.gemm(dY_pre_mish, X, trans1=True))\n        model.inc_grad(""b"", dY_pre_mish.sum(axis=0))\n        dX = model.ops.gemm(dY_pre_mish, W)\n        return dX\n\n    return Y, backprop\n\n\ndef init(\n    init_W: Callable,\n    init_b: Callable,\n    model: Model[InT, OutT],\n    X: Optional[InT] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X))\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y))\n    model.set_param(""W"", init_W(model.ops, (model.get_dim(""nO""), model.get_dim(""nI""))))\n    model.set_param(""b"", init_b(model.ops, (model.get_dim(""nO""),)))\n    return model\n'"
thinc/layers/multisoftmax.py,0,"b'from typing import Optional, Tuple, Callable, cast\n\nfrom ..types import Floats2d, Floats1d\nfrom ..model import Model\nfrom ..config import registry\nfrom ..util import get_width\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""MultiSoftmax.v1"")\ndef MultiSoftmax(nOs: Tuple[int, ...], nI: Optional[int] = None) -> Model[InT, OutT]:\n    """"""Neural network layer that predicts several multi-class attributes at once.\n    For instance, we might predict one class with 6 variables, and another with 5.\n    We predict the 11 neurons required for this, and then softmax them such\n    that columns 0-6 make a probability distribution and columns 6-11 make another.\n    """"""\n    return Model(\n        ""multisoftmax"",\n        forward,\n        init=init,\n        dims={""nO"": sum(nOs), ""nI"": nI},\n        attrs={""nOs"": nOs},\n        params={""W"": None, ""b"": None},\n    )\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    nOs = model.attrs[""nOs""]\n    W = cast(Floats2d, model.get_param(""W""))\n    b = cast(Floats1d, model.get_param(""b""))\n\n    def backprop(dY: OutT) -> InT:\n        model.inc_grad(""W"", model.ops.gemm(dY, X, trans1=True))\n        model.inc_grad(""b"", dY.sum(axis=0))\n        return model.ops.gemm(dY, W)\n\n    Y = model.ops.gemm(X, W)\n    Y += b\n    i = 0\n    for out_size in nOs:\n        model.ops.softmax(Y[:, i : i + out_size], inplace=True)\n        i += out_size\n    return Y, backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X))\n    nO = model.get_dim(""nO"")\n    nI = model.get_dim(""nI"")\n    model.set_param(""W"", model.ops.alloc2f(nO, nI))\n    model.set_param(""b"", model.ops.alloc1f(nO))\n    return model\n'"
thinc/layers/mxnetwrapper.py,0,"b'from typing import Callable, Tuple, Optional, Any, Type\n\nfrom ..model import Model\nfrom ..shims import MXNetShim\nfrom ..config import registry\nfrom ..util import is_xp_array, is_mxnet_array\nfrom ..util import mxnet2xp, xp2mxnet, convert_recursive\nfrom ..types import ArgsKwargs\n\n\n@registry.layers(""MXNetWrapper.v1"")\ndef MXNetWrapper(\n    mxnet_model,\n    convert_inputs: Optional[Callable] = None,\n    convert_outputs: Optional[Callable] = None,\n    model_class: Type[Model] = Model,\n    model_name: str = ""mxnet"",\n) -> Model[Any, Any]:\n    """"""Wrap a MXNet model, so that it has the same API as Thinc models.\n    To optimize the model, you\'ll need to create a MXNet optimizer and call\n    optimizer.step() after each batch.\n\n    Your MXNet model\'s forward method can take arbitrary args and kwargs,\n    but must return either a single tensor as output or a tuple. You may find the\n    MXNet register_forward_hook helpful if you need to adapt the output.\n\n    The convert functions are used to map inputs and outputs to and from your\n    MXNet model. Each function should return the converted output, and a callback\n    to use during the backward pass. So:\n\n        Xmxnet, get_dX = convert_inputs(X)\n        Ymxnet, mxnet_backprop = model.shims[0](Xmxnet, is_train)\n        Y, get_dYmxnet = convert_outputs(Ymxnet)\n\n    To allow maximum flexibility, the MXNetShim expects ArgsKwargs objects\n    on the way into the forward and backward passes. The ArgsKwargs objects\n    will be passed straight into the model in the forward pass, and straight\n    into `mxnet.autograd.backward` during the backward pass.\n    """"""\n    if convert_inputs is None:\n        convert_inputs = convert_mxnet_default_inputs\n    if convert_outputs is None:\n        convert_outputs = convert_mxnet_default_outputs\n    return model_class(\n        model_name,\n        forward,\n        attrs={""convert_inputs"": convert_inputs, ""convert_outputs"": convert_outputs},\n        shims=[MXNetShim(mxnet_model)],\n    )\n\n\ndef forward(model: Model, X: Any, is_train: bool) -> Tuple[Any, Callable]:\n    """"""Return the output of the wrapped MXNet model for the given input,\n    along with a callback to handle the backward pass.\n    """"""\n    convert_inputs = model.attrs[""convert_inputs""]\n    convert_outputs = model.attrs[""convert_outputs""]\n\n    Xmxnet, get_dX = convert_inputs(model, X, is_train)\n    Ymxnet, mxnet_backprop = model.shims[0](Xmxnet, is_train)\n    Y, get_dYmxnet = convert_outputs(model, (X, Ymxnet), is_train)\n\n    def backprop(dY: Any) -> Any:\n        dYmxnet = get_dYmxnet(dY)\n        dXmxnet = mxnet_backprop(dYmxnet)\n        dX = get_dX(dXmxnet)\n        return dX\n\n    return Y, backprop\n\n\n# Default conversion functions\n\n\ndef convert_mxnet_default_inputs(\n    model: Model, X: Any, is_train: bool\n) -> Tuple[ArgsKwargs, Callable[[ArgsKwargs], Any]]:\n    xp2mxnet_ = lambda x: xp2mxnet(x, requires_grad=is_train)\n    converted = convert_recursive(is_xp_array, xp2mxnet_, X)\n    if isinstance(converted, ArgsKwargs):\n\n        def reverse_conversion(dXmxnet):\n            return convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n\n        return converted, reverse_conversion\n    elif isinstance(converted, dict):\n\n        def reverse_conversion(dXmxnet):\n            dX = convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n            return dX.kwargs\n\n        return ArgsKwargs(args=tuple(), kwargs=converted), reverse_conversion\n    elif isinstance(converted, (tuple, list)):\n\n        def reverse_conversion(dXmxnet):\n            dX = convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n            return dX.args\n\n        return ArgsKwargs(args=tuple(converted), kwargs={}), reverse_conversion\n    else:\n\n        def reverse_conversion(dXmxnet):\n            dX = convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n            return dX.args[0]\n\n        return ArgsKwargs(args=(converted,), kwargs={}), reverse_conversion\n\n\ndef convert_mxnet_default_outputs(model: Model, X_Ymxnet: Any, is_train: bool):\n    X, Ymxnet = X_Ymxnet\n    Y = convert_recursive(is_mxnet_array, mxnet2xp, Ymxnet)\n\n    def reverse_conversion(dY: Any) -> ArgsKwargs:\n        dYmxnet = convert_recursive(is_xp_array, xp2mxnet, dY)\n        return ArgsKwargs(args=((Ymxnet,),), kwargs={""head_grads"": dYmxnet})\n\n    return Y, reverse_conversion\n'"
thinc/layers/noop.py,0,"b'from typing import Tuple, Callable, TypeVar\n\nfrom ..model import Model\nfrom ..config import registry\n\n\nInOutT = TypeVar(""InOutT"")\n\n\n@registry.layers(""noop.v1"")\ndef noop(*layers: Model) -> Model[InOutT, InOutT]:\n    """"""Transform a sequences of layers into a null operation.""""""\n    return Model(""noop"", forward, layers=layers)\n\n\ndef forward(\n    model: Model[InOutT, InOutT], X: InOutT, is_train: bool\n) -> Tuple[InOutT, Callable]:\n    def backprop(dY: InOutT) -> InOutT:\n        return dY\n\n    return X, backprop\n'"
thinc/layers/padded2list.py,0,"b'from typing import Tuple, Callable\n\nfrom ..types import Padded, List2d\nfrom ..model import Model\nfrom ..config import registry\n\n\nInT = Padded\nOutT = List2d\n\n\n@registry.layers(""padded2list.v1"")\ndef padded2list() -> Model[InT, OutT]:\n    """"""Create a layer to convert a Padded input into a list of arrays.""""""\n    return Model(f""padded2list"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xp: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Ys = model.ops.padded2list(Xp)  # type: ignore\n\n    def backprop(dYs: OutT) -> InT:\n        dYp = model.ops.list2padded(dYs)  # type: ignore\n        assert isinstance(dYp, Padded)\n        return dYp\n\n    return Ys, backprop\n'"
thinc/layers/parametricattention.py,0,"b'from typing import Tuple, Callable, Optional\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Ragged\nfrom ..util import get_width\n\n\nInT = Ragged\nOutT = Ragged\n\n\n@registry.layers(""ParametricAttention.v1"")\ndef ParametricAttention(nO: Optional[int] = None) -> Model[InT, OutT]:\n    """"""Weight inputs by similarity to a learned vector""""""\n    return Model(""para-attn"", forward, init=init, params={""Q"": None}, dims={""nO"": nO})\n\n\ndef forward(model: Model[InT, OutT], Xr: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Q = model.get_param(""Q"")\n    attention, bp_attention = _get_attention(model.ops, Q, Xr.data, Xr.lengths)\n    output, bp_output = _apply_attention(model.ops, attention, Xr.data, Xr.lengths)\n\n    def backprop(dYr: OutT) -> InT:\n        dX, d_attention = bp_output(dYr.data)\n        dQ, dX2 = bp_attention(d_attention)\n        model.inc_grad(""Q"", dQ.ravel())\n        dX += dX2\n        return Ragged(dX, dYr.lengths)\n\n    return Ragged(output, Xr.lengths), backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y.data))\n    model.set_param(""Q"", model.ops.alloc1f(model.get_dim(""nO"")))\n    return model\n\n\ndef _get_attention(ops, Q, X, lengths):\n    attention = ops.gemm(X, ops.reshape2f(Q, -1, 1))\n    attention = ops.softmax_sequences(attention, lengths)\n\n    def get_attention_bwd(d_attention):\n        d_attention = ops.backprop_softmax_sequences(d_attention, attention, lengths)\n        dQ = ops.gemm(X, d_attention, trans1=True)\n        dX = ops.xp.outer(d_attention, Q)\n        return dQ, dX\n\n    return attention, get_attention_bwd\n\n\ndef _apply_attention(self, attention, X, lengths):\n    output = X * attention\n\n    def apply_attention_bwd(d_output):\n        d_attention = (X * d_output).sum(axis=1, keepdims=True)\n        dX = d_output * attention\n        return dX, d_attention\n\n    return output, apply_attention_bwd\n'"
thinc/layers/pytorchwrapper.py,2,"b'from typing import Callable, Tuple, Optional, Any, cast\n\nfrom ..model import Model\nfrom ..shims import PyTorchShim\nfrom ..config import registry\nfrom ..util import is_xp_array, is_torch_array\nfrom ..util import xp2torch, torch2xp, convert_recursive\nfrom ..types import Floats3d, ArgsKwargs, Padded\n\n\n@registry.layers(""PyTorchRNNWrapper.v1"")\ndef PyTorchRNNWrapper(\n    pytorch_model,\n    convert_inputs: Optional[Callable] = None,\n    convert_outputs: Optional[Callable] = None,\n) -> Model[Padded, Padded]:\n    """"""Wrap a PyTorch RNN model for use in Thinc.""""""\n    if convert_inputs is None:\n        convert_inputs = convert_rnn_inputs\n    if convert_outputs is None:\n        convert_outputs = convert_rnn_outputs\n    return cast(\n        Model[Padded, Padded],\n        PyTorchWrapper(\n            pytorch_model,\n            convert_inputs=convert_inputs,\n            convert_outputs=convert_outputs,\n        ),\n    )\n\n\n@registry.layers(""PyTorchWrapper.v1"")\ndef PyTorchWrapper(\n    pytorch_model,\n    convert_inputs: Optional[Callable] = None,\n    convert_outputs: Optional[Callable] = None,\n) -> Model[Any, Any]:\n    """"""Wrap a PyTorch model, so that it has the same API as Thinc models.\n    To optimize the model, you\'ll need to create a PyTorch optimizer and call\n    optimizer.step() after each batch. See examples/wrap_pytorch.py\n\n    Your PyTorch model\'s forward method can take arbitrary args and kwargs,\n    but must return either a single tensor as output or a tuple. You may find the\n    PyTorch register_forward_hook helpful if you need to adapt the output.\n\n    The convert functions are used to map inputs and outputs to and from your\n    PyTorch model. Each function should return the converted output, and a callback\n    to use during the backward pass. So:\n\n        Xtorch, get_dX = convert_inputs(X)\n        Ytorch, torch_backprop = model.shims[0](Xtorch, is_train)\n        Y, get_dYtorch = convert_outputs(Ytorch)\n\n    To allow maximum flexibility, the PyTorchShim expects ArgsKwargs objects\n    on the way into the forward and backward passed. The ArgsKwargs objects\n    will be passed straight into the model in the forward pass, and straight\n    into `torch.autograd.backward` during the backward pass.\n    """"""\n    if convert_inputs is None:\n        convert_inputs = convert_pytorch_default_inputs\n    if convert_outputs is None:\n        convert_outputs = convert_pytorch_default_outputs\n    return Model(\n        ""pytorch"",\n        forward,\n        attrs={""convert_inputs"": convert_inputs, ""convert_outputs"": convert_outputs},\n        shims=[PyTorchShim(pytorch_model)],\n    )\n\n\ndef forward(model: Model, X: Any, is_train: bool) -> Tuple[Any, Callable]:\n    """"""Return the output of the wrapped PyTorch model for the given input,\n    along with a callback to handle the backward pass.\n    """"""\n    convert_inputs = model.attrs[""convert_inputs""]\n    convert_outputs = model.attrs[""convert_outputs""]\n\n    Xtorch, get_dX = convert_inputs(model, X, is_train)\n    Ytorch, torch_backprop = model.shims[0](Xtorch, is_train)\n    Y, get_dYtorch = convert_outputs(model, (X, Ytorch), is_train)\n\n    def backprop(dY: Any) -> Any:\n        dYtorch = get_dYtorch(dY)\n        dXtorch = torch_backprop(dYtorch)\n        dX = get_dX(dXtorch)\n        return dX\n\n    return Y, backprop\n\n\n# Default conversion functions\n\n\ndef convert_pytorch_default_inputs(\n    model: Model, X: Any, is_train: bool\n) -> Tuple[ArgsKwargs, Callable[[ArgsKwargs], Any]]:\n    xp2torch_ = lambda x: xp2torch(x, requires_grad=is_train)\n    converted = convert_recursive(is_xp_array, xp2torch_, X)\n    if isinstance(converted, ArgsKwargs):\n\n        def reverse_conversion(dXtorch):\n            return convert_recursive(is_torch_array, torch2xp, dXtorch)\n\n        return converted, reverse_conversion\n    elif isinstance(converted, dict):\n\n        def reverse_conversion(dXtorch):\n            dX = convert_recursive(is_torch_array, torch2xp, dXtorch)\n            return dX.kwargs\n\n        return ArgsKwargs(args=tuple(), kwargs=converted), reverse_conversion\n    elif isinstance(converted, (tuple, list)):\n\n        def reverse_conversion(dXtorch):\n            dX = convert_recursive(is_torch_array, torch2xp, dXtorch)\n            return dX.args\n\n        return ArgsKwargs(args=tuple(converted), kwargs={}), reverse_conversion\n    else:\n\n        def reverse_conversion(dXtorch):\n            dX = convert_recursive(is_torch_array, torch2xp, dXtorch)\n            return dX.args[0]\n\n        return ArgsKwargs(args=(converted,), kwargs={}), reverse_conversion\n\n\ndef convert_pytorch_default_outputs(model: Model, X_Ytorch: Any, is_train: bool):\n    X, Ytorch = X_Ytorch\n    Y = convert_recursive(is_torch_array, torch2xp, Ytorch)\n\n    def reverse_conversion(dY: Any) -> ArgsKwargs:\n        dYtorch = convert_recursive(is_xp_array, xp2torch, dY)\n        return ArgsKwargs(args=((Ytorch,),), kwargs={""grad_tensors"": dYtorch})\n\n    return Y, reverse_conversion\n\n\n# BiLSTM conversion functions\n\n\ndef convert_rnn_inputs(model: Model, Xp: Padded, is_train: bool):\n    size_at_t = Xp.size_at_t\n    lengths = Xp.lengths\n    indices = Xp.indices\n\n    def convert_from_torch_backward(d_inputs: ArgsKwargs) -> Padded:\n        dX = torch2xp(d_inputs.args[0])\n        return Padded(dX, size_at_t, lengths, indices)  # type: ignore\n\n    output = ArgsKwargs(args=(xp2torch(Xp.data, requires_grad=True), None), kwargs={})\n    return output, convert_from_torch_backward\n\n\ndef convert_rnn_outputs(model: Model, inputs_outputs: Tuple, is_train):\n    Xp, (Ytorch, _) = inputs_outputs\n\n    def convert_for_torch_backward(dYp: Padded) -> ArgsKwargs:\n        dYtorch = xp2torch(dYp.data, requires_grad=True)\n        return ArgsKwargs(args=(Ytorch,), kwargs={""grad_tensors"": dYtorch})\n\n    Y = cast(Floats3d, torch2xp(Ytorch))\n    Yp = Padded(Y, Xp.size_at_t, Xp.lengths, Xp.indices)\n    return Yp, convert_for_torch_backward\n'"
thinc/layers/ragged2list.py,0,"b'from typing import Tuple, Callable\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Ragged, List2d\n\n\nInT = Ragged\nOutT = List2d\n\n\n@registry.layers(""ragged2list.v1"")\ndef ragged2list() -> Model[InT, OutT]:\n    """"""Transform sequences from a ragged format into lists.""""""\n    return Model(""ragged2list"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xr: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    lengths = Xr.lengths\n\n    def backprop(dXs: OutT) -> InT:\n        return Ragged(model.ops.flatten(dXs, pad=0), lengths)  # type: ignore\n\n    data = model.ops.unflatten(Xr.data, Xr.lengths)  # type: ignore\n    return data, backprop\n'"
thinc/layers/reduce_max.py,0,"b'from typing import Tuple, Callable, cast\n\nfrom ..types import Floats2d, Ragged\nfrom ..model import Model\nfrom ..config import registry\n\n\nInT = Ragged\nOutT = Floats2d\n\n\n@registry.layers(""reduce_max.v1"")\ndef reduce_max() -> Model[InT, OutT]:\n    return Model(""reduce_max"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xr: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Y, which = model.ops.reduce_max(cast(Floats2d, Xr.data), Xr.lengths)\n    lengths = Xr.lengths\n\n    def backprop(dY: OutT) -> InT:\n        return Ragged(model.ops.backprop_reduce_max(dY, which, lengths), lengths)\n\n    return Y, backprop\n'"
thinc/layers/reduce_mean.py,0,"b'from typing import Tuple, Callable, cast\n\nfrom ..types import Floats2d, Ragged\nfrom ..model import Model\nfrom ..config import registry\n\n\nInT = Ragged\nOutT = Floats2d\n\n\n@registry.layers(""reduce_mean.v1"")\ndef reduce_mean() -> Model[InT, OutT]:\n    return Model(""reduce_mean"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xr: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Y = model.ops.reduce_mean(cast(Floats2d, Xr.data), Xr.lengths)\n    lengths = Xr.lengths\n\n    def backprop(dY: OutT) -> InT:\n        return Ragged(model.ops.backprop_reduce_mean(dY, lengths), lengths)\n\n    return Y, backprop\n'"
thinc/layers/reduce_sum.py,0,"b'from typing import Callable, Tuple, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats2d, Ragged\n\n\nInT = Ragged\nOutT = Floats2d\n\n\n@registry.layers(""reduce_sum.v1"")\ndef reduce_sum() -> Model[InT, OutT]:\n    return Model(""reduce_sum"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xr: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Y = model.ops.reduce_sum(cast(Floats2d, Xr.data), Xr.lengths)\n    lengths = Xr.lengths\n\n    def backprop(dY: OutT) -> InT:\n        return Ragged(model.ops.backprop_reduce_sum(dY, lengths), lengths)\n\n    return Y, backprop\n'"
thinc/layers/relu.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..initializers import glorot_uniform_init, zero_init\nfrom ..config import registry\nfrom ..types import Floats2d, Floats1d\nfrom ..util import get_width, partial\nfrom .chain import chain\nfrom .layernorm import LayerNorm\nfrom .dropout import Dropout\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""Relu.v1"")\ndef Relu(\n    nO: Optional[int] = None,\n    nI: Optional[int] = None,\n    *,\n    init_W: Callable = glorot_uniform_init,\n    init_b: Callable = zero_init,\n    dropout: Optional[float] = None,\n    normalize: bool = False,\n) -> Model[InT, OutT]:\n    model: Model[InT, OutT] = Model(\n        ""relu"",\n        forward,\n        init=partial(init, init_W, init_b),\n        dims={""nO"": nO, ""nI"": nI},\n        params={""W"": None, ""b"": None},\n    )\n    if normalize:\n        model = chain(model, LayerNorm(nI=nO))\n    if dropout is not None:\n        model = chain(model, cast(Model[Floats2d, Floats2d], Dropout(dropout)))\n    return model\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    W = cast(Floats2d, model.get_param(""W""))\n    b = cast(Floats1d, model.get_param(""b""))\n    Y = model.ops.affine(X, W, b)\n    Y = model.ops.relu(Y)\n\n    def backprop(dY: OutT) -> InT:\n        dY = model.ops.backprop_relu(dY, Y)\n        model.inc_grad(""b"", dY.sum(axis=0))\n        model.inc_grad(""W"", model.ops.gemm(dY, X, trans1=True))\n        return model.ops.gemm(dY, W)\n\n    return Y, backprop\n\n\ndef init(\n    init_W: Callable,\n    init_b: Callable,\n    model: Model[InT, OutT],\n    X: Optional[InT] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X))\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y))\n    model.set_param(""W"", init_W(model.ops, (model.get_dim(""nO""), model.get_dim(""nI""))))\n    model.set_param(""b"", init_b(model.ops, (model.get_dim(""nO""),)))\n    return model\n'"
thinc/layers/remap_ids.py,0,"b'from typing import Tuple, Callable, Sequence, Dict, Any\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Ints2d, DTypes\n\n\nInT = Sequence[Any]\nOutT = Ints2d\n\n\n@registry.layers(""remap_ids.v1"")\ndef remap_ids(\n    mapping_table: Dict[Any, int] = {}, default: int = 0, dtype: DTypes = ""i""\n) -> Model[InT, OutT]:\n    """"""Remap string or integer inputs using a mapping table, usually as a\n    preprocess before embeddings. The mapping table can be passed in on input,\n    or updated after the layer has been created. The mapping table is stored in\n    the ""mapping_table"" attribute.\n    """"""\n    return Model(\n        ""remap_ids"",\n        forward,\n        attrs={""mapping_table"": mapping_table, ""dtype"": dtype, ""default"": default},\n    )\n\n\ndef forward(\n    model: Model[InT, OutT], inputs: InT, is_train: bool\n) -> Tuple[OutT, Callable]:\n    table = model.attrs[""mapping_table""]\n    default = model.attrs[""default""]\n    dtype = model.attrs[""dtype""]\n    values = [table.get(x, default) for x in inputs]\n    arr = model.ops.asarray2i(values, dtype=dtype)\n    output = model.ops.reshape2i(arr, -1, 1)\n\n    def backprop(dY: OutT) -> InT:\n        return []\n\n    return output, backprop\n'"
thinc/layers/residual.py,0,"b'from typing import Tuple, Callable, Optional, List, TypeVar\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats1d, Floats2d, Floats3d, Floats4d, FloatsXd, Ragged, Padded\n\n\n# fmt: off\nInT = TypeVar(""InT"", List[Floats1d], List[Floats2d], List[Floats3d], List[Floats4d], Ragged, Padded, FloatsXd)\n# fmt: on\n\n\n@registry.layers(""residual.v1"")\ndef residual(layer: Model[InT, InT]) -> Model[InT, InT]:\n    return Model(\n        ""residual"",\n        forward,\n        init=init,\n        layers=[layer],\n        dims={\n            ""nO"": layer.get_dim(""nO"") if layer.has_dim(""nO"") else None,\n            ""nI"": layer.get_dim(""nI"") if layer.has_dim(""nI"") else None,\n        },\n    )\n\n\ndef forward(model: Model[InT, InT], X: InT, is_train: bool) -> Tuple[InT, Callable]:\n    def backprop(d_output: InT) -> InT:\n        dX = backprop_layer(d_output)\n        if isinstance(d_output, list):\n            return [d_output[i] + dX[i] for i in range(len(d_output))]\n        elif isinstance(d_output, Ragged):\n            return Ragged(d_output.data + dX.data, dX.lengths)\n        elif isinstance(X, Padded):\n            dX.data += d_output.data\n            return dX\n        else:\n            return d_output + dX\n\n    Y, backprop_layer = model.layers[0](X, is_train)\n    if isinstance(X, list):\n        return [X[i] + Y[i] for i in range(len(X))], backprop\n    elif isinstance(X, Ragged):\n        return Ragged(X.data + Y.data, X.lengths), backprop\n    elif isinstance(X, Padded):\n        Y.data += X.data\n        return Y, backprop\n    else:\n        return X + Y, backprop\n\n\ndef init(\n    model: Model[InT, InT], X: Optional[InT] = None, Y: Optional[InT] = None\n) -> Model[InT, InT]:\n    first_layer = model.layers[0]\n    first_layer.initialize(X=X, Y=Y)\n    if first_layer.has_dim(""nO""):\n        model.set_dim(""nO"", first_layer.get_dim(""nO""))\n    if first_layer.has_dim(""nI""):\n        model.set_dim(""nI"", first_layer.get_dim(""nI""))\n    return model\n'"
thinc/layers/siamese.py,0,"b'from typing import Tuple, Callable, Optional, TypeVar\n\nfrom ..model import Model\nfrom ..types import ArrayXd\nfrom ..config import registry\nfrom ..util import get_width\n\n\nLayerT = TypeVar(""LayerT"")\nSimT = TypeVar(""SimT"")\nInT = Tuple[LayerT, LayerT]\nOutT = ArrayXd\n\n\n@registry.layers(""siamese.v1"")\ndef siamese(\n    layer: Model[LayerT, SimT], similarity: Model[Tuple[SimT, SimT], OutT]\n) -> Model[InT, OutT]:\n    return Model(\n        f""siamese({layer.name}, {similarity.name})"",\n        forward,\n        init=init,\n        layers=[layer, similarity],\n        dims={""nI"": layer.get_dim(""nI""), ""nO"": similarity.get_dim(""nO"")},\n    )\n\n\ndef forward(\n    model: Model[InT, OutT], X1_X2: InT, is_train: bool\n) -> Tuple[OutT, Callable]:\n    X1, X2 = X1_X2\n    vec1, bp_vec1 = model.layers[0](X1, is_train)\n    vec2, bp_vec2 = model.layers[0](X2, is_train)\n    output, bp_output = model.layers[1]((vec1, vec2), is_train)\n\n    def finish_update(d_output: OutT) -> InT:\n        d_vec1, d_vec2 = bp_output(d_output)\n        d_input1 = bp_vec1(d_vec1)\n        d_input2 = bp_vec2(d_vec2)\n        return (d_input1, d_input2)\n\n    return output, finish_update\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.layers[0].set_dim(""nI"", get_width(X[1]))\n        model.layers[0].initialize(X=X[0])\n        X = (model.layers[0].predict(X[0]), model.layers[0].predict(X[1]))\n    model.layers[1].initialize(X=X, Y=Y)\n    model.set_dim(""nI"", model.layers[0].get_dim(""nI""))\n    model.set_dim(""nO"", model.layers[1].get_dim(""nO""))\n    return model\n'"
thinc/layers/softmax.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats2d, Floats1d\nfrom ..initializers import zero_init\nfrom ..util import get_width, partial\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""Softmax.v1"")\ndef Softmax(\n    nO: Optional[int] = None,\n    nI: Optional[int] = None,\n    *,\n    init_W: Callable = zero_init,\n    init_b: Callable = zero_init\n) -> Model[InT, OutT]:\n    return Model(\n        ""softmax"",\n        forward,\n        init=partial(init, init_W, init_b),\n        dims={""nO"": nO, ""nI"": nI},\n        params={""W"": None, ""b"": None},\n    )\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    W = cast(Floats2d, model.get_param(""W""))\n    b = cast(Floats1d, model.get_param(""b""))\n    Y = model.ops.affine(X, W, b)\n    Y = model.ops.softmax(Y)\n\n    def backprop(dY: InT) -> OutT:\n        model.inc_grad(""b"", dY.sum(axis=0))\n        model.inc_grad(""W"", model.ops.gemm(dY, X, trans1=True))\n        return model.ops.gemm(dY, W)\n\n    return Y, backprop\n\n\ndef init(\n    init_W: Callable,\n    init_b: Callable,\n    model: Model[InT, OutT],\n    X: Optional[InT] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    if X is not None:\n        model.set_dim(""nI"", get_width(X))\n    if Y is not None:\n        model.set_dim(""nO"", get_width(Y))\n    model.set_param(""W"", init_W(model.ops, (model.get_dim(""nO""), model.get_dim(""nI""))))\n    model.set_param(""b"", init_b(model.ops, (model.get_dim(""nO""),)))\n    return model\n'"
thinc/layers/softmax_activation.py,0,"b'from typing import Tuple, Callable\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Floats2d\n\n\nInT = Floats2d\nOutT = Floats2d\n\n\n@registry.layers(""softmax_activation.v1"")\ndef softmax_activation() -> Model[InT, OutT]:\n    return Model(""softmax_activation"", forward)\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    Y = model.ops.softmax(X, inplace=False)\n\n    def backprop(dY: OutT) -> InT:\n        return model.ops.backprop_softmax(Y, dY, axis=-1)\n\n    return Y, backprop\n'"
thinc/layers/staticvectors.py,0,"b'from typing import Tuple, Callable, Optional, cast, Union, Dict, Any\n\nfrom thinc.initializers import glorot_uniform_init\nfrom thinc.util import partial\n\nfrom .chain import chain\nfrom .array_getitem import ints_getitem\nfrom ..types import Ints1d, Floats2d, Ints2d, Floats1d, Unserializable\nfrom ..model import Model\nfrom ..config import registry\nfrom contextvars import ContextVar\n\n\nInT = Union[Ints1d, Ints2d]\nOutT = Floats2d\n\ncontext_vectors: ContextVar[dict] = ContextVar(""context_vectors"", default={})\n\n\n@registry.layers(""StaticVectors.v1"")\ndef StaticVectors(\n    nO: Optional[int] = None,\n    vectors: Optional[Floats2d] = None,\n    *,\n    column: Optional[int] = None,\n    dropout: Optional[float] = None,\n    init_W: Callable = glorot_uniform_init,\n) -> Model[InT, OutT]:\n    attrs: Dict[str, Any] = {""column"": column, ""vectors"": Unserializable(vectors)}\n    if dropout is not None:\n        attrs[""dropout_rate""] = dropout\n    model = Model(  # type: ignore\n        ""static_vectors"",\n        forward,\n        init=partial(init, init_W),\n        params={""W"": None},\n        attrs=attrs,\n        dims={""nM"": None, ""nV"": None, ""nO"": nO},\n    )\n    if column is not None:\n        # This is equivalent to array[:, column]. What you\'re actually doing\n        # there is passing in a tuple: array[(:, column)], except in the context\n        # of array indexing, the "":"" creates an object slice(0, None).\n        # So array[:, column] is array.__getitem__(slice(0), column).\n        model = chain(ints_getitem((slice(0, None), column)), model)\n    model.attrs[""column""] = column\n    return cast(Model[InT, OutT], model)\n\n\ndef forward(\n    model: Model[InT, OutT], ids: Ints1d, is_train: bool\n) -> Tuple[OutT, Callable]:\n    # Assume the original \'vectors\' object contains the actual data and is compatible with Floats2d\n    vectors = cast(Floats2d, model.attrs[""vectors""].obj)\n    W = cast(Floats2d, model.get_param(""W""))\n    nN = ids.shape[0]\n    vectors = vectors[ids * (ids < vectors.shape[0])]\n    vectors = model.ops.as_contig(vectors)\n    assert vectors.shape[0] == ids.shape[0]\n\n    output = model.ops.gemm(vectors, W, trans2=True)\n    dropout: Optional[float] = model.attrs.get(""dropout_rate"")\n    drop_mask = cast(Floats1d, model.ops.get_dropout_mask((output.shape[1],), dropout))\n\n    def backprop(d_output: OutT) -> Ints1d:\n        d_output *= drop_mask\n        model.inc_grad(""W"", model.ops.gemm(d_output, vectors, trans1=True))\n        dX = model.ops.alloc1i(nN)\n        return dX\n\n    output *= drop_mask\n    return output, backprop\n\n\ndef init(\n    init_W: Callable,\n    model: Model[InT, OutT],\n    X: Optional[InT] = None,\n    Y: Optional[OutT] = None,\n) -> Model[InT, OutT]:\n    # Assume the original \'vectors\' object contains the actual data\n    vectors = model.attrs[""vectors""].obj\n    if vectors is None:\n        raise ValueError(""Can\'t initialize: vectors attribute unset"")\n    model.set_dim(""nV"", vectors.shape[0] + 1)\n    model.set_dim(""nM"", vectors.shape[1])\n    model.set_param(""W"", init_W(model.ops, (model.get_dim(""nO""), model.get_dim(""nM""))))\n    return model\n'"
thinc/layers/strings2arrays.py,0,"b'from typing import Tuple, List, Callable, Sequence\nfrom murmurhash import hash_unicode\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Ints2d\n\n\nInT = Sequence[Sequence[str]]\nOutT = List[Ints2d]\n\n\n@registry.layers(""strings2arrays.v1"")\ndef strings2arrays() -> Model[InT, OutT]:\n    """"""Transform a sequence of string sequences to a list of arrays.""""""\n    return Model(""strings2arrays"", forward)\n\n\ndef forward(model: Model[InT, OutT], Xs: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    hashes = [[hash_unicode(word) for word in X] for X in Xs]\n    hash_arrays = [model.ops.asarray2i(h, dtype=""uint64"") for h in hashes]\n    arrays = [model.ops.reshape2i(array, -1, 1) for array in hash_arrays]\n\n    def backprop(dX: OutT) -> InT:\n        return []\n\n    return arrays, backprop\n'"
thinc/layers/tensorflowwrapper.py,0,"b'from typing import Any, Callable, Dict, Optional, Tuple, Type, TypeVar\n\nimport srsly\n\nfrom ..model import Model\nfrom ..shims import TensorFlowShim, keras_model_fns, maybe_handshake_model\nfrom ..util import xp2tensorflow, tensorflow2xp, assert_tensorflow_installed\nfrom ..util import is_tensorflow_array, convert_recursive, is_xp_array\nfrom ..types import ArrayXd, ArgsKwargs\n\ntry:\n    import tensorflow as tf\nexcept ImportError:  # pragma: no cover\n    pass\n\nInT = TypeVar(""InT"")\nOutT = TypeVar(""OutT"")\nInFunc = TypeVar(""InFunc"")\nXType = TypeVar(""XType"", bound=ArrayXd)\nYType = TypeVar(""YType"", bound=ArrayXd)\n\n\ndef keras_subclass(\n    name: str,\n    X: XType,\n    Y: YType,\n    input_shape: Tuple[int, ...],\n    compile_args: Optional[Dict[str, Any]] = None,\n) -> Callable[[InFunc], InFunc]:\n    """"""Decorate a custom keras subclassed model with enough information to\n    serialize and deserialize it reliably in the face of the many restrictions\n    on keras subclassed models.\n\n    name (str): The unique namespace string to use to represent this model class.\n    X (Any): A sample X input for performing a forward pass on the network.\n    Y (Any): A sample Y input for performing a backward pass on the network.\n    input_shape (Tuple[int, ...]): A set of input shapes for building the network.\n    compile: Arguments to pass directly to the keras `model.compile` call.\n\n    RETURNS (Callable): The decorated class.\n    """"""\n\n    compile_defaults = {""optimizer"": ""adam"", ""loss"": ""mse""}\n    if compile_args is None:\n        compile_args = compile_defaults\n    else:\n        compile_args = {**compile_defaults, **compile_args}\n\n    def call_fn(clazz):\n\n        clazz.catalogue_name = property(lambda inst: name)\n        clazz.eg_shape = property(lambda inst: input_shape)\n        clazz.eg_compile = property(lambda inst: compile_args)\n        clazz.eg_x = property(lambda inst: X)\n        clazz.eg_y = property(lambda inst: Y)\n\n        @keras_model_fns(name)\n        def create_component(*call_args, **call_kwargs):\n            return clazz(*call_args, **call_kwargs)\n\n        # Capture construction args and store them on the instance\n        wrapped_init = clazz.__init__\n\n        def __init__(self, *args, **kwargs):\n            wrapped_init(self, *args, **kwargs)\n            try:\n                srsly.json_dumps(args)\n                srsly.json_dumps(kwargs)\n            except BaseException as _err:\n                raise ValueError(\n                    ""In order to serialize Keras Subclass models, the constructor ""\n                    ""arguments must be serializable. This allows thinc to recreate ""\n                    ""the code-based model with the same configuration.\\n""\n                    f""The encountered error is: {_err}""\n                )\n            self.eg_args = ArgsKwargs(args, kwargs)\n\n        clazz.__init__ = __init__\n\n        return clazz\n\n    return call_fn\n\n\ndef TensorFlowWrapper(\n    tensorflow_model: Any,\n    convert_inputs: Optional[Callable] = None,\n    convert_outputs: Optional[Callable] = None,\n    optimizer: Optional[Any] = None,\n    model_class: Type[Model] = Model,\n    model_name: str = ""tensorflow"",\n) -> Model[InT, OutT]:\n    """"""Wrap a TensorFlow model, so that it has the same API as Thinc models.\n    To optimize the model, you\'ll need to create a TensorFlow optimizer and call\n    optimizer.apply_gradients after each batch.\n    """"""\n    assert_tensorflow_installed()\n    if not isinstance(tensorflow_model, tf.keras.models.Model):\n        err = f""Expected tf.keras.models.Model, got: {type(tensorflow_model)}""\n        raise ValueError(err)\n    tensorflow_model = maybe_handshake_model(tensorflow_model)\n    if convert_inputs is None:\n        convert_inputs = _convert_inputs\n    if convert_outputs is None:\n        convert_outputs = _convert_outputs\n    return model_class(\n        model_name,\n        forward,\n        shims=[TensorFlowShim(tensorflow_model, optimizer=optimizer)],\n        attrs={""convert_inputs"": convert_inputs, ""convert_outputs"": convert_outputs},\n    )\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    """"""Return the output of the wrapped TensorFlow model for the given input,\n    along with a callback to handle the backward pass.\n    """"""\n    convert_inputs = model.attrs[""convert_inputs""]\n    convert_outputs = model.attrs[""convert_outputs""]\n    tensorflow_model = model.shims[0]\n    X_tensorflow, get_dX = convert_inputs(model, X, is_train)\n    if is_train:\n        Y_tensorflow, tensorflow_backprop = tensorflow_model(X_tensorflow, is_train)\n    else:\n        Y_tensorflow = tensorflow_model(X_tensorflow, is_train)\n    Y, get_dY_tensorflow = convert_outputs(model, Y_tensorflow, is_train)\n\n    def backprop(dY: OutT) -> InT:\n        dY_tensorflow = get_dY_tensorflow(dY)\n        dX_tensorflow = tensorflow_backprop(dY_tensorflow)\n        return get_dX(dX_tensorflow)\n\n    return Y, backprop\n\n\n# Default conversion functions\n# These are pretty much the same as the PyTorch one, but I think we should\n# leave the duplication -- I think the abstraction could get pretty messy,\n# and then may need to be undone, as there can always be different specifics.\n\n\ndef _convert_inputs(model, X, is_train):\n    xp2tensorflow_ = lambda x: xp2tensorflow(x, requires_grad=is_train)\n    converted = convert_recursive(is_xp_array, xp2tensorflow_, X)\n    if isinstance(converted, ArgsKwargs):\n\n        def reverse_conversion(dXtf):\n            return convert_recursive(is_tensorflow_array, tensorflow2xp, dXtf)\n\n        return converted, reverse_conversion\n    elif isinstance(converted, dict):\n\n        def reverse_conversion(dXtf):\n            dX = convert_recursive(is_tensorflow_array, tensorflow2xp, dXtf)\n            return dX.kwargs\n\n        return ArgsKwargs(args=tuple(), kwargs=converted), reverse_conversion\n    elif isinstance(converted, (tuple, list)):\n\n        def reverse_conversion(dXtf):\n            dX = convert_recursive(is_tensorflow_array, tensorflow2xp, dXtf)\n            return dX.args\n\n        return ArgsKwargs(args=converted, kwargs={}), reverse_conversion\n    else:\n\n        def reverse_conversion(dXtf):\n            dX = convert_recursive(is_tensorflow_array, tensorflow2xp, dXtf)\n            return dX.args[0]\n\n        return ArgsKwargs(args=(converted,), kwargs={}), reverse_conversion\n\n\ndef _convert_outputs(model, Ytf, is_train):\n    Y = convert_recursive(is_tensorflow_array, tensorflow2xp, Ytf)\n\n    def reverse_conversion(dY):\n        return convert_recursive(is_xp_array, xp2tensorflow, dY)\n\n    return Y, reverse_conversion\n'"
thinc/layers/uniqued.py,0,"b'from typing import Tuple, Callable, Optional\nimport numpy\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Ints2d, Floats2d\n\n\nInT = Ints2d\nOutT = Floats2d\n\n\n@registry.layers(""uniqued.v1"")\ndef uniqued(layer: Model, *, column: int = 0) -> Model[InT, OutT]:\n    """"""Group inputs to a layer, so that the layer only has to compute for the\n    unique values. The data is transformed back before output, and the same\n    transformation is applied for the gradient. Effectively, this is a cache\n    local to each minibatch.\n    """"""\n    return Model(\n        f""uniqued-{layer.name}"",\n        forward,\n        init=init,\n        layers=[layer],\n        dims={""nO"": None, ""nI"": None},\n        attrs={""column"": column},\n    )\n\n\ndef forward(model: Model[InT, OutT], X: InT, is_train: bool) -> Tuple[OutT, Callable]:\n    column: int = model.attrs[""column""]\n    layer = model.layers[0]\n    if X.size < 2:\n        return layer(X, is_train)\n    keys = X[:, column]\n    if not isinstance(keys, numpy.ndarray):\n        keys = keys.get()  # pragma: no cover\n    uniq_keys, ind, inv, counts = layer.ops.xp.unique(\n        keys, return_index=True, return_inverse=True, return_counts=True\n    )\n    counts = model.ops.reshape2i(counts, -1, 1)\n    X_uniq = X[ind]\n    Y_uniq, bp_Y_uniq = layer(X_uniq, is_train)\n    Y = Y_uniq[inv].reshape((X.shape[0],) + Y_uniq.shape[1:])\n    uniq_shape = tuple(Y_uniq.shape)\n\n    def backprop(dY: OutT) -> InT:\n        dY_uniq = layer.ops.alloc2f(*uniq_shape)\n        layer.ops.scatter_add(dY_uniq, layer.ops.asarray_i(inv), dY)\n        d_uniques = bp_Y_uniq(dY_uniq)\n        # This confusing bit of indexing ""ununiques""\n        return (d_uniques / counts)[inv]\n\n    return Y, backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    layer = model.layers[0]\n    layer.initialize(X=X, Y=Y)\n    if layer.has_dim(""nI""):\n        model.set_dim(""nI"", layer.get_dim(""nI""))  # pragma: no cover\n    if layer.has_dim(""nO""):\n        model.set_dim(""nO"", layer.get_dim(""nO""))\n    return model\n'"
thinc/layers/with_array.py,0,"b'from typing import Tuple, Callable, Optional, TypeVar, Union, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Array2d, Floats2d, Padded, Ragged, ArrayXd\nfrom ..types import List2d\n\n\nValT = TypeVar(""ValT"", bound=Array2d)\nSeqT = TypeVar(""SeqT"", bound=Union[Padded, Ragged, List2d, Array2d])\n\n\n@registry.layers(""with_array.v1"")\ndef with_array(layer: Model[ValT, ValT], pad: int = 0) -> Model[SeqT, SeqT]:\n    """"""Transform sequence data into a contiguous 2d array on the way into and\n    out of a model. Handles a variety of sequence types: lists, padded and ragged.\n    If the input is a 2d array, it is passed through unchanged.\n    """"""\n    return Model(\n        f""with_array-{layer.name}"",\n        forward,\n        init=init,\n        layers=[layer],\n        attrs={""pad"": pad},\n    )\n\n\ndef forward(model: Model[SeqT, SeqT], Xseq: SeqT, is_train: bool):\n    if isinstance(Xseq, Ragged):\n        return _ragged_forward(\n            cast(Model[Ragged, Ragged], model), cast(Ragged, Xseq), is_train\n        )\n    elif isinstance(Xseq, Padded):\n        return _padded_forward(\n            cast(Model[Padded, Padded], model), cast(Padded, Xseq), is_train\n        )\n    elif not isinstance(Xseq, (list, tuple)):\n        return model.layers[0](Xseq, is_train)\n    else:\n        return _list_forward(cast(Model[List2d, List2d], model), Xseq, is_train)\n\n\ndef init(\n    model: Model[SeqT, SeqT], X: Optional[SeqT] = None, Y: Optional[SeqT] = None\n) -> Model[SeqT, SeqT]:\n    layer: Model[Array2d, Array2d] = model.layers[0]\n    layer.initialize(\n        X=_get_array(model, X) if X is not None else X,\n        Y=_get_array(model, Y) if Y is not None else Y,\n    )\n    return model\n\n\ndef _get_array(model, X: SeqT) -> Array2d:\n    if isinstance(X, Ragged):\n        return X.data\n    elif isinstance(X, Padded):\n        return model.ops.reshape2f(\n            X.data, X.data.shape[0] * X.data.shape[1], X.data.shape[2]\n        )\n    elif not isinstance(X, (list, tuple)):\n        return cast(Array2d, X)\n    else:\n        return model.ops.flatten(X)\n\n\ndef _list_forward(\n    model: Model[List2d, List2d], Xs: List2d, is_train: bool\n) -> Tuple[List2d, Callable]:\n    layer = model.layers[0]\n    pad = model.attrs[""pad""]\n    lengths = layer.ops.asarray1i([len(seq) for seq in Xs])\n    Xf = layer.ops.flatten(Xs, pad=pad)  # type: ignore\n    Yf, get_dXf = layer(Xf, is_train)\n\n    def backprop(dYs: List2d) -> List2d:\n        dYf = layer.ops.flatten(dYs, pad=pad)  # type: ignore\n        dXf = get_dXf(dYf)\n        return layer.ops.unflatten(dXf, lengths, pad=pad)\n\n    return layer.ops.unflatten(Yf, lengths, pad=pad), backprop\n\n\ndef _ragged_forward(\n    model: Model[Ragged, Ragged], Xr: Ragged, is_train: bool\n) -> Tuple[Ragged, Callable]:\n    layer: Model[ArrayXd, ArrayXd] = model.layers[0]\n    Y, get_dX = layer(Xr.dataXd, is_train)\n\n    def backprop(dYr: Ragged) -> Ragged:\n        return Ragged(get_dX(dYr.dataXd), dYr.lengths)\n\n    return Ragged(cast(Floats2d, Y), Xr.lengths), backprop\n\n\ndef _padded_forward(\n    model: Model[Padded, Padded], Xp: Padded, is_train: bool\n) -> Tuple[Padded, Callable]:\n    layer: Model[Array2d, Array2d] = model.layers[0]\n    X = model.ops.reshape2f(\n        Xp.data, Xp.data.shape[0] * Xp.data.shape[1], Xp.data.shape[2]\n    )\n    Y2d, get_dX = layer(X, is_train)\n    Y = model.ops.reshape3f(\n        cast(Floats2d, Y2d), Xp.data.shape[0], Xp.data.shape[1], Y2d.shape[1]\n    )\n\n    def backprop(dYp: Padded) -> Padded:\n        assert isinstance(dYp, Padded)\n        dY = model.ops.reshape2f(\n            dYp.data, dYp.data.shape[0] * dYp.data.shape[1], dYp.data.shape[2]\n        )\n        dX2d = get_dX(dY)\n        dX = model.ops.reshape3f(\n            dX2d, dYp.data.shape[0], dYp.data.shape[1], dX2d.shape[1]\n        )\n        return Padded(dX, dYp.size_at_t, dYp.lengths, dYp.indices)\n\n    return Padded(Y, Xp.size_at_t, Xp.lengths, Xp.indices), backprop\n'"
thinc/layers/with_cpu.py,0,"b'from typing import Tuple, Callable, Any\n\nimport numpy\nfrom thinc.backends import Ops\n\nfrom ..model import Model\nfrom ..config import registry\n\n\n@registry.layers(""with_cpu.v1"")\ndef with_cpu(layer: Model, ops: Ops) -> Model:\n    layer.to_cpu()\n    return Model(f""with_cpu"", forward, layers=[layer], ops=ops, init=init)\n\n\ndef forward(model: Model, X: Any, is_train: bool) -> Tuple[Any, Callable]:\n    cpu_outputs, backprop = model.layers[0].begin_update(_to_cpu(X))\n    gpu_outputs = _to_device(model.ops, cpu_outputs)\n\n    def with_cpu_backprop(d_outputs):\n        cpu_d_outputs = _to_cpu(d_outputs)\n        return backprop(cpu_d_outputs)\n\n    return gpu_outputs, with_cpu_backprop\n\n\ndef init(model: Model, X: Any, Y: Any) -> Model:\n    return model.layers[0].initialize(X, Y)\n\n\ndef _to_cpu(X):\n    if isinstance(X, numpy.ndarray):\n        return X\n    elif isinstance(X, tuple):\n        return tuple([_to_cpu(x) for x in X])\n    elif isinstance(X, list):\n        return [_to_cpu(x) for x in X]\n    elif hasattr(X, ""get""):\n        return X.get()\n    else:\n        return X\n\n\ndef _to_device(ops, X):\n    if isinstance(X, tuple):\n        return tuple([_to_device(ops, x) for x in X])\n    elif isinstance(X, list):\n        return [_to_device(ops, x) for x in X]\n    else:\n        return ops.asarray(X)\n'"
thinc/layers/with_debug.py,0,"b'from typing import Optional, Callable, Any, Tuple\n\nfrom ..model import Model\n\n\ndo_nothing = lambda *args, **kwargs: None\n\n\ndef with_debug(\n    layer: Model,\n    name: Optional[str] = None,\n    *,\n    on_init: Callable[[Model, Any, Any], None] = do_nothing,\n    on_forward: Callable[[Model, Any, bool], None] = do_nothing,\n    on_backprop: Callable[[Any], None] = do_nothing,\n):\n    """"""Debugging layer that wraps any layer and allows executing callbacks\n    during the forward pass, backward pass and initialization. The callbacks\n    will receive the same arguments as the functions they\'re called in.\n    """"""\n    name = layer.name if name is None else name\n\n    def forward(model: Model, X: Any, is_train: bool) -> Tuple[Any, Callable]:\n        layer = model.layers[0]\n        on_forward(model, X, is_train)\n        layer_Y, layer_callback = layer(X, is_train=is_train)\n\n        def backprop(dY: Any) -> Any:\n            on_backprop(dY)\n            return layer_callback(dY)\n\n        return layer_Y, backprop\n\n    def init(model: Model, X: Any, Y: Any) -> Model:\n        on_init(model, X, Y)\n        return layer.initialize(X, Y)\n\n    return Model(f""debug:{name}"", forward, init=init, layers=[layer])\n'"
thinc/layers/with_flatten.py,0,"b'from typing import Tuple, Callable, Sequence, Any, List, TypeVar\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Array2d, List2d\n\n\nItemT = TypeVar(""ItemT"")\nInT = Sequence[Sequence[ItemT]]\nOutT = List2d\n\n\n@registry.layers(""with_flatten.v1"")\ndef with_flatten(layer: Model) -> Model[InT, OutT]:\n    return Model(f""with_flatten-{layer.name}"", forward, layers=[layer], init=init)\n\n\ndef forward(\n    model: Model[InT, OutT], Xnest: InT, is_train: bool\n) -> Tuple[OutT, Callable]:\n    layer: Model[Sequence[Any], Array2d] = model.layers[0]\n    Xflat: Sequence[Any] = _flatten(Xnest)\n    Yflat, backprop_layer = layer(Xflat, is_train)\n    # Get the split points. We want n-1 splits for n items.\n    arr = layer.ops.asarray1i([len(x) for x in Xnest[:-1]])\n    splits = arr.cumsum()\n    Ynest = layer.ops.xp.split(Yflat, splits, axis=-1)\n\n    def backprop(dYnest: OutT) -> InT:\n        # I think the input/output types might be wrong here?\n        dYflat = []  # type: ignore\n        for d_item in dYnest:\n            dYflat.extend(d_item)  # type: ignore\n        dXflat = backprop_layer(dYflat)\n        dXnest = layer.ops.xp.split(dXflat, splits, axis=-1)\n        return dXnest\n\n    return Ynest, backprop\n\n\ndef _flatten(nested: InT) -> List[ItemT]:\n    flat: List[ItemT] = []\n    for item in nested:\n        flat.extend(item)\n    return flat\n\n\ndef init(model, X=None, Y=None):\n    model.layers[0].initialize(\n        _flatten(X) if X is not None else None,\n        model.layers[0].ops.xp.hstack(Y) if Y is not None else None,\n    )\n'"
thinc/layers/with_getitem.py,0,"b'from typing import Callable, Optional, Tuple, Any\n\nfrom ..model import Model\nfrom ..config import registry\n\n\nInT = Tuple[Any, ...]\nOutT = Tuple[Any, ...]\n\n\n@registry.layers(""with_getitem.v1"")\ndef with_getitem(idx: int, layer: Model) -> Model[InT, OutT]:\n    """"""Transform data on the way into and out of a layer, by plucking an item\n    from a tuple.\n    """"""\n    return Model(\n        f""with_getitem-{layer.name}"",\n        forward,\n        init=init,\n        layers=[layer],\n        attrs={""idx"": idx},\n    )\n\n\ndef forward(\n    model: Model[InT, OutT], items: InT, is_train: bool\n) -> Tuple[OutT, Callable]:\n    idx = model.attrs[""idx""]\n    Y_i, backprop_item = model.layers[0](items[idx], is_train)\n\n    def backprop(d_output: OutT) -> InT:\n        dY_i = backprop_item(d_output[idx])\n        return d_output[:idx] + (dY_i,) + items[idx + 1 :]\n\n    return items[:idx] + (Y_i,) + items[idx + 1 :], backprop\n\n\ndef init(\n    model: Model[InT, OutT], X: Optional[InT] = None, Y: Optional[OutT] = None\n) -> Model[InT, OutT]:\n    idx = model.attrs[""idx""]\n    X_i = X[idx] if X is not None else X\n    Y_i = Y[idx] if Y is not None else Y\n    model.layers[0].initialize(X=X_i, Y=Y_i)\n    return model\n'"
thinc/layers/with_list.py,0,"b'from typing import Tuple, Callable, List, Optional, TypeVar, Union, cast\n\nfrom ..types import Padded, Ragged, Floats2d, List2d\nfrom ..model import Model\nfrom ..config import registry\n\n\nSeqT = TypeVar(""SeqT"", bound=Union[Padded, Ragged, List2d])\n\n\n@registry.layers(""with_list.v1"")\ndef with_list(layer: Model[List2d, List2d]) -> Model[SeqT, SeqT]:\n    return Model(f""with_list-{layer.name}"", forward, init=init, layers=[layer])\n\n\ndef forward(\n    model: Model[SeqT, SeqT], Xseq: SeqT, is_train: bool\n) -> Tuple[SeqT, Callable]:\n    layer: Model[List2d, List2d] = model.layers[0]\n    Y: Union[Padded, Ragged, List2d]\n    if isinstance(Xseq, Padded):\n        Y, backprop = _padded_forward(layer, cast(Padded, Xseq), is_train)\n    elif isinstance(Xseq, Ragged):\n        Y, backprop = _ragged_forward(layer, cast(Ragged, Xseq), is_train)\n    else:\n        Y, backprop = layer(cast(List2d, Xseq), is_train)\n    return cast(Tuple[SeqT, Callable], (Y, backprop))\n\n\ndef init(\n    model: Model[SeqT, SeqT], X: Optional[SeqT] = None, Y: Optional[SeqT] = None\n) -> Model[SeqT, SeqT]:\n    model.layers[0].initialize(\n        X=_get_list(model, X) if X is not None else None,\n        Y=_get_list(model, Y) if Y is not None else None,\n    )\n    return model\n\n\ndef _get_list(model, seq):\n    if isinstance(seq, Padded):\n        return model.ops.padded2list(seq)\n    elif isinstance(seq, Ragged):\n        return model.ops.unflatten(seq.data, seq.lengths)\n    else:\n        return seq\n\n\ndef _ragged_forward(layer, Xr, is_train):\n    # Assign these to locals, to keep code a bit shorter.\n    unflatten = layer.ops.unflatten\n    flatten = layer.ops.flatten\n    # It\'s worth being a bit careful about memory here, as the activations\n    # are potentially large on GPU. So we make nested function calls instead\n    # of assigning to temporaries where possible, so memory can be reclaimed\n    # sooner.\n    Ys, get_dXs = layer(unflatten(Xr.data, Xr.lengths), is_train)\n\n    def backprop(dYr: Ragged):\n        return Ragged(flatten(get_dXs(unflatten(dYr.data, dYr.lengths))), dYr.lengths)\n\n    return Ragged(flatten(Ys), Xr.lengths), backprop\n\n\ndef _padded_forward(layer, Xp, is_train):\n    # Assign these to locals, to keep code a bit shorter.\n    padded2list = layer.ops.padded2list\n    list2padded = layer.ops.list2padded\n    # It\'s worth being a bit careful about memory here, as the activations\n    # are potentially large on GPU. So we make nested function calls instead\n    # of assigning to temporaries where possible, so memory can be reclaimed\n    # sooner.\n    Ys, get_dXs = layer(padded2list(Xp), is_train)\n\n    def backprop(dYp):\n        return list2padded(get_dXs(padded2list(dYp)))\n\n    return list2padded(cast(List[Floats2d], Ys)), backprop\n'"
thinc/layers/with_padded.py,0,"b'from typing import Tuple, Callable, Optional, TypeVar, Union, cast\n\nfrom ..types import Padded, Ragged, Array2d, Floats3d, Ints1d, Floats2d, List2d\nfrom ..model import Model\nfrom ..config import registry\nfrom ..util import is_xp_array\n\n\nPaddedData = Tuple[Floats3d, Ints1d, Ints1d, Ints1d]\nValT = TypeVar(""ValT"", bound=Array2d)\nSeqT = TypeVar(""SeqT"", bound=Union[Padded, Ragged, List2d, Floats3d, PaddedData])\n\n\n@registry.layers(""with_padded.v1"")\ndef with_padded(layer: Model[Padded, Padded]) -> Model[SeqT, SeqT]:\n    return Model(f""with_padded-{layer.name}"", forward, init=init, layers=[layer])\n\n\ndef forward(\n    model: Model[SeqT, SeqT], Xseq: SeqT, is_train: bool\n) -> Tuple[SeqT, Callable]:\n    layer: Model[Padded, Padded] = model.layers[0]\n    Y: Union[Padded, Ragged, List2d, PaddedData]\n    if isinstance(Xseq, Padded):\n        Y, backprop = layer(Xseq, is_train)\n    elif isinstance(Xseq, Ragged):\n        Y, backprop = _ragged_forward(layer, cast(Ragged, Xseq), is_train)\n    elif _is_padded_data(Xseq):\n        Y, backprop = _tuple_forward(layer, cast(PaddedData, Xseq), is_train)\n    elif is_xp_array(Xseq):\n        Y, backprop = _array_forward(layer, cast(Floats3d, Xseq), is_train)\n    else:\n        Y, backprop = _list_forward(layer, cast(List2d, Xseq), is_train)\n    return cast(Tuple[SeqT, Callable], (Y, backprop))\n\n\ndef init(\n    model: Model[SeqT, SeqT], X: Optional[SeqT] = None, Y: Optional[SeqT] = None\n) -> Model[SeqT, SeqT]:\n    model.layers[0].initialize(\n        X=_get_padded(model, X) if X is not None else None,\n        Y=_get_padded(model, Y) if Y is not None else None,\n    )\n    return model\n\n\ndef _is_padded_data(seq):\n    return isinstance(seq, tuple) and len(seq) == 4 and all(map(is_xp_array, seq))\n\n\ndef _get_padded(model, seq):\n    if isinstance(seq, Padded):\n        return seq\n    elif isinstance(seq, Ragged):\n        return model.ops.list2padded(model.ops.unflatten(seq.data, seq.lengths))\n    elif _is_padded_data(seq):\n        return Padded(*seq)  # type: ignore\n    elif is_xp_array(seq):\n        size_at_t = model.ops.asarray1i([seq.shape[1]] * seq.shape[0])\n        lengths = model.ops.asarray1i([seq.shape[0]] * seq.shape[1])\n        indices = model.ops.xp.arange(seq.shape[1])\n        return Padded(seq, size_at_t, lengths, indices)\n    else:\n        assert isinstance(seq, list), seq\n        return model.ops.list2padded(seq)\n\n\ndef _array_forward(layer, X, is_train):\n    # Create bogus metadata for Padded.\n    Xp = _get_padded(layer, X)\n    Yp, get_dXp = layer(Xp, is_train)\n    size_at_t = Xp.size_at_t\n    lengths = Xp.lengths\n    indices = Xp.indices\n\n    def backprop(dY: Floats3d) -> Floats3d:\n        dYp = Padded(dY, size_at_t, lengths, indices)\n        dXp = get_dXp(dYp)\n        return dXp.data\n\n    return Yp.data, backprop\n\n\ndef _tuple_forward(layer, X, is_train: bool):\n    Yp, get_dXp = layer(Padded(*X), is_train)\n\n    def backprop(dY):\n        dXp = get_dXp(Padded(*dY))\n        return (dXp.data, dXp.size_at_t, dXp.lengths, dXp.indices)\n\n    return (Yp.data, Yp.size_at_t, Yp.lengths, Yp.indices), backprop\n\n\ndef _ragged_forward(layer, Xr, is_train):\n    # Assign these to locals, to keep code a bit shorter.\n    list2padded = layer.ops.list2padded\n    padded2list = layer.ops.padded2list\n    unflatten = layer.ops.unflatten\n    flatten = layer.ops.flatten\n    # It\'s worth being a bit careful about memory here, as the activations\n    # are potentially large on GPU. So we make nested function calls instead\n    # of assigning to temporaries where possible, so memory can be reclaimed\n    # sooner.\n    Yp, get_dXp = layer(list2padded(unflatten(Xr.data, Xr.lengths)), is_train)\n\n    def backprop(dYr: Ragged):\n        flattened = flatten(\n            padded2list(get_dXp(list2padded(unflatten(dYr.data, dYr.lengths))))\n        )\n        return Ragged(cast(Floats2d, flattened), dYr.lengths)\n\n    flattened = flatten(padded2list(Yp))\n    return Ragged(flattened, Xr.lengths), backprop\n\n\ndef _list_forward(layer, Xs, is_train):\n    # Assign these to locals, to keep code a bit shorter.\n    list2padded = layer.ops.list2padded\n    padded2list = layer.ops.padded2list\n\n    Yp, get_dXp = layer(list2padded(Xs), is_train)  # type: ignore\n\n    def backprop(dYs):\n        return padded2list(get_dXp(list2padded(dYs)))  # type: ignore\n\n    return padded2list(Yp), backprop\n'"
thinc/layers/with_ragged.py,0,"b'from typing import Tuple, Callable, Optional, TypeVar, Union, cast\n\nfrom ..types import Padded, Ragged, Ints1d, Array2d, List2d\nfrom ..model import Model\nfrom ..config import registry\n\n\nRaggedData = Tuple[Array2d, Ints1d]\nSeqT = TypeVar(""SeqT"", bound=Union[Padded, Ragged, List2d, RaggedData])\n\n\n@registry.layers(""with_ragged.v1"")\ndef with_ragged(layer: Model[Ragged, Ragged]) -> Model[SeqT, SeqT]:\n    return Model(f""with_ragged-{layer.name}"", forward, init=init, layers=[layer])\n\n\ndef forward(\n    model: Model[SeqT, SeqT], Xseq: SeqT, is_train: bool\n) -> Tuple[SeqT, Callable]:\n    layer: Model[Ragged, Ragged] = model.layers[0]\n    Y: Union[Padded, Ragged, List2d, RaggedData]\n    if isinstance(Xseq, Ragged):\n        Y, backprop = layer(Xseq, is_train)\n    elif isinstance(Xseq, Padded):\n        Y, backprop = _padded_forward(layer, cast(Padded, Xseq), is_train)\n    elif _is_ragged_data(Xseq):\n        Y, backprop = _tuple_forward(layer, cast(RaggedData, Xseq), is_train)\n    else:\n        Y, backprop = _list_forward(layer, cast(List2d, Xseq), is_train)\n    return cast(Tuple[SeqT, Callable], (Y, backprop))\n\n\ndef init(\n    model: Model[SeqT, SeqT], X: Optional[SeqT] = None, Y: Optional[SeqT] = None\n) -> Model[SeqT, SeqT]:\n    model.layers[0].initialize(\n        X=_get_ragged(model, X) if X is not None else None,\n        Y=_get_ragged(model, Y) if Y is not None else None,\n    )\n    return model\n\n\ndef _is_ragged_data(seq):\n    return isinstance(seq, tuple) and len(seq) == 2\n\n\ndef _get_ragged(model, seq):\n    if isinstance(seq, Ragged):\n        return seq\n    elif isinstance(seq, Padded):\n        lists = model.ops.padded2list(seq)\n        lengths = model.ops.asarray1i([len(x) for x in lists])\n        return Ragged(model.ops.flatten(lists), lengths)\n    elif _is_ragged_data(seq):\n        return Ragged(*seq)\n    else:\n        lengths = model.ops.asarray1i([len(x) for x in seq])\n        return Ragged(model.ops.flatten(seq), lengths)\n\n\ndef _tuple_forward(layer: Model[Ragged, Ragged], X: RaggedData, is_train: bool):\n    Yr, get_dXr = layer(Ragged(*X), is_train)\n\n    def backprop(dY: RaggedData) -> RaggedData:\n        dXr = get_dXr(Ragged(*dY))\n        return (dXr.data, dXr.lengths)\n\n    return (Yr.data, Yr.lengths), backprop\n\n\ndef _padded_forward(layer, Xp, is_train):\n    # Assign these to locals, to keep code a bit shorter.\n    list2padded = layer.ops.list2padded\n    padded2list = layer.ops.padded2list\n    unflatten = layer.ops.unflatten\n    flatten = layer.ops.flatten\n    # It\'s worth being a bit careful about memory here, as the activations\n    # are potentially large on GPU. So we make nested function calls instead\n    # of assigning to temporaries where possible, so memory can be reclaimed\n    # sooner.\n    Xs = padded2list(Xp)\n    # Bit annoying here: padded is in a different order, so we need to make new\n    # lengths.\n    lengths = layer.ops.asarray1i([len(x) for x in Xs])\n    Yr, get_dXr = layer(Ragged(flatten(Xs), lengths), is_train)\n\n    def backprop(dYp: Padded):\n        flattened = flatten(padded2list(dYp))\n        return list2padded(unflatten(get_dXr(Ragged(flattened, lengths)).data, lengths))\n\n    return list2padded(unflatten(Yr.data, Yr.lengths)), backprop\n\n\ndef _list_forward(layer, Xs, is_train: bool):\n    # Assign these to locals, to keep code a bit shorter.\n    flatten = layer.ops.flatten\n    unflatten = layer.ops.unflatten\n\n    lengths = layer.ops.asarray1i([len(x) for x in Xs])\n    Yr, get_dXr = layer(Ragged(flatten(Xs), lengths), is_train)\n\n    def backprop(dYs):\n        flattened = flatten(dYs)\n        return unflatten(get_dXr(Ragged(flattened, lengths)).data, lengths)\n\n    return unflatten(Yr.data, Yr.lengths), backprop\n'"
thinc/layers/with_reshape.py,0,"b'from typing import Tuple, Callable, Optional, cast\n\nfrom ..model import Model\nfrom ..config import registry\nfrom ..types import Array3d, Array2d, Floats3d\n\n\nInT = Array3d\n\n\n@registry.layers(""with_reshape.v1"")\ndef with_reshape(layer: Model[Array2d, Array2d]) -> Model[InT, InT]:\n    """"""Reshape data on the way into and out from a layer.""""""\n    return Model(\n        f""with_reshape-{layer.name}"",\n        forward,\n        init=init,\n        layers=[layer],\n        dims={""nO"": None, ""nI"": None},\n    )\n\n\ndef forward(model: Model[InT, InT], X: InT, is_train: bool) -> Tuple[InT, Callable]:\n    layer = model.layers[0]\n    initial_shape = X.shape\n    final_shape = list(initial_shape[:-1]) + [layer.get_dim(""nO"")]\n    nB = X.shape[0]\n    nT = X.shape[1]\n    X2d = cast(InT, model.ops.reshape(X, (-1, X.shape[2])))\n    Y2d, Y2d_backprop = layer(X2d, is_train=is_train)\n    Y = model.ops.reshape3f(Y2d, *final_shape)\n\n    def backprop(dY: InT) -> InT:\n        dY_floats = model.ops.asarray3f(cast(Floats3d, dY))\n        reshaped = model.ops.reshape2f(dY_floats, nB * nT, -1)\n        return Y2d_backprop(model.ops.reshape3f(reshaped, *initial_shape))\n\n    return Y, backprop\n\n\ndef init(\n    model: Model[InT, InT], X: Optional[Array3d] = None, Y: Optional[Array3d] = None\n) -> Model[InT, InT]:\n    layer = model.layers[0]\n    if X is None and Y is None:\n        layer.initialize()\n        return model\n    X2d: Optional[Array2d] = None\n    Y2d: Optional[Array2d] = None\n    if X is not None:\n        X2d = cast(Array2d, model.ops.reshape(X, (-1, X.shape[-1])))\n    if Y is not None:\n        Y2d = cast(Array2d, model.ops.reshape(Y, (-1, Y.shape[-1])))\n    layer.initialize(X=X2d, Y=Y2d)\n    if layer.has_dim(""nI""):\n        model.set_dim(""nI"", layer.get_dim(""nI""))\n    if layer.has_dim(""nO""):\n        model.set_dim(""nO"", layer.get_dim(""nO""))\n    return model\n'"
thinc/shims/__init__.py,0,"b'from .shim import Shim\nfrom .pytorch import PyTorchShim\nfrom .tensorflow import keras_model_fns, TensorFlowShim, maybe_handshake_model\nfrom .mxnet import MXNetShim\n'"
thinc/shims/mxnet.py,0,"b'from typing import Any, cast\nimport srsly\nimport copy\n\ntry:\n    import mxnet.autograd\n    import mxnet.optimizer\n    import mxnet as mx\nexcept ImportError:  # pragma: no cover\n    pass\n\nfrom ..util import mxnet2xp, convert_recursive, make_tempfile, xp2mxnet\nfrom ..util import get_array_module\nfrom ..optimizers import Optimizer\nfrom ..types import ArgsKwargs, FloatsXd\nfrom .shim import Shim\n\n\nclass MXNetShim(Shim):\n    """"""Interface between a MXNet model and a Thinc Model. This container is\n    *not* a Thinc Model subclass itself.\n    """"""\n\n    def __call__(self, inputs, is_train):\n        if is_train:\n            return self.begin_update(inputs)\n        else:\n            return self.predict(inputs), lambda a: ...\n\n    def predict(self, inputs: ArgsKwargs) -> Any:\n        """"""Pass inputs through to the underlying MXNet model, and return the\n        output. No conversions are performed. The MXNet model is set into\n        evaluation mode.\n        """"""\n        mx.autograd.set_training(train_mode=False)\n        with mxnet.autograd.pause():\n            outputs = self._model(*inputs.args, **inputs.kwargs)\n        mx.autograd.set_training(train_mode=True)\n        return outputs\n\n    def begin_update(self, inputs: ArgsKwargs):\n        """"""Pass the inputs through to the underlying MXNet model, keeping\n        track of which items in the input are tensors requiring gradients.\n        If the model returns a single value, it is converted into a one-element\n        tuple. Return the outputs and a callback to backpropagate.\n        """"""\n        mx.autograd.set_training(train_mode=True)\n        mx.autograd.set_recording(True)\n        output = self._model(*inputs.args, **inputs.kwargs)\n\n        def backprop(grads):\n            mx.autograd.set_recording(False)\n            mxnet.autograd.backward(*grads.args, **grads.kwargs)\n            return convert_recursive(\n                lambda x: hasattr(x, ""grad""), lambda x: x.grad, inputs\n            )\n\n        return output, backprop\n\n    def finish_update(self, optimizer: Optimizer):\n        params = []\n        grads = []\n        shapes = []\n        ctx = mx.current_context()\n        for key, value in self._model.collect_params().items():\n            grad = cast(FloatsXd, mxnet2xp(value.grad(ctx)))\n            param = cast(FloatsXd, mxnet2xp(value.data(ctx)))\n            params.append(param.ravel())\n            grads.append(grad.ravel())\n            shapes.append((param.size, param.shape))\n        if not params:\n            return\n        xp = get_array_module(params[0])\n        flat_params, flat_grads = optimizer(\n            (self.id, ""mxnet-shim""), xp.concatenate(params), xp.concatenate(grads)\n        )\n        start = 0\n        for key, value in self._model.collect_params().items():\n            size, shape = shapes.pop(0)\n            param = flat_params[start : start + size].reshape(shape)\n            value.set_data(xp2mxnet(param))\n            value.zero_grad()\n            start += size\n\n    def copy(self, ctx: ""mx.context.Context"" = None):\n        if ctx is None:\n            ctx = mx.current_context()\n        model_bytes = self.to_bytes()\n        copied = copy.deepcopy(self)\n        copied._model.initialize(ctx=ctx)\n        copied.from_bytes(model_bytes)\n        return copied\n\n    def to_device(self, device_type: str, device_id: int):\n        if device_type == ""cpu"":\n            self._model = self.copy(mx.cpu())\n        elif device_type == ""gpu"":\n            self._model = self.copy(mx.gpu())\n        else:\n            msg = f""Unexpected device_type: {device_type}. Try \'cpu\' or \'gpu\'.""\n            raise ValueError(msg)\n\n    def to_bytes(self):\n        # MXNet doesn\'t implement save/load without a filename\n        with make_tempfile(""w+b"") as temp:\n            self._model.save_parameters(temp.name)\n            temp.seek(0)\n            weights_bytes = temp.read()\n        msg = {""config"": self.cfg, ""state"": weights_bytes}\n        return srsly.msgpack_dumps(msg)\n\n    def from_bytes(self, bytes_data):\n        msg = srsly.msgpack_loads(bytes_data)\n        self.cfg = msg[""config""]\n        self._load_params(msg[""state""])\n        return self\n\n    def _load_params(self, params):\n        # MXNet doesn\'t implement save/load without a filename :(\n        with make_tempfile(""w+b"") as temp:\n            temp.write(params)\n            self._model.load_parameters(temp.name, ctx=mx.current_context())\n'"
thinc/shims/pytorch.py,8,"b'from typing import Any, cast\nimport contextlib\nfrom io import BytesIO\nimport srsly\n\ntry:\n    import torch.autograd\n    import torch.optim\n    import torch\nexcept ImportError:  # pragma: no cover\n    pass\n\nfrom ..util import torch2xp, xp2torch, get_array_module, convert_recursive\nfrom ..backends import get_current_ops, get_array_ops\nfrom ..optimizers import Optimizer\nfrom ..types import ArgsKwargs, FloatsXd\nfrom .shim import Shim\n\n\nclass PyTorchShim(Shim):\n    """"""Interface between a PyTorch model and a Thinc Model. This container is\n    *not* a Thinc Model subclass itself.\n    """"""\n\n    def __call__(self, inputs, is_train):\n        if is_train:\n            return self.begin_update(inputs)\n        else:\n            return self.predict(inputs), lambda a: ...\n\n    def predict(self, inputs: ArgsKwargs) -> Any:\n        """"""Pass inputs through to the underlying PyTorch model, and return the\n        output. No conversions are performed. The PyTorch model is set into\n        evaluation mode.\n        """"""\n        self._model.eval()\n        with torch.no_grad():\n            outputs = self._model(*inputs.args, **inputs.kwargs)\n        self._model.train()\n        return outputs\n\n    def begin_update(self, inputs: ArgsKwargs):\n        """"""Pass the inputs through to the underlying PyTorch model, keeping\n        track of which items in the input are tensors requiring gradients.\n        If the model returns a single value, it is converted into a one-element tuple. Return the outputs and a callback to backpropagate.  """"""\n        self._model.train()\n        output = self._model(*inputs.args, **inputs.kwargs)\n\n        def backprop(grads):\n            torch.autograd.backward(*grads.args, **grads.kwargs)\n            return convert_recursive(\n                lambda x: hasattr(x, ""grad""), lambda x: x.grad, inputs\n            )\n\n        return output, backprop\n\n    def finish_update(self, optimizer: Optimizer):\n        params = []\n        grads = []\n        shapes = []\n        for name, torch_data in self._model.named_parameters():\n            xp_data = cast(FloatsXd, torch2xp(torch_data.data))\n            if torch_data.grad is not None:\n                xp_grad = cast(FloatsXd, torch2xp(torch_data.grad))\n            else:\n                xp_grad = cast(FloatsXd, torch2xp(torch.zeros_like(torch_data)))\n            params.append(xp_data.ravel())\n            grads.append(xp_grad.ravel())\n            shapes.append((xp_data.size, xp_data.shape))\n        if not params:\n            return\n        xp = get_array_module(params[0])\n        flat_params, flat_grads = optimizer(\n            (self.id, ""pytorch-shim""), xp.concatenate(params), xp.concatenate(grads)\n        )\n        start = 0\n        for name, torch_data in self._model.named_parameters():\n            size, shape = shapes.pop(0)\n            param = flat_params[start : start + size].reshape(shape)\n            torch_data.data = xp2torch(param, requires_grad=True)\n            if torch_data.grad is not None:\n                torch_data.grad.zero_()\n            start += size\n\n    @contextlib.contextmanager\n    def use_params(self, params):\n        key_prefix = f""pytorch_{self.id}_""\n        state_dict = {}\n        for k, v in params.items():\n            if hasattr(k, ""startswith"") and k.startswith(key_prefix):\n                state_dict[k.replace(key_prefix, """")] = xp2torch(v)\n        if state_dict:\n            backup = {k: v.clone() for k, v in self._model.state_dict().items()}\n            self._model.load_state_dict(state_dict)\n            yield\n            self._model.load_state_dict(backup)\n        else:\n            yield\n\n    def to_device(self, device_type: str, device_id: int):  # pragma: no cover\n        if device_type == ""cpu"":\n            self._model.cpu()\n        elif device_type == ""gpu"":\n            self._model.cuda(device_id)\n        else:\n            msg = f""Invalid device_type: {device_type}. Try \'cpu\' or \'gpu\'""\n            raise ValueError(msg)\n\n    def to_bytes(self):\n        filelike = BytesIO()\n        torch.save(self._model.state_dict(), filelike)\n        filelike.seek(0)\n        weights_bytes = filelike.getvalue()\n        msg = {""config"": self.cfg, ""state"": weights_bytes}\n        return srsly.msgpack_dumps(msg)\n\n    def from_bytes(self, bytes_data):\n        ops = get_current_ops()\n        msg = srsly.msgpack_loads(bytes_data)\n        self.cfg = msg[""config""]\n        filelike = BytesIO(msg[""state""])\n        filelike.seek(0)\n        if ops.device_type == ""cpu"":\n            map_location = ""cpu""\n        else:  # pragma: no cover\n            device_id = torch.cuda.current_device()\n            map_location = ""cuda:%d"" % device_id\n        self._model.load_state_dict(torch.load(filelike, map_location=map_location))\n        self._model.to(map_location)\n        return self\n'"
thinc/shims/shim.py,0,"b'from typing import Any, Optional, Tuple, Callable, Dict, Union\nimport copy\nimport contextlib\nfrom pathlib import Path\nimport threading\n\n\nclass Shim:  # pragma: no cover\n    """"""Define a basic interface for external models. Users can create subclasses\n    of \'shim\' to wrap external libraries. We provide shims for PyTorch.\n\n    The Thinc Model class treats Shim objects as a sort of special type of\n    sublayer: it knows they\'re not actual Thinc Model instances, but it also\n    knows to talk to the shim instances when doing things like using transferring\n    between devices, loading in parameters, optimization. It also knows Shim\n    objects need to be serialized and deserialized with to/from bytes/disk,\n    rather than expecting that they\'ll be msgpack-serializable.\n    """"""\n\n    global_id: int = 0\n    global_id_lock: threading.Lock = threading.Lock()\n    cfg: Dict\n    _model: Any\n    _optimizer: Optional[Any]\n\n    def __init__(self, model: Any, config=None, optimizer: Any = None):\n        with Shim.global_id_lock:\n            Shim.global_id += 1\n        self.id = Shim.global_id\n        self.cfg = dict(config) if config is not None else {}\n        self._model = model\n        self._optimizer = optimizer\n\n    def __call__(self, inputs, is_train: bool) -> Tuple[Any, Callable[..., Any]]:\n        raise NotImplementedError\n\n    def predict(self, fwd_args: Any) -> Any:\n        Y, backprop = self(fwd_args, is_train=False)\n        return Y\n\n    def begin_update(self, fwd_args: Any) -> Tuple[Any, Callable[..., Any]]:\n        return self(fwd_args, is_train=True)\n\n    def finish_update(self, optimizer):\n        raise NotImplementedError\n\n    @contextlib.contextmanager\n    def use_params(self, params):\n        yield\n\n    def copy(self):\n        return copy.deepcopy(self)\n\n    def to_device(self, device_type: str, device_id: int):\n        raise NotImplementedError\n\n    def to_disk(self, path: Union[str, Path]):\n        bytes_data = self.to_bytes()\n        with Path(path).open(""wb"") as file_:\n            file_.write(bytes_data)\n\n    def from_disk(self, path: Union[str, Path]) -> ""Shim"":\n        with Path(path).open(""rb"") as file_:\n            bytes_data = file_.read()\n        return self.from_bytes(bytes_data)\n\n    def to_bytes(self):\n        raise NotImplementedError\n\n    def from_bytes(self, data) -> ""Shim"":\n        raise NotImplementedError\n'"
thinc/shims/tensorflow.py,0,"b'from typing import Any, Dict, List, Optional\nimport catalogue\nimport contextlib\nimport copy\nimport itertools\nfrom io import BytesIO\nimport numpy\n\nfrom ..backends import Ops, get_current_ops, get_array_ops\nfrom ..optimizers import Optimizer\nfrom ..types import ArgsKwargs, ArrayXd\nfrom ..util import tensorflow2xp, get_array_module\nfrom .shim import Shim\n\ntry:\n    import cupy\nexcept ImportError:\n    cupy = None\n\ntry:\n    import tensorflow as tf\nexcept ImportError:  # pragma: no cover\n    pass\n\ntry:\n    import h5py\nexcept ImportError:  # pragma: no cover\n    pass\n\nkeras_model_fns = catalogue.create(""thinc"", ""keras"", entry_points=True)\n\n\ndef maybe_handshake_model(keras_model):\n    """"""Call the required predict/compile/build APIs to initialize a model if it\n    is a subclass of tf.keras.Model. This is required to be able to call set_weights\n    on subclassed layers.""""""\n    try:\n        keras_model.get_config()\n        return keras_model\n    except (AttributeError, NotImplementedError):\n        # Subclassed models don\'t implement get_config\n        pass\n\n    for prop_name in [""catalogue_name"", ""eg_x"", ""eg_y"", ""eg_shape""]:\n        if not hasattr(keras_model, prop_name):\n            raise ValueError(\n                ""Keras subclassed models are not whole-model serializable by ""\n                ""TensorFlow. To work around this, you must decorate your keras ""\n                ""model subclasses with the \'keras_subclass\' decorator. The decorator ""\n                ""requires a single X/Y input of fake-data that can be used to initialize ""\n                ""your subclass model properly when loading the saved version.""\n            )\n\n    ops: Ops = get_current_ops()\n    if ops.device_type == ""cpu"":\n        device = ""CPU""\n    else:  # pragma: no cover\n        device = tf.test.gpu_device_name()\n\n    compile_args = keras_model.eg_compile\n\n    with tf.device(device):\n        # Calling predict creates layers and weights for subclassed models\n        keras_model.compile(**compile_args)\n        keras_model.build(keras_model.eg_shape)\n        keras_model.predict(keras_model.eg_x)\n        # Made public in 2.2.x\n        if hasattr(keras_model, ""_make_train_function""):\n            keras_model._make_train_function()\n        else:\n            keras_model.make_train_function()\n    return keras_model\n\n\nclass TensorFlowShim(Shim):\n    """"""Interface between a TensorFlow model and a Thinc Model. This container is\n    *not* a Thinc Model subclass itself.\n\n    Reference for custom training:\n    https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\n    """"""\n\n    gradients: Optional[List[""tf.Tensor""]]\n\n    def __init__(self, model: Any, config=None, optimizer: Any = None):\n        super().__init__(model, config, optimizer)\n        self.gradients = None\n\n    def __str__(self):\n        lines: List[str] = []\n\n        def accumulate(line: str):\n            lines.append(line)\n\n        self._model.summary(print_fn=accumulate)\n        return ""\\n"".join(lines)\n\n    def __call__(self, X: ArgsKwargs, is_train: bool):\n        if is_train:\n            return self.begin_update(X)\n        else:\n            return self.predict(X)\n\n    def predict(self, X: ArgsKwargs):\n        old_phase = tf.keras.backend.learning_phase()\n        tf.keras.backend.set_learning_phase(0)\n        Y = self._model(*X.args, **X.kwargs)\n        tf.keras.backend.set_learning_phase(old_phase)\n        return Y\n\n    def begin_update(self, X: ArgsKwargs):\n        tf.keras.backend.set_learning_phase(1)\n        tape = tf.GradientTape()\n        tape.__enter__()\n        tape.watch(X.args)  # watch the input layers\n        output = self._model(*X.args, **X.kwargs)\n\n        def backprop(d_output):\n            # d_args[0] contains derivative of loss wrt output (d_loss/d_output)\n            tape.__exit__(None, None, None)\n            # We need to handle a tuple of inputs\n            if len(X.args) == 1:\n                wrt_tensors = [X.args[0]]  # add the input layer also for d_loss/d_input\n            else:\n                wrt_tensors = list(X.args[0])\n            wrt_tensors.extend(self._model.trainable_variables)\n            all_gradients = tape.gradient(\n                output, wrt_tensors, output_gradients=d_output\n            )\n            dX = all_gradients[: len(X.args)]\n            opt_grads = all_gradients[1:]\n            # Accumulate gradients\n            if self.gradients is not None:\n                assert len(opt_grads) == len(self.gradients), ""gradients must match""\n                variable: tf.Variable\n                for variable, new_variable in zip(self.gradients, opt_grads):\n                    variable.assign_add(new_variable)\n            else:\n                # Create variables from the grads to allow accumulation\n                self.gradients = [tf.Variable(f) for f in opt_grads]\n            return ArgsKwargs(args=tuple(dX), kwargs={})\n\n        return output, backprop\n\n    def finish_update(self, optimizer: Optimizer):\n        if self.gradients is None:\n            raise ValueError(\n                ""There are no gradients for optimization. Be sure to call begin_update""\n                "" before calling finish_update.""\n            )\n        assert len(self.gradients) == len(self._model.trainable_variables)\n        grad: tf.Tensor\n        variable: tf.Variable\n        params = []\n        grads = []\n        shapes = []\n\n        for grad, variable in zip(self.gradients, self._model.trainable_variables):\n            param = variable.numpy()\n            grad = grad.numpy()\n            shapes.append((param.size, param.shape))\n            params.append(param.ravel())\n            grads.append(grad.ravel())\n        xp = get_array_module(params[0])\n        flat_params, flat_grads = optimizer(\n            (self.id, ""tensorflow-shim""), xp.concatenate(params), xp.concatenate(grads)\n        )\n        start = 0\n        for grad, variable in zip(self.gradients, self._model.trainable_variables):\n            size, shape = shapes.pop(0)\n            param = flat_params[start : start + size].reshape(shape)\n            variable.assign(param)\n            start += size\n        self.gradients = None\n\n    def _load_weights_from_state_dict(\n        self, state_dict: Optional[Dict[str, ArrayXd]] = None\n    ):\n        if state_dict is None:\n            state_dict = self._create_state_dict()\n        for layer in self._model.layers:\n            current_layer_weights = []\n            for weight in layer.weights:\n                current_layer_weights.append(state_dict[weight.name])\n            layer.set_weights(current_layer_weights)\n\n    # Create a state dict similar to PyTorch\n    def _create_state_dict(self):\n        # key as variable name and value as numpy arrays\n        state_dict = {}\n        for layer in self._model.layers:\n            for weight in layer.weights:\n                state_dict[weight.name] = weight.numpy()\n        return state_dict\n\n    @contextlib.contextmanager\n    def use_params(self, params):\n        key_prefix = f""tensorflow_{self.id}_""\n        # state dict stores key as name and value as numpy array\n        state_dict = {}\n        for k, v in params.items():\n            if hasattr(k, ""startswith"") and k.startswith(key_prefix):\n                if cupy is None:\n                    assert isinstance(v, numpy.ndarray)\n                else:  # pragma: no cover\n                    if isinstance(v, cupy.core.core.ndarray):\n                        v = cupy.asnumpy(v)\n                    assert isinstance(v, numpy.ndarray)\n                state_dict[k.replace(key_prefix, """")] = v\n        if state_dict:\n            backup = self._create_state_dict()\n            self._load_weights_from_state_dict(state_dict)\n            yield\n            self._load_weights_from_state_dict(backup)\n        else:\n            yield\n\n    def _clone_model(self):\n        """"""similar to tf.keras.models.clone_model()\n        But the tf.keras.models.clone_model changes the names of tf.Variables.\n        This method even preserves that\n        """"""\n        model_json_config = self._model.to_json()\n        tf.keras.backend.clear_session()\n        self._model = tf.keras.models.model_from_json(model_json_config)\n        self._load_weights_from_state_dict()\n\n    def copy(self):\n        model_json_config = self._model.to_json()\n        self._model = None\n        tf.keras.backend.clear_session()\n        copied = copy.deepcopy(self)\n        copied._model = tf.keras.models.model_from_json(model_json_config)\n        copied._load_weights_from_state_dict()\n        return copied\n\n    def to_device(self, device_type: str, device_id: int):  # pragma: no cover\n        if device_type == ""cpu"":\n            with tf.device(""/CPU""):  # pragma: no cover\n                self._clone_model()\n        elif device_type == ""gpu"":\n            with tf.device(""/GPU:{}"".format(device_id)):\n                self._clone_model()\n\n    def to_bytes(self):\n        filelike = BytesIO()\n        try:\n            with h5py.File(filelike, ""w"") as f:\n                self._model.save(f, save_format=""h5"")\n            return filelike.getvalue()\n        except NotImplementedError:\n            if not hasattr(self._model, ""catalogue_name""):\n                raise ValueError(\n                    ""Couldn\'t serialize to h5, and model has no factory ""\n                    ""function for component serialization.""\n                )\n        # Check the factory function and throw ValueError if it doesn\'t exist\n        keras_model_fns.get(self._model.catalogue_name)\n        return self._model.catalogue_name, self._model.get_weights()\n\n    def from_bytes(self, data):\n        ops: Ops = get_current_ops()\n        if ops.device_type == ""cpu"":\n            device = ""CPU""\n        else:  # pragma: no cover\n            device = tf.test.gpu_device_name()\n\n        # Plain bytes\n        if isinstance(data, (str, bytes)):\n            tf.keras.backend.clear_session()\n            filelike = BytesIO(data)\n            filelike.seek(0)\n            with h5py.File(filelike, ""r"") as f:\n                with tf.device(device):\n                    self._model = tf.keras.models.load_model(f)\n                return\n        # We only have to create the model if it doesn\'t already exist.\n        catalogue_name, model_weights = data\n        if self._model is None:\n            model_fn = keras_model_fns.get(catalogue_name)\n            tf.keras.backend.clear_session()\n            with tf.device(device):\n                if hasattr(self._model, ""eg_args""):\n                    ak: ArgsKwargs = self._model.eg_args\n                    new_model = model_fn(*ak.args, **ak.kwargs)\n                else:\n                    new_model = model_fn()\n            self._model_initialized = maybe_handshake_model(new_model)\n        self._model.set_weights(model_weights)\n'"
thinc/tests/__init__.py,0,b''
thinc/tests/conftest.py,0,"b'import pytest\n\n\ndef pytest_addoption(parser):\n    parser.addoption(""--slow"", action=""store_true"", help=""include slow tests"")\n\n\ndef pytest_runtest_setup(item):\n    def getopt(opt):\n        # When using \'pytest --pyargs thinc\' to test an installed copy of\n        # thinc, pytest skips running our pytest_addoption() hook. Later, when\n        # we call getoption(), pytest raises an error, because it doesn\'t\n        # recognize the option we\'re asking about. To avoid this, we need to\n        # pass a default value. We default to False, i.e., we act like all the\n        # options weren\'t given.\n        return item.config.getoption(f""--{opt}"", False)\n\n    for opt in [""slow""]:\n        if opt in item.keywords and not getopt(opt):\n            pytest.skip(f""need --{opt} option to run"")\n'"
thinc/tests/strategies.py,0,"b'import numpy\nfrom hypothesis.strategies import just, tuples, integers, floats\nfrom hypothesis.extra.numpy import arrays\nfrom thinc.api import NumpyOps, Linear\n\n\ndef get_ops():\n    return NumpyOps()\n\n\ndef get_model(W_values, b_values):\n    model = Linear(W_values.shape[0], W_values.shape[1], ops=NumpyOps())\n    model.initialize()\n    model.set_param(""W"", W_values)\n    model.set_param(""b"", b_values)\n    return model\n\n\ndef get_output(input_, W_values, b_values):\n    return numpy.einsum(""oi,bi->bo"", W_values, input_) + b_values\n\n\ndef get_input(nr_batch, nr_in):\n    ops = NumpyOps()\n    return ops.alloc2f(nr_batch, nr_in)\n\n\ndef lengths(lo=1, hi=10):\n    return integers(min_value=lo, max_value=hi)\n\n\ndef shapes(min_rows=1, max_rows=100, min_cols=1, max_cols=100):\n    return tuples(lengths(lo=min_rows, hi=max_rows), lengths(lo=min_cols, hi=max_cols))\n\n\ndef ndarrays_of_shape(shape, lo=-10.0, hi=10.0, dtype=""float32"", width=32):\n    return arrays(\n        dtype, shape=shape, elements=floats(min_value=lo, max_value=hi, width=width)\n    )\n\n\ndef ndarrays(min_len=0, max_len=10, min_val=-10.0, max_val=10.0):\n    return lengths(lo=1, hi=2).flatmap(\n        lambda n: ndarrays_of_shape(n, lo=min_val, hi=max_val)\n    )\n\n\ndef arrays_BI(min_B=1, max_B=10, min_I=1, max_I=100):\n    shapes = tuples(lengths(lo=min_B, hi=max_B), lengths(lo=min_I, hi=max_I))\n    return shapes.flatmap(ndarrays_of_shape)\n\n\ndef arrays_BOP(min_B=1, max_B=10, min_O=1, max_O=100, min_P=1, max_P=5):\n    shapes = tuples(\n        lengths(lo=min_B, hi=max_B),\n        lengths(lo=min_O, hi=max_O),\n        lengths(lo=min_P, hi=max_P),\n    )\n    return shapes.flatmap(ndarrays_of_shape)\n\n\ndef arrays_BOP_BO(min_B=1, max_B=10, min_O=1, max_O=100, min_P=1, max_P=5):\n    shapes = tuples(\n        lengths(lo=min_B, hi=max_B),\n        lengths(lo=min_O, hi=max_O),\n        lengths(lo=min_P, hi=max_P),\n    )\n    return shapes.flatmap(\n        lambda BOP: tuples(ndarrays_of_shape(BOP), ndarrays_of_shape(BOP[:-1]))\n    )\n\n\ndef arrays_BI_BO(min_B=1, max_B=10, min_I=1, max_I=100, min_O=1, max_O=100):\n    shapes = tuples(\n        lengths(lo=min_B, hi=max_B),\n        lengths(lo=min_I, hi=max_I),\n        lengths(lo=min_O, hi=max_O),\n    )\n    return shapes.flatmap(\n        lambda BIO: tuples(\n            ndarrays_of_shape((BIO[0], BIO[1])), ndarrays_of_shape((BIO[0], BIO[2]))\n        )\n    )\n\n\ndef arrays_OI_O_BI(\n    min_batch=1, max_batch=16, min_out=1, max_out=16, min_in=1, max_in=16\n):\n    shapes = tuples(\n        lengths(lo=min_batch, hi=max_batch),\n        lengths(lo=min_in, hi=max_out),\n        lengths(lo=min_in, hi=max_in),\n    )\n\n    def W_b_inputs(shape):\n        batch_size, nr_out, nr_in = shape\n        W = ndarrays_of_shape((nr_out, nr_in))\n        b = ndarrays_of_shape((nr_out,))\n        input_ = ndarrays_of_shape((batch_size, nr_in))\n        return tuples(W, b, input_)\n\n    return shapes.flatmap(W_b_inputs)\n\n\ndef arrays_OPFI_BI_lengths(max_B=5, max_P=3, max_F=5, max_I=8):\n    shapes = tuples(\n        lengths(hi=max_B),\n        lengths(hi=max_P),\n        lengths(hi=max_F),\n        lengths(hi=max_I),\n        arrays(""int32"", shape=(5,), elements=integers(min_value=1, max_value=10)),\n    )\n\n    strat = shapes.flatmap(\n        lambda opfi_lengths: tuples(\n            ndarrays_of_shape(opfi_lengths[:-1]),\n            ndarrays_of_shape((sum(opfi_lengths[-1]), opfi_lengths[-2])),\n            just(opfi_lengths[-1]),\n        )\n    )\n    return strat\n'"
thinc/tests/test_config.py,0,"b'import pytest\nfrom typing import Iterable, Union, Optional, List, Callable, Dict\nfrom types import GeneratorType\nfrom pydantic import BaseModel, StrictBool, StrictFloat, PositiveInt, constr\nimport catalogue\nimport thinc.config\nfrom thinc.config import ConfigValidationError\nfrom thinc.types import Generator\nfrom thinc.api import Config, RAdam, Model, NumpyOps\nfrom thinc.util import partial\nimport numpy\nimport inspect\n\nfrom .util import make_tempdir\n\n\nEXAMPLE_CONFIG = """"""\n[optimizer]\n@optimizers = ""Adam.v1""\nbeta1 = 0.9\nbeta2 = 0.999\nuse_averages = true\n\n[optimizer.learn_rate]\n@schedules = ""warmup_linear.v1""\ninitial_rate = 0.1\nwarmup_steps = 10000\ntotal_steps = 100000\n\n[pipeline]\n\n[pipeline.parser]\nname = ""parser""\nfactory = ""parser""\n\n[pipeline.parser.model]\n@layers = ""spacy.ParserModel.v1""\nhidden_depth = 1\nhidden_width = 64\ntoken_vector_width = 128\n\n[pipeline.parser.model.tok2vec]\n@layers = ""Tok2Vec.v1""\nwidth = ${pipeline.parser.model:token_vector_width}\n\n[pipeline.parser.model.tok2vec.embed]\n@layers = ""spacy.MultiFeatureHashEmbed.v1""\nwidth = ${pipeline.parser.model.tok2vec:width}\n\n[pipeline.parser.model.tok2vec.embed.hidden]\n@layers = ""MLP.v1""\ndepth = 1\npieces = 3\nlayer_norm = true\noutputs = ${pipeline.parser.model.tok2vec.embed:width}\n\n[pipeline.parser.model.tok2vec.encode]\n@layers = ""spacy.MaxoutWindowEncoder.v1""\ndepth = 4\npieces = 3\nwindow_size = 1\n\n[pipeline.parser.model.lower]\n@layers = ""spacy.ParserLower.v1""\n\n[pipeline.parser.model.upper]\n@layers = ""thinc.Linear.v1""\n""""""\n\nOPTIMIZER_CFG = """"""\n[optimizer]\n@optimizers = ""Adam.v1""\nbeta1 = 0.9\nbeta2 = 0.999\nuse_averages = true\n\n[optimizer.learn_rate]\n@schedules = ""warmup_linear.v1""\ninitial_rate = 0.1\nwarmup_steps = 10000\ntotal_steps = 100000\n""""""\n\n\nclass my_registry(thinc.config.registry):\n    cats = catalogue.create(""thinc"", ""tests"", ""cats"", entry_points=False)\n\n\nclass HelloIntsSchema(BaseModel):\n    hello: int\n    world: int\n\n    class Config:\n        extra = ""forbid""\n\n\nclass DefaultsSchema(BaseModel):\n    required: int\n    optional: str = ""default value""\n\n    class Config:\n        extra = ""forbid""\n\n\nclass ComplexSchema(BaseModel):\n    outer_req: int\n    outer_opt: str = ""default value""\n\n    level2_req: HelloIntsSchema\n    level2_opt: DefaultsSchema = DefaultsSchema(required=1)\n\n\n@my_registry.cats.register(""catsie.v1"")\ndef catsie_v1(evil: StrictBool, cute: bool = True) -> str:\n    if evil:\n        return ""scratch!""\n    else:\n        return ""meow""\n\n\n@my_registry.cats.register(""catsie.v2"")\ndef catsie_v2(evil: StrictBool, cute: bool = True, cute_level: int = 1) -> str:\n    if evil:\n        return ""scratch!""\n    else:\n        if cute_level > 2:\n            return ""meow <3""\n        return ""meow""\n\n\ngood_catsie = {""@cats"": ""catsie.v1"", ""evil"": False, ""cute"": True}\nok_catsie = {""@cats"": ""catsie.v1"", ""evil"": False, ""cute"": False}\nbad_catsie = {""@cats"": ""catsie.v1"", ""evil"": True, ""cute"": True}\nworst_catsie = {""@cats"": ""catsie.v1"", ""evil"": True, ""cute"": False}\n\n\ndef test_validate_simple_config():\n    simple_config = {""hello"": 1, ""world"": 2}\n    f, _, v = my_registry._fill(simple_config, HelloIntsSchema)\n    assert f == simple_config\n    assert v == simple_config\n\n\ndef test_invalidate_simple_config():\n    invalid_config = {""hello"": 1, ""world"": ""hi!""}\n    with pytest.raises(ConfigValidationError):\n        my_registry._fill(invalid_config, HelloIntsSchema)\n\n\ndef test_invalidate_extra_args():\n    invalid_config = {""hello"": 1, ""world"": 2, ""extra"": 3}\n    with pytest.raises(ConfigValidationError):\n        my_registry._fill(invalid_config, HelloIntsSchema)\n\n\ndef test_fill_defaults_simple_config():\n    valid_config = {""required"": 1}\n    filled, _, v = my_registry._fill(valid_config, DefaultsSchema)\n    assert filled[""required""] == 1\n    assert filled[""optional""] == ""default value""\n    invalid_config = {""optional"": ""some value""}\n    with pytest.raises(ConfigValidationError):\n        my_registry._fill(invalid_config, DefaultsSchema)\n\n\ndef test_fill_recursive_config():\n    valid_config = {""outer_req"": 1, ""level2_req"": {""hello"": 4, ""world"": 7}}\n    filled, _, validation = my_registry._fill(valid_config, ComplexSchema)\n    assert filled[""outer_req""] == 1\n    assert filled[""outer_opt""] == ""default value""\n    assert filled[""level2_req""][""hello""] == 4\n    assert filled[""level2_req""][""world""] == 7\n    assert filled[""level2_opt""][""required""] == 1\n    assert filled[""level2_opt""][""optional""] == ""default value""\n\n\ndef test_is_promise():\n    assert my_registry.is_promise(good_catsie)\n    assert not my_registry.is_promise({""hello"": ""world""})\n    assert not my_registry.is_promise(1)\n    invalid = {""@complex"": ""complex.v1"", ""rate"": 1.0, ""@cats"": ""catsie.v1""}\n    assert my_registry.is_promise(invalid)\n\n\ndef test_get_constructor():\n    func = my_registry.get_constructor(good_catsie)\n    assert func is catsie_v1\n\n\ndef test_parse_args():\n    args, kwargs = my_registry.parse_args(bad_catsie)\n    assert args == []\n    assert kwargs == {""evil"": True, ""cute"": True}\n\n\ndef test_make_promise_schema():\n    schema = my_registry.make_promise_schema(good_catsie)\n    assert ""evil"" in schema.__fields__\n    assert ""cute"" in schema.__fields__\n\n\ndef test_validate_promise():\n    config = {""required"": 1, ""optional"": good_catsie}\n    filled, _, validated = my_registry._fill(config, DefaultsSchema)\n    assert filled == config\n    assert validated == {""required"": 1, ""optional"": ""meow""}\n\n\ndef test_fill_validate_promise():\n    config = {""required"": 1, ""optional"": {""@cats"": ""catsie.v1"", ""evil"": False}}\n    filled, _, validated = my_registry._fill(config, DefaultsSchema)\n    assert filled[""optional""][""cute""] is True\n\n\ndef test_fill_invalidate_promise():\n    config = {""required"": 1, ""optional"": {""@cats"": ""catsie.v1"", ""evil"": False}}\n    with pytest.raises(ConfigValidationError):\n        my_registry._fill(config, HelloIntsSchema)\n    config[""optional""][""whiskers""] = True\n    with pytest.raises(ConfigValidationError):\n        my_registry._fill(config, DefaultsSchema)\n\n\ndef test_create_registry():\n    with pytest.raises(ValueError):\n        my_registry.create(""cats"")\n    my_registry.create(""dogs"")\n    assert hasattr(my_registry, ""dogs"")\n    assert len(my_registry.dogs.get_all()) == 0\n    my_registry.dogs.register(""good_boy.v1"", func=lambda x: x)\n    assert len(my_registry.dogs.get_all()) == 1\n    with pytest.raises(ValueError):\n        my_registry.create(""dogs"")\n\n\ndef test_registry_methods():\n    with pytest.raises(ValueError):\n        my_registry.get(""dfkoofkds"", ""catsie.v1"")\n    my_registry.cats.register(""catsie.v123"")(None)\n    with pytest.raises(ValueError):\n        my_registry.get(""cats"", ""catsie.v123"")\n\n\ndef test_make_from_config_no_schema():\n    config = {""one"": 1, ""two"": {""three"": {""@cats"": ""catsie.v1"", ""evil"": True}}}\n    result = my_registry.make_from_config(config)\n    assert result[""one""] == 1\n    assert result[""two""] == {""three"": ""scratch!""}\n    with pytest.raises(ConfigValidationError):\n        config = {""one"": 1, ""two"": {""three"": {""@cats"": ""catsie.v1"", ""evil"": ""true""}}}\n        my_registry.make_from_config(config)\n\n\ndef test_make_from_config_schema():\n    class TestBaseSubSchema(BaseModel):\n        three: str\n\n    class TestBaseSchema(BaseModel):\n        one: PositiveInt\n        two: TestBaseSubSchema\n\n        class Config:\n            extra = ""forbid""\n\n    config = {""one"": 1, ""two"": {""three"": {""@cats"": ""catsie.v1"", ""evil"": True}}}\n    my_registry.make_from_config(config, schema=TestBaseSchema)\n    config = {""one"": -1, ""two"": {""three"": {""@cats"": ""catsie.v1"", ""evil"": True}}}\n    with pytest.raises(ConfigValidationError):\n        # ""one"" is not a positive int\n        my_registry.make_from_config(config, schema=TestBaseSchema)\n    config = {""one"": 1, ""two"": {""four"": {""@cats"": ""catsie.v1"", ""evil"": True}}}\n    with pytest.raises(ConfigValidationError):\n        # ""three"" is required in subschema\n        my_registry.make_from_config(config, schema=TestBaseSchema)\n\n\ndef test_make_from_config_schema_coerced():\n    class TestBaseSchema(BaseModel):\n        test1: str\n        test2: bool\n        test3: float\n\n    config = {""test1"": 123, ""test2"": 1, ""test3"": 5}\n    result = my_registry.make_from_config(config, schema=TestBaseSchema)\n    assert result[""test1""] == ""123""\n    assert result[""test2""] is True\n    assert result[""test3""] == 5.0\n\n\ndef test_read_config():\n    byte_string = EXAMPLE_CONFIG.encode(""utf8"")\n    cfg = Config().from_bytes(byte_string)\n\n    assert cfg[""optimizer""][""beta1""] == 0.9\n    assert cfg[""optimizer""][""learn_rate""][""initial_rate""] == 0.1\n    assert cfg[""pipeline""][""parser""][""factory""] == ""parser""\n    assert cfg[""pipeline""][""parser""][""model""][""tok2vec""][""width""] == 128\n\n\ndef test_optimizer_config():\n    cfg = Config().from_str(OPTIMIZER_CFG)\n    result = my_registry.make_from_config(cfg, validate=True)\n    optimizer = result[""optimizer""]\n    assert optimizer.b1 == 0.9\n\n\ndef test_config_to_str():\n    cfg = Config().from_str(OPTIMIZER_CFG)\n    assert cfg.to_str().strip() == OPTIMIZER_CFG.strip()\n    cfg = Config({""optimizer"": {""foo"": ""bar""}}).from_str(OPTIMIZER_CFG)\n    assert cfg.to_str().strip() == OPTIMIZER_CFG.strip()\n\n\ndef test_config_roundtrip_bytes():\n    cfg = Config().from_str(OPTIMIZER_CFG)\n    cfg_bytes = cfg.to_bytes()\n    new_cfg = Config().from_bytes(cfg_bytes)\n    assert new_cfg.to_str().strip() == OPTIMIZER_CFG.strip()\n\n\ndef test_config_roundtrip_disk():\n    cfg = Config().from_str(OPTIMIZER_CFG)\n    with make_tempdir() as path:\n        cfg_path = path / ""config.cfg""\n        cfg.to_disk(cfg_path)\n        new_cfg = Config().from_disk(cfg_path)\n    assert new_cfg.to_str().strip() == OPTIMIZER_CFG.strip()\n\n\ndef test_validation_custom_types():\n    def complex_args(\n        rate: StrictFloat,\n        steps: PositiveInt = 10,  # type: ignore\n        log_level: constr(regex=""(DEUG|INFO|WARNING|ERROR)"") = ""ERROR"",\n    ):\n        return None\n\n    my_registry.create(""complex"")\n    my_registry.complex(""complex.v1"")(complex_args)\n    cfg = {""@complex"": ""complex.v1"", ""rate"": 1.0, ""steps"": 20, ""log_level"": ""INFO""}\n    my_registry.make_from_config({""config"": cfg})\n    cfg = {""@complex"": ""complex.v1"", ""rate"": 1.0, ""steps"": -1, ""log_level"": ""INFO""}\n    with pytest.raises(ConfigValidationError):\n        # steps is not a positive int\n        my_registry.make_from_config({""config"": cfg})\n    cfg = {""@complex"": ""complex.v1"", ""rate"": 1.0, ""steps"": 20, ""log_level"": ""none""}\n    with pytest.raises(ConfigValidationError):\n        # log_level is not a string matching the regex\n        my_registry.make_from_config({""config"": cfg})\n    cfg = {""@complex"": ""complex.v1"", ""rate"": 1.0, ""steps"": 20, ""log_level"": ""INFO""}\n    with pytest.raises(ConfigValidationError):\n        # top-level object is promise\n        my_registry.make_from_config(cfg)\n    with pytest.raises(ConfigValidationError):\n        # top-level object is promise\n        my_registry.fill_config(cfg)\n    cfg = {""@complex"": ""complex.v1"", ""rate"": 1.0, ""@cats"": ""catsie.v1""}\n    with pytest.raises(ConfigValidationError):\n        # two constructors\n        my_registry.make_from_config({""config"": cfg})\n\n\ndef test_validation_no_validate():\n    config = {""one"": 1, ""two"": {""three"": {""@cats"": ""catsie.v1"", ""evil"": ""false""}}}\n    result = my_registry.make_from_config(config, validate=False)\n    assert result[""one""] == 1\n    assert result[""two""] == {""three"": ""scratch!""}\n\n\ndef test_validation_fill_defaults():\n    config = {""one"": 1, ""two"": {""@cats"": ""catsie.v1"", ""evil"": ""hello""}}\n    result = my_registry.fill_config(config, validate=False)\n    assert len(result[""two""]) == 3\n    with pytest.raises(ConfigValidationError):\n        # Required arg ""evil"" is not defined\n        my_registry.fill_config(config)\n    config = {""one"": 1, ""two"": {""@cats"": ""catsie.v2"", ""evil"": False}}\n    # Fill in with new defaults\n    result = my_registry.fill_config(config)\n    assert len(result[""two""]) == 4\n    assert result[""two""][""evil""] is False\n    assert result[""two""][""cute""] is True\n    assert result[""two""][""cute_level""] == 1\n\n\ndef test_make_config_positional_args():\n    @my_registry.cats(""catsie.v567"")\n    def catsie_567(*args: Optional[str], foo: str = ""bar""):\n        assert args[0] == ""^_^""\n        assert args[1] == ""^(*.*)^""\n        assert foo == ""baz""\n        return args[0]\n\n    args = [""^_^"", ""^(*.*)^""]\n    cfg = {""config"": {""@cats"": ""catsie.v567"", ""foo"": ""baz"", ""*"": args}}\n    filled_cfg = my_registry.make_from_config(cfg)\n    assert filled_cfg[""config""] == ""^_^""\n\n\ndef test_make_config_positional_args_complex():\n    @my_registry.cats(""catsie.v890"")\n    def catsie_890(*args: Optional[Union[StrictBool, PositiveInt]]):\n        assert args[0] == 123\n        return args[0]\n\n    cfg = {""config"": {""@cats"": ""catsie.v890"", ""*"": [123, True, 1, False]}}\n    filled_cfg = my_registry.make_from_config(cfg)\n    assert filled_cfg[""config""] == 123\n    cfg = {""config"": {""@cats"": ""catsie.v890"", ""*"": [123, ""True""]}}\n    with pytest.raises(ConfigValidationError):\n        # ""True"" is not a valid boolean or positive int\n        my_registry.make_from_config(cfg)\n\n\ndef test_positional_args_to_from_string():\n    cfg = """"""[a]\\nb = 1\\n* = [""foo"",""bar""]""""""\n    assert Config().from_str(cfg).to_str() == cfg\n    cfg = """"""[a]\\nb = 1\\n\\n[a.*.foo]\\ntest = 1\\n\\n[a.*.bar]\\ntest = 2""""""\n    assert Config().from_str(cfg).to_str() == cfg\n\n    @my_registry.cats(""catsie.v666"")\n    def catsie_666(*args, meow=False):\n        return args\n\n    cfg = """"""[a]\\n@cats = ""catsie.v666""\\n* = [""foo"",""bar""]""""""\n    filled = my_registry.fill_config(Config().from_str(cfg)).to_str()\n    assert filled == """"""[a]\\n@cats = ""catsie.v666""\\n* = [""foo"",""bar""]\\nmeow = false""""""\n    assert my_registry.make_from_config(Config().from_str(cfg)) == {""a"": (""foo"", ""bar"")}\n    cfg = """"""[a]\\n@cats = ""catsie.v666""\\n\\n[a.*.foo]\\nx = 1""""""\n    filled = my_registry.fill_config(Config().from_str(cfg)).to_str()\n    assert filled == """"""[a]\\n@cats = ""catsie.v666""\\nmeow = false\\n\\n[a.*.foo]\\nx = 1""""""\n    assert my_registry.make_from_config(Config().from_str(cfg)) == {""a"": ({""x"": 1},)}\n\n    @my_registry.cats(""catsie.v777"")\n    def catsie_777(y: int = 1):\n        return ""meow"" * y\n\n    cfg = """"""[a]\\n@cats = ""catsie.v666""\\n\\n[a.*.foo]\\n@cats = ""catsie.v777\\""""""""\n    filled = my_registry.fill_config(Config().from_str(cfg)).to_str()\n    expected = """"""[a]\\n@cats = ""catsie.v666""\\nmeow = false\\n\\n[a.*.foo]\\n@cats = ""catsie.v777""\\ny = 1""""""\n    assert filled == expected\n    cfg = """"""[a]\\n@cats = ""catsie.v666""\\n\\n[a.*.foo]\\n@cats = ""catsie.v777""\\ny = 3""""""\n    result = my_registry.make_from_config(Config().from_str(cfg))\n    assert result == {""a"": (""meowmeowmeow"",)}\n\n\ndef test_make_config_positional_args_dicts():\n    cfg = {\n        ""hyper_params"": {""n_hidden"": 512, ""dropout"": 0.2, ""learn_rate"": 0.001},\n        ""model"": {\n            ""@layers"": ""chain.v1"",\n            ""*"": {\n                ""relu1"": {""@layers"": ""Relu.v1"", ""nO"": 512, ""dropout"": 0.2},\n                ""relu2"": {""@layers"": ""Relu.v1"", ""nO"": 512, ""dropout"": 0.2},\n                ""softmax"": {""@layers"": ""Softmax.v1""},\n            },\n        },\n        ""optimizer"": {""@optimizers"": ""Adam.v1"", ""learn_rate"": 0.001},\n    }\n    loaded = my_registry.make_from_config(cfg)\n    model = loaded[""model""]\n    X = numpy.ones((784, 1), dtype=""f"")\n    model.initialize(X=X, Y=numpy.zeros((784, 1), dtype=""f""))\n    model.begin_update(X)\n    model.finish_update(loaded[""optimizer""])\n\n\ndef test_validation_generators_iterable():\n    @my_registry.optimizers(""test_optimizer.v1"")\n    def test_optimizer_v1(rate: float,) -> None:\n        return None\n\n    @my_registry.schedules(""test_schedule.v1"")\n    def test_schedule_v1(some_value: float = 1.0) -> Iterable[float]:\n        while True:\n            yield some_value\n\n    config = {""optimizer"": {""@optimizers"": ""test_optimizer.v1"", ""rate"": 0.1}}\n    my_registry.make_from_config(config)\n\n\ndef test_validation_unset_type_hints():\n    """"""Test that unset type hints are handled correctly (and treated as Any).""""""\n\n    @my_registry.optimizers(""test_optimizer.v2"")\n    def test_optimizer_v2(rate, steps: int = 10) -> None:\n        return None\n\n    config = {""test"": {""@optimizers"": ""test_optimizer.v2"", ""rate"": 0.1, ""steps"": 20}}\n    my_registry.make_from_config(config)\n\n\ndef test_validation_bad_function():\n    @my_registry.optimizers(""bad.v1"")\n    def bad() -> None:\n        raise ValueError(""This is an error in the function"")\n        return None\n\n    @my_registry.optimizers(""good.v1"")\n    def good() -> None:\n        return None\n\n    # Bad function\n    config = {""test"": {""@optimizers"": ""bad.v1""}}\n    with pytest.raises(ConfigValidationError):\n        my_registry.make_from_config(config)\n    # Bad function call\n    config = {""test"": {""@optimizers"": ""good.v1"", ""invalid_arg"": 1}}\n    with pytest.raises(ConfigValidationError):\n        my_registry.make_from_config(config)\n\n\ndef test_objects_from_config():\n    config = {\n        ""optimizer"": {\n            ""@optimizers"": ""my_cool_optimizer.v1"",\n            ""beta1"": 0.2,\n            ""learn_rate"": {\n                ""@schedules"": ""my_cool_repetitive_schedule.v1"",\n                ""base_rate"": 0.001,\n                ""repeat"": 4,\n            },\n        }\n    }\n\n    @thinc.registry.optimizers.register(""my_cool_optimizer.v1"")\n    def make_my_optimizer(learn_rate: List[float], beta1: float):\n        return RAdam(learn_rate, beta1=beta1)\n\n    @thinc.registry.schedules(""my_cool_repetitive_schedule.v1"")\n    def decaying(base_rate: float, repeat: int) -> List[float]:\n        return repeat * [base_rate]\n\n    loaded = my_registry.make_from_config(config)\n    optimizer = loaded[""optimizer""]\n    assert optimizer.b1 == 0.2\n    assert ""learn_rate"" in optimizer.schedules\n    assert optimizer.learn_rate == 0.001\n\n\ndef test_partials_from_config():\n    """"""Test that functions registered with partial applications are handled\n    correctly (e.g. initializers).""""""\n    name = ""uniform_init.v1""\n    cfg = {""test"": {""@initializers"": name, ""lo"": -0.2}}\n    func = my_registry.make_from_config(cfg)[""test""]\n    assert hasattr(func, ""__call__"")\n    # The partial will still have lo as an arg, just with default\n    assert len(inspect.signature(func).parameters) == 4\n    # Make sure returned partial function has correct value set\n    assert inspect.signature(func).parameters[""lo""].default == -0.2\n    # Actually call the function and verify\n    func(NumpyOps(), (2, 3))\n    # Make sure validation still works\n    bad_cfg = {""test"": {""@initializers"": name, ""lo"": [0.5]}}\n    with pytest.raises(ConfigValidationError):\n        my_registry.make_from_config(bad_cfg)\n    bad_cfg = {""test"": {""@initializers"": name, ""lo"": -0.2, ""other"": 10}}\n    with pytest.raises(ConfigValidationError):\n        my_registry.make_from_config(bad_cfg)\n\n\ndef test_partials_from_config_nested():\n    """"""Test that partial functions are passed correctly to other registered\n    functions that consume them (e.g. initializers -> layers).""""""\n\n    def test_initializer(a: int, b: int = 1) -> int:\n        return a * b\n\n    @my_registry.initializers(""test_initializer.v1"")\n    def configure_test_initializer(b: int = 1) -> Callable[[int], int]:\n        return partial(test_initializer, b=b)\n\n    @my_registry.layers(""test_layer.v1"")\n    def test_layer(init: Callable[[int], int], c: int = 1) -> Callable[[int], int]:\n        return lambda x: x + init(c)\n\n    cfg = {\n        ""@layers"": ""test_layer.v1"",\n        ""c"": 5,\n        ""init"": {""@initializers"": ""test_initializer.v1"", ""b"": 10},\n    }\n    func = my_registry.make_from_config({""test"": cfg})[""test""]\n    assert func(1) == 51\n    assert func(100) == 150\n\n\ndef test_validate_generator():\n    """"""Test that generator replacement for validation in config doesn\'t\n    actually replace the returned value.""""""\n\n    @my_registry.schedules(""test_schedule.v2"")\n    def test_schedule():\n        while True:\n            yield 10\n\n    cfg = {""@schedules"": ""test_schedule.v2""}\n    result = my_registry.make_from_config({""test"": cfg})[""test""]\n    assert isinstance(result, GeneratorType)\n\n    @my_registry.optimizers(""test_optimizer.v2"")\n    def test_optimizer2(rate: Generator) -> Generator:\n        return rate\n\n    cfg = {\n        ""@optimizers"": ""test_optimizer.v2"",\n        ""rate"": {""@schedules"": ""test_schedule.v2""},\n    }\n    result = my_registry.make_from_config({""test"": cfg})[""test""]\n    assert isinstance(result, GeneratorType)\n\n    @my_registry.optimizers(""test_optimizer.v3"")\n    def test_optimizer3(schedules: Dict[str, Generator]) -> Generator:\n        return schedules[""rate""]\n\n    cfg = {\n        ""@optimizers"": ""test_optimizer.v3"",\n        ""schedules"": {""rate"": {""@schedules"": ""test_schedule.v2""}},\n    }\n    result = my_registry.make_from_config({""test"": cfg})[""test""]\n    assert isinstance(result, GeneratorType)\n\n    @my_registry.optimizers(""test_optimizer.v4"")\n    def test_optimizer4(*schedules: Generator) -> Generator:\n        return schedules[0]\n\n\ndef test_handle_generic_model_type():\n    """"""Test that validation can handle checks against arbitrary generic\n    types in function argument annotations.""""""\n\n    @my_registry.layers(""my_transform.v1"")\n    def my_transform(model: Model[int, int]):\n        model.name = ""transformed_model""\n        return model\n\n    cfg = {""@layers"": ""my_transform.v1"", ""model"": {""@layers"": ""Linear.v1""}}\n    model = my_registry.make_from_config({""test"": cfg})[""test""]\n    assert isinstance(model, Model)\n    assert model.name == ""transformed_model""\n\n\n@pytest.mark.parametrize(\n    ""cfg"",\n    [\n        ""[a]\\nb = 1\\nc = 2\\n\\n[a.c]\\nd = 3"",\n        ""[a]\\nb = 1\\n\\n[a.c]\\nd = 2\\n\\n[a.c.d]\\ne = 3"",\n    ],\n)\ndef test_handle_error_duplicate_keys(cfg):\n    """"""This would cause very cryptic error when interpreting config.\n    (TypeError: \'X\' object does not support item assignment)\n    """"""\n    with pytest.raises(ConfigValidationError):\n        Config().from_str(cfg)\n'"
thinc/tests/test_examples.py,0,"b'import os\nimport sys\nfrom pathlib import Path\n\nimport nbformat\nimport pytest\nfrom nbconvert.preprocessors import ExecutePreprocessor\n\n\n@pytest.fixture\ndef test_files(nb_file):\n    if not Path(nb_file).exists():\n        return\n    kernel_name = os.environ.get(""NOTEBOOK_KERNEL"", ""python3"")\n    with open(nb_file) as f:\n        nb = nbformat.read(f, as_version=4)\n    proc = ExecutePreprocessor(timeout=600, kernel_name=kernel_name)\n    proc.allow_errors = True\n    proc.preprocess(nb, {""metadata"": {""path"": ""/""}})\n    cells_with_outputs = [c for c in nb.cells if ""outputs"" in c]\n    for cell in cells_with_outputs:\n        for output in cell[""outputs""]:\n            if output.output_type == ""error"":\n                for l in output.traceback:\n                    print(l)\n                raise Exception(f""{output.ename}: {output.evalue}"")\n\n\n@pytest.mark.parametrize(\n    ""nb_file"",\n    (\n        ""examples/01_intro_model_definition_methods.ipynb"",\n        ""examples/05_benchmarking_layers.ipynb"",\n    ),\n)\ndef test_ipython_notebooks(test_files: None):\n    ...\n\n\n@pytest.mark.skip(reason=""these notebooks need special software or hardware"")\n@pytest.mark.parametrize(\n    ""nb_file"",\n    (\n        ""examples/00_intro_to_thinc.ipynb"",\n        ""examples/02_transformers_tagger_bert.ipynb"",\n        ""examples/03_pos_tagger_basic_cnn.ipynb"",\n        ""examples/03_textcat_basic_neural_bow.ipynb"",\n        ""examples/04_configure_gpu_memory.ipynb"",\n        ""examples/04_parallel_training_ray.ipynb"",\n        ""examples/05_visualizing_models.ipynb"",\n        ""examples/06_predicting_like_terms.ipynb"",\n    ),\n)\ndef test_ipython_notebooks_slow(test_files: None):\n    ...\n'"
thinc/tests/test_indexing.py,0,"b'import pytest\nimport numpy\nfrom numpy.testing import assert_allclose\nfrom thinc.types import Ragged, Pairs\n\n\n@pytest.fixture\ndef ragged():\n    data = numpy.zeros((20, 4), dtype=""f"")\n    lengths = numpy.array([4, 2, 8, 1, 4], dtype=""i"")\n    data[0] = 0\n    data[1] = 1\n    data[2] = 2\n    data[3] = 3\n    data[4] = 4\n    data[5] = 5\n    return Ragged(data, lengths)\n\n\ndef test_ragged_starts_ends(ragged):\n    starts = ragged._get_starts()\n    ends = ragged._get_ends()\n    assert list(starts) == [0, 4, 6, 14, 15]\n    assert list(ends) == [4, 6, 14, 15, 19]\n\n\ndef test_ragged_simple_index(ragged, i=1):\n    r = ragged[i]\n    assert_allclose(r.data, ragged.data[4:6])\n    assert_allclose(r.lengths, ragged.lengths[i : i + 1])\n\n\ndef test_ragged_slice_index(ragged, start=0, end=2):\n    r = ragged[start:end]\n    size = ragged.lengths[start:end].sum()\n    assert r.data.shape == (size, r.data.shape[1])\n    assert_allclose(r.lengths, ragged.lengths[start:end])\n\n\ndef test_ragged_array_index(ragged):\n    arr = numpy.array([2, 1, 4], dtype=""i"")\n    r = ragged[arr]\n    assert r.data.shape[0] == ragged.lengths[arr].sum()\n\n\ndef test_pairs_arrays():\n    one = numpy.zeros((128, 45), dtype=""f"")\n    two = numpy.zeros((128, 12), dtype=""f"")\n    pairs = Pairs(one, two)\n    assert pairs[:2].one.shape == (2, 45)\n    assert pairs[0].two.shape == (12,)\n    assert pairs[-1:].one.shape == (1, 45)\n    assert pairs[-1:].two.shape == (1, 12)\n'"
thinc/tests/test_initializers.py,0,"b'import pytest\nfrom thinc.api import glorot_uniform_init, zero_init, uniform_init, normal_init\nfrom thinc.api import NumpyOps\nfrom thinc import registry\nimport numpy\n\n\n@pytest.mark.parametrize(\n    ""init_func"", [glorot_uniform_init, zero_init, uniform_init, normal_init]\n)\ndef test_initializer_func_setup(init_func):\n    ops = NumpyOps()\n    data = numpy.ndarray([1, 2, 3, 4], dtype=""f"")\n    result = init_func(ops, data.shape)\n    assert not numpy.array_equal(data, result)\n\n\n@pytest.mark.parametrize(\n    ""name,kwargs"",\n    [\n        (""glorot_uniform_init.v1"", {}),\n        (""zero_init.v1"", {}),\n        (""uniform_init.v1"", {""lo"": -0.5, ""hi"": 0.5}),\n        (""normal_init.v1"", {""mean"": 0.1}),\n    ],\n)\ndef test_initializer_from_config(name, kwargs):\n    """"""Test that initializers are loaded and configured correctly from registry\n    (as partials).""""""\n    cfg = {""test"": {""@initializers"": name, **kwargs}}\n    func = registry.make_from_config(cfg)[""test""]\n    func(NumpyOps(), (1, 2, 3, 4))\n'"
thinc/tests/test_loss.py,0,"b'import pytest\nimport numpy\nfrom thinc.api import CategoricalCrossentropy, SequenceCategoricalCrossentropy\nfrom thinc.api import L2Distance, CosineDistance\nfrom thinc import registry\n\n# some simple arrays\nscores0 = numpy.zeros((3, 3), dtype=""f"")\nlabels0 = numpy.asarray([0, 1, 1], dtype=""i"")\n\n# a few more diverse ones to test realistic values\nguesses1 = numpy.asarray([[0.1, 0.5, 0.6], [0.4, 0.6, 0.3], [1, 1, 1], [0, 0, 0]])\nlabels1 = numpy.asarray([2, 1, 0, 2])\nlabels1_full = numpy.asarray([[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 0, 1]])\n\nguesses2 = numpy.asarray([[0.2, 0.3]])\nlabels2 = numpy.asarray([1])\n\neps = 0.0001\n\n\ndef test_loss():\n    d_scores = CategoricalCrossentropy().get_grad(scores0, labels0)\n    assert d_scores.dtype == ""float32""\n    assert d_scores.shape == scores0.shape\n    d_scores = SequenceCategoricalCrossentropy().get_grad([scores0], [labels0])\n    assert d_scores[0].dtype == ""float32""\n    assert d_scores[0].shape == scores0.shape\n    assert SequenceCategoricalCrossentropy().get_grad([], []) == []\n\n\n@pytest.mark.parametrize(\n    ""dist"", [CategoricalCrossentropy(), CosineDistance(ignore_zeros=True), L2Distance()]\n)\n@pytest.mark.parametrize(""vect"", [scores0, guesses1, guesses2])\ndef test_equality(dist, vect):\n    assert int(dist.get_grad(vect, vect)[0][0]) == pytest.approx(0, eps)\n    assert dist.get_loss(vect, vect) == pytest.approx(0, eps)\n\n\n@pytest.mark.parametrize(\n    ""guesses, labels"", [(guesses1, labels1), (guesses1, labels1_full)]\n)\ndef test_categorical_crossentropy(guesses, labels):\n    d_scores = CategoricalCrossentropy(normalize=True).get_grad(guesses, labels)\n    assert d_scores.shape == guesses.shape\n\n    # The normalization divides the difference (e.g. 0.4) by the number of vectors (4)\n    assert d_scores[1][0] == pytest.approx(0.1, eps)\n    assert d_scores[1][1] == pytest.approx(-0.1, eps)\n\n    # The third vector predicted all labels, but only the first one was correct\n    assert d_scores[2][0] == pytest.approx(0, eps)\n    assert d_scores[2][1] == pytest.approx(0.25, eps)\n    assert d_scores[2][2] == pytest.approx(0.25, eps)\n\n    # The fourth vector predicted no labels but should have predicted the last one\n    assert d_scores[3][0] == pytest.approx(0, eps)\n    assert d_scores[3][1] == pytest.approx(0, eps)\n    assert d_scores[3][2] == pytest.approx(-0.25, eps)\n\n    loss = CategoricalCrossentropy(normalize=True).get_loss(guesses, labels)\n    assert loss == pytest.approx(0.239375, eps)\n\n\n@pytest.mark.parametrize(\n    ""guesses, labels"", [(guesses1, labels1), (guesses1, labels1_full)],\n)\ndef test_categorical_crossentropy_missing(guesses, labels):\n    d_scores = CategoricalCrossentropy(normalize=True).get_grad(guesses, labels)\n    assert d_scores.shape == guesses.shape\n\n\n@pytest.mark.parametrize(\n    ""guesses, labels"",\n    [\n        ([guesses1, guesses2], [labels1, labels2]),\n        ([guesses1, guesses2], [labels1_full, labels2]),\n    ],\n)\ndef test_sequence_categorical_crossentropy(guesses, labels):\n    d_scores = SequenceCategoricalCrossentropy(normalize=True).get_grad(guesses, labels)\n    d_scores1 = d_scores[0]\n    d_scores2 = d_scores[1]\n    assert d_scores1.shape == guesses1.shape\n    assert d_scores2.shape == guesses2.shape\n\n    # The normalization divides the difference (e.g. 0.4) by the number of entries\n    assert d_scores1[1][0] == pytest.approx(0.08, eps)\n    assert d_scores1[1][1] == pytest.approx(-0.08, eps)\n\n    # The third vector predicted all labels, but only the first one was correct\n    assert d_scores1[2][0] == pytest.approx(0, eps)\n    assert d_scores1[2][1] == pytest.approx(0.2, eps)\n    assert d_scores1[2][2] == pytest.approx(0.2, eps)\n\n    # The fourth vector predicted no labels but should have predicted the last one\n    assert d_scores1[3][0] == pytest.approx(0, eps)\n    assert d_scores1[3][1] == pytest.approx(0, eps)\n    assert d_scores1[3][2] == pytest.approx(-0.2, eps)\n\n    # Test the second batch\n    assert d_scores2[0][0] == pytest.approx(0.04, eps)\n    assert d_scores2[0][1] == pytest.approx(-0.14, eps)\n\n    loss = SequenceCategoricalCrossentropy(normalize=True).get_loss(guesses, labels)\n    assert loss == pytest.approx(0.1744, eps)\n\n\ndef test_L2():\n    # L2 loss = 2\xc2\xb2+4\xc2\xb2=20 (or normalized: 1\xc2\xb2+2\xc2\xb2=5)\n    vec1 = numpy.asarray([[1, 2], [8, 9]])\n    vec2 = numpy.asarray([[1, 2], [10, 5]])\n    d_vecs = L2Distance().get_grad(vec1, vec2)\n    assert d_vecs.shape == vec1.shape\n    numpy.testing.assert_allclose(\n        d_vecs[0], numpy.zeros(d_vecs[0].shape), rtol=eps, atol=eps\n    )\n\n    loss_not_normalized = L2Distance(normalize=False).get_loss(vec1, vec2)\n    assert loss_not_normalized == pytest.approx(20, eps)\n\n    loss_normalized = L2Distance(normalize=True).get_loss(vec1, vec2)\n    assert loss_normalized == pytest.approx(5, eps)\n\n\ndef test_cosine_orthogonal():\n    # These are orthogonal, i.e. loss is 1\n    vec1 = numpy.asarray([[0, 2], [0, 5]])\n    vec2 = numpy.asarray([[8, 0], [7, 0]])\n\n    d_vecs = CosineDistance(normalize=True).get_grad(vec1, vec2)\n    assert d_vecs.shape == vec1.shape\n    assert d_vecs[0][0] < 0\n    assert d_vecs[0][1] > 0\n    assert d_vecs[1][0] < 0\n    assert d_vecs[1][1] > 0\n\n    loss_not_normalized = CosineDistance(normalize=False).get_loss(vec1, vec2)\n    assert loss_not_normalized == pytest.approx(2, eps)\n\n    loss_normalized = CosineDistance(normalize=True).get_loss(vec1, vec2)\n    assert loss_normalized == pytest.approx(1, eps)\n\n\ndef test_cosine_equal():\n    # These 3 vectors are equal when measured with Cosine similarity, i.e. loss is 0\n    vec1 = numpy.asarray([[1, 2], [8, 9], [3, 3]])\n    vec2 = numpy.asarray([[1, 2], [80, 90], [300, 300]])\n\n    d_vec1 = CosineDistance().get_grad(vec1, vec2)\n    assert d_vec1.shape == vec1.shape\n    numpy.testing.assert_allclose(d_vec1, numpy.zeros(d_vec1.shape), rtol=eps, atol=eps)\n\n    loss_not_normalized = CosineDistance(normalize=False).get_loss(vec1, vec2)\n    assert loss_not_normalized == pytest.approx(0, eps)\n\n    loss_normalized = CosineDistance(normalize=True).get_loss(vec1, vec2)\n    assert loss_normalized == pytest.approx(0, eps)\n\n\ndef test_cosine_unmatched():\n    vec1 = numpy.asarray([[1, 2, 3]])\n    vec2 = numpy.asarray([[1, 2]])\n    with pytest.raises(ValueError):\n        CosineDistance().get_grad(vec1, vec2)\n\n\n@pytest.mark.parametrize(\n    ""name,kwargs,args"",\n    [\n        (""CategoricalCrossentropy.v1"", {}, (scores0, labels0)),\n        (""SequenceCategoricalCrossentropy.v1"", {}, ([scores0], [labels0])),\n        (""L2Distance.v1"", {}, (scores0, scores0)),\n        (\n            ""CosineDistance.v1"",\n            {""normalize"": True, ""ignore_zeros"": True},\n            (scores0, scores0),\n        ),\n    ],\n)\ndef test_loss_from_config(name, kwargs, args):\n    """"""Test that losses are loaded and configured correctly from registry\n    (as partials).""""""\n    cfg = {""test"": {""@losses"": name, **kwargs}}\n    func = registry.make_from_config(cfg)[""test""]\n    loss = func.get_grad(*args)\n    if isinstance(loss, (list, tuple)):\n        loss = loss[0]\n    assert loss.ndim == 2\n    func.get_loss(*args)\n    func(*args)\n'"
thinc/tests/test_optimizers.py,0,"b'import pytest\nfrom thinc.api import registry, Optimizer\nimport numpy\n\n\ndef _test_schedule_valid():\n    while True:\n        yield 0.456\n\n\ndef _test_schedule_invalid():\n    yield from []\n\n\n@pytest.fixture(\n    params=[\n        (lambda: 0.123, 0.123, 0.123, 0.123),\n        (lambda: _test_schedule_valid(), 0.456, 0.456, 0.456),\n        (lambda: (i for i in [0.2, 0.1, 0.4, 0.5, 0.6, 0.7, 0.8]), 0.2, 0.1, 0.4),\n        (lambda: (i for i in [0.333, 0.666]), 0.333, 0.666, 0.666),\n        (lambda: [0.9, 0.8, 0.7], 0.9, 0.8, 0.7),\n        (lambda: [0.0, 0.123], 0.0, 0.123, 0.123),\n    ],\n    scope=""function"",\n)\ndef schedule_valid(request):\n    # Use lambda to prevent iterator from being consumed by first test\n    r_func, r1, r2, r3 = request.param\n    return r_func(), r1, r2, r3\n\n\n@pytest.fixture(\n    params=[\n        (lambda: ""hello""),\n        (lambda: _test_schedule_invalid()),\n        (lambda: (_ for _ in [])),\n        (lambda: []),\n    ],\n    scope=""function"",\n)\ndef schedule_invalid(request):\n    # Use lambda to prevent iterator from being consumed by first test\n    r_func = request.param\n    return r_func()\n\n\n@pytest.mark.parametrize(""name"", [""RAdam.v1"", ""Adam.v1"", ""SGD.v1""])\ndef test_optimizers_from_config(name):\n    learn_rate = 0.123\n    cfg = {""@optimizers"": name, ""learn_rate"": learn_rate}\n    optimizer = registry.make_from_config({""config"": cfg})[""config""]\n    assert optimizer.learn_rate == learn_rate\n\n\ndef test_optimizer_schedules_from_config(schedule_valid):\n    lr, lr_next1, lr_next2, lr_next3 = schedule_valid\n    cfg = {""@optimizers"": ""Adam.v1"", ""learn_rate"": lr}\n    optimizer = registry.make_from_config({""cfg"": cfg})[""cfg""]\n    assert optimizer.learn_rate == lr_next1\n    optimizer.step_schedules()\n    assert optimizer.learn_rate == lr_next2\n    optimizer.step_schedules()\n    assert optimizer.learn_rate == lr_next3\n    optimizer.learn_rate = 1.0\n    assert optimizer.learn_rate == 1.0\n\n\ndef test_optimizer_schedules_valid(schedule_valid):\n    lr, lr_next1, lr_next2, lr_next3 = schedule_valid\n    optimizer = Optimizer(learn_rate=lr)\n    assert optimizer.learn_rate == lr_next1\n    optimizer.step_schedules()\n    assert optimizer.learn_rate == lr_next2\n    optimizer.step_schedules()\n    assert optimizer.learn_rate == lr_next3\n    optimizer.learn_rate = 1.0\n    assert optimizer.learn_rate == 1.0\n\n\ndef test_optimizer_schedules_invalid(schedule_invalid):\n    with pytest.raises(ValueError):\n        Optimizer(learn_rate=schedule_invalid)\n\n\ndef test_optimizer_init():\n    optimizer = Optimizer(\n        learn_rate=0.123,\n        use_averages=False,\n        use_radam=True,\n        L2=0.1,\n        L2_is_weight_decay=False,\n    )\n    _, gradient = optimizer((0, ""x""), numpy.zeros((1, 2)), numpy.zeros(0))\n    assert numpy.array_equal(gradient, numpy.zeros(0))\n    W = numpy.asarray([1.0, 0.0, 0.0, 1.0], dtype=""f"").reshape((4,))\n    dW = numpy.asarray([[-1.0, 0.0, 0.0, 1.0]], dtype=""f"").reshape((4,))\n    optimizer((0, ""x""), W, dW)\n    optimizer = Optimizer(learn_rate=0.123, beta1=0.1, beta2=0.1)\n    optimizer((1, ""x""), W, dW)\n'"
thinc/tests/test_schedules.py,0,"b'from thinc.api import decaying, compounding, slanted_triangular, constant_then\nfrom thinc.api import constant, warmup_linear, cyclic_triangular\n\n\ndef test_decaying_rate():\n    rates = decaying(0.001, 1e-4)\n    rate = next(rates)\n    assert rate == 0.001\n    next_rate = next(rates)\n    assert next_rate < rate\n    assert next_rate > 0\n    assert next_rate > next(rates)\n\n\ndef test_compounding_rate():\n    rates = compounding(1, 16, 1.01)\n    rate0 = next(rates)\n    assert rate0 == 1.0\n    rate1 = next(rates)\n    rate2 = next(rates)\n    rate3 = next(rates)\n    assert rate3 > rate2 > rate1 > rate0\n    assert (rate3 - rate2) > (rate2 - rate1) > (rate1 - rate0)\n\n\ndef test_slanted_triangular_rate():\n    rates = slanted_triangular(1.0, 20.0, ratio=10)\n    rate0 = next(rates)\n    assert rate0 < 1.0\n    rate1 = next(rates)\n    assert rate1 > rate0\n    rate2 = next(rates)\n    assert rate2 < rate1\n    rate3 = next(rates)\n    assert rate0 < rate3 < rate2\n\n\ndef test_constant_then_schedule():\n    rates = constant_then(1.0, 2, [100, 200])\n    assert next(rates) == 1.0\n    assert next(rates) == 1.0\n    assert next(rates) == 100\n    assert next(rates) == 200\n\n\ndef test_constant():\n    rates = constant(123)\n    assert next(rates) == 123\n    assert next(rates) == 123\n\n\ndef test_warmup_linear():\n    rates = warmup_linear(1.0, 2, 10)\n    expected = [0.0, 0.5, 1.0, 0.875, 0.75, 0.625, 0.5, 0.375, 0.25, 0.125, 0.0]\n    for i in range(11):\n        assert next(rates) == expected[i]\n\n\ndef test_cyclic_triangular():\n    rates = cyclic_triangular(0.1, 1.0, 2)\n    expected = [0.55, 1.0, 0.55, 0.1, 0.55, 1.0, 0.55, 0.1, 0.55, 1.0]\n    for i in range(10):\n        assert next(rates) == expected[i]\n'"
thinc/tests/test_serialize.py,0,"b'import pytest\nimport srsly\nfrom thinc.api import with_array, Linear, Maxout, chain, Model, Shim\nfrom thinc.api import serialize_attr, deserialize_attr\n\n\n@pytest.fixture\ndef linear():\n    return Linear(5, 3)\n\n\nclass SerializableAttr:\n    value = ""foo""\n\n    def to_bytes(self):\n        return self.value.encode(""utf8"")\n\n    def from_bytes(self, data):\n        self.value = f""{data.decode(\'utf8\')} from bytes""\n        return self\n\n\nclass SerializableShim(Shim):\n    name = ""testshim""\n    value = ""shimdata""\n\n    def to_bytes(self):\n        return self.value.encode(""utf8"")\n\n    def from_bytes(self, data):\n        self.value = f""{data.decode(\'utf8\')} from bytes""\n        return self\n\n\ndef test_pickle_with_flatten(linear):\n    Xs = [linear.ops.alloc2f(2, 3), linear.ops.alloc2f(4, 3)]\n    model = with_array(linear).initialize()\n    pickled = srsly.pickle_dumps(model)\n    loaded = srsly.pickle_loads(pickled)\n    Ys = loaded.predict(Xs)\n    assert len(Ys) == 2\n    assert Ys[0].shape == (Xs[0].shape[0], linear.get_dim(""nO""))\n    assert Ys[1].shape == (Xs[1].shape[0], linear.get_dim(""nO""))\n\n\ndef test_simple_model_roundtrip_bytes():\n    model = Maxout(5, 10, nP=2).initialize()\n    b = model.get_param(""b"")\n    b += 1\n    data = model.to_bytes()\n    b = model.get_param(""b"")\n    b -= 1\n    model = model.from_bytes(data)\n    assert model.get_param(""b"")[0, 0] == 1\n\n\ndef test_simple_model_roundtrip_bytes_length():\n    """""" Ensure that serialization of non-initialized weight matrices goes fine """"""\n    model1 = Maxout(5, 10, nP=2)\n    model2 = Maxout(5, 10, nP=2)\n\n    data1 = model1.to_bytes()\n    model2 = model2.from_bytes(data1)\n    data2 = model2.to_bytes()\n\n    assert data1 == data2\n    assert len(data1) == len(data2)\n\n\ndef test_simple_model_roundtrip_bytes_serializable_attrs():\n    fwd = lambda model, X, is_train: (X, lambda dY: dY)\n    attr = SerializableAttr()\n    assert attr.value == ""foo""\n    assert attr.to_bytes() == b""foo""\n    model = Model(""test"", fwd, attrs={""test"": attr})\n    model.initialize()\n\n    @serialize_attr.register(SerializableAttr)\n    def serialize_attr_custom(_, value, name, model):\n        return value.to_bytes()\n\n    @deserialize_attr.register(SerializableAttr)\n    def deserialize_attr_custom(_, value, name, model):\n        return SerializableAttr().from_bytes(value)\n\n    model_bytes = model.to_bytes()\n    model = model.from_bytes(model_bytes)\n    assert ""test"" in model.attrs\n    assert model.attrs[""test""].value == ""foo from bytes""\n\n\ndef test_multi_model_roundtrip_bytes():\n    model = chain(Maxout(5, 10, nP=2), Maxout(2, 3)).initialize()\n    b = model.layers[0].get_param(""b"")\n    b += 1\n    b = model.layers[1].get_param(""b"")\n    b += 2\n    data = model.to_bytes()\n    b = model.layers[0].get_param(""b"")\n    b -= 1\n    b = model.layers[1].get_param(""b"")\n    b -= 2\n    model = model.from_bytes(data)\n    assert model.layers[0].get_param(""b"")[0, 0] == 1\n    assert model.layers[1].get_param(""b"")[0, 0] == 2\n\n\ndef test_multi_model_load_missing_dims():\n    model = chain(Maxout(5, 10, nP=2), Maxout(2, 3)).initialize()\n    b = model.layers[0].get_param(""b"")\n    b += 1\n    b = model.layers[1].get_param(""b"")\n    b += 2\n    data = model.to_bytes()\n\n    model2 = chain(Maxout(5, nP=None), Maxout(nP=None))\n    model2 = model2.from_bytes(data)\n    assert model2.layers[0].get_param(""b"")[0, 0] == 1\n    assert model2.layers[1].get_param(""b"")[0, 0] == 2\n\n\ndef test_serialize_model_shims_roundtrip_bytes():\n    fwd = lambda model, X, is_train: (X, lambda dY: dY)\n    test_shim = SerializableShim(None)\n    shim_model = Model(""shimmodel"", fwd, shims=[test_shim])\n    model = chain(Linear(2, 3), shim_model, Maxout(2, 3))\n    model.initialize()\n    assert model.layers[1].shims[0].value == ""shimdata""\n    model_bytes = model.to_bytes()\n    with pytest.raises(ValueError):\n        Linear(2, 3).from_bytes(model_bytes)\n    test_shim = SerializableShim(None)\n    shim_model = Model(""shimmodel"", fwd, shims=[test_shim])\n    new_model = chain(Linear(2, 3), shim_model, Maxout(2, 3)).from_bytes(model_bytes)\n    assert new_model.layers[1].shims[0].value == ""shimdata from bytes""\n\n\ndef test_serialize_refs_roundtrip_bytes():\n    fwd = lambda model, X, is_train: (X, lambda dY: dY)\n    model_a = Model(""a"", fwd)\n    model = Model(""test"", fwd, refs={""a"": model_a, ""b"": None}).initialize()\n    with pytest.raises(ValueError):  # ref not in nodes\n        model.to_bytes()\n    model = Model(""test"", fwd, refs={""a"": model_a, ""b"": None}, layers=[model_a])\n    assert model.ref_names == (""a"", ""b"")\n    model_bytes = model.to_bytes()\n    with pytest.raises(ValueError):\n        Model(""test"", fwd).from_bytes(model_bytes)\n    new_model = Model(""test"", fwd, layers=[model_a])\n    new_model.from_bytes(model_bytes)\n    assert new_model.ref_names == (""a"", ""b"")\n\n\ndef test_serialize_attrs():\n    fwd = lambda model, X, is_train: (X, lambda dY: dY)\n    attrs = {""test"": ""foo""}\n    model1 = Model(""test"", fwd, attrs=attrs).initialize()\n    bytes_attr = serialize_attr(model1.attrs[""test""], attrs[""test""], ""test"", model1)\n    assert bytes_attr == srsly.msgpack_dumps(""foo"")\n    model2 = Model(""test"", fwd, attrs={""test"": """"})\n    result = deserialize_attr(model2.attrs[""test""], bytes_attr, ""test"", model2)\n    assert result == ""foo""\n\n    # Test objects with custom serialization functions\n    @serialize_attr.register(SerializableAttr)\n    def serialize_attr_custom(_, value, name, model):\n        return value.to_bytes()\n\n    @deserialize_attr.register(SerializableAttr)\n    def deserialize_attr_custom(_, value, name, model):\n        return SerializableAttr().from_bytes(value)\n\n    attrs = {""test"": SerializableAttr()}\n    model3 = Model(""test"", fwd, attrs=attrs)\n    bytes_attr = serialize_attr(model3.attrs[""test""], attrs[""test""], ""test"", model3)\n    assert bytes_attr == b""foo""\n    model4 = Model(""test"", fwd, attrs=attrs)\n    assert model4.attrs[""test""].value == ""foo""\n    result = deserialize_attr(model4.attrs[""test""], bytes_attr, ""test"", model4)\n    assert result.value == ""foo from bytes""\n'"
thinc/tests/test_types.py,0,"b'import numpy\nfrom pydantic import create_model, ValidationError\nfrom thinc.types import Floats1d, Floats2d, Floats3d, Floats4d\nfrom thinc.types import Ints1d, Ints2d, Ints3d, Ints4d\nimport pytest\n\n\n@pytest.mark.parametrize(\n    ""arr,arr_type"",\n    [\n        (numpy.zeros(0, dtype=numpy.float32), Floats1d),\n        (numpy.zeros((0, 0), dtype=numpy.float32), Floats2d),\n        (numpy.zeros((0, 0, 0), dtype=numpy.float32), Floats3d),\n        (numpy.zeros((0, 0, 0, 0), dtype=numpy.float32), Floats4d),\n        (numpy.zeros(0, dtype=numpy.int32), Ints1d),\n        (numpy.zeros((0, 0), dtype=numpy.int32), Ints2d),\n        (numpy.zeros((0, 0, 0), dtype=numpy.int32), Ints3d),\n        (numpy.zeros((0, 0, 0, 0), dtype=numpy.int32), Ints4d),\n    ],\n)\ndef test_array_validation_valid(arr, arr_type):\n    test_model = create_model(""TestModel"", arr=(arr_type, ...))\n    result = test_model(arr=arr)\n    assert numpy.array_equal(arr, result.arr)\n\n\n@pytest.mark.parametrize(\n    ""arr,arr_type"",\n    [\n        (numpy.zeros((0, 0), dtype=numpy.float32), Floats1d),\n        (numpy.zeros((0, 0), dtype=numpy.float32), Ints2d),\n    ],\n)\ndef test_array_validation_invalid(arr, arr_type):\n    test_model = create_model(""TestModel"", arr=(arr_type, ...))\n    with pytest.raises(ValidationError):\n        test_model(arr=arr)\n'"
thinc/tests/test_util.py,0,"b'import pytest\nimport numpy\nfrom thinc.api import get_width, Ragged, Padded\nfrom thinc.util import get_array_module, is_numpy_array, to_categorical\nfrom thinc.util import convert_recursive\nfrom thinc.types import ArgsKwargs\n\n\n@pytest.mark.parametrize(\n    ""obj,width"",\n    [\n        (numpy.zeros((1, 2, 3, 4)), 4),\n        (numpy.array(1), 0),\n        (numpy.array([1, 2]), 3),\n        ([numpy.zeros((1, 2)), numpy.zeros((1))], 2),\n        (Ragged(numpy.zeros((1, 2)), numpy.zeros(1)), 2),\n        (\n            Padded(\n                numpy.zeros((2, 1, 2)),\n                numpy.zeros(2),\n                numpy.array([1, 0]),\n                numpy.array([0, 1]),\n            ),\n            2,\n        ),\n        ([], 0),\n    ],\n)\ndef test_get_width(obj, width):\n    assert get_width(obj) == width\n\n\n@pytest.mark.parametrize(""obj"", [1234, ""foo"", {""a"": numpy.array(0)}])\ndef test_get_width_fail(obj):\n    with pytest.raises(ValueError):\n        get_width(obj)\n\n\ndef test_array_module_cpu_gpu_helpers():\n    xp = get_array_module(0)\n    assert hasattr(xp, ""ndarray"")\n    assert is_numpy_array(numpy.zeros((1, 2)))\n    assert not is_numpy_array((1, 2))\n\n\ndef test_to_categorical():\n    # Test without n_classes\n    one_hot = to_categorical(numpy.asarray([1, 2], dtype=""i""))\n    assert one_hot.shape == (2, 3)\n    # From keras\n    # https://github.com/keras-team/keras/blob/master/tests/keras/utils/np_utils_test.py\n    nc = 5\n    shapes = [(1,), (3,), (4, 3), (5, 4, 3), (3, 1), (3, 2, 1)]\n    expected_shapes = [\n        (1, nc),\n        (3, nc),\n        (4, 3, nc),\n        (5, 4, 3, nc),\n        (3, 1, nc),\n        (3, 2, 1, nc),\n    ]\n    labels = [numpy.random.randint(0, nc, shape) for shape in shapes]\n    one_hots = [to_categorical(label, nc) for label in labels]\n    for label, one_hot, expected_shape in zip(labels, one_hots, expected_shapes):\n        assert one_hot.shape == expected_shape\n        assert numpy.array_equal(one_hot, one_hot.astype(bool))\n        assert numpy.all(one_hot.sum(axis=-1) == 1)\n        assert numpy.all(numpy.argmax(one_hot, -1).reshape(label.shape) == label)\n\n\ndef test_convert_recursive():\n    is_match = lambda obj: obj == ""foo""\n    convert_item = lambda obj: obj.upper()\n    obj = {\n        ""a"": {(""b"", ""foo""): {""c"": ""foo"", ""d"": [""foo"", {""e"": ""foo"", ""f"": (1, ""foo"")}]}}\n    }\n    result = convert_recursive(is_match, convert_item, obj)\n    assert result[""a""][(""b"", ""FOO"")][""c""] == ""FOO""\n    assert result[""a""][(""b"", ""FOO"")][""d""] == [""FOO"", {""e"": ""FOO"", ""f"": (1, ""FOO"")}]\n    obj = {""a"": ArgsKwargs((""foo"", [{""b"": ""foo""}]), {""a"": [""x"", ""foo""]})}\n    result = convert_recursive(is_match, convert_item, obj)\n    assert result[""a""].args == (""FOO"", [{""b"": ""FOO""}])\n    assert result[""a""].kwargs == {""a"": [""x"", ""FOO""]}\n'"
thinc/tests/util.py,0,"b'import contextlib\nfrom pathlib import Path\nimport tempfile\nimport shutil\nfrom thinc.api import Linear, Ragged, Padded, ArgsKwargs\nimport numpy\nimport pytest\n\n\n@contextlib.contextmanager\ndef make_tempdir():\n    d = Path(tempfile.mkdtemp())\n    yield d\n    shutil.rmtree(str(d))\n\n\ndef get_model(W_b_input, cls=Linear):\n    W, b, input_ = W_b_input\n    nr_out, nr_in = W.shape\n    model = cls(nr_out, nr_in)\n    model.set_param(""W"", W)\n    model.set_param(""b"", b)\n    model.initialize()\n    return model\n\n\ndef get_shape(W_b_input):\n    W, b, input_ = W_b_input\n    return input_.shape[0], W.shape[0], W.shape[1]\n\n\ndef get_data_checker(inputs):\n    if isinstance(inputs, Ragged):\n        return assert_raggeds_match\n    elif isinstance(inputs, Padded):\n        return assert_paddeds_match\n    elif isinstance(inputs, list):\n        return assert_lists_match\n    elif isinstance(inputs, tuple) and len(inputs) == 4:\n        return assert_padded_data_match\n    elif isinstance(inputs, tuple) and len(inputs) == 2:\n        return assert_ragged_data_match\n    else:\n        return assert_arrays_match\n\n\ndef assert_arrays_match(X, Y):\n    assert X.dtype == Y.dtype\n    # Transformations are allowed to change last dimension, but not batch size.\n    assert X.shape[0] == Y.shape[0]\n    return True\n\n\ndef assert_lists_match(X, Y):\n    assert isinstance(X, list)\n    assert isinstance(Y, list)\n    assert len(X) == len(Y)\n    for x, y in zip(X, Y):\n        assert_arrays_match(x, y)\n    return True\n\n\ndef assert_raggeds_match(X, Y):\n    assert isinstance(X, Ragged)\n    assert isinstance(Y, Ragged)\n    assert_arrays_match(X.lengths, Y.lengths)\n    assert_arrays_match(X.data, Y.data)\n    return True\n\n\ndef assert_paddeds_match(X, Y):\n    assert isinstance(X, Padded)\n    assert isinstance(Y, Padded)\n    assert_arrays_match(X.size_at_t, Y.size_at_t)\n    assert assert_arrays_match(X.lengths, Y.lengths)\n    assert assert_arrays_match(X.indices, Y.indices)\n    assert X.data.dtype == Y.data.dtype\n    assert X.data.shape[1] == Y.data.shape[1]\n    assert X.data.shape[0] == Y.data.shape[0]\n    return True\n\n\ndef assert_padded_data_match(X, Y):\n    return assert_paddeds_match(Padded(*X), Padded(*Y))\n\n\ndef assert_ragged_data_match(X, Y):\n    return assert_raggeds_match(Ragged(*X), Ragged(*Y))\n\n\ndef check_input_converters(Y, backprop, data, n_args, kwargs_keys, type_):\n    assert isinstance(Y, ArgsKwargs)\n    assert len(Y.args) == n_args\n    assert list(Y.kwargs.keys()) == kwargs_keys\n    assert all(isinstance(arg, type_) for arg in Y.args)\n    assert all(isinstance(arg, type_) for arg in Y.kwargs.values())\n    dX = backprop(Y)\n    input_type = type(data) if not isinstance(data, list) else tuple\n    assert isinstance(dX, input_type)\n    if isinstance(data, dict):\n        assert list(dX.keys()) == kwargs_keys\n        assert all(isinstance(arr, numpy.ndarray) for arr in dX.values())\n    elif isinstance(data, (list, tuple)):\n        assert isinstance(dX, tuple)\n        assert all(isinstance(arr, numpy.ndarray) for arr in dX)\n    elif isinstance(data, ArgsKwargs):\n        assert len(dX.args) == n_args\n        assert list(dX.kwargs.keys()) == kwargs_keys\n        assert all(isinstance(arg, numpy.ndarray) for arg in dX.args)\n        assert all(isinstance(arg, numpy.ndarray) for arg in dX.kwargs.values())\n    elif not isinstance(data, numpy.ndarray):\n        pytest.fail(f""Bad data type: {dX}"")\n'"
thinc/extra/tests/__init__.py,0,b''
thinc/tests/backends/__init__.py,0,b''
thinc/tests/backends/test_lstm.py,0,"b'from thinc.backends.jax_ops import lstm_weights_forward, backprop_lstm_weights\nfrom thinc.backends.jax_ops import lstm_gates_forward, backprop_lstm_gates\nimport numpy.testing\nimport pytest\n\nfrom hypothesis import given, settings\nfrom ..strategies import ndarrays_of_shape\n\ntry:\n    import jax\n\n    has_jax = True\nexcept ImportError:\n    has_jax = False\n\nMAX_EXAMPLES = 20\n\nnL = 6\nnB = 3\nnO = 4\nnI = 2\nt = 3\n\n\ndef assert_arrays_equal(arrays1, arrays2):\n    assert len(arrays1) == len(arrays2)\n    shapes1 = [tuple(a.shape) for a in arrays1]\n    shapes2 = [tuple(a.shape) for a in arrays2]\n    assert shapes1 == shapes2\n    for arr1, arr2 in zip(arrays1, arrays2):\n        assert arr1.shape == arr2.shape\n        numpy.testing.assert_allclose(arr1, arr2, rtol=0.001, atol=0.001)\n\n\n# See thinc/backends/jax_ops for notation\n\n\n@pytest.mark.skipif(not has_jax, reason=""needs Jax"")\n@pytest.mark.filterwarnings(""ignore"")\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(\n    Xt3=ndarrays_of_shape((nB, nI), dtype=""f""),\n    Yt2=ndarrays_of_shape((nB, nO), dtype=""f""),\n    dAt3=ndarrays_of_shape((nB, nO * 4), dtype=""f""),\n    W=ndarrays_of_shape((nO * 4, nO + nI), dtype=""f""),\n    b=ndarrays_of_shape((nO * 4,), dtype=""f""),\n)\ndef test_lstm_weights_gradients(Xt3, Yt2, W, b, dAt3):\n    At3, jax_backprop = jax.vjp(lstm_weights_forward, Xt3, Yt2, W, b)\n    jax_grads = jax_backprop(dAt3)\n    St3 = jax.numpy.hstack((Xt3, Yt2))\n    our_grads = backprop_lstm_weights(dAt3, (St3, W, b))\n    assert_arrays_equal(our_grads, jax_grads)\n\n\n@pytest.mark.skipif(not has_jax, reason=""needs Jax"")\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(\n    At3=ndarrays_of_shape((nB, nO * 4), dtype=""f""),\n    Ct2=ndarrays_of_shape((nB, nO), dtype=""f""),\n    dYt3=ndarrays_of_shape((nB, nO), dtype=""f""),\n    dCt3=ndarrays_of_shape((nB, nO), dtype=""f""),\n)\ndef test_lstm_gates_gradients(At3, Ct2, dYt3, dCt3):\n    # At3 = (At3 * 0) + 1\n    # Ct2 = (Ct2 * 0) + 1\n    # dYt3 = (dYt3 * 0) + 1\n    # dCt3 = (dCt3 * 0) + 1\n    (Yt3, Ct3, Gt3), get_jax_grads = jax.vjp(lstm_gates_forward, At3, Ct2)\n    jax_grads = get_jax_grads((dYt3, dCt3, Gt3 * 0))\n    Yt3, Ct3, Gt3 = lstm_gates_forward(At3, Ct2)\n    our_grads = backprop_lstm_gates(dYt3, dCt3, Gt3, Ct3, Ct2)\n    assert_arrays_equal(our_grads, jax_grads)\n'"
thinc/tests/backends/test_mem.py,0,"b'from thinc.backends._param_server import ParamServer\nimport numpy\n\n\ndef test_param_server_init():\n    array = numpy.zeros((5,), dtype=""f"")\n    params = {(""a"", 1): array, (""b"", 2): array}\n    grads = {(""a"", 1): array, (""c"", 3): array}\n    ps = ParamServer(params, grads)\n    assert ps.param_keys == ((""a"", 1), (""b"", 2))\n    assert ps.grad_keys == ((""a"", 1),)\n'"
thinc/tests/backends/test_ops.py,0,"b'import pytest\nimport numpy\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_allclose\nfrom thinc.api import NumpyOps, CupyOps, Ops, get_ops\nfrom thinc.api import JaxOps, has_jax, get_current_ops, use_ops\nfrom thinc.api import fix_random_seed\nimport inspect\n\nfrom .. import strategies\n\n\nMAX_EXAMPLES = 10\n\nVANILLA_OPS = Ops(numpy)\nNUMPY_OPS = NumpyOps()\nBLIS_OPS = NumpyOps(use_blis=True)\nCPU_OPS = [NUMPY_OPS, VANILLA_OPS]\nif has_jax:\n    CPU_OPS.append(JaxOps())\nXP_OPS = [NUMPY_OPS]\nif CupyOps.xp is not None:\n    XP_OPS.append(CupyOps())\nALL_OPS = XP_OPS + [VANILLA_OPS]\n\n\n@pytest.mark.parametrize(""op"", [NumpyOps, CupyOps, JaxOps])\ndef test_ops_consistency(op):\n    """"""Test that specific ops don\'t define any methods that are not on the\n    Ops base class and that all ops methods define the exact same arguments.""""""\n    attrs = [m for m in dir(op) if not m.startswith(""_"")]\n    for attr in attrs:\n        assert hasattr(Ops, attr)\n        method = getattr(op, attr)\n        if hasattr(method, ""__call__""):\n            sig = inspect.signature(method)\n            params = [p for p in sig.parameters][1:]\n            base_sig = inspect.signature(getattr(Ops, attr))\n            base_params = [p for p in base_sig.parameters][1:]\n            assert params == base_params, attr\n            defaults = [p.default for p in sig.parameters.values()][1:]\n            base_defaults = [p.default for p in base_sig.parameters.values()][1:]\n            assert defaults == base_defaults, attr\n            # If args are type annotated, their types should be the same\n            annots = [p.annotation for p in sig.parameters.values()][1:]\n            base_annots = [p.annotation for p in base_sig.parameters.values()][1:]\n            for i, (p1, p2) in enumerate(zip(annots, base_annots)):\n                if p1 != inspect.Parameter.empty and p2 != inspect.Parameter.empty:\n                    # Need to check string value to handle TypeVars etc.\n                    assert str(p1) == str(p2), attr\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\ndef test_alloc(ops):\n    float_methods = (ops.alloc1f, ops.alloc2f, ops.alloc3f, ops.alloc4f)\n    for i, method in enumerate(float_methods):\n        shape = (1,) * (i + 1)\n        arr = method(*shape)\n        assert arr.dtype == numpy.float32\n        assert arr.ndim == len(shape)\n        arr = ops.alloc_f(shape)\n        assert arr.dtype == numpy.float32\n        assert arr.ndim == len(shape)\n    int_methods = (ops.alloc1i, ops.alloc2i, ops.alloc3i, ops.alloc4i)\n    for i, method in enumerate(int_methods):\n        shape = (1,) * (i + 1)\n        arr = method(*shape)\n        assert arr.dtype == numpy.int32\n        assert arr.ndim == len(shape)\n        arr = ops.alloc_i(shape)\n        assert arr.dtype == numpy.int32\n        assert arr.ndim == len(shape)\n    assert ops.alloc(1).ndim == 1\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\ndef test_hash_gives_distinct_keys(ops):\n    ids = ops.alloc1f(5, dtype=""uint64"")\n    keys = ops.hash(ids, 0)\n    assert keys.shape == (5, 4)\n    assert keys.dtype == ""uint32""\n    for i in range(len(ids)):\n        for j in range(keys.shape[1]):\n            assert keys[i, j] != 0\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\ndef test_get_dropout_empty(ops):\n    shape = (2, 2)\n    drop = 0.0\n    mask = ops.get_dropout_mask(shape, drop)\n    if drop <= 0.0:\n        assert mask[mask == 1.0].all()\n    else:\n        assert mask[mask != 1.0].all()\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\ndef test_get_dropout_not_empty(ops):\n    shape = (200, 200)\n    drop = 0.5\n    mask = ops.get_dropout_mask(shape, drop)\n    assert (mask > 1.0).any()\n    assert (mask == 0.0).any()\n    assert mask.shape == shape\n\n\n@pytest.mark.parametrize(""ops"", CPU_OPS)\ndef test_seq2col_window_one_small(ops):\n    seq = ops.asarray([[1.0], [3.0], [4.0], [5]], dtype=""float32"")\n    cols = ops.seq2col(seq, 1)\n    if hasattr(cols, ""get""):\n        cols = cols.get()\n    assert_allclose(cols[0], [0.0, 1.0, 3.0])\n    assert_allclose(cols[1], [1.0, 3.0, 4.0])\n    assert_allclose(cols[2], [3.0, 4.0, 5.0])\n    assert_allclose(cols[3], [4.0, 5.0, 0.0])\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BOP())\ndef test_maxout(ops, X):\n    X = ops.asarray(X)\n    expected_best = X.max(axis=-1)\n    predicted_best, which = ops.maxout(X)\n    ops.xp.testing.assert_allclose(\n        expected_best, predicted_best, rtol=0.001, atol=0.001\n    )\n    # Can\'t compare \'which\' directly, as sort order might be different\n    # We could check that using the \'which\', we get the right results?\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_seq2col_window_one(ops, X):\n    X = ops.asarray(X)\n    base_ops = Ops()\n    base_ops.xp = ops.xp\n    baseX = base_ops.alloc(X.shape) + X\n    target = base_ops.seq2col(base_ops.asarray(baseX), nW=1)\n    predicted = ops.seq2col(X, nW=1)\n    ops.xp.testing.assert_allclose(target, predicted, atol=0.001, rtol=0.001)\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\ndef test_backprop_seq2col_window_one_small(ops):\n    cols = ops.asarray(\n        [[0.0, 0.0, 0.0], [-1.0, 0.0, 1.0], [2.0, 0.0, 0.0]], dtype=""float32""\n    )\n    expected = [[-1.0], [2.0], [1.0]]\n    seq = ops.backprop_seq2col(cols, 1)\n    if not isinstance(seq, numpy.ndarray):\n        seq = seq.get()\n    assert_allclose(seq, expected, atol=0.001, rtol=0.001)\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_backprop_seq2col_window_one(ops, X):\n    if X.shape[1] % 3:\n        return None\n    X = ops.asarray(X)\n    if ops.xp.abs(X).max() >= 30:\n        return None\n    base_ops = Ops()\n    base_ops.xp = ops.xp\n    target = base_ops.backprop_seq2col(X, nW=1)\n    predicted = ops.backprop_seq2col(X, nW=1)\n    for row in range(target.shape[0]):\n        diff = target[row].sum() - predicted[row].sum()\n        if diff < -0.1 or diff > 0.1:\n            print(row, diff)\n            print(target[row])\n            print(predicted[row])\n    ops.xp.testing.assert_allclose(target, predicted, atol=0.001, rtol=0.001)\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\ndef test_seq2col_window_two(ops):\n    seq = ops.asarray([[1.0], [2.0], [3.0], [4]], dtype=""float32"")\n    cols = ops.seq2col(seq, 2)\n    if not isinstance(cols, numpy.ndarray):\n        cols = cols.get()\n    assert_allclose(cols[0], [0.0, 0.0, 1.0, 2.0, 3.0])\n    assert_allclose(cols[1], [0.0, 1.0, 2.0, 3.0, 4.0])\n    assert_allclose(cols[2], [1.0, 2.0, 3.0, 4.0, 0.0])\n    assert_allclose(cols[3], [2.0, 3.0, 4.0, 0.0, 0.0])\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\ndef test_backprop_seq2col_window_two(ops):\n    cols = ops.asarray(\n        [\n            [0.0, 0.0, 1.0, 2.0, 3.0],\n            [0.0, 1.0, 2.0, 3.0, 4.0],\n            [1.0, 2.0, 3.0, 4.0, 0.0],\n            [2.0, 3.0, 4.0, 0.0, 0.0],\n        ],\n        dtype=""float32"",\n    )\n    # We\'re summing the values that each row\n    # was used as a feature. So row 0 had a\n    # gradient of 1 in row 0, 1 in row 2, and\n    # 1 in row 3.\n    expected = ops.asarray(\n        [\n            [1 + 1 + 1.0 + 0.0],\n            [2.0 + 2.0 + 2.0 + 2.0],\n            [3.0 + 3.0 + 3.0 + 3.0],\n            [0.0 + 4.0 + 4.0 + 4.0],\n        ],\n        dtype=""f"",\n    )\n    seq = ops.backprop_seq2col(cols, 2)\n    ops.xp.testing.assert_allclose(seq, expected, atol=0.001, rtol=0.001)\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_backprop_reduce_sum(ops, X):\n    X = ops.asarray(X)\n    if ops.xp.abs(X).max() >= 5:\n        return None\n    lengths = ops.asarray([3] * len(X), dtype=""i"")\n    out = ops.backprop_reduce_sum(X, lengths)\n    assert out.shape == (sum(lengths), X.shape[1])\n    start = 0\n    for i, length in enumerate(lengths):\n        ops.xp.testing.assert_allclose(\n            out[start : start + length].sum(axis=0), X[i] * length, rtol=0.01, atol=0.01\n        )\n        start += length\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_softmax_sums_to_one(ops, X):\n    y = ops.softmax(ops.asarray(X))\n    for row in y:\n        assert 0.99999 <= row.sum() <= 1.0001\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_softmax_works_inplace(ops, X):\n    X = ops.asarray(X)\n    X = ops.softmax(X, inplace=True)\n    for row in X:\n        assert 0.99999 <= row.sum() <= 1.00001\n\n\n@pytest.mark.parametrize(""cpu_ops"", [*CPU_OPS, BLIS_OPS])\ndef test_gemm_computes_correctly(cpu_ops):\n    W = numpy.zeros((3, 2), dtype=""f"")\n    X = numpy.zeros((4, 2), dtype=""f"")\n    W += numpy.random.uniform(size=W.size).reshape(W.shape)\n    X += numpy.random.uniform(size=X.size).reshape(X.shape)\n    Y = cpu_ops.gemm(X, W, trans2=True)\n    expected = numpy.dot(X, W.T)\n    assert_allclose(expected, Y, atol=1e-4, rtol=1e-4)\n    W = numpy.zeros((2, 3), dtype=""f"")\n    X = numpy.zeros((2, 4), dtype=""f"")\n    W += numpy.random.uniform(size=W.size).reshape(W.shape)\n    X += numpy.random.uniform(size=X.size).reshape(X.shape)\n    Y = cpu_ops.gemm(X, W, trans1=True)\n    expected = numpy.dot(X.T, W)\n    assert_allclose(expected, Y, atol=1e-4, rtol=1e-4)\n    cpu_ops.gemm(X, W, trans1=True, out=Y)\n\n\n@pytest.mark.parametrize(""cpu_ops"", CPU_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_flatten_unflatten_roundtrip(cpu_ops, X):\n    flat = cpu_ops.flatten([x for x in X])\n    assert flat.ndim == 1\n    unflat = cpu_ops.unflatten(flat, [len(x) for x in X])\n    assert_allclose(X, unflat)\n    flat2 = cpu_ops.flatten([x for x in X], pad=1, dtype=""f"")\n    assert len(flat2) > len(flat)\n    unflat2 = cpu_ops.unflatten(flat2, [len(x) for x in X], pad=1)\n    assert_allclose(X, unflat2)\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\ndef test_reduce_sum(ops):\n    m = ops.xp.zeros((19, 5), dtype=""f"")\n    m += 1\n    lengths = ops.xp.array([5, 5, 3, 6], dtype=""i"")\n    output = ops.reduce_sum(m, lengths)\n    assert output.sum() == m.sum(), (output.sum(), m.sum())\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\ndef test_reduce_max_sm(ops):\n    X = ops.xp.zeros((6, 3), dtype=""f"")\n    X += ops.xp.random.uniform(-1, 1, X.shape)\n    lengths = ops.xp.array([2, 2, 2], dtype=""i"")\n    maxes, which = ops.reduce_max(X, lengths)\n    start = 0\n    for i, length in enumerate(lengths):\n        truth = X[start : start + length].max(axis=0)\n        ops.xp.testing.assert_allclose(maxes[i], truth)\n        start += length\n\n\n@pytest.mark.parametrize(""ops"", XP_OPS)\ndef test_reduce_max(ops):\n    m = ops.xp.zeros((19, 5), dtype=""f"")\n    m += ops.xp.random.uniform(-1, 1, m.shape)\n    lengths = ops.xp.array([5, 5, 3, 6], dtype=""i"")\n    # m[4, 0] = 1\n    # m[0, 1] = 2\n    # m[1, 3] = 3\n    maxes, which = ops.reduce_max(m, lengths)\n    start = 0\n    for i, length in enumerate(lengths):\n        truth = m[start : start + length].max(axis=0)\n        ops.xp.testing.assert_allclose(maxes[i], truth)\n        start += length\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_mish(ops, X):\n    X = ops.asarray(X)\n    Y = ops.mish(X)\n    assert Y.shape == X.shape\n    assert not ops.xp.isnan(Y).any()\n\n\n@pytest.mark.parametrize(""ops"", ALL_OPS)\n@settings(max_examples=MAX_EXAMPLES, deadline=None)\n@given(X=strategies.arrays_BI())\ndef test_backprop_mish(ops, X):\n    X = ops.asarray(X)\n    # Test zero gradients result in 0 dX\n    zeros = ops.alloc(X.shape)\n    dX = ops.backprop_mish(zeros, X)\n    assert dX.shape == X.shape\n    assert (dX == 0).all()\n\n\ndef test_get_ops():\n    assert isinstance(get_ops(""numpy""), NumpyOps)\n    assert isinstance(get_ops(""cupy""), CupyOps)\n    assert isinstance(get_ops(""jax""), JaxOps)\n    with pytest.raises(ValueError):\n        get_ops(""blah"")\n    ops = Ops(numpy)\n    assert ops.xp == numpy\n\n\ndef test_use_ops():\n    class_ops = get_current_ops()\n    assert class_ops.name == ""numpy""\n    with use_ops(""numpy""):\n        new_ops = get_current_ops()\n        assert new_ops.name == ""numpy""\n    with use_ops(""cupy""):\n        new_ops = get_current_ops()\n        assert new_ops.name == ""cupy""\n    with use_ops(""jax""):\n        new_ops = get_current_ops()\n        assert new_ops.name == ""jax""\n    new_ops = get_current_ops()\n    assert new_ops.name == ""numpy""\n\n\ndef test_minibatch():\n    fix_random_seed(0)\n    ops = get_current_ops()\n    items = [1, 2, 3, 4, 5, 6]\n    batches = ops.minibatch(3, items)\n    assert list(batches) == [[1, 2, 3], [4, 5, 6]]\n    batches = ops.minibatch((i for i in (3, 2, 1)), items)\n    assert list(batches) == [[1, 2, 3], [4, 5], [6]]\n    batches = list(ops.minibatch(3, numpy.asarray(items)))\n    assert isinstance(batches[0], numpy.ndarray)\n    assert numpy.array_equal(batches[0], numpy.asarray([1, 2, 3]))\n    assert numpy.array_equal(batches[1], numpy.asarray([4, 5, 6]))\n    batches = list(ops.minibatch((i for i in (3, 2, 1)), items, shuffle=True))\n    assert batches != [[1, 2, 3], [4, 5], [6]]\n    assert len(batches[0]) == 3\n    assert len(batches[1]) == 2\n    assert len(batches[2]) == 1\n    with pytest.raises(ValueError):\n        ops.minibatch(10, (i for i in range(100)))\n    with pytest.raises(ValueError):\n        ops.minibatch(10, True)\n\n\ndef test_multibatch():\n    fix_random_seed(0)\n    ops = get_current_ops()\n    arr1 = numpy.asarray([1, 2, 3, 4])\n    arr2 = numpy.asarray([5, 6, 7, 8])\n    batches = list(ops.multibatch(2, arr1, arr2))\n    assert numpy.concatenate(batches).tolist() == [[1, 2], [5, 6], [3, 4], [7, 8]]\n    batches = list(ops.multibatch(2, arr1, arr2, shuffle=True))\n    assert len(batches) == 2\n    assert len(batches[0]) == 2\n    assert len(batches[1]) == 2\n    batches = list(ops.multibatch(2, [1, 2, 3, 4], [5, 6, 7, 8]))\n    assert batches == [[[1, 2], [5, 6]], [[3, 4], [7, 8]]]\n    with pytest.raises(ValueError):\n        ops.multibatch(10, (i for i in range(100)), (i for i in range(100)))\n    with pytest.raises(ValueError):\n        ops.multibatch(10, arr1, (i for i in range(100)), arr2)\n'"
thinc/tests/extra/__init__.py,0,b''
thinc/tests/extra/test_beam_search.py,0,b'from thinc.extra.search import MaxViolation\n\n\ndef test_init_violn():\n    MaxViolation()\n'
thinc/tests/layers/__init__.py,0,b''
thinc/tests/layers/test_basic_tagger.py,0,"b'import pytest\nimport random\nfrom thinc.api import Model, Relu, Softmax, HashEmbed, expand_window\nfrom thinc.api import chain, with_array, Adam, strings2arrays\nimport ml_datasets\n\n\n@pytest.fixture(scope=""module"")\ndef ancora():\n    return ml_datasets.ud_ancora_pos_tags()\n\n\ndef create_embed_relu_relu_softmax(depth, width, vector_length):\n    with Model.define_operators({"">>"": chain}):\n        model = strings2arrays() >> with_array(\n            HashEmbed(width, vector_length)\n            >> expand_window(window_size=1)\n            >> Relu(width, width * 3)\n            >> Relu(width, width)\n            >> Softmax(17, width)\n        )\n    return model\n\n\n@pytest.fixture(params=[create_embed_relu_relu_softmax])\ndef create_model(request):\n    return request.param\n\n\ndef evaluate_tagger(model, dev_X, dev_Y, batch_size):\n    correct = 0.0\n    total = 0.0\n    for i in range(0, len(dev_X), batch_size):\n        Yh = model.predict(dev_X[i : i + batch_size])\n        Y = dev_Y[i : i + batch_size]\n        for j in range(len(Yh)):\n            correct += (Yh[j].argmax(axis=1) == Y[j].argmax(axis=1)).sum()\n            total += Yh[j].shape[0]\n    return correct / total\n\n\ndef get_shuffled_batches(Xs, Ys, batch_size):\n    zipped = list(zip(Xs, Ys))\n    random.shuffle(zipped)\n    for i in range(0, len(zipped), batch_size):\n        batch_X, batch_Y = zip(*zipped[i : i + batch_size])\n        yield list(batch_X), list(batch_Y)\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(\n    (""depth"", ""width"", ""vector_width"", ""nb_epoch""), [(2, 32, 16, 5)]\n)\ndef test_small_end_to_end(depth, width, vector_width, nb_epoch, create_model, ancora):\n    (train_X, train_Y), (dev_X, dev_Y) = ancora\n    batch_size = 8\n    model = create_model(depth, width, vector_width).initialize()\n    optimizer = Adam(0.001)\n    losses = []\n    scores = []\n    for _ in range(nb_epoch):\n        losses.append(0.0)\n        for X, Y in get_shuffled_batches(train_X, train_Y, batch_size):\n            Yh, backprop = model.begin_update(X)\n            d_loss = []\n            for i in range(len(Yh)):\n                d_loss.append(Yh[i] - Y[i])\n                losses[-1] += ((Yh[i] - Y[i]) ** 2).sum()\n            backprop(d_loss)\n            model.finish_update(optimizer)\n        scores.append(evaluate_tagger(model, dev_X, dev_Y, batch_size))\n    assert losses[-1] < losses[0]\n    assert scores[-1] > scores[0]\n'"
thinc/tests/layers/test_combinators.py,0,"b'import pytest\nimport numpy\nfrom thinc.api import clone, concatenate, noop, add\nfrom thinc.api import Linear, Dropout, Model, NumpyOps\nfrom thinc.layers import chain\n\n\n@pytest.fixture(params=[1, 2, 9])\ndef nB(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 6])\ndef nI(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 5, 3])\ndef nH(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 2, 7, 9])\ndef nO(request):\n    return request.param\n\n\n@pytest.fixture\ndef model1(nH, nI):\n    return Linear(nH, nI)\n\n\n@pytest.fixture\ndef model2(nO, nH):\n    return Linear(nO, nH)\n\n\n@pytest.fixture\ndef model3(nO):\n    return Linear(nO, nO)\n\n\ndef test_chain_zero():\n    with pytest.raises(TypeError):\n        chain()\n\n\ndef test_chain_one(model1):\n    with pytest.raises(TypeError):\n        chain(model1)\n\n\ndef test_chain_two(model1, model2):\n    model = chain(model1, model2)\n    assert len(model.layers) == 2\n\n\ndef test_chain_operator_two(model1, model2):\n    with Model.define_operators({"">>"": chain}):\n        model = model1 >> model2\n        assert len(model.layers) == 2\n\n\ndef test_chain_three(model1, model2, model3):\n    model = chain(model1, model2, model3)\n    assert len(model.layers) == 3\n\n\ndef test_chain_operator_three(model1, model2, model3):\n    # Previously we \'flattened\' these nested calls. We might opt to do so\n    # again, especially for the operators.\n    with Model.define_operators({"">>"": chain}):\n        model = model1 >> model2 >> model3\n        assert len(model.layers) == 2\n        assert len(model.layers[0].layers) == 2\n\n\ndef test_chain_right_branch(model1, model2, model3):\n    # Previously we \'flattened\' these nested calls. We might opt to do so\n    # again, especially for the operators.\n    merge1 = chain(model1, model2)\n    merge2 = chain(merge1, model3)\n    assert len(merge1.layers) == 2\n    assert len(merge2.layers) == 2\n\n\n@pytest.mark.parametrize(""ops"", [NumpyOps(), NumpyOps(use_blis=True)])\ndef test_chain(ops):\n    data = numpy.asarray([[1, 2, 3, 4]], dtype=""f"")\n    model = chain(Linear(1), Dropout(), Linear(1))\n    model.ops = ops\n    model.initialize(data, data)\n    Y, backprop = model(data, is_train=True)\n    backprop(Y)\n    # Layers with and without nO/nI\n    model = chain(Linear(1), Dropout(), Linear(1, 1))\n    model.initialize(data, data)\n    # Setting dim on model\n    model = chain(Linear(1), Dropout(), Linear(1))\n    model.set_dim(""nO"", 1)\n    model.initialize(data, None)\n    model = chain(Linear(1, 1), Dropout(), Linear(1, 1))\n    model.set_dim(""nI"", 1)\n    model.initialize(None, data)\n    # Not enough arguments\n    with pytest.raises(TypeError):\n        chain(Linear())\n    with pytest.raises(TypeError):\n        chain()\n\n\ndef test_concatenate_one(model1):\n    model = concatenate(model1)\n    assert isinstance(model, Model)\n\n\ndef test_concatenate_two(model1, model2):\n    model = concatenate(model1, model2)\n    assert len(model.layers) == 2\n\n\ndef test_concatenate_operator_two(model1, model2):\n    with Model.define_operators({""|"": concatenate}):\n        model = model1 | model2\n        assert len(model.layers) == 2\n\n\ndef test_concatenate_three(model1, model2, model3):\n    model = concatenate(model1, model2, model3)\n    assert len(model.layers) == 3\n\n\ndef test_concatenate_operator_three(model1, model2, model3):\n    with Model.define_operators({""|"": concatenate}):\n        model = model1 | model2 | model3\n        assert len(model.layers) == 3\n\n\ndef test_clone_changes_predictions(nH, nI):\n    model1 = Linear(nH)\n    model = clone(model1, 10)\n    ones = numpy.ones((10, nI), dtype=""f"")\n    model.initialize(X=ones)\n    output_from_cloned = model.predict(ones)\n    output_from_orig = model1.predict(ones)\n    assert output_from_cloned.sum() != output_from_orig.sum()\n\n\ndef test_clone_gives_distinct_ids(nH, nI):\n    model = clone(Linear(nH), 5)\n    assert len(model.layers) == 5\n    seen_ids = set()\n    for node in model.walk():\n        assert node.id not in seen_ids\n        seen_ids.add(node.id)\n    assert len(seen_ids) == 6\n\n\ndef test_clone_noop():\n    model = clone(Linear(), 0)\n    assert len(model.layers) == 0\n    assert model.name == ""noop""\n\n\ndef test_concatenate_noop():\n    model = concatenate()\n    assert len(model.layers) == 0\n    assert model.name == ""noop""\n\n\ndef test_noop():\n    data = numpy.asarray([1, 2, 3], dtype=""f"")\n    model = noop(Linear(), Linear())\n    model.initialize(data, data)\n    Y, backprop = model(data, is_train=True)\n    assert numpy.array_equal(Y, data)\n    dX = backprop(Y)\n    assert numpy.array_equal(dX, data)\n\n\ndef test_add():\n    data = numpy.asarray([[1, 2, 3, 4]], dtype=""f"")\n    model = add(Linear(), Linear())\n    model.initialize(data, data)\n    Y, backprop = model(data, is_train=True)\n    Y2 = sum(layer.predict(data) for layer in model.layers)\n    assert numpy.array_equal(Y, Y2)\n    dX = backprop(Y)\n    assert dX.shape == data.shape\n    # Test that nesting works\n    model2 = add(model, Linear())\n    assert len(model2.layers) == 3\n    model.initialize(data, data)\n    Y = model2.predict(data)\n    Y2 = sum(layer.predict(data) for layer in model2.layers)\n    assert numpy.array_equal(Y, Y2)\n\n\ndef test_add_edge_cases():\n    data = numpy.asarray([[1, 2, 3, 4]], dtype=""f"")\n    with pytest.raises(TypeError):\n        add()\n    model = add(Linear(), Linear())\n    model._layers = []\n    Y, backprop = model(data, is_train=True)\n    assert numpy.array_equal(data, Y)\n    dX = backprop(Y)\n    assert numpy.array_equal(dX, data)\n\n\ndef test_concatenate():\n    data = numpy.asarray([[1, 2, 3], [4, 5, 6]], dtype=""f"")\n    model = concatenate(Linear(), Linear())\n    model.initialize(data, data)\n    Y, backprop = model(data, is_train=True)\n    assert Y.shape[1] == sum([layer.predict(data).shape[1] for layer in model.layers])\n    dX = backprop(Y)\n    assert dX.shape == data.shape\n'"
thinc/tests/layers/test_feature_extractor.py,0,"b'from srsly import cloudpickle as pickle\nfrom thinc.api import FeatureExtractor\n\n\ndef test_pickle():\n    model = FeatureExtractor([100, 200])\n    bytes_data = pickle.dumps(model)\n    loaded = pickle.loads(bytes_data)\n    assert loaded._attrs == model._attrs\n'"
thinc/tests/layers/test_feed_forward.py,0,"b'import pytest\nimport numpy\nfrom functools import partial\nfrom numpy.testing import assert_allclose\nfrom thinc.api import chain, Linear, Relu, NumpyOps\n\n\n@pytest.fixture(params=[1, 2, 9])\ndef nB(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 6])\ndef nI(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 5, 3])\ndef nH(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 2, 7, 9])\ndef nO(request):\n    return request.param\n\n\n@pytest.fixture\ndef model1(nH, nI):\n    model = Relu(nH, nI).initialize()\n    return model\n\n\n@pytest.fixture\ndef model2(nO, nH):\n    model = Linear(nO, nH).initialize()\n    return model\n\n\n@pytest.fixture\ndef input_data(nB, nI):\n    return numpy.ones((nB, nI), dtype=""f"") + 1.0\n\n\n@pytest.fixture\ndef gradient_data(nB, nO):\n    return numpy.zeros((nB, nO), dtype=""f"") - 1.0\n\n\n@pytest.fixture\ndef model(model1, model2):\n    return chain(model1, model2).initialize()\n\n\ndef get_expected_predict(input_data, Ws, bs):\n    numpy_ops = NumpyOps()\n    X = input_data\n    for i, (W, b) in enumerate(zip(Ws, bs)):\n        X = numpy_ops.asarray(X)\n        if i > 0:\n            X *= X > 0\n        X = numpy.tensordot(X, W, axes=[[1], [1]]) + b\n    return X\n\n\ndef numeric_gradient(predict, weights, epsilon=1e-4):\n    out1 = predict(weights + epsilon)\n    out2 = predict(weights - epsilon)\n    return (out1 - out2) / (2 * epsilon)\n\n\ndef test_models_have_shape(model1, model2, nI, nH, nO):\n    assert model1.get_param(""W"").shape == (nH, nI)\n    assert model1.get_param(""b"").shape == (nH,)\n    assert model2.get_param(""W"").shape == (nO, nH)\n    assert model2.get_param(""b"").shape == (nO,)\n\n\ndef test_model_shape(model, model1, model2, nI, nH, nO):\n    assert model.get_dim(""nI"") == model1.get_dim(""nI"")\n    assert model.get_dim(""nO"") == model2.get_dim(""nO"")\n\n\ndef test_infer_output_shape():\n    model = Relu(dropout=0.2)\n    X = model.ops.alloc2f(4, 5)\n    Y = model.ops.alloc2f(4, 2)\n    assert model.has_dim(""nI"") is None\n    assert model.has_dim(""nO"") is None\n    model.initialize(X=X, Y=Y)\n    assert model.get_dim(""nI"") == 5\n    assert model.get_dim(""nO"") == 2\n\n\ndef test_predict_and_begin_update_match(model, model1, model2, input_data):\n    model = chain(model1, model2)\n    via_predict = model.predict(input_data)\n    via_update, _ = model.begin_update(input_data)\n    assert_allclose(via_predict, via_update)\n    expected = get_expected_predict(\n        input_data,\n        [model1.get_param(""W""), model2.get_param(""W"")],\n        [model1.get_param(""b""), model2.get_param(""b"")],\n    )\n    assert_allclose(via_update, expected, atol=1e-2, rtol=1e-4)\n\n\ndef test_init_functions_are_called():\n    init_was_called = {}\n\n    def register_init(name, model, X=None, Y=None):\n        init_was_called[name] = True\n\n    layer1 = Linear(5)\n    layer2 = Linear(5)\n    layer3 = Linear(5)\n    layer1.init = partial(register_init, ""one"")\n    layer2.init = partial(register_init, ""two"")\n    layer3.init = partial(register_init, ""three"")\n    # This is the nesting we\'ll get from operators.\n    model = chain(layer1, chain(layer2, layer3))\n    assert not init_was_called\n    model.initialize()\n    assert init_was_called[""one""]\n    assert init_was_called[""two""]\n    assert init_was_called[""three""]\n\n\nclass GradientSpy(object):\n    def __init__(self):\n        self.weights = None\n        self.d_weights = None\n\n    def __call__(self, weights, grad):\n        self.weights = weights\n        self.d_weights = grad\n\n\n# I don\'t know how to get this working properly after the refactor. It\'s a numeric\n# gradient check. I suspect the test is the problem, not the code.\n@pytest.mark.skip\n# This is the actual definition -- it\'s just annoying to see tonnes of skips.\n# def test_gradient(model, input_data, nB, nH, nI, nO):\ndef test_gradient():\n    truth = numpy.zeros((nB, nO), dtype=""float32"")\n    truth[0] = 1.0\n\n    guess, backprop = model.begin_update(input_data)\n    backprop(guess - truth)\n\n    for layer in model.layers:\n        for name in layer.param_names:\n            agrad = layer.get_grad(name).ravel()  # Should have grads for all params.\n            predict = get_predict(layer, name, input_data)\n            ngrad = get_numeric_gradient(predict, agrad.size, truth)\n            assert_allclose(agrad, ngrad, atol=0.2, rtol=0.2)\n\n\ndef get_predict(layer, param_name, inputs):\n    """"""Helper for gradient check. To do the numeric gradient check, we have\n    to be able to wiggle one value in a parameter, and check the prediction\n    before and after. So we need to get a callback that gives an output\n    given a change to one weight.\n    """"""\n\n    def predict(i, epsilon):\n        param = layer.get_param(param_name)\n        shape = param.shape\n        param = param.ravel()\n        param[i] += epsilon\n        layer.set_param(param_name, param.reshape(shape))\n        outputs = layer.predict(inputs)\n        param[i] -= epsilon\n        layer.set_param(param_name, param.reshape(shape))\n        return outputs.reshape(shape)\n\n    return predict\n\n\ndef get_numeric_gradient(predict, n, target):\n    gradient = numpy.zeros(n)\n    for i in range(n):\n        out1 = predict(i, 1e-4)\n        out2 = predict(i, -1e-4)\n\n        err1 = _get_loss(out1, target)\n        err2 = _get_loss(out2, target)\n        gradient[i] = (err1 - err2) / (2 * 1e-4)\n        print(""NGrad"", i, err1, err2)\n    return gradient\n\n\ndef _get_loss(truth, guess):\n    return numpy.sum(numpy.sum(0.5 * numpy.square(truth - guess), 1))\n'"
thinc/tests/layers/test_hash_embed.py,0,"b'import numpy\nfrom thinc.api import HashEmbed\n\n\ndef test_init():\n    model = HashEmbed(64, 1000).initialize()\n    assert model.get_dim(""nV"") == 1000\n    assert model.get_dim(""nO"") == 64\n    assert model.get_param(""E"").shape == (1001, 64)\n\n\ndef test_seed_changes_bucket():\n    model1 = HashEmbed(64, 1000, seed=2).initialize()\n    model2 = HashEmbed(64, 1000, seed=1).initialize()\n    arr = numpy.ones((1,), dtype=""uint64"")\n    vector1 = model1.predict(arr)\n    vector2 = model2.predict(arr)\n    assert vector1.sum() != vector2.sum()\n'"
thinc/tests/layers/test_layers_api.py,0,"b'from thinc.api import registry, with_padded, Dropout, get_current_ops\nfrom thinc.model import DATA_VALIDATION\nfrom thinc.types import Ragged, Padded\nfrom thinc.util import has_torch\nimport numpy\nimport pytest\n\n\nclass FakeDoc:\n    def to_array(self, attr_ids):\n        return attr_ids\n\n\nclass FakeSpan:\n    doc = FakeDoc()\n    start = 0\n    end = -1\n\n\nOPS = get_current_ops()\n\narray1d = OPS.xp.asarray([1, 2, 3], dtype=""f"")\narray1dint = OPS.xp.asarray([1, 2, 3], dtype=""i"")\narray2d = OPS.xp.asarray([[1, 2, 3, 4], [4, 5, 3, 4]], dtype=""f"")\narray2dint = OPS.xp.asarray([[1, 2, 3], [4, 5, 6]], dtype=""i"")\narray3d = OPS.xp.zeros((3, 3, 3), dtype=""f"")\nragged = Ragged(array2d, OPS.xp.asarray([1, 1], dtype=""i""))\npadded = Padded(\n    array3d, array1d, OPS.asarray1i([1, 2, 3, 4]), OPS.asarray1i([1, 2, 3, 4])\n)\ndoc = FakeDoc()\nspan = FakeSpan()\nwidth = array2d.shape[1]\nvectors = numpy.zeros((array2dint.max(), 1), dtype=""f"")\n\n\ndef assert_data_match(Y, out_data):\n    assert type(Y) == type(out_data)\n    if isinstance(out_data, OPS.xp.ndarray):\n        assert isinstance(Y, OPS.xp.ndarray)\n        assert out_data.ndim == Y.ndim\n    elif isinstance(out_data, Ragged):\n        assert isinstance(Y, Ragged)\n        assert out_data.data.ndim == Y.data.ndim\n        assert out_data.lengths.ndim == Y.lengths.ndim\n    elif isinstance(out_data, Padded):\n        assert isinstance(Y, Padded)\n        assert out_data.data.ndim == Y.data.ndim\n        assert out_data.size_at_t.ndim == Y.size_at_t.ndim\n        assert len(out_data.lengths) == len(Y.lengths)\n        assert len(out_data.indices) == len(Y.indices)\n    elif isinstance(out_data, (list, tuple)):\n        assert isinstance(Y, (list, tuple))\n        assert all(isinstance(x, numpy.ndarray) for x in Y)\n    else:\n        pytest.fail(f""wrong output of {type(Y)}: {Y}"")\n\n\nTEST_CASES_SUMMABLE = [\n    # Array to array\n    (""Dropout.v1"", {}, array2d, array2d),\n    (""LayerNorm.v1"", {}, array2d, array2d),\n    (""Linear.v1"", {}, array2d, array2d),\n    (""Logistic.v1"", {}, array2d, array2d),\n    (""Maxout.v1"", {}, array2d, array2d),\n    (""Maxout.v1"", {""normalize"": True, ""dropout"": 0.2}, array2d, array2d),\n    (""Maxout.v1"", {""nO"": 4, ""nI"": 4}, array2d, array2d),\n    (""Mish.v1"", {}, array2d, array2d),\n    (""Mish.v1"", {""nO"": 4, ""nI"": 4}, array2d, array2d),\n    (""Mish.v1"", {""normalize"": True, ""dropout"": 0.2}, array2d, array2d),\n    (""Relu.v1"", {}, array2d, array2d),\n    (""Relu.v1"", {""normalize"": True, ""dropout"": 0.2}, array2d, array2d),\n    (""Softmax.v1"", {}, array2d, array2d),\n    (""Softmax.v1"", {""nO"": 4, ""nI"": 4}, array2d, array2d),\n    # fmt: off\n    # List to list\n    (""LSTM.v1"", {""bi"": False}, [array2d, array2d], [array2d, array2d]),\n    pytest.param(""PyTorchLSTM.v1"", {""bi"": False, ""nO"": width, ""nI"": width}, [array2d, array2d], [array2d, array2d], marks=pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")),\n    # fmt: on\n]\n\nTEST_CASES = [\n    *TEST_CASES_SUMMABLE,\n    pytest.param(\n        ""PyTorchLSTM.v1"",\n        {""bi"": True, ""nO"": width * 2, ""nI"": width},\n        [array2d, array2d],\n        [array2d, array2d],\n        marks=pytest.mark.skipif(not has_torch, reason=""needs PyTorch""),\n    ),\n    (""LSTM.v1"", {""bi"": True}, [array2d, array2d], [array2d, array2d]),\n    (""StaticVectors.v1"", {""nO"": 1, ""vectors"": vectors}, array1dint, array2d),\n    (\n        ""StaticVectors.v1"",\n        {""nO"": 1, ""vectors"": vectors, ""column"": 0},\n        array2dint,\n        array2d,\n    ),\n    # Ragged to array\n    (""reduce_max.v1"", {}, ragged, array2d),\n    (""reduce_mean.v1"", {}, ragged, array2d),\n    (""reduce_sum.v1"", {}, ragged, array2d),\n    # fmt: off\n    # Other\n    (""expand_window.v1"", {}, array2d, array2d),\n    (""Embed.v1"", {""nO"": 4, ""nV"": array2dint.max() + 1, ""column"": 0}, array2dint, array2d),\n    (""Embed.v1"", {""nO"": 4, ""nV"": array1dint.max() + 1}, array1dint, array2d),\n    (""HashEmbed.v1"", {""nO"": 1, ""nV"": array2dint.max(), ""column"": 0}, array2dint, array2d),\n    (""HashEmbed.v1"", {""nO"": 1, ""nV"": 2}, array1dint, array2d),\n    (""MultiSoftmax.v1"", {""nOs"": (1, 3)}, array2d, array2d),\n    (""CauchySimilarity.v1"", {}, (array2d, array2d), array1d),\n    (""FeatureExtractor.v1"", {""columns"": [1, 2]}, [doc, doc, doc], [array2d, array2d, array2d]),\n    (""FeatureExtractor.v1"", {""columns"": [1, 2]}, [span, span], [array2d, array2d]),\n    (""ParametricAttention.v1"", {}, ragged, ragged),\n    (""SparseLinear.v1"", {}, (numpy.asarray([1, 2, 3], dtype=""uint64""), array1d, numpy.asarray([1, 1], dtype=""i"")), array2d),\n    (""remap_ids.v1"", {""dtype"": ""f""}, [""a"", 1, 5.0], array2dint)\n    # fmt: on\n]\n\n\n@pytest.mark.parametrize(""name,kwargs,in_data,out_data"", TEST_CASES)\ndef test_layers_from_config(name, kwargs, in_data, out_data):\n    cfg = {""@layers"": name, **kwargs}\n    filled = registry.fill_config({""config"": cfg})\n    model = registry.make_from_config(filled)[""config""]\n    if ""LSTM"" in name:\n        model = with_padded(model)\n    if ""FeatureExtractor"" in name:  # can\'t validate fake docs:\n        DATA_VALIDATION.set(False)\n    model.initialize(in_data, out_data)\n    Y, backprop = model(in_data, is_train=True)\n    assert_data_match(Y, out_data)\n    dX = backprop(Y)\n    assert_data_match(dX, in_data)\n    DATA_VALIDATION.set(True)\n\n\n@pytest.mark.parametrize(""name,kwargs,in_data,out_data"", TEST_CASES_SUMMABLE)\ndef test_layers_with_residual(name, kwargs, in_data, out_data):\n    cfg = {""@layers"": ""residual.v1"", ""layer"": {""@layers"": name, **kwargs}}\n    filled = registry.fill_config({""config"": cfg})\n    model = registry.make_from_config(filled)[""config""]\n    if ""LSTM"" in name:\n        model = with_padded(model)\n    model.initialize(in_data, out_data)\n    Y, backprop = model(in_data, is_train=True)\n    assert_data_match(Y, out_data)\n    dX = backprop(Y)\n    assert_data_match(dX, in_data)\n\n\n@pytest.mark.parametrize(""data"", [array2d, ragged, padded, [array2d, array2d]])\ndef test_dropout(data):\n    model = Dropout(0.2)\n    model.initialize(data, data)\n    Y, backprop = model(data, is_train=False)\n    assert_data_match(Y, data)\n    dX = backprop(Y)\n    assert_data_match(dX, data)\n'"
thinc/tests/layers/test_linear.py,0,"b'import pytest\nfrom mock import MagicMock\nfrom hypothesis import given, settings\nimport numpy\nfrom numpy.testing import assert_allclose\nfrom thinc.api import Linear, chain, Dropout, SGD\n\nfrom ..strategies import arrays_OI_O_BI\nfrom ..util import get_model, get_shape\n\n\n@pytest.fixture\ndef model():\n    model = Linear()\n    return model\n\n\ndef test_linear_default_name(model):\n    assert model.name == ""linear""\n\n\ndef test_linear_dimensions_on_data():\n    X = MagicMock(shape=(5, 10), spec=numpy.ndarray)\n    X.ndim = 2\n    X.dtype = ""float32""\n    y = MagicMock(shape=(8,), spec=numpy.ndarray)\n    y.ndim = 2\n    y.dtype = ""float32""\n    y.max = MagicMock()\n    model = Linear()\n    model.initialize(X, y)\n    assert model.get_dim(""nI"") is not None\n    y.max.assert_called_with()\n\n\n@given(arrays_OI_O_BI(max_batch=8, max_out=8, max_in=8))\ndef test_begin_update_matches_predict(W_b_input):\n    model = get_model(W_b_input)\n    nr_batch, nr_out, nr_in = get_shape(W_b_input)\n    W, b, input_ = W_b_input\n    fwd_via_begin_update, finish_update = model.begin_update(input_)\n    fwd_via_predict_batch = model.predict(input_)\n    assert_allclose(fwd_via_begin_update, fwd_via_predict_batch)\n\n\n@given(arrays_OI_O_BI(max_batch=8, max_out=8, max_in=8))\ndef test_finish_update_calls_optimizer_with_weights(W_b_input):\n    model = get_model(W_b_input)\n    nr_batch, nr_out, nr_in = get_shape(W_b_input)\n    W, b, input_ = W_b_input\n    output, finish_update = model.begin_update(input_)\n\n    seen_keys = set()\n\n    def sgd(key, data, gradient, **kwargs):\n        seen_keys.add(key)\n        assert data.shape == gradient.shape\n        return data, gradient\n\n    grad_BO = numpy.ones((nr_batch, nr_out), dtype=""f"")\n    grad_BI = finish_update(grad_BO)  # noqa: F841\n    model.finish_update(sgd)\n    assert seen_keys == {(model.id, model.name)}\n\n\n@settings(max_examples=100)\n@given(arrays_OI_O_BI(max_batch=8, max_out=8, max_in=8))\ndef test_predict_small(W_b_input):\n    W, b, input_ = W_b_input\n    nr_out, nr_in = W.shape\n    model = Linear(nr_out, nr_in)\n    model.set_param(""W"", W)\n    model.set_param(""b"", b)\n\n    einsummed = numpy.einsum(\n        ""oi,bi->bo"",\n        numpy.asarray(W, dtype=""float64""),\n        numpy.asarray(input_, dtype=""float64""),\n        optimize=False,\n    )\n\n    expected_output = einsummed + b\n\n    predicted_output = model.predict(input_)\n    assert_allclose(predicted_output, expected_output, rtol=0.01, atol=0.01)\n\n\n@given(arrays_OI_O_BI(max_batch=20, max_out=30, max_in=30))\ndef test_predict_extensive(W_b_input):\n    W, b, input_ = W_b_input\n    nr_out, nr_in = W.shape\n    model = Linear(nr_out, nr_in)\n    model.set_param(""W"", W)\n    model.set_param(""b"", b)\n\n    einsummed = numpy.einsum(\n        ""bi,oi->bo"",\n        numpy.asarray(input_, dtype=""float32""),\n        numpy.asarray(W, dtype=""float32""),\n        optimize=False,\n    )\n\n    expected_output = einsummed + b\n\n    predicted_output = model.predict(input_)\n    assert_allclose(predicted_output, expected_output, rtol=1e-04, atol=0.0001)\n\n\n@given(arrays_OI_O_BI(max_batch=8, max_out=8, max_in=8))\ndef test_dropout_gives_zero_activations(W_b_input):\n    model = chain(get_model(W_b_input), Dropout(1.0))\n    nr_batch, nr_out, nr_in = get_shape(W_b_input)\n    W, b, input_ = W_b_input\n    fwd_dropped, _ = model.begin_update(input_)\n    assert all(val == 0.0 for val in fwd_dropped.flatten())\n\n\n@given(arrays_OI_O_BI(max_batch=8, max_out=8, max_in=8))\ndef test_dropout_gives_zero_gradients(W_b_input):\n    model = chain(get_model(W_b_input), Dropout(1.0))\n    nr_batch, nr_out, nr_in = get_shape(W_b_input)\n    W, b, input_ = W_b_input\n    for node in model.walk():\n        if node.name == ""dropout"":\n            node.attrs[""dropout_rate""] = 1.0\n    fwd_dropped, finish_update = model.begin_update(input_)\n    grad_BO = numpy.ones((nr_batch, nr_out), dtype=""f"")\n    grad_BI = finish_update(grad_BO)\n    assert all(val == 0.0 for val in grad_BI.flatten())\n\n\n@pytest.fixture\ndef model2():\n    model = Linear(2, 2).initialize()\n    return model\n\n\ndef test_init(model2):\n    assert model2.get_dim(""nO"") == 2\n    assert model2.get_dim(""nI"") == 2\n    assert model2.get_param(""W"") is not None\n    assert model2.get_param(""b"") is not None\n\n\ndef test_predict_bias(model2):\n    input_ = model2.ops.alloc2f(1, model2.get_dim(""nI""))\n    target_scores = model2.ops.alloc2f(1, model2.get_dim(""nI""))\n    scores = model2.predict(input_)\n    assert_allclose(scores[0], target_scores[0])\n    # Set bias for class 0\n    model2.get_param(""b"")[0] = 2.0\n    target_scores[0, 0] = 2.0\n    scores = model2.predict(input_)\n    assert_allclose(scores, target_scores)\n    # Set bias for class 1\n    model2.get_param(""b"")[1] = 5.0\n    target_scores[0, 1] = 5.0\n    scores = model2.predict(input_)\n    assert_allclose(scores, target_scores)\n\n\n@pytest.mark.parametrize(\n    ""X,expected"",\n    [\n        (numpy.asarray([0.0, 0.0], dtype=""f""), [0.0, 0.0]),\n        (numpy.asarray([1.0, 0.0], dtype=""f""), [1.0, 0.0]),\n        (numpy.asarray([0.0, 1.0], dtype=""f""), [0.0, 1.0]),\n        (numpy.asarray([1.0, 1.0], dtype=""f""), [1.0, 1.0]),\n    ],\n)\ndef test_predict_weights(X, expected):\n    W = numpy.asarray([1.0, 0.0, 0.0, 1.0], dtype=""f"").reshape((2, 2))\n    bias = numpy.asarray([0.0, 0.0], dtype=""f"")\n\n    model = Linear(W.shape[0], W.shape[1])\n    model.set_param(""W"", W)\n    model.set_param(""b"", bias)\n\n    scores = model.predict(X.reshape((1, -1)))\n    assert_allclose(scores.ravel(), expected)\n\n\ndef test_update():\n    W = numpy.asarray([1.0, 0.0, 0.0, 1.0], dtype=""f"").reshape((2, 2))\n    bias = numpy.asarray([0.0, 0.0], dtype=""f"")\n\n    model = Linear(2, 2)\n    model.set_param(""W"", W)\n    model.set_param(""b"", bias)\n    sgd = SGD(1.0, L2=0.0, grad_clip=0.0)\n    sgd.averages = None\n\n    ff = numpy.asarray([[0.0, 0.0]], dtype=""f"")\n    tf = numpy.asarray([[1.0, 0.0]], dtype=""f"")\n    ft = numpy.asarray([[0.0, 1.0]], dtype=""f"")  # noqa: F841\n    tt = numpy.asarray([[1.0, 1.0]], dtype=""f"")  # noqa: F841\n\n    # ff, i.e. 0, 0\n    scores, backprop = model.begin_update(ff)\n    assert_allclose(scores[0, 0], scores[0, 1])\n    # Tell it the answer was \'f\'\n    gradient = numpy.asarray([[-1.0, 0.0]], dtype=""f"")\n    backprop(gradient)\n    for key, (param, d_param) in model.get_gradients().items():\n        param, d_param = sgd(key, param, d_param)\n        model.set_param(key[1], param)\n        model.set_grad(key[1], d_param)\n\n    b = model.get_param(""b"")\n    W = model.get_param(""W"")\n    assert b[0] == 1.0\n    assert b[1] == 0.0\n    # Unchanged -- input was zeros, so can\'t get gradient for weights.\n    assert W[0, 0] == 1.0\n    assert W[0, 1] == 0.0\n    assert W[1, 0] == 0.0\n    assert W[1, 1] == 1.0\n\n    # tf, i.e. 1, 0\n    scores, finish_update = model.begin_update(tf)\n    # Tell it the answer was \'T\'\n    gradient = numpy.asarray([[0.0, -1.0]], dtype=""f"")\n    finish_update(gradient)\n    for key, (W, dW) in model.get_gradients().items():\n        sgd(key, W, dW)\n    b = model.get_param(""b"")\n    W = model.get_param(""W"")\n    assert b[0] == 1.0\n    assert b[1] == 1.0\n    # Gradient for weights should have been outer(gradient, input)\n    # so outer([0, -1.], [1., 0.])\n    # =  [[0., 0.], [-1., 0.]]\n    assert W[0, 0] == 1.0 - 0.0\n    assert W[0, 1] == 0.0 - 0.0\n    assert W[1, 0] == 0.0 - -1.0\n    assert W[1, 1] == 1.0 - 0.0\n'"
thinc/tests/layers/test_lstm.py,0,"b'import numpy\nimport timeit\nfrom thinc.api import NumpyOps, LSTM, PyTorchLSTM, with_padded, fix_random_seed\nfrom thinc.util import has_torch\nimport pytest\n\n\n@pytest.fixture(params=[1, 6])\ndef nI(request):\n    return request.param\n\n\n@pytest.fixture(params=[1, 2, 7, 9])\ndef nO(request):\n    return request.param\n\n\ndef test_list2padded():\n    ops = NumpyOps()\n    seqs = [numpy.zeros((5, 4)), numpy.zeros((8, 4)), numpy.zeros((2, 4))]\n    padded = ops.list2padded(seqs)\n    arr = padded.data\n    size_at_t = padded.size_at_t\n    assert arr.shape == (8, 3, 4)\n    assert size_at_t[0] == 3\n    assert size_at_t[1] == 3\n    assert size_at_t[2] == 2\n    assert size_at_t[3] == 2\n    assert size_at_t[4] == 2\n    assert size_at_t[5] == 1\n    assert size_at_t[6] == 1\n    assert size_at_t[7] == 1\n    unpadded = ops.padded2list(padded)\n    assert unpadded[0].shape == (5, 4)\n    assert unpadded[1].shape == (8, 4)\n    assert unpadded[2].shape == (2, 4)\n\n\n@pytest.mark.parametrize(""nO,nI"", [(1, 2), (2, 2), (100, 200), (9, 6)])\ndef test_LSTM_init_with_sizes(nO, nI):\n    model = with_padded(LSTM(nO, nI)).initialize()\n    for node in model.walk():\n        # Check no unallocated params.\n        assert node.has_param(""W"") is not None\n        assert node.has_param(""b"") is not None\n        assert node.has_param(""initial_hiddens"") is not None\n        assert node.has_param(""initial_cells"") is not None\n    for node in model.walk():\n        # Check param sizes.\n        if node.has_param(""W""):\n            W = node.get_param(""W"")\n            assert W.shape == (nO * 4, nO + nI)\n        if node.has_param(""b""):\n            b = node.get_param(""b"")\n            assert b.shape == (nO * 4,)\n        if node.has_param(""initial_hiddens""):\n            initial_hiddens = node.get_param(""initial_hiddens"")\n            assert initial_hiddens.shape == (nO,)\n        if node.has_param(""initial_cells""):\n            initial_cells = node.get_param(""initial_cells"")\n            assert initial_cells.shape == (nO,)\n\n\ndef test_LSTM_fwd_bwd_shapes(nO, nI):\n    nO = 1\n    nI = 2\n    X = numpy.asarray([[0.1, 0.1], [-0.1, -0.1], [1.0, 1.0]], dtype=""f"")\n    model = with_padded(LSTM(nO, nI)).initialize(X=[X])\n    ys, backprop_ys = model([X], is_train=False)\n    dXs = backprop_ys(ys)\n    assert numpy.vstack(dXs).shape == numpy.vstack([X]).shape\n\n\ndef test_LSTM_learns():\n    fix_random_seed(0)\n\n    nO = 2\n    nI = 2\n\n    def sgd(key, weights, gradient):\n        weights -= 0.001 * gradient\n        return weights, gradient * 0\n\n    model = with_padded(LSTM(nO, nI))\n    X = [[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]]\n    Y = [[0.2, 0.2], [0.3, 0.3], [0.4, 0.4]]\n    X = [model.ops.asarray(x, dtype=""f"").reshape((1, -1)) for x in X]\n    Y = [model.ops.asarray(y, dtype=""f"").reshape((1, -1)) for y in Y]\n    model = model.initialize(X, Y)\n    Yhs, bp_Yhs = model.begin_update(X)\n    loss1 = sum([((yh - y) ** 2).sum() for yh, y in zip(Yhs, Y)])\n    Yhs, bp_Yhs = model.begin_update(X)\n    dYhs = [yh - y for yh, y in zip(Yhs, Y)]\n    dXs = bp_Yhs(dYhs)\n    model.finish_update(sgd)\n    Yhs, bp_Yhs = model.begin_update(X)\n    dYhs = [yh - y for yh, y in zip(Yhs, Y)]\n    dXs = bp_Yhs(dYhs)  # noqa: F841\n    loss2 = sum([((yh - y) ** 2).sum() for yh, y in zip(Yhs, Y)])\n    assert loss1 > loss2, (loss1, loss2)\n\n\n@pytest.mark.skip\ndef test_benchmark_LSTM_fwd():\n    nO = 128\n    nI = 128\n    n_batch = 1000\n    batch_size = 30\n    seq_len = 30\n    lengths = numpy.random.normal(scale=10, loc=30, size=n_batch * batch_size)\n    lengths = numpy.maximum(lengths, 1)\n    batches = []\n    uniform_lengths = False\n    model = with_padded(LSTM(nO, nI)).initialize()\n    for batch_lengths in model.ops.minibatch(batch_size, lengths):\n        batch_lengths = list(batch_lengths)\n        if uniform_lengths:\n            seq_len = max(batch_lengths)\n            batch = [\n                numpy.asarray(\n                    numpy.random.uniform(0.0, 1.0, (int(seq_len), nI)), dtype=""f""\n                )\n                for _ in batch_lengths\n            ]\n        else:\n            batch = [\n                numpy.asarray(\n                    numpy.random.uniform(0.0, 1.0, (int(seq_len), nI)), dtype=""f""\n                )\n                for seq_len in batch_lengths\n            ]\n        batches.append(batch)\n    start = timeit.default_timer()\n    for Xs in batches:\n        ys, bp_ys = model.begin_update(list(Xs))\n        # _ = bp_ys(ys)\n    end = timeit.default_timer()\n    n_samples = n_batch * batch_size\n    print(\n        ""--- %i samples in %s seconds (%f samples/s, %.7f s/sample) ---""\n        % (n_samples, end - start, n_samples / (end - start), (end - start) / n_samples)\n    )\n\n\ndef test_lstm_init():\n    model = with_padded(LSTM(2, 2, bi=True)).initialize()\n    model.initialize()\n    with pytest.raises(NotImplementedError):\n        with_padded(LSTM(2, dropout=0.2))\n\n\n@pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")\ndef test_pytorch_lstm_init():\n    model = with_padded(PyTorchLSTM(2, 2, depth=0)).initialize()\n    assert model.name.endswith(""noop"")\n'"
thinc/tests/layers/test_mnist.py,7,"b'import pytest\nfrom thinc.api import Relu, Softmax, chain, clone, Adam\nfrom thinc.api import PyTorchWrapper, TensorFlowWrapper\nfrom thinc.util import has_torch, has_tensorflow\nimport ml_datasets\n\n\n@pytest.fixture(scope=""module"")\ndef mnist(limit=5000):\n    (train_X, train_Y), (dev_X, dev_Y) = ml_datasets.mnist()\n    return (train_X[:limit], train_Y[:limit]), (dev_X[:limit], dev_Y[:limit])\n\n\ndef create_relu_softmax(width, dropout, nI, nO):\n    return chain(clone(Relu(nO=width, dropout=dropout), 2), Softmax(10, width))\n\n\ndef create_wrapped_pytorch(width, dropout, nI, nO):\n    import torch\n    import torch.nn\n    import torch.nn.functional as F\n\n    class PyTorchModel(torch.nn.Module):\n        def __init__(self, width, nO, nI, dropout):\n            super(PyTorchModel, self).__init__()\n            self.dropout1 = torch.nn.Dropout2d(dropout)\n            self.dropout2 = torch.nn.Dropout2d(dropout)\n            self.fc1 = torch.nn.Linear(nI, width)\n            self.fc2 = torch.nn.Linear(width, nO)\n\n        def forward(self, x):\n            x = F.relu(x)\n            x = self.dropout1(x)\n            x = self.fc1(x)\n            x = F.relu(x)\n            x = self.dropout2(x)\n            x = self.fc2(x)\n            output = F.log_softmax(x, dim=1)\n            return output\n\n    return PyTorchWrapper(PyTorchModel(width, nO, nI, dropout))\n\n\ndef create_wrapped_tensorflow(width, dropout, nI, nO):\n    from tensorflow.keras.layers import Dense, Dropout\n    from tensorflow.keras.models import Sequential\n\n    tf_model = Sequential()\n    tf_model.add(Dense(width, activation=""relu"", input_shape=(nI,)))\n    tf_model.add(Dropout(dropout))\n    tf_model.add(Dense(width, activation=""relu""))\n    tf_model.add(Dropout(dropout))\n    tf_model.add(Dense(nO, activation=None))\n    return TensorFlowWrapper(tf_model)\n\n\n@pytest.fixture(\n    # fmt: off\n    params=[\n        create_relu_softmax,\n        pytest.param(create_wrapped_pytorch, marks=pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")),\n        pytest.param(create_wrapped_tensorflow, marks=pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow""))\n    ]\n    # fmt: on\n)\ndef create_model(request):\n    return request.param\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize((""width"", ""nb_epoch"", ""min_score""), [(32, 20, 0.8)])\ndef test_small_end_to_end(width, nb_epoch, min_score, create_model, mnist):\n    batch_size = 128\n    dropout = 0.2\n    (train_X, train_Y), (dev_X, dev_Y) = mnist\n    model = create_model(width, dropout, nI=train_X.shape[1], nO=train_Y.shape[1])\n    model.initialize(X=train_X[:5], Y=train_Y[:5])\n    optimizer = Adam(0.001)\n    losses = []\n    scores = []\n    for i in range(nb_epoch):\n        for X, Y in model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True):\n            Yh, backprop = model.begin_update(X)\n            backprop(Yh - Y)\n            model.finish_update(optimizer)\n            losses.append(((Yh - Y) ** 2).sum())\n        correct = 0\n        total = 0\n        for X, Y in model.ops.multibatch(batch_size, dev_X, dev_Y):\n            Yh = model.predict(X)\n            correct += (Yh.argmax(axis=1) == Y.argmax(axis=1)).sum()\n            total += Yh.shape[0]\n        score = correct / total\n        scores.append(score)\n    assert losses[-1] < losses[0], losses\n    if scores[0] < 1.0:\n        assert scores[-1] > scores[0], scores\n    assert any([score > min_score for score in scores]), scores\n'"
thinc/tests/layers/test_mxnet_wrapper.py,0,"b'from typing import cast\n\nimport numpy\nimport pytest\nfrom thinc.api import Adam, ArgsKwargs, Model, Ops, MXNetWrapper\nfrom thinc.api import get_current_ops, mxnet2xp, xp2mxnet\nfrom thinc.types import Array2d, Array1d, IntsXd\nfrom thinc.util import has_mxnet, to_categorical\n\nfrom ..util import check_input_converters, make_tempdir\n\n\n@pytest.fixture\ndef n_hidden() -> int:\n    return 12\n\n\n@pytest.fixture\ndef input_size() -> int:\n    return 784\n\n\n@pytest.fixture\ndef n_classes() -> int:\n    return 10\n\n\n@pytest.fixture\ndef answer() -> int:\n    return 1\n\n\n@pytest.fixture\ndef X(input_size: int) -> Array2d:\n    ops: Ops = get_current_ops()\n    return ops.alloc(shape=(1, input_size))\n\n\n@pytest.fixture\ndef Y(answer: int, n_classes: int) -> Array2d:\n    ops: Ops = get_current_ops()\n    return cast(\n        Array2d,\n        to_categorical(cast(IntsXd, ops.asarray([answer])), n_classes=n_classes),\n    )\n\n\n@pytest.fixture\ndef mx_model(n_hidden: int, input_size: int, X: Array2d):\n    import mxnet as mx\n\n    mx_model = mx.gluon.nn.Sequential()\n    mx_model.add(\n        mx.gluon.nn.Dense(n_hidden),\n        mx.gluon.nn.LayerNorm(),\n        mx.gluon.nn.Dense(n_hidden, activation=""relu""),\n        mx.gluon.nn.LayerNorm(),\n        mx.gluon.nn.Dense(10, activation=""softrelu""),\n    )\n    mx_model.initialize()\n    return mx_model\n\n\n@pytest.fixture\ndef model(mx_model) -> Model[Array2d, Array2d]:\n    return MXNetWrapper(mx_model)\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_roundtrip_conversion():\n    import mxnet as mx\n\n    xp_tensor = numpy.zeros((2, 3), dtype=""f"")\n    mx_tensor = xp2mxnet(xp_tensor)\n    assert isinstance(mx_tensor, mx.nd.NDArray)\n    new_xp_tensor = mxnet2xp(mx_tensor)\n    assert numpy.array_equal(xp_tensor, new_xp_tensor)\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_gluon_sequential():\n    import mxnet as mx\n\n    mx_model = mx.gluon.nn.Sequential()\n    mx_model.add(mx.gluon.nn.Dense(12))\n    wrapped = MXNetWrapper(mx_model)\n    assert isinstance(wrapped, Model)\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_built_model(\n    model: Model[Array2d, Array2d], X: Array2d, Y: Array1d\n):\n    # built models are validated more and can perform useful operations:\n    assert model.predict(X) is not None\n    # They can de/serialized\n    assert model.from_bytes(model.to_bytes()) is not None\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_predict(model: Model[Array2d, Array2d], X: Array2d):\n    model.predict(X)\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_train_overfits(\n    model: Model[Array2d, Array2d], X: Array2d, Y: Array1d, answer: int\n):\n    optimizer = Adam()\n    for i in range(100):\n        guesses, backprop = model(X, is_train=True)\n        d_guesses = (guesses - Y) / guesses.shape[0]\n        backprop(d_guesses)\n        model.finish_update(optimizer)\n    predicted = model.predict(X).argmax()\n    assert predicted == answer\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_can_copy_model(model: Model[Array2d, Array2d], X: Array2d):\n    model.predict(X)\n    copy: Model[Array2d, Array2d] = model.copy()\n    assert copy is not None\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_to_bytes(model: Model[Array2d, Array2d], X: Array2d):\n    model.predict(X)\n    # And can be serialized\n    model_bytes = model.to_bytes()\n    assert model_bytes is not None\n    model.from_bytes(model_bytes)\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_to_from_disk(model: Model[Array2d, Array2d], X: Array2d):\n    model.predict(X)\n    with make_tempdir() as tmp_path:\n        model_file = tmp_path / ""model.bytes""\n        model.to_disk(model_file)\n        another_model = model.from_disk(model_file)\n        assert another_model is not None\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_from_bytes(model: Model[Array2d, Array2d], X: Array2d):\n    model.predict(X)\n    model_bytes = model.to_bytes()\n    another_model = model.from_bytes(model_bytes)\n    assert another_model is not None\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_to_cpu(mx_model, X: Array2d):\n    model = MXNetWrapper(mx_model)\n    model.predict(X)\n    model.to_cpu()\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_to_gpu(model: Model[Array2d, Array2d], X: Array2d):\n    # Raises while failing to import cupy\n    with pytest.raises(ImportError):\n        model.to_gpu(0)\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\n@pytest.mark.parametrize(\n    ""data,n_args,kwargs_keys"",\n    [\n        # fmt: off\n        (numpy.zeros((2, 3), dtype=""f""), 1, []),\n        ([numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")], 2, []),\n        ((numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")), 2, []),\n        ({""a"": numpy.zeros((2, 3), dtype=""f""), ""b"": numpy.zeros((2, 3), dtype=""f"")}, 0, [""a"", ""b""]),\n        (ArgsKwargs((numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")), {""c"": numpy.zeros((2, 3), dtype=""f"")}), 2, [""c""]),\n        # fmt: on\n    ],\n)\ndef test_mxnet_wrapper_convert_inputs(data, n_args, kwargs_keys):\n    import mxnet as mx\n\n    mx_model = mx.gluon.nn.Sequential()\n    mx_model.add(mx.gluon.nn.Dense(12))\n    mx_model.initialize()\n    model = MXNetWrapper(mx_model)\n    convert_inputs = model.attrs[""convert_inputs""]\n    Y, backprop = convert_inputs(model, data, is_train=True)\n    check_input_converters(Y, backprop, data, n_args, kwargs_keys, mx.nd.NDArray)\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_thinc_model_subclass(mx_model):\n    class CustomModel(Model):\n        def fn(self) -> int:\n            return 1337\n\n    model = MXNetWrapper(mx_model, model_class=CustomModel)\n    assert isinstance(model, CustomModel)\n    assert model.fn() == 1337\n\n\n@pytest.mark.skipif(not has_mxnet, reason=""needs MXNet"")\ndef test_mxnet_wrapper_thinc_set_model_name(mx_model):\n    model = MXNetWrapper(mx_model, model_name=""cool"")\n    assert model.name == ""cool""\n'"
thinc/tests/layers/test_pytorch_wrapper.py,10,"b'from thinc.api import Linear, SGD, PyTorchWrapper, xp2torch, torch2xp, ArgsKwargs\nfrom thinc.util import has_torch\nimport numpy\nimport pytest\n\nfrom ..util import make_tempdir, check_input_converters\n\n\ndef check_learns_zero_output(model, sgd, X, Y):\n    """"""Check we can learn to output a zero vector""""""\n    Yh, get_dX = model.begin_update(X)\n    dYh = (Yh - Y) / Yh.shape[0]\n    dX = get_dX(dYh)\n    model.finish_update(sgd)\n    prev = numpy.abs(Yh.sum())\n    for i in range(100):\n        Yh, get_dX = model.begin_update(X)\n        total = numpy.abs(Yh.sum())\n        dX = get_dX(Yh - Y)  # noqa: F841\n        model.finish_update(sgd)\n        assert total < prev\n        prev = total\n\n\n@pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")\n@pytest.mark.parametrize(""nN,nI,nO"", [(2, 3, 4)])\ndef test_pytorch_unwrapped(nN, nI, nO):\n    model = Linear(nO, nI).initialize()\n    X = numpy.zeros((nN, nI), dtype=""f"")\n    X += numpy.random.uniform(size=X.size).reshape(X.shape)\n    sgd = SGD(0.001)\n    Y = numpy.zeros((nN, nO), dtype=""f"")\n    check_learns_zero_output(model, sgd, X, Y)\n\n\n@pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")\n@pytest.mark.parametrize(""nN,nI,nO"", [(2, 3, 4)])\ndef test_pytorch_wrapper(nN, nI, nO):\n    import torch.nn\n\n    model = PyTorchWrapper(torch.nn.Linear(nI, nO)).initialize()\n    sgd = SGD(0.001)\n    X = numpy.zeros((nN, nI), dtype=""f"")\n    X += numpy.random.uniform(size=X.size).reshape(X.shape)\n    Y = numpy.zeros((nN, nO), dtype=""f"")\n    Yh, get_dX = model.begin_update(X)\n    assert isinstance(Yh, numpy.ndarray)\n    assert Yh.shape == (nN, nO)\n    dYh = (Yh - Y) / Yh.shape[0]\n    dX = get_dX(dYh)\n    model.finish_update(sgd)\n    assert dX.shape == (nN, nI)\n    check_learns_zero_output(model, sgd, X, Y)\n    assert isinstance(model.predict(X), numpy.ndarray)\n\n\n@pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")\ndef test_pytorch_roundtrip_conversion():\n    import torch\n\n    xp_tensor = numpy.zeros((2, 3), dtype=""f"")\n    torch_tensor = xp2torch(xp_tensor)\n    assert isinstance(torch_tensor, torch.Tensor)\n    new_xp_tensor = torch2xp(torch_tensor)\n    assert numpy.array_equal(xp_tensor, new_xp_tensor)\n\n\n@pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")\ndef test_pytorch_wrapper_roundtrip():\n    import torch.nn\n\n    model = PyTorchWrapper(torch.nn.Linear(2, 3))\n    model_bytes = model.to_bytes()\n    PyTorchWrapper(torch.nn.Linear(2, 3)).from_bytes(model_bytes)\n    with make_tempdir() as path:\n        model_path = path / ""model""\n        model.to_disk(model_path)\n        new_model = PyTorchWrapper(torch.nn.Linear(2, 3)).from_bytes(model_bytes)\n        new_model.from_disk(model_path)\n\n\n@pytest.mark.skipif(not has_torch, reason=""needs PyTorch"")\n@pytest.mark.parametrize(\n    ""data,n_args,kwargs_keys"",\n    [\n        # fmt: off\n        (numpy.zeros((2, 3), dtype=""f""), 1, []),\n        ([numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")], 2, []),\n        ((numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")), 2, []),\n        ({""a"": numpy.zeros((2, 3), dtype=""f""), ""b"": numpy.zeros((2, 3), dtype=""f"")}, 0, [""a"", ""b""]),\n        (ArgsKwargs((numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")), {""c"": numpy.zeros((2, 3), dtype=""f"")}), 2, [""c""]),\n        # fmt: on\n    ],\n)\ndef test_pytorch_convert_inputs(data, n_args, kwargs_keys):\n    import torch.nn\n\n    model = PyTorchWrapper(torch.nn.Linear(3, 4))\n    convert_inputs = model.attrs[""convert_inputs""]\n    Y, backprop = convert_inputs(model, data, is_train=True)\n    check_input_converters(Y, backprop, data, n_args, kwargs_keys, torch.Tensor)\n'"
thinc/tests/layers/test_sparse_linear.py,0,"b'import numpy\nimport pytest\nfrom thinc.api import SGD, to_categorical, SparseLinear\n\n\n@pytest.fixture\ndef instances():\n    lengths = numpy.asarray([5, 4], dtype=""int32"")\n    keys = numpy.arange(9, dtype=""uint64"")\n    values = numpy.ones(9, dtype=""float32"")\n    X = (keys, values, lengths)\n    y = numpy.asarray([0, 2], dtype=""int32"")\n    return X, to_categorical(y, n_classes=3)\n\n\n@pytest.fixture\ndef sgd():\n    return SGD(0.001)\n\n\ndef test_basic(instances, sgd):\n    X, y = instances\n    nr_class = 3\n    model = SparseLinear(nr_class).initialize()\n    yh, backprop = model.begin_update(X)\n    loss1 = ((yh - y) ** 2).sum()\n    backprop(yh - y)\n    model.finish_update(sgd)\n    yh, backprop = model.begin_update(X)\n    loss2 = ((yh - y) ** 2).sum()\n    assert loss2 < loss1\n\n\ndef test_init():\n    model = SparseLinear(3).initialize()\n    keys = numpy.ones((5,), dtype=""uint64"")\n    values = numpy.ones((5,), dtype=""f"")\n    lengths = numpy.zeros((2,), dtype=""int32"")\n    lengths[0] = 3\n    lengths[1] = 2\n    scores, backprop = model.begin_update((keys, values, lengths))\n    assert scores.shape == (2, 3)\n    d_feats = backprop(scores)\n    assert len(d_feats) == 3\n'"
thinc/tests/layers/test_tensorflow_wrapper.py,0,"b'import numpy\nimport pytest\nfrom thinc.api import Adam, ArgsKwargs, Linear, Model, TensorFlowWrapper\nfrom thinc.api import get_current_ops, keras_subclass, tensorflow2xp, xp2tensorflow\nfrom thinc.util import has_cupy, has_tensorflow, to_categorical\n\nfrom ..util import check_input_converters, make_tempdir\n\n\n@pytest.fixture\ndef n_hidden():\n    return 12\n\n\n@pytest.fixture\ndef input_size():\n    return 784\n\n\n@pytest.fixture\ndef n_classes():\n    return 10\n\n\n@pytest.fixture\ndef answer():\n    return 1\n\n\n@pytest.fixture\ndef X(input_size):\n    ops = get_current_ops()\n    return ops.alloc(shape=(1, input_size))\n\n\n@pytest.fixture\ndef Y(answer, n_classes):\n    ops = get_current_ops()\n    return to_categorical(ops.asarray1i([answer]), n_classes=n_classes)\n\n\n@pytest.fixture\ndef tf_model(n_hidden, input_size):\n    import tensorflow as tf\n\n    tf_model = tf.keras.Sequential(\n        [\n            tf.keras.layers.Dense(n_hidden, input_shape=(input_size,)),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dense(n_hidden, activation=""relu""),\n            tf.keras.layers.LayerNormalization(),\n            tf.keras.layers.Dense(10, activation=""softmax""),\n        ]\n    )\n    return tf_model\n\n\n@pytest.fixture\ndef model(tf_model):\n    return TensorFlowWrapper(tf_model)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_roundtrip_conversion():\n    import tensorflow as tf\n\n    xp_tensor = numpy.zeros((2, 3), dtype=""f"")\n    tf_tensor = xp2tensorflow(xp_tensor)\n    assert isinstance(tf_tensor, tf.Tensor)\n    new_xp_tensor = tensorflow2xp(tf_tensor)\n    assert numpy.array_equal(xp_tensor, new_xp_tensor)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_construction_requires_keras_model():\n    import tensorflow as tf\n\n    keras_model = tf.keras.Sequential([tf.keras.layers.Dense(12, input_shape=(12,))])\n    assert isinstance(TensorFlowWrapper(keras_model), Model)\n    with pytest.raises(ValueError):\n        TensorFlowWrapper(Linear(2, 3))\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_built_model(model, X, Y):\n    # built models are validated more and can perform useful operations:\n    assert model.predict(X) is not None\n    # Can print a keras summary\n    assert str(model.shims[0]) != """"\n    # They can de/serialized\n    assert model.from_bytes(model.to_bytes()) is not None\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_predict(model, X):\n    model.predict(X)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_train_overfits(model, X, Y, answer):\n    optimizer = Adam()\n    for i in range(100):\n        guesses, backprop = model(X, is_train=True)\n        d_guesses = (guesses - Y) / guesses.shape[0]\n        backprop(d_guesses)\n        model.finish_update(optimizer)\n    predicted = model.predict(X).argmax()\n    assert predicted == answer\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_accumulate_gradients(model, X, Y, answer):\n    import tensorflow as tf\n\n    optimizer = Adam()\n    gradients = []\n    for i in range(3):\n        guesses, backprop = model(X, is_train=True)\n        d_guesses = (guesses - Y) / guesses.shape[0]\n        backprop(d_guesses)\n        shim_grads = [tf.identity(var) for var in model.shims[0].gradients]\n        gradients.append(shim_grads)\n\n    # Apply the gradients\n    model.finish_update(optimizer)\n    assert model.shims[0].gradients is None\n\n    # Compare prev/next pairs and ensure their gradients have changed\n    for i in range(len(gradients)):\n        # Skip the first one\n        if i == 0:\n            continue\n        found_diff = False\n        curr_grads = gradients[i]\n        prev_grads = gradients[i - 1]\n        for curr, prev in zip(curr_grads, prev_grads):\n            if (prev != curr).numpy().any():\n                found_diff = True\n        assert found_diff is True\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_serialize_model_subclass(\n    X, Y, input_size, n_classes, answer\n):\n    import tensorflow as tf\n\n    input_shape = (1, input_size)\n    ops = get_current_ops()\n\n    @keras_subclass(\n        ""foo.v1"",\n        X=ops.alloc2f(*input_shape),\n        Y=to_categorical(ops.asarray1i([1]), n_classes=n_classes),\n        input_shape=input_shape,\n    )\n    class CustomKerasModel(tf.keras.Model):\n        def __init__(self, **kwargs):\n            super(CustomKerasModel, self).__init__(**kwargs)\n            self.in_dense = tf.keras.layers.Dense(\n                12, name=""in_dense"", input_shape=input_shape\n            )\n            self.out_dense = tf.keras.layers.Dense(\n                n_classes, name=""out_dense"", activation=""softmax""\n            )\n\n        def call(self, inputs) -> tf.Tensor:\n            x = self.in_dense(inputs)\n            return self.out_dense(x)\n\n    model = TensorFlowWrapper(CustomKerasModel())\n    # Train the model to predict the right single answer\n    optimizer = Adam()\n    for i in range(50):\n        guesses, backprop = model(X, is_train=True)\n        d_guesses = (guesses - Y) / guesses.shape[0]\n        backprop(d_guesses)\n        model.finish_update(optimizer)\n    predicted = model.predict(X).argmax()\n    assert predicted == answer\n\n    # Save then Load the model from bytes\n    model.from_bytes(model.to_bytes())\n\n    # The from_bytes model gets the same answer\n    assert model.predict(X).argmax() == answer\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_keras_subclass_decorator_compile_args():\n    import tensorflow as tf\n\n    class UndecoratedModel(tf.keras.Model):\n        def call(self, inputs):\n            return inputs\n\n    # Can\'t wrap an undecorated keras subclass model\n    with pytest.raises(ValueError):\n        TensorFlowWrapper(UndecoratedModel())\n\n    @keras_subclass(\n        ""TestModel"",\n        X=numpy.array([0.0, 0.0]),\n        Y=numpy.array([0.5]),\n        input_shape=(2,),\n        compile_args={""loss"": ""binary_crossentropy""},\n    )\n    class TestModel(tf.keras.Model):\n        def call(self, inputs):\n            return inputs\n\n    model = TensorFlowWrapper(TestModel())\n    model = model.from_bytes(model.to_bytes())\n\n    assert model.shims[0]._model.loss == ""binary_crossentropy""\n    assert isinstance(model, Model)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_keras_subclass_decorator():\n    import tensorflow as tf\n\n    class UndecoratedModel(tf.keras.Model):\n        def call(self, inputs):\n            return inputs\n\n    # Can\'t wrap an undecorated keras subclass model\n    with pytest.raises(ValueError):\n        TensorFlowWrapper(UndecoratedModel())\n\n    @keras_subclass(\n        ""TestModel"", X=numpy.array([0.0, 0.0]), Y=numpy.array([0.5]), input_shape=(2,)\n    )\n    class TestModel(tf.keras.Model):\n        def call(self, inputs):\n            return inputs\n\n    # Can wrap an decorated keras subclass model\n    assert isinstance(TensorFlowWrapper(TestModel()), Model)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_keras_subclass_decorator_capture_args_kwargs(\n    X, Y, input_size, n_classes, answer\n):\n    import tensorflow as tf\n\n    @keras_subclass(\n        ""TestModel"", X=numpy.array([0.0, 0.0]), Y=numpy.array([0.5]), input_shape=(2,)\n    )\n    class TestModel(tf.keras.Model):\n        def __init__(self, custom=False, **kwargs):\n            super().__init__(self)\n            # This is to force the mode to pass the captured arguments\n            # or fail.\n            assert custom is True\n            assert kwargs.get(""other"", None) is not None\n\n        def call(self, inputs):\n            return inputs\n\n    # Can wrap an decorated keras subclass model\n    model = TensorFlowWrapper(TestModel(True, other=1337))\n\n    assert hasattr(model.shims[0]._model, ""eg_args"")\n    args_kwargs = model.shims[0]._model.eg_args\n    assert True in args_kwargs.args\n    assert ""other"" in args_kwargs.kwargs\n\n    # Raises an error if the args/kwargs is not serializable\n    obj = {}\n    obj[""key""] = obj\n    with pytest.raises(ValueError):\n        TensorFlowWrapper(TestModel(True, other=obj))\n\n    # Provides the same arguments when copying a capture model\n    model = model.from_bytes(model.to_bytes())\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_can_copy_model(model):\n    copy = model.copy()\n    assert copy is not None\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_print_summary(model, X):\n    summary = str(model.shims[0])\n    # Summary includes the layers of our model\n    assert ""layer_normalization"" in summary\n    assert ""dense"" in summary\n    # And counts of params\n    assert ""Total params"" in summary\n    assert ""Trainable params"" in summary\n    assert ""Non-trainable params"" in summary\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_to_bytes(model, X):\n    # And can be serialized\n    model_bytes = model.to_bytes()\n    assert model_bytes is not None\n    model.from_bytes(model_bytes)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_to_from_disk(model, X, Y, answer):\n    with make_tempdir() as tmp_path:\n        model_file = tmp_path / ""model.h5""\n        model.to_disk(model_file)\n        another_model = model.from_disk(model_file)\n        assert another_model is not None\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_from_bytes(model, X):\n    model.predict(X)\n    model_bytes = model.to_bytes()\n    another_model = model.from_bytes(model_bytes)\n    assert another_model is not None\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_use_params(model, X, Y, answer):\n    optimizer = Adam()\n    with model.use_params(optimizer.averages):\n        assert model.predict(X).argmax() is not None\n    for i in range(10):\n        guesses, backprop = model.begin_update(X)\n        d_guesses = (guesses - Y) / guesses.shape[0]\n        backprop(d_guesses)\n        model.finish_update(optimizer)\n    with model.use_params(optimizer.averages):\n        predicted = model.predict(X).argmax()\n    assert predicted == answer\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_to_cpu(tf_model):\n    model = TensorFlowWrapper(tf_model)\n    model.to_cpu()\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\n@pytest.mark.skipif(not has_cupy, reason=""needs cupy"")\ndef test_tensorflow_wrapper_to_gpu(model, X):\n    model.to_gpu(0)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\n@pytest.mark.parametrize(\n    ""data,n_args,kwargs_keys"",\n    [\n        # fmt: off\n        (numpy.zeros((2, 3), dtype=""f""), 1, []),\n        ([numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")], 2, []),\n        ((numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")), 2, []),\n        ({""a"": numpy.zeros((2, 3), dtype=""f""), ""b"": numpy.zeros((2, 3), dtype=""f"")}, 0, [""a"", ""b""]),\n        (ArgsKwargs((numpy.zeros((2, 3), dtype=""f""), numpy.zeros((2, 3), dtype=""f"")), {""c"": numpy.zeros((2, 3), dtype=""f"")}), 2, [""c""]),\n        # fmt: on\n    ],\n)\ndef test_tensorflow_wrapper_convert_inputs(data, n_args, kwargs_keys):\n    import tensorflow as tf\n\n    keras_model = tf.keras.Sequential([tf.keras.layers.Dense(12, input_shape=(12,))])\n    model = TensorFlowWrapper(keras_model)\n    convert_inputs = model.attrs[""convert_inputs""]\n    Y, backprop = convert_inputs(model, data, is_train=True)\n    check_input_converters(Y, backprop, data, n_args, kwargs_keys, tf.Tensor)\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_thinc_model_subclass(tf_model):\n    class CustomModel(Model):\n        def fn(self):\n            return 1337\n\n    model = TensorFlowWrapper(tf_model, model_class=CustomModel)\n    assert isinstance(model, CustomModel)\n    assert model.fn() == 1337\n\n\n@pytest.mark.skipif(not has_tensorflow, reason=""needs TensorFlow"")\ndef test_tensorflow_wrapper_thinc_set_model_name(tf_model):\n    model = TensorFlowWrapper(tf_model, model_name=""cool"")\n    assert model.name == ""cool""\n'"
thinc/tests/layers/test_transforms.py,0,"b'from thinc.api import strings2arrays, NumpyOps, Ragged, registry\nimport numpy\nimport pytest\n\nfrom ..util import get_data_checker\n\n\n@pytest.fixture(params=[[], [(10, 2)], [(5, 3), (1, 3)], [(2, 3), (0, 3), (1, 3)]])\ndef shapes(request):\n    return request.param\n\n\n@pytest.fixture\ndef ops():\n    return NumpyOps()\n\n\n@pytest.fixture\ndef list_data(shapes):\n    return [numpy.zeros(shape, dtype=""f"") for shape in shapes]\n\n\n@pytest.fixture\ndef ragged_data(ops, list_data):\n    lengths = numpy.array([len(x) for x in list_data], dtype=""i"")\n    if not list_data:\n        return Ragged(ops.alloc2f(0, 0), lengths)\n    else:\n        return Ragged(ops.flatten(list_data), lengths)\n\n\n@pytest.fixture\ndef padded_data(ops, list_data):\n    return ops.list2padded(list_data)\n\n\n@pytest.fixture\ndef array_data(ragged_data):\n    return ragged_data.data\n\n\ndef check_transform(transform, in_data, out_data):\n    model = registry.make_from_config({""config"": {""@layers"": transform}})[""config""]\n    input_checker = get_data_checker(in_data)\n    output_checker = get_data_checker(out_data)\n    model.initialize(in_data, out_data)\n    Y, backprop = model(in_data, is_train=True)\n    output_checker(Y, out_data)\n    dX = backprop(Y)\n    input_checker(dX, in_data)\n\n\ndef test_list2array(list_data, array_data):\n    check_transform(""list2array.v1"", list_data, array_data)\n\n\ndef test_list2ragged(list_data, ragged_data):\n    check_transform(""list2ragged.v1"", list_data, ragged_data)\n\n\ndef test_list2padded(list_data, padded_data):\n    check_transform(""list2padded.v1"", list_data, padded_data)\n\n\ndef test_ragged2list(ragged_data, list_data):\n    check_transform(""ragged2list.v1"", ragged_data, list_data)\n\n\ndef test_padded2list(padded_data, list_data):\n    check_transform(""padded2list.v1"", padded_data, list_data)\n\n\ndef test_strings2arrays():\n    strings = [""hello"", ""world""]\n    model = strings2arrays()\n    Y, backprop = model.begin_update(strings)\n    assert len(Y) == len(strings)\n    assert backprop([]) == []\n'"
thinc/tests/layers/test_uniqued.py,0,"b'import pytest\nimport numpy\nfrom thinc.layers import Embed\nfrom ...layers.uniqued import uniqued\nfrom numpy.testing import assert_allclose\nfrom hypothesis import given\nfrom hypothesis.strategies import integers, lists, composite\n\nROWS = 10\n\n# This test uses a newer hypothesis feature than the skanky flatmap-style\n# I used previously. This is much nicer, although it still takes some getting\n# used to. The key feature is this composite decorator. It injects a function,\n# \'draw\'.\n@composite\ndef lists_of_integers(draw, columns=2, lo=0, hi=ROWS - 1):\n    # We call draw to get example values, which we can manipulate.\n    # Here we get a list of integers, where each member of the list\n    # should be between a min and max value.\n    int_list = draw(lists(integers(min_value=lo, max_value=hi)))\n    # Now we can use this int list to make an array, and it\'ll be the arrays\n    # that our functions receive.\n    # We trim the list, so we\'re of length divisible by columns.\n    int_list = int_list[len(int_list) % columns :]\n    # And make the array and reshape it.\n    array = numpy.array(int_list, dtype=""uint64"")\n    return array.reshape((-1, columns))\n\n\n@pytest.fixture\ndef model(nO=128):\n    return Embed(nO, ROWS, column=0).initialize()\n\n\ndef test_uniqued_calls_init():\n    calls = []\n    embed = Embed(5, 5, column=0)\n    embed.init = lambda *args, **kwargs: calls.append(True)\n    embed.initialize()\n    assert calls == [True]\n    uembed = uniqued(embed)\n    uembed.initialize()\n    assert calls == [True, True]\n\n\n@given(X=lists_of_integers(lo=0, hi=ROWS - 1))\ndef test_uniqued_doesnt_change_result(model, X):\n    umodel = uniqued(model, column=model.attrs[""column""]).initialize()\n    Y, bp_Y = model(X, is_train=True)\n    Yu, bp_Yu = umodel(X, is_train=True)\n    assert_allclose(Y, Yu)\n    dX = bp_Y(Y)\n    dXu = bp_Yu(Yu)\n    assert_allclose(dX, dXu)\n    if X.size:\n        pass\n        # TODO: This test is a problem, because we exceed the embedding table.\n        # Fix it with a better cap.\n        # Check that different inputs do give different results\n        # Z, bp_Z = model(X + 1, is_train=True)\n        # with pytest.raises(AssertionError):\n        #    assert_allclose(Y, Z)\n'"
thinc/tests/layers/test_with_debug.py,0,"b'from mock import MagicMock\nfrom thinc.api import with_debug, Linear\n\n\ndef test_with_debug():\n    on_init = MagicMock()\n    on_forward = MagicMock()\n    on_backprop = MagicMock()\n    model = with_debug(\n        Linear(), on_init=on_init, on_forward=on_forward, on_backprop=on_backprop\n    )\n    on_init.assert_not_called()\n    on_forward.assert_not_called()\n    on_backprop.assert_not_called()\n    X = model.ops.alloc2f(1, 1)\n    Y = model.ops.alloc2f(1, 1)\n    model.initialize(X=X, Y=Y)\n    on_init.assert_called_once_with(model, X, Y)\n    on_forward.assert_not_called()\n    on_backprop.assert_not_called()\n    Yh, backprop = model(X, is_train=True)\n    on_forward.assert_called_once_with(model, X, True)\n    on_backprop.assert_not_called()\n    backprop(Y)\n    on_backprop.assert_called_once_with(Y)\n'"
thinc/tests/layers/test_with_transforms.py,0,"b'import pytest\nimport numpy\nfrom thinc.api import NumpyOps, Model, Linear\nfrom thinc.api import with_array, with_padded, with_list, with_ragged, with_getitem\nfrom thinc.types import Padded, Ragged\n\n\nfrom ..util import get_data_checker\n\n\n@pytest.fixture(params=[[], [(10, 2)], [(5, 3), (1, 3)], [(2, 3), (0, 3), (1, 3)]])\ndef shapes(request):\n    return request.param\n\n\n@pytest.fixture\ndef ops():\n    return NumpyOps()\n\n\n@pytest.fixture\ndef list_input(shapes):\n    return [numpy.zeros(shape, dtype=""f"") for shape in shapes]\n\n\n@pytest.fixture\ndef ragged_input(ops, list_input):\n    lengths = numpy.array([len(x) for x in list_input], dtype=""i"")\n    if not list_input:\n        return Ragged(ops.alloc2f(0, 0), lengths)\n    else:\n        return Ragged(ops.flatten(list_input), lengths)\n\n\n@pytest.fixture\ndef padded_input(ops, list_input):\n    return ops.list2padded(list_input)\n\n\n@pytest.fixture\ndef array_input(ragged_input):\n    return ragged_input.data\n\n\n@pytest.fixture\ndef padded_data_input(padded_input):\n    x = padded_input\n    return (x.data, x.size_at_t, x.lengths, x.indices)\n\n\n@pytest.fixture\ndef ragged_data_input(ragged_input):\n    return (ragged_input.data, ragged_input.lengths)\n\n\n# As an example operation, lets just trim the last dimension. That\n# should catch stuff that confuses the input and output.\n\n\ndef get_array_model():\n    def _trim_array_forward(model, X, is_train):\n        def backprop(dY):\n            return model.ops.alloc2f(dY.shape[0], dY.shape[1] + 1)\n\n        return X[:, :-1], backprop\n\n    return with_array(Model(""trimarray"", _trim_array_forward))\n\n\ndef get_list_model():\n    def _trim_list_forward(model, Xs, is_train):\n        def backprop(dYs):\n            dXs = []\n            for dY in dYs:\n                dXs.append(model.ops.alloc2f(dY.shape[0], dY.shape[1] + 1))\n            return dXs\n\n        Ys = [X[:, :-1] for X in Xs]\n        return Ys, backprop\n\n    return with_list(Model(""trimlist"", _trim_list_forward))\n\n\ndef get_padded_model():\n    def _trim_padded_forward(model, Xp, is_train):\n        def backprop(dYp):\n            dY = dYp.data\n            dX = model.ops.alloc3f(dY.shape[0], dY.shape[1], dY.shape[2] + 1)\n            return Padded(dX, dYp.size_at_t, dYp.lengths, dYp.indices)\n\n        assert isinstance(Xp, Padded)\n        X = Xp.data\n        X = X.reshape((X.shape[0] * X.shape[1], X.shape[2]))\n        X = X[:, :-1]\n        X = X.reshape((Xp.data.shape[0], Xp.data.shape[1], X.shape[1]))\n        return Padded(X, Xp.size_at_t, Xp.lengths, Xp.indices), backprop\n\n    return with_padded(Model(""trimpadded"", _trim_padded_forward))\n\n\ndef get_ragged_model():\n    def _trim_ragged_forward(model, Xr, is_train):\n        def backprop(dYr):\n            dY = dYr.data\n            dX = model.ops.alloc2f(dY.shape[0], dY.shape[1] + 1)\n            return Ragged(dX, dYr.lengths)\n\n        return Ragged(Xr.data[:, :-1], Xr.lengths), backprop\n\n    return with_ragged(Model(""trimragged"", _trim_ragged_forward))\n\n\ndef check_initialize(model, inputs):\n    # Just check that these run and don\'t hit errors. I guess we should add a\n    # spy and check that model.layers[0].initialize gets called, but shrug?\n    model.initialize()\n    model.initialize(X=inputs)\n    model.initialize(X=inputs, Y=model.predict(inputs))\n\n\ndef check_transform_produces_correct_output_type_forward(model, inputs, checker):\n    # It\'s pretty redundant to check these three assertions, so if the tests\n    # get slow this could be removed. I think it should be fine though?\n    outputs = model.predict(inputs)\n    assert checker(inputs, outputs)\n    outputs, _ = model(inputs, is_train=True)\n    assert checker(inputs, outputs)\n    outputs, _ = model(inputs, is_train=False)\n    assert checker(inputs, outputs)\n\n\ndef check_transform_produces_correct_output_type_backward(model, inputs, checker):\n    # It\'s pretty redundant to check these three assertions, so if the tests\n    # get slow this could be removed. I think it should be fine though?\n    outputs, backprop = model.begin_update(inputs)\n    d_inputs = backprop(outputs)\n    assert checker(inputs, d_inputs)\n\n\ndef test_with_array_initialize(ragged_input, padded_input, list_input, array_input):\n    for inputs in (ragged_input, padded_input, list_input, array_input):\n        check_initialize(get_array_model(), inputs)\n\n\ndef test_with_padded_initialize(\n    ragged_input, padded_input, list_input, padded_data_input\n):\n    for inputs in (ragged_input, padded_input, list_input, padded_data_input):\n        check_initialize(get_padded_model(), inputs)\n\n\ndef test_with_list_initialize(ragged_input, padded_input, list_input):\n    for inputs in (ragged_input, padded_input, list_input):\n        check_initialize(get_list_model(), inputs)\n\n\ndef test_with_ragged_initialize(\n    ragged_input, padded_input, list_input, ragged_data_input\n):\n    for inputs in (ragged_input, padded_input, list_input, ragged_data_input):\n        check_initialize(get_ragged_model(), inputs)\n\n\ndef test_with_array_forward(ragged_input, padded_input, list_input, array_input):\n    for inputs in (ragged_input, padded_input, list_input, array_input):\n        checker = get_data_checker(inputs)\n        model = get_array_model()\n        check_transform_produces_correct_output_type_forward(model, inputs, checker)\n\n\ndef test_with_list_forward(ragged_input, padded_input, list_input):\n    for inputs in (ragged_input, padded_input, list_input):\n        checker = get_data_checker(inputs)\n        model = get_list_model()\n        check_transform_produces_correct_output_type_forward(model, inputs, checker)\n\n\ndef test_with_padded_forward(ragged_input, padded_input, list_input, padded_data_input):\n    for inputs in (ragged_input, padded_input, list_input, padded_data_input):\n        checker = get_data_checker(inputs)\n        model = get_padded_model()\n        check_transform_produces_correct_output_type_forward(model, inputs, checker)\n\n\ndef test_with_ragged_forward(ragged_input, padded_input, list_input, ragged_data_input):\n    for inputs in (ragged_input, padded_input, list_input, ragged_data_input):\n        checker = get_data_checker(inputs)\n        model = get_ragged_model()\n        check_transform_produces_correct_output_type_forward(model, inputs, checker)\n\n\ndef test_with_array_backward(ragged_input, padded_input, list_input, array_input):\n    for inputs in (ragged_input, padded_input, list_input, array_input):\n        checker = get_data_checker(inputs)\n        model = get_array_model()\n        check_transform_produces_correct_output_type_backward(model, inputs, checker)\n\n\ndef test_with_list_backward(ragged_input, padded_input, list_input):\n    for inputs in (ragged_input, padded_input, list_input):\n        checker = get_data_checker(inputs)\n        model = get_list_model()\n        check_transform_produces_correct_output_type_backward(model, inputs, checker)\n\n\ndef test_with_ragged_backward(\n    ragged_input, padded_input, list_input, ragged_data_input\n):\n    for inputs in (ragged_input, padded_input, list_input, ragged_data_input):\n        checker = get_data_checker(inputs)\n        model = get_ragged_model()\n        check_transform_produces_correct_output_type_backward(model, inputs, checker)\n\n\ndef test_with_padded_backward(\n    ragged_input, padded_input, list_input, padded_data_input\n):\n    for inputs in (ragged_input, padded_input, list_input, padded_data_input):\n        checker = get_data_checker(inputs)\n        model = get_padded_model()\n        check_transform_produces_correct_output_type_backward(model, inputs, checker)\n\n\ndef test_with_getitem():\n    data = (\n        numpy.asarray([[1, 2, 3, 4]], dtype=""f""),\n        numpy.asarray([[5, 6, 7, 8]], dtype=""f""),\n    )\n    model = with_getitem(1, Linear())\n    model.initialize(data, data)\n    Y, backprop = model.begin_update(data)\n    assert len(Y) == len(data)\n    assert numpy.array_equal(Y[0], data[0])  # the other item stayed the same\n    assert not numpy.array_equal(Y[1], data[1])\n    dX = backprop(Y)\n    assert numpy.array_equal(dX[0], data[0])\n    assert not numpy.array_equal(dX[1], data[1])\n'"
thinc/tests/model/__init__.py,0,b''
thinc/tests/model/test_jax_model.py,0,"b'import pytest\nimport contextlib\nfrom numpy.testing import assert_allclose\nimport thinc.api\nfrom thinc.api import has_jax\nfrom thinc.backends import jax_jit\n\n\n@contextlib.contextmanager\ndef with_current_ops(ops):\n    prev = thinc.api.get_current_ops()\n    thinc.api.set_current_ops(ops)\n    yield\n    thinc.api.set_current_ops(prev)\n\n\ndef get_batch(ops, nB, nO, nI):\n    X = ops.xp.zeros((nB, nI), dtype=""f"")\n    X += ops.xp.random.uniform(-1, 1, X.shape)\n    Y = ops.xp.zeros((nB, nO), dtype=""f"")\n    Y += ops.xp.random.uniform(-1, 1, Y.shape)\n    return X, Y\n\n\ndef make_linear(nO, nI):\n    model = thinc.api.Linear(nO, nI).initialize()\n    model.attrs[""registry_name""] = ""Linear.v1""\n    return model\n\n\n@jax_jit()\ndef accepts_thinc_model(model):\n    return model.get_param(""W"").sum()\n\n\n@pytest.mark.skipif(not has_jax, reason=""needs Jax"")\ndef test_jax_jit_function_accepts_model():\n    with with_current_ops(thinc.api.JaxOps()):\n        model = make_linear(2, 2)\n        sum_W = accepts_thinc_model(model)\n        assert_allclose(float(sum_W), float(model.get_param(""W"").sum()), atol=1e-4)\n\n\n@pytest.mark.skipif(not has_jax, reason=""needs Jax"")\ndef test_jax_jit_linear_forward(nB=8, nI=4, nO=3):\n    with with_current_ops(thinc.api.JaxOps()):\n        model = make_linear(nO=nO, nI=nI)\n        X, Y = get_batch(model.ops, nB=nB, nO=nO, nI=nI)\n        Yh = model.predict(X)\n        model._func = jax_jit()(model._func)\n        Yh_jit = model.predict(X)\n        assert_allclose(Yh, Yh_jit)\n\n\n@pytest.mark.skipif(not has_jax, reason=""needs Jax"")\ndef test_jax_jit_static_arg_linear_forward(nB=8, nI=4, nO=3):\n    with with_current_ops(thinc.api.JaxOps()):\n        thinc.api.set_current_ops(thinc.api.JaxOps())\n        model = make_linear(nO=nO, nI=nI)\n        X, Y = get_batch(model.ops, nB=nB, nO=nO, nI=nI)\n        Yh = model.predict(X)\n        model._func = jax_jit(0)(model._func)\n        Yh_jit = model.predict(X)\n        assert_allclose(Yh, Yh_jit)\n'"
thinc/tests/model/test_model.py,0,"b'import pytest\nimport threading\nimport time\nimport ml_datasets\nfrom thinc.api import (\n    CupyOps,\n    prefer_gpu,\n    Linear,\n    Dropout,\n    Model,\n    Shim,\n    change_attr_values,\n)\nfrom thinc.api import set_dropout_rate, chain, Relu, Softmax, Adam\nimport numpy\n\nfrom ..util import make_tempdir\n\n\n@pytest.fixture\ndef model_with_no_args():\n    return Linear()\n\n\ndef create_model(name):\n    return Model(name, lambda X: (X, lambda dY: dY))\n\n\ndef test_model_defaults_to_cpu(model_with_no_args):\n    assert not isinstance(model_with_no_args.ops, CupyOps)\n\n\ndef test_models_get_different_ids(model_with_no_args):\n    model1 = Linear()\n    model2 = Linear()\n    assert model1.id != model2.id\n\n\ndef test_model_init():\n    class MyShim(Shim):\n        name = ""testshim""\n\n    model_a = create_model(""a"")\n    model = Model(\n        ""test"",\n        lambda X: (X, lambda dY: dY),\n        dims={""nI"": 10, ""nO"": None},\n        params={""W"": numpy.zeros((10,)), ""b"": None},\n        refs={""a"": model_a, ""b"": None},\n        attrs={""foo"": ""bar""},\n        shims=[MyShim(None)],\n        layers=[model_a, model_a],\n    )\n    assert model.has_param(""W"")\n    assert model.get_param(""W"").shape == (10,)\n    assert model.has_param(""b"") is None\n    with pytest.raises(KeyError):\n        model.get_param(""b"")\n    with pytest.raises(KeyError):\n        model.get_param(""X"")\n    model.set_param(""X"", numpy.zeros((10,)))\n    assert model.has_param(""X"")\n    assert model.get_param(""X"").shape == (10,)\n    with model.use_params({(model.id, ""X""): numpy.ones((10,))}):\n        assert numpy.array_equal(model.get_param(""X""), numpy.ones((10,)))\n    assert numpy.array_equal(model.get_param(""X""), numpy.zeros((10,)))\n    assert not model.has_grad(""W"")\n    assert not model.has_grad(""xyz"")\n    with pytest.raises(KeyError):\n        model.get_grad(""b"")\n    model.set_param(""W"", model.ops.alloc1f(10))\n    model.set_grad(""W"", model.ops.alloc1f(10))\n    with pytest.raises(ValueError):\n        model.inc_grad(""W"", numpy.zeros((5, 0)))\n    assert model.has_dim(""nI"")\n    assert model.get_dim(""nI"") == 10\n    with pytest.raises(KeyError):\n        model.get_dim(""xyz"")\n    with pytest.raises(ValueError):\n        model.get_dim(""nO"")\n    with pytest.raises(KeyError):\n        model.set_dim(""xyz"", 20)\n    with pytest.raises(ValueError):\n        model.set_dim(""nI"", 20)\n    assert model.has_ref(""a"")\n    assert model.get_ref(""a"").name == ""a""\n    assert not model.has_ref(""xyz"")\n    with pytest.raises(KeyError):\n        model.get_ref(""xyz"")\n    assert model.has_ref(""b"") is None\n    with pytest.raises(ValueError):\n        model.get_ref(""b"")\n    model.set_ref(""c"", model_a)\n    assert model.has_ref(""c"")\n    assert model.get_ref(""c"").name == ""a""\n    with pytest.raises(ValueError):\n        model.set_ref(""c"", create_model(""c""))\n    assert ""foo"" in model.attrs\n    assert ""bar"" not in model.attrs\n    assert model.attrs[""foo""] == ""bar""\n    with pytest.raises(KeyError):\n        model.attrs[""bar""]\n    model.attrs[""bar""] = ""baz""\n    model_copy = model.copy()\n    assert model_copy.name == ""test""\n\n\ndef test_param_names():\n    model = create_model(""tmp"")\n    assert model.param_names == tuple()\n    model.set_param(""param1"", None)\n    assert model.param_names == (""param1"",)\n    model.set_param(""param2"", None)\n    assert model.param_names == (""param1"", ""param2"")\n\n\ndef test_grad_names():\n    model = create_model(""tmp"")\n    assert model.grad_names == tuple()\n    model.set_param(""param1"", model.ops.alloc2f(4, 4))\n    model.set_grad(""param1"", model.ops.alloc2f(4, 4) + 1)\n    assert model.grad_names == (""param1"",)\n\n\ndef test_dim_names():\n    model = Linear(5, 3)\n    assert model.dim_names == (""nO"", ""nI"")\n\n\ndef test_model_set_reference():\n    parent = create_model(""parent"")\n    child = create_model(""child"")\n    grandchild = create_model(""child"")\n    parent.layers.append(child)\n    assert parent.ref_names == tuple()\n    parent.set_ref(""kid"", child)\n    assert parent.ref_names == (""kid"",)\n    assert parent.get_ref(""kid"") is child\n    child.layers.append(grandchild)\n    with pytest.raises(KeyError):\n        parent.get_ref(""grandkid"")\n    parent.set_ref(""grandkid"", grandchild)\n    assert parent.get_ref(""grandkid"") is grandchild\n    parent.remove_node(grandchild)\n    assert grandchild not in child.layers\n    assert not parent.has_ref(""grandkind"")\n\n\ndef test_model_can_save_to_disk(model_with_no_args):\n    with make_tempdir() as path:\n        model_with_no_args.to_disk(path / ""thinc_model"")\n\n\ndef test_model_can_load_from_disk(model_with_no_args):\n    with make_tempdir() as path:\n        model_with_no_args.to_disk(path / ""thinc_model"")\n        m2 = model_with_no_args.from_disk(path / ""thinc_model"")\n    assert model_with_no_args.to_bytes() == m2.to_bytes()\n\n\ndef test_change_attr_values(model_with_no_args):\n    model = model_with_no_args\n    model.name = ""target""\n    model.attrs[""has_var""] = False\n    change_attr_values(model, {""target"": {""has_var"": True, ""error"": True}})\n    assert model.attrs[""has_var""] is True\n    assert ""error"" not in model.attrs\n\n\ndef test_set_dropout():\n    model = Dropout()\n    assert model.attrs[""dropout_rate""] == 0.0\n    set_dropout_rate(model, 0.2)\n    assert model.attrs[""dropout_rate""] == 0.2\n\n\ndef test_set_dropout_2(model_with_no_args):\n    model = model_with_no_args\n    model.name = ""dropout""\n    model.attrs[""dropout_rate""] = 0.0\n    set_dropout_rate(model, 0.2)\n    assert model.attrs[""dropout_rate""] == 0.2\n\n\ndef test_bind_plus():\n    with Model.define_operators({""+"": lambda a, b: (a.name, b.name)}):\n        m = create_model(name=""a"") + create_model(name=""b"")\n        assert m == (""a"", ""b"")\n\n\ndef test_plus_chain():\n    with Model.define_operators({""+"": lambda a, b: a}):\n        m = (\n            create_model(name=""a"")\n            + create_model(name=""b"")\n            + create_model(name=""c"")\n            + create_model(name=""d"")\n        )\n        assert m.name == ""a""\n\n\ndef test_overload_operators_in_subthread():\n    """"""Test we can create a model in a child thread with overloaded operators.""""""\n    # Worker1 will start and run, while worker 2 sleeps after Model.define_operators.\n    # Without thread-safety, worker2 will find that its operator definitions\n    # have been removed, causing an error.\n    worker1 = threading.Thread(target=_overload_plus, args=(""+"", 0))\n    worker2 = threading.Thread(target=_overload_plus, args=(""*"", 1))\n    worker2.start()\n    worker1.start()\n    worker1.join()\n    worker2.join()\n\n    worker1 = threading.Thread(target=_overload_plus, args=(""+"", 1))\n    worker2 = threading.Thread(target=_overload_plus, args=(""*"", 0))\n    worker2.start()\n    worker1.start()\n    worker1.join()\n    worker2.join()\n\n\ndef _overload_plus(operator, sleep):\n    m1 = create_model(name=""a"")\n    m2 = create_model(name=""b"")\n    with Model.define_operators({operator: lambda a, b: a.name + b.name}):\n        time.sleep(sleep)\n        if operator == ""+"":\n            value = m1 + m2\n        else:\n            value = m1 * m2\n    assert value == ""ab""\n    assert Model._context_operators.get() == {}\n\n\ndef test_nested_operator_contexts():\n    m1 = create_model(name=""a"")\n    m2 = create_model(name=""b"")\n    assert Model._context_operators.get() == {}\n    with Model.define_operators({""+"": lambda a, b: a.name + b.name}):\n        value = m1 + m2\n        with pytest.raises(TypeError):\n            value = m1 * m2\n        with Model.define_operators({""*"": lambda a, b: a.name + b.name}):\n            with pytest.raises(TypeError):\n                value = m1 + m2\n            value = m1 * m2\n            with Model.define_operators({""-"": lambda a, b: a.name + b.name}):\n                with pytest.raises(TypeError):\n                    value = m1 + m2\n                value = m1 - m2\n            with pytest.raises(TypeError):\n                value = m1 + m2\n            value = m1 * m2\n        value = m1 + m2\n        with pytest.raises(TypeError):\n            value = m1 * m2\n    assert value == ""ab""\n    assert Model._context_operators.get() == {}\n\n\n@pytest.mark.parametrize(""op"", ""+ - * @ / // % ** << >> & ^ |"".split())\ndef test_all_operators(op):\n    m1 = Linear()\n    m2 = Linear()\n    with Model.define_operators({op: lambda a, b: a.name + b.name}):\n        if op == ""+"":\n            value = m1 + m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 + m2\n        if op == ""-"":\n            value = m1 - m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 - m2\n\n        if op == ""*"":\n            value = m1 * m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 * m2\n\n        if op == ""@"":\n            value = m1.__matmul__(m2)  # Be kind to Python 2...\n        else:\n            with pytest.raises(TypeError):\n                value = m1.__matmul__(m2)\n\n        if op == ""/"":\n            value = m1 / m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 / m2\n\n        if op == ""//"":\n            value = m1 // m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 // m2\n        if op == ""^"":\n            value = m1 ^ m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 ^ m2\n        if op == ""%"":\n            value = m1 % m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 % m2\n        if op == ""**"":\n            value = m1 ** m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 ** m2\n        if op == ""<<"":\n            value = m1 << m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 << m2\n        if op == "">>"":\n            value = m1 >> m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 >> m2\n        if op == ""&"":\n            value = m1 & m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 & m2\n        if op == ""^"":\n            value = m1 ^ m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 ^ m2\n        if op == ""|"":\n            value = m1 | m2\n        else:\n            with pytest.raises(TypeError):\n                value = m1 | m2  # noqa: F841\n    assert Model._context_operators.get() == {}\n\n\ndef test_unique_id_multithreading():\n    """"""Create a bunch of threads and assert they all get unique IDs""""""\n\n    list_of_ids = []\n\n    def get_model_id(id_list, index):\n        id_list.append(create_model(name=f""worker{index}"").id)\n\n    counter = 0\n    while len(list_of_ids) < 1000:\n        workers = []\n        for i in range(50):\n            w = threading.Thread(target=get_model_id, args=(list_of_ids, counter))\n            workers.append(w)\n            counter += 1\n        for w in workers:\n            w.start()\n        for w in workers:\n            w.join()\n\n    assert len(list_of_ids) == len(list(set(list_of_ids)))\n\n\ndef test_model_gpu():\n    prefer_gpu()\n    n_hidden = 32\n    dropout = 0.2\n    (train_X, train_Y), (dev_X, dev_Y) = ml_datasets.mnist()\n    model = chain(\n        Relu(nO=n_hidden, dropout=dropout),\n        Relu(nO=n_hidden, dropout=dropout),\n        Softmax(),\n    )\n    # making sure the data is on the right device\n    train_X = model.ops.asarray(train_X)\n    train_Y = model.ops.asarray(train_Y)\n    dev_X = model.ops.asarray(dev_X)\n    dev_Y = model.ops.asarray(dev_Y)\n\n    model.initialize(X=train_X[:5], Y=train_Y[:5])\n    optimizer = Adam(0.001)\n    batch_size = 128\n\n    for i in range(2):\n        batches = model.ops.multibatch(batch_size, train_X, train_Y, shuffle=True)\n        for X, Y in batches:\n            Yh, backprop = model.begin_update(X)\n            backprop(Yh - Y)\n            model.finish_update(optimizer)\n        # Evaluate and print progress\n        correct = 0\n        total = 0\n        for X, Y in model.ops.multibatch(batch_size, dev_X, dev_Y):\n            Yh = model.predict(X)\n            correct += (Yh.argmax(axis=1) == Y.argmax(axis=1)).sum()\n            total += Yh.shape[0]\n'"
thinc/tests/model/test_validation.py,0,"b'import pytest\nfrom thinc.api import chain, Relu, reduce_max, Softmax, with_ragged\nfrom thinc.api import ParametricAttention, list2ragged, reduce_sum\nfrom thinc.util import DataValidationError\n\n\ndef test_validation():\n    model = chain(Relu(10), Relu(10), with_ragged(reduce_max()), Softmax())\n    with pytest.raises(DataValidationError):\n        model.initialize(X=model.ops.alloc2f(1, 10), Y=model.ops.alloc2f(1, 10))\n    with pytest.raises(DataValidationError):\n        model.initialize(X=model.ops.alloc3f(1, 10, 1), Y=model.ops.alloc2f(1, 10))\n    with pytest.raises(DataValidationError):\n        model.initialize(X=[model.ops.alloc2f(1, 10)], Y=model.ops.alloc2f(1, 10))\n\n\ndef test_validation_complex():\n    good_model = chain(list2ragged(), reduce_sum(), Relu(12, dropout=0.5), Relu(1))\n    X = [good_model.ops.xp.zeros((4, 75), dtype=""f"")]\n    Y = good_model.ops.xp.zeros((1,), dtype=""f"")\n    good_model.initialize(X, Y)\n    good_model.predict(X)\n\n    bad_model = chain(\n        list2ragged(),\n        reduce_sum(),\n        Relu(12, dropout=0.5),\n        # ERROR: Why can\'t I attach a Relu to an attention layer?\n        ParametricAttention(12),\n        Relu(1),\n    )\n    with pytest.raises(DataValidationError):\n        bad_model.initialize(X, Y)\n'"
thinc/tests/mypy/__init__.py,0,b''
thinc/tests/mypy/test_mypy.py,0,"b'import os\nimport re\nfrom pathlib import Path\nimport shutil\nimport sys\n\nimport pytest\n\nfrom mypy import api as mypy_api\n\n# You can change the following variable to True during development to overwrite expected output with generated output\nGENERATE = False\n\ncases = [\n    (""mypy-plugin.ini"", ""success_plugin.py"", ""success-plugin.txt""),\n    (""mypy-plugin.ini"", ""fail_plugin.py"", ""fail-plugin.txt""),\n    (""mypy-default.ini"", ""success_no_plugin.py"", ""success-no-plugin.txt""),\n    (""mypy-default.ini"", ""fail_no_plugin.py"", ""fail-no-plugin.txt""),\n]\n\n\n@pytest.mark.parametrize(""config_filename,python_filename,output_filename"", cases)\ndef test_mypy_results(\n    config_filename, python_filename, output_filename, tmpdir, monkeypatch\n):\n    os.chdir(tmpdir)\n    root_dir = Path(__file__).parent\n    thinc_root_dir = Path(__file__).parent.parent.parent.parent\n    if ""--pyargs"" not in sys.argv:\n        monkeypatch.setenv(""MYPYPATH"", str(thinc_root_dir))\n    tmpdir_path = Path(tmpdir)\n\n    full_config_path: Path = root_dir / f""configs/{config_filename}""\n    full_module_path: Path = root_dir / f""modules/{python_filename}""\n    full_output_path: Path = root_dir / f""outputs/{output_filename}""\n\n    full_tmp_config_path: Path = tmpdir_path / config_filename\n    full_tmp_module_path: Path = tmpdir_path / python_filename\n\n    shutil.copy(str(full_config_path), tmpdir)\n    shutil.copy(str(full_module_path), tmpdir)\n\n    expected_out = """"\n    expected_err = """"\n    expected_returncode = 1\n    expected_out = full_output_path.read_text()\n\n    # Specifying a different cache dir for each configuration dramatically speeds up subsequent execution\n    # It also prevents cache-invalidation-related bugs in the tests\n    cache_dir = tmpdir_path / f"".mypy_cache/test-{config_filename[:-4]}""\n    command = [\n        str(full_tmp_module_path),\n        ""--config-file"",\n        str(full_tmp_config_path),\n        ""--cache-dir"",\n        str(cache_dir),\n        ""--show-error-codes"",\n    ]\n    print(\n        f""\\nExecuting: mypy {\' \'.join(command)}""\n    )  # makes it easier to debug as necessary\n    actual_result = mypy_api.run(command)\n    actual_out, actual_err, actual_returncode = actual_result\n    # Need to strip filenames due to differences in formatting by OS\n    actual_out = ""\\n"".join(\n        ["".py:"".join(line.split("".py:"")[1:]) for line in actual_out.split(""\\n"") if line]\n    ).strip()\n    actual_out = re.sub(r""\\n\\s*\\n"", r""\\n"", actual_out)\n\n    if GENERATE and output_filename is not None:\n        full_output_path.write_text(actual_out)\n    else:\n        assert actual_out.strip() == expected_out.strip(), actual_out\n\n    assert actual_err == expected_err\n    assert actual_returncode == expected_returncode\n\n\ndef test_generation_is_disabled():\n    """"""\n    Makes sure we don\'t accidentally leave generation on\n    """"""\n    assert not GENERATE\n'"
thinc/tests/regression/__init__.py,0,b''
thinc/tests/regression/test_issue208.py,0,"b'from thinc.api import chain, Linear\n\n\ndef test_issue208():\n    """"""Test issue that was caused by trying to flatten nested chains.""""""\n    layer1 = Linear(nO=9, nI=3)\n    layer2 = Linear(nO=12, nI=9)\n    layer3 = Linear(nO=5, nI=12)\n    model = chain(layer1, chain(layer2, layer3)).initialize()\n    assert model.get_dim(""nO"") == 5\n'"
thinc/tests/mypy/modules/__init__.py,0,b''
thinc/tests/mypy/modules/fail_no_plugin.py,0,"b'from thinc.api import chain, Relu, reduce_max, Softmax, add\n\nbad_model = chain(Relu(10), reduce_max(), Softmax())\n\nbad_model2 = add(Relu(10), reduce_max(), Softmax())\n'"
thinc/tests/mypy/modules/fail_plugin.py,0,"b'from thinc.api import chain, Relu, reduce_max, Softmax, add, concatenate\n\nbad_model = chain(Relu(10), reduce_max(), Softmax())\n\nbad_model2 = add(Relu(10), reduce_max(), Softmax())\n\nbad_model_only_plugin = chain(\n    Relu(10), Relu(10), Relu(10), Relu(10), reduce_max(), Softmax()\n)\n\nbad_model_only_plugin2 = add(\n    Relu(10), Relu(10), Relu(10), Relu(10), reduce_max(), Softmax()\n)\nreveal_type(bad_model_only_plugin2)\n\nbad_model_only_plugin3 = concatenate(\n    Relu(10), Relu(10), Relu(10), Relu(10), reduce_max(), Softmax()\n)\n\nreveal_type(bad_model_only_plugin3)\n'"
thinc/tests/mypy/modules/success_no_plugin.py,0,"b'from thinc.api import chain, Relu, reduce_max, Softmax, add\n\ngood_model = chain(Relu(10), Relu(10), Softmax())\nreveal_type(good_model)\n\ngood_model2 = add(Relu(10), Relu(10), Softmax())\nreveal_type(good_model2)\n\nbad_model_undetected = chain(Relu(10), Relu(10), reduce_max(), Softmax())\nreveal_type(bad_model_undetected)\n\nbad_model_undetected2 = add(Relu(10), Relu(10), reduce_max(), Softmax())\nreveal_type(bad_model_undetected2)\n'"
thinc/tests/mypy/modules/success_plugin.py,0,"b'from typing import Any, TypeVar\n\nfrom thinc.api import chain, Relu, reduce_max, Softmax, add, Model\n\ngood_model = chain(Relu(10), Relu(10), Softmax())\nreveal_type(good_model)\n\ngood_model2 = add(Relu(10), Relu(10), Softmax())\nreveal_type(good_model2)\n\nbad_model_undetected = chain(Relu(10), Relu(10), Relu(10), Relu(10), Softmax())\nreveal_type(bad_model_undetected)\n\nbad_model_undetected2 = add(Relu(10), Relu(10), Relu(10), Relu(10), Softmax())\nreveal_type(bad_model_undetected2)\n\n\ndef forward() -> None:\n    pass\n\n\nOtherType = TypeVar(""OtherType"")\n\n\ndef other_function(\n    layer1: Model, layer2: Model, *layers: Model\n) -> Model[Any, OtherType]:\n    return Model(""some_model"", forward)\n\n\nnon_combinator_model = other_function(\n    Model(""x"", forward), Model(""y"", forward), Model(""z"", forward)\n)\nreveal_type(non_combinator_model)\n'"
