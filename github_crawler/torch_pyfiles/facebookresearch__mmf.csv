file_path,api_count,code
setup.py,2,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport codecs\nimport os\nimport platform\nimport re\nfrom glob import glob\n\nimport setuptools\nfrom setuptools import Extension\nfrom setuptools.command.build_ext import build_ext\n\n\ndef clean_html(raw_html):\n    cleanr = re.compile(""<.*?>"")\n    cleantext = re.sub(cleanr, """", raw_html).strip()\n    return cleantext\n\n\n# Single sourcing code from here:\n# https://packaging.python.org/guides/single-sourcing-package-version/\ndef find_version(*file_paths):\n    here = os.path.abspath(os.path.dirname(__file__))\n\n    def read(*parts):\n        with codecs.open(os.path.join(here, *parts), ""r"") as fp:\n            return fp.read()\n\n    version_file = read(*file_paths)\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n\ndef fetch_long_description():\n    with open(""README.md"", encoding=""utf8"") as f:\n        readme = f.read()\n        # https://stackoverflow.com/a/12982689\n        readme = clean_html(readme)\n    return readme\n\n\ndef fetch_requirements():\n    requirements_file = ""requirements.txt""\n\n    if platform.system() == ""Windows"":\n        DEPENDENCY_LINKS.append(""https://download.pytorch.org/whl/torch_stable.html"")\n\n    with open(requirements_file) as f:\n        reqs = f.read()\n\n    reqs = reqs.strip().split(""\\n"")\n    reqs = remove_specific_requirements(reqs)\n    return reqs\n\n\ndef remove_specific_requirements(reqs):\n    rtd = ""READTHEDOCS"" in os.environ\n    excluded = {""fasttext"": rtd}\n    updated_reqs = []\n    for req in reqs:\n        without_version = req.split(""=="")[0]\n        if not excluded.get(without_version, False):\n            updated_reqs.append(req)\n    return updated_reqs\n\n\ndef fetch_files_from_folder(folder):\n    options = glob(f""{folder}/**"", recursive=True)\n    data_files = []\n    # All files inside the folder need to be added to package_data\n    # which would include yaml configs as well as project READMEs\n    for option in options:\n        if os.path.isdir(option):\n            files = []\n            for f in glob(os.path.join(option, ""*"")):\n                if os.path.isfile(f):\n                    files.append(f)\n                data_files += files\n    return data_files\n\n\ndef fetch_package_data():\n    current_dir = os.getcwd()\n    mmf_folder = os.path.dirname(os.path.abspath(__file__))\n    # The files for package data need to be relative to mmf package dir\n    os.chdir(os.path.join(mmf_folder, ""mmf""))\n    data_files = fetch_files_from_folder(""projects"")\n    data_files += fetch_files_from_folder(""tools"")\n    data_files += fetch_files_from_folder(""configs"")\n    data_files += glob(os.path.join(""utils"", ""phoc"", ""cphoc.*""))\n    os.chdir(current_dir)\n    return data_files\n\n\nDISTNAME = ""mmf""\nDESCRIPTION = ""mmf: a modular framework for vision and language multimodal \\\nresearch.""\nLONG_DESCRIPTION = fetch_long_description()\nLONG_DESCRIPTION_CONTENT_TYPE = ""text/markdown""\nAUTHOR = ""Facebook AI Research""\nAUTHOR_EMAIL = ""mmf@fb.com""\nDEPENDENCY_LINKS = []\nREQUIREMENTS = (fetch_requirements(),)\n# Need to exclude folders in tests as well so as they don\'t create an extra package\n# If something from tools is regularly used consider converting it into a cli command\nEXCLUDES = (""data"", ""docs"", ""tests"", ""tests.*"", ""tools"", ""tools.*"")\nCMD_CLASS = {""build_ext"": build_ext}\nEXT_MODULES = [\n    Extension(\n        ""mmf.utils.phoc.cphoc"", sources=[""mmf/utils/phoc/src/cphoc.c""], language=""c""\n    )\n]\n\nif ""READTHEDOCS"" in os.environ:\n    # Don\'t build extensions when generating docs\n    EXT_MODULES = []\n    CMD_CLASS.pop(""build_ext"", None)\n    # use CPU build of PyTorch\n    DEPENDENCY_LINKS.append(\n        ""https://download.pytorch.org/whl/cpu/torch-1.5.0%2B""\n        + ""cpu-cp36-cp36m-linux_x86_64.whl""\n    )\n\n\nif __name__ == ""__main__"":\n    setuptools.setup(\n        name=DISTNAME,\n        install_requires=REQUIREMENTS,\n        include_package_data=True,\n        package_data={""mmf"": fetch_package_data()},\n        packages=setuptools.find_packages(exclude=EXCLUDES),\n        python_requires="">=3.6"",\n        ext_modules=EXT_MODULES,\n        cmdclass=CMD_CLASS,\n        version=find_version(""mmf"", ""version.py""),\n        description=DESCRIPTION,\n        long_description=LONG_DESCRIPTION,\n        long_description_content_type=LONG_DESCRIPTION_CONTENT_TYPE,\n        author=AUTHOR,\n        author_email=AUTHOR_EMAIL,\n        dependency_links=DEPENDENCY_LINKS,\n        classifiers=[\n            ""Programming Language :: Python :: 3.6"",\n            ""Programming Language :: Python :: 3.7"",\n            ""Programming Language :: Python :: 3.8"",\n            ""License :: OSI Approved :: BSD License"",\n            ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n            ""Operating System :: OS Independent"",\n        ],\n        entry_points={\n            ""console_scripts"": [\n                ""mmf_run = mmf_cli.run:run"",\n                ""mmf_predict = mmf_cli.predict:predict"",\n                ""mmf_convert_hm = mmf_cli.hm_convert:main"",\n            ]\n        },\n    )\n'"
mmf/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# isort:skip_file\n# flake8: noqa: F401\n\nfrom mmf import utils, common, modules, datasets, models\nfrom mmf.modules import losses, schedulers, optimizers, metrics\nfrom mmf.version import __version__\n\n\n__all__ = [\n    ""utils"",\n    ""common"",\n    ""modules"",\n    ""datasets"",\n    ""models"",\n    ""losses"",\n    ""schedulers"",\n    ""optimizers"",\n    ""metrics"",\n]\n'"
mmf/version.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport sys\n\n__version__ = ""1.0.0rc8""\n\nmsg = ""MMF is only compatible with Python 3.6 and newer.""\n\n\nif sys.version_info < (3, 6):\n    raise ImportError(msg)\n'"
mmf_cli/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf_cli/hm_convert.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport argparse\nimport hashlib\nimport os\nimport subprocess\nimport zipfile\n\nfrom mmf.utils.configuration import Configuration\nfrom mmf.utils.download import decompress, move\nfrom mmf.utils.file_io import PathManager\n\n\nclass HMConverter:\n    IMAGE_FILES = [""img.tar.gz"", ""img""]\n    JSONL_FILES = [""train.jsonl"", ""dev.jsonl"", ""test.jsonl""]\n    POSSIBLE_CHECKSUMS = [\n        ""d8f1073f5fbf1b08a541cc2325fc8645619ab8ed768091fb1317d5c3a6653a77"",\n        ""a424c003b7d4ea3f3b089168b5f5ea73b90a3ff043df4b8ff4d7ed87c51cb572"",\n    ]\n\n    def __init__(self):\n        self.parser = self.get_parser()\n        self.args = self.parser.parse_args()\n        self.configuration = Configuration()\n\n    def assert_files(self, folder):\n        files_needed = self.JSONL_FILES\n\n        for file in files_needed:\n            assert PathManager.exists(\n                os.path.join(folder, ""data"", file)\n            ), f""{file} doesn\'t exist in {folder}""\n\n        files_needed = self.IMAGE_FILES\n\n        exists = False\n\n        for file in files_needed:\n            exists = exists or PathManager.exists(os.path.join(folder, ""data"", file))\n\n        if not exists:\n            raise AssertionError(""Neither img or img.tar.gz exists in current zip"")\n\n    def get_parser(self):\n        parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n\n        parser.add_argument(\n            ""--zip_file"",\n            required=True,\n            type=str,\n            help=""Zip file downloaded from the DrivenData"",\n        )\n\n        parser.add_argument(\n            ""--password"", required=True, type=str, help=""Password for the zip file""\n        )\n        parser.add_argument(\n            ""--mmf_data_folder"", required=None, type=str, help=""MMF Data folder""\n        )\n        parser.add_argument(\n            ""--bypass_checksum"",\n            required=None,\n            type=int,\n            help=""Pass 1 if you want to skip checksum"",\n        )\n        return parser\n\n    def convert(self):\n        config = self.configuration.get_config()\n        data_dir = config.env.data_dir\n\n        if self.args.mmf_data_folder:\n            data_dir = self.args.mmf_data_folder\n\n        bypass_checksum = False\n        if self.args.bypass_checksum:\n            bypass_checksum = bool(self.args.bypass_checksum)\n\n        print(f""Data folder is {data_dir}"")\n        print(f""Zip path is {self.args.zip_file}"")\n\n        base_path = os.path.join(data_dir, ""datasets"", ""hateful_memes"", ""defaults"")\n\n        images_path = os.path.join(base_path, ""images"")\n        PathManager.mkdirs(images_path)\n\n        if not bypass_checksum:\n            self.checksum(self.args.zip_file, self.POSSIBLE_CHECKSUMS)\n\n        src = self.args.zip_file\n        print(f""Moving {src}"")\n        dest = images_path\n        move(src, dest)\n\n        print(f""Unzipping {src}"")\n        self.decompress_zip(\n            dest, fname=os.path.basename(src), password=self.args.password\n        )\n\n        self.assert_files(images_path)\n\n        annotations_path = os.path.join(base_path, ""annotations"")\n        PathManager.mkdirs(annotations_path)\n        annotations = self.JSONL_FILES\n\n        for annotation in annotations:\n            print(f""Moving {annotation}"")\n            src = os.path.join(images_path, ""data"", annotation)\n            dest = annotations_path\n            move(src, dest)\n\n        images = self.IMAGE_FILES\n\n        for image_file in images:\n            src = os.path.join(images_path, ""data"", image_file)\n            if PathManager.exists(src):\n                print(f""Moving {image_file}"")\n            else:\n                continue\n            dest = images_path\n            move(src, dest)\n            if src.endswith("".tar.gz""):\n                decompress(dest, fname=image_file, delete_original=False)\n\n    def checksum(self, file, hashes):\n        sha256_hash = hashlib.sha256()\n        destination = file\n\n        with PathManager.open(destination, ""rb"") as f:\n            print(""Starting checksum for {}"".format(os.path.basename(file)))\n            for byte_block in iter(lambda: f.read(65536), b""""):\n                sha256_hash.update(byte_block)\n            if sha256_hash.hexdigest() not in hashes:\n                # remove_dir(download_path)\n                raise AssertionError(\n                    f""Checksum of downloaded file does not match the expected ""\n                    + ""checksum. Please try again.""\n                )\n            else:\n                print(""Checksum successful"")\n\n    def decompress_zip(self, dest, fname, password=None):\n        path = os.path.join(dest, fname)\n        print(""Extracting the zip can take time. Sit back and relax."")\n        try:\n            # Python\'s zip file module is very slow with password encrypted files\n            # Try command line\n            command = [""unzip"", ""-q"", ""-d"", dest]\n            if password:\n                command += [""-P"", password]\n            command += [path]\n            subprocess.run(command, check=True)\n        except Exception:\n            obj = zipfile.ZipFile(path, ""r"")\n            if password:\n                obj.setpassword(password.encode(""utf-8""))\n            obj.extractall(path=dest)\n            obj.close()\n\n\ndef main():\n    converter = HMConverter()\n    converter.convert()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
mmf_cli/predict.py,0,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport sys\n\nfrom mmf_cli.run import run\n\n\ndef predict():\n    sys.argv.extend([""evaluation.predict=true""])\n    run(predict=True)\n\n\nif __name__ == ""__main__"":\n    predict()\n'"
mmf_cli/run.py,8,"b'#!/usr/bin/env python3 -u\n# Copyright (c) Facebook, Inc. and its affiliates.\nimport random\n\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.build import build_config, build_trainer\nfrom mmf.utils.configuration import Configuration\nfrom mmf.utils.distributed import distributed_init, infer_init_method\nfrom mmf.utils.env import set_seed, setup_imports\nfrom mmf.utils.flags import flags\nfrom mmf.utils.logger import Logger\n\n\ndef main(configuration, init_distributed=False, predict=False):\n    # A reload might be needed for imports\n    setup_imports()\n    configuration.import_user_dir()\n    config = configuration.get_config()\n\n    if torch.cuda.is_available():\n        torch.cuda.set_device(config.device_id)\n        torch.cuda.init()\n\n    if init_distributed:\n        distributed_init(config)\n\n    config.training.seed = set_seed(config.training.seed)\n    registry.register(""seed"", config.training.seed)\n    print(f""Using seed {config.training.seed}"")\n\n    config = build_config(configuration)\n\n    # Logger should be registered after config is registered\n    registry.register(""writer"", Logger(config, name=""mmf.train""))\n    trainer = build_trainer(config)\n    trainer.load()\n    if predict:\n        trainer.inference()\n    else:\n        trainer.train()\n\n\ndef distributed_main(device_id, configuration, predict=False):\n    config = configuration.get_config()\n    config.device_id = device_id\n\n    if config.distributed.rank is None:\n        config.distributed.rank = config.start_rank + device_id\n\n    main(configuration, init_distributed=True, predict=predict)\n\n\ndef run(predict=False):\n    setup_imports()\n    parser = flags.get_parser()\n    args = parser.parse_args()\n    print(args)\n    configuration = Configuration(args)\n    # Do set runtime args which can be changed by MMF\n    configuration.args = args\n    config = configuration.get_config()\n    config.start_rank = 0\n    if config.distributed.init_method is None:\n        infer_init_method(config)\n\n    if config.distributed.init_method is not None:\n        if torch.cuda.device_count() > 1 and not config.distributed.no_spawn:\n            config.start_rank = config.distributed.rank\n            config.distributed.rank = None\n            torch.multiprocessing.spawn(\n                fn=distributed_main,\n                args=(configuration, predict),\n                nprocs=torch.cuda.device_count(),\n            )\n        else:\n            distributed_main(0, configuration, predict)\n    elif config.distributed.world_size > 1:\n        assert config.distributed.world_size <= torch.cuda.device_count()\n        port = random.randint(10000, 20000)\n        config.distributed.init_method = f""tcp://localhost:{port}""\n        config.distributed.rank = None\n        torch.multiprocessing.spawn(\n            fn=distributed_main,\n            args=(configuration, predict),\n            nprocs=config.distributed.world_size,\n        )\n    else:\n        config.device_id = 0\n        main(configuration, predict=predict)\n\n\nif __name__ == ""__main__"":\n    run()\n'"
tests/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
tests/test_utils.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport argparse\nimport socket\nimport unittest\n\nimport torch\n\n\ndef compare_tensors(a, b):\n    return torch.equal(a, b)\n\n\ndef dummy_args(model=""cnn_lstm"", dataset=""clevr""):\n    args = argparse.Namespace()\n    args.opts = [f""model={model}"", f""dataset={dataset}""]\n    args.config_override = None\n    return args\n\n\ndef is_network_reachable():\n    try:\n        # check if host name can be resolved\n        host = socket.gethostbyname(""one.one.one.one"")\n        # check if host is actually reachable\n        s = socket.create_connection((host, 80), 2)\n        s.close()\n        return True\n    except OSError as e:\n        if e.errno == 101:\n            pass\n    return False\n\n\nNETWORK_AVAILABLE = is_network_reachable()\nCUDA_AVAILBLE = torch.cuda.is_available()\n\n\ndef skip_if_no_network(testfn, reason=""Network is not available""):\n    return unittest.skipUnless(NETWORK_AVAILABLE, reason)(testfn)\n\n\ndef skip_if_no_cuda(testfn, reason=""Cuda is not available""):\n    return unittest.skipUnless(CUDA_AVAILBLE, reason)(testfn)\n\n\ndef compare_state_dicts(a, b):\n    same = True\n    same = same and (list(a.keys()) == list(b.keys()))\n    if not same:\n        return same\n\n    for val1, val2 in zip(a.values(), b.values()):\n        if isinstance(val1, torch.Tensor):\n            same = same and compare_tensors(val1, val2)\n        elif not isinstance(val2, torch.Tensor):\n            same = same and val1 == val2\n        else:\n            same = False\n        if not same:\n            return same\n\n    return same\n'"
tools/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n#\n# mmf documentation build configuration file, created by\n# sphinx-quickstart on Tue Apr 23 10:42:55 2019.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nimport datetime\n\nimport pytorch_sphinx_theme\nfrom packaging.version import parse\nfrom recommonmark.transform import AutoStructify\n\nfrom mmf import version\n\nparsed_version = parse(version.__version__)\n\n\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.doctest"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.ifconfig"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.napoleon"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinxcontrib.programoutput"",\n    ""recommonmark"",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\nsource_suffix = ["".rst"", "".md""]\n# source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# General information about the project.\nproject = ""MMF""\ncopyright = str(datetime.datetime.now().year) + "", Facebook AI Research""\nauthor = ""Facebook AI Research""\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = parsed_version.base_version\n# The full version, including alpha/beta/rc tags.\nrelease = str(parsed_version)\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [""_build"", ""Thumbs.db"", "".DS_Store""]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = ""sphinx""\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""pytorch_sphinx_theme""\nhtml_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\ntemplates_path = [""_templates""]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    ""includehidden"": False,\n    ""canonical_url"": ""/"",\n    ""pytorch_project"": ""docs"",\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    ""**"": [\n        ""relations.html"",  # needs \'show_related\': True theme option to display\n        ""searchbox.html"",\n    ]\n}\n\nhtml_baseurl = ""/""\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""mmfdoc""\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, ""mmf.tex"", ""MMF Documentation"", ""Facebook AI Research"", ""manual"")\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""mmf"", ""MMF Documentation"", [author], 1)]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""mmf"",\n        ""MMF Documentation"",\n        author,\n        ""mmf"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    )\n]\n\ngithub_doc_root = ""https://github.com/facebookresearch/mmf/tree/master""\n\n\n# At the bottom of conf.py\ndef setup(app):\n    app.add_config_value(\n        ""recommonmark_config"",\n        {\n            ""url_resolver"": lambda url: github_doc_root + url,\n            ""auto_toc_tree_section"": ""Contents"",\n        },\n        True,\n    )\n    app.add_transform(AutoStructify)\n    app.add_css_file(""css/customize.css"")\n'"
mmf/common/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom .meter import Meter\nfrom .registry import registry\nfrom .sample import Sample, SampleList\n\n__all__ = [""Sample"", ""SampleList"", ""Meter"", ""registry""]\n'"
mmf/common/batch_collator.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.sample import SampleList\n\n\nclass BatchCollator:\n    def __init__(self, dataset_name, dataset_type):\n        self._dataset_name = dataset_name\n        self._dataset_type = dataset_type\n\n    def __call__(self, batch):\n        # Create and return sample list with proper name and type set\n        sample_list = SampleList(batch)\n        sample_list.dataset_name = self._dataset_name\n        sample_list.dataset_type = self._dataset_type\n        return sample_list\n'"
mmf/common/constants.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimdb_version = 1\nFASTTEXT_WIKI_URL = (\n    ""https://dl.fbaipublicfiles.com/pythia/pretrained_models/fasttext/wiki.en.bin""\n)\n\nCLEVR_DOWNLOAD_URL = ""https://dl.fbaipublicfiles.com/clevr/CLEVR_v1.0.zip""\n\nVISUAL_GENOME_CONSTS = {\n    ""imdb_url"": ""https://dl.fbaipublicfiles.com/pythia/data/imdb/visual_genome.tar.gz"",\n    ""features_url"": ""https://dl.fbaipublicfiles.com/pythia/features/visual_genome.tar.gz"",\n    ""synset_file"": ""vg_synsets.txt"",\n    ""vocabs"": ""https://dl.fbaipublicfiles.com/pythia/data/vocab.tar.gz"",\n}\n\nVISUAL_DIALOG_CONSTS = {\n    ""imdb_url"": {\n        ""train"": ""https://www.dropbox.com/s/ix8keeudqrd8hn8/visdial_1.0_train.zip?dl=1"",\n        ""val"": ""https://www.dropbox.com/s/ibs3a0zhw74zisc/visdial_1.0_val.zip?dl=1"",\n        ""test"": ""https://www.dropbox.com/s/ibs3a0zhw74zisc/visdial_1.0_test.zip?dl=1"",\n    },\n    ""features_url"": {\n        ""visual_dialog"": ""https://dl.fbaipublicfiles.com/pythia/features/visual_dialog.tar.gz"",\n        ""coco"": ""https://dl.fbaipublicfiles.com/pythia/features/coco.tar.gz"",\n    },\n    ""vocabs"": ""https://dl.fbaipublicfiles.com/pythia/data/vocab.tar.gz"",\n}\n\nDOWNLOAD_CHUNK_SIZE = 1024 * 1024\n'"
mmf/common/dataset_loader.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.common.sample import SampleList\nfrom mmf.common.test_reporter import TestReporter\nfrom mmf.datasets.multi_dataset_loader import MultiDatasetLoader\n\n\nclass DatasetLoader:\n    def __init__(self, config):\n        self.config = config\n\n    def load_datasets(self):\n        self.train_dataset = MultiDatasetLoader(""train"")\n        self.val_dataset = MultiDatasetLoader(""val"")\n        self.test_dataset = MultiDatasetLoader(""test"")\n\n        self.train_dataset.load(self.config)\n        self.val_dataset.load(self.config)\n        self.test_dataset.load(self.config)\n\n        # If number of datasets is one, this will return the first loader\n        self.train_loader = self.train_dataset\n        self.val_loader = self.val_dataset\n        self.test_loader = self.test_dataset\n\n        self.mapping = {\n            ""train"": self.train_dataset,\n            ""val"": self.val_dataset,\n            ""test"": self.test_dataset,\n        }\n\n        self.test_reporter = None\n        self.should_not_log = self.config.training.should_not_log\n\n    @property\n    def dataset_config(self):\n        return self._dataset_config\n\n    @dataset_config.setter\n    def dataset_config(self, config):\n        self._dataset_config = config\n\n    def get_config(self):\n        return self._dataset_config\n\n    def get_test_reporter(self, dataset_type):\n        dataset = getattr(self, f""{dataset_type}_dataset"")\n        return TestReporter(dataset)\n\n    def prepare_batch(self, batch, *args, **kwargs):\n        batch = SampleList(batch)\n        return self.mapping[batch.dataset_type].prepare_batch(batch)\n\n    def verbose_dump(self, report, *args, **kwargs):\n        if self.config.training.verbose_dump:\n            dataset_type = report.dataset_type\n            self.mapping[dataset_type].verbose_dump(report, *args, **kwargs)\n\n    def seed_sampler(self, dataset_type, seed):\n        dataset = getattr(self, f""{dataset_type}_dataset"")\n        dataset.seed_sampler(seed)\n'"
mmf/common/meter.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# Inspired from maskrcnn benchmark\nfrom collections import defaultdict, deque\n\nimport torch\n\n\nclass SmoothedValue:\n    """"""Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    """"""\n\n    def __init__(self, window_size=20):\n        self.window_size = window_size\n        self.reset()\n\n    def reset(self):\n        self.deque = deque(maxlen=self.window_size)\n        self.averaged_value_deque = deque(maxlen=self.window_size)\n        self.batch_sizes = deque(maxlen=self.window_size)\n        self.total_samples = 0\n        self.total = 0.0\n        self.count = 0\n\n    def update(self, value, batch_size):\n        self.deque.append(value * batch_size)\n        self.averaged_value_deque.append(value)\n        self.batch_sizes.append(batch_size)\n\n        self.count += 1\n        self.total_samples += batch_size\n        self.total += value * batch_size\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.averaged_value_deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque))\n        s = torch.tensor(list(self.batch_sizes))\n        return d.sum().item() / s.sum().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.total_samples\n\n    def get_latest(self):\n        return self.averaged_value_deque[-1]\n\n\nclass Meter:\n    def __init__(self, delimiter="", ""):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, update_dict, batch_size):\n        for k, v in update_dict.items():\n            if isinstance(v, torch.Tensor):\n                if v.dim() != 0:\n                    v = v.mean()\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v, batch_size)\n\n    def update_from_meter(self, meter):\n        for key, value in meter.meters.items():\n            assert isinstance(value, SmoothedValue)\n            self.meters[key] = value\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\n            ""\'{}\' object has no attribute \'{}\'"".format(type(self).__name__, attr)\n        )\n\n    def get_scalar_dict(self):\n        scalar_dict = {}\n        for k, v in self.meters.items():\n            scalar_dict[k] = v.get_latest()\n\n        return scalar_dict\n\n    def get_log_dict(self):\n        log_dict = {}\n        for k, v in self.meters.items():\n            if ""train"" in k:\n                log_dict[k] = f""{v.median:.4f}""\n                log_dict[f""{k}/avg""] = f""{v.global_avg:.4f}""\n            else:\n                log_dict[k] = f""{v.global_avg:.4f}""\n        return log_dict\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            if ""train"" in name:\n                loss_str.append(f""{name}: {meter.median:.4f} ({meter.global_avg:.4f})"")\n            else:\n                # In case of val print global avg\n                loss_str.append(f""{name}: {meter.global_avg:.4f}"")\n\n        return self.delimiter.join(loss_str)\n'"
mmf/common/registry.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\nRegistry is central source of truth in MMF. Inspired from Redux\'s\nconcept of global store, Registry maintains mappings of various information\nto unique keys. Special functions in registry can be used as decorators to\nregister different kind of classes.\n\nImport the global registry object using\n\n``from mmf.common.registry import registry``\n\nVarious decorators for registry different kind of classes with unique keys\n\n- Register a trainer: ``@registry.register_trainer``\n- Register a dataset builder: ``@registry.register_builder``\n- Register a metric: ``@registry.register_metric``\n- Register a loss: ``@registry.register_loss``\n- Register a fusion technique: ``@registery.register_fusion``\n- Register a model: ``@registry.register_model``\n- Register a processor: ``@registry.register_processor``\n- Register a optimizer: ``@registry.register_optimizer``\n- Register a scheduler: ``@registry.register_scheduler``\n- Register a decoder: ``@registry.register_decoder``\n""""""\nfrom mmf.utils.env import setup_imports\n\n\nclass Registry:\n    r""""""Class for registry object which acts as central source of truth\n    for MMF\n    """"""\n    mapping = {\n        # Mappings of builder name to their respective classes\n        # Use `registry.register_builder` to register a builder class\n        # with a specific name\n        # Further, use the name with the class is registered in the\n        # command line or configuration to load that specific dataset\n        ""builder_name_mapping"": {},\n        # Similar to the builder_name_mapping above except that this\n        # one is used to keep a mapping for dataset to its trainer class.\n        ""trainer_name_mapping"": {},\n        ""model_name_mapping"": {},\n        ""metric_name_mapping"": {},\n        ""loss_name_mapping"": {},\n        ""fusion_name_mapping"": {},\n        ""optimizer_name_mapping"": {},\n        ""scheduler_name_mapping"": {},\n        ""processor_name_mapping"": {},\n        ""decoder_name_mapping"": {},\n        ""state"": {},\n    }\n\n    @classmethod\n    def register_trainer(cls, name):\n        r""""""Register a trainer to registry with key \'name\'\n\n        Args:\n            name: Key with which the trainer will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from mmf.trainers.custom_trainer import CustomTrainer\n\n\n            @registry.register_trainer(""custom_trainer"")\n            class CustomTrainer():\n                ...\n\n        """"""\n\n        def wrap(trainer_cls):\n            cls.mapping[""trainer_name_mapping""][name] = trainer_cls\n            return trainer_cls\n\n        return wrap\n\n    @classmethod\n    def register_builder(cls, name):\n        r""""""Register a dataset builder to registry with key \'name\'\n\n        Args:\n            name: Key with which the metric will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from mmf.datasets.base_dataset_builder import BaseDatasetBuilder\n\n\n            @registry.register_builder(""vqa2"")\n            class VQA2Builder(BaseDatasetBuilder):\n                ...\n\n        """"""\n\n        def wrap(builder_cls):\n            from mmf.datasets.base_dataset_builder import BaseDatasetBuilder\n\n            assert issubclass(\n                builder_cls, BaseDatasetBuilder\n            ), ""All builders must inherit BaseDatasetBuilder class""\n            cls.mapping[""builder_name_mapping""][name] = builder_cls\n            return builder_cls\n\n        return wrap\n\n    @classmethod\n    def register_metric(cls, name):\n        r""""""Register a metric to registry with key \'name\'\n\n        Args:\n            name: Key with which the metric will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from mmf.modules.metrics import BaseMetric\n\n\n            @registry.register_metric(""r@1"")\n            class RecallAt1(BaseMetric):\n                ...\n\n        """"""\n\n        def wrap(func):\n            from mmf.modules.metrics import BaseMetric\n\n            assert issubclass(\n                func, BaseMetric\n            ), ""All Metric must inherit BaseMetric class""\n            cls.mapping[""metric_name_mapping""][name] = func\n            return func\n\n        return wrap\n\n    @classmethod\n    def register_loss(cls, name):\n        r""""""Register a loss to registry with key \'name\'\n\n        Args:\n            name: Key with which the loss will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from torch import nn\n\n            @registry.register_task(""logit_bce"")\n            class LogitBCE(nn.Module):\n                ...\n\n        """"""\n\n        def wrap(func):\n            from torch import nn\n\n            assert issubclass(\n                func, nn.Module\n            ), ""All loss must inherit torch.nn.Module class""\n            cls.mapping[""loss_name_mapping""][name] = func\n            return func\n\n        return wrap\n\n    @classmethod\n    def register_fusion(cls, name):\n        r""""""Register a fusion technique to registry with key \'name\'\n\n        Args:\n            name: Key with which the fusion technique will be registered\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from torch import nn\n\n            @registry.register_fusion(""linear_sum"")\n            class LinearSum():\n                ...\n        """"""\n\n        def wrap(func):\n            from torch import nn\n\n            assert issubclass(\n                func, nn.Module\n            ), ""All Fusion must inherit torch.nn.Module class""\n            cls.mapping[""fusion_name_mapping""][name] = func\n            return func\n\n        return wrap\n\n    @classmethod\n    def register_model(cls, name):\n        r""""""Register a model to registry with key \'name\'\n\n        Args:\n            name: Key with which the model will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from mmf.models.base_model import BaseModel\n\n            @registry.register_task(""pythia"")\n            class Pythia(BaseModel):\n                ...\n        """"""\n\n        def wrap(func):\n            from mmf.models.base_model import BaseModel\n\n            assert issubclass(\n                func, BaseModel\n            ), ""All models must inherit BaseModel class""\n            cls.mapping[""model_name_mapping""][name] = func\n            return func\n\n        return wrap\n\n    @classmethod\n    def register_processor(cls, name):\n        r""""""Register a processor to registry with key \'name\'\n\n        Args:\n            name: Key with which the processor will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from mmf.datasets.processors import BaseProcessor\n\n            @registry.register_task(""glove"")\n            class GloVe(BaseProcessor):\n                ...\n\n        """"""\n\n        def wrap(func):\n            from mmf.datasets.processors.processors import BaseProcessor\n\n            assert issubclass(\n                func, BaseProcessor\n            ), ""All Processor classes must inherit BaseProcessor class""\n            cls.mapping[""processor_name_mapping""][name] = func\n            return func\n\n        return wrap\n\n    @classmethod\n    def register_optimizer(cls, name):\n        def wrap(func):\n            cls.mapping[""optimizer_name_mapping""][name] = func\n            return func\n\n        return wrap\n\n    @classmethod\n    def register_scheduler(cls, name):\n        def wrap(func):\n            cls.mapping[""scheduler_name_mapping""][name] = func\n            return func\n\n        return wrap\n\n    @classmethod\n    def register_decoder(cls, name):\n        r""""""Register a decoder to registry with key \'name\'\n\n        Args:\n            name: Key with which the decoder will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n            from mmf.utils.text import TextDecoder\n\n\n            @registry.register_decoder(""nucleus_sampling"")\n            class NucleusSampling(TextDecoder):\n                ...\n\n        """"""\n\n        def wrap(decoder_cls):\n            from mmf.utils.text import TextDecoder\n\n            assert issubclass(\n                decoder_cls, TextDecoder\n            ), ""All decoders must inherit TextDecoder class""\n            cls.mapping[""decoder_name_mapping""][name] = decoder_cls\n            return decoder_cls\n\n        return wrap\n\n    @classmethod\n    def register(cls, name, obj):\n        r""""""Register an item to registry with key \'name\'\n\n        Args:\n            name: Key with which the item will be registered.\n\n        Usage::\n\n            from mmf.common.registry import registry\n\n            registry.register(""config"", {})\n        """"""\n        path = name.split(""."")\n        current = cls.mapping[""state""]\n\n        for part in path[:-1]:\n            if part not in current:\n                current[part] = {}\n            current = current[part]\n\n        current[path[-1]] = obj\n\n    @classmethod\n    def get_trainer_class(cls, name):\n        return cls.mapping[""trainer_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_builder_class(cls, name):\n        return cls.mapping[""builder_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_model_class(cls, name):\n        return cls.mapping[""model_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_processor_class(cls, name):\n        return cls.mapping[""processor_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_metric_class(cls, name):\n        return cls.mapping[""metric_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_loss_class(cls, name):\n        return cls.mapping[""loss_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_optimizer_class(cls, name):\n        return cls.mapping[""optimizer_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_scheduler_class(cls, name):\n        return cls.mapping[""scheduler_name_mapping""].get(name, None)\n\n    @classmethod\n    def get_decoder_class(cls, name):\n        return cls.mapping[""decoder_name_mapping""].get(name, None)\n\n    @classmethod\n    def get(cls, name, default=None, no_warning=False):\n        r""""""Get an item from registry with key \'name\'\n\n        Args:\n            name (string): Key whose value needs to be retrieved.\n            default: If passed and key is not in registry, default value will\n                     be returned with a warning. Default: None\n            no_warning (bool): If passed as True, warning when key doesn\'t exist\n                               will not be generated. Useful for MMF\'s\n                               internal operations. Default: False\n        Usage::\n\n            from mmf.common.registry import registry\n\n            config = registry.get(""config"")\n        """"""\n        original_name = name\n        name = name.split(""."")\n        value = cls.mapping[""state""]\n        for subname in name:\n            value = value.get(subname, default)\n            if value is default:\n                break\n\n        if (\n            ""writer"" in cls.mapping[""state""]\n            and value == default\n            and no_warning is False\n        ):\n            cls.mapping[""state""][""writer""].write(\n                ""Key {} is not present in registry, returning default value ""\n                ""of {}"".format(original_name, default)\n            )\n        return value\n\n    @classmethod\n    def unregister(cls, name):\n        r""""""Remove an item from registry with key \'name\'\n\n        Args:\n            name: Key which needs to be removed.\n        Usage::\n\n            from mmf.common.registry import registry\n\n            config = registry.unregister(""config"")\n        """"""\n        return cls.mapping[""state""].pop(name, None)\n\n\nregistry = Registry()\nsetup_imports()\n'"
mmf/common/report.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport collections\nimport warnings\nfrom collections import OrderedDict\n\nimport torch\n\nfrom mmf.common.registry import registry\n\n\nclass Report(OrderedDict):\n    def __init__(self, batch, model_output=None, *args):\n        super().__init__(self)\n        if model_output is None:\n            model_output = {}\n        if self._check_and_load_tuple(batch):\n            return\n\n        all_args = [batch, model_output] + [*args]\n        for idx, arg in enumerate(all_args):\n            if not isinstance(arg, collections.abc.Mapping):\n                raise TypeError(\n                    ""Argument {:d}, {} must be of instance of ""\n                    ""collections.abc.Mapping"".format(idx, arg)\n                )\n\n        self.writer = registry.get(""writer"")\n        self.batch_size = batch.get_batch_size()\n        self.warning_string = (\n            ""Updating forward report with key {}""\n            ""{}, but it already exists in {}. ""\n            ""Please consider using a different key, ""\n            ""as this can cause issues during loss and ""\n            ""metric calculations.""\n        )\n\n        for idx, arg in enumerate(all_args):\n            for key, item in arg.items():\n                if key in self and idx >= 2:\n                    log = self.warning_string.format(\n                        key, """", ""in previous arguments to report""\n                    )\n                    warnings.warn(log)\n                self[key] = item\n\n    def get_batch_size(self):\n        return self.batch_size\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @batch_size.setter\n    def batch_size(self, batch_size):\n        self._batch_size = batch_size\n\n    def _check_and_load_tuple(self, batch):\n        if isinstance(batch, collections.abc.Mapping):\n            return False\n\n        if isinstance(batch[0], (tuple, list)) and isinstance(batch[0][0], str):\n            for kv_pair in batch:\n                self[kv_pair[0]] = kv_pair[1]\n            return True\n        else:\n            return False\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key)\n\n    def fields(self):\n        return list(self.keys())\n\n    def accumulate_tensor_fields(self, report, field_list):\n        for key in field_list:\n            if key not in self.keys():\n                raise AttributeError(key)\n            if isinstance(self[key], torch.Tensor):\n                self[key] = torch.cat((self[key], report[key]), dim=0)\n'"
mmf/common/sample.py,16,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\n``Sample`` and ``SampleList`` are data structures for arbitrary data returned from a\ndataset. To work with MMF, minimum requirement for datasets is to return\nan object of ``Sample`` class and for models to accept an object of type `SampleList`\nas an argument.\n\n``Sample`` is used to represent an arbitrary sample from dataset, while ``SampleList``\nis list of Sample combined in an efficient way to be used by the model.\nIn simple term, ``SampleList`` is a batch of Sample but allow easy access of\nattributes from ``Sample`` while taking care of properly batching things.\n""""""\n\nimport collections\nfrom collections import OrderedDict\nfrom typing import Any, Dict\n\nimport torch\n\n\nclass Sample(OrderedDict):\n    """"""Sample represent some arbitrary data. All datasets in MMF must\n    return an object of type ``Sample``.\n\n    Args:\n        init_dict (Dict): Dictionary to init ``Sample`` class with.\n\n    Usage::\n\n        >>> sample = Sample({""text"": torch.tensor(2)})\n        >>> sample.text.zero_()\n        # Custom attributes can be added to ``Sample`` after initialization\n        >>> sample.context = torch.tensor(4)\n    """"""\n\n    def __init__(self, init_dict=None):\n        if init_dict is None:\n            init_dict = {}\n        super().__init__(init_dict)\n\n    def __setattr__(self, key, value):\n        if isinstance(value, collections.abc.Mapping):\n            value = Sample(value)\n        self[key] = value\n\n    def __setitem__(self, key, value):\n        if isinstance(value, collections.abc.Mapping):\n            value = Sample(value)\n        super().__setitem__(key, value)\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key)\n\n    def fields(self):\n        """"""Get current attributes/fields registered under the sample.\n\n        Returns:\n            List[str]: Attributes registered under the Sample.\n\n        """"""\n        return list(self.keys())\n\n\nclass SampleList(OrderedDict):\n    """"""``SampleList`` is used to collate a list of ``Sample`` into a batch during batch\n    preparation. It can be thought of as a merger of list of Dicts into a single Dict.\n\n    If ``Sample`` contains an attribute \'text\' of size (2) and there are 10 samples in\n    list, the returned ``SampleList`` will have an attribute \'text\' which is a tensor\n    of size (10, 2).\n\n    Args:\n        samples (type): List of ``Sample`` from which the ``SampleList``\n                        will be created.\n\n    Usage::\n\n        >>> sample_list = [\n                Sample({""text"": torch.tensor(2)}),\n                Sample({""text"": torch.tensor(2)})\n            ]\n        >>> sample_list.text\n        torch.tensor([2, 2])\n    """"""\n\n    _TENSOR_FIELD_ = ""_tensor_field""\n\n    def __init__(self, samples=None):\n        super().__init__(self)\n        if samples is None:\n            samples = []\n\n        if len(samples) == 0:\n            return\n\n        if self._check_and_load_dict(samples):\n            return\n        # If passed sample list was in form of key, value pairs of tuples\n        # return after loading these\n        if self._check_and_load_tuple(samples):\n            return\n\n        fields = samples[0].keys()\n\n        for field in fields:\n            if isinstance(samples[0][field], torch.Tensor):\n                size = (len(samples), *samples[0][field].size())\n                self[field] = samples[0][field].new_empty(size)\n                if self._get_tensor_field() is None:\n                    self._set_tensor_field(field)\n            else:\n                self[field] = [None for _ in range(len(samples))]\n\n            for idx, sample in enumerate(samples):\n                # it should be a tensor but not a 0-d tensor\n                if (\n                    isinstance(sample[field], torch.Tensor)\n                    and len(sample[field].size()) != 0\n                    and sample[field].size(0) != samples[0][field].size(0)\n                ):\n                    raise AssertionError(\n                        ""Fields for all samples must be equally sized. ""\n                        ""{} is of different sizes"".format(field)\n                    )\n\n                self[field][idx] = self._get_data_copy(sample[field])\n\n            if isinstance(samples[0][field], collections.abc.Mapping):\n                self[field] = SampleList(self[field])\n\n    def _check_and_load_tuple(self, samples):\n        if isinstance(samples[0], (tuple, list)) and isinstance(samples[0][0], str):\n            for kv_pair in samples:\n                self.add_field(kv_pair[0], kv_pair[1])\n            return True\n        else:\n            return False\n\n    def _check_and_load_dict(self, samples):\n        if isinstance(samples, collections.abc.Mapping):\n            for key, value in samples.items():\n                self.add_field(key, value)\n            return True\n        else:\n            return False\n\n    def _fix_sample_type(self, samples):\n        if not isinstance(samples[0], Sample):\n            proper_samples = []\n            for sample in samples:\n                proper_samples.append(Sample(sample))\n            samples = proper_samples\n        return samples\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\n    def __getattr__(self, key):\n        if key not in self:\n            raise AttributeError(\n                ""Key {} not found in the SampleList. ""\n                ""Valid choices are {}"".format(key, self.fields())\n            )\n        fields = self.keys()\n\n        if key in fields:\n            return self[key]\n\n        sample = Sample()\n\n        for field in fields:\n            sample[field] = self[field][key]\n\n        return sample\n\n    def get_item_list(self, key):\n        """"""Get ``SampleList`` of only one particular attribute that is present\n        in the ``SampleList``.\n\n        Args:\n            key (str): Attribute whose ``SampleList`` will be made.\n\n        Returns:\n            SampleList: SampleList containing only the attribute value of the key\n            which was passed.\n\n        """"""\n        sample = self[key]\n\n        return SampleList([sample])\n\n    def copy(self):\n        """"""Get a copy of the current SampleList\n\n        Returns:\n            SampleList: Copy of current SampleList.\n\n        """"""\n        sample_list = SampleList()\n\n        fields = self.fields()\n\n        for field in fields:\n            sample_list.add_field(field, self[field])\n\n        return sample_list\n\n    def fields(self):\n        """"""Get current attributes/fields registered under the SampleList.\n\n        Returns:\n            List[str]: list of attributes of the SampleList.\n\n        """"""\n        return list(self.keys())\n\n    def get_fields(self, fields):\n        """"""Get a new ``SampleList`` generated from the current ``SampleList``\n        but contains only the attributes passed in `fields` argument\n\n        Args:\n            fields (List[str]): Attributes whose ``SampleList`` will be made.\n\n        Returns:\n            SampleList: SampleList containing only the attribute values of the fields\n            which were passed.\n\n        """"""\n        current_fields = self.fields()\n\n        return_list = SampleList()\n\n        for field in fields:\n            if field not in current_fields:\n                raise AttributeError(\n                    ""{} not present in SampleList. ""\n                    ""Valid choices are {}"".format(field, current_fields)\n                )\n            return_list.add_field(field, self[field])\n\n        return return_list\n\n    def get_field(self, field):\n        """"""Get value of a particular attribute\n\n        Args:\n            field (str): Attribute whose value is to be returned.\n        """"""\n        return self[field]\n\n    def _get_data_copy(self, data):\n        # if isinstance(data, torch.Tensor):\n        #     copy_ = data.clone()\n        # else:\n        #     copy_ = deepcopy(data)\n        # return copy_\n        return data\n\n    def _get_tensor_field(self):\n        return self.__dict__.get(SampleList._TENSOR_FIELD_, None)\n\n    def _set_tensor_field(self, value):\n        self.__dict__[SampleList._TENSOR_FIELD_] = value\n\n    def get_batch_size(self):\n        """"""Get batch size of the current ``SampleList``. There must be a tensor\n\n    def __getitem__(self, key):\n        return self.__dict__[key]\n        field present in the ``SampleList`` currently.\n\n        Returns:\n\n    def __getitem__(self, key):\n        return self.__dict__[key]\n            int: Size of the batch in ``SampleList``.\n\n        """"""\n        tensor_field = self._get_tensor_field()\n        assert tensor_field is not None, ""There is no tensor yet in SampleList""\n\n        return self[tensor_field].size(0)\n\n    def add_field(self, field, data):\n        """"""Add an attribute ``field`` with value ``data`` to the SampleList\n\n        Args:\n            field (str): Key under which the data will be added.\n            data (object): Data to be added, can be a ``torch.Tensor``, ``list``\n                         or ``Sample``\n        """"""\n        fields = self.fields()\n        tensor_field = self._get_tensor_field()\n\n        if (\n            len(fields) != 0\n            and isinstance(data, torch.Tensor)\n            and len(data.size()) != 0\n            and tensor_field is not None\n            and data.size(0) != self[tensor_field].size(0)\n        ):\n            raise AssertionError(\n                ""A tensor field to be added must ""\n                ""have same size as existing tensor ""\n                ""fields in SampleList. ""\n                ""Passed size: {}, Required size: {}"".format(\n                    len(data), len(self[fields[0]])\n                )\n            )\n\n        if isinstance(data, collections.abc.Mapping):\n            self[field] = SampleList(data)\n        else:\n            self[field] = self._get_data_copy(data)\n\n            if isinstance(self[field], torch.Tensor) and tensor_field is None:\n                self._set_tensor_field(field)\n\n    def to(self, device, non_blocking=True):\n        """"""Similar to ``.to`` function on a `torch.Tensor`. Moves all of the\n        tensors present inside the ``SampleList`` to a particular device. If an\n        attribute\'s value is not a tensor, it is ignored and kept as it is.\n\n        Args:\n            device (str|torch.device): Device on which the ``SampleList`` should\n                                       moved.\n            non_blocking (bool): Whether the move should be non_blocking. Default: True\n\n        Returns:\n            SampleList: a SampleList moved to the ``device``.\n\n        """"""\n        fields = self.keys()\n        sample_list = self.copy()\n        if not isinstance(device, torch.device):\n            if not isinstance(device, str):\n                raise TypeError(\n                    ""device must be either \'str\' or ""\n                    ""\'torch.device\' type, {} found"".format(type(device))\n                )\n            device = torch.device(device)\n\n        for field in fields:\n            if hasattr(sample_list[field], ""to""):\n                sample_list[field] = sample_list[field].to(\n                    device, non_blocking=non_blocking\n                )\n\n        return sample_list\n\n    def pin_memory(self):\n        """"""In custom batch object, we need to define pin_memory function so that\n        PyTorch can actually apply pinning. This function just individually pins\n        all of the tensor fields\n        """"""\n        fields = self.keys()\n\n        for field in fields:\n            if hasattr(self[field], ""pin_memory""):\n                # This will also handle nested sample list recursively\n                self[field] = self[field].pin_memory()\n\n        return self\n\n    def to_dict(self) -> Dict[str, Any]:\n        """"""Converts a sample list to dict, this is useful for TorchScript and for\n        other internal API unification efforts.\n\n        Returns:\n            Dict[str, Any]: A dict representation of current sample list\n        """"""\n        sample_dict = {}\n        fields = self.keys()\n\n        for field in fields:\n            # Handle nested sample list recursively\n            if hasattr(self[field], ""to_dict""):\n                sample_dict[field] = self[field].to_dict()\n            else:\n                sample_dict[field] = self[field]\n\n        return sample_dict\n'"
mmf/common/test_reporter.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport csv\nimport json\nimport os\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom mmf.common.batch_collator import BatchCollator\nfrom mmf.common.registry import registry\nfrom mmf.utils.configuration import get_mmf_env\nfrom mmf.utils.distributed import gather_tensor, is_master\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import (\n    ckpt_name_from_core_args,\n    foldername_from_config_override,\n    get_batch_size,\n)\nfrom mmf.utils.timer import Timer\n\n\nclass TestReporter(Dataset):\n    def __init__(self, multi_task_instance):\n        self.test_task = multi_task_instance\n        self.task_type = multi_task_instance.dataset_type\n        self.config = registry.get(""config"")\n        self.writer = registry.get(""writer"")\n        self.report = []\n        self.timer = Timer()\n        self.training_config = self.config.training\n        self.num_workers = self.training_config.num_workers\n        self.batch_size = self.training_config.batch_size\n        self.report_folder_arg = get_mmf_env(key=""report_dir"")\n        self.experiment_name = self.training_config.experiment_name\n\n        self.datasets = []\n\n        for dataset in self.test_task.get_datasets():\n            self.datasets.append(dataset)\n\n        self.current_dataset_idx = -1\n        self.current_dataset = self.datasets[self.current_dataset_idx]\n\n        self.save_dir = get_mmf_env(key=""save_dir"")\n        self.report_folder = ckpt_name_from_core_args(self.config)\n        self.report_folder += foldername_from_config_override(self.config)\n\n        self.report_folder = os.path.join(self.save_dir, self.report_folder)\n        self.report_folder = os.path.join(self.report_folder, ""reports"")\n\n        if self.report_folder_arg:\n            self.report_folder = self.report_folder_arg\n\n        PathManager.mkdirs(self.report_folder)\n\n    def next_dataset(self):\n        if self.current_dataset_idx >= 0:\n            self.flush_report()\n\n        self.current_dataset_idx += 1\n\n        if self.current_dataset_idx == len(self.datasets):\n            return False\n        else:\n            self.current_dataset = self.datasets[self.current_dataset_idx]\n            self.writer.write(""Predicting for "" + self.current_dataset.dataset_name)\n            return True\n\n    def flush_report(self):\n        if not is_master():\n            return\n\n        name = self.current_dataset.dataset_name\n        time_format = ""%Y-%m-%dT%H:%M:%S""\n        time = self.timer.get_time_hhmmss(None, format=time_format)\n\n        filename = name + ""_""\n\n        if len(self.experiment_name) > 0:\n            filename += self.experiment_name + ""_""\n\n        filename += self.task_type + ""_""\n        filename += time\n\n        if self.config.evaluation.predict_file_format == ""csv"":\n            filepath = os.path.join(self.report_folder, filename + "".csv"")\n            self.csv_dump(filepath)\n        else:\n            filepath = os.path.join(self.report_folder, filename + "".json"")\n            self.json_dump(filepath)\n\n        self.writer.write(\n            ""Wrote evalai predictions for {} to {}"".format(\n                name, os.path.abspath(filepath)\n            )\n        )\n        self.report = []\n\n    def csv_dump(self, filepath):\n        with PathManager.open(filepath, ""w"") as f:\n            title = self.report[0].keys()\n            cw = csv.DictWriter(f, title, delimiter="","", quoting=csv.QUOTE_MINIMAL)\n            cw.writeheader()\n            cw.writerows(self.report)\n\n    def json_dump(self, filepath):\n        with PathManager.open(filepath, ""w"") as f:\n            json.dump(self.report, f)\n\n    def get_dataloader(self):\n        other_args = self._add_extra_args_for_dataloader()\n        return DataLoader(\n            dataset=self.current_dataset,\n            collate_fn=BatchCollator(\n                self.current_dataset.dataset_name, self.current_dataset.dataset_type\n            ),\n            num_workers=self.num_workers,\n            pin_memory=self.config.training.pin_memory,\n            **other_args\n        )\n\n    def _add_extra_args_for_dataloader(self, other_args=None):\n        if other_args is None:\n            other_args = {}\n\n        if torch.distributed.is_initialized():\n            other_args[""sampler""] = DistributedSampler(\n                self.current_dataset, shuffle=False\n            )\n        else:\n            other_args[""shuffle""] = False\n\n        other_args[""batch_size""] = get_batch_size()\n\n        return other_args\n\n    def prepare_batch(self, batch):\n        return self.current_dataset.prepare_batch(batch)\n\n    def __len__(self):\n        return len(self.current_dataset)\n\n    def __getitem__(self, idx):\n        return self.current_dataset[idx]\n\n    def add_to_report(self, report, model):\n        # TODO: Later gather whole report for no opinions\n        if self.current_dataset.dataset_name == ""coco"":\n            report.captions = gather_tensor(report.captions)\n            if isinstance(report.image_id, torch.Tensor):\n                report.image_id = gather_tensor(report.image_id).view(-1)\n        else:\n            report.scores = gather_tensor(report.scores).view(\n                -1, report.scores.size(-1)\n            )\n            if ""id"" in report:\n                report.id = gather_tensor(report.id).view(-1)\n            if ""question_id"" in report:\n                report.question_id = gather_tensor(report.question_id).view(-1)\n            if ""image_id"" in report:\n                _, enc_size = report.image_id.size()\n                report.image_id = gather_tensor(report.image_id)\n                report.image_id = report.image_id.view(-1, enc_size)\n            if ""context_tokens"" in report:\n                _, enc_size = report.context_tokens.size()\n                report.context_tokens = gather_tensor(report.context_tokens)\n                report.context_tokens = report.context_tokens.view(-1, enc_size)\n\n        if not is_master():\n            return\n\n        results = self.current_dataset.format_for_prediction(report)\n        if hasattr(model, ""format_for_prediction""):\n            results = model.format_for_prediction(results, report)\n        elif hasattr(model.module, ""format_for_prediction""):\n            results = model.module.format_for_prediction(results, report)\n\n        self.report = self.report + results\n'"
mmf/common/typings.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom typing import Any, Dict, Optional, Tuple, Type\n\nimport omegaconf\nimport torch\n\nfrom mmf.datasets.base_dataset import BaseDataset\nfrom mmf.datasets.base_dataset_builder import BaseDatasetBuilder\nfrom mmf.datasets.processors.processors import Processor\n\nDatasetType = Type[BaseDataset]\nDatasetBuilderType = Type[BaseDatasetBuilder]\nDictConfig = Type[omegaconf.DictConfig]\nDataLoaderAndSampler = Tuple[\n    Type[torch.utils.data.DataLoader], Optional[torch.utils.data.Sampler]\n]\nDataLoaderArgsType = Optional[Dict[str, Any]]\nProcessorType = Type[Processor]\nProcessorDict = Dict[str, ProcessorType]\n'"
mmf/datasets/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom .base_dataset import BaseDataset\nfrom .base_dataset_builder import BaseDatasetBuilder\nfrom .concat_dataset import ConcatDataset\nfrom .mmf_dataset import MMFDataset\nfrom .mmf_dataset_builder import MMFDatasetBuilder\nfrom .multi_dataset_loader import MultiDatasetLoader\n\n__all__ = [\n    ""BaseDataset"",\n    ""BaseDatasetBuilder"",\n    ""ConcatDataset"",\n    ""MultiDatasetLoader"",\n    ""MMFDataset"",\n    ""MMFDatasetBuilder"",\n]\n'"
mmf/datasets/base_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom torch.utils.data.dataset import Dataset\n\nfrom mmf.common.registry import registry\nfrom mmf.common.sample import SampleList\n\n\nclass BaseDataset(Dataset):\n    """"""Base class for implementing a dataset. Inherits from PyTorch\'s Dataset class\n    but adds some custom functionality on top. Processors mentioned in the\n    configuration are automatically initialized for the end user.\n\n    Args:\n        dataset_name (str): Name of your dataset to be used a representative\n            in text strings\n        dataset_type (str): Type of your dataset. Normally, train|val|test\n        config (DictConfig): Configuration for the current dataset\n    """"""\n\n    def __init__(self, dataset_name, config, dataset_type=""train"", *args, **kwargs):\n        super().__init__()\n        if config is None:\n            config = {}\n        self.config = config\n        self._dataset_name = dataset_name\n        self._dataset_type = dataset_type\n        self.writer = registry.get(""writer"")\n        self._global_config = registry.get(""config"")\n        self._device = registry.get(""current_device"")\n        self.use_cuda = ""cuda"" in str(self._device)\n\n    def load_item(self, idx):\n        """"""\n        Implement if you need to separately load the item and cache it.\n\n        Args:\n            idx (int): Index of the sample to be loaded.\n        """"""\n        return\n\n    def __getitem__(self, idx):\n        """"""\n        Basically, __getitem__ of a torch dataset.\n\n        Args:\n            idx (int): Index of the sample to be loaded.\n        """"""\n\n        raise NotImplementedError\n\n    def init_processors(self):\n        if not hasattr(self.config, ""processors""):\n            return\n\n        from mmf.utils.build import build_processors\n\n        extra_params = {""data_dir"": self.config.data_dir}\n        reg_key = f""{self._dataset_name}_{{}}""\n        processor_dict = build_processors(\n            self.config.processors, reg_key, **extra_params\n        )\n        for processor_key, processor_instance in processor_dict.items():\n            setattr(self, processor_key, processor_instance)\n            full_key = reg_key.format(processor_key)\n            registry.register(full_key, processor_instance)\n\n    def try_fast_read(self):\n        return\n\n    def prepare_batch(self, batch):\n        """"""\n        Can be possibly overridden in your child class\n\n        Prepare batch for passing to model. Whatever returned from here will\n        be directly passed to model\'s forward function. Currently moves the batch to\n        proper device.\n\n        Args:\n            batch (SampleList): sample list containing the currently loaded batch\n\n        Returns:\n            sample_list (SampleList): Returns a sample representing current\n                batch loaded\n        """"""\n        # Should be a SampleList\n        if not isinstance(batch, SampleList):\n            # Try converting to SampleList\n            batch = SampleList(batch)\n        batch = batch.to(self._device)\n        return batch\n\n    @property\n    def dataset_type(self):\n        return self._dataset_type\n\n    @property\n    def name(self):\n        return self._dataset_name\n\n    @property\n    def dataset_name(self):\n        return self._dataset_name\n\n    @dataset_name.setter\n    def dataset_name(self, name):\n        self._dataset_name = name\n\n    def format_for_prediction(self, report):\n        return []\n\n    def verbose_dump(self, *args, **kwargs):\n        return\n\n    def visualize(self, num_samples=1, *args, **kwargs):\n        raise NotImplementedError(\n            f""{self.dataset_name} doesn\'t implement visualize function""\n        )\n'"
mmf/datasets/base_dataset_builder.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\nIn MMF, for adding new datasets, dataset builder for datasets need to be\nadded. A new dataset builder must inherit ``BaseDatasetBuilder`` class and\nimplement ``load`` and ``build`` functions.\n\n``build`` is used to build a dataset when it is not available. For e.g.\ndownloading the ImDBs for a dataset. In future, we plan to add a ``build``\nto add dataset builder to ease setup of MMF.\n\n``load`` is used to load a dataset from specific path. ``load`` needs to return\nan instance of subclass of ``mmf.datasets.base_dataset.BaseDataset``.\n\nSee complete example for ``VQA2DatasetBuilder`` here_.\n\nExample::\n\n    from torch.utils.data import Dataset\n\n    from mmf.datasets.base_dataset_builder import BaseDatasetBuilder\n    from mmf.common.registry import registry\n\n    @registry.register_builder(""my"")\n    class MyBuilder(BaseDatasetBuilder):\n        def __init__(self):\n            super().__init__(""my"")\n\n        def load(self, config, dataset_type, *args, **kwargs):\n            ...\n            return Dataset()\n\n        def build(self, config, dataset_type, *args, **kwargs):\n            ...\n\n.. _here: https://github.com/facebookresearch/mmf/blob/master/mmf/datasets/vqa/vqa2/builder.py\n""""""\n\nfrom mmf.utils.distributed import is_master, synchronize\n\n\nclass BaseDatasetBuilder:\n    """"""Base class for implementing dataset builders. See more information\n    on top. Child class needs to implement ``build`` and ``load``.\n\n    Args:\n        dataset_name (str): Name of the dataset passed from child.\n    """"""\n\n    def __init__(self, dataset_name):\n        self.dataset_name = dataset_name\n\n    @property\n    def dataset_name(self):\n        return self._dataset_name\n\n    @dataset_name.setter\n    def dataset_name(self, dataset_name):\n        self._dataset_name = dataset_name\n\n    def build_dataset(self, config, dataset_type=""train"", *args, **kwargs):\n        """"""\n        Similar to load function, used by MMF to build a dataset for first\n        time when it is not available. This internally calls \'build\' function.\n        Override that function in your child class.\n\n        Args:\n            config (DictConfig): Configuration of this dataset loaded from\n                                 config.\n            dataset_type (str): Type of dataset, train|val|test\n\n        .. warning::\n\n            DO NOT OVERRIDE in child class. Instead override ``build``.\n        """"""\n        # Only build in main process, so none of the others have to build\n        if is_master():\n            self.build(config, dataset_type, *args, **kwargs)\n        synchronize()\n\n    def load_dataset(self, config, dataset_type=""train"", *args, **kwargs):\n        """"""Main load function use by MMF. This will internally call ``load``\n        function. Calls ``init_processors`` and ``try_fast_read`` on the\n        dataset returned from ``load``\n\n        Args:\n            config (DictConfig): Configuration of this dataset loaded from config.\n            dataset_type (str): Type of dataset, train|val|test\n\n        Returns:\n            dataset (BaseDataset): Dataset containing data to be trained on\n\n        .. warning::\n\n            DO NOT OVERRIDE in child class. Instead override ``load``.\n        """"""\n        dataset = self.load(config, dataset_type, *args, **kwargs)\n        if dataset is not None:\n            dataset.init_processors()\n            dataset.try_fast_read()\n        return dataset\n\n    def load(self, config, dataset_type=""train"", *args, **kwargs):\n        """"""\n        This is used to prepare the dataset and load it from a path.\n        Override this method in your child dataset builder class.\n\n        Args:\n            config (DictConfig): Configuration of this dataset loaded from config.\n            dataset_type (str): Type of dataset, train|val|test\n\n        Returns:\n            dataset (BaseDataset): Dataset containing data to be trained on\n        """"""\n        raise NotImplementedError(\n            ""This dataset builder doesn\'t implement a load method""\n        )\n\n    @classmethod\n    def config_path(cls):\n        return None\n\n    def build(self, config, dataset_type=""train"", *args, **kwargs):\n        """"""\n        This is used to build a dataset first time.\n        Implement this method in your child dataset builder class.\n\n        Args:\n            config (DictConfig): Configuration of this dataset loaded from\n                                 config.\n            dataset_type (str): Type of dataset, train|val|test\n        """"""\n        raise NotImplementedError(\n            ""This dataset builder doesn\'t implement a build method""\n        )\n'"
mmf/datasets/concat_dataset.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport functools\nimport types\n\nfrom torch.utils.data import ConcatDataset\n\n\nclass MMFConcatDataset(ConcatDataset):\n    # These functions should only be called once even if they return nothing\n    _SINGLE_CALL_FUNCS = []\n\n    def __init__(self, datasets):\n        super().__init__(datasets)\n        self._dir_representation = dir(self)\n\n    def __getattr__(self, name):\n        if ""_dir_representation"" in self.__dict__ and name in self._dir_representation:\n            return getattr(self, name)\n        elif ""datasets"" in self.__dict__ and hasattr(self.datasets[0], name):\n            attr = getattr(self.datasets[0], name)\n            # Check if the current attribute is class method function\n            if isinstance(attr, types.MethodType):\n                # if it is the, we to call this function for\n                # each of the child datasets\n                attr = functools.partial(self._call_all_datasets_func, name)\n            return attr\n        else:\n            raise AttributeError(name)\n\n    def _get_single_call_funcs(self):\n        return MMFConcatDataset._SINGLE_CALL_FUNCS\n\n    def _call_all_datasets_func(self, name, *args, **kwargs):\n        for dataset in self.datasets:\n            value = getattr(dataset, name)(*args, **kwargs)\n            if value is not None:\n                # TODO: Log a warning here\n                return value\n                # raise RuntimeError(""Functions returning values can\'t be ""\n                #                    ""called through MMFConcatDataset"")\n            if (\n                hasattr(dataset, ""get_single_call_funcs"")\n                and name in dataset.get_single_call_funcs()\n            ):\n                return\n'"
mmf/datasets/mmf_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport collections\nimport os\n\nfrom mmf.datasets.base_dataset import BaseDataset\nfrom mmf.datasets.databases.annotation_database import AnnotationDatabase\nfrom mmf.datasets.databases.features_database import FeaturesDatabase\nfrom mmf.datasets.databases.image_database import ImageDatabase\n\n\nclass MMFDataset(BaseDataset):\n    """"""This dataset is useful for external open source dataset which\n    usually have annotation files, images and features (which we generate).\n    The dataset takes care of creating annotation db, features db and image db\n    if the configuration follows a set format. Also, you can optionally enable\n    image or features. The class has a resources method which can be overridden\n    to download data. More details to come.\n    """"""\n\n    def __init__(\n        self, dataset_name, config, dataset_type=""train"", index=0, *args, **kwargs\n    ):\n        super().__init__(dataset_name, config, dataset_type, *args, **kwargs)\n        self._index = index\n        self.annotation_db = self._build_annotation_db()\n\n        self._use_images = self.config.get(""use_images"", False)\n        if self._use_images:\n            self.image_db = self._build_image_db()\n\n        self._use_features = self.config.get(""use_features"", False)\n        if self._use_features:\n            self.features_db = self._build_features_db()\n\n    def _build_annotation_db(self):\n        annotation_path = self._get_path_based_on_index(\n            self.config, ""annotations"", self._index\n        )\n        return AnnotationDatabase(self.config, annotation_path)\n\n    def _build_features_db(self):\n        features_path = self._get_path_based_on_index(\n            self.config, ""features"", self._index\n        )\n        return FeaturesDatabase(\n            self.config, features_path, annotation_db=self.annotation_db\n        )\n\n    def _build_image_db(self):\n        image_path = self._get_path_based_on_index(self.config, ""images"", self._index)\n        return ImageDatabase(self.config, image_path, annotation_db=self.annotation_db)\n\n    def _get_path_based_on_index(self, config, attribute, index):\n        if attribute not in config:\n            raise ValueError(f""{attribute} not present in config"")\n\n        config = config.get(attribute, None)\n\n        if (\n            self.dataset_type not in config\n            or len(config.get(self.dataset_type, [])) == 0\n        ):\n            raise ValueError(f""No {attribute} present for type {self.dataset_type}"")\n\n        paths = config[self.dataset_type]\n\n        if isinstance(paths, str):\n            selected_path = paths\n        else:\n            assert isinstance(paths, collections.abc.MutableSequence)\n            selected_path = paths[self._index]\n\n        selected_path = self._add_root_dir(selected_path)\n\n        return selected_path\n\n    def _add_root_dir(self, path):\n        path = path.split("","")\n        for idx, p in enumerate(path):\n            path[idx] = os.path.join(self.config.data_dir, p)\n\n        return "","".join(path)\n\n    def __len__(self):\n        return len(self.annotation_db)\n'"
mmf/datasets/mmf_dataset_builder.py,0,"b'import collections\nimport os\nimport typing\nimport warnings\n\nimport mmf.utils.download as download\nfrom mmf.datasets.base_dataset_builder import BaseDatasetBuilder\nfrom mmf.datasets.concat_dataset import MMFConcatDataset\nfrom mmf.utils.configuration import get_global_config, get_mmf_env, get_zoo_config\nfrom mmf.utils.general import get_absolute_path\n\n\nclass MMFDatasetBuilder(BaseDatasetBuilder):\n    ZOO_CONFIG_PATH = None\n    ZOO_VARIATION = None\n\n    def __init__(\n        self,\n        dataset_name,\n        dataset_class=None,\n        zoo_variation=""defaults"",\n        *args,\n        **kwargs\n    ):\n        super().__init__(dataset_name)\n        self.dataset_class = dataset_class\n        self.zoo_type = ""datasets""\n        self.zoo_variation = zoo_variation\n\n    @property\n    def dataset_class(self):\n        return self._dataset_class\n\n    @dataset_class.setter\n    def dataset_class(self, dataset_class):\n        self._dataset_class = dataset_class\n\n    @property\n    def zoo_variation(self):\n        return self._zoo_variation\n\n    @zoo_variation.setter\n    def zoo_variation(self, zoo_variation):\n        self._zoo_variation = zoo_variation\n\n    @property\n    def zoo_config_path(self):\n        if self.ZOO_CONFIG_PATH is None:\n            self.ZOO_CONFIG_PATH = get_global_config(""env.dataset_zoo"")\n        return self.ZOO_CONFIG_PATH\n\n    @zoo_config_path.setter\n    def zoo_config_path(self, zoo_config_path):\n        self.ZOO_CONFIG_PATH = zoo_config_path\n\n    def set_dataset_class(self, dataset_cls):\n        self.dataset_class = dataset_cls\n\n    def build(self, config, dataset_type=""train"", *args, **kwargs):\n        requirements = config.get(""zoo_requirements"", [])\n\n        if len(requirements) == 0:\n            # If nothing is specified, build the default requirement\n            self._download_requirement(config, self.dataset_name, self.zoo_variation)\n        else:\n            # Else build all of the requirements one by one\n            # Default must also be specified in these requirements if needed\n            for requirement in requirements:\n                self._download_requirement(config, requirement)\n\n    def _download_requirement(\n        self, config, requirement_key, requirement_variation=""defaults""\n    ):\n        version, resources = get_zoo_config(\n            requirement_key, requirement_variation, self.zoo_config_path, self.zoo_type\n        )\n\n        if resources is None:\n            return\n\n        requirement_split = requirement_key.split(""."")\n        dataset_name = requirement_split[0]\n\n        # The dataset variation has been directly passed in the key so use it instead\n        if len(requirement_split) >= 2:\n            dataset_variation = requirement_split[1]\n        else:\n            dataset_variation = requirement_variation\n\n        # We want to use root env data_dir so that we don\'t mix up our download\n        # root dir with the dataset ones\n        download_path = os.path.join(\n            get_mmf_env(""data_dir""), ""datasets"", dataset_name, dataset_variation\n        )\n        download_path = get_absolute_path(download_path)\n\n        if not isinstance(resources, collections.abc.Mapping):\n            self._download_resources(resources, download_path, version)\n        else:\n            use_features = config.get(""use_features"", False)\n            use_images = config.get(""use_images"", False)\n\n            if use_features:\n                self._download_based_on_attribute(\n                    resources, download_path, version, ""features""\n                )\n\n            if use_images:\n                self._download_based_on_attribute(\n                    resources, download_path, version, ""images""\n                )\n\n            self._download_based_on_attribute(\n                resources, download_path, version, ""annotations""\n            )\n            self._download_resources(\n                resources.get(""extras"", []), download_path, version\n            )\n\n    def load(self, config, dataset_type, *args, **kwargs):\n        self.config = config\n        annotations = config.get(""annotations"", {}).get(dataset_type, [])\n\n        # User can pass a single string as well\n        if isinstance(annotations, str):\n            annotations = [annotations]\n\n        if len(annotations) == 0:\n            warnings.warn(\n                ""Dataset type {} is not present or empty in ""\n                + ""annotations of dataset config or either annotations ""\n                + ""key is not present. Returning None. ""\n                + ""This dataset won\'t be used."".format(dataset_type)\n            )\n            return None\n\n        datasets = []\n\n        for imdb_idx in range(len(annotations)):\n            dataset_class = self.dataset_class\n            dataset = dataset_class(config, dataset_type, imdb_idx)\n            datasets.append(dataset)\n\n        dataset = MMFConcatDataset(datasets)\n        self.dataset = dataset\n        return self.dataset\n\n    def _download_based_on_attribute(\n        self, resources, download_path, version, attribute\n    ):\n        path = os.path.join(download_path, attribute)\n        self._download_resources(resources.get(attribute, []), path, version)\n\n    def _download_resources(self, resources, path, version):\n        download.download_resources(resources, path, version)\n'"
mmf/datasets/multi_dataset_loader.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\nMultiDatasetLoader class is used by DatasetLoader class to load multiple datasets\nand more granular\n""""""\n\nimport sys\n\nimport numpy as np\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.build import build_dataloader_and_sampler, build_dataset\nfrom mmf.utils.distributed import broadcast_scalar, is_master\nfrom mmf.utils.general import get_batch_size\n\n\nclass MultiDatasetLoader:\n    """"""\n    MultiDatasetLoader class that is used for training on multiple datasets together.\n    """"""\n\n    def __init__(self, dataset_type=""train""):\n        self._dataset_type = dataset_type\n        self.writer = registry.get(""writer"")\n        self._is_master = is_master()\n\n        self._datasets = []\n        self._loaders = []\n        self._samplers = []\n        self._iterators = []\n\n        self._total_length = 0\n        self._per_dataset_lengths = []\n        self._num_datasets = 0\n        self._finished_iterators = {}\n        self._used_once = {}\n\n    @property\n    def dataset_type(self):\n        return self._dataset_type\n\n    @property\n    def current_dataset_name(self):\n        return self.current_dataset.name\n\n    @property\n    def num_datasets(self):\n        return self._num_datasets\n\n    @property\n    def datasets(self):\n        return self._datasets\n\n    @property\n    def loaders(self):\n        return self._loaders\n\n    @property\n    def samplers(self):\n        return self._samplers\n\n    @property\n    def iterators(self):\n        return self._iterators\n\n    @property\n    def current_dataset(self):\n        return self._chosen_dataset\n\n    # Setter only for functions which users should also be able to set\n    @current_dataset.setter\n    def current_dataset(self, dataset):\n        self._chosen_dataset = dataset\n\n    @property\n    def current_loader(self):\n        return self._chosen_loader\n\n    @current_loader.setter\n    def current_loader(self, loader):\n        self._chosen_loader = loader\n\n    @property\n    def current_index(self):\n        return self._loader_index\n\n    @current_index.setter\n    def current_index(self, index: int):\n        self._loader_index = index\n\n    def get_datasets(self):\n        return self.datasets\n\n    @property\n    def first_loader(self):\n        return self.loaders[0]\n\n    def _process_datasets(self):\n        if ""datasets"" not in self.config:\n            self.writer.write(\n                ""No datasets attribute present. Setting default to vqa2."" ""warning""\n            )\n            datasets = ""vqa2""\n        else:\n            datasets = self.config.datasets\n\n        if type(datasets) == str:\n            datasets = list(map(lambda x: x.strip(), datasets.split("","")))\n\n        self._given_datasets = datasets\n\n    def load(self, config):\n        self.build_datasets(config)\n        self.build_dataloaders()\n\n    def build_datasets(self, config):\n        self.config = config\n        self._process_datasets()\n\n        for dataset in self._given_datasets:\n            if dataset in self.config.dataset_config:\n                dataset_config = self.config.dataset_config[dataset]\n            else:\n                self.writer.write(\n                    ""Dataset %s is missing from "" ""dataset_config in config."" % dataset,\n                    ""error"",\n                )\n                sys.exit(1)\n\n            dataset_instance = build_dataset(dataset, dataset_config, self.dataset_type)\n            if dataset_instance is None:\n                continue\n            self.datasets.append(dataset_instance)\n            self._per_dataset_lengths.append(len(dataset_instance))\n            self._total_length += len(dataset_instance)\n\n        self._num_datasets = len(self.datasets)\n        self.current_index = 0\n        self.current_dataset = self.datasets[self.current_index]\n\n        self._infer_dataset_probabilities()\n\n    def build_dataloaders(self):\n        assert len(self._datasets) > 0, ""Call build_datasets first""\n\n        for dataset_instance in self.datasets:\n            loader_instance, sampler_instance = build_dataloader_and_sampler(\n                dataset_instance, self.config.training\n            )\n\n            self.loaders.append(loader_instance)\n            self.samplers.append(sampler_instance)\n\n        self.current_loader = self.loaders[self.current_index]\n\n    def _infer_dataset_probabilities(self):\n        self._dataset_probabilities = [\n            1 / self._num_datasets for _ in range(self.num_datasets)\n        ]\n\n        training = self.config.get(""training"", {})\n        self._proportional_sampling = training.get(\n            ""dataset_size_proportional_sampling"", True\n        )\n\n        if self._dataset_type != ""train"":\n            # If it is val or test, it needs to be all datasets need to be\n            # fully iterated as metrics will be calculated in eval mode\n            # over complete datasets\n            self._proportional_sampling = True\n\n        if self._proportional_sampling is True:\n            self._dataset_probabilities = self._per_dataset_lengths[:]\n            self._dataset_probabilities = [\n                prob / self._total_length for prob in self._dataset_probabilities\n            ]\n\n    def __len__(self):\n        # Since, this is iterator, we need to return total length == number of batches\n        return self._total_length // get_batch_size()\n\n    def __iter__(self):\n        if self._num_datasets == 1:\n            return iter(self.loaders[0])\n\n        for loader in self.loaders:\n            self.iterators.append(iter(loader))\n\n        self._chosen_iterator = self.iterators[self.current_index]\n\n        return self\n\n    def __next__(self):\n        try:\n            next_batch = next(self._chosen_iterator)\n        except StopIteration:\n            if (\n                self._proportional_sampling is True\n                or len(self._used_once) != self.num_datasets\n            ):\n                self._finished_iterators[self.current_index] = 1\n\n                if len(self._finished_iterators) == self.num_datasets:\n                    raise\n                else:\n                    self.change_dataloader()\n                next_batch = next(self._chosen_iterator)\n            else:\n                raise\n\n        self._used_once[self.current_index] = 1\n        return next_batch\n\n    def change_dataloader(self):\n        if self.num_datasets <= 1:\n            return\n        choice = 0\n\n        if self._is_master:\n            choice = np.random.choice(\n                self.num_datasets, 1, p=self._dataset_probabilities\n            )[0]\n\n            while choice in self._finished_iterators:\n                choice = np.random.choice(\n                    self.num_datasets, 1, p=self._dataset_probabilities\n                )[0]\n\n        choice = broadcast_scalar(choice, 0, device=registry.get(""current_device""))\n        self.current_index = choice\n        self.current_dataset = self.datasets[self.current_index]\n        self.current_loader = self.loaders[self.current_index]\n        self._chosen_iterator = self.iterators[self.current_index]\n\n    def verbose_dump(self, *args, **kwargs):\n        self._chosen_dataset.verbose_dump(*args, **kwargs)\n\n    def prepare_batch(self, batch):\n        batch = self._chosen_dataset.prepare_batch(batch)\n        self.change_dataloader()\n        return batch\n\n    def seed_sampler(self, epoch):\n        if torch.distributed.is_initialized():\n            for sampler in self._samplers:\n                assert hasattr(\n                    sampler, ""set_epoch""\n                ), ""Can\'t seed without `set_epoch` method""\n                sampler.set_epoch(epoch)\n'"
mmf/models/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# isort:skip_file\n\nfrom .base_model import BaseModel\nfrom .pythia import Pythia\nfrom .ban import BAN\nfrom .lorra import LoRRA\nfrom .top_down_bottom_up import TopDownBottomUp\nfrom .butd import BUTD\nfrom .mmbt import MMBT, MMBTForClassification, MMBTForPreTraining\nfrom .cnn_lstm import CNNLSTM\nfrom .m4c import M4C\nfrom .m4c_captioner import M4CCaptioner\nfrom .fusions import FusionBase, ConcatBERT, ConcatBoW, LateFusion\nfrom .unimodal import UnimodalBase, UnimodalText, UnimodalModal\nfrom .visual_bert import VisualBERT\nfrom .vilbert import ViLBERT\n\n\n__all__ = [\n    ""TopDownBottomUp"",\n    ""Pythia"",\n    ""LoRRA"",\n    ""BAN"",\n    ""BaseModel"",\n    ""BUTD"",\n    ""MMBTForClassification"",\n    ""MMBTForPreTraining"",\n    ""FusionBase"",\n    ""ConcatBoW"",\n    ""ConcatBERT"",\n    ""LateFusion"",\n    ""CNNLSTM"",\n    ""M4C"",\n    ""M4CCaptioner"",\n    ""MMBT"",\n    ""VisualBERT"",\n    ""ViLBERT"",\n    ""UnimodalBase"",\n    ""UnimodalModal"",\n    ""UnimodalText"",\n]\n'"
mmf/models/ban.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\nfrom torch import nn\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.modules.embeddings import BiLSTMTextEmbedding\nfrom mmf.modules.layers import BCNet, BiAttention, FCNet, WeightNormClassifier\n\n\n@registry.register_model(""ban"")\nclass BAN(BaseModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.config = config\n        self._global_config = registry.get(""config"")\n        self._datasets = self._global_config.datasets.split("","")\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/ban/defaults.yaml""\n\n    def build(self):\n        self._build_word_embedding()\n        self._init_text_embedding()\n        self._init_classifier()\n        self._init_bilinear_attention()\n\n    def _build_word_embedding(self):\n        text_processor = registry.get(self._datasets[0] + ""_text_processor"")\n        vocab = text_processor.vocab\n        self.word_embedding = vocab.get_embedding(torch.nn.Embedding, embedding_dim=300)\n\n    def _init_text_embedding(self):\n        module_config = self.config.text_embedding\n        q_mod = BiLSTMTextEmbedding(\n            module_config.num_hidden,\n            module_config.emb_size,\n            module_config.num_layers,\n            module_config.dropout,\n            module_config.bidirectional,\n            module_config.rnn_type,\n        )\n        self.q_emb = q_mod\n\n    def _init_bilinear_attention(self):\n        module_config = self.config.bilinear_attention\n        num_hidden = self.config.text_embedding.num_hidden\n        v_dim = module_config.visual_feat_dim\n\n        v_att = BiAttention(v_dim, num_hidden, num_hidden, module_config.gamma)\n\n        b_net = []\n        q_prj = []\n\n        for _ in range(module_config.gamma):\n            b_net.append(\n                BCNet(v_dim, num_hidden, num_hidden, None, k=module_config.bc_net.k)\n            )\n\n            q_prj.append(\n                FCNet(\n                    dims=[num_hidden, num_hidden],\n                    act=module_config.fc_net.activation,\n                    dropout=module_config.fc_net.dropout,\n                )\n            )\n\n        self.b_net = nn.ModuleList(b_net)\n        self.q_prj = nn.ModuleList(q_prj)\n        self.v_att = v_att\n\n    def _init_classifier(self):\n        num_hidden = self.config.text_embedding.num_hidden\n        num_choices = registry.get(self._datasets[0] + ""_num_final_outputs"")\n        dropout = self.config.classifier.dropout\n        self.classifier = WeightNormClassifier(\n            num_hidden, num_choices, num_hidden * 2, dropout\n        )\n\n    def forward(self, sample_list):\n\n        v = sample_list.image_feature_0\n        q = self.word_embedding(sample_list.text)\n\n        q_emb = self.q_emb.forward_all(q)\n\n        b_emb = [0] * self.config.bilinear_attention.gamma\n        att, logits = self.v_att.forward_all(v, q_emb)\n\n        for g in range(self.config.bilinear_attention.gamma):\n            g_att = att[:, g, :, :]\n            b_emb[g] = self.b_net[g].forward_with_weights(v, q_emb, g_att)\n            q_emb = self.q_prj[g](b_emb[g].unsqueeze(1)) + q_emb\n\n        logits = self.classifier(q_emb.sum(1))\n\n        return {""scores"": logits}\n'"
mmf/models/base_model.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n""""""\nModels built on top of Pythia need to inherit ``BaseModel`` class and adhere to\nsome format. To create a model for MMF, follow this quick cheatsheet.\n\n1. Inherit ``BaseModel`` class, make sure to call ``super().__init__()`` in your\n   class\'s ``__init__`` function.\n2. Implement `build` function for your model. If you build everything in ``__init__``,\n   you can just return in this function.\n3. Write a `forward` function which takes in a ``SampleList`` as an argument and\n   returns a dict.\n4. Register using ``@registry.register_model(""key"")`` decorator on top of the\n   class.\n\nIf you are doing logits based predictions, the dict you return from your model\nshould contain a `scores` field. Losses are automatically calculated by the\n``BaseModel`` class and added to this dict if not present.\n\nExample::\n\n    import torch\n\n    from mmf.common.registry import registry\n    from mmf.models.base_model import BaseModel\n\n\n    @registry.register(""pythia"")\n    class Pythia(BaseModel):\n        # config is model_config from global config\n        def __init__(self, config):\n            super().__init__(config)\n\n        def build(self):\n            ....\n\n        def forward(self, sample_list):\n            scores = torch.rand(sample_list.get_batch_size(), 3127)\n            return {""scores"": scores}\n""""""\n\n\nimport collections\nimport warnings\nfrom copy import deepcopy\n\nfrom torch import nn\n\nfrom mmf.common.registry import registry\nfrom mmf.modules.losses import Losses\nfrom mmf.utils.checkpoint import load_pretrained_model\nfrom mmf.utils.download import download_pretrained_model\n\n\nclass BaseModel(nn.Module):\n    """"""For integration with Pythia\'s trainer, datasets and other features,\n    models needs to inherit this class, call `super`, write a build function,\n    write a forward function taking a ``SampleList`` as input and returning a\n    dict as output and finally, register it using ``@registry.register_model``\n\n    Args:\n        config (DictConfig): ``model_config`` configuration from global config.\n\n    """"""\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self._logged_warning = {""losses_present"": False}\n        self.writer = registry.get(""writer"")\n        self._is_pretrained = False\n\n    @property\n    def is_pretrained(self):\n        return self._is_pretrained\n\n    @is_pretrained.setter\n    def is_pretrained(self, x):\n        self._is_pretrained = x\n\n    def build(self):\n        """"""Function to be implemented by the child class, in case they need to\n        build their model separately than ``__init__``. All model related\n        downloads should also happen here.\n        """"""\n        raise NotImplementedError(\n            ""Build method not implemented in the child model class.""\n        )\n\n    def init_losses(self):\n        """"""Initializes loss for the model based ``losses`` key. Automatically called by\n        MMF internally after building the model.\n        """"""\n        losses = self.config.get(""losses"", [])\n        if len(losses) == 0 and not self.is_pretrained:\n            warnings.warn(\n                ""No losses are defined in model configuration. You are expected ""\n                ""to return loss in your return dict from forward.""\n            )\n\n        self.losses = Losses(losses)\n\n    @classmethod\n    def config_path(cls):\n        return None\n\n    @classmethod\n    def format_state_key(cls, key):\n        """"""Can be implemented if something special needs to be done\n        key when pretrained model is being load. This will adapt and return\n        keys according to that. Useful for backwards compatibility. See\n        updated load_state_dict below. For an example, see VisualBERT model\'s\n        code.\n\n        Args:\n            key (string): key to be formatted\n\n        Returns:\n            string: formatted key\n        """"""\n        return key\n\n    def load_state_dict(self, state_dict, *args, **kwargs):\n        copied_state_dict = deepcopy(state_dict)\n        for key in list(copied_state_dict.keys()):\n            formatted_key = self.format_state_key(key)\n            copied_state_dict[formatted_key] = copied_state_dict.pop(key)\n\n        return super().load_state_dict(copied_state_dict, *args, **kwargs)\n\n    def forward(self, sample_list, *args, **kwargs):\n        """"""To be implemented by child class. Takes in a ``SampleList`` and\n        returns back a dict.\n\n        Args:\n            sample_list (SampleList): SampleList returned by the DataLoader for\n            current iteration\n\n        Returns:\n            Dict: Dict containing scores object.\n\n        """"""\n        raise NotImplementedError(\n            ""Forward of the child model class needs to be implemented.""\n        )\n\n    def __call__(self, sample_list, *args, **kwargs):\n        model_output = super().__call__(sample_list, *args, **kwargs)\n\n        # Don\'t do anything fancy to output if it is pretrained\n        if self.is_pretrained:\n            return model_output\n\n        # Make sure theat the output from the model is a Mapping\n        assert isinstance(\n            model_output, collections.abc.Mapping\n        ), ""A dict must be returned from the forward of the model.""\n\n        if ""losses"" in model_output:\n            if not self._logged_warning[""losses_present""]:\n                warnings.warn(\n                    ""\'losses\' already present in model output. ""\n                    ""No calculation will be done in base model.""\n                )\n                self._logged_warning[""losses_present""] = True\n\n            assert isinstance(\n                model_output[""losses""], collections.abc.Mapping\n            ), ""\'losses\' must be a dict.""\n        else:\n            model_output[""losses""] = self.losses(sample_list, model_output)\n\n        return model_output\n\n    def load_requirements(self, *args, **kwargs):\n        requirements = self.config.get(""zoo_requirements"", [])\n        if isinstance(requirements, str):\n            requirements = [requirements]\n        for item in requirements:\n            download_pretrained_model(item, *args, **kwargs)\n\n    def format_for_prediction(self, results, report):\n        """"""Implement this method in models if it requires to modify prediction\n        results using report fields. Note that the required fields in report\n        should already be gathered in report.\n        """"""\n        return results\n\n    @classmethod\n    def from_pretrained(cls, model_name, *args, **kwargs):\n        model_key = model_name.split(""."")[0]\n        model_cls = registry.get_model_class(model_key)\n        assert (\n            model_cls == cls\n        ), f""Incorrect pretrained model key {model_name} for class {cls.__name__}""\n        output = load_pretrained_model(model_name, *args, **kwargs)\n        config, checkpoint = output[""config""], output[""checkpoint""]\n\n        # Some models need registry updates to be load pretrained model\n        # If they have this method, call it so they can update accordingly\n        if hasattr(cls, ""update_registry_for_pretrained""):\n            cls.update_registry_for_pretrained(config, checkpoint, output)\n\n        instance = cls(config)\n        instance.is_pretrained = True\n        instance.build()\n        instance.load_state_dict(checkpoint)\n        instance.eval()\n\n        return instance\n'"
mmf/models/butd.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.models.pythia import Pythia\nfrom mmf.modules.layers import ClassifierLayer\n\n\n@registry.register_model(""butd"")\nclass BUTD(Pythia):\n    def __init__(self, config):\n        super().__init__(config)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/butd/defaults.yaml""\n\n    def build(self):\n        self._build_word_embedding()\n        self._init_feature_encoders(""image"")\n        self._init_feature_embeddings(""image"")\n        self._init_classifier()\n        self._init_extras()\n\n    def _build_word_embedding(self):\n        self.text_processor = registry.get(self._datasets[0] + ""_text_processor"")\n        self.vocab = self.text_processor.vocab\n        self.vocab_size = self.vocab.get_size()\n        self.word_embedding = self.vocab.get_embedding(\n            torch.nn.Embedding, embedding_dim=self.config.embedding_dim\n        )\n        self.text_embeddings_out_dim = self.config.embedding_dim\n\n    def _init_classifier(self):\n        self.classifier = ClassifierLayer(\n            self.config.classifier.type,\n            in_dim=self.config.classifier.params.feature_dim,\n            out_dim=self.vocab_size,\n            **self.config.classifier.params,\n        )\n\n    def get_optimizer_parameters(self, config):\n        params = [\n            {""params"": self.word_embedding.parameters()},\n            {""params"": self.image_feature_embeddings_list.parameters()},\n            {""params"": self.classifier.parameters()},\n            {\n                ""params"": self.image_feature_encoders.parameters(),\n                ""lr"": (config.optimizer.params.lr * 0.1),\n            },\n        ]\n        return params\n\n    def prepare_data(self, sample_list, batch_size):\n        # turn off teacher forcing during beam search\n        # (otherwise one cannot run beam search on val set)\n        self.teacher_forcing = self.config.inference.type != ""beam_search"" and hasattr(\n            sample_list, ""text""\n        )\n        data = {}\n        if self.teacher_forcing:\n            caption_lengths, sort_ind = sample_list.caption_len.sort(\n                dim=0, descending=True\n            )\n            data[""decode_lengths""] = (caption_lengths - 1).tolist()\n            sample_list.text = sample_list.text[sort_ind]\n            sample_list.answers = sample_list.answers[sort_ind]\n            sample_list.image_feature_0 = sample_list.image_feature_0[sort_ind]\n            data[""texts""] = sample_list.text\n            timesteps = max(data[""decode_lengths""])\n            sample_list.add_field(""targets"", sample_list.text[:, 1:])\n        else:\n            data[""texts""] = sample_list.answers.new_full(\n                (batch_size, 1), self.vocab.SOS_INDEX, dtype=torch.long\n            )\n            timesteps = self.text_processor.max_length\n            sample_list.add_field(""targets"", sample_list.answers[:, 0, 1:])\n        return data, sample_list, timesteps\n\n    def init_hidden_state(self, features):\n        h = features.new_zeros(\n            (features.size(0), self.config.classifier.params.hidden_dim),\n            dtype=torch.float,\n        )\n        c = features.new_zeros(\n            (features.size(0), self.config.classifier.params.hidden_dim),\n            dtype=torch.float,\n        )\n        return h, c\n\n    def get_data_t(self, t, data, batch_size_t, prev_output):\n        if self.teacher_forcing:\n            # Modify batch_size for timestep t\n            batch_size_t = sum([l > t for l in data[""decode_lengths""]])\n        elif prev_output is not None and self.config.inference.type == ""greedy"":\n            # Adding t-1 output words to data[""text""] for greedy decoding\n            output_softmax = torch.log_softmax(prev_output, dim=1)\n            _, indices = torch.max(output_softmax, dim=1, keepdim=True)\n            data[""texts""] = torch.cat(\n                (data[""texts""], indices.view(batch_size_t, 1)), dim=1\n            )\n\n        # Slice data based on batch_size at timestep t\n        data[""texts""] = data[""texts""][:batch_size_t]\n        if ""state"" in data:\n            h1 = data[""state""][""td_hidden""][0][:batch_size_t]\n            c1 = data[""state""][""td_hidden""][1][:batch_size_t]\n            h2 = data[""state""][""lm_hidden""][0][:batch_size_t]\n            c2 = data[""state""][""lm_hidden""][1][:batch_size_t]\n        else:\n            h1, c1 = self.init_hidden_state(data[""texts""])\n            h2, c2 = self.init_hidden_state(data[""texts""])\n        data[""state""] = {""td_hidden"": (h1, c1), ""lm_hidden"": (h2, c2)}\n        registry.register(f""{h1.device}_lstm_state"", data[""state""])\n\n        return data, batch_size_t\n\n    def forward(self, sample_list):\n        # Stores the output probabilites.\n        scores = sample_list.answers.new_ones(\n            (\n                sample_list.answers.size(0),\n                self.text_processor.max_length,\n                self.vocab_size,\n            ),\n            dtype=torch.float,\n        )\n\n        if self.config[""inference""][""type""] in [""beam_search"", ""nucleus_sampling""]:\n            decoder = registry.get_decoder_class(self.config[""inference""][""type""])(\n                self.vocab, self.config\n            )\n            sample_list = decoder.init_batch(sample_list)\n\n        batch_size = sample_list.image_feature_0.size(0)\n        data, sample_list, timesteps = self.prepare_data(sample_list, batch_size)\n        output = None\n        batch_size_t = batch_size\n        for t in range(timesteps):\n            data, batch_size_t = self.get_data_t(t, data, batch_size_t, output)\n            if self.config.inference.type in [""beam_search"", ""nucleus_sampling""]:\n                pi_t = data[""texts""]\n            else:\n                pi_t = data[""texts""][:, t].unsqueeze(-1)\n            embedding = self.word_embedding(pi_t)\n            attention_feature, _ = self.process_feature_embedding(\n                ""image"", sample_list, embedding[:, 0, :], batch_size_t=batch_size_t\n            )\n            output = self.classifier(attention_feature)\n            # Compute decoding\n            if self.config.inference.type in [""beam_search"", ""nucleus_sampling""]:\n                finish, data, batch_size_t = decoder.decode(t, data, output)\n                if finish:\n                    break\n            else:\n                scores[:batch_size_t, t] = output\n\n        model_output = {""scores"": scores}\n        if self.config.inference.type in [""beam_search"", ""nucleus_sampling""]:\n            model_output[""captions""] = decoder.get_result()\n\n        return model_output\n'"
mmf/models/cnn_lstm.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom copy import deepcopy\n\nimport torch\nfrom torch import nn\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.modules.layers import ClassifierLayer, ConvNet, Flatten\n\n_TEMPLATES = {\n    ""question_vocab_size"": ""{}_text_vocab_size"",\n    ""number_of_answers"": ""{}_num_final_outputs"",\n}\n\n_CONSTANTS = {""hidden_state_warning"": ""hidden state (final) should have 1st dim as 2""}\n\n\n@registry.register_model(""cnn_lstm"")\nclass CNNLSTM(BaseModel):\n    """"""CNNLSTM is a simple model for vision and language tasks. CNNLSTM is supposed\n    to acts as a baseline to test out your stuff without any complex functionality.\n    Passes image through a CNN, and text through an LSTM and fuses them using\n    concatenation. Then, it finally passes the fused representation from a MLP to\n    generate scores for each of the possible answers.\n\n    Args:\n        config (DictConfig): Configuration node containing all of the necessary\n                             config required to initialize CNNLSTM.\n\n    Inputs: sample_list (SampleList)\n        - **sample_list** should contain image attribute for image, text for\n          question split into word indices, targets for answer scores\n    """"""\n\n    def __init__(self, config):\n        super().__init__(config)\n        self._global_config = registry.get(""config"")\n        self._datasets = self._global_config.datasets.split("","")\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/cnn_lstm/defaults.yaml""\n\n    def build(self):\n        assert len(self._datasets) > 0\n        num_question_choices = registry.get(\n            _TEMPLATES[""question_vocab_size""].format(self._datasets[0])\n        )\n        num_answer_choices = registry.get(\n            _TEMPLATES[""number_of_answers""].format(self._datasets[0])\n        )\n\n        self.text_embedding = nn.Embedding(\n            num_question_choices, self.config.text_embedding.embedding_dim\n        )\n        self.lstm = nn.LSTM(**self.config.lstm)\n\n        layers_config = self.config.cnn.layers\n        conv_layers = []\n        for i in range(len(layers_config.input_dims)):\n            conv_layers.append(\n                ConvNet(\n                    layers_config.input_dims[i],\n                    layers_config.output_dims[i],\n                    kernel_size=layers_config.kernel_sizes[i],\n                )\n            )\n        conv_layers.append(Flatten())\n        self.cnn = nn.Sequential(*conv_layers)\n\n        # As we generate output dim dynamically, we need to copy the config\n        # to update it\n        classifier_config = deepcopy(self.config.classifier)\n        classifier_config.params.out_dim = num_answer_choices\n        self.classifier = ClassifierLayer(\n            classifier_config.type, **classifier_config.params\n        )\n\n    def forward(self, sample_list):\n        self.lstm.flatten_parameters()\n\n        question = sample_list.text\n        image = sample_list.image\n\n        # Get (h_n, c_n), last hidden and cell state\n        _, hidden = self.lstm(self.text_embedding(question))\n        # X x B x H => B x X x H where X = num_layers * num_directions\n        hidden = hidden[0].transpose(0, 1)\n\n        # X should be 2 so we can merge in that dimension\n        assert hidden.size(1) == 2, _CONSTANTS[""hidden_state_warning""]\n\n        hidden = torch.cat([hidden[:, 0, :], hidden[:, 1, :]], dim=-1)\n        image = self.cnn(image)\n\n        # Fuse into single dimension\n        fused = torch.cat([hidden, image], dim=-1)\n        scores = self.classifier(fused)\n\n        return {""scores"": scores}\n'"
mmf/models/fusions.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom copy import deepcopy\n\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.modules.encoders import MultiModalEncoderBase\nfrom mmf.utils.build import build_classifier_layer\nfrom mmf.utils.modeling import get_bert_configured_parameters\n\n\nclass FusionBase(MultiModalEncoderBase):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n    def build(self):\n        encoders = self._build_encoders(self.config)\n        text_encoder, modal_encoder = encoders[0], encoders[1]\n\n        self._modal_encoder_config = self.config.modal_encoder\n        self._is_direct_features_input = self.config.direct_features_input\n        self._encoder_config = getattr(text_encoder, ""config"", None)\n        self.text = text_encoder\n        self.modal = modal_encoder\n\n    def forward(\n        self,\n        text,\n        modal,\n        text_args=None,\n        modal_args=None,\n        text_kwargs=None,\n        modal_kwargs=None,\n    ):\n        if text_args is None:\n            text_args = []\n        if modal_args is None:\n            modal_args = []\n        if text_kwargs is None:\n            text_kwargs = {}\n        if modal_kwargs is None:\n            modal_kwargs = {}\n        text = self.text(text, *text_args, **text_kwargs)\n\n        # Case of bert encoder, we only need pooled output\n        if len(text) == 2:\n            text = text[1]\n\n        modal = self.modal(modal, *modal_args, **modal_kwargs)\n        modal = torch.flatten(modal, start_dim=1)\n        text = torch.flatten(text, start_dim=1)\n        return text, modal\n\n\n@registry.register_model(""concat_bert"")\nclass ConcatBERT(BaseModel):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config)\n        self._is_direct_features_input = config.direct_features_input\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/fusions/concat_bert.yaml""\n\n    def build(self):\n        self.base = FusionBase(self.config)\n        num_features = self.config.num_features\n        if not self._is_direct_features_input:\n            num_features = self.config.modal_encoder.params.num_output_features\n\n        # As the in_dim is dynamically calculated we need to copy classifier_config\n        classifier_config = deepcopy(self.config.classifier)\n        classifier_config.params.in_dim = num_features * self.config.modal_hidden_size\n        classifier_config.params.in_dim += self.config.text_hidden_size\n        self.classifier = build_classifier_layer(classifier_config)\n\n        if self.config.freeze_text or self.config.freeze_complete_base:\n            for p in self.base.text.parameters():\n                p.requires_grad = False\n\n        if self.config.freeze_modal or self.config.freeze_complete_base:\n            for p in self.base.modal.parameters():\n                p.requires_grad = False\n\n    def get_optimizer_parameters(self, config):\n        # For finetuning setup, we have classifier\n        lr = config.optimizer.params.lr\n        model_config = getattr(config.model_config, config.model, {})\n        finetune_lr_multiplier = getattr(model_config, ""finetune_lr_multiplier"", 1)\n        # Finetune the bert pretrained part with finetune_lr_multiplier if it is set\n        parameters = get_bert_configured_parameters(\n            self.base, lr * finetune_lr_multiplier\n        )\n        parameters += get_bert_configured_parameters(self.classifier, lr)\n        return parameters\n\n    def forward(self, sample_list):\n        text = sample_list.input_ids\n        mask = sample_list.input_mask\n        segment = sample_list.segment_ids\n\n        if self._is_direct_features_input:\n            modal = sample_list.image_features_0\n        else:\n            modal = sample_list.image\n\n        text_embedding, modal_embedding = self.base(text, modal, [mask, segment])\n        embedding = torch.cat([text_embedding, modal_embedding], dim=-1)\n        output = {}\n        output[""scores""] = self.classifier(embedding)\n        return output\n\n\n@registry.register_model(""concat_bow"")\nclass ConcatBoW(BaseModel):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config)\n        self._is_direct_features_input = config.direct_features_input\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/fusions/concat_bow.yaml""\n\n    def build(self):\n        self.base = FusionBase(self.config)\n        num_features = self.config.num_features\n        if not self._is_direct_features_input:\n            num_features = self.config.modal_encoder.params.num_output_features\n\n        # As the in_dim is dynamically calculated we need to copy classifier_config\n        classifier_config = deepcopy(self.config.classifier)\n        classifier_config.params.in_dim = num_features * self.config.modal_hidden_size\n        classifier_config.params.in_dim += self.config.text_hidden_size\n        self.classifier = build_classifier_layer(classifier_config)\n\n    def forward(self, sample_list):\n        text = sample_list.text\n        if self._is_direct_features_input:\n            modal = sample_list.image_feature_0\n        else:\n            modal = sample_list.image\n\n        text_embedding, modal_embedding = self.base(text, modal)\n        embedding = torch.cat([text_embedding, modal_embedding], dim=-1)\n        output = {}\n        output[""scores""] = self.classifier(embedding)\n        return output\n\n\n@registry.register_model(""late_fusion"")\nclass LateFusion(BaseModel):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config)\n        self._is_direct_features_input = config.direct_features_input\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/fusions/late_fusion.yaml""\n\n    def build(self):\n        self.base = FusionBase(self.config)\n        num_features = self.config.num_features\n        if not self._is_direct_features_input:\n            num_features = self.config.modal_encoder.params.num_output_features\n\n        # As the in_dim is dynamically calculated we need to copy classifier_config\n        modal_classifier_config = deepcopy(self.config.modal_classifier)\n        modal_classifier_config.params.in_dim = (\n            num_features * self.config.modal_hidden_size\n        )\n        self.modal_classifier = build_classifier_layer(modal_classifier_config)\n\n        text_classifier_config = deepcopy(self.config.text_classifier)\n        text_classifier_config.params.in_dim = self.config.text_hidden_size\n        self.text_classifier = build_classifier_layer(text_classifier_config)\n\n    def forward(self, sample_list):\n        text = sample_list.input_ids\n        mask = sample_list.input_mask\n        segment = sample_list.segment_ids\n\n        if self._is_direct_features_input:\n            modal = sample_list.image_feature_0\n        else:\n            modal = sample_list.image\n\n        text_embedding, modal_embedding = self.base(text, modal, [mask, segment])\n        text = self.text_classifier(text_embedding)\n        modal = self.modal_classifier(modal_embedding)\n        output = {}\n        output[""scores""] = (text + modal) / 2\n        return output\n'"
mmf/models/lorra.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.common.registry import registry\nfrom mmf.models.pythia import Pythia\n\n\n@registry.register_model(""lorra"")\nclass LoRRA(Pythia):\n    def __init__(self, config):\n        super().__init__(config)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/lorra/defaults.yaml""\n\n    def build(self):\n        self._init_text_embeddings(""text"")\n        # For LoRRA context feature and text embeddings would be identity\n        # but to keep a unified API, we will init them also\n        # and we need to build them first before building pythia\'s other\n        # modules as some of the modules require context attributes to be set\n        self._init_text_embeddings(""context"")\n        self._init_feature_encoders(""context"")\n        self._init_feature_embeddings(""context"")\n        super().build()\n\n    def get_optimizer_parameters(self, config):\n        params = super().get_optimizer_parameters(config)\n        params += [\n            {""params"": self.context_feature_embeddings_list.parameters()},\n            {""params"": self.context_embeddings.parameters()},\n            {""params"": self.context_feature_encoders.parameters()},\n        ]\n\n        return params\n\n    def _get_classifier_input_dim(self):\n        # Now, the classifier\'s input will be cat of image and context based\n        # features\n        return 2 * super()._get_classifier_input_dim()\n\n    def forward(self, sample_list):\n        sample_list.text = self.word_embedding(sample_list.text)\n        text_embedding_total = self.process_text_embedding(sample_list)\n\n        image_embedding_total, _ = self.process_feature_embedding(\n            ""image"", sample_list, text_embedding_total\n        )\n\n        context_embedding_total, _ = self.process_feature_embedding(\n            ""context"", sample_list, text_embedding_total, [""order_vectors""]\n        )\n\n        if self.inter_model is not None:\n            image_embedding_total = self.inter_model(image_embedding_total)\n\n        joint_embedding = self.combine_embeddings(\n            [""image"", ""text""],\n            [image_embedding_total, text_embedding_total, context_embedding_total],\n        )\n\n        scores = self.calculate_logits(joint_embedding)\n\n        return {""scores"": scores}\n'"
mmf/models/m4c.py,23,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport functools\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom transformers.modeling_bert import (\n    BertConfig,\n    BertEmbeddings,\n    BertEncoder,\n    BertLayerNorm,\n    BertPreTrainedModel,\n)\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.modules.encoders import ImageFeatureEncoder\nfrom mmf.modules.layers import ClassifierLayer\n\n\n@registry.register_model(""m4c"")\nclass M4C(BaseModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.mmt_config = BertConfig(**self.config.mmt)\n        self._datasets = registry.get(""config"").datasets.split("","")\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/m4c/defaults.yaml""\n\n    def build(self):\n        # modules requiring custom learning rates (usually for finetuning)\n        self.finetune_modules = []\n\n        # split model building into several components\n        self._build_txt_encoding()\n        self._build_obj_encoding()\n        self._build_ocr_encoding()\n        self._build_mmt()\n        self._build_output()\n\n    def _build_txt_encoding(self):\n        TEXT_BERT_HIDDEN_SIZE = 768\n\n        self.text_bert_config = BertConfig(**self.config.text_bert)\n        if self.config.text_bert_init_from_bert_base:\n            self.text_bert = TextBert.from_pretrained(\n                ""bert-base-uncased"", config=self.text_bert_config\n            )\n            # Use a smaller learning rate on text bert when initializing\n            # from BERT_BASE\n            self.finetune_modules.append(\n                {""module"": self.text_bert, ""lr_scale"": self.config.lr_scale_text_bert}\n            )\n        else:\n            self.writer.write(""NOT initializing text_bert from BERT_BASE"")\n            self.text_bert = TextBert(self.text_bert_config)\n\n        # if the text bert output dimension doesn\'t match the\n        # multimodal transformer (mmt) hidden dimension,\n        # add a linear projection layer between the two\n        if self.mmt_config.hidden_size != TEXT_BERT_HIDDEN_SIZE:\n            self.writer.write(\n                ""Projecting text_bert output to {} dim"".format(\n                    self.mmt_config.hidden_size\n                )\n            )\n            self.text_bert_out_linear = nn.Linear(\n                TEXT_BERT_HIDDEN_SIZE, self.mmt_config.hidden_size\n            )\n        else:\n            self.text_bert_out_linear = nn.Identity()\n\n    def _build_obj_encoding(self):\n        # object appearance feature: Faster R-CNN\n        self.obj_faster_rcnn_fc7 = ImageFeatureEncoder(\n            encoder_type=""finetune_faster_rcnn_fpn_fc7"",\n            in_dim=2048,\n            weights_file=""models/detectron.defaults/fc7_w.pkl"",\n            bias_file=""models/detectron.defaults/fc7_b.pkl"",\n            model_data_dir=self.config.model_data_dir,\n        )\n        # apply smaller lr to pretrained Faster R-CNN fc7\n        self.finetune_modules.append(\n            {""module"": self.obj_faster_rcnn_fc7, ""lr_scale"": self.config.lr_scale_frcn}\n        )\n        self.linear_obj_feat_to_mmt_in = nn.Linear(\n            self.config.obj.mmt_in_dim, self.mmt_config.hidden_size\n        )\n\n        # object location feature: relative bounding box coordinates (4-dim)\n        self.linear_obj_bbox_to_mmt_in = nn.Linear(4, self.mmt_config.hidden_size)\n\n        self.obj_feat_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n        self.obj_bbox_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n        self.obj_drop = nn.Dropout(self.config.obj.dropout_prob)\n\n    def _build_ocr_encoding(self):\n        self.remove_ocr_fasttext = getattr(\n            self.config.ocr, ""remove_ocr_fasttext"", False\n        )\n        self.remove_ocr_phoc = getattr(self.config.ocr, ""remove_ocr_phoc"", False)\n        self.remove_ocr_frcn = getattr(self.config.ocr, ""remove_ocr_frcn"", False)\n        self.remove_ocr_semantics = getattr(\n            self.config.ocr, ""remove_ocr_semantics"", False\n        )\n        self.remove_ocr_bbox = getattr(self.config.ocr, ""remove_ocr_bbox"", False)\n\n        # OCR appearance feature: Faster R-CNN\n        self.ocr_faster_rcnn_fc7 = ImageFeatureEncoder(\n            encoder_type=""finetune_faster_rcnn_fpn_fc7"",\n            in_dim=2048,\n            weights_file=""models/detectron.defaults/fc7_w.pkl"",\n            bias_file=""models/detectron.defaults/fc7_b.pkl"",\n            model_data_dir=self.config.model_data_dir,\n        )\n        self.finetune_modules.append(\n            {""module"": self.ocr_faster_rcnn_fc7, ""lr_scale"": self.config.lr_scale_frcn}\n        )\n\n        self.linear_ocr_feat_to_mmt_in = nn.Linear(\n            self.config.ocr.mmt_in_dim, self.mmt_config.hidden_size\n        )\n\n        # OCR location feature: relative bounding box coordinates (4-dim)\n        self.linear_ocr_bbox_to_mmt_in = nn.Linear(4, self.mmt_config.hidden_size)\n\n        self.ocr_feat_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n        self.ocr_bbox_layer_norm = BertLayerNorm(self.mmt_config.hidden_size)\n        self.ocr_drop = nn.Dropout(self.config.ocr.dropout_prob)\n\n    def _build_mmt(self):\n        self.mmt = MMT(self.mmt_config)\n\n        # allow specifying a different/scaled lr for multimodal transformer\n        self.finetune_modules.append(\n            {""module"": self.mmt, ""lr_scale"": self.config.lr_scale_mmt}\n        )\n\n    def _build_output(self):\n        # dynamic OCR-copying scores with pointer network\n        self.ocr_ptr_net = OcrPtrNet(**self.config.classifier.ocr_ptr_net)\n\n        # fixed answer vocabulary scores\n        num_choices = registry.get(self._datasets[0] + ""_num_final_outputs"")\n        # remove the OCR copying dimensions in LoRRA\'s classifier output\n        # (OCR copying will be handled separately)\n        num_choices -= self.config.classifier.ocr_max_num\n        self.classifier = ClassifierLayer(\n            self.config.classifier.type,\n            in_dim=self.mmt_config.hidden_size,\n            out_dim=num_choices,\n            **self.config.classifier.params,\n        )\n\n        self.answer_processor = registry.get(self._datasets[0] + ""_answer_processor"")\n\n    def forward(self, sample_list):\n        # fwd_results holds intermediate forward pass results\n        # TODO possibly replace it with another sample list\n        fwd_results = {}\n        self._forward_txt_encoding(sample_list, fwd_results)\n        self._forward_obj_encoding(sample_list, fwd_results)\n        self._forward_ocr_encoding(sample_list, fwd_results)\n        self._forward_mmt_and_output(sample_list, fwd_results)\n\n        # only keep scores in the forward pass results\n        results = {""scores"": fwd_results[""scores""]}\n        return results\n\n    def _forward_txt_encoding(self, sample_list, fwd_results):\n        fwd_results[""txt_inds""] = sample_list.text\n\n        # binary mask of valid text (question words) vs padding\n        fwd_results[""txt_mask""] = _get_mask(\n            sample_list.text_len, sample_list.text.size(1)\n        )\n\n    def _forward_obj_encoding(self, sample_list, fwd_results):\n        # object appearance feature: Faster R-CNN fc7\n        obj_fc6 = sample_list.image_feature_0\n        obj_fc7 = self.obj_faster_rcnn_fc7(obj_fc6)\n        obj_fc7 = F.normalize(obj_fc7, dim=-1)\n\n        obj_feat = obj_fc7\n        obj_bbox = sample_list.obj_bbox_coordinates\n        obj_mmt_in = self.obj_feat_layer_norm(\n            self.linear_obj_feat_to_mmt_in(obj_feat)\n        ) + self.obj_bbox_layer_norm(self.linear_obj_bbox_to_mmt_in(obj_bbox))\n        obj_mmt_in = self.obj_drop(obj_mmt_in)\n        fwd_results[""obj_mmt_in""] = obj_mmt_in\n\n        # binary mask of valid object vs padding\n        obj_nums = sample_list.image_info_0.max_features\n        fwd_results[""obj_mask""] = _get_mask(obj_nums, obj_mmt_in.size(1))\n\n    def _forward_ocr_encoding(self, sample_list, fwd_results):\n        # OCR FastText feature (300-dim)\n        ocr_fasttext = sample_list.context_feature_0\n        ocr_fasttext = F.normalize(ocr_fasttext, dim=-1)\n        assert ocr_fasttext.size(-1) == 300\n\n        # OCR PHOC feature (604-dim)\n        ocr_phoc = sample_list.context_feature_1\n        ocr_phoc = F.normalize(ocr_phoc, dim=-1)\n        assert ocr_phoc.size(-1) == 604\n\n        # OCR appearance feature: Faster R-CNN fc7\n        ocr_fc6 = sample_list.image_feature_1[:, : ocr_fasttext.size(1), :]\n        ocr_fc7 = self.ocr_faster_rcnn_fc7(ocr_fc6)\n        ocr_fc7 = F.normalize(ocr_fc7, dim=-1)\n\n        # OCR order vectors (legacy from LoRRA model; set to all zeros)\n        # TODO remove OCR order vectors; they are not needed\n        ocr_order_vectors = torch.zeros_like(sample_list.order_vectors)\n\n        if self.remove_ocr_fasttext:\n            ocr_fasttext = torch.zeros_like(ocr_fasttext)\n        if self.remove_ocr_phoc:\n            ocr_phoc = torch.zeros_like(ocr_phoc)\n        if self.remove_ocr_frcn:\n            ocr_fc7 = torch.zeros_like(ocr_fc7)\n        ocr_feat = torch.cat(\n            [ocr_fasttext, ocr_phoc, ocr_fc7, ocr_order_vectors], dim=-1\n        )\n        ocr_bbox = sample_list.ocr_bbox_coordinates\n        if self.remove_ocr_semantics:\n            ocr_feat = torch.zeros_like(ocr_feat)\n        if self.remove_ocr_bbox:\n            ocr_bbox = torch.zeros_like(ocr_bbox)\n        ocr_mmt_in = self.ocr_feat_layer_norm(\n            self.linear_ocr_feat_to_mmt_in(ocr_feat)\n        ) + self.ocr_bbox_layer_norm(self.linear_ocr_bbox_to_mmt_in(ocr_bbox))\n        ocr_mmt_in = self.ocr_drop(ocr_mmt_in)\n        fwd_results[""ocr_mmt_in""] = ocr_mmt_in\n\n        # binary mask of valid OCR vs padding\n        ocr_nums = sample_list.context_info_0.max_features\n        fwd_results[""ocr_mask""] = _get_mask(ocr_nums, ocr_mmt_in.size(1))\n\n    def _forward_mmt(self, sample_list, fwd_results):\n        # first forward the text BERT layers\n        text_bert_out = self.text_bert(\n            txt_inds=fwd_results[""txt_inds""], txt_mask=fwd_results[""txt_mask""]\n        )\n        fwd_results[""txt_emb""] = self.text_bert_out_linear(text_bert_out)\n\n        mmt_results = self.mmt(\n            txt_emb=fwd_results[""txt_emb""],\n            txt_mask=fwd_results[""txt_mask""],\n            obj_emb=fwd_results[""obj_mmt_in""],\n            obj_mask=fwd_results[""obj_mask""],\n            ocr_emb=fwd_results[""ocr_mmt_in""],\n            ocr_mask=fwd_results[""ocr_mask""],\n            fixed_ans_emb=self.classifier.module.weight,\n            prev_inds=fwd_results[""prev_inds""],\n        )\n        fwd_results.update(mmt_results)\n\n    def _forward_output(self, sample_list, fwd_results):\n        mmt_dec_output = fwd_results[""mmt_dec_output""]\n        mmt_ocr_output = fwd_results[""mmt_ocr_output""]\n        ocr_mask = fwd_results[""ocr_mask""]\n\n        fixed_scores = self.classifier(mmt_dec_output)\n        dynamic_ocr_scores = self.ocr_ptr_net(mmt_dec_output, mmt_ocr_output, ocr_mask)\n        scores = torch.cat([fixed_scores, dynamic_ocr_scores], dim=-1)\n        fwd_results[""scores""] = scores\n\n    def _forward_mmt_and_output(self, sample_list, fwd_results):\n        if self.training:\n            fwd_results[""prev_inds""] = sample_list.train_prev_inds.clone()\n            self._forward_mmt(sample_list, fwd_results)\n            self._forward_output(sample_list, fwd_results)\n        else:\n            dec_step_num = sample_list.train_prev_inds.size(1)\n            # fill prev_inds with BOS_IDX at index 0, and zeros elsewhere\n            fwd_results[""prev_inds""] = torch.zeros_like(sample_list.train_prev_inds)\n            fwd_results[""prev_inds""][:, 0] = self.answer_processor.BOS_IDX\n\n            # greedy decoding at test time\n            for _ in range(dec_step_num):\n                self._forward_mmt(sample_list, fwd_results)\n                self._forward_output(sample_list, fwd_results)\n\n                # find the highest scoring output (either a fixed vocab\n                # or an OCR), and add it to prev_inds for auto-regressive\n                # decoding\n                argmax_inds = fwd_results[""scores""].argmax(dim=-1)\n                fwd_results[""prev_inds""][:, 1:] = argmax_inds[:, :-1]\n\n    def get_optimizer_parameters(self, config):\n        optimizer_param_groups = []\n\n        base_lr = config.optimizer.params.lr\n        # collect all the parameters that need different/scaled lr\n        finetune_params_set = set()\n        for m in self.finetune_modules:\n            optimizer_param_groups.append(\n                {\n                    ""params"": list(m[""module""].parameters()),\n                    ""lr"": base_lr * m[""lr_scale""],\n                }\n            )\n            finetune_params_set.update(list(m[""module""].parameters()))\n        # remaining_params are those parameters w/ default lr\n        remaining_params = [\n            p for p in self.parameters() if p not in finetune_params_set\n        ]\n        # put the default lr parameters at the beginning\n        # so that the printed lr (of group 0) matches the default lr\n        optimizer_param_groups.insert(0, {""params"": remaining_params})\n\n        return optimizer_param_groups\n\n    @classmethod\n    def update_registry_for_pretrained(cls, config, checkpoint, full_output):\n        from omegaconf import OmegaConf\n\n        # Hack datasets using OmegaConf\n        datasets = full_output[""full_config""].datasets\n        dataset = datasets.split("","")[0]\n        config_mock = OmegaConf.create({""datasets"": datasets})\n        registry.register(""config"", config_mock)\n        registry.register(\n            f""{dataset}_num_final_outputs"",\n            # Need to add as it is subtracted\n            checkpoint[""classifier.module.weight""].size(0)\n            + config.classifier.ocr_max_num,\n        )\n        # Fix this later, when processor pipeline is available\n        answer_processor = OmegaConf.create({""BOS_IDX"": 1})\n        registry.register(f""{dataset}_answer_processor"", answer_processor)\n\n\nclass TextBert(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.init_weights()\n\n    def forward(self, txt_inds, txt_mask):\n        encoder_inputs = self.embeddings(txt_inds)\n        attention_mask = txt_mask\n\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        assert not extended_attention_mask.requires_grad\n        head_mask = [None] * self.config.num_hidden_layers\n\n        encoder_outputs = self.encoder(\n            encoder_inputs, extended_attention_mask, head_mask=head_mask\n        )\n        seq_output = encoder_outputs[0]\n\n        return seq_output\n\n\nclass MMT(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.prev_pred_embeddings = PrevPredEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.init_weights()\n\n    def forward(\n        self,\n        txt_emb,\n        txt_mask,\n        obj_emb,\n        obj_mask,\n        ocr_emb,\n        ocr_mask,\n        fixed_ans_emb,\n        prev_inds,\n    ):\n\n        # build embeddings for predictions in previous decoding steps\n        # fixed_ans_emb is an embedding lookup table for each fixed vocabulary\n        dec_emb = self.prev_pred_embeddings(fixed_ans_emb, ocr_emb, prev_inds)\n\n        # a zero mask for decoding steps, so the encoding steps elements can\'t\n        # attend to decoding steps.\n        # A triangular causal mask will be filled for the decoding steps\n        # later in extended_attention_mask\n        dec_mask = torch.zeros(\n            dec_emb.size(0), dec_emb.size(1), dtype=torch.float32, device=dec_emb.device\n        )\n        encoder_inputs = torch.cat([txt_emb, obj_emb, ocr_emb, dec_emb], dim=1)\n        attention_mask = torch.cat([txt_mask, obj_mask, ocr_mask, dec_mask], dim=1)\n\n        # offsets of each modality in the joint embedding space\n        txt_max_num = txt_mask.size(-1)\n        obj_max_num = obj_mask.size(-1)\n        ocr_max_num = ocr_mask.size(-1)\n        dec_max_num = dec_mask.size(-1)\n        txt_begin = 0\n        txt_end = txt_begin + txt_max_num\n        ocr_begin = txt_max_num + obj_max_num\n        ocr_end = ocr_begin + ocr_max_num\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, from_seq_length, to_seq_length]\n        # So we can broadcast to\n        # [batch_size, num_heads, from_seq_length, to_seq_length]\n        to_seq_length = attention_mask.size(1)\n        from_seq_length = to_seq_length\n\n        # generate the attention mask similar to prefix LM\n        # all elements can attend to the elements in encoding steps\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        extended_attention_mask = extended_attention_mask.repeat(\n            1, 1, from_seq_length, 1\n        )\n        # decoding step elements can attend to themselves in a causal manner\n        extended_attention_mask[:, :, -dec_max_num:, -dec_max_num:] = _get_causal_mask(\n            dec_max_num, encoder_inputs.device\n        )\n\n        # flip the mask, so that invalid attention pairs have -10000.\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        assert not extended_attention_mask.requires_grad\n        head_mask = [None] * self.config.num_hidden_layers\n\n        encoder_outputs = self.encoder(\n            encoder_inputs, extended_attention_mask, head_mask=head_mask\n        )\n\n        mmt_seq_output = encoder_outputs[0]\n        mmt_txt_output = mmt_seq_output[:, txt_begin:txt_end]\n        mmt_ocr_output = mmt_seq_output[:, ocr_begin:ocr_end]\n        mmt_dec_output = mmt_seq_output[:, -dec_max_num:]\n\n        results = {\n            ""mmt_seq_output"": mmt_seq_output,\n            ""mmt_txt_output"": mmt_txt_output,\n            ""mmt_ocr_output"": mmt_ocr_output,\n            ""mmt_dec_output"": mmt_dec_output,\n        }\n        return results\n\n\nclass OcrPtrNet(nn.Module):\n    def __init__(self, hidden_size, query_key_size=None):\n        super().__init__()\n\n        if query_key_size is None:\n            query_key_size = hidden_size\n        self.hidden_size = hidden_size\n        self.query_key_size = query_key_size\n\n        self.query = nn.Linear(hidden_size, query_key_size)\n        self.key = nn.Linear(hidden_size, query_key_size)\n\n    def forward(self, query_inputs, key_inputs, attention_mask):\n        extended_attention_mask = (1.0 - attention_mask) * -10000.0\n        assert extended_attention_mask.dim() == 2\n        extended_attention_mask = extended_attention_mask.unsqueeze(1)\n\n        query_layer = self.query(query_inputs)\n        if query_layer.dim() == 2:\n            query_layer = query_layer.unsqueeze(1)\n            squeeze_result = True\n        else:\n            squeeze_result = False\n        key_layer = self.key(key_inputs)\n\n        scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        scores = scores / math.sqrt(self.query_key_size)\n        scores = scores + extended_attention_mask\n        if squeeze_result:\n            scores = scores.squeeze(1)\n\n        return scores\n\n\nclass PrevPredEmbeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        MAX_DEC_LENGTH = 100\n        MAX_TYPE_NUM = 5\n        hidden_size = config.hidden_size\n        ln_eps = config.layer_norm_eps\n\n        self.position_embeddings = nn.Embedding(MAX_DEC_LENGTH, hidden_size)\n        self.token_type_embeddings = nn.Embedding(MAX_TYPE_NUM, hidden_size)\n\n        self.ans_layer_norm = BertLayerNorm(hidden_size, eps=ln_eps)\n        self.ocr_layer_norm = BertLayerNorm(hidden_size, eps=ln_eps)\n        self.emb_layer_norm = BertLayerNorm(hidden_size, eps=ln_eps)\n        self.emb_dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, ans_emb, ocr_emb, prev_inds):\n        assert prev_inds.dim() == 2 and prev_inds.dtype == torch.long\n        assert ans_emb.dim() == 2\n\n        batch_size = prev_inds.size(0)\n        seq_length = prev_inds.size(1)\n        ans_num = ans_emb.size(0)\n\n        # apply layer normalization to both answer embedding and OCR embedding\n        # before concatenation, so that they have the same scale\n        ans_emb = self.ans_layer_norm(ans_emb)\n        ocr_emb = self.ocr_layer_norm(ocr_emb)\n        assert ans_emb.size(-1) == ocr_emb.size(-1)\n        ans_emb = ans_emb.unsqueeze(0).expand(batch_size, -1, -1)\n        ans_ocr_emb_cat = torch.cat([ans_emb, ocr_emb], dim=1)\n        raw_dec_emb = _batch_gather(ans_ocr_emb_cat, prev_inds)\n\n        # Add position and type embedding for previous predictions\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=ocr_emb.device)\n        position_ids = position_ids.unsqueeze(0).expand(batch_size, seq_length)\n        position_embeddings = self.position_embeddings(position_ids)\n        # Token type ids: 0 -- vocab; 1 -- OCR\n        token_type_ids = prev_inds.ge(ans_num).long()\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        embeddings = position_embeddings + token_type_embeddings\n        embeddings = self.emb_layer_norm(embeddings)\n        embeddings = self.emb_dropout(embeddings)\n        dec_emb = raw_dec_emb + embeddings\n\n        return dec_emb\n\n\ndef _get_mask(nums, max_num):\n    # non_pad_mask: b x lq, torch.float32, 0. on PAD\n    batch_size = nums.size(0)\n    arange = torch.arange(0, max_num).unsqueeze(0).expand(batch_size, -1)\n    non_pad_mask = arange.to(nums.device).lt(nums.unsqueeze(-1))\n    non_pad_mask = non_pad_mask.type(torch.float32)\n    return non_pad_mask\n\n\n@functools.lru_cache(maxsize=32)\ndef _get_causal_mask(seq_length, device):\n    # generate a lower triangular mask\n    mask = torch.zeros(seq_length, seq_length, device=device)\n    for i in range(seq_length):\n        for j in range(i + 1):\n            mask[i, j] = 1.0\n    return mask\n\n\ndef _batch_gather(x, inds):\n    assert x.dim() == 3\n    batch_size = x.size(0)\n    length = x.size(1)\n    dim = x.size(2)\n    x_flat = x.view(batch_size * length, dim)\n\n    batch_offsets = torch.arange(batch_size, device=inds.device) * length\n    batch_offsets = batch_offsets.unsqueeze(-1)\n    assert batch_offsets.dim() == inds.dim()\n    inds_flat = batch_offsets + inds\n    results = F.embedding(inds_flat, x_flat)\n    return results\n'"
mmf/models/m4c_captioner.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import registry\nfrom mmf.models.m4c import M4C\n\n\n@registry.register_model(""m4c_captioner"")\nclass M4CCaptioner(M4C):\n    def __init__(self, config):\n        super().__init__(config)\n        self.remove_unk_in_pred = self.config.remove_unk_in_pred\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/m4c_captioner/defaults.yaml""\n\n    def _forward_output(self, sample_list, fwd_results):\n        super()._forward_output(sample_list, fwd_results)\n\n        if self.remove_unk_in_pred:\n            # avoid outputting <unk> in the generated captions\n            fwd_results[""scores""][..., self.answer_processor.UNK_IDX] = -1e10\n\n        return fwd_results\n'"
mmf/models/mmbt.py,20,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n# MMBTModel, ModalEmbeddings is copied from [1]\n# as we have internal dependency on transformers v2.3.\n# These will be removed when we upgrade to package v2.5+.\n# [1]: https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_mmbt.py # noqa\n\nimport os\nfrom copy import deepcopy\n\nimport torch\nfrom omegaconf import OmegaConf\nfrom torch import nn\nfrom transformers.modeling_bert import BertForPreTraining, BertPredictionHeadTransform\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.models.interfaces.mmbt import MMBTGridHMInterface\nfrom mmf.modules.encoders import MultiModalEncoderBase\nfrom mmf.utils.checkpoint import load_pretrained_model\nfrom mmf.utils.configuration import get_mmf_cache_dir\nfrom mmf.utils.modeling import get_optimizer_parameters_for_bert\n\n\n# TODO: Remove after transformers package upgrade to 2.5\nclass MMBTConfig:\n    """"""Configuration class to store the configuration of a `MMBT Model`.\n    Args:\n        config (:obj:`~transformers.PreTrainedConfig`):\n            Config of the underlying Transformer models. Its values are\n            copied over to use a single config.\n        num_labels (:obj:`int` or :obj:`None`, optional, defaults to `None`):\n            Size of final Linear layer for classification.\n        modal_hidden_size (:obj:`int`, optional, defautls to 2048):\n            Embedding dimension of the non-text modality encoder.\n    """"""\n\n    def __init__(self, config, num_labels=None, modal_hidden_size=2048):\n        self.__dict__ = config.__dict__\n        self.modal_hidden_size = modal_hidden_size\n        if num_labels:\n            self.num_labels = num_labels\n\n\n# TODO: Remove after transformers package upgrade to 2.5\nclass ModalEmbeddings(nn.Module):\n    """"""Generic Modal Embeddings which takes in an encoder, and a transformer embedding.\n    """"""\n\n    def __init__(self, config, encoder, embeddings):\n        super().__init__()\n        self.config = config\n        self.encoder = encoder\n        self.proj_embeddings = nn.Linear(config.modal_hidden_size, config.hidden_size)\n        self.position_embeddings = embeddings.position_embeddings\n        self.token_type_embeddings = embeddings.token_type_embeddings\n        self.word_embeddings = embeddings.word_embeddings\n        self.LayerNorm = embeddings.LayerNorm\n        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)\n\n    def forward(\n        self,\n        input_modal,\n        start_token=None,\n        end_token=None,\n        position_ids=None,\n        token_type_ids=None,\n    ):\n        token_embeddings = self.proj_embeddings(self.encoder(input_modal))\n        seq_length = token_embeddings.size(1)\n\n        if start_token is not None:\n            start_token_embeds = self.word_embeddings(start_token)\n            seq_length += 1\n            token_embeddings = torch.cat(\n                [start_token_embeds.unsqueeze(1), token_embeddings], dim=1\n            )\n\n        if end_token is not None:\n            end_token_embeds = self.word_embeddings(end_token)\n            seq_length += 1\n            token_embeddings = torch.cat(\n                [token_embeddings, end_token_embeds.unsqueeze(1)], dim=1\n            )\n\n        if position_ids is None:\n            position_ids = torch.arange(\n                seq_length, dtype=torch.long, device=input_modal.device\n            )\n            position_ids = position_ids.unsqueeze(0).expand(\n                input_modal.size(0), seq_length\n            )\n\n        if token_type_ids is None:\n            token_type_ids = torch.zeros(\n                (input_modal.size(0), seq_length),\n                dtype=torch.long,\n                device=input_modal.device,\n            )\n\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        embeddings = token_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\n# TODO: Remove after transformers package upgrade to 2.5\nclass MMBTModel(nn.Module):\n    r""""""\n        Outputs: `Tuple` comprising various elements depending on the configuration\n            (config) and inputs:\n            **last_hidden_state**: ``torch.FloatTensor`` of shape\n                ``(batch_size, sequence_length, hidden_size)``. Sequence of\n                hidden-states at the output of the last layer of the model.\n            **pooler_output**: ``torch.FloatTensor`` of shape\n                ``(batch_size, hidden_size)``. Last layer hidden-state of the\n                first token of the sequence (classification token) further processed\n                by a Linear layer and a Tanh activation function. The Linear\n                layer weights are trained from the next sentence prediction\n                (classification) objective during Bert pretraining. This output\n                is usually *not* a good summary of the semantic content of the\n                input, you\'re often better with averaging or pooling\n                the sequence of hidden-states for the whole input sequence.\n            **hidden_states**: (`optional`, returned when\n                ``config.output_hidden_states=True``)\n                list of ``torch.FloatTensor`` (one for the output of each layer +\n                the output of the embeddings)\n                of shape ``(batch_size, sequence_length, hidden_size)``:\n                Hidden-states of the model at the output of each layer plus the\n                initial embedding outputs.\n            **attentions**: (`optional`, returned when\n                ``config.output_attentions=True``) list of ``torch.FloatTensor``\n                (one for each layer) of shape ``(batch_size, num_heads,\n                sequence_length, sequence_length)``: Attentions weights after\n                the attention softmax, used to compute the weighted average in the\n                self-attention heads.\n        Examples::\n            # For example purposes. Not runnable.\n            transformer = BertModel.from_pretrained(\'bert-base-uncased\')\n            encoder = ImageEncoder(args)\n            mmbt = MMBTModel(config, transformer, encoder)\n        """"""\n\n    def __init__(self, config, transformer, encoder):\n        super().__init__()\n        self.config = config\n        self.transformer = transformer\n        self.modal_encoder = ModalEmbeddings(config, encoder, transformer.embeddings)\n\n    def forward(\n        self,\n        input_modal,\n        input_ids=None,\n        modal_start_tokens=None,\n        modal_end_tokens=None,\n        attention_mask=None,\n        token_type_ids=None,\n        modal_token_type_ids=None,\n        position_ids=None,\n        modal_position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n    ):\n\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\n                ""You cannot specify both input_ids and inputs_embeds at the same time""\n            )\n        elif input_ids is not None:\n            input_txt_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_txt_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(""You have to specify either input_ids or inputs_embeds"")\n\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n\n        modal_embeddings = self.modal_encoder(\n            input_modal,\n            start_token=modal_start_tokens,\n            end_token=modal_end_tokens,\n            position_ids=modal_position_ids,\n            token_type_ids=modal_token_type_ids,\n        )\n\n        input_modal_shape = modal_embeddings.size()[:-1]\n\n        if token_type_ids is None:\n            token_type_ids = torch.ones(\n                input_txt_shape, dtype=torch.long, device=device\n            )\n\n        txt_embeddings = self.transformer.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            token_type_ids=token_type_ids,\n            inputs_embeds=inputs_embeds,\n        )\n\n        embedding_output = torch.cat([modal_embeddings, txt_embeddings], 1)\n\n        input_shape = embedding_output.size()[:-1]\n\n        if attention_mask is None:\n            attention_mask = torch.ones(input_shape, device=device)\n        else:\n            attention_mask = torch.cat(\n                [\n                    torch.ones(input_modal_shape, device=device, dtype=torch.long),\n                    attention_mask,\n                ],\n                dim=1,\n            )\n\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(input_shape, device=device)\n        else:\n            encoder_attention_mask = torch.cat(\n                [torch.ones(input_modal_shape, device=device), encoder_attention_mask],\n                dim=1,\n            )\n\n        # We can provide a self-attention mask of dimensions\n        # [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n\n        # Provided a padding mask of dimensions [batch_size, seq_length]\n        # - if the model is a decoder, apply a causal mask in addition to the\n        #   padding mask\n        # - if the model is an encoder, make the mask broadcastable to\n        # [batch_size, num_heads, seq_length, seq_length]\n        if attention_mask.dim() == 2:\n            if self.config.is_decoder:\n                batch_size, seq_length = input_shape\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # If a 2D ou 3D attention mask is provided for the cross-attention\n        # we need to make broadcastabe to\n        # [batch_size, num_heads, seq_length, seq_length]\n        if encoder_attention_mask.dim() == 3:\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n        if encoder_attention_mask.dim() == 2:\n            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n\n        encoder_extended_attention_mask = encoder_extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        encoder_extended_attention_mask = (\n            1.0 - encoder_extended_attention_mask\n        ) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape\n        # [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = (\n                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                )\n                head_mask = head_mask.expand(\n                    self.config.num_hidden_layers, -1, -1, -1, -1\n                )\n            elif head_mask.dim() == 2:\n                head_mask = (\n                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n                )  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype\n            )  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        encoder_outputs = self.transformer.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n        )\n\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.transformer.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output) + encoder_outputs[\n            1:\n        ]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n\nclass MMBTBase(MultiModalEncoderBase):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n    def build(self):\n        encoders = self._build_encoders(self.config)\n        text_encoder, modal_encoder = encoders[0], encoders[1]\n        self._encoder_config = text_encoder.config\n\n        self._mmbt_config = MMBTConfig(\n            self._encoder_config,\n            num_labels=self.config.num_labels,\n            modal_hidden_size=self.config.modal_hidden_size,\n        )\n\n        self.mmbt = MMBTModel(self._mmbt_config, text_encoder, modal_encoder)\n\n    def forward(self, sample_list):\n        if self._is_direct_features_input:\n            input_modal = sample_list.image_feature_0\n        else:\n            input_modal = sample_list.image\n\n        modal_start_token = None\n        if self.config.use_modal_start_token:\n            modal_start_token = sample_list.input_ids[:, 0].clone().detach()\n\n        modal_end_token = None\n        if self.config.use_modal_end_token:\n            modal_end_token = sample_list.input_ids[:, -1].clone().detach()\n\n        # See details of inputs at\n        # https://github.com/huggingface/transformers/blob/1789c7/src/transformers/modeling_mmbt.py#L101 # noqa\n        output = self.mmbt(\n            input_modal,\n            input_ids=sample_list.input_ids,\n            modal_start_tokens=modal_start_token,\n            modal_end_tokens=modal_end_token,\n            attention_mask=sample_list.input_mask,\n            token_type_ids=sample_list.segment_ids,\n            modal_token_type_ids=None,\n            position_ids=None,\n            modal_position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            encoder_hidden_states=None,\n            encoder_attention_mask=None,\n        )\n\n        return output\n\n\nclass MMBTForPreTraining(nn.Module):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__()\n        self.config = config\n        self.bert = MMBTBase(config, *args, **kwargs)\n        self.encoder_config = self.bert.encoder_config\n\n        # TODO : Switch to AutoModelForPreTraining after transformers\n        # package upgrade to 2.5\n        pretraining_module = BertForPreTraining.from_pretrained(\n            self.config.bert_model_name,\n            config=self.encoder_config,\n            cache_dir=os.path.join(get_mmf_cache_dir(), ""distributed_{}"".format(-1)),\n        )\n\n        self.cls = deepcopy(pretraining_module.cls)\n        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we\n            are cloning them instead.\n        """"""\n        if hasattr(self, ""cls""):\n            self.bert.mmbt.transformer._tie_or_clone_weights(\n                self.cls.predictions.decoder,\n                self.bert.mmbt.transformer.embeddings.word_embeddings,\n            )\n\n    def forward(self, sample_list):\n        module_output = self.bert(sample_list)\n        sequence_output, pooled_output = module_output[0], module_output[1]\n        prediction_scores, seq_relationship_score = self.cls(\n            sequence_output, pooled_output\n        )\n\n        output = {}\n        if (\n            self.encoder_config.output_hidden_states\n            or self.encoder_config.output_attentions\n        ):\n            output[""extras""] = module_output[2:]\n\n        loss_key = f""{sample_list.dataset_name}/{sample_list.dataset_type}""\n\n        if ""lm_label_ids"" in sample_list and sample_list.lm_label_ids is not None:\n            output[""logits""] = prediction_scores\n            lm_label_ids = sample_list.lm_label_ids\n            # Only take last scores which are text\'s scores and ignore image scores\n            text_scores = (\n                prediction_scores[:, -lm_label_ids.size(1) :]\n                .contiguous()\n                .view(-1, self.encoder_config.vocab_size)\n            )\n            masked_lm_loss = self.loss_fct(\n                text_scores, sample_list.lm_label_ids.contiguous().view(-1)\n            )\n            output[""losses""] = {}\n            output[""losses""][f""{loss_key}/masked_lm_loss""] = masked_lm_loss\n\n        # Add alignment loss if present\n        if (\n            ""image_text_alignment"" in sample_list\n            and sample_list.image_text_alignment is not None\n        ):\n            output[""seq_relationship_logits""] = seq_relationship_score\n            alignment_loss = self.loss_fct(\n                seq_relationship_score.contiguous().view(-1),\n                sample_list.image_text_alignment.contiguous().view(-1),\n            )\n            output[""losses""][f""{loss_key}/alignment_loss""] = alignment_loss\n\n        return output\n\n\nclass MMBTForClassification(nn.Module):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__()\n        self.config = config\n        self.bert = MMBTBase(config, *args, **kwargs)\n        self.encoder_config = self.bert.encoder_config\n\n        self.dropout = nn.Dropout(self.encoder_config.hidden_dropout_prob)\n        self.classifier = nn.Sequential(\n            BertPredictionHeadTransform(self.encoder_config),\n            nn.Linear(self.encoder_config.hidden_size, self.config.num_labels),\n        )\n\n    def forward(self, sample_list):\n        module_output = self.bert(sample_list)\n        pooled_output = module_output[1]\n        output = {}\n\n        if (\n            self.encoder_config.output_hidden_states\n            or self.encoder_config.output_attentions\n        ):\n            output[""extras""] = module_output[2:]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.contiguous().view(-1, self.config.num_labels)\n        output[""scores""] = reshaped_logits\n\n        return output\n\n\n@registry.register_model(""mmbt"")\nclass MMBT(BaseModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n    def build(self):\n        if self.config.training_head_type == ""pretraining"":\n            self.model = MMBTForPreTraining(self.config)\n        else:\n            self.model = MMBTForClassification(self.config)\n\n        if self.config.freeze_complete_base or self.config.freeze_text:\n            for p in self.model.bert.mmbt.transformer.parameters():\n                p.requires_grad = False\n\n        if self.config.freeze_complete_base or self.config.freeze_modal:\n            for p in self.model.bert.mmbt.modal_encoder.parameters():\n                p.requires_grad = False\n\n    # Backward compatibility for code from older mmbt\n    @classmethod\n    def format_state_key(cls, key):\n        return (\n            key.replace(""base.bert"", ""model.bert"")\n            .replace(""base.cls"", ""model.cls"")\n            .replace(""base.classifier"", ""model.classifier"")\n        )\n\n    @classmethod\n    def from_pretrained(cls, model_name, *args, **kwargs):\n        model = super().from_pretrained(model_name, *args, **kwargs)\n        config = load_pretrained_model(model_name)[""full_config""]\n        OmegaConf.set_struct(config, True)\n        if model_name == ""mmbt.hateful_memes.images"":\n            return MMBTGridHMInterface(model, config)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/mmbt/pretrain.yaml""\n\n    def forward(self, sample_list):\n        return self.model(sample_list)\n\n    def get_optimizer_parameters(self, config):\n        return get_optimizer_parameters_for_bert(self.model, config)\n'"
mmf/models/mmf_bert.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport torch\nfrom torch import nn\nfrom transformers.modeling_bert import (\n    BertConfig,\n    BertEmbeddings,\n    BertForPreTraining,\n    BertLayerNorm,\n    BertPooler,\n    BertPredictionHeadTransform,\n    BertPreTrainingHeads,\n)\n\nfrom mmf.common.registry import registry\nfrom mmf.models.pythia import Pythia\nfrom mmf.modules.embeddings import ProjectionEmbedding\nfrom mmf.utils.transform import transform_to_batch_sequence\n\n\n@registry.register_model(""mmf_bert"")\nclass MMFBert(Pythia):\n    def __init__(self, config):\n        super().__init__(config)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/mmf_bert/defaults.yaml""\n\n    def build(self):\n        super().build()\n        self.tie_weights()\n\n        if getattr(self.config, ""freeze_base"", False):\n            for n, p in self.named_parameters():\n                if ""classifier"" not in n:\n                    p.requires_grad = False\n\n    def _build_word_embedding(self):\n        self.bert_config = BertConfig.from_pretrained(self.config.bert_model_name)\n        if self.config.pretrained_bert:\n            bert_model = BertForPreTraining.from_pretrained(self.config.bert_model_name)\n            self.word_embedding = bert_model.bert.embeddings\n            self.pooler = bert_model.bert.pooler\n            self.pooler.apply(self.init_weights)\n\n        else:\n            self.pooler = BertPooler(self.bert_config)\n            self.word_embedding = BertEmbeddings(self.bert_config)\n\n    def _init_classifier(self, hidden_size):\n        if ""pretraining"" in self.config.training_head_type:\n            self.classifier = BertPreTrainingHeads(self.bert_config)\n        if ""vqa"" in self.config.training_head_type:\n            self.dropout = nn.Dropout(self.bert_config.hidden_dropout_prob)\n            self.answer_space_size = 3129\n            self.classifier = nn.Sequential(\n                BertPredictionHeadTransform(self.bert_config),\n                nn.Linear(self.bert_config.hidden_size, self.answer_space_size),\n            )\n            # self.classifier = nn.Linear(self.bert_config.hidden_size,\n            # self.answer_space_size)\n        elif ""vizwiz"" in self.config.training_head_type:\n            self.dropout = nn.Dropout(self.bert_config.hidden_dropout_prob)\n            self.answer_space_size = 7371\n            self.classifier = nn.Sequential(\n                BertPredictionHeadTransform(self.bert_config),\n                nn.Linear(self.bert_config.hidden_size, self.answer_space_size),\n            )\n            # self.classifier = nn.Linear(self.bert_config.hidden_size,\n            # self.answer_space_size)\n        elif self.config.training_head_type == ""visual_entailment"":\n            self.dropout = nn.Dropout(self.bert_config.hidden_dropout_prob)\n            self.classifier = nn.Sequential(\n                BertPredictionHeadTransform(self.bert_config),\n                nn.Linear(self.bert_config.hidden_size, 3),\n            )\n            # self.classifier = nn.Linear(self.bert_config.hidden_size, 3)\n\n    def _init_text_embeddings(self, attr=""text""):\n        self.text_embeddings_out_dim = self.bert_config.hidden_size\n        self.text_embedding = nn.MultiheadAttention(**self.config.text_embeddings[0])\n\n    def _tie_or_clone_weights(self, first_module, second_module):\n        """""" Tie or clone module weights depending of weither we are using\n        TorchScript or not\n        """"""\n        if self.config.torchscript:\n            first_module.weight = nn.Parameter(second_module.weight.clone())\n        else:\n            first_module.weight = second_module.weight\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning\n            them instead.\n        """"""\n        if hasattr(self, ""cls""):\n            self._tie_or_clone_weights(\n                self.cls.predictions.decoder, self.word_embeddings.word_embeddings\n            )\n\n    def _init_feature_embeddings(self, attr):\n        feature_embeddings_list = []\n        num_feature_feat = len(getattr(self.config, f""{attr}_feature_encodings""))\n\n        self.image_feature_projection = ProjectionEmbedding(\n            **self.config.image_feature_projection\n        )\n        self.feature_embeddings_out_dim = 0\n\n        if self.config.image_intra_attention:\n            self.image_feature_intra_attention = nn.MultiheadAttention(\n                **self.config.image_feature_attentions[0]\n            )\n\n        for _ in range(num_feature_feat):\n            feature_embeddings = []\n            feature_attn_model_list = self.config[attr + ""_feature_embeddings""]\n\n            for feature_attn_model_params in feature_attn_model_list:\n                feature_embedding = nn.MultiheadAttention(**feature_attn_model_params)\n                feature_embeddings.append(feature_embedding)\n                self.feature_embeddings_out_dim += feature_attn_model_params[\n                    ""embed_dim""\n                ]\n\n            feature_embeddings = nn.ModuleList(feature_embeddings)\n            feature_embeddings_list.append(feature_embeddings)\n\n        setattr(\n            self, attr + ""_feature_embeddings_out_dim"", self.feature_embeddings_out_dim\n        )\n        del self.feature_embeddings_out_dim\n        setattr(\n            self,\n            attr + ""_feature_embeddings_list"",\n            nn.ModuleList(feature_embeddings_list),\n        )\n\n    def get_optimizer_parameters(self, config):\n        param_optimizer = list(self.named_parameters())\n        image_feature_encoders_params = [\n            n for n in param_optimizer if ""image_feature_encoders"" in n[0]\n        ]\n        param_optimizer = [\n            n for n in param_optimizer if ""image_feature_encoders"" not in n[0]\n        ]\n\n        no_decay = [""bias"", ""LayerNorm.bias"", ""LayerNorm.weight""]\n        optimizer_grouped_parameters = [\n            {\n                ""params"": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.01,\n            },\n            {\n                ""params"": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.0,\n            },\n            {\n                ""params"": [p for _, p in image_feature_encoders_params],\n                ""lr"": (config.optimizer.params.lr * 0.1),\n                ""weight_decay"": 0.01,\n            },\n        ]\n\n        return optimizer_grouped_parameters\n\n    # WARNING(ASG): This doesn\'t have finetune_lr_multiplier option enabled yet\n\n    def process_text_embedding(self, text_embedding, key_padding_mask=None):\n        text_embedding = text_embedding.transpose(0, 1)\n        embedding, _ = self.text_embedding(\n            text_embedding,\n            text_embedding,\n            text_embedding,\n            key_padding_mask=key_padding_mask,\n        )\n\n        return embedding.transpose(0, 1)\n\n    def process_feature_embedding(\n        self,\n        attr,\n        sample_list,\n        text_embedding_total,\n        key_padding_mask=None,\n        attn_mask=None,\n        extra=None,\n        batch_size_t=None,\n    ):\n        if extra is None:\n            extra = []\n        feature_embeddings = []\n        feature_attentions = []\n        features = []\n        batch_size_t = (\n            sample_list.get_batch_size() if batch_size_t is None else batch_size_t\n        )\n\n        # Convert list of keys to the actual values\n        extra = sample_list.get_fields(extra)\n\n        feature_idx = 0\n\n        # Get all of the features, which are in the form, ""image_feature_0""\n        # ""image_feature_1"" ...\n        while True:\n            feature = getattr(sample_list, f""{attr}_feature_{feature_idx:d}"", None)\n            if feature is None:\n                break\n            feature_idx += 1\n            feature = feature[:batch_size_t]\n            features.append(feature)\n\n        feature_encoders = getattr(self, attr + ""_feature_encoders"")\n        # Each feature should have a separate image feature encoders\n        assert len(features) == len(feature_encoders), (\n            ""Number of feature encoders, {} are not equal ""\n            ""to number of features, {}."".format(len(feature_encoders), len(features))\n        )\n\n        # Now, iterate to get final attended image features\n        for i, feature in enumerate(features):\n            # Get info related to the current feature. info is generally\n            # in key of format ""image_info_0"" for 0th feature\n            feature_info = getattr(sample_list, f""{attr}_info_{i:d}"", {})\n            # For Pythia, we need max_features to mask attention\n            feature_dim = getattr(feature_info, ""max_features"", None)\n            if feature_dim is not None:\n                feature_dim = feature_dim[:batch_size_t]\n\n            # Attribute in which encoders are saved, for ""image"" it\n            # will be ""image_feature_encoders"", other example is\n            # ""context_feature_encoders""\n            encoders_attr = attr + ""_feature_encoders""\n            feature_encoder = getattr(self, encoders_attr)[i]\n\n            # Encode the features\n            encoded_feature = feature_encoder(feature)\n\n            # Get all of the feature embeddings\n            list_attr = attr + ""_feature_embeddings_list""\n            feature_embedding_models = getattr(self, list_attr)[i]\n            encoded_feature = self.image_feature_projection(encoded_feature)\n            encoded_feature = encoded_feature.transpose(0, 1)\n            text_embedding_total = text_embedding_total.transpose(0, 1)\n\n            if self.config.image_intra_attention:\n                encoded_feature, _ = self.image_feature_intra_attention(\n                    encoded_feature,\n                    encoded_feature,\n                    encoded_feature,\n                    key_padding_mask=key_padding_mask,\n                    attn_mask=attn_mask,\n                )\n            # Forward through these embeddings one by one\n            for feature_embedding_model in feature_embedding_models:\n                inp = (text_embedding_total, encoded_feature, encoded_feature)\n                embedding, attention = feature_embedding_model(\n                    *inp, key_padding_mask=key_padding_mask, attn_mask=attn_mask\n                )\n                feature_embeddings.append(embedding.transpose(0, 1))\n                feature_attentions.append(attention.squeeze(-1))\n\n        # Concatenate all features embeddings and return along with attention\n        feature_embedding_total = torch.cat(feature_embeddings, dim=1)\n        return feature_embedding_total, feature_attentions\n\n    def init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal\n            # for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.bert_config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    def forward(self, sample_list):\n        # bert text input\n        input_ids = sample_list.input_ids\n        input_mask = sample_list.input_mask\n        input_type_ids = sample_list.segment_ids\n        input_ids = transform_to_batch_sequence(input_ids)\n        input_type_ids = transform_to_batch_sequence(input_type_ids)\n        input_mask = transform_to_batch_sequence(input_mask)\n\n        if input_mask is None:\n            input_mask = torch.ones_like(input_ids)\n        if input_type_ids is None:\n            input_type_ids = torch.zeros_like(input_ids)\n        attention_mask = input_mask.unsqueeze(1).unsqueeze(2)\n\n        # pretraining labels\n        masked_lm_labels = getattr(sample_list, ""lm_label_ids"", None)\n        masked_lm_labels = transform_to_batch_sequence(masked_lm_labels)\n        # pretraining labels\n        # is_random_next = getattr(sample_list, ""is_correct"", None)\n        # TODO(aps): Fix later on dataset side\n        is_random_next = None\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        # fp16 compatibility\n        attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)\n        attention_mask = (1.0 - attention_mask) * -10000.0\n\n        text_embedding = self.word_embedding(input_ids, input_type_ids)\n        text_embedding_total = self.process_text_embedding(\n            text_embedding, input_mask == 0\n        )\n\n        image_embedding_total, _ = self.process_feature_embedding(\n            ""image"", sample_list, text_embedding_total\n        )\n\n        if self.inter_model is not None:\n            image_embedding_total = self.inter_model(image_embedding_total)\n\n        # image_embedding_total = image_embedding_total *\n        # input_mask.unsqueeze(-1).float()\n        # text_embedding_total = text_embedding_total *\n        # input_mask.unsqueeze(-1).float()\n\n        if self.config.combine_embeddings:\n            joint_embedding = self.combine_embeddings(\n                [""image"", ""text""], [image_embedding_total, text_embedding_total]\n            )\n        else:\n            joint_embedding = image_embedding_total\n\n        output_dict = {}\n\n        pooled_output = self.pooler(joint_embedding)\n\n        if ""pretraining"" in self.config.training_head_type:\n            prediction_scores, seq_relationship_score = self.classifier(\n                joint_embedding, pooled_output\n            )\n            output_dict[""logits""] = prediction_scores\n\n            if masked_lm_labels is not None:\n                loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n                masked_lm_loss = loss_fct(\n                    prediction_scores.contiguous().view(\n                        -1, self.bert_config.vocab_size\n                    ),\n                    masked_lm_labels.contiguous().view(-1),\n                )\n                # print(seq_relationship_score.argmax(dim=1), is_random_next)\n                loss_key = ""{}/{}"".format(\n                    sample_list.dataset_name, sample_list.dataset_type\n                )\n\n                output_dict[""losses""] = {}\n                output_dict[""losses""][loss_key + ""/masked_lm_loss""] = masked_lm_loss\n\n                if is_random_next is not None:\n                    output_dict[""seq_relationship_score""] = seq_relationship_score\n\n                    next_sentence_loss = loss_fct(\n                        seq_relationship_score.contiguous().view(-1, 2),\n                        is_random_next.contiguous().view(-1),\n                    )\n                    output_dict[""losses""][\n                        loss_key + ""/next_sentence_loss""\n                    ] = next_sentence_loss\n            return output_dict\n        elif (\n            ""vqa"" in self.config.training_head_type\n            or self.config.training_head_type == ""vizwiz""\n        ):\n            index_to_gather = input_mask.sum(1) - 2\n\n            pooled_output = torch.gather(\n                joint_embedding,\n                1,\n                index_to_gather.unsqueeze(-1)\n                .unsqueeze(-1)\n                .expand(index_to_gather.size(0), 1, joint_embedding.size(-1)),\n            )\n\n            pooled_output = self.dropout(pooled_output)\n            logits = self.classifier(pooled_output)\n            reshaped_logits = logits.contiguous().view(-1, self.answer_space_size)\n\n            output_dict[""scores""] = reshaped_logits\n            return output_dict\n        elif (\n            self.config.training_head_type == ""nlvr2""\n            or self.config.training_head_type == ""visual_entailment""\n        ):\n            pooled_output = self.dropout(pooled_output)\n            logits = self.classifier(pooled_output)\n            output_dict[""scores""] = logits\n            return output_dict\n\n        return output_dict\n'"
mmf/models/pythia.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport copy\n\nimport torch\nfrom torch import nn\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.modules.embeddings import (\n    ImageFeatureEmbedding,\n    MultiHeadImageFeatureEmbedding,\n    PreExtractedEmbedding,\n    TextEmbedding,\n)\nfrom mmf.modules.encoders import ImageFeatureEncoder\nfrom mmf.modules.layers import ClassifierLayer, ModalCombineLayer\n\n\n@registry.register_model(""pythia"")\nclass Pythia(BaseModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.config = config\n        self._global_config = registry.get(""config"")\n        self._datasets = self._global_config.datasets.split("","")\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/pythia/defaults.yaml""\n\n    @classmethod\n    def format_state_key(cls, key):\n        return key.replace(""fa_history"", ""fa_context"")\n\n    def build(self):\n        self._build_word_embedding()\n        self._init_text_embeddings(""text"")\n        self._init_feature_encoders(""image"")\n        self._init_feature_embeddings(""image"")\n        self._init_combine_layer(""image"", ""text"")\n        self._init_classifier(self._get_classifier_input_dim())\n        self._init_extras()\n\n    def _build_word_embedding(self):\n        assert len(self._datasets) > 0\n        text_processor = registry.get(self._datasets[0] + ""_text_processor"")\n        vocab = text_processor.vocab\n        self.word_embedding = vocab.get_embedding(torch.nn.Embedding, embedding_dim=300)\n\n    def _init_text_embeddings(self, attr=""text""):\n        if ""embeddings"" not in attr:\n            attr += ""_embeddings""\n\n        text_embeddings = []\n        text_embeddings_list_config = self.config[attr]\n\n        embeddings_out_dim = 0\n\n        for text_embedding in text_embeddings_list_config:\n            embedding_type = text_embedding.type\n            embedding_kwargs = copy.deepcopy(text_embedding.params)\n\n            self._update_text_embedding_args(embedding_kwargs)\n\n            embedding = TextEmbedding(embedding_type, **embedding_kwargs)\n\n            text_embeddings.append(embedding)\n            embeddings_out_dim += embedding.text_out_dim\n\n        setattr(self, attr + ""_out_dim"", embeddings_out_dim)\n        setattr(self, attr, nn.ModuleList(text_embeddings))\n\n    def _update_text_embedding_args(self, args):\n        # Add model_data_dir to kwargs\n        args.model_data_dir = self.config.model_data_dir\n\n    def _init_feature_encoders(self, attr):\n        feat_encoders = []\n        feat_encoders_list_config = self.config[attr + ""_feature_encodings""]\n        feature_dim = self.config[attr + ""_feature_dim""]\n        setattr(self, attr + ""_feature_dim"", feature_dim)\n\n        for feat_encoder in feat_encoders_list_config:\n            encoder_type = feat_encoder.type\n            encoder_kwargs = copy.deepcopy(feat_encoder.params)\n            encoder_kwargs.model_data_dir = self.config.model_data_dir\n\n            feat_model = ImageFeatureEncoder(\n                encoder_type, feature_dim, **encoder_kwargs\n            )\n\n            feat_encoders.append(feat_model)\n            setattr(self, attr + ""_feature_dim"", feat_model.out_dim)\n\n        setattr(self, attr + ""_feature_encoders"", nn.ModuleList(feat_encoders))\n\n    def _init_feature_embeddings(self, attr):\n        feature_embeddings_list = []\n        num_feature_feat = len(getattr(self.config, f""{attr}_feature_encodings""))\n\n        self.feature_embeddings_out_dim = 0\n\n        for _ in range(num_feature_feat):\n            feature_embeddings = []\n            feature_attn_model_list = self.config[attr + ""_feature_embeddings""]\n\n            for feature_attn_model_params in feature_attn_model_list:\n                feature_embedding = ImageFeatureEmbedding(\n                    getattr(self, attr + ""_feature_dim""),\n                    self.text_embeddings_out_dim,\n                    **feature_attn_model_params,\n                )\n                feature_embeddings.append(feature_embedding)\n                self.feature_embeddings_out_dim += feature_embedding.out_dim\n\n            feature_embeddings = nn.ModuleList(feature_embeddings)\n            feature_embeddings_list.append(feature_embeddings)\n\n        self.feature_embeddings_out_dim *= getattr(self, attr + ""_feature_dim"")\n\n        setattr(\n            self, attr + ""_feature_embeddings_out_dim"", self.feature_embeddings_out_dim\n        )\n        del self.feature_embeddings_out_dim\n        setattr(\n            self,\n            attr + ""_feature_embeddings_list"",\n            nn.ModuleList(feature_embeddings_list),\n        )\n\n    def _get_embeddings_attr(self, attr):\n        embedding_attr1 = attr\n        if hasattr(self, attr + ""_embeddings_out_dim""):\n            embedding_attr1 = attr + ""_embeddings_out_dim""\n        else:\n            embedding_attr1 = attr + ""_feature_embeddings_out_dim""\n\n        return embedding_attr1\n\n    def _init_combine_layer(self, attr1, attr2):\n        config_attr = attr1 + ""_"" + attr2 + ""_modal_combine""\n\n        multi_modal_combine_layer = ModalCombineLayer(\n            self.config[config_attr].type,\n            getattr(self, self._get_embeddings_attr(attr1)),\n            getattr(self, self._get_embeddings_attr(attr2)),\n            **self.config[config_attr].params,\n        )\n\n        setattr(\n            self,\n            attr1 + ""_"" + attr2 + ""_multi_modal_combine_layer"",\n            multi_modal_combine_layer,\n        )\n\n    def _init_classifier(self, combined_embedding_dim):\n        # TODO: Later support multihead\n        num_choices = registry.get(self._datasets[0] + ""_num_final_outputs"")\n\n        self.classifier = ClassifierLayer(\n            self.config.classifier.type,\n            in_dim=combined_embedding_dim,\n            out_dim=num_choices,\n            **self.config.classifier.params,\n        )\n\n    def _init_extras(self):\n        self.inter_model = None\n\n    def get_optimizer_parameters(self, config):\n        combine_layer = self.image_text_multi_modal_combine_layer\n        params = [\n            {""params"": self.word_embedding.parameters()},\n            {""params"": self.image_feature_embeddings_list.parameters()},\n            {""params"": self.text_embeddings.parameters()},\n            {""params"": combine_layer.parameters()},\n            {""params"": self.classifier.parameters()},\n            {\n                ""params"": self.image_feature_encoders.parameters(),\n                ""lr"": (config.optimizer.params.lr * 0.1),\n            },\n        ]\n\n        return params\n\n    def _get_classifier_input_dim(self):\n        return self.image_text_multi_modal_combine_layer.out_dim\n\n    def process_text_embedding(\n        self, sample_list, embedding_attr=""text_embeddings"", info=None\n    ):\n        text_embeddings = []\n\n        # Get ""text"" attribute in case of ""text_embeddings"" case\n        # and ""context"" attribute in case of ""context_embeddings""\n        texts = getattr(sample_list, embedding_attr.split(""_"")[0])\n\n        # Get embedding models\n        text_embedding_models = getattr(self, embedding_attr)\n\n        for text_embedding_model in text_embedding_models:\n            # TODO: Move this logic inside\n            if isinstance(text_embedding_model, PreExtractedEmbedding):\n                embedding = text_embedding_model(sample_list.question_id)\n            else:\n                embedding = text_embedding_model(texts)\n            text_embeddings.append(embedding)\n\n        text_embeddding_total = torch.cat(text_embeddings, dim=1)\n\n        return text_embeddding_total\n\n    def process_feature_embedding(\n        self, attr, sample_list, text_embedding_total, extra=None, batch_size_t=None\n    ):\n        if extra is None:\n            extra = []\n        feature_embeddings = []\n        feature_attentions = []\n        features = []\n        batch_size_t = (\n            sample_list.get_batch_size() if batch_size_t is None else batch_size_t\n        )\n\n        # Convert list of keys to the actual values\n        extra = sample_list.get_fields(extra)\n\n        feature_idx = 0\n\n        # Get all of the features, which are in the form, ""image_feature_0""\n        # ""image_feature_1"" ...\n        while True:\n            feature = getattr(sample_list, f""{attr}_feature_{feature_idx:d}"", None)\n            if feature is None:\n                break\n            feature_idx += 1\n            feature = feature[:batch_size_t]\n            features.append(feature)\n\n        feature_encoders = getattr(self, attr + ""_feature_encoders"")\n        # Each feature should have a separate image feature encoders\n        assert len(features) == len(feature_encoders), (\n            ""Number of feature encoders, {} are not equal ""\n            ""to number of features, {}."".format(len(feature_encoders), len(features))\n        )\n\n        # Now, iterate to get final attended image features\n        for i, feature in enumerate(features):\n            # Get info related to the current feature. info is generally\n            # in key of format ""image_info_0"" for 0th feature\n            feature_info = getattr(sample_list, f""{attr}_info_{i:d}"", {})\n            # For Pythia, we need max_features to mask attention\n            feature_dim = getattr(feature_info, ""max_features"", None)\n            if feature_dim is not None:\n                feature_dim = feature_dim[:batch_size_t]\n\n            # Attribute in which encoders are saved, for ""image"" it\n            # will be ""image_feature_encoders"", other example is\n            # ""context_feature_encoders""\n            encoders_attr = attr + ""_feature_encoders""\n            feature_encoder = getattr(self, encoders_attr)[i]\n\n            # Encode the features\n            encoded_feature = feature_encoder(feature)\n\n            # Get all of the feature embeddings\n            list_attr = attr + ""_feature_embeddings_list""\n            feature_embedding_models = getattr(self, list_attr)[i]\n\n            # Forward through these embeddings one by one\n            for feature_embedding_model in feature_embedding_models:\n                inp = (encoded_feature, text_embedding_total, feature_dim, extra)\n\n                embedding, attention = feature_embedding_model(*inp)\n                feature_embeddings.append(embedding)\n                feature_attentions.append(attention.squeeze(-1))\n\n        # Concatenate all features embeddings and return along with attention\n        feature_embedding_total = torch.cat(feature_embeddings, dim=1)\n        return feature_embedding_total, feature_attentions\n\n    def combine_embeddings(self, *args):\n        feature_names = args[0]\n        feature_embeddings = args[1]\n\n        layer = ""_"".join(feature_names) + ""_multi_modal_combine_layer""\n        return getattr(self, layer)(*feature_embeddings)\n\n    def calculate_logits(self, joint_embedding, **kwargs):\n        return self.classifier(joint_embedding)\n\n    def forward(self, sample_list):\n        sample_list.text = self.word_embedding(sample_list.text)\n        text_embedding_total = self.process_text_embedding(sample_list)\n\n        image_embedding_total, _ = self.process_feature_embedding(\n            ""image"", sample_list, text_embedding_total\n        )\n\n        if self.inter_model is not None:\n            image_embedding_total = self.inter_model(image_embedding_total)\n\n        joint_embedding = self.combine_embeddings(\n            [""image"", ""text""], [image_embedding_total, text_embedding_total]\n        )\n\n        model_output = {""scores"": self.calculate_logits(joint_embedding)}\n\n        return model_output\n\n\n# TODO: Update\n@registry.register_model(""pythia_question_only"")\nclass PythiaQuestionOnly(Pythia):\n    def __init__(self, config):\n        super().__init__(config)\n\n    def forward(self, sample_list):\n        text_embedding_total = self.process_text_embedding(sample_list)\n        text_embedding_total = text_embedding_total.new_zeros(\n            text_embedding_total.size()\n        )\n\n        fa_txt = self.image_text_multi_modal_combine_layer.module.fa_txt\n        dropout = self.image_text_multi_modal_combine_layer.module.dropout\n\n        joint_embedding = dropout(fa_txt(text_embedding_total))\n\n        linear_text = self.classifier.module.linear_text\n        f_o_text = self.classifier.module.f_o_text\n        scores = linear_text(f_o_text(joint_embedding))\n\n        model_output = {""scores"": scores}\n\n        return model_output\n\n\n# TODO: Update\n@registry.register_model(""pythia_image_only"")\nclass PythiaImageOnly(Pythia):\n    def __init__(self, config):\n        super().__init__(config)\n\n    def forward(self, sample_list):\n        text_embedding_total = self.process_text_embedding(sample_list)\n        text_embedding_total = text_embedding_total.new_zeros(\n            text_embedding_total.size()\n        )\n\n        image_embedding_total, _ = self.process_feature_embedding(\n            ""image"", sample_list, text_embedding_total\n        )\n\n        if self.inter_model is not None:\n            image_embedding_total = self.inter_model(image_embedding_total)\n\n        fa_image = self.image_text_multi_modal_combine_layer.module.fa_image\n        dropout = self.image_text_multi_modal_combine_layer.module.dropout\n\n        joint_embedding = dropout(fa_image(image_embedding_total))\n\n        model_output = {""scores"": self.calculate_logits(joint_embedding)}\n\n        return model_output\n\n\n@registry.register_model(""multihead"")\nclass PythiaMultiHead(Pythia):\n    def __init__(self, config):\n        super().__init__(config)\n\n    @classmethod\n    def config_path(cls):\n        return None\n\n    def build(self):\n        self._build_word_embedding()\n        self._init_text_embeddings(""text"")\n        self._init_feature_encoders(""image"")\n        self._init_feature_projectors(""image"")\n        self._init_feature_embeddings(""image"")\n        self._init_combine_layer(""image"", ""text"")\n        self._init_classifier(self._get_classifier_input_dim())\n        self._init_extras()\n\n    def _init_feature_projectors(self, attr):\n        feature_projectors = []\n        feat_encoders_list_config = self.config[attr + ""_feature_projections""]\n        feat_dim = getattr(self, attr + ""_feature_dim"")\n\n        for feat_encoder in feat_encoders_list_config:\n            encoder_type = feat_encoder.type\n            encoder_kwargs = feat_encoder.params\n\n            feat_model = ImageFeatureEncoder(encoder_type, feat_dim, **encoder_kwargs)\n\n            feature_projectors.append(feat_model)\n            setattr(self, attr + ""_feature_dim"", feat_model.out_dim)\n\n        setattr(self, attr + ""_feature_projectors"", nn.ModuleList(feature_projectors))\n\n    def _init_feature_embeddings(self, attr):\n        feature_embeddings_list = []\n        num_feature_feat = len(getattr(self.config, f""{attr}_feature_encodings""))\n\n        self.feature_embeddings_out_dim = 0\n\n        for _ in range(num_feature_feat):\n            feature_embeddings = []\n            feature_attn_model_list = self.config[attr + ""_feature_embeddings""]\n\n            for feature_attn_model_params in feature_attn_model_list:\n                feature_embedding = MultiHeadImageFeatureEmbedding(\n                    getattr(self, attr + ""_feature_dim""),\n                    self.text_embeddings_out_dim,\n                    **feature_attn_model_params,\n                )\n                feature_embeddings.append(feature_embedding)\n                self.feature_embeddings_out_dim += feature_embedding.out_dim\n\n            feature_embeddings = nn.ModuleList(feature_embeddings)\n            feature_embeddings_list.append(feature_embeddings)\n\n        setattr(\n            self, attr + ""_feature_embeddings_out_dim"", self.feature_embeddings_out_dim\n        )\n        del self.feature_embeddings_out_dim\n        setattr(\n            self,\n            attr + ""_feature_embeddings_list"",\n            nn.ModuleList(feature_embeddings_list),\n        )\n\n    def process_feature_embedding(\n        self, attr, sample_list, text_embedding_total, extra=None, batch_size_t=None\n    ):\n        if extra is None:\n            extra = []\n        feature_embeddings = []\n        feature_attentions = []\n        features = []\n        batch_size_t = (\n            sample_list.get_batch_size() if batch_size_t is None else batch_size_t\n        )\n\n        # Convert list of keys to the actual values\n        extra = sample_list.get_fields(extra)\n\n        feature_idx = 0\n\n        # Get all of the features, which are in the form, ""image_feature_0""\n        # ""image_feature_1"" ...\n        while True:\n            feature = getattr(sample_list, f""{attr}_feature_{feature_idx:d}"", None)\n            if feature is None:\n                break\n            feature_idx += 1\n            feature = feature[:batch_size_t]\n            features.append(feature)\n\n        feature_encoders = getattr(self, attr + ""_feature_encoders"")\n        # Each feature should have a separate image feature encoders\n        assert len(features) == len(feature_encoders), (\n            ""Number of feature encoders, {} are not equal ""\n            ""to number of features, {}."".format(len(feature_encoders), len(features))\n        )\n\n        # Now, iterate to get final attended image features\n        for i, feature in enumerate(features):\n            # Get info related to the current feature. info is generally\n            # in key of format ""image_info_0"" for 0th feature\n            feature_info = getattr(sample_list, f""{attr}_info_{i:d}"", {})\n            # For Pythia, we need max_features to mask attention\n            feature_dim = getattr(feature_info, ""max_features"", None)\n            if feature_dim is not None:\n                feature_dim = feature_dim[:batch_size_t]\n\n            # Attribute in which encoders are saved, for ""image"" it\n            # will be ""image_feature_encoders"", other example is\n            # ""context_feature_encoders""\n            encoders_attr = attr + ""_feature_encoders""\n            feature_encoder = getattr(self, encoders_attr)[i]\n\n            # Encode the features\n            encoded_feature = feature_encoder(feature)\n\n            projector_attr = attr + ""_feature_projectors""\n            feature_projector = getattr(self, projector_attr)[i]\n\n            encoded_feature = feature_projector(encoded_feature)\n            # Get all of the feature embeddings\n            list_attr = attr + ""_feature_embeddings_list""\n            feature_embedding_models = getattr(self, list_attr)[i]\n\n            # Forward through these embeddings one by one\n            for feature_embedding_model in feature_embedding_models:\n                inp = (encoded_feature, text_embedding_total, feature_dim, extra)\n\n                embedding, attention = feature_embedding_model(*inp)\n                feature_embeddings.append(embedding)\n                feature_attentions.append(attention.squeeze(-1))\n\n        # Concatenate all features embeddings and return along with attention\n        feature_embedding_total = torch.cat(feature_embeddings, dim=1)\n        return feature_embedding_total, feature_attentions\n'"
mmf/models/top_down_bottom_up.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.modules.layers import ReLUWithWeightNormFC\n\n\n# Note: Doesn\'t work currently. Needs to be migrated to new API\n@registry.register_model(""top_down_bottom_up"")\nclass TopDownBottomUp(BaseModel):\n    def __init__(self, image_attention_model, text_embedding_models, classifier):\n        super().__init__()\n        self.image_attention_model = image_attention_model\n        self.text_embedding_models = text_embedding_models\n        self.classifier = classifier\n        text_lstm_dim = sum([q.text_out_dim for q in text_embedding_models])\n        joint_embedding_out_dim = classifier.input_dim\n        image_feat_dim = image_attention_model.image_feat_dim\n        self.non_linear_text = ReLUWithWeightNormFC(\n            text_lstm_dim, joint_embedding_out_dim\n        )\n        self.non_linear_image = ReLUWithWeightNormFC(\n            image_feat_dim, joint_embedding_out_dim\n        )\n\n    @classmethod\n    def config_path(self):\n        return None\n\n    def build(self):\n        return\n\n    def forward(\n        self, image_feat_variable, input_text_variable, input_answers=None, **kwargs\n    ):\n        text_embeddings = []\n        for q_model in self.text_embedding_models:\n            q_embedding = q_model(input_text_variable)\n            text_embeddings.append(q_embedding)\n        text_embedding = torch.cat(text_embeddings, dim=1)\n\n        if isinstance(image_feat_variable, list):\n            image_embeddings = []\n            for idx, image_feat in enumerate(image_feat_variable):\n                ques_embedding_each = torch.unsqueeze(text_embedding[idx, :], 0)\n                image_feat_each = torch.unsqueeze(image_feat, dim=0)\n                attention_each = self.image_attention_model(\n                    image_feat_each, ques_embedding_each\n                )\n                image_embedding_each = torch.sum(attention_each * image_feat, dim=1)\n                image_embeddings.append(image_embedding_each)\n            image_embedding = torch.cat(image_embeddings, dim=0)\n        else:\n            attention = self.image_attention_model(image_feat_variable, text_embedding)\n            image_embedding = torch.sum(attention * image_feat_variable, dim=1)\n\n        joint_embedding = self.non_linear_text(text_embedding) * self.non_linear_image(\n            image_embedding\n        )\n        logit_res = self.classifier(joint_embedding)\n\n        return logit_res\n'"
mmf/models/unimodal.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom copy import deepcopy\n\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.modules.encoders import MultiModalEncoderBase\nfrom mmf.utils.build import build_classifier_layer\n\n\nclass UnimodalBase(MultiModalEncoderBase):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n    def build(self):\n        encoders = self._build_encoders(self.config)\n        # Text Encoder mode\n        if ""modal_encoder"" not in self.config:\n            self.encoder = encoders[0]\n        # Modal encoder mode\n        elif ""text_encoder"" not in self.config:\n            self.encoder = encoders[1]\n        else:\n            raise RuntimeError(\n                ""Unimodal Encoder can\'t have both text and modal encoder""\n            )\n\n    def forward(self, x, *args, **kwargs):\n        x = self.encoder(x, *args, **kwargs)\n        # Case of bert encoder, we only need pooled output\n        if len(x) == 2:\n            x = x[1]\n\n        x = torch.flatten(x, start_dim=1)\n\n        return x\n\n\n@registry.register_model(""unimodal_text"")\nclass UnimodalText(BaseModel):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/unimodal/text.yaml""\n\n    def build(self):\n        self.base = UnimodalBase(self.config)\n        # As the in_dim is dynamically calculated we need to copy classifier_config\n        classifier_config = deepcopy(self.config.classifier)\n        classifier_config.params.in_dim = self.config.text_hidden_size\n        self.classifier = build_classifier_layer(classifier_config)\n\n    def forward(self, sample_list):\n        # BERT Based Encoders\n        args = []\n        if ""input_ids"" in sample_list:\n            text = sample_list.input_ids\n            args.append(sample_list.input_mask)\n            args.append(sample_list.segment_ids)\n        else:\n            text = sample_list.text\n\n        embedding = self.base(text, *args)\n        output = {}\n        output[""scores""] = self.classifier(embedding)\n\n        return output\n\n\n@registry.register_model(""unimodal_image"")\nclass UnimodalModal(BaseModel):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/unimodal/image.yaml""\n\n    def build(self):\n        self.base = UnimodalBase(self.config)\n        self._is_direct_features_input = self.config.direct_features_input\n        num_features = self.config.modal_encoder.params.num_output_features\n\n        # As the in_dim is dynamically calculated we need to copy classifier_config\n        classifier_config = deepcopy(self.config.classifier)\n        classifier_config.params.in_dim = num_features * self.config.modal_hidden_size\n        self.classifier = build_classifier_layer(classifier_config)\n\n    def forward(self, sample_list):\n        # BERT Based Encoders\n        args = []\n        if self._is_direct_features_input:\n            modal = sample_list.image_feature_0\n            modal = torch.mean(modal, dim=1)\n        else:\n            modal = sample_list.image\n\n        embedding = self.base(modal, *args)\n        output = {}\n        output[""scores""] = self.classifier(embedding)\n\n        return output\n'"
mmf/models/vilbert.py,42,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport math\nimport os\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom omegaconf import OmegaConf\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers.modeling_bert import (\n    ACT2FN,\n    BertConfig,\n    BertEmbeddings,\n    BertIntermediate,\n    BertLayerNorm,\n    BertLMPredictionHead,\n    BertOutput,\n    BertPredictionHeadTransform,\n    BertPreTrainedModel,\n    BertSelfOutput,\n)\n\nfrom mmf.common.registry import registry\nfrom mmf.models import BaseModel\nfrom mmf.utils.configuration import get_mmf_cache_dir\nfrom mmf.utils.modeling import get_optimizer_parameters_for_bert\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads)\n            )\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.visualization = config.visualization\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw\n        # attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in\n        # BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        if self.visualization:\n            attn_data = {\n                ""attn"": attention_probs,\n                ""queries"": query_layer,\n                ""keys"": key_layer,\n            }\n        else:\n            attn_data = None\n\n        return context_layer, attn_data\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output, attention_probs = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output, attention_probs\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask):\n        attention_output, attention_probs = self.attention(\n            hidden_states, attention_mask\n        )\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, attention_probs\n\n\nclass BertImageSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.v_hidden_size % config.v_num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.v_hidden_size, config.v_num_attention_heads)\n            )\n        self.dynamic_attention = config.dynamic_attention\n        self.num_attention_heads = config.v_num_attention_heads\n        self.attention_head_size = int(\n            config.v_hidden_size / config.v_num_attention_heads\n        )\n\n        self.visualization = config.visualization\n\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.query = nn.Linear(config.v_hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.v_hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.v_hidden_size, self.all_head_size)\n\n        if self.dynamic_attention:\n            self.dyLinear_q = nn.Linear(config.hidden_size, self.all_head_size)\n            self.dyLinear_k = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.v_attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask, txt_embedding, txt_attention_mask):\n\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        if self.dynamic_attention:\n            pool_embedding = (txt_embedding * txt_attention_mask).sum(1)\n            pool_embedding = pool_embedding / txt_attention_mask.sum(1)\n\n            # given pool embedding, Linear and Sigmoid layer.\n            gate_q = 1 + torch.sigmoid(self.dyLinear_q(pool_embedding))\n            gate_k = 1 + torch.sigmoid(self.dyLinear_k(pool_embedding))\n\n            mixed_query_layer = mixed_query_layer * gate_q.unsqueeze(1)\n            mixed_key_layer = mixed_key_layer * gate_k.unsqueeze(1)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the\n        # raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel\n        # forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        if self.visualization:\n            attn_data = {\n                ""attn"": attention_probs,\n                ""queries"": query_layer,\n                ""keys"": key_layer,\n            }\n        else:\n            attn_data = None\n\n        return context_layer, attn_data\n\n\nclass BertImageSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.v_hidden_size, config.v_hidden_size)\n        self.LayerNorm = BertLayerNorm(config.v_hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.v_hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertImageAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.self = BertImageSelfAttention(config)\n        self.output = BertImageSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask, txt_embedding, txt_attention_mask):\n        self_output, attention_probs = self.self(\n            input_tensor, attention_mask, txt_embedding, txt_attention_mask\n        )\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output, attention_probs\n\n\nclass BertImageIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.v_hidden_size, config.v_intermediate_size)\n        if isinstance(config.v_hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.v_hidden_act]\n        else:\n            self.intermediate_act_fn = config.v_hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertImageOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.v_intermediate_size, config.v_hidden_size)\n        self.LayerNorm = BertLayerNorm(config.v_hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.v_hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertImageLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = BertImageAttention(config)\n        self.intermediate = BertImageIntermediate(config)\n        self.output = BertImageOutput(config)\n\n    def forward(self, hidden_states, attention_mask, txt_embedding, txt_attention_mask):\n        attention_output, attention_probs = self.attention(\n            hidden_states, attention_mask, txt_embedding, txt_attention_mask\n        )\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output, attention_probs\n\n\nclass BertBiAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if config.bi_hidden_size % config.bi_num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.bi_hidden_size, config.bi_num_attention_heads)\n            )\n\n        self.visualization = config.visualization\n        self.num_attention_heads = config.bi_num_attention_heads\n        self.attention_head_size = int(\n            config.bi_hidden_size / config.bi_num_attention_heads\n        )\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        # self.scale = nn.Linear(1, self.num_attention_heads, bias=False)\n        # self.scale_act_fn = ACT2FN[\'relu\']\n\n        self.query1 = nn.Linear(config.v_hidden_size, self.all_head_size)\n        self.key1 = nn.Linear(config.v_hidden_size, self.all_head_size)\n        self.value1 = nn.Linear(config.v_hidden_size, self.all_head_size)\n        # self.logit1 = nn.Linear(config.hidden_size, self.num_attention_heads)\n\n        self.dropout1 = nn.Dropout(config.v_attention_probs_dropout_prob)\n\n        self.query2 = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key2 = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value2 = nn.Linear(config.hidden_size, self.all_head_size)\n        # self.logit2 = nn.Linear(config.hidden_size, self.num_attention_heads)\n\n        self.dropout2 = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        input_tensor1,\n        attention_mask1,\n        input_tensor2,\n        attention_mask2,\n        co_attention_mask=None,\n        use_co_attention_mask=False,\n    ):\n\n        # for vision input.\n        mixed_query_layer1 = self.query1(input_tensor1)\n        mixed_key_layer1 = self.key1(input_tensor1)\n        mixed_value_layer1 = self.value1(input_tensor1)\n        # mixed_logit_layer1 = self.logit1(input_tensor1)\n\n        query_layer1 = self.transpose_for_scores(mixed_query_layer1)\n        key_layer1 = self.transpose_for_scores(mixed_key_layer1)\n        value_layer1 = self.transpose_for_scores(mixed_value_layer1)\n        # logit_layer1 = self.transpose_for_logits(mixed_logit_layer1)\n\n        # for text input:\n        mixed_query_layer2 = self.query2(input_tensor2)\n        mixed_key_layer2 = self.key2(input_tensor2)\n        mixed_value_layer2 = self.value2(input_tensor2)\n        # mixed_logit_layer2 = self.logit2(input_tensor2)\n\n        query_layer2 = self.transpose_for_scores(mixed_query_layer2)\n        key_layer2 = self.transpose_for_scores(mixed_key_layer2)\n        value_layer2 = self.transpose_for_scores(mixed_value_layer2)\n        # logit_layer2 = self.transpose_for_logits(mixed_logit_layer2)\n\n        # Take the dot product between ""query2"" and ""key1"" to get the raw\n        # attention scores for value 1.\n        attention_scores1 = torch.matmul(query_layer2, key_layer1.transpose(-1, -2))\n        attention_scores1 = attention_scores1 / math.sqrt(self.attention_head_size)\n        attention_scores1 = attention_scores1 + attention_mask1\n        # if use_co_attention_mask:\n        # attention_scores1 = attention_scores1 + co_attention_mask.permute(0,1,3,2)\n\n        # Normalize the attention scores to probabilities.\n        attention_probs1 = nn.Softmax(dim=-1)(attention_scores1)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs1 = self.dropout1(attention_probs1)\n\n        context_layer1 = torch.matmul(attention_probs1, value_layer1)\n        context_layer1 = context_layer1.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape1 = context_layer1.size()[:-2] + (self.all_head_size,)\n        context_layer1 = context_layer1.view(*new_context_layer_shape1)\n\n        # Take the dot product between ""query1"" and ""key2"" to get the\n        # raw attention scores for value 2.\n        attention_scores2 = torch.matmul(query_layer1, key_layer2.transpose(-1, -2))\n        attention_scores2 = attention_scores2 / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel\n        # forward() function)\n\n        # we can comment this line for single flow.\n        attention_scores2 = attention_scores2 + attention_mask2\n        # if use_co_attention_mask:\n        # attention_scores2 = attention_scores2 + co_attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs2 = nn.Softmax(dim=-1)(attention_scores2)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs2 = self.dropout2(attention_probs2)\n\n        context_layer2 = torch.matmul(attention_probs2, value_layer2)\n        context_layer2 = context_layer2.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape2 = context_layer2.size()[:-2] + (self.all_head_size,)\n        context_layer2 = context_layer2.view(*new_context_layer_shape2)\n\n        attn_data = None\n\n        if self.visualization:\n            attn_data = {\n                ""attn1"": attention_probs1,\n                ""queries1"": query_layer2,\n                ""keys1"": key_layer1,\n                ""attn2"": attention_probs2,\n                ""querues2"": query_layer1,\n                ""keys2"": key_layer2,\n            }\n\n        return context_layer1, context_layer2, attn_data\n\n\nclass BertBiOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.dense1 = nn.Linear(config.bi_hidden_size, config.v_hidden_size)\n        self.LayerNorm1 = BertLayerNorm(config.v_hidden_size, eps=1e-12)\n        self.dropout1 = nn.Dropout(config.v_hidden_dropout_prob)\n\n        self.q_dense1 = nn.Linear(config.bi_hidden_size, config.v_hidden_size)\n        self.q_dropout1 = nn.Dropout(config.v_hidden_dropout_prob)\n\n        self.dense2 = nn.Linear(config.bi_hidden_size, config.hidden_size)\n        self.LayerNorm2 = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout2 = nn.Dropout(config.hidden_dropout_prob)\n\n        self.q_dense2 = nn.Linear(config.bi_hidden_size, config.hidden_size)\n        self.q_dropout2 = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states1, input_tensor1, hidden_states2, input_tensor2):\n\n        context_state1 = self.dense1(hidden_states1)\n        context_state1 = self.dropout1(context_state1)\n\n        context_state2 = self.dense2(hidden_states2)\n        context_state2 = self.dropout2(context_state2)\n\n        hidden_states1 = self.LayerNorm1(context_state1 + input_tensor1)\n        hidden_states2 = self.LayerNorm2(context_state2 + input_tensor2)\n\n        return hidden_states1, hidden_states2\n\n\nclass BertConnectionLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.biattention = BertBiAttention(config)\n\n        self.biOutput = BertBiOutput(config)\n\n        self.v_intermediate = BertImageIntermediate(config)\n        self.v_output = BertImageOutput(config)\n\n        self.t_intermediate = BertIntermediate(config)\n        self.t_output = BertOutput(config)\n\n    def forward(\n        self,\n        input_tensor1,\n        attention_mask1,\n        input_tensor2,\n        attention_mask2,\n        co_attention_mask=None,\n        use_co_attention_mask=False,\n    ):\n\n        bi_output1, bi_output2, co_attention_probs = self.biattention(\n            input_tensor1,\n            attention_mask1,\n            input_tensor2,\n            attention_mask2,\n            co_attention_mask,\n            use_co_attention_mask,\n        )\n\n        attention_output1, attention_output2 = self.biOutput(\n            bi_output2, input_tensor1, bi_output1, input_tensor2\n        )\n\n        intermediate_output1 = self.v_intermediate(attention_output1)\n        layer_output1 = self.v_output(intermediate_output1, attention_output1)\n\n        intermediate_output2 = self.t_intermediate(attention_output2)\n        layer_output2 = self.t_output(intermediate_output2, attention_output2)\n\n        return layer_output1, layer_output2, co_attention_probs\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        # in the bert encoder, we need to extract three things here.\n        # text bert layer: BertLayer\n        # vision bert layer: BertImageLayer\n        # Bi-Attention: Given the output of two bertlayer, perform bi-directional\n        # attention and add on two layers.\n\n        self.FAST_MODE = config.fast_mode\n        self.with_coattention = config.with_coattention\n        self.v_biattention_id = config.v_biattention_id\n        self.t_biattention_id = config.t_biattention_id\n        self.in_batch_pairs = config.in_batch_pairs\n        self.fixed_t_layer = config.fixed_t_layer\n        self.fixed_v_layer = config.fixed_v_layer\n        layer = BertLayer(config)\n        v_layer = BertImageLayer(config)\n        connect_layer = BertConnectionLayer(config)\n\n        self.layer = nn.ModuleList(\n            [deepcopy(layer) for _ in range(config.num_hidden_layers)]\n        )\n        self.v_layer = nn.ModuleList(\n            [deepcopy(v_layer) for _ in range(config.v_num_hidden_layers)]\n        )\n        self.c_layer = nn.ModuleList(\n            [deepcopy(connect_layer) for _ in range(len(config.v_biattention_id))]\n        )\n\n    def forward(\n        self,\n        txt_embedding,\n        image_embedding,\n        txt_attention_mask,\n        txt_attention_mask2,\n        image_attention_mask,\n        co_attention_mask=None,\n        output_all_encoded_layers=True,\n        output_all_attention_masks=False,\n    ):\n\n        v_start = 0\n        t_start = 0\n        count = 0\n        all_encoder_layers_t = []\n        all_encoder_layers_v = []\n\n        all_attention_mask_t = []\n        all_attnetion_mask_v = []\n        all_attention_mask_c = []\n\n        batch_size, num_words, t_hidden_size = txt_embedding.size()\n        _, num_regions, v_hidden_size = image_embedding.size()\n\n        use_co_attention_mask = False\n        for v_layer_id, t_layer_id in zip(self.v_biattention_id, self.t_biattention_id):\n\n            v_end = v_layer_id\n            t_end = t_layer_id\n\n            assert self.fixed_t_layer <= t_end\n            assert self.fixed_v_layer <= v_end\n\n            for idx in range(t_start, self.fixed_t_layer):\n                with torch.no_grad():\n                    txt_embedding, txt_attention_probs = self.layer[idx](\n                        txt_embedding, txt_attention_mask\n                    )\n                    t_start = self.fixed_t_layer\n                    if output_all_attention_masks:\n                        all_attention_mask_t.append(txt_attention_probs)\n\n            for idx in range(t_start, t_end):\n                txt_embedding, txt_attention_probs = self.layer[idx](\n                    txt_embedding, txt_attention_mask\n                )\n                if output_all_attention_masks:\n                    all_attention_mask_t.append(txt_attention_probs)\n\n            for idx in range(v_start, self.fixed_v_layer):\n                with torch.no_grad():\n                    image_embedding, image_attention_probs = self.v_layer[idx](\n                        image_embedding,\n                        image_attention_mask,\n                        txt_embedding,\n                        txt_attention_mask2,\n                    )\n                    v_start = self.fixed_v_layer\n\n                    if output_all_attention_masks:\n                        all_attnetion_mask_v.append(image_attention_probs)\n\n            for idx in range(v_start, v_end):\n                image_embedding, image_attention_probs = self.v_layer[idx](\n                    image_embedding,\n                    image_attention_mask,\n                    txt_embedding,\n                    txt_attention_mask2,\n                )\n\n                if output_all_attention_masks:\n                    all_attnetion_mask_v.append(image_attention_probs)\n\n            if count == 0 and self.in_batch_pairs:\n                # new batch size is the batch_size ^2\n                image_embedding = (\n                    image_embedding.unsqueeze(0)\n                    .expand(batch_size, batch_size, num_regions, v_hidden_size)\n                    .contiguous()\n                    .view(batch_size * batch_size, num_regions, v_hidden_size)\n                )\n                image_attention_mask = (\n                    image_attention_mask.unsqueeze(0)\n                    .expand(batch_size, batch_size, 1, 1, num_regions)\n                    .contiguous()\n                    .view(batch_size * batch_size, 1, 1, num_regions)\n                )\n\n                txt_embedding = (\n                    txt_embedding.unsqueeze(1)\n                    .expand(batch_size, batch_size, num_words, t_hidden_size)\n                    .contiguous()\n                    .view(batch_size * batch_size, num_words, t_hidden_size)\n                )\n                txt_attention_mask = (\n                    txt_attention_mask.unsqueeze(1)\n                    .expand(batch_size, batch_size, 1, 1, num_words)\n                    .contiguous()\n                    .view(batch_size * batch_size, 1, 1, num_words)\n                )\n                co_attention_mask = (\n                    co_attention_mask.unsqueeze(1)\n                    .expand(batch_size, batch_size, 1, num_regions, num_words)\n                    .contiguous()\n                    .view(batch_size * batch_size, 1, num_regions, num_words)\n                )\n\n            if count == 0 and self.FAST_MODE:\n                txt_embedding = txt_embedding.expand(\n                    image_embedding.size(0),\n                    txt_embedding.size(1),\n                    txt_embedding.size(2),\n                )\n                txt_attention_mask = txt_attention_mask.expand(\n                    image_embedding.size(0),\n                    txt_attention_mask.size(1),\n                    txt_attention_mask.size(2),\n                    txt_attention_mask.size(3),\n                )\n\n            if self.with_coattention:\n                # do the bi attention.\n                image_embedding, txt_embedding, co_attention_probs = self.c_layer[\n                    count\n                ](\n                    image_embedding,\n                    image_attention_mask,\n                    txt_embedding,\n                    txt_attention_mask,\n                    co_attention_mask,\n                    use_co_attention_mask,\n                )\n\n                if output_all_attention_masks:\n                    all_attention_mask_c.append(co_attention_probs)\n\n            v_start = v_end\n            t_start = t_end\n            count += 1\n\n            if output_all_encoded_layers:\n                all_encoder_layers_t.append(txt_embedding)\n                all_encoder_layers_v.append(image_embedding)\n\n        for idx in range(v_start, len(self.v_layer)):\n            image_embedding, image_attention_probs = self.v_layer[idx](\n                image_embedding,\n                image_attention_mask,\n                txt_embedding,\n                txt_attention_mask2,\n            )\n\n            if output_all_attention_masks:\n                all_attnetion_mask_v.append(image_attention_probs)\n\n        for idx in range(t_start, len(self.layer)):\n            txt_embedding, txt_attention_probs = self.layer[idx](\n                txt_embedding, txt_attention_mask\n            )\n\n            if output_all_attention_masks:\n                all_attention_mask_t.append(txt_attention_probs)\n\n        # add the end part to finish.\n        if not output_all_encoded_layers:\n            all_encoder_layers_t.append(txt_embedding)\n            all_encoder_layers_v.append(image_embedding)\n\n        return (\n            all_encoder_layers_t,\n            all_encoder_layers_v,\n            (all_attention_mask_t, all_attnetion_mask_v, all_attention_mask_c),\n        )\n\n\nclass BertTextPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.bi_hidden_size)\n        self.activation = nn.ReLU()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertImagePooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.v_hidden_size, config.bi_hidden_size)\n        self.activation = nn.ReLU()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertImgPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.v_hidden_size, config.v_hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.v_hidden_act\n        self.LayerNorm = BertLayerNorm(config.v_hidden_size, eps=1e-12)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertImagePredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertImgPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.v_hidden_size, config.v_target_size)\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n        self.bi_seq_relationship = nn.Linear(config.bi_hidden_size, 2)\n        self.imagePredictions = BertImagePredictionHead(config)\n        self.fusion_method = config.fusion_method\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(\n        self, sequence_output_t, sequence_output_v, pooled_output_t, pooled_output_v\n    ):\n\n        if self.fusion_method == ""sum"":\n            pooled_output = self.dropout(pooled_output_t + pooled_output_v)\n        elif self.fusion_method == ""mul"":\n            pooled_output = self.dropout(pooled_output_t * pooled_output_v)\n        else:\n            raise AssertionError\n\n        prediction_scores_t = self.predictions(sequence_output_t)\n        seq_relationship_score = self.bi_seq_relationship(pooled_output)\n        prediction_scores_v = self.imagePredictions(sequence_output_v)\n\n        return prediction_scores_t, prediction_scores_v, seq_relationship_score\n\n\nclass BertImageFeatureEmbeddings(nn.Module):\n    """"""Construct the embeddings from image, spatial location (omit now) and\n    token_type embeddings.\n    """"""\n\n    def __init__(self, config):\n        super().__init__()\n\n        self.image_embeddings = nn.Linear(config.v_feature_size, config.v_hidden_size)\n        self.image_location_embeddings = nn.Linear(5, config.v_hidden_size)\n        self.LayerNorm = BertLayerNorm(config.v_hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, image_feature, image_location):\n\n        img_embeddings = self.image_embeddings(image_feature)\n        loc_embeddings = self.image_location_embeddings(image_location)\n\n        # TODO: we want to make the padding_idx==0, however, with custom initilization,\n        # it seems it will have a bias. Let\'s do masking for now\n        embeddings = self.LayerNorm(img_embeddings + loc_embeddings)\n        embeddings = self.dropout(embeddings)\n\n        return embeddings\n\n\nclass ViLBERTBase(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n        # initilize word embedding\n        self.embeddings = BertEmbeddings(config)\n\n        self.task_specific_tokens = config.task_specific_tokens\n\n        # initlize the vision embedding\n        self.v_embeddings = BertImageFeatureEmbeddings(config)\n\n        self.encoder = BertEncoder(config)\n        self.t_pooler = BertTextPooler(config)\n        self.v_pooler = BertImagePooler(config)\n\n        self.init_weights()\n\n    def forward(\n        self,\n        input_txt,\n        image_feature,\n        image_location,\n        token_type_ids=None,\n        attention_mask=None,\n        image_attention_mask=None,\n        co_attention_mask=None,\n        task_ids=None,\n        output_all_encoded_layers=False,\n        output_all_attention_masks=False,\n    ):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_txt)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_txt)\n        if image_attention_mask is None:\n            image_attention_mask = torch.ones(\n                image_feature.size(0), image_feature.size(1)\n            ).type_as(input_txt)\n\n        if self.task_specific_tokens:\n            # extend the mask\n            mask_tokens = input_txt.new().resize_(input_txt.size(0), 1).fill_(1)\n            attention_mask = torch.cat([mask_tokens, attention_mask], dim=1)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of\n        # causal attention used in OpenAI GPT, we just need to prepare the\n        # broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n        extended_image_attention_mask = image_attention_mask.unsqueeze(1).unsqueeze(2)\n\n        extended_attention_mask2 = attention_mask.unsqueeze(2)\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        extended_attention_mask2 = extended_attention_mask2.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n\n        extended_image_attention_mask = extended_image_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_image_attention_mask = (1.0 - extended_image_attention_mask) * -10000.0\n\n        if co_attention_mask is None:\n            co_attention_mask = torch.zeros(\n                input_txt.size(0), image_feature.size(1), input_txt.size(1)\n            ).type_as(extended_image_attention_mask)\n\n        extended_co_attention_mask = co_attention_mask.unsqueeze(1)\n\n        # extended_co_attention_mask = co_attention_mask.unsqueeze(-1)\n        extended_co_attention_mask = extended_co_attention_mask * 5.0\n        extended_co_attention_mask = extended_co_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n\n        embedding_output = self.embeddings(input_txt, token_type_ids, task_ids)\n        v_embedding_output = self.v_embeddings(image_feature, image_location)\n        encoded_layers_t, encoded_layers_v, all_attention_mask = self.encoder(\n            embedding_output,\n            v_embedding_output,\n            extended_attention_mask,\n            extended_attention_mask2,\n            extended_image_attention_mask,\n            extended_co_attention_mask,\n            output_all_encoded_layers=output_all_encoded_layers,\n            output_all_attention_masks=output_all_attention_masks,\n        )\n\n        sequence_output_t = encoded_layers_t[-1]\n        sequence_output_v = encoded_layers_v[-1]\n\n        pooled_output_t = self.t_pooler(sequence_output_t)\n        pooled_output_v = self.v_pooler(sequence_output_v)\n\n        if not output_all_encoded_layers:\n            encoded_layers_t = encoded_layers_t[-1]\n            encoded_layers_v = encoded_layers_v[-1]\n\n        return (\n            encoded_layers_t,\n            encoded_layers_v,\n            pooled_output_t,\n            pooled_output_v,\n            all_attention_mask,\n        )\n\n\nclass ViLBERTForPretraining(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.bert = ViLBERTBase.from_pretrained(\n            self.config.bert_model_name,\n            config=BertConfig.from_dict(\n                OmegaConf.to_container(self.config, resolve=True)\n            ),\n            cache_dir=os.path.join(get_mmf_cache_dir(), ""distributed_{}"".format(-1)),\n        )\n        self.cls = BertPreTrainingHeads(config)\n        self.vocab_size = self.config.vocab_size\n        self.visual_target = config.visual_target\n        self.num_negative = config.num_negative\n        self.loss_fct = CrossEntropyLoss(ignore_index=-1)\n\n        if self.visual_target == 0:\n            self.vis_criterion = nn.KLDivLoss(reduction=""none"")\n        elif self.visual_target == 1:\n            self.vis_criterion = nn.MSELoss(reduction=""none"")\n        elif self.visual_target == 2:\n            self.vis_criterion = CrossEntropyLoss()\n\n    def init_weights(self):\n        if self.config.random_initialize is False:\n            if self.config.bert_model_name is None:\n                # No pretrained model, init weights\n                self.bert.init_weights()\n                self.cls.apply(self.bert._init_weights)\n\n            self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning\n            them instead.\n        """"""\n        self._tie_or_clone_weights(\n            self.cls.predictions.decoder, self.bert.embeddings.word_embeddings\n        )\n\n    def forward(\n        self,\n        input_ids,\n        image_feature,\n        image_location,\n        token_type_ids=None,\n        attention_mask=None,\n        image_attention_mask=None,\n        masked_lm_labels=None,\n        image_label=None,\n        image_target=None,\n        next_sentence_label=None,\n        output_all_attention_masks=False,\n    ):\n\n        (\n            sequence_output_t,\n            sequence_output_v,\n            pooled_output_t,\n            pooled_output_v,\n            attention_weights,\n        ) = self.bert(\n            input_ids,\n            image_feature,\n            image_location,\n            token_type_ids,\n            attention_mask,\n            image_attention_mask,\n            output_all_encoded_layers=False,\n            output_all_attention_masks=output_all_attention_masks,\n        )\n\n        prediction_scores_t, prediction_scores_v, seq_relationship_score = self.cls(\n            sequence_output_t, sequence_output_v, pooled_output_t, pooled_output_v\n        )\n        output = {}\n        if output_all_attention_masks:\n            output[""attention_weights""] = attention_weights\n\n        if image_target is not None:\n            if self.visual_target == 1:\n                img_loss = self.vis_criterion(prediction_scores_v, image_target)\n                masked_img_loss = torch.sum(\n                    img_loss * (image_label == 1).unsqueeze(2).float()\n                ) / max(\n                    torch.sum((image_label == 1).unsqueeze(2).expand_as(img_loss)), 1\n                )\n\n            elif self.visual_target == 0:\n                img_loss = self.vis_criterion(\n                    F.log_softmax(prediction_scores_v, dim=2), image_target\n                )\n\n                masked_img_loss = torch.sum(\n                    img_loss * (image_label == 1).unsqueeze(2).float()\n                ) / max(torch.sum(image_label == 1), 0)\n            elif self.visual_target == 2:\n                # generate negative sampled index.\n                num_across_batch = int(self.num_negative * 0.7)\n                num_inside_batch = int(self.num_negative * 0.3)\n\n                batch_size, num_regions, _ = prediction_scores_v.size()\n                assert batch_size != 0\n                # random negative across batches.\n                row_across_index = input_ids.new(\n                    batch_size, num_regions, num_across_batch\n                ).random_(0, batch_size - 1)\n                col_across_index = input_ids.new(\n                    batch_size, num_regions, num_across_batch\n                ).random_(0, num_regions)\n\n                for i in range(batch_size - 1):\n                    row_across_index[i][row_across_index[i] == i] = batch_size - 1\n                final_across_index = row_across_index * num_regions + col_across_index\n\n                # random negative inside batches.\n                row_inside_index = input_ids.new(\n                    batch_size, num_regions, num_inside_batch\n                ).zero_()\n                col_inside_index = input_ids.new(\n                    batch_size, num_regions, num_inside_batch\n                ).random_(0, num_regions - 1)\n\n                for i in range(batch_size):\n                    row_inside_index[i] = i\n                for i in range(num_regions - 1):\n                    col_inside_index[:, i, :][col_inside_index[:, i, :] == i] = (\n                        num_regions - 1\n                    )\n                final_inside_index = row_inside_index * num_regions + col_inside_index\n\n                final_index = torch.cat((final_across_index, final_inside_index), dim=2)\n\n                # Let\'s first sample where we need to compute.\n                predict_v = prediction_scores_v[image_label == 1]\n                neg_index_v = final_index[image_label == 1]\n\n                flat_image_target = image_target.view(batch_size * num_regions, -1)\n                # we also need to append the target feature at the beginning.\n                negative_v = flat_image_target[neg_index_v]\n                positive_v = image_target[image_label == 1]\n                sample_v = torch.cat((positive_v.unsqueeze(1), negative_v), dim=1)\n\n                # calculate the loss.\n                score = torch.bmm(sample_v, predict_v.unsqueeze(2)).squeeze(2)\n                masked_img_loss = self.vis_criterion(\n                    score, input_ids.new(score.size(0)).zero_()\n                )\n\n            output[""masked_img_loss""] = masked_img_loss.unsqueeze(0)\n\n        masked_lm_loss = self.loss_fct(\n            prediction_scores_t.view(-1, self.vocab_size), masked_lm_labels.view(-1)\n        )\n        output[""masked_lm_loss""] = masked_lm_loss.unsqueeze(0)\n        # next_sentence_loss = self.loss_fct(\n        #     seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)\n        # )\n        # output[""next_sentence_loss""] = next_sentence_loss.unsqueeze(0)\n        return output\n\n\nclass ViLBERTForClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.bert = ViLBERTBase.from_pretrained(\n            self.config.bert_model_name,\n            config=BertConfig.from_dict(\n                OmegaConf.to_container(self.config, resolve=True)\n            ),\n            cache_dir=os.path.join(get_mmf_cache_dir(), ""distributed_{}"".format(-1)),\n        )\n\n        self.training_head_type = self.config.training_head_type\n        self.num_labels = self.config.num_labels\n        self.fusion_method = config.fusion_method\n        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n\n        # Create a copy of config since struct mode won\'t allow direct overrides\n        # classifier_config is only needed for initializing the classifier\n        classifier_config = deepcopy(config)\n        classifier_config.hidden_size = config.bi_hidden_size\n        if self.config.training_head_type == ""nlvr2"":\n            classifier_config.hidden_size *= 2\n        self.classifier = nn.Sequential(\n            BertPredictionHeadTransform(classifier_config),\n            nn.Linear(classifier_config.hidden_size, self.num_labels),\n        )\n        self.init_weights()\n\n    def init_weights(self):\n        if self.config.random_initialize is False:\n            if self.config.bert_model_name is None:\n                # No pretrained model, init weights\n                self.bert.init_weights()\n\n            # Classifier needs to be initialized always as it is task specific\n            self.classifier.apply(self.bert._init_weights)\n\n    def forward(\n        self,\n        input_ids,\n        image_feature,\n        image_location,\n        token_type_ids=None,\n        attention_mask=None,\n        image_attention_mask=None,\n        masked_lm_labels=None,\n        image_label=None,\n        image_target=None,\n        next_sentence_label=None,\n        output_all_attention_masks=False,\n    ):\n\n        (\n            sequence_output_t,\n            sequence_output_v,\n            pooled_output_t,\n            pooled_output_v,\n            attention_weights,\n        ) = self.bert(\n            input_ids,\n            image_feature,\n            image_location,\n            token_type_ids,\n            attention_mask,\n            image_attention_mask,\n            output_all_encoded_layers=False,\n            output_all_attention_masks=output_all_attention_masks,\n        )\n\n        output = {}\n        if output_all_attention_masks:\n            output[""attention_weights""] = attention_weights\n\n        if self.fusion_method == ""sum"":\n            pooled_output = self.dropout(pooled_output_t + pooled_output_v)\n        elif self.fusion_method == ""mul"":\n            pooled_output = self.dropout(pooled_output_t * pooled_output_v)\n        else:\n            raise AssertionError\n\n        if self.training_head_type == ""nlvr2"":\n            pooled_output = pooled_output.view(-1, pooled_output.size(1) * 2)\n\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.contiguous().view(-1, self.num_labels)\n        output[""scores""] = reshaped_logits\n\n        return output\n\n\n@registry.register_model(""vilbert"")\nclass ViLBERT(BaseModel):\n    def __init__(self, config):\n        super().__init__(config)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/vilbert/pretrain.yaml""\n\n    # Backward compatibility\n    @classmethod\n    def format_state_key(cls, key):\n        return (\n            key.replace(""bert.bert"", ""model.bert"")\n            .replace(""bert.cls"", ""model.cls"")\n            .replace(""bert.classifier"", ""model.classifier"")\n        )\n\n    def build(self):\n        if self.config.training_head_type == ""pretraining"":\n            self.model = ViLBERTForPretraining(self.config)\n        else:\n            self.model = ViLBERTForClassification(self.config)\n\n        if getattr(self.config, ""freeze_base"", False):\n            for p in self.model.bert.parameters():\n                p.requires_grad = False\n\n    def get_image_and_text_features(self, sample_list):\n        bert_input_ids = sample_list.input_ids\n        bert_input_mask = sample_list.input_mask\n        bert_input_type_ids = sample_list.segment_ids\n\n        if sample_list.dataset_name == ""nlvr2"":\n            bert_input_ids = torch.cat([bert_input_ids, bert_input_ids])\n            bert_input_mask = torch.cat([bert_input_mask, bert_input_mask])\n            bert_input_type_ids = torch.cat([bert_input_type_ids, bert_input_type_ids])\n\n            # image input\n            img0 = getattr(sample_list, ""img0"", {})\n            image_info = getattr(img0, ""image_info_0"", {})\n            image_dim_variable_0 = getattr(image_info, ""max_features"", None)\n            image_feature_variable_0 = getattr(img0, ""image_feature_0"", None)\n\n            bbox = np.array(getattr(image_info, ""bbox"", None), dtype=np.float32)\n            image_w = np.array(\n                getattr(image_info, ""image_width"", None), dtype=np.float32\n            )\n            image_h = np.array(\n                getattr(image_info, ""image_height"", None), dtype=np.float32\n            )\n            image_location = np.zeros(\n                (bbox.shape[0], bbox.shape[1], 5), dtype=np.float32\n            )\n            image_location[:, :, :4] = bbox\n            image_location[:, :, 4] = (\n                (image_location[:, :, 3] - image_location[:, :, 1])\n                * (image_location[:, :, 2] - image_location[:, :, 0])\n                / (image_w * image_h)[:, None]\n            )\n            image_location[:, :, 0] = image_location[:, :, 0] / image_w[:, None]\n            image_location[:, :, 1] = image_location[:, :, 1] / image_h[:, None]\n            image_location[:, :, 2] = image_location[:, :, 2] / image_w[:, None]\n            image_location[:, :, 3] = image_location[:, :, 3] / image_h[:, None]\n            image_location_variable_0 = torch.tensor(\n                image_location, dtype=torch.float\n            ).cuda()\n\n            img1 = getattr(sample_list, ""img1"", {})\n            image_info = getattr(img1, ""image_info_0"", {})\n            image_dim_variable_1 = getattr(image_info, ""max_features"", None)\n            image_feature_variable_1 = getattr(img1, ""image_feature_0"", None)\n            bbox = np.array(getattr(image_info, ""bbox"", None), dtype=np.float32)\n            image_w = np.array(\n                getattr(image_info, ""image_width"", None), dtype=np.float32\n            )\n            image_h = np.array(\n                getattr(image_info, ""image_height"", None), dtype=np.float32\n            )\n            image_location = np.zeros(\n                (bbox.shape[0], bbox.shape[1], 5), dtype=np.float32\n            )\n            image_location[:, :, :4] = bbox\n            image_location[:, :, 4] = (\n                (image_location[:, :, 3] - image_location[:, :, 1])\n                * (image_location[:, :, 2] - image_location[:, :, 0])\n                / (image_w * image_h)[:, None]\n            )\n            image_location[:, :, 0] = image_location[:, :, 0] / image_w[:, None]\n            image_location[:, :, 1] = image_location[:, :, 1] / image_h[:, None]\n            image_location[:, :, 2] = image_location[:, :, 2] / image_w[:, None]\n            image_location[:, :, 3] = image_location[:, :, 3] / image_h[:, None]\n            image_location_variable_1 = torch.tensor(\n                image_location, dtype=torch.float\n            ).cuda()\n\n            image_feature_variable = torch.cat(\n                [image_feature_variable_0, image_feature_variable_1]\n            )\n            image_location_variable = torch.cat(\n                [image_location_variable_0, image_location_variable_1]\n            )\n            image_dim_variable = torch.cat([image_dim_variable_0, image_dim_variable_1])\n            image_label_variable = None\n            image_target_variable = None\n        else:\n            image_info = getattr(sample_list, ""image_info_0"", {})\n            image_dim_variable = getattr(image_info, ""max_features"", None)\n            image_feature_variable = getattr(sample_list, ""image_feature_0"", None)\n            image_label_variable = getattr(sample_list, ""image_labels"", None)\n            if image_label_variable is not None:\n                image_label_variable = torch.tensor(\n                    image_label_variable, dtype=torch.long\n                ).cuda()\n\n            bbox = np.array(getattr(image_info, ""bbox"", None), dtype=np.float32)\n            image_w = np.array(\n                getattr(image_info, ""image_width"", None), dtype=np.float32\n            )\n            image_h = np.array(\n                getattr(image_info, ""image_height"", None), dtype=np.float32\n            )\n            image_location = np.zeros(\n                (bbox.shape[0], bbox.shape[1], 5), dtype=np.float32\n            )\n            image_location[:, :, :4] = bbox\n            image_location[:, :, 4] = (\n                (image_location[:, :, 3] - image_location[:, :, 1])\n                * (image_location[:, :, 2] - image_location[:, :, 0])\n                / (image_w * image_h)[:, None]\n            )\n            image_location[:, :, 0] = image_location[:, :, 0] / image_w[:, None]\n            image_location[:, :, 1] = image_location[:, :, 1] / image_h[:, None]\n            image_location[:, :, 2] = image_location[:, :, 2] / image_w[:, None]\n            image_location[:, :, 3] = image_location[:, :, 3] / image_h[:, None]\n            image_location_variable = torch.tensor(\n                image_location, dtype=torch.float\n            ).cuda()\n\n            cls_prob = getattr(image_info, ""cls_prob"", None)\n            image_target = np.array(cls_prob, dtype=np.float32)\n            image_target_variable = torch.tensor(image_target, dtype=torch.float).cuda()\n\n        return {\n            ""input_ids"": bert_input_ids,\n            ""attention_mask"": bert_input_mask,\n            ""token_type_ids"": bert_input_type_ids,\n            ""image_dim"": image_dim_variable,\n            ""image_feature"": image_feature_variable,\n            ""image_location"": image_location_variable,\n            ""image_target"": image_target_variable,\n            ""image_label"": image_label_variable,\n        }\n\n    def get_optimizer_parameters(self, config):\n        return get_optimizer_parameters_for_bert(self.model, config)\n\n    def forward(self, sample_list):\n        params = self.get_image_and_text_features(sample_list)\n        # pretraining labels\n        params[""masked_lm_labels""] = getattr(sample_list, ""lm_label_ids"", None)\n        # is_random_next = getattr(sample_list, ""is_correct"", None)\n        # TODO(aps): Fix on dataset side\n        # params[""is_random_next""] = None\n\n        # Prepare Mask\n        if params[""image_feature""] is not None and params[""image_dim""] is not None:\n            image_mask = (\n                torch.arange(params[""image_feature""].size(-2))\n                .expand(*params[""image_feature""].size()[:-1])\n                .cuda()\n            )\n            if len(params[""image_dim""].size()) < len(image_mask.size()):\n                params[""image_dim""] = params[""image_dim""].unsqueeze(-1)\n                assert len(params[""image_dim""].size()) == len(image_mask.size())\n            image_mask = image_mask < params[""image_dim""]\n            params[""image_attention_mask""] = image_mask.long()\n        else:\n            params[""image_attention_mask""] = None\n        params.pop(""image_dim"")\n\n        output_dict = self.model(\n            params[""input_ids""],\n            params[""image_feature""],\n            params[""image_location""],\n            params[""token_type_ids""],\n            params[""attention_mask""],\n            params[""image_attention_mask""],\n            params[""masked_lm_labels""],\n            params[""image_label""],\n            params[""image_target""],\n        )\n\n        if self.config.training_head_type == ""pretraining"":\n            loss_key = ""{}/{}"".format(\n                sample_list.dataset_name, sample_list.dataset_type\n            )\n            output_dict[""losses""] = {}\n            output_dict[""losses""][loss_key + ""/masked_lm_loss""] = output_dict.pop(\n                ""masked_lm_loss""\n            )\n            output_dict[""losses""][loss_key + ""/masked_img_loss""] = output_dict.pop(\n                ""masked_img_loss""\n            )\n            # if params[""is_random_next""] is not None:\n            #     output_dict[""losses""][loss_key + ""/next_sentence_loss""]\n            #       = output_dict.pop(""next_sentence_loss"")\n\n        return output_dict\n'"
mmf/models/visdial_multi_modal.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.models.pythia import Pythia\nfrom mmf.modules.decoders import VisDialDiscriminator\n\n\nclass VisDialMultiModalModel(Pythia):\n    def __init__(self, config):\n        super().__init__(config)\n\n    def build(self):\n        self._init_text_embedding()\n        self._init_image_encoders()\n        self._init_image_embeddings()\n        self._init_combine_layer()\n        self._init_decoder()\n        self._init_extras()\n\n    def _init_text_embedding(self):\n        parent = super()\n        parent._init_text_embedding(""text_embeddings"", False)\n        parent._init_text_embedding(""history_embeddings"", True)\n\n    def get_optimizer_parameters(self, config):\n        # TODO: Update after implementing decoder\n        params = [\n            {""params"": self.img_embeddings_list.parameters()},\n            {""params"": self.text_embeddings.parameters()},\n            {""params"": self.multi_modal_combine_layer.parameters()},\n            {""params"": self.decoder.projection_layer.parameters()},\n            {\n                ""params"": self.img_feat_encoders.parameters(),\n                ""lr"": (config.optimizer.params.lr * 0.1),\n            },\n        ]\n\n        return params\n\n    def _update_text_embedding_args(self, args):\n        parent = super()\n        parent._update_text_embedding_args(args)\n        # Add embedding vectors to args\n        args.embedding_vectors = self.config.embedding_vectors\n\n    def _init_decoder(self):\n        embedding = self.text_embeddings[0].module\n        embedding_dim = self.text_embeddings[0].embedding_dim\n        hidden_dim = self.multi_modal_combine_layer.out_dim\n\n        self.decoder = VisDialDiscriminator(\n            {""embedding_dim"": embedding_dim, ""hidden_dim"": hidden_dim}, embedding\n        )\n\n    def combine_embeddings(self, *args):\n        return self.multi_modal_combine_layer(*args)\n\n    def calculate_logits(self, joint_embedding, **kwargs):\n        return self.decoder(joint_embedding, kwargs)\n\n    def forward(\n        self, texts, answer_options, histories, image_features, image_dims, **kwargs\n    ):\n\n        texts = texts.view(-1, texts.size(2))\n        histories = histories.view(-1, histories.size(2))\n        text_embedding_total = self.process_text_embedding(texts)\n        histories_total = self.process_text_embedding(histories, ""history_embeddings"")\n\n        for idx, image_feature in enumerate(image_features):\n            feature_size = image_feature.size()[2:]\n            image_features[idx] = image_feature.view(-1, *feature_size)\n\n        size = image_dims.size()[2:]\n        image_dims = image_dims.view(-1, *size)\n\n        assert len(image_features) == len(\n            self.img_feat_encoders\n        ), ""number of image feature model doesnot equal \\\n                 to number of image features""\n\n        image_embedding_total = self.process_image_embedding(\n            image_features, image_dims, text_embedding_total\n        )\n\n        if self.inter_model is not None:\n            image_embedding_total = self.inter_model(image_embedding_total)\n\n        joint_embedding = self.combine_embeddings(\n            image_embedding_total, text_embedding_total, histories_total\n        )\n\n        decoder_info = {\n            ""answer_options"": answer_options,\n            ""answer_options_len"": kwargs[""answer_options_len""],\n        }\n        return self.calculate_logits(joint_embedding, **decoder_info)\n'"
mmf/models/visual_bert.py,14,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# Initial version was taken from https://github.com/uclanlp/visualbert\n# which was cleaned up and adapted for MMF.\n\nimport os\nfrom copy import deepcopy\n\nimport torch\nfrom omegaconf import OmegaConf\nfrom torch import nn\nfrom transformers.modeling_bert import (\n    BertConfig,\n    BertEncoder,\n    BertForPreTraining,\n    BertLayer,\n    BertPooler,\n    BertPredictionHeadTransform,\n    BertPreTrainedModel,\n)\n\nfrom mmf.common.registry import registry\nfrom mmf.models import BaseModel\nfrom mmf.modules.embeddings import BertVisioLinguisticEmbeddings\nfrom mmf.utils.configuration import get_mmf_cache_dir\nfrom mmf.utils.modeling import get_optimizer_parameters_for_bert\nfrom mmf.utils.transform import (\n    transform_to_batch_sequence,\n    transform_to_batch_sequence_dim,\n)\n\n\nclass VisualBERTBase(BertPreTrainedModel):\n    def __init__(\n        self,\n        config,\n        visual_embedding_dim=512,\n        embedding_strategy=""plain"",\n        bypass_transformer=False,\n        output_attentions=False,\n        output_hidden_states=False,\n    ):\n        super().__init__(config)\n        self.config = config\n\n        config.visual_embedding_dim = visual_embedding_dim\n        config.embedding_strategy = embedding_strategy\n        config.bypass_transformer = bypass_transformer\n        config.output_attentions = output_attentions\n        config.output_hidden_states = output_hidden_states\n\n        self.embeddings = BertVisioLinguisticEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.bypass_transformer = config.bypass_transformer\n\n        if self.bypass_transformer:\n            self.additional_layer = BertLayer(config)\n\n        self.output_attentions = self.config.output_attentions\n        self.output_hidden_states = self.config.output_hidden_states\n        self.fixed_head_masks = [None for _ in range(len(self.encoder.layer))]\n        self.init_weights()\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask=None,\n        token_type_ids=None,\n        visual_embeddings=None,\n        position_embeddings_visual=None,\n        visual_embeddings_type=None,\n        image_text_alignment=None,\n    ):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of\n        # causal attention used in OpenAI GPT, we just need to prepare the\n        # broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=next(self.parameters()).dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(\n            input_ids,\n            token_type_ids,\n            visual_embeddings=visual_embeddings,\n            position_embeddings_visual=position_embeddings_visual,\n            visual_embeddings_type=visual_embeddings_type,\n            image_text_alignment=image_text_alignment,\n        )\n\n        if self.bypass_transformer and visual_embeddings is not None:\n            assert (\n                not self.output_hidden_states\n            )  # Don\'t support this for the bypass model\n            text_length = input_ids.size(1)\n            text_embedding_output = embedding_output[:, :text_length, :]\n            visual_part = embedding_output[:, text_length:, :]\n\n            text_extended_attention_mask = extended_attention_mask[\n                :, :, :text_length, :text_length\n            ]\n\n            encoded_layers = self.encoder(\n                text_embedding_output,\n                text_extended_attention_mask,\n                self.fixed_head_masks,\n            )\n            sequence_output = encoded_layers[0]\n            new_input = torch.cat((sequence_output, visual_part), dim=1)\n            final_sequence_output = self.additional_layer(\n                new_input, extended_attention_mask\n            )\n            pooled_output = self.pooler(final_sequence_output)\n            return final_sequence_output, pooled_output\n\n        if self.output_attentions:\n            encoded_layers = self.encoder(\n                embedding_output, extended_attention_mask, self.fixed_head_masks\n            )\n            sequence_output = encoded_layers[0]\n            attn_data_list = encoded_layers[1:]\n            pooled_output = self.pooler(sequence_output)\n            return encoded_layers, pooled_output, attn_data_list\n        else:\n            encoded_layers = self.encoder(\n                embedding_output, extended_attention_mask, self.fixed_head_masks\n            )\n            sequence_output = encoded_layers[0]\n            pooled_output = self.pooler(sequence_output)\n            return sequence_output, pooled_output, []\n\n\nclass VisualBERTForPretraining(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.output_attentions = self.config.output_attentions\n        self.output_hidden_states = self.config.output_hidden_states\n\n        # If bert_model_name is not specified, you will need to specify\n        # all of the required parameters for BERTConfig and a pretrained\n        # model won\'t be loaded\n        self.bert_model_name = getattr(self.config, ""bert_model_name"", None)\n        self.bert_config = BertConfig.from_dict(\n            OmegaConf.to_container(self.config, resolve=True)\n        )\n        if self.bert_model_name is None:\n            self.bert = VisualBERTBase(\n                self.bert_config,\n                visual_embedding_dim=self.config.visual_embedding_dim,\n                embedding_strategy=self.config.embedding_strategy,\n                bypass_transformer=self.config.bypass_transformer,\n                output_attentions=self.config.output_attentions,\n                output_hidden_states=self.config.output_hidden_states,\n            )\n        else:\n            self.bert = VisualBERTBase.from_pretrained(\n                self.config.bert_model_name,\n                config=self.bert_config,\n                cache_dir=os.path.join(\n                    get_mmf_cache_dir(), ""distributed_{}"".format(-1)\n                ),\n                visual_embedding_dim=self.config.visual_embedding_dim,\n                embedding_strategy=self.config.embedding_strategy,\n                bypass_transformer=self.config.bypass_transformer,\n                output_attentions=self.config.output_attentions,\n                output_hidden_states=self.config.output_hidden_states,\n            )\n\n        self.vocab_size = self.bert.config.vocab_size\n\n        # TODO: Once omegaconf fixes int keys issue, bring this back\n        # See https://github.com/omry/omegaconf/issues/149\n        # with omegaconf.open_dict(self.config):\n        #     # Add bert config such as hidden_state to our main config\n        #     self.config.update(self.bert.config.to_dict())\n        if self.bert_model_name is None:\n            bert_masked_lm = BertForPreTraining(self.bert.config)\n        else:\n            bert_masked_lm = BertForPreTraining.from_pretrained(\n                self.config.bert_model_name,\n                cache_dir=os.path.join(\n                    get_mmf_cache_dir(), ""distributed_{}"".format(-1)\n                ),\n            )\n        self.cls = deepcopy(bert_masked_lm.cls)\n        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n        self.init_weights()\n\n    def init_weights(self):\n        if self.config.random_initialize is False:\n            if self.bert_model_name is None:\n                # No pretrained model, init weights\n                self.bert.init_weights()\n                self.cls.apply(self.bert._init_weights)\n\n            self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them\n            instead.\n        """"""\n        self.bert._tie_or_clone_weights(\n            self.cls.predictions.decoder, self.bert.embeddings.word_embeddings\n        )\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask=None,\n        token_type_ids=None,\n        visual_embeddings=None,\n        position_embeddings_visual=None,\n        visual_embeddings_type=None,\n        image_text_alignment=None,\n        masked_lm_labels=None,\n    ):\n        sequence_output, pooled_output, attention_weights = self.bert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            visual_embeddings,\n            position_embeddings_visual,\n            visual_embeddings_type,\n            image_text_alignment,\n        )\n\n        output_dict = {}\n\n        if self.output_attentions:\n            output_dict[""attention_weights""] = attention_weights\n\n        if self.output_hidden_states:\n            output_dict[""sequence_output""] = sequence_output\n            output_dict[""pooled_output""] = pooled_output\n\n        prediction_scores, seq_relationship_score = self.cls(\n            sequence_output, pooled_output\n        )\n        if masked_lm_labels is not None:\n            output_dict[""logits""] = prediction_scores\n            masked_lm_loss = self.loss_fct(\n                prediction_scores.contiguous().view(-1, self.vocab_size),\n                masked_lm_labels.contiguous().view(-1),\n            )\n            output_dict[""masked_lm_loss""] = masked_lm_loss\n            output_dict[""loss""] = masked_lm_loss\n\n        return output_dict\n\n\nclass VisualBERTForClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.output_attentions = self.config.output_attentions\n        self.output_hidden_states = self.config.output_hidden_states\n\n        # If bert_model_name is not specified, you will need to specify\n        # all of the required parameters for BERTConfig and a pretrained\n        # model won\'t be loaded\n        self.bert_model_name = getattr(self.config, ""bert_model_name"", None)\n        self.bert_config = BertConfig.from_dict(\n            OmegaConf.to_container(self.config, resolve=True)\n        )\n        if self.bert_model_name is None:\n            self.bert = VisualBERTBase(\n                self.bert_config,\n                visual_embedding_dim=self.config.visual_embedding_dim,\n                embedding_strategy=self.config.embedding_strategy,\n                bypass_transformer=self.config.bypass_transformer,\n                output_attentions=self.config.output_attentions,\n                output_hidden_states=self.config.output_hidden_states,\n            )\n        else:\n            self.bert = VisualBERTBase.from_pretrained(\n                self.config.bert_model_name,\n                config=self.bert_config,\n                cache_dir=os.path.join(\n                    get_mmf_cache_dir(), ""distributed_{}"".format(-1)\n                ),\n                visual_embedding_dim=self.config.visual_embedding_dim,\n                embedding_strategy=self.config.embedding_strategy,\n                bypass_transformer=self.config.bypass_transformer,\n                output_attentions=self.config.output_attentions,\n                output_hidden_states=self.config.output_hidden_states,\n            )\n\n        self.training_head_type = self.config.training_head_type\n        self.num_labels = self.config.num_labels\n        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n        if self.config.training_head_type == ""nlvr2"":\n            self.bert.config.hidden_size *= 2\n        self.classifier = nn.Sequential(\n            BertPredictionHeadTransform(self.bert.config),\n            nn.Linear(self.bert.config.hidden_size, self.config.num_labels),\n        )\n\n        self.init_weights()\n\n    def init_weights(self):\n        if self.config.random_initialize is False:\n            if self.bert_model_name is None:\n                # No pretrained model, init weights\n                self.bert.init_weights()\n\n            # Classifier needs to be initialized always as it is task specific\n            self.classifier.apply(self.bert._init_weights)\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask=None,\n        token_type_ids=None,\n        visual_embeddings=None,\n        position_embeddings_visual=None,\n        visual_embeddings_type=None,\n        image_text_alignment=None,\n        masked_lm_labels=None,\n    ):\n        sequence_output, pooled_output, attention_weights = self.bert(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n            visual_embeddings,\n            position_embeddings_visual,\n            visual_embeddings_type,\n            image_text_alignment,\n        )\n\n        if self.training_head_type == ""nlvr2"":\n            # 2B * H => B * 2H\n            b, h = pooled_output.size()\n            pooled_output = torch.cat(\n                [pooled_output[: b // 2], pooled_output[b // 2 :]], dim=1\n            )\n\n        output_dict = {}\n        if self.output_attentions:\n            output_dict[""attention_weights""] = attention_weights\n\n        if self.output_hidden_states:\n            output_dict[""sequence_output""] = sequence_output\n            output_dict[""pooled_output""] = pooled_output\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.contiguous().view(-1, self.num_labels)\n        output_dict[""scores""] = reshaped_logits\n        return output_dict\n\n\n@registry.register_model(""visual_bert"")\nclass VisualBERT(BaseModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.config = config\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/models/visual_bert/pretrain.yaml""\n\n    def build(self):\n        if self.config.training_head_type == ""pretraining"":\n            self.model = VisualBERTForPretraining(self.config)\n        else:\n            self.model = VisualBERTForClassification(self.config)\n\n        if self.config.special_visual_initialize:\n            self.model.bert.embeddings.initialize_visual_from_pretrained()\n\n        if getattr(self.config, ""freeze_base"", False):\n            for p in self.model.bert.parameters():\n                p.requires_grad = False\n\n    def flatten(self, sample_list, to_be_flattened=None, to_be_flattened_dim=None):\n        if to_be_flattened is None:\n            to_be_flattened = {}\n        if to_be_flattened_dim is None:\n            to_be_flattened_dim = {}\n        for key in to_be_flattened:\n            # Make sure these keys are present or otherwise set these keys to None\n            sample_list[key] = getattr(sample_list, key, None)\n            sample_list[key] = transform_to_batch_sequence(sample_list[key])\n        for key in to_be_flattened_dim:\n            sample_list[key] = getattr(sample_list, key, None)\n            sample_list[key] = transform_to_batch_sequence_dim(sample_list[key])\n\n        if sample_list.visual_embeddings_type is None:\n            if sample_list.image_mask is not None:\n                sample_list.visual_embeddings_type = torch.zeros_like(\n                    sample_list.image_mask, dtype=torch.long\n                )\n\n        if sample_list.image_mask is not None:\n            attention_mask = torch.cat(\n                (sample_list.input_mask, sample_list.image_mask), dim=-1\n            )\n            if sample_list.masked_lm_labels is not None:\n                assert sample_list.masked_lm_labels.size(\n                    -1\n                ) == sample_list.input_mask.size(-1)\n                new_lm_labels = torch.ones_like(attention_mask) * -1\n                size_masked_lm_labels = sample_list.masked_lm_labels.size()\n                assert len(size_masked_lm_labels) == 2\n                new_lm_labels[\n                    : size_masked_lm_labels[0], : size_masked_lm_labels[1]\n                ] = sample_list.masked_lm_labels\n                sample_list.masked_lm_labels = new_lm_labels\n        else:\n            attention_mask = sample_list.input_mask\n\n        sample_list.attention_mask = attention_mask\n\n        return sample_list\n\n    def get_optimizer_parameters(self, config):\n        return get_optimizer_parameters_for_bert(self.model, config)\n\n    def flatten_for_bert(self, sample_list, **kwargs):\n        to_be_flattened = [\n            ""input_ids"",\n            ""token_type_ids"",\n            ""input_mask"",\n            ""image_mask"",\n            ""masked_lm_labels"",\n            ""position_embeddings_visual"",\n            ""visual_embeddings_type"",\n        ]\n        to_be_flattened_dim = [""image_text_alignment"", ""visual_embeddings""]\n\n        # We want to convert everything into: batch x sequence_length x (dim).\n        flattened = self.flatten(sample_list, to_be_flattened, to_be_flattened_dim)\n        return flattened\n\n    def update_sample_list_based_on_head(self, sample_list):\n        bert_input_ids = sample_list.input_ids\n        bert_input_mask = sample_list.input_mask\n        bert_input_type_ids = sample_list.segment_ids\n\n        if self.config.training_head_type == ""nlvr2"":\n            bert_input_ids = torch.cat([bert_input_ids, bert_input_ids])\n            bert_input_mask = torch.cat([bert_input_mask, bert_input_mask])\n            bert_input_type_ids = torch.cat([bert_input_type_ids, bert_input_type_ids])\n\n            # image input\n            img0 = getattr(sample_list, ""img0"", {})\n            image_info = getattr(img0, ""image_info_0"", {})\n            image_dim_variable_0 = getattr(image_info, ""max_features"", None)\n            image_feat_variable_0 = getattr(img0, ""image_feature_0"", None)\n\n            img1 = getattr(sample_list, ""img1"", {})\n            image_info = getattr(img1, ""image_info_0"", {})\n            image_dim_variable_1 = getattr(image_info, ""max_features"", None)\n            image_feat_variable_1 = getattr(img1, ""image_feature_0"", None)\n\n            image_feat_variable = torch.cat(\n                [image_feat_variable_0, image_feat_variable_1]\n            )\n            image_dim_variable = torch.cat([image_dim_variable_0, image_dim_variable_1])\n        else:\n            image_info = getattr(sample_list, ""image_info_0"", {})\n            image_dim_variable = getattr(image_info, ""max_features"", None)\n            image_feat_variable = getattr(sample_list, ""image_feature_0"", None)\n\n        sample_list.visual_embeddings = image_feat_variable\n        sample_list.image_dim = image_dim_variable\n        sample_list.input_ids = bert_input_ids\n        sample_list.input_mask = bert_input_mask\n        sample_list.token_type_ids = bert_input_type_ids\n        return sample_list\n\n    def add_custom_params(self, sample_list):\n        visual_embeddings = getattr(sample_list, ""visual_embeddings"", None)\n        image_dim = getattr(sample_list, ""image_dim"", None)\n        # pretraining labels\n        sample_list.masked_lm_labels = getattr(sample_list, ""lm_label_ids"", None)\n        # image_feat_variable = batch x ( num_choice x ) image_feature_length x dim\n        # Prepare Mask\n        if visual_embeddings is not None and image_dim is not None:\n            image_mask = (\n                torch.arange(visual_embeddings.size(-2))\n                .expand(*visual_embeddings.size()[:-1])\n                .cuda()\n            )\n            if len(image_dim.size()) < len(image_mask.size()):\n                image_dim = image_dim.unsqueeze(-1)\n                assert len(image_dim.size()) == len(image_mask.size())\n            image_mask = image_mask < image_dim\n            sample_list.image_mask = image_mask.long()\n        else:\n            sample_list.image_mask = None\n\n        sample_list.position_embeddings_visual = None\n\n        return sample_list\n\n    # Backward compatibility for code from original VisualBERT\n    @classmethod\n    def format_state_key(cls, key):\n        return (\n            key.replace(""bert.bert"", ""model.bert"")\n            .replace(""bert.cls"", ""model.cls"")\n            .replace(""bert.classifier"", ""model.classifier"")\n        )\n\n    def forward(self, sample_list):\n        sample_list = self.update_sample_list_based_on_head(sample_list)\n        sample_list = self.add_custom_params(sample_list)\n        sample_list = self.flatten_for_bert(sample_list)\n\n        output_dict = self.model(\n            sample_list.input_ids,\n            sample_list.attention_mask,\n            sample_list.token_type_ids,\n            sample_list.visual_embeddings,\n            sample_list.position_embeddings_visual,\n            sample_list.visual_embeddings_type,\n            sample_list.image_text_alignment,\n            sample_list.masked_lm_labels,\n        )\n\n        if ""pretraining"" in self.config.training_head_type:\n            loss_key = ""{}/{}"".format(\n                sample_list.dataset_name, sample_list.dataset_type\n            )\n            output_dict[""losses""] = {}\n            output_dict[""losses""][loss_key + ""/masked_lm_loss""] = output_dict.pop(\n                ""masked_lm_loss""\n            )\n\n        return output_dict\n'"
mmf/modules/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport mmf.modules.losses\nimport mmf.modules.metrics\nimport mmf.modules.optimizers\nimport mmf.modules.schedulers\n'"
mmf/modules/attention.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\nfrom torch import nn\n\nfrom mmf.modules.layers import GatedTanh, ModalCombineLayer, TransformLayer\n\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, image_dim, question_dim, **kwargs):\n        super().__init__()\n\n        combine_type = kwargs[""modal_combine""][""type""]\n        combine_params = kwargs[""modal_combine""][""params""]\n        modal_combine_layer = ModalCombineLayer(\n            combine_type, image_dim, question_dim, **combine_params\n        )\n\n        transform_type = kwargs[""transform""][""type""]\n        transform_params = kwargs[""transform""][""params""]\n        transform_layer = TransformLayer(\n            transform_type, modal_combine_layer.out_dim, **transform_params\n        )\n\n        normalization = kwargs[""normalization""]\n\n        self.module = TopDownAttention(\n            modal_combine_layer, transform_layer, normalization\n        )\n\n        if hasattr(self.module, ""out_dim""):\n            self.out_dim = self.module.out_dim\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass ConcatenationAttention(nn.Module):\n    def __init__(self, image_feat_dim, txt_rnn_embeding_dim, hidden_size):\n        super().__init__()\n        self.image_feat_dim = image_feat_dim\n        self.txt_embeding_dim = txt_rnn_embeding_dim\n        self.fa = GatedTanh(image_feat_dim + txt_rnn_embeding_dim, hidden_size)\n        self.lc = nn.Linear(hidden_size, 1)\n\n    def forward(self, image_feat, question_embedding):\n        _, num_location, _ = image_feat.shape\n        question_embedding_expand = torch.unsqueeze(question_embedding, 1).expand(\n            -1, num_location, -1\n        )\n        concat_feature = torch.cat((image_feat, question_embedding_expand), dim=2)\n        raw_attention = self.lc(self.fa(concat_feature))\n        # softmax across locations\n        attention_weights = nn.functional.softmax(raw_attention, dim=1)\n        attention_weights = attention_weights.expand_as(image_feat)\n        return attention_weights\n\n\nclass ProjectAttention(nn.Module):\n    def __init__(self, image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout=0.2):\n        super().__init__()\n        self.image_feat_dim = image_feat_dim\n        self.txt_embeding_dim = txt_rnn_embeding_dim\n        self.fa_image = GatedTanh(image_feat_dim, hidden_size)\n        self.fa_txt = GatedTanh(txt_rnn_embeding_dim, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.lc = nn.Linear(hidden_size, 1)\n\n    def compute_raw_att(self, image_feat, question_embedding):\n        num_location = image_feat.shape[1]\n        image_fa = self.fa_image(image_feat)\n        question_fa = self.fa_txt(question_embedding)\n        question_fa_expand = torch.unsqueeze(question_fa, 1).expand(\n            -1, num_location, -1\n        )\n        joint_feature = image_fa * question_fa_expand\n        joint_feature = self.dropout(joint_feature)\n        raw_attention = self.lc(joint_feature)\n        return raw_attention\n\n    def forward(self, image_feat, question_embedding):\n        raw_attention = self.compute_raw_att(image_feat, question_embedding)\n        # softmax across locations\n        attention_weights = nn.functional.softmax(raw_attention, dim=1)\n        attention_weights = attention_weights.expand_as(image_feat)\n        return attention_weights\n\n\nclass DoubleProjectAttention(nn.Module):\n    def __init__(self, image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout=0.2):\n        super().__init__()\n        self.att1 = ProjectAttention(\n            image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout\n        )\n        self.att2 = ProjectAttention(\n            image_feat_dim, txt_rnn_embeding_dim, hidden_size, dropout\n        )\n        self.image_feat_dim = image_feat_dim\n        self.txt_embeding_dim = txt_rnn_embeding_dim\n\n    def forward(self, image_feat, question_embedding):\n        att1 = self.att1.compute_raw_att(image_feat, question_embedding)\n        att2 = self.att2.compute_raw_att(image_feat, question_embedding)\n        raw_attn_weights = att1 + att2\n        # softmax across locations\n        attention_weights = nn.functional.softmax(raw_attn_weights, dim=1)\n        attention_weights = attention_weights.expand_as(image_feat)\n        return attention_weights\n\n\nclass TopDownAttention(nn.Module):\n    EPS = 1.0e-08\n\n    def __init__(self, combination_layer, transform_module, normalization):\n        super().__init__()\n        self.combination_layer = combination_layer\n        self.normalization = normalization\n        self.transform = transform_module\n        self.out_dim = self.transform.out_dim\n\n    @staticmethod\n    def _mask_attentions(attention, image_locs):\n        batch_size, num_loc, n_att = attention.size()\n        tmp1 = attention.new_zeros(num_loc)\n        tmp1[:num_loc] = torch.arange(0, num_loc, dtype=attention.dtype).unsqueeze(\n            dim=0\n        )\n\n        tmp1 = tmp1.expand(batch_size, num_loc)\n        tmp2 = image_locs.type(tmp1.type())\n        tmp2 = tmp2.unsqueeze(dim=1).expand(batch_size, num_loc)\n        mask = torch.ge(tmp1, tmp2)\n        mask = mask.unsqueeze(dim=2).expand_as(attention)\n        attention = attention.masked_fill(mask, 0)\n        return attention\n\n    def forward(self, image_feat, question_embedding, image_locs=None):\n        # N x K x joint_dim\n        joint_feature = self.combination_layer(image_feat, question_embedding)\n        # N x K x n_att\n        raw_attn = self.transform(joint_feature)\n\n        if self.normalization.lower() == ""softmax"":\n            attention = nn.functional.softmax(raw_attn, dim=1)\n            if image_locs is not None:\n                masked_attention = self._mask_attentions(attention, image_locs)\n                masked_attention_sum = torch.sum(masked_attention, dim=1, keepdim=True)\n                masked_attention_sum += masked_attention_sum.eq(0).float() + self.EPS\n                masked_attention = masked_attention / masked_attention_sum\n            else:\n                masked_attention = attention\n\n        elif self.normalization.lower() == ""sigmoid"":\n            attention = torch.sigmoid(raw_attn)\n            masked_attention = attention\n            if image_locs is not None:\n                masked_attention = self._mask_attentions(attention, image_locs)\n\n        return masked_attention\n'"
mmf/modules/decoders.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\nfrom torch import nn\nfrom torch.nn.utils.weight_norm import weight_norm\n\nfrom mmf.common.registry import registry\n\n\nclass VisDialDiscriminator(nn.Module):\n    def __init__(self, config, embedding):\n        super().__init__()\n        self.config = config\n        self.embedding = embedding\n\n        self.emb_out_dim = embedding.text_out_dim\n        self.hidden_dim = self.config.hidden_dim\n\n        self.projection_layer = nn.Linear(self.emb_out_dim, self.hidden_dim)\n\n    def forward(self, encoder_output, batch):\n        answer_options_len = batch[""answer_options_len""]\n\n        # BATCH_SIZE X DIALOGUES X 100 X SEQ_LEN\n        answer_options = batch[""answer_options""]\n\n        max_seq_len = answer_options.size(-1)\n\n        batch_size, ndialogues, noptions, seq_len = answer_options.size()\n\n        # (B X D X 100) X SEQ_LEN\n        answer_options = answer_options.view(-1, max_seq_len)\n        answer_options_len = answer_options_len.view(-1)\n\n        # (B x D x 100) x EMB_OUT_DIM\n        answer_options = self.embedding(answer_options)\n\n        # (B x D x 100) x HIDDEN_DIM\n        answer_options = self.projection_layer(answer_options)\n\n        # (B x D) x 100 x HIDDEN_DIM\n        answer_options = answer_options.view(\n            batch_size * ndialogues, noptions, self.hidden_dim\n        )\n\n        # (B x D) x HIDDEN_DIM => (B x D) x 100 x HIDDEN_DIM\n        encoder_output = encoder_output.unsqueeze(1).expand(-1, noptions, -1)\n\n        # (B x D) x 100 x HIDDEN_DIM * (B x D) x 100 x HIDDEN_DIM = SAME THING\n        # SUM => (B x D) x 100\n        scores = torch.sum(answer_options * encoder_output, dim=2)\n\n        return scores\n\n\nclass LanguageDecoder(nn.Module):\n    def __init__(self, in_dim, out_dim, **kwargs):\n        super().__init__()\n\n        self.language_lstm = nn.LSTMCell(\n            in_dim + kwargs[""hidden_dim""], kwargs[""hidden_dim""], bias=True\n        )\n        self.fc = weight_norm(nn.Linear(kwargs[""hidden_dim""], out_dim))\n        self.dropout = nn.Dropout(p=kwargs[""dropout""])\n        self.init_weights(kwargs[""fc_bias_init""])\n\n    def init_weights(self, fc_bias_init):\n        self.fc.bias.data.fill_(fc_bias_init)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, weighted_attn):\n        # Get LSTM state\n        state = registry.get(f""{weighted_attn.device}_lstm_state"")\n        h1, c1 = state[""td_hidden""]\n        h2, c2 = state[""lm_hidden""]\n\n        # Language LSTM\n        h2, c2 = self.language_lstm(torch.cat([weighted_attn, h1], dim=1), (h2, c2))\n        predictions = self.fc(self.dropout(h2))\n\n        # Update hidden state for t+1\n        state[""lm_hidden""] = (h2, c2)\n\n        return predictions\n'"
mmf/modules/embeddings.py,15,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# TODO: Update kwargs with defaults\nimport os\nimport pickle\nfrom copy import deepcopy\nfrom functools import lru_cache\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom transformers.modeling_bert import BertEmbeddings\n\nfrom mmf.modules.attention import AttentionLayer\nfrom mmf.modules.layers import Identity\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.vocab import Vocab\n\n\nclass TextEmbedding(nn.Module):\n    def __init__(self, emb_type, **kwargs):\n        super().__init__()\n        self.model_data_dir = kwargs.get(""model_data_dir"", None)\n        self.embedding_dim = kwargs.get(""embedding_dim"", None)\n\n        # Update kwargs here\n        if emb_type == ""identity"":\n            self.module = Identity()\n            self.module.text_out_dim = self.embedding_dim\n        elif emb_type == ""vocab"":\n            self.module = VocabEmbedding(**kwargs)\n            self.module.text_out_dim = self.embedding_dim\n        elif emb_type == ""projection"":\n            self.module = ProjectionEmbedding(**kwargs)\n            self.module.text_out_dim = self.module.out_dim\n        elif emb_type == ""preextracted"":\n            self.module = PreExtractedEmbedding(**kwargs)\n        elif emb_type == ""bilstm"":\n            self.module = BiLSTMTextEmbedding(**kwargs)\n        elif emb_type == ""attention"":\n            self.module = AttentionTextEmbedding(**kwargs)\n        elif emb_type == ""torch"":\n            vocab_size = kwargs[""vocab_size""]\n            embedding_dim = kwargs[""embedding_dim""]\n            self.module = nn.Embedding(vocab_size, embedding_dim)\n            self.module.text_out_dim = self.embedding_dim\n        else:\n            raise NotImplementedError(""Unknown question embedding \'%s\'"" % emb_type)\n\n        self.text_out_dim = self.module.text_out_dim\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass VocabEmbedding(nn.Module):\n    def __init__(self, embedding_dim, **vocab_params):\n        super().__init__()\n        self.vocab = Vocab(**vocab_params)\n        self.module = self.vocab.get_embedding(\n            nn.Embedding, embedding_dim=embedding_dim\n        )\n\n    def forward(self, x):\n        return self.module(x)\n\n\nclass BiLSTMTextEmbedding(nn.Module):\n    def __init__(\n        self,\n        hidden_dim,\n        embedding_dim,\n        num_layers,\n        dropout,\n        bidirectional=False,\n        rnn_type=""GRU"",\n    ):\n        super().__init__()\n        self.text_out_dim = hidden_dim\n        self.bidirectional = bidirectional\n\n        if rnn_type == ""LSTM"":\n            rnn_cls = nn.LSTM\n        elif rnn_type == ""GRU"":\n            rnn_cls = nn.GRU\n\n        self.recurrent_encoder = rnn_cls(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            dropout=dropout,\n            bidirectional=bidirectional,\n            batch_first=True,\n        )\n\n    def forward(self, x):\n        out, _ = self.recurrent_encoder(x)\n        # Return last state\n        if self.bidirectional:\n            return out[:, -1]\n\n        forward_ = out[:, -1, : self.num_hid]\n        backward = out[:, 0, self.num_hid :]\n        return torch.cat((forward_, backward), dim=1)\n\n    def forward_all(self, x):\n        output, _ = self.recurrent_encoder(x)\n        return output\n\n\nclass PreExtractedEmbedding(nn.Module):\n    def __init__(self, out_dim, base_path):\n        super().__init__()\n        self.text_out_dim = out_dim\n        self.base_path = base_path\n        self.cache = {}\n\n    def forward(self, qids):\n        embeddings = []\n        for qid in qids:\n            embeddings.append(self.get_item(qid))\n        return torch.stack(embeddings, dim=0)\n\n    @lru_cache(maxsize=5000)\n    def get_item(self, qid):\n        return np.load(os.path.join(self.base_path, str(qid.item()) + "".npy""))\n\n\nclass AttentionTextEmbedding(nn.Module):\n    def __init__(self, hidden_dim, embedding_dim, num_layers, dropout, **kwargs):\n        super().__init__()\n\n        self.text_out_dim = hidden_dim * kwargs[""conv2_out""]\n\n        bidirectional = kwargs.get(""bidirectional"", False)\n\n        self.recurrent_unit = nn.LSTM(\n            input_size=embedding_dim,\n            hidden_size=hidden_dim // 2 if bidirectional else hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            bidirectional=bidirectional,\n        )\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        conv1_out = kwargs[""conv1_out""]\n        conv2_out = kwargs[""conv2_out""]\n        kernel_size = kwargs[""kernel_size""]\n        padding = kwargs[""padding""]\n\n        self.conv1 = nn.Conv1d(\n            in_channels=hidden_dim,\n            out_channels=conv1_out,\n            kernel_size=kernel_size,\n            padding=padding,\n        )\n\n        self.conv2 = nn.Conv1d(\n            in_channels=conv1_out,\n            out_channels=conv2_out,\n            kernel_size=kernel_size,\n            padding=padding,\n        )\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        self.recurrent_unit.flatten_parameters()\n        # self.recurrent_unit.flatten_parameters()\n        lstm_out, _ = self.recurrent_unit(x)  # N * T * hidden_dim\n        lstm_drop = self.dropout(lstm_out)  # N * T * hidden_dim\n        lstm_reshape = lstm_drop.permute(0, 2, 1)  # N * hidden_dim * T\n\n        qatt_conv1 = self.conv1(lstm_reshape)  # N x conv1_out x T\n        qatt_relu = self.relu(qatt_conv1)\n        qatt_conv2 = self.conv2(qatt_relu)  # N x conv2_out x T\n\n        # Over last dim\n        qtt_softmax = nn.functional.softmax(qatt_conv2, dim=2)\n        # N * conv2_out * hidden_dim\n        qtt_feature = torch.bmm(qtt_softmax, lstm_drop)\n        # N * (conv2_out * hidden_dim)\n        qtt_feature_concat = qtt_feature.view(batch_size, -1)\n\n        return qtt_feature_concat\n\n\nclass ProjectionEmbedding(nn.Module):\n    def __init__(self, module, in_dim, out_dim, **kwargs):\n        super().__init__()\n        if module == ""linear"":\n            self.layers = nn.Linear(in_dim, out_dim)\n            self.out_dim = out_dim\n        elif module == ""conv"":\n            last_out_channels = in_dim\n            layers = []\n            for conv in kwargs[""convs""]:\n                layers.append(nn.Conv1d(in_channels=last_out_channels, **conv))\n                last_out_channels = conv[""out_channels""]\n            self.layers = nn.ModuleList(*layers)\n            self.out_dim = last_out_channels\n        else:\n            raise TypeError(\n                ""Unknown module type for \'ProjectionEmbedding\',""\n                ""use either \'linear\' or \'conv\'""\n            )\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass ImageFeatureEmbedding(nn.Module):\n    """"""\n    parameters:\n\n    input:\n    image_feat_variable: [batch_size, num_location, image_feat_dim]\n    or a list of [num_location, image_feat_dim]\n    when using adaptive number of objects\n    question_embedding:[batch_size, txt_embeding_dim]\n\n    output:\n    image_embedding:[batch_size, image_feat_dim]\n\n\n    """"""\n\n    def __init__(self, img_dim, question_dim, **kwargs):\n        super().__init__()\n\n        self.image_attention_model = AttentionLayer(img_dim, question_dim, **kwargs)\n        self.out_dim = self.image_attention_model.out_dim\n\n    def forward(self, image_feat_variable, question_embedding, image_dims, extra=None):\n        if extra is None:\n            extra = {}\n        # N x K x n_att\n        attention = self.image_attention_model(\n            image_feat_variable, question_embedding, image_dims\n        )\n        att_reshape = attention.permute(0, 2, 1)\n\n        order_vectors = getattr(extra, ""order_vectors"", None)\n\n        if order_vectors is not None:\n            image_feat_variable = torch.cat(\n                [image_feat_variable, order_vectors], dim=-1\n            )\n        tmp_embedding = torch.bmm(\n            att_reshape, image_feat_variable\n        )  # N x n_att x image_dim\n        batch_size = att_reshape.size(0)\n        image_embedding = tmp_embedding.view(batch_size, -1)\n\n        return image_embedding, attention\n\n\nclass MultiHeadImageFeatureEmbedding(nn.Module):\n    def __init__(self, img_dim, question_dim, **kwargs):\n        super().__init__()\n        self.module = nn.MultiheadAttention(\n            embed_dim=question_dim, kdim=img_dim, vdim=img_dim, **kwargs\n        )\n        self.out_dim = question_dim\n\n    def forward(self, image_feat_variable, question_embedding, image_dims, extra=None):\n        if extra is None:\n            extra = {}\n        image_feat_variable = image_feat_variable.transpose(0, 1)\n        question_embedding = question_embedding.unsqueeze(1).transpose(0, 1)\n        output, weights = self.module(\n            question_embedding, image_feat_variable, image_feat_variable\n        )\n        output = output.transpose(0, 1)\n\n        return output.squeeze(), weights\n\n\nclass ImageFinetune(nn.Module):\n    def __init__(self, in_dim, weights_file, bias_file):\n        super().__init__()\n        with PathManager.open(weights_file, ""rb"") as w:\n            weights = pickle.load(w)\n        with PathManager.open(bias_file, ""rb"") as b:\n            bias = pickle.load(b)\n        out_dim = bias.shape[0]\n\n        self.lc = nn.Linear(in_dim, out_dim)\n        self.lc.weight.data.copy_(torch.from_numpy(weights))\n        self.lc.bias.data.copy_(torch.from_numpy(bias))\n        self.out_dim = out_dim\n\n    def forward(self, image):\n        i2 = self.lc(image)\n        i3 = nn.functional.relu(i2)\n        return i3\n\n\nclass BertVisioLinguisticEmbeddings(BertEmbeddings):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config)\n        self.token_type_embeddings_visual = nn.Embedding(\n            config.type_vocab_size, config.hidden_size\n        )\n        self.position_embeddings_visual = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size\n        )\n\n        self.projection = nn.Linear(config.visual_embedding_dim, config.hidden_size)\n\n    def initialize_visual_from_pretrained(self):\n        self.token_type_embeddings_visual.weight = nn.Parameter(\n            deepcopy(self.token_type_embeddings.weight.data), requires_grad=True\n        )\n        self.position_embeddings_visual.weight = nn.Parameter(\n            deepcopy(self.position_embeddings.weight.data), requires_grad=True\n        )\n\n    def forward(\n        self,\n        input_ids,\n        token_type_ids=None,\n        visual_embeddings=None,\n        visual_embeddings_type=None,\n        position_embeddings_visual=None,\n        image_text_alignment=None,\n    ):\n        """"""\n        input_ids = [batch_size, sequence_length]\n        token_type_ids = [batch_size, sequence_length]\n        visual_embedding = [batch_size, image_feature_length, image_feature_dim]\n        image_text_alignment = [batch_size, image_feature_length, alignment_dim]\n        """"""\n\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(\n            seq_length, dtype=torch.long, device=input_ids.device\n        )\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n\n        if visual_embeddings is not None:\n            visual_embeddings = self.projection(visual_embeddings)\n            token_type_embeddings_visual = self.token_type_embeddings_visual(\n                visual_embeddings_type\n            )\n\n            if image_text_alignment is not None:\n                # image_text_alignment = Batch x image_length x alignment_number.\n                # Each element denotes the position of the word corresponding to the\n                # image feature. -1 is the padding value.\n                image_text_alignment_mask = (image_text_alignment != -1).long()\n                # Get rid of the -1.\n                image_text_alignment = image_text_alignment_mask * image_text_alignment\n\n                # position_embeddings_visual\n                # = Batch x image_length x alignment length x dim\n                position_embeddings_visual = self.position_embeddings(\n                    image_text_alignment\n                ) * image_text_alignment_mask.to(\n                    dtype=next(self.parameters()).dtype\n                ).unsqueeze(\n                    -1\n                )\n                position_embeddings_visual = position_embeddings_visual.sum(2)\n\n                # We want to averge along the alignment_number dimension.\n                image_text_alignment_mask = image_text_alignment_mask.to(\n                    dtype=next(self.parameters()).dtype\n                ).sum(2)\n                image_text_alignment_mask[\n                    image_text_alignment_mask == 0\n                ] = 1  # Avoid devide by zero error\n                position_embeddings_visual = (\n                    position_embeddings_visual / image_text_alignment_mask.unsqueeze(-1)\n                )\n\n                position_ids_visual = torch.zeros(\n                    *visual_embeddings.size()[:-1], dtype=torch.long\n                ).cuda()\n\n                # When fine-tuning the detector , the image_text_alignment is\n                # sometimes padded too long.\n                if position_embeddings_visual.size(1) != visual_embeddings.size(1):\n                    assert position_embeddings_visual.size(1) >= visual_embeddings.size(\n                        1\n                    )\n                    position_embeddings_visual = position_embeddings_visual[\n                        :, : visual_embeddings.size(1), :\n                    ]\n\n                position_embeddings_visual = (\n                    position_embeddings_visual\n                    + self.position_embeddings_visual(position_ids_visual)\n                )\n            else:\n                position_ids_visual = torch.zeros(\n                    *visual_embeddings.size()[:-1], dtype=torch.long\n                ).cuda()\n                position_embeddings_visual = self.position_embeddings_visual(\n                    position_ids_visual\n                )\n\n            v_embeddings = (\n                visual_embeddings\n                + position_embeddings_visual\n                + token_type_embeddings_visual\n            )\n\n            # Concate the two:\n            embeddings = torch.cat(\n                (embeddings, v_embeddings), dim=1\n            )  # concat the visual embeddings after the attentions\n\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n'"
mmf/modules/encoders.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport pickle\n\nimport torch\nimport torchvision\nfrom omegaconf import OmegaConf\nfrom torch import nn\nfrom transformers.configuration_auto import AutoConfig\nfrom transformers.modeling_auto import AutoModel\n\nfrom mmf.modules.embeddings import ProjectionEmbedding, TextEmbedding\nfrom mmf.modules.layers import Identity\nfrom mmf.utils.build import build_image_encoder, build_text_encoder\nfrom mmf.utils.configuration import get_mmf_cache_dir\nfrom mmf.utils.download import download_pretrained_model\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_absolute_path\n\n\nclass ImageFeatureEncoder(nn.Module):\n    def __init__(self, encoder_type, in_dim, **kwargs):\n        super().__init__()\n\n        if encoder_type == ""default"":\n            self.module = Identity()\n            self.module.in_dim = in_dim\n            self.module.out_dim = in_dim\n        elif encoder_type == ""projection"":\n            module_type = kwargs.pop(""module"", ""linear"")\n            self.module = ProjectionEmbedding(module_type, in_dim, **kwargs)\n        elif encoder_type == ""finetune_faster_rcnn_fpn_fc7"":\n            self.module = FinetuneFasterRcnnFpnFc7(in_dim, **kwargs)\n        else:\n            raise NotImplementedError(""Unknown Image Encoder: %s"" % encoder_type)\n\n        self.out_dim = self.module.out_dim\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass FinetuneFasterRcnnFpnFc7(nn.Module):\n    def __init__(\n        self, in_dim, weights_file, bias_file, model_data_dir, *args, **kwargs\n    ):\n        super().__init__()\n        model_data_dir = get_absolute_path(model_data_dir)\n\n        if not os.path.isabs(weights_file):\n            weights_file = os.path.join(model_data_dir, weights_file)\n        if not os.path.isabs(bias_file):\n            bias_file = os.path.join(model_data_dir, bias_file)\n\n        if not PathManager.exists(bias_file) or not PathManager.exists(weights_file):\n            download_path = download_pretrained_model(""detectron.vmb_weights"")\n            weights_file = get_absolute_path(os.path.join(download_path, ""fc7_w.pkl""))\n            bias_file = get_absolute_path(os.path.join(download_path, ""fc7_b.pkl""))\n\n        with PathManager.open(weights_file, ""rb"") as w:\n            weights = pickle.load(w)\n        with PathManager.open(bias_file, ""rb"") as b:\n            bias = pickle.load(b)\n        out_dim = bias.shape[0]\n\n        self.lc = nn.Linear(in_dim, out_dim)\n        self.lc.weight.data.copy_(torch.from_numpy(weights))\n        self.lc.bias.data.copy_(torch.from_numpy(bias))\n        self.out_dim = out_dim\n\n    def forward(self, image):\n        i2 = self.lc(image)\n        i3 = nn.functional.relu(i2)\n        return i3\n\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__()\n        self._type = config.type\n        params = config.params\n\n        if self._type == ""default"":\n            self.module = nn.Identity()\n            self.module.out_dim = params.in_dim\n        elif self._type == ""resnet152"":\n            self.module = ResNet152ImageEncoder(params)\n        else:\n            raise NotImplementedError(""Unknown Image Encoder: %s"" % self._type)\n\n    @property\n    def out_dim(self):\n        return self.module.out_dim\n\n    def forward(self, image):\n        return self.module(image)\n\n\n# Taken from facebookresearch/mmbt with some modifications\nclass ResNet152ImageEncoder(nn.Module):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__()\n        self.config = config\n        model = torchvision.models.resnet152(pretrained=config.get(""pretrained"", True))\n        modules = list(model.children())[:-2]\n        self.model = nn.Sequential(*modules)\n\n        pool_func = (\n            nn.AdaptiveAvgPool2d if config.pool_type == ""avg"" else nn.AdaptiveMaxPool2d\n        )\n\n        # -1 will keep the original feature size\n        if config.num_output_features == -1:\n            self.pool = nn.Identity()\n        elif config.num_output_features in [1, 2, 3, 5, 7]:\n            self.pool = pool_func((config.num_output_features, 1))\n        elif config.num_output_features == 4:\n            self.pool = pool_func((2, 2))\n        elif config.num_output_features == 6:\n            self.pool = pool_func((3, 2))\n        elif config.num_output_features == 8:\n            self.pool = pool_func((4, 2))\n        elif config.num_output_features == 9:\n            self.pool = pool_func((3, 3))\n\n        self.out_dim = 2048\n\n    def forward(self, x):\n        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n        out = self.pool(self.model(x))\n        out = torch.flatten(out, start_dim=2)\n        out = out.transpose(1, 2).contiguous()\n        return out  # BxNx2048\n\n\nclass TextEncoder(nn.Module):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__()\n        self._type = config.type\n\n        if self._type == ""identity"":\n            self.module = nn.Identity()\n        elif self._type == ""transformer"":\n            self._module = TransformerEncoder(config.params)\n            self.module = self._module.module\n        elif self._type == ""embedding"":\n            self.module = TextEmbeddingEncoder(config.params)\n        else:\n            raise NotImplementedError(f""Unknown Text Encoder {self._type}"")\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass TextEmbeddingEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self._operator = config.operator\n        self._embedding_params = config.embedding_params\n\n        self.module = TextEmbedding(\n            self._embedding_params.type, **self._embedding_params.params\n        )\n\n    def forward(self, x):\n        x = self.module(x)\n        if self._operator == ""sum"":\n            x = x.sum(dim=1)\n        elif self._operator == ""concat"":\n            x = torch.cat(x, dim=1)\n        elif self._operator == ""mul"":\n            x = torch.prod(x, dim=1)\n\n        return x.squeeze()\n\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__()\n        self.config = config\n        self.module = AutoModel.from_pretrained(\n            self.config.bert_model_name,\n            config=self._build_encoder_config(config),\n            cache_dir=os.path.join(get_mmf_cache_dir(), ""distributed_{}"".format(-1)),\n        )\n        self.embeddings = self.module.embeddings\n        self.config = self.module.config\n\n    def _build_encoder_config(self, config):\n        return AutoConfig.from_pretrained(\n            self.config.bert_model_name, **OmegaConf.to_container(self.config)\n        )\n\n    def forward(self, *args, **kwargs):\n        # Only return pooled output\n        return self.module(*args, **kwargs)[1]\n\n\nclass MultiModalEncoderBase(nn.Module):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__()\n        self.config = config\n\n        self._modal_encoder_config = self.config.get(""modal_encoder"", None)\n\n        self._is_direct_features_input = self.config.get(""direct_features_input"", False)\n\n        self.build()\n        self.modal_hidden_size = self.config.get(""modal_hidden_size"", None)\n        self.text_hidden_size = self.config.get(""text_hidden_size"", None)\n\n    def build(self):\n        encoders = self._build_encoders(self.config)\n        self.text_encoder, self.modal_encoder = encoders[0], encoders[1]\n\n        self._encoder_config = None\n        if self.text_encoder:\n            self._encoder_config = self.text_encoder.config\n\n    @property\n    def encoder_config(self):\n        return self._encoder_config\n\n    def _build_encoders(self, config):\n        text_encoder = None\n        if config.get(""text_encoder"", None):\n            text_encoder = build_text_encoder(config.text_encoder)\n\n        modal_encoder = None\n        if config.get(""modal_encoder"", None):\n            modal_encoder = self._build_modal_encoder(config.modal_encoder)\n\n        return (text_encoder, modal_encoder)\n\n    def _build_modal_encoder(self, config):\n        return build_image_encoder(\n            config, direct_features=self._is_direct_features_input\n        )\n'"
mmf/modules/fusions.py,32,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n""""""\nThe fusions module contains various Fusion techniques, some based on BLOCK:\nBilinear Superdiagonal Fusion for VQA and VRD. For e.g. LinearSum, ConcatMLP\netc taken from https://github.com/Cadene/block.bootstrap.pytorch#fusions.\n\nFor implementing your own fusion technique, you need to follow these steps:\n\n.. code::\n    from torch import nn\n    from mmf.common.registry import registry\n    from mmf.modules.fusions import Block\n    from mmf.modules.fusions import LinearSum\n    from mmf.modules.fusions import ConcatMLP\n    from mmf.modules.fusions import MLB\n    from mmf.modules.fusions import Mutan\n    from mmf.modules.fusions import Tucker\n    from mmf.modules.fusions import BlockTucker\n    from mmf.modules.fusions import MFH\n    from mmf.modules.fusions import MFB\n    from mmf.modules.fusions import MCB\n\n    @regitery.register_fusion(""custom"")\n    class CustomFusion(nn.Module):\n        def __init__(self, params=None):\n            super().__init__(""Custom"")\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.general import get_chunks, get_sizes_list\n\n\nclass CompactBilinearPooling(nn.Module):\n    def __init__(self, input_dim1, input_dim2, output_dim, sum_pool=True):\n        super().__init__()\n        self.output_dim = output_dim\n        self.sum_pool = sum_pool\n        self.sketch1 = nn.Parameter(\n            self.generate_sketch_matrix(\n                torch.randint(output_dim, size=(input_dim1,)),\n                2 * torch.randint(2, size=(input_dim1,)) - 1,\n                input_dim1,\n                output_dim,\n            ),\n            requires_grad=False,\n        )\n        self.sketch2 = nn.Parameter(\n            self.generate_sketch_matrix(\n                torch.randint(output_dim, size=(input_dim2,)),\n                2 * torch.randint(2, size=(input_dim2,)) - 1,\n                input_dim2,\n                output_dim,\n            ),\n            requires_grad=False,\n        )\n\n    def generate_sketch_matrix(self, rand_h, rand_s, input_dim, output_dim):\n        return torch.sparse.FloatTensor(\n            torch.stack(\n                [torch.arange(input_dim, out=torch.LongTensor()), rand_h.long()]\n            ),\n            rand_s.float(),\n            [input_dim, output_dim],\n        ).to_dense()\n\n    def forward(self, x1, x2):\n        assert len(x1.shape) == len(x2.shape)\n        if len(x1.shape) == 4 and len(x2.shape) == 4:\n            fft1 = torch.rfft(\n                x1.permute(0, 2, 3, 1).matmul(self.sketch1), signal_ndim=1\n            )\n            fft2 = torch.rfft(\n                x2.permute(0, 2, 3, 1).matmul(self.sketch2), signal_ndim=1\n            )\n        else:\n            fft1 = torch.rfft(x1.matmul(self.sketch1), signal_ndim=1)\n            fft2 = torch.rfft(x2.matmul(self.sketch2), signal_ndim=1)\n        fft_product = torch.stack(\n            [\n                fft1[..., 0] * fft2[..., 0] - fft1[..., 1] * fft2[..., 1],\n                fft1[..., 0] * fft2[..., 1] + fft1[..., 1] * fft2[..., 0],\n            ],\n            dim=-1,\n        )\n        cbp = (\n            torch.irfft(fft_product, signal_ndim=1, signal_sizes=(self.output_dim,))\n            * self.output_dim\n        )\n        if len(x1.shape) == 4 and len(x2.shape) == 4:\n            cbp = cbp.sum(dim=[1, 2]) if self.sum_pool else cbp.permute(0, 3, 1, 2)\n        return cbp\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, dimensions, activation=""relu"", dropout=0.0):\n        super().__init__()\n        self.input_dim = input_dim\n        self.dimensions = dimensions\n        self.activation = activation\n        self.dropout = dropout\n        self.linears = nn.ModuleList([nn.Linear(input_dim, dimensions[0])])\n        for din, dout in zip(dimensions[:-1], dimensions[1:]):\n            self.linears.append(nn.Linear(din, dout))\n\n    def forward(self, x):\n        for i, lin in enumerate(self.linears):\n            x = lin(x)\n            if i < len(self.linears) - 1:\n                x = F.__dict__[self.activation](x)\n                if self.dropout > 0:\n                    x = F.dropout(x, self.dropout, training=self.training)\n        return x\n\n\n@registry.register_fusion(""block"")\nclass Block(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1600,\n        chunks=20,\n        rank=15,\n        shared=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n        pos_norm=""before_cat"",\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n        self.mm_dim = mm_dim\n        self.chunks = chunks\n        self.rank = rank\n        self.shared = shared\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        assert pos_norm in [""before_cat"", ""after_cat""]\n        self.pos_norm = pos_norm\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim)\n        if shared:\n            self.linear1 = self.linear0\n        else:\n            self.linear1 = nn.Linear(input_dims[1], mm_dim)\n        merge_linears0, merge_linears1 = [], []\n        self.sizes_list = get_sizes_list(mm_dim, chunks)\n        for size in self.sizes_list:\n            ml0 = nn.Linear(size, size * rank)\n            merge_linears0.append(ml0)\n            if self.shared:\n                ml1 = ml0\n            else:\n                ml1 = nn.Linear(size, size * rank)\n            merge_linears1.append(ml1)\n        self.merge_linears0 = nn.ModuleList(merge_linears0)\n        self.merge_linears1 = nn.ModuleList(merge_linears1)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n        bsize = x1.size(0)\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n        x0_chunks = get_chunks(x0, self.sizes_list)\n        x1_chunks = get_chunks(x1, self.sizes_list)\n        zs = []\n        for chunk_id, m0, m1 in zip(\n            range(len(self.sizes_list)), self.merge_linears0, self.merge_linears1\n        ):\n            x0_c = x0_chunks[chunk_id]\n            x1_c = x1_chunks[chunk_id]\n            m = m0(x0_c) * m1(x1_c)  # bsize x split_size*rank\n            m = m.view(bsize, self.rank, -1)\n            z = torch.sum(m, 1)\n            if self.pos_norm == ""before_cat"":\n                z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n                z = F.normalize(z, p=2)\n            zs.append(z)\n        z = torch.cat(zs, 1)\n        if self.pos_norm == ""after_cat"":\n            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n            z = F.normalize(z, p=2)\n\n        if self.dropout_pre_lin > 0:\n            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)\n        z = self.linear_out(z)\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""block_tucker"")\nclass BlockTucker(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1600,\n        chunks=20,\n        shared=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n        pos_norm=""before_cat"",\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n        self.mm_dim = mm_dim\n        self.chunks = chunks\n        self.shared = shared\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        assert pos_norm in [""before_cat"", ""after_cat""]\n        self.pos_norm = pos_norm\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim)\n        if self.shared:\n            self.linear1 = self.linear0\n        else:\n            self.linear1 = nn.Linear(input_dims[1], mm_dim)\n\n        self.sizes_list = get_sizes_list(mm_dim, chunks)\n        bilinears = []\n        for size in self.sizes_list:\n            bilinears.append(nn.Bilinear(size, size, size))\n        self.bilinears = nn.ModuleList(bilinears)\n        self.linear_out = nn.Linear(self.mm_dim, self.output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n        if self.dropout_input:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n        x0_chunks = get_chunks(x0, self.sizes_list)\n        x1_chunks = get_chunks(x1, self.sizes_list)\n        zs = []\n        for chunk_id, bilinear in enumerate(self.bilinears):\n            x0_c = x0_chunks[chunk_id]\n            x1_c = x1_chunks[chunk_id]\n            z = bilinear(x0_c, x1_c)\n            if self.pos_norm == ""before_cat"":\n                z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n                z = F.normalize(z, p=2)\n            zs.append(z)\n        z = torch.cat(zs, 1)\n        if self.pos_norm == ""after_cat"":\n            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n            z = F.normalize(z, p=2)\n\n        if self.dropout_pre_lin > 0:\n            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)\n        z = self.linear_out(z)\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""mutan"")\nclass Mutan(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1600,\n        rank=15,\n        shared=False,\n        normalize=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.shared = shared\n        self.mm_dim = mm_dim\n        self.rank = rank\n        self.output_dim = output_dim\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        self.normalize = normalize\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim)\n        self.merge_linear0 = nn.Linear(mm_dim, mm_dim * rank)\n        if self.shared:\n            self.linear1 = self.linear0\n            self.merge_linear1 = self.merge_linear0\n        else:\n            self.linear1 = nn.Linear(input_dims[1], mm_dim)\n            self.merge_linear1 = nn.Linear(mm_dim, mm_dim * rank)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n\n        m0 = self.merge_linear0(x0)\n        m1 = self.merge_linear1(x1)\n        m = m0 * m1\n        m = m.view(-1, self.rank, self.mm_dim)\n        z = torch.sum(m, 1)\n        if self.normalize:\n            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n            z = F.normalize(z, p=2)\n\n        if self.dropout_pre_lin > 0:\n            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)\n\n        z = self.linear_out(z)\n\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""tucker"")\nclass Tucker(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1600,\n        shared=False,\n        normalize=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.shared = shared\n        self.mm_dim = mm_dim\n        self.output_dim = output_dim\n        self.normalize = normalize\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim)\n        if shared:\n            self.linear1 = self.linear0\n        else:\n            self.linear1 = nn.Linear(input_dims[1], mm_dim)\n        self.linear1 = nn.Linear(input_dims[1], mm_dim)\n        self.bilinear = nn.Bilinear(mm_dim, mm_dim, mm_dim)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n\n        z = self.bilinear(x0, x1)\n\n        if self.normalize:\n            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n            z = F.normalize(z, p=2)\n\n        if self.dropout_pre_lin > 0:\n            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)\n\n        z = self.linear_out(z)\n\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""mlb"")\nclass MLB(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1200,\n        activ_input=""relu"",\n        activ_output=""relu"",\n        normalize=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.mm_dim = mm_dim\n        self.output_dim = output_dim\n        self.activ_input = activ_input\n        self.activ_output = activ_output\n        self.normalize = normalize\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim)\n        self.linear1 = nn.Linear(input_dims[1], mm_dim)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n\n        if self.activ_input:\n            x0 = getattr(F, self.activ_input)(x0)\n            x1 = getattr(F, self.activ_input)(x1)\n\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n\n        z = x0 * x1\n\n        if self.normalize:\n            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n            z = F.normalize(z, p=2)\n\n        if self.dropout_pre_lin > 0:\n            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)\n\n        z = self.linear_out(z)\n\n        if self.activ_output:\n            z = getattr(F, self.activ_output)(z)\n\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""mfb"")\nclass MFB(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1200,\n        factor=2,\n        activ_input=""relu"",\n        activ_output=""relu"",\n        normalize=False,\n        dropout_input=0.0,\n        dropout_pre_norm=0.0,\n        dropout_output=0.0,\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.mm_dim = mm_dim\n        self.factor = factor\n        self.output_dim = output_dim\n        self.activ_input = activ_input\n        self.activ_output = activ_output\n        self.normalize = normalize\n        self.dropout_input = dropout_input\n        self.dropout_pre_norm = dropout_pre_norm\n        self.dropout_output = dropout_output\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim * factor)\n        self.linear1 = nn.Linear(input_dims[1], mm_dim * factor)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n\n        if self.activ_input:\n            x0 = getattr(F, self.activ_input)(x0)\n            x1 = getattr(F, self.activ_input)(x1)\n\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n\n        z = x0 * x1\n\n        if self.dropout_pre_norm > 0:\n            z = F.dropout(z, p=self.dropout_pre_norm, training=self.training)\n\n        z = z.view(z.size(0), self.mm_dim, self.factor)\n        z = z.sum(2)\n\n        if self.normalize:\n            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n            z = F.normalize(z, p=2)\n\n        z = self.linear_out(z)\n\n        if self.activ_output:\n            z = getattr(F, self.activ_output)(z)\n\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""mfh"")\nclass MFH(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1200,\n        factor=2,\n        activ_input=""relu"",\n        activ_output=""relu"",\n        normalize=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n        self.mm_dim = mm_dim\n        self.factor = factor\n        self.activ_input = activ_input\n        self.activ_output = activ_output\n        self.normalize = normalize\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        # Modules\n        self.linear0_0 = nn.Linear(input_dims[0], mm_dim * factor)\n        self.linear1_0 = nn.Linear(input_dims[1], mm_dim * factor)\n        self.linear0_1 = nn.Linear(input_dims[0], mm_dim * factor)\n        self.linear1_1 = nn.Linear(input_dims[1], mm_dim * factor)\n        self.linear_out = nn.Linear(mm_dim * 2, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0_0(x[0])\n        x1 = self.linear1_0(x[1])\n\n        if self.activ_input:\n            x0 = getattr(F, self.activ_input)(x0)\n            x1 = getattr(F, self.activ_input)(x1)\n\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n\n        z_0_skip = x0 * x1\n\n        if self.dropout_pre_lin:\n            z_0_skip = F.dropout(\n                z_0_skip, p=self.dropout_pre_lin, training=self.training\n            )\n\n        z_0 = z_0_skip.view(z_0_skip.size(0), self.mm_dim, self.factor)\n        z_0 = z_0.sum(2)\n\n        if self.normalize:\n            z_0 = torch.sqrt(F.relu(z_0)) - torch.sqrt(F.relu(-z_0))\n            z_0 = F.normalize(z_0, p=2)\n\n        #\n        x0 = self.linear0_1(x[0])\n        x1 = self.linear1_1(x[1])\n\n        if self.activ_input:\n            x0 = getattr(F, self.activ_input)(x0)\n            x1 = getattr(F, self.activ_input)(x1)\n\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n\n        z_1 = x0 * x1 * z_0_skip\n\n        if self.dropout_pre_lin > 0:\n            z_1 = F.dropout(z_1, p=self.dropout_pre_lin, training=self.training)\n\n        z_1 = z_1.view(z_1.size(0), self.mm_dim, self.factor)\n        z_1 = z_1.sum(2)\n\n        if self.normalize:\n            z_1 = torch.sqrt(F.relu(z_1)) - torch.sqrt(F.relu(-z_1))\n            z_1 = F.normalize(z_1, p=2)\n\n        #\n        cat_dim = z_0.dim() - 1\n        z = torch.cat([z_0, z_1], cat_dim)\n        z = self.linear_out(z)\n\n        if self.activ_output:\n            z = getattr(F, self.activ_output)(z)\n\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""mcb"")\nclass MCB(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=16000,\n        activ_output=""relu"",\n        dropout_output=0.0,\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n        self.mm_dim = mm_dim\n        self.activ_output = activ_output\n        self.dropout_output = dropout_output\n        # Modules\n        self.mcb = CompactBilinearPooling(input_dims[0], input_dims[1], mm_dim)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        z = self.mcb(x[0], x[1])\n        z = self.linear_out(z)\n        if self.activ_output:\n            z = getattr(F, self.activ_output)(z)\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""linear_sum"")\nclass LinearSum(nn.Module):\n    def __init__(\n        self,\n        input_dims,\n        output_dim,\n        mm_dim=1200,\n        activ_input=""relu"",\n        activ_output=""relu"",\n        normalize=False,\n        dropout_input=0.0,\n        dropout_pre_lin=0.0,\n        dropout_output=0.0,\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n        self.mm_dim = mm_dim\n        self.activ_input = activ_input\n        self.activ_output = activ_output\n        self.normalize = normalize\n        self.dropout_input = dropout_input\n        self.dropout_pre_lin = dropout_pre_lin\n        self.dropout_output = dropout_output\n        # Modules\n        self.linear0 = nn.Linear(input_dims[0], mm_dim)\n        self.linear1 = nn.Linear(input_dims[1], mm_dim)\n        self.linear_out = nn.Linear(mm_dim, output_dim)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        x0 = self.linear0(x[0])\n        x1 = self.linear1(x[1])\n\n        if self.activ_input:\n            x0 = getattr(F, self.activ_input)(x0)\n            x1 = getattr(F, self.activ_input)(x1)\n\n        if self.dropout_input > 0:\n            x0 = F.dropout(x0, p=self.dropout_input, training=self.training)\n            x1 = F.dropout(x1, p=self.dropout_input, training=self.training)\n\n        z = x0 + x1\n\n        if self.normalize:\n            z = torch.sqrt(F.relu(z)) - torch.sqrt(F.relu(-z))\n            z = F.normalize(z, p=2)\n\n        if self.dropout_pre_lin > 0:\n            z = F.dropout(z, p=self.dropout_pre_lin, training=self.training)\n\n        z = self.linear_out(z)\n\n        if self.activ_output:\n            z = getattr(F, self.activ_output)(z)\n\n        if self.dropout_output > 0:\n            z = F.dropout(z, p=self.dropout_output, training=self.training)\n        return z\n\n\n@registry.register_fusion(""concat_mlp"")\nclass ConcatMLP(nn.Module):\n    def __init__(\n        self, input_dims, output_dim, dimensions=None, activation=""relu"", dropout=0.0\n    ):\n        super().__init__()\n        self.input_dims = input_dims\n        self.output_dim = output_dim\n        self.input_dim = sum(input_dims)\n        if dimensions is None:\n            dimensions = [500, 500]\n        self.dimensions = dimensions + [output_dim]\n        self.activation = activation\n        self.dropout = dropout\n        # Modules\n        self.mlp = MLP(self.input_dim, self.dimensions, self.activation, self.dropout)\n        self.n_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n\n    def forward(self, x):\n        if x[0].dim() == 3 and x[1].dim() == 2:\n            x[1] = x[1].unsqueeze(1).reshape_as(x[0])\n        if x[1].dim() == 3 and x[0].dim() == 2:\n            x[0] = x[0].unsqueeze(1).reshape_as(x[1])\n        z = torch.cat(x, dim=x[0].dim() - 1)\n        z = self.mlp(z)\n        return z\n'"
mmf/modules/layers.py,24,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\nfrom torch import nn\nfrom torch.nn.utils.weight_norm import weight_norm\n\nfrom mmf.common.registry import registry\nfrom mmf.modules.decoders import LanguageDecoder\n\n\nclass ConvNet(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding_size=""same"",\n        pool_stride=2,\n        batch_norm=True,\n    ):\n        super().__init__()\n\n        if padding_size == ""same"":\n            padding_size = kernel_size // 2\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, kernel_size, padding=padding_size\n        )\n        self.max_pool2d = nn.MaxPool2d(pool_stride, stride=pool_stride)\n        self.batch_norm = batch_norm\n\n        if self.batch_norm:\n            self.batch_norm_2d = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        x = self.max_pool2d(nn.functional.leaky_relu(self.conv(x)))\n\n        if self.batch_norm:\n            x = self.batch_norm_2d(x)\n\n        return x\n\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        if input.dim() > 1:\n            input = input.view(input.size(0), -1)\n\n        return input\n\n\nclass UnFlatten(nn.Module):\n    def forward(self, input, sizes=None):\n        if sizes is None:\n            sizes = []\n        return input.view(input.size(0), *sizes)\n\n\nclass GatedTanh(nn.Module):\n    """"""\n    From: https://arxiv.org/pdf/1707.07998.pdf\n    nonlinear_layer (f_a) : x\\\\in R^m => y \\\\in R^n\n    \\tilda{y} = tanh(Wx + b)\n    g = sigmoid(W\'x + b\')\n    y = \\tilda(y) \\\\circ g\n    input: (N, *, in_dim)\n    output: (N, *, out_dim)\n    """"""\n\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.fc = nn.Linear(in_dim, out_dim)\n        self.gate_fc = nn.Linear(in_dim, out_dim)\n\n    def forward(self, x):\n        y_tilda = torch.tanh(self.fc(x))\n        gated = torch.sigmoid(self.gate_fc(x))\n\n        # Element wise multiplication\n        y = y_tilda * gated\n\n        return y\n\n\n# TODO: Do clean implementation without Sequential\nclass ReLUWithWeightNormFC(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n\n        layers = []\n        layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))\n        layers.append(nn.ReLU())\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\n\nclass ClassifierLayer(nn.Module):\n    def __init__(self, classifier_type, in_dim, out_dim, **kwargs):\n        super().__init__()\n\n        if classifier_type == ""weight_norm"":\n            self.module = WeightNormClassifier(in_dim, out_dim, **kwargs)\n        elif classifier_type == ""logit"":\n            self.module = LogitClassifier(in_dim, out_dim, **kwargs)\n        elif classifier_type == ""language_decoder"":\n            self.module = LanguageDecoder(in_dim, out_dim, **kwargs)\n        elif classifier_type == ""bert"":\n            self.module = BertClassifierHead(\n                in_dim, out_dim, kwargs.get(""config"", None)\n            ).module\n        elif classifier_type == ""mlp"":\n            self.module = MLPClassifer(in_dim, out_dim, **kwargs)\n        elif classifier_type == ""linear"":\n            self.module = nn.Linear(in_dim, out_dim)\n        else:\n            raise NotImplementedError(""Unknown classifier type: %s"" % classifier_type)\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass BertClassifierHead(nn.Module):\n    def __init__(self, in_dim=768, out_dim=2, config=None, *args, **kwargs):\n        super().__init__()\n        from transformers.modeling_bert import BertPredictionHeadTransform\n\n        if config is None:\n            from transformers.configuration_bert import BertConfig\n\n            config = BertConfig.from_pretrained(""bert-base-uncased"")\n\n        assert config.hidden_size == in_dim\n\n        self.module = nn.Sequential(\n            nn.Dropout(config.hidden_dropout_prob),\n            BertPredictionHeadTransform(config),\n            nn.Linear(in_dim, out_dim),\n        )\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass MLPClassifer(nn.Module):\n    def __init__(\n        self,\n        in_dim,\n        out_dim,\n        hidden_dim=None,\n        num_layers=0,\n        dropout=0.5,\n        hidden_act=""relu"",\n        batch_norm=True,\n        **kwargs,\n    ):\n        super().__init__()\n        from mmf.utils.modeling import ACT2FN\n\n        activation = ACT2FN[hidden_act]\n        self.layers = nn.ModuleList()\n\n        if hidden_dim is None:\n            hidden_dim = in_dim\n\n        for _ in range(num_layers):\n            self.layers.append(nn.Linear(in_dim, hidden_dim))\n            if batch_norm:\n                self.layers.append(nn.BatchNorm1d(hidden_dim))\n            self.layers.append(activation())\n            self.layers.append(nn.Dropout(dropout))\n            in_dim = hidden_dim\n\n        self.layers.append(nn.Linear(in_dim, out_dim))\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return x\n\n\nclass LogitClassifier(nn.Module):\n    def __init__(self, in_dim, out_dim, **kwargs):\n        super().__init__()\n        input_dim = in_dim\n        num_ans_candidates = out_dim\n        text_non_linear_dim = kwargs[""text_hidden_dim""]\n        image_non_linear_dim = kwargs[""img_hidden_dim""]\n\n        self.f_o_text = ReLUWithWeightNormFC(input_dim, text_non_linear_dim)\n        self.f_o_image = ReLUWithWeightNormFC(input_dim, image_non_linear_dim)\n        self.linear_text = nn.Linear(text_non_linear_dim, num_ans_candidates)\n        self.linear_image = nn.Linear(image_non_linear_dim, num_ans_candidates)\n\n        if ""pretrained_image"" in kwargs and kwargs[""pretrained_text""] is not None:\n            self.linear_text.weight.data.copy_(\n                torch.from_numpy(kwargs[""pretrained_text""])\n            )\n\n        if ""pretrained_image"" in kwargs and kwargs[""pretrained_image""] is not None:\n            self.linear_image.weight.data.copy_(\n                torch.from_numpy(kwargs[""pretrained_image""])\n            )\n\n    def forward(self, joint_embedding):\n        text_val = self.linear_text(self.f_o_text(joint_embedding))\n        image_val = self.linear_image(self.f_o_image(joint_embedding))\n        logit_value = text_val + image_val\n\n        return logit_value\n\n\nclass WeightNormClassifier(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden_dim, dropout):\n        super().__init__()\n        layers = [\n            weight_norm(nn.Linear(in_dim, hidden_dim), dim=None),\n            nn.ReLU(),\n            nn.Dropout(dropout, inplace=True),\n            weight_norm(nn.Linear(hidden_dim, out_dim), dim=None),\n        ]\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x):\n        logits = self.main(x)\n        return logits\n\n\nclass Identity(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n\n    def forward(self, x):\n        return x\n\n\nclass ModalCombineLayer(nn.Module):\n    def __init__(self, combine_type, img_feat_dim, txt_emb_dim, **kwargs):\n        super().__init__()\n        if combine_type == ""MFH"":\n            self.module = MFH(img_feat_dim, txt_emb_dim, **kwargs)\n        elif combine_type == ""non_linear_element_multiply"":\n            self.module = NonLinearElementMultiply(img_feat_dim, txt_emb_dim, **kwargs)\n        elif combine_type == ""two_layer_element_multiply"":\n            self.module = TwoLayerElementMultiply(img_feat_dim, txt_emb_dim, **kwargs)\n        elif combine_type == ""top_down_attention_lstm"":\n            self.module = TopDownAttentionLSTM(img_feat_dim, txt_emb_dim, **kwargs)\n        else:\n            raise NotImplementedError(""Not implemented combine type: %s"" % combine_type)\n\n        self.out_dim = self.module.out_dim\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass MfbExpand(nn.Module):\n    def __init__(self, img_feat_dim, txt_emb_dim, hidden_dim, dropout):\n        super().__init__()\n        self.lc_image = nn.Linear(in_features=img_feat_dim, out_features=hidden_dim)\n        self.lc_ques = nn.Linear(in_features=txt_emb_dim, out_features=hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, image_feat, question_embed):\n        image1 = self.lc_image(image_feat)\n        ques1 = self.lc_ques(question_embed)\n        if len(image_feat.data.shape) == 3:\n            num_location = image_feat.data.size(1)\n            ques1_expand = torch.unsqueeze(ques1, 1).expand(-1, num_location, -1)\n        else:\n            ques1_expand = ques1\n        joint_feature = image1 * ques1_expand\n        joint_feature = self.dropout(joint_feature)\n        return joint_feature\n\n\nclass MFH(nn.Module):\n    def __init__(self, image_feat_dim, ques_emb_dim, **kwargs):\n        super().__init__()\n        self.mfb_expand_list = nn.ModuleList()\n        self.mfb_sqz_list = nn.ModuleList()\n        self.relu = nn.ReLU()\n\n        hidden_sizes = kwargs[""hidden_sizes""]\n        self.out_dim = int(sum(hidden_sizes) / kwargs[""pool_size""])\n\n        self.order = kwargs[""order""]\n        self.pool_size = kwargs[""pool_size""]\n\n        for i in range(self.order):\n            mfb_exp_i = MfbExpand(\n                img_feat_dim=image_feat_dim,\n                txt_emb_dim=ques_emb_dim,\n                hidden_dim=hidden_sizes[i],\n                dropout=kwargs[""dropout""],\n            )\n            self.mfb_expand_list.append(mfb_exp_i)\n            self.mfb_sqz_list.append(self.mfb_squeeze)\n\n    def forward(self, image_feat, question_embedding):\n        feature_list = []\n        prev_mfb_exp = 1\n\n        for i in range(self.order):\n            mfb_exp = self.mfb_expand_list[i]\n            mfb_sqz = self.mfb_sqz_list[i]\n            z_exp_i = mfb_exp(image_feat, question_embedding)\n            if i > 0:\n                z_exp_i = prev_mfb_exp * z_exp_i\n            prev_mfb_exp = z_exp_i\n            z = mfb_sqz(z_exp_i)\n            feature_list.append(z)\n\n        # append at last feature\n        cat_dim = len(feature_list[0].size()) - 1\n        feature = torch.cat(feature_list, dim=cat_dim)\n        return feature\n\n    def mfb_squeeze(self, joint_feature):\n        # joint_feature dim: N x k x dim or N x dim\n\n        orig_feature_size = len(joint_feature.size())\n\n        if orig_feature_size == 2:\n            joint_feature = torch.unsqueeze(joint_feature, dim=1)\n\n        batch_size, num_loc, dim = joint_feature.size()\n\n        if dim % self.pool_size != 0:\n            exit(\n                ""the dim %d is not multiply of \\\n             pool_size %d""\n                % (dim, self.pool_size)\n            )\n\n        joint_feature_reshape = joint_feature.view(\n            batch_size, num_loc, int(dim / self.pool_size), self.pool_size\n        )\n\n        # N x 100 x 1000 x 1\n        iatt_iq_sumpool = torch.sum(joint_feature_reshape, 3)\n\n        iatt_iq_sqrt = torch.sqrt(self.relu(iatt_iq_sumpool)) - torch.sqrt(\n            self.relu(-iatt_iq_sumpool)\n        )\n\n        iatt_iq_sqrt = iatt_iq_sqrt.view(batch_size, -1)  # N x 100000\n        iatt_iq_l2 = nn.functional.normalize(iatt_iq_sqrt)\n        iatt_iq_l2 = iatt_iq_l2.view(batch_size, num_loc, int(dim / self.pool_size))\n\n        if orig_feature_size == 2:\n            iatt_iq_l2 = torch.squeeze(iatt_iq_l2, dim=1)\n\n        return iatt_iq_l2\n\n\n# need to handle two situations,\n# first: image (N, K, i_dim), question (N, q_dim);\n# second: image (N, i_dim), question (N, q_dim);\nclass NonLinearElementMultiply(nn.Module):\n    def __init__(self, image_feat_dim, ques_emb_dim, **kwargs):\n        super().__init__()\n        self.fa_image = ReLUWithWeightNormFC(image_feat_dim, kwargs[""hidden_dim""])\n        self.fa_txt = ReLUWithWeightNormFC(ques_emb_dim, kwargs[""hidden_dim""])\n\n        context_dim = kwargs.get(""context_dim"", None)\n        if context_dim is not None:\n            self.fa_context = ReLUWithWeightNormFC(context_dim, kwargs[""hidden_dim""])\n\n        self.dropout = nn.Dropout(kwargs[""dropout""])\n        self.out_dim = kwargs[""hidden_dim""]\n\n    def forward(self, image_feat, question_embedding, context_embedding=None):\n        image_fa = self.fa_image(image_feat)\n        question_fa = self.fa_txt(question_embedding)\n\n        if len(image_feat.size()) == 3 and len(question_fa.size()) != 3:\n            question_fa_expand = question_fa.unsqueeze(1)\n        else:\n            question_fa_expand = question_fa\n\n        joint_feature = image_fa * question_fa_expand\n\n        if context_embedding is not None:\n            context_fa = self.fa_context(context_embedding)\n\n            context_text_joint_feaure = context_fa * question_fa_expand\n            joint_feature = torch.cat([joint_feature, context_text_joint_feaure], dim=1)\n\n        joint_feature = self.dropout(joint_feature)\n\n        return joint_feature\n\n\nclass TopDownAttentionLSTM(nn.Module):\n    def __init__(self, image_feat_dim, embed_dim, **kwargs):\n        super().__init__()\n        self.fa_image = weight_norm(nn.Linear(image_feat_dim, kwargs[""attention_dim""]))\n        self.fa_hidden = weight_norm(\n            nn.Linear(kwargs[""hidden_dim""], kwargs[""attention_dim""])\n        )\n        self.top_down_lstm = nn.LSTMCell(\n            embed_dim + image_feat_dim + kwargs[""hidden_dim""],\n            kwargs[""hidden_dim""],\n            bias=True,\n        )\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(kwargs[""dropout""])\n        self.out_dim = kwargs[""attention_dim""]\n\n    def forward(self, image_feat, embedding):\n        image_feat_mean = image_feat.mean(1)\n\n        # Get LSTM state\n        state = registry.get(f""{image_feat.device}_lstm_state"")\n        h1, c1 = state[""td_hidden""]\n        h2, c2 = state[""lm_hidden""]\n\n        h1, c1 = self.top_down_lstm(\n            torch.cat([h2, image_feat_mean, embedding], dim=1), (h1, c1)\n        )\n\n        state[""td_hidden""] = (h1, c1)\n\n        image_fa = self.fa_image(image_feat)\n        hidden_fa = self.fa_hidden(h1)\n\n        joint_feature = self.relu(image_fa + hidden_fa.unsqueeze(1))\n        joint_feature = self.dropout(joint_feature)\n\n        return joint_feature\n\n\nclass TwoLayerElementMultiply(nn.Module):\n    def __init__(self, image_feat_dim, ques_emb_dim, **kwargs):\n        super().__init__()\n\n        self.fa_image1 = ReLUWithWeightNormFC(image_feat_dim, kwargs[""hidden_dim""])\n        self.fa_image2 = ReLUWithWeightNormFC(\n            kwargs[""hidden_dim""], kwargs[""hidden_dim""]\n        )\n        self.fa_txt1 = ReLUWithWeightNormFC(ques_emb_dim, kwargs[""hidden_dim""])\n        self.fa_txt2 = ReLUWithWeightNormFC(kwargs[""hidden_dim""], kwargs[""hidden_dim""])\n\n        self.dropout = nn.Dropout(kwargs[""dropout""])\n\n        self.out_dim = kwargs[""hidden_dim""]\n\n    def forward(self, image_feat, question_embedding):\n        image_fa = self.fa_image2(self.fa_image1(image_feat))\n        question_fa = self.fa_txt2(self.fa_txt1(question_embedding))\n\n        if len(image_feat.size()) == 3:\n            num_location = image_feat.size(1)\n            question_fa_expand = torch.unsqueeze(question_fa, 1).expand(\n                -1, num_location, -1\n            )\n        else:\n            question_fa_expand = question_fa\n\n        joint_feature = image_fa * question_fa_expand\n        joint_feature = self.dropout(joint_feature)\n\n        return joint_feature\n\n\nclass TransformLayer(nn.Module):\n    def __init__(self, transform_type, in_dim, out_dim, hidden_dim=None):\n        super().__init__()\n\n        if transform_type == ""linear"":\n            self.module = LinearTransform(in_dim, out_dim)\n        elif transform_type == ""conv"":\n            self.module = ConvTransform(in_dim, out_dim, hidden_dim)\n        else:\n            raise NotImplementedError(\n                ""Unknown post combine transform type: %s"" % transform_type\n            )\n        self.out_dim = self.module.out_dim\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n\nclass LinearTransform(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.lc = weight_norm(\n            nn.Linear(in_features=in_dim, out_features=out_dim), dim=None\n        )\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        return self.lc(x)\n\n\nclass ConvTransform(nn.Module):\n    def __init__(self, in_dim, out_dim, hidden_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels=in_dim, out_channels=hidden_dim, kernel_size=1\n        )\n        self.conv2 = nn.Conv2d(\n            in_channels=hidden_dim, out_channels=out_dim, kernel_size=1\n        )\n        self.out_dim = out_dim\n\n    def forward(self, x):\n        if len(x.size()) == 3:  # N x k xdim\n            # N x dim x k x 1\n            x_reshape = torch.unsqueeze(x.permute(0, 2, 1), 3)\n        elif len(x.size()) == 2:  # N x dim\n            # N x dim x 1 x 1\n            x_reshape = torch.unsqueeze(torch.unsqueeze(x, 2), 3)\n\n        iatt_conv1 = self.conv1(x_reshape)  # N x hidden_dim x * x 1\n        iatt_relu = nn.functional.relu(iatt_conv1)\n        iatt_conv2 = self.conv2(iatt_relu)  # N x out_dim x * x 1\n\n        if len(x.size()) == 3:\n            iatt_conv3 = torch.squeeze(iatt_conv2, 3).permute(0, 2, 1)\n        elif len(x.size()) == 2:\n            iatt_conv3 = torch.squeeze(torch.squeeze(iatt_conv2, 3), 2)\n\n        return iatt_conv3\n\n\nclass BCNet(nn.Module):\n    """"""\n    Simple class for non-linear bilinear connect network\n    """"""\n\n    def __init__(self, v_dim, q_dim, h_dim, h_out, act=""ReLU"", dropout=None, k=3):\n        super().__init__()\n\n        self.c = 32\n        self.k = k\n        self.v_dim = v_dim\n        self.q_dim = q_dim\n        self.h_dim = h_dim\n        self.h_out = h_out\n        if dropout is None:\n            dropout = [0.2, 0.5]\n\n        self.v_net = FCNet([v_dim, h_dim * self.k], act=act, dropout=dropout[0])\n        self.q_net = FCNet([q_dim, h_dim * self.k], act=act, dropout=dropout[0])\n        self.dropout = nn.Dropout(dropout[1])\n\n        if k > 1:\n            self.p_net = nn.AvgPool1d(self.k, stride=self.k)\n\n        if h_out is None:\n            pass\n\n        elif h_out <= self.c:\n            self.h_mat = nn.Parameter(\n                torch.Tensor(1, h_out, 1, h_dim * self.k).normal_()\n            )\n            self.h_bias = nn.Parameter(torch.Tensor(1, h_out, 1, 1).normal_())\n        else:\n            self.h_net = weight_norm(nn.Linear(h_dim * self.k, h_out), dim=None)\n\n    def forward(self, v, q):\n        if self.h_out is None:\n            v_ = self.v_net(v).transpose(1, 2).unsqueeze(3)\n            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)\n            d_ = torch.matmul(v_, q_)\n            logits = d_.transpose(1, 2).transpose(2, 3)\n            return logits\n\n        # broadcast Hadamard product, matrix-matrix production\n        # fast computation but memory inefficient\n        elif self.h_out <= self.c:\n            v_ = self.dropout(self.v_net(v)).unsqueeze(1)\n            q_ = self.q_net(q)\n            h_ = v_ * self.h_mat\n            logits = torch.matmul(h_, q_.unsqueeze(1).transpose(2, 3))\n            logits = logits + self.h_bias\n            return logits\n\n        # batch outer product, linear projection\n        # memory efficient but slow computation\n        else:\n            v_ = self.dropout(self.v_net(v)).transpose(1, 2).unsqueeze(3)\n            q_ = self.q_net(q).transpose(1, 2).unsqueeze(2)\n            d_ = torch.matmul(v_, q_)\n            logits = self.h_net(d_.transpose(1, 2).transpose(2, 3))\n            return logits.transpose(2, 3).transpose(1, 2)\n\n    def forward_with_weights(self, v, q, w):\n        v_ = self.v_net(v).transpose(1, 2).unsqueeze(2)\n        q_ = self.q_net(q).transpose(1, 2).unsqueeze(3)\n        logits = torch.matmul(torch.matmul(v_, w.unsqueeze(1)), q_)\n        logits = logits.squeeze(3).squeeze(2)\n\n        if self.k > 1:\n            logits = logits.unsqueeze(1)\n            logits = self.p_net(logits).squeeze(1) * self.k\n\n        return logits\n\n\nclass FCNet(nn.Module):\n    """"""\n    Simple class for non-linear fully connect network\n    """"""\n\n    def __init__(self, dims, act=""ReLU"", dropout=0):\n        super().__init__()\n\n        layers = []\n        for i in range(len(dims) - 2):\n            in_dim = dims[i]\n            out_dim = dims[i + 1]\n\n            if dropout > 0:\n                layers.append(nn.Dropout(dropout))\n\n            layers.append(weight_norm(nn.Linear(in_dim, out_dim), dim=None))\n\n            if act is not None:\n                layers.append(getattr(nn, act)())\n\n        if dropout > 0:\n            layers.append(nn.Dropout(dropout))\n\n        layers.append(weight_norm(nn.Linear(dims[-2], dims[-1]), dim=None))\n\n        if act is not None:\n            layers.append(getattr(nn, act)())\n\n        self.main = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.main(x)\n\n\nclass BiAttention(nn.Module):\n    def __init__(self, x_dim, y_dim, z_dim, glimpse, dropout=None):\n        super().__init__()\n        if dropout is None:\n            dropout = [0.2, 0.5]\n        self.glimpse = glimpse\n        self.logits = weight_norm(\n            BCNet(x_dim, y_dim, z_dim, glimpse, dropout=dropout, k=3),\n            name=""h_mat"",\n            dim=None,\n        )\n\n    def forward(self, v, q, v_mask=True):\n        p, logits = self.forward_all(v, q, v_mask)\n        return p, logits\n\n    def forward_all(self, v, q, v_mask=True):\n        v_num = v.size(1)\n        q_num = q.size(1)\n        logits = self.logits(v, q)\n\n        if v_mask:\n            v_abs_sum = v.abs().sum(2)\n            mask = (v_abs_sum == 0).unsqueeze(1).unsqueeze(3)\n            mask = mask.expand(logits.size())\n            logits.masked_fill_(mask, -float(""inf""))\n\n        expanded_logits = logits.view(-1, self.glimpse, v_num * q_num)\n        p = nn.functional.softmax(expanded_logits, 2)\n\n        return p.view(-1, self.glimpse, v_num, q_num), logits\n'"
mmf/modules/losses.py,29,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\nLosses module contains implementations for various losses used generally\nin vision and language space. One can register custom losses to be detected by\nMMF using the following example.\n\n.. code::\n\n   from mmf.common.registry import registry\n   from torch import nn\n\n\n   @registry.register_loss(""custom"")\n   class CustomLoss(nn.Module):\n       ...\n\nThen in your model\'s config you can specify ``losses`` attribute to use this loss\nin the following way:\n\n.. code::\n\n   model_config:\n       some_model:\n           losses:\n               - type: custom\n               - params: {}\n""""""\nimport collections\nimport warnings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom mmf.common.registry import registry\n\n\nclass Losses(nn.Module):\n    """"""``Losses`` acts as an abstraction for instantiating and calculating\n    losses. ``BaseModel`` instantiates this class based on the `losses`\n    attribute in the model\'s configuration `model_config`. ``loss_list``\n    needs to be a list for each separate loss containing `type` and `params`\n    attributes.\n\n    Args:\n        loss_list (ListConfig): Description of parameter `loss_list`.\n\n    Example::\n\n        # losses:\n        # - type: logit_bce\n        # Can also contain `params` to specify that particular loss\'s init params\n        # - type: combined\n        config = [{""type"": ""logit_bce""}, {""type"": ""combined""}]\n        losses = Losses(config)\n\n    .. note::\n\n        Since, ``Losses`` is instantiated in the ``BaseModel``, normal end user\n        mostly doesn\'t need to use this class.\n\n    Attributes:\n        losses: List containing instanttions of each loss\n                                   passed in config\n    """"""\n\n    def __init__(self, loss_list):\n        super().__init__()\n        self.losses = []\n        self._evaluation_predict = registry.get(""config"").evaluation.predict\n        for loss in loss_list:\n            self.losses.append(MMFLoss(loss))\n\n    def forward(self, sample_list, model_output, *args, **kwargs):\n        """"""Takes in the original ``SampleList`` returned from DataLoader\n        and `model_output` returned from the model and returned a Dict containing\n        loss for each of the losses in `losses`.\n\n        Args:\n            sample_list (SampleList): SampleList given be the dataloader.\n            model_output (Dict): Dict returned from model as output.\n\n        Returns:\n            Dict: Dictionary containing loss value for each of the loss.\n\n        """"""\n        output = {}\n        if not hasattr(sample_list, ""targets""):\n            if not self._evaluation_predict:\n                warnings.warn(\n                    ""Sample list has not field \'targets\', are you ""\n                    ""sure that your ImDB has labels? you may have ""\n                    ""wanted to run with evaluation.predict=true""\n                )\n            return output\n\n        for loss in self.losses:\n            output.update(loss(sample_list, model_output, *args, **kwargs))\n\n        registry_loss_key = ""{}.{}.{}"".format(\n            ""losses"", sample_list.dataset_name, sample_list.dataset_type\n        )\n        # Register the losses to registry\n        registry.register(registry_loss_key, output)\n\n        return output\n\n\nclass MMFLoss(nn.Module):\n    """"""Internal MMF helper and wrapper class for all Loss classes.\n    It makes sure that the value returned from a Loss class is a dict and\n    contain proper dataset type in keys, so that it is easy to figure out\n    which one is the val loss and which one is train loss.\n\n    For example: it will return ``{""val/vqa2/logit_bce"": 27.4}``, in case\n    `logit_bce` is used and SampleList is from `val` set of dataset `vqa2`.\n\n    Args:\n        params (type): Description of parameter `params`.\n\n    .. note::\n\n        Since, ``MMFLoss`` is used by the ``Losses`` class, end user\n        doesn\'t need to worry about it.\n    """"""\n\n    def __init__(self, params=None):\n        super().__init__()\n        if params is None:\n            params = {}\n        self.writer = registry.get(""writer"")\n\n        is_mapping = isinstance(params, collections.abc.MutableMapping)\n\n        if is_mapping:\n            if ""type"" not in params:\n                raise ValueError(\n                    ""Parameters to loss must have \'type\' field to""\n                    ""specify type of loss to instantiate""\n                )\n            else:\n                loss_name = params[""type""]\n        else:\n            assert isinstance(\n                params, str\n            ), ""loss must be a string or dictionary with \'type\' key""\n            loss_name = params\n\n        self.name = loss_name\n\n        loss_class = registry.get_loss_class(loss_name)\n\n        if loss_class is None:\n            raise ValueError(f""No loss named {loss_name} is registered to registry"")\n        # Special case of multi as it requires an array\n        if loss_name == ""multi"":\n            assert is_mapping\n            self.loss_criterion = loss_class(params)\n        else:\n            if is_mapping:\n                loss_params = params.get(""params"", {})\n            else:\n                loss_params = {}\n            self.loss_criterion = loss_class(**loss_params)\n\n    def forward(self, sample_list, model_output, *args, **kwargs):\n        loss = self.loss_criterion(sample_list, model_output, *args, **kwargs)\n\n        if not isinstance(loss, torch.Tensor):\n            loss = torch.tensor(loss, dtype=torch.float)\n\n        if loss.dim() == 0:\n            loss = loss.view(1)\n\n        key = ""{}/{}/{}"".format(\n            sample_list.dataset_type, sample_list.dataset_name, self.name\n        )\n\n        return {key: loss}\n\n\n@registry.register_loss(""logit_bce"")\nclass LogitBinaryCrossEntropy(nn.Module):\n    """"""Returns Binary Cross Entropy for logits.\n\n    Attention:\n        `Key`: logit_bce\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, sample_list, model_output):\n        """"""Calculates and returns the binary cross entropy for logits\n\n        Args:\n            sample_list (SampleList): SampleList containing `targets` attribute.\n            model_output (Dict): Model output containing `scores` attribute.\n\n        Returns:\n            torch.FloatTensor: Float value for loss.\n\n        """"""\n        scores = model_output[""scores""]\n        targets = sample_list[""targets""]\n        loss = F.binary_cross_entropy_with_logits(scores, targets, reduction=""mean"")\n\n        return loss * targets.size(1)\n\n\n@registry.register_loss(""bce"")\nclass BinaryCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, sample_list, model_output):\n        """"""Calculates and returns the binary cross entropy.\n\n        Args:\n            sample_list (SampleList): SampleList containing `targets` attribute.\n            model_output (Dict): Model output containing `scores` attribute.\n\n        Returns:\n            torch.FloatTensor: Float value for loss.\n\n        """"""\n        scores = model_output[""scores""]\n        targets = sample_list[""targets""]\n\n        loss = F.binary_cross_entropy(scores, targets, reduction=""mean"")\n\n        return loss * targets.size(1)\n\n\n@registry.register_loss(""caption_cross_entropy"")\nclass CaptionCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, sample_list, model_output):\n        """"""Calculates and returns the cross entropy loss for captions.\n\n        Args:\n            sample_list (SampleList): SampleList containing `targets` attribute.\n            model_output (Dict): Model output containing `scores` attribute.\n\n        Returns:\n            torch.FloatTensor: Float value for loss.\n\n        """"""\n        scores = model_output[""scores""]\n        targets = sample_list[""targets""]\n\n        # If no captions(test dataset) then assume decode length to be uniform\n        if hasattr(sample_list, ""caption_len""):\n            caption_lengths, _ = sample_list.caption_len.sort(dim=0, descending=True)\n            decode_lengths = (caption_lengths - 1).tolist()\n        else:\n            decode_lengths = [targets.size(1)] * targets.size(0)\n        if torch.__version__ >= ""1.1"":\n            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n            targets = pack_padded_sequence(\n                targets, decode_lengths, batch_first=True\n            ).data\n        else:\n            scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n\n        loss = F.cross_entropy(scores, targets)\n\n        return loss\n\n\n@registry.register_loss(""nll_loss"")\nclass NLLLoss(nn.Module):\n    """"""Negative log likelikehood loss.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, sample_list, model_output):\n        """"""Calculates and returns the negative log likelihood.\n\n        Args:\n            sample_list (SampleList): SampleList containing `targets` attribute.\n            model_output (Dict): Model output containing `scores` attribute.\n\n        Returns:\n            torch.FloatTensor: Float value for loss.\n\n        """"""\n        scores = model_output[""scores""]\n        targets = sample_list[""targets""]\n        _, idx = targets.max(dim=1)\n        loss = F.nll_loss(scores, idx, reduction=""mean"")\n\n        return loss * targets.size(1)\n\n\ndef kl_div(log_x, y):\n    y_is_0 = torch.eq(y.data, 0)\n    y.data.masked_fill_(y_is_0, 1)\n    log_y = torch.log(y)\n    y.data.masked_fill_(y_is_0, 0)\n    res = y * (log_y - log_x)\n\n    return torch.sum(res, dim=1, keepdim=True)\n\n\n@registry.register_loss(""multi"")\nclass MultiLoss(nn.Module):\n    """"""A loss for combining multiple losses with weights.\n\n    Args:\n        params (List(Dict)): A list containing parameters for each different loss\n                             and their weights.\n\n    Example::\n\n        # MultiLoss works with config like below where each loss\'s params and\n        # weights are defined\n        losses:\n        - type: multi\n          params:\n          - type: logit_bce\n            weight: 0.3\n            params: {}\n          - type: attention_supervision\n            weight: 0.7\n            params: {}\n\n    """"""\n\n    def __init__(self, params):\n        super().__init__()\n        self.losses = []\n        self.losses_weights = []\n        self.writer = registry.get(""writer"")\n\n        self.loss_names = []\n\n        for loss_params in params[""params""]:\n            self.loss_names.append(loss_params[""type""])\n            loss_fn = MMFLoss(loss_params)\n            loss_weight = loss_params.get(""weight"", {})\n            self.losses.append(loss_fn)\n            self.losses_weights.append(loss_weight)\n\n    def forward(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculates and returns the multi loss.\n\n        Args:\n            sample_list (SampleList): SampleList containing `attentions` attribute.\n            model_output (Dict): Model output containing `attention_supervision`\n                                 attribute.\n\n        Returns:\n            torch.FloatTensor: Float value for loss.\n\n        """"""\n        loss = 0\n        for idx, loss_fn in enumerate(self.losses):\n            value = loss_fn(sample_list, model_output, *args, **kwargs)\n            loss += self.losses_weights[idx] * value\n        return loss\n\n\n@registry.register_loss(""attention_supervision"")\nclass AttentionSupervisionLoss(nn.Module):\n    """"""Loss for attention supervision. Used in case you want to make attentions\n    similar to some particular values.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = lambda *args, **kwargs: nn.functional.binary_cross_entropy(\n            *args, **kwargs\n        )\n\n    def forward(self, sample_list, model_output):\n        """"""Calculates and returns the multi loss.\n\n        Args:\n            sample_list (SampleList): SampleList containing `targets` attribute.\n            model_output (Dict): Model output containing `scores` attribute.\n\n        Returns:\n            torch.FloatTensor: Float value for loss.\n\n        """"""\n        context_attentions = model_output[""attentions""]\n        attention_supervision = sample_list[""info""][""attention_supervision""]\n\n        loss = self.loss_fn(\n            context_attentions[0],\n            attention_supervision.float(),\n            weight=attention_supervision.float(),\n        )\n\n        # Multiply average loss back with target size to get actual loss\n        return loss * attention_supervision.size(1)\n\n\n@registry.register_loss(""weighted_softmax"")\nclass WeightedSoftmaxLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, sample_list, model_output):\n        pred_score = model_output[""scores""]\n        target_score = sample_list[""targets""]\n\n        tar_sum = torch.sum(target_score, dim=1, keepdim=True)\n        tar_sum_is_0 = torch.eq(tar_sum, 0)\n        tar_sum.masked_fill_(tar_sum_is_0, 1.0e-06)\n        tar = target_score / tar_sum\n\n        res = F.log_softmax(pred_score, dim=1)\n        loss = kl_div(res, tar)\n        loss = loss * tar_sum\n        loss = torch.sum(loss) / loss.size(0)\n        return loss\n\n\n@registry.register_loss(""softmax_kldiv"")\nclass SoftmaxKlDivLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, sample_list, model_output):\n        pred_score = model_output[""scores""]\n        target_score = sample_list[""targets""]\n\n        tar_sum = torch.sum(target_score, dim=1, keepdim=True)\n        tar_sum_is_0 = torch.eq(tar_sum, 0)\n        tar_sum.masked_fill_(tar_sum_is_0, 1.0e-06)\n        tar = target_score / tar_sum\n\n        res = F.log_softmax(pred_score, dim=1)\n        loss = kl_div(res, tar)\n        loss = torch.sum(loss) / loss.size(0)\n        return loss\n\n\n@registry.register_loss(""wrong"")\nclass WrongLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, sample_list, model_output):\n        pred_score = model_output[""scores""]\n        target_score = sample_list[""targets""]\n\n        tar_sum = torch.sum(target_score, dim=1, keepdim=True)\n        tar_sum_is_0 = torch.eq(tar_sum, 0)\n        tar_sum.masked_fill_(tar_sum_is_0, 1.0e-06)\n        tar = target_score / tar_sum\n\n        res = F.log_softmax(pred_score, dim=1)\n        loss = F.kl_div(res, tar, reduction=""mean"")\n        loss *= target_score.size(1)\n        return loss\n\n\n@registry.register_loss(""bce_kl_combined"")\nclass CombinedLoss(nn.Module):\n    def __init__(self, weight_softmax):\n        super().__init__()\n        self.weight_softmax = weight_softmax\n\n    def forward(self, sample_list, model_output):\n        pred_score = model_output[""scores""]\n        target_score = sample_list[""targets""]\n\n        tar_sum = torch.sum(target_score, dim=1, keepdim=True)\n        tar_sum_is_0 = torch.eq(tar_sum, 0)\n        tar_sum.masked_fill_(tar_sum_is_0, 1.0e-06)\n        tar = target_score / tar_sum\n\n        res = F.log_softmax(pred_score, dim=1)\n        loss1 = kl_div(res, tar)\n        loss1 = torch.sum(loss1) / loss1.size(0)\n\n        loss2 = F.binary_cross_entropy_with_logits(\n            pred_score, target_score, reduction=""mean""\n        )\n        loss2 *= target_score.size(1)\n\n        loss = self.weight_softmax * loss1 + loss2\n\n        return loss\n\n\n@registry.register_loss(""m4c_decoding_bce_with_mask"")\nclass M4CDecodingBCEWithMaskLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.one = torch.Tensor([1.0])\n\n    def forward(self, sample_list, model_output):\n        scores = model_output[""scores""]\n        targets = sample_list[""targets""]\n        loss_mask = sample_list[""train_loss_mask""]\n        assert scores.dim() == 3 and loss_mask.dim() == 2\n\n        losses = F.binary_cross_entropy_with_logits(scores, targets, reduction=""none"")\n        losses *= loss_mask.unsqueeze(-1)\n\n        count = torch.max(torch.sum(loss_mask), self.one.to(losses.device))\n        loss = torch.sum(losses) / count\n        return loss\n\n\n@registry.register_loss(""cross_entropy"")\nclass CrossEntropyLoss(nn.Module):\n    def __init__(self, params=None):\n        super().__init__()\n        if params is None:\n            params = {}\n        self.loss_fn = nn.CrossEntropyLoss(**params)\n\n    def forward(self, sample_list, model_output):\n        return self.loss_fn(model_output[""scores""], sample_list.targets)\n'"
mmf/modules/metrics.py,40,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\nThe metrics module contains implementations of various metrics used commonly to\nunderstand how well our models are performing. For e.g. accuracy, vqa_accuracy,\nr@1 etc.\n\nFor implementing your own metric, you need to follow these steps:\n\n1. Create your own metric class and inherit ``BaseMetric`` class.\n2. In the ``__init__`` function of your class, make sure to call\n   ``super().__init__(\'name\')`` where \'name\' is the name of your metric. If\n   you require any parameters in your ``__init__`` function, you can use\n   keyword arguments to represent them and metric constructor will take care of\n   providing them to your class from config.\n3. Implement a ``calculate`` function which takes in ``SampleList`` and\n   `model_output` as input and return back a float tensor/number.\n4. Register your metric with a key \'name\' by using decorator,\n   ``@registry.register_metric(\'name\')``.\n\nExample::\n\n    import torch\n\n    from mmf.common.registry import registry\n    from mmf.modules.metrics import BaseMetric\n\n    @registry.register_metric(""some"")\n    class SomeMetric(BaseMetric):\n        def __init__(self, some_param=None):\n            super().__init__(""some"")\n            ....\n\n        def calculate(self, sample_list, model_output):\n            metric = torch.tensor(2, dtype=torch.float)\n            return metric\n\nExample config for above metric::\n\n    model_config:\n        pythia:\n            metrics:\n            - type: some\n              params:\n                some_param: a\n""""""\n\nimport collections\n\nimport torch\nfrom sklearn.metrics import average_precision_score, f1_score, roc_auc_score\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.processors.processors import EvalAIAnswerProcessor\n\n\nclass Metrics:\n    """"""Internally used by MMF, Metrics acts as wrapper for handling\n    calculation of metrics over various metrics specified by the model in\n    the config. It initializes all of the metrics and when called it runs\n    calculate on each of them one by one and returns back a dict with proper\n    naming back. For e.g. an example dict returned by Metrics class:\n    ``{\'val/vqa_accuracy\': 0.3, \'val/r@1\': 0.8}``\n\n    Args:\n        metric_list (ListConfig): List of DictConfigs where each DictConfig\n                                        specifies name and parameters of the\n                                        metrics used.\n    """"""\n\n    def __init__(self, metric_list):\n        if not isinstance(metric_list, collections.abc.Sequence):\n            metric_list = [metric_list]\n\n        self.writer = registry.get(""writer"")\n        self.metrics = self._init_metrics(metric_list)\n\n    def _init_metrics(self, metric_list):\n        metrics = {}\n        self.required_params = {""dataset_name"", ""dataset_type""}\n        for metric in metric_list:\n            params = {}\n            if isinstance(metric, collections.abc.Mapping):\n                if not hasattr(metric, ""type""):\n                    raise ValueError(f""Metric {metric} needs to have \'type\' attribute"")\n                metric = metric.type\n                params = getattr(metric, ""params"", {})\n            else:\n                if not isinstance(metric, str):\n                    raise TypeError(\n                        ""Metric {} has inappropriate type""\n                        ""\'dict\' or \'str\' allowed"".format(metric)\n                    )\n\n            metric_cls = registry.get_metric_class(metric)\n            if metric_cls is None:\n                raise ValueError(f""No metric named {metric} registered to registry"")\n            metrics[metric] = metric_cls(**params)\n            self.required_params.update(metrics[metric].required_params)\n\n        return metrics\n\n    def __call__(self, sample_list, model_output, *args, **kwargs):\n        values = {}\n\n        dataset_type = sample_list.dataset_type\n        dataset_name = sample_list.dataset_name\n\n        with torch.no_grad():\n            for metric_name, metric_object in self.metrics.items():\n                key = f""{dataset_type}/{dataset_name}/{metric_name}""\n                values[key] = metric_object._calculate_with_checks(\n                    sample_list, model_output, *args, **kwargs\n                )\n\n                if not isinstance(values[key], torch.Tensor):\n                    values[key] = torch.tensor(values[key], dtype=torch.float)\n                else:\n                    values[key] = values[key].float()\n\n                if values[key].dim() == 0:\n                    values[key] = values[key].view(1)\n\n        registry.register(\n            ""{}.{}.{}"".format(""metrics"", sample_list.dataset_name, dataset_type), values\n        )\n\n        return values\n\n\nclass BaseMetric:\n    """"""Base class to be inherited by all metrics registered to MMF. See\n    the description on top of the file for more information. Child class must\n    implement ``calculate`` function.\n\n    Args:\n        name (str): Name of the metric.\n\n    """"""\n\n    def __init__(self, name, *args, **kwargs):\n        self.name = name\n        self.required_params = [""scores"", ""targets""]\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Abstract method to be implemented by the child class. Takes\n        in a ``SampleList`` and a dict returned by model as output and\n        returns back a float tensor/number indicating value for this metric.\n\n        Args:\n            sample_list (SampleList): SampleList provided by the dataloader for the\n                                current iteration.\n            model_output (Dict): Output dict from the model for the current\n                                 SampleList\n\n        Returns:\n            torch.Tensor|float: Value of the metric.\n\n        """"""\n        # Override in your child class\n        raise NotImplementedError(""\'calculate\' must be implemented in the child class"")\n\n    def __call__(self, *args, **kwargs):\n        return self.calculate(*args, **kwargs)\n\n    def _calculate_with_checks(self, *args, **kwargs):\n        value = self.calculate(*args, **kwargs)\n        return value\n\n\n@registry.register_metric(""accuracy"")\nclass Accuracy(BaseMetric):\n    """"""Metric for calculating accuracy.\n\n    **Key:** ``accuracy``\n    """"""\n\n    def __init__(self):\n        super().__init__(""accuracy"")\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate accuracy and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: accuracy.\n\n        """"""\n        output = model_output[""scores""]\n        expected = sample_list[""targets""]\n\n        assert (\n            output.dim() <= 2\n        ), ""Output from model shouldn\'t have more than dim 2 for accuracy""\n        assert (\n            expected.dim() <= 2\n        ), ""Expected target shouldn\'t have more than dim 2 for accuracy""\n\n        if output.dim() == 2:\n            output = torch.max(output, 1)[1]\n\n        # If more than 1\n        # If last dim is 1, we directly have class indices\n        if expected.dim() == 2 and expected.size(-1) != 1:\n            expected = torch.max(expected, 1)[1]\n\n        correct = (expected == output.squeeze()).sum().float()\n        total = len(expected)\n\n        value = correct / total\n        return value\n\n\n@registry.register_metric(""caption_bleu4"")\nclass CaptionBleu4Metric(BaseMetric):\n    """"""Metric for calculating caption accuracy using BLEU4 Score.\n\n    **Key:** ``caption_bleu4``\n    """"""\n\n    def __init__(self):\n        import nltk.translate.bleu_score as bleu_score\n\n        self._bleu_score = bleu_score\n        super().__init__(""caption_bleu4"")\n        self.caption_processor = registry.get(""coco_caption_processor"")\n        self.required_params = [""scores"", ""answers""]\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate accuracy and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: bleu4 score.\n\n        """"""\n        # Create reference and hypotheses captions.\n        references = []\n        hypotheses = []\n\n        # References\n        targets = sample_list.answers\n        for j, _ in enumerate(targets):\n            img_captions = [\n                self.caption_processor(c)[""tokens""] for c in targets[j].tolist()\n            ]\n            references.append(img_captions)\n\n        # Hypotheses\n        scores = torch.max(model_output[""scores""], dim=-1)[1]\n        scores = scores.tolist()\n        predictions = []\n        for j, _ in enumerate(scores):\n            caption = self.caption_processor(scores[j])[""tokens""]\n            predictions.append(caption)\n        hypotheses.extend(predictions)\n\n        assert len(references) == len(hypotheses)\n\n        bleu4 = self._bleu_score.corpus_bleu(references, hypotheses)\n\n        return targets.new_tensor(bleu4, dtype=torch.float)\n\n\n@registry.register_metric(""vqa_accuracy"")\nclass VQAAccuracy(BaseMetric):\n    """"""\n    Calculate VQAAccuracy. Find more information here_\n\n    **Key**: ``vqa_accuracy``.\n\n    .. _here: https://visualqa.org/evaluation.html\n    """"""\n\n    def __init__(self):\n        super().__init__(""vqa_accuracy"")\n\n    def _masked_unk_softmax(self, x, dim, mask_idx):\n        x1 = torch.nn.functional.softmax(x, dim=dim)\n        x1[:, mask_idx] = 0\n        x1_sum = torch.sum(x1, dim=1, keepdim=True)\n        y = x1 / x1_sum\n        return y\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate vqa accuracy and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: VQA Accuracy\n\n        """"""\n        output = model_output[""scores""]\n        expected = sample_list[""targets""]\n\n        output = self._masked_unk_softmax(output, 1, 0)\n        output = output.argmax(dim=1)  # argmax\n\n        one_hots = expected.new_zeros(*expected.size())\n        one_hots.scatter_(1, output.view(-1, 1), 1)\n        scores = one_hots * expected\n        accuracy = torch.sum(scores) / expected.size(0)\n\n        return accuracy\n\n\n@registry.register_metric(""vqa_evalai_accuracy"")\nclass VQAEvalAIAccuracy(BaseMetric):\n    """"""\n    Calculate Eval AI VQAAccuracy. Find more information here_\n    This is more accurate and similar comparision to Eval AI\n    but is slower compared to vqa_accuracy.\n\n    **Key**: ``vqa_evalai_accuracy``.\n\n    .. _here: https://visualqa.org/evaluation.html\n    """"""\n\n    def __init__(self):\n        super().__init__(""vqa_evalai_accuracy"")\n        self.evalai_answer_processor = EvalAIAnswerProcessor()\n        self.required_params = [""scores"", ""answers"", ""context_tokens""]\n\n    def _masked_unk_softmax(self, x, dim, mask_idx):\n        x1 = torch.nn.functional.softmax(x, dim=dim)\n        x1[:, mask_idx] = 0\n        x1_sum = torch.sum(x1, dim=1, keepdim=True)\n        y = x1 / x1_sum\n        return y\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate vqa accuracy and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: VQA Accuracy\n\n        """"""\n        output = model_output[""scores""]\n        expected = sample_list[""answers""]\n\n        answer_processor = registry.get(sample_list.dataset_name + ""_answer_processor"")\n        answer_space_size = answer_processor.get_true_vocab_size()\n\n        output = self._masked_unk_softmax(output, 1, 0)\n        output = output.argmax(dim=1).clone().tolist()\n        accuracy = []\n\n        for idx, answer_id in enumerate(output):\n            if answer_id >= answer_space_size:\n                answer_id -= answer_space_size\n                answer = sample_list[""context_tokens""][idx][answer_id]\n            else:\n                answer = answer_processor.idx2word(answer_id)\n\n            answer = self.evalai_answer_processor(answer)\n\n            gt_answers = [self.evalai_answer_processor(x) for x in expected[idx]]\n            gt_answers = list(enumerate(gt_answers))\n\n            gt_acc = []\n            for gt_answer in gt_answers:\n                other_answers = [item for item in gt_answers if item != gt_answer]\n                matching_answers = [item for item in other_answers if item[1] == answer]\n                acc = min(1, float(len(matching_answers)) / 3)\n                gt_acc.append(acc)\n            avgGTAcc = float(sum(gt_acc)) / len(gt_acc)\n            accuracy.append(avgGTAcc)\n\n        accuracy = float(sum(accuracy)) / len(accuracy)\n\n        return model_output[""scores""].new_tensor(accuracy, dtype=torch.float)\n\n\nclass RecallAtK(BaseMetric):\n    def __init__(self, name=""recall@k""):\n        super().__init__(name)\n\n    def score_to_ranks(self, scores):\n        # sort in descending order - largest score gets highest rank\n        sorted_ranks, ranked_idx = scores.sort(1, descending=True)\n\n        # convert from ranked_idx to ranks\n        ranks = ranked_idx.clone().fill_(0)\n        for i in range(ranked_idx.size(0)):\n            for j in range(100):\n                ranks[i][ranked_idx[i][j]] = j\n        ranks += 1\n        return ranks\n\n    def get_gt_ranks(self, ranks, ans_ind):\n        _, ans_ind = ans_ind.max(dim=1)\n        ans_ind = ans_ind.view(-1)\n        gt_ranks = torch.LongTensor(ans_ind.size(0))\n\n        for i in range(ans_ind.size(0)):\n            gt_ranks[i] = int(ranks[i, ans_ind[i].long()])\n        return gt_ranks\n\n    def get_ranks(self, sample_list, model_output, *args, **kwargs):\n        output = model_output[""scores""]\n        expected = sample_list[""targets""]\n\n        ranks = self.score_to_ranks(output)\n        gt_ranks = self.get_gt_ranks(ranks, expected)\n\n        ranks = self.process_ranks(gt_ranks)\n        return ranks.float()\n\n    def calculate(self, sample_list, model_output, k, *args, **kwargs):\n        ranks = self.get_ranks(sample_list, model_output)\n        recall = float(torch.sum(torch.le(ranks, k))) / ranks.size(0)\n        return recall\n\n\n@registry.register_metric(""r@1"")\nclass RecallAt1(RecallAtK):\n    """"""\n    Calculate Recall@1 which specifies how many time the chosen candidate\n    was rank 1.\n\n    **Key**: ``r@1``.\n    """"""\n\n    def __init__(self):\n        super().__init__(""r@1"")\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate Recall@1 and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: Recall@1\n\n        """"""\n        return self.calculate(sample_list, model_output, k=1)\n\n\n@registry.register_metric(""r@5"")\nclass RecallAt5(RecallAtK):\n    """"""\n    Calculate Recall@5 which specifies how many time the chosen candidate\n    was among first 5 rank.\n\n    **Key**: ``r@5``.\n    """"""\n\n    def __init__(self):\n        super().__init__(""r@5"")\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate Recall@5 and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: Recall@5\n\n        """"""\n        return self.calculate(sample_list, model_output, k=5)\n\n\n@registry.register_metric(""r@10"")\nclass RecallAt10(RecallAtK):\n    """"""\n    Calculate Recall@10 which specifies how many time the chosen candidate\n    was among first 10 ranks.\n\n    **Key**: ``r@10``.\n    """"""\n\n    def __init__(self):\n        super().__init__(""r@10"")\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate Recall@10 and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: Recall@10\n\n        """"""\n        return self.calculate(sample_list, model_output, k=10)\n\n\n@registry.register_metric(""mean_r"")\nclass MeanRank(RecallAtK):\n    """"""\n    Calculate MeanRank which specifies what was the average rank of the chosen\n    candidate.\n\n    **Key**: ``mean_r``.\n    """"""\n\n    def __init__(self):\n        super().__init__(""mean_r"")\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate Mean Rank and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: mean rank\n\n        """"""\n        ranks = self.get_ranks(sample_list, model_output)\n        return torch.mean(ranks)\n\n\n@registry.register_metric(""mean_rr"")\nclass MeanReciprocalRank(RecallAtK):\n    """"""\n    Calculate reciprocal of mean rank..\n\n    **Key**: ``mean_rr``.\n    """"""\n\n    def __init__(self):\n        super().__init__(""mean_rr"")\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate Mean Reciprocal Rank and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: Mean Reciprocal Rank\n\n        """"""\n        ranks = self.get_ranks(sample_list, model_output)\n        return torch.mean(ranks.reciprocal())\n\n\n@registry.register_metric(""textvqa_accuracy"")\nclass TextVQAAccuracy(BaseMetric):\n    def __init__(self):\n        super().__init__(""textvqa_accuracy"")\n        import mmf.utils.m4c_evaluators as evaluators\n\n        self.evaluator = evaluators.TextVQAAccuracyEvaluator()\n        self.required_params = [""scores"", ""answers"", ""context_tokens""]\n        self.gt_key = ""answers""\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        answer_processor = registry.get(sample_list.dataset_name + ""_answer_processor"")\n\n        batch_size = sample_list.context_tokens.size(0)\n        pred_answers = model_output[""scores""].argmax(dim=-1)\n        context_tokens = sample_list.context_tokens.cpu().numpy()\n        answers = sample_list.get(self.gt_key).cpu().numpy()\n        answer_space_size = answer_processor.get_true_vocab_size()\n\n        predictions = []\n        from mmf.utils.distributed import byte_tensor_to_object\n        from mmf.utils.text import word_tokenize\n\n        for idx in range(batch_size):\n            tokens = byte_tensor_to_object(context_tokens[idx])\n            answer_words = []\n            for answer_id in pred_answers[idx].tolist():\n                if answer_id >= answer_space_size:\n                    answer_id -= answer_space_size\n                    answer_words.append(word_tokenize(tokens[answer_id]))\n                else:\n                    if answer_id == answer_processor.EOS_IDX:\n                        break\n                    answer_words.append(\n                        answer_processor.answer_vocab.idx2word(answer_id)\n                    )\n\n            pred_answer = "" "".join(answer_words).replace("" \'s"", ""\'s"")\n            gt_answers = byte_tensor_to_object(answers[idx])\n            predictions.append({""pred_answer"": pred_answer, ""gt_answers"": gt_answers})\n\n        accuracy = self.evaluator.eval_pred_list(predictions)\n        accuracy = torch.tensor(accuracy).to(sample_list.context_tokens.device)\n\n        return accuracy\n\n\n@registry.register_metric(""stvqa_anls"")\nclass STVQAANLS(TextVQAAccuracy):\n    def __init__(self):\n        super().__init__()\n        self.name = ""stvqa_anls""\n        import mmf.utils.m4c_evaluators as evaluators\n\n        self.evaluator = evaluators.STVQAANLSEvaluator()\n\n\n@registry.register_metric(""stvqa_accuracy"")\nclass STVQAAccuracy(TextVQAAccuracy):\n    def __init__(self):\n        super().__init__()\n        self.name = ""stvqa_accuracy""\n        import mmf.utils.m4c_evaluators as evaluators\n\n        self.evaluator = evaluators.STVQAAccuracyEvaluator()\n\n\n@registry.register_metric(""ocrvqa_accuracy"")\nclass OCRVQAAccuracy(STVQAAccuracy):\n    def __init__(self):\n        super().__init__()\n        # same as STVQAAccuracy except for the name\n        self.name = ""ocrvqa_accuracy""\n\n\n@registry.register_metric(""textcaps_bleu4"")\nclass TextCapsBleu4(TextVQAAccuracy):\n    def __init__(self):\n        super().__init__()\n        self.name = ""textcaps_bleu4""\n        self.required_params = [""scores"", ""ref_strs"", ""context_tokens""]\n        self.gt_key = ""ref_strs""\n        import mmf.utils.m4c_evaluators as evaluators\n\n        self.evaluator = evaluators.TextCapsBleu4Evaluator()\n\n\n@registry.register_metric(""f1"")\nclass F1(BaseMetric):\n    """"""Metric for calculating F1. Can be used with type and params\n    argument for customization. params will be directly passed to sklearn\n    f1 function.\n    **Key:** ``f1``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(""f1"")\n        self._multilabel = kwargs.pop(""multilabel"", False)\n        self._sk_kwargs = kwargs\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate f1 and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: f1.\n        """"""\n        scores = model_output[""scores""]\n        expected = sample_list[""targets""]\n\n        if self._multilabel:\n            output = torch.sigmoid(scores)\n            output = torch.round(output)\n            expected = _convert_to_one_hot(expected, output)\n        else:\n            # Multiclass, or binary case\n            output = scores.argmax(dim=-1)\n            if expected.dim() != 1:\n                # Probably one-hot, convert back to class indices array\n                expected = expected.argmax(dim=-1)\n\n        value = f1_score(expected.cpu(), output.cpu(), **self._sk_kwargs)\n\n        return expected.new_tensor(value, dtype=torch.float)\n\n\n@registry.register_metric(""macro_f1"")\nclass MacroF1(F1):\n    """"""Metric for calculating Macro F1.\n\n    **Key:** ``macro_f1``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""macro"", **kwargs)\n        self.name = ""macro_f1""\n\n\n@registry.register_metric(""micro_f1"")\nclass MicroF1(F1):\n    """"""Metric for calculating Micro F1.\n\n    **Key:** ``micro_f1``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""micro"", **kwargs)\n        self.name = ""micro_f1""\n\n\n@registry.register_metric(""binary_f1"")\nclass BinaryF1(F1):\n    """"""Metric for calculating Binary F1.\n\n    **Key:** ``binary_f1``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""micro"", labels=[1], **kwargs)\n        self.name = ""binary_f1""\n\n\n@registry.register_metric(""multilabel_f1"")\nclass MultiLabelF1(F1):\n    """"""Metric for calculating Multilabel F1.\n\n    **Key:** ``multilabel_f1``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(multilabel=True, **kwargs)\n        self.name = ""multilabel_f1""\n\n\n@registry.register_metric(""multilabel_micro_f1"")\nclass MultiLabelMicroF1(MultiLabelF1):\n    """"""Metric for calculating Multilabel Micro F1.\n\n    **Key:** ``multilabel_micro_f1``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""micro"", **kwargs)\n        self.name = ""multilabel_micro_f1""\n\n\n@registry.register_metric(""multilabel_macro_f1"")\nclass MultiLabelMacroF1(F1):\n    """"""Metric for calculating Multilabel Macro F1.\n\n    **Key:** ``multilabel_macro_f1``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""macro"", **kwargs)\n        self.name = ""multilabel_macro_f1""\n\n\n@registry.register_metric(""roc_auc"")\nclass ROC_AUC(BaseMetric):\n    """"""Metric for calculating ROC_AUC.\n    See more details at `sklearn.metrics.roc_auc_score <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score>`_ # noqa\n\n    **Note**: ROC_AUC is not defined when expected tensor only contains one\n    label. Make sure you have both labels always or use it on full val only\n\n    **Key:** ``roc_auc``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(""roc_auc"")\n        self._sk_kwargs = kwargs\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate ROC_AUC and returns it back. The function performs softmax\n        on the logits provided and then calculated the ROC_AUC.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration.\n            model_output (Dict): Dict returned by model. This should contain ""scores""\n                                 field pointing to logits returned from the model.\n\n        Returns:\n            torch.FloatTensor: ROC_AUC.\n\n        """"""\n\n        output = torch.nn.functional.softmax(model_output[""scores""], dim=-1)\n        expected = sample_list[""targets""]\n        expected = _convert_to_one_hot(expected, output)\n        value = roc_auc_score(expected.cpu(), output.cpu(), **self._sk_kwargs)\n        return expected.new_tensor(value, dtype=torch.float)\n\n\n@registry.register_metric(""micro_roc_auc"")\nclass MicroROC_AUC(ROC_AUC):\n    """"""Metric for calculating Micro ROC_AUC.\n\n    **Key:** ``micro_roc_auc``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""micro"", **kwargs)\n        self.name = ""micro_roc_auc""\n\n\n@registry.register_metric(""macro_roc_auc"")\nclass MacroROC_AUC(ROC_AUC):\n    """"""Metric for calculating Macro ROC_AUC.\n\n    **Key:** ``macro_roc_auc``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""macro"", **kwargs)\n        self.name = ""macro_roc_auc""\n\n\n@registry.register_metric(""ap"")\nclass AveragePrecision(BaseMetric):\n    """"""Metric for calculating Average Precision.\n    See more details at `sklearn.metrics.average_precision_score <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score>`_ # noqa\n\n    **Key:** ``ap``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(""ap"")\n        self._sk_kwargs = kwargs\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """"""Calculate AP and returns it back. The function performs softmax\n        on the logits provided and then calculated the AP.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration.\n            model_output (Dict): Dict returned by model. This should contain ""scores""\n                                 field pointing to logits returned from the model.\n\n        Returns:\n            torch.FloatTensor: AP.\n\n        """"""\n\n        output = torch.nn.functional.softmax(model_output[""scores""], dim=-1)\n        expected = sample_list[""targets""]\n        expected = _convert_to_one_hot(expected, output)\n        value = average_precision_score(expected.cpu(), output.cpu(), **self._sk_kwargs)\n        return expected.new_tensor(value, dtype=torch.float)\n\n\n@registry.register_metric(""micro_ap"")\nclass MicroAP(ROC_AUC):\n    """"""Metric for calculating Micro Average Precision.\n\n    **Key:** ``micro_ap``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""micro"", **kwargs)\n        self.name = ""micro_ap""\n\n\n@registry.register_metric(""macro_ap"")\nclass MacroAP(ROC_AUC):\n    """"""Metric for calculating Macro Average Precision.\n\n    **Key:** ``macro_ap``\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average=""macro"", **kwargs)\n        self.name = ""macro_ap""\n\n\ndef _convert_to_one_hot(expected, output):\n    # This won\'t get called in case of multilabel, only multiclass or binary\n    # as multilabel will anyways be multi hot vector\n    if output.squeeze().dim() != expected.squeeze().dim() and expected.dim() == 1:\n        expected = torch.nn.functional.one_hot(\n            expected.long(), num_classes=output.size(-1)\n        ).float()\n    return expected\n'"
mmf/modules/optimizers.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom transformers.optimization import AdamW\n\nfrom mmf.common.registry import registry\n\nregistry.register_optimizer(""adam_w"")(AdamW)\n'"
mmf/modules/schedulers.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom transformers.optimization import (\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\n\nfrom mmf.common.registry import registry\n\n\n@registry.register_scheduler(""pythia"")\nclass PythiaScheduler(LambdaLR):\n    def __init__(self, optimizer, *args, **kwargs):\n        from mmf.utils.general import lr_lambda_update\n\n        self._lambda_func = lr_lambda_update\n        self._global_config = registry.get(""config"")\n\n        super().__init__(optimizer, self.lr_lambda, *args, **kwargs)\n\n    def lr_lambda(self, step):\n        return self._lambda_func(step, self._global_config)\n\n\n@registry.register_scheduler(""warmup_linear"")\nclass WarmupLinearScheduler(LambdaLR):\n    def __new__(cls, optimizer, *args, **kwargs):\n        return get_linear_schedule_with_warmup(optimizer, *args, **kwargs)\n\n\n@registry.register_scheduler(""warmup_cosine"")\nclass WarmupCosineScheduler(LambdaLR):\n    def __new__(cls, optimizer, *args, **kwargs):\n        return get_cosine_schedule_with_warmup(optimizer, *args, **kwargs)\n'"
mmf/trainers/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n__all__ = [""BaseTrainer""]\n\nfrom .base_trainer import BaseTrainer\n'"
mmf/trainers/base_trainer.py,17,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport gc\nimport math\nimport time\n\nimport omegaconf\nimport torch\nfrom tqdm import tqdm\n\nfrom mmf.common.dataset_loader import DatasetLoader\nfrom mmf.common.meter import Meter\nfrom mmf.common.registry import registry\nfrom mmf.common.report import Report\nfrom mmf.modules.metrics import Metrics\nfrom mmf.utils.build import build_model, build_optimizer, build_scheduler\nfrom mmf.utils.checkpoint import Checkpoint\nfrom mmf.utils.configuration import get_mmf_env\nfrom mmf.utils.distributed import (\n    broadcast_scalar,\n    get_world_size,\n    is_master,\n    reduce_dict,\n)\nfrom mmf.utils.early_stopping import EarlyStopping\nfrom mmf.utils.general import clip_gradients, print_model_parameters\nfrom mmf.utils.logger import Logger, TensorboardLogger\nfrom mmf.utils.timer import Timer\n\n\n@registry.register_trainer(""base_trainer"")\nclass BaseTrainer:\n    def __init__(self, config):\n        self.config = config\n        self.profiler = Timer()\n        self.total_timer = Timer()\n\n    def load(self):\n        self._set_device()\n\n        self.run_type = self.config.get(""run_type"", ""train"")\n        self.dataset_loader = DatasetLoader(self.config)\n        self._datasets = self.config.datasets\n\n        # Check if loader is already defined, else init it\n        writer = registry.get(""writer"", no_warning=True)\n        if writer:\n            self.writer = writer\n        else:\n            self.writer = Logger(self.config)\n            registry.register(""writer"", self.writer)\n\n        configuration = registry.get(""configuration"", no_warning=True)\n        if configuration:\n            configuration.pretty_print()\n\n        self.config_based_setup()\n\n        self.load_datasets()\n        self.load_model_and_optimizer()\n        self.load_metrics()\n\n    def _set_device(self):\n        self.local_rank = self.config.device_id\n        self.device = self.local_rank\n        self.distributed = False\n\n        # Will be updated later based on distributed setup\n        registry.register(""global_device"", self.device)\n\n        if self.config.distributed.init_method is not None:\n            self.distributed = True\n            self.device = torch.device(""cuda"", self.local_rank)\n        elif torch.cuda.is_available():\n            self.device = torch.device(""cuda"")\n        else:\n            self.device = torch.device(""cpu"")\n\n        registry.register(""current_device"", self.device)\n\n    def load_datasets(self):\n        self.writer.write(""Loading datasets"", ""info"")\n        self.dataset_loader.load_datasets()\n\n        self.train_dataset = self.dataset_loader.train_dataset\n        self.val_dataset = self.dataset_loader.val_dataset\n\n        # Total iterations for snapshot\n        self.snapshot_iterations = len(self.val_dataset)\n        self.snapshot_iterations //= self.config.training.batch_size\n\n        self.test_dataset = self.dataset_loader.test_dataset\n\n        self.train_loader = self.dataset_loader.train_loader\n        self.val_loader = self.dataset_loader.val_loader\n        self.test_loader = self.dataset_loader.test_loader\n\n    def load_metrics(self):\n        metrics = self.config.evaluation.get(""metrics"", [])\n        self.metrics = Metrics(metrics)\n        self.metrics_params = self.metrics.required_params\n\n    def load_model_and_optimizer(self):\n        attributes = self.config.model_config[self.config.model]\n        # Easy way to point to config for other model\n        if isinstance(attributes, str):\n            attributes = self.config.model_config[attributes]\n\n        with omegaconf.open_dict(attributes):\n            attributes.model = self.config.model\n\n        self.model = build_model(attributes)\n\n        if ""cuda"" in str(self.device):\n            device_info = ""CUDA Device {} is: {}"".format(\n                self.config.distributed.rank,\n                torch.cuda.get_device_name(self.local_rank),\n            )\n            registry.register(""global_device"", self.config.distributed.rank)\n            self.writer.write(device_info, log_all=True)\n\n        self.model = self.model.to(self.device)\n        self.optimizer = build_optimizer(self.model, self.config)\n\n        registry.register(""data_parallel"", False)\n        registry.register(""distributed"", False)\n\n        self.load_extras()\n        self.parallelize_model()\n\n    def parallelize_model(self):\n        training = self.config.training\n        if (\n            ""cuda"" in str(self.device)\n            and torch.cuda.device_count() > 1\n            and not self.distributed\n        ):\n            registry.register(""data_parallel"", True)\n            self.model = torch.nn.DataParallel(self.model)\n\n        if ""cuda"" in str(self.device) and self.distributed:\n            registry.register(""distributed"", True)\n            self.model = torch.nn.parallel.DistributedDataParallel(\n                self.model,\n                device_ids=[self.local_rank],\n                output_device=self.local_rank,\n                check_reduction=True,\n                find_unused_parameters=training.find_unused_parameters,\n            )\n\n    def load_extras(self):\n        self.writer.write(""Torch version is: "" + torch.__version__)\n        self.checkpoint = Checkpoint(self)\n        self.meter = Meter()\n\n        self.training_config = self.config.training\n\n        early_stop_criteria = self.training_config.early_stop.criteria\n        early_stop_minimize = self.training_config.early_stop.minimize\n        early_stop_enabled = self.training_config.early_stop.enabled\n        early_stop_patience = self.training_config.early_stop.patience\n\n        self.log_interval = self.training_config.log_interval\n        self.evaluation_interval = self.training_config.evaluation_interval\n        self.checkpoint_interval = self.training_config.checkpoint_interval\n        self.max_updates = self.training_config.max_updates\n        self.should_clip_gradients = self.training_config.clip_gradients\n        self.max_epochs = self.training_config.max_epochs\n\n        self.early_stopping = EarlyStopping(\n            self.model,\n            self.checkpoint,\n            early_stop_criteria,\n            patience=early_stop_patience,\n            minimize=early_stop_minimize,\n            should_stop=early_stop_enabled,\n        )\n        self.current_epoch = 0\n        self.current_iteration = 0\n        self.num_updates = 0\n\n        self.checkpoint.load_state_dict()\n\n        self.not_debug = self.training_config.logger_level != ""debug""\n\n        self.lr_scheduler = None\n\n        if self.training_config.lr_scheduler is True:\n            self.lr_scheduler = build_scheduler(self.optimizer, self.config)\n\n        self.tb_writer = None\n\n        if self.training_config.tensorboard:\n            log_dir = self.writer.log_folder\n            env_tb_logdir = get_mmf_env(key=""tensorboard_logdir"")\n            if env_tb_logdir:\n                log_dir = env_tb_logdir\n\n            self.tb_writer = TensorboardLogger(log_dir, self.current_iteration)\n\n    def config_based_setup(self):\n        seed = self.config.training.seed\n        if seed is None:\n            return\n\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    def train(self):\n        self.writer.write(""===== Model ====="")\n        self.writer.write(self.model)\n\n        print_model_parameters(self.model)\n\n        if ""train"" not in self.run_type:\n            self.inference()\n            return\n\n        should_break = False\n\n        if self.max_epochs is None:\n            self.max_epochs = math.inf\n        else:\n            self.max_updates = math.inf\n\n        self.model.train()\n        self.train_timer = Timer()\n        self.snapshot_timer = Timer()\n\n        self.profile(""Setup Time"")\n\n        torch.autograd.set_detect_anomaly(True)\n        self.writer.write(""Starting training..."")\n\n        while self.num_updates < self.max_updates and not should_break:\n            self.current_epoch += 1\n            registry.register(""current_epoch"", self.current_epoch)\n\n            # Seed the sampler in case if it is distributed\n            self.dataset_loader.seed_sampler(""train"", self.current_epoch)\n\n            if self.current_epoch > self.max_epochs:\n                break\n\n            for batch in self.train_loader:\n                self.profile(""Batch load time"")\n                self.current_iteration += 1\n                self.writer.write(self.num_updates + 1, ""debug"")\n\n                report = self._forward_pass(batch)\n                loss = self._extract_loss(report)\n                self._backward(loss)\n                should_break = self._logistics(report)\n\n                if self.num_updates > self.max_updates:\n                    should_break = True\n\n                if should_break:\n                    break\n\n            # In distributed, each worker will complete one epoch when we reach this\n            # as each worker is an individual instance\n            self.current_epoch += get_world_size() - 1\n        self.finalize()\n\n    def _run_scheduler(self):\n        if self.lr_scheduler is not None:\n            self.lr_scheduler.step(self.num_updates)\n\n    def _forward_pass(self, batch):\n        prepared_batch = self.dataset_loader.prepare_batch(batch)\n        self.profile(""Batch prepare time"")\n        # Arguments should be a dict at this point\n        model_output = self.model(prepared_batch)\n        report = Report(prepared_batch, model_output)\n        self.profile(""Forward time"")\n\n        return report\n\n    def _backward(self, loss):\n        self.optimizer.zero_grad()\n        loss.backward()\n\n        if self.should_clip_gradients:\n            clip_gradients(self.model, self.num_updates, self.tb_writer, self.config)\n\n        self.optimizer.step()\n        self._run_scheduler()\n        self.num_updates += 1\n        self.profile(""Backward time"")\n\n    def _extract_loss(self, report):\n        loss_dict = report.losses\n        loss = sum([loss.mean() for loss in loss_dict.values()])\n        return loss\n\n    def finalize(self):\n        self.writer.write(""Stepping into final validation check"")\n\n        # Only do when run_type has train as it shouldn\'t happen on validation and\n        # inference runs. Inference will take care of this anyways. Also, don\'t run\n        # if current iteration is divisble by snapshot interval as it will just\n        # be a repeat\n        if (\n            ""train"" in self.run_type\n            and self.num_updates % self.evaluation_interval != 0\n        ):\n            self._try_full_validation(force=True)\n\n        self.checkpoint.restore()\n        self.checkpoint.finalize()\n        self.inference()\n\n        self.writer.write(f""Finished run in {self.total_timer.get_time_since_start()}"")\n\n    def _update_meter(self, report, meter=None, eval_mode=False):\n        if meter is None:\n            meter = self.meter\n\n        if hasattr(report, ""metrics""):\n            metrics_dict = report.metrics\n            reduced_metrics_dict = reduce_dict(metrics_dict)\n\n        if not eval_mode:\n            loss_dict = report.losses\n            reduced_loss_dict = reduce_dict(loss_dict)\n\n        with torch.no_grad():\n            # Add metrics to meter only when mode is `eval`\n            meter_update_dict = {}\n            if not eval_mode:\n                loss_key = report.dataset_type + ""/total_loss""\n                reduced_loss = sum([loss.mean() for loss in reduced_loss_dict.values()])\n                if hasattr(reduced_loss, ""item""):\n                    reduced_loss = reduced_loss.item()\n\n                registry.register(loss_key, reduced_loss)\n                meter_update_dict.update({loss_key: reduced_loss})\n                meter_update_dict.update(reduced_loss_dict)\n            if hasattr(report, ""metrics""):\n                meter_update_dict.update(reduced_metrics_dict)\n            meter.update(meter_update_dict, report.batch_size)\n\n    def _logistics(self, report):\n        registry.register(""current_iteration"", self.current_iteration)\n        registry.register(""num_updates"", self.num_updates)\n\n        should_print = self.num_updates % self.log_interval == 0\n        should_break = False\n        extra = {}\n\n        if should_print is True:\n            if ""cuda"" in str(self.device):\n                extra[""max mem""] = torch.cuda.max_memory_allocated() / 1024\n                extra[""max mem""] //= 1024\n\n            if self.training_config.experiment_name:\n                extra[""experiment""] = self.training_config.experiment_name\n\n            extra.update(\n                {\n                    ""epoch"": self.current_epoch,\n                    ""num_updates"": self.num_updates,\n                    ""iterations"": self.current_iteration,\n                    ""max_updates"": self.max_updates,\n                    ""lr"": ""{:.5f}"".format(self.optimizer.param_groups[0][""lr""]).rstrip(\n                        ""0""\n                    ),\n                    ""ups"": ""{:.2f}"".format(\n                        self.log_interval / self.train_timer.unix_time_since_start()\n                    ),\n                    ""time"": self.train_timer.get_time_since_start(),\n                    ""time_since_start"": self.total_timer.get_time_since_start(),\n                    ""eta"": self._calculate_time_left(),\n                }\n            )\n\n            self.train_timer.reset()\n            # Calculate metrics every log interval for debugging\n            if self.training_config.evaluate_metrics:\n                report.metrics = self.metrics(report, report)\n            self._update_meter(report, self.meter)\n\n            self._summarize_report(self.meter, should_print=should_print, extra=extra)\n\n        self._try_snapshot()\n        should_break = self._try_full_validation()\n\n        return should_break\n\n    def _try_snapshot(self):\n        if self.num_updates % self.checkpoint_interval == 0:\n            self.writer.write(""Checkpoint time. Saving a checkpoint."")\n            self.checkpoint.save(\n                self.num_updates, self.current_iteration, update_best=False\n            )\n\n    def _try_full_validation(self, force=False):\n        should_break = False\n\n        if self.num_updates % self.evaluation_interval == 0 or force:\n            self.snapshot_timer.reset()\n            self.writer.write(""Evaluation time. Running on full validation set..."")\n            # Validation and Early stopping\n            # Create a new meter for this case\n            report, meter = self.evaluate(self.val_loader)\n\n            extra = {\n                ""num_updates"": self.num_updates,\n                ""epoch"": self.current_epoch,\n                ""iterations"": self.current_iteration,\n                ""max_updates"": self.max_updates,\n                ""val_time"": self.snapshot_timer.get_time_since_start(),\n            }\n\n            stop = self.early_stopping(self.num_updates, self.current_iteration, meter)\n            stop = bool(broadcast_scalar(stop, src=0, device=self.device))\n\n            extra.update(self.early_stopping.get_info())\n\n            self._summarize_report(meter, extra=extra)\n            gc.collect()\n\n            if ""cuda"" in str(self.device):\n                torch.cuda.empty_cache()\n\n            if stop is True:\n                self.writer.write(""Early stopping activated"")\n                should_break = True\n\n            self.train_timer.reset()\n\n        return should_break\n\n    def evaluate(self, loader, use_tqdm=False, single_batch=False):\n        meter = Meter()\n\n        with torch.no_grad():\n            self.model.eval()\n            disable_tqdm = not use_tqdm or not is_master()\n            combined_report = None\n\n            for batch in tqdm(loader, disable=disable_tqdm):\n                report = self._forward_pass(batch)\n                self._update_meter(report, meter)\n\n                # accumulate necessary params for metric calculation\n                if combined_report is None:\n                    combined_report = report\n                else:\n                    combined_report.accumulate_tensor_fields(\n                        report, self.metrics.required_params\n                    )\n                    combined_report.batch_size += report.batch_size\n\n                if single_batch is True:\n                    break\n\n            combined_report.metrics = self.metrics(combined_report, combined_report)\n            self._update_meter(combined_report, meter, eval_mode=True)\n\n            self.model.train()\n\n        return combined_report, meter\n\n    def _summarize_report(self, meter, should_print=True, extra=None):\n        if extra is None:\n            extra = {}\n        if not is_master():\n            return\n\n        if self.training_config.tensorboard:\n            scalar_dict = meter.get_scalar_dict()\n            self.tb_writer.add_scalars(scalar_dict, self.current_iteration)\n\n        if not should_print:\n            return\n        log_dict = {""progress"": f""{self.num_updates}/{self.max_updates}""}\n        log_dict.update(meter.get_log_dict())\n        log_dict.update(extra)\n\n        self.writer.log_progress(log_dict)\n\n    def inference(self):\n        if ""val"" in self.run_type:\n            self._inference_run(""val"")\n\n        if any(rt in self.run_type for rt in [""inference"", ""test"", ""predict""]):\n            self._inference_run(""test"")\n\n    def _inference_run(self, dataset_type):\n        if self.config.evaluation.predict:\n            self.predict(dataset_type)\n            return\n\n        self.writer.write(f""Starting inference on {dataset_type} set"")\n\n        report, meter = self.evaluate(\n            getattr(self, f""{dataset_type}_loader""), use_tqdm=True\n        )\n        prefix = f""{report.dataset_name}: full {dataset_type}""\n        self._summarize_report(meter, prefix)\n\n    def _calculate_time_left(self):\n        time_taken_for_log = time.time() * 1000 - self.train_timer.start\n        iterations_left = self.max_updates - self.num_updates\n        num_logs_left = iterations_left / self.log_interval\n        time_left = num_logs_left * time_taken_for_log\n\n        snapshot_iteration = self.snapshot_iterations / self.log_interval\n        snapshot_iteration *= iterations_left / self.evaluation_interval\n        time_left += snapshot_iteration * time_taken_for_log\n\n        return self.train_timer.get_time_hhmmss(gap=time_left)\n\n    def profile(self, text):\n        if self.not_debug:\n            return\n        self.writer.write(text + "": "" + self.profiler.get_time_since_start(), ""debug"")\n        self.profiler.reset()\n\n    def predict(self, dataset_type):\n        reporter = self.dataset_loader.get_test_reporter(dataset_type)\n        with torch.no_grad():\n            self.model.eval()\n            message = f""Starting {dataset_type} inference predictions""\n            self.writer.write(message)\n\n            while reporter.next_dataset():\n                dataloader = reporter.get_dataloader()\n\n                for batch in tqdm(dataloader):\n                    prepared_batch = reporter.prepare_batch(batch)\n                    model_output = self.model(prepared_batch)\n                    report = Report(prepared_batch, model_output)\n                    reporter.add_to_report(report, self.model)\n\n            self.writer.write(""Finished predicting"")\n            self.model.train()\n'"
mmf/utils/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/utils/build.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport os\nimport warnings\nfrom typing import Any, Type\n\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom mmf.common import typings as mmf_typings\nfrom mmf.common.registry import registry\nfrom mmf.utils.configuration import Configuration\nfrom mmf.utils.general import get_optimizer_parameters\n\n\ndef build_config(\n    configuration: Type[Configuration], *args, **kwargs\n) -> mmf_typings.DictConfig:\n    """"""Builder function for config. Freezes the configuration and registers \n    configuration object and config DictConfig object to registry.\n\n    Args:\n        configuration (Configuration): Configuration object that will be \n            used to create the config.\n\n    Returns:\n        (DictConfig): A config which is of type Omegaconf.DictConfig\n    """"""\n    configuration.freeze()\n    config = configuration.get_config()\n    registry.register(""config"", config)\n    registry.register(""configuration"", configuration)\n\n    return config\n\n\ndef build_trainer(config: mmf_typings.DictConfig) -> Any:\n    """"""Builder function for creating a trainer class. Trainer class name\n    is picked from the config.\n\n    Args:\n        config (DictConfig): Configuration that will be used to create\n            the trainer.\n\n    Returns:\n        (BaseTrainer): A trainer instance\n    """"""\n    trainer_type = config.training.trainer\n    trainer_cls = registry.get_trainer_class(trainer_type)\n    trainer_obj = trainer_cls(config)\n\n    return trainer_obj\n\n\ndef build_model(config):\n    model_name = config.model\n\n    model_class = registry.get_model_class(model_name)\n\n    if model_class is None:\n        registry.get(""writer"").write(""No model registered for name: %s"" % model_name)\n    model = model_class(config)\n\n    if hasattr(model, ""build""):\n        model.load_requirements()\n        model.build()\n        model.init_losses()\n\n    return model\n\n\ndef build_dataset(\n    dataset_key: str, config=None, dataset_type=""train""\n) -> mmf_typings.DatasetType:\n    """"""Builder function for creating a dataset. If dataset_key is passed\n    the dataset is created from default config of the dataset and thus is\n    disable config even if it is passed. Otherwise, we use MultiDatasetLoader to\n    build and return an instance of dataset based on the config\n\n    Args:\n        dataset_key (str): Key of dataset to build.\n        config (DictConfig, optional): Configuration that will be used to create\n            the dataset. If not passed, dataset\'s default config will be used.\n            Defaults to {}.\n        dataset_type (str, optional): Type of the dataset to build, train|val|test.\n            Defaults to ""train"".\n\n    Returns:\n        (DatasetType): A dataset instance of type BaseDataset\n    """"""\n    from mmf.utils.configuration import load_yaml_with_defaults\n\n    dataset_builder = registry.get_builder_class(dataset_key)\n    assert dataset_builder, (\n        f""Key {dataset_key} doesn\'t have a registered "" + ""dataset builder""\n    )\n\n    # If config is not provided, we take it from default one\n    if not config:\n        config = load_yaml_with_defaults(dataset_builder.config_path())\n        config = OmegaConf.select(config, f""dataset_config.{dataset_key}"")\n        OmegaConf.set_struct(config, True)\n\n    builder_instance: mmf_typings.DatasetBuilderType = dataset_builder()\n    builder_instance.build_dataset(config, dataset_type)\n    dataset = builder_instance.load_dataset(config, dataset_type)\n    builder_instance.update_registry_for_model(config)\n\n    return dataset\n\n\ndef build_dataloader_and_sampler(\n    dataset_instance: mmf_typings.DatasetType, training_config: mmf_typings.DictConfig\n) -> mmf_typings.DataLoaderAndSampler:\n    """"""Builds and returns a dataloader along with its sample\n\n    Args:\n        dataset_instance (mmf_typings.DatasetType): Instance of dataset for which\n            dataloader has to be created\n        training_config (mmf_typings.DictConfig): Training configuration; required\n            for infering params for dataloader\n\n    Returns:\n        mmf_typings.DataLoaderAndSampler: Tuple of Dataloader and Sampler instance\n    """"""\n    from mmf.common.batch_collator import BatchCollator\n\n    num_workers = training_config.num_workers\n    pin_memory = training_config.pin_memory\n\n    other_args = {}\n\n    other_args = _add_extra_args_for_dataloader(dataset_instance, other_args)\n\n    loader = torch.utils.data.DataLoader(\n        dataset=dataset_instance,\n        pin_memory=pin_memory,\n        collate_fn=BatchCollator(\n            dataset_instance.dataset_name, dataset_instance.dataset_type\n        ),\n        num_workers=num_workers,\n        **other_args,\n    )\n\n    if num_workers >= 0:\n        # Suppress leaking semaphore warning\n        os.environ[""PYTHONWARNINGS""] = ""ignore:semaphore_tracker:UserWarning""\n\n    loader.dataset_type = dataset_instance.dataset_type\n\n    return loader, other_args.get(""sampler"", None)\n\n\ndef _add_extra_args_for_dataloader(\n    dataset_instance: mmf_typings.DatasetType,\n    other_args: mmf_typings.DataLoaderArgsType = None,\n) -> mmf_typings.DataLoaderArgsType:\n    from mmf.utils.general import get_batch_size\n\n    if other_args is None:\n        other_args = {}\n    dataset_type = dataset_instance.dataset_type\n\n    other_args[""shuffle""] = False\n    if dataset_type != ""test"":\n        other_args[""shuffle""] = True\n\n    # In distributed mode, we use DistributedSampler from PyTorch\n    if torch.distributed.is_initialized():\n        other_args[""sampler""] = torch.utils.data.DistributedSampler(\n            dataset_instance, shuffle=other_args[""shuffle""]\n        )\n        # Shuffle is mutually exclusive with sampler, let DistributedSampler\n        # take care of shuffle and pop from main args\n        other_args.pop(""shuffle"")\n\n    other_args[""batch_size""] = get_batch_size()\n\n    return other_args\n\n\ndef build_optimizer(model, config):\n    optimizer_config = config.optimizer\n    if not hasattr(optimizer_config, ""type""):\n        raise ValueError(\n            ""Optimizer attributes must have a \'type\' key ""\n            ""specifying the type of optimizer. ""\n            ""(Custom or PyTorch)""\n        )\n    optimizer_type = optimizer_config.type\n\n    if not hasattr(optimizer_config, ""params""):\n        warnings.warn(""optimizer attributes has no params defined, defaulting to {}."")\n\n    params = getattr(optimizer_config, ""params"", {})\n\n    if hasattr(torch.optim, optimizer_type):\n        optimizer_class = getattr(torch.optim, optimizer_type)\n    else:\n        optimizer_class = registry.get_optimizer_class(optimizer_type)\n        if optimizer_class is None:\n            raise ValueError(\n                ""No optimizer class of type {} present in ""\n                ""either torch or registered to registry""\n            )\n\n    parameters = get_optimizer_parameters(model, config)\n    optimizer = optimizer_class(parameters, **params)\n    return optimizer\n\n\ndef build_scheduler(optimizer, config):\n    scheduler_config = config.get(""scheduler"", {})\n\n    if not hasattr(scheduler_config, ""type""):\n        warnings.warn(\n            ""No type for scheduler specified even though lr_scheduler is True, ""\n            ""setting default to \'Pythia\'""\n        )\n    scheduler_type = getattr(scheduler_config, ""type"", ""pythia"")\n\n    if not hasattr(scheduler_config, ""params""):\n        warnings.warn(""scheduler attributes has no params defined, defaulting to {}."")\n    params = getattr(scheduler_config, ""params"", {})\n    scheduler_class = registry.get_scheduler_class(scheduler_type)\n    scheduler = scheduler_class(optimizer, **params)\n\n    return scheduler\n\n\ndef build_classifier_layer(config, *args, **kwargs):\n    from mmf.modules.layers import ClassifierLayer\n\n    classifier = ClassifierLayer(config.type, *args, **config.params, **kwargs)\n    return classifier.module\n\n\ndef build_text_encoder(config, *args, **kwargs):\n    from mmf.modules.encoders import TextEncoder\n\n    text_encoder = TextEncoder(config, *args, **kwargs)\n    return text_encoder.module\n\n\ndef build_image_encoder(config, direct_features=False, **kwargs):\n    from mmf.modules.encoders import ImageFeatureEncoder, ImageEncoder\n\n    if direct_features:\n        module = ImageFeatureEncoder(config.type, **config.params)\n    else:\n        module = ImageEncoder(config)\n    return module.module\n\n\ndef build_processors(\n    processors_config: mmf_typings.DictConfig, registry_key: str = None, *args, **kwargs\n) -> mmf_typings.ProcessorDict:\n    """"""Given a processor config, builds the processors present and returns back\n    a dict containing processors mapped to keys as per the config\n\n    Args:\n        processors_config (mmf_typings.DictConfig): OmegaConf DictConfig describing\n            the parameters and type of each processor passed here\n\n        registry_key (str, optional): If passed, function would look into registry for\n            this particular key and return it back. .format with processor_key will\n            be called on this string. Defaults to None.\n\n    Returns:\n        mmf_typings.ProcessorDict: Dictionary containing key to\n            processor mapping\n    """"""\n    from mmf.datasets.processors.processors import Processor\n\n    processor_dict = {}\n\n    for processor_key, processor_params in processors_config.items():\n        if not processor_params:\n            continue\n\n        processor_instance = None\n        if registry_key is not None:\n            full_key = registry_key.format(processor_key)\n            processor_instance = registry.get(full_key, no_warning=True)\n\n        if processor_instance is None:\n            processor_instance = Processor(processor_params, *args, **kwargs)\n            # We don\'t register back here as in case of hub interface, we\n            # want the processors to be instantiate every time. BaseDataset\n            # can register at its own end\n        processor_dict[processor_key] = processor_instance\n\n    return processor_dict\n'"
mmf/utils/checkpoint.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport glob\nimport importlib\nimport os\nimport sys\nimport warnings\n\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.configuration import get_mmf_env, load_yaml\nfrom mmf.utils.distributed import is_master, synchronize\nfrom mmf.utils.download import download_pretrained_model\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import updir\n\ntry:\n    import git\nexcept ImportError:\n    git = None\n\n\ndef _hack_imports():\n    # NOTE: This can probably be made universal to support backwards\n    # compatibility with name ""pythia"" if needed.\n    sys.modules[""pythia""] = importlib.import_module(""mmf"")\n    sys.modules[""pythia.utils.configuration""] = importlib.import_module(\n        ""mmf.utils.configuration""\n    )\n\n\ndef load_pretrained_model(model_name_or_path, *args, **kwargs):\n    # If this is a file, then load this directly else download and load\n    if PathManager.exists(model_name_or_path):\n        download_path = model_name_or_path\n        model_name = model_name_or_path\n    else:\n        download_path = download_pretrained_model(model_name_or_path, *args, **kwargs)\n        model_name = model_name_or_path\n\n    configs = glob.glob(os.path.join(download_path, ""*.yaml""))\n    assert len(configs) <= 1, (\n        ""Multiple yaml files with the pretrained model. ""\n        + ""MMF doesn\'t know what to do.""\n    )\n\n    ckpts = []\n    allowed_ckpt_types = (""*.ckpt"", ""*.pth"", ""*.pt"")\n    for ckpt_type in allowed_ckpt_types:\n        ckpts.extend(glob.glob(os.path.join(download_path, ckpt_type)))\n\n    assert (\n        len(ckpts) == 1\n    ), ""None or multiple checkpoints files. MMF doesn\'t know what to do.""\n\n    _hack_imports()\n\n    ckpt = torch.load(ckpts[0], map_location=lambda storage, loc: storage)\n    # If configs are not present, will ckpt provide the config?\n    if len(configs) == 0:\n        assert ""config"" in ckpt, (\n            ""No configs provided with pretrained model ""\n            "" while checkpoint also doesn\'t have configuration.""\n        )\n        config = ckpt[""config""]\n    else:\n        config = load_yaml(configs[0])\n\n    model_config = config.get(""model_config"", config)\n    ckpt = ckpt.get(""model"", ckpt)\n    # Also handle the case of model_name is path\n    model_config = model_config.get(model_name.split(os.path.sep)[-1].split(""."")[0])\n\n    return {""config"": model_config, ""checkpoint"": ckpt, ""full_config"": config}\n\n\nclass Checkpoint:\n    def __init__(self, trainer):\n        """"""\n        Generates a path for saving model which can also be used for resuming\n        from a checkpoint.\n        """"""\n        self.trainer = trainer\n\n        self.config = self.trainer.config\n        self.save_dir = get_mmf_env(key=""save_dir"")\n        self.model_name = self.config.model\n\n        self.ckpt_foldername = self.save_dir\n\n        self.device = registry.get(""current_device"")\n\n        self.ckpt_prefix = """"\n\n        if hasattr(self.trainer.model, ""get_ckpt_name""):\n            self.ckpt_prefix = self.trainer.model.get_ckpt_name() + ""_""\n\n        self.pth_filepath = os.path.join(\n            self.ckpt_foldername, self.ckpt_prefix + self.model_name + ""_final.pth""\n        )\n\n        self.models_foldername = os.path.join(self.ckpt_foldername, ""models"")\n        if not PathManager.exists(self.models_foldername):\n            PathManager.mkdirs(self.models_foldername)\n\n        self.save_config()\n\n        self.repo_path = updir(os.path.abspath(__file__), n=3)\n        self.git_repo = None\n        if git and self.config.checkpoint.save_git_details:\n            try:\n                self.git_repo = git.Repo(self.repo_path)\n            except git.exc.InvalidGitRepositoryError:\n                # Not a git repo, don\'t do anything\n                pass\n\n    def save_config(self):\n        cfg_file = os.path.join(self.ckpt_foldername, ""config.yaml"")\n        with PathManager.open(cfg_file, ""w"") as f:\n            f.write(self.config.pretty(resolve=True))\n\n    def load_state_dict(self):\n        ckpt_config = self.config.checkpoint\n\n        suffix = ""best.ckpt"" if ckpt_config.resume_best else ""current.ckpt""\n        reverse_suffix = ""best.ckpt"" if not ckpt_config.resume_best else ""current.ckpt""\n        ckpt_filepath = os.path.join(self.ckpt_foldername, self.ckpt_prefix + suffix)\n\n        # In case of interrupts and resume, ckpt_config.resume_file would be there\n        # But, if the checkpoints are already created in the save dir\n        # and resume is true signifying the interrupt resume, we should skip\n        # loading the resume file.\n        if (\n            ckpt_config.resume_file is not None or ckpt_config.resume_zoo is not None\n        ) and (not ckpt_config.resume or not PathManager.exists(ckpt_filepath)):\n            if ckpt_config.resume_file and PathManager.exists(ckpt_config.resume_file):\n                self._load(\n                    ckpt_config.resume_file,\n                    load_pretrained=ckpt_config.resume_pretrained,\n                )\n                return\n            # resume_file doesn\'t exist, try from zoo now\n            elif ckpt_config.resume_zoo is not None:\n                self._load(\n                    ckpt_config.resume_zoo,\n                    load_zoo=True,\n                    load_pretrained=ckpt_config.resume_pretrained,\n                )\n            else:\n                raise RuntimeError(f""{ckpt_config.resume_file} doesn\'t exist"")\n\n        if ckpt_config.resume is True:\n            if PathManager.exists(ckpt_filepath):\n                self._load(ckpt_filepath)\n            else:\n                warnings.warn(\n                    ""Tried to resume but checkpoint filepath {} ""\n                    ""is not present. Trying {}, otherwise skipping."".format(\n                        ckpt_filepath, reverse_suffix\n                    )\n                )\n                ckpt_filepath = ckpt_filepath.replace(suffix, reverse_suffix)\n                if PathManager.exists(ckpt_filepath):\n                    self._load(ckpt_filepath)\n\n    def _load(self, file, force=False, load_zoo=False, load_pretrained=False):\n        ckpt_config = self.config.checkpoint\n        self.trainer.writer.write(""Loading checkpoint"")\n        if load_zoo:\n            ckpt, should_continue = self._load_from_zoo(file)\n            if not should_continue:\n                return\n        else:\n            ckpt = self._torch_load(file)\n\n        if ""model"" in ckpt:\n            ckpt_model = ckpt[""model""]\n        else:\n            ckpt_model = ckpt\n            ckpt = {""model"": ckpt}\n\n        pretrained_state_mapping = ckpt_config.pretrained_state_mapping\n\n        if not load_pretrained or force is True:\n            pretrained_state_mapping = {}\n\n        new_dict = {}\n\n        new_dict = self.upgrade_state_dict(ckpt_model)\n\n        if len(pretrained_state_mapping.items()) == 0:\n            final_dict = new_dict\n\n            self.trainer.model.load_state_dict(final_dict, strict=False)\n\n            reset_optimizer = ckpt_config.reset.optimizer or ckpt_config.reset.all\n            if not reset_optimizer:\n                self._load_optimizer(ckpt)\n\n            self.trainer.early_stopping.init_from_checkpoint(ckpt)\n\n            self.trainer.writer.write(""Checkpoint loaded"")\n\n            reset_counts = ckpt_config.reset.all or ckpt_config.reset.counts\n\n            if not reset_counts:\n                self._load_counts(ckpt)\n        else:\n            self._load_pretrained(new_dict)\n\n    def _load_optimizer(self, ckpt):\n        if ""optimizer"" in ckpt:\n            try:\n                self.trainer.optimizer.load_state_dict(ckpt[""optimizer""])\n            except ValueError:\n                self.trainer.writer.write(\n                    ""Optimizer failed to load. Try with ""\n                    + ""checkpoint.reset.optimizer=True""\n                )\n                raise\n        else:\n            warnings.warn(\n                ""\'optimizer\' key is not present in the ""\n                ""checkpoint asked to be loaded. Skipping.""\n            )\n\n    def _load_counts(self, ckpt):\n        ckpt_config = self.trainer.config.checkpoint\n        if ""best_update"" in ckpt:\n            if ckpt_config.resume_best:\n                self.trainer.num_updates = ckpt.get(\n                    ""best_update"", self.trainer.num_updates\n                )\n                self.trainer.current_iteration = ckpt.get(\n                    ""best_iteration"", self.trainer.current_iteration\n                )\n            else:\n                self.trainer.num_updates = ckpt.get(\n                    ""num_updates"", self.trainer.num_updates\n                )\n                self.trainer.current_iteration = ckpt.get(\n                    ""current_iteration"", self.trainer.current_iteration\n                )\n\n            self.trainer.current_epoch = ckpt.get(\n                ""current_epoch"", self.trainer.current_epoch\n            )\n        elif ""best_iteration"" in ckpt:\n            # Preserve old behavior for old checkpoints where we always\n            # load best iteration\n            if ckpt_config.resume_best and ""current_iteration"" in ckpt:\n                self.trainer.current_iteration = ckpt[""current_iteration""]\n            else:\n                self.trainer.current_iteration = ckpt.get(\n                    ""best_iteration"", self.trainer.current_iteration\n                )\n\n            self.trainer.num_updates = self.trainer.current_iteration\n\n        registry.register(""current_iteration"", self.trainer.current_iteration)\n        registry.register(""num_updates"", self.trainer.num_updates)\n\n        self.trainer.current_epoch = ckpt.get(""best_epoch"", self.trainer.current_epoch)\n        registry.register(""current_epoch"", self.trainer.current_epoch)\n\n    def _load_pretrained(self, ckpt):\n        model = self.trainer.model\n        own_state = model.state_dict()\n        mapping = self.trainer.config.checkpoint.pretrained_state_mapping\n        for key, value in mapping.items():\n            key += "".""\n            value += "".""\n            for attr in ckpt:\n                for own_attr in own_state:\n                    if hasattr(model, ""format_state_key""):\n                        formatted_attr = model.format_state_key(attr)\n                    else:\n                        formatted_attr = attr\n                    if (\n                        key in own_attr\n                        and value in formatted_attr\n                        and own_attr.replace(key, """")\n                        == formatted_attr.replace(value, """")\n                    ):\n                        self.trainer.writer.write(\n                            ""Copying "" + own_attr + "" from "" + attr\n                        )\n                        own_state[own_attr].copy_(ckpt[attr])\n        self.trainer.writer.write(""Pretrained model loaded"")\n\n    def upgrade_state_dict(self, state_dict):\n        data_parallel = registry.get(""data_parallel"") or registry.get(""distributed"")\n        data_parallel = data_parallel or isinstance(\n            self.trainer.model,\n            (torch.nn.DataParallel, torch.nn.parallel.DistributedDataParallel),\n        )\n        new_dict = {}\n        for attr in state_dict:\n            new_attr = attr\n\n            if not data_parallel and attr.startswith(""module.""):\n                # In case the ckpt was actually a data parallel model\n                # replace first module. from dataparallel with empty string\n                new_dict[new_attr.replace(""module."", """", 1)] = state_dict[attr]\n            elif data_parallel and not attr.startswith(""module.""):\n                new_dict[""module."" + new_attr] = state_dict[attr]\n            else:\n                new_dict[new_attr] = state_dict[attr]\n        return new_dict\n\n    def _load_from_zoo(self, file):\n        ckpt_config = self.trainer.config.checkpoint\n        zoo_ckpt = load_pretrained_model(file)\n\n        # If zoo_config_override, load the model directly using `from_pretrained`\n        if ckpt_config.zoo_config_override:\n            model_cls = registry.get_model_class(self.trainer.config.model)\n            self.trainer.model = model_cls.from_pretrained(ckpt_config.resume_zoo)\n            self.trainer.config.model_config = zoo_ckpt[""full_config""].model_config\n            return None, False\n        else:\n            return self.upgrade_state_dict(zoo_ckpt[""checkpoint""]), True\n\n    def _torch_load(self, file):\n        # Backwards compatibility to Pythia\n        _hack_imports()\n\n        if ""cuda"" in str(self.device):\n            return torch.load(file, map_location=self.device)\n        else:\n            return torch.load(file, map_location=lambda storage, loc: storage)\n\n    def _get_vcs_fields(self):\n        """"""Returns a dict with git fields of the current repository\n\n           To reproduce an experiment directly from a checkpoint\n\n           1) Export `config` key as a yaml\n           2) Clone repository and checkout at given commit on given branch\n           3) Any local change (diff) while running the experiment is stored\n              in the value with key `git/diff`, output the diff to a `path.diff`\n              file and apply the patch to the current state by simply\n\n                           `patch -p0 < path.diff`\n        """"""\n\n        return {\n            ""git/branch"": self.git_repo.active_branch.name,\n            ""git/commit_hash"": self.git_repo.head.commit.name_rev,\n            ""git/commit_author"": self.git_repo.head.commit.author.name,\n            ""git/commit_message"": self.git_repo.head.commit.message,\n            ""git/diff"": self.git_repo.git.diff(""--no-prefix""),\n        }\n\n    def save(self, update, iteration=None, update_best=False):\n        # Only save in main process\n        if not is_master():\n            return\n\n        if not iteration:\n            iteration = update\n\n        ckpt_filepath = os.path.join(self.models_foldername, ""model_%d.ckpt"" % update)\n        best_ckpt_filepath = os.path.join(\n            self.ckpt_foldername, self.ckpt_prefix + ""best.ckpt""\n        )\n        current_ckpt_filepath = os.path.join(\n            self.ckpt_foldername, self.ckpt_prefix + ""current.ckpt""\n        )\n\n        best_iteration = self.trainer.early_stopping.best_monitored_iteration\n        best_update = self.trainer.early_stopping.best_monitored_update\n        best_metric = self.trainer.early_stopping.best_monitored_value\n        model = self.trainer.model\n        data_parallel = registry.get(""data_parallel"") or registry.get(""distributed"")\n\n        if data_parallel is True:\n            model = model.module\n\n        ckpt = {\n            ""model"": model.state_dict(),\n            ""optimizer"": self.trainer.optimizer.state_dict(),\n            ""best_iteration"": best_iteration,\n            ""current_iteration"": iteration,\n            ""current_epoch"": self.trainer.current_epoch,\n            ""num_updates"": update,\n            ""best_update"": best_update,\n            ""best_metric_value"": best_metric,\n            # Convert to container to avoid any dependencies\n            ""config"": OmegaConf.to_container(self.config, resolve=True),\n        }\n\n        if self.git_repo:\n            git_metadata_dict = self._get_vcs_fields()\n            ckpt.update(git_metadata_dict)\n\n        torch.save(ckpt, ckpt_filepath)\n\n        if update_best:\n            torch.save(ckpt, best_ckpt_filepath)\n\n        # Save current always\n        torch.save(ckpt, current_ckpt_filepath)\n\n    def restore(self):\n        synchronize()\n        self.trainer.writer.write(""Restoring checkpoint"")\n        best_path = os.path.join(self.ckpt_foldername, self.ckpt_prefix + ""best.ckpt"")\n\n        if PathManager.exists(best_path):\n            self._load(best_path, force=True)\n\n    def finalize(self):\n        if is_master():\n            torch.save(self.trainer.model.state_dict(), self.pth_filepath)\n'"
mmf/utils/configuration.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport collections\nimport json\nimport os\nimport warnings\nfrom ast import literal_eval\n\nimport demjson\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.env import import_user_module\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_absolute_path, get_mmf_root\n\n\ndef load_yaml(f):\n    # Convert to absolute path for loading includes\n    abs_f = get_absolute_path(f)\n\n    try:\n        mapping = OmegaConf.load(abs_f)\n        f = abs_f\n    except FileNotFoundError as e:\n        # Check if this file might be relative to root?\n        # TODO: Later test if this can be removed\n        relative = os.path.abspath(os.path.join(get_mmf_root(), f))\n        if not PathManager.isfile(relative):\n            raise e\n        else:\n            f = relative\n            mapping = OmegaConf.load(f)\n\n    if mapping is None:\n        mapping = OmegaConf.create()\n\n    includes = mapping.get(""includes"", [])\n\n    if not isinstance(includes, collections.abc.Sequence):\n        raise AttributeError(\n            ""Includes must be a list, {} provided"".format(type(includes))\n        )\n\n    include_mapping = OmegaConf.create()\n\n    mmf_root_dir = get_mmf_root()\n\n    for include in includes:\n        original_include_path = include\n        include = os.path.join(mmf_root_dir, include)\n\n        # If path doesn\'t exist relative to MMF root, try relative to current file\n        if not PathManager.exists(include):\n            include = os.path.join(os.path.dirname(f), original_include_path)\n\n        current_include_mapping = load_yaml(include)\n        include_mapping = OmegaConf.merge(include_mapping, current_include_mapping)\n\n    mapping.pop(""includes"", None)\n\n    mapping = OmegaConf.merge(include_mapping, mapping)\n\n    return mapping\n\n\ndef get_default_config_path():\n    directory = os.path.dirname(os.path.abspath(__file__))\n    return os.path.join(directory, "".."", ""configs"", ""defaults.yaml"")\n\n\ndef load_yaml_with_defaults(f):\n    default_config = get_default_config_path()\n    return OmegaConf.merge(load_yaml(default_config), load_yaml(f))\n\n\ndef get_zoo_config(\n    key, variation=""defaults"", zoo_config_path=None, zoo_type=""datasets""\n):\n    version = None\n    resources = None\n    if zoo_config_path is None:\n        zoo_config_path = os.path.join(""configs"", ""zoo"", f""{zoo_type}.yaml"")\n    zoo = load_yaml(zoo_config_path)\n\n    # Set struct on zoo so that unidentified access is not allowed\n    OmegaConf.set_struct(zoo, True)\n\n    try:\n        item = OmegaConf.select(zoo, key)\n    except Exception:\n        # Key wasn\'t present or something else happened, return None, None\n        return version, resources\n\n    if not item:\n        return version, resources\n\n    if variation not in item:\n        # If variation is not present, then key value should\n        # be directly returned if ""defaults"" was selected as the variation\n        assert (\n            variation == ""defaults""\n        ), f""\'{variation}\' variation not present in zoo config""\n        return _get_version_and_resources(item)\n    elif ""resources"" in item:\n        # Case where full key is directly passed\n        return _get_version_and_resources(item)\n    else:\n        return _get_version_and_resources(item[variation])\n\n\ndef _get_version_and_resources(item):\n    assert ""version"" in item, ""\'version\' key should be present in zoo config {}"".format(\n        item._get_full_key("""")\n    )\n    assert (\n        ""resources"" in item\n    ), ""\'resources\' key should be present in zoo config {}"".format(\n        item._get_full_key("""")\n    )\n\n    return item.version, item.resources\n\n\ndef get_global_config(key=None):\n    config = registry.get(""config"")\n    if config is None:\n        configuration = Configuration()\n        config = configuration.get_config()\n        registry.register(""config"", config)\n\n    if key:\n        config = OmegaConf.select(config, key)\n\n    return config\n\n\ndef get_mmf_cache_dir():\n    config = get_global_config()\n    cache_dir = config.env.cache_dir\n    # If cache_dir path exists do not join to mmf root\n    if not os.path.exists(cache_dir):\n        cache_dir = os.path.join(get_mmf_root(), cache_dir)\n    return cache_dir\n\n\ndef get_mmf_env(key=None):\n    config = get_global_config()\n    if key:\n        return OmegaConf.select(config.env, key)\n    else:\n        return config.env\n\n\ndef resolve_cache_dir(env_variable=""MMF_CACHE_DIR"", default=""mmf""):\n    # Some of this follow what ""transformers"" does for there cache resolving\n    try:\n        from torch.hub import _get_torch_home\n\n        torch_cache_home = _get_torch_home()\n    except ImportError:\n        torch_cache_home = os.path.expanduser(\n            os.getenv(\n                ""TORCH_HOME"",\n                os.path.join(os.getenv(""XDG_CACHE_HOME"", ""~/.cache""), ""torch""),\n            )\n        )\n    default_cache_path = os.path.join(torch_cache_home, default)\n\n    cache_path = os.getenv(env_variable, default_cache_path)\n\n    if not PathManager.exists(cache_path):\n        try:\n            PathManager.mkdirs(cache_path)\n        except PermissionError:\n            cache_path = os.path.join(get_mmf_root(), "".mmf_cache"")\n            PathManager.mkdirs(cache_path)\n\n    return cache_path\n\n\ndef resolve_dir(env_variable, default=""data""):\n    default_dir = os.path.join(resolve_cache_dir(), default)\n    dir_path = os.getenv(env_variable, default_dir)\n\n    if not PathManager.exists(dir_path):\n        PathManager.mkdirs(dir_path)\n\n    return dir_path\n\n\nclass Configuration:\n    def __init__(self, args=None, default_only=False):\n        self.config = {}\n\n        if not args:\n            import argparse\n\n            args = argparse.Namespace(opts=[])\n            default_only = True\n\n        self.args = args\n        self._register_resolvers()\n\n        self._default_config = self._build_default_config()\n\n        if default_only:\n            other_configs = {}\n        else:\n            other_configs = self._build_other_configs()\n\n        self.config = OmegaConf.merge(self._default_config, other_configs)\n\n        self.config = self._merge_with_dotlist(self.config, args.opts)\n        self._update_specific(self.config)\n        self.upgrade(self.config)\n\n        registry.register(""config"", self.config)\n\n    def _build_default_config(self):\n        self.default_config_path = get_default_config_path()\n        default_config = load_yaml(self.default_config_path)\n        return default_config\n\n    def _build_other_configs(self):\n        opts_config = self._build_opt_list(self.args.opts)\n        user_config = self._build_user_config(opts_config)\n\n        self._opts_config = opts_config\n        self._user_config = user_config\n\n        self.import_user_dir()\n\n        model_config = self._build_model_config(opts_config)\n        dataset_config = self._build_dataset_config(opts_config)\n        args_overrides = self._build_demjson_config(self.args.config_override)\n        other_configs = OmegaConf.merge(\n            model_config, dataset_config, user_config, args_overrides\n        )\n\n        return other_configs\n\n    def _build_opt_list(self, opts):\n        opts_dot_list = self._convert_to_dot_list(opts)\n        return OmegaConf.from_dotlist(opts_dot_list)\n\n    def _build_user_config(self, opts):\n        user_config = {}\n\n        # Update user_config with opts if passed\n        self.config_path = opts.config\n        if self.config_path is not None:\n            user_config = load_yaml(self.config_path)\n\n        return user_config\n\n    def import_user_dir(self):\n        # Try user_dir options in order of MMF configuration hierarchy\n        # First try the default one, which can be set via environment as well\n        user_dir = self._default_config.env.user_dir\n\n        # Now, check user\'s config\n        user_config_user_dir = self._user_config.get(""env"", {}).get(""user_dir"", None)\n\n        if user_config_user_dir:\n            user_dir = user_config_user_dir\n\n        # Finally, check opts\n        opts_user_dir = self._opts_config.get(""env"", {}).get(""user_dir"", None)\n        if opts_user_dir:\n            user_dir = opts_user_dir\n\n        if user_dir:\n            import_user_module(user_dir)\n\n    def _build_model_config(self, config):\n        model = config.model\n        if model is None:\n            raise KeyError(""Required argument \'model\' not passed"")\n        model_cls = registry.get_model_class(model)\n\n        if model_cls is None:\n            warning = f""No model named \'{model}\' has been registered""\n            warnings.warn(warning)\n            return OmegaConf.create()\n\n        default_model_config_path = model_cls.config_path()\n\n        if default_model_config_path is None:\n            warning = ""Model {}\'s class has no default configuration provided"".format(\n                model\n            )\n            warnings.warn(warning)\n            return OmegaConf.create()\n\n        return load_yaml(default_model_config_path)\n\n    def _build_dataset_config(self, config):\n        dataset = config.dataset\n        datasets = config.datasets\n\n        if dataset is None and datasets is None:\n            raise KeyError(""Required argument \'dataset|datasets\' not passed"")\n\n        if datasets is None:\n            config.datasets = dataset\n            datasets = dataset.split("","")\n        else:\n            datasets = datasets.split("","")\n\n        dataset_config = OmegaConf.create()\n\n        for dataset in datasets:\n            builder_cls = registry.get_builder_class(dataset)\n\n            if builder_cls is None:\n                warning = f""No dataset named \'{dataset}\' has been registered""\n                warnings.warn(warning)\n                continue\n            default_dataset_config_path = builder_cls.config_path()\n\n            if default_dataset_config_path is None:\n                warning = (\n                    ""Dataset {}\'s builder class has no default configuration ""\n                    + f""provided""\n                )\n                warnings.warn(warning)\n                continue\n\n            dataset_config = OmegaConf.merge(\n                dataset_config, load_yaml(default_dataset_config_path)\n            )\n\n        return dataset_config\n\n    def get_config(self):\n        self._register_resolvers()\n        return self.config\n\n    def _build_demjson_config(self, demjson_string):\n        if demjson_string is None:\n            return OmegaConf.create()\n\n        demjson_dict = demjson.decode(demjson_string)\n        return OmegaConf.create(demjson_dict)\n\n    def _get_args_config(self, args):\n        args_dict = vars(args)\n        return OmegaConf.create(args_dict)\n\n    def _register_resolvers(self):\n        OmegaConf.clear_resolvers()\n        # Device count resolver\n        device_count = max(1, torch.cuda.device_count())\n        OmegaConf.register_resolver(""device_count"", lambda: device_count)\n        OmegaConf.register_resolver(""resolve_cache_dir"", resolve_cache_dir)\n        OmegaConf.register_resolver(""resolve_dir"", resolve_dir)\n\n    def _merge_with_dotlist(self, config, opts):\n        # TODO: To remove technical debt, a possible solution is to use\n        # struct mode to update with dotlist OmegaConf node. Look into this\n        # in next iteration\n        if opts is None:\n            opts = []\n\n        if len(opts) == 0:\n            return config\n\n        # Support equal e.g. model=visual_bert for better future hydra support\n        has_equal = opts[0].find(""="") != -1\n\n        if has_equal:\n            opt_values = [opt.split(""="") for opt in opts]\n        else:\n            assert len(opts) % 2 == 0, ""Number of opts should be multiple of 2""\n            opt_values = zip(opts[0::2], opts[1::2])\n\n        for opt, value in opt_values:\n            if opt == ""dataset"":\n                opt = ""datasets""\n\n            splits = opt.split(""."")\n            current = config\n            for idx, field in enumerate(splits):\n                array_index = -1\n                if field.find(""["") != -1 and field.find(""]"") != -1:\n                    stripped_field = field[: field.find(""["")]\n                    array_index = int(field[field.find(""["") + 1 : field.find(""]"")])\n                else:\n                    stripped_field = field\n                if stripped_field not in current:\n                    raise AttributeError(\n                        ""While updating configuration""\n                        "" option {} is missing from""\n                        "" configuration at field {}"".format(opt, stripped_field)\n                    )\n                if isinstance(current[stripped_field], collections.abc.Mapping):\n                    current = current[stripped_field]\n                elif (\n                    isinstance(current[stripped_field], collections.abc.Sequence)\n                    and array_index != -1\n                ):\n                    current_value = current[stripped_field][array_index]\n\n                    # Case where array element to be updated is last element\n                    if not isinstance(\n                        current_value,\n                        (collections.abc.Mapping, collections.abc.Sequence),\n                    ):\n                        print(f""Overriding option {opt} to {value}"")\n                        current[stripped_field][array_index] = self._decode_value(value)\n                    else:\n                        # Otherwise move on down the chain\n                        current = current_value\n                else:\n                    if idx == len(splits) - 1:\n                        print(f""Overriding option {opt} to {value}"")\n                        current[stripped_field] = self._decode_value(value)\n                    else:\n                        raise AttributeError(\n                            ""While updating configuration"",\n                            ""option {} is not present ""\n                            ""after field {}"".format(opt, stripped_field),\n                        )\n\n        return config\n\n    def _decode_value(self, value):\n        # https://github.com/rbgirshick/yacs/blob/master/yacs/config.py#L400\n        if not isinstance(value, str):\n            return value\n\n        if value == ""None"":\n            value = None\n\n        try:\n            value = literal_eval(value)\n        except ValueError:\n            pass\n        except SyntaxError:\n            pass\n        return value\n\n    def freeze(self):\n        OmegaConf.set_struct(self.config, True)\n\n    def defrost(self):\n        OmegaConf.set_struct(self.config, False)\n\n    def _convert_to_dot_list(self, opts):\n        if opts is None:\n            opts = []\n\n        if len(opts) == 0:\n            return opts\n\n        # Support equal e.g. model=visual_bert for better future hydra support\n        has_equal = opts[0].find(""="") != -1\n\n        if has_equal:\n            return opts\n\n        return [(opt + ""="" + value) for opt, value in zip(opts[0::2], opts[1::2])]\n\n    def pretty_print(self):\n        if not self.config.training.log_detailed_config:\n            return\n\n        self.writer = registry.get(""writer"")\n\n        self.writer.write(""=====  Training Parameters    ====="", ""info"")\n        self.writer.write(self._convert_node_to_json(self.config.training), ""info"")\n\n        self.writer.write(""======  Dataset Attributes  ======"", ""info"")\n        datasets = self.config.datasets.split("","")\n\n        for dataset in datasets:\n            if dataset in self.config.dataset_config:\n                self.writer.write(f""======== {dataset} ======="", ""info"")\n                dataset_config = self.config.dataset_config[dataset]\n                self.writer.write(self._convert_node_to_json(dataset_config), ""info"")\n            else:\n                self.writer.write(\n                    f""No dataset named \'{dataset}\' in config. Skipping"", ""warning""\n                )\n\n        self.writer.write(""======  Optimizer Attributes  ======"", ""info"")\n        self.writer.write(self._convert_node_to_json(self.config.optimizer), ""info"")\n\n        if self.config.model not in self.config.model_config:\n            raise ValueError(f""{self.config.model} not present in model attributes"")\n\n        self.writer.write(\n            f""======  Model ({self.config.model}) Attributes  ======"", ""info""\n        )\n        self.writer.write(\n            self._convert_node_to_json(self.config.model_config[self.config.model]),\n            ""info"",\n        )\n\n    def _convert_node_to_json(self, node):\n        container = OmegaConf.to_container(node, resolve=True)\n        return json.dumps(container, indent=4, sort_keys=True)\n\n    def _update_specific(self, config):\n        self.writer = registry.get(""writer"")\n        # tp = self.config.training\n\n        # if args[""seed""] is not None or tp[\'seed\'] is not None:\n        #     print(\n        #         ""You have chosen to seed the training. This will turn on CUDNN ""\n        #         ""deterministic setting which can slow down your training ""\n        #         ""considerably! You may see unexpected behavior when restarting ""\n        #         ""from checkpoints.""\n        #     )\n\n        # if args[""seed""] == -1:\n        #     self.config[""training""][""seed""] = random.randint(1, 1000000)\n\n        if config.learning_rate:\n            if ""optimizer"" in config and ""params"" in config.optimizer:\n                lr = config.learning_rate\n                config.optimizer.params.lr = lr\n\n        if not torch.cuda.is_available() and ""cuda"" in config.training.device:\n            warnings.warn(\n                ""Device specified is \'cuda\' but cuda is not present. ""\n                + ""Switching to CPU version.""\n            )\n            config.training.device = ""cpu""\n\n        return config\n\n    def upgrade(self, config):\n        mapping = {\n            ""training.resume_file"": ""checkpoint.resume_file"",\n            ""training.resume"": ""checkpoint.resume"",\n            ""training.resume_best"": ""checkpoint.resume_best"",\n            ""training.load_pretrained"": ""checkpoint.resume_pretrained"",\n            ""training.pretrained_state_mapping"": ""checkpoint.pretrained_state_mapping"",\n            ""training.run_type"": ""run_type"",\n        }\n\n        for old, new in mapping.items():\n            value = OmegaConf.select(config, old)\n            if value:\n                OmegaConf.update(config, new, value)\n\n\n# This is still here due to legacy reasons around\n# older checkpoint loading from v0.3\nclass ConfigNode(collections.OrderedDict):\n    pass\n'"
mmf/utils/dataset.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport warnings\n\nimport torch\n\nfrom mmf.common.sample import Sample\n\n\ndef build_bbox_tensors(infos, max_length):\n    num_bbox = min(max_length, len(infos))\n\n    # After num_bbox, everything else should be zero\n    coord_tensor = torch.zeros((max_length, 4), dtype=torch.float)\n    width_tensor = torch.zeros(max_length, dtype=torch.float)\n    height_tensor = torch.zeros(max_length, dtype=torch.float)\n    bbox_types = [""xyxy""] * max_length\n\n    infos = infos[:num_bbox]\n    sample = Sample()\n\n    for idx, info in enumerate(infos):\n        bbox = info[""bounding_box""]\n        x = bbox[""top_left_x""]\n        y = bbox[""top_left_y""]\n        width = bbox[""width""]\n        height = bbox[""height""]\n\n        coord_tensor[idx][0] = x\n        coord_tensor[idx][1] = y\n        coord_tensor[idx][2] = x + width\n        coord_tensor[idx][3] = y + height\n\n        width_tensor[idx] = width\n        height_tensor[idx] = height\n    sample.coordinates = coord_tensor\n    sample.width = width_tensor\n    sample.height = height_tensor\n    sample.bbox_types = bbox_types\n\n    return sample\n\n\ndef build_dataset_from_multiple_imdbs(config, dataset_cls, dataset_type):\n    from mmf.datasets.concat_dataset import MMFConcatDataset\n\n    if dataset_type not in config.imdb_files:\n        warnings.warn(\n            ""Dataset type {} is not present in ""\n            ""imdb_files of dataset config. Returning None. ""\n            ""This dataset won\'t be used."".format(dataset_type)\n        )\n        return None\n\n    imdb_files = config[""imdb_files""][dataset_type]\n\n    datasets = []\n\n    for imdb_idx in range(len(imdb_files)):\n        dataset = dataset_cls(dataset_type, imdb_idx, config)\n        datasets.append(dataset)\n\n    dataset = MMFConcatDataset(datasets)\n\n    return dataset\n'"
mmf/utils/distributed.py,12,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# Inspired from maskrcnn_benchmark, fairseq\nimport os\nimport pickle\nimport socket\nimport subprocess\nimport warnings\n\nimport torch\nfrom torch import distributed as dist\n\nMAX_SIZE_LIMIT = 65533\nBYTE_SIZE = 256\n\n\ndef synchronize():\n    if not dist.is_nccl_available():\n        return\n    if not dist.is_initialized():\n        return\n\n    world_size = dist.get_world_size()\n\n    if world_size == 1:\n        return\n\n    dist.barrier()\n\n\ndef get_rank():\n    if not dist.is_nccl_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_master():\n    return get_rank() == 0\n\n\ndef get_world_size():\n    if not dist.is_nccl_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef broadcast_tensor(tensor, src=0):\n    world_size = get_world_size()\n    if world_size < 2:\n        return tensor\n\n    with torch.no_grad():\n        dist.broadcast(tensor, src=0)\n\n    return tensor\n\n\ndef broadcast_scalar(scalar, src=0, device=""cpu""):\n    if get_world_size() < 2:\n        return scalar\n    scalar_tensor = torch.tensor(scalar).long().to(device)\n    scalar_tensor = broadcast_tensor(scalar_tensor, src)\n    return scalar_tensor.item()\n\n\ndef reduce_tensor(tensor):\n    world_size = get_world_size()\n\n    if world_size < 2:\n        return tensor\n\n    with torch.no_grad():\n        dist.reduce(tensor, dst=0)\n        if dist.get_rank() == 0:\n            tensor = tensor.div(world_size)\n\n    return tensor\n\n\ndef gather_tensor(tensor):\n    world_size = get_world_size()\n\n    if world_size < 2:\n        return tensor\n\n    with torch.no_grad():\n        tensor_list = []\n\n        for _ in range(world_size):\n            tensor_list.append(torch.zeros_like(tensor))\n\n        dist.all_gather(tensor_list, tensor)\n        tensor_list = torch.stack(tensor_list, dim=0)\n    return tensor_list\n\n\ndef reduce_dict(dictionary):\n    world_size = get_world_size()\n    if world_size < 2:\n        return dictionary\n\n    with torch.no_grad():\n        if len(dictionary) == 0:\n            return dictionary\n\n        keys, values = zip(*sorted(dictionary.items()))\n        values = torch.stack(values, dim=0)\n\n        dist.reduce(values, dst=0)\n\n        if dist.get_rank() == 0:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(keys, values)}\n    return reduced_dict\n\n\n# Object byte tensor utilities have been adopted from\n# https://github.com/pytorch/fairseq/blob/master/fairseq/distributed_utils.py\ndef object_to_byte_tensor(obj, max_size=4094):\n    """"""\n    Encode Python objects to PyTorch byte tensors\n    """"""\n    assert max_size <= MAX_SIZE_LIMIT\n    byte_tensor = torch.zeros(max_size, dtype=torch.uint8)\n\n    obj_enc = pickle.dumps(obj)\n    obj_size = len(obj_enc)\n    if obj_size > max_size:\n        raise Exception(\n            f""objects too large: object size {obj_size}, max size {max_size}""\n        )\n\n    byte_tensor[0] = obj_size // 256\n    byte_tensor[1] = obj_size % 256\n    byte_tensor[2 : 2 + obj_size] = torch.ByteTensor(list(obj_enc))\n    return byte_tensor\n\n\ndef byte_tensor_to_object(byte_tensor, max_size=4094):\n    """"""\n    Decode PyTorch byte tensors to Python objects\n    """"""\n    assert max_size <= MAX_SIZE_LIMIT\n\n    obj_size = byte_tensor[0].item() * 256 + byte_tensor[1].item()\n    obj_enc = bytes(byte_tensor[2 : 2 + obj_size].tolist())\n    obj = pickle.loads(obj_enc)\n    return obj\n\n\ndef infer_init_method(config):\n    if config.distributed.init_method is not None:\n        return\n    # support torch.distributed.launch\n    if all(\n        key in os.environ\n        for key in [""MASTER_ADDR"", ""MASTER_PORT"", ""WORLD_SIZE"", ""RANK""]\n    ):\n        config.distributed.init_method = ""env://""\n        config.distributed.world_size = int(os.environ[""WORLD_SIZE""])\n        config.distributed.rank = int(os.environ[""RANK""])\n        config.distributed.no_spawn = True\n\n    # we can determine the init method automatically for Slurm\n    elif config.distributed.port > 0:\n        node_list = os.environ.get(""SLURM_STEP_NODELIST"")\n        if node_list is None:\n            node_list = os.environ.get(""SLURM_JOB_NODELIST"")\n        if node_list is not None:\n            try:\n                hostnames = subprocess.check_output(\n                    [""scontrol"", ""show"", ""hostnames"", node_list]\n                )\n                config.distributed.init_method = ""tcp://{host}:{port}"".format(\n                    host=hostnames.split()[0].decode(""utf-8""),\n                    port=config.distributed.port,\n                )\n                nnodes = int(os.environ.get(""SLURM_NNODES""))\n                ntasks_per_node = os.environ.get(""SLURM_NTASKS_PER_NODE"")\n                if ntasks_per_node is not None:\n                    ntasks_per_node = int(ntasks_per_node)\n                else:\n                    ntasks = int(os.environ.get(""SLURM_NTASKS""))\n                    nnodes = int(os.environ.get(""SLURM_NNODES""))\n                    assert ntasks % nnodes == 0\n                    ntasks_per_node = int(ntasks / nnodes)\n                if ntasks_per_node == 1:\n                    assert config.distributed.world_size % nnodes == 0\n                    gpus_per_node = config.distributed.world_size // nnodes\n                    node_id = int(os.environ.get(""SLURM_NODEID""))\n                    config.distributed.rank = node_id * gpus_per_node\n                else:\n                    assert ntasks_per_node == config.distributed.world_size // nnodes\n                    config.distributed.no_spawn = True\n                    config.distributed.rank = int(os.environ.get(""SLURM_PROCID""))\n                    config.device_id = int(os.environ.get(""SLURM_LOCALID""))\n            except subprocess.CalledProcessError as e:  # scontrol failed\n                raise e\n            except FileNotFoundError:  # Slurm is not installed\n                pass\n\n\ndef distributed_init(config):\n    if config.distributed.world_size == 1:\n        raise ValueError(""Cannot initialize distributed with distributed_world_size=1"")\n\n    if dist.is_initialized():\n        warnings.warn(""Distributed is already initialized, cannot initialize twice!"")\n    else:\n        print(\n            ""Distributed Init (Rank {}): {}"".format(\n                config.distributed.rank, config.distributed.init_method\n            ),\n            flush=True,\n        )\n        dist.init_process_group(\n            backend=config.distributed.backend,\n            init_method=config.distributed.init_method,\n            world_size=config.distributed.world_size,\n            rank=config.distributed.rank,\n        )\n        print(\n            ""Initialized Host {} as Rank {}"".format(\n                socket.gethostname(), config.distributed.rank\n            ),\n            flush=True,\n        )\n\n        # perform a dummy all-reduce to initialize the NCCL communicator\n        dist.all_reduce(torch.zeros(1).cuda())\n\n        suppress_output(is_master())\n\n    config.distributed.rank = dist.get_rank()\n    return config.distributed.rank\n\n\ndef suppress_output(is_master):\n    """"""Suppress printing on the current device. Force printing with `force=True`.""""""\n    import builtins as __builtin__\n\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop(""force"", False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n    import warnings\n\n    builtin_warn = warnings.warn\n\n    def warn(*args, **kwargs):\n        force = kwargs.pop(""force"", False)\n        if is_master or force:\n            builtin_warn(*args, **kwargs)\n\n    # Log warnings only once\n    warnings.warn = warn\n    warnings.simplefilter(""once"", UserWarning)\n\n\n# def is_master(config=None):\n#     if config is None:\n#         from mmf.common.registry import registry\n#         config = registry.get(""configuration"").args\n\n#     return config.distributed.rank == 0\n'"
mmf/utils/download.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n# Original taken from ParlAI https://git.io/JvjfS, this file has been\n# adapted for MMF use cases.\n\n""""""\nUtilities for downloading and building data.\n\nThese can be replaced if your particular file system does not support them.\n""""""\nimport collections\nimport datetime\nimport hashlib\nimport json\nimport os\nimport shutil\nimport time\nfrom pathlib import Path\n\nimport requests\nimport tqdm\n\nfrom mmf.utils.distributed import is_master, synchronize\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_absolute_path\n\n\nclass DownloadableFile:\n    """"""\n    A class used to abstract any file that has to be downloaded online.\n\n    Originally taken from ParlAI, this file has been modified for MMF specific\n    use cases.\n\n    Any dataset/model that needs to download a file needs to have a list RESOURCES\n    that have objects of this class as elements.\n\n    The class automatically figures out if the file is from Google Drive.\n\n    This class provides the following functionality:\n\n    - Download a file from a URL / Google Drive\n    - Decompress the file if compressed\n    - Checksum for the downloaded file\n    - Send HEAD request to validate URL or Google Drive link\n    - If the file is present and checksum is same, it won\'t be redownloaded\n\n    Raises:\n        AssertionError: If while downloading checksum of the files fails.\n    """"""\n\n    GOOGLE_DRIVE_SUBSTR = ""drive.google""\n    MMF_PREFIX = ""mmf://""\n    MMF_PREFIX_REPLACEMENT = ""https://dl.fbaipublicfiles.com/mmf/data/""\n\n    def __init__(\n        self,\n        url,\n        file_name,\n        hashcode=None,\n        compressed=True,\n        delete_original=False,\n        dest_folder=None,\n    ):\n        """"""\n        An object of this class needs to be created with:\n\n        Args:\n            url (string): URL or Google Drive id to download from\n            file_name (string): File name that the file should be named\n            hashcode (string, optional): SHA256 hashcode of the downloaded file.\n                                         Defaults to None. Won\'t be checked if not\n                                         passed.\n            compressed (bool, optional): False if the file is not compressed.\n                                         Defaults to True.\n            delete_original (bool, optional): If compressed whether to delete original.\n                                              Defaults to False.\n            dest_folder (str, optional): Folder which will be appended to destination\n                path provided when downloading. Defaults to None.\n        """"""\n        self._url = self._parse_url(url)\n        self._file_name = file_name\n        self._hashcode = hashcode\n        self._compressed = compressed\n        self._from_google = self._url.find(self.GOOGLE_DRIVE_SUBSTR) != -1\n\n        if self._from_google:\n            assert ""id="" in self._url, ""Google Drive URL should have Google Drive ID""\n            self._url = self._url.split(""="")[-1]\n\n        self._delete_original = delete_original\n        self._dest_folder = dest_folder\n\n    def _parse_url(self, url):\n        if url.find(self.MMF_PREFIX) == -1:\n            return url\n        else:\n            return self.MMF_PREFIX_REPLACEMENT + url[len(self.MMF_PREFIX) :]\n\n    def checksum(self, download_path):\n        """"""\n        Checksum on a given file.\n\n        Args:\n            download_path (string): path to the downloaded file.\n        """"""\n        if self._hashcode is None:\n            print(f""[ Checksum not provided, skipping for {self._file_name}]"")\n            return\n\n        sha256_hash = hashlib.sha256()\n        destination = os.path.join(download_path, self._file_name)\n\n        if not PathManager.isfile(destination):\n            # File is not present, nothing to checksum\n            return\n\n        with PathManager.open(destination, ""rb"") as f:\n            print(f""[ Starting checksum for {self._file_name}]"")\n            for byte_block in iter(lambda: f.read(65536), b""""):\n                sha256_hash.update(byte_block)\n            if sha256_hash.hexdigest() != self._hashcode:\n                # remove_dir(download_path)\n                raise AssertionError(\n                    f""[ Checksum for {self._file_name} from \\n{self._url}\\n""\n                    ""does not match the expected checksum. Please try again. ]""\n                )\n            else:\n                print(f""[ Checksum successful for {self._file_name}]"")\n\n    def download_file(self, download_path):\n        downloaded = False\n        redownload = False\n\n        if self._dest_folder is not None:\n            download_path = str(Path(f""{download_path}/{self._dest_folder}""))\n            make_dir(download_path)\n\n        try:\n            self.checksum(download_path)\n        except AssertionError:\n            # File exists but checksum has changed. Will be redownloaded\n            print(""[ Checksum changed for {}. Redownloading"")\n            redownload = True\n\n        if self._from_google:\n            downloaded = download_from_google_drive(\n                self._url,\n                os.path.join(download_path, self._file_name),\n                redownload=redownload,\n            )\n        else:\n            downloaded = download(\n                self._url, download_path, self._file_name, redownload=redownload\n            )\n\n        # If download actually happened, then only checksum again and decompress\n        if downloaded:\n            self.checksum(download_path)\n\n            if self._compressed:\n                decompress(download_path, self._file_name, self._delete_original)\n\n\ndef built(path, version_string=None):\n    """"""\n    Check if \'.built\' flag has been set for that task.\n\n    If a version_string is provided, this has to match, or the version\n    is regarded as not built.\n\n    Version_string are generally the dataset version + the date the file was\n    last updated. If this doesn\'t match, dataset will be mark not built. This makes\n    sure that if we update our features or anything else features are updated\n    for the end user.\n    """"""\n    if version_string:\n        fname = os.path.join(path, "".built.json"")\n        if not PathManager.isfile(fname):\n            return False\n        else:\n            with PathManager.open(fname, ""r"") as read:\n                text = json.load(read)\n            return text.get(""version"", None) == version_string\n    else:\n        return PathManager.isfile(os.path.join(path, "".built.json""))\n\n\ndef mark_done(path, version_string=None):\n    """"""\n    Mark this path as prebuilt.\n\n    Marks the path as done by adding a \'.built\' file with the current timestamp\n    plus a version description string if specified.\n\n    Args:\n        path (str): The file path to mark as built\n        version_string (str): The version of this dataset\n    """"""\n    data = {}\n    data[""created_at""] = str(datetime.datetime.today())\n    data[""version""] = version_string\n    with PathManager.open(os.path.join(path, "".built.json""), ""w"") as f:\n        json.dump(data, f)\n\n\ndef download(url, path, fname, redownload=True, disable_tqdm=False):\n    """"""\n    Download file using `requests`.\n\n    If ``redownload`` is set to false, then will not download tar file again if it is\n    present (default ``True``).\n\n    Returns whether download actually happened or not\n    """"""\n    outfile = os.path.join(path, fname)\n    download = not PathManager.isfile(outfile) or redownload\n    retry = 5\n    exp_backoff = [2 ** r for r in reversed(range(retry))]\n\n    pbar = None\n    if download:\n        # First test if the link is actually downloadable\n        check_header(url)\n        if not disable_tqdm:\n            print(""[ Downloading: "" + url + "" to "" + outfile + "" ]"")\n        pbar = tqdm.tqdm(\n            unit=""B"", unit_scale=True, desc=f""Downloading {fname}"", disable=disable_tqdm\n        )\n\n    while download and retry >= 0:\n        resume_file = outfile + "".part""\n        resume = PathManager.isfile(resume_file)\n        if resume:\n            resume_pos = os.path.getsize(resume_file)\n            mode = ""ab""\n        else:\n            resume_pos = 0\n            mode = ""wb""\n        response = None\n\n        with requests.Session() as session:\n            try:\n                header = (\n                    {""Range"": ""bytes=%d-"" % resume_pos, ""Accept-Encoding"": ""identity""}\n                    if resume\n                    else {}\n                )\n                response = session.get(url, stream=True, timeout=5, headers=header)\n\n                # negative reply could be \'none\' or just missing\n                if resume and response.headers.get(""Accept-Ranges"", ""none"") == ""none"":\n                    resume_pos = 0\n                    mode = ""wb""\n\n                CHUNK_SIZE = 32768\n                total_size = int(response.headers.get(""Content-Length"", -1))\n                # server returns remaining size if resuming, so adjust total\n                total_size += resume_pos\n                pbar.total = total_size\n                done = resume_pos\n\n                with PathManager.open(resume_file, mode) as f:\n                    for chunk in response.iter_content(CHUNK_SIZE):\n                        if chunk:  # filter out keep-alive new chunks\n                            f.write(chunk)\n                        if total_size > 0:\n                            done += len(chunk)\n                            if total_size < done:\n                                # don\'t freak out if content-length was too small\n                                total_size = done\n                                pbar.total = total_size\n                            pbar.update(len(chunk))\n                    break\n            except (\n                requests.exceptions.ConnectionError,\n                requests.exceptions.ReadTimeout,\n            ):\n                retry -= 1\n                pbar.clear()\n                if retry >= 0:\n                    print(""Connection error, retrying. (%d retries left)"" % retry)\n                    time.sleep(exp_backoff[retry])\n                else:\n                    print(""Retried too many times, stopped retrying."")\n            finally:\n                if response:\n                    response.close()\n    if retry < 0:\n        raise RuntimeWarning(""Connection broken too many times. Stopped retrying."")\n\n    if download and retry > 0:\n        pbar.update(done - pbar.n)\n        if done < total_size:\n            raise RuntimeWarning(\n                ""Received less data than specified in ""\n                + ""Content-Length header for ""\n                + url\n                + "". There may be a download problem.""\n            )\n        move(resume_file, outfile)\n\n    if pbar:\n        pbar.close()\n\n    return download\n\n\ndef check_header(url, from_google=False):\n    """"""\n    Performs a HEAD request to check if the URL / Google Drive ID is live.\n    """"""\n    session = requests.Session()\n    if from_google:\n        URL = ""https://docs.google.com/uc?export=download""\n        response = session.head(URL, params={""id"": url}, stream=True)\n    else:\n        headers = {\n            ""User-Agent"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) ""\n            + ""AppleWebKit/537.36 (KHTML, like Gecko) ""\n            + ""Chrome/77.0.3865.90 Safari/537.36""\n        }\n        response = session.head(url, allow_redirects=True, headers=headers)\n    status = response.status_code\n    session.close()\n\n    assert status == 200, (\n        ""The url {} is broken. If this is not your own url,""\n        + "" please open up an issue on GitHub""\n    ).format(url)\n\n\ndef download_pretrained_model(model_name, *args, **kwargs):\n    import omegaconf\n    from omegaconf import OmegaConf\n\n    from mmf.utils.configuration import load_yaml, get_mmf_env\n\n    model_zoo = load_yaml(get_mmf_env(key=""model_zoo""))\n    OmegaConf.set_struct(model_zoo, True)\n    OmegaConf.set_readonly(model_zoo, True)\n\n    data_dir = get_absolute_path(get_mmf_env(""data_dir""))\n    model_data_dir = os.path.join(data_dir, ""models"")\n    download_path = os.path.join(model_data_dir, model_name)\n\n    try:\n        model_config = OmegaConf.select(model_zoo, model_name)\n    except omegaconf.errors.OmegaConfBaseException as e:\n        print(f""No such model name {model_name} defined in mmf zoo"")\n        raise e\n\n    if ""version"" not in model_config or ""resources"" not in model_config:\n        # Version and Resources are not present time to try the defaults\n        try:\n            model_config = model_config.defaults\n            download_path = os.path.join(model_data_dir, model_name + "".defaults"")\n        except omegaconf.errors.OmegaConfBaseException as e:\n            print(\n                f""Model name {model_name} doesn\'t specify \'resources\' and \'version\' ""\n                ""while no defaults have been provided""\n            )\n            raise e\n\n    # Download requirements if any specified by ""zoo_requirements"" field\n    # This can either be a list or a string\n    if ""zoo_requirements"" in model_config:\n        requirements = model_config.zoo_requirements\n        if isinstance(requirements, str):\n            requirements = [requirements]\n        for item in requirements:\n            download_pretrained_model(item, *args, **kwargs)\n\n    version = model_config.version\n    resources = model_config.resources\n\n    if is_master():\n        download_resources(resources, download_path, version)\n    synchronize()\n\n    return download_path\n\n\ndef download_resources(resources, download_path, version):\n    is_built = built(download_path, version_string=version)\n\n    if not is_built:\n        make_dir(download_path)\n\n        # Make it list if it isn\'t\n        if not isinstance(resources, collections.abc.Sequence):\n            resources = [resources]\n\n        if len(resources) == 0:\n            return\n\n        for resource in resources:\n            download_resource(resource, download_path)\n        mark_done(download_path, version_string=version)\n\n\ndef download_resource(resource, download_path):\n    if isinstance(resource, collections.abc.Mapping):\n        # Try building DownloadableFile class object from resource dict\n        resource = DownloadableFile(**resource)\n    assert isinstance(resource, DownloadableFile)\n    resource.download_file(download_path)\n\n\ndef make_dir(path):\n    """"""\n    Make the directory and any nonexistent parent directories (`mkdir -p`).\n    """"""\n    # the current working directory is a fine path\n    if path != """":\n        PathManager.mkdirs(path)\n\n\ndef move(path1, path2):\n    """"""\n    Rename the given file.\n    """"""\n    shutil.move(path1, path2)\n\n\ndef remove_dir(path):\n    """"""\n    Remove the given directory, if it exists.\n    """"""\n    shutil.rmtree(path, ignore_errors=True)\n\n\ndef decompress(path, fname, delete_original=True):\n    """"""\n    Unpack the given archive file to the same directory.\n\n    Args:\n        path(str): The folder containing the archive. Will contain the contents.\n        fname (str): The filename of the archive file.\n        delete_original (bool, optional): If true, the archive will be deleted\n                                          after extraction. Default to True.\n    """"""\n    print(""Unpacking "" + fname)\n    fullpath = os.path.join(path, fname)\n    shutil.unpack_archive(fullpath, path)\n    if delete_original:\n        os.remove(fullpath)\n\n\ndef _get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(""download_warning""):\n            return value\n    return None\n\n\ndef download_from_google_drive(gd_id, destination, redownload=True):\n    """"""\n    Use the requests package to download a file from Google Drive.\n    """"""\n    download = not PathManager.isfile(destination) or redownload\n\n    URL = ""https://docs.google.com/uc?export=download""\n\n    if not download:\n        return download\n    else:\n        # Check first if link is live\n        check_header(gd_id, from_google=True)\n\n    with requests.Session() as session:\n        response = session.get(URL, params={""id"": gd_id}, stream=True)\n        token = _get_confirm_token(response)\n\n        if token:\n            response.close()\n            params = {""id"": gd_id, ""confirm"": token}\n            response = session.get(URL, params=params, stream=True)\n\n        CHUNK_SIZE = 32768\n        with PathManager.open(destination, ""wb"") as f:\n            for chunk in response.iter_content(CHUNK_SIZE):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n        response.close()\n\n    return download\n'"
mmf/utils/early_stopping.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport numpy as np\nimport torch\n\nfrom mmf.utils.distributed import is_master\n\n\nclass EarlyStopping:\n    """"""\n    Provides early stopping functionality. Keeps track of an early stop criteria,\n    and if it doesn\'t improve over time restores last best performing\n    parameters.\n    """"""\n\n    def __init__(\n        self,\n        model,\n        checkpoint_instance,\n        early_stop_criteria=""total_loss"",\n        patience=1000,\n        minimize=False,\n        should_stop=True,\n    ):\n        self.minimize = minimize\n        self.patience = patience\n        self.model = model\n        self.checkpoint = checkpoint_instance\n        self.early_stop_criteria = early_stop_criteria\n\n        if ""val"" not in self.early_stop_criteria:\n            self.early_stop_criteria = f""val/{self.early_stop_criteria}""\n\n        self.best_monitored_value = -np.inf if not minimize else np.inf\n        self.best_monitored_iteration = 0\n        self.best_monitored_update = 0\n        self.should_stop = should_stop\n        self.activated = False\n        self.metric = self.early_stop_criteria\n\n    def __call__(self, update, iteration, meter):\n        """"""\n        Method to be called everytime you need to check whether to\n        early stop or not\n        Arguments:\n            update {number}: Current update number\n            iteration {number}: Current iteration number\n        Returns:\n            bool -- Tells whether early stopping occurred or not\n        """"""\n        if not is_master():\n            return False\n\n        value = meter.meters.get(self.early_stop_criteria, None)\n        if value is None:\n            raise ValueError(\n                ""Criteria used for early stopping ({}) is not ""\n                ""present in meter."".format(self.early_stop_criteria)\n            )\n\n        value = value.global_avg\n\n        if isinstance(value, torch.Tensor):\n            value = value.item()\n\n        if (self.minimize and value < self.best_monitored_value) or (\n            not self.minimize and value > self.best_monitored_value\n        ):\n            self.best_monitored_value = value\n            self.best_monitored_iteration = iteration\n            self.best_monitored_update = update\n            self.checkpoint.save(update, iteration, update_best=True)\n\n        elif self.best_monitored_update + self.patience < update:\n            self.activated = True\n            if self.should_stop is True:\n                self.checkpoint.restore()\n                self.checkpoint.finalize()\n                return True\n            else:\n                return False\n        else:\n            self.checkpoint.save(update, iteration, update_best=False)\n\n        return False\n\n    def is_activated(self):\n        return self.activated\n\n    def init_from_checkpoint(self, load):\n        if ""best_iteration"" in load:\n            self.best_monitored_iteration = load[""best_iteration""]\n\n        if ""best_metric_value"" in load:\n            self.best_monitored_value = load[""best_metric_value""]\n\n    def get_info(self):\n        return {\n            ""best_update"": self.best_monitored_update,\n            ""best_iteration"": self.best_monitored_iteration,\n            f""best_{self.metric}"": f""{self.best_monitored_value:.6f}"",\n        }\n'"
mmf/utils/env.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport glob\nimport importlib\nimport os\nimport random\nimport sys\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\n\nfrom mmf.utils.general import get_absolute_path\n\n\ndef set_seed(seed):\n    if seed:\n        if seed == -1:\n            # From detectron2\n            seed = (\n                os.getpid()\n                + int(datetime.now().strftime(""%S%f""))\n                + int.from_bytes(os.urandom(2), ""big"")\n            )\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        random.seed(seed)\n\n    return seed\n\n\ndef import_user_module(user_dir: str, no_print: bool = False):\n    """"""Given a user dir, this function imports it as a module.\n\n    This user_module is expected to have an __init__.py at its root.\n    You can use import_files to import your python files easily in\n    __init__.py\n\n    Args:\n        user_dir (str): directory which has to be imported\n        no_print (bool): This function won\'t print anything if set to true\n    """"""\n    if user_dir:\n        user_dir = get_absolute_path(user_dir)\n        module_parent, module_name = os.path.split(user_dir)\n\n        if module_name not in sys.modules:\n            sys.path.insert(0, module_parent)\n            if not no_print:\n                print(f""Importing user_dir from {user_dir}"")\n            importlib.import_module(module_name)\n            sys.path.pop(0)\n\n\ndef import_files(file_path: str, module_name: str = None):\n    """"""The function imports all of the files present in file_path\'s directory.\n    This is useful for end user in case they want to easily import files without\n    mentioning each of them in their __init__.py. module_name if specified\n    is the full path to module under which all modules will be imported.\n\n    my_project/\n        my_models/\n            my_model.py\n            __init__.py\n\n    Contents of __init__.py\n\n    ```\n    from mmf.utils.env import import_files\n\n    import_files(__file__, ""my_project.my_models"")\n    ```\n\n    This will then allow you to import `my_project.my_models.my_model` anywhere.\n\n    Args:\n        file_path (str): Path to file in whose directory everything will be imported\n        module_name (str): Module name if this file under some specified structure\n    """"""\n    for file in os.listdir(os.path.dirname(file_path)):\n        if file.endswith("".py"") and not file.startswith(""_""):\n            import_name = file[: file.find("".py"")]\n            if module_name:\n                importlib.import_module(f""{module_name}.{import_name}"")\n            else:\n                importlib.import_module(f""{import_name}"")\n\n\ndef setup_imports():\n    from mmf.common.registry import registry\n\n    # First, check if imports are already setup\n    has_already_setup = registry.get(""imports_setup"", no_warning=True)\n    if has_already_setup:\n        return\n    # Automatically load all of the modules, so that\n    # they register with registry\n    root_folder = registry.get(""mmf_root"", no_warning=True)\n\n    if root_folder is None:\n        root_folder = os.path.dirname(os.path.abspath(__file__))\n        root_folder = os.path.join(root_folder, "".."")\n\n        environment_pythia_path = os.environ.get(""PYTHIA_PATH"")\n\n        if environment_pythia_path is not None:\n            root_folder = environment_pythia_path\n\n        registry.register(""pythia_path"", root_folder)\n\n    trainer_folder = os.path.join(root_folder, ""trainers"")\n    trainer_pattern = os.path.join(trainer_folder, ""**"", ""*.py"")\n    datasets_folder = os.path.join(root_folder, ""datasets"")\n    datasets_pattern = os.path.join(datasets_folder, ""**"", ""*.py"")\n    model_folder = os.path.join(root_folder, ""models"")\n    model_pattern = os.path.join(model_folder, ""**"", ""*.py"")\n\n    importlib.import_module(""mmf.common.meter"")\n\n    files = (\n        glob.glob(datasets_pattern, recursive=True)\n        + glob.glob(model_pattern, recursive=True)\n        + glob.glob(trainer_pattern, recursive=True)\n    )\n\n    for f in files:\n        if f.find(""models"") != -1:\n            splits = f.split(os.sep)\n            file_name = splits[-1]\n            module_name = file_name[: file_name.find("".py"")]\n            importlib.import_module(""mmf.models."" + module_name)\n        elif f.find(""trainer"") != -1:\n            splits = f.split(os.sep)\n            file_name = splits[-1]\n            module_name = file_name[: file_name.find("".py"")]\n            importlib.import_module(""mmf.trainers."" + module_name)\n        elif f.endswith(""builder.py""):\n            splits = f.split(os.sep)\n            folder_name = splits[-3]\n            dataset_name = splits[-2]\n            if folder_name == ""datasets"" or dataset_name == ""datasets"":\n                continue\n            file_name = splits[-1]\n            module_name = file_name[: file_name.find("".py"")]\n            importlib.import_module(\n                ""mmf.datasets."" + folder_name + ""."" + dataset_name + ""."" + module_name\n            )\n\n    registry.register(""imports_setup"", True)\n'"
mmf/utils/file_io.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport os\nimport shutil\nfrom typing import List, Optional\n\ntry:\n    from fvcore.common.file_io import PathManager as FVCorePathManager\nexcept ImportError:\n    FVCorePathManager = None\n\n\nclass PathManager:\n    """"""\n    Wrapper for insulating OSS I/O (using Python builtin operations) from\n    fvcore\'s PathManager abstraction (for transparently handling various\n    internal backends). This is adapted from\n    `fairseq <https://github.com/pytorch/fairseq/blob/master/fairseq/file_io.py>`.\n    """"""\n\n    @staticmethod\n    def open(\n        path: str,\n        mode: str = ""r"",\n        buffering: int = -1,\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n    ):\n        if FVCorePathManager:\n            return FVCorePathManager.open(\n                path=path,\n                mode=mode,\n                buffering=buffering,\n                encoding=encoding,\n                errors=errors,\n                newline=newline,\n            )\n        return open(\n            path,\n            mode=mode,\n            buffering=buffering,\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n        )\n\n    @staticmethod\n    def copy(src_path: str, dst_path: str, overwrite: bool = False) -> bool:\n        if FVCorePathManager:\n            return FVCorePathManager.copy(\n                src_path=src_path, dst_path=dst_path, overwrite=overwrite\n            )\n        return shutil.copyfile(src_path, dst_path)\n\n    @staticmethod\n    def get_local_path(path: str, **kwargs) -> str:\n        if FVCorePathManager:\n            return FVCorePathManager.get_local_path(path, **kwargs)\n        return path\n\n    @staticmethod\n    def exists(path: str) -> bool:\n        if FVCorePathManager:\n            return FVCorePathManager.exists(path)\n        return os.path.exists(path)\n\n    @staticmethod\n    def isfile(path: str) -> bool:\n        if FVCorePathManager:\n            return FVCorePathManager.isfile(path)\n        return os.path.isfile(path)\n\n    @staticmethod\n    def ls(path: str) -> List[str]:\n        if FVCorePathManager:\n            return FVCorePathManager.ls(path)\n        return os.listdir(path)\n\n    @staticmethod\n    def mkdirs(path: str) -> None:\n        if FVCorePathManager:\n            return FVCorePathManager.mkdirs(path)\n        os.makedirs(path, exist_ok=True)\n\n    @staticmethod\n    def rm(path: str) -> None:\n        if FVCorePathManager:\n            return FVCorePathManager.rm(path)\n        os.remove(path)\n\n    @staticmethod\n    def register_handler(handler) -> None:\n        if FVCorePathManager:\n            return FVCorePathManager.register_handler(handler=handler)\n'"
mmf/utils/flags.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport argparse\n\n\nclass Flags:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.add_core_args()\n\n    def get_parser(self):\n        return self.parser\n\n    def add_core_args(self):\n        self.parser.add_argument_group(""Core Arguments"")\n        # TODO: Add Help flag here describing MMF Configuration\n        # and point to configuration documentation\n        self.parser.add_argument(\n            ""-co"",\n            ""--config_override"",\n            type=str,\n            default=None,\n            help=""Use to override config from command line directly"",\n        )\n        # This is needed to support torch.distributed.launch\n        self.parser.add_argument(\n            ""--local_rank"", type=int, default=None, help=""Local rank of the argument""\n        )\n        self.parser.add_argument(\n            ""opts"",\n            default=None,\n            nargs=argparse.REMAINDER,\n            help=""Modify config options from command line"",\n        )\n\n\nflags = Flags()\n'"
mmf/utils/general.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport collections\nimport gc\nimport os\nfrom bisect import bisect\n\nimport torch\nfrom torch import nn\n\nfrom mmf.utils.distributed import get_world_size\nfrom mmf.utils.file_io import PathManager\n\n\ndef lr_lambda_update(i_iter, cfg):\n    if cfg.training.use_warmup is True and i_iter <= cfg.training.warmup_iterations:\n        alpha = float(i_iter) / float(cfg.training.warmup_iterations)\n        return cfg.training.warmup_factor * (1.0 - alpha) + alpha\n    else:\n        idx = bisect(cfg.training.lr_steps, i_iter)\n        return pow(cfg.training.lr_ratio, idx)\n\n\ndef clip_gradients(model, i_iter, writer, config):\n    # TODO: Fix question model retrieval\n    max_grad_l2_norm = config.training.max_grad_l2_norm\n    clip_norm_mode = config.training.clip_norm_mode\n\n    if max_grad_l2_norm is not None:\n        if clip_norm_mode == ""all"":\n            norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad_l2_norm)\n            if writer is not None:\n                writer.add_scalars({""grad_norm"": norm}, i_iter)\n\n        elif clip_norm_mode == ""question"":\n            question_embedding = model.module.question_embedding_module\n            norm = nn.utils.clip_grad_norm(\n                question_embedding.parameters(), max_grad_l2_norm\n            )\n            if writer is not None:\n                writer.add_scalars({""question_grad_norm"": norm}, i_iter)\n        else:\n            raise NotImplementedError(\n                ""Clip norm mode %s not implemented"" % clip_norm_mode\n            )\n\n\ndef ckpt_name_from_core_args(config):\n    seed = config.training.seed\n\n    ckpt_name = f""{config.datasets}_{config.model}""\n\n    if seed is not None:\n        ckpt_name += f""_{seed:d}""\n\n    return ckpt_name\n\n\ndef foldername_from_config_override(args):\n    cfg_override = None\n    if hasattr(args, ""config_override""):\n        cfg_override = args.config_override\n    elif ""config_override"" in args:\n        cfg_override = args[""config_override""]\n\n    folder_name = """"\n    if cfg_override is not None and len(cfg_override) > 0:\n        folder_name = str(cfg_override)\n        folder_name = folder_name.replace("":"", ""."").replace(""\\n"", "" "")\n        folder_name = folder_name.replace(""/"", ""_"")\n        folder_name = "" "".join(folder_name.split())\n        folder_name = folder_name.replace("". "", ""."").replace("" "", ""_"")\n        folder_name = ""_"" + folder_name\n    return folder_name\n\n\ndef get_mmf_root():\n    from mmf.common.registry import registry\n\n    mmf_root = registry.get(""mmf_root"", no_warning=True)\n    if mmf_root is None:\n        mmf_root = os.path.dirname(os.path.abspath(__file__))\n        mmf_root = os.path.abspath(os.path.join(mmf_root, ""..""))\n        registry.register(""mmf_root"", mmf_root)\n    return mmf_root\n\n\ndef get_absolute_path(paths):\n    # String check should be first as Sequence would pass for string too\n    if isinstance(paths, str):\n        # If path is absolute return it directly\n        if os.path.isabs(paths):\n            return paths\n\n        possible_paths = [\n            # Direct path\n            paths\n        ]\n        # Now, try relative to user_dir if it exists\n        from mmf.utils.configuration import get_mmf_env\n\n        user_dir = get_mmf_env(key=""user_dir"")\n        if user_dir:\n            possible_paths.append(os.path.join(user_dir, paths))\n\n        mmf_root = get_mmf_root()\n        # Relative to root folder of mmf install\n        possible_paths.append(os.path.join(mmf_root, "".."", paths))\n        # Relative to mmf root\n        possible_paths.append(os.path.join(mmf_root, paths))\n\n        # Test all these paths, if any exists return\n        for path in possible_paths:\n            if PathManager.exists(path):\n                # URIs\n                if path.find(""://"") == -1:\n                    return os.path.abspath(path)\n                else:\n                    return path\n\n        # If nothing works, return original path so that it throws an error\n        return paths\n    elif isinstance(paths, collections.abc.Iterable):\n        return [get_absolute_path(path) for path in paths]\n    else:\n        raise TypeError(""Paths passed to dataset should either be "" ""string or list"")\n\n\ndef get_optimizer_parameters(model, config):\n    parameters = model.parameters()\n\n    has_custom = hasattr(model, ""get_optimizer_parameters"")\n    if has_custom:\n        parameters = model.get_optimizer_parameters(config)\n\n    is_parallel = isinstance(model, nn.DataParallel) or isinstance(\n        model, nn.parallel.DistributedDataParallel\n    )\n\n    if is_parallel and hasattr(model.module, ""get_optimizer_parameters""):\n        parameters = model.module.get_optimizer_parameters(config)\n\n    return parameters\n\n\ndef dict_to_string(dictionary):\n    logs = []\n    if dictionary is None:\n        return """"\n    for key, val in dictionary.items():\n        if hasattr(val, ""item""):\n            val = val.item()\n        # if key.count(\'_\') == 2:\n        #     key = key[key.find(\'_\') + 1:]\n        logs.append(f""{key}: {val:.4f}"")\n\n    return "", "".join(logs)\n\n\ndef get_overlap_score(candidate, target):\n    """"""Takes a candidate word and a target word and returns the overlap\n    score between the two.\n\n    Parameters\n    ----------\n    candidate : str\n        Candidate word whose overlap has to be detected.\n    target : str\n        Target word against which the overlap will be detected\n\n    Returns\n    -------\n    float\n        Overlap score betwen candidate and the target.\n\n    """"""\n    if len(candidate) < len(target):\n        temp = candidate\n        candidate = target\n        target = temp\n    overlap = 0.0\n    while len(target) >= 2:\n        if target in candidate:\n            overlap = len(target)\n            return overlap * 1.0 / len(candidate)\n        else:\n            target = target[:-1]\n    return 0.0\n\n\ndef updir(d, n):\n    """"""Given path d, go up n dirs from d and return that path""""""\n    ret_val = d\n    for _ in range(n):\n        ret_val = os.path.dirname(ret_val)\n    return ret_val\n\n\ndef print_cuda_usage():\n    print(""Memory Allocated:"", torch.cuda.memory_allocated() / (1024 * 1024))\n    print(""Max Memory Allocated:"", torch.cuda.max_memory_allocated() / (1024 * 1024))\n    print(""Memory Cached:"", torch.cuda.memory_cached() / (1024 * 1024))\n    print(""Max Memory Cached:"", torch.cuda.max_memory_cached() / (1024 * 1024))\n\n\ndef get_current_tensors():\n    for obj in gc.get_objects():\n        try:\n            if torch.is_tensor(obj) or (\n                hasattr(obj, ""data"") and torch.is_tensor(obj.data)\n            ):\n                print(type(obj), obj.size())\n        except Exception:\n            pass\n\n\ndef get_batch_size():\n    from mmf.utils.configuration import get_global_config\n\n    batch_size = get_global_config(""training.batch_size"")\n\n    world_size = get_world_size()\n\n    if batch_size % world_size != 0:\n        raise RuntimeError(\n            ""Batch size {} must be divisible by number ""\n            ""of GPUs {} used."".format(batch_size, world_size)\n        )\n\n    return batch_size // world_size\n\n\ndef print_model_parameters(model, return_only=False):\n    from mmf.common.registry import registry\n\n    writer = registry.get(""writer"")\n    total_params = sum(p.numel() for p in model.parameters())\n    trained_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    if not return_only:\n        writer.write(\n            ""Total Parameters: {}. Trained Parameters: {}"".format(\n                total_params, trained_params\n            )\n        )\n    return total_params, trained_params\n\n\ndef get_sizes_list(dim, chunks):\n    split_size = (dim + chunks - 1) // chunks\n    sizes_list = [split_size] * chunks\n    sizes_list[-1] = sizes_list[-1] - (sum(sizes_list) - dim)  # Adjust last\n    assert sum(sizes_list) == dim\n    if sizes_list[-1] < 0:\n        n_miss = sizes_list[-2] - sizes_list[-1]\n        sizes_list[-1] = sizes_list[-2]\n        for j in range(n_miss):\n            sizes_list[-j - 1] -= 1\n        assert sum(sizes_list) == dim\n        assert min(sizes_list) > 0\n    return sizes_list\n\n\ndef get_chunks(x, sizes):\n    out = []\n    begin = 0\n    for s in sizes:\n        y = x.narrow(1, begin, s)\n        out.append(y)\n        begin += s\n    return out\n'"
mmf/utils/logger.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport collections\nimport json\nimport logging\nimport os\nimport sys\nfrom typing import Type\n\nfrom mmf.utils.configuration import get_mmf_env\nfrom mmf.utils.distributed import is_master\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.timer import Timer\n\n\nclass Logger:\n    def __init__(self, config, name=None):\n        self._logger = None\n        self._is_master = is_master()\n\n        self.timer = Timer()\n        self.config = config\n        self.save_dir = get_mmf_env(key=""save_dir"")\n        self.log_format = config.training.log_format\n        self.time_format = ""%Y-%m-%dT%H:%M:%S""\n        self.log_filename = ""train_""\n        self.log_filename += self.timer.get_time_hhmmss(None, format=self.time_format)\n        self.log_filename += "".log""\n\n        self.log_folder = os.path.join(self.save_dir, ""logs"")\n\n        env_log_dir = get_mmf_env(key=""log_dir"")\n        if env_log_dir:\n            self.log_folder = env_log_dir\n\n        if not PathManager.exists(self.log_folder):\n            PathManager.mkdirs(self.log_folder)\n\n        self.log_filename = os.path.join(self.log_folder, self.log_filename)\n\n        if not self._is_master:\n            return\n        if self._is_master:\n            print(""Logging to:"", self.log_filename)\n\n        logging.captureWarnings(True)\n\n        if not name:\n            name = __name__\n        self._logger = logging.getLogger(name)\n        self._file_only_logger = logging.getLogger(name)\n        self._warnings_logger = logging.getLogger(""py.warnings"")\n\n        # Set level\n        level = config.training.logger_level\n        self._logger.setLevel(getattr(logging, level.upper()))\n        self._file_only_logger.setLevel(getattr(logging, level.upper()))\n\n        # Capture stdout to logger\n        self._stdout_logger = None\n        if self.config.training.stdout_capture:\n            self._stdout_logger = StreamToLogger(\n                logging.getLogger(""stdout""), getattr(logging, level.upper())\n            )\n            sys.stdout = self._stdout_logger\n\n        formatter = logging.Formatter(\n            ""%(asctime)s | %(levelname)s | %(name)s : %(message)s"",\n            datefmt=""%Y-%m-%dT%H:%M:%S"",\n        )\n\n        # Add handler to file\n        channel = logging.FileHandler(filename=self.log_filename, mode=""a"")\n        channel.setFormatter(formatter)\n        self.add_handlers(channel)\n\n        # Add handler to train.log. train.log is full log that is also used\n        # by slurm/fbl output\n        channel = logging.FileHandler(\n            filename=os.path.join(self.save_dir, ""train.log""), mode=""a""\n        )\n        channel.setFormatter(formatter)\n        self.add_handlers(channel)\n\n        # Add handler to stdout. Only when we are not capturing stdout in\n        # the logger\n        if not self._stdout_logger:\n            channel = logging.StreamHandler(sys.stdout)\n            channel.setFormatter(formatter)\n\n            self._logger.addHandler(channel)\n            self._warnings_logger.addHandler(channel)\n\n        should_not_log = self.config.training.should_not_log\n        self.should_log = not should_not_log\n\n        # Single log wrapper map\n        self._single_log_map = set()\n\n    def add_handlers(self, channel: Type[logging.Handler]):\n        self._logger.addHandler(channel)\n        self._file_only_logger.addHandler(channel)\n        self._warnings_logger.addHandler(channel)\n        if self._stdout_logger:\n            self._stdout_logger.addHandler(channel)\n\n    def write(self, x, level=""info"", donot_print=False, log_all=False):\n        if self._logger is None:\n            return\n\n        if log_all is False and not self._is_master:\n            return\n\n        # if it should not log then just print it\n        if self.should_log:\n            if hasattr(self._logger, level):\n                if donot_print:\n                    getattr(self._file_only_logger, level)(str(x))\n                else:\n                    getattr(self._logger, level)(str(x))\n            else:\n                self._logger.error(""Unknown log level type: %s"" % level)\n        else:\n            print(str(x) + ""\\n"")\n\n    def log_progress(self, info):\n        if not isinstance(info, collections.Mapping):\n            self.write(info)\n\n        if not self._is_master:\n            return\n\n        if self.log_format == ""simple"":\n            output = "", "".join([f""{key}: {value}"" for key, value in info.items()])\n        elif self.log_format == ""json"":\n            output = json.dumps(info)\n        else:\n            output = str(info)\n\n        self.write(output)\n\n    def single_write(self, x, level=""info"", log_all=False):\n        if self._logger is None:\n            return\n        if log_all is False and not self._is_master:\n            return\n        if x + ""_"" + level in self._single_log_map:\n            return\n        else:\n            self.write(x, level)\n\n\nclass StreamToLogger:\n    """"""\n    Adapted from <https://fburl.com/2qkv0wq2>\n    Fake file-like stream object that redirects writes to a logger instance.\n    """"""\n\n    def __init__(self, logger: Type[logging.Logger], log_level: str = logging.INFO):\n        self._logger = logger\n        self.log_level = log_level\n\n    def addHandler(self, handler: Type[logging.Handler]):\n        self._logger.addHandler(handler)\n\n    def write(self, buf: str):\n        for line in buf.rstrip().splitlines():\n            self._logger.log(self.log_level, line.rstrip())\n\n    def flush(self):\n        pass\n\n\nclass TensorboardLogger:\n    def __init__(self, log_folder=""./logs"", iteration=0):\n        # This would handle warning of missing tensorboard\n        from torch.utils.tensorboard import SummaryWriter\n\n        self.summary_writer = None\n        self._is_master = is_master()\n        self.timer = Timer()\n        self.log_folder = log_folder\n        self.time_format = ""%Y-%m-%dT%H:%M:%S""\n\n        if self._is_master:\n            current_time = self.timer.get_time_hhmmss(None, format=self.time_format)\n            tensorboard_folder = os.path.join(\n                self.log_folder, f""tensorboard_{current_time}""\n            )\n            self.summary_writer = SummaryWriter(tensorboard_folder)\n\n    def __del__(self):\n        if getattr(self, ""summary_writer"", None) is not None:\n            self.summary_writer.close()\n\n    def _should_log_tensorboard(self):\n        if self.summary_writer is None or not self._is_master:\n            return False\n        else:\n            return True\n\n    def add_scalar(self, key, value, iteration):\n        if not self._should_log_tensorboard():\n            return\n\n        self.summary_writer.add_scalar(key, value, iteration)\n\n    def add_scalars(self, scalar_dict, iteration):\n        if not self._should_log_tensorboard():\n            return\n\n        for key, val in scalar_dict.items():\n            self.summary_writer.add_scalar(key, val, iteration)\n\n    def add_histogram_for_model(self, model, iteration):\n        if not self._should_log_tensorboard():\n            return\n\n        for name, param in model.named_parameters():\n            np_param = param.clone().cpu().data.numpy()\n            self.summary_writer.add_histogram(name, np_param, iteration)\n'"
mmf/utils/m4c_evaluators.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport re\n\n\nclass EvalAIAnswerProcessor:\n    """"""\n    Processes an answer similar to Eval AI\n        copied from\n        https://github.com/facebookresearch/mmf/blob/c46b3b3391275b4181567db80943473a89ab98ab/pythia/tasks/processors.py#L897\n    """"""\n\n    CONTRACTIONS = {\n        ""aint"": ""ain\'t"",\n        ""arent"": ""aren\'t"",\n        ""cant"": ""can\'t"",\n        ""couldve"": ""could\'ve"",\n        ""couldnt"": ""couldn\'t"",\n        ""couldn\'tve"": ""couldn\'t\'ve"",\n        ""couldnt\'ve"": ""couldn\'t\'ve"",\n        ""didnt"": ""didn\'t"",\n        ""doesnt"": ""doesn\'t"",\n        ""dont"": ""don\'t"",\n        ""hadnt"": ""hadn\'t"",\n        ""hadnt\'ve"": ""hadn\'t\'ve"",\n        ""hadn\'tve"": ""hadn\'t\'ve"",\n        ""hasnt"": ""hasn\'t"",\n        ""havent"": ""haven\'t"",\n        ""hed"": ""he\'d"",\n        ""hed\'ve"": ""he\'d\'ve"",\n        ""he\'dve"": ""he\'d\'ve"",\n        ""hes"": ""he\'s"",\n        ""howd"": ""how\'d"",\n        ""howll"": ""how\'ll"",\n        ""hows"": ""how\'s"",\n        ""Id\'ve"": ""I\'d\'ve"",\n        ""I\'dve"": ""I\'d\'ve"",\n        ""Im"": ""I\'m"",\n        ""Ive"": ""I\'ve"",\n        ""isnt"": ""isn\'t"",\n        ""itd"": ""it\'d"",\n        ""itd\'ve"": ""it\'d\'ve"",\n        ""it\'dve"": ""it\'d\'ve"",\n        ""itll"": ""it\'ll"",\n        ""let\'s"": ""let\'s"",\n        ""maam"": ""ma\'am"",\n        ""mightnt"": ""mightn\'t"",\n        ""mightnt\'ve"": ""mightn\'t\'ve"",\n        ""mightn\'tve"": ""mightn\'t\'ve"",\n        ""mightve"": ""might\'ve"",\n        ""mustnt"": ""mustn\'t"",\n        ""mustve"": ""must\'ve"",\n        ""neednt"": ""needn\'t"",\n        ""notve"": ""not\'ve"",\n        ""oclock"": ""o\'clock"",\n        ""oughtnt"": ""oughtn\'t"",\n        ""ow\'s\'at"": ""\'ow\'s\'at"",\n        ""\'ows\'at"": ""\'ow\'s\'at"",\n        ""\'ow\'sat"": ""\'ow\'s\'at"",\n        ""shant"": ""shan\'t"",\n        ""shed\'ve"": ""she\'d\'ve"",\n        ""she\'dve"": ""she\'d\'ve"",\n        ""she\'s"": ""she\'s"",\n        ""shouldve"": ""should\'ve"",\n        ""shouldnt"": ""shouldn\'t"",\n        ""shouldnt\'ve"": ""shouldn\'t\'ve"",\n        ""shouldn\'tve"": ""shouldn\'t\'ve"",\n        ""somebody\'d"": ""somebodyd"",\n        ""somebodyd\'ve"": ""somebody\'d\'ve"",\n        ""somebody\'dve"": ""somebody\'d\'ve"",\n        ""somebodyll"": ""somebody\'ll"",\n        ""somebodys"": ""somebody\'s"",\n        ""someoned"": ""someone\'d"",\n        ""someoned\'ve"": ""someone\'d\'ve"",\n        ""someone\'dve"": ""someone\'d\'ve"",\n        ""someonell"": ""someone\'ll"",\n        ""someones"": ""someone\'s"",\n        ""somethingd"": ""something\'d"",\n        ""somethingd\'ve"": ""something\'d\'ve"",\n        ""something\'dve"": ""something\'d\'ve"",\n        ""somethingll"": ""something\'ll"",\n        ""thats"": ""that\'s"",\n        ""thered"": ""there\'d"",\n        ""thered\'ve"": ""there\'d\'ve"",\n        ""there\'dve"": ""there\'d\'ve"",\n        ""therere"": ""there\'re"",\n        ""theres"": ""there\'s"",\n        ""theyd"": ""they\'d"",\n        ""theyd\'ve"": ""they\'d\'ve"",\n        ""they\'dve"": ""they\'d\'ve"",\n        ""theyll"": ""they\'ll"",\n        ""theyre"": ""they\'re"",\n        ""theyve"": ""they\'ve"",\n        ""twas"": ""\'twas"",\n        ""wasnt"": ""wasn\'t"",\n        ""wed\'ve"": ""we\'d\'ve"",\n        ""we\'dve"": ""we\'d\'ve"",\n        ""weve"": ""we\'ve"",\n        ""werent"": ""weren\'t"",\n        ""whatll"": ""what\'ll"",\n        ""whatre"": ""what\'re"",\n        ""whats"": ""what\'s"",\n        ""whatve"": ""what\'ve"",\n        ""whens"": ""when\'s"",\n        ""whered"": ""where\'d"",\n        ""wheres"": ""where\'s"",\n        ""whereve"": ""where\'ve"",\n        ""whod"": ""who\'d"",\n        ""whod\'ve"": ""who\'d\'ve"",\n        ""who\'dve"": ""who\'d\'ve"",\n        ""wholl"": ""who\'ll"",\n        ""whos"": ""who\'s"",\n        ""whove"": ""who\'ve"",\n        ""whyll"": ""why\'ll"",\n        ""whyre"": ""why\'re"",\n        ""whys"": ""why\'s"",\n        ""wont"": ""won\'t"",\n        ""wouldve"": ""would\'ve"",\n        ""wouldnt"": ""wouldn\'t"",\n        ""wouldnt\'ve"": ""wouldn\'t\'ve"",\n        ""wouldn\'tve"": ""wouldn\'t\'ve"",\n        ""yall"": ""y\'all"",\n        ""yall\'ll"": ""y\'all\'ll"",\n        ""y\'allll"": ""y\'all\'ll"",\n        ""yall\'d\'ve"": ""y\'all\'d\'ve"",\n        ""y\'alld\'ve"": ""y\'all\'d\'ve"",\n        ""y\'all\'dve"": ""y\'all\'d\'ve"",\n        ""youd"": ""you\'d"",\n        ""youd\'ve"": ""you\'d\'ve"",\n        ""you\'dve"": ""you\'d\'ve"",\n        ""youll"": ""you\'ll"",\n        ""youre"": ""you\'re"",\n        ""youve"": ""you\'ve"",\n    }\n\n    NUMBER_MAP = {\n        ""none"": ""0"",\n        ""zero"": ""0"",\n        ""one"": ""1"",\n        ""two"": ""2"",\n        ""three"": ""3"",\n        ""four"": ""4"",\n        ""five"": ""5"",\n        ""six"": ""6"",\n        ""seven"": ""7"",\n        ""eight"": ""8"",\n        ""nine"": ""9"",\n        ""ten"": ""10"",\n    }\n    ARTICLES = [""a"", ""an"", ""the""]\n    PERIOD_STRIP = re.compile(r""(?!<=\\d)(\\.)(?!\\d)"")\n    COMMA_STRIP = re.compile(r""(?<=\\d)(\\,)+(?=\\d)"")\n    PUNCTUATIONS = [\n        "";"",\n        r""/"",\n        ""["",\n        ""]"",\n        \'""\',\n        ""{"",\n        ""}"",\n        ""("",\n        "")"",\n        ""="",\n        ""+"",\n        ""\\\\"",\n        ""_"",\n        ""-"",\n        "">"",\n        ""<"",\n        ""@"",\n        ""`"",\n        "","",\n        ""?"",\n        ""!"",\n    ]\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def word_tokenize(self, word):\n        word = word.lower()\n        word = word.replace("","", """").replace(""?"", """").replace(""\'s"", "" \'s"")\n        return word.strip()\n\n    def process_punctuation(self, in_text):\n        out_text = in_text\n        for p in self.PUNCTUATIONS:\n            if (p + "" "" in in_text or "" "" + p in in_text) or (\n                re.search(self.COMMA_STRIP, in_text) is not None\n            ):\n                out_text = out_text.replace(p, """")\n            else:\n                out_text = out_text.replace(p, "" "")\n        out_text = self.PERIOD_STRIP.sub("""", out_text, re.UNICODE)\n        return out_text\n\n    def process_digit_article(self, in_text):\n        out_text = []\n        temp_text = in_text.lower().split()\n        for word in temp_text:\n            word = self.NUMBER_MAP.setdefault(word, word)\n            if word not in self.ARTICLES:\n                out_text.append(word)\n            else:\n                pass\n        for word_id, word in enumerate(out_text):\n            if word in self.CONTRACTIONS:\n                out_text[word_id] = self.CONTRACTIONS[word]\n        out_text = "" "".join(out_text)\n        return out_text\n\n    def __call__(self, item):\n        item = self.word_tokenize(item)\n        item = item.replace(""\\n"", "" "").replace(""\\t"", "" "").strip()\n        item = self.process_punctuation(item)\n        item = self.process_digit_article(item)\n        return item\n\n\nclass TextVQAAccuracyEvaluator:\n    def __init__(self):\n        self.answer_processor = EvalAIAnswerProcessor()\n\n    def _compute_answer_scores(self, raw_answers):\n        """"""\n        compute the accuracy (soft score) of human answers\n        """"""\n        answers = [self.answer_processor(a) for a in raw_answers]\n        assert len(answers) == 10\n        gt_answers = list(enumerate(answers))\n        unique_answers = set(answers)\n        unique_answer_scores = {}\n\n        for unique_answer in unique_answers:\n            accs = []\n            for gt_answer in gt_answers:\n                other_answers = [item for item in gt_answers if item != gt_answer]\n                matching_answers = [\n                    item for item in other_answers if item[1] == unique_answer\n                ]\n                acc = min(1, float(len(matching_answers)) / 3)\n                accs.append(acc)\n            unique_answer_scores[unique_answer] = sum(accs) / len(accs)\n\n        return unique_answer_scores\n\n    def eval_pred_list(self, pred_list):\n        pred_scores = []\n        for entry in pred_list:\n            pred_answer = self.answer_processor(entry[""pred_answer""])\n            unique_answer_scores = self._compute_answer_scores(entry[""gt_answers""])\n            score = unique_answer_scores.get(pred_answer, 0.0)\n            pred_scores.append(score)\n\n        accuracy = sum(pred_scores) / len(pred_scores)\n        return accuracy\n\n\nclass STVQAAccuracyEvaluator:\n    def __init__(self):\n        self.answer_processor = EvalAIAnswerProcessor()\n\n    def eval_pred_list(self, pred_list):\n        pred_scores = []\n        for entry in pred_list:\n            pred_answer = self.answer_processor(entry[""pred_answer""])\n            gts = [self.answer_processor(a) for a in entry[""gt_answers""]]\n            score = 1.0 if pred_answer in gts else 0.0\n            pred_scores.append(score)\n\n        accuracy = sum(pred_scores) / len(pred_scores)\n        return accuracy\n\n\nclass STVQAANLSEvaluator:\n    def __init__(self):\n        import editdistance  # install with `pip install editdistance`\n\n        self.get_edit_distance = editdistance.eval\n\n    def get_anls(self, s1, s2):\n        s1 = s1.lower().strip()\n        s2 = s2.lower().strip()\n        iou = 1 - self.get_edit_distance(s1, s2) / max(len(s1), len(s2))\n        anls = iou if iou >= 0.5 else 0.0\n        return anls\n\n    def eval_pred_list(self, pred_list):\n        pred_scores = []\n        for entry in pred_list:\n            anls = max(\n                self.get_anls(entry[""pred_answer""], gt) for gt in entry[""gt_answers""]\n            )\n            pred_scores.append(anls)\n\n        accuracy = sum(pred_scores) / len(pred_scores)\n        return accuracy\n\n\nclass TextCapsBleu4Evaluator:\n    def __init__(self):\n        # The following script requires Java 1.8.0 and pycocotools installed.\n        # The pycocoevalcap can be installed with pip as\n        # pip install git+https://github.com/ronghanghu/coco-caption.git@python23\n        # Original pycocoevalcap code is at https://github.com/tylin/coco-caption\n        # but has no python3 support yet.\n        try:\n            from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n            from pycocoevalcap.bleu.bleu import Bleu\n        except ModuleNotFoundError:\n            print(\n                ""Please install pycocoevalcap module using ""\n                ""pip install git+https://github.com/ronghanghu/coco-caption.git@python23""  # noqa\n            )\n            raise\n\n        self.tokenizer = PTBTokenizer()\n        self.scorer = Bleu(4)\n\n    def eval_pred_list(self, pred_list):\n        # Create reference and hypotheses captions.\n        gts = {}\n        res = {}\n        for idx, entry in enumerate(pred_list):\n            gts[idx] = [{""caption"": a} for a in entry[""gt_answers""]]\n            res[idx] = [{""caption"": entry[""pred_answer""]}]\n\n        gts = self.tokenizer.tokenize(gts)\n        res = self.tokenizer.tokenize(res)\n        score, _ = self.scorer.compute_score(gts, res)\n\n        bleu4 = score[3]  # score is (Bleu-1, Bleu-2, Bleu-3, Bleu-4)\n        return bleu4\n'"
mmf/utils/modeling.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom torch import nn\n\nACT2FN = {\n    ""relu"": nn.ReLU,\n    ""sigmoid"": nn.Sigmoid,\n    ""tanh"": nn.Tanh,\n    ""leaky_relu"": nn.LeakyReLU,\n}\n\n\ndef get_bert_configured_parameters(module, lr=None):\n    param_optimizer = list(module.named_parameters())\n\n    no_decay = [""bias"", ""LayerNorm.bias"", ""LayerNorm.weight""]\n    optimizer_grouped_parameters = [\n        {\n            ""params"": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            ""weight_decay"": 0.01,\n        },\n        {\n            ""params"": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            ""weight_decay"": 0.0,\n        },\n    ]\n\n    if lr is not None:\n        for p in optimizer_grouped_parameters:\n            p[""lr""] = lr\n\n    return optimizer_grouped_parameters\n\n\ndef get_optimizer_parameters_for_bert(module, config):\n    # Pretraining has same LR for all of the parts\n    if module.config.training_head_type == ""pretraining"":\n        return get_bert_configured_parameters(module)\n\n    # For finetuning setup, we have classifier\n    lr = config.optimizer.params.lr\n    model_config = getattr(config.model_config, config.model, {})\n    finetune_lr_multiplier = getattr(model_config, ""finetune_lr_multiplier"", 1)\n    # Finetune the bert pretrained part with finetune_lr_multiplier if it is set\n    parameters = get_bert_configured_parameters(\n        module.bert, lr * finetune_lr_multiplier\n    )\n    # Classifier will be trained on the normal lr\n    parameters += get_bert_configured_parameters(module.classifier, lr)\n\n    return parameters\n'"
mmf/utils/process_answers.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport argparse\nimport json\nimport os\n\nfrom mmf.datasets.processors.processors import EvalAIAnswerProcessor\n\n\ndef get_score(occurences):\n    if occurences == 0:\n        return 0\n    elif occurences == 1:\n        return 0.3\n    elif occurences == 2:\n        return 0.6\n    elif occurences == 3:\n        return 0.9\n    else:\n        return 1\n\n\ndef multiple_replace(text, wordDict):\n    for key in wordDict:\n        text = text.replace(key, wordDict[key])\n    return text\n\n\ndef filter_answers(answers_dset, min_occurence):\n    """"""This will change the answer to preprocessed version\n    """"""\n    occurence = {}\n    answer_list = []\n    evalai_answer_processor = EvalAIAnswerProcessor()\n    for ans_entry in answers_dset:\n        gtruth = ans_entry[""multiple_choice_answer""]\n        gtruth = evalai_answer_processor(gtruth)\n        if gtruth not in occurence:\n            occurence[gtruth] = set()\n        occurence[gtruth].add(ans_entry[""question_id""])\n    for answer in occurence.keys():\n        if len(occurence[answer]) >= min_occurence:\n            answer_list.append(answer)\n\n    print(\n        ""Num of answers that appear >= %d times: %d"" % (min_occurence, len(answer_list))\n    )\n    return answer_list\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--annotation_file"",\n        type=str,\n        required=True,\n        help=""input train annotationjson file"",\n    )\n    parser.add_argument(\n        ""--val_annotation_file"",\n        type=str,\n        required=False,\n        help=""input val annotation json file"",\n    )\n    parser.add_argument(\n        ""--out_dir"",\n        type=str,\n        default=""./"",\n        help=""output directory, default is current directory"",\n    )\n    parser.add_argument(\n        ""--min_freq"",\n        type=int,\n        default=0,\n        help=""the minimum times of answer occurrence \\\n                              to be included in vocabulary, default 0"",\n    )\n    args = parser.parse_args()\n\n    train_annotation_file = args.annotation_file\n    out_dir = args.out_dir\n    min_freq = args.min_freq\n\n    answer_file_name = ""answers_vqa.txt""\n    os.makedirs(out_dir, exist_ok=True)\n\n    train_answers = json.load(open(train_annotation_file))[""annotations""]\n    answers = train_answers\n\n    if args.val_annotation_file is not None:\n        val_annotation_file = args.val_annotation_file\n        val_answers = json.load(open(val_annotation_file))[""annotations""]\n        answers = train_answers + val_answers\n\n    answer_list = filter_answers(answers, min_freq)\n    answer_list = [t.strip() for t in answer_list if len(t.strip()) > 0]\n    answer_list.sort()\n\n    if ""<unk>"" not in answer_list:\n        answer_list = [""<unk>""] + answer_list\n\n    answer_file = os.path.join(out_dir, answer_file_name)\n    with open(answer_file, ""w"") as f:\n        f.writelines([w + ""\\n"" for w in answer_list])\n'"
mmf/utils/text.py,12,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\nText utils module contains implementations for various decoding strategies like\nGreedy, Beam Search and Nucleus Sampling.\n\nIn your model\'s config you can specify ``inference`` attribute to use these strategies\nin the following way:\n\n.. code::\n\n   model_config:\n       some_model:\n           inference:\n               - type: greedy\n               - params: {}\n""""""\nimport os\nimport re\nfrom collections import Counter\nfrom itertools import chain\n\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_mmf_root\n\nSENTENCE_SPLIT_REGEX = re.compile(r""(\\W+)"")\n\n\ndef generate_ngrams(tokens, n=1):\n    """"""Generate ngrams for particular \'n\' from a list of tokens\n\n    Args:\n        tokens (List[str]): List of tokens for which the ngram are to be generated\n        n (int, optional): n for which ngrams are to be generated. Defaults to 1.\n\n    Returns:\n        List[str]: List of ngrams generated.\n    """"""\n    shifted_tokens = (tokens[i:] for i in range(n))\n    tuple_ngrams = zip(*shifted_tokens)\n    return ("" "".join(i) for i in tuple_ngrams)\n\n\ndef generate_ngrams_range(tokens, ngram_range=(1, 3)):\n    """"""Generates and returns a list of ngrams for all n present in ngram_range\n\n    Args:\n        tokens (List[str]): List of string tokens for which ngram are to be generated\n        ngram_range (List[int], optional): List of \'n\' for which ngrams are to be\n            generated. For e.g. if ngram_range = (1, 4) then it will returns\n            1grams, 2grams and 3grams. Defaults to (1, 3).\n\n    Returns:\n        List[str]: List of ngrams for each n in ngram_range\n    """"""\n    assert len(ngram_range) == 2, (\n        ""\'ngram_range\' should be a tuple"" "" of two elements which is range of numbers""\n    )\n    return chain(*(generate_ngrams(tokens, i) for i in range(*ngram_range)))\n\n\ndef tokenize(sentence, regex=SENTENCE_SPLIT_REGEX, keep=None, remove=None):\n    if keep is None:\n        keep = [""\'s""]\n    if remove is None:\n        remove = ["","", ""?""]\n    sentence = sentence.lower()\n\n    for token in keep:\n        sentence = sentence.replace(token, "" "" + token)\n\n    for token in remove:\n        sentence = sentence.replace(token, """")\n\n    tokens = regex.split(sentence)\n    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n    return tokens\n\n\ndef word_tokenize(word, remove=None):\n    if remove is None:\n        remove = ["","", ""?""]\n    word = word.lower()\n\n    for item in remove:\n        word = word.replace(item, """")\n    word = word.replace(""\'s"", "" \'s"")\n\n    return word.strip()\n\n\ndef load_str_list(fname):\n    with PathManager.open(fname) as f:\n        lines = f.readlines()\n    lines = [l.strip() for l in lines]\n    return lines\n\n\nclass VocabDict:\n    UNK_TOKEN = ""<unk>""\n    PAD_TOKEN = ""<pad>""\n    START_TOKEN = ""<s>""\n    END_TOKEN = ""</s>""\n\n    PAD_INDEX = 0\n    SOS_INDEX = 1\n    EOS_INDEX = 2\n    UNK_INDEX = 3\n\n    def __init__(self, vocab_file, data_dir=None):\n        if not os.path.isabs(vocab_file) and data_dir is not None:\n            mmf_root = get_mmf_root()\n            vocab_file = os.path.abspath(os.path.join(mmf_root, data_dir, vocab_file))\n\n        if not PathManager.exists(vocab_file):\n            raise RuntimeError(f""Vocab file {vocab_file} for vocab dict doesn\'t exist"")\n\n        self.word_list = load_str_list(vocab_file)\n        self._build()\n\n    def _build(self):\n        if self.UNK_TOKEN not in self.word_list:\n            self.word_list = [self.UNK_TOKEN] + self.word_list\n\n        self.word2idx_dict = {w: n_w for n_w, w in enumerate(self.word_list)}\n\n        # String (word) to integer (index) dict mapping\n        self.stoi = self.word2idx_dict\n        # Integer to string (word) reverse mapping\n        self.itos = self.word_list\n        self.num_vocab = len(self.word_list)\n\n        self.UNK_INDEX = (\n            self.word2idx_dict[self.UNK_TOKEN]\n            if self.UNK_TOKEN in self.word2idx_dict\n            else None\n        )\n\n        self.PAD_INDEX = (\n            self.word2idx_dict[self.PAD_TOKEN]\n            if self.PAD_TOKEN in self.word2idx_dict\n            else None\n        )\n\n    def idx2word(self, n_w):\n        return self.word_list[n_w]\n\n    def __len__(self):\n        return len(self.word_list)\n\n    def get_size(self):\n        return len(self.word_list)\n\n    def get_unk_index(self):\n        return self.UNK_INDEX\n\n    def get_unk_token(self):\n        return self.UNK_TOKEN\n\n    def word2idx(self, w):\n        if w in self.word2idx_dict:\n            return self.word2idx_dict[w]\n        elif self.UNK_INDEX is not None:\n            return self.UNK_INDEX\n        else:\n            raise ValueError(\n                ""word %s not in dictionary \\\n                             (while dictionary does not contain <unk>)""\n                % w\n            )\n\n    def tokenize_and_index(self, sentence):\n        inds = [self.word2idx(w) for w in tokenize(sentence)]\n        return inds\n\n\nclass VocabFromText(VocabDict):\n    DEFAULT_TOKENS = [\n        VocabDict.PAD_TOKEN,\n        VocabDict.UNK_TOKEN,\n        VocabDict.START_TOKEN,\n        VocabDict.END_TOKEN,\n    ]\n\n    def __init__(\n        self,\n        sentences,\n        min_count=1,\n        regex=SENTENCE_SPLIT_REGEX,\n        keep=None,\n        remove=None,\n        only_unk_extra=False,\n    ):\n        if keep is None:\n            keep = []\n        if remove is None:\n            remove = []\n        token_counter = Counter()\n\n        for sentence in sentences:\n            tokens = tokenize(sentence, regex=regex, keep=keep, remove=remove)\n            token_counter.update(tokens)\n\n        token_list = []\n        for token in token_counter:\n            if token_counter[token] >= min_count:\n                token_list.append(token)\n\n        extras = self.DEFAULT_TOKENS\n\n        if only_unk_extra:\n            extras = [self.UNK_TOKEN]\n\n        self.word_list = extras + token_list\n        self._build()\n\n\nclass TextDecoder:\n    """"""Base class to be inherited by all decoding strategies. Contains\n    implementations that are common for all strategies.\n\n    Args:\n        vocab (list): Collection of all words in vocabulary.\n\n    """"""\n\n    def __init__(self, vocab):\n        self._vocab = vocab\n        self._vocab_size = vocab.get_size()\n\n        # Lists to store completed sequences and scores\n        self._complete_seqs = []\n        self._complete_seqs_scores = []\n\n    def init_batch(self, sample_list):\n        self.seqs = sample_list.answers.new_full(\n            (self._decode_size, 1), self._vocab.SOS_INDEX, dtype=torch.long\n        )\n\n        sample_list.image_feature_0 = (\n            sample_list.image_feature_0.unsqueeze(1)\n            .expand(-1, self._decode_size, -1, -1)\n            .squeeze(0)\n        )\n        return sample_list\n\n    def add_next_word(self, seqs, prev_word_inds, next_word_inds):\n        return torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)\n\n    def find_complete_inds(self, next_word_inds):\n        incomplete_inds = []\n        for ind, next_word in enumerate(next_word_inds):\n            if next_word != self._vocab.EOS_INDEX:\n                incomplete_inds.append(ind)\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n        return complete_inds, incomplete_inds\n\n    def update_data(self, data, prev_word_inds, next_word_inds, incomplete_inds):\n        data[""texts""] = next_word_inds[incomplete_inds].unsqueeze(1)\n        h1 = data[""state""][""td_hidden""][0][prev_word_inds[incomplete_inds]]\n        c1 = data[""state""][""td_hidden""][1][prev_word_inds[incomplete_inds]]\n        h2 = data[""state""][""lm_hidden""][0][prev_word_inds[incomplete_inds]]\n        c2 = data[""state""][""lm_hidden""][1][prev_word_inds[incomplete_inds]]\n        data[""state""] = {""td_hidden"": (h1, c1), ""lm_hidden"": (h2, c2)}\n        return data\n\n\n@registry.register_decoder(""beam_search"")\nclass BeamSearch(TextDecoder):\n    def __init__(self, vocab, config):\n        super().__init__(vocab)\n        self._decode_size = config[""inference""][""params""][""beam_length""]\n\n    def init_batch(self, sample_list):\n        assert sample_list.get_batch_size() == 1, (\n            ""Beam Search can only work with batch size = 1 ""\n            + ""Use training.batch_size=1""\n        )\n        self.top_k_scores = sample_list.answers.new_zeros(\n            (self._decode_size, 1), dtype=torch.float\n        )\n        return super().init_batch(sample_list)\n\n    def decode(self, t, data, scores):\n        # Add predicted scores to top_k_scores\n        scores = torch.nn.functional.log_softmax(scores, dim=1)\n        scores = self.top_k_scores.expand_as(scores) + scores\n\n        # Find next top k scores and words. We flatten the scores tensor here\n        # and get the top_k_scores and their indices top_k_words\n        if t == 0:\n            self.top_k_scores, top_k_words = scores[0].topk(\n                self._decode_size, 0, True, True\n            )\n        else:\n            self.top_k_scores, top_k_words = scores.view(-1).topk(\n                self._decode_size, 0, True, True\n            )\n\n        # Convert to vocab indices. top_k_words contain indices from a flattened\n        # k x vocab_size tensor. To get prev_word_indices we divide top_k_words\n        # by vocab_size to determine which index in the beam among k generated\n        # the next top_k_word. To get next_word_indices we take top_k_words\n        # modulo vocab_size index. For example :\n        # vocab_size : 9491\n        # top_k_words : [610, 7, 19592, 9529, 292]\n        # prev_word_inds : [0, 0, 2, 1, 0]\n        # next_word_inds : [610, 7, 610, 38, 292]\n        prev_word_inds = top_k_words // self._vocab_size\n        next_word_inds = top_k_words % self._vocab_size\n\n        # Add new words to sequences\n        self.seqs = self.add_next_word(self.seqs, prev_word_inds, next_word_inds)\n\n        # Find completed sequences\n        complete_inds, incomplete_inds = self.find_complete_inds(next_word_inds)\n\n        # Add to completed sequences\n        if len(complete_inds) > 0:\n            self._complete_seqs.extend(self.seqs[complete_inds].tolist())\n            self._complete_seqs_scores.extend(self.top_k_scores[complete_inds])\n\n        # Reduce beam length\n        self._decode_size -= len(complete_inds)\n\n        # Proceed with incomplete sequences\n        if self._decode_size == 0:\n            return True, data, 0\n\n        self.seqs = self.seqs[incomplete_inds]\n        self.top_k_scores = self.top_k_scores[incomplete_inds].unsqueeze(1)\n\n        # TODO: Make the data update generic for any type of model\n        # This is specific to BUTD model only.\n        data = self.update_data(data, prev_word_inds, next_word_inds, incomplete_inds)\n\n        next_beam_length = len(prev_word_inds[incomplete_inds])\n\n        return False, data, next_beam_length\n\n    def get_result(self):\n        if len(self._complete_seqs_scores) == 0:\n            captions = torch.FloatTensor([0] * 5).unsqueeze(0)\n        else:\n            i = self._complete_seqs_scores.index(max(self._complete_seqs_scores))\n            captions = torch.FloatTensor(self._complete_seqs[i]).unsqueeze(0)\n        return captions\n\n\n@registry.register_decoder(""nucleus_sampling"")\nclass NucleusSampling(TextDecoder):\n    """"""Nucleus Sampling is a new text decoding strategy that avoids likelihood\n    maximization. Rather, it works by sampling from the smallest set of top\n    tokens which have a cumulative probability greater than a specified\n    threshold.\n\n    Present text decoding strategies like beam search do not work well on open-ended\n    generation tasks (even on strong language models like GPT-2). They tend to repeat\n    text a lot and the main reason behind it is that they try to maximize likelihood,\n    which is a contrast from human-generated text which has a mix of high and low\n    probability tokens.\n\n    Nucleus Sampling is a stochastic approach and resolves this issue. Moreover,\n    it improves upon other stochastic methods like top-k sampling by choosing the\n    right amount of tokens to sample from. The overall result is better text\n    generation on the same language model.\n\n    Link to the paper introducing Nucleus Sampling (Section 6) -\n    https://arxiv.org/pdf/1904.09751.pdf\n\n    Args:\n        vocab (list): Collection of all words in vocabulary.\n        sum_threshold (float): Ceiling of sum of probabilities of tokens to\n            sample from.\n    """"""\n\n    def __init__(self, vocab, config):\n        super().__init__(vocab)\n        self._decode_size = 1\n        # Threshold for sum of probability\n        self._threshold = config[""inference""][""params""][""sum_threshold""]\n\n    def decode(self, t, data, scores):\n        # Convert scores to probabilities\n        scores = torch.nn.functional.softmax(scores, dim=1)\n        # Sort scores in descending order and then select the top m elements having\n        # sum more than threshold.\n        # We get the top_m_scores and their indices top_m_words\n        if t == 0:\n            top_m_scores, top_m_words = scores[0].sort(0, True)\n        else:\n            top_m_scores, top_m_words = scores.view(-1).sort(0, True)\n\n        last_index = 0\n        score_sum = 0\n        for score in top_m_scores:\n            last_index += 1\n            score_sum += score\n            if score_sum >= self._threshold:\n                break\n\n        top_m_scores = torch.div(top_m_scores[:last_index], score_sum)\n        top_m_words = top_m_words[:last_index]\n\n        # Zero value inside prev_word_inds because we are predicting a single\n        # stream of output.\n        prev_word_ind = torch.tensor([0])\n        # Get next word based on probabilities of top m words.\n        next_word_ind = top_m_words[torch.multinomial(top_m_scores, 1)]\n        # Add next word to sequence\n\n        self.seqs = self.add_next_word(self.seqs, prev_word_ind, next_word_ind)\n        # Check if sequence is complete\n        complete_inds, incomplete_inds = self.find_complete_inds(next_word_ind)\n        # If sequence is complete then return\n        if len(complete_inds) > 0:\n            self._complete_seqs.extend(self.seqs[complete_inds].tolist())\n            return True, data, 0\n\n        self.seqs = self.seqs[incomplete_inds]\n\n        data = self.update_data(data, prev_word_ind, next_word_ind, incomplete_inds)\n\n        return False, data, 1\n\n    def get_result(self):\n        if len(self._complete_seqs) == 0:\n            captions = torch.FloatTensor([0] * 5).unsqueeze(0)\n        else:\n            captions = torch.FloatTensor(self._complete_seqs[0]).unsqueeze(0)\n        return captions\n'"
mmf/utils/timer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport time\n\n\nclass Timer:\n    DEFAULT_TIME_FORMAT_DATE_TIME = ""%Y/%m/%d %H:%M:%S""\n    DEFAULT_TIME_FORMAT = [""%03dms"", ""%02ds"", ""%02dm"", ""%02dh""]\n\n    def __init__(self):\n        self.start = time.time() * 1000\n\n    def get_current(self):\n        return self.get_time_hhmmss(self.start)\n\n    def reset(self):\n        self.start = time.time() * 1000\n\n    def get_time_since_start(self, format=None):\n        return self.get_time_hhmmss(self.start, format)\n\n    def unix_time_since_start(self, in_seconds=True):\n        gap = time.time() * 1000 - self.start\n\n        if in_seconds:\n            gap = gap // 1000\n\n        # Prevent 0 division errors\n        if gap == 0:\n            gap = 1\n        return gap\n\n    def get_time_hhmmss(self, start=None, end=None, gap=None, format=None):\n        """"""\n        Calculates time since `start` and formats as a string.\n        """"""\n        if start is None and gap is None:\n\n            if format is None:\n                format = self.DEFAULT_TIME_FORMAT_DATE_TIME\n\n            return time.strftime(format)\n\n        if end is None:\n            end = time.time() * 1000\n        if gap is None:\n            gap = end - start\n\n        s, ms = divmod(gap, 1000)\n        m, s = divmod(s, 60)\n        h, m = divmod(m, 60)\n\n        if format is None:\n            format = self.DEFAULT_TIME_FORMAT\n\n        items = [ms, s, m, h]\n        assert len(items) == len(format), ""Format length should be same as items""\n\n        time_str = """"\n        for idx, item in enumerate(items):\n            if item != 0:\n                time_str = format[idx] % item + "" "" + time_str\n\n        # Means no more time is left.\n        if len(time_str) == 0:\n            time_str = ""0ms""\n\n        return time_str.strip()\n'"
mmf/utils/transform.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n\ndef transform_to_batch_sequence(tensor):\n    if tensor is not None:\n        if len(tensor.size()) == 2:\n            return tensor\n        else:\n            assert len(tensor.size()) == 3\n            return tensor.contiguous().view(-1, tensor.size(-1))\n    else:\n        return None\n\n\ndef transform_to_batch_sequence_dim(tensor):\n    if tensor is not None:\n        if len(tensor.size()) == 3:\n            return tensor\n        else:\n            assert len(tensor.size()) == 4\n            return tensor.contiguous().view(-1, tensor.size(-2), tensor.size(-1))\n    else:\n        return None\n'"
mmf/utils/visualize.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom typing import Any, List, Optional, Tuple\n\nimport torch\nimport torchvision\n\n\ndef visualize_images(\n    images: List[Any], size: Optional[Tuple[int, int]] = (224, 224), *args, **kwargs\n):\n    """"""Visualize a set of images using torchvision\'s make grid function. Expects\n    PIL images which it will convert to tensor and optionally resize them. If resize is\n    not passed, it will only accept a list with single image\n\n    Args:\n        images (List[Any]): List of images to be visualized\n        size (Optional[Tuple[int, int]], optional): Size to which Images can be resized.\n            If not passed, the function will only accept list with single image.\n            Defaults to (224, 224).\n    """"""\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        print(\n            ""Visualization tools require matplotlib. ""\n            + ""Install using pip install matplotlib.""\n        )\n        raise\n\n    transform_list = []\n\n    assert (\n        size is not None or len(images) == 1\n    ), ""If size is not passed, only one image can be visualized""\n\n    if size is not None:\n        transform_list.append(torchvision.transforms.Resize(size=size))\n\n    transform_list.append(torchvision.transforms.ToTensor())\n    transform = torchvision.transforms.Compose(transform_list)\n\n    img_tensors = torch.stack([transform(image) for image in images])\n    grid = torchvision.utils.make_grid(img_tensors, *args, **kwargs)\n\n    plt.axis(""off"")\n    plt.imshow(grid.permute(1, 2, 0))\n'"
mmf/utils/vocab.py,15,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom torchtext import vocab\n\nfrom mmf.utils.configuration import get_mmf_cache_dir\nfrom mmf.utils.distributed import is_master, synchronize\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_mmf_root\n\nEMBEDDING_NAME_CLASS_MAPPING = {""glove"": ""GloVe"", ""fasttext"": ""FastText""}\n\n\nclass Vocab:\n    def __init__(self, *args, **params):\n        vocab_type = params.get(""type"", ""pretrained"")\n        # Stores final parameters extracted from vocab_params\n\n        if vocab_type == ""random"":\n            if params[""vocab_file""] is None:\n                raise ValueError(""No vocab path passed for vocab"")\n\n            self.vocab = BaseVocab(*args, **params)\n\n        elif vocab_type == ""custom"":\n            if params[""vocab_file""] is None or params[""embedding_file""] is None:\n                raise ValueError(""No vocab path or embedding_file passed for vocab"")\n            self.vocab = CustomVocab(*args, **params)\n\n        elif vocab_type == ""pretrained"":\n            self.vocab = PretrainedVocab(*args, **params)\n\n        elif vocab_type == ""intersected"":\n            if params[""vocab_file""] is None or params[""embedding_name""] is None:\n                raise ValueError(""No vocab path or embedding_name passed for vocab"")\n\n            self.vocab = IntersectedVocab(*args, **params)\n\n        elif vocab_type == ""extracted"":\n            if params[""base_path""] is None or params[""embedding_dim""] is None:\n                raise ValueError(""No base_path or embedding_dim passed for vocab"")\n            self.vocab = ExtractedVocab(*args, **params)\n\n        elif vocab_type == ""model"":\n            if params[""name""] is None or params[""model_file""] is None:\n                raise ValueError(""No name or model_file passed for vocab"")\n            if params[""name""] == ""fasttext"":\n                self.vocab = ModelVocab(*args, **params)\n        else:\n            raise ValueError(""Unknown vocab type: %s"" % vocab_type)\n\n        self._dir_representation = dir(self)\n\n    def __call__(self, *args, **kwargs):\n        return self.vocab(*args, **kwargs)\n\n    def __getattr__(self, name):\n        if ""_dir_representation"" in self.__dict__ and name in self._dir_representation:\n            return getattr(self, name)\n        elif ""vocab"" in self.__dict__ and hasattr(self.vocab, name):\n            return getattr(self.vocab, name)\n        else:\n            type_vocab = ""Vocab""\n            if ""vocab"" in self.__dict__:\n                type_vocab = type(self.vocab)\n\n            raise AttributeError(f""{type_vocab} vocab type has no attribute {name}."")\n\n\nclass BaseVocab:\n    PAD_TOKEN = ""<pad>""\n    SOS_TOKEN = ""<s>""\n    EOS_TOKEN = ""</s>""\n    UNK_TOKEN = ""<unk>""\n\n    PAD_INDEX = 0\n    SOS_INDEX = 1\n    EOS_INDEX = 2\n    UNK_INDEX = 3\n\n    def __init__(\n        self, vocab_file=None, embedding_dim=300, data_dir=None, *args, **kwargs\n    ):\n        """"""Vocab class to be used when you want to train word embeddings from\n        scratch based on a custom vocab. This will initialize the random\n        vectors for the vocabulary you pass. Get the vectors using\n        `get_vectors` function. This will also create random embeddings for\n        some predefined words like PAD - <pad>, SOS - <s>, EOS - </s>,\n        UNK - <unk>.\n\n        Parameters\n        ----------\n        vocab_file : str\n            Path of the vocabulary file containing one word per line\n        embedding_dim : int\n            Size of the embedding\n\n        """"""\n        self.type = ""base""\n        self.word_dict = {}\n        self.itos = {}\n\n        self.itos[self.PAD_INDEX] = self.PAD_TOKEN\n        self.itos[self.SOS_INDEX] = self.SOS_TOKEN\n        self.itos[self.EOS_INDEX] = self.EOS_TOKEN\n        self.itos[self.UNK_INDEX] = self.UNK_TOKEN\n\n        self.word_dict[self.SOS_TOKEN] = self.SOS_INDEX\n        self.word_dict[self.EOS_TOKEN] = self.EOS_INDEX\n        self.word_dict[self.PAD_TOKEN] = self.PAD_INDEX\n        self.word_dict[self.UNK_TOKEN] = self.UNK_INDEX\n\n        index = len(self.itos.keys())\n\n        self.total_predefined = len(self.itos.keys())\n\n        if vocab_file is not None:\n            if not os.path.isabs(vocab_file) and data_dir is not None:\n                mmf_root = get_mmf_root()\n                vocab_file = os.path.join(mmf_root, data_dir, vocab_file)\n            if not PathManager.exists(vocab_file):\n                raise RuntimeError(""Vocab not found at "" + vocab_file)\n\n            with PathManager.open(vocab_file, ""r"") as f:\n                for line in f:\n                    self.itos[index] = line.strip()\n                    self.word_dict[line.strip()] = index\n                    index += 1\n\n        self.word_dict[self.SOS_TOKEN] = self.SOS_INDEX\n        self.word_dict[self.EOS_TOKEN] = self.EOS_INDEX\n        self.word_dict[self.PAD_TOKEN] = self.PAD_INDEX\n        self.word_dict[self.UNK_TOKEN] = self.UNK_INDEX\n        # Return unk index by default\n        self.stoi = defaultdict(self.get_unk_index)\n        self.stoi.update(self.word_dict)\n\n        self.vectors = torch.FloatTensor(self.get_size(), embedding_dim)\n\n    def get_itos(self):\n        return self.itos\n\n    def get_stoi(self):\n        return self.stoi\n\n    def get_size(self):\n        return len(self.itos)\n\n    def get_pad_index(self):\n        return self.PAD_INDEX\n\n    def get_pad_token(self):\n        return self.PAD_TOKEN\n\n    def get_start_index(self):\n        return self.SOS_INDEX\n\n    def get_start_token(self):\n        return self.SOS_TOKEN\n\n    def get_end_index(self):\n        return self.EOS_INDEX\n\n    def get_end_token(self):\n        return self.EOS_TOKEN\n\n    def get_unk_index(self):\n        return self.UNK_INDEX\n\n    def get_unk_token(self):\n        return self.UNK_TOKEN\n\n    def get_vectors(self):\n        return getattr(self, ""vectors"", None)\n\n    def get_embedding(self, cls, **embedding_kwargs):\n        vector_dim = len(self.vectors[0])\n        embedding_kwargs[""vocab_size""] = self.get_size()\n\n        embedding_dim = embedding_kwargs[""embedding_dim""]\n        embedding_kwargs[""embedding_dim""] = vector_dim\n\n        embedding = None\n\n        if cls == torch.nn.Embedding:\n            embedding = torch.nn.Embedding(self.get_size(), vector_dim)\n        else:\n            embedding = cls(**embedding_kwargs)\n\n        if hasattr(embedding, ""embedding""):\n            embedding.embedding = torch.nn.Embedding.from_pretrained(\n                self.vectors, freeze=False\n            )\n        else:\n            embedding = torch.nn.Embedding.from_pretrained(self.vectors, freeze=False)\n\n        if vector_dim == embedding_dim:\n            return embedding\n        else:\n            return torch.nn.Sequential(\n                [embedding, torch.nn.Linear(vector_dim, embedding_dim)]\n            )\n\n\nclass CustomVocab(BaseVocab):\n    def __init__(self, vocab_file, embedding_file, data_dir=None, *args, **kwargs):\n        """"""Use this vocab class when you have a custom vocab as well as a\n        custom embeddings file.\n\n        This will inherit vocab class, so you will get predefined tokens with\n        this one.\n\n        IMPORTANT: To init your embedding, get your vectors from this class\'s\n        object by calling `get_vectors` function\n\n        Parameters\n        ----------\n        vocab_file : str\n            Path of custom vocabulary\n        embedding_file : str\n            Path to custom embedding inititalization file\n        data_dir : str\n            Path to data directory if embedding file is not an absolute path.\n            Default: None\n        """"""\n        super().__init__(vocab_file)\n        self.type = ""custom""\n\n        if not os.path.isabs(embedding_file) and data_dir is not None:\n            mmf_root = get_mmf_root()\n            embedding_file = os.path.join(mmf_root, data_dir, embedding_file)\n\n        if not PathManager.exists(embedding_file):\n            from mmf.common.registry import registry\n\n            writer = registry.get(""writer"")\n            error = ""Embedding file path %s doesn\'t exist"" % embedding_file\n            if writer is not None:\n                writer.write(error, ""error"")\n            raise RuntimeError(error)\n\n        embedding_vectors = torch.from_numpy(np.load(embedding_file))\n\n        self.vectors = torch.FloatTensor(self.get_size(), len(embedding_vectors[0]))\n\n        for i in range(0, 4):\n            self.vectors[i] = torch.ones_like(self.vectors[i]) * 0.1 * i\n\n        for i in range(4, self.get_size()):\n            self.vectors[i] = embedding_vectors[i - 4]\n\n\nclass IntersectedVocab(BaseVocab):\n    def __init__(self, vocab_file, embedding_name, *args, **kwargs):\n        """"""Use this vocab class when you have a custom vocabulary class but you\n        want to use pretrained embedding vectos for it. This will only load\n        the vectors which intersect with your vocabulary. Use the\n        embedding_name specified in torchtext\'s pretrained aliases:\n        [\'charngram.100d\', \'fasttext.en.300d\', \'fasttext.simple.300d\',\n         \'glove.42B.300d\', \'glove.840B.300d\', \'glove.twitter.27B.25d\',\n         \'glove.twitter.27B.50d\', \'glove.twitter.27B.100d\',\n         \'glove.twitter.27B.200d\', \'glove.6B.50d\', \'glove.6B.100d\',\n         \'glove.6B.200d\', \'glove.6B.300d\']\n\n        Parameters\n        ----------\n        vocab_file : str\n            Vocabulary file containing list of words with one word per line\n            which will be used to collect vectors\n        embedding_name : str\n            Embedding name picked up from the list of the pretrained aliases\n            mentioned above\n        """"""\n        super().__init__(vocab_file, *args, **kwargs)\n\n        self.type = ""intersected""\n\n        name = embedding_name.split(""."")[0]\n        dim = embedding_name.split(""."")[2][:-1]\n        middle = embedding_name.split(""."")[1]\n\n        class_name = EMBEDDING_NAME_CLASS_MAPPING[name]\n\n        if not hasattr(vocab, class_name):\n            from mmf.common.registry import registry\n\n            writer = registry.get(""writer"")\n            error = ""Unknown embedding type: %s"" % name, ""error""\n            if writer is not None:\n                writer.write(error, ""error"")\n            raise RuntimeError(error)\n\n        params = [middle]\n\n        if name == ""glove"":\n            params.append(int(dim))\n\n        vector_cache = get_mmf_cache_dir()\n\n        # First test loading the vectors in master so that everybody doesn\'t\n        # download it in case it doesn\'t exist\n        if is_master():\n            vocab.pretrained_aliases[embedding_name](cache=vector_cache)\n        synchronize()\n\n        embedding = getattr(vocab, class_name)(*params, cache=vector_cache)\n\n        self.vectors = torch.empty(\n            (self.get_size(), len(embedding.vectors[0])), dtype=torch.float\n        )\n\n        self.embedding_dim = len(embedding.vectors[0])\n\n        for i in range(0, 4):\n            self.vectors[i] = torch.ones_like(self.vectors[i]) * 0.1 * i\n\n        for i in range(4, self.get_size()):\n            word = self.itos[i]\n            embedding_index = embedding.stoi.get(word, None)\n\n            if embedding_index is None:\n                self.vectors[i] = self.vectors[self.UNK_INDEX]\n            else:\n                self.vectors[i] = embedding.vectors[embedding_index]\n\n    def get_embedding_dim(self):\n        return self.embedding_dim\n\n\nclass PretrainedVocab(BaseVocab):\n    def __init__(self, embedding_name, *args, **kwargs):\n        """"""Use this if you want to use pretrained embedding. See description\n        of IntersectedVocab to get a list of the embedding available from\n        torchtext\n\n        Parameters\n        ----------\n        embedding_name : str\n            Name of the pretrained alias for the embedding to used\n        """"""\n        self.type = ""pretrained""\n\n        if embedding_name not in vocab.pretrained_aliases:\n            from mmf.common.registry import registry\n\n            writer = registry.get(""writer"")\n            error = ""Unknown embedding type: %s"" % embedding_name, ""error""\n            if writer is not None:\n                writer.write(error, ""error"")\n            raise RuntimeError(error)\n\n        vector_cache = get_mmf_cache_dir()\n\n        # First test loading the vectors in master so that everybody doesn\'t\n        # download it in case it doesn\'t exist\n        if is_master():\n            vocab.pretrained_aliases[embedding_name](cache=vector_cache)\n        synchronize()\n\n        embedding = vocab.pretrained_aliases[embedding_name](cache=vector_cache)\n\n        self.UNK_INDEX = 3\n        self.stoi = defaultdict(lambda: self.UNK_INDEX)\n        self.itos = {}\n\n        self.itos[self.PAD_INDEX] = self.PAD_TOKEN\n        self.itos[self.SOS_INDEX] = self.SOS_TOKEN\n        self.itos[self.EOS_INDEX] = self.EOS_TOKEN\n        self.itos[self.UNK_INDEX] = self.UNK_TOKEN\n\n        self.stoi[self.SOS_TOKEN] = self.SOS_INDEX\n        self.stoi[self.EOS_TOKEN] = self.EOS_INDEX\n        self.stoi[self.PAD_TOKEN] = self.PAD_INDEX\n        self.stoi[self.UNK_TOKEN] = self.UNK_INDEX\n\n        self.vectors = torch.FloatTensor(\n            len(self.itos.keys()) + len(embedding.itos), len(embedding.vectors[0])\n        )\n\n        for i in range(4):\n            self.vectors[i] = torch.ones_like(self.vectors[i]) * 0.1 * i\n\n        index = 4\n        for word in embedding.stoi:\n            self.itos[index] = word\n            self.stoi[word] = index\n            actual_index = embedding.stoi[word]\n            self.vectors[index] = embedding.vectors[actual_index]\n            index += 1\n\n\nclass WordToVectorDict:\n    def __init__(self, model):\n        self.model = model\n\n    def __getitem__(self, word):\n        # Check if mean for word split needs to be done here\n        return np.mean([self.model.get_word_vector(w) for w in word.split("" "")], axis=0)\n\n\nclass ModelVocab(BaseVocab):\n    def __init__(self, name, model_file, *args, **kwargs):\n        """"""Special vocab which is not really vocabulary but instead a model\n        which returns embedding directly instead of vocabulary. This is just\n        an abstraction over a model which generates embeddings directly.\n        For e.g. for fasttext model we encapsulate it inside this and provide\n        it as a vocab so that the API of the vocab remains same.\n\n        NOTE: stoi\'s functionality will remain same but it is actually calling\n        a function to get word vectors. Currently, only fasttext is supported.\n\n        Parameters\n        ----------\n        name : str\n            Name of the embedding model which this vocab currently is loading\n        model_file : str\n            File from which model will be loaded. This API might need to be\n            changed in future.\n        """"""\n        super().__init__(*args, **kwargs)\n        self.type = ""model""\n        if name != ""fasttext"":\n            raise ValueError(""Model vocab only supports fasttext as of now"")\n        else:\n            self._load_fasttext_model(model_file)\n\n    def _load_fasttext_model(self, model_file):\n        from fastText import load_model\n        from mmf.common.registry import registry\n\n        model_file = os.path.join(get_mmf_cache_dir(), model_file)\n\n        registry.get(""writer"").write(""Loading fasttext model now from %s"" % model_file)\n\n        self.model = load_model(model_file)\n        self.stoi = WordToVectorDict(self.model)\n\n    def get_embedding_dim(self):\n        return self.model.get_dimension()\n\n\nclass ExtractedVocab(BaseVocab):\n    def __init__(self, base_path, emb_dim, *args, **kwargs):\n        """"""Special vocab which is not really vocabulary but instead a class\n        which returns embedding pre-extracted from files. Can be used load\n        word embeddings from popular models like ELMo and BERT\n\n\n        Parameters\n        ----------\n        base_path: str\n            path containing saved files with embeddings one file per txt item\n        """"""\n        super().__init__(*args, **kwargs)\n        self.type = ""extracted""\n        self.emb_dim = emb_dim\n        self.base_path = base_path\n\n    def get_dim(self):\n        return self.emb_dim\n'"
tests/common/test_sample.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport random\nimport unittest\n\nimport torch\n\nimport tests.test_utils as test_utils\nfrom mmf.common.sample import Sample, SampleList\n\n\nclass TestSample(unittest.TestCase):\n    def test_sample_working(self):\n        initial = Sample()\n        initial.x = 1\n        initial[""y""] = 2\n        # Assert setter and getter\n        self.assertEqual(initial.x, 1)\n        self.assertEqual(initial[""x""], 1)\n        self.assertEqual(initial.y, 2)\n        self.assertEqual(initial[""y""], 2)\n\n        update_dict = {""a"": 3, ""b"": {""c"": 4}}\n\n        initial.update(update_dict)\n        self.assertEqual(initial.a, 3)\n        self.assertEqual(initial[""a""], 3)\n        self.assertEqual(initial.b.c, 4)\n        self.assertEqual(initial[""b""].c, 4)\n\n\nclass TestSampleList(unittest.TestCase):\n    @test_utils.skip_if_no_cuda\n    def test_pin_memory(self):\n        sample_list = self._build_random_sample_list()\n        sample_list.pin_memory()\n\n        pin_list = [sample_list.y, sample_list.z.y]\n        non_pin_list = [sample_list.x, sample_list.z.x]\n\n        all_pinned = True\n\n        for pin in pin_list:\n            all_pinned = all_pinned and pin.is_pinned()\n\n        self.assertTrue(all_pinned)\n\n        any_pinned = False\n\n        for pin in non_pin_list:\n            any_pinned = any_pinned or (hasattr(pin, ""is_pinned"") and pin.is_pinned())\n\n        self.assertFalse(any_pinned)\n\n    def test_to_dict(self):\n        sample_list = self._build_random_sample_list()\n        sample_dict = sample_list.to_dict()\n        self.assertTrue(isinstance(sample_dict, dict))\n        # hasattr won\'t work anymore\n        self.assertFalse(hasattr(sample_dict, ""x""))\n        keys_to_assert = [""x"", ""y"", ""z"", ""z.x"", ""z.y""]\n\n        all_keys = True\n        for key in keys_to_assert:\n            current = sample_dict\n            if ""."" in key:\n                sub_keys = key.split(""."")\n                for sub_key in sub_keys:\n                    all_keys = all_keys and sub_key in current\n                    current = current[sub_key]\n            else:\n                all_keys = all_keys and key in current\n\n        self.assertTrue(all_keys)\n        self.assertTrue(isinstance(sample_dict, dict))\n\n    def _build_random_sample_list(self):\n        first = Sample()\n        first.x = random.randint(0, 100)\n        first.y = torch.rand((5, 4))\n        first.z = Sample()\n        first.z.x = random.randint(0, 100)\n        first.z.y = torch.rand((6, 4))\n\n        second = Sample()\n        second.x = random.randint(0, 100)\n        second.y = torch.rand((5, 4))\n        second.z = Sample()\n        second.z.x = random.randint(0, 100)\n        second.z.y = torch.rand((6, 4))\n\n        return SampleList([first, second])\n'"
tests/configs/test_configs_for_keys.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport contextlib\nimport unittest\nimport warnings\nfrom io import StringIO\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.configuration import Configuration\nfrom mmf.utils.env import setup_imports\nfrom tests.test_utils import dummy_args\n\n\nclass TestConfigsForKeys(unittest.TestCase):\n    def setUp(self):\n        setup_imports()\n\n    def test_model_configs_for_keys(self):\n        models_mapping = registry.mapping[""model_name_mapping""]\n\n        for model_key, model_cls in models_mapping.items():\n            if model_cls.config_path() is None:\n                warnings.warn(\n                    (\n                        ""Model {} has no default configuration defined. ""\n                        + ""Skipping it. Make sure it is intentional""\n                    ).format(model_key)\n                )\n                continue\n\n            with contextlib.redirect_stdout(StringIO()):\n                args = dummy_args(model=model_key)\n                configuration = Configuration(args)\n                configuration.freeze()\n                config = configuration.get_config()\n                self.assertTrue(\n                    model_key in config.model_config,\n                    ""Key for model {} doesn\'t exists in its configuration"".format(\n                        model_key\n                    ),\n                )\n\n    def test_dataset_configs_for_keys(self):\n        builder_name = registry.mapping[""builder_name_mapping""]\n\n        for builder_key, builder_cls in builder_name.items():\n            if builder_cls.config_path() is None:\n                warnings.warn(\n                    (\n                        ""Dataset {} has no default configuration defined. ""\n                        + ""Skipping it. Make sure it is intentional""\n                    ).format(builder_key)\n                )\n                continue\n\n            with contextlib.redirect_stdout(StringIO()):\n                args = dummy_args(dataset=builder_key)\n                configuration = Configuration(args)\n                configuration.freeze()\n                config = configuration.get_config()\n                self.assertTrue(\n                    builder_key in config.dataset_config,\n                    ""Key for dataset {} doesn\'t exists in its configuration"".format(\n                        builder_key\n                    ),\n                )\n'"
tests/configs/test_zoo_urls.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport unittest\n\nfrom omegaconf import OmegaConf\n\nfrom mmf.utils.configuration import load_yaml\nfrom mmf.utils.download import DownloadableFile, check_header\nfrom tests.test_utils import skip_if_no_network\n\n\nclass TestConfigsForKeys(unittest.TestCase):\n    def _test_zoo_for_keys(self, path):\n        zoo_config = load_yaml(path)\n        self._recurse_on_config(zoo_config)\n\n    def _recurse_on_config(self, config):\n        if OmegaConf.is_list(config) and len(config) > 0 and ""url"" in config[0]:\n            # Found the urls, let\'s test them\n            for item in config:\n                # First try making the DownloadableFile class to make sure\n                # everything is fine\n                download = DownloadableFile(**item)\n                # Now check the actual header\n                check_header(download._url, from_google=download._from_google)\n        elif OmegaConf.is_dict(config):\n            # Both version and resources should be present\n            if ""version"" in config:\n                self.assertIn(""resources"", config)\n            if ""resources"" in config:\n                self.assertIn(""version"", config)\n\n            # Let\'s continue recursing\n            for item in config:\n                self._recurse_on_config(config[item])\n\n    @skip_if_no_network\n    def test_zoos(self):\n        self._test_zoo_for_keys(""configs/zoo/datasets.yaml"")\n        self._test_zoo_for_keys(""configs/zoo/models.yaml"")\n'"
tests/datasets/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
tests/datasets/test_base_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport unittest\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.base_dataset import BaseDataset\nfrom mmf.utils.configuration import Configuration\n\nfrom ..test_utils import dummy_args\n\n\nclass TestBaseDataset(unittest.TestCase):\n    def test_init_processors(self):\n        path = os.path.join(\n            os.path.abspath(__file__),\n            ""../../../mmf/configs/datasets/vqa2/defaults.yaml"",\n        )\n        args = dummy_args()\n        args.opts.append(f""config={path}"")\n        configuration = Configuration(args)\n        answer_processor = (\n            configuration.get_config().dataset_config.vqa2.processors.answer_processor\n        )\n        vocab_path = os.path.join(\n            os.path.abspath(__file__), "".."", "".."", ""data"", ""vocab.txt""\n        )\n        answer_processor.params.vocab_file = os.path.abspath(vocab_path)\n        self._fix_configuration(configuration)\n        configuration.freeze()\n\n        base_dataset = BaseDataset(\n            ""vqa2"", configuration.get_config().dataset_config.vqa2, ""train""\n        )\n        expected_processors = [\n            ""answer_processor"",\n            ""ocr_token_processor"",\n            ""bbox_processor"",\n        ]\n\n        # Check no processors are initialized before init_processors call\n        self.assertFalse(any(hasattr(base_dataset, key) for key in expected_processors))\n\n        for processor in expected_processors:\n            self.assertIsNone(registry.get(""{}_{}"".format(""vqa2"", processor)))\n\n        # Check processors are initialized after init_processors\n        base_dataset.init_processors()\n        self.assertTrue(all(hasattr(base_dataset, key) for key in expected_processors))\n        for processor in expected_processors:\n            self.assertIsNotNone(registry.get(""{}_{}"".format(""vqa2"", processor)))\n\n    def _fix_configuration(self, configuration):\n        vqa2_config = configuration.config.dataset_config.vqa2\n        processors = vqa2_config.processors\n        processors.pop(""text_processor"")\n        processors.pop(""context_processor"")\n'"
tests/datasets/test_processors.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport unittest\n\nimport torch\n\nfrom mmf.datasets.processors.processors import (\n    CaptionProcessor,\n    EvalAIAnswerProcessor,\n    MultiHotAnswerFromVocabProcessor,\n)\nfrom mmf.utils.configuration import load_yaml\n\nfrom ..test_utils import compare_tensors\n\n\nclass TestDatasetProcessors(unittest.TestCase):\n    def _get_config(self, path):\n        path = os.path.join(os.path.abspath(__file__), path)\n        config = load_yaml(os.path.abspath(path))\n        return config\n\n    def test_caption_processor(self):\n        config = self._get_config(""../../../mmf/configs/datasets/coco/defaults.yaml"")\n        captioning_config = config.dataset_config.coco\n        caption_processor_config = captioning_config.processors.caption_processor\n\n        vocab_path = os.path.join(\n            os.path.abspath(__file__), "".."", "".."", ""data"", ""vocab.txt""\n        )\n        caption_processor_config.params.vocab.type = ""random""\n        caption_processor_config.params.vocab.vocab_file = os.path.abspath(vocab_path)\n        caption_processor = CaptionProcessor(caption_processor_config.params)\n\n        tokens = [1, 4, 5, 6, 4, 7, 8, 2, 0, 0, 0]\n        caption = caption_processor(tokens)\n\n        # Test start, stop, pad are removed\n        self.assertNotIn(""<s>"", caption[""tokens""])\n        self.assertNotIn(""</s>"", caption[""tokens""])\n        self.assertNotIn(""<pad>"", caption[""tokens""])\n\n        # Test caption is correct\n        self.assertEqual(caption[""caption""], ""a man with a red helmet"")\n\n    def test_multi_hot_answer_from_vocab_processor(self):\n        config = self._get_config(""../../../mmf/configs/datasets/clevr/defaults.yaml"")\n        clevr_config = config.dataset_config.clevr\n        answer_processor_config = clevr_config.processors.answer_processor\n\n        # Test num_answers==1 case\n        vocab_path = os.path.join(\n            os.path.abspath(__file__), "".."", "".."", ""data"", ""vocab.txt""\n        )\n        answer_processor_config.params.vocab_file = os.path.abspath(vocab_path)\n        answer_processor = MultiHotAnswerFromVocabProcessor(\n            answer_processor_config.params\n        )\n        processed = answer_processor({""answers"": [""helmet""]})\n        answers_indices = processed[""answers_indices""]\n        answers_scores = processed[""answers_scores""]\n\n        self.assertTrue(\n            compare_tensors(answers_indices, torch.tensor([5] * 10, dtype=torch.long))\n        )\n        expected_answers_scores = torch.zeros(19, dtype=torch.float)\n        expected_answers_scores[5] = 1.0\n        self.assertTrue(compare_tensors(answers_scores, expected_answers_scores))\n\n        # Test multihot when num answers greater than 1\n        answer_processor_config.params.vocab_file = os.path.abspath(vocab_path)\n        answer_processor_config.params.num_answers = 3\n        answer_processor = MultiHotAnswerFromVocabProcessor(\n            answer_processor_config.params\n        )\n        processed = answer_processor({""answers"": [""man"", ""with"", ""countryside""]})\n        answers_indices = processed[""answers_indices""]\n        answers_scores = processed[""answers_scores""]\n        self.assertTrue(\n            compare_tensors(\n                answers_indices,\n                torch.tensor([2, 3, 15, 2, 3, 15, 2, 3, 15, 2], dtype=torch.long),\n            )\n        )\n        expected_answers_scores = torch.zeros(19, dtype=torch.float)\n        expected_answers_scores[2] = 1.0\n        expected_answers_scores[3] = 1.0\n        expected_answers_scores[15] = 1.0\n        self.assertTrue(compare_tensors(answers_scores, expected_answers_scores))\n\n        # Test unk\n        processed = answer_processor({""answers"": [""test"", ""answer"", ""man""]})\n        answers_indices = processed[""answers_indices""]\n        answers_scores = processed[""answers_scores""]\n        self.assertTrue(\n            compare_tensors(\n                answers_indices,\n                torch.tensor([0, 0, 2, 0, 0, 2, 0, 0, 2, 0], dtype=torch.long),\n            )\n        )\n        expected_answers_scores = torch.zeros(19, dtype=torch.float)\n        expected_answers_scores[2] = 1.0\n        self.assertTrue(compare_tensors(answers_scores, expected_answers_scores))\n\n    def test_evalai_answer_processor(self):\n        evalai_answer_processor = EvalAIAnswerProcessor()\n\n        # Test number\n        processed = evalai_answer_processor(""two"")\n        expected = ""2""\n        self.assertEqual(processed, expected)\n\n        # Test article\n        processed = evalai_answer_processor(""a building"")\n        expected = ""building""\n        self.assertEqual(processed, expected)\n\n        # Test tokenize\n        processed = evalai_answer_processor(""snow, mountain"")\n        expected = ""snow mountain""\n        self.assertEqual(processed, expected)\n\n        # Test contractions\n        processed = evalai_answer_processor(""isnt"")\n        expected = ""isn\'t""\n        self.assertEqual(processed, expected)\n\n        # Test processor\n        processed = evalai_answer_processor(""the two mountain\'s \\t \\n   "")\n        expected = ""2 mountain \'s""\n        self.assertEqual(processed, expected)\n'"
tests/models/test_cnn_lstm.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.common.sample import Sample, SampleList\nfrom mmf.models.cnn_lstm import CNNLSTM\nfrom mmf.utils.configuration import Configuration\nfrom mmf.utils.general import get_mmf_root\nfrom tests.test_utils import dummy_args\n\n\nclass TestModelCNNLSTM(unittest.TestCase):\n    def setUp(self):\n        torch.manual_seed(1234)\n        registry.register(""clevr_text_vocab_size"", 80)\n        registry.register(""clevr_num_final_outputs"", 32)\n        config_path = os.path.join(\n            get_mmf_root(),\n            "".."",\n            ""projects"",\n            ""others"",\n            ""cnn_lstm"",\n            ""clevr"",\n            ""defaults.yaml"",\n        )\n        config_path = os.path.abspath(config_path)\n        args = dummy_args(model=""cnn_lstm"", dataset=""clevr"")\n        args.opts.append(f""config={config_path}"")\n        configuration = Configuration(args)\n        configuration.config.datasets = ""clevr""\n        configuration.freeze()\n        self.config = configuration.config\n        registry.register(""config"", self.config)\n\n    def test_forward(self):\n        model_config = self.config.model_config.cnn_lstm\n\n        cnn_lstm = CNNLSTM(model_config)\n        cnn_lstm.build()\n        cnn_lstm.init_losses()\n\n        self.assertTrue(isinstance(cnn_lstm, torch.nn.Module))\n\n        test_sample = Sample()\n        test_sample.text = torch.randint(1, 79, (10,), dtype=torch.long)\n        test_sample.image = torch.randn(3, 320, 480)\n        test_sample.targets = torch.randn(32)\n\n        test_sample_list = SampleList([test_sample])\n        test_sample_list.dataset_type = ""train""\n        test_sample_list.dataset_name = ""clevr""\n        output = cnn_lstm(test_sample_list)\n\n        scores = output[""scores""]\n        loss = output[""losses""][""train/clevr/logit_bce""]\n\n        np.testing.assert_almost_equal(loss.item(), 19.2635, decimal=4)\n        self.assertEqual(scores.size(), torch.Size((1, 32)))\n'"
tests/modules/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
tests/modules/test_fusions.py,21,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport unittest\n\nimport torch\n\nimport mmf.modules.fusions as fusions\n\n\nclass TestModuleFusions(unittest.TestCase):\n    def setUp(self):\n        bsize = 2\n        self.x = [torch.randn(bsize, 10), torch.randn(bsize, 20)]\n        self.input_dims = [self.x[0].shape[-1], self.x[1].shape[-1]]\n        self.output_dims = 2\n\n    def test_BlockFusion(self):\n        fusion = fusions.Block(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_BlockTucker(self):\n        fusion = fusions.BlockTucker(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_Mutan(self):\n        fusion = fusions.Mutan(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_Tucker(self):\n        fusion = fusions.Tucker(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_MLB(self):\n        fusion = fusions.MLB(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_MFB(self):\n        fusion = fusions.MFB(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_MFH(self):\n        fusion = fusions.MFH(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_MCB(self):\n        fusion = fusions.MCB(self.input_dims, self.output_dims, mm_dim=100)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_LinearSum(self):\n        fusion = fusions.LinearSum(self.input_dims, self.output_dims, mm_dim=20)\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n\n    def test_ConcatMLP(self):\n        fusion = fusions.ConcatMLP(self.input_dims, self.output_dims, dimensions=[5, 5])\n        out = fusion(self.x)\n        if torch.cuda.is_available():\n            fusion.cuda()\n            out = fusion([self.x[0].cuda(), self.x[1].cuda()])\n        assert torch.Size([2, 2]) == out.shape\n'"
tests/modules/test_layers.py,19,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport functools\nimport operator\nimport random\nimport unittest\n\nimport numpy as np\nimport torch\nfrom omegaconf import OmegaConf\n\nimport mmf.modules.layers as layers\n\n\nclass TestModuleLayers(unittest.TestCase):\n    def setUp(self):\n        torch.manual_seed(1234)\n\n    def test_conv_net(self):\n        conv_net = layers.ConvNet(150, 75, 3)\n\n        input_tensor = torch.randn(4, 150, 64, 64)\n        output = conv_net(input_tensor)\n        expected_size = torch.Size((4, 75, 32, 32))\n        self.assertEqual(output.size(), expected_size)\n        # Since seed is fix we can check some of tensor values\n        np.testing.assert_almost_equal(output[0][0][0][0].item(), 0.149190, decimal=5)\n        np.testing.assert_almost_equal(\n            output[3][74][31][31].item(), -0.25199, decimal=5\n        )\n\n    def test_flatten(self):\n        flatten = layers.Flatten()\n\n        # Test 3 dim\n        input_tensor = torch.randn(5, 6, 10)\n        expected_size = torch.Size((5, 60))\n        actual_size = flatten(input_tensor).size()\n        self.assertEqual(actual_size, expected_size)\n\n        # Test 1 dim\n        input_tensor = torch.randn(5)\n        expected_size = torch.Size((5,))\n        actual_size = flatten(input_tensor).size()\n        self.assertEqual(actual_size, expected_size)\n\n        # Test 6 dim\n        size_list = [random.randint(2, 4) for _ in range(7)]\n        expected_size = torch.Size(\n            (size_list[0], functools.reduce(operator.mul, size_list[1:]))\n        )\n        input_tensor = torch.randn(*size_list)\n        actual_size = flatten(input_tensor).size()\n        self.assertEqual(actual_size, expected_size)\n\n    def test_unflatten(self):\n        unflatten = layers.UnFlatten()\n\n        # Test 2 dim to 3 dim\n        input_tensor = torch.randn(5, 60)\n        expected_size = torch.Size((5, 6, 10))\n        actual_size = unflatten(input_tensor, sizes=[6, 10]).size()\n        self.assertEqual(actual_size, expected_size)\n\n        # Test 1 dim\n        input_tensor = torch.randn(5)\n        expected_size = torch.Size((5,))\n        actual_size = unflatten(input_tensor, sizes=[]).size()\n        self.assertEqual(expected_size, actual_size)\n\n    def test_mlp(self):\n        mlp = layers.ClassifierLayer(""mlp"", in_dim=300, out_dim=1)\n        self.assertEqual(len(list(mlp.module.layers.children())), 1)\n        self.assertEqual(len(list(mlp.parameters())), 2)\n\n        inp = torch.rand(3, 300)\n\n        output = mlp(inp)\n        self.assertEqual(output.size(), torch.Size((3, 1)))\n        np.testing.assert_almost_equal(\n            output.squeeze().tolist(), [0.1949174, 0.4030975, -0.0109139]\n        )\n\n        mlp = layers.ClassifierLayer(\n            ""mlp"", in_dim=300, out_dim=1, hidden_dim=150, num_layers=1\n        )\n\n        self.assertEqual(len(list(mlp.module.layers.children())), 5)\n        self.assertEqual(len(list(mlp.parameters())), 6)\n\n        inp = torch.rand(3, 300)\n\n        output = mlp(inp)\n        self.assertEqual(output.size(), torch.Size((3, 1)))\n        np.testing.assert_almost_equal(\n            output.squeeze().tolist(), [-0.503411, 0.1725615, -0.6833304], decimal=3\n        )\n\n    def test_bert_classifier_head(self):\n        config = {}\n        config[""hidden_size""] = 768\n        config[""hidden_act""] = ""gelu""\n        config[""layer_norm_eps""] = 1e-12\n        config[""hidden_dropout_prob""] = 0.1\n        config = OmegaConf.create(config)\n        clf = layers.ClassifierLayer(""bert"", 768, 1, config=config)\n        self.assertEqual(len(list(clf.module.children())), 3)\n        self.assertEqual(len(list(clf.parameters())), 6)\n\n        inp = torch.rand(3, 768)\n\n        output = clf(inp)\n        self.assertEqual(output.size(), torch.Size((3, 1)))\n        np.testing.assert_almost_equal(\n            output.squeeze().tolist(), [0.5452202, -0.0437842, -0.377468], decimal=3\n        )\n'"
tests/modules/test_losses.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport collections\nimport unittest\nfrom unittest.mock import MagicMock\n\nimport torch\n\nimport mmf.modules.losses as losses\nfrom mmf.common.registry import registry\nfrom mmf.common.sample import SampleList\n\nRETURN_VALUE = torch.tensor(1.0)\n\n\ndef build_loss_side_effect(return_value=RETURN_VALUE):\n    def loss_side_effect(item):\n        loss_object_mock = MagicMock(return_value=return_value)\n        loss_class_mock = MagicMock(return_value=loss_object_mock)\n        valid_losses = [""cross_entropy"", ""multi""]\n        if isinstance(item, collections.abc.MutableMapping):\n            if item[""type""] not in valid_losses:\n                return None\n        elif item not in valid_losses:\n            return None\n        else:\n            return loss_class_mock\n\n    return loss_side_effect\n\n\nclass TestModuleLosses(unittest.TestCase):\n    def setUp(self):\n        torch.manual_seed(1234)\n\n    def test_mmf_loss(self):\n        get_loss_class_mock = MagicMock(side_effect=build_loss_side_effect())\n        registry.get_loss_class = get_loss_class_mock\n        # Test if MMFLoss accepts empty parameters\n        self.assertRaises(ValueError, losses.MMFLoss)\n        self.assertTrue(losses.MMFLoss({""type"": ""cross_entropy""}).name, ""cross_entropy"")\n        self.assertTrue(losses.MMFLoss(""cross_entropy"").name, ""cross_entropy"")\n        self.assertRaises(AssertionError, losses.MMFLoss, [])\n        # Multi requires dict\n        self.assertRaises(AssertionError, losses.MMFLoss, ""multi"")\n\n        cross_entropy = losses.MMFLoss(""cross_entropy"")\n        cross_entropy_from_dict = losses.MMFLoss({""type"": ""cross_entropy""})\n        sample_list = SampleList()\n        sample_list.dataset_type = ""val""\n        sample_list.dataset_name = ""vqa2""\n\n        output = cross_entropy(sample_list, {})\n        output_from_dict = cross_entropy_from_dict(sample_list, {})\n\n        self.assertEqual(output, {""val/vqa2/cross_entropy"": torch.tensor(1.0)})\n        self.assertEqual(output_from_dict, output)\n\n        get_loss_class_mock.side_effect = build_loss_side_effect(1.0)\n        output = cross_entropy(sample_list, {})\n\n        self.assertEqual(output, {""val/vqa2/cross_entropy"": torch.tensor(1.0)})\n        self.assertEqual(output_from_dict, output)\n\n        self.assertTrue(get_loss_class_mock.called)\n        self.assertEqual(get_loss_class_mock.call_count, 5)\n\n    def test_caption_cross_entropy(self):\n        caption_ce_loss = losses.CaptionCrossEntropyLoss()\n\n        expected = dict()\n        predicted = dict()\n\n        # Test complete match\n        expected[""targets""] = torch.empty((1, 10), dtype=torch.long)\n        expected[""targets""].fill_(4)\n        predicted[""scores""] = torch.zeros((1, 10, 10))\n        predicted[""scores""][:, :, 4] = 100.0\n\n        self.assertEqual(caption_ce_loss(expected, predicted).item(), 0.0)\n\n        # Test random initialized\n        torch.manual_seed(1234)\n        expected[""targets""] = torch.randint(0, 9491, (5, 10))\n        predicted[""scores""] = torch.rand((5, 10, 9491))\n\n        self.assertAlmostEqual(caption_ce_loss(expected, predicted).item(), 9.2507, 4)\n'"
tests/modules/test_metrics.py,18,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport unittest\n\nimport torch\n\nimport mmf.modules.metrics as metrics\nfrom mmf.common.registry import registry\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.processors import CaptionProcessor\nfrom mmf.utils.configuration import load_yaml\n\n\nclass TestModuleMetrics(unittest.TestCase):\n    def test_caption_bleu4(self):\n        path = os.path.join(\n            os.path.abspath(__file__),\n            ""../../../mmf/configs/datasets/coco/defaults.yaml"",\n        )\n        config = load_yaml(os.path.abspath(path))\n        captioning_config = config.dataset_config.coco\n        caption_processor_config = captioning_config.processors.caption_processor\n        vocab_path = os.path.join(\n            os.path.abspath(__file__), "".."", "".."", ""data"", ""vocab.txt""\n        )\n        caption_processor_config.params.vocab.type = ""random""\n        caption_processor_config.params.vocab.vocab_file = os.path.abspath(vocab_path)\n        caption_processor = CaptionProcessor(caption_processor_config.params)\n        registry.register(""coco_caption_processor"", caption_processor)\n\n        caption_bleu4 = metrics.CaptionBleu4Metric()\n        expected = Sample()\n        predicted = dict()\n\n        # Test complete match\n        expected.answers = torch.empty((5, 5, 10))\n        expected.answers.fill_(4)\n        predicted[""scores""] = torch.zeros((5, 10, 19))\n        predicted[""scores""][:, :, 4] = 1.0\n\n        self.assertEqual(caption_bleu4.calculate(expected, predicted).item(), 1.0)\n\n        # Test partial match\n        expected.answers = torch.empty((5, 5, 10))\n        expected.answers.fill_(4)\n        predicted[""scores""] = torch.zeros((5, 10, 19))\n        predicted[""scores""][:, 0:5, 4] = 1.0\n        predicted[""scores""][:, 5:, 18] = 1.0\n\n        self.assertAlmostEqual(\n            caption_bleu4.calculate(expected, predicted).item(), 0.3928, 4\n        )\n\n    def _test_binary_metric(self, metric, value):\n        sample = Sample()\n        predicted = dict()\n\n        sample.targets = torch.tensor(\n            [[0, 1], [1, 0], [1, 0], [0, 1]], dtype=torch.float\n        )\n        predicted[""scores""] = torch.tensor(\n            [\n                [-0.9332, 0.8149],\n                [-0.8391, 0.6797],\n                [-0.7235, 0.7220],\n                [-0.9043, 0.3078],\n            ],\n            dtype=torch.float,\n        )\n        self.assertAlmostEqual(metric.calculate(sample, predicted).item(), value, 4)\n\n        sample.targets = torch.tensor([1, 0, 0, 1], dtype=torch.long)\n        self.assertAlmostEqual(metric.calculate(sample, predicted).item(), value, 4)\n\n    def _test_multiclass_metric(self, metric, value):\n        sample = Sample()\n        predicted = dict()\n\n        sample.targets = torch.tensor(\n            [[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 0, 1]], dtype=torch.float\n        )\n        predicted[""scores""] = torch.tensor(\n            [\n                [-0.9332, 0.8149, 0.3491],\n                [-0.8391, 0.6797, -0.3410],\n                [-0.7235, 0.7220, 0.9104],\n                [0.9043, 0.3078, -0.4210],\n            ],\n            dtype=torch.float,\n        )\n        self.assertAlmostEqual(metric.calculate(sample, predicted).item(), value, 4)\n\n        sample.targets = torch.tensor([1, 2, 0, 2], dtype=torch.long)\n        self.assertAlmostEqual(metric.calculate(sample, predicted).item(), value, 4)\n\n    def _test_multilabel_metric(self, metric, value):\n        sample = Sample()\n        predicted = dict()\n\n        sample.targets = torch.tensor(\n            [[0, 1, 1], [1, 0, 1], [1, 0, 1], [0, 0, 1]], dtype=torch.float\n        )\n        predicted[""scores""] = torch.tensor(\n            [\n                [-0.9332, 0.8149, 0.3491],\n                [-0.8391, 0.6797, -0.3410],\n                [-0.7235, 0.7220, 0.9104],\n                [0.9043, 0.3078, -0.4210],\n            ],\n            dtype=torch.float,\n        )\n        self.assertAlmostEqual(metric.calculate(sample, predicted).item(), value, 4)\n\n    def test_micro_f1(self):\n        metric = metrics.MicroF1()\n        self._test_binary_metric(metric, 0.5)\n        self._test_multiclass_metric(metric, 0.25)\n\n    def test_macro_f1(self):\n        metric = metrics.MacroF1()\n        self._test_binary_metric(metric, 0.3333)\n        self._test_multiclass_metric(metric, 0.2222)\n\n    def test_binary_f1(self):\n        metric = metrics.BinaryF1()\n        self._test_binary_metric(metric, 0.66666666)\n\n    def test_multilabel_micro_f1(self):\n        metric = metrics.MultiLabelMicroF1()\n        self._test_binary_metric(metric, 0.5)\n\n    def test_multilabel_macro_f1(self):\n        metric = metrics.MultiLabelMacroF1()\n        self._test_multilabel_metric(metric, 0.13333)\n\n    def test_macro_roc_auc(self):\n        metric = metrics.MacroROC_AUC()\n        self._test_binary_metric(metric, 0.5)\n        self._test_multiclass_metric(metric, 0.2222)\n\n    def test_micro_roc_auc(self):\n        metric = metrics.MicroROC_AUC()\n        self._test_binary_metric(metric, 0.5)\n        self._test_multiclass_metric(metric, 0.34375)\n\n    def test_micro_ap(self):\n        metric = metrics.MicroAP()\n        self._test_binary_metric(metric, 0.5)\n        self._test_multiclass_metric(metric, 0.34375)\n\n    def test_macro_ap(self):\n        metric = metrics.MacroAP()\n        self._test_binary_metric(metric, 0.5)\n        self._test_multiclass_metric(metric, 0.2222)\n'"
tests/utils/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
tests/utils/test_checkpoint.py,15,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport contextlib\nimport os\nimport tempfile\nimport unittest\nfrom copy import deepcopy\nfrom io import StringIO\nfrom unittest.mock import Mock, patch\n\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom mmf.common.registry import registry\nfrom mmf.models.base_model import BaseModel\nfrom mmf.utils.checkpoint import Checkpoint\nfrom mmf.utils.configuration import load_yaml\nfrom mmf.utils.early_stopping import EarlyStopping\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.logger import Logger\nfrom tests.test_utils import compare_state_dicts\n\n\n@contextlib.contextmanager\ndef mock_env_with_temp():\n    d = tempfile.TemporaryDirectory()\n    patched = patch(""mmf.utils.checkpoint.get_mmf_env"", return_value=d.name)\n    patched.start()\n    yield d.name\n    d.cleanup()\n    patched.stop()\n\n\nclass SimpleModule(BaseModel):\n    def __init__(self, config={}):\n        super().__init__(config)\n        self.base = torch.nn.Sequential(\n            torch.nn.Linear(5, 4), torch.nn.Tanh(), torch.nn.Linear(4, 5)\n        )\n\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(5, 4), torch.nn.Tanh(), torch.nn.Linear(4, 5)\n        )\n\n        self.loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, x, target):\n        x = self.classifier(self.base(x))\n        return {""losses"": {""total_loss"": self.loss(x, target)}}\n\n\nclass OnlyBase(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.base_test = torch.nn.Sequential(\n            torch.nn.Linear(5, 4), torch.nn.Tanh(), torch.nn.Linear(4, 5)\n        )\n\n    def format_state_key(self, key):\n        return key\n\n\nclass TestUtilsCheckpoint(unittest.TestCase):\n    def setUp(self):\n        import argparse\n\n        torch.manual_seed(1234)\n        # An easy way to get a AttributeDict object\n        self.trainer = argparse.Namespace()\n        self.config = OmegaConf.create(\n            {\n                ""model"": ""simple"",\n                ""model_config"": {},\n                ""checkpoint"": {\n                    ""save_git_details"": False,\n                    ""reset"": {""optimizer"": False, ""counts"": False, ""all"": False},\n                    ""pretrained_state_mapping"": {""base_test"": ""base""},\n                },\n                ""config_override"": ""test"",\n            }\n        )\n        # Keep original copy for testing purposes\n        self.trainer.config = deepcopy(self.config)\n        self.trainer.writer = Mock(spec=Logger)\n\n        self.trainer.model = SimpleModule()\n\n        self.trainer.optimizer = torch.optim.Adam(\n            self.trainer.model.parameters(), lr=1e-01\n        )\n\n    def test_save_config(self):\n        with mock_env_with_temp() as d:\n            Checkpoint(self.trainer)\n            config = load_yaml(os.path.join(d, ""config.yaml""))\n            self.assertTrue(config == self.config)\n            self.assertTrue(config == self.trainer.config)\n\n    def test_save_and_load_state_dict(self):\n        with mock_env_with_temp() as d:\n            checkpoint = Checkpoint(self.trainer)\n            self._init_early_stopping(checkpoint)\n            self._do_a_pass()\n            # Test normal case\n            checkpoint.save(1500)\n\n            self.assertTrue(\n                PathManager.exists(os.path.join(d, ""models"", ""model_1500.ckpt""))\n            )\n            self.assertTrue(PathManager.exists(os.path.join(d, ""current.ckpt"")))\n            self.assertFalse(PathManager.exists(os.path.join(d, ""best.ckpt"")))\n            os.remove(os.path.join(d, ""models"", ""model_1500.ckpt""))\n            os.remove(os.path.join(d, ""current.ckpt""))\n\n            best_model = deepcopy(self.trainer.model)\n            best_optimizer = deepcopy(self.trainer.optimizer)\n            # Test with update_best\n            checkpoint.save(2000, update_best=True)\n\n            self.assertTrue(\n                PathManager.exists(os.path.join(d, ""models"", ""model_2000.ckpt""))\n            )\n            self.assertTrue(PathManager.exists(os.path.join(d, ""best.ckpt"")))\n            self.assertTrue(PathManager.exists(os.path.join(d, ""current.ckpt"")))\n\n            self._do_a_pass()\n            checkpoint.save(2500)\n\n            # Test resume\n            self.trainer.config.checkpoint.resume = True\n\n            current_model = deepcopy(self.trainer.model)\n            current_optimizer = deepcopy(self.trainer.optimizer)\n            checkpoint.load_state_dict()\n\n            self.assertFalse(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), best_model.state_dict()\n                )\n            )\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), current_model.state_dict()\n                )\n            )\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, best_optimizer)\n            )\n            self.assertFalse(\n                self._compare_optimizers(\n                    self.trainer.optimizer, best_optimizer, skip_keys=True\n                )\n            )\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, current_optimizer)\n            )\n            self.assertTrue(\n                self._compare_optimizers(\n                    self.trainer.optimizer, current_optimizer, skip_keys=True\n                )\n            )\n\n            base_0_weight_current = self.trainer.model.base[0].weight.data.clone()\n\n            # Test resume_best\n            self.trainer.config.checkpoint.resume = True\n            self.trainer.config.checkpoint.resume_best = True\n\n            checkpoint.load_state_dict()\n\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), best_model.state_dict()\n                )\n            )\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, best_optimizer)\n            )\n            self.assertTrue(\n                self._compare_optimizers(\n                    self.trainer.optimizer, best_optimizer, skip_keys=True\n                )\n            )\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, current_optimizer)\n            )\n            self.assertFalse(\n                self._compare_optimizers(\n                    self.trainer.optimizer, current_optimizer, skip_keys=True\n                )\n            )\n            base_0_weight_best = self.trainer.model.base[0].weight.data.clone()\n\n            self.trainer.config.checkpoint.resume_best = False\n            # Test distributed settings\n            self.trainer.model = torch.nn.DataParallel(self.trainer.model)\n            checkpoint.load_state_dict()\n\n            weight_to_be_tested = self.trainer.model.module.base[0].weight\n            weight_device = weight_to_be_tested.device\n\n            self.assertTrue(\n                torch.equal(\n                    weight_to_be_tested, base_0_weight_current.to(weight_device)\n                )\n            )\n            self.assertFalse(\n                torch.equal(weight_to_be_tested, base_0_weight_best.to(weight_device))\n            )\n\n    def test_finalize_and_restore_from_it(self):\n        with mock_env_with_temp():\n            checkpoint = Checkpoint(self.trainer)\n            self._init_early_stopping(checkpoint)\n            original_model = deepcopy(self.trainer.model)\n            self._do_a_pass()\n            model_1500 = deepcopy(self.trainer.model)\n            checkpoint.save(1500)\n\n            swap = self.trainer.model\n            self.trainer.model = original_model\n            checkpoint.restore()\n            # First test without best.ckpt\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), original_model.state_dict()\n                )\n            )\n            self.assertFalse(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), model_1500.state_dict()\n                )\n            )\n\n            self.trainer.model = swap\n\n            self._do_a_pass()\n            model_2000 = deepcopy(self.trainer.model)\n            checkpoint.save(2000, update_best=True)\n\n            self._do_a_pass()\n            model_2500 = deepcopy(self.trainer.model)\n            checkpoint.save(2500)\n\n            checkpoint.restore()\n\n            self.assertFalse(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), original_model.state_dict()\n                )\n            )\n            self.assertFalse(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), model_1500.state_dict()\n                )\n            )\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), model_2000.state_dict()\n                )\n            )\n            self.assertFalse(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), model_2500.state_dict()\n                )\n            )\n\n    def test_finalize_and_resume_file(self):\n        with mock_env_with_temp() as d:\n            checkpoint = Checkpoint(self.trainer)\n            self._init_early_stopping(checkpoint)\n            self._do_a_pass()\n            checkpoint.finalize()\n            original = deepcopy(self.trainer.model)\n            pth_path = os.path.join(d, ""simple_final.pth"")\n            self.assertTrue(PathManager.exists(pth_path))\n\n            self._do_a_pass()\n\n            after_a_pass = deepcopy(self.trainer.model)\n            original_optimizer = deepcopy(self.trainer.optimizer)\n            self.trainer.config.checkpoint.resume_file = pth_path\n\n            with contextlib.redirect_stdout(StringIO()):\n                checkpoint.load_state_dict()\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), original.state_dict()\n                )\n            )\n            self.assertFalse(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), after_a_pass.state_dict()\n                )\n            )\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, original_optimizer)\n            )\n            # Keys will not be same as we just updated the model\n            self.assertTrue(\n                self._compare_optimizers(\n                    self.trainer.optimizer, original_optimizer, skip_keys=True\n                )\n            )\n\n    def test_resets(self):\n        with mock_env_with_temp():\n            checkpoint = Checkpoint(self.trainer)\n            self._init_early_stopping(checkpoint)\n            self._do_a_pass()\n\n            original_optimizer = deepcopy(self.trainer.optimizer)\n            original_model = deepcopy(self.trainer.model)\n\n            self.trainer.current_epoch = 3\n            checkpoint.save(2000, update_best=True)\n            self.trainer.current_epoch = 4\n            # Test reset all\n            self.trainer.config.checkpoint.resume = True\n            self.trainer.config.checkpoint.reset.all = True\n            checkpoint.load_state_dict()\n\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), original_model.state_dict()\n                )\n            )\n\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, original_optimizer)\n            )\n            self.assertTrue(\n                self._compare_optimizers(\n                    self.trainer.optimizer, original_optimizer, skip_keys=True\n                )\n            )\n            self.assertEqual(self.trainer.num_updates, 0)\n            self.assertEqual(self.trainer.current_iteration, 0)\n            self.assertEqual(self.trainer.current_epoch, 4)\n\n            # Test reset_optimizer\n            self._init_early_stopping(checkpoint)\n            self.trainer.config.checkpoint.reset.all = False\n            self.trainer.config.checkpoint.reset.optimizer = True\n            checkpoint.load_state_dict()\n\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), original_model.state_dict()\n                )\n            )\n\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, original_optimizer)\n            )\n            self.assertTrue(\n                self._compare_optimizers(\n                    self.trainer.optimizer, original_optimizer, skip_keys=True\n                )\n            )\n\n            self.assertEqual(self.trainer.num_updates, 2000)\n            self.assertEqual(self.trainer.current_iteration, 2000)\n            self.assertEqual(self.trainer.current_epoch, 3)\n\n            self._init_early_stopping(checkpoint)\n            # Test reset_counts\n            self.trainer.config.checkpoint.reset.all = False\n            self.trainer.config.checkpoint.reset.optimizer = False\n            self.trainer.config.checkpoint.reset.counts = True\n            checkpoint.load_state_dict()\n\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), original_model.state_dict()\n                )\n            )\n\n            self.assertTrue(\n                self._compare_optimizers(\n                    self.trainer.optimizer, original_optimizer, skip_keys=True\n                )\n            )\n            self.assertEqual(self.trainer.num_updates, 0)\n            self.assertEqual(self.trainer.current_iteration, 0)\n            self.assertEqual(self.trainer.current_epoch, 2)\n\n            # Test with resume_best\n            self._do_a_pass()\n            checkpoint.save(3000)\n            self._init_early_stopping(checkpoint)\n            self.trainer.config.checkpoint.reset.all = False\n            self.trainer.config.checkpoint.resume_best = True\n            self.trainer.config.checkpoint.reset.optimizer = True\n            self.trainer.config.checkpoint.reset.counts = False\n            checkpoint.load_state_dict()\n\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.state_dict(), original_model.state_dict()\n                )\n            )\n\n            self.assertFalse(\n                self._compare_optimizers(self.trainer.optimizer, original_optimizer)\n            )\n            self.assertFalse(\n                self._compare_optimizers(\n                    self.trainer.optimizer, original_optimizer, skip_keys=True\n                )\n            )\n            self.assertEqual(self.trainer.num_updates, 1000)\n            self.assertEqual(self.trainer.current_iteration, 1000)\n            self.assertEqual(self.trainer.current_epoch, 3)\n\n    def test_zoo_load(self):\n        with mock_env_with_temp():\n            checkpoint = Checkpoint(self.trainer)\n            self._init_early_stopping(checkpoint)\n            self._do_a_pass()\n\n            original_model = deepcopy(self.trainer.model)\n            ret_load_pretrained_zoo = {\n                ""config"": self.config.model_config,\n                ""checkpoint"": deepcopy(self.trainer.model.state_dict()),\n                ""full_config"": self.config,\n            }\n\n            self._do_a_pass()\n\n            with patch(\n                ""mmf.utils.checkpoint.load_pretrained_model"",\n                return_value=ret_load_pretrained_zoo,\n            ):\n                self.trainer.config.checkpoint.resume_zoo = ""random""\n                with contextlib.redirect_stdout(StringIO()):\n                    checkpoint.load_state_dict()\n                self.assertTrue(\n                    compare_state_dicts(\n                        self.trainer.model.state_dict(), original_model.state_dict()\n                    )\n                )\n\n                # Now, test zoo override\n                self.trainer.config.checkpoint.zoo_config_override = True\n                SimpleModule.from_pretrained = Mock(\n                    return_value=deepcopy(original_model)\n                )\n                registry.register_model(""simple"")(SimpleModule)\n                with contextlib.redirect_stdout(StringIO()):\n                    checkpoint.load_state_dict()\n                self.assertTrue(\n                    compare_state_dicts(\n                        self.trainer.model.state_dict(), original_model.state_dict()\n                    )\n                )\n\n    def test_pretrained_load(self):\n        with mock_env_with_temp() as d:\n            checkpoint = Checkpoint(self.trainer)\n            self._init_early_stopping(checkpoint)\n            self._do_a_pass()\n            original_model = deepcopy(self.trainer.model)\n            # Test with zoo now\n            ret_load_pretrained_zoo = {\n                ""config"": self.config.model_config,\n                ""checkpoint"": deepcopy(self.trainer.model.state_dict()),\n                ""full_config"": self.config,\n            }\n\n            checkpoint.save(2000)\n            self.trainer.config.checkpoint.resume_file = os.path.join(d, ""current.ckpt"")\n            self.trainer.config.checkpoint.resume_pretrained = True\n            self.trainer.model = OnlyBase()\n            checkpoint.load_state_dict()\n\n            self.assertTrue(\n                compare_state_dicts(\n                    self.trainer.model.base_test.state_dict(),\n                    original_model.base.state_dict(),\n                )\n            )\n\n            with patch(\n                ""mmf.utils.checkpoint.load_pretrained_model"",\n                return_value=ret_load_pretrained_zoo,\n            ):\n                self.trainer.config.checkpoint.resume_zoo = ""random""\n                self.trainer.config.checkpoint.resume_file = None\n                self.trainer.model = OnlyBase()\n                checkpoint.load_state_dict()\n\n                self.assertTrue(\n                    compare_state_dicts(\n                        self.trainer.model.base_test.state_dict(),\n                        original_model.base.state_dict(),\n                    )\n                )\n\n    def _init_early_stopping(self, checkpoint):\n        self.trainer.num_updates = 0\n        self.trainer.current_iteration = 0\n        self.trainer.current_epoch = 0\n        self.trainer.early_stopping = EarlyStopping(self.trainer.model, checkpoint)\n        self.trainer.early_stopping.best_monitored_iteration = 1000\n        self.trainer.early_stopping.best_monitored_update = 1000\n        self.trainer.early_stopping.best_monitored_value = 0.1\n        self.trainer.current_epoch = 2\n\n    def _do_a_pass(self):\n        self.trainer.optimizer.zero_grad()\n        self.trainer.model.train()\n        with contextlib.redirect_stdout(StringIO()):\n            loss = self.trainer.model(\n                torch.rand(5, 5, requires_grad=True),\n                torch.empty(5, dtype=torch.long).random_(5),\n            )\n\n        loss[""losses""][""total_loss""].sum().backward()\n        self.trainer.optimizer.step()\n\n    def _compare_optimizers(self, a, b, skip_keys=False):\n        state_dict_a = a.state_dict()\n        state_dict_b = b.state_dict()\n        state_a = state_dict_a[""state""]\n        state_b = state_dict_b[""state""]\n\n        same = True\n        if not skip_keys:\n            same = same and list(state_a.keys()) == list(state_b.keys())\n            same = same and state_dict_a[""param_groups""] == state_dict_b[""param_groups""]\n\n        for item1, item2 in zip(state_a.values(), state_b.values()):\n            same = same and compare_state_dicts(item1, item2)\n\n        return same\n'"
tests/utils/test_configuration.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport unittest\n\nimport mmf.utils.configuration as configuration\n\n\nclass TestUtilsConfiguration(unittest.TestCase):\n    def test_get_zoo_config(self):\n        # Test direct key\n        version, resources = configuration.get_zoo_config(""textvqa.ocr_en"")\n        self.assertIsNotNone(version)\n        self.assertIsNotNone(resources)\n\n        # Test default variation\n        version, resources = configuration.get_zoo_config(""textvqa"")\n        self.assertIsNotNone(version)\n        self.assertIsNotNone(resources)\n\n        # Test non-default variation\n        version, resources = configuration.get_zoo_config(""textvqa"", variation=""ocr_en"")\n        self.assertIsNotNone(version)\n        self.assertIsNotNone(resources)\n\n        # Test random key\n        version, resources = configuration.get_zoo_config(""some_random"")\n        self.assertIsNone(version)\n        self.assertIsNone(resources)\n\n        # Test non-existent variation\n        self.assertRaises(\n            AssertionError,\n            configuration.get_zoo_config,\n            ""textvqa"",\n            variation=""some_random"",\n        )\n\n        # Test different zoo_type\n        version, resources = configuration.get_zoo_config(\n            ""visual_bert.pretrained"", zoo_type=""models""\n        )\n        self.assertIsNotNone(version)\n        self.assertIsNotNone(resources)\n\n        # Test direct config\n        version, resources = configuration.get_zoo_config(\n            ""visual_bert.pretrained"",\n            zoo_config_path=os.path.join(""configs"", ""zoo"", ""models.yaml""),\n        )\n        self.assertIsNotNone(version)\n        self.assertIsNotNone(resources)\n'"
tests/utils/test_distributed.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport unittest\n\nimport mmf.utils.distributed as distributed\n\n\nclass TestUtilsDistributed(unittest.TestCase):\n    def test_object_byte_tensor_conversion(self):\n        test_obj = [1, ""2"", {3: 4}, [5]]\n        test_obj_bytes = distributed.object_to_byte_tensor(test_obj)\n        test_obj_dec = distributed.byte_tensor_to_object(test_obj_bytes)\n        self.assertEqual(test_obj_dec, test_obj)\n'"
tests/utils/test_download.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport contextlib\nimport os\nimport tempfile\nimport unittest\nfrom io import StringIO\nfrom unittest import mock\n\nimport mmf.utils.download as download\nimport tests.test_utils as test_utils\n\nTEST_DOWNLOAD_URL = (\n    ""https://dl.fbaipublicfiles.com/mmf/data/tests/visual_entailment_small.zip""\n)\nTEST_DOWNLOAD_SHASUM = (\n    ""e5831397710b71f58a02c243bb6e731989c8f37ef603aaf3ce18957ecd075bf5""\n)\n\n\nclass TestUtilsDownload(unittest.TestCase):\n    @test_utils.skip_if_no_network\n    def test_download_file_class(self):\n        # Test normal scenario\n        resource = download.DownloadableFile(\n            TEST_DOWNLOAD_URL,\n            ""visual_entailment_small.zip"",\n            hashcode=TEST_DOWNLOAD_SHASUM,\n            compressed=True,\n        )\n\n        with tempfile.TemporaryDirectory() as d:\n            with contextlib.redirect_stdout(StringIO()):\n                resource.download_file(d)\n            self.assertTrue(os.path.exists(os.path.join(d, ""visual_entailment_small"")))\n            self.assertTrue(\n                os.path.exists(\n                    os.path.join(d, ""visual_entailment_small"", ""db"", ""train.jsonl"")\n                )\n            )\n            self.assertTrue(\n                os.path.exists(\n                    os.path.join(\n                        d,\n                        ""visual_entailment_small"",\n                        ""features"",\n                        ""features.lmdb"",\n                        ""data.mdb"",\n                    )\n                )\n            )\n            self.assertTrue(\n                os.path.exists(os.path.join(d, ""visual_entailment_small.zip""))\n            )\n\n        # Test when checksum fails\n        resource = download.DownloadableFile(\n            TEST_DOWNLOAD_URL,\n            ""visual_entailment_small.zip"",\n            hashcode=""some_random_string"",\n            compressed=True,\n        )\n\n        with tempfile.TemporaryDirectory() as d:\n            with contextlib.redirect_stdout(StringIO()):\n                self.assertRaises(AssertionError, resource.download_file, d)\n\n        # Test when not compressed\n        resource = download.DownloadableFile(\n            TEST_DOWNLOAD_URL,\n            ""visual_entailment_small.zip"",\n            hashcode=TEST_DOWNLOAD_SHASUM,\n            compressed=False,\n        )\n\n        with tempfile.TemporaryDirectory() as d:\n            with contextlib.redirect_stdout(StringIO()):\n                resource.download_file(d)\n            self.assertTrue(\n                os.path.exists(os.path.join(d, ""visual_entailment_small.zip""))\n            )\n            # Check already downloaded scenarios\n\n            with mock.patch.object(resource, ""checksum"") as mocked:\n                with contextlib.redirect_stdout(StringIO()):\n                    resource.download_file(d)\n                mocked.assert_called_once_with(d)\n\n            with mock.patch(""mmf.utils.download.download"") as mocked:\n                with contextlib.redirect_stdout(StringIO()):\n                    resource.download_file(d)\n                mocked.assert_called_once_with(\n                    resource._url, d, resource._file_name, redownload=False\n                )\n            with mock.patch.object(resource, ""checksum"") as mocked:\n                resource._hashcode = ""some_random_string""\n                with contextlib.redirect_stdout(StringIO()):\n                    resource.download_file(d)\n                self.assertTrue(mocked.call_count, 2)\n\n            with mock.patch(""mmf.utils.download.download"") as mocked:\n                resource._hashcode = ""some_random_string""\n                with contextlib.redirect_stdout(StringIO()):\n                    self.assertRaises(AssertionError, resource.download_file, d)\n                mocked.assert_called_once_with(\n                    resource._url, d, resource._file_name, redownload=True\n                )\n\n        # Test delete original\n        resource = download.DownloadableFile(\n            TEST_DOWNLOAD_URL,\n            ""visual_entailment_small.zip"",\n            hashcode=TEST_DOWNLOAD_SHASUM,\n            compressed=True,\n            delete_original=True,\n        )\n\n        with tempfile.TemporaryDirectory() as d:\n            with contextlib.redirect_stdout(StringIO()):\n                resource.download_file(d)\n            self.assertFalse(\n                os.path.exists(os.path.join(d, ""visual_entailment_small.zip""))\n            )\n\n    def test_mark_done(self):\n        with tempfile.TemporaryDirectory() as d:\n            path = os.path.join(d, "".built.json"")\n            self.assertFalse(os.path.exists(path))\n            download.mark_done(d, ""0.1"")\n            self.assertTrue(os.path.exists(path))\n\n            with open(path) as f:\n                import json\n\n                data = json.load(f)\n                self.assertEqual(list(data.keys()), [""created_at"", ""version""])\n\n    def test_built(self):\n        with tempfile.TemporaryDirectory() as d:\n            # First, test without built file\n            self.assertFalse(download.built(d, ""0.2""))\n            download.mark_done(d, ""0.1"")\n            # Test correct version\n            self.assertTrue(download.built(d, ""0.1""))\n            # Test wrong version\n            self.assertFalse(download.built(d, ""0.2""))\n'"
tests/utils/test_file_io.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\nimport uuid\nfrom typing import Optional\n\nfrom mmf.utils.file_io import PathManager\n\n\nclass TestFileIO(unittest.TestCase):\n\n    _tmpdir: Optional[str] = None\n    _tmpfile: Optional[str] = None\n    _tmpfile_contents = ""Hello, World""\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls._tmpdir = tempfile.mkdtemp()\n        with open(os.path.join(cls._tmpdir, ""test.txt""), ""w"") as f:\n            cls._tmpfile = f.name\n            f.write(cls._tmpfile_contents)\n            f.flush()\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        # Cleanup temp working dir.\n        if cls._tmpdir is not None:\n            shutil.rmtree(cls._tmpdir)\n\n    def test_file_io_open(self):\n        with PathManager.open(self._tmpfile, mode=""r"") as f:\n            s = f.read()\n        self.assertEqual(s, self._tmpfile_contents)\n\n    def test_file_io_copy(self):\n        PathManager.copy(self._tmpfile, os.path.join(self._tmpdir, ""test_copy.txt""))\n        with open(os.path.join(self._tmpdir, ""test_copy.txt"")) as f:\n            s = f.read()\n        self.assertEqual(s, self._tmpfile_contents)\n\n    def test_file_io_exists(self):\n        self.assertEqual(\n            PathManager.exists(self._tmpfile), os.path.exists(self._tmpfile)\n        )\n        fake_path = os.path.join(self._tmpdir, uuid.uuid4().hex)\n        self.assertEqual(PathManager.exists(fake_path), os.path.exists(fake_path))\n\n    def test_file_io_mkdirs(self):\n        dir_path = os.path.join(self._tmpdir, ""test_dir"")\n        PathManager.mkdirs(dir_path)\n        self.assertTrue(os.path.isdir(dir_path))\n'"
tests/utils/test_general.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport unittest\n\nfrom mmf.utils.general import dict_to_string, get_overlap_score\n\n\nclass TestUtilsGeneral(unittest.TestCase):\n    def test_dict_to_string(self):\n        dictionary = {""one"": 1, ""two"": 2, ""three"": 3}\n        expected = ""one: 1.0000, two: 2.0000, three: 3.0000""\n\n        self.assertEqual(dict_to_string(dictionary), expected)\n\n    # TODO: Move later to configuration tests\n    # def test_nested_dict_update(self):\n    #     # Updates value\n    #     dictionary = {""level1"": {""level2"": {""levelA"": 0, ""levelB"": 1}}}\n    #     update = {""level1"": {""level2"": {""levelB"": 10}}}\n    #     expected = {""level1"": {""level2"": {""levelA"": 0, ""levelB"": 10}}}\n    #\n    #     self.assertEqual(nested_dict_update(dictionary, update), expected)\n    #\n    #     # Adds new value\n    #     dictionary = {""level1"": {""level2"": {""levelA"": 0}}}\n    #     update = {""level1"": {""level2"": {""levelB"": 10}}}\n    #     expected = {""level1"": {""level2"": {""levelA"": 0, ""levelB"": 10}}}\n    #\n    #     self.assertEqual(nested_dict_update(dictionary, update), expected)\n\n    def test_get_overlap_score(self):\n        # Full overlap\n        candidate = ""pythia""\n        target = ""pythia""\n        self.assertEqual(get_overlap_score(candidate, target), 1.0)\n\n        # Partial overlap\n        candidate = ""pythia""\n        target = ""python""\n        self.assertEqual(get_overlap_score(candidate, target), 2 / 3)\n\n        # No overlap\n        candidate = ""pythia""\n        target = ""vqa""\n        self.assertEqual(get_overlap_score(candidate, target), 0.0)\n'"
tests/utils/test_logger.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport argparse\nimport os\nimport shutil\nimport tempfile\nimport unittest\nfrom typing import Optional\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.configuration import Configuration\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.logger import Logger\n\n\nclass TestLogger(unittest.TestCase):\n\n    _tmpdir: Optional[str] = None\n    _tmpfile_write_contents: str = ""print writer contents""\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls._tmpdir = tempfile.mkdtemp()\n        args = argparse.Namespace()\n        args.opts = [f""env.save_dir={cls._tmpdir}"", f""model=cnn_lstm"", f""dataset=clevr""]\n        args.config_override = None\n        configuration = Configuration(args)\n        configuration.freeze()\n        cls.config = configuration.get_config()\n        registry.register(""config"", cls.config)\n        cls.writer = Logger(cls.config)\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        # Cleanup temp working dir.\n        if cls._tmpdir is not None:\n            shutil.rmtree(cls._tmpdir)\n\n    def test_logger_files(self) -> None:\n        self.assertTrue(PathManager.exists(os.path.join(self._tmpdir, ""train.log"")))\n        self.assertTrue(PathManager.exists(os.path.join(self._tmpdir, ""logs"")))\n\n    def test_log_writer(self) -> None:\n        self.writer.write(self._tmpfile_write_contents)\n        f = PathManager.open(os.path.join(self._tmpdir, ""train.log""))\n        self.assertTrue(\n            any(self._tmpfile_write_contents in line for line in f.readlines())\n        )\n'"
tests/utils/test_model.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\nfrom torch import nn\n\nfrom mmf.common.registry import registry\n\n\nclass TestDecoderModel(nn.Module):\n    def __init__(self, config, vocab):\n        super().__init__()\n        self.config = config\n        self.vocab = vocab\n\n    def build(self):\n        return\n\n    def init_hidden_state(self, features):\n\n        h = features.new_zeros(\n            (features.size(0), self.config.classifier.params.hidden_dim),\n            dtype=torch.float,\n        )\n        c = features.new_zeros(\n            (features.size(0), self.config.classifier.params.hidden_dim),\n            dtype=torch.float,\n        )\n        return h, c\n\n    def get_data_t(self, data, batch_size_t):\n        data[""texts""] = data[""texts""][:batch_size_t]\n        if ""state"" in data:\n            h1 = data[""state""][""td_hidden""][0][:batch_size_t]\n            c1 = data[""state""][""td_hidden""][1][:batch_size_t]\n            h2 = data[""state""][""lm_hidden""][0][:batch_size_t]\n            c2 = data[""state""][""lm_hidden""][1][:batch_size_t]\n        else:\n            h1, c1 = self.init_hidden_state(data[""texts""])\n            h2, c2 = self.init_hidden_state(data[""texts""])\n        data[""state""] = {""td_hidden"": (h1, c1), ""lm_hidden"": (h2, c2)}\n        registry.register(f""{h1.device}_lstm_state"", data[""state""])\n\n        return data, batch_size_t\n\n    def forward(self, sample_list):\n        scores = torch.rand(sample_list.get_batch_size(), 3127)\n        decoder = registry.get_decoder_class(self.config.inference.type)(\n            self.vocab, self.config\n        )\n        sample_list = decoder.init_batch(sample_list)\n        batch_size = sample_list.image_feature_0.size(0)\n        data = {}\n        data[""texts""] = sample_list.answers.new_full(\n            (batch_size, 1), self.vocab.SOS_INDEX, dtype=torch.long\n        )\n        timesteps = 10\n        sample_list.add_field(""targets"", sample_list.answers[:, 0, 1:])\n        output = None\n        batch_size_t = batch_size\n        for t in range(timesteps):\n            data, batch_size_t = self.get_data_t(data, batch_size_t)\n            output = torch.randn(1, 9491)\n            if t == timesteps - 1:\n                output = torch.ones(1, 9491) * -30\n                output[0][2] = 10\n            finish, data, batch_size_t = decoder.decode(t, data, output)\n            if finish:\n                break\n\n        model_output = {""scores"": scores}\n        model_output[""captions""] = decoder.get_result()\n\n        return model_output\n'"
tests/utils/test_text.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport unittest\n\nimport torch\n\nimport mmf.utils.text as text_utils\nfrom mmf.common.registry import registry\nfrom mmf.common.sample import Sample, SampleList\nfrom mmf.utils.configuration import Configuration\nfrom mmf.utils.env import setup_imports\nfrom mmf.utils.general import get_mmf_root\nfrom tests.test_utils import dummy_args\nfrom tests.utils.test_model import TestDecoderModel\n\n\nclass TestUtilsText(unittest.TestCase):\n    TOKENS = [""this"", ""will"", ""be"", ""a"", ""test"", ""of"", ""tokens""]\n    TOKENIZE_EXAMPLE = ""This will be a test of tokens?""\n    VOCAB_EXAMPLE_SENTENCES = [\n        ""Are there more big green things than large purple shiny cubes?""\n        ""How many other things are there of the same shape as the tiny ""\n        + ""cyan matte object?"",\n        ""Is the color of the large sphere the same as the large matte cube?""\n        ""What material is the big object that is right of the brown cylinder and ""\n        ""left of the large brown sphere?"",\n        ""How big is the brown shiny sphere? ;"",\n    ]\n\n    def setUp(self):\n        setup_imports()\n        torch.manual_seed(1234)\n        config_path = os.path.join(\n            get_mmf_root(),\n            "".."",\n            ""projects"",\n            ""butd"",\n            ""configs"",\n            ""coco"",\n            ""nucleus_sampling.yaml"",\n        )\n        config_path = os.path.abspath(config_path)\n        args = dummy_args(model=""butd"", dataset=""coco"")\n        args.opts.append(f""config={config_path}"")\n        configuration = Configuration(args)\n        configuration.config.datasets = ""coco""\n        configuration.config.model_config.butd.inference.params.sum_threshold = 0.5\n        configuration.freeze()\n        self.config = configuration.config\n        registry.register(""config"", self.config)\n\n    def test_tokenize(self):\n        tokens = text_utils.tokenize(self.TOKENIZE_EXAMPLE)\n\n        self.assertEqual(list(tokens), self.TOKENS)\n\n    def test_generate_ngrams(self):\n        ngrams = text_utils.generate_ngrams(self.TOKENS, 2)\n\n        self.assertEqual(\n            list(ngrams),\n            [""this will"", ""will be"", ""be a"", ""a test"", ""test of"", ""of tokens""],\n        )\n\n        ngrams = text_utils.generate_ngrams(self.TOKENS, 3)\n\n        self.assertEqual(\n            list(ngrams),\n            [""this will be"", ""will be a"", ""be a test"", ""a test of"", ""test of tokens""],\n        )\n\n    def test_generate_ngrams_range(self):\n        # Test generation of 1grams to 3gram\n        ngrams = text_utils.generate_ngrams_range(self.TOKENS, (1, 4))\n\n        expected_ngrams = self.TOKENS + [\n            ""this will"",\n            ""will be"",\n            ""be a"",\n            ""a test"",\n            ""test of"",\n            ""of tokens"",\n            ""this will be"",\n            ""will be a"",\n            ""be a test"",\n            ""a test of"",\n            ""test of tokens"",\n        ]\n\n        self.assertEqual(list(ngrams), expected_ngrams)\n\n    def test_vocab_from_text(self):\n        vocab = text_utils.VocabFromText(self.VOCAB_EXAMPLE_SENTENCES)\n\n        self.assertEqual(vocab.get_size(), 41)\n        self.assertEqual(len(vocab), 41)\n        self.assertEqual(vocab.get_unk_index(), 1)\n\n        self.assertEqual(vocab.itos[0], vocab.DEFAULT_TOKENS[0])\n        self.assertEqual(vocab.itos[34], ""that"")\n        self.assertEqual(vocab.itos[31], ""cube"")\n        self.assertEqual(vocab.itos[25], ""cyan"")\n        self.assertEqual(vocab.itos[20], ""the"")\n        self.assertEqual(vocab.itos[10], ""than"")\n\n        self.assertEqual(vocab.stoi[""sphere""], 30)\n        self.assertEqual(vocab.stoi[""shape""], 22)\n\n        vocab = text_utils.VocabFromText(self.VOCAB_EXAMPLE_SENTENCES, min_count=10)\n        self.assertEqual(vocab.get_size(), 5)\n        self.assertEqual(vocab.itos[vocab.get_size() - 1], ""the"")\n\n        vocab = text_utils.VocabFromText(self.VOCAB_EXAMPLE_SENTENCES, min_count=11)\n        self.assertEqual(vocab.get_size(), 4)\n\n        vocab = text_utils.VocabFromText(\n            self.VOCAB_EXAMPLE_SENTENCES, min_count=11, only_unk_extra=True\n        )\n        self.assertEqual(vocab.get_size(), 1)\n        self.assertEqual(vocab.itos[vocab.get_size() - 1], ""<unk>"")\n\n        vocab = text_utils.VocabFromText(\n            self.VOCAB_EXAMPLE_SENTENCES, min_count=1, remove=["";""]\n        )\n        self.assertEqual(vocab.get_size(), 40)\n\n        vocab = text_utils.VocabFromText(\n            self.VOCAB_EXAMPLE_SENTENCES, min_count=1, remove=["";"", "","", ""?""]\n        )\n        self.assertEqual(vocab.get_size(), 38)\n\n        vocab = text_utils.VocabFromText(\n            self.VOCAB_EXAMPLE_SENTENCES, min_count=1, keep=[""?""], remove="";""\n        )\n        self.assertEqual(vocab.get_size(), 40)\n\n    def test_nucleus_sampling(self):\n        vocab = text_utils.VocabFromText(self.VOCAB_EXAMPLE_SENTENCES)\n\n        model_config = self.config.model_config.butd\n        model = TestDecoderModel(model_config, vocab)\n        model.build()\n        model.to(""cuda"")\n        model.eval()\n\n        sample = Sample()\n        sample.dataset_name = ""coco""\n        sample.dataset_type = ""test""\n        sample.image_feature_0 = torch.randn(100, 2048)\n        sample.answers = torch.zeros((5, 10), dtype=torch.long)\n        sample_list = SampleList([sample])\n\n        tokens = model(sample_list)[""captions""]\n\n        # these are expected tokens for sum_threshold = 0.5\n        expected_tokens = [\n            1.0000e00,\n            2.9140e03,\n            5.9210e03,\n            2.2040e03,\n            5.0550e03,\n            9.2240e03,\n            4.5120e03,\n            1.8200e02,\n            3.6490e03,\n            6.4090e03,\n            2.0000e00,\n        ]\n\n        self.assertEqual(tokens[0].tolist(), expected_tokens)\n'"
tests/utils/test_timer.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport time\nimport unittest\n\nfrom mmf.utils.timer import Timer\n\n\nclass TestUtilsTimer(unittest.TestCase):\n    def test_get_current(self):\n        timer = Timer()\n        expected = ""000ms""\n\n        self.assertEqual(timer.get_current(), expected)\n\n    def test_reset(self):\n        timer = Timer()\n        time.sleep(2)\n        timer.reset()\n        expected = ""000ms""\n\n        self.assertEqual(timer.get_current(), expected)\n\n    def test_get_time_since_start(self):\n        timer = Timer()\n        time.sleep(2)\n        expected = ""02s ""\n\n        self.assertTrue(expected in timer.get_time_since_start())\n'"
tools/scripts/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
tools/sweeps/sweep_visual_bert.py,0,"b'#!/usr/bin/env python\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport lib as sweep\nfrom lib import hyperparam\n\n\ndef get_grid(args):\n    max_update = 22000\n\n    return [\n        hyperparam(""run_type"", ""train_val""),\n        hyperparam(""config"", ""projects/visual_bert/configs/vqa2/defaults.yaml""),\n        # hyperparam(""--fp16"", save_dir_key=lambda val: ""fp16""),\n        hyperparam(""training.num_workers"", 5),\n        hyperparam(""dataset"", ""vqa2""),\n        hyperparam(""model"", ""visual_bert"", save_dir_key=lambda val: val),\n        # For nlvr2, we are able to fit batch of size 16 on single GPU with 16GB\n        # memory. Same number is 32 for VQA2, so scale accordingly\n        hyperparam(\n            ""training.batch_size"", [512, 256], save_dir_key=lambda val: f""bs{val}""\n        ),\n        hyperparam(""training.seed"", 1, save_dir_key=lambda val: f""s{val}""),\n        hyperparam(""scheduler.type"", [""warmup_cosine""]),\n        hyperparam(""scheduler.params.num_warmup_steps"", 2000),\n        hyperparam(""scheduler.params.num_training_steps"", max_update),\n        hyperparam(""optimizer.type"", ""adam_w"", save_dir_key=lambda val: val),\n        hyperparam(\n            ""optimizer.params.lr"", [5e-5, 1e-5], save_dir_key=lambda val: f""lr{val}""\n        ),\n        hyperparam(""optimizer.params.eps"", 1e-8),\n        hyperparam(\n            ""training.max_updates"", max_update, save_dir_key=lambda val: f""mu{val}""\n        ),\n        hyperparam(""training.log_format"", ""json""),\n        hyperparam(""training.pin_memory"", True),\n        hyperparam(""training.log_interval"", 1000),\n        hyperparam(""training.checkpoint_interval"", 1000),\n        hyperparam(""training.evaluation_interval"", 4000),\n        hyperparam(""training.find_unused_parameters"", True),\n        hyperparam(\n            ""model_config.visual_bert.freeze_base"",\n            [False],\n            save_dir_key=lambda val: f""fb{val}"",\n        ),\n    ]\n\n\ndef postprocess_hyperparams(args, config):\n    """"""Postprocess a given hyperparameter configuration.""""""\n    pass\n\n\nif __name__ == ""__main__"":\n    sweep.main(get_grid, postprocess_hyperparams)\n'"
mmf/datasets/builders/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/databases/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport mmf.datasets.databases.readers  # noqa\n\nfrom .annotation_database import AnnotationDatabase\nfrom .features_database import FeaturesDatabase\nfrom .image_database import ImageDatabase\nfrom .scene_graph_database import SceneGraphDatabase\n\n__all__ = [\n    ""AnnotationDatabase"",\n    ""FeaturesDatabase"",\n    ""ImageDatabase"",\n    ""SceneGraphDatabase"",\n]\n'"
mmf/datasets/databases/annotation_database.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport json\n\nimport numpy as np\nimport torch\n\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_absolute_path\n\n\nclass AnnotationDatabase(torch.utils.data.Dataset):\n    """"""\n    Dataset for Annotations used in MMF\n\n    TODO: Update on docs sprint\n    """"""\n\n    def __init__(self, config, path, *args, **kwargs):\n        super().__init__()\n        self.metadata = {}\n        self.config = config\n        self.start_idx = 0\n        path = get_absolute_path(path)\n        self._load_annotation_db(path)\n\n    def _load_annotation_db(self, path):\n        if path.find(""visdial"") != -1 or path.find(""visual_dialog"") != -1:\n            self._load_visual_dialog(path)\n        elif path.endswith("".npy""):\n            self._load_npy(path)\n        elif path.endswith("".jsonl""):\n            self._load_jsonl(path)\n        elif path.endswith("".json""):\n            self._load_json(path)\n        else:\n            raise ValueError(""Unknown file format for annotation db"")\n\n    def _load_jsonl(self, path):\n        with PathManager.open(path, ""r"") as f:\n            db = f.readlines()\n            for idx, line in enumerate(db):\n                db[idx] = json.loads(line.strip(""\\n""))\n            self.data = db\n            self.start_idx = 0\n\n    def _load_npy(self, path):\n        self.db = np.load(path, allow_pickle=True)\n        self.start_idx = 0\n\n        if type(self.db) == dict:\n            self.metadata = self.db.get(""metadata"", {})\n            self.data = self.db.get(""data"", [])\n        else:\n            # TODO: Deprecate support for this\n            self.metadata = {""version"": 1}\n            self.data = self.db\n            # Handle old imdb support\n            if ""image_id"" not in self.data[0]:\n                self.start_idx = 1\n\n        if len(self.data) == 0:\n            self.data = self.db\n\n    def _load_json(self, path):\n        with PathManager.open(path, ""r"") as f:\n            data = json.load(f)\n        self.metadata = data.get(""metadata"", {})\n        self.data = data.get(""data"", [])\n\n        if len(self.data) == 0:\n            raise RuntimeError(""Dataset is empty"")\n\n    def _load_visual_dialog(self, path):\n        from mmf.datasets.builders.visual_dialog.database import VisualDialogDatabase\n\n        self.data = VisualDialogDatabase(path)\n        self.metadata = self.data.metadata\n        self.start_idx = 0\n\n    def __len__(self):\n        return len(self.data) - self.start_idx\n\n    def __getitem__(self, idx):\n        data = self.data[idx + self.start_idx]\n\n        # Hacks for older IMDBs\n        if ""answers"" not in data:\n            if ""all_answers"" in data and ""valid_answers"" not in data:\n                data[""answers""] = data[""all_answers""]\n            if ""valid_answers"" in data:\n                data[""answers""] = data[""valid_answers""]\n\n        # TODO: Clean up VizWiz IMDB from copy tokens\n        if ""answers"" in data and data[""answers""][-1] == ""<copy>"":\n            data[""answers""] = data[""answers""][:-1]\n\n        return data\n\n    def get_version(self):\n        return self.metadata.get(""version"", None)\n'"
mmf/datasets/databases/features_database.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom multiprocessing.pool import ThreadPool\n\nimport tqdm\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.databases.image_database import ImageDatabase\nfrom mmf.datasets.databases.readers.feature_readers import FeatureReader\nfrom mmf.utils.distributed import is_master\nfrom mmf.utils.general import get_absolute_path\n\n\nclass FeaturesDatabase(ImageDatabase):\n    def __init__(\n        self, config, path, annotation_db=None, feature_key=None, *args, **kwargs\n    ):\n        super().__init__(config, path, annotation_db, *args, **kwargs)\n        self.feature_readers = []\n        self.feature_dict = {}\n        self.feature_key = config.get(""feature_key"", ""feature_path"")\n        self.feature_key = feature_key if feature_key else self.feature_key\n        self._fast_read = config.get(""fast_read"", False)\n        self.writer = registry.get(""writer"")\n\n        path = path.split("","")\n\n        for image_feature_dir in path:\n            feature_reader = FeatureReader(\n                base_path=get_absolute_path(image_feature_dir),\n                depth_first=config.get(""depth_first"", False),\n                max_features=config.get(""max_features"", 100),\n            )\n            self.feature_readers.append(feature_reader)\n\n        self.paths = path\n        self.annotation_db = annotation_db\n        self._should_return_info = config.get(""return_features_info"", True)\n\n        if self._fast_read:\n            self.writer.write(""Fast reading features from {}"".format("", "".join(path)))\n            self.writer.write(""Hold tight, this may take a while..."")\n            self._threaded_read()\n\n    def _threaded_read(self):\n        elements = [idx for idx in range(1, len(self.annotation_db))]\n        pool = ThreadPool(processes=4)\n\n        with tqdm.tqdm(total=len(elements), disable=not is_master()) as pbar:\n            for i, _ in enumerate(pool.imap_unordered(self._fill_cache, elements)):\n                if i % 100 == 0:\n                    pbar.update(100)\n        pool.close()\n\n    def _fill_cache(self, idx):\n        feat_file = self.annotation_db[idx][""feature_path""]\n        features, info = self._read_features_and_info(feat_file)\n        self.feature_dict[feat_file] = (features, info)\n\n    def _read_features_and_info(self, feat_file):\n        features = []\n        infos = []\n        for feature_reader in self.feature_readers:\n            feature, info = feature_reader.read(feat_file)\n            # feature = torch.from_numpy(feature).share_memory_()\n\n            features.append(feature)\n            infos.append(info)\n\n        if not self._should_return_info:\n            infos = None\n        return features, infos\n\n    def _get_image_features_and_info(self, feat_file):\n        assert isinstance(feat_file, str)\n        image_feats, infos = self.feature_dict.get(feat_file, (None, None))\n\n        if image_feats is None:\n            image_feats, infos = self._read_features_and_info(feat_file)\n\n        return image_feats, infos\n\n    def __len__(self):\n        self._check_annotation_db_present()\n        return len(self.annotation_db)\n\n    def __getitem__(self, idx):\n        self._check_annotation_db_present()\n        image_info = self.annotation_db[idx]\n        return self.get(image_info)\n\n    def get(self, item):\n        feature_path = item.get(self.feature_key, None)\n\n        if feature_path is None:\n            feature_path = self._get_feature_path_based_on_image(item)\n\n        return self.from_path(feature_path)\n\n    def from_path(self, path):\n        assert isinstance(path, str)\n\n        if ""genome"" in path:\n            path = str(int(path.split(""_"")[-1].split(""."")[0])) + "".npy""\n\n        features, infos = self._get_image_features_and_info(path)\n\n        item = {}\n        for idx, image_feature in enumerate(features):\n            item[""image_feature_%s"" % idx] = image_feature\n            if infos is not None:\n                # infos[idx].pop(""cls_prob"", None)\n                item[""image_info_%s"" % idx] = infos[idx]\n\n        return item\n\n    def _get_feature_path_based_on_image(self, item):\n        image_path = self._get_attrs(item)[0]\n        feature_path = ""."".join(image_path.split(""."")[:-1]) + "".npy""\n        return feature_path\n'"
mmf/datasets/databases/image_database.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport collections\nimport os\n\nimport torch\nimport torchvision\nimport torchvision.datasets.folder as tv_helpers\n\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_absolute_path\n\n\ndef get_possible_image_paths(path):\n    for ext in tv_helpers.IMG_EXTENSIONS:\n        image_ext = ""."".join(path.split(""."")[:-1]) + ext\n        if PathManager.isfile(image_ext):\n            path = image_ext\n            break\n    return path\n\n\nclass ImageDatabase(torch.utils.data.Dataset):\n    """"""ImageDatabase can be used to load images in MMF.\n    This goes either in conjunction with AnnotationDatabase or\n    can be separately used with function such as `from_path`.\n    MMFDataset initializes its own copy of ImageDatabase if `use_images`\n    is True. Rest everything works same as a normal torch Dataset if\n    you pass the annotation_db as a parameter. For example for item\n    1 from annotation db, you can pass same id to ImageDatabase to loads\n    its image. If you don\'t pass it, you have two options. Either use\n    .get which takes in an annotation db item or .from_path which directly\n    takes in an image path. You are free to use your own dataset instead\n    of image database or free to update or ignore MMFDataset\'s ImageDataset\n    initialization. You can either reinitialize with transform and other\n    params or use any of torchvision\'s datasets.\n    """"""\n\n    def __init__(\n        self,\n        config,\n        path,\n        annotation_db=None,\n        transform=None,\n        loader=tv_helpers.default_loader,\n        is_valid_file=None,\n        image_key=None,\n        *args,\n        **kwargs\n    ):\n        """"""Initialize an instance of ImageDatabase\n\n        Args:\n            torch ([type]): [description]\n            config (DictConfig): Config object from dataset_config\n            path (str): Path to images folder\n            annotation_db (AnnotationDB, optional): Annotation DB to be used\n                to be figure out image paths. Defaults to None.\n            transform (callable, optional): Transform to be called upon loaded image.\n                Defaults to None.\n            loader (callable, optional): Custom loader for image which given a path\n                returns a PIL Image. Defaults to torchvision\'s default loader.\n            is_valid_file (callable, optional): Custom callable to filter out invalid\n                files. If image is invalid, {""images"": []} will returned which you can\n                filter out in your dataset. Defaults to None.\n            image_key (str, optional): Key that points to image path in annotation db.\n                If not specified, ImageDatabase will make some intelligent guesses\n                about the possible key. Defaults to None.\n        """"""\n        super().__init__()\n        self.config = config\n        self.base_path = get_absolute_path(path)\n        self.transform = transform\n        self.annotation_db = annotation_db\n        self.loader = loader\n        self.image_key = config.get(""image_key"", None)\n        self.image_key = image_key if image_key else self.image_key\n        self.is_valid_file = is_valid_file\n\n    @property\n    def annotation_db(self):\n        return self._annotation_db\n\n    @annotation_db.setter\n    def annotation_db(self, annotation_db):\n        self._annotation_db = annotation_db\n\n    @property\n    def transform(self):\n        return self._transform\n\n    @transform.setter\n    def transform(self, transform):\n        if isinstance(transform, collections.abc.MutableSequence):\n            transform = torchvision.Compose(transform)\n        self._transform = transform\n\n    def __len__(self):\n        self._check_annotation_db_present()\n        return len(self.annotation_db)\n\n    def __getitem__(self, idx):\n        self._check_annotation_db_present()\n        item = self.annotation_db[idx]\n        return self.get(item)\n\n    def _check_annotation_db_present(self):\n        if not self.annotation_db:\n            raise AttributeError(\n                ""\'annotation_db\' must be set for the database to use __getitem__.""\n                + "" Use image_database.annotation_db to set it.""\n            )\n\n    def get(self, item):\n        possible_images = self._get_attrs(item)\n        return self.from_path(possible_images)\n\n    def from_path(self, paths, use_transforms=True):\n        if isinstance(paths, str):\n            paths = [paths]\n\n        assert isinstance(\n            paths, collections.abc.Iterable\n        ), ""Path needs to a string or an iterable""\n\n        loaded_images = []\n        for image in paths:\n            path = None\n            path = get_possible_image_paths(image)\n\n            valid = self.is_valid_file(path) if self.is_valid_file is not None else True\n\n            if not valid:\n                continue\n\n            if not path:\n                # Create the full path without extension so it can be printed\n                # for the error\n                possible_path = os.path.join(\n                    self.base_path, ""."".join(image.split(""."")[:-1])\n                )\n\n                raise RuntimeError(\n                    ""Image not found at path {}.{{jpeg|jpg|svg|png}}."".format(\n                        possible_path\n                    )\n                )\n            path = os.path.join(self.base_path, path)\n            image = self.open_image(path)\n\n            if self.transform and use_transforms:\n                image = self.transform(image)\n            loaded_images.append(image)\n\n        return {""images"": loaded_images}\n\n    def open_image(self, path):\n        return self.loader(path)\n\n    def _get_attrs(self, item):\n        """"""Returns possible attribute that can point to image id\n\n        Args:\n            item (Object): Object from the DB\n\n        Returns:\n            List[str]: List of possible images that will be copied later\n        """"""\n        if self.image_key:\n            image = item[self.image_key]\n            if isinstance(image, str):\n                image = [image]\n            return image\n\n        image = None\n        pick = None\n        attrs = self._get_possible_attrs()\n\n        for attr in attrs:\n            image = item.get(attr, None)\n            if image is not None:\n                pick = attr\n                break\n\n        # Check if first one is nlvr2\n        if pick == ""identifier"" and ""left_url"" in item and ""right_url"" in item:\n            return [image + ""-img0.jpg"", image + ""-img1.jpg""]\n        elif pick == ""image_name"" or pick == ""image_id"":\n            return [image + "".jpg""]\n        else:\n            return [image]\n\n    def _get_possible_attrs(self):\n        return [\n            ""Flickr30kID"",\n            ""Flikr30kID"",\n            ""identifier"",\n            ""image_path"",\n            ""image_name"",\n            ""img"",\n            ""image_id"",\n        ]\n'"
mmf/datasets/databases/scene_graph_database.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.datasets.databases.annotation_database import AnnotationDatabase\n\n\nclass SceneGraphDatabase(AnnotationDatabase):\n    def __init__(self, config, scene_graph_path, *args, **kwargs):\n        super().__init__(config, scene_graph_path, *args, **kwargs)\n        self.data_dict = {}\n        for item in self.data:\n            self.data_dict[item[""image_id""]] = item\n\n    def __getitem__(self, idx):\n        return self.data_dict[idx]\n'"
mmf/datasets/processors/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.datasets.processors.bert_processors import MaskedTokenProcessor\nfrom mmf.datasets.processors.image_processors import TorchvisionTransforms\nfrom mmf.datasets.processors.processors import (\n    BaseProcessor,\n    BBoxProcessor,\n    CaptionProcessor,\n    FastTextProcessor,\n    GloVeProcessor,\n    MultiHotAnswerFromVocabProcessor,\n    Processor,\n    SimpleSentenceProcessor,\n    SimpleWordProcessor,\n    SoftCopyAnswerProcessor,\n    VocabProcessor,\n    VQAAnswerProcessor,\n)\n\n__all__ = [\n    ""BaseProcessor"",\n    ""Processor"",\n    ""VocabProcessor"",\n    ""GloVeProcessor"",\n    ""FastTextProcessor"",\n    ""VQAAnswerProcessor"",\n    ""MultiHotAnswerFromVocabProcessor"",\n    ""SoftCopyAnswerProcessor"",\n    ""SimpleWordProcessor"",\n    ""SimpleSentenceProcessor"",\n    ""BBoxProcessor"",\n    ""CaptionProcessor"",\n    ""MaskedTokenProcessor"",\n    ""TorchvisionTransforms"",\n]\n'"
mmf/datasets/processors/bert_processors.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport random\n\nimport torch\nfrom transformers.tokenization_auto import AutoTokenizer\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.processors.processors import BaseProcessor\n\n\n@registry.register_processor(""masked_token"")\nclass MaskedTokenProcessor(BaseProcessor):\n    _CLS_TOKEN = ""[CLS]""\n    _SEP_TOKEN = ""[SEP]""\n\n    def __init__(self, config, *args, **kwargs):\n        tokenizer_config = config.tokenizer_config\n        self._tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_config.type, **tokenizer_config.params\n        )\n\n        self._max_seq_length = config.max_seq_length\n        self._probability = getattr(config, ""mask_probability"", 0.15)\n\n    def get_vocab_size(self):\n        return len(self._tokenizer)\n\n    def _random_word(self, tokens, probability=0.15):\n        labels = []\n        for idx, token in enumerate(tokens):\n            prob = random.random()\n\n            if prob < probability:\n                prob /= probability\n\n                # 80% randomly change token to mask token\n                if prob < 0.8:\n                    tokens[idx] = ""[MASK]""\n                # 10% randomly change token to random token\n                elif prob < 0.9:\n                    tokens[idx] = self._tokenizer.convert_ids_to_tokens(\n                        torch.randint(len(self._tokenizer), (1,), dtype=torch.long)\n                    )[0]\n\n                # rest 10% keep the original token as it is\n\n                labels.append(self._tokenizer.convert_tokens_to_ids(token))\n            else:\n                labels.append(-1)\n\n        return tokens, labels\n\n    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n        """"""Truncates a sequence pair in place to the maximum length.""""""\n\n        # This is a simple heuristic which will always truncate the longer sequence\n        # one token at a time. This makes more sense than truncating an equal percent\n        # of tokens from each, since if one sequence is very short then each token\n        # that\'s truncated likely contains more information than a longer sequence.\n        if tokens_b is None:\n            tokens_b = []\n\n        while True:\n            total_length = len(tokens_a) + len(tokens_b)\n            if total_length <= max_length:\n                break\n            if len(tokens_a) > len(tokens_b):\n                tokens_a.pop()\n            else:\n                tokens_b.pop()\n\n    def _convert_to_indices(self, tokens_a, tokens_b=None, probability=0.15):\n        tokens_a, label_a = self._random_word(tokens_a, probability=probability)\n\n        tokens = [self._CLS_TOKEN]\n        segment_ids = [0]\n\n        tokens += tokens_a\n        segment_ids += [0] * len(tokens_a)\n\n        tokens.append(self._SEP_TOKEN)\n        segment_ids.append(0)\n\n        if tokens_b:\n            tokens_b, label_b = self._random_word(tokens_b, probability=probability)\n            lm_label_ids = [-1] + label_a + [-1] + label_b + [-1]\n            assert len(tokens_b) > 0\n            tokens += tokens_b\n            segment_ids += [1] * len(tokens_b)\n            tokens.append(self._SEP_TOKEN)\n            segment_ids.append(1)\n        else:\n            lm_label_ids = [-1] + label_a + [-1]\n\n        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < self._max_seq_length:\n            input_ids.append(0)\n            input_mask.append(0)\n            segment_ids.append(0)\n            lm_label_ids.append(-1)\n\n        assert len(input_ids) == self._max_seq_length\n        assert len(input_mask) == self._max_seq_length\n        assert len(segment_ids) == self._max_seq_length\n        assert len(lm_label_ids) == self._max_seq_length\n\n        input_ids = torch.tensor(input_ids, dtype=torch.long)\n        input_mask = torch.tensor(input_mask, dtype=torch.long)\n        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n        lm_label_ids = torch.tensor(lm_label_ids, dtype=torch.long)\n        return {\n            ""input_ids"": input_ids,\n            ""input_mask"": input_mask,\n            ""segment_ids"": segment_ids,\n            ""lm_label_ids"": lm_label_ids,\n            ""tokens"": tokens,\n        }\n\n    def __call__(self, item):\n        text_a = item[""text_a""]\n        text_b = item.get(""text_b"", None)\n\n        tokens_a = self._tokenizer.tokenize(text_a)\n        tokens_b = None\n\n        if text_b:\n            tokens_b = self._tokenizer.tokenize(text_b)\n\n        self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 2)\n        output = self._convert_to_indices(\n            tokens_a, tokens_b, probability=self._probability\n        )\n        output[""is_correct""] = torch.tensor(item[""is_correct""], dtype=torch.long)\n\n        return output\n\n\n@registry.register_processor(""bert_tokenizer"")\nclass BertTokenizer(MaskedTokenProcessor):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n        self._probability = 0\n\n    def __call__(self, item):\n        if ""text"" in item:\n            text_a = item[""text""]\n        else:\n            text_a = "" "".join(item[""tokens""])\n\n        tokens_a = self._tokenizer.tokenize(text_a)\n\n        self._truncate_seq_pair(tokens_a, None, self._max_seq_length - 2)\n        output = self._convert_to_indices(tokens_a, None, probability=self._probability)\n        output[""text""] = output[""tokens""]\n        return output\n'"
mmf/datasets/processors/image_processors.py,2,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport collections\n\nimport torch\nfrom omegaconf import OmegaConf\nfrom torchvision import transforms\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.processors.processors import BaseProcessor\n\n\n@registry.register_processor(""torchvision_transforms"")\nclass TorchvisionTransforms(BaseProcessor):\n    def __init__(self, config, *args, **kwargs):\n        transform_params = config.transforms\n        assert OmegaConf.is_dict(transform_params) or OmegaConf.is_list(\n            transform_params\n        )\n        if OmegaConf.is_dict(transform_params):\n            transform_params = [transform_params]\n\n        transforms_list = []\n\n        for param in transform_params:\n            if OmegaConf.is_dict(param):\n                # This will throw config error if missing\n                transform_type = param.type\n                transform_param = param.get(""params"", OmegaConf.create({}))\n            else:\n                assert isinstance(param, str), (\n                    ""Each transform should either be str or dict containing ""\n                    + ""type and params""\n                )\n                transform_type = param\n                transform_param = OmegaConf.create([])\n\n            transform = getattr(transforms, transform_type, None)\n            # If torchvision doesn\'t contain this, check our registry if we\n            # implemented a custom transform as processor\n            if transform is None:\n                transform = registry.get_processor_class(transform_type)\n            assert (\n                transform is not None\n            ), f""torchvision.transforms has no transform {transform_type}""\n\n            # https://github.com/omry/omegaconf/issues/248\n            transform_param = OmegaConf.to_container(transform_param)\n            # If a dict, it will be passed as **kwargs, else a list is *args\n            if isinstance(transform_param, collections.abc.Mapping):\n                transform_object = transform(**transform_param)\n            else:\n                transform_object = transform(*transform_param)\n\n            transforms_list.append(transform_object)\n\n        self.transform = transforms.Compose(transforms_list)\n\n    def __call__(self, x):\n        # Support both dict and normal mode\n        if isinstance(x, collections.abc.Mapping):\n            x = x[""image""]\n            return {""image"": self.transform(x)}\n        else:\n            return self.transform(x)\n\n\n@registry.register_processor(""GrayScaleTo3Channels"")\nclass GrayScaleTo3Channels(BaseProcessor):\n    def __init__(self, *args, **kwargs):\n        return\n\n    def __call__(self, x):\n        if isinstance(x, collections.abc.Mapping):\n            x = x[""image""]\n            return {""image"": self.transform(x)}\n        else:\n            return self.transform(x)\n\n    def transform(self, x):\n        assert isinstance(x, torch.Tensor)\n        # Handle grayscale, tile 3 times\n        if x.size(0) == 1:\n            x = torch.cat([x] * 3, dim=0)\n        return x\n'"
mmf/datasets/processors/processors.py,22,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n""""""\nThe processors exist in MMF to make data processing pipelines in various\ndatasets as similar as possible while allowing code reuse.\n\nThe processors also help maintain proper abstractions to keep only what matters\ninside the dataset\'s code. This allows us to keep the dataset ``__getitem__``\nlogic really clean and no need about maintaining opinions about data type.\nProcessors can work on both images and text due to their generic structure.\n\nTo create a new processor, follow these steps:\n\n1. Inherit the ``BaseProcessor`` class.\n2. Implement ``_call`` function which takes in a dict and returns a dict with\n   same keys preprocessed as well as any extra keys that need to be returned.\n3. Register the processor using ``@registry.register_processor(\'name\')`` to\n   registry where \'name\' will be used to refer to your processor later.\n\nIn processor\'s config you can specify ``preprocessor`` option to specify\ndifferent kind of preprocessors you want in your dataset.\n\nLet\'s break down processor\'s config inside a dataset (VQA2.0) a bit to understand\ndifferent moving parts.\n\nConfig::\n\n\n    dataset_config:\n        vqa2:\n            processors:\n                text_processor:\n                type: vocab\n                params:\n                    max_length: 14\n                    vocab:\n                    type: intersected\n                    embedding_name: glove.6B.300d\n                    vocab_file: vocabs/vocabulary_100k.txt\n                    answer_processor:\n                    type: vqa_answer\n                    params:\n                        num_answers: 10\n                        vocab_file: vocabs/answers_vqa.txt\n                        preprocessor:\n                        type: simple_word\n                        params: {}\n\n``BaseDataset`` will init the processors and they will available inside your\ndataset with same attribute name as the key name, for e.g. `text_processor` will\nbe available as `self.text_processor` inside your dataset. As is with every module\nin MMF, processor also accept a ``DictConfig`` with a `type` and `params`\nattributes. `params` defined the custom parameters for each of the processors.\nBy default, processor initialization process will also init `preprocessor` attribute\nwhich can be a processor config in itself. `preprocessor` can be then be accessed\ninside the processor\'s functions.\n\nExample::\n\n    from mmf.common.registry import registry\n    from mmf.datasets.processors import BaseProcessor\n\n\n    class MyProcessor(BaseProcessor):\n        def __init__(self, config, *args, **kwargs):\n            return\n\n        def __call__(self, item, *args, **kwargs):\n            text = item[\'text\']\n            text = [t.strip() for t in text.split("" "")]\n            return {""text"": text}\n""""""\n\n\nimport copy\nimport os\nimport re\nimport warnings\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport torch\n\nfrom mmf.common.registry import registry\nfrom mmf.utils.configuration import get_mmf_cache_dir\nfrom mmf.utils.distributed import is_master, synchronize\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.text import VocabDict\nfrom mmf.utils.vocab import Vocab, WordToVectorDict\n\n\nclass BaseProcessor:\n    """"""Every processor in MMF needs to inherit this class for compatibility\n    with MMF. End user mainly needs to implement ``__call__`` function.\n\n    Args:\n        config (DictConfig): Config for this processor, containing `type` and\n                             `params` attributes if available.\n\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        return\n\n    def __call__(self, item, *args, **kwargs):\n        """"""Main function of the processor. Takes in a dict and returns back\n        a dict\n\n        Args:\n            item (Dict): Some item that needs to be processed.\n\n        Returns:\n            Dict: Processed dict.\n\n        """"""\n        return item\n\n\nclass Processor:\n    """"""Wrapper class used by MMF to initialized processor based on their\n    ``type`` as passed in configuration. It retrieves the processor class\n    registered in registry corresponding to the ``type`` key and initializes\n    with ``params`` passed in configuration. All functions and attributes of\n    the processor initialized are directly available via this class.\n\n    Args:\n        config (DictConfig): DictConfig containing ``type`` of the processor to\n                             be initialized and ``params`` of that procesor.\n\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        self.writer = registry.get(""writer"")\n\n        if not hasattr(config, ""type""):\n            raise AttributeError(\n                ""Config must have \'type\' attribute to specify type of processor""\n            )\n\n        processor_class = registry.get_processor_class(config.type)\n\n        params = {}\n        if not hasattr(config, ""params""):\n            self.writer.write(\n                ""Config doesn\'t have \'params\' attribute to ""\n                ""specify parameters of the processor ""\n                ""of type {}. Setting to default {{}}"".format(config.type)\n            )\n        else:\n            params = config.params\n\n        self.processor = processor_class(params, *args, **kwargs)\n\n        self._dir_representation = dir(self)\n\n    def __call__(self, item, *args, **kwargs):\n        return self.processor(item, *args, **kwargs)\n\n    def __getattr__(self, name):\n        if ""_dir_representation"" in self.__dict__ and name in self._dir_representation:\n            return getattr(self, name)\n        elif ""processor"" in self.__dict__ and hasattr(self.processor, name):\n            return getattr(self.processor, name)\n        else:\n            raise AttributeError(name)\n\n\n@registry.register_processor(""vocab"")\nclass VocabProcessor(BaseProcessor):\n    """"""Use VocabProcessor when you have vocab file and you want to process\n    words to indices. Expects UNK token as ""<unk>"" and pads sentences using\n    ""<pad>"" token. Config parameters can have ``preprocessor`` property which\n    is used to preprocess the item passed and ``max_length`` property which\n    points to maximum length of the sentence/tokens which can be convert to\n    indices. If the length is smaller, the sentence will be padded. Parameters\n    for ""vocab"" are necessary to be passed.\n\n    **Key**: vocab\n\n    Example Config::\n\n        task_attributes:\n            vqa:\n                vqa2:\n                    processors:\n                      text_processor:\n                        type: vocab\n                        params:\n                          max_length: 14\n                          vocab:\n                            type: intersected\n                            embedding_name: glove.6B.300d\n                            vocab_file: vocabs/vocabulary_100k.txt\n\n    Args:\n        config (DictConfig): node containing configuration parameters of\n                             the processor\n\n    Attributes:\n        vocab (Vocab): Vocab class object which is abstraction over the vocab\n                       file passed.\n    """"""\n\n    MAX_LENGTH_DEFAULT = 50\n    PAD_TOKEN = ""<pad>""\n    PAD_INDEX = 0\n\n    def __init__(self, config, *args, **kwargs):\n        if not hasattr(config, ""vocab""):\n            raise AttributeError(\n                ""config passed to the processor has no attribute vocab""\n            )\n\n        self.vocab = Vocab(*args, **config.vocab, **kwargs)\n        self._init_extras(config)\n\n    def _init_extras(self, config, *args, **kwargs):\n        self.writer = registry.get(""writer"")\n        self.preprocessor = None\n\n        if hasattr(config, ""max_length""):\n            self.max_length = config.max_length\n        else:\n            warnings.warn(\n                ""No \'max_length\' parameter in Processor\'s ""\n                ""configuration. Setting to {}."".format(self.MAX_LENGTH_DEFAULT)\n            )\n            self.max_length = self.MAX_LENGTH_DEFAULT\n\n        if ""preprocessor"" in config:\n            self.preprocessor = Processor(config.preprocessor, *args, **kwargs)\n\n            if self.preprocessor is None:\n                raise ValueError(\n                    f""No text processor named {config.preprocessor} is defined.""\n                )\n\n    def __call__(self, item):\n        """"""Call requires item to have either ""tokens"" attribute or either\n        ""text"" attribute. If ""text"" is present, it will tokenized using\n        the preprocessor.\n\n        Args:\n            item (Dict): Dict containing the ""text"" or ""tokens"".\n\n        Returns:\n            Dict: Dict containing indices in ""text"" key, ""tokens"" in ""tokens""\n                  key and ""length"" of the string in ""length"" key.\n\n        """"""\n        indices = None\n        if not isinstance(item, dict):\n            raise TypeError(\n                ""Argument passed to the processor must be ""\n                ""a dict with either \'text\' or \'tokens\' as ""\n                ""keys""\n            )\n        if ""tokens"" in item:\n            tokens = item[""tokens""]\n            indices = self._map_strings_to_indices(item[""tokens""])\n        elif ""text"" in item:\n            if self.preprocessor is None:\n                raise AssertionError(\n                    ""If tokens are not provided, a text ""\n                    ""processor must be defined in the config""\n                )\n\n            tokens = self.preprocessor({""text"": item[""text""]})[""text""]\n            indices = self._map_strings_to_indices(tokens)\n        else:\n            raise AssertionError(\n                ""A dict with either \'text\' or \'tokens\' keys ""\n                ""must be passed to the processor""\n            )\n\n        tokens, length = self._pad_tokens(tokens)\n\n        return {""text"": indices, ""tokens"": tokens, ""length"": length}\n\n    def _pad_tokens(self, tokens):\n        padded_tokens = [self.PAD_TOKEN] * self.max_length\n        token_length = min(len(tokens), self.max_length)\n        padded_tokens[:token_length] = tokens[:token_length]\n        token_length = torch.tensor(token_length, dtype=torch.long)\n        return padded_tokens, token_length\n\n    def get_pad_index(self):\n        """"""Get index of padding <pad> token in vocabulary.\n\n        Returns:\n            int: index of the padding token.\n\n        """"""\n        return self.vocab.get_pad_index()\n\n    def get_vocab_size(self):\n        """"""Get size of the vocabulary.\n\n        Returns:\n            int: size of the vocabulary.\n\n        """"""\n        return self.vocab.get_size()\n\n    def _map_strings_to_indices(self, tokens):\n        length = min(len(tokens), self.max_length)\n        tokens = tokens[:length]\n\n        output = torch.zeros(self.max_length, dtype=torch.long)\n        output.fill_(self.vocab.get_pad_index())\n\n        for idx, token in enumerate(tokens):\n            output[idx] = self.vocab.stoi[token]\n\n        return output\n\n\n@registry.register_processor(""glove"")\nclass GloVeProcessor(VocabProcessor):\n    """"""Inherits VocabProcessor, and returns GloVe vectors for each of the\n    words. Maps them to index using vocab processor, and then gets GloVe vectors\n    corresponding to those indices.\n\n    Args:\n        config (DictConfig): Configuration parameters for GloVe same as\n                             :func:`~VocabProcessor`.\n\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        if not hasattr(config, ""vocab""):\n            raise AttributeError(\n                ""Config passed to the processor has no attribute vocab""\n            )\n        vocab_processor_config = copy.deepcopy(config)\n        # GloVeProcessor needs vocab type to be ""intersected""\n        vocab_processor_config.vocab.type = ""intersected""\n\n        if ""vocab_file"" not in vocab_processor_config.vocab:\n            warnings.warn(\n                ""\'vocab_file\' key is not present in the config.""\n                "" Switching to pretrained vocab.""\n            )\n\n            vocab_processor_config.vocab.type = ""pretrained""\n\n        super().__init__(vocab_processor_config, *args, **kwargs)\n\n    def __call__(self, item):\n        indices = super().__call__(item)[""text""]\n        embeddings = torch.zeros(\n            (len(indices), self.vocab.get_embedding_dim()), dtype=torch.float\n        )\n\n        for idx, index in enumerate(indices):\n            embeddings[idx] = self.vocab.vectors[index]\n\n        return {""text"": embeddings}\n\n\n@registry.register_processor(""fasttext"")\nclass FastTextProcessor(VocabProcessor):\n    """"""FastText processor, similar to GloVe processor but returns FastText vectors.\n\n    Args:\n        config (DictConfig): Configuration values for the processor.\n\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        self._init_extras(config)\n        self.config = config\n        self._download_initially = config.get(""download_initially"", True)\n        self._already_downloaded = False\n        self._already_loaded = False\n\n        if self._download_initially:\n            self._try_download()\n\n    def _try_download(self):\n        _is_master = is_master()\n\n        if self._already_downloaded:\n            return\n\n        needs_download = False\n\n        if not hasattr(self.config, ""model_file""):\n            if _is_master:\n                warnings.warn(\n                    ""\'model_file\' key is required but missing ""\n                    ""from FastTextProcessor\'s config.""\n                )\n            needs_download = True\n\n        model_file = self.config.model_file\n        # If model_file is already an existing path don\'t join to cache dir\n        if not PathManager.exists(model_file):\n            model_file = os.path.join(get_mmf_cache_dir(), model_file)\n\n        if not PathManager.exists(model_file):\n            if _is_master:\n                warnings.warn(f""No model file present at {model_file}."")\n            needs_download = True\n\n        if needs_download:\n            self.writer.write(""Downloading FastText bin"", ""info"")\n            model_file = self._download_model()\n\n        self.model_file = model_file\n        self._already_downloaded = True\n        synchronize()\n\n    def _download_model(self):\n        _is_master = is_master()\n\n        model_file_path = os.path.join(get_mmf_cache_dir(), ""wiki.en.bin"")\n\n        if not _is_master:\n            return model_file_path\n\n        if PathManager.exists(model_file_path):\n            self.writer.write(f""Vectors already present at {model_file_path}."", ""info"")\n            return model_file_path\n\n        import requests\n        from mmf.common.constants import FASTTEXT_WIKI_URL\n        from tqdm import tqdm\n\n        PathManager.mkdirs(os.path.dirname(model_file_path))\n        response = requests.get(FASTTEXT_WIKI_URL, stream=True)\n\n        with PathManager.open(model_file_path, ""wb"") as f:\n            pbar = tqdm(\n                total=int(response.headers[""Content-Length""]) / 4096,\n                miniters=50,\n                disable=not _is_master,\n            )\n\n            idx = 0\n            for data in response.iter_content(chunk_size=4096):\n                if data:\n                    if idx % 50 == 0:\n                        pbar.update(len(data))\n                    f.write(data)\n                    idx += 1\n\n            pbar.close()\n\n        self.writer.write(f""fastText bin downloaded at {model_file_path}."", ""info"")\n\n        return model_file_path\n\n    def _load_fasttext_model(self, model_file):\n        if self._already_loaded:\n            return\n\n        from fasttext import load_model\n\n        self.writer.write(""Loading fasttext model now from %s"" % model_file)\n\n        self.model = load_model(model_file)\n        # String to Vector\n        self.stov = WordToVectorDict(self.model)\n        self.writer.write(""Finished loading fasttext model"")\n\n        self._already_loaded = True\n\n    def _map_strings_to_indices(self, tokens):\n        length = min(len(tokens), self.max_length)\n        tokens = tokens[:length]\n\n        output = torch.full(\n            (self.max_length, self.model.get_dimension()),\n            fill_value=self.PAD_INDEX,\n            dtype=torch.float,\n        )\n\n        for idx, token in enumerate(tokens):\n            output[idx] = torch.from_numpy(self.stov[token])\n\n        return output\n\n    def __call__(self, item):\n        self._load_fasttext_model(self.model_file)\n        return super().__call__(item)\n\n\n@registry.register_processor(""vqa_answer"")\nclass VQAAnswerProcessor(BaseProcessor):\n    """"""Processor for generating answer scores for answers passed using VQA\n    accuracy formula. Using VocabDict class to represent answer vocabulary,\n    so parameters must specify ""vocab_file"". ""num_answers"" in parameter config\n    specify the max number of answers possible. Takes in dict containing\n    ""answers"" or ""answers_tokens"". ""answers"" are preprocessed to generate\n    ""answers_tokens"" if passed.\n\n    Args:\n        config (DictConfig): Configuration for the processor\n\n    Attributes:\n        answer_vocab (VocabDict): Class representing answer vocabulary\n    """"""\n\n    DEFAULT_NUM_ANSWERS = 10\n\n    def __init__(self, config, *args, **kwargs):\n        self.writer = registry.get(""writer"")\n        if not hasattr(config, ""vocab_file""):\n            raise AttributeError(\n                ""\'vocab_file\' argument required, but not ""\n                ""present in AnswerProcessor\'s config""\n            )\n\n        self.answer_vocab = VocabDict(config.vocab_file, *args, **kwargs)\n\n        self.preprocessor = None\n\n        if hasattr(config, ""preprocessor""):\n            self.preprocessor = Processor(config.preprocessor)\n\n            if self.preprocessor is None:\n                raise ValueError(\n                    f""No processor named {config.preprocessor} is defined.""\n                )\n\n        if hasattr(config, ""num_answers""):\n            self.num_answers = config.num_answers\n        else:\n            self.num_answers = self.DEFAULT_NUM_ANSWERS\n            warnings.warn(\n                ""\'num_answers\' not defined in the config. ""\n                ""Setting to default of {}"".format(self.DEFAULT_NUM_ANSWERS)\n            )\n\n    def __call__(self, item):\n        """"""Takes in dict with answers or answers_tokens, and returns back\n        a dict with answers (processed), ""answers_indices"" which point to\n        indices of the answers if present and ""answers_scores"" which represent\n        VQA style scores for the answers.\n\n        Args:\n            item (Dict): Dict containing answers or answers_tokens\n\n        Returns:\n            Dict: Processed answers, indices and scores.\n\n        """"""\n        tokens = None\n\n        if not isinstance(item, dict):\n            raise TypeError(""\'item\' passed to processor must be a dict"")\n\n        if ""answer_tokens"" in item:\n            tokens = item[""answer_tokens""]\n        elif ""answers"" in item:\n            if self.preprocessor is None:\n                raise AssertionError(\n                    ""\'preprocessor\' must be defined if you ""\n                    ""don\'t pass \'answer_tokens\'""\n                )\n\n            tokens = [\n                self.preprocessor({""text"": answer})[""text""]\n                for answer in item[""answers""]\n            ]\n        else:\n            raise AssertionError(\n                ""\'answers\' or \'answer_tokens\' must be passed""\n                "" to answer processor in a dict""\n            )\n\n        tokens = self._increase_to_ten(tokens)\n        answers_indices = torch.zeros(self.DEFAULT_NUM_ANSWERS, dtype=torch.long)\n        answers_indices.fill_(self.answer_vocab.get_unk_index())\n\n        for idx, token in enumerate(tokens):\n            answers_indices[idx] = self.answer_vocab.word2idx(token)\n\n        answers_scores = self.compute_answers_scores(answers_indices)\n\n        return {\n            ""answers"": tokens,\n            ""answers_indices"": answers_indices,\n            ""answers_scores"": answers_scores,\n        }\n\n    def get_vocab_size(self):\n        """"""Get vocab size of the answer vocabulary. Can also include\n        soft copy dynamic answer space size.\n\n        Returns:\n            int: size of the answer vocabulary\n\n        """"""\n        return self.answer_vocab.num_vocab\n\n    def get_true_vocab_size(self):\n        """"""True vocab size can be different from normal vocab size in some cases\n        such as soft copy where dynamic answer space is added.\n\n        Returns:\n            int: True vocab size.\n\n        """"""\n        return self.answer_vocab.num_vocab\n\n    def word2idx(self, word):\n        """"""Convert a word to its index according to vocabulary\n\n        Args:\n            word (str): Word to be converted to index.\n\n        Returns:\n            int: Index of the word.\n\n        """"""\n        return self.answer_vocab.word2idx(word)\n\n    def idx2word(self, idx):\n        """"""Index to word according to the vocabulary.\n\n        Args:\n            idx (int): Index to be converted to the word.\n\n        Returns:\n            str: Word corresponding to the index.\n\n        """"""\n        return self.answer_vocab.idx2word(idx)\n\n    def compute_answers_scores(self, answers_indices):\n        """"""Generate VQA based answer scores for answers_indices.\n\n        Args:\n            answers_indices (torch.LongTensor): tensor containing indices of the answers\n\n        Returns:\n            torch.FloatTensor: tensor containing scores.\n\n        """"""\n        scores = torch.zeros(self.get_vocab_size(), dtype=torch.float)\n        gt_answers = list(enumerate(answers_indices))\n        unique_answers = set(answers_indices.tolist())\n\n        for answer in unique_answers:\n            accs = []\n            for gt_answer in gt_answers:\n                other_answers = [item for item in gt_answers if item != gt_answer]\n\n                matching_answers = [item for item in other_answers if item[1] == answer]\n                acc = min(1, float(len(matching_answers)) / 3)\n                accs.append(acc)\n            avg_acc = sum(accs) / len(accs)\n\n            if answer != self.answer_vocab.UNK_INDEX:\n                scores[answer] = avg_acc\n\n        return scores\n\n    def _increase_to_ten(self, tokens):\n        while len(tokens) < self.DEFAULT_NUM_ANSWERS:\n            tokens += tokens[: self.DEFAULT_NUM_ANSWERS - len(tokens)]\n\n        return tokens\n\n\n@registry.register_processor(""multi_hot_answer_from_vocab"")\nclass MultiHotAnswerFromVocabProcessor(VQAAnswerProcessor):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n    def compute_answers_scores(self, answers_indices):\n        scores = torch.zeros(self.get_vocab_size(), dtype=torch.float)\n        scores[answers_indices] = 1\n        scores[self.answer_vocab.UNK_INDEX] = 0\n        return scores\n\n\n@registry.register_processor(""soft_copy_answer"")\nclass SoftCopyAnswerProcessor(VQAAnswerProcessor):\n    """"""Similar to Answer Processor but adds soft copy dynamic answer space to it.\n    Read https://arxiv.org/abs/1904.08920 for extra information on soft copy\n    and LoRRA.\n\n    Args:\n        config (DictConfig): Configuration for soft copy processor.\n\n    """"""\n\n    DEFAULT_MAX_LENGTH = 50\n\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n        if hasattr(config, ""max_length""):\n            self.max_length = config.max_length\n        else:\n            self.max_length = self.DEFAULT_MAX_LENGTH\n            warnings.warn(\n                ""\'max_length\' not defined in the config. ""\n                ""Setting to default of {}"".format(self.DEFAULT_MAX_LENGTH)\n            )\n\n        self.context_preprocessor = None\n        if hasattr(config, ""context_preprocessor""):\n            self.context_preprocessor = Processor(config.context_preprocessor)\n\n    def get_vocab_size(self):\n        """"""Size of Vocab + Size of Dynamic soft-copy based answer space\n\n        Returns:\n            int: Size of vocab + size of dynamic soft-copy answer space.\n\n        """"""\n        answer_vocab_nums = self.answer_vocab.num_vocab\n        answer_vocab_nums += self.max_length\n\n        return answer_vocab_nums\n\n    def get_true_vocab_size(self):\n        """"""Actual vocab size which only include size of the vocabulary file.\n\n        Returns:\n            int: Actual size of vocabs.\n\n        """"""\n        return self.answer_vocab.num_vocab\n\n    def __call__(self, item):\n        answers = item[""answers""]\n        scores = super().__call__({""answers"": answers})\n\n        indices = scores[""answers_indices""]\n        answers = scores[""answers""]\n        scores = scores[""answers_scores""]\n\n        tokens_scores = scores.new_zeros(self.max_length)\n        tokens = item[""tokens""]\n        length = min(len(tokens), self.max_length)\n\n        gt_answers = list(enumerate(answers))\n\n        if self.context_preprocessor is not None:\n            tokens = [\n                self.context_preprocessor({""text"": token})[""text""] for token in tokens\n            ]\n\n        answer_counter = Counter(answers)\n\n        for idx, token in enumerate(tokens[:length]):\n            if answer_counter[token] == 0:\n                continue\n            accs = []\n\n            for gt_answer in gt_answers:\n                other_answers = [item for item in gt_answers if item != gt_answer]\n                matching_answers = [item for item in other_answers if item[1] == token]\n                acc = min(1, float(len(matching_answers)) / 3)\n                accs.append(acc)\n\n            tokens_scores[idx] = sum(accs) / len(accs)\n\n        # Scores are already proper size, see L314. Now,\n        # fix scores for soft copy candidates\n        scores[-len(tokens_scores) :] = tokens_scores\n        return {\n            ""answers"": answers,\n            ""answers_indices"": indices,\n            ""answers_scores"": scores,\n        }\n\n\n@registry.register_processor(""simple_word"")\nclass SimpleWordProcessor(BaseProcessor):\n    """"""Tokenizes a word and processes it.\n\n    Attributes:\n        tokenizer (function): Type of tokenizer to be used.\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        from mmf.utils.text import word_tokenize\n\n        self.tokenizer = word_tokenize\n\n    def __call__(self, item, *args, **kwargs):\n        return {""text"": self.tokenizer(item[""text""], *args, **kwargs)}\n\n\n@registry.register_processor(""simple_sentence"")\nclass SimpleSentenceProcessor(BaseProcessor):\n    """"""Tokenizes a sentence and processes it.\n\n    Attributes:\n        tokenizer (function): Type of tokenizer to be used.\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        from mmf.utils.text import tokenize\n\n        self.tokenizer = tokenize\n\n    def __call__(self, item, *args, **kwargs):\n        return {""text"": self.tokenizer(item[""text""], *args, **kwargs)}\n\n\n@registry.register_processor(""bbox"")\nclass BBoxProcessor(VocabProcessor):\n    """"""Generates bboxes in proper format.\n    Takes in a dict which contains ""info"" key which is a list of dicts\n    containing following for each of the the bounding box\n\n    Example bbox input::\n\n        {\n            ""info"": [\n                {\n                    ""bounding_box"": {\n                        ""top_left_x"": 100,\n                        ""top_left_y"": 100,\n                        ""width"": 200,\n                        ""height"": 300\n                    }\n                },\n                ...\n            ]\n        }\n\n\n    This will further return a Sample in a dict with key ""bbox"" with last\n    dimension of 4 corresponding to ""xyxy"". So sample will look like following:\n\n    Example Sample::\n\n        Sample({\n            ""coordinates"": torch.Size(n, 4),\n            ""width"": List[number], # size n\n            ""height"": List[number], # size n\n            ""bbox_types"": List[str] # size n, either xyxy or xywh.\n            # currently only supports xyxy.\n        })\n\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        from mmf.utils.dataset import build_bbox_tensors\n\n        self.lambda_fn = build_bbox_tensors\n        self._init_extras(config)\n\n    def __call__(self, item):\n        info = item[""info""]\n        if self.preprocessor is not None:\n            info = self.preprocessor(info)\n\n        return {""bbox"": self.lambda_fn(info, self.max_length)}\n\n\n@registry.register_processor(""caption"")\nclass CaptionProcessor(BaseProcessor):\n    """"""Processes a caption with start, end and pad tokens and returns raw string.\n\n    Args:\n        config (DictConfig): Configuration for caption processor.\n\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        if not hasattr(config, ""vocab""):\n            raise AttributeError(\n                ""config passed to the processor has no "" ""attribute vocab""\n            )\n\n        self.vocab = Vocab(*args, **config.vocab, **kwargs)\n\n    def __call__(self, item):\n        for idx, v in enumerate(item):\n            if v == self.vocab.EOS_INDEX:\n                item = item[:idx]\n                break\n        tokens = [\n            self.vocab.get_itos()[w]\n            for w in item\n            if w\n            not in {self.vocab.SOS_INDEX, self.vocab.EOS_INDEX, self.vocab.PAD_INDEX}\n        ]\n        caption = "" "".join(tokens)\n        return {""tokens"": tokens, ""caption"": caption}\n\n\n@registry.register_processor(""evalai_answer"")\nclass EvalAIAnswerProcessor(BaseProcessor):\n    """"""Processes an answer similar to Eval AI\n\n    """"""\n\n    CONTRACTIONS = {\n        ""aint"": ""ain\'t"",\n        ""arent"": ""aren\'t"",\n        ""cant"": ""can\'t"",\n        ""couldve"": ""could\'ve"",\n        ""couldnt"": ""couldn\'t"",\n        ""couldn\'tve"": ""couldn\'t\'ve"",\n        ""couldnt\'ve"": ""couldn\'t\'ve"",\n        ""didnt"": ""didn\'t"",\n        ""doesnt"": ""doesn\'t"",\n        ""dont"": ""don\'t"",\n        ""hadnt"": ""hadn\'t"",\n        ""hadnt\'ve"": ""hadn\'t\'ve"",\n        ""hadn\'tve"": ""hadn\'t\'ve"",\n        ""hasnt"": ""hasn\'t"",\n        ""havent"": ""haven\'t"",\n        ""hed"": ""he\'d"",\n        ""hed\'ve"": ""he\'d\'ve"",\n        ""he\'dve"": ""he\'d\'ve"",\n        ""hes"": ""he\'s"",\n        ""howd"": ""how\'d"",\n        ""howll"": ""how\'ll"",\n        ""hows"": ""how\'s"",\n        ""Id\'ve"": ""I\'d\'ve"",\n        ""I\'dve"": ""I\'d\'ve"",\n        ""Im"": ""I\'m"",\n        ""Ive"": ""I\'ve"",\n        ""isnt"": ""isn\'t"",\n        ""itd"": ""it\'d"",\n        ""itd\'ve"": ""it\'d\'ve"",\n        ""it\'dve"": ""it\'d\'ve"",\n        ""itll"": ""it\'ll"",\n        ""let\'s"": ""let\'s"",\n        ""maam"": ""ma\'am"",\n        ""mightnt"": ""mightn\'t"",\n        ""mightnt\'ve"": ""mightn\'t\'ve"",\n        ""mightn\'tve"": ""mightn\'t\'ve"",\n        ""mightve"": ""might\'ve"",\n        ""mustnt"": ""mustn\'t"",\n        ""mustve"": ""must\'ve"",\n        ""neednt"": ""needn\'t"",\n        ""notve"": ""not\'ve"",\n        ""oclock"": ""o\'clock"",\n        ""oughtnt"": ""oughtn\'t"",\n        ""ow\'s\'at"": ""\'ow\'s\'at"",\n        ""\'ows\'at"": ""\'ow\'s\'at"",\n        ""\'ow\'sat"": ""\'ow\'s\'at"",\n        ""shant"": ""shan\'t"",\n        ""shed\'ve"": ""she\'d\'ve"",\n        ""she\'dve"": ""she\'d\'ve"",\n        ""she\'s"": ""she\'s"",\n        ""shouldve"": ""should\'ve"",\n        ""shouldnt"": ""shouldn\'t"",\n        ""shouldnt\'ve"": ""shouldn\'t\'ve"",\n        ""shouldn\'tve"": ""shouldn\'t\'ve"",\n        ""somebody\'d"": ""somebodyd"",\n        ""somebodyd\'ve"": ""somebody\'d\'ve"",\n        ""somebody\'dve"": ""somebody\'d\'ve"",\n        ""somebodyll"": ""somebody\'ll"",\n        ""somebodys"": ""somebody\'s"",\n        ""someoned"": ""someone\'d"",\n        ""someoned\'ve"": ""someone\'d\'ve"",\n        ""someone\'dve"": ""someone\'d\'ve"",\n        ""someonell"": ""someone\'ll"",\n        ""someones"": ""someone\'s"",\n        ""somethingd"": ""something\'d"",\n        ""somethingd\'ve"": ""something\'d\'ve"",\n        ""something\'dve"": ""something\'d\'ve"",\n        ""somethingll"": ""something\'ll"",\n        ""thats"": ""that\'s"",\n        ""thered"": ""there\'d"",\n        ""thered\'ve"": ""there\'d\'ve"",\n        ""there\'dve"": ""there\'d\'ve"",\n        ""therere"": ""there\'re"",\n        ""theres"": ""there\'s"",\n        ""theyd"": ""they\'d"",\n        ""theyd\'ve"": ""they\'d\'ve"",\n        ""they\'dve"": ""they\'d\'ve"",\n        ""theyll"": ""they\'ll"",\n        ""theyre"": ""they\'re"",\n        ""theyve"": ""they\'ve"",\n        ""twas"": ""\'twas"",\n        ""wasnt"": ""wasn\'t"",\n        ""wed\'ve"": ""we\'d\'ve"",\n        ""we\'dve"": ""we\'d\'ve"",\n        ""weve"": ""we\'ve"",\n        ""werent"": ""weren\'t"",\n        ""whatll"": ""what\'ll"",\n        ""whatre"": ""what\'re"",\n        ""whats"": ""what\'s"",\n        ""whatve"": ""what\'ve"",\n        ""whens"": ""when\'s"",\n        ""whered"": ""where\'d"",\n        ""wheres"": ""where\'s"",\n        ""whereve"": ""where\'ve"",\n        ""whod"": ""who\'d"",\n        ""whod\'ve"": ""who\'d\'ve"",\n        ""who\'dve"": ""who\'d\'ve"",\n        ""wholl"": ""who\'ll"",\n        ""whos"": ""who\'s"",\n        ""whove"": ""who\'ve"",\n        ""whyll"": ""why\'ll"",\n        ""whyre"": ""why\'re"",\n        ""whys"": ""why\'s"",\n        ""wont"": ""won\'t"",\n        ""wouldve"": ""would\'ve"",\n        ""wouldnt"": ""wouldn\'t"",\n        ""wouldnt\'ve"": ""wouldn\'t\'ve"",\n        ""wouldn\'tve"": ""wouldn\'t\'ve"",\n        ""yall"": ""y\'all"",\n        ""yall\'ll"": ""y\'all\'ll"",\n        ""y\'allll"": ""y\'all\'ll"",\n        ""yall\'d\'ve"": ""y\'all\'d\'ve"",\n        ""y\'alld\'ve"": ""y\'all\'d\'ve"",\n        ""y\'all\'dve"": ""y\'all\'d\'ve"",\n        ""youd"": ""you\'d"",\n        ""youd\'ve"": ""you\'d\'ve"",\n        ""you\'dve"": ""you\'d\'ve"",\n        ""youll"": ""you\'ll"",\n        ""youre"": ""you\'re"",\n        ""youve"": ""you\'ve"",\n    }\n\n    NUMBER_MAP = {\n        ""none"": ""0"",\n        ""zero"": ""0"",\n        ""one"": ""1"",\n        ""two"": ""2"",\n        ""three"": ""3"",\n        ""four"": ""4"",\n        ""five"": ""5"",\n        ""six"": ""6"",\n        ""seven"": ""7"",\n        ""eight"": ""8"",\n        ""nine"": ""9"",\n        ""ten"": ""10"",\n    }\n    ARTICLES = [""a"", ""an"", ""the""]\n    PERIOD_STRIP = re.compile(r""(?!<=\\d)(\\.)(?!\\d)"")\n    COMMA_STRIP = re.compile(r""(?<=\\d)(\\,)+(?=\\d)"")\n    PUNCTUATIONS = [\n        "";"",\n        r""/"",\n        ""["",\n        ""]"",\n        \'""\',\n        ""{"",\n        ""}"",\n        ""("",\n        "")"",\n        ""="",\n        ""+"",\n        ""\\\\"",\n        ""_"",\n        ""-"",\n        "">"",\n        ""<"",\n        ""@"",\n        ""`"",\n        "","",\n        ""?"",\n        ""!"",\n    ]\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def word_tokenize(self, word):\n        word = word.lower()\n        word = word.replace("","", """").replace(""?"", """").replace(""\'s"", "" \'s"")\n        return word.strip()\n\n    def process_punctuation(self, in_text):\n        out_text = in_text\n        for p in self.PUNCTUATIONS:\n            if (p + "" "" in in_text or "" "" + p in in_text) or (\n                re.search(self.COMMA_STRIP, in_text) is not None\n            ):\n                out_text = out_text.replace(p, """")\n            else:\n                out_text = out_text.replace(p, "" "")\n        out_text = self.PERIOD_STRIP.sub("""", out_text, re.UNICODE)\n        return out_text\n\n    def process_digit_article(self, in_text):\n        out_text = []\n        temp_text = in_text.lower().split()\n        for word in temp_text:\n            word = self.NUMBER_MAP.setdefault(word, word)\n            if word not in self.ARTICLES:\n                out_text.append(word)\n            else:\n                pass\n        for word_id, word in enumerate(out_text):\n            if word in self.CONTRACTIONS:\n                out_text[word_id] = self.CONTRACTIONS[word]\n        out_text = "" "".join(out_text)\n        return out_text\n\n    def __call__(self, item):\n        item = self.word_tokenize(item)\n        item = item.replace(""\\n"", "" "").replace(""\\t"", "" "").strip()\n        item = self.process_punctuation(item)\n        item = self.process_digit_article(item)\n        return item\n\n\n@registry.register_processor(""phoc"")\nclass PhocProcessor(VocabProcessor):\n    """"""\n    Compute PHOC features from text tokens\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        from mmf.utils.phoc import build_phoc\n\n        self._build_phoc = build_phoc\n        self._init_extras(config)\n        self.config = config\n\n    def _map_strings_to_indices(self, tokens):\n        length = min(len(tokens), self.max_length)\n        tokens = tokens[:length]\n\n        phoc_dim = 604\n        output = torch.full(\n            (self.max_length, phoc_dim), fill_value=self.PAD_INDEX, dtype=torch.float\n        )\n\n        for idx, token in enumerate(tokens):\n            output[idx] = torch.from_numpy(self._build_phoc(token))\n\n        return output\n\n\n@registry.register_processor(""copy"")\nclass CopyProcessor(BaseProcessor):\n    """"""\n    Copy boxes from numpy array\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        self.max_length = config.max_length\n\n    def __call__(self, item):\n        blob = item[""blob""]\n        final_blob = np.zeros((self.max_length,) + blob.shape[1:], blob.dtype)\n        final_blob[: len(blob)] = blob[: len(final_blob)]\n\n        return {""blob"": torch.from_numpy(final_blob)}\n\n\n@registry.register_processor(""m4c_answer"")\nclass M4CAnswerProcessor(BaseProcessor):\n    """"""\n    Process a TextVQA answer for iterative decoding in M4C\n    """"""\n\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n        self.answer_vocab = VocabDict(config.vocab_file, *args, **kwargs)\n        self.PAD_IDX = self.answer_vocab.word2idx(""<pad>"")\n        self.BOS_IDX = self.answer_vocab.word2idx(""<s>"")\n        self.EOS_IDX = self.answer_vocab.word2idx(""</s>"")\n        self.UNK_IDX = self.answer_vocab.UNK_INDEX\n\n        # make sure PAD_IDX, BOS_IDX and PAD_IDX are valid (not <unk>)\n        assert self.PAD_IDX != self.answer_vocab.UNK_INDEX\n        assert self.BOS_IDX != self.answer_vocab.UNK_INDEX\n        assert self.EOS_IDX != self.answer_vocab.UNK_INDEX\n        assert self.PAD_IDX == 0\n\n        self.answer_preprocessor = Processor(config.preprocessor)\n        assert self.answer_preprocessor is not None\n\n        self.num_answers = config.num_answers\n        self.max_length = config.max_length\n        self.max_copy_steps = config.max_copy_steps\n        assert self.max_copy_steps >= 1\n\n        self.match_answer_to_unk = False\n\n    def tokenize(self, sentence):\n        return sentence.split()\n\n    def match_answer_to_vocab_ocr_seq(\n        self, answer, vocab2idx_dict, ocr2inds_dict, max_match_num=20\n    ):\n        """"""\n        Match an answer to a list of sequences of indices\n        each index corresponds to either a fixed vocabulary or an OCR token\n        (in the index address space, the OCR tokens are after the fixed vocab)\n        """"""\n        num_vocab = len(vocab2idx_dict)\n\n        answer_words = self.tokenize(answer)\n        answer_word_matches = []\n        for word in answer_words:\n            # match answer word to fixed vocabulary\n            matched_inds = []\n            if word in vocab2idx_dict:\n                matched_inds.append(vocab2idx_dict.get(word))\n            # match answer word to OCR\n            # we put OCR after the fixed vocabulary in the answer index space\n            # so add num_vocab offset to the OCR index\n            matched_inds.extend([num_vocab + idx for idx in ocr2inds_dict[word]])\n            if len(matched_inds) == 0:\n                if self.match_answer_to_unk:\n                    matched_inds.append(vocab2idx_dict.get(""<unk>""))\n                else:\n                    return []\n            answer_word_matches.append(matched_inds)\n\n        # expand per-word matched indices into the list of matched sequences\n        if len(answer_word_matches) == 0:\n            return []\n        idx_seq_list = [()]\n        for matched_inds in answer_word_matches:\n            idx_seq_list = [\n                seq + (idx,) for seq in idx_seq_list for idx in matched_inds\n            ]\n            if len(idx_seq_list) > max_match_num:\n                idx_seq_list = idx_seq_list[:max_match_num]\n\n        return idx_seq_list\n\n    def get_vocab_size(self):\n        answer_vocab_nums = self.answer_vocab.num_vocab\n        answer_vocab_nums += self.max_length\n\n        return answer_vocab_nums\n\n    def get_true_vocab_size(self):\n        return self.answer_vocab.num_vocab\n\n    def compute_answer_scores(self, answers):\n        gt_answers = list(enumerate(answers))\n        unique_answers = sorted(set(answers))\n        unique_answer_scores = [0] * len(unique_answers)\n        for idx, unique_answer in enumerate(unique_answers):\n            accs = []\n            for gt_answer in gt_answers:\n                other_answers = [item for item in gt_answers if item != gt_answer]\n                matching_answers = [\n                    item for item in other_answers if item[1] == unique_answer\n                ]\n                acc = min(1, float(len(matching_answers)) / 3)\n                accs.append(acc)\n            unique_answer_scores[idx] = sum(accs) / len(accs)\n        unique_answer2score = {\n            a: s for a, s in zip(unique_answers, unique_answer_scores)\n        }\n        return unique_answer2score\n\n    def __call__(self, item):\n        answers = item[""answers""]\n\n        if not answers:\n            return {\n                ""sampled_idx_seq"": None,\n                ""train_prev_inds"": torch.zeros(self.max_copy_steps, dtype=torch.long),\n            }\n\n        answers = [self.answer_preprocessor({""text"": a})[""text""] for a in answers]\n        assert len(answers) == self.num_answers\n\n        # Step 1: calculate the soft score of ground-truth answers\n        unique_answer2score = self.compute_answer_scores(answers)\n\n        # Step 2: fill the first step soft scores for tokens\n        scores = torch.zeros(\n            self.max_copy_steps, self.get_vocab_size(), dtype=torch.float\n        )\n\n        # match answers to fixed vocabularies and OCR tokens.\n        ocr2inds_dict = defaultdict(list)\n        for idx, token in enumerate(item[""tokens""]):\n            ocr2inds_dict[token].append(idx)\n        answer_dec_inds = [\n            self.match_answer_to_vocab_ocr_seq(\n                a, self.answer_vocab.word2idx_dict, ocr2inds_dict\n            )\n            for a in answers\n        ]\n\n        # Collect all the valid decoding sequences for each answer.\n        # This part (idx_seq_list) was pre-computed in imdb (instead of online)\n        # to save time\n        all_idx_seq_list = []\n        for answer, idx_seq_list in zip(answers, answer_dec_inds):\n            all_idx_seq_list.extend(idx_seq_list)\n            # fill in the soft score for the first decoding step\n            score = unique_answer2score[answer]\n            for idx_seq in idx_seq_list:\n                score_idx = idx_seq[0]\n                # the scores for the decoding Step 0 will be the maximum\n                # among all answers starting with that vocab\n                # for example:\n                # if ""red apple"" has score 0.7 and ""red flag"" has score 0.8\n                # the score for ""red"" at Step 0 will be max(0.7, 0.8) = 0.8\n                scores[0, score_idx] = max(scores[0, score_idx], score)\n\n        # train_prev_inds is the previous prediction indices in auto-regressive\n        # decoding\n        train_prev_inds = torch.zeros(self.max_copy_steps, dtype=torch.long)\n        # train_loss_mask records the decoding steps where losses are applied\n        train_loss_mask = torch.zeros(self.max_copy_steps, dtype=torch.float)\n        if len(all_idx_seq_list) > 0:\n            # sample a random decoding answer sequence for teacher-forcing\n            idx_seq = all_idx_seq_list[np.random.choice(len(all_idx_seq_list))]\n            dec_step_num = min(1 + len(idx_seq), self.max_copy_steps)\n            train_loss_mask[:dec_step_num] = 1.0\n\n            train_prev_inds[0] = self.BOS_IDX\n            for t in range(1, dec_step_num):\n                train_prev_inds[t] = idx_seq[t - 1]\n                score_idx = idx_seq[t] if t < len(idx_seq) else self.EOS_IDX\n                scores[t, score_idx] = 1.0\n        else:\n            idx_seq = ()\n\n        answer_info = {\n            ""answers"": answers,\n            ""answers_scores"": scores,\n            ""sampled_idx_seq"": idx_seq,\n            ""train_prev_inds"": train_prev_inds,\n            ""train_loss_mask"": train_loss_mask,\n        }\n        return answer_info\n\n\n@registry.register_processor(""m4c_caption"")\nclass M4CCaptionProcessor(M4CAnswerProcessor):\n    def __init__(self, config, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n        import re\n\n        self.SENTENCE_SPLIT_REGEX = re.compile(r""(\\W+)"")\n\n        self.match_answer_to_unk = True\n\n    def tokenize(self, sentence):\n        sentence = sentence.lower()\n        sentence = (\n            sentence.replace("","", """")\n            .replace(""?"", """")\n            .replace(""."", """")\n            .replace(""\'s"", "" \'s"")\n        )\n        tokens = self.SENTENCE_SPLIT_REGEX.split(sentence)\n        tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n        return tokens\n\n    def compute_answer_scores(self, answers):\n        unique_answer2score = {a: 1.0 for a in answers}\n        return unique_answer2score\n'"
mmf/models/interfaces/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/models/interfaces/mmbt.py,1,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport os\nimport tempfile\nfrom pathlib import Path\nfrom typing import Type, Union\n\nimport torch\nimport torchvision.datasets.folder as tv_helpers\nfrom PIL import Image\nfrom torch import nn\n\nfrom mmf.common import typings as mmf_typings\nfrom mmf.common.sample import Sample, SampleList\nfrom mmf.models.base_model import BaseModel\nfrom mmf.utils.build import build_processors\nfrom mmf.utils.download import download\n\nMMBT_GRID_HM_CONFIG_PATH = Path(""projects/hateful_memes/configs/mmbt/defaults.yaml"")\nImageType = Union[Type[Image.Image], str]\nPathType = Union[Type[Path], str]\nBaseModelType = Type[BaseModel]\n\n\nclass MMBTGridHMInterface(nn.Module):\n    """"""Interface for MMBT Grid for Hateful Memes.\n    """"""\n\n    def __init__(self, model: BaseModelType, config: mmf_typings.DictConfig):\n        super().__init__()\n        self.model = model\n        self.config = config\n        self.init_processors()\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs)\n\n    def init_processors(self):\n        config = self.config.dataset_config.hateful_memes\n        extra_params = {""data_dir"": config.data_dir}\n        self.processor_dict = build_processors(config.processors, **extra_params)\n\n    def classify(self, image: ImageType, text: str):\n        """"""Classifies a given image and text in it into Hateful/Non-Hateful.\n        Image can be a url or a local path or you can directly pass a PIL.Image.Image\n        object. Text needs to be a sentence containing all text in the image.\n\n            >>> from mmf.models.mmbt import MMBT\n            >>> model = MMBT.from_pretrained(""mmbt.hateful_memes.images"")\n            >>> model.classify(""some_url"", ""some_text"")\n            {""label"": 0, ""confidence"": 0.56}\n\n        Args:\n            image (ImageType): Image to be classified\n            text (str): Text in the image\n\n        Returns:\n            bool: Whether image is hateful (1) or non hateful (0)\n        """"""\n        if isinstance(image, str):\n            if image.startswith(""http""):\n                temp_file = tempfile.NamedTemporaryFile()\n                download(image, *os.path.split(temp_file.name), disable_tqdm=True)\n                image = tv_helpers.default_loader(temp_file.name)\n                temp_file.close()\n            else:\n                image = tv_helpers.default_loader(image)\n\n        text = self.processor_dict[""text_processor""]({""text"": text})\n        image = self.processor_dict[""image_processor""](image)\n\n        sample = Sample()\n        sample.text = text[""text""]\n        if ""input_ids"" in text:\n            sample.update(text)\n\n        sample.image = image\n        sample_list = SampleList([sample])\n        device = next(self.model.parameters()).device\n        sample_list = sample_list.to(device)\n\n        output = self.model(sample_list)\n        scores = nn.functional.softmax(output[""scores""], dim=1)\n        confidence, label = torch.max(scores, dim=1)\n\n        return {""label"": label.item(), ""confidence"": confidence.item()}\n'"
mmf/utils/phoc/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom .build_phoc import build_phoc  # NoQA\n'"
mmf/utils/phoc/build_phoc.py,0,"b'import numpy as np\n\nfrom .cphoc import build_phoc as _build_phoc_raw\n\n_alphabet = {\n    ""a"",\n    ""b"",\n    ""c"",\n    ""d"",\n    ""e"",\n    ""f"",\n    ""g"",\n    ""h"",\n    ""i"",\n    ""j"",\n    ""k"",\n    ""l"",\n    ""m"",\n    ""n"",\n    ""o"",\n    ""p"",\n    ""q"",\n    ""r"",\n    ""s"",\n    ""t"",\n    ""u"",\n    ""v"",\n    ""w"",\n    ""x"",\n    ""y"",\n    ""z"",\n    ""0"",\n    ""1"",\n    ""2"",\n    ""3"",\n    ""4"",\n    ""5"",\n    ""6"",\n    ""7"",\n    ""8"",\n    ""9"",\n}  # NoQA\n\n\ndef build_phoc(token):\n    token = token.lower().strip()\n    token = """".join([c for c in token if c in _alphabet])\n    phoc = _build_phoc_raw(token)\n    phoc = np.array(phoc, dtype=np.float32)\n    return phoc\n'"
projects/m4c/scripts/extract_ocr_frcn_feature.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n# install `vqa-maskrcnn-benchmark` from\n# https://github.com/ronghanghu/vqa-maskrcnn-benchmark-m4c\nimport argparse\nimport os\nimport sys\n\nimport cv2\nimport numpy as np\nimport torch\nimport tqdm\nfrom maskrcnn_benchmark.config import cfg\nfrom maskrcnn_benchmark.layers import nms\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\nfrom maskrcnn_benchmark.utils.model_serialization import load_state_dict\nfrom PIL import Image\n\nsys.path.append(""/private/home/ronghanghu/workspace/vqa-maskrcnn-benchmark"")  # NoQA\n\n\ndef load_detection_model(yaml_file, yaml_ckpt):\n    cfg.merge_from_file(yaml_file)\n    cfg.freeze()\n\n    model = build_detection_model(cfg)\n    checkpoint = torch.load(yaml_ckpt, map_location=torch.device(""cpu""))\n\n    load_state_dict(model, checkpoint.pop(""model""))\n\n    model.to(""cuda"")\n    model.eval()\n    return model\n\n\ndef _image_transform(image_path):\n    img = Image.open(image_path)\n    im = np.array(img).astype(np.float32)\n    # handle a few corner cases\n    if im.ndim == 2:  # gray => RGB\n        im = np.tile(im[:, :, None], (1, 1, 3))\n    if im.shape[2] > 3:  # RGBA => RGB\n        im = im[:, :, :3]\n\n    im = im[:, :, ::-1]  # RGB => BGR\n    im -= np.array([102.9801, 115.9465, 122.7717])\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(800) / float(im_size_min)\n    # Prevent the biggest axis from being more than max_size\n    if np.round(im_scale * im_size_max) > 1333:\n        im_scale = float(1333) / float(im_size_max)\n    im = cv2.resize(\n        im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR\n    )\n    img = torch.from_numpy(im).permute(2, 0, 1)\n    return img, im_scale\n\n\ndef _process_feature_extraction(output, im_scales, feat_name=""fc6""):\n    batch_size = len(output[0][""proposals""])\n    n_boxes_per_image = [len(_) for _ in output[0][""proposals""]]\n    score_list = output[0][""scores""].split(n_boxes_per_image)\n    score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n    feats = output[0][feat_name].split(n_boxes_per_image)\n    cur_device = score_list[0].device\n\n    feat_list = []\n    bbox_list = []\n\n    for i in range(batch_size):\n        dets = output[0][""proposals""][i].bbox / im_scales[i]\n        scores = score_list[i]\n\n        max_conf = torch.zeros(scores.shape[0]).to(cur_device)\n\n        for cls_ind in range(1, scores.shape[1]):\n            cls_scores = scores[:, cls_ind]\n            keep = nms(dets, cls_scores, 0.5)\n            max_conf[keep] = torch.where(\n                cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep]\n            )\n\n        keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n        feat_list.append(feats[i][keep_boxes])\n        bbox_list.append(output[0][""proposals""][i].bbox[keep_boxes])\n    return feat_list, bbox_list\n\n\ndef extract_features(detection_model, image_path, input_boxes=None, feat_name=""fc6""):\n    im, im_scale = _image_transform(image_path)\n    if input_boxes is not None:\n        if isinstance(input_boxes, np.ndarray):\n            input_boxes = torch.from_numpy(input_boxes.copy())\n        input_boxes *= im_scale\n    img_tensor, im_scales = [im], [im_scale]\n    current_img_list = to_image_list(img_tensor, size_divisible=32)\n    current_img_list = current_img_list.to(""cuda"")\n    with torch.no_grad():\n        output = detection_model(current_img_list, input_boxes=input_boxes)\n\n    if input_boxes is None:\n        feat_list, bbox_list = _process_feature_extraction(output, im_scales, feat_name)\n        feat = feat_list[0].cpu().numpy()\n        bbox = bbox_list[0].cpu().numpy() / im_scale\n    else:\n        feat = output[0][feat_name].cpu().numpy()\n        bbox = output[0][""proposals""][0].bbox.cpu().numpy() / im_scale\n\n    return feat, bbox\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--detection_cfg"",\n        type=str,\n        default=""/private/home/ronghanghu/workspace/pythia/data/""\n        + ""frcn_feature_extraction/detectron_model.yaml"",\n        help=""Detectron config file; download it from ""\n        + ""https://dl.fbaipublicfiles.com/pythia/detectron_model/""\n        + ""detectron_model.yaml"",\n    )\n    parser.add_argument(\n        ""--detection_model"",\n        type=str,\n        default=""/private/home/ronghanghu/workspace/pythia/data/""\n        + ""frcn_feature_extraction/detectron_model.pth"",\n        help=""Detectron model file; download it""\n        + "" from https://dl.fbaipublicfiles.com/pythia/detectron_model/""\n        + ""detectron_model.pth"",\n    )\n    parser.add_argument(\n        ""--imdb_file"",\n        type=str,\n        default=""/private/home/ronghanghu/workspace/pythia/data/""\n        + ""imdb/m4c_textvqa/imdb_train_ocr_en.npy"",\n        help=""The imdb to extract features"",\n    )\n    parser.add_argument(\n        ""--image_dir"",\n        type=str,\n        default=""/private/home/ronghanghu/workspace/DATASETS/TextVQA"",\n        help=""The directory containing images"",\n    )\n    parser.add_argument(\n        ""--save_dir"",\n        type=str,\n        default=""/private/home/ronghanghu/workspace/pythia/data/""\n        + ""m4c_textvqa_ocr_en_frcn_features_2/train_images"",\n        help=""The directory to save extracted features"",\n    )\n    args = parser.parse_args()\n\n    DETECTION_YAML = args.detection_cfg\n    DETECTION_CKPT = args.detection_model\n    IMDB_FILE = args.imdb_file\n    IMAGE_DIR = args.image_dir\n    SAVE_DIR = args.save_dir\n\n    imdb = np.load(IMDB_FILE, allow_pickle=True)[1:]\n    # keep only one entry per image_id\n    image_id2info = {info[""image_id""]: info for info in imdb}\n    imdb = list(image_id2info[k] for k in sorted(image_id2info))\n\n    detection_model = load_detection_model(DETECTION_YAML, DETECTION_CKPT)\n    print(""Faster R-CNN OCR features"")\n    print(""\\textracting from"", IMDB_FILE)\n    print(""\\tsaving to"", SAVE_DIR)\n    for _, info in enumerate(tqdm.tqdm(imdb)):\n        image_path = os.path.join(IMAGE_DIR, info[""image_path""])\n        save_feat_path = os.path.join(SAVE_DIR, info[""feature_path""])\n        save_info_path = save_feat_path.replace("".npy"", ""_info.npy"")\n        os.makedirs(os.path.dirname(save_feat_path), exist_ok=True)\n\n        w = info[""image_width""]\n        h = info[""image_height""]\n        ocr_normalized_boxes = np.array(info[""ocr_normalized_boxes""])\n        ocr_boxes = ocr_normalized_boxes.reshape(-1, 4) * [w, h, w, h]\n        ocr_tokens = info[""ocr_tokens""]\n        if len(ocr_boxes) > 0:\n            extracted_feat, _ = extract_features(\n                detection_model, image_path, input_boxes=ocr_boxes\n            )\n        else:\n            extracted_feat = np.zeros((0, 2048), np.float32)\n\n        np.save(save_info_path, {""ocr_boxes"": ocr_boxes, ""ocr_tokens"": ocr_tokens})\n        np.save(save_feat_path, extracted_feat)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
projects/m4c_captioner/scripts/coco_eval.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport json\n\nimport numpy as np\n\nimport tools.scripts.coco.coco_caption_eval as coco_caption_eval\n\n\ndef print_metrics(res_metrics):\n    print(res_metrics)\n    keys = [\n        ""Bleu_1"",\n        ""Bleu_2"",\n        ""Bleu_3"",\n        ""Bleu_4"",\n        ""METEOR"",\n        ""ROUGE_L"",\n        ""SPICE"",\n        ""CIDEr"",\n    ]\n    print(""\\n\\n**********\\nFinal model performance:\\n**********"")\n    for k in keys:\n        print(k, "": %.1f"" % (res_metrics[k] * 100))\n\n\nif __name__ == ""__main__"":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--pred_file"", type=str, required=True)\n    parser.add_argument(""--annotation_file"", type=str, required=True)\n    parser.add_argument(""--set"", type=str, default=""karpathy_val"")\n    args = parser.parse_args()\n\n    with open(args.pred_file) as f:\n        preds = json.load(f)\n    annotation_file = args.annotation_file\n    imdb = np.load(annotation_file, allow_pickle=True)\n    imdb = imdb[1:]\n\n    gts = [\n        {""image_id"": info[""image_id""], ""caption"": info[""caption_str""]} for info in imdb\n    ]\n    preds = [{""image_id"": int(p[""image_id""]), ""caption"": p[""caption""]} for p in preds]\n    imgids = list({g[""image_id""] for g in gts})\n\n    metrics = coco_caption_eval.calculate_metrics(\n        imgids, {""annotations"": gts}, {""annotations"": preds}\n    )\n\n    print_metrics(metrics)\n'"
projects/m4c_captioner/scripts/textcaps_eval.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport json\n\nimport numpy as np\n\nimport tools.scripts.coco.coco_caption_eval as coco_caption_eval\n\n\ndef print_metrics(res_metrics):\n    print(res_metrics)\n    keys = [\n        ""Bleu_1"",\n        ""Bleu_2"",\n        ""Bleu_3"",\n        ""Bleu_4"",\n        ""METEOR"",\n        ""ROUGE_L"",\n        ""SPICE"",\n        ""CIDEr"",\n    ]\n    print(""\\n\\n**********\\nFinal model performance:\\n**********"")\n    for k in keys:\n        print(k, "": %.1f"" % (res_metrics[k] * 100))\n\n\nif __name__ == ""__main__"":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--pred_file"", type=str, required=True)\n    parser.add_argument(""--annotation_file"", type=str, required=True)\n    parser.add_argument(""--set"", type=str, default=""val"")\n    args = parser.parse_args()\n\n    if args.set not in [""train"", ""val""]:\n        raise Exception(\n            ""this script only supports TextCaps train and val set. ""\n            ""Please use the EvalAI server for test set evaluation""\n        )\n\n    with open(args.pred_file) as f:\n        preds = json.load(f)\n    annotation_file = args.annotation_file\n    imdb = np.load(annotation_file, allow_pickle=True)\n    imdb = imdb[1:]\n\n    gts = [\n        {""image_id"": info[""image_id""], ""caption"": info[""caption_str""]} for info in imdb\n    ]\n    preds = [{""image_id"": p[""image_id""], ""caption"": p[""caption""]} for p in preds]\n    imgids = list({g[""image_id""] for g in gts})\n\n    metrics = coco_caption_eval.calculate_metrics(\n        imgids, {""annotations"": gts}, {""annotations"": preds}\n    )\n\n    print_metrics(metrics)\n'"
tests/models/interfaces/test_interfaces.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport unittest\n\nimport numpy as np\n\nfrom mmf.models.mmbt import MMBT\nfrom tests.test_utils import skip_if_no_network\n\n\nclass TestModelInterfaces(unittest.TestCase):\n    @skip_if_no_network\n    def test_mmbt_hm_interface(self):\n        model = MMBT.from_pretrained(""mmbt.hateful_memes.images"")\n        result = model.classify(\n            ""https://i.imgur.com/tEcsk5q.jpg"", ""look how many people love you""\n        )\n\n        self.assertEqual(result[""label""], 0)\n        np.testing.assert_almost_equal(result[""confidence""], 0.9999, decimal=4)\n        result = model.classify(\n            ""https://i.imgur.com/tEcsk5q.jpg"", ""they have the privilege""\n        )\n        self.assertEqual(result[""label""], 0)\n        np.testing.assert_almost_equal(result[""confidence""], 0.9869, decimal=4)\n        result = model.classify(\n            ""https://i.imgur.com/tEcsk5q.jpg"", ""a black man doing nothing""\n        )\n        self.assertEqual(result[""label""], 1)\n        np.testing.assert_almost_equal(result[""confidence""], 0.6941, decimal=4)\n'"
tools/scripts/bert/extract_bert_embeddings.py,3,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport argparse\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom transformers.modeling_bert import BertModel\nfrom transformers.tokenization_auto import AutoTokenizer\n\n\nclass BertFeatExtractor:\n    def __init__(self, model_name):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = BertModel.from_pretrained(model_name).eval()\n        self.model.cuda()\n\n    def get_bert_embedding(self, text):\n        tokenized_text = self.tokenizer.tokenize(text)\n        tokenized_text = [""[CLS]""] + tokenized_text + [""[SEP]""]\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n        tokens_tensor = torch.Tensor([indexed_tokens]).long()\n        segments_tensor = torch.Tensor([0] * len(tokenized_text)).long()\n        with torch.no_grad():\n            encoded_layers, _ = self.model(\n                tokens_tensor.cuda(),\n                segments_tensor.cuda(),\n                output_all_encoded_layers=False,\n            )\n        return encoded_layers.squeeze()[0]\n\n\ndef extract_bert(imdb_path, out_path, group_id=0, n_groups=1):\n    imdb = np.load(imdb_path)\n\n    feat_extractor = BertFeatExtractor(""bert-base-uncased"")\n\n    if group_id == 0:\n        iterator_obj = tqdm(imdb[1:])\n    else:\n        iterator_obj = imdb[1:]\n\n    for idx, el in enumerate(iterator_obj):\n        if idx % n_groups != group_id:\n            continue\n        emb = feat_extractor.get_bert_embedding(el[""question_str""])\n        save_path = out_path + str(el[""question_id""])\n        np.save(save_path, emb.cpu().numpy())\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--imdb_path"", type=str, default=None)\n    parser.add_argument(""--out_path"", type=str, default=None)\n    parser.add_argument(""--group_id"", type=int, default=0)\n    parser.add_argument(""--n_groups"", type=int, default=1)\n    args = parser.parse_args()\n    extract_bert(args.imdb_path, args.out_path, args.group_id, args.n_groups)\n'"
tools/scripts/coco/coco_caption_eval.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n# The following script requires Java 1.8.0 and pycocotools installed.\n# The pycocoevalcap can be installed with pip as\n# pip install git+https://github.com/flauted/coco-caption.git@python23\n# Original pycocoevalcap code is at https://github.com/tylin/coco-caption\n# but has no python3 support yet.\n\nimport argparse\nimport json\n\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom pycocoevalcap.cider.cider import Cider\nfrom pycocoevalcap.meteor.meteor import Meteor\nfrom pycocoevalcap.rouge.rouge import Rouge\nfrom pycocoevalcap.spice.spice import Spice\nfrom pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n\n\nclass COCOEvalCap:\n    """"""\n    COCOEvalCap code is adopted from https://github.com/tylin/coco-caption\n    """"""\n\n    def __init__(self, img_ids, coco, coco_res):\n        self.eval_imgs = []\n        self.eval = dict()\n        self.img_to_eval = dict()\n        self.coco = coco\n        self.coco_res = coco_res\n\n    def evaluate(self):\n        gts = self.coco\n        res = self.coco_res\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print(""tokenization..."")\n        tokenizer = PTBTokenizer()\n        gts = tokenizer.tokenize(gts)\n        res = tokenizer.tokenize(res)\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print(""setting up scorers..."")\n        scorers = [\n            (Bleu(4), [""Bleu_1"", ""Bleu_2"", ""Bleu_3"", ""Bleu_4""]),\n            (Meteor(), ""METEOR""),\n            (Rouge(), ""ROUGE_L""),\n            (Cider(), ""CIDEr""),\n            (Spice(), ""SPICE""),\n        ]\n\n        # =================================================\n        # Compute scores\n        # =================================================\n        for scorer, method in scorers:\n            print(""computing %s score..."" % (scorer.method()))\n            score, scores = scorer.compute_score(gts, res)\n            if type(method) == list:\n                for sc, scs, m in zip(score, scores, method):\n                    self.set_eval(sc, m)\n                    self.set_img_to_eval_imgs(scs, gts.keys(), m)\n                    print(f""{m}: {sc:0.3f}"")\n            else:\n                self.set_eval(score, method)\n                self.set_img_to_eval_imgs(scores, gts.keys(), method)\n                print(f""{method}: {score:0.3f}"")\n        self.set_eval_imgs()\n\n    def set_eval(self, score, method):\n        self.eval[method] = score\n\n    def set_img_to_eval_imgs(self, scores, img_ids, method):\n        for img_id, score in zip(img_ids, scores):\n            if img_id not in self.img_to_eval:\n                self.img_to_eval[img_id] = dict()\n                self.img_to_eval[img_id][""image_id""] = img_id\n            self.img_to_eval[img_id][method] = score\n\n    def set_eval_imgs(self):\n        self.eval_imgs = [eval for img_id, eval in self.img_to_eval.items()]\n\n\ndef calculate_metrics(img_ids, dataset_gts, dataset_res):\n    img_to_anns_gts = {id: [] for id in img_ids}\n    for ann in dataset_gts[""annotations""]:\n        img_to_anns_gts[ann[""image_id""]] += [ann]\n\n    img_to_anns_res = {id: [] for id in img_ids}\n    for ann in dataset_res[""annotations""]:\n        img_to_anns_res[ann[""image_id""]] += [ann]\n\n    eval_obj = COCOEvalCap(img_ids, img_to_anns_gts, img_to_anns_res)\n    eval_obj.evaluate()\n    return eval_obj.eval\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Image captioning metrics"")\n    parser.add_argument(""--reference_json"", help=""Path to reference captions json"")\n    parser.add_argument(""--predicted_json"", help=""Path to predicted captions json"")\n    args = parser.parse_args()\n\n    with open(args.reference_json) as f:\n        captions = json.load(f)\n\n    references = []\n    img_ids = []\n\n    for img in captions[""images""]:\n        if img[""split""] == ""test"":\n            for c in img[""sentences""]:\n                d = {}\n                d[""image_id""] = c[""imgid""]\n                img_ids.append(c[""imgid""])\n                d[""caption""] = c[""raw""]\n                references.append(d)\n    img_ids = list(set(img_ids))\n\n    with open(args.predicted_json) as f:\n        preds = json.load(f)\n\n    dataset_gts = {""annotations"": references}\n    dataset_res = {""annotations"": preds}\n    print(calculate_metrics(img_ids, dataset_gts, dataset_res))\n'"
tools/scripts/features/extract_features_vmb.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n# Requires vqa-maskrcnn-benchmark to be built and installed\n# Category mapping for visual genome can be downloaded from\n# https://dl.fbaipublicfiles.com/pythia/data/visual_genome_categories.json\n# When the --background flag is set, the index saved with key ""objects"" in\n# info_list will be +1 of the Visual Genome category mapping above and 0\n# is the background class. When the --background flag is not set, the\n# index saved with key ""objects"" in info list will match the Visual Genome\n# category mapping.\nimport argparse\nimport glob\nimport os\n\nimport cv2\nimport numpy as np\nimport torch\nfrom maskrcnn_benchmark.config import cfg\nfrom maskrcnn_benchmark.layers import nms\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\nfrom maskrcnn_benchmark.utils.model_serialization import load_state_dict\nfrom PIL import Image\n\nfrom mmf.utils.general import download_file\n\n\nclass FeatureExtractor:\n    MODEL_URL = (\n        ""https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.pth""\n    )\n    CONFIG_URL = (\n        ""https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model.yaml""\n    )\n    MAX_SIZE = 1333\n    MIN_SIZE = 800\n\n    def __init__(self):\n        self.args = self.get_parser().parse_args()\n        self.detection_model = self._build_detection_model()\n\n        os.makedirs(self.args.output_folder, exist_ok=True)\n\n    def _try_downloading_necessities(self):\n        if self.args.model_file is None:\n            print(""Downloading model and configuration"")\n            self.args.model_file = self.MODEL_URL.split(""/"")[-1]\n            self.args.config_file = self.CONFIG_URL.split(""/"")[-1]\n            download_file(self.MODEL_URL)\n            download_file(self.CONFIG_URL)\n\n    def get_parser(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            ""--model_file"", default=None, type=str, help=""Detectron model file""\n        )\n        parser.add_argument(\n            ""--config_file"", default=None, type=str, help=""Detectron config file""\n        )\n        parser.add_argument(\n            ""--start_index"", default=0, type=int, help=""Index to start from ""\n        )\n        parser.add_argument(""--end_index"", default=None, type=int, help="""")\n        parser.add_argument(""--batch_size"", type=int, default=2, help=""Batch size"")\n        parser.add_argument(\n            ""--num_features"",\n            type=int,\n            default=100,\n            help=""Number of features to extract."",\n        )\n        parser.add_argument(\n            ""--output_folder"", type=str, default=""./output"", help=""Output folder""\n        )\n        parser.add_argument(""--image_dir"", type=str, help=""Image directory or file"")\n        parser.add_argument(\n            ""--feature_name"",\n            type=str,\n            help=""The name of the feature to extract"",\n            default=""fc6"",\n        )\n        parser.add_argument(\n            ""--exclude_list"",\n            type=str,\n            help=""List of images to be excluded from feature conversion. ""\n            + ""Each image on a new line"",\n            default=""./list"",\n        )\n        parser.add_argument(\n            ""--confidence_threshold"",\n            type=float,\n            default=0,\n            help=""Threshold of detection confidence above which boxes will be selected"",\n        )\n        parser.add_argument(\n            ""--background"",\n            action=""store_true"",\n            help=""The model will output predictions for the background class when set"",\n        )\n        return parser\n\n    def _build_detection_model(self):\n        cfg.merge_from_file(self.args.config_file)\n        cfg.freeze()\n\n        model = build_detection_model(cfg)\n        checkpoint = torch.load(self.args.model_file, map_location=torch.device(""cpu""))\n\n        load_state_dict(model, checkpoint.pop(""model""))\n\n        model.to(""cuda"")\n        model.eval()\n        return model\n\n    def _image_transform(self, path):\n        img = Image.open(path)\n        im = np.array(img).astype(np.float32)\n\n        if im.shape[-1] > 3:\n            im = np.array(img.convert(""RGB"")).astype(np.float32)\n\n        # IndexError: too many indices for array, grayscale images\n        if len(im.shape) < 3:\n            im = np.repeat(im[:, :, np.newaxis], 3, axis=2)\n\n        im = im[:, :, ::-1]\n        im -= np.array([102.9801, 115.9465, 122.7717])\n        im_shape = im.shape\n        im_height = im_shape[0]\n        im_width = im_shape[1]\n        im_size_min = np.min(im_shape[0:2])\n        im_size_max = np.max(im_shape[0:2])\n\n        # Scale based on minimum size\n        im_scale = self.MIN_SIZE / im_size_min\n\n        # Prevent the biggest axis from being more than max_size\n        # If bigger, scale it down\n        if np.round(im_scale * im_size_max) > self.MAX_SIZE:\n            im_scale = self.MAX_SIZE / im_size_max\n\n        im = cv2.resize(\n            im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR\n        )\n        img = torch.from_numpy(im).permute(2, 0, 1)\n\n        im_info = {""width"": im_width, ""height"": im_height}\n\n        return img, im_scale, im_info\n\n    def _process_feature_extraction(\n        self, output, im_scales, im_infos, feature_name=""fc6"", conf_thresh=0\n    ):\n        batch_size = len(output[0][""proposals""])\n        n_boxes_per_image = [len(boxes) for boxes in output[0][""proposals""]]\n        score_list = output[0][""scores""].split(n_boxes_per_image)\n        score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n        feats = output[0][feature_name].split(n_boxes_per_image)\n        cur_device = score_list[0].device\n\n        feat_list = []\n        info_list = []\n\n        for i in range(batch_size):\n            dets = output[0][""proposals""][i].bbox / im_scales[i]\n            scores = score_list[i]\n            max_conf = torch.zeros(scores.shape[0]).to(cur_device)\n            conf_thresh_tensor = torch.full_like(max_conf, conf_thresh)\n            start_index = 1\n            # Column 0 of the scores matrix is for the background class\n            if self.args.background:\n                start_index = 0\n            for cls_ind in range(start_index, scores.shape[1]):\n                cls_scores = scores[:, cls_ind]\n                keep = nms(dets, cls_scores, 0.5)\n                max_conf[keep] = torch.where(\n                    # Better than max one till now and minimally greater\n                    # than conf_thresh\n                    (cls_scores[keep] > max_conf[keep])\n                    & (cls_scores[keep] > conf_thresh_tensor[keep]),\n                    cls_scores[keep],\n                    max_conf[keep],\n                )\n\n            sorted_scores, sorted_indices = torch.sort(max_conf, descending=True)\n            num_boxes = (sorted_scores[: self.args.num_features] != 0).sum()\n            keep_boxes = sorted_indices[: self.args.num_features]\n            feat_list.append(feats[i][keep_boxes])\n            bbox = output[0][""proposals""][i][keep_boxes].bbox / im_scales[i]\n            # Predict the class label using the scores\n            objects = torch.argmax(scores[keep_boxes][:, start_index:], dim=1)\n\n            info_list.append(\n                {\n                    ""bbox"": bbox.cpu().numpy(),\n                    ""num_boxes"": num_boxes.item(),\n                    ""objects"": objects.cpu().numpy(),\n                    ""cls_prob"": scores[keep_boxes][:, start_index:].cpu().numpy(),\n                    ""image_width"": im_infos[i][""width""],\n                    ""image_height"": im_infos[i][""height""],\n                }\n            )\n\n        return feat_list, info_list\n\n    def get_detectron_features(self, image_paths):\n        img_tensor, im_scales, im_infos = [], [], []\n\n        for image_path in image_paths:\n            im, im_scale, im_info = self._image_transform(image_path)\n            img_tensor.append(im)\n            im_scales.append(im_scale)\n            im_infos.append(im_info)\n\n        # Image dimensions should be divisible by 32, to allow convolutions\n        # in detector to work\n        current_img_list = to_image_list(img_tensor, size_divisible=32)\n        current_img_list = current_img_list.to(""cuda"")\n\n        with torch.no_grad():\n            output = self.detection_model(current_img_list)\n\n        feat_list = self._process_feature_extraction(\n            output,\n            im_scales,\n            im_infos,\n            self.args.feature_name,\n            self.args.confidence_threshold,\n        )\n\n        return feat_list\n\n    def _chunks(self, array, chunk_size):\n        for i in range(0, len(array), chunk_size):\n            yield array[i : i + chunk_size]\n\n    def _save_feature(self, file_name, feature, info):\n        file_base_name = os.path.basename(file_name)\n        file_base_name = file_base_name.split(""."")[0]\n        info_file_base_name = file_base_name + ""_info.npy""\n        file_base_name = file_base_name + "".npy""\n\n        np.save(\n            os.path.join(self.args.output_folder, file_base_name), feature.cpu().numpy()\n        )\n        np.save(os.path.join(self.args.output_folder, info_file_base_name), info)\n\n    def extract_features(self):\n        image_dir = self.args.image_dir\n\n        if os.path.isfile(image_dir):\n            features, infos = self.get_detectron_features([image_dir])\n            self._save_feature(image_dir, features[0], infos[0])\n        else:\n            files = glob.glob(os.path.join(image_dir, ""*.png""))\n            files.extend(glob.glob(os.path.join(image_dir, ""*.jpg"")))\n            files.extend(glob.glob(os.path.join(image_dir, ""*.jpeg"")))\n            files = {f: 1 for f in files}\n            exclude = {}\n\n            if os.path.exists(self.args.exclude_list):\n                with open(self.args.exclude_list) as f:\n                    lines = f.readlines()\n                    for line in lines:\n                        exclude[\n                            line.strip(""\\n"").split(os.path.sep)[-1].split(""."")[0]\n                        ] = 1\n            output_files = glob.glob(os.path.join(self.args.output_folder, ""*.npy""))\n            output_dict = {}\n            for f in output_files:\n                file_name = f.split(os.path.sep)[-1].split(""."")[0]\n                output_dict[file_name] = 1\n\n            for f in list(files.keys()):\n                file_name = f.split(os.path.sep)[-1].split(""."")[0]\n                if file_name in output_dict or file_name in exclude:\n                    files.pop(f)\n\n            files = list(files.keys())\n\n            finished = 0\n\n            end_index = self.args.end_index\n            if end_index is None:\n                end_index = len(files)\n            start_index = self.args.start_index\n\n            total = len(files[start_index:end_index])\n\n            for chunk in self._chunks(\n                files[start_index:end_index], self.args.batch_size\n            ):\n                features, infos = self.get_detectron_features(chunk)\n                for idx, file_name in enumerate(chunk):\n                    self._save_feature(file_name, features[idx], infos[idx])\n                finished += len(chunk)\n\n                if finished % 200 == 0:\n                    print(f""Processed {finished}/{total}"")\n\n\nif __name__ == ""__main__"":\n    feature_extractor = FeatureExtractor()\n    feature_extractor.extract_features()\n'"
tools/scripts/features/extract_resnet152_feat.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport argparse\nimport os\nfrom glob import glob\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.autograd import Variable\n\nTARGET_IMAGE_SIZE = [448, 448]\nCHANNEL_MEAN = [0.485, 0.456, 0.406]\nCHANNEL_STD = [0.229, 0.224, 0.225]\ndata_transforms = transforms.Compose(\n    [\n        transforms.Resize(TARGET_IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(CHANNEL_MEAN, CHANNEL_STD),\n    ]\n)\n\nuse_cuda = torch.cuda.is_available()\n\n# NOTE feat path ""https://download.pytorch.org/models/resnet152-b121ed2d.pth""\nRESNET152_MODEL = models.resnet152(pretrained=True)\nRESNET152_MODEL.eval()\n\nif use_cuda:\n    RESNET152_MODEL = RESNET152_MODEL.cuda()\n\n\nclass ResNet152FeatModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        modules = list(RESNET152_MODEL.children())[:-2]\n        self.feature_module = nn.Sequential(*modules)\n\n    def forward(self, x):\n        return self.feature_module(x)\n\n\n_resnet_module = ResNet152FeatModule()\nif use_cuda:\n    _resnet_module = _resnet_module.cuda()\n\n\ndef extract_image_feat(img_file):\n    img = Image.open(img_file).convert(""RGB"")\n    img_transform = data_transforms(img)\n    # make sure grey scale image is processed correctly\n    if img_transform.shape[0] == 1:\n        img_transform = img_transform.expand(3, -1, -1)\n    img_var = Variable(img_transform.unsqueeze(0))\n    if use_cuda:\n        img_var = img_var.cuda()\n\n    img_feat = _resnet_module(img_var)\n    return img_feat\n\n\ndef get_image_id(image_name):\n    image_id = int(image_name.split(""."")[0].split(""_"")[-1])\n    return image_id\n\n\ndef extract_dataset_pool5(image_dir, save_dir, total_group, group_id, ext_filter):\n    image_list = glob(image_dir + ""/*."" + ext_filter)\n    image_list = {f: 1 for f in image_list}\n    exclude = {}\n    with open(""./list"") as f:\n        lines = f.readlines()\n        for line in lines:\n            exclude[line.strip(""\\n"").split(os.path.sep)[-1].split(""."")[0]] = 1\n    output_files = glob(os.path.join(save_dir, ""*.npy""))\n    output_dict = {}\n    for f in output_files:\n        file_name = f.split(os.path.sep)[-1].split(""."")[0]\n        output_dict[file_name] = 1\n\n    for f in list(image_list.keys()):\n        file_name = f.split(os.path.sep)[-1].split(""."")[0]\n        if file_name in output_dict or file_name in exclude:\n            image_list.pop(f)\n\n    image_list = list(image_list.keys())\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for n_im, impath in enumerate(image_list):\n        if (n_im + 1) % 100 == 0:\n            print(""processing %d / %d"" % (n_im + 1, len(image_list)))\n        image_name = os.path.basename(impath)\n        image_id = get_image_id(image_name)\n        if image_id % total_group != group_id:\n            continue\n\n        feat_name = image_name.replace(ext_filter, ""npy"")\n        save_path = os.path.join(save_dir, feat_name)\n        tmp_lock = save_path + "".lock""\n\n        if os.path.exists(save_path) and not os.path.exists(tmp_lock):\n            continue\n        if not os.path.exists(tmp_lock):\n            os.makedirs(tmp_lock)\n\n        # pool5_val = extract_image_feat(impath).permute(0, 2, 3, 1)\n        try:\n            pool5_val = extract_image_feat(impath).permute(0, 2, 3, 1)\n        except Exception:\n            print(""error for"" + image_name)\n            continue\n\n        feat = pool5_val.data.cpu().numpy()\n        np.save(save_path, feat)\n        os.rmdir(tmp_lock)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--total_group"", type=int, default=1)\n    parser.add_argument(""--group_id"", type=int, default=0)\n    parser.add_argument(""--data_dir"", type=str, required=True)\n    parser.add_argument(""--out_dir"", type=str, required=True)\n    parser.add_argument(""--image_ext"", type=str, default=""jpg"")\n\n    args = parser.parse_args()\n\n    extract_dataset_pool5(\n        args.data_dir, args.out_dir, args.total_group, args.group_id, args.image_ext\n    )\n'"
tools/scripts/features/lmdb_conversion.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport argparse\nimport glob\nimport os\nimport pickle\n\nimport lmdb\nimport numpy as np\nimport tqdm\n\n\nclass LMDBConversion:\n    def __init__(self):\n        self.args = self.get_parser().parse_args()\n\n    def get_parser(self):\n        parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n\n        parser.add_argument(\n            ""--mode"",\n            required=True,\n            type=str,\n            help=""Mode can either be `convert` (for conversion of \\n""\n            + ""features to an LMDB file) or `extract` (extract \\n""\n            + ""raw features from a LMDB file)"",\n        )\n        parser.add_argument(\n            ""--lmdb_path"", required=True, type=str, help=""LMDB file path""\n        )\n        parser.add_argument(\n            ""--features_folder"", required=True, type=str, help=""Features folder""\n        )\n        return parser\n\n    def convert(self):\n        env = lmdb.open(self.args.lmdb_path, map_size=1099511627776)\n        id_list = []\n        all_features = glob.glob(\n            os.path.join(self.args.features_folder, ""**"", ""*.npy""), recursive=True\n        )\n\n        features = []\n        for feature in all_features:\n            if not feature.endswith(""_info.npy""):\n                features.append(feature)\n\n        with env.begin(write=True) as txn:\n            for infile in tqdm.tqdm(features):\n                reader = np.load(infile, allow_pickle=True)\n                item = {}\n                split = os.path.relpath(infile, self.args.features_folder).split(\n                    "".npy""\n                )[0]\n                item[""feature_path""] = split\n                key = split.encode()\n                id_list.append(key)\n                item[""features""] = reader\n                info_file = infile.split("".npy"")[0] + ""_info.npy""\n                if not os.path.isfile(info_file):\n                    txn.put(key, pickle.dumps(item))\n                    continue\n\n                reader = np.load(info_file, allow_pickle=True)\n                item[""image_height""] = reader.item().get(""image_height"")\n                item[""image_width""] = reader.item().get(""image_width"")\n                item[""num_boxes""] = reader.item().get(""num_boxes"")\n                item[""objects""] = reader.item().get(""objects"")\n                item[""cls_prob""] = reader.item().get(""cls_prob"", None)\n                item[""bbox""] = reader.item().get(""bbox"")\n\n                txn.put(key, pickle.dumps(item))\n\n            txn.put(b""keys"", pickle.dumps(id_list))\n\n    def extract(self):\n        os.makedirs(self.args.features_folder, exist_ok=True)\n        env = lmdb.open(\n            self.args.lmdb_path,\n            max_readers=1,\n            readonly=True,\n            lock=False,\n            readahead=False,\n            meminit=False,\n        )\n        with env.begin(write=False) as txn:\n            _image_ids = pickle.loads(txn.get(b""keys""))\n            for img_id in tqdm.tqdm(_image_ids):\n                item = pickle.loads(txn.get(img_id))\n                img_id = img_id.decode(""utf-8"")\n                tmp_dict = {}\n                tmp_dict[""image_id""] = img_id\n                tmp_dict[""bbox""] = item[""bbox""]\n                tmp_dict[""num_boxes""] = item[""num_boxes""]\n                tmp_dict[""image_height""] = item[""image_width""]\n                tmp_dict[""image_width""] = item[""image_width""]\n                tmp_dict[""objects""] = item[""objects""]\n                tmp_dict[""cls_prob""] = item[""cls_prob""]\n\n                info_file_base_name = str(img_id) + ""_info.npy""\n                file_base_name = str(img_id) + "".npy""\n\n                np.save(\n                    os.path.join(self.args.features_folder, file_base_name),\n                    item[""features""],\n                )\n                np.save(\n                    os.path.join(self.args.features_folder, info_file_base_name),\n                    tmp_dict,\n                )\n\n    def execute(self):\n        if self.args.mode == ""convert"":\n            self.convert()\n        elif self.args.mode == ""extract"":\n            self.extract()\n        else:\n            raise ValueError(""mode must be either `convert` or `extract` "")\n\n\nif __name__ == ""__main__"":\n    lmdb_converter = LMDBConversion()\n    lmdb_converter.execute()\n'"
tools/scripts/gqa/convert_gqa_to_vqa.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport argparse\nimport json\nimport os\nimport re\nfrom collections import Counter\n\nimport h5py\nimport numpy as np\nimport tqdm\n\nfrom mmf.utils.process_answers import preprocess_answer\nfrom mmf.utils.text_processing import text_tokenize as tokenize\n\n\ndef merge_train(train_q_dir):\n    merged_dic = {}\n\n    for file_name in tqdm.tqdm(os.listdir(train_q_dir)):\n        full_path = os.path.join(train_q_dir, file_name)\n        partial_questions = json.load(open(full_path))\n        merged_dic.update(partial_questions)\n\n    save_dir = os.path.abspath(os.path.join(train_q_dir, os.pardir))\n\n    with open(os.path.join(save_dir, ""train_all_questions.json""), ""w"") as fp:\n        json.dump(merged_dic, fp)\n\n\ndef get_objects(semantic_str):\n    matches = re.findall(r""\\(([^)]+)"", semantic_str)\n    result = []\n    for match in matches:\n        if "","" in match:\n            result += list(map(int, match.split("","")))\n        elif match.isdigit():\n            result += [int(match)]\n        else:\n            pass\n    return result\n\n\ndef get_imdb(file_path):\n\n    imdb = [{""dataset_name"": ""gqa""}]\n\n    questions = json.load(open(file_path))\n    print(f""Processing file {file_path}"")\n\n    for qid, item in tqdm.tqdm(questions.items()):\n        entry = {\n            ""image_name"": item[""imageId""] + ""jpg"",\n            ""image_id"": item[""imageId""],\n            ""question_id"": qid,\n            ""question_str"": item[""question""],\n            ""question_tokens"": tokenize(item[""question""]),\n        }\n\n        if ""answer"" in item:\n            entry[""all_answers""] = [item[""answer""] for _ in range(10)]\n            entry[""valid_answers""] = [item[""answer""] for _ in range(10)]\n            entry[""semantic_string""] = (item[""semanticStr""],)\n            entry[""gt_object_ids""] = (get_objects(item[""semanticStr""]),)\n            entry[""meta_data""] = item[""types""]\n\n        imdb.append(entry)\n\n    return np.array(imdb)\n\n\ndef extract_bbox_feats(feat_dir, out_dir):\n\n    info_json_path = os.path.join(feat_dir, ""gqa_objects_info.json"")\n    info_dict = json.load(open(info_json_path))\n\n    file_mapping = {k: [] for k in range(16)}\n\n    for k, v in info_dict.items():\n        file_mapping[v[""file""]] += [(k, v)]\n\n    for i in range(16):\n        file_path = os.path.join(feat_dir, f""gqa_objects_{i}.h5"")\n        print(f""Processing file {file_path}"")\n\n        feat_db = h5py.File(file_path, ""r"")\n        for entry in tqdm.tqdm(file_mapping[i]):\n            image_id = entry[0]\n            meta = entry[1]\n            to_save = {\n                ""image_id"": image_id,\n                ""boxes"": feat_db[""bboxes""][meta[""idx""]],\n                ""feats"": feat_db[""features""][meta[""idx""]],\n                ""height"": meta[""height""],\n                ""width"": meta[""width""],\n                ""n_objects"": meta[""objectsNum""],\n            }\n\n            save_path = os.path.join(out_dir, str(image_id) + "".npy"")\n            np.save(save_path, to_save)\n\n\ndef extract_spatial_feats(feat_dir, out_dir):\n    info_json_path = os.path.join(feat_dir, ""gqa_spatial_info.json"")\n    info_dict = json.load(open(info_json_path))\n\n    file_mapping = {k: [] for k in range(16)}\n\n    for k, v in info_dict.items():\n        file_mapping[v[""file""]] += [(k, v)]\n\n    for i in range(16):\n        file_path = os.path.join(feat_dir, f""gqa_spatial_{i}.h5"")\n        print(f""Processing file {file_path}"")\n\n        feat_db = h5py.File(file_path, ""r"")\n        for entry in tqdm.tqdm(file_mapping[i]):\n            image_id = entry[0]\n            meta = entry[1]\n            to_save = feat_db[""features""][meta[""idx""]]\n            to_save = to_save.reshape(1, 7, 7, 2048)\n            save_path = os.path.join(out_dir, str(image_id) + "".npy"")\n            np.save(save_path, to_save)\n\n\ndef extract_image_features(image_dir, out_dir):\n    extract_bbox_feats(\n        os.path.join(image_dir, ""objects""), os.path.join(out_dir, ""objects"")\n    )\n\n    extract_spatial_feats(\n        os.path.join(image_dir, ""spatial""), os.path.join(out_dir, ""spatial"")\n    )\n\n\ndef convert_gqa_to_vqa(gqa_dir, out_dir):\n    """"""\n    Takes GQA dataset and converts it into VQA format\n\n    Assumes GQA dir structure as:\n\n    -gqa_dir/\n      -images/\n         -images/\n         -objects/\n         -spatial/\n      -questions/\n      -scenegraphs/\n    """"""\n\n    image_feat_path = os.path.join(gqa_dir, ""images"")\n    extract_image_features(image_feat_path, out_dir)\n\n    questions_dir = os.path.join(gqa_dir, ""questions"")\n\n    if os.path.isfile(os.path.join(questions_dir, ""train_all_questions.json"")):\n        print(""Using previously generated train_all_questions.json file"")\n    else:\n        merge_train(os.path.join(gqa_dir, ""questions"", ""train_all_questions""))\n\n    split_mapping = {\n        ""test"": ""test_all_questions.json"",\n        ""val"": ""val_all_questions.json"",\n        ""challenge"": ""challenge_all_questions.json"",\n        ""train"": ""train_all_questions.json"",\n    }\n\n    for split in split_mapping:\n        for balance_type in [""balanced"", ""all""]:\n            filename = split_mapping[split]\n            csplit = split\n            if balance_type == ""balanced"":\n                filename = filename.replace(""_all"", ""_balanced"")\n                csplit = split + ""_balanced""\n\n            file_path = os.path.join(questions_dir, filename)\n            imdb = get_imdb(file_path)\n\n            save_path = os.path.join(out_dir, f""imdb_{csplit}.npy"")\n            np.save(save_path, imdb)\n\n    splits = [""val"", ""train""]\n    split_type = [""balanced"", ""all""]\n\n    global_answer = Counter()\n    global_q = Counter()\n    question_len = Counter()\n\n    for s in splits:\n        for st in split_type:\n            questions_json = os.path.join(questions_dir, f""{s}_{st}_questions.json"")\n            questions = json.load(open(questions_json))\n\n            print(f""Processing split {s}_{st}"")\n\n            answers = Counter()\n            q_tokens = Counter()\n\n            for _, q in tqdm.tqdm(questions.items()):\n                tokens = tokenize(q[""question""])\n                q_tokens.update(tokens)\n                global_q.update(tokens)\n                answers.update([q[""answer""].lower()])\n                global_answer.update([q[""answer""].lower()])\n                question_len.update([len(tokens)])\n\n    print(""N_unique answers :"", len(global_answer))\n    print(""N unique q tokens:"", len(global_q))\n    print(""Min Q length"", min([x for x in question_len]))\n    print(""Max Q length"", max([x for x in question_len]))\n    print(""Q length distribution"", question_len)\n\n    # Save question vocabulary\n    q_vocabulary = [w[0] for w in global_q.items()]\n    q_vocabulary.sort()\n    q_vocabulary = [""<unk>""] + q_vocabulary\n\n    vocab_file = os.path.join(out_dir, ""vocabulary_gqa.txt"")\n    with open(vocab_file, ""w"") as f:\n        f.writelines([w + ""\\n"" for w in q_vocabulary])\n\n    # Save answer vocabulary\n    answer_list = [preprocess_answer(ans[0]) for ans in global_answer.items()]\n    answer_list = [t.strip() for t in answer_list if len(t.strip()) > 0]\n    answer_list.sort()\n\n    if ""<unk>"" not in answer_list:\n        answer_list = [""<unk>""] + answer_list\n\n    answer_file = os.path.join(out_dir, ""answers_gqa.txt"")\n    with open(answer_file, ""w"") as fp:\n        fp.writelines([w + ""\\n"" for w in answer_list])\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--gqa_dir"", default=None)\n    parser.add_argument(""--out_dir"", default=None)\n    args = parser.parse_args()\n    convert_gqa_to_vqa(args.gqa_dir, args.out_dir)\n'"
tools/scripts/gqa/extract_vocabulary.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n\nimport argparse\nimport json\nimport os\nfrom collections import Counter\n\nfrom mmf.utils.text import tokenize\n\n\nclass ExtractVocabulary:\n    def __init__(self):\n        self.args = self.get_args()\n        self.input_files = self.args.input_files\n        self.out_dir = self.args.out_dir\n        self.min_freq = self.args.min_freq\n        self.vocab_file_name = self.args.vocab_file_name\n\n    def extract(self):\n        os.makedirs(self.out_dir, exist_ok=True)\n\n        word_count = Counter()\n\n        texts = self.get_text()\n        text_lengths = [None] * len(texts)\n\n        for inx, text in enumerate(texts):\n            words = tokenize(text)\n            text_lengths[inx] = len(words)\n            word_count.update(words)\n\n        # UNK token will added on fly if you use Vocab class in core/text\n        vocabulary = [w[0] for w in word_count.items() if w[1] >= self.min_freq]\n        vocabulary.sort()\n\n        self.save_vocabulary(vocabulary)\n\n        print(""min text len="", min(text_lengths))\n        print(""max text len="", max(text_lengths))\n\n    def save_vocabulary(self, vocabulary):\n        vocab_file = os.path.join(self.out_dir, self.vocab_file_name)\n        with open(vocab_file, ""w"") as f:\n            f.writelines([w + ""\\n"" for w in vocabulary])\n\n    def get_text(self):\n        """"""\n        Override this in your child class to extract custom text\n        Default for VQA. Make sure to return a list of all possible text\n        """"""\n        text = []\n\n        for input_file in self.input_files:\n            with open(input_file) as f:\n                text += json.load(f)[""questions""]\n\n        return text\n\n    def get_args(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            ""--input_files"",\n            nargs=""+"",\n            required=True,\n            help=""input question json files, \\\n                                 if more than 1, split by space"",\n        )\n        parser.add_argument(\n            ""--out_dir"",\n            type=str,\n            default=""./"",\n            help=""output directory, default is current directory"",\n        )\n        parser.add_argument(\n            ""--min_freq"",\n            type=int,\n            default=0,\n            help=""the minimum times of word occurrence \\\n                                  to be included in vocabulary, default 0"",\n        )\n        parser.add_argument(\n            ""--vocab_file_name"",\n            type=str,\n            default=""vocabulary.txt"",\n            help=""Name of the file in vocabulary will be stored"",\n        )\n\n        args = parser.parse_args()\n\n        return args\n\n\nif __name__ == ""__main__"":\n    extractor = ExtractVocabulary()\n    extractor.extract()\n'"
tools/scripts/tests/generate_test_data.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n""""""\nFile can be used for generating test data\n\nTakes in the DB file, features folder, image folder and will generate a test data\nfolder for a certain amount of samples in the following folder\n\noutput_folder/\n    images/\n        a.jpg\n        b.jpg\n        ...\n    features/\n        features.lmdb/\n            data.mdb\n            lock.mdb\n        raw/\n            a.npy\n            a_info.npy\n            b.npy\n            b_info.npy\n            ...\n    db/\n        train.jsonl\n        dev.jsonl\n        test.jsonl\n""""""\n# Copyright (c) 2017-present, Facebook, Inc.\n\nimport argparse\nimport json\nimport os\nimport shutil\n\nimport numpy as np\n\nfrom tools.scripts.features.lmdb_conversion import LMDBConversion\n\n\nclass TestDataBuilder:\n    def __init__(self):\n        parser = self.get_parser()\n        self.args = parser.parse_args()\n\n    def get_parser(self):\n        parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n\n        parser.add_argument(\n            ""--train_db_file"",\n            required=True,\n            type=str,\n            help=""DB file that will be used for generating the test data for training."",\n        )\n        parser.add_argument(\n            ""--dev_db_file"",\n            required=True,\n            type=str,\n            help=""DB file that will be used for generating the test data for ""\n            + ""validation."",\n        )\n        parser.add_argument(\n            ""--num_samples"",\n            type=int,\n            default=100,\n            help=""Number of samples to be extracted from the db file."",\n        )\n        parser.add_argument(\n            ""--train_images_folder"",\n            type=str,\n            default=None,\n            help=""Images folder for training set"",\n        )\n        parser.add_argument(\n            ""--dev_images_folder"",\n            type=str,\n            default=None,\n            help=""Images folder for dev set"",\n        )\n        parser.add_argument(\n            ""--train_features_folder"",\n            required=True,\n            type=str,\n            help=""Features folder."",\n            default=None,\n        )\n        parser.add_argument(\n            ""--dev_features_folder"",\n            required=True,\n            type=str,\n            help=""Features folder."",\n            default=None,\n        )\n        parser.add_argument(\n            ""--output_folder"", required=True, type=str, help=""Output folder.""\n        )\n        return parser\n\n    def build(self):\n        self.generate_and_save_data(\n            self.args.train_db_file,\n            self.args.train_images_folder,\n            self.args.train_features_folder,\n            ""train"",\n            self.args.num_samples,\n            self.args.output_folder,\n        )\n        self.generate_and_save_data(\n            self.args.dev_db_file,\n            self.args.dev_images_folder,\n            self.args.dev_features_folder,\n            ""dev"",\n            # Number of dev and test samples, we generate 1/10th of the training data\n            self.args.num_samples // 10,\n            self.args.output_folder,\n        )\n        # Test data is generated from dev data\n        self.generate_and_save_data(\n            self.args.dev_db_file,\n            self.args.dev_images_folder,\n            self.args.dev_features_folder,\n            ""test"",\n            self.args.num_samples // 10,\n            self.args.output_folder,\n        )\n\n    def generate_and_save_data(\n        self, db_file, image_folder, features_folder, name, num_samples, output_folder\n    ):\n        """"""This will generate features, db and images folder in proper format\n        and save them to output folder\n\n        Args:\n            db_file (str): Path to DB file from which samples will be generated\n            image_folder (str): Folder where images are present\n            features_folder (str): Folder where raw features are present\n            name (str): Type of the dataset set\n            num_samples (int): Number of objects to be sampled\n            output_folder (str): Path where output files will be stored\n        """"""\n        data, _ = self._load_db_file(db_file, num_samples)\n        assert len(data) == num_samples and num_samples > 0\n        image_paths = self._get_image_paths(data, image_folder)\n        feature_paths = self._get_feature_paths(data, features_folder)\n\n        image_output_folder = os.path.join(output_folder, ""images/"")\n        os.makedirs(image_output_folder, exist_ok=True)\n\n        for path in image_paths:\n            shutil.copy(path, image_output_folder)\n\n        features_output_folder = os.path.join(output_folder, ""features"", ""raw/"")\n        os.makedirs(features_output_folder, exist_ok=True)\n\n        for path in feature_paths:\n            shutil.copy(path, features_output_folder)\n\n        db_output_folder = os.path.join(output_folder, ""db/"")\n        os.makedirs(db_output_folder, exist_ok=True)\n\n        output = []\n        for d in data:\n            output.append(json.dumps(d))\n        output = ""\\n"".join(output)\n\n        with open(os.path.join(db_output_folder, f""{name}.jsonl""), ""w"") as f:\n            f.write(output)\n\n        lmdb_folder = os.path.join(output_folder, ""features"")\n        LMDBConversion.get_parser = mock_lmdb_parser(\n            features_output_folder, lmdb_folder\n        )\n        lmdb_converter = LMDBConversion()\n        lmdb_converter.execute()\n\n    def _load_db_file(self, db_file: str, num_samples: int):\n        """"""Load db file based on the format and return back a randomly sampled\n        list of \'num_samples\' objects from DB.\n\n        Args:\n            db_file (str): Path to DB file\n            num_samples (int): Number of samples that will be generated\n\n        Raises:\n            ValueError: Raised if DB file is not among "".json|.jsonl|.npy""\n\n        Returns:\n            Tupe(List[Object], str): A tuple containing both the selected data and\n            actual file path\n        """"""\n        file_type = None\n        if db_file.endswith("".npy""):\n            file_type = ""npy""\n            data = np.load(db_file, allow_pickle=True)\n            selected_data = np.random.choice(data[1:], size=num_samples, replace=False)\n        elif db_file.endswith("".jsonl""):\n            file_type = ""jsonl""\n            with open(db_file) as f:\n                data = []\n                for item in f.readlines():\n                    data.append(json.loads(item.strip(""\\n"")))\n                selected_data = np.random.choice(data, size=num_samples, replace=False)\n\n        # Expecting JSON to be in COCOJSONFormat or contain ""data"" attribute\n        elif db_file.endswith("".json""):\n            file_type = ""json""\n            with open(db_file) as f:\n                data = json.load(f)\n                selected_data = np.random.choice(data, size=num_samples, replace=False)\n        else:\n            raise ValueError(""Unexpected DB file type. Valid options {json|jsonl|npy}"")\n\n        return selected_data, file_type\n\n    def _get_image_paths(self, data, image_folder):\n        if image_folder is None:\n            return []\n\n        images = set()\n        for item in data:\n            possible_images = self._get_attrs(item)\n            for image in possible_images:\n                images.add(os.path.join(image_folder, image))\n\n        return images\n\n    def _get_feature_paths(self, data, feature_folder):\n        if feature_folder is None:\n            return []\n\n        features = set()\n        for item in data:\n            possible_images = self._get_attrs(item)\n            for image in possible_images:\n                image = ""."".join(image.split(""."")[:-1])\n                feature = image + "".npy""\n                info = image + ""_info.npy""\n                features.add(os.path.join(feature_folder, feature))\n                features.add(os.path.join(feature_folder, info))\n\n        return features\n\n    def _get_attrs(self, item):\n        """"""Returns possible attribute that can point to image id\n\n        Args:\n            item (Object): Object from the DB\n\n        Returns:\n            List[str]: List of possible images that will be copied later\n        """"""\n        image = None\n        pick = None\n        attrs = self._get_possible_attrs()\n\n        for attr in attrs:\n            image = item.get(attr, None)\n            if image is not None:\n                pick = attr\n                break\n\n        if pick == ""identifier"":\n            return [image + ""-img0.jpg"", image + ""-img1.jpg""]\n        elif pick == ""image_name"" or pick == ""image_id"":\n            return [image + "".jpg""]\n        else:\n            return [image]\n\n    def _get_possible_attrs(self):\n        return [\n            ""Flickr30kID"",\n            ""Flikr30kID"",\n            ""identifier"",\n            ""image_path"",\n            ""image_name"",\n            ""img"",\n            ""image_id"",\n        ]\n\n\ndef mock_lmdb_parser(features_folder, output_folder):\n    args = argparse.Namespace()\n    args.mode = ""convert""\n    args.features_folder = features_folder\n    args.lmdb_path = os.path.join(output_folder, ""features.lmdb"")\n\n    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n    parser.parse_args = lambda: args\n    return lambda _: parser\n\n\nif __name__ == ""__main__"":\n    builder = TestDataBuilder()\n    builder.build()\n'"
tools/sweeps/lib/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n# Copied from fairseq. Mostly written by @myleott. Adapted accordingly for mmf\nimport argparse\nimport datetime\nimport os\nimport socket\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""Script for launching hyperparameter sweeps"")\n    parser.add_argument(\n        ""-p"",\n        ""--prefix"",\n        required=True,\n        help=""save checkpoints and logs in <checkpoints-dir>/<prefix>.<save_dir_key>"",\n    )\n    parser.add_argument(\n        ""-t"",\n        ""--num_trials"",\n        required=True,\n        type=int,\n        help=""number of random hyperparam configurations to try (-1 for grid search)"",\n    )\n    parser.add_argument(\n        ""-g"", ""--num_gpus"", type=int, required=True, help=""number of GPUs per node""\n    )\n    parser.add_argument(\n        ""-n"",\n        ""--num_nodes"",\n        type=int,\n        default=1,\n        help=""number of nodes for distributed training"",\n    )\n    parser.add_argument(""--seed"", type=int, default=1234)\n    parser.add_argument(\n        ""--config"", type=str, default=None, help=""configuration for model""\n    )\n    parser.add_argument(\n        ""--baseline_model"", help=""path to baseline model from which to resume training""\n    )\n    parser.add_argument(\n        ""--force_checkpoints_dir"", help=""force using a given checkpoint dir""\n    )\n    parser.add_argument(\n        ""--resume_failed"",\n        action=""store_true"",\n        help=""resume any runs that failed (assumes --num_trials and --seed""\n        + "" are the same)"",\n    )\n    parser.add_argument(\n        ""--resume_finished"",\n        action=""store_true"",\n        help=""force any runs that finished to begin again (uncommon)"",\n    )\n    parser.add_argument(\n        ""--dry_run"",\n        action=""store_true"",\n        help=""output only a list of actions to perform without performing them"",\n    )\n    parser.add_argument(""--local"", action=""store_true"", help=""run job locally"")\n    parser.add_argument(""--debug"", action=""store_true"", help=""debug"")\n\n    hostname = socket.gethostname()\n    if ""fair"" in hostname:\n        default_backend = ""slurm""\n        parser.add_argument(\n            ""--checkpoints_dir"",\n            default=os.path.join(\n                ""/checkpoint"", os.environ[""USER""], str(datetime.date.today())\n            ),\n            help=""save checkpoints and logs in ""\n            + ""<checkpoints-dir>/<prefix>.<save_dir_key>"",\n        )\n    else:\n        default_backend = ""fblearner""\n        parser.add_argument(\n            ""--checkpoints-dir"",\n            default=os.path.join(\n                ""/mnt/vol/gfsai-east/ai-group/users"",\n                os.environ[""USER""],\n                ""checkpoints"",\n                str(datetime.date.today()),\n            ),\n            help=""save checkpoints and logs in ""\n            + ""<checkpoints-dir>/<prefix>.<save_dir_key>"",\n        )\n\n    parser.add_argument(\n        ""--backend"", choices=[""slurm"", ""fblearner""], default=default_backend\n    )\n\n    # FBLearner params\n    parser.add_argument(\n        ""--entitlement"", help=""entitlement to use"", default=""bigbasin_atn_fair""\n    )\n    parser.add_argument(\n        ""--run-as-secure-group"",\n        help=""secure group to use"",\n        default=""fair_research_and_engineering"",\n    )\n    parser.add_argument(\n        ""--torch_home"",\n        help=""torch cache path"",\n        default=""/mnt/vol/gfsfblearner-oregon/users/vedanuj/torch/"",\n    )\n\n    # Slurm params\n    parser.add_argument(\n        ""--salloc"", action=""store_true"", help=""run agaist current allocation""\n    )\n    parser.add_argument(""--partition"", help=""partition to run on"", default=""learnfair"")\n    parser.add_argument(""--reservation"", help=""reservation to run on"")\n    parser.add_argument(\n        ""--exclusive"", action=""store_true"", help=""if set, get exclusive host""\n    )\n    parser.add_argument(\n        ""--dep"",\n        metavar=""JOBID"",\n        type=int,\n        help=""add JOBID as a dependency (i.e., wait for it to finish)"",\n    )\n    parser.add_argument(\n        ""--sequential"", action=""store_true"", help=""schedule jobs to run sequentially""\n    )\n    parser.add_argument(\n        ""--time"", default=""4320"", help=""expected job duration in minutes""\n    )\n    parser.add_argument(""--mem"", ""--mem"", help=""memory to request"")\n    parser.add_argument(""--gpu-type"", default=""volta"")\n    parser.add_argument(\n        ""--constraint"",\n        metavar=""CONSTRAINT"",\n        help=""gpu constraint, if any. e.g. \'volta\'"",\n    )\n    parser.add_argument(""--comment"", help=""comment string"")\n    parser.add_argument(\n        ""--snapshot_code"",\n        action=""store_true"",\n        default=False,\n        help=""Flag for creating a snapshot of training code while creating slurm job,""\n        "" path is \'./slurm_snapshot_code/<TIME_ISO_FORMAT/>:\', ""\n        ""can find time from comment of slurm job."",\n    )\n    parser.add_argument(\n        ""--tensorboard_logdir"",\n        default=os.path.join(\n            ""/checkpoint"",\n            os.environ[""USER""],\n            ""tensorboard_logs"",\n            str(datetime.date.today()),\n        ),\n        help=""save tensorboard logs in <tensorboard-logdir>/<prefix>.<save_dir_key>"",\n    )\n    parser.add_argument(\n        ""--tensorboard"",\n        default=0,\n        type=int,\n        help=""enable tensorboard logging by passing --tensorboard 1"",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\nclass hyperparam:\n    """"""Base class for defining hyperparameters.""""""\n\n    def __init__(self, name, values=None, binary_flag=False, save_dir_key=None):\n        """"""\n        Arguments:\n        - name : the name of the hyperparameter (e.g., `--dropout`)\n        - values : the set of values to sweep over (e.g., `[0.0, 0.1, 0.2]`)\n        - binary_flag : whether the hyperparameter uses a boolean flag\n                        (e.g., `--no-save`)\n        - save_dir_key : function that takes the hyperparameter value and returns\n                        the ""key"" to be appended to the output directory name\n        """"""\n        self.name = name\n        if values is None:  # syntactic sugar for binary flags\n            self.values = [True]\n            self.binary_flag = True\n        else:\n            self.values = values if isinstance(values, list) else [values]\n            self.binary_flag = binary_flag\n        self.save_dir_key = save_dir_key\n        self.current_value = None\n\n        if len(self.values) > 1 and self.save_dir_key is None:\n            raise ValueError(\n                f""{name} has more than one value but is missing a save_dir_key!""\n            )\n\n    def get_cli_args(self):\n        if self.binary_flag:\n            return [self.name] if self.current_value else []\n        else:\n            return [self.name, self.current_value]\n\n    def get_save_dir_key(self):\n        if self.save_dir_key is None:\n            return None\n        if self.binary_flag:\n            return self.save_dir_key(1) if self.current_value else None\n        return self.save_dir_key(self.current_value)\n\n\ndef main(get_grid, postprocess_hyperparams):\n    args = get_args()\n\n    if args.backend == ""slurm"":\n        from .slurm import main as backend_main\n    elif args.backend == ""fblearner"":\n        from .fblearner import main as backend_main\n\n    backend_main(get_grid, postprocess_hyperparams, args)\n'"
tools/sweeps/lib/slurm.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n# Copied from fairseq. Mostly written by @myleott. Adapted accordingly for mmf\nimport datetime\nimport itertools\nimport os\nimport random\nimport shlex\nimport shutil\nimport subprocess\nfrom collections import OrderedDict\nfrom glob import glob\n\n\ndef main(get_grid, postprocess_hyperparams, args):\n    if args.local:\n        args.num_nodes = 1\n\n    # compute all possible hyperparameter configurations\n    grid = get_grid(args)\n    grid_product = list(itertools.product(*[hp.values for hp in grid]))\n\n    # randomly shuffle configurations\n    random.seed(args.seed)\n    random.shuffle(grid_product)\n\n    for i, hp_values in enumerate(grid_product):\n        config = OrderedDict()\n        for hp, value in zip(grid, hp_values):\n            config[hp.name] = hp\n            config[hp.name].current_value = value\n\n        # postprocess hyperparams\n        postprocess_hyperparams(args, config)\n\n        # launch training\n        job_id = launch_train(args, config)\n        if job_id is not None:\n            print(f""Launched {job_id}"")\n\n        if args.sequential and not args.local and job_id is not None:\n            args.dep = job_id\n\n        if i == args.num_trials - 1:\n            break\n\n\ndef copy_all_python_files(source, snapshot_main_dir, code_snapshot_hash):\n    """"""\n    Copies following files from source to destination:\n        a) all *.py files at direct source location.\n        b) all mmf/*.py recursively.\n    """"""\n    os.makedirs(snapshot_main_dir, exist_ok=True)\n    destination = os.path.join(snapshot_main_dir, code_snapshot_hash)\n    assert not os.path.exists(\n        destination\n    ), f""Code snapshot: {code_snapshot_hash} alredy exists""\n    os.makedirs(destination)\n    all_pys = (\n        glob(os.path.join(source, ""mmf/**/*.py""), recursive=True)\n        + glob(os.path.join(source, ""tools/**/*.py""), recursive=True)\n        + glob(os.path.join(source, ""*.py""))\n    )\n\n    for filepath in all_pys:\n        directory, filename = os.path.split(filepath)\n        if directory:\n            os.makedirs(os.path.join(destination, directory), exist_ok=True)\n        shutil.copy2(\n            os.path.join(source, filepath), os.path.join(destination, filepath)\n        )\n    return destination\n\n\ndef launch_train(args, config):\n    def dry_run(msg):\n        if args.dry_run:\n            print(f""| dry-run:  {msg}"")\n        return args.dry_run\n\n    destination = """"\n    if args.snapshot_code:\n        # Currently hash is just the current time in ISO format.\n        code_snapshot_hash = datetime.datetime.now().isoformat()\n        destination = copy_all_python_files(\n            ""."", ""slurm_snapshot_code"", code_snapshot_hash\n        )\n\n    # compute save_dir\n    save_dir_key = ""."".join(\n        filter(\n            lambda save_dir_key: save_dir_key is not None,\n            [hp.get_save_dir_key() for hp in config.values()],\n        )\n    )\n    save_dir_key = save_dir_key.replace("","", ""_"")\n    num_total_gpus = args.num_nodes * args.num_gpus\n    save_dir = os.path.join(\n        args.checkpoints_dir, f""{args.prefix}.{save_dir_key}.ngpu{num_total_gpus}""\n    )\n    tensorboard_logdir = os.path.join(\n        args.tensorboard_logdir, f""{args.prefix}.{save_dir_key}.ngpu{num_total_gpus}""\n    )\n\n    # create save directory if it doesn""t exist\n    if not os.path.exists(save_dir):\n        if not dry_run(f""create directory: {save_dir}""):\n            os.makedirs(save_dir)\n\n        # copy baseline model\n        checkpoint_last = os.path.join(save_dir, ""current.ckpt"")\n        if (\n            args.baseline_model\n            and not os.path.exists(checkpoint_last)\n            and not dry_run(f""initialize with baseline model: {args.baseline_model}"")\n        ):\n            if not os.path.exists(args.baseline_model):\n                raise FileNotFoundError(\n                    f""Cannot find baseline model: {args.baseline_model}""\n                )\n            shutil.copyfile(args.baseline_model, checkpoint_last)\n\n    # check for whether the run failed\n    if has_finished(save_dir):\n        if args.resume_finished:\n            dry_run(f""restart previously finished run: {save_dir}"")\n        else:\n            print(f""skip finished run (override with --resume-finished): {save_dir}"")\n            return\n    elif has_failed(save_dir):\n        if args.resume_failed:\n            dry_run(f""resume failed run: {save_dir}"")\n        else:\n            print(f""skip failed run (override with --resume-failed): {save_dir}"")\n            return\n    elif has_started(save_dir):\n        print(f""skip in progress run: {save_dir}"")\n        return\n\n    # generate train command\n    train_cmd = [""python"", ""-u"", os.path.join(destination, ""mmf_cli/run.py"")]\n    train_cmd.extend([""distributed.world_size"", str(args.num_nodes * args.num_gpus)])\n    if args.num_nodes > 1:\n        train_cmd.extend([""distributed.port"", str(get_random_port())])\n\n    if args.config is not None:\n        train_cmd.extend([""config"", args.config])\n    train_cmd.extend([""checkpoint.resume"", ""1""])\n    train_cmd.extend([""env.save_dir"", save_dir])\n    if args.tensorboard:\n        train_cmd.extend([""training.tensorboard"", ""1""])\n        train_cmd.extend([""env.tensorboard_logdir"", tensorboard_logdir])\n    for hp in config.values():\n        train_cmd.extend(map(str, hp.get_cli_args()))\n    if args.dry_run:\n        print(train_cmd)\n        train_cmd_str = "" "".join(train_cmd)\n        dry_run(f""train command: {train_cmd_str}"")\n\n    # start training\n    env = os.environ.copy()\n    env[""OMP_NUM_THREADS""] = ""2""\n    if args.local:\n        assert (\n            args.num_nodes == 1\n        ), ""distributed training cannot be combined with --local""\n        if not dry_run(""start training locally""):\n            if ""CUDA_VISIBLE_DEVICES"" not in env:\n                env[""CUDA_VISIBLE_DEVICES""] = "","".join(map(str, range(args.num_gpus)))\n            env[""NCCL_DEBUG""] = ""INFO""\n            train_proc = subprocess.Popen(train_cmd, env=env)\n            train_proc.wait()\n    else:\n        train_log = os.path.join(save_dir, ""train.log"")\n        train_stderr = os.path.join(save_dir, ""train.stderr.%j"")  # %j = slurm job id\n\n        # set environment\n        if args.num_nodes > 1:\n            env[""NCCL_SOCKET_IFNAME""] = ""^docker0,lo""\n            env[""NCCL_DEBUG""] = ""INFO""\n\n        srun_cmd = [\n            ""srun"",\n            ""--job-name"",\n            f""{args.prefix}.{save_dir_key}"",\n            ""--output"",\n            train_log,\n            ""--error"",\n            train_stderr,\n            ""--open-mode"",\n            ""append"",\n            ""--unbuffered"",\n        ]\n        if args.salloc:\n            srun_cmd += [\n                ""--nodes"",\n                str(args.num_nodes),\n                ""--ntasks"",\n                str(args.num_nodes),\n            ]\n        srun_cmd += train_cmd\n        srun_cmd_str = "" "".join(map(shlex.quote, srun_cmd)) + "" &""\n\n        # build command\n        if not args.salloc:\n            excluded_hosts = os.environ.get(""EXCLUDED_HOSTS"", None)\n            included_hosts = os.environ.get(""INCLUDED_HOSTS"", None)\n            gres = f""gpu:{args.gpu_type}:{args.num_gpus}""\n            sbatch_cmd = [\n                ""sbatch"",\n                ""--job-name"",\n                f""{args.prefix}.{save_dir_key}"",\n                ""--gres"",\n                gres,\n                ""--nodes"",\n                str(args.num_nodes),\n                ""--ntasks-per-node"",\n                ""1"",\n                ""--cpus-per-task"",\n                str(int(8 * args.num_gpus)),\n                ""--output"",\n                train_log,\n                ""--error"",\n                train_stderr,\n                ""--open-mode"",\n                ""append"",\n                # ""--no-requeue"",\n                ""--signal"",\n                ""B:USR1@180"",\n            ]\n            if args.constraint:\n                sbatch_cmd += [""-C"", args.constraint]\n\n            if args.partition:\n                sbatch_cmd += [""--partition"", args.partition]\n            if args.reservation:\n                sbatch_cmd += [""--reservation"", args.reservation]\n            if args.exclusive:\n                sbatch_cmd += [""--exclusive""]\n            if args.comment:\n                comment = args.comment\n                if args.snapshot_code:\n                    comment += f"", Code Location: {destination}""\n                sbatch_cmd += [""--comment"", comment]\n\n            if args.snapshot_code:\n                sbatch_cmd += [""--comment"", f""Code Location: {destination}""]\n\n            if args.dep is not None:\n                sbatch_cmd.extend([""-d"", str(args.dep)])\n            if args.time is not None:\n                sbatch_cmd.extend([""--time"", args.time])\n            if args.mem is not None:\n                sbatch_cmd += [""--mem"", args.mem]\n            else:\n                sbatch_cmd += [""--mem-per-cpu"", ""7G""]\n            sbatch_cmd += [""-x"", excluded_hosts] if excluded_hosts is not None else []\n            sbatch_cmd += [""-w"", included_hosts] if included_hosts is not None else []\n\n            wrapped_cmd = (\n                requeue_support()\n                + ""\\n""\n                + srun_cmd_str\n                + "" \\n wait $! \\n sleep 610 & \\n wait $!""\n            )\n\n            sbatch_cmd += [""--wrap"", wrapped_cmd]\n            sbatch_cmd_str = "" "".join(map(shlex.quote, sbatch_cmd))\n        else:\n            sbatch_cmd = srun_cmd\n            sbatch_cmd_str = srun_cmd_str\n\n        if args.dry_run:\n            dry_run(""start remote training"")\n            dry_run(f""- log stdout to: {train_log}"")\n            dry_run(f""- log stderr to: {train_stderr}"")\n            dry_run(f""- run command: {sbatch_cmd_str}"")\n            sbatch_cmd += [""--test-only""]\n            with subprocess.Popen(\n                sbatch_cmd, stdout=subprocess.PIPE, env=env\n            ) as train_proc:\n                stdout = train_proc.stdout.read().decode(""utf-8"")\n                print(stdout)\n        else:\n            with open(train_log, ""a"") as train_log_h:\n                # log most recent git commit\n                git_commit = subprocess.check_output(\n                    ""git log | head -n 1"", shell=True, encoding=""utf-8""\n                )\n                print(git_commit.rstrip(), file=train_log_h)\n                if args.baseline_model:\n                    print(f""baseline model: {args.baseline_model}"", file=train_log_h)\n            with open(train_log, ""a"") as train_log_h:\n                print(f""running command: {sbatch_cmd_str}\\n"")\n                print(f""running command: {sbatch_cmd_str}\\n"", file=train_log_h)\n                with subprocess.Popen(\n                    sbatch_cmd, stdout=subprocess.PIPE, env=env\n                ) as train_proc:\n                    stdout = train_proc.stdout.read().decode(""utf-8"")\n                    print(stdout, file=train_log_h)\n                    try:\n                        job_id = int(stdout.rstrip().split()[-1])\n                        return job_id\n                    except IndexError:\n                        return None\n\n\ndef has_finished(save_dir):\n    train_log = os.path.join(save_dir, ""train.log"")\n    if not os.path.exists(train_log):\n        return False\n    with open(train_log) as h:\n        lines = h.readlines()\n        if len(lines) == 0:\n            return False\n        if ""Finished run"" in lines[-1]:\n            return True\n    return False\n\n\ndef has_failed(save_dir):\n    if not os.path.exists(save_dir):\n        return False\n\n    # find max job id\n    job_ids = []\n    for fn in os.listdir(save_dir):\n        if fn.startswith(""train.stderr.""):\n            job_ids.append(int(fn.split(""."")[-1]))\n    if len(job_ids) == 0:\n        return False\n    max_job_id = max(job_ids)\n\n    def _has_failed(stderr_fn):\n        with open(stderr_fn) as h:\n            for line in h:\n                if len(line.strip()) > 0:\n                    # assume that any output in stderr indicates an error\n                    return True\n        return False\n\n    return _has_failed(os.path.join(save_dir, f""train.stderr.{max_job_id}""))\n\n\ndef has_started(save_dir):\n    train_log = os.path.join(save_dir, ""train.log"")\n    if not os.path.exists(train_log):\n        return False\n    return True\n\n\ndef get_random_port():\n    old_state = random.getstate()\n    random.seed()\n    port = random.randint(10000, 20000)\n    random.setstate(old_state)\n    return port\n\n\ndef requeue_support():\n    return """"""\n        trap_handler () {\n           echo ""Caught signal: "" $1\n           # SIGTERM must be bypassed\n           if [ ""$1"" = ""TERM"" ]; then\n               echo ""bypass sigterm""\n           else\n             # Submit a new job to the queue\n             echo ""Requeuing "" $SLURM_JOB_ID\n             scontrol requeue $SLURM_JOB_ID\n           fi\n        }\n\n\n        # Install signal handler\n        trap \'trap_handler USR1\' USR1\n        trap \'trap_handler TERM\' TERM\n    """"""\n'"
mmf/datasets/builders/clevr/__init__.py,0,b''
mmf/datasets/builders/clevr/builder.py,0,"b'import json\nimport math\nimport os\nimport zipfile\nfrom collections import Counter\n\nfrom mmf.common.constants import CLEVR_DOWNLOAD_URL\nfrom mmf.common.registry import registry\nfrom mmf.datasets.base_dataset_builder import BaseDatasetBuilder\nfrom mmf.datasets.builders.clevr.dataset import CLEVRDataset\nfrom mmf.utils.download import download\nfrom mmf.utils.general import get_mmf_root\n\n\n@registry.register_builder(""clevr"")\nclass CLEVRBuilder(BaseDatasetBuilder):\n    def __init__(self):\n        super().__init__(""clevr"")\n        self.writer = registry.get(""writer"")\n        self.dataset_class = CLEVRDataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/clevr/defaults.yaml""\n\n    def build(self, config, dataset_type):\n        download_folder = os.path.join(\n            get_mmf_root(), config.data_dir, config.data_folder\n        )\n\n        file_name = CLEVR_DOWNLOAD_URL.split(""/"")[-1]\n        local_filename = os.path.join(download_folder, file_name)\n\n        extraction_folder = os.path.join(\n            download_folder, ""."".join(file_name.split(""."")[:-1])\n        )\n        self.data_folder = extraction_folder\n\n        # Either if the zip file is already present or if there are some\n        # files inside the folder we don\'t continue download process\n        if os.path.exists(local_filename):\n            self.writer.write(""CLEVR dataset is already present. Skipping download."")\n            return\n\n        if (\n            os.path.exists(extraction_folder)\n            and len(os.listdir(extraction_folder)) != 0\n        ):\n            return\n\n        self.writer.write(""Downloading the CLEVR dataset now"")\n        download(CLEVR_DOWNLOAD_URL, download_folder, CLEVR_DOWNLOAD_URL.split(""/"")[-1])\n\n        self.writer.write(""Downloaded. Extracting now. This can take time."")\n        with zipfile.ZipFile(local_filename, ""r"") as zip_ref:\n            zip_ref.extractall(download_folder)\n\n    def load(self, config, dataset_type, *args, **kwargs):\n        self.dataset = CLEVRDataset(config, dataset_type, data_folder=self.data_folder)\n        return self.dataset\n\n    def update_registry_for_model(self, config):\n        registry.register(\n            self.dataset_name + ""_text_vocab_size"",\n            self.dataset.text_processor.get_vocab_size(),\n        )\n        registry.register(\n            self.dataset_name + ""_num_final_outputs"",\n            self.dataset.answer_processor.get_vocab_size(),\n        )\n'"
mmf/datasets/builders/clevr/dataset.py,1,"b'import json\nimport os\n\nimport numpy as np\nimport torch\nfrom PIL import Image\n\nfrom mmf.common.registry import registry\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.base_dataset import BaseDataset\nfrom mmf.utils.distributed import is_master, synchronize\nfrom mmf.utils.general import get_mmf_root\nfrom mmf.utils.text import VocabFromText, tokenize\n\n_CONSTANTS = {\n    ""questions_folder"": ""questions"",\n    ""dataset_key"": ""clevr"",\n    ""empty_folder_error"": ""CLEVR dataset folder is empty."",\n    ""questions_key"": ""questions"",\n    ""question_key"": ""question"",\n    ""answer_key"": ""answer"",\n    ""train_dataset_key"": ""train"",\n    ""images_folder"": ""images"",\n    ""vocabs_folder"": ""vocabs"",\n}\n\n_TEMPLATES = {\n    ""data_folder_missing_error"": ""Data folder {} for CLEVR is not present."",\n    ""question_json_file"": ""CLEVR_{}_questions.json"",\n    ""vocab_file_template"": ""{}_{}_vocab.txt"",\n}\n\n\nclass CLEVRDataset(BaseDataset):\n    """"""Dataset for CLEVR. CLEVR is a reasoning task where given an image with some\n    3D shapes you have to answer basic questions.\n\n    Args:\n        dataset_type (str): type of dataset, train|val|test\n        config (DictConfig): Configuration Node representing all of the data necessary\n                             to initialize CLEVR dataset class\n        data_folder: Root folder in which all of the data will be present if passed\n                     replaces default based on data_dir and data_folder in config.\n\n    """"""\n\n    def __init__(self, config, dataset_type, data_folder=None, *args, **kwargs):\n        super().__init__(_CONSTANTS[""dataset_key""], config, dataset_type)\n        self._data_folder = data_folder\n        self._data_dir = os.path.join(get_mmf_root(), config.data_dir)\n\n        if not self._data_folder:\n            self._data_folder = os.path.join(self._data_dir, config.data_folder)\n\n        if not os.path.exists(self._data_folder):\n            raise RuntimeError(\n                _TEMPLATES[""data_folder_missing_error""].format(self._data_folder)\n            )\n\n        # Check if the folder was actually extracted in the subfolder\n        if config.data_folder in os.listdir(self._data_folder):\n            self._data_folder = os.path.join(self._data_folder, config.data_folder)\n\n        if len(os.listdir(self._data_folder)) == 0:\n            raise FileNotFoundError(_CONSTANTS[""empty_folder_error""])\n\n        self.load()\n\n    def load(self):\n        self.image_path = os.path.join(\n            self._data_folder, _CONSTANTS[""images_folder""], self._dataset_type\n        )\n\n        with open(\n            os.path.join(\n                self._data_folder,\n                _CONSTANTS[""questions_folder""],\n                _TEMPLATES[""question_json_file""].format(self._dataset_type),\n            )\n        ) as f:\n            self.questions = json.load(f)[_CONSTANTS[""questions_key""]]\n\n            # Vocab should only be built in main process, as it will repetition of same task\n            if is_master():\n                self._build_vocab(self.questions, _CONSTANTS[""question_key""])\n                self._build_vocab(self.questions, _CONSTANTS[""answer_key""])\n            synchronize()\n\n    def __len__(self):\n        return len(self.questions)\n\n    def _get_vocab_path(self, attribute):\n        return os.path.join(\n            self._data_dir,\n            _CONSTANTS[""vocabs_folder""],\n            _TEMPLATES[""vocab_file_template""].format(self.dataset_name, attribute),\n        )\n\n    def _build_vocab(self, questions, attribute):\n        # Vocab should only be built from ""train"" as val and test are not observed in training\n        if self._dataset_type != _CONSTANTS[""train_dataset_key""]:\n            return\n\n        vocab_file = self._get_vocab_path(attribute)\n\n        # Already exists, no need to recreate\n        if os.path.exists(vocab_file):\n            return\n\n        # Create necessary dirs if not present\n        os.makedirs(os.path.dirname(vocab_file), exist_ok=True)\n\n        sentences = [question[attribute] for question in questions]\n        build_attributes = self.config.build_attributes\n\n        # Regex is default one in tokenize i.e. space\n        kwargs = {\n            ""min_count"": build_attributes.get(""min_count"", 1),\n            ""keep"": build_attributes.get(""keep"", ["";"", "",""]),\n            ""remove"": build_attributes.get(""remove"", [""?"", "".""]),\n        }\n\n        if attribute == _CONSTANTS[""answer_key""]:\n            kwargs[""only_unk_extra""] = False\n\n        vocab = VocabFromText(sentences, **kwargs)\n\n        with open(vocab_file, ""w"") as f:\n            f.write(""\\n"".join(vocab.word_list))\n\n    def __getitem__(self, idx):\n        data = self.questions[idx]\n\n        # Each call to __getitem__ from dataloader returns a Sample class object which\n        # collated by our special batch collator to a SampleList which is basically\n        # a attribute based batch in layman terms\n        current_sample = Sample()\n\n        question = data[""question""]\n        tokens = tokenize(question, keep=["";"", "",""], remove=[""?"", "".""])\n        processed = self.text_processor({""tokens"": tokens})\n        current_sample.text = processed[""text""]\n\n        processed = self.answer_processor({""answers"": [data[""answer""]]})\n        current_sample.answers = processed[""answers""]\n        current_sample.targets = processed[""answers_scores""]\n\n        image_path = os.path.join(self.image_path, data[""image_filename""])\n        image = np.true_divide(Image.open(image_path).convert(""RGB""), 255)\n        image = image.astype(np.float32)\n        current_sample.image = torch.from_numpy(image.transpose(2, 0, 1))\n\n        return current_sample\n'"
mmf/datasets/builders/coco/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n__all__ = [""COCOBuilder"", ""COCODataset"", ""MaskedCOCOBuilder"", ""MaskedCOCODataset""]\n\nfrom .builder import COCOBuilder\nfrom .dataset import COCODataset\nfrom .masked_builder import MaskedCOCOBuilder\nfrom .masked_dataset import MaskedCOCODataset\n'"
mmf/datasets/builders/coco/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.coco.dataset import COCODataset\nfrom mmf.datasets.builders.textcaps.dataset import TextCapsDataset\nfrom mmf.datasets.builders.vqa2 import VQA2Builder\n\n\n@registry.register_builder(""coco"")\nclass COCOBuilder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""coco""\n        self.set_dataset_class(COCODataset)\n\n    # TODO: Deprecate this method and move configuration updates directly to processors\n    def update_registry_for_model(self, config):\n        registry.register(\n            self.dataset_name + ""_text_vocab_size"",\n            self.dataset.text_processor.get_vocab_size(),\n        )\n\n        if hasattr(self.dataset, ""answer_processor""):\n            registry.register(\n                self.dataset_name + ""_num_final_outputs"",\n                self.dataset.answer_processor.get_vocab_size(),\n            )\n\n            registry.register(\n                self.dataset_name + ""_answer_processor"", self.dataset.answer_processor,\n            )\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/coco/defaults.yaml""\n\n    def load(self, config, *args, **kwargs):\n        annotation_style = config.get(""annotation_style"", self.dataset_name)\n        if annotation_style == ""textcaps"":\n            self.dataset_class = TextCapsDataset\n\n        dataset = super().load(config, *args, **kwargs)\n        dataset.dataset_name = self.dataset_name\n        return dataset\n'"
mmf/datasets/builders/coco/dataset.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.vqa2 import VQA2Dataset\nfrom mmf.utils.distributed import byte_tensor_to_object, object_to_byte_tensor\n\n\nclass COCODataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config, dataset_type, imdb_file_index, dataset_name=""coco"", *args, **kwargs\n        )\n\n    def preprocess_sample_info(self, sample_info):\n        # COCO Annotation DBs have corrext feature_path\n        if ""COCO"" not in sample_info[""feature_path""]:\n            sample_info[""feature_path""] = sample_info[""image_path""].replace(\n                "".jpg"", "".npy""\n            )\n        return sample_info\n\n    def load_item(self, idx):\n        sample_info = self.annotation_db[idx]\n        sample_info = self.preprocess_sample_info(sample_info)\n        current_sample = Sample()\n\n        if self._dataset_type != ""test"":\n            text_processor_argument = {""tokens"": sample_info[""caption_tokens""]}\n            processed_caption = self.text_processor(text_processor_argument)\n            current_sample.text = processed_caption[""text""]\n            current_sample.caption_id = torch.tensor(\n                sample_info[""caption_id""], dtype=torch.int\n            )\n            current_sample.caption_len = torch.tensor(\n                len(sample_info[""caption_tokens""]), dtype=torch.int\n            )\n\n        if isinstance(sample_info[""image_id""], int):\n            current_sample.image_id = torch.tensor(\n                sample_info[""image_id""], dtype=torch.int\n            )\n        else:\n            current_sample.image_id = object_to_byte_tensor(sample_info[""image_id""])\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            current_sample.update(features)\n\n        # Add reference captions to sample\n        current_sample = self.add_reference_caption(sample_info, current_sample)\n\n        return current_sample\n\n    def add_reference_caption(self, sample_info, sample):\n        reference_list = []\n        for reference in sample_info[""reference_tokens""]:\n            text_processor_argument = {""tokens"": reference}\n            processed_reference = self.text_processor(text_processor_argument)\n            reference_list.append(processed_reference[""text""])\n\n        # Restrict to minimum reference captions available per image\n        sample.answers = torch.stack(reference_list)[: self.config.min_captions_per_img]\n\n        return sample\n\n    def format_for_prediction(self, report):\n        captions = report.captions.tolist()\n        predictions = []\n        remove_unk_from_caption_prediction = getattr(\n            self.config, ""remove_unk_from_caption_prediction"", False\n        )\n\n        for idx, image_id in enumerate(report.image_id):\n            image_id = byte_tensor_to_object(image_id)\n            caption = self.caption_processor(captions[idx])[""caption""]\n            if remove_unk_from_caption_prediction:\n                caption = caption.replace(""<unk>"", """")\n                caption = caption.replace(""  "", "" "").strip()\n            if isinstance(image_id, torch.Tensor):\n                image_id = image_id.item()\n            predictions.append({""image_id"": image_id, ""caption"": caption})\n\n        return predictions\n'"
mmf/datasets/builders/coco/masked_builder.py,0,"b'from mmf.common.registry import registry\nfrom mmf.datasets.builders.coco.builder import COCOBuilder\n\nfrom .masked_dataset import MaskedCOCODataset\n\n\n@registry.register_builder(""masked_coco"")\nclass MaskedCOCOBuilder(COCOBuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""masked_coco""\n        self.set_dataset_class(MaskedCOCODataset)\n\n    def update_registry_for_model(self, config):\n        registry.register(\n            self.dataset_name + ""_text_vocab_size"",\n            self.dataset.masked_token_processor.get_vocab_size(),\n        )\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/coco/masked.yaml""\n'"
mmf/datasets/builders/coco/masked_dataset.py,0,"b'import random\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.coco import COCODataset\n\n\nclass MaskedCOCODataset(COCODataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(config, dataset_type, imdb_file_index, *args, **kwargs)\n        self.dataset_name = ""masked_coco""\n        self._two_sentence = config.get(""two_sentence"", True)\n        self._false_caption = config.get(""false_caption"", True)\n        self._two_sentence_probability = config.get(""two_sentence_probability"", 0.5)\n        self._false_caption_probability = config.get(""false_caption_probability"", 0.5)\n\n    def load_item(self, idx):\n        sample_info = self.annotation_db[idx]\n        current_sample = Sample()\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            image_labels = []\n\n            for i in range(features[""image_feature_0""].shape[0]):\n                prob = random.random()\n                # mask token with 15% probability\n                if prob < 0.15:\n                    prob /= 0.15\n\n                    if prob < 0.9:\n                        features[""image_feature_0""][i] = 0\n                    image_labels.append(1)\n                else:\n                    # no masking token (will be ignored by loss function later)\n                    image_labels.append(-1)\n            item = {}\n\n            if self.config.get(""use_image_feature_masks"", False):\n                item[""image_labels""] = image_labels\n            current_sample.update(item)\n            current_sample.update(features)\n\n        current_sample = self._add_masked_caption(sample_info, current_sample)\n        return current_sample\n\n    def _add_masked_caption(self, sample_info, current_sample):\n        captions = sample_info[""captions""]\n        image_id = sample_info[""image_id""]\n        num_captions = len(captions)\n        selected_caption_index = random.randint(0, num_captions - 1)\n        other_caption_indices = [\n            i for i in range(num_captions) if i != selected_caption_index\n        ]\n        selected_caption = captions[selected_caption_index]\n        other_caption = None\n        is_correct = -1\n\n        if self._dataset_type == ""train"":\n            if self._two_sentence:\n                if random.random() > self._two_sentence_probability:\n                    other_caption = self._get_mismatching_caption(image_id)\n                    is_correct = False\n                else:\n                    other_caption = captions[random.choice(other_caption_indices)]\n                    is_correct = True\n            elif self._false_caption:\n                if random.random() < self._false_caption_probability:\n                    selected_caption = self._get_mismatching_caption(image_id)\n                    is_correct = False\n                else:\n                    is_correct = True\n\n        processed = self.masked_token_processor(\n            {\n                ""text_a"": selected_caption,\n                ""text_b"": other_caption,\n                ""is_correct"": is_correct,\n            }\n        )\n        processed.pop(""tokens"")\n        current_sample.update(processed)\n\n        return current_sample\n\n    def _get_mismatching_caption(self, image_id):\n        other_item = self.imdb[random.randint(0, len(self.imdb) - 1)]\n\n        while other_item[""image_id""] == image_id:\n            other_item = self.imdb[random.randint(0, len(self.imdb) - 1)]\n\n        other_caption = other_item[""captions""][\n            random.randint(0, len(other_item[""captions""]) - 1)\n        ]\n        return other_caption\n'"
mmf/datasets/builders/conceptual_captions/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n__all__ = [\n    ""ConceptualCaptionsBuilder"",\n    ""ConceptualCaptionsDataset"",\n    ""MaskedConceptualCaptionsBuilder"",\n    ""MaskedConceptualCaptionsDataset"",\n]\n\nfrom .builder import ConceptualCaptionsBuilder\nfrom .dataset import ConceptualCaptionsDataset\nfrom .masked_builder import MaskedConceptualCaptionsBuilder\nfrom .masked_dataset import MaskedConceptualCaptionsDataset\n'"
mmf/datasets/builders/conceptual_captions/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.coco import COCOBuilder\n\nfrom .dataset import ConceptualCaptionsDataset\n\n\n@registry.register_builder(""conceptual_captions"")\nclass ConceptualCaptionsBuilder(COCOBuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""conceptual_captions""\n        self.set_dataset_class(ConceptualCaptionsDataset)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/conceptual_captions/defaults.yaml""\n'"
mmf/datasets/builders/conceptual_captions/dataset.py,5,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.coco import COCODataset\n\n\nclass ConceptualCaptionsDataset(COCODataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(config, dataset_type, imdb_file_index, *args, **kwargs)\n        self.dataset_name = ""conceptual_captions""\n\n    def load_item(self, idx):\n        sample_info = self.imdb[idx]\n        current_sample = Sample()\n\n        processed_caption = self.text_processor({""text"": sample_info[""captions""][0]})\n        current_sample.text = processed_caption[""text""]\n        current_sample.caption_len = torch.tensor(\n            len(processed_caption[""text""]), dtype=torch.int\n        )\n\n        if isinstance(sample_info[""image_id""], int):\n            current_sample.image_id = torch.tensor(\n                sample_info[""image_id""], dtype=torch.int\n            )\n        else:\n            current_sample.image_id = sample_info[""image_id""]\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            current_sample.update(features)\n\n        current_sample.answers = torch.stack([processed_caption[""text""]])\n\n        return current_sample\n'"
mmf/datasets/builders/conceptual_captions/masked_builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.coco import MaskedCOCOBuilder\n\nfrom .masked_dataset import MaskedConceptualCaptionsDataset\n\n\n@registry.register_builder(""masked_conceptual_captions"")\nclass MaskedConceptualCaptionsBuilder(MaskedCOCOBuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""masked_conceptual_captions""\n        self.set_dataset_class(MaskedConceptualCaptionsDataset)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/conceptual_captions/masked.yaml""\n'"
mmf/datasets/builders/conceptual_captions/masked_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.datasets.builders.coco import MaskedCOCODataset\n\n\nclass MaskedConceptualCaptionsDataset(MaskedCOCODataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(config, dataset_type, imdb_file_index, *args, **kwargs)\n        self.dataset_name = ""masked_conceptual_captions""\n        self._two_sentence = config.get(""two_sentence"", True)\n        self._false_caption = config.get(""false_caption"", True)\n        self._two_sentence_probability = config.get(""two_sentence_probability"", 0.5)\n        self._false_caption_probability = config.get(""false_caption_probability"", 0.5)\n'"
mmf/datasets/builders/hateful_memes/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/hateful_memes/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nimport os\nimport warnings\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.hateful_memes.dataset import (\n    HatefulMemesFeaturesDataset,\n    HatefulMemesImageDataset,\n)\nfrom mmf.datasets.mmf_dataset_builder import MMFDatasetBuilder\nfrom mmf.utils.configuration import get_mmf_env\nfrom mmf.utils.file_io import PathManager\nfrom mmf.utils.general import get_absolute_path\n\n\n@registry.register_builder(""hateful_memes"")\nclass HatefulMemesBuilder(MMFDatasetBuilder):\n    def __init__(\n        self,\n        dataset_name=""hateful_memes"",\n        dataset_class=HatefulMemesImageDataset,\n        *args,\n        **kwargs\n    ):\n        super().__init__(dataset_name, dataset_class, *args, **kwargs)\n        self.dataset_class = HatefulMemesImageDataset\n\n    @classmethod\n    def config_path(self):\n        return ""configs/datasets/hateful_memes/defaults.yaml""\n\n    def load(self, config, dataset_type, *args, **kwargs):\n        config = config\n\n        if config.use_features:\n            self.dataset_class = HatefulMemesFeaturesDataset\n\n        self.dataset = super().load(config, dataset_type, *args, **kwargs)\n\n        return self.dataset\n\n    def build(self, config, *args, **kwargs):\n        # First, check whether manual downloads have been performed\n        data_dir = get_mmf_env(key=""data_dir"")\n        test_path = get_absolute_path(\n            os.path.join(\n                data_dir,\n                ""datasets"",\n                self.dataset_name,\n                ""defaults"",\n                ""annotations"",\n                ""train.jsonl"",\n            )\n        )\n        # NOTE: This doesn\'t check for files, but that is a fine assumption for now\n        assert PathManager.exists(test_path), (\n            ""Hateful Memes Dataset doesn\'t do automatic downloads; please ""\n            + ""follow instructions at https://fb.me/hm_prerequisites""\n        )\n        super().build(config, *args, **kwargs)\n\n    def update_registry_for_model(self, config):\n        if hasattr(self.dataset, ""text_processor"") and hasattr(\n            self.dataset.text_processor, ""get_vocab_size""\n        ):\n            registry.register(\n                self.dataset_name + ""_text_vocab_size"",\n                self.dataset.text_processor.get_vocab_size(),\n            )\n        registry.register(\n            self.dataset_name + ""_num_final_outputs"", 2,\n        )\n'"
mmf/datasets/builders/hateful_memes/dataset.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport copy\nimport os\n\nimport numpy as np\nimport omegaconf\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.mmf_dataset import MMFDataset\nfrom mmf.utils.general import get_mmf_root\nfrom mmf.utils.visualize import visualize_images\n\n\nclass HatefulMemesFeaturesDataset(MMFDataset):\n    def __init__(self, config, *args, dataset_name=""hateful_memes"", **kwargs):\n        super().__init__(dataset_name, config, *args, **kwargs)\n        assert (\n            self._use_features\n        ), ""config\'s \'use_images\' must be true to use image dataset""\n\n    def preprocess_sample_info(self, sample_info):\n        image_path = sample_info[""img""]\n        # img/02345.png -> 02345\n        feature_path = image_path.split(""/"")[-1].split(""."")[0]\n        # Add feature_path key for feature_database access\n        sample_info[""feature_path""] = f""{feature_path}.npy""\n        return sample_info\n\n    def __getitem__(self, idx):\n        sample_info = self.annotation_db[idx]\n        sample_info = self.preprocess_sample_info(sample_info)\n\n        current_sample = Sample()\n\n        processed_text = self.text_processor({""text"": sample_info[""text""]})\n        current_sample.text = processed_text[""text""]\n        if ""input_ids"" in processed_text:\n            current_sample.update(processed_text)\n\n        current_sample.id = torch.tensor(int(sample_info[""id""]), dtype=torch.int)\n\n        # Instead of using idx directly here, use sample_info to fetch\n        # the features as feature_path has been dynamically added\n        features = self.features_db.get(sample_info)\n        current_sample.update(features)\n\n        if ""label"" in sample_info:\n            current_sample.targets = torch.tensor(\n                sample_info[""label""], dtype=torch.long\n            )\n\n        return current_sample\n\n    def format_for_prediction(self, report):\n        return generate_prediction(report)\n\n\nclass HatefulMemesImageDataset(MMFDataset):\n    def __init__(self, config, *args, dataset_name=""hateful_memes"", **kwargs):\n        super().__init__(dataset_name, config, *args, **kwargs)\n        assert (\n            self._use_images\n        ), ""config\'s \'use_images\' must be true to use image dataset""\n\n    def init_processors(self):\n        super().init_processors()\n        # Assign transforms to the image_db\n        self.image_db.transform = self.image_processor\n\n    def __getitem__(self, idx):\n        sample_info = self.annotation_db[idx]\n        current_sample = Sample()\n\n        processed_text = self.text_processor({""text"": sample_info[""text""]})\n        current_sample.text = processed_text[""text""]\n        if ""input_ids"" in processed_text:\n            current_sample.update(processed_text)\n\n        current_sample.id = torch.tensor(int(sample_info[""id""]), dtype=torch.int)\n\n        # Get the first image from the set of images returned from the image_db\n        current_sample.image = self.image_db[idx][""images""][0]\n\n        if ""label"" in sample_info:\n            current_sample.targets = torch.tensor(\n                sample_info[""label""], dtype=torch.long\n            )\n\n        return current_sample\n\n    def format_for_prediction(self, report):\n        return generate_prediction(report)\n\n    def visualize(self, num_samples=1, use_transforms=False, *args, **kwargs):\n        image_paths = []\n        random_samples = np.random.randint(0, len(self), size=num_samples)\n\n        for idx in random_samples:\n            image_paths.append(self.annotation_db[idx][""img""])\n\n        images = self.image_db.from_path(image_paths, use_transforms=use_transforms)\n        visualize_images(images[""images""], *args, **kwargs)\n\n\ndef generate_prediction(report):\n    scores = torch.nn.functional.softmax(report.scores, dim=1)\n    _, labels = torch.max(scores, 1)\n    # Probability that the meme is hateful, (1)\n    probabilities = scores[:, 1]\n\n    predictions = []\n\n    for idx, image_id in enumerate(report.id):\n        proba = probabilities[idx].item()\n        label = labels[idx].item()\n        predictions.append(\n            {""id"": image_id.item(), ""proba"": proba, ""label"": label,}\n        )\n    return predictions\n'"
mmf/datasets/builders/mmimdb/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/mmimdb/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.mmimdb.dataset import MMIMDbDataset\nfrom mmf.datasets.builders.vqa2.builder import VQA2Builder\n\n\n@registry.register_builder(""mmimdb"")\nclass MMIMDbBuilder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""mmimdb""\n        self.dataset_class = MMIMDbDataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/mmimdb/defaults.yaml""\n'"
mmf/datasets/builders/mmimdb/dataset.py,0,"b'import copy\nimport json\n\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.mmf_dataset import MMFDataset\n\n\nclass MMIMDbDataset(MMFDataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            ""mmimdb"", config, dataset_type, imdb_file_index, *args, **kwargs\n        )\n\n    def __getitem__(self, idx):\n        sample_info = self.annotation_db[idx]\n        current_sample = Sample()\n        plot = sample_info[""plot""]\n        if isinstance(plot, list):\n            plot = plot[0]\n        processed_sentence = self.text_processor({""text"": plot})\n\n        current_sample.text = processed_sentence[""text""]\n        if ""input_ids"" in processed_sentence:\n            current_sample.update(processed_sentence)\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            current_sample.update(features)\n\n        processed = self.answer_processor({""answers"": sample_info[""genres""]})\n        current_sample.answers = processed[""answers""]\n        current_sample.targets = processed[""answers_scores""]\n\n        return current_sample\n'"
mmf/datasets/builders/mmimdb/masked_builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.mmimdb.masked_dataset import MaskedMMImdbDataset\nfrom mmf.datasets.builders.vqa2.builder import VQA2Builder\n\n\n@registry.register_builder(""masked_mmimdb"")\nclass MaskedMMImdbBuilder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""masked_mmimdb""\n        self.dataset_class = MaskedMMImdbDataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/mmimdb/masked.yaml""\n'"
mmf/datasets/builders/mmimdb/masked_dataset.py,0,"b'import random\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.vqa2.dataset import VQA2Dataset\n\n\nclass MaskedMMImdbDataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config,\n            dataset_type,\n            imdb_file_index,\n            dataset_name=""masked_mmimdb"",\n            *args,\n            **kwargs\n        )\n        self._add_answer = config.get(""add_answer"", True)\n\n    def load_item(self, idx):\n        sample_info = self.imdb[idx]\n        current_sample = Sample()\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            current_sample.update(features)\n            image_labels = []\n\n            for i in range(features[""image_feature_0""].shape[0]):\n                prob = random.random()\n                # mask token with 15% probability\n                if prob < 0.15:\n                    prob /= 0.15\n\n                    if prob < 0.9:\n                        features[""image_feature_0""][i] = 0\n                    image_labels.append(1)\n                else:\n                    # no masking token (will be ignored by loss function later)\n                    image_labels.append(-1)\n            item = {}\n            if self.config.get(""use_image_feature_masks"", False):\n                item[""image_labels""] = image_labels\n            current_sample.update(item)\n        current_sample = self._add_masked_question(sample_info, current_sample)\n\n        return current_sample\n\n    def _add_masked_question(self, sample_info, current_sample):\n        plot = sample_info[""plot""]\n        if isinstance(plot, list):\n            plot = plot[0]\n        question = plot\n        random_answer = random.choice(sample_info[""genres""])\n\n        processed = self.masked_token_processor(\n            {""text_a"": question, ""text_b"": random_answer, ""is_correct"": -1}\n        )\n\n        processed.pop(""tokens"")\n        current_sample.update(processed)\n\n        return current_sample\n'"
mmf/datasets/builders/nlvr2/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/nlvr2/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.nlvr2.dataset import NLVR2Dataset\nfrom mmf.datasets.builders.vqa2.builder import VQA2Builder\n\n\n@registry.register_builder(""nlvr2"")\nclass NLVR2Builder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""nlvr2""\n        self.dataset_class = NLVR2Dataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/nlvr2/defaults.yaml""\n'"
mmf/datasets/builders/nlvr2/dataset.py,1,"b'import copy\nimport json\n\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.vqa2 import VQA2Dataset\n\n\nclass NLVR2Dataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config, dataset_type, imdb_file_index, dataset_name=""nlvr2"", *args, **kwargs\n        )\n\n    def load_item(self, idx):\n        sample_info = self.imdb[idx]\n        current_sample = Sample()\n\n        processed_sentence = self.text_processor({""text"": sample_info[""sentence""]})\n\n        current_sample.text = processed_sentence[""text""]\n        if ""input_ids"" in processed_sentence:\n            current_sample.update(processed_sentence)\n\n        if self._use_features is True:\n            # Remove sentence id from end\n            identifier = ""-"".join(sample_info[""identifier""].split(""-"")[:-1])\n            # Load img0 and img1 features\n            sample_info[""feature_path""] = ""{}-img0.npy"".format(identifier)\n            features = self.features_db[idx]\n            current_sample.img0 = Sample()\n            current_sample.img0.update(features)\n\n            sample_info[""feature_path""] = ""{}-img1.npy"".format(identifier)\n            features = self.features_db[idx]\n            current_sample.img1 = Sample()\n            current_sample.img1.update(features)\n\n        is_correct = 1 if sample_info[""label""] == ""True"" else 0\n        current_sample.targets = torch.tensor(is_correct, dtype=torch.long)\n\n        return current_sample\n'"
mmf/datasets/builders/ocrvqa/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/ocrvqa/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import Registry\nfrom mmf.datasets.builders.ocrvqa.dataset import OCRVQADataset\nfrom mmf.datasets.builders.textvqa.builder import TextVQABuilder\n\n\n@Registry.register_builder(""ocrvqa"")\nclass OCRVQABuilder(TextVQABuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""ocrvqa""\n        self.set_dataset_class(OCRVQADataset)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/ocrvqa/defaults.yaml""\n'"
mmf/datasets/builders/ocrvqa/dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.datasets.builders.textvqa.dataset import TextVQADataset\n\n\nclass OCRVQADataset(TextVQADataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(config, dataset_type, imdb_file_index, *args, **kwargs)\n        self.dataset_name = ""ocrvqa""\n\n    def preprocess_sample_info(self, sample_info):\n        # Do nothing in this case\n        return sample_info\n'"
mmf/datasets/builders/sbu_captions/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\n__all__ = [""MaskedSBUBuilder"", ""MaskedSBUDataset""]\n\nfrom .masked_builder import MaskedSBUBuilder\nfrom .masked_dataset import MaskedSBUDataset\n'"
mmf/datasets/builders/sbu_captions/masked_builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.coco import MaskedCOCOBuilder\n\nfrom .masked_dataset import MaskedSBUDataset\n\n\n@registry.register_builder(""masked_sbu"")\nclass MaskedSBUBuilder(MaskedCOCOBuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""masked_sbu""\n        self.set_dataset_class(MaskedSBUDataset)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/sbu_captions/masked.yaml""\n'"
mmf/datasets/builders/sbu_captions/masked_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n\nfrom mmf.datasets.builders.coco import MaskedCOCODataset\n\n\nclass MaskedSBUDataset(MaskedCOCODataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(config, dataset_type, imdb_file_index, *args, **kwargs)\n        self.dataset_name = ""masked_sbu""\n        self._two_sentence = config.get(""two_sentence"", True)\n        self._false_caption = config.get(""false_caption"", True)\n        self._two_sentence_probability = config.get(""two_sentence_probability"", 0.5)\n        self._false_caption_probability = config.get(""false_caption_probability"", 0.5)\n'"
mmf/datasets/builders/stvqa/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/stvqa/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import Registry\nfrom mmf.datasets.builders.stvqa.dataset import STVQADataset\nfrom mmf.datasets.builders.textvqa.builder import TextVQABuilder\n\n\n@Registry.register_builder(""stvqa"")\nclass STVQABuilder(TextVQABuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""stvqa""\n        self.set_dataset_class(STVQADataset)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/stvqa/defaults.yaml""\n'"
mmf/datasets/builders/stvqa/dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.datasets.builders.textvqa.dataset import TextVQADataset\n\n\nclass STVQADataset(TextVQADataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(config, dataset_type, imdb_file_index, *args, **kwargs)\n        self.dataset_name = ""stvqa""\n\n    def preprocess_sample_info(self, sample_info):\n        feature_path = sample_info[""feature_path""]\n        append = ""train""\n\n        if self.dataset_type == ""test"":\n            append = ""test_task3""\n\n        if not feature_path.startswith(append):\n            feature_path = append + ""/"" + feature_path\n\n        sample_info[""feature_path""] = feature_path\n\n        return sample_info\n'"
mmf/datasets/builders/textcaps/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/textcaps/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import Registry\nfrom mmf.datasets.builders.coco.dataset import COCODataset\nfrom mmf.datasets.builders.textcaps.dataset import TextCapsDataset\nfrom mmf.datasets.builders.textvqa.builder import TextVQABuilder\n\n\n@Registry.register_builder(""textcaps"")\nclass TextCapsBuilder(TextVQABuilder):\n    def __init__(\n        self, dataset_name=""textcaps"", dataset_class=TextCapsDataset, *args, **kwargs\n    ):\n        super().__init__(dataset_name, dataset_class, *args, **kwargs)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/textcaps/defaults.yaml""\n\n    def load(self, config, *args, **kwargs):\n        annotation_style = config.get(""annotation_style"", self.dataset_name)\n        if annotation_style == ""coco"":\n            self.dataset_class = COCODataset\n\n        dataset = super().load(config, *args, **kwargs)\n        dataset.dataset_name = self.dataset_name\n        return dataset\n'"
mmf/datasets/builders/textcaps/dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.datasets.builders.textvqa.dataset import TextVQADataset\nfrom mmf.utils.distributed import object_to_byte_tensor\n\n\nclass TextCapsDataset(TextVQADataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(config, dataset_type, imdb_file_index, *args, **kwargs)\n        self.dataset_name = ""textcaps""\n\n    def preprocess_sample_info(self, sample_info):\n        sample_info = super().preprocess_sample_info(sample_info)\n        # add dummy questions to train with M4C (for TextVQA)\n        sample_info[""question_str""] = """"  # empty question\n        sample_info[""question_id""] = sample_info[""caption_id""]\n        return sample_info\n\n    def postprocess_evalai_entry(self, entry):\n        new_entry = {\n            ""caption_id"": entry[""question_id""],\n            ""image_id"": entry[""image_id""],\n            ""caption"": entry[""answer""],\n            ""pred_source"": entry[""pred_source""],\n        }\n        return new_entry\n\n    def add_answer_info(self, sample_info, sample):\n        sample_has_caption = ""caption_str"" in sample_info\n        if sample_has_caption:\n            sample_info[""answers""] = [sample_info[""caption_str""]]\n\n        sample = super().add_answer_info(sample_info, sample)\n\n        if sample_has_caption:\n            sample.caption_str = object_to_byte_tensor(sample_info[""caption_str""])\n            sample.ref_strs = object_to_byte_tensor(sample_info[""reference_strs""])\n            sample.pop(""answers"")\n\n        return sample\n'"
mmf/datasets/builders/textvqa/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/textvqa/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.textvqa.dataset import TextVQADataset\nfrom mmf.datasets.mmf_dataset_builder import MMFDatasetBuilder\n\n\n@registry.register_builder(""textvqa"")\nclass TextVQABuilder(MMFDatasetBuilder):\n    def __init__(\n        self, dataset_name=""textvqa"", dataset_class=TextVQADataset, *args, **kwargs\n    ):\n        super().__init__(dataset_name, dataset_class, *args, **kwargs)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/textvqa/defaults.yaml""\n\n    # TODO: Deprecate this method and move configuration updates directly to processors\n    def update_registry_for_model(self, config):\n        if hasattr(self.dataset, ""text_processor""):\n            registry.register(\n                self.dataset_name + ""_text_vocab_size"",\n                self.dataset.text_processor.get_vocab_size(),\n            )\n            registry.register(\n                f""{self.dataset_name}_text_processor"", self.dataset.text_processor\n            )\n        if hasattr(self.dataset, ""answer_processor""):\n            registry.register(\n                self.dataset_name + ""_num_final_outputs"",\n                self.dataset.answer_processor.get_vocab_size(),\n            )\n            registry.register(\n                f""{self.dataset_name}_answer_processor"", self.dataset.answer_processor\n            )\n'"
mmf/datasets/builders/textvqa/dataset.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport numpy as np\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.mmf_dataset import MMFDataset\nfrom mmf.utils.distributed import byte_tensor_to_object, object_to_byte_tensor\nfrom mmf.utils.text import word_tokenize\n\n\nclass TextVQADataset(MMFDataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(""textvqa"", config, dataset_type, index=imdb_file_index)\n        self.use_ocr = self.config.use_ocr\n        self.use_ocr_info = self.config.use_ocr_info\n\n    def preprocess_sample_info(self, sample_info):\n        # COCO Annotation DBs have corrext feature_path\n        if ""COCO"" not in sample_info[""feature_path""]:\n            sample_info[""feature_path""] = sample_info[""image_path""].replace(\n                "".jpg"", "".npy""\n            )\n        return sample_info\n\n    def postprocess_evalai_entry(self, entry):\n        return entry  # Do nothing\n\n    def format_for_prediction(self, report):\n        answer_processor = self.answer_processor\n\n        batch_size = len(report.question_id)\n        pred_answers = report.scores.argmax(dim=-1).view(batch_size, -1)\n        answer_space_size = answer_processor.get_true_vocab_size()\n\n        image_ids = report.image_id.cpu().numpy()\n        context_tokens = report.context_tokens.cpu().numpy()\n        predictions = []\n        for idx, question_id in enumerate(report.question_id):\n            # collect VQA answers\n            image_id = byte_tensor_to_object(image_ids[idx])\n            tokens = byte_tensor_to_object(context_tokens[idx])\n            answer_words = []\n            pred_source = []\n            for answer_id in pred_answers[idx].tolist():\n                if answer_id >= answer_space_size:\n                    answer_id -= answer_space_size\n                    answer_words.append(word_tokenize(tokens[answer_id]))\n                    pred_source.append(""OCR"")\n                else:\n                    if answer_id == answer_processor.EOS_IDX:\n                        break\n                    answer_words.append(\n                        answer_processor.answer_vocab.idx2word(answer_id)\n                    )\n                    pred_source.append(""VOCAB"")\n            # join all the answer tokens with space\n            # (this should be correct for almost all cases)\n            pred_answer = "" "".join(answer_words).replace("" \'s"", ""\'s"")\n            entry = {\n                ""question_id"": question_id.item(),\n                ""image_id"": image_id,\n                ""answer"": pred_answer,\n                ""pred_source"": pred_source,\n            }\n            entry = self.postprocess_evalai_entry(entry)\n\n            predictions.append(entry)\n\n        return predictions\n\n    def __getitem__(self, idx):\n        sample_info = self.annotation_db[idx]\n        sample_info = self.preprocess_sample_info(sample_info)\n        current_sample = Sample()\n\n        # breaking change from VQA2Dataset: load question_id\n        current_sample.question_id = torch.tensor(\n            sample_info[""question_id""], dtype=torch.int\n        )\n\n        if isinstance(sample_info[""image_id""], int):\n            current_sample.image_id = str(sample_info[""image_id""])\n        else:\n            current_sample.image_id = sample_info[""image_id""]\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            current_sample.update(features)\n\n        current_sample = self.add_sample_details(sample_info, current_sample)\n        current_sample = self.add_answer_info(sample_info, current_sample)\n\n        # only the \'max_features\' key is needed\n        # pop other keys to minimize data loading overhead\n        if hasattr(current_sample, ""image_info_0""):\n            for k in list(current_sample.image_info_0):\n                if k != ""max_features"":\n                    current_sample.image_info_0.pop(k)\n        if hasattr(current_sample, ""image_info_1""):\n            for k in list(current_sample.image_info_1):\n                if k != ""max_features"":\n                    current_sample.image_info_1.pop(k)\n\n        return current_sample\n\n    def add_sample_details(self, sample_info, sample):\n        sample.image_id = object_to_byte_tensor(sample_info[""image_id""])\n\n        # 1. Load text (question words)\n        question_str = (\n            sample_info[""question""]\n            if ""question"" in sample_info\n            else sample_info[""question_str""]\n        )\n        text_processor_args = {""text"": question_str}\n\n        if ""question_tokens"" in sample_info:\n            text_processor_args[""tokens""] = sample_info[""question_tokens""]\n\n        processed_question = self.text_processor(text_processor_args)\n\n        if ""input_ids"" in processed_question:\n            sample.text = processed_question[""input_ids""]\n            sample.text_len = torch.tensor(\n                len(processed_question[""tokens""]), dtype=torch.long\n            )\n        else:\n            # For GLoVe based processors\n            sample.text = processed_question[""text""]\n            sample.text_len = processed_question[""length""]\n\n        # 2. Load object\n        # object bounding box information\n        if ""obj_normalized_boxes"" in sample_info and hasattr(self, ""copy_processor""):\n            sample.obj_bbox_coordinates = self.copy_processor(\n                {""blob"": sample_info[""obj_normalized_boxes""]}\n            )[""blob""]\n\n        # 3. Load OCR\n        if not self.use_ocr:\n            # remove all OCRs from the sample\n            # (i.e. make an empty OCR list)\n            sample_info[""ocr_tokens""] = []\n            sample_info[""ocr_info""] = []\n            if ""ocr_normalized_boxes"" in sample_info:\n                sample_info[""ocr_normalized_boxes""] = np.zeros((0, 4), np.float32)\n            # clear OCR visual features\n            if ""image_feature_1"" in sample:\n                sample.image_feature_1 = torch.zeros_like(sample.image_feature_1)\n            return sample\n\n        # Preprocess OCR tokens\n        if hasattr(self, ""ocr_token_processor""):\n            ocr_tokens = [\n                self.ocr_token_processor({""text"": token})[""text""]\n                for token in sample_info[""ocr_tokens""]\n            ]\n        else:\n            ocr_tokens = sample_info[""ocr_tokens""]\n        # Get FastText embeddings for OCR tokens\n        context = self.context_processor({""tokens"": ocr_tokens})\n        sample.context = context[""text""]\n        sample.ocr_tokens = context[""tokens""]\n\n        sample.context_tokens = object_to_byte_tensor(context[""tokens""])\n        sample.context_feature_0 = context[""text""]\n        sample.context_info_0 = Sample()\n        sample.context_info_0.max_features = context[""length""]\n\n        # Get PHOC embeddings for OCR tokens\n        if hasattr(self, ""phoc_processor""):\n            context_phoc = self.phoc_processor({""tokens"": ocr_tokens})\n            sample.context_feature_1 = context_phoc[""text""]\n            sample.context_info_1 = Sample()\n            sample.context_info_1.max_features = context_phoc[""length""]\n\n        # OCR order vectors\n        if self.config.get(""use_order_vectors"", False):\n            order_vectors = np.eye(len(sample.ocr_tokens), dtype=np.float32)\n            order_vectors = torch.from_numpy(order_vectors)\n            order_vectors[context[""length""] :] = 0\n            sample.order_vectors = order_vectors\n\n        # OCR bounding box information\n        if ""ocr_normalized_boxes"" in sample_info and hasattr(self, ""copy_processor""):\n            # New imdb format: OCR bounding boxes are already pre-computed\n            max_len = self.config.processors.answer_processor.params.max_length\n            sample.ocr_bbox_coordinates = self.copy_processor(\n                {""blob"": sample_info[""ocr_normalized_boxes""]}\n            )[""blob""][:max_len]\n        elif self.use_ocr_info and ""ocr_info"" in sample_info:\n            # Old imdb format: OCR bounding boxes are computed on-the-fly\n            # from ocr_info\n            sample.ocr_bbox_coordinates = self.bbox_processor(\n                {""info"": sample_info[""ocr_info""]}\n            )[""bbox""].coordinates\n\n        return sample\n\n    def add_answer_info(self, sample_info, sample):\n        # Load real answers from sample_info\n        answers = sample_info.get(""answers"", None)\n        answer_processor_arg = {""answers"": answers}\n\n        answer_processor_arg[""tokens""] = sample.pop(""ocr_tokens"", [])\n\n        processed_answers = self.answer_processor(answer_processor_arg)\n\n        assert not self.config.fast_read, (\n            ""In TextVQADataset, online OCR sampling is incompatible ""\n            ""with fast_read, so fast_read is currently not supported.""\n        )\n\n        sample.update(processed_answers)\n        sample.answers = object_to_byte_tensor(answers)\n\n        if ""answers_scores"" in sample:\n            sample.targets = sample.pop(""answers_scores"")\n\n        return sample\n'"
mmf/datasets/builders/visual_dialog/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/visual_dialog/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport shutil\n\nfrom mmf.common.constants import VISUAL_DIALOG_CONSTS\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.visual_dialog.dataset import VisualDialogDataset\nfrom mmf.datasets.builders.visual_genome.builder import VisualGenomeBuilder\nfrom mmf.utils.download import decompress, download\nfrom mmf.utils.general import get_mmf_root\n\n\n@registry.register_builder(""visual_dialog"")\nclass VisualDialogBuilder(VisualGenomeBuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""visual_dialog""\n        self.dataset_class = VisualDialogDataset\n        self.writer = registry.get(""writer"")\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/visual_dialog/defaults.yaml""\n\n    def build(self, config, dataset_type):\n        self._dataset_type = dataset_type\n        self._config = config\n        data_folder = os.path.join(get_mmf_root(), self._config.data_dir)\n\n        self._download_and_extract_imdb(data_folder)\n\n        if self._dataset_type != ""train"":\n            return\n\n        self._download_and_extract(\n            ""vocabs"", VISUAL_DIALOG_CONSTS[""vocabs""], data_folder\n        )\n        self._download_and_extract_features(data_folder)\n\n    def _download_and_extract_imdb(self, data_folder):\n        download_folder = os.path.join(data_folder, ""imdb"")\n\n        self._download_and_extract(\n            ""imdb_url"",\n            VISUAL_DIALOG_CONSTS[""imdb_url""][self._dataset_type],\n            download_folder,\n        )\n\n    def _download_and_extract_features(self, data_folder):\n        # Visual Dialog features will contain val and test\n        self._download_and_extract(\n            ""features_url"",\n            VISUAL_DIALOG_CONSTS[""features_url""][""visual_dialog""],\n            data_folder,\n        )\n        # But since train is same as COCO, we reuse those features if already downloaded\n        self._download_and_extract(\n            ""features_url"", VISUAL_DIALOG_CONSTS[""features_url""][""coco""], data_folder\n        )\n'"
mmf/datasets/builders/visual_dialog/database.py,1,"b'import json\n\nimport torch\n\n\nclass VisualDialogDatabase(torch.utils.data.Dataset):\n    def __init__(self, imdb_path):\n        super().__init__()\n        self._load_json(imdb_path)\n        self._metadata = {}\n\n    @property\n    def metadata(self):\n        return self._metadata\n\n    @metadata.setter\n    def metadata(self, x):\n        self._metadata = x\n\n    def _load_json(self, imdb_path):\n        with open(imdb_path, ""r""):\n            data = json.load(imdb_path)\n            self._is_test = data[""split""] == ""test""\n            self._question = data[""questions""]\n            self._answers = data[""answers""]\n            self._dialogs = data[""dialogs""]\n\n        # Test has only one round per dialog\n        self._multiplier = 1 if self._is_test else 10\n        self._qa_length = len(self._dialogs) * self._multiplier\n\n    def __len__(self):\n        return self._qa_length\n\n    def __getitem__(self, idx):\n        data = {}\n\n        dialog_id = idx / self._multiplier\n        round_id = idx % self._multiplier\n        dialog = self._dialogs[dialog_id]\n        data[""id""] = idx\n        data[""dialog_id""] = dialog_id\n        data[""round_id""] = round_id\n        round = dialog[""dialog""][round_id]\n        data[""question""] = self._questions[round[""question""]]\n        # data[""answers""] = [self.]\n'"
mmf/datasets/builders/visual_dialog/dataset.py,0,"b'import copy\nimport json\n\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.visual_dialog.database import VisualDialogDatabase\nfrom mmf.datasets.builders.vqa2 import VQA2Dataset\n\n\nclass VisualDialogDataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config,\n            dataset_type,\n            imdb_file_index,\n            dataset_name=""visual_dialog"",\n            *args,\n            **kwargs\n        )\n\n        discriminative = config.discriminative\n        self._discriminative = discriminative.enabled\n        self._return_indices = discriminative.return_indices\n        self._no_unk = config.no_unk\n        self._return_history = config.return_history\n'"
mmf/datasets/builders/visual_dialog/original.py,6,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils.data import ConcatDataset\n\nfrom mmf.datasets.vqa2.task import VQA2Task\n\nfrom .dataset import VisualDialogDataset\n\n\nclass VisualDialogTask(VQA2Task):\n    def __init__(self, dataset_type):\n        super(VisualDialogTask, self).__init__(dataset_type)\n        self.task_name = ""VisualDialog""\n\n    def prepare_data_set(self, imdb_file_label, image_feature_dir_label, **data_config):\n        data_dir = data_config[""data_dir""]\n\n        vocab_file = os.path.join(data_dir, data_config[""vocab_file""])\n        embedding_name = data_config[""embedding_name""]\n        max_seq_len = data_config[""max_seq_len""]\n        max_history_len = data_config[""max_history_len""]\n        depth_first = data_config[""depth_first""]\n        image_fast_reader = data_config[""image_fast_reader""]\n\n        if ""verbose"" in data_config:\n            verbose = data_config[""verbose""]\n        else:\n            verbose = False\n\n        if ""test_mode"" in data_config:\n            test_mode = data_config[""test_mode""]\n        else:\n            test_mode = False\n\n        if ""image_max_loc"" in data_config:\n            image_max_loc = data_config[""image_max_loc""]\n        else:\n            image_max_loc = False\n\n        annotations = data_config[imdb_file_label]\n        image_feat_dirs = data_config[image_feature_dir_label]\n\n        condition = len(annotations) == len(image_feat_dirs)\n        error = imdb_file_label + ""length != "" + image_feature_dir_label\n        error += ""length""\n        assert condition, error\n\n        datasets = []\n\n        for imdb_file, image_feature_dir in zip(annotations, image_feat_dirs):\n            imdb_file = os.path.join(data_dir, imdb_file)\n            image_feat_dirs = [\n                os.path.join(data_dir, d) for d in image_feature_dir.split("","")\n            ]\n            args = {\n                ""imdb_file"": imdb_file,\n                ""image_feat_directories"": image_feat_dirs,\n                ""max_seq_len"": max_seq_len,\n                ""max_history_len"": max_history_len,\n                ""vocab_file"": vocab_file,\n                ""depth_first"": depth_first,\n                ""fast_read"": image_fast_reader,\n                ""verbose"": verbose,\n                ""test_mode"": test_mode,\n                ""image_max_loc"": image_max_loc,\n                ""embedding_name"": embedding_name,\n            }\n\n            train_dataset = VisualDialogDataset(**args)\n            datasets.append(train_dataset)\n\n        return VisualDialogConcatDataset(datasets)\n\n    def prepare_batch(self, batch, use_cuda):\n        questions = batch[""questions""]\n        input_image_features = batch[""image_feat_batch""]\n        answer_options = batch[""answer_options""]\n        histories = batch[""histories""]\n\n        questions = Variable(questions.type(torch.LongTensor))\n        histories = Variable(histories.type(torch.LongTensor))\n        answer_options = Variable(answer_options.type(torch.LongTensor))\n        input_image_features = Variable(input_image_features)\n\n        if use_cuda:\n            questions = questions.cuda()\n            histories = histories.cuda()\n            answer_options = answer_options.cuda()\n            input_image_features = input_image_features.cuda()\n\n        image_feature_variables = [input_image_features]\n        image_dim_variable = None\n\n        if ""image_dim"" in batch:\n            image_dims = batch[""image_dim""]\n            image_dim_variable = Variable(\n                image_dims, requires_grad=False, volatile=False\n            )\n\n            if use_cuda:\n                image_dim_variable = image_dim_variable.cuda()\n\n        # check if more than 1 image_feat_batch\n        i = 1\n        image_feat_key = ""image_feat_batch_%s""\n        while image_feat_key % str(i) in batch:\n            tmp_image_variable = Variable(batch[image_feat_key % str(i)])\n            if use_cuda:\n                tmp_image_variable = tmp_image_variable.cuda()\n            image_feature_variables.append(tmp_image_variable)\n            i += 1\n\n        y = batch[""expected""]\n        y = Variable(y.type(torch.FloatTensor))\n        y = y.view(-1, y.size(-1))\n        if use_cuda:\n            y = y.cuda()\n\n        out = {\n            ""texts"": questions,\n            ""answer_options"": answer_options,\n            ""histories"": histories,\n            ""features"": image_feature_variables,\n            ""image_dims"": image_dim_variable,\n            ""texts_len"": batch[""questions_len""],\n            ""answer_options_len"": batch[""answer_options_len""],\n            ""histories_len"": batch[""histories_len""],\n        }\n\n        return out, y\n\n    def update_registry_for_model(self, config):\n        config[""num_vocab_txt""] = self.dataset.vocab.get_size()\n        config[""vocab_size""] = self.dataset.vocab.get_size()\n        config[""num_image_features""] = self.num_image_features\n        config[""embedding_vectors""] = self.dataset.vocab.vectors\n\n    def clean_config(self, config):\n        config.pop(""embedding_vectors"", None)\n\n\nclass VisualDialogConcatDataset(ConcatDataset):\n    def __init__(self, datasets):\n        super(VisualDialogConcatDataset, self).__init__(datasets)\n        self.vocab = datasets[0].vocab\n'"
mmf/datasets/builders/visual_entailment/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/visual_entailment/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.visual_entailment.dataset import VisualEntailmentDataset\nfrom mmf.datasets.builders.vqa2.builder import VQA2Builder\n\n\n@registry.register_builder(""visual_entailment"")\nclass VisualEntailmentBuilder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""visual_entailment""\n        self.dataset_class = VisualEntailmentDataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/visual_entailment/defaults.yaml""\n'"
mmf/datasets/builders/visual_entailment/dataset.py,1,"b'import copy\nimport json\n\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.vqa2 import VQA2Dataset\n\nLABEL_TO_INT_MAPPING = {""entailment"": 0, ""neutral"": 1, ""contradiction"": 2}\n\n\nclass VisualEntailmentDataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config,\n            dataset_type,\n            imdb_file_index,\n            dataset_name=""visual_entailment"",\n            *args,\n            **kwargs\n        )\n\n    def load_item(self, idx):\n        sample_info = self.annotation_db[idx]\n        current_sample = Sample()\n\n        processed_sentence = self.text_processor({""text"": sample_info[""sentence2""]})\n\n        current_sample.text = processed_sentence[""text""]\n        if ""input_ids"" in processed_sentence:\n            current_sample.update(processed_sentence)\n\n        if self._use_features is True:\n            # Remove sentence id from end\n            identifier = sample_info[""Flikr30kID""].split(""."")[0]\n            # Load img0 and img1 features\n            sample_info[""feature_path""] = ""{}.npy"".format(identifier)\n            features = self.features_db[idx]\n            current_sample.update(features)\n\n        label = LABEL_TO_INT_MAPPING[sample_info[""gold_label""]]\n        current_sample.targets = torch.tensor(label, dtype=torch.long)\n\n        return current_sample\n'"
mmf/datasets/builders/visual_genome/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/builders/visual_genome/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport shutil\n\nfrom mmf.common.constants import VISUAL_GENOME_CONSTS\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.visual_genome.dataset import VisualGenomeDataset\nfrom mmf.datasets.builders.vqa2.builder import VQA2Builder\nfrom mmf.utils.download import decompress, download\nfrom mmf.utils.general import get_mmf_root\n\n\n@registry.register_builder(""visual_genome"")\nclass VisualGenomeBuilder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""visual_genome""\n        self.dataset_proper_name = ""Visual Genome""\n        self.dataset_class = VisualGenomeDataset\n        self.writer = registry.get(""writer"")\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/visual_genome/defaults.yaml""\n\n    def build(self, config, dataset_type):\n        self._dataset_type = dataset_type\n        self._config = config\n        data_folder = os.path.join(get_mmf_root(), self._config.data_dir)\n\n        # Since the imdb tar file contains all of the sets, we won\'t download them\n        # except in case of train\n        if self._dataset_type != ""train"":\n            return\n\n        self._download_and_extract_imdb(data_folder)\n        self._download_and_extract_features(data_folder)\n\n    def _download_and_extract_imdb(self, data_folder):\n        download_folder = os.path.join(data_folder, ""imdb"")\n        vocab_folder = os.path.join(data_folder, ""vocabs"")\n        vocab_file = os.path.join(vocab_folder, VISUAL_GENOME_CONSTS[""synset_file""])\n        os.makedirs(vocab_folder, exist_ok=True)\n\n        self._download_and_extract(\n            ""vocabs"", VISUAL_GENOME_CONSTS[""vocabs""], data_folder\n        )\n        extraction_folder = self._download_and_extract(\n            ""imdb_url"", VISUAL_GENOME_CONSTS[""imdb_url""], download_folder\n        )\n\n        if not os.path.exists(vocab_file):\n            shutil.move(\n                os.path.join(extraction_folder, VISUAL_GENOME_CONSTS[""synset_file""]),\n                vocab_file,\n            )\n\n    def _download_and_extract_features(self, data_folder):\n        self._download_and_extract(\n            ""features_url"", VISUAL_GENOME_CONSTS[""features_url""], data_folder\n        )\n\n    def _download_and_extract(self, key, url, download_folder):\n        file_type = key.split(""_"")[0]\n        os.makedirs(download_folder, exist_ok=True)\n        local_filename = url.split(""/"")[-1]\n        extraction_folder = os.path.join(download_folder, local_filename.split(""."")[0])\n        local_filename = os.path.join(download_folder, local_filename)\n\n        if (\n            os.path.exists(local_filename)\n            or (\n                os.path.exists(extraction_folder) and len(os.listdir(extraction_folder))\n            )\n            != 0\n        ):\n            self.writer.write(\n                ""{} {} already present. Skipping download."".format(\n                    self.dataset_proper_name, file_type\n                )\n            )\n            return extraction_folder\n\n        self.writer.write(\n            ""Downloading the {} {} now."".format(self.dataset_proper_name, file_type)\n        )\n        download(url, download_folder, url.split(""/"")[-1])\n\n        self.writer.write(\n            ""Extracting the {} {} now. This may take time"".format(\n                self.dataset_proper_name, file_type\n            )\n        )\n        decompress(download_folder, url.split(""/"")[-1])\n\n        return extraction_folder\n'"
mmf/datasets/builders/visual_genome/dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport copy\nimport json\n\nimport torch\n\nfrom mmf.common.sample import Sample, SampleList\nfrom mmf.datasets.builders.vqa2 import VQA2Dataset\nfrom mmf.datasets.databases.scene_graph_database import SceneGraphDatabase\n\n_CONSTANTS = {""image_id_key"": ""image_id""}\n\n\nclass VisualGenomeDataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config,\n            dataset_type,\n            imdb_file_index,\n            dataset_name=""visual_genome"",\n            *args,\n            **kwargs\n        )\n\n        self._return_scene_graph = config.return_scene_graph\n        self._return_objects = config.return_objects\n        self._return_relationships = config.return_relationships\n        self._no_unk = config.get(""no_unk"", False)\n        self.scene_graph_db = None\n\n        build_scene_graph_db = (\n            self._return_scene_graph\n            or self._return_objects\n            or self._return_relationships\n        )\n\n        if build_scene_graph_db:\n            scene_graph_file = config.scene_graph_files[dataset_type][imdb_file_index]\n            scene_graph_file = self._get_absolute_path(scene_graph_file)\n            self.scene_graph_db = SceneGraphDatabase(config, scene_graph_file)\n\n    def load_item(self, idx):\n        sample_info = self.imdb[idx]\n        sample_info = self._preprocess_answer(sample_info)\n        sample_info[""question_id""] = sample_info[""id""]\n        if self._check_unk(sample_info):\n            return self.load_item((idx + 1) % len(self.imdb))\n\n        current_sample = super().load_item(idx)\n        current_sample = self._load_scene_graph(idx, current_sample)\n\n        return current_sample\n\n    def _get_image_id(self, idx):\n        return self.imdb[idx][_CONSTANTS[""image_id_key""]]\n\n    def _get_image_info(self, idx):\n        # Deep copy so that we can directly update the nested dicts\n        return copy.deepcopy(self.scene_graph_db[self._get_image_id(idx)])\n\n    def _preprocess_answer(self, sample_info):\n        sample_info[""answers""] = [\n            self.vg_answer_preprocessor(\n                {""text"": sample_info[""answers""][0]},\n                remove=[""?"", "","", ""."", ""a"", ""an"", ""the""],\n            )[""text""]\n        ]\n\n        return sample_info\n\n    def _check_unk(self, sample_info):\n        if not self._no_unk:\n            return False\n        else:\n            index = self.answer_processor.word2idx(sample_info[""answers""][0])\n            return index == self.answer_processor.answer_vocab.UNK_INDEX\n\n    def _load_scene_graph(self, idx, sample):\n        if self.scene_graph_db is None:\n            return sample\n\n        image_info = self._get_image_info(idx)\n        regions = image_info[""regions""]\n\n        objects, object_map = self._load_objects(idx)\n\n        if self._return_objects:\n            sample.objects = objects\n\n        relationships, relationship_map = self._load_relationships(idx, object_map)\n\n        if self._return_relationships:\n            sample.relationships = relationships\n\n        regions, _ = self._load_regions(idx, object_map, relationship_map)\n\n        if self._return_scene_graph:\n            sample.scene_graph = regions\n\n        return sample\n\n    def _load_objects(self, idx):\n        image_info = self._get_image_info(idx)\n        image_height = image_info[""height""]\n        image_width = image_info[""width""]\n        object_map = {}\n        objects = []\n\n        for obj in image_info[""objects""]:\n            obj[""synsets""] = self.synset_processor({""tokens"": obj[""synsets""]})[""text""]\n            obj[""names""] = self.name_processor({""tokens"": obj[""names""]})[""text""]\n            obj[""height""] = obj[""h""] / image_height\n            obj.pop(""h"")\n            obj[""width""] = obj[""w""] / image_width\n            obj.pop(""w"")\n            obj[""y""] /= image_height\n            obj[""x""] /= image_width\n            obj[""attributes""] = self.attribute_processor({""tokens"": obj[""attributes""]})[\n                ""text""\n            ]\n            obj = Sample(obj)\n            object_map[obj[""object_id""]] = obj\n            objects.append(obj)\n        objects = SampleList(objects)\n\n        return objects, object_map\n\n    def _load_relationships(self, idx, object_map):\n        if self._return_relationships is None and self._return_scene_graph is None:\n            return None, None\n\n        image_info = self._get_image_info(idx)\n        relationship_map = {}\n        relationships = []\n\n        for relationship in image_info[""relationships""]:\n            relationship[""synsets""] = self.synset_processor(\n                {""tokens"": relationship[""synsets""]}\n            )[""text""]\n            relationship[""predicate""] = self.predicate_processor(\n                {""tokens"": relationship[""predicate""]}\n            )[""text""]\n            relationship[""object""] = object_map[relationship[""object_id""]]\n            relationship[""subject""] = object_map[relationship[""subject_id""]]\n\n            relationship = Sample(relationship)\n            relationship_map[relationship[""relationship_id""]] = relationship\n            relationships.append(relationship)\n\n        relationships = SampleList(relationships)\n        return relationships, relationship_map\n\n    def _load_regions(self, idx, object_map, relationship_map):\n        if self._return_scene_graph is None:\n            return None, None\n\n        image_info = self._get_image_info(idx)\n        image_height = image_info[""height""]\n        image_width = image_info[""width""]\n        region_map = {}\n        regions = []\n\n        for region in image_info[""regions""]:\n            for synset in region[""synsets""]:\n                synset[""entity_name""] = self.name_processor(\n                    {""tokens"": [synset[""entity_name""]]}\n                )[""text""]\n                synset[""synset_name""] = self.synset_processor(\n                    {""tokens"": [synset[""synset_name""]]}\n                )[""text""]\n\n            region[""height""] /= image_height\n            region[""width""] /= image_width\n            region[""y""] /= image_height\n            region[""x""] /= image_width\n\n            relationships = []\n            objects = []\n\n            for relationship_idx in region[""relationships""]:\n                relationships.append(relationship_map[relationship_idx])\n\n            for object_idx in region[""objects""]:\n                objects.append(object_map[object_idx])\n\n            region[""relationships""] = relationships\n            region[""objects""] = objects\n            region[""phrase""] = self.text_processor({""text"": region[""phrase""]})[""text""]\n\n            region = Sample(region)\n            region_map[region[""region_id""]] = region\n            regions.append(region)\n\n        regions = SampleList(regions)\n        return regions, region_map\n'"
mmf/datasets/builders/vizwiz/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom .builder import VizWizBuilder\nfrom .dataset import VizWizDataset\n\n__all__ = [""VizWizBuilder"", ""VizWizDataset""]\n'"
mmf/datasets/builders/vizwiz/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.vizwiz.dataset import VizWizDataset\nfrom mmf.datasets.builders.vqa2 import VQA2Builder\n\n\n@registry.register_builder(""vizwiz"")\nclass VizWizBuilder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""vizwiz""\n        self.set_dataset_class(VizWizDataset)\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/vizwiz/defaults.yaml""\n\n    def update_registry_for_model(self, config):\n        super().update_registry_for_model(config)\n'"
mmf/datasets/builders/vizwiz/dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport torch\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.vqa2 import VQA2Dataset\n\n\nclass VizWizDataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config,\n            dataset_type,\n            imdb_file_index,\n            dataset_name=""vizwiz"",\n            *args,\n            **kwargs\n        )\n\n    def load_item(self, idx):\n        sample = super().load_item(idx)\n\n        # sample_info = self.imdb[idx]\n\n        # if ""image_name"" in sample_info:\n        #     sample.image_id = sample_info[""image_name""]\n\n        return sample\n\n    def format_for_prediction(self, report):\n        answers = report.scores.argmax(dim=1)\n\n        predictions = []\n        answer_space_size = self.answer_processor.get_true_vocab_size()\n\n        for idx, image_id in enumerate(report.image_id):\n            answer_id = answers[idx].item()\n\n            if answer_id >= answer_space_size:\n                answer_id -= answer_space_size\n                answer = report.context_tokens[idx][answer_id]\n            else:\n                answer = self.answer_processor.idx2word(answer_id)\n            # if answer == self.context_processor.PAD_TOKEN:\n            #     answer = ""unanswerable""\n            if answer == ""<unk>"" or answer == ""<pad>"":\n                answer = ""unanswerable""\n            predictions.append(\n                {\n                    # ""image"": ""_"".join([""VizWiz""] + image_id.split(""_"")[2:]) + "".jpg"",\n                    ""image"": ""VizWiz_""\n                    + self._dataset_type\n                    + ""_""\n                    + str(image_id.item()).zfill(12)\n                    + "".jpg"",\n                    ""answer"": answer,\n                }\n            )\n\n        return predictions\n'"
mmf/datasets/builders/vqa2/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n__all__ = [""VQA2Builder"", ""VQA2Dataset""]\n\nfrom .builder import VQA2Builder\nfrom .dataset import VQA2Dataset\n'"
mmf/datasets/builders/vqa2/builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.vqa2.dataset import VQA2Dataset\nfrom mmf.datasets.mmf_dataset_builder import MMFDatasetBuilder\n\n\n@registry.register_builder(""vqa2"")\nclass VQA2Builder(MMFDatasetBuilder):\n    def __init__(self, dataset_name=""vqa2"", dataset_class=VQA2Dataset, *args, **kwargs):\n        super().__init__(dataset_name, dataset_class)\n        self.dataset_class = VQA2Dataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/vqa2/defaults.yaml""\n\n    # TODO: Deprecate this method and move configuration updates directly to processors\n    def update_registry_for_model(self, config):\n        if hasattr(self.dataset, ""text_processor""):\n            registry.register(\n                self.dataset_name + ""_text_vocab_size"",\n                self.dataset.text_processor.get_vocab_size(),\n            )\n        if hasattr(self.dataset, ""answer_processor""):\n            registry.register(\n                self.dataset_name + ""_num_final_outputs"",\n                self.dataset.answer_processor.get_vocab_size(),\n            )\n\n\n@registry.register_builder(""vqa2_train_val"")\nclass VQA2TrainValBuilder(VQA2Builder):\n    def __init__(self, dataset_name=""vqa2_train_val""):\n        super().__init__(dataset_name)\n\n    @classmethod\n    def config_path(self):\n        return ""configs/datasets/vqa2/train_val.yaml""\n'"
mmf/datasets/builders/vqa2/dataset.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\n\nimport torch\nimport tqdm\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.mmf_dataset import MMFDataset\nfrom mmf.utils.distributed import is_master\n\n\nclass VQA2Dataset(MMFDataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        if ""name"" in kwargs:\n            name = kwargs[""name""]\n        elif ""dataset_name"" in kwargs:\n            name = kwargs[""dataset_name""]\n        else:\n            name = ""vqa2""\n        super().__init__(name, config, dataset_type, index=imdb_file_index)\n\n        self._should_fast_read = self.config.get(""fast_read"", False)\n        self.use_ocr = self.config.use_ocr\n        self.use_ocr_info = self.config.use_ocr_info\n\n    def try_fast_read(self):\n        # Don\'t fast read in case of test set.\n        if self._dataset_type == ""test"":\n            return\n\n        if hasattr(self, ""_should_fast_read"") and self._should_fast_read is True:\n            self.writer.write(\n                ""Starting to fast read {} {} dataset"".format(\n                    self.dataset_name, self.dataset_type\n                )\n            )\n            self.cache = {}\n            for idx in tqdm.tqdm(\n                range(len(self.annotation_db)), miniters=100, disable=not is_master()\n            ):\n                self.cache[idx] = self.load_item(idx)\n\n    def __getitem__(self, idx):\n        if self._should_fast_read is True and self._dataset_type != ""test"":\n            return self.cache[idx]\n        else:\n            return self.load_item(idx)\n\n    def load_item(self, idx):\n        sample_info = self.annotation_db[idx]\n        current_sample = Sample()\n\n        if ""question_tokens"" in sample_info:\n            text_processor_argument = {\n                ""tokens"": sample_info[""question_tokens""],\n                ""text"": sample_info[""question_str""],\n            }\n        else:\n            text_processor_argument = {""text"": sample_info[""question""]}\n\n        processed_question = self.text_processor(text_processor_argument)\n\n        current_sample.text = processed_question[""text""]\n        if ""input_ids"" in processed_question:\n            current_sample.update(processed_question)\n\n        current_sample.question_id = torch.tensor(\n            sample_info[""question_id""], dtype=torch.int\n        )\n\n        if isinstance(sample_info[""image_id""], int):\n            current_sample.image_id = torch.tensor(\n                sample_info[""image_id""], dtype=torch.int\n            )\n        else:\n            current_sample.image_id = sample_info[""image_id""]\n\n        current_sample.text_len = torch.tensor(\n            len(sample_info[""question_tokens""]), dtype=torch.int\n        )\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            current_sample.update(features)\n\n        # Add details for OCR like OCR bbox, vectors, tokens here\n        current_sample = self.add_ocr_details(sample_info, current_sample)\n        # Depending on whether we are using soft copy this can add\n        # dynamic answer space\n        current_sample = self.add_answer_info(sample_info, current_sample)\n        return current_sample\n\n    def add_ocr_details(self, sample_info, sample):\n        if self.use_ocr:\n            # Preprocess OCR tokens\n            ocr_tokens = [\n                self.ocr_token_processor({""text"": token})[""text""]\n                for token in sample_info[""ocr_tokens""]\n            ]\n            # Get embeddings for tokens\n            context = self.context_processor({""tokens"": ocr_tokens})\n            sample.context = context[""text""]\n            sample.context_tokens = context[""tokens""]\n            sample.context_feature_0 = context[""text""]\n            sample.context_info_0 = Sample()\n            sample.context_info_0.max_features = context[""length""]\n\n            order_vectors = torch.eye(len(sample.context_tokens))\n            order_vectors[context[""length""] :] = 0\n            sample.order_vectors = order_vectors\n\n        if self.use_ocr_info and ""ocr_info"" in sample_info:\n            sample.ocr_bbox = self.bbox_processor({""info"": sample_info[""ocr_info""]})[\n                ""bbox""\n            ]\n\n        return sample\n\n    def add_answer_info(self, sample_info, sample):\n        if ""answers"" in sample_info:\n            answers = sample_info[""answers""]\n            answer_processor_arg = {""answers"": answers}\n\n            if self.use_ocr:\n                answer_processor_arg[""tokens""] = sample_info[""ocr_tokens""]\n            processed_soft_copy_answers = self.answer_processor(answer_processor_arg)\n\n            # sample.answers = processed_soft_copy_answers[""answers""]\n            sample.targets = processed_soft_copy_answers[""answers_scores""]\n\n        return sample\n\n    def idx_to_answer(self, idx):\n        return self.answer_processor.convert_idx_to_answer(idx)\n\n    def format_for_prediction(self, report):\n        answers = report.scores.argmax(dim=1)\n\n        predictions = []\n        answer_space_size = self.answer_processor.get_true_vocab_size()\n\n        for idx, question_id in enumerate(report.question_id):\n            answer_id = answers[idx].item()\n\n            if answer_id >= answer_space_size:\n                answer_id -= answer_space_size\n                answer = report.context_tokens[idx][answer_id]\n                if answer == self.context_processor.PAD_TOKEN:\n                    answer = ""unanswerable""\n            else:\n                answer = self.answer_processor.idx2word(answer_id)\n            # actual_answer = report.answers[idx]\n\n            predictions.append(\n                {\n                    ""question_id"": question_id.item(),\n                    ""answer"": answer,\n                    # ""actual_answers"": actual_answer,\n                    # ""question_tokens"": report.question_tokens[idx],\n                    # ""image_id"": report.image_id[idx].item()\n                }\n            )\n\n        return predictions\n'"
mmf/datasets/builders/vqa2/masked_builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.vqa2.builder import VQA2Builder\nfrom mmf.datasets.builders.vqa2.masked_dataset import MaskedVQA2Dataset\n\n\n@registry.register_builder(""masked_vqa2"")\nclass MaskedVQA2Builder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""masked_vqa2""\n        self.dataset_class = MaskedVQA2Dataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/vqa2/masked.yaml""\n'"
mmf/datasets/builders/vqa2/masked_dataset.py,0,"b'import random\n\nfrom mmf.common.sample import Sample\nfrom mmf.datasets.builders.vqa2.dataset import VQA2Dataset\n\n\nclass MaskedVQA2Dataset(VQA2Dataset):\n    def __init__(self, config, dataset_type, imdb_file_index, *args, **kwargs):\n        super().__init__(\n            config,\n            dataset_type,\n            imdb_file_index,\n            dataset_name=""masked_vqa2"",\n            *args,\n            **kwargs\n        )\n        self._add_answer = config.get(""add_answer"", True)\n\n    def load_item(self, idx):\n        sample_info = self.imdb[idx]\n        current_sample = Sample()\n\n        if self._use_features is True:\n            features = self.features_db[idx]\n            current_sample.update(features)\n            image_labels = []\n\n            for i in range(features[""image_feature_0""].shape[0]):\n                prob = random.random()\n                # mask token with 15% probability\n                if prob < 0.15:\n                    prob /= 0.15\n\n                    if prob < 0.9:\n                        features[""image_feature_0""][i] = 0\n                    image_labels.append(1)\n                else:\n                    # no masking token (will be ignored by loss function later)\n                    image_labels.append(-1)\n            item = {}\n            if self.config.get(""use_image_feature_masks"", False):\n                item[""image_labels""] = image_labels\n            current_sample.update(item)\n        current_sample = self._add_masked_question(sample_info, current_sample)\n\n        return current_sample\n\n    def _add_masked_question(self, sample_info, current_sample):\n        question = sample_info[""question_str""]\n        random_answer = random.choice(sample_info[""all_answers""])\n\n        processed = self.masked_token_processor(\n            {""text_a"": question, ""text_b"": random_answer, ""is_correct"": -1}\n        )\n\n        processed.pop(""tokens"")\n        current_sample.update(processed)\n\n        return current_sample\n'"
mmf/datasets/builders/vqa2/masked_q_vqa2_builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n# All rights reserved.\n#\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n#\n\n\nimport os\nimport warnings\n\nfrom mmf.common.registry import registry\nfrom mmf.datasets.builders.vqa2.builder import VQA2Builder\nfrom mmf.datasets.builders.vqa2.masked_q_vqa2_dataset import MaskedQVQA2Dataset\nfrom mmf.datasets.concat_dataset import MMFConcatDataset\n\n\n@registry.register_builder(""masked_q_vqa2"")\nclass MaskedQVQA2Builder(VQA2Builder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""masked_q_vqa2""\n        self.dataset_class = MaskedQVQA2Dataset\n\n    @classmethod\n    def config_path(cls):\n        return ""configs/datasets/vqa2/masked_q.yaml""\n'"
mmf/datasets/builders/vqa2/masked_q_vqa2_dataset.py,0,"b'import random\n\nfrom mmf.datasets.builders.vqa2.dataset import VQA2Dataset\n\n\nclass MaskedQVQA2Dataset(VQA2Dataset):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataset_name = ""masked_q_vqa2""\n\n    def add_answer_info(self, sample_info, current_sample):\n        length = min(len(current_sample.text), current_sample.text_len)\n        index = random.randint(0, length - 1)\n\n        word = self.text_processor.vocab.get_itos()[current_sample.text[index].item()]\n        current_sample.text[index] = self.text_processor.vocab.get_stoi()[""<mask>""]\n        answer_processor_arg = {""answers"": [word]}\n\n        processed_soft_copy_answers = self.answer_processor(answer_processor_arg)\n\n        current_sample.answers = processed_soft_copy_answers[""answers""]\n        current_sample.targets = processed_soft_copy_answers[""answers_scores""]\n\n        if self.answer_processor.word2idx(word) == self.answer_processor.word2idx(\n            ""<unk>""\n        ):\n            current_sample.targets.zero_()\n        return current_sample\n'"
mmf/datasets/builders/vqa2/ocr_builder.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.common.registry import Registry\nfrom mmf.datasets.builders.vizwiz import VizWizBuilder\nfrom mmf.datasets.builders.vqa2.ocr_dataset import VQA2OCRDataset\n\n\n@Registry.register_builder(""vqa2_ocr"")\nclass TextVQABuilder(VizWizBuilder):\n    def __init__(self):\n        super().__init__()\n        self.dataset_name = ""VQA2_OCR""\n        self.set_dataset_class(VQA2OCRDataset)\n\n    @classmethod\n    def config_path(self):\n        return None\n'"
mmf/datasets/builders/vqa2/ocr_dataset.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nfrom mmf.datasets.builders.vizwiz import VizWizDataset\nfrom mmf.utils.text import word_tokenize\n\n\nclass VQA2OCRDataset(VizWizDataset):\n    def __init__(self, imdb_file, image_feat_directories, verbose=False, **data_params):\n        super(VQA2OCRDataset, self).__init__(\n            imdb_file, image_feat_directories, verbose, **data_params\n        )\n        self.name = ""vqa2_ocr""\n\n    def format_for_prediction(self, batch, answers):\n        answers = answers.argmax(dim=1)\n\n        predictions = []\n        for idx, question_id in enumerate(batch[""question_id""]):\n            answer_id = answers[idx]\n\n            if answer_id >= self.answer_space_size:\n                answer_id -= self.answer_space_size\n                answer = word_tokenize(batch[""ocr_tokens""][answer_id][idx])\n            else:\n                answer = self.answer_dict.idx2word(answer_id)\n            predictions.append({""question_id"": question_id.item(), ""answer"": answer})\n\n        return predictions\n\n    def __getitem__(self, idx):\n        sample = super(VQA2OCRDataset, self).__getitem__(idx)\n\n        if sample[""question_id""] is None:\n            sample[""question_id""] = -1\n        return sample\n'"
mmf/datasets/databases/readers/__init__.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n'"
mmf/datasets/databases/readers/feature_readers.py,9,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport os\nimport pickle\n\nimport lmdb\nimport numpy as np\nimport torch\n\nfrom mmf.utils.file_io import PathManager\n\n\nclass FeatureReader:\n    def __init__(self, base_path, depth_first, max_features=None):\n        """"""Feature Reader class for reading features.\n\n        Note: Deprecation: ndim and image_feature will be deprecated later\n        and the format will be standardize using features from detectron.\n\n        Parameters\n        ----------\n        ndim : int\n            Number of expected dimensions in features\n        depth_first : bool\n            CHW vs HWC\n        max_features : int\n            Number of maximum bboxes to keep\n\n        Returns\n        -------\n        type\n            Description of returned object.\n\n        """"""\n        self.base_path = base_path\n        ndim = None\n        self.feat_reader = None\n        self.depth_first = depth_first\n        self.max_features = max_features\n        self.ndim = ndim\n\n    def _init_reader(self):\n        # Currently all lmdb features are with ndim == 2\n        if self.base_path.endswith("".lmdb""):\n            self.feat_reader = LMDBFeatureReader(self.max_features, self.base_path)\n        elif self.ndim == 2 or self.ndim == 0:\n            if self.max_features is None:\n                self.feat_reader = FasterRCNNFeatureReader()\n            else:\n                # TODO: Fix later when we move to proper standardized features\n                # if isinstance(self.image_feature.item(0), dict):\n                #     self.feat_reader = \\\n                #         PaddedFeatureRCNNWithBBoxesFeatureReader(\n                #             self.max_features\n                #         )\n                # else:\n                self.feat_reader = PaddedFasterRCNNFeatureReader(self.max_features)\n        elif self.ndim == 3 and not self.depth_first:\n            self.feat_reader = Dim3FeatureReader()\n        elif self.ndim == 4 and self.depth_first:\n            self.feat_reader = CHWFeatureReader()\n        elif self.ndim == 4 and not self.depth_first:\n            self.feat_reader = HWCFeatureReader()\n        else:\n            raise TypeError(""unknown image feature format"")\n\n    def read(self, image_feat_path):\n        if not image_feat_path.endswith(""npy""):\n            return None\n        image_feat_path = os.path.join(self.base_path, image_feat_path)\n\n        if self.feat_reader is None:\n            # Currently all lmdb features are with ndim == 2 so we are\n            # avoiding loading the lmdb to determine feature ndim\n            if not self.base_path.endswith("".lmdb"") and self.ndim is None:\n                feat = np.load(image_feat_path)\n                self.ndim = feat.ndim\n            self._init_reader()\n\n        return self.feat_reader.read(image_feat_path)\n\n\nclass FasterRCNNFeatureReader:\n    def read(self, image_feat_path):\n        return torch.from_numpy(np.load(image_feat_path)), None\n\n\nclass CHWFeatureReader:\n    def read(self, image_feat_path):\n        feat = np.load(image_feat_path)\n        assert feat.shape[0] == 1, ""batch is not 1""\n        feat = torch.from_numpy(feat.squeeze(0))\n        return feat, None\n\n\nclass Dim3FeatureReader:\n    def read(self, image_feat_path):\n        tmp = np.load(image_feat_path)\n        _, _, c_dim = tmp.shape\n        image_feature = torch.from_numpy(np.reshape(tmp, (-1, c_dim)))\n        return image_feature, None\n\n\nclass HWCFeatureReader:\n    def read(self, image_feat_path):\n        tmp = np.load(image_feat_path)\n        assert tmp.shape[0] == 1, ""batch is not 1""\n        _, _, _, c_dim = tmp.shape\n        image_feature = torch.from_numpy(np.reshape(tmp, (-1, c_dim)))\n        return image_feature, None\n\n\nclass PaddedFasterRCNNFeatureReader:\n    def __init__(self, max_loc):\n        self.max_loc = max_loc\n        self.first = True\n        self.take_item = False\n\n    def _load(self, image_feat_path):\n        image_info = {}\n        image_info[""features""] = np.load(image_feat_path, allow_pickle=True)\n\n        info_path = ""{}_info.npy"".format(image_feat_path.split("".npy"")[0])\n        if PathManager.exists(info_path):\n            image_info.update(np.load(info_path, allow_pickle=True).item())\n\n        return image_info\n\n    def read(self, image_feat_path):\n        image_info = self._load(image_feat_path)\n        if self.first:\n            self.first = False\n            if (\n                image_info[""features""].size == 1\n                and ""image_feat"" in image_info[""features""].item()\n            ):\n                self.take_item = True\n\n        image_feature = image_info[""features""]\n\n        if self.take_item:\n            item = image_info[""features""].item()\n            if ""image_text"" in item:\n                image_info[""image_text""] = item[""image_text""]\n                image_info[""is_ocr""] = item[""image_bbox_source""]\n                image_feature = item[""image_feat""]\n\n            if ""info"" in item:\n                if ""image_text"" in item[""info""]:\n                    image_info.update(item[""info""])\n                image_feature = item[""feature""]\n\n        # Handle the case of ResNet152 features\n        if len(image_feature.shape) > 2:\n            shape = image_feature.shape\n            image_feature = image_feature.reshape(-1, shape[-1])\n\n        image_loc, image_dim = image_feature.shape\n        tmp_image_feat = np.zeros((self.max_loc, image_dim), dtype=np.float32)\n        tmp_image_feat[0:image_loc,] = image_feature[: self.max_loc, :]  # noqa\n        image_feature = torch.from_numpy(tmp_image_feat)\n\n        del image_info[""features""]\n        image_info[""max_features""] = torch.tensor(image_loc, dtype=torch.long)\n        return image_feature, image_info\n\n\nclass LMDBFeatureReader(PaddedFasterRCNNFeatureReader):\n    def __init__(self, max_loc, base_path):\n        super().__init__(max_loc)\n        self.db_path = base_path\n\n        if not os.path.exists(self.db_path):\n            raise RuntimeError(\n                ""{} path specified for LMDB features doesn\'t exists."".format(\n                    self.db_path\n                )\n            )\n        self.env = None\n\n    def _init_db(self):\n        self.env = lmdb.open(\n            self.db_path,\n            subdir=os.path.isdir(self.db_path),\n            readonly=True,\n            lock=False,\n            readahead=False,\n            meminit=False,\n        )\n        with self.env.begin(write=False, buffers=True) as txn:\n            self.image_ids = pickle.loads(txn.get(b""keys""))\n            self.image_id_indices = {\n                self.image_ids[i]: i for i in range(0, len(self.image_ids))\n            }\n\n    def _load(self, image_file_path):\n        if self.env is None:\n            self._init_db()\n\n        split = os.path.relpath(image_file_path, self.db_path).split("".npy"")[0]\n\n        try:\n            image_id = int(split.split(""_"")[-1])\n            # Try fetching to see if it actually exists otherwise fall back to\n            # default\n            img_id_idx = self.image_id_indices[str(image_id).encode()]\n        except (ValueError, KeyError):\n            # The image id is complex or involves folder, use it directly\n            image_id = str(split).encode()\n            img_id_idx = self.image_id_indices[image_id]\n\n        with self.env.begin(write=False, buffers=True) as txn:\n            image_info = pickle.loads(txn.get(self.image_ids[img_id_idx]))\n\n        return image_info\n\n\nclass PaddedFeatureRCNNWithBBoxesFeatureReader:\n    def __init__(self, max_loc):\n        self.max_loc = max_loc\n\n    def read(self, image_feat_path):\n        image_feat_bbox = np.load(image_feat_path)\n        image_boxes = image_feat_bbox.item().get(""image_bboxes"")\n        tmp_image_feat = image_feat_bbox.item().get(""image_feature"")\n        image_loc, image_dim = tmp_image_feat.shape\n        tmp_image_feat_2 = np.zeros((self.max_loc, image_dim), dtype=np.float32)\n        tmp_image_feat_2[0:image_loc,] = tmp_image_feat  # noqa\n        tmp_image_feat_2 = torch.from_numpy(tmp_image_feat_2)\n        tmp_image_box = np.zeros((self.max_loc, 4), dtype=np.int32)\n        tmp_image_box[0:image_loc] = image_boxes\n        tmp_image_box = torch.from_numpy(tmp_image_box)\n        image_info = {\n            ""image_bbox"": tmp_image_box,\n            ""max_features"": torch.tensor(image_loc, dtype=torch.int),\n        }\n\n        return tmp_image_feat_2, image_info\n'"
mmf/datasets/builders/visual_dialog/scripts/build_imdb.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport argparse\nimport glob\nimport json\nimport os\n\nfrom mmf.utils.preprocessing import text_tokenize\n\n\nclass IMDBBuilder:\n    def __init__(self):\n        self.args = self.get_args()\n\n    def get_args(self):\n        parser = argparse.ArgumentParser(""Build IMDB for VisDial"")\n        parser.add_argument(\n            ""-o"",\n            ""--out_file"",\n            type=str,\n            default=""./imdb.npy"",\n            help=""Output file for IMDB"",\n        )\n        parser.add_argument(\n            ""-i"",\n            ""--image_root"",\n            type=str,\n            default=""./COCO"",\n            help=""Image directory for COCO"",\n        )\n        parser.add_argument(\n            ""-v"", ""--version"", type=float, default=0.9, help=""Visdial version""\n        )\n        parser.add_argument(\n            ""-d"",\n            ""--data_dir"",\n            type=str,\n            default=""./visdial"",\n            help=""Directory which contains visdial jsons"",\n        )\n        parser.add_argument(\n            ""-s"",\n            ""--set_type"",\n            type=str,\n            default=""train"",\n            help=""Dataset type train|val|test"",\n        )\n\n        return parser.parse_args()\n\n    def get_id_to_path_dict(self):\n        id2path = {}\n        globs = glob.iglob(os.path.join(self.args.image_root, ""*"", ""*.npy""))\n        # NOTE: based on assumption that image_id is unique across all splits\n        for image_path in globs:\n            path = ""/"".join(image_path.split(""/"")[-2:])\n            image_id = int(image_path[-16:-4])\n            id2path[image_id] = path\n\n        return id2path\n\n    def build(self):\n        visdial_json_file = os.path.join(\n            self.args.data_dir,\n            ""visdial_%.1f_%s.json"" % (self.args.version, self.args.set_type),\n        )\n        data = None\n\n        with open(visdial_json_file, ""r"") as f:\n            data = json.load(f)[""data""]\n\n        final_questions = self.get_tokens(data[""questions""])\n        final_answers = self.get_tokens(data[""answers""])\n        dialogs = data[""dialogs""]\n\n        dialogs_with_features = self.parse_dialogs(dialogs)\n\n        imdb = {\n            ""questions"": final_questions,\n            ""answers"": final_answers,\n            ""dialogs"": dialogs_with_features,\n        }\n\n        self.save_imdb(imdb)\n\n    def save_imdb(self, imdb):\n        with open(self.args.out_file, ""w"") as f:\n            json.dump(imdb, f)\n\n    def get_tokens(self, sentences):\n        if not isinstance(sentences, list):\n            sentences = [sentences]\n        final_sentences = []\n        for _, sentence in enumerate(sentences):\n            tokens = text_tokenize(sentence)\n            final_sentences.append(tokens)\n\n        return final_sentences\n\n    def parse_dialogs(self, dialogs):\n        id2path = self.get_id_to_path_dict()\n\n        for dialog in dialogs:\n            image_id = dialog[""image_id""]\n            image_feature_path = id2path[image_id]\n            dialog[""image_feature_path""] = image_feature_path\n            dialog[""caption""] = self.get_tokens(dialog[""caption""])\n\n        return dialogs\n\n\nif __name__ == ""__main__"":\n    imdb_builder = IMDBBuilder()\n    imdb_builder.build()\n'"
mmf/datasets/builders/visual_dialog/scripts/extract_vocabulary.py,0,"b'# Copyright (c) Facebook, Inc. and its affiliates.\nimport json\n\nfrom mmf.scripts.extract_vocabulary import ExtractVocabulary\n\n\nclass ExtractVisdialVocabulary(ExtractVocabulary):\n    def __init__(self):\n        super(ExtractVisdialVocabulary, self).__init__()\n\n    def get_text(self):\n        text = []\n\n        for input_file in self.input_files:\n            with open(input_file, ""r"") as f:\n                f_json = json.load(f)\n                # Add \'questions\' from visdial\n                text += f_json[""data""][""questions""]\n                # Add \'answers\' from visdial\n                text += f_json[""data""][""answers""]\n\n                for dialog in f_json[""data""][""dialogs""]:\n                    text += [dialog[""caption""]]\n        return text\n\n\nif __name__ == ""__main__"":\n    extractor = ExtractVisdialVocabulary()\n    extractor.extract()\n'"
