file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\nsetup(\n    name='wavetorch',\n    version='0.2.1',\n    description='Numerically solving and backpropagating through the wave equation using pytorch',\n    url='https://github.com/fancompute/wavetorch',\n    author='Ian Williamson, Tyler Hughes, Momchil Minkov, Shanhui Fan',\n    author_email='ian.williamson@ieee.org',\n    license='MIT',\n    packages=['wavetorch'],\n    scripts=[],\n    zip_safe=False)\n"""
study/optimize_lens.py,21,"b'""""""Optimize a toy lens model\n""""""\nimport torch\nimport wavetorch\nimport numpy as np\nimport skimage\nimport matplotlib.pyplot as plt\nimport librosa\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--use_vowel\', action=\'store_true\')\nargs = parser.parse_args()\n\ndomain_shape = (151, 151)\n\ndt = 0.707\nh  = 1.0\n\nsr = 10000\n\ndomain = torch.zeros(domain_shape)\nrr, cc = skimage.draw.circle( int(domain_shape[0]/2) , int(domain_shape[1]/2), 30)\ndomain[rr, cc] = 0.5\n\ngeom  = wavetorch.WaveGeometryFreeForm(domain_shape, h, c0=1.0, c1=0.5, rho=domain, design_region=None)\ncell  = wavetorch.WaveCell(dt, geom)\n# src   = wavetorch.WaveSource(25, 75) # Point source\nsrc   = wavetorch.WaveLineSource(25, 50, 25, 100) # Line source\nprobe = [wavetorch.WaveIntensityProbe(125, 100),\n         wavetorch.WaveIntensityProbe(125, 75),\n         wavetorch.WaveIntensityProbe(125, 50)]\n\nmodel = wavetorch.WaveRNN(cell, src, probe)\n\n# Define the source\nif args.use_vowel:\n    x, _, _ = wavetorch.data.load_all_vowels(\n        [\'ae\', \'ei\', \'iy\'],\n        gender=\'men\', \n        sr=sr, \n        normalize=True, \n        max_samples=3)\n    X = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n    X = X[0,1000:3500].unsqueeze(0)\nelse:\n    t = np.arange(0, 500*dt, dt)\n    omega1 = 2*np.pi*1/dt/15\n    X = np.sin(omega1*t) * t / (1 + t)\n    X = torch.tensor(X, dtype=torch.get_default_dtype()).unsqueeze(0)\n\n###\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1.5e-3)\ncriterion = torch.nn.CrossEntropyLoss()\n\nbeta_schedule       = torch.tensor([100, 400, 800, 1000, 1500, 2000])\nbeta_schedule_epoch = torch.tensor([-1,  10,  20,  30,  40, 50])\n\nloss_iter = []\nfor i in range(0, 60):\n    model.cell.geom.beta = beta_schedule[beta_schedule_epoch<i][-1]\n\n    def closure():\n        optimizer.zero_grad()\n        u = wavetorch.utils.normalize_power(model(X).sum(dim=1))\n        loss = criterion(u, torch.tensor([2]))\n        loss.backward()\n        return loss\n\n    loss = optimizer.step(closure)\n    model.cell.geom.constrain_to_design_region()\n    print(""Epoch: {} -- Loss: {}"".format(i, loss))\n    loss_iter.append(loss.item())\n\nplt.figure()\nplt.plot(loss_iter, \'o-\')\nplt.xlabel(""Epoch"")\nplt.ylabel(""Cross entropy loss"")\n\nwith torch.no_grad():\n    u = model(X, output_fields=True)\n\nNshots = 6\nNtime  = X.shape[1]\ntimes = [i for i in range(int(Ntime/Nshots), Ntime, int(Ntime/Nshots))]\nwavetorch.plot.field_snapshot(\n    model,\n    u, \n    times, \n    ylabel=None, \n    label=True, \n    cbar=True, \n    Ny=2,\n    fig_width=7)\n\nwavetorch.plot.geometry(model)\n'"
study/propagate.py,19,"b'""""""Propagate some waves through a domain\n""""""\n\nimport torch\nimport wavetorch\nimport numpy as np\nimport skimage\nimport matplotlib.pyplot as plt\nimport librosa\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--show_fields\', \'-fields\', \'-f\', action=\'store_true\')\nparser.add_argument(\'--use_vowel\', action=\'store_true\')\nargs = parser.parse_args()\n\ndomain_shape = (151, 151)\n\ndt = 0.707\nh  = 1.0\n\nsr = 10000\n\ndomain = torch.zeros(domain_shape)\nrr, cc = skimage.draw.circle( int(domain_shape[0]/2) , int(domain_shape[1]/2), 30)\ndomain[rr, cc] = 1\n\ngeom  = wavetorch.WaveGeometryFreeForm(domain_shape, h, c0=1.0, c1=0.5, rho=domain)\ncell  = wavetorch.WaveCell(dt, geom)\n# src   = wavetorch.WaveSource(25, 75) # Point source\nsrc   = wavetorch.WaveLineSource(25, 50, 25, 100) # Line source\nprobe = [wavetorch.WaveIntensityProbe(125, 100),\n         wavetorch.WaveIntensityProbe(125, 75),\n         wavetorch.WaveIntensityProbe(125, 50)]\n\nmodel = wavetorch.WaveRNN(cell, src, probe)\n\n# Define the source\nif args.use_vowel:\n    x, _, _ = wavetorch.data.load_all_vowels(\n        [\'ae\', \'ei\', \'iy\'],\n        gender=\'men\', \n        sr=sr, \n        normalize=True, \n        max_samples=3)\n    X = torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n    X = X[0,1000:3500].unsqueeze(0)\nelse:\n    t = np.arange(0, 500*dt, dt)\n    omega1 = 2*np.pi*1/dt/15\n    X = np.sin(omega1*t) * t / (1 + t)\n    X = torch.tensor(X, dtype=torch.get_default_dtype()).unsqueeze(0)\n\nwith torch.no_grad():\n    u = model.forward(X, output_fields=args.show_fields)\n\nif args.show_fields:\n    Nshots = 10\n    Ntime  = X.shape[1]\n    times = [i for i in range(int(Ntime/Nshots), Ntime, int(Ntime/Nshots))]\n    wavetorch.plot.field_snapshot(\n        model,\n        u, \n        times, \n        ylabel=None, \n        label=True, \n        cbar=True, \n        Ny=3,\n        fig_width=10)\n\n# torch.save(model.state_dict(), \'./tmp.pt\')\n# new_cell  = wavetorch.WaveCell(1.0, None)\n# new_src   = wavetorch.WaveSource(0, 0)\n# newprobe = [wavetorch.WaveIntensityProbe(0, 0)]\n# model = wavetorch.WaveRNN(new_cell, new_src, new_probe)\n'"
study/vowel_analyze.py,23,"b'""""""Perform various analysis tasks on a saved model.\n""""""\n\nimport torch\nimport wavetorch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport argparse\nimport yaml\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib as mpl\n\nimport pandas as pd\n\nimport librosa\nimport librosa.display\n\nCOL_TRAIN = ""#1f77b4""\nCOL_TEST  = ""#2ca02c""\n\nparser = argparse.ArgumentParser() \nparser.add_argument(\'command\', type=str)\nparser.add_argument(\'filename\', type=str)\nparser.add_argument(\'--times\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--saveprefix\', type=str, default=None)\nparser.add_argument(\'--vowel_samples\', nargs=\'+\', type=int, default=None)\nparser.add_argument(\'--num_threads\', type=int, default=4)\nparser.add_argument(\'--labels\', action=\'store_true\')\nparser.add_argument(\'--use-cuda\', action=\'store_true\')\n\nclass WaveTorch(object):\n\n    def __init__(self):\n        args = parser.parse_args()\n\n        if args.use_cuda and torch.cuda.is_available():\n            args.dev = torch.device(\'cuda\')\n        else:\n            args.dev = torch.device(\'cpu\')\n\n        torch.set_num_threads(args.num_threads)\n\n        if not hasattr(self, args.command):\n            print(\'Unrecognized command\')\n            parser.print_help()\n            exit(1)\n\n        getattr(self, args.command)(args)\n\n    def fields(self, args):\n        model, history, history_state, cfg = wavetorch.io.load_model(args.filename)\n\n        print(""Configuration for model in %s is:"" % args.filename)\n        print(yaml.dump(cfg, default_flow_style=False))\n\n        sr = cfg[\'data\'][\'sr\']\n        gender = cfg[\'data\'][\'gender\']\n        vowels = cfg[\'data\'][\'vowels\']\n        N_classes = len(vowels)\n\n        X, Y, F = wavetorch.data.load_all_vowels(vowels, gender=\'both\', sr=sr, normalize=True, random_state=0)\n\n        # fig, axs = plt.subplots(N_classes, 1, constrained_layout=True, figsize=(4, 3), sharex=True, sharey=True)\n        fig, axs = plt.subplots(N_classes, len(args.times), constrained_layout=True, figsize=(6.5, 6.5), sharex=True, sharey=True)\n        fig2, axs2 = plt.subplots(3, 2, constrained_layout=True, figsize=(3.7, 2))\n        for i in range(N_classes):\n            xb, yb = wavetorch.data.select_vowel_sample(X, Y, F, i, ind=args.vowel_samples[i] if args.vowel_samples is not None else None)\n            with torch.no_grad():\n                fields = model.forward(xb, output_fields=True)\n                # wavetorch.plot.probe_integrals(model, fields, yb, xb, ax=axs2)\n                wavetorch.plot.field_snapshot(model, fields, args.times, yb, fig_width=6, block=False, axs=axs[i,:])\n                axs[i,0].text(-0.05, 0.5, vowels[i] + \' vowel\', transform=axs[i,0].transAxes, ha=""right"", va=""center"")\n                # axs[i].set_ylabel(r""Probe $\\int \\vert u_n \\vert^2 dt$"")\n\n        # axs[-1].set_xlabel(""Time"")\n        if args.labels:\n            wavetorch.plot.apply_sublabels(axs.ravel(), xy=[(5,-5)], size=\'medium\', weight=\'bold\', ha=\'left\', va=\'top\')\n        plt.show()\n\n    # def stft(self, args):\n    #     model, history, history_state, cfg = wavetorch.utils.load_model(args.filename)\n\n    #     print(""Configuration for model in %s is:"" % args.filename)\n    #     print(yaml.dump(cfg, default_flow_style=False))\n\n    #     sr = cfg[\'data\'][\'sr\']\n    #     gender = cfg[\'data\'][\'gender\']\n    #     vowels = cfg[\'data\'][\'vowels\']\n    #     N_classes = len(vowels)\n\n    #     X, Y, F = wavetorch.data.load_all_vowels(vowels, gender=\'both\', sr=sr, normalize=True, random_state=0)\n\n    #     fig, axs = plt.subplots(N_classes, N_classes+1, constrained_layout=True, figsize=(4.5*(N_classes+1)/N_classes,4.5), sharex=True, sharey=True)\n\n    #     for i in range(N_classes):\n    #         xb, yb = wavetorch.data.select_vowel_sample(X, Y, F, i, ind=args.vowel_samples[i] if args.vowel_samples is not None else None)\n    #         with torch.no_grad():\n    #             j = yb.argmax().item()\n    #             ax = axs[j, 0]\n    #             ax.set_facecolor(\'black\')\n\n    #             model.output_probe = torch.tensor(True)\n    #             field_dist = model(xb)\n    #             probe_series = field_dist\n\n    #             input_stft = np.abs(librosa.stft(xb.numpy().squeeze(), n_fft=256))\n\n    #             librosa.display.specshow(\n    #                 librosa.amplitude_to_db(input_stft,ref=np.max(input_stft)),\n    #                 sr=sr,\n    #                 vmax=0,\n    #                 ax=ax,\n    #                 vmin=-50,\n    #                 y_axis=\'linear\',\n    #                 x_axis=\'time\',\n    #                 cmap=plt.cm.inferno\n    #             )\n    #             ax.set_ylim([0,sr/2])\n    #             if j == 0:\n    #                 ax.set_title(""Input signal"")\n\n    #             for k in range(1, probe_series.shape[1]+1):\n    #                 ax = axs[j, k]\n                    \n    #                 output_stft = np.abs(librosa.stft(probe_series[:,k-1].numpy(), n_fft=256))\n\n    #                 librosa.display.specshow(\n    #                     librosa.amplitude_to_db(output_stft,ref=np.max(input_stft)),\n    #                     sr=sr,\n    #                     vmax=0,\n    #                     ax=ax,\n    #                     vmin=-50,\n    #                     y_axis=\'linear\',\n    #                     x_axis=\'time\',\n    #                     cmap=plt.cm.inferno\n    #                 )\n    #                 ax.set_ylim([0,sr/2])\n\n    #                 if j == 0:\n    #                     ax.set_title(""Output probe %d"" % (k))\n    #                 if k == 1:\n    #                     ax.text(-0.3, 0.5, vowels[j] + \' vowel\', transform=ax.transAxes, ha=""right"", va=""center"")\n                    \n    #                 if k > 0:\n    #                     ax.set_ylabel(\'\')\n    #                 if j < N_classes-1:\n    #                     ax.set_xlabel(\'\')\n    #                 # if j == k:\n    #                     # ax.text(0.5, 0.95, \'%s at probe #%d\' % (vowels[j], k+1), color=""w"", transform=ax.transAxes, ha=""center"", va=""top"", fontsize=""large"")\n    #     plt.show()\n\n    def animate(self, args):\n        model, history, history_state, cfg = wavetorch.utils.load_model(args.filename)\n\n        print(""Configuration for model in %s is:"" % args.filename)\n        print(yaml.dump(cfg, default_flow_style=False))\n\n        X, Y, F = wavetorch.data.load_all_vowels(cfg[\'data\'][\'vowels\'], gender=cfg[\'data\'][\'gender\'], sr=cfg[\'data\'][\'sr\'], normalize=True, random_state=0)\n\n        model.load_state_dict(history_state[cfg[\'training\'][\'N_epochs\']])\n\n        for i in range(len(cfg[\'data\'][\'vowels\'])):\n            xb, yb = wavetorch.data.select_vowel_sample(X, Y, F, i, ind=args.vowel_samples[i] if args.vowel_samples is not None else None)\n            with torch.no_grad():\n                this_savename = None if args.saveprefix is None else args.saveprefix + str(i) + \'.mp4\'\n                model.output_probe = torch.tensor(False)\n                field_dist = model(xb)\n                wavetorch.plot.animate_fields(model, field_dist, yb, filename=this_savename, interval=1)\n\nif __name__ == \'__main__\':\n    WaveTorch()\n'"
study/vowel_helpers.py,0,"b'\nfrom wavetorch import WaveSource, WaveIntensityProbe\n\ndef setup_src_coords(src_x, src_y, Nx, Ny, Npml):\n    if (src_x is not None) and (src_y is not None):\n        # Coordinate are specified\n        return [WaveSource(src_x, src_y)]\n    else:\n        # Center at left\n        return [WaveSource(Npml + 20, int(Ny / 2))]\n\n\ndef setup_probe_coords(N_classes, px, py, pd, Nx, Ny, Npml):\n    if (py is not None) and (px is not None):\n        # All probe coordinate are specified\n        assert len(px) == len(py), ""Length of px and py must match""\n\n        return [WaveIntensityProbe(px[j], py[j]) for j in range(0, len(px))]\n\n    if (py is None) and (pd is not None):\n        # Center the probe array in y\n        span = (N_classes - 1) * pd\n        y0 = int((Ny - span) / 2)\n        assert y0 > Npml, ""Bottom element of array is inside the PML""\n        y = [y0 + i * pd for i in range(N_classes)]\n\n        if px is not None:\n            assert len(px) == 1, ""If py is not specified then px must be of length 1""\n            x = [px[0] for i in range(N_classes)]\n        else:\n            x = [Nx - Npml - 20 for i in range(N_classes)]\n\n        return [WaveIntensityProbe(x[j], y[j]) for j in range(0, len(x))]\n\n    raise ValueError(""px = {}, py = {}, pd = {} is an invalid probe configuration"".format(pd))\n'"
study/vowel_spectrum.py,2,"b'""""""Generate plot of the mean vowel sample spectra\n""""""\n\nimport torch\nimport wavetorch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport argparse\nimport yaml\nimport time\nimport os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib.gridspec import GridSpec\n\nimport pandas as pd\n\nimport librosa\n\ntry:\n    from helpers.plot import mpl_set_latex\n    mpl_set_latex()\nexcept ImportError:\n    import warnings\n    warnings.warn(\'The helpers package is unavailable\', ImportWarning)\n\nn_fft = 2048\nsr = 10000\nvowels = [\'ae\', \'ei\', \'iy\']\ncolors = [\'#fcaf3e\', \'#ad7fa8\', \'#ef2929\']\n\n# vowels = [\'ae\', \'eh\', \'ih\', \'oo\', \'ah\', \'ei\', \'iy\', \'uh\', \'aw\', \'er\', \'oa\', \'uw\']\n\ngender = \'both\'\n\nfig,ax=plt.subplots(1,1,constrained_layout=True, figsize=(3.5,2.75))\n\nfor i, vowel in enumerate(vowels):\n    X, _, _ = wavetorch.data.load_all_vowels([vowel], gender=gender, sr=sr)\n    X_ft = [np.abs(librosa.core.stft(Xi.numpy(),n_fft=n_fft)) for Xi in X]\n\n    X_ft_int = np.vstack([Xi.sum(axis=1) for Xi in X_ft])\n\n    X_ft_mean = np.mean(X_ft_int,axis=0)\n    X_ft_std = np.std(X_ft_int,axis=0)\n\n    ax.fill_between(librosa.core.fft_frequencies(sr=sr, n_fft=n_fft),\n                     X_ft_mean,\n                     alpha=0.30, color=colors[i], edgecolor=""none"", zorder=i ,lw=0)\n    ax.plot(librosa.core.fft_frequencies(sr=sr, n_fft=n_fft),\n                     X_ft_mean,\n                     color=colors[i],zorder=i, label=vowel + \' vowel class\', lw=1.0)\n    # ax.plot(librosa.core.fft_frequencies(sr=sr, n_fft=n_fft),\n    #                  X_ft_std, \'-\',\n    #                  label=vowel + \' vowel class\', color=colors[i], lw=1, zorder=i)\n\n# ax.set_xlim([0,5000])\n# ax.set_ylim([0,13])\nax.set_xlabel(""Frequency (Hz)"")\nax.set_ylabel(""Mean energy spectrum (a.u.)"")\nax.legend()\nplt.show(block=False)\n\n\n'"
study/vowel_summary.py,12,"b'""""""Generate a summary of a previously trained vowel recognition model.\n""""""\n\nimport torch\nimport wavetorch\n\nimport argparse\nimport yaml\nimport os\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\ntry:\n    from helpers.plot import mpl_set_latex\n    mpl_set_latex()\nexcept ImportError:\n    import warnings\n    warnings.warn(\'The helpers package is unavailable\', ImportWarning)\n\nCOL_TRAIN = ""#1f77b4""\nCOL_TEST  = ""#2ca02c""\n\nparser = argparse.ArgumentParser() \nparser.add_argument(\'filename\', type=str)\nparser.add_argument(\'--vmin\', type=float, default=1e-3)\nparser.add_argument(\'--vmax\', type=float, default=1.0)\nparser.add_argument(\'--fig\', type=str, default=None)\nparser.add_argument(\'--title_off\', action=\'store_true\')\nparser.add_argument(\'--labels\', action=\'store_true\')\nparser.add_argument(\'--vowel_samples\', nargs=\'+\', type=int, default=None)\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    model, history, history_state, cfg = wavetorch.io.load_model(args.filename)\n\n    try:\n        if cfg[\'seed\'] is not None:\n            torch.manual_seed(cfg[\'seed\'])\n    except:\n        pass\n    print(""Configuration for model in %s is:"" % args.filename)\n    print(yaml.dump(cfg, default_flow_style=False))\n\n    sr = cfg[\'data\'][\'sr\']\n    gender = cfg[\'data\'][\'gender\']\n    vowels = cfg[\'data\'][\'vowels\']\n    N_classes = len(vowels)\n\n    fig = plt.figure( figsize=(7, 4.75), constrained_layout=True)\n\n    gs = fig.add_gridspec(1, 2, width_ratios=[1, 0.4])\n    gs_left  = gs[0].subgridspec(3, 2)\n    gs_right = gs[1].subgridspec(N_classes+1, 1, height_ratios=[1 for i in range(0,N_classes)] + [0.05])\n    gs_bot  = gs_left[2,:].subgridspec(1, 2)\n\n    ax_cm_train0 = fig.add_subplot(gs_left[0,0])\n    ax_cm_test0  = fig.add_subplot(gs_left[0,1])\n\n    ax_cm_train1 = fig.add_subplot(gs_left[1,0])\n    ax_cm_test1  = fig.add_subplot(gs_left[1,1])\n\n    ax_loss = fig.add_subplot(gs_bot[0])\n    ax_acc = fig.add_subplot(gs_bot[1])\n\n    ax_fields = [fig.add_subplot(gs_right[i]) for i in range(0, N_classes+1)] \n\n    history_mean = history.groupby(\'epoch\').mean()\n    history_std  = history.groupby(\'epoch\').std()\n\n    epochs = history_mean.index.values\n\n    ax_loss.fill_between(epochs,\n                         history_mean[\'loss_train\'].values-history_std[\'loss_train\'].values,\n                         history_mean[\'loss_train\'].values+history_std[\'loss_train\'].values, color=COL_TRAIN, alpha=0.15)\n    ax_loss.plot(epochs, history_mean[\'loss_train\'].values, ""-"", label=""Training dataset"", ms=4, color=COL_TRAIN)\n    ax_loss.fill_between(epochs,\n                         history_mean[\'loss_test\'].values-history_std[\'loss_test\'].values,\n                         history_mean[\'loss_test\'].values+history_std[\'loss_test\'].values, color=COL_TEST, alpha=0.15)\n    ax_loss.plot(epochs, history_mean[\'loss_test\'].values, ""-"", label=""Testing dataset"", ms=4, color=COL_TEST)\n    ax_loss.set_ylabel(\'Loss\')\n    ax_loss.set_xlabel(\'Training epoch \\#\')\n\n    ax_acc.plot(epochs, history_mean[\'acc_train\'].values*100, ""-"", label=""Training dataset"", ms=4, color=COL_TRAIN)\n    ax_acc.fill_between(epochs,\n                        history_mean[\'acc_train\'].values*100-history_std[\'acc_train\'].values*100,\n                        history_mean[\'acc_train\'].values*100+history_std[\'acc_train\'].values*100, color=COL_TRAIN, alpha=0.15)\n    ax_acc.plot(epochs, history_mean[\'acc_test\'].values*100, ""-"", label=""Testing dataset"", ms=4, color=COL_TEST)\n    ax_acc.fill_between(epochs,\n                        history_mean[\'acc_test\'].values*100-history_std[\'acc_test\'].values*100,\n                        history_mean[\'acc_test\'].values*100+history_std[\'acc_test\'].values*100, color=COL_TEST, alpha=0.15)\n    ax_acc.set_xlabel(\'Training epoch \\#\')\n    ax_acc.set_ylabel(\'Accuracy\')\n    \n    ax_acc.yaxis.set_major_locator(mpl.ticker.MultipleLocator(base=10))\n    # ax_acc.set_ylim([20,100])\n    ax_loss.yaxis.set_major_locator(mpl.ticker.MultipleLocator(base=0.1))\n    # ax_loss.set_ylim([0.7,1.2])\n\n    ax_acc.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(\'%.0f\\%%\'))\n\n    ax_loss.legend(fontsize=\'small\')\n\n    # ax_acc.annotate(""%.1f%% training set accuracy"" % (history_mean[\'acc_train\'].tail(1).iloc[0]*100), xy=(0.1,0.1), xytext=(0,10), textcoords=""offset points"",  xycoords=""axes fraction"", ha=""left"", va=""bottom"", color=COL_TRAIN)\n    # ax_acc.annotate(""%.1f%% testing set accuracy"" % (history_mean[\'acc_test\'].tail(1).iloc[0]*100), xy=(0.1,0.1), xycoords=""axes fraction"", ha=""left"", va=""bottom"", color=COL_TEST)\n    ax_acc.annotate(\'%.1f\\%%\' % (history_mean[\'acc_train\'].tail(1).iloc[0]*100),\n                    xy=(epochs[-1], history_mean[\'acc_train\'].tail(1).iloc[0]*100), xycoords=\'data\',\n                    xytext=(-1, 5), textcoords=\'offset points\', ha=\'left\', va=\'center\', fontsize=\'small\',\n                    color=COL_TRAIN, bbox=wavetorch.plot.bbox_white)\n    ax_acc.annotate(\'%.1f\\%%\' % (history_mean[\'acc_test\'].tail(1).iloc[0]*100),\n                    xy=(epochs[-1], history_mean[\'acc_test\'].tail(1).iloc[0]*100), xycoords=\'data\',\n                    xytext=(-1, -5), textcoords=\'offset points\', ha=\'left\', va=\'center\', fontsize=\'small\',\n                    color=COL_TEST, bbox=wavetorch.plot.bbox_white)\n    print(\'Accuracy (train): %.1f%% +/- %.1f%%\' % (history_mean[\'acc_train\'].tail(1).iloc[0]*100, history_std[\'acc_train\'].tail(1).iloc[0]*100))\n    print(\'Accuracy  (test): %.1f%% +/- %.1f%%\' % (history_mean[\'acc_test\'].tail(1).iloc[0]*100, history_std[\'acc_test\'].tail(1).iloc[0]*100))\n\n    cm_train = history.groupby(\'epoch\')[\'cm_train\'].apply(np.mean).head(1).iloc[0]\n    cm_test = history.groupby(\'epoch\')[\'cm_test\'].apply(np.mean).head(1).iloc[0]\n    wavetorch.plot.confusion_matrix(cm_train, title=""Training dataset"", normalize=True, ax=ax_cm_train0, labels=vowels)\n    wavetorch.plot.confusion_matrix(cm_test, title=""Testing dataset"", normalize=True, ax=ax_cm_test0, labels=vowels)\n\n    cm_train = history.groupby(\'epoch\')[\'cm_train\'].apply(np.mean).tail(1).iloc[0]\n    cm_test = history.groupby(\'epoch\')[\'cm_test\'].apply(np.mean).tail(1).iloc[0]\n    wavetorch.plot.confusion_matrix(cm_train, title=""Training dataset"", normalize=True, ax=ax_cm_train1, labels=vowels)\n    wavetorch.plot.confusion_matrix(cm_test, title=""Testing dataset"", normalize=True, ax=ax_cm_test1, labels=vowels)\n\n    X, Y, F = wavetorch.data.load_all_vowels(vowels, gender=\'both\', sr=sr, random_state=0)\n\n    # model.load_state_dict(history_state[cfg[\'training\'][\'N_epochs\']])\n    for i in range(N_classes):\n        xb, yb = wavetorch.data.select_vowel_sample(X, Y, F, i, ind=args.vowel_samples[i] if args.vowel_samples is not None else None)\n        with torch.no_grad():\n            field_dist = model(xb, output_fields=True)\n            wavetorch.plot.total_field(model, field_dist, yb, ax=ax_fields[yb.argmax().item()], cbar=True, cax=ax_fields[-1], vmin=args.vmin, vmax=args.vmax)\n\n    if args.labels:\n        try:\n            from helpers.plot import apply_panel_labels\n            apply_panel_labels([ax_cm_train0, ax_cm_test0, ax_cm_train1, ax_cm_test1, ax_loss, ax_acc] + ax_fields[0:-1],\n                                xy=[(-35,0), (-35,0), (-35,0), (-35,0), (-25,0), (-40,0), (8,-6), (8,-6), (8,-6)],\n                                color=[\'k\', \'k\', \'k\', \'k\', \'k\', \'k\', \'w\', \'w\', \'w\'],\n                                case=\'upper\')\n        except ImportError:\n            import warnings\n            warnings.warn(\'The helpers package is unavailable\', ImportWarning)\n    \n\n    plt.show()\n    if args.fig is not None:\n        fig.savefig(args.fig, dpi=300)\n    else:\n        fig.savefig(os.path.splitext(args.filename)[0]+""_summary.png"", dpi=300)\n'"
study/vowel_train.py,22,"b'""""""Perform vowel recognition training.\n""""""\n\nimport torch\nimport wavetorch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport argparse\nimport time\n\nfrom yaml import load, dump\ntry:\n    from yaml import CLoader as Loader, CDumper as Dumper\nexcept ImportError:\n    from yaml import Loader, Dumper\n\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom vowel_helpers import setup_src_coords, setup_probe_coords\n\nparser = argparse.ArgumentParser() \nparser.add_argument(\'config\', type=str, \n                    help=\'Configuration file for geometry, training, and data preparation\')\nparser.add_argument(\'--num_threads\', type=int, default=4,\n                    help=\'Number of threads to use\')\nparser.add_argument(\'--use-cuda\', action=\'store_true\',\n                    help=\'Use CUDA to perform computations\')\nparser.add_argument(\'--name\', type=str, default=time.strftime(\'%Y%m%d%H%M%S\'),\n                    help=\'Name to use when saving or loading the model file. If not specified when saving a time and date stamp is used\')\nparser.add_argument(\'--savedir\', type=str, default=\'./study/\',\n                    help=\'Directory in which the model file is saved. Defaults to ./study/\')\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    if args.use_cuda and torch.cuda.is_available():\n        args.dev = torch.device(\'cuda\')\n    else:\n        args.dev = torch.device(\'cpu\')\n\n    torch.set_num_threads(args.num_threads)\n\n    print(""Configuration: %s"" % args.config)\n    with open(args.config, \'r\') as ymlfile:\n        cfg = load(ymlfile, Loader=Loader)\n\n    wavetorch.utils.set_dtype(cfg[\'dtype\'])\n    \n    if cfg[\'seed\'] is not None:\n        torch.manual_seed(cfg[\'seed\'])\n\n    if cfg[\'training\'][\'prefix\'] is not None:\n        args.name = cfg[\'training\'][\'prefix\'] + \'_\' + args.name\n\n    N_classes = len(cfg[\'data\'][\'vowels\'])\n\n    X, Y, _ = wavetorch.data.load_all_vowels(cfg[\'data\'][\'vowels\'], gender=cfg[\'data\'][\'gender\'], sr=cfg[\'data\'][\'sr\'], normalize=True, max_samples=cfg[\'training\'][\'max_samples\'], random_state=cfg[\'seed\'])\n\n    skf = StratifiedKFold(n_splits=cfg[\'training\'][\'N_folds\'], random_state=cfg[\'seed\'], shuffle=True)\n    samps = [y.argmax().item() for y in Y]\n\n    history = None\n    history_model_state = []\n    for num, (train_index, test_index) in enumerate(skf.split(np.zeros(len(samps)), samps)):\n        if cfg[\'training\'][\'cross_validation\']: print(""Cross Validation Fold %2d/%2d"" % (num+1, cfg[\'training\'][\'N_folds\']))\n\n        if cfg[\'data\'][\'window_size\']:\n            x_train = torch.nn.utils.rnn.pad_sequence([wavetorch.utils.window_data(X[i], cfg[\'data\'][\'window_size\']) for i in train_index], batch_first=True)\n        else:\n            x_train = torch.nn.utils.rnn.pad_sequence([X[i] for i in train_index], batch_first=True)\n\n        x_test = torch.nn.utils.rnn.pad_sequence([X[i] for i in test_index], batch_first=True)\n        y_train = torch.nn.utils.rnn.pad_sequence([Y[i] for i in train_index], batch_first=True)\n        y_test = torch.nn.utils.rnn.pad_sequence([Y[i] for i in test_index], batch_first=True)\n\n        x_train = x_train.to(args.dev)\n        x_test  = x_test.to(args.dev)\n        y_train = y_train.to(args.dev)\n        y_test  = y_test.to(args.dev)\n\n        train_ds = TensorDataset(x_train, y_train)\n        test_ds  = TensorDataset(x_test, y_test)\n\n        train_dl = DataLoader(train_ds, batch_size=cfg[\'training\'][\'batch_size\'], shuffle=True)\n        test_dl  = DataLoader(test_ds, batch_size=cfg[\'training\'][\'batch_size\'])\n\n        ### Define model\n        probes = setup_probe_coords(\n                            N_classes, cfg[\'geom\'][\'px\'], cfg[\'geom\'][\'py\'], cfg[\'geom\'][\'pd\'], \n                            cfg[\'geom\'][\'Nx\'], cfg[\'geom\'][\'Ny\'], cfg[\'geom\'][\'pml\'][\'N\']\n                            )\n        source = setup_src_coords(\n                            cfg[\'geom\'][\'src_x\'], cfg[\'geom\'][\'src_y\'], cfg[\'geom\'][\'Nx\'],\n                            cfg[\'geom\'][\'Ny\'], cfg[\'geom\'][\'pml\'][\'N\']\n                            )\n\n        design_region = torch.zeros(cfg[\'geom\'][\'Nx\'], cfg[\'geom\'][\'Ny\'], dtype=torch.uint8)\n        design_region[source[0].x.item()+5:probes[0].x.item()-5] = 1\n\n        geom  = wavetorch.WaveGeometryFreeForm((cfg[\'geom\'][\'Nx\'], cfg[\'geom\'][\'Ny\']), cfg[\'geom\'][\'h\'],             \n            c0=cfg[\'geom\'][\'c0\'], \n            c1=cfg[\'geom\'][\'c1\'],\n            eta=cfg[\'geom\'][\'binarization\'][\'eta\'],\n            beta=cfg[\'geom\'][\'binarization\'][\'beta\'],\n            abs_sig=cfg[\'geom\'][\'pml\'][\'max\'], \n            abs_N=cfg[\'geom\'][\'pml\'][\'N\'], \n            abs_p=cfg[\'geom\'][\'pml\'][\'p\'],\n            rho=cfg[\'geom\'][\'init\'],\n            blur_radius=cfg[\'geom\'][\'blur_radius\'],\n            blur_N=cfg[\'geom\'][\'blur_N\'],\n            design_region=design_region\n        )\n\n        cell  = wavetorch.WaveCell(cfg[\'geom\'][\'dt\'], geom,\n            satdamp_b0=cfg[\'geom\'][\'nonlinearity\'][\'b0\'],\n            satdamp_uth=cfg[\'geom\'][\'nonlinearity\'][\'uth\'],\n            c_nl=cfg[\'geom\'][\'nonlinearity\'][\'cnl\']\n        )\n\n        model = wavetorch.WaveRNN(cell, source, probes)\n        model.to(args.dev)\n\n        ### Train\n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\'training\'][\'lr\'])\n        criterion = torch.nn.CrossEntropyLoss()\n        \n        model.train()\n\n        history, history_model_state = wavetorch.train(\n                                            model,\n                                            optimizer,\n                                            criterion, \n                                            train_dl, \n                                            test_dl, \n                                            cfg[\'training\'][\'N_epochs\'], \n                                            cfg[\'training\'][\'batch_size\'], \n                                            history=history,\n                                            history_model_state=history_model_state,\n                                            fold=num if cfg[\'training\'][\'cross_validation\'] else -1,\n                                            name=args.name,\n                                            savedir=args.savedir,\n                                            accuracy=wavetorch.utils.accuracy_onehot,\n                                            cfg=cfg)\n        \n        wavetorch.io.save_model(model, args.name, args.savedir, history, history_model_state, cfg)\n\n        if not cfg[\'training\'][\'cross_validation\']:\n            break\n'"
study/vowel_train_sklearn.py,15,"b'""""""Perform vowel recognition training.\n\nThis script is a **work-in-progress** of trying to get training to work with skorch / scikit-learn\n\n""""""\n\nimport torch\nimport wavetorch\n\nimport argparse\nimport yaml\nimport time\n\nimport numpy as np\nimport sklearn\nimport skorch\n\nclass CroppedDataset(torch.utils.data.dataset.Dataset):\n    def __init__(self, dataset, indices):\n        self.dataset = dataset\n        self.indices = indices\n\n    def __getitem__(self, idx):\n        return self.dataset[self.indices[idx]]\n\n    def __len__(self):\n        return len(self.indices)\n\n# TODO: move this into lib\nclass ClipDesignRegion(skorch.callbacks.Callback):\n    def on_batch_end(self, net, Xi=None, yi=None, training=None, **kwargs):\n        if training:\n            net.module_.clip_to_design_region()\n\nparser = argparse.ArgumentParser() \nparser.add_argument(\'config\', type=str, \n                    help=\'Configuration file for geometry, training, and data preparation\')\nparser.add_argument(\'--num_threads\', type=int, default=4,\n                    help=\'Number of threads to use\')\nparser.add_argument(\'--use-cuda\', action=\'store_true\',\n                    help=\'Use CUDA to perform computations\')\nparser.add_argument(\'--name\', type=str, default=time.strftime(\'%Y%m%d%H%M%S\'),\n                    help=\'Name to use when saving or loading the model file. If not specified when saving a time and date stamp is used\')\nparser.add_argument(\'--savedir\', type=str, default=\'./study/\',\n                    help=\'Directory in which the model file is saved. Defaults to ./study/\')\n\nargs = parser.parse_args()\n\nif args.use_cuda and torch.cuda.is_available():\n    args.dev = torch.device(\'cuda\')\nelse:\n    args.dev = torch.device(\'cpu\')\n\ntorch.set_num_threads(args.num_threads)\n\nprint(""Using configuration from %s: "" % args.config)\nwith open(args.config, \'r\') as ymlfile:\n     cfg = yaml.load(ymlfile)\n     print(yaml.dump(cfg, default_flow_style=False))\n\nif cfg[\'seed\'] is not None:\n    torch.manual_seed(cfg[\'seed\'])\n\nif cfg[\'training\'][\'prefix\'] is not None:\n    args.name = cfg[\'training\'][\'prefix\'] + \'_\' + args.name\n\nN_classes = len(cfg[\'data\'][\'vowels\'])\n\n### Define the geometry\nprobes = wavetorch.utils.setup_probe_coords(\n                    N_classes, cfg[\'geom\'][\'px\'], cfg[\'geom\'][\'py\'], cfg[\'geom\'][\'pd\'], \n                    cfg[\'geom\'][\'Nx\'], cfg[\'geom\'][\'Ny\'], cfg[\'geom\'][\'pml\'][\'N\']\n                    )\nsource = wavetorch.utils.setup_src_coords(\n                    cfg[\'geom\'][\'src_x\'], cfg[\'geom\'][\'src_y\'], cfg[\'geom\'][\'Nx\'],\n                    cfg[\'geom\'][\'Ny\'], cfg[\'geom\'][\'pml\'][\'N\']\n                    )\n\ndesign_region = torch.zeros(cfg[\'geom\'][\'Nx\'], cfg[\'geom\'][\'Ny\'], dtype=torch.uint8)\ndesign_region[source[0].x.item()+5:probes[0].x.item()-5] = 1\n\ndef my_train_split(ds, y):\n    return ds, skorch.dataset.Dataset(corpus.valid[:200], y=None)\n\n### Perform training\nnet = skorch.NeuralNetClassifier(\n    module=wavetorch.WaveCell,\n\n    # Training configuration\n    max_epochs=cfg[\'training\'][\'N_epochs\'],\n    batch_size=cfg[\'training\'][\'batch_size\'],\n    lr=cfg[\'training\'][\'lr\'],\n    # train_split=skorch.dataset.CVSplit(cfg[\'training\'][\'N_folds\'], stratified=True, random_state=cfg[\'seed\']),\n    optimizer=torch.optim.Adam,\n    criterion=torch.nn.CrossEntropyLoss,\n    callbacks=[\n        ClipDesignRegion,\n        skorch.callbacks.EpochScoring(\'accuracy\', lower_is_better=False, on_train=True, name=\'train_acc\'),\n        skorch.callbacks.Checkpoint(monitor=None, fn_prefix=\'1234_\', dirname=\'test\', f_params=""params_{last_epoch[epoch]}.pt"", f_optimizer=\'optimizer.pt\', f_history=\'history.json\')\n        ],\n    callbacks__print_log__keys_ignored=None,\n    train_split=None,\n\n    # These al get passed as options to WaveCell\n    module__Nx=cfg[\'geom\'][\'Nx\'],\n    module__Ny=cfg[\'geom\'][\'Ny\'],\n    module__h=cfg[\'geom\'][\'h\'],\n    module__dt=cfg[\'geom\'][\'dt\'],\n    module__init=cfg[\'geom\'][\'init\'], \n    module__c0=cfg[\'geom\'][\'c0\'], \n    module__c1=cfg[\'geom\'][\'c1\'], \n    module__sigma=cfg[\'geom\'][\'pml\'][\'max\'], \n    module__N=cfg[\'geom\'][\'pml\'][\'N\'], \n    module__p=cfg[\'geom\'][\'pml\'][\'p\'],\n    module__design_region=design_region,\n    module__output_probe=True,\n    module__probes=probes,\n    module__sources=source\n    )\n\nX, Y, _ = wavetorch.data.load_all_vowels(cfg[\'data\'][\'vowels\'], gender=cfg[\'data\'][\'gender\'], sr=cfg[\'data\'][\'sr\'], normalize=True, max_samples=cfg[\'training\'][\'max_samples\'], random_state=cfg[\'seed\'])\nX = torch.nn.utils.rnn.pad_sequence(X).numpy().transpose()\nY = torch.nn.utils.rnn.pad_sequence(Y).argmax(dim=0, keepdim=False).numpy().transpose()\n\n# TODO(ian): Need to implement cropping of the training samples inside the data loader\n# x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.25, random_state=42)\nif cfg[\'data\'][\'window_size\']:\n    t_mid = int(X.shape[1]/2)\n    t_half_window = int(cfg[\'data\'][\'window_size\']/2)\n    X = X[:, (t_mid-t_half_window):(t_mid+t_half_window)]\n\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\ny_pred = cross_validate(net, X, Y, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=None))\n\n# model = net.fit(X, Y)\n'"
wavetorch/__init__.py,0,"b'from . import rnn, cell, geom, source, probe, data, plot, utils, io\nfrom .cell import WaveCell\nfrom .geom import WaveGeometryHoley, WaveGeometryFreeForm\nfrom .probe import WaveProbe, WaveIntensityProbe\nfrom .rnn import WaveRNN\nfrom .source import WaveSource, WaveLineSource\nfrom .train import train\n\n__all__ = [""WaveCell"", ""WaveGeometryHoley"", ""WaveGeometryFreeForm"", ""WaveProbe"", ""WaveIntensityProbe"", ""WaveRNN"",\n\t\t   ""WaveSource"", ""WaveLineSource""]\n\n__version__ = ""0.2.1""\n'"
wavetorch/cell.py,6,"b'import numpy as np\nimport torch\n\nfrom .operators import _laplacian\nfrom .utils import to_tensor\n\n\ndef saturable_damping(u, uth, b0):\n    return b0 / (1 + torch.abs(u / uth).pow(2))\n\n\ndef _time_step(b, c, y1, y2, dt, h):\n    y = torch.mul((dt**-2 + b * dt**-1).pow(-1),\n                  (2 / dt**2 * y1 - torch.mul((dt**-2 - b * dt**-1), y2)\n                   + torch.mul(c.pow(2), _laplacian(y1, h)))\n                  )\n    return y\n\n\nclass TimeStep(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, b, c, y1, y2, dt, h):\n        ctx.save_for_backward(b, c, y1, y2, dt, h)\n        return _time_step(b, c, y1, y2, dt, h)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        b, c, y1, y2, dt, h = ctx.saved_tensors\n\n        grad_b = grad_c = grad_y1 = grad_y2 = grad_dt = grad_h = None\n\n        if ctx.needs_input_grad[0]:\n            grad_b = - (dt * b + 1).pow(-2) * dt * (\n                        c.pow(2) * dt**(2) * _laplacian(y1, h) + 2 * y1 - 2 * y2) * grad_output\n        if ctx.needs_input_grad[1]:\n            grad_c = (b * dt + 1).pow(-1) * (2 * c * dt**(2) * _laplacian(y1, h)) * grad_output\n        if ctx.needs_input_grad[2]:\n            # grad_y1 = ( dt.pow(2) * _laplacian(c.pow(2) *grad_output, h) + 2*grad_output) * (b*dt + 1).pow(-1)\n            c2_grad = (b * dt + 1)**(-1) * c.pow(2) * grad_output\n            grad_y1 = dt**(2) * _laplacian(c2_grad, h) + 2 * grad_output * (b * dt + 1).pow(-1)\n        if ctx.needs_input_grad[3]:\n            grad_y2 = (b * dt - 1) * (b * dt + 1).pow(-1) * grad_output\n\n        return grad_b, grad_c, grad_y1, grad_y2, grad_dt, grad_h\n\n\nclass WaveCell(torch.nn.Module):\n    """"""The recurrent neural network cell implementing the scalar wave equation""""""\n\n    def __init__(self,\n                 dt : float,\n                 geometry,\n                 satdamp_b0 : float = 0.0,\n                 satdamp_uth : float = 0.0,\n                 c_nl : float = 0.0):\n\n        super().__init__()\n\n        # Set values\n        self.register_buffer(""dt"", to_tensor(dt))\n        self.geom = geometry\n        self.register_buffer(""satdamp_b0"", to_tensor(satdamp_b0))\n        self.register_buffer(""satdamp_uth"", to_tensor(satdamp_uth))\n        self.register_buffer(""c_nl"", to_tensor(c_nl))\n\n        # Validate inputs\n        cmax = self.geom.cmax\n        h = self.geom.h\n\n        if dt > 1 / cmax * h / np.sqrt(2):\n            raise ValueError(\n                \'The spatial discretization defined by the geometry `h = %f` and the temporal discretization defined by the model `dt = %f` do not satisfy the CFL stability criteria\' % (\n                h, dt))\n\n    def parameters(self, recursive=True):\n        for param in self.geom.parameters():\n            yield param\n\n    def forward(self, h1, h2, c_linear, rho):\n        """"""Take a step through time\n\n        Parameters\n        ----------\n        h1 : \n            Scalar wave field one time step ago (part of the hidden state)\n        h2 : \n            Scalar wave field two time steps ago (part of the hidden state)\n        c_linear :\n            Scalar wave speed distribution (this gets passed in to avoid generating it on each time step, saving memory for backprop)\n        rho : \n            Projected density, required for nonlinear response (this gets passed in to avoid generating it on each time step, saving memory for backprop)\n        """"""\n\n        if self.satdamp_b0 > 0:\n            b = self.geom.b + rho * saturable_damping(h1, uth=self.satdamp_uth, b0=self.satdamp_b0)\n        else:\n            b = self.geom.b\n\n        if self.c_nl != 0:\n            c = c_linear + rho * self.c_nl * h1.pow(2)\n        else:\n            c = c_linear\n\n        y = TimeStep.apply(b, c, h1, h2, self.dt, self.geom.h)\n        # y = _time_step(b, c, h1, h2, dt, h)\n\n        return y, h1\n'"
wavetorch/geom.py,34,"b'from copy import deepcopy\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.nn.functional import conv2d\nfrom skimage.draw import circle\n\nfrom .utils import to_tensor\n\nclass WaveGeometry(torch.nn.Module):\n\tdef __init__(self, domain_shape: Tuple, h: float, c0: float, c1: float, abs_N: int = 20, abs_sig: float = 11,\n\t\t\t\t abs_p: float = 4.0):\n\t\tsuper().__init__()\n\n\t\tassert len(\n\t\t\tdomain_shape) == 2, ""len(domain_shape) must be equal to 2: only two-dimensional (2D) domains are supported""\n\n\t\tself.domain_shape = domain_shape\n\t\tself.register_buffer(""h"", to_tensor(h))\n\t\tself.register_buffer(""c0"", to_tensor(c0))\n\t\tself.register_buffer(""c1"", to_tensor(c1))\n\n\t\tself.register_buffer(""abs_N"", to_tensor(abs_N, dtype=torch.uint8))\n\t\tself.register_buffer(""abs_sig"", to_tensor(abs_sig))\n\t\tself.register_buffer(""abs_p"", to_tensor(abs_p, dtype=torch.uint8))\n\n\t\tself._init_b(abs_N, abs_sig, abs_p)\n\n\tdef state_reconstruction_args(self):\n\t\treturn {""domain_shape"": self.domain_shape,\n\t\t\t\t""h"": self.h.item(),\n\t\t\t\t""c0"": self.c0.item(),\n\t\t\t\t""c1"": self.c1.item(),\n\t\t\t\t""abs_N"": self.abs_N.item(),\n\t\t\t\t""abs_sig"": self.abs_sig.item(),\n\t\t\t\t""abs_p"": self.abs_p.item()}\n\n\tdef __repr__(self):\n\t\treturn ""WaveGeometry shape={}, h={}"".format(self.domain_shape, self.h)\n\n\tdef forward(self):\n\t\traise NotImplementedError(""WaveGeometry forward() is not implemented. "" \\\n\t\t\t\t\t\t\t\t  ""Although WaveGeometry is a subclass of a torch.nn.Module, its forward() method should never be called. "" \\\n\t\t\t\t\t\t\t\t  ""It only exists as a torch.nn.Module to hook into pytorch as a component of a WaveCell."")\n\n\t@property\n\tdef c(self):\n\t\traise NotImplementedError\n\n\t@property\n\tdef b(self):\n\t\treturn self._b\n\n\t@property\n\tdef cmax(self):\n\t\t""""""Helper function for getting the maximum wave speed for calculating CFL""""""\n\t\treturn np.max([self.c0.item(), self.c1.item()])\n\n\tdef constrain_to_design_region(self):\n\t\tpass\n\n\tdef _init_b(self, abs_N: int, abs_sig: float, abs_p: float):\n\t\t""""""Initialize the distribution of the damping parameter for the PML""""""\n\n\t\tNx, Ny = self.domain_shape\n\n\t\tassert Nx > 2 * abs_N + 1, ""The domain isn\'t large enough in the x-direction to fit absorbing layer. Nx = {} and N = {}"".format(\n\t\t\tNx, abs_N)\n\t\tassert Ny > 2 * abs_N + 1, ""The domain isn\'t large enough in the y-direction to fit absorbing layer. Ny = {} and N = {}"".format(\n\t\t\tNy, abs_N)\n\n\t\tb_vals = abs_sig * torch.linspace(0.0, 1.0, abs_N + 1) ** abs_p\n\n\t\tb_x = torch.zeros(Nx, Ny)\n\t\tb_y = torch.zeros(Nx, Ny)\n\n\t\tif abs_N > 0:\n\t\t\tb_x[0:abs_N + 1, :] = torch.flip(b_vals, [0]).repeat(Ny, 1).transpose(0, 1)\n\t\t\tb_x[(Nx - abs_N - 1):Nx, :] = b_vals.repeat(Ny, 1).transpose(0, 1)\n\n\t\t\tb_y[:, 0:abs_N + 1] = torch.flip(b_vals, [0]).repeat(Nx, 1)\n\t\t\tb_y[:, (Ny - abs_N - 1):Ny] = b_vals.repeat(Nx, 1)\n\n\t\tself.register_buffer(""_b"", torch.sqrt(b_x ** 2 + b_y ** 2))\n\n\nclass WaveGeometryHoley(WaveGeometry):\n\tdef __init__(self, domain_shape: Tuple, h: float, c0: float, c1: float, abs_N: int = 20, abs_sig: float = 11,\n\t\t\t\t abs_p: float = 4.0, eta: float = 0.5, beta: float = 100.0, x = None, y = None, r = None):\n\n\t\tsuper().__init__(domain_shape, h, c0, c1, abs_N, abs_sig, abs_p)\n\n\t\tself.x = torch.nn.Parameter(to_tensor(x))\n\t\tself.y = torch.nn.Parameter(to_tensor(y))\n\t\tself.r = torch.nn.Parameter(to_tensor(r))\n\n\t\tself.register_buffer(""eta"", to_tensor(eta))\n\t\tself.register_buffer(""beta"", to_tensor(beta))\n\n\tdef state_reconstruction_args(self):\n\t\tmy_args = {""eta"": self.eta.item(),\n\t\t\t\t   ""beta"": self.beta.item(),\n\t\t\t\t   ""x"": deepcopy(self.x.detach()),\n\t\t\t\t   ""y"": deepcopy(self.y.detach()),\n\t\t\t\t   ""r"": deepcopy(self.r.detach())}\n\t\treturn {**super().state_reconstruction_args(), **my_args}\n\n\tdef _rho(self):\n\t\teta = self.eta.item()\n\t\tbeta = self.beta.item()\n\n\t\txv = torch.arange(0, self.domain_shape[0], dtype=torch.get_default_dtype())\n\t\tyv = torch.arange(0, self.domain_shape[1], dtype=torch.get_default_dtype())\n\t\tx, y = torch.meshgrid(xv, yv)\n\n\t\trho = torch.zeros(self.domain_shape)\n\n\t\tfor i, (ri, xi, yi) in enumerate(zip(self.r, self.x, self.y)):\n\t\t\tr = torch.sqrt((x-xi).pow(2) + (y-yi).pow(2))\n\t\t\trho = rho + torch.exp(-r/ri)\n\n\t\treturn (np.tanh(beta * eta) + torch.tanh(beta * (rho - eta))) / (\n\t\t\t\tnp.tanh(beta * eta) + np.tanh(beta * (1 - eta)))\n\n\t@property\n\tdef rho(self):\n\t\treturn self._rho()\n\n\t@property\n\tdef c(self):\n\t\treturn self.c0.item() + (self.c1.item() - self.c0.item()) * self._rho()\n\n\n\nclass WaveGeometryFreeForm(WaveGeometry):\n\tdef __init__(self, domain_shape: Tuple, h: float, c0: float, c1: float, abs_N: int = 20, abs_sig: float = 11,\n\t\t\t\t abs_p: float = 4.0, eta: float = 0.5, beta: float = 100.0, design_region=None, rho=\'half\',\n\t\t\t\t blur_radius : int = 1, blur_N : int = 1):\n\n\t\tsuper().__init__(domain_shape, h, c0, c1, abs_N, abs_sig, abs_p)\n\n\t\tself.register_buffer(""eta"", to_tensor(eta))\n\t\tself.register_buffer(""beta"", to_tensor(beta))\n\n\t\tself._init_design_region(design_region, domain_shape)\n\t\tself._init_rho(rho, domain_shape)\n\n\t\trr, cc = circle(blur_radius, blur_radius, blur_radius+1)\n\t\tblur_kernel = torch.zeros((2*blur_radius+1, 2*blur_radius+1), dtype=torch.get_default_dtype())\n\t\tblur_kernel[rr, cc] = 1\n\t\tblur_kernel=blur_kernel/blur_kernel.sum()\n\n\t\tself.register_buffer(""blur_kernel"", blur_kernel.unsqueeze(0).unsqueeze(0))\n\t\tself.register_buffer(""blur_N"", to_tensor(blur_N, dtype=torch.int))\n\t\tself.register_buffer(""blur_radius"", to_tensor(blur_N, dtype=torch.int))\n\n\t\tself.constrain_to_design_region()\n\n\tdef state_reconstruction_args(self):\n\t\tmy_args = {""eta"": self.eta.item(),\n\t\t\t\t   ""beta"": self.beta.item(),\n\t\t\t\t   ""design_region"": deepcopy(self.design_region),\n\t\t\t\t   ""rho"": deepcopy(self.rho.detach()),\n\t\t\t\t   ""blur_radius"": self.blur_radius.item(),\n\t\t\t\t   ""blur_N"": self.blur_N.item()}\n\t\treturn {**super().state_reconstruction_args(), **my_args}\n\n\tdef __repr__(self):\n\t\treturn super().__repr__() + "", "" + str(self.design_region.sum().item()) + "" DOFs""\n\n\tdef _init_design_region(self, design_region, domain_shape):\n\t\tif design_region is not None:\n\t\t\t# Use the specified design region\n\t\t\tassert design_region.shape == domain_shape, ""The design region shape must match domain shape; design_region.shape = {} domain_shape = {}"".format(\n\t\t\t\tdesign_region.shape, domain_shape)\n\t\t\tif type(design_region) is np.ndarray:\n\t\t\t\tdesign_region = torch.from_numpy(design_region, dtype=torch.unit8)\n\t\telse:\n\t\t\t# Just use the whole domain as the design region\n\t\t\tdesign_region = torch.ones(domain_shape, dtype=torch.uint8)\n\n\t\tself.register_buffer(""design_region"", design_region)\n\n\tdef _init_rho(self, rho, domain_shape):\n\t\tif isinstance(rho, torch.Tensor) | isinstance(rho, np.ndarray):\n\t\t\tassert rho.shape == domain_shape\n\t\t\tself.rho = torch.nn.Parameter(to_tensor(rho))\n\t\telif isinstance(rho, str):\n\t\t\tif rho == \'rand\':\n\t\t\t\tself.rho = torch.nn.Parameter(torch.round(torch.rand(domain_shape)))\n\t\t\telif rho == \'half\':\n\t\t\t\tself.rho = torch.nn.Parameter(torch.ones(domain_shape) * 0.5)\n\t\t\telif rho == \'blank\':\n\t\t\t\tself.rho = torch.nn.Parameter(torch.zeros(domain_shape))\n\t\t\telse:\n\t\t\t\traise ValueError(\'The domain initialization defined by `rho = %s` is invalid\' % init)\n\t\telse:\n\t\t\traise ValueError(\'The domain initialization is invalid\')\n\n\tdef constrain_to_design_region(self):\n\t\t""""""Clip the wave speed to its background value outside of the design region.""""""\n\t\twith torch.no_grad():\n\t\t\tself.rho[self.design_region == 0] = 0.0\n\t\t\tself.rho[self.b > 0] = 0.0\n\n\tdef _apply_blur(self, rho):\n\t\t""""""Applies the blur parameterization operator""""""\n\t\tblur_N = self.blur_N.item()\n\t\tblur_radius = self.blur_radius.item()\n\n\t\tfor i in range(blur_N):\n\t\t\trho = conv2d(rho.unsqueeze(0).unsqueeze(0), self.blur_kernel, padding=self.blur_kernel.shape[-1]//2).squeeze() \n\n\t\treturn rho\n\n\tdef _apply_projection(self, rho):\n\t\t""""""Applies the projection parameterization operator""""""\n\t\teta = self.eta.item()\n\t\tbeta = self.beta.item()\n\t\treturn (np.tanh(beta * eta) + torch.tanh(beta * (rho - eta))) / (\n\t\t\t\tnp.tanh(beta * eta) + np.tanh(beta * (1 - eta)))\n\n\tdef _rho_model(self):\n\t\t""""""Runs the complete parameterization model to return rho""""""\n\t\trho = self.rho\n\t\trho = self._apply_blur(rho)\n\t\trho = self._apply_projection(rho)\n\t\treturn rho\n\n\t@property\n\tdef c(self):\n\t\treturn self.c0.item() + (self.c1.item() - self.c0.item()) * self._rho_model()\n'"
wavetorch/io.py,2,"b'import copy\nimport os\n\nimport torch\n\nfrom . import geom\nfrom .cell import WaveCell\nfrom .probe import WaveIntensityProbe\nfrom .rnn import WaveRNN\nfrom .source import WaveSource\nfrom .utils import set_dtype\n\ndef save_model(model,\n\t\t\t   name,\n\t\t\t   savedir=\'./study/\',\n\t\t\t   history=None,\n\t\t\t   history_geom_state=None,\n\t\t\t   cfg=None,\n\t\t\t   verbose=True):\n\t""""""Save the model state and history to a file\n\t""""""\n\tstr_filename = name + \'.pt\'\n\tif not os.path.exists(savedir):\n\t\tos.makedirs(savedir)\n\tstr_savepath = savedir + str_filename\n\n\tif history_geom_state is None:\n\t\thistory_geom_state = [model.cell.geom.state_reconstruction_args()]\n\n\tdata = {\'model_geom_class_str\': model.cell.geom.__class__.__name__,\n\t\t\t# Class name so we know which constructor to call in load()\n\t\t\t\'model_state\': model.state_dict(),\n\t\t\t# For now just store model state without history (only geom is likely to change)\n\t\t\t\'history\': history,\n\t\t\t\'history_geom_state\': history_geom_state,  # Full history of the geometry state,\n\t\t\t\'cfg\': cfg}\n\n\tif verbose:\n\t\tprint(""Saving model to %s"" % str_savepath)\n\ttorch.save(data, str_savepath)\n\n\ndef new_geometry(class_str, state):\n\tWaveGeometryClass = getattr(geom, class_str)\n\tgeom_state = copy.deepcopy(state)\n\treturn WaveGeometryClass(**geom_state)\n\n\ndef load_model(str_filename, which_iteration=-1):\n\t""""""Load a previously saved model and its history from a file\n\t""""""\n\n\tprint(""Loading model from %s"" % str_filename)\n\n\tdata = torch.load(str_filename)\n\n\t# Set the type for floats from the save\n\tset_dtype(data[\'cfg\'][\'dtype\'])\n\n\t# Reconstruct Geometry\n\tnew_geom = new_geometry(data[\'model_geom_class_str\'], data[\'history_geom_state\'][which_iteration])\n\n\t# Get model state to recreate probes and sources\n\tmodel_state = copy.deepcopy(data[\'model_state\'])\n\n\t# Parse out the probe and source coords\n\tpx = [model_state[k].item() for k in model_state if \'probes\' in k and \'x\' in k]\n\tpy = [model_state[k].item() for k in model_state if \'probes\' in k and \'y\' in k]\n\tsx = [model_state[k].item() for k in model_state if \'sources\' in k and \'x\' in k]\n\tsy = [model_state[k].item() for k in model_state if \'sources\' in k and \'y\' in k]\n\n\t# Manually add the probes and sources\n\tnew_probes = []\n\tfor (x, y) in zip(px, py):\n\t\tnew_probes.append(WaveIntensityProbe(x, y))\n\t\t# TODO(ian): here we should actually try to infer the type of probe (e.g. intensity or not)\n\n\tnew_sources = []\n\tfor (x, y) in zip(sx, sy):\n\t\tnew_sources.append(WaveSource(x, y))\n\n\tnew_cell = WaveCell(model_state[\'cell.dt\'].item(), new_geom)\n\tnew_model = WaveRNN(new_cell, new_sources, new_probes)\n\t# Put into eval mode (doesn\'t really matter for us but whatever)\n\tnew_model.eval()\n\n\treturn new_model, data[\'history\'], data[\'history_geom_state\'], data[\'cfg\']\n'"
wavetorch/operators.py,2,"b'import torch\nfrom torch.nn.functional import conv2d\n\n\ndef _laplacian(y, h):\n    """"""Laplacian operator""""""\n    operator = h ** (-2) * torch.tensor([[[[0.0, 1.0, 0.0], [1.0, -4.0, 1.0], [0.0, 1.0, 0.0]]]])\n    y = y.unsqueeze(1)\n    # y = pad(y,pad=(0,0,1,1), mode=\'circular\')\n    # y = pad(y,pad=(1,1,0,0),mode=\'circular\')\n    return conv2d(y, operator, padding=1).squeeze(1)\n'"
wavetorch/plot.py,2,"b'import warnings\n\nwarnings.filterwarnings(""ignore"")\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport matplotlib as mpl\nimport seaborn as sns\n\nfrom mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n\nimport torch\n\nfrom .io import new_geometry\nfrom .geom import WaveGeometry\nfrom .rnn import WaveRNN\n\npoint_properties = {\'markerfacecolor\': \'none\',\n\t\t\t\t\t\'markeredgewidth\': 1.0,\n\t\t\t\t\t\'ms\': 3}\n\nbbox_white = {\'boxstyle\': \'round,pad=0.3\',\n\t\t\t  \'fc\': \'white\',\n\t\t\t  \'ec\': \'none\',\n\t\t\t  \'alpha\': 0.75}\n\ncolor_dim = {\'light\': \'#cccccc\',\n\t\t\t \'dark\': \'#555555\'}\n\ncolor_txt = {\'light\': \'#000000\',\n\t\t\t \'dark\': \'#ffffff\'}\n\ncolor_highlight = \'#a1d99b\'\n\n\ndef total_field(model, yb, ylabel, block=False, ax=None, fig_width=4, cbar=True, cax=None, vmin=1e-3, vmax=1.0):\n\t""""""Plot the total (time-integrated) field over the computatonal domain for a given vowel sample\n\t""""""\n\twith torch.no_grad():\n\t\ty_tot = torch.abs(yb).pow(2).sum(dim=1)\n\n\t\tif ax is None:\n\t\t\tfig, ax = plt.subplots(1, 1, constrained_layout=True,\n\t\t\t\t\t\t\t\t   figsize=(1.1 * fig_width, model.Ny.item() / model.Nx.item() * fig_width))\n\n\t\tZ = y_tot[0, :, :].numpy().transpose()\n\t\tZ = Z / Z.max()\n\t\th = ax.imshow(Z, cmap=plt.cm.magma, origin=""bottom"", norm=mpl.colors.LogNorm(vmin=vmin, vmax=vmax))\n\t\tif cbar:\n\t\t\tif cax is None:\n\t\t\t\tax_divider = make_axes_locatable(ax)\n\t\t\t\tcax = ax_divider.append_axes(""top"", size=""5%"", pad=""20%"")\n\t\t\tif vmax < 1.0:\n\t\t\t\textend = \'both\'\n\t\t\telse:\n\t\t\t\textend = \'min\'\n\t\t\tplt.colorbar(h, cax=cax, orientation=\'horizontal\', label=r""$\\sum_t{ { u_t{\\left(x,y\\right)} }^2 }$"")\n\t\t\t# cax.set_title(r""$\\sum_n \\vert u_n \\vert^2$"")\n\n\t\tgeometry(model, ax=ax, outline=True, outline_pml=True, vowel_probe_labels=None, highlight_onehot=ylabel,\n\t\t\t\t bg=\'dark\', alpha=0.5)\n\n\t\tax.set_xticks([])\n\t\tax.set_yticks([])\n\t\t# ax.annotate(r""$\\sum_n \\vert u_n \\vert^2$"", xy=(0.5, 0.01), fontsize=""smaller"", ha=""center"", va=""bottom"", color=""w"", xycoords=""axes fraction"")\n\t\tif ax is not None:\n\t\t\tplt.show(block=block)\n\n\ndef _plot_probes(probes, ax, vowel_probe_labels=None, highlight_onehot=None, bg=\'light\'):\n\tmarkers = []\n\tfor i, probe in enumerate(probes):\n\t\tif highlight_onehot is None:\n\t\t\tcolor_probe = \'k\'\n\t\telse:\n\t\t\tcolor_probe = color_highlight if highlight_onehot[0, i].item() == 1 else color_dim[bg]\n\n\t\tmarker = probe.plot(ax, color=color_probe)\n\t\tmarkers.append(marker)\n\n\treturn markers\n\n\ndef _plot_sources(sources, ax, bg=\'light\'):\n\tmarkers = []\n\tfor i, source in enumerate(sources):\n\t\tmarker = source.plot(ax, color=color_dim[bg])\n\t\tmarkers.append(marker)\n\n\treturn markers\n\n\ndef geometry(input,\n\t\t\t ax=None,\n\t\t\t outline=False,\n\t\t\t outline_pml=True,\n\t\t\t vowel_probe_labels=None,\n\t\t\t highlight_onehot=None,\n\t\t\t bg=\'light\',\n\t\t\t alpha=1.0,\n\t\t\t cbar=False):\n\t""""""Plot the spatial distribution of the wave speed\n\t""""""\n\tlc = \'#000000\' if bg == \'light\' else \'#ffffff\'\n\n\tif isinstance(input, WaveGeometry):\n\t\tgeom = input\n\t\tprobes = None\n\t\tsource = None\n\telif isinstance(input, WaveRNN):\n\t\tgeom = input.cell.geom\n\t\tprobes = input.probes\n\t\tsources = input.sources\n\telse:\n\t\traise ValueError(""Invalid input for plot.geometry(); should be either a WaveGeometry or a WaveCell"")\n\n\trho = geom.rho.detach().numpy().transpose()\n\n\t# Make axis if needed\n\tshow = False\n\tif ax is None:\n\t\tshow = True\n\t\tfig, ax = plt.subplots(1, 1, constrained_layout=True)\n\n\tmarkers = []\n\tif outline:\n\t\th = ax.contour(rho, levels=[0.5], colors=[lc], linewidths=[0.75], alpha=alpha)\n\t\tmarkers += h.collections\n\telse:\n\t\tlimits = np.array([geom.c0, geom.c1])\n\t\tcmap = plt.cm.Greens if geom.c0 < geom.c1 else plt.cm.Greens_r\n\t\th = ax.imshow(geom.c.detach().numpy().transpose(), origin=""bottom"", rasterized=True, cmap=cmap,\n\t\t\t\t\t  vmin=limits.min(), vmax=limits.max())\n\n\tif cbar and not outline:\n\t\tplt.colorbar(h, ax=ax, label=\'Wave speed\', ticks=limits)\n\n\tif outline_pml:\n\t\tb_boundary = geom.b.numpy().transpose()\n\t\th2 = ax.contour(b_boundary > 0, levels=[0], colors=[lc], linestyles=[\'dotted\'], linewidths=[0.75], alpha=alpha)\n\n\tif probes is not None:\n\t\tmarkers += _plot_probes(probes, ax, vowel_probe_labels=vowel_probe_labels, highlight_onehot=highlight_onehot,\n\t\t\t\t\t\t\t\tbg=bg)\n\t\tmarkers += h2.collections\n\n\tif sources is not None:\n\t\tmarkers += _plot_sources(sources, ax, bg=bg)\n\n\tax.set_xticks([])\n\tax.set_yticks([])\n\n\tif show:\n\t\tplt.show()\n\n\treturn h, markers\n\n\ndef geometry_evolution(model, model_geom_class_str, history_geom_state, quantity=\'c\', figsize=(5.6, 1.5)):\n\t""""""Plot the spatial distribution of material for the given epochs\n\t""""""\n\tNx = int(len(history_geom_state))\n\tNy = 1\n\n\tfig, axs = plt.subplots(Ny, Nx, constrained_layout=True, figsize=figsize)\n\taxs = np.asarray(axs)\n\taxs = axs.ravel()\n\tfor i, state in enumerate(history_geom_state):\n\t\tnew_geom = new_geometry(model_geom_class_str, state)\n\t\tmodel.cell.geom = new_geom\n\t\th, _ = geometry(model, ax=axs[i], outline=False, outline_pml=True, vowel_probe_labels=None,\n\t\t\t\t\t\thighlight_onehot=None, bg=\'light\', alpha=1.0)\n\t\taxs[i].set_title(\'Epoch %d\' % i)\n\n\tfor j in range(i + 1, len(axs)):\n\t\taxs[j].set_xticks([])\n\t\taxs[j].set_yticks([])\n\t\taxs[j].axis(\'image\')\n\t\taxs[j].axis(\'off\')\n\n\tplt.colorbar(h, ax=axs, shrink=0.5, label=\'Wave speed\', ticks=np.array([model.c0.item(), model.c1.item()]))\n\n\ndef probe_integrals(model, fields_in, ylabel, x, block=False, ax=None):\n\t""""""Plot the time integrated probe signals\n\t""""""\n\t# probe_fields = fields_in[0, :, model.px, model.py].numpy()\n\n\t# I = np.cumsum(np.abs(probe_fields)**2, axis=0)\n\n\t# if ax is None:\n\t#     fig, axs = plt.subplots(3, 2, constrained_layout=True, figsize=(3.7, 2))\n\n\t# for j in range(I.shape[1]):\n\t#     ax[j,1].plot(I[:,j], ""-"" if ylabel[0,j].item() == 1 else ""--"")\n\n\t# ax[ylabel[0,:].argmax().item(),0].plot(x.squeeze().numpy(), linewidth=0.75)\n\t# plt.show(block=block)\n\tpass\n\n\ndef field_snapshot(model, fields, times, ylabel, fig_width=6, block=False, axs=None, label=True, cbar=True, Ny=1,\n\t\t\t\t   sat=1.0):\n\t""""""Plot snapshots in time of the scalar wave field\n\t""""""\n\tfield_slices = fields[0, times, :, :]\n\n\tif axs is None:\n\t\tNx = int(len(times) / Ny)\n\t\tfig, axs = plt.subplots(Ny, Nx, constrained_layout=True)\n\n\taxs = np.atleast_1d(axs)\n\taxs = axs.ravel()\n\n\tfield_max = field_slices.max().item()\n\n\tfor i, time in enumerate(times):\n\t\tfield = field_slices[i, :, :].numpy().transpose()\n\n\t\th = axs[i].imshow(field, cmap=plt.cm.RdBu, vmin=-sat * field_max, vmax=+sat * field_max, origin=""bottom"",\n\t\t\t\t\t\t  rasterized=True)\n\t\tgeometry(model, ax=axs[i], outline=True, outline_pml=True, highlight_onehot=ylabel, bg=\'light\')\n\n\t\taxs[i].set_xticks([])\n\t\taxs[i].set_yticks([])\n\n\t\tif label:\n\t\t\taxs[i].text(0.5, 0.03, ""time step %d/%d"" % (time, fields.shape[1]), transform=axs[i].transAxes, ha=""center"",\n\t\t\t\t\t\tva=""bottom"", bbox=bbox_white, fontsize=\'smaller\')\n\n\tif cbar:\n\t\tplt.colorbar(h, ax=axs, label=r""$u_n{(x,y)}$"", shrink=0.80)\n\n\tfor j in range(i + 1, len(axs)):\n\t\taxs[j].set_xticks([])\n\t\taxs[j].set_yticks([])\n\t\taxs[j].axis(\'image\')\n\t\taxs[j].axis(\'off\')\n\n\tplt.show(block=block)\n\n\treturn axs\n\n\ndef animate_fields(model, fields, ylabel, batch=0, block=True, filename=None, interval=30, window_length=None,\n\t\t\t\t   fig_width=3.5):\n\tfield_max = fields[batch, :, :, :].max().item()\n\n\tfig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(fig_width, fig_width * model.Ny / model.Nx))\n\tim = ax.imshow(np.zeros((model.Ny, model.Nx)), cmap=plt.cm.RdBu, animated=True, vmin=-field_max, vmax=+field_max,\n\t\t\t\t   origin=""bottom"")\n\n\t_, markers = geometry(model, ax=ax, outline=True, outline_pml=True, highlight_onehot=ylabel, bg=\'light\')\n\tmarkers = tuple(markers)\n\n\ttitle = ax.text(0.03, 0.03, """", transform=ax.transAxes, ha=""left"", va=""bottom"", bbox=bbox_white)\n\n\tdef animate(i):\n\t\ttitle.set_text(""Time step n = %d"" % i)\n\t\tim.set_array(fields[batch, i, :, :].numpy().transpose())\n\t\treturn (im, title, *markers)\n\n\tframes = None if window_length == None else range(int(fields.shape[1] / 2 - window_length / 2),\n\t\t\t\t\t\t\t\t\t\t\t\t\t  int(fields.shape[1] / 2 + window_length / 2))\n\tanim = animation.FuncAnimation(fig, animate, interval=interval, frames=frames, blit=True, repeat_delay=1)\n\n\tif filename is not None:\n\t\tanim.save(filename, writer=\'imagemagick\')\n\n\tplt.show(block=block)\n\n\ndef confusion_matrix(cm, ax=None, figsize=(4, 4), title=None, normalize=False, labels=""auto""):\n\tN_classes = cm.shape[0]\n\n\tif normalize:\n\t\tcm = 100 * (cm.transpose() / cm.sum(axis=1)).transpose()\n\t\tfmt = "".1f""\n\telse:\n\t\t# fmt = ""d""\n\t\tfmt = "".1f""\n\n\tif ax is None:\n\t\tfig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=figsize)\n\n\tmask1 = np.eye(N_classes) == 0\n\tmask2 = np.eye(N_classes) == 1\n\n\tpal1 = sns.blend_palette([""#f7f7f7"", ""#d1e5f0"", ""#92c5de"", ""#4393c3"", ""#2166ac"", ""#053061""], as_cmap=True)\n\tpal2 = sns.blend_palette([""#f7f7f7"", ""#fddbc7"", ""#f4a582"", ""#d6604d"", ""#b2182b"", ""#67001f""], as_cmap=True)\n\n\tsns.heatmap(cm.transpose(),\n\t\t\t\tfmt=fmt,\n\t\t\t\tannot=True,\n\t\t\t\tcmap=pal1,\n\t\t\t\tlinewidths=0,\n\t\t\t\tcbar=False,\n\t\t\t\tmask=mask1,\n\t\t\t\tax=ax,\n\t\t\t\txticklabels=labels,\n\t\t\t\tyticklabels=labels,\n\t\t\t\tsquare=True,\n\t\t\t\tannot_kws={\'size\': \'small\'})\n\n\tsns.heatmap(cm.transpose(),\n\t\t\t\tfmt=fmt,\n\t\t\t\tannot=True,\n\t\t\t\tcmap=pal2,\n\t\t\t\tlinewidths=0,\n\t\t\t\tcbar=False,\n\t\t\t\tmask=mask2,\n\t\t\t\tax=ax,\n\t\t\t\txticklabels=labels,\n\t\t\t\tyticklabels=labels,\n\t\t\t\tsquare=True,\n\t\t\t\tannot_kws={\'size\': \'small\'})\n\n\tfor _, spine in ax.spines.items():\n\t\tspine.set_visible(True)\n\tax.set_xlabel(\'Input\')\n\tax.set_ylabel(\'Predicted\')\n\tax.axis(""image"")\n\n\tif title is not None:\n\t\tax.set_title(title)\n'"
wavetorch/probe.py,3,"b""import torch\n\nfrom .utils import to_tensor\n\n\nclass WaveProbe(torch.nn.Module):\n\tdef __init__(self, x, y):\n\t\tsuper().__init__()\n\n\t\t# Need to be int64\n\t\tself.register_buffer('x', to_tensor(x, dtype=torch.int64))\n\t\tself.register_buffer('y', to_tensor(y, dtype=torch.int64))\n\n\tdef forward(self, x):\n\t\treturn x[:, self.x, self.y]\n\n\tdef plot(self, ax, color='k'):\n\t\tmarker, = ax.plot(self.x.numpy(), self.y.numpy(), 'o', markeredgecolor=color, markerfacecolor='none', markeredgewidth=1.0, markersize=4)\n\t\treturn marker\n\n\nclass WaveIntensityProbe(WaveProbe):\n\tdef __init__(self, x, y):\n\t\tsuper().__init__(x, y)\n\n\tdef forward(self, x):\n\t\treturn super().forward(x).pow(2)\n"""
wavetorch/rnn.py,9,"b'import torch\n\n\nclass WaveRNN(torch.nn.Module):\n\tdef __init__(self, cell, sources, probes=[]):\n\n\t\tsuper().__init__()\n\n\t\tself.cell = cell\n\n\t\tif type(sources) is list:\n\t\t\tself.sources = torch.nn.ModuleList(sources)\n\t\telse:\n\t\t\tself.sources = torch.nn.ModuleList([sources])\n\n\t\tif type(probes) is list:\n\t\t\tself.probes = torch.nn.ModuleList(probes)\n\t\telse:\n\t\t\tself.probes = torch.nn.ModuleList([probes])\n\n\tdef forward(self, x, output_fields=False):\n\t\t""""""Propagate forward in time for the length of the inputs\n\n\t\tParameters\n\t\t----------\n\t\tx :\n\t\t\tInput sequence(s), batched in first dimension\n\t\toutput_fields :\n\t\t\tOverride flag for probe output (to get fields)\n\t\t""""""\n\n\t\t# Hacky way of figuring out if we\'re on the GPU from inside the model\n\t\tdevice = ""cuda"" if next(self.parameters()).is_cuda else ""cpu""\n\n\t\t# First dim is batch\n\t\tbatch_size = x.shape[0]\n\n\t\t# Init hidden states\n\t\thidden_state_shape = (batch_size,) + self.cell.geom.domain_shape\n\t\th1 = torch.zeros(hidden_state_shape, device=device)\n\t\th2 = torch.zeros(hidden_state_shape, device=device)\n\t\ty_all = []\n\n\t\t# Because these will not change with time we should pull them out here to avoid unnecessary calculations on each\n\t\t# tme step, dramatically reducing the memory load from backpropagation\n\t\tc = self.cell.geom.c\n\t\trho = self.cell.geom.rho\n\n\t\t# Loop through time\n\t\tfor i, xi in enumerate(x.chunk(x.size(1), dim=1)):\n\n\t\t\t# Propagate the fields\n\t\t\th1, h2 = self.cell(h1, h2, c, rho)\n\n\t\t\t# Inject source(s)\n\t\t\tfor source in self.sources:\n\t\t\t\th1 = source(h1, xi.squeeze(-1))\n\n\t\t\tif len(self.probes) > 0 and not output_fields:\n\t\t\t\t# Measure probe(s)\n\t\t\t\tprobe_values = []\n\t\t\t\tfor probe in self.probes:\n\t\t\t\t\tprobe_values.append(probe(h1))\n\t\t\t\ty_all.append(torch.stack(probe_values, dim=-1))\n\t\t\telse:\n\t\t\t\t# No probe, so just return the fields\n\t\t\t\ty_all.append(h1)\n\n\t\t# Combine outputs into a single tensor\n\t\ty = torch.stack(y_all, dim=1)\n\n\t\treturn y\n'"
wavetorch/source.py,4,"b""import skimage\nimport torch\n\nfrom .utils import to_tensor\n\n\nclass WaveSource(torch.nn.Module):\n\tdef __init__(self, x, y):\n\t\tsuper().__init__()\n\n\t\t# These need to be longs for advanced indexing to work\n\t\tself.register_buffer('x', to_tensor(x, dtype=torch.int64))\n\t\tself.register_buffer('y', to_tensor(y, dtype=torch.int64))\n\n\tdef forward(self, Y, X, dt=1.0):\n\t\t# Y[:, self.x, self.y] = Y[:, self.x, self.y] + dt**2 * X.expand_as(Y[:, self.x, self.y])\n\n\t\t# Thanks to Erik Peterson for this fix\n\t\tX_expanded = torch.zeros(Y.size()).detach()\n\t\tX_expanded[:, self.x, self.y] = X\n\n\t\treturn Y + dt ** 2 * X_expanded\n\n\tdef plot(self, ax, color='r'):\n\t\tmarker, = ax.plot(self.x.numpy(), self.y.numpy(), 'o', markeredgecolor=color, markerfacecolor='none', markeredgewidth=1.0, markersize=4)\n\t\treturn marker\n\n\nclass WaveLineSource(WaveSource):\n\tdef __init__(self, r0, c0, r1, c1):\n\t\tx, y = skimage.draw.line(r0, c0, r1, c1)\n\n\t\tself.r0 = r0\n\t\tself.c0 = c0\n\t\tself.r1 = r1\n\t\tself.c1 = c1\n\t\tsuper().__init__(x, y)\n\n\tdef plot(self, ax, color='r'):\n\t\tline, = ax.plot([self.r0, self.r1], [self.c0, self.c1], '-', color=color, lw=2)\n\t\treturn line\n"""
wavetorch/train.py,6,"b'import copy\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import confusion_matrix\n\nfrom .io import save_model\nfrom .utils import accuracy_onehot, normalize_power\n\n\ndef train(model, optimizer, criterion, train_dl, test_dl,\n\t\t  N_epochs: int, batch_size: int, history=None, history_model_state=[],\n\t\t  fold=None, name=None, savedir=None, cfg=None, accuracy=None):\n\t""""""Trains the model.\n\n\tParameters\n\t----------\n\tmodel :\n\t\tThe model to be trained\n\toptimizer :\n\t\tThe pytorch optimizer used to perform training\n\tcriterion :\n\t\tThe pytorch loss function\n\ttrain_dl :\n\t\tThe training dataset data loader\n\ttest_dl :\n\t\tThe testing dataset data loader\n\tN_epochs : int\n\t\tNumber of epochs to perform training\n\tbatch_size : int\n\t\tThe batch size to use during training\n\thistory :\n\t\t...\n\thistory_model_state :\n\t\t...\n\tfold :\n\t\t...\n\tname :\n\t\t...\n\tsavedir :\n\t\t...\n\tcfg :\n\t\t...\n\t""""""\n\n\tif history is None:\n\t\thistory = pd.DataFrame(\n\t\t\tcolumns=[\'time\', \'epoch\', \'fold\', \'loss_train\', \'loss_test\', \'acc_train\', \'acc_test\', \'cm_train\',\n\t\t\t\t\t \'cm_test\'])\n\n\tt_start = time.time()\n\tfor epoch in range(0, N_epochs + 1):\n\t\tt_epoch = time.time()\n\n\t\tloss_iter = []\n\t\tfor num, (xb, yb) in enumerate(train_dl):\n\t\t\tdef closure():\n\t\t\t\toptimizer.zero_grad()\n\t\t\t\tyb_pred = normalize_power(model(xb).sum(dim=1))\n\t\t\t\tloss = criterion(yb_pred, yb.argmax(dim=1))\n\t\t\t\tloss.backward()\n\t\t\t\treturn loss\n\n\t\t\tif epoch == 0:  # Don\'t take a step and just characterize the starting structure\n\t\t\t\twith torch.no_grad():\n\t\t\t\t\tyb_pred = normalize_power(model(xb).sum(dim=1))\n\t\t\t\t\tloss = criterion(yb_pred, yb.argmax(dim=1))\n\t\t\telse:  # Take an optimization step\n\t\t\t\tloss = optimizer.step(closure)\n\t\t\t\tmodel.cell.geom.constrain_to_design_region()\n\n\t\t\tloss_iter.append(loss.item())\n\n\t\twith torch.no_grad():\n\t\t\tacc_train_tmp = []\n\n\t\t\tlist_yb_pred = []\n\t\t\tlist_yb = []\n\t\t\tfor num, (xb, yb) in enumerate(train_dl):\n\t\t\t\tyb_pred = normalize_power(model(xb).sum(dim=1))\n\t\t\t\tlist_yb_pred.append(yb_pred)\n\t\t\t\tlist_yb.append(yb)\n\t\t\t\tif accuracy is not None:\n\t\t\t\t\tacc_train_tmp.append(accuracy(yb_pred, yb.argmax(dim=1)))\n\n\t\t\ty_pred = torch.cat(list_yb_pred, dim=0)\n\t\t\ty_truth = torch.cat(list_yb, dim=0)\n\t\t\tcm_train = confusion_matrix(y_truth.argmax(dim=1).numpy(), y_pred.argmax(dim=1).numpy())\n\n\t\t\tacc_test_tmp = []\n\t\t\tloss_test_tmp = []\n\t\t\tlist_yb_pred = []\n\t\t\tlist_yb = []\n\t\t\tcm_test = None\n\t\t\tif test_dl is not None:\n\t\t\t\tfor num, (xb, yb) in enumerate(test_dl):\n\t\t\t\t\tyb_pred = normalize_power(model(xb).sum(dim=1))\n\t\t\t\t\tlist_yb_pred.append(yb_pred)\n\t\t\t\t\tlist_yb.append(yb)\n\t\t\t\t\tloss_test_tmp.append(criterion(yb_pred, yb.argmax(dim=1)))\n\t\t\t\t\tif accuracy is not None:\n\t\t\t\t\t\tacc_test_tmp.append(accuracy_onehot(yb_pred, yb.argmax(dim=1)))\n\n\t\t\t\ty_pred = torch.cat(list_yb_pred, dim=0)\n\t\t\t\ty_truth = torch.cat(list_yb, dim=0)\n\t\t\t\tcm_test = confusion_matrix(y_truth.argmax(dim=1).numpy(), y_pred.argmax(dim=1).numpy())\n\n\t\tprint(\n\t\t\t\'Epoch %2d/%2d --- Elapsed Time:  %4.2f min | Training Loss:  %.4e | Testing Loss:  %.4e | Training Accuracy:  %.4f | Testing Accuracy:  %.4f\' %\n\t\t\t(epoch, N_epochs, (time.time() - t_epoch) / 60, np.mean(loss_iter), np.mean(loss_test_tmp),\n\t\t\t np.mean(acc_train_tmp), np.mean(acc_test_tmp)))\n\n\t\thistory = history.append({\'time\': pd.to_datetime(\'now\'),\n\t\t\t\t\t\t\t\t  \'epoch\': epoch,\n\t\t\t\t\t\t\t\t  \'fold\': fold,\n\t\t\t\t\t\t\t\t  \'loss_train\': np.mean(loss_iter),\n\t\t\t\t\t\t\t\t  \'loss_test\': np.mean(loss_test_tmp),\n\t\t\t\t\t\t\t\t  \'acc_train\': np.mean(acc_train_tmp),\n\t\t\t\t\t\t\t\t  \'acc_test\': np.mean(acc_test_tmp),\n\t\t\t\t\t\t\t\t  \'cm_train\': cm_train,\n\t\t\t\t\t\t\t\t  \'cm_test\': cm_test},\n\t\t\t\t\t\t\t\t ignore_index=True)\n\n\t\thistory_model_state.append(copy.deepcopy(model.cell.geom.state_reconstruction_args()))\n\n\t\tif name is not None:\n\t\t\tsave_model(model, name, savedir, history, history_model_state, cfg, verbose=False)\n\n\tprint(\'Total Time: %.2f min\\n\' % ((time.time() - t_start) / 60))\n\n\treturn history, history_model_state\n'"
wavetorch/utils.py,6,"b'import numpy as np\nimport torch\nfrom sklearn.metrics import confusion_matrix\n\n\ndef to_tensor(x, dtype=None):\n\tdtype = dtype if dtype is not None else torch.get_default_dtype()\n\tif type(x) is np.ndarray:\n\t\treturn torch.from_numpy(x).type(dtype=dtype)\n\telse:\n\t\treturn torch.tensor(x, dtype=dtype)\n\n\ndef set_dtype(dtype=None):\n\tif dtype == \'float32\' or dtype is None:\n\t\ttorch.set_default_dtype(torch.float32)\n\telif dtype == \'float64\':\n\t\ttorch.set_default_dtype(torch.float64)\n\telse:\n\t\traise ValueError(\'Unsupported data type: %s; should be either float32 or float64\' % dtype)\n\n\ndef window_data(X, window_length):\n\t""""""Window the sample, X, to a length of window_length centered at the middle of the original sample\n\t""""""\n\treturn X[int(len(X) / 2 - window_length / 2):int(len(X) / 2 + window_length / 2)]\n\n\ndef accuracy_onehot(y_pred, y_label):\n\t""""""Compute the accuracy for a onehot\n\t""""""\n\treturn (y_pred.argmax(dim=1) == y_label).float().mean().item()\n\n\ndef normalize_power(X):\n\treturn X / torch.sum(X, dim=1, keepdim=True)\n'"
study/utils/animate_structure.py,2,"b'from wavetorch.core import load_model\nfrom wavetorch.viz import plot_structure\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nmodel, history, history_state, cfg = load_model(""./study/nonlinear_speed/nonlinear_speed_616.pt"")\n\nfig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(6.5, 6.5*model.Ny/model.Nx))\nim = ax.imshow(np.zeros((model.Ny, model.Nx)), animated=True, origin=\'bottom\')\ntitle = ax.text(0.5, 0.95, """", transform=ax.transAxes, ha=""center"", va=""top"", size=20)\n\ndef animate(i):\n    title.set_text(\'Epoch %d\' % i)\n    model.load_state_dict(history_state[i])\n    im, _ = plot_structure(model, ax=ax);\n    return (im, title)\n\nanim = animation.FuncAnimation(fig, animate, frames=range(0,30), blit=True, repeat_delay=5)\n\nanim.save(\'./tmp.gif\', writer=\'imagemagick\')\n'"
study/utils/dof.py,1,"b'import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom wavetorch.core import load_model\n\nmpl.rcParams[\'text.usetex\'] = True\n\nCOL_TRAIN = ""#1f77b4""\nCOL_TEST = ""#2ca02c""\n\nfiles = [\'./study/nonlinear_speed/kerr_264_cv.pt\',\n         \'./study/nonlinear_speed/kerr_587_cv.pt\',\n         \'./study/nonlinear_speed/kerr_588_cv.pt\',\n         \'./study/nonlinear_speed/kerr_589_cv.pt\',\n         \'./study/nonlinear_speed/kerr_590_cv.pt\',\n         \'./study/nonlinear_speed/kerr_599_cv.pt\',\n         ]\n\nnum_dof = []\nacc_train_mean = []\nacc_train_std = []\nacc_test_mean = []\nacc_test_std = []\n\nfor file in files:\n    model, history, history_state, cfg = load_model(file)\n\n    history_mean = history.groupby(\'epoch\').mean()\n    history_std = history.groupby(\'epoch\').std()\n\n    acc_train_mean.append(history_mean[\'acc_train\'].tail(1).item() * 100)\n    acc_train_std.append(history_std[\'acc_train\'].tail(1).item() * 100)\n    acc_test_mean.append(history_mean[\'acc_test\'].tail(1).item() * 100)\n    acc_test_std.append(history_std[\'acc_test\'].tail(1).item() * 100)\n\n    num_dof.append(model.design_region.sum().item())\n\ninds = np.argsort(num_dof)\n\nnum_dof = np.array(num_dof)[inds]\nacc_train_mean = np.array(acc_train_mean)[inds]\nacc_train_std = np.array(acc_train_std)[inds]\nacc_test_mean = np.array(acc_test_mean)[inds]\nacc_test_std = np.array(acc_test_std)[inds]\n\nfig, ax = plt.subplots(1, 1, constrained_layout=True, figsize=(4.5, 2.25))\nax.plot(num_dof, acc_train_mean, \'o-\', color=COL_TRAIN, label=""Training dataset"")\nax.fill_between(num_dof, acc_train_mean - acc_train_std, acc_train_mean + acc_train_std, color=COL_TRAIN, alpha=0.15)\nax.plot(num_dof, acc_test_mean, \'o-\', color=COL_TEST, label=""Testing dataset"")\nax.fill_between(num_dof, acc_test_mean - acc_test_std, acc_test_mean + acc_test_std, color=COL_TEST, alpha=0.15)\nax.set_xlabel(""Trainable degrees of freedom"")\nax.set_ylabel(""Accuracy"")\n\nax.legend()\n\nax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(base=10))\nax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter(\'%.0f\\%%\'))\nax.set_ylim([20, 100])\n\nplt.show()\n'"
study/utils/plot_mem.py,0,"b'"""""" Helper script for plotting memory usage from memory profiler\n\nInstall memory_profiler:\n\tconda install memory_profiler\n\nProfile the code:\n\tmprof run study/vowel_train.py study/example.yml\n\nThis will generate a mprofile dat file which you can then plot with this script\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\n\nparser = argparse.ArgumentParser() \nparser.add_argument(\'files\', nargs=\'+\')\n\nargs = parser.parse_args()\n\nfig, ax = plt.subplots(1,1, constrained_layout=True, figsize=(4,3))\n\nfor file in args.files:\n\tdata = np.loadtxt(file, usecols=(1,2), skiprows=1, delimiter=\' \')\n\tmem = data[:,0]\n\tt = data[:,1]\n\tt = t-t.min()\n\tax.plot(t, mem/1e3)\n\nax.set_xlabel(\'Time (sec)\')\nax.set_ylabel(\'Memory (GB)\')\nax.grid()\nplt.show()\n'"
study/utils/test_grad.py,12,"b'""""""Perform vowel recognition training.\n""""""\n\nimport torch\nimport wavetorch\nimport numpy as np\nfrom torch.nn.functional import conv2d, pad\n\n\ndef _laplacian(y, h):\n    """"""Laplacian operator""""""\n    operator = h**(-2) * torch.tensor([[[[0.0,  1.0, 0.0], [1.0, -4.0, 1.0], [0.0,  1.0, 0.0]]]])\n    y = y.unsqueeze(1)\n    # y = pad(y,pad=(0,0,1,1),mode=\'circular\')\n    # y = pad(y,pad=(1,1,0,0),mode=\'circular\')\n    return conv2d(y, operator, padding=1).squeeze(1)\n\ndef step(b, c, y1, y2, dt, h):\n        y = torch.mul((dt.pow(-2) + b * dt.pow(-1)).pow(-1),\n              (2/dt.pow(2)*y1 - torch.mul( (dt.pow(-2) - b * dt.pow(-1)), y2)\n                       + torch.mul(c.pow(2), _laplacian(y1, h)))\n             )\n        return y\n\nclass HardcodedStep(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, b, c, y1, y2, dt, h):\n        ctx.save_for_backward(b, c, y1, y2, dt, h)\n        return step(b, c, y1, y2, dt, h)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        b, c, y1, y2, dt, h = ctx.saved_tensors\n\n        grad_b = grad_c = grad_y1 = grad_y2 = grad_dt = grad_h = None\n\n        if ctx.needs_input_grad[0]:\n            grad_b = - (dt * b + 1).pow(-2) * dt * (c.pow(2) * dt.pow(2) * _laplacian(y1, h) + 2*y1 - 2 * y2 ) * grad_output\n        if ctx.needs_input_grad[1]:\n            grad_c = (b*dt + 1).pow(-1) * (2 * c * dt.pow(2) * _laplacian(y1, h) ) * grad_output\n        if ctx.needs_input_grad[2]:\n            # grad_y1 = ( dt.pow(2) * _laplacian(c.pow(2) *grad_output, h) + 2*grad_output) * (b*dt + 1).pow(-1)\n            c2_grad =  (b*dt + 1).pow(-1) * c.pow(2) * grad_output\n            grad_y1 = dt.pow(2) * _laplacian(c2_grad, h) + 2*grad_output * (b*dt + 1).pow(-1)\n        if ctx.needs_input_grad[3]:\n            grad_y2 = (b*dt -1) * (b*dt + 1).pow(-1) * grad_output\n\n        return grad_b, grad_c, grad_y1, grad_y2, grad_dt, grad_h\n\nN = 5\nM = 1\nparams = [\n    torch.rand(N,N, requires_grad=True), # b\n    torch.rand(N,N, requires_grad=True), # c\n    torch.rand(M, N,N, requires_grad=True), # y1\n    torch.rand(M, N,N, requires_grad=True), # y2\n    torch.tensor(0.10),\n    torch.tensor(0.25)\n]\n\n# Take derivative using pytorch\'s AD\ny_ad = step(*params)\ny_ad.sum().backward()\n\nparams_grad_ad = []\nfor param in params:\n    if param.grad is not None:\n        params_grad_ad.append(param.grad.clone())\n        param.grad.detach_()\n        param.grad.zero_() # Detach and zero for subsequent calculations\n\ny_hardcoded = HardcodedStep.apply(*params)\ny_hardcoded.sum().backward() # Do the backprop\n\nparams_grad_hard = []\nfor param in params:\n    if param.grad is not None:\n        params_grad_hard.append(param.grad.clone())\n        param.grad.detach_()\n        param.grad.zero_() # Detach and zero for subsequent calculations\n\n# Compare\ndiff = [(params_grad_hard[i]-params_grad_ad[i]).norm().item()/params_grad_ad[i].norm().item() for i in range(0,len(params_grad_hard))]\nprint(diff)\n'"
study/vanilla/models.py,21,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass CustomRNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size, batch_first=True, W_scale=1e-1, f_hidden=None):\n        super(CustomRNN, self).__init__()\n        self.input_size  = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.f_hidden = f_hidden\n\n        self.W1 = nn.Parameter((torch.rand(hidden_size, input_size)-0.5)*W_scale)\n        self.W2 = nn.Parameter((torch.rand(hidden_size, hidden_size)-0.5)*W_scale)\n        self.W3 = nn.Parameter((torch.rand(output_size, hidden_size)-0.5)*W_scale)\n        self.b_h = nn.Parameter(torch.zeros(hidden_size))\n\n    def forward(self, x):\n        h1 = torch.zeros(x.shape[0], self.hidden_size)\n        ys = []\n\n        for i, xi in enumerate(x.chunk(x.size(1), dim=1)):\n            h1 = (torch.matmul(self.W2, h1.t()) + torch.matmul(self.W1, xi.t())).t() + self.b_h\n            if self.f_hidden is not None:\n                h1 = getattr(F, self.f_hidden)(h1)\n            y = torch.matmul(self.W3, h1.t()).t()\n            ys.append(y)\n\n        ys = torch.stack(ys, dim=1)\n        return ys\n\nclass CustomRes(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size, batch_first=True, W_scale=1e-1, f_hidden=None):\n        super(CustomRes, self).__init__()\n        self.input_size  = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.f_hidden = f_hidden\n\n        self.W1 = torch.nn.Parameter((torch.rand(hidden_size, input_size)-0.5)*W_scale)\n        self.W2 = torch.nn.Parameter((torch.rand(hidden_size, hidden_size)-0.5)*W_scale)\n        self.W3 = torch.nn.Parameter((torch.rand(output_size, hidden_size)-0.5)*W_scale)\n        self.b_h = torch.nn.Parameter(torch.zeros(hidden_size))\n\n    def forward(self, x):\n        h1 = torch.zeros(x.shape[0], self.hidden_size)\n        ys = []\n\n        for i, xi in enumerate(x.chunk(x.size(1), dim=1)):\n            hprev = h1\n            h1 = (torch.matmul(self.W2, h1.t()) + torch.matmul(self.W1, xi.t())).t() + self.b_h\n            if self.f_hidden is not None:\n                h1 = getattr(F, self.f_hidden)(h1)\n            y = torch.matmul(self.W3, h1.t()).t()\n            ys.append(y)\n            h1 = h1 + hprev\n\n        ys = torch.stack(ys, dim=1)\n        return ys\n\nclass CustomLSTM(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size, batch_first=True, W_scale=1e-1):\n        super(CustomLSTM, self).__init__()\n        self.input_size  = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=batch_first)\n        self.W3 = torch.nn.Parameter((torch.rand(output_size, hidden_size)-0.5))\n\n    def forward(self, x):\n\n        # out should have size [N_batch, T, N_hidden] \n        out, hidden = self.lstm(x.unsqueeze(2))\n        # print(torch.max(x, 1))\n        # print(x[:, 100])\n        # print(out[:, 100, 0].detach())\n        # ys should have size [N_batch, T, N_classes]\n        ys = torch.matmul(out, self.W3.t())\n\n        return ys'"
study/vanilla/script_all_params.py,1,"b""import yaml\nimport numpy as np\nimport torch\nfrom study.vanilla import vanilla_rnn\nfrom itertools import product\n\n# Will be setting the random seed before every run\nrand_seed = 2019\n\n# The starting configuration is whatever is in the file below\nstart_file = './study/vanilla/laptop_rnn.yml'\n\n# Define various parameter values to be explored\nparams = {\t'f_hidden': ['', 'leaky_relu', 'tanh'],\n\t\t\t'prefix': ['3vowels_cv_both_window']\n\t\t\t}\n\n# We'll print results to file so that we can access later\nout_file = './study/vanilla/results_3vowels_cv_both_scalelr_window_Nh100.txt'\n\n# Temp file we'll use do define configurations\ntemp_file = './study/vanilla/temp.yml'\n\ndef product_dict(**kwargs):\n    keys = kwargs.keys()\n    vals = kwargs.values()\n    for instance in product(*vals):\n        yield dict(zip(keys, instance))\n\n\nif __name__ == '__main__':\n\tstart_conf = open(start_file, 'r')\n\tstr_cnf = start_conf.read()\n\tprint_out = open(out_file, 'w')\n\n\t# Print out starting configuration\n\tprint('Starting configuration: \\n', file=print_out)\n\twith open(start_file, 'r') as ymlfile:\n\t\tcfg = yaml.load(ymlfile)\n\t\tprint(yaml.dump(cfg, default_flow_style=False), file=print_out)\n\n\t# Iterate over all parameters that are to be changed\n\tfor current_params in list(product_dict(**params)):\n\t\tstr_temp = str_cnf\n\t\tfor key, value in current_params.items():\n\t\t\t# Get the starting value from the file \n\t\t\tind_param = str_temp.find(key + ': ')\n\t\t\tind_val = ind_param + len(key + ': ')\n\t\t\tvc = 0\n\t\t\tval = ''\n\t\t\tc = str_temp[ind_val]\n\t\t\twhile c != '\\n':\n\t\t\t\tval += c\n\t\t\t\tvc += 1\n\t\t\t\tc = str_temp[ind_val + vc]\t\t\n\t\t\tstr_temp = str_temp[:ind_val] + str(value) + str_temp[ind_val+vc:] \n\n\t\ttemp_cnf = open(temp_file, 'w')\n\t\ttemp_cnf.write(str_temp)\n\t\ttemp_cnf.close()\n\n\t\t# Run the temporary configuration\n\t\ttorch.manual_seed(rand_seed)\n\t\tnp.random.seed(rand_seed)\n\t\t(acc_train, acc_test) = vanilla_rnn.main(['--config', temp_file])\n\n\t\tprint('For parameters ' + str(current_params) + ' final train and test accuracies are ' \n\t\t\t+ str(np.float16(acc_train)) + ', ' + str(np.float16(acc_test)), file=print_out)\n\n\t\tif cfg['training']['use_cross_validation']:\n\t\t\tprint('Average values are: ' + str(np.mean(np.float16(acc_train))) + ', ' + str(np.mean(np.float16(acc_test))), file=print_out)\n\n\n\tprint_out.close()"""
study/vanilla/script_params.py,2,"b""import yaml\nimport numpy as np\nimport torch\nfrom study.vanilla import vanilla_rnn\n\n# Will be setting the random seed before every run\nrand_seed = 2019\n\n# The starting configuration is whatever is in the file below\nstart_file = './study/vanilla/laptop_rnn.yml'\n\n# Define various parameter values to be explored\nparams = { \n\t\t\t'window_size': ['', '1000']\n\t\t }\n\n# We'll print results to file so that we can access later\nout_file = './study/vanilla/params_3vowels_nocv.txt'\n\n# Temp file we'll use do define configurations\ntemp_file = './study/vanilla/temp.yml'\n\nif __name__ == '__main__':\n\tstart_conf = open(start_file, 'r')\n\tstr_cnf = start_conf.read()\n\tprint_out = open(out_file, 'w')\n\n\t# Print out starting configuration\n\tprint('Starting configuration: \\n', file=print_out)\n\twith open(start_file, 'r') as ymlfile:\n\t\tcfg = yaml.load(ymlfile)\n\t\tprint(yaml.dump(cfg, default_flow_style=False), file=print_out)\n\n\t# Run the starting configuration\n\ttorch.manual_seed(rand_seed)\n\tnp.random.seed(rand_seed)\n\t# (acc_train_st, acc_test_st) = vanilla_rnn.main(['--config', start_file])\n\n\t# print('Starting conf. train accuracy: %1.2f; test accuracy: %1.2f. \\n' % (np.float16(acc_train_st), np.float16(acc_test_st)), file=print_out)\n\n\t# Iterate over all parameters that are to be changed\n\tfor key, values in params.items():\n\t\t# Get the starting value from the file \n\t\tind_param = str_cnf.find(key + ': ')\n\t\tind_val = ind_param + len(key + ': ')\n\t\tvc = 0\n\t\tval = ''\n\t\tc = str_cnf[ind_val]\n\t\twhile c != '\\n':\n\t\t\tval += c\n\t\t\tvc += 1\n\t\t\tc = str_cnf[ind_val + vc]\t\t\n\n\t\t# Iterate over all values of each parameter\n\t\tacc_train_temp, acc_test_temp = [], []\n\t\tfor iv, value in enumerate(values):\n\t\t\tstr_temp = str_cnf[:ind_val] + str(value) + str_cnf[ind_val+vc:] \n\n\t\t\ttemp_cnf = open(temp_file, 'w')\n\t\t\ttemp_cnf.write(str_temp)\n\t\t\ttemp_cnf.close()\n\n\t\t\t# Run the temporary configuration\n\t\t\ttorch.manual_seed(rand_seed)\n\t\t\tnp.random.seed(rand_seed)\n\t\t\t(acc_train, acc_test) = vanilla_rnn.main(['--config', temp_file])\n\n\t\t\tacc_train_temp.append(acc_train)\n\t\t\tacc_test_temp.append(acc_test)\n\n\t\tprint('For parameter ' + key + ' with values ' + str(values) + ' accuracies are, respectively:', file=print_out)\n\t\tprint('Training: ' + str(np.float16(acc_train_temp)), file=print_out)\n\t\tprint('Testing: ' + str(np.float16(acc_test_temp)) + '\\n', file=print_out)\n\n\tprint_out.close()"""
study/vanilla/vanilla_rnn.py,26,"b'import argparse\nimport yaml\nimport time\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.nn.utils.clip_grad import clip_grad_norm_ as clip_grad\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom study.vanilla.models import CustomRNN, CustomRes, CustomLSTM\n\nfrom wavetorch.data import load_all_vowels\nfrom wavetorch.core.utils import accuracy\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport math, random\n\nimport os\n\nfrom torch.nn.utils.rnn import pad_sequence\n\ndef compute_acc(model, elements_set):\n    acc_tmp = []\n    num = 1\n    for xb, yb in elements_set:\n        y_ = norm_int_y(model(xb))\n        acc_tmp.append(accuracy(y_, yb.argmax(dim=1)))\n    return np.mean(np.array(acc_tmp))\n\ndef norm_int_y(y, scale=50):\n    # Input dim: [N_batch, T, N_classes]\n    # Output dim: [N_batch, N_classes]\n    # Integrates abs(y)^2 along the T-dimension and normalizes it such that the output adds to 1 along the N_classes dimension\n    \n    y_int = torch.sum(torch.pow(torch.abs(y), 2), dim=1)\n    return y_int / torch.sum(y_int, dim=1, keepdim=True)  \n\ndef save_model(model, name, savedir=\'./study/vanilla/data/\', history=None, args=None):\n    str_filename = name +  \'.pt\'\n    if not os.path.exists(savedir):\n        os.makedirs(savedir)\n    str_savepath = savedir + str_filename\n    dsave = {""model_state"": model.state_dict(),\n             ""history"": history,\n             ""args"": args}\n    print(""Saving model to %s"" % str_savepath)\n    torch.save(dsave, str_savepath)\n\ndef load_model(str_filename):\n    print(""Loading model from %s"" % str_filename)\n    data = torch.load(str_filename)\n    model_state = data[\'model_state\']\n    model = CustomRNN(model_state[\'W1\'].size(1), model_state[\'W3\'].size(0), model_state[\'W1\'].size(0))\n    model.load_state_dict(model_state)\n    model.eval()\n    return data[\'history\'], model\n\n\ndef main(args):\n    # Parse command line arguments\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\'--use-cuda\', action=\'store_true\')\n    argparser.add_argument(\'--config\', type=str, required=True,\n                            help=\'Config file to use\')\n    argparser.add_argument(\'--name\', type=str, default=None,\n                            help=\'Name to use when saving or loading the model file. If not specified when saving a time and date stamp is used\')\n    argparser.add_argument(\'--savedir\', type=str, default=\'./study/vanilla/data/\',\n                            help=\'Directory in which the model file is saved. Defaults to ./study/\')\n    args = argparser.parse_args(args)\n\n    if args.use_cuda and torch.cuda.is_available():\n        print(""Using GPU..."")\n        args.dev = torch.device(\'cuda\')\n    else:\n        print(""Using CPU..."")\n        args.dev = torch.device(\'cpu\')\n\n    print(""Using configuration from %s: "" % args.config)\n    with open(args.config, \'r\') as ymlfile:\n        cfg = yaml.load(ymlfile)\n        print(yaml.dump(cfg, default_flow_style=False))\n\n    if cfg[\'general\'][\'rand_seed\'] is not None:\n        torch.manual_seed(cfg[\'general\'][\'rand_seed\'])\n        np.random.seed(cfg[\'general\'][\'rand_seed\'])\n\n    N_classes = len(cfg[\'data\'][\'vowels\'])\n    N_epochs = cfg[\'training\'][\'N_epochs\']\n    N_batch = cfg[\'training\'][\'batch_size\']\n    disp_step = cfg[\'training\'][\'display_step\']\n\n    # # Load the data; x_train is dim [N_train, T], while y_train is dim [N_train, N_classes]\n    # x_train, x_test, y_train, y_test = load_selected_vowels(\n    #     cfg[\'data\'][\'vowels\'],\n    #     gender=cfg[\'data\'][\'gender\'], \n    #     sr=cfg[\'data\'][\'sr\'], \n    #     normalize=True, \n    #     train_size=cfg[\'training\'][\'train_size\'], \n    #     test_size=cfg[\'training\'][\'test_size\']\n    #     )\n\n    X, Y = load_all_vowels(\n            cfg[\'data\'][\'vowels\'],\n            gender=cfg[\'data\'][\'gender\'], \n            sr=cfg[\'data\'][\'sr\'], \n            normalize=True,\n            max_samples=cfg[\'training\'][\'max_samples\']\n            )\n\n    # Some manual scaling\n    mean = 0\n    std = 0\n    X_norm = []\n    for samp in X:\n        mean += samp.mean()\n        std += samp.std()\n    mean = mean/len(X)\n    std = std/len(X)\n\n    for samp in X:\n        X_norm.append((samp - mean)/std)\n\n    X = X_norm\n\n    skf = StratifiedKFold(n_splits=cfg[\'training\'][\'train_test_divide\'], random_state=None, shuffle=True)\n    samps = [y.argmax().item() for y in Y]\n    num = 1\n\n    history = {""loss_iter"": [],\n               ""acc_train"": [],\n               ""acc_test"": [],\n               ""acc_epoch"": []}\n\n    acc_fin_train = []\n    acc_fin_test = []\n\n    for train_index, test_index in skf.split(np.zeros(len(samps)), samps):\n        if cfg[\'training\'][\'use_cross_validation\']: print(""Cross validation fold #%d"" % num)\n\n        if cfg[\'data\'][\'window_size\']:\n            crop = cfg[\'data\'][\'window_size\']\n            x_train = torch.nn.utils.rnn.pad_sequence([X[i][int(len(X[i])/2-crop/2):int(len(X[i])/2+crop/2)] for i in train_index], batch_first=True)\n        else:\n            x_train = torch.nn.utils.rnn.pad_sequence([X[i] for i in train_index], batch_first=True)\n\n        x_test = torch.nn.utils.rnn.pad_sequence([X[i] for i in test_index], batch_first=True)\n        y_train = torch.nn.utils.rnn.pad_sequence([Y[i] for i in train_index], batch_first=True)\n        y_test = torch.nn.utils.rnn.pad_sequence([Y[i] for i in test_index], batch_first=True)\n\n        x_train = x_train.to(args.dev)\n        x_test  = x_test.to(args.dev)\n        y_train = y_train.to(args.dev)\n        y_test  = y_test.to(args.dev)\n\n        # # --- Define model\n        if cfg[\'rnn\'][\'model\']==\'rnn\':\n            model = CustomRNN(1, N_classes, cfg[\'rnn\'][\'N_hidden\'], W_scale=cfg[\'rnn\'][\'W_scale\'], f_hidden=cfg[\'rnn\'][\'f_hidden\'])\n        elif cfg[\'rnn\'][\'model\']==\'rnn\':\n            model = CustomRes(1, N_classes, cfg[\'rnn\'][\'N_hidden\'], W_scale=cfg[\'rnn\'][\'W_scale\'], f_hidden=cfg[\'rnn\'][\'f_hidden\'])\n        elif cfg[\'rnn\'][\'model\']==\'lstm\':\n            model = CustomLSTM(1, N_classes, cfg[\'rnn\'][\'N_hidden\'], W_scale=cfg[\'rnn\'][\'W_scale\'])\n        model.to(args.dev)\n\n        # Print the total number of parameters in the model\n        print(""Total number of parameters in model: %d"" % sum(p.numel() for p in model.parameters()))\n\n        # # Output of the model is dim [N_train, T, N_classes]\n        # y_pred = model(x_train)\n        # # Plot the N_classes intensities recorded for the first training element at the three outputs for the un-optimized model\n        # t = np.arange(0, y_pred.shape[1])\n        # plt.figure()\n        # plt.plot(t, np.square(np.abs(y_pred[1, :, :].detach().numpy())))\n        # plt.title(""Before training"")\n        # plt.show(block=False)\n\n        # # --- Define optimizer\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg[\'training\'][\'lr\'], weight_decay=2*cfg[\'rnn\'][\'L2_reg\'])\n        if cfg[\'training\'][\'lr_step\'] and cfg[\'training\'][\'lr_gamma\']: \n            scheduler = StepLR(optimizer, step_size=cfg[\'training\'][\'lr_step\'],\n            gamma=cfg[\'training\'][\'lr_gamma\'])\n            scale_lr = True\n\n        # Split data into batches\n        train_ds = TensorDataset(x_train, y_train)\n        test_ds  = TensorDataset(x_test, y_test)\n\n        train_dl = DataLoader(train_ds, batch_size=N_batch, shuffle=True)\n        test_dl  = DataLoader(test_ds, batch_size=N_batch)\n\n        # # --- Run training\n        print(""Training for %d epochs..."" % N_epochs)\n        t_start = time.time()\n        for epoch in range(1, N_epochs + 1):\n            t_epoch = time.time()\n            if scale_lr:\n                scheduler.step()\n            for xb, yb in train_dl:\n                optimizer.zero_grad()\n                y = model(xb)\n                y_ = norm_int_y(y)\n                # print(y_, torch.max(yb, 1)[1])\n                loss = criterion(y_, torch.max(yb, 1)[1])\n                history[\'loss_iter\'].append(loss.item())\n                loss.backward()\n                if cfg[\'rnn\'][\'grad_clip\'] is not None:\n                    clip_grad(model.parameters(), cfg[\'rnn\'][\'grad_clip\'], norm_type=1)\n                optimizer.step()\n\n            if epoch % disp_step == 0 or epoch == 1:\n                with torch.no_grad():\n                    tep = time.time()-t_epoch\n                    acc_train = compute_acc(model, train_dl)\n                    acc_test = compute_acc(model, test_dl)\n                    print(\'Epoch: %d/%d %d%%  | Time for last epoch %.1f sec  |  L = %.3e\' \n                        % (epoch, N_epochs, epoch/N_epochs*100, tep, loss))\n                    print(\'Training accuracy: %.2f | Testing accuracy: %.2f\' \n                        % (acc_train, acc_test))\n                    history[\'acc_train\'].append(acc_train)\n                    history[\'acc_test\'].append(acc_test)\n                    history[\'acc_epoch\'].append(epoch)            \n\n        print(\'Total time: %.1f min\' % ((time.time()-t_start)/60))\n\n        # Print the final training set accuracy\n        with torch.no_grad():\n            (acc_final_train, acc_final_test) = (compute_acc(model, train_dl), compute_acc(model, test_dl))\n            history[\'acc_train\'].append(acc_final_train)\n            history[\'acc_test\'].append(acc_final_test)\n            history[\'acc_epoch\'].append(N_epochs)  \n            print(""Final accuracy - train: {:.2f} %, test: {:.2f} %"".format(\n                acc_final_train, acc_final_test))\n            acc_fin_train.append(acc_final_train)\n            acc_fin_test.append(acc_final_test)\n\n        ### Save model and results\n        if args.name is None:\n            args.name = time.strftime(""%Y_%m_%d-%H_%M_%S"")\n        if (cfg[\'training\'][\'prefix\'] is not None) and (num == 1):\n            args.name = cfg[\'training\'][\'prefix\'] + \'_\' + args.name\n\n        if cfg[\'training\'][\'use_cross_validation\']:\n            # If we are doing cross validation, then save this model\'s iteration            \n            save_model(model, args.name + ""_cv_"" + str(num), args.savedir, history, cfg)\n            num += 1\n        else:\n            # If not doing cross validation, save and finish\n            save_model(model, args.name, args.savedir, history, cfg)\n            break\n\n    # y_pred = model(x_train)\n    # # Plot the N_classes intensities recorded for the first training element at the three outputs for the optimized model\n    # t = np.arange(0, y_pred.shape[1])\n    # plt.figure()\n    # plt.plot(t, np.square(np.abs(y_pred[1, :, :].detach().numpy())))\n    # plt.title(""After training"")\n    # plt.show(block=False)\n    return (acc_fin_train, acc_fin_test)\n\nif __name__ == \'__main__\':\n    (acc_final_train, acc_final_test) = main(sys.argv[1:])\n\n\n'"
wavetorch/data/__init__.py,0,"b'from .vowels import load_all_vowels, select_vowel_sample\n'"
wavetorch/data/vowels.py,6,"b'import glob\nimport math\nimport os\nimport random\n\nimport librosa\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n\ndef normalize_vowel(wav_data):\n\t""""""Normalize the amplitude of a vowel waveform\n\t""""""\n\ttotal_power = np.square(wav_data).sum()\n\n\treturn wav_data / np.sqrt(total_power)\n\n\ndef load_vowel(file, sr=None, normalize=True):\n\t""""""Use librosa to to load a single vowel with a specified sample rate\n\t""""""\n\n\tdata, rate = librosa.load(file, sr=sr)\n\n\tif normalize:\n\t\tdata = normalize_vowel(data)\n\n\treturn data\n\n\ndef load_all_vowels(str_classes, gender=\'both\', sr=None, normalize=True, dir=\'data/vowels/\', ext=\'.wav\',\n\t\t\t\t\tmax_samples=None, random_state=None):\n\t""""""Loads all available vowel samples associated with `str_classes` and `gender`\n\n\tIf `max_samples` is specified, then the *total* number of samples returned is limited to this number. In the case of\n\tboth genders being sampled, the vowels are equally distributed among men and women.\n\t""""""\n\n\tassert gender in [\'women\', \'men\', \'both\'], ""gender must be either \'women\', \'men\', or \'both\'""\n\n\tx_w = []\n\ty_w = []\n\tF_w = []\n\tx_m = []\n\ty_m = []\n\tF_m = []\n\tfor i, str_class in enumerate(str_classes):\n\t\ty = np.eye(len(str_classes))[i]\n\n\t\t# Women\n\t\tfiles = os.path.join(dir, \'w*\' + str_class + ext)\n\t\tfor file in sorted(glob.glob(files)):\n\t\t\tx = load_vowel(file, sr=sr, normalize=normalize)\n\t\t\tF_w.append(file)\n\t\t\tx_w.append(x)\n\t\t\ty_w.append(y)\n\n\t\t# Men\n\t\tfiles = os.path.join(dir, \'m*\' + str_class + ext)\n\t\tfor file in sorted(glob.glob(files)):\n\t\t\tx = load_vowel(file, sr=sr, normalize=normalize)\n\t\t\tF_m.append(file)\n\t\t\tx_m.append(x)\n\t\t\ty_m.append(y)\n\n\tif max_samples is not None:\n\t\t# Limit the number of returned samples\n\t\tif gender == \'both\':\n\t\t\tnum_samples_men = math.floor(max_samples / 2)\n\t\t\tnum_samples_women = math.ceil(max_samples / 2)\n\t\telse:\n\t\t\tnum_samples_men = num_samples_women = max_samples\n\n\t\t# Here, we ""abuse"" train_test_split() to get a stratified subset by only\n\t\t# utilizing the returned training set (testing set is dropped)\n\t\tx_m, _, y_m, _, F_m, _ = train_test_split(x_m, y_m, F_m, train_size=num_samples_men, test_size=len(str_classes),\n\t\t\t\t\t\t\t\t\t\t\t\t  stratify=y_m, shuffle=True, random_state=random_state)\n\t\tx_w, _, y_w, _, F_w, _ = train_test_split(x_w, y_w, F_w, train_size=num_samples_women,\n\t\t\t\t\t\t\t\t\t\t\t\t  test_size=len(str_classes), stratify=y_w, shuffle=True,\n\t\t\t\t\t\t\t\t\t\t\t\t  random_state=random_state)\n\n\t# Pack the samples into a list of tensors\n\tif gender == \'both\':\n\t\tX = [torch.tensor(x, dtype=torch.get_default_dtype()) for x in x_m + x_w]\n\t\tY = [torch.tensor(y, dtype=torch.get_default_dtype()) for y in y_m + y_w]\n\t\tF = F_m + F_w\n\telif gender == \'women\':\n\t\tX = [torch.tensor(x, dtype=torch.get_default_dtype()) for x in x_w]\n\t\tY = [torch.tensor(y, dtype=torch.get_default_dtype()) for y in y_w]\n\t\tF = F_w\n\telse:\n\t\tX = [torch.tensor(x, dtype=torch.get_default_dtype()) for x in x_m]\n\t\tY = [torch.tensor(y, dtype=torch.get_default_dtype()) for y in y_m]\n\t\tF = F_m\n\n\tprint(""Dataset: %d vowel samples"" % len(X))\n\n\treturn X, Y, F\n\n\ndef select_vowel_sample(X, Y, F, y_class, ind=None):\n\t""""""Select a specific vowel sample from the set\n\t""""""\n\tlabels_ints = [y.argmax().item() for y in Y]\n\tinds_this_clss = [i for i in range(len(labels_ints)) if labels_ints[i] == y_class]\n\n\tif ind is None:\n\t\tind = int(random.random() * len(inds_this_clss))\n\n\tprint(\'Selected sample %d, corresponding to file %s\' % (ind, F[inds_this_clss[ind]]))\n\n\treturn X[inds_this_clss[ind]].unsqueeze(0), Y[inds_this_clss[ind]].unsqueeze(0)\n'"
