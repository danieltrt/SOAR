file_path,api_count,code
criteria.py,4,"b'import torch\nimport torch.nn as nn\n\nloss_names = [\'l1\', \'l2\']\n\n\nclass MaskedMSELoss(nn.Module):\n    def __init__(self):\n        super(MaskedMSELoss, self).__init__()\n\n    def forward(self, pred, target):\n        assert pred.dim() == target.dim(), ""inconsistent dimensions""\n        valid_mask = (target > 0).detach()\n        diff = target - pred\n        diff = diff[valid_mask]\n        self.loss = (diff**2).mean()\n        return self.loss\n\n\nclass MaskedL1Loss(nn.Module):\n    def __init__(self):\n        super(MaskedL1Loss, self).__init__()\n\n    def forward(self, pred, target, weight=None):\n        assert pred.dim() == target.dim(), ""inconsistent dimensions""\n        valid_mask = (target > 0).detach()\n        diff = target - pred\n        diff = diff[valid_mask]\n        self.loss = diff.abs().mean()\n        return self.loss\n\n\nclass PhotometricLoss(nn.Module):\n    def __init__(self):\n        super(PhotometricLoss, self).__init__()\n\n    def forward(self, target, recon, mask=None):\n\n        assert recon.dim(\n        ) == 4, ""expected recon dimension to be 4, but instead got {}."".format(\n            recon.dim())\n        assert target.dim(\n        ) == 4, ""expected target dimension to be 4, but instead got {}."".format(\n            target.dim())\n        assert recon.size()==target.size(), ""expected recon and target to have the same size, but got {} and {} instead""\\\n            .format(recon.size(), target.size())\n        diff = (target - recon).abs()\n        diff = torch.sum(diff, 1)  # sum along the color channel\n\n        # compare only pixels that are not black\n        valid_mask = (torch.sum(recon, 1) > 0).float() * (torch.sum(target, 1)\n                                                          > 0).float()\n        if mask is not None:\n            valid_mask = valid_mask * torch.squeeze(mask).float()\n        valid_mask = valid_mask.byte().detach()\n        if valid_mask.numel() > 0:\n            diff = diff[valid_mask]\n            if diff.nelement() > 0:\n                self.loss = diff.mean()\n            else:\n                print(\n                    ""warning: diff.nelement()==0 in PhotometricLoss (this is expected during early stage of training, try larger batch size).""\n                )\n                self.loss = 0\n        else:\n            print(""warning: 0 valid pixel in PhotometricLoss"")\n            self.loss = 0\n        return self.loss\n\n\nclass SmoothnessLoss(nn.Module):\n    def __init__(self):\n        super(SmoothnessLoss, self).__init__()\n\n    def forward(self, depth):\n        def second_derivative(x):\n            assert x.dim(\n            ) == 4, ""expected 4-dimensional data, but instead got {}"".format(\n                x.dim())\n            horizontal = 2 * x[:, :, 1:-1, 1:-1] - x[:, :, 1:-1, :\n                                                     -2] - x[:, :, 1:-1, 2:]\n            vertical = 2 * x[:, :, 1:-1, 1:-1] - x[:, :, :-2, 1:\n                                                   -1] - x[:, :, 2:, 1:-1]\n            der_2nd = horizontal.abs() + vertical.abs()\n            return der_2nd.mean()\n\n        self.loss = second_derivative(depth)\n        return self.loss\n'"
helper.py,3,"b'import math\nimport os, time\nimport shutil\nimport torch\nimport csv\nimport vis_utils\nfrom metrics import Result\n\nfieldnames = [\n    \'epoch\', \'rmse\', \'photo\', \'mae\', \'irmse\', \'imae\', \'mse\', \'absrel\', \'lg10\',\n    \'silog\', \'squared_rel\', \'delta1\', \'delta2\', \'delta3\', \'data_time\',\n    \'gpu_time\'\n]\n\n\nclass logger:\n    def __init__(self, args, prepare=True):\n        self.args = args\n        output_directory = get_folder_name(args)\n        self.output_directory = output_directory\n        self.best_result = Result()\n        self.best_result.set_to_worst()\n\n        if not prepare:\n            return\n        if not os.path.exists(output_directory):\n            os.makedirs(output_directory)\n        self.train_csv = os.path.join(output_directory, \'train.csv\')\n        self.val_csv = os.path.join(output_directory, \'val.csv\')\n        self.best_txt = os.path.join(output_directory, \'best.txt\')\n\n        # backup the source code\n        if args.resume == \'\':\n            print(""=> creating source code backup ..."")\n            backup_directory = os.path.join(output_directory, ""code_backup"")\n            self.backup_directory = backup_directory\n            backup_source_code(backup_directory)\n            # create new csv files with only header\n            with open(self.train_csv, \'w\') as csvfile:\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writeheader()\n            with open(self.val_csv, \'w\') as csvfile:\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writeheader()\n            print(""=> finished creating source code backup."")\n\n    def conditional_print(self, split, i, epoch, lr, n_set, blk_avg_meter,\n                          avg_meter):\n        if (i + 1) % self.args.print_freq == 0:\n            avg = avg_meter.average()\n            blk_avg = blk_avg_meter.average()\n            print(\'=> output: {}\'.format(self.output_directory))\n            print(\n                \'{split} Epoch: {0} [{1}/{2}]\\tlr={lr} \'\n                \'t_Data={blk_avg.data_time:.3f}({average.data_time:.3f}) \'\n                \'t_GPU={blk_avg.gpu_time:.3f}({average.gpu_time:.3f})\\n\\t\'\n                \'RMSE={blk_avg.rmse:.2f}({average.rmse:.2f}) \'\n                \'MAE={blk_avg.mae:.2f}({average.mae:.2f}) \'\n                \'iRMSE={blk_avg.irmse:.2f}({average.irmse:.2f}) \'\n                \'iMAE={blk_avg.imae:.2f}({average.imae:.2f})\\n\\t\'\n                \'silog={blk_avg.silog:.2f}({average.silog:.2f}) \'\n                \'squared_rel={blk_avg.squared_rel:.2f}({average.squared_rel:.2f}) \'\n                \'Delta1={blk_avg.delta1:.3f}({average.delta1:.3f}) \'\n                \'REL={blk_avg.absrel:.3f}({average.absrel:.3f})\\n\\t\'\n                \'Lg10={blk_avg.lg10:.3f}({average.lg10:.3f}) \'\n                \'Photometric={blk_avg.photometric:.3f}({average.photometric:.3f}) \'\n                .format(epoch,\n                        i + 1,\n                        n_set,\n                        lr=lr,\n                        blk_avg=blk_avg,\n                        average=avg,\n                        split=split.capitalize()))\n            blk_avg_meter.reset()\n\n    def conditional_save_info(self, split, average_meter, epoch):\n        avg = average_meter.average()\n        if split == ""train"":\n            csvfile_name = self.train_csv\n        elif split == ""val"":\n            csvfile_name = self.val_csv\n        elif split == ""eval"":\n            eval_filename = os.path.join(self.output_directory, \'eval.txt\')\n            self.save_single_txt(eval_filename, avg, epoch)\n            return avg\n        elif ""test"" in split:\n            return avg\n        else:\n            raise ValueError(""wrong split provided to logger"")\n        with open(csvfile_name, \'a\') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n            writer.writerow({\n                \'epoch\': epoch,\n                \'rmse\': avg.rmse,\n                \'photo\': avg.photometric,\n                \'mae\': avg.mae,\n                \'irmse\': avg.irmse,\n                \'imae\': avg.imae,\n                \'mse\': avg.mse,\n                \'silog\': avg.silog,\n                \'squared_rel\': avg.squared_rel,\n                \'absrel\': avg.absrel,\n                \'lg10\': avg.lg10,\n                \'delta1\': avg.delta1,\n                \'delta2\': avg.delta2,\n                \'delta3\': avg.delta3,\n                \'gpu_time\': avg.gpu_time,\n                \'data_time\': avg.data_time\n            })\n        return avg\n\n    def save_single_txt(self, filename, result, epoch):\n        with open(filename, \'w\') as txtfile:\n            txtfile.write(\n                (""rank_metric={}\\n"" + ""epoch={}\\n"" + ""rmse={:.3f}\\n"" +\n                 ""mae={:.3f}\\n"" + ""silog={:.3f}\\n"" + ""squared_rel={:.3f}\\n"" +\n                 ""irmse={:.3f}\\n"" + ""imae={:.3f}\\n"" + ""mse={:.3f}\\n"" +\n                 ""absrel={:.3f}\\n"" + ""lg10={:.3f}\\n"" + ""delta1={:.3f}\\n"" +\n                 ""t_gpu={:.4f}"").format(self.args.rank_metric, epoch,\n                                        result.rmse, result.mae, result.silog,\n                                        result.squared_rel, result.irmse,\n                                        result.imae, result.mse, result.absrel,\n                                        result.lg10, result.delta1,\n                                        result.gpu_time))\n\n    def save_best_txt(self, result, epoch):\n        self.save_single_txt(self.best_txt, result, epoch)\n\n    def _get_img_comparison_name(self, mode, epoch, is_best=False):\n        if mode == \'eval\':\n            return self.output_directory + \'/comparison_eval.png\'\n        if mode == \'val\':\n            if is_best:\n                return self.output_directory + \'/comparison_best.png\'\n            else:\n                return self.output_directory + \'/comparison_\' + str(\n                    epoch) + \'.png\'\n\n    def conditional_save_img_comparison(self, mode, i, ele, pred, epoch):\n        # save 8 images for visualization\n        if mode == \'val\' or mode == \'eval\':\n            skip = 100\n            if i == 0:\n                self.img_merge = vis_utils.merge_into_row(ele, pred)\n            elif i % skip == 0 and i < 8 * skip:\n                row = vis_utils.merge_into_row(ele, pred)\n                self.img_merge = vis_utils.add_row(self.img_merge, row)\n            elif i == 8 * skip:\n                filename = self._get_img_comparison_name(mode, epoch)\n                vis_utils.save_image(self.img_merge, filename)\n\n    def save_img_comparison_as_best(self, mode, epoch):\n        if mode == \'val\':\n            filename = self._get_img_comparison_name(mode, epoch, is_best=True)\n            vis_utils.save_image(self.img_merge, filename)\n\n    def get_ranking_error(self, result):\n        return getattr(result, self.args.rank_metric)\n\n    def rank_conditional_save_best(self, mode, result, epoch):\n        error = self.get_ranking_error(result)\n        best_error = self.get_ranking_error(self.best_result)\n        is_best = error < best_error\n        if is_best and mode == ""val"":\n            self.old_best_result = self.best_result\n            self.best_result = result\n            self.save_best_txt(result, epoch)\n        return is_best\n\n    def conditional_save_pred(self, mode, i, pred, epoch):\n        if (""test"" in mode or mode == ""eval"") and self.args.save_pred:\n\n            # save images for visualization/ testing\n            image_folder = os.path.join(self.output_directory,\n                                        mode + ""_output"")\n            if not os.path.exists(image_folder):\n                os.makedirs(image_folder)\n            img = torch.squeeze(pred.data.cpu()).numpy()\n            filename = os.path.join(image_folder, \'{0:010d}.png\'.format(i))\n            vis_utils.save_depth_as_uint16png(img, filename)\n\n    def conditional_summarize(self, mode, avg, is_best):\n        print(""\\n*\\nSummary of "", mode, ""round"")\n        print(\'\'\n              \'RMSE={average.rmse:.3f}\\n\'\n              \'MAE={average.mae:.3f}\\n\'\n              \'Photo={average.photometric:.3f}\\n\'\n              \'iRMSE={average.irmse:.3f}\\n\'\n              \'iMAE={average.imae:.3f}\\n\'\n              \'squared_rel={average.squared_rel}\\n\'\n              \'silog={average.silog}\\n\'\n              \'Delta1={average.delta1:.3f}\\n\'\n              \'REL={average.absrel:.3f}\\n\'\n              \'Lg10={average.lg10:.3f}\\n\'\n              \'t_GPU={time:.3f}\'.format(average=avg, time=avg.gpu_time))\n        if is_best and mode == ""val"":\n            print(""New best model by %s (was %.3f)"" %\n                  (self.args.rank_metric,\n                   self.get_ranking_error(self.old_best_result)))\n        elif mode == ""val"":\n            print(""(best %s is %.3f)"" %\n                  (self.args.rank_metric,\n                   self.get_ranking_error(self.best_result)))\n        print(""*\\n"")\n\n\nignore_hidden = shutil.ignore_patterns(""."", "".."", "".git*"", ""*pycache*"",\n                                       ""*build"", ""*.fuse*"", ""*_drive_*"")\n\n\ndef backup_source_code(backup_directory):\n    if os.path.exists(backup_directory):\n        shutil.rmtree(backup_directory)\n    shutil.copytree(\'.\', backup_directory, ignore=ignore_hidden)\n\n\ndef adjust_learning_rate(lr_init, optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 5 epochs""""""\n    lr = lr_init * (0.1**(epoch // 5))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\n\ndef save_checkpoint(state, is_best, epoch, output_directory):\n    checkpoint_filename = os.path.join(output_directory,\n                                       \'checkpoint-\' + str(epoch) + \'.pth.tar\')\n    torch.save(state, checkpoint_filename)\n    if is_best:\n        best_filename = os.path.join(output_directory, \'model_best.pth.tar\')\n        shutil.copyfile(checkpoint_filename, best_filename)\n    if epoch > 0:\n        prev_checkpoint_filename = os.path.join(\n            output_directory, \'checkpoint-\' + str(epoch - 1) + \'.pth.tar\')\n        if os.path.exists(prev_checkpoint_filename):\n            os.remove(prev_checkpoint_filename)\n\n\ndef get_folder_name(args):\n    current_time = time.strftime(\'%Y-%m-%d@%H-%M\')\n    if args.use_pose:\n        prefix = ""mode={}.w1={}.w2={}."".format(args.train_mode, args.w1,\n                                               args.w2)\n    else:\n        prefix = ""mode={}."".format(args.train_mode)\n    return os.path.join(args.result,\n        prefix + \'input={}.resnet{}.criterion={}.lr={}.bs={}.wd={}.pretrained={}.jitter={}.time={}\'.\n        format(args.input, args.layers, args.criterion, \\\n            args.lr, args.batch_size, args.weight_decay, \\\n            args.pretrained, args.jitter, current_time\n            ))\n\n\navgpool = torch.nn.AvgPool2d(kernel_size=2, stride=2).cuda()\n\n\ndef multiscale(img):\n    img1 = avgpool(img)\n    img2 = avgpool(img1)\n    img3 = avgpool(img2)\n    img4 = avgpool(img3)\n    img5 = avgpool(img4)\n    return img5, img4, img3, img2, img1\n'"
inverse_warp.py,8,"b""import torch\nimport torch.nn.functional as F\n\n\nclass Intrinsics:\n    def __init__(self, width, height, fu, fv, cu=0, cv=0):\n        self.height, self.width = height, width\n        self.fu, self.fv = fu, fv  # fu, fv: focal length along the horizontal and vertical axes\n\n        # cu, cv: optical center along the horizontal and vertical axes\n        self.cu = cu if cu > 0 else (width - 1) / 2.0\n        self.cv = cv if cv > 0 else (height - 1) / 2.0\n\n        # U, V represent the homogeneous horizontal and vertical coordinates in the pixel space\n        self.U = torch.arange(start=0, end=width).expand(height, width).float()\n        self.V = torch.arange(start=0, end=height).expand(width,\n                                                          height).t().float()\n\n        # X_cam, Y_cam represent the homogeneous x, y coordinates (assuming depth z=1) in the camera coordinate system\n        self.X_cam = (self.U - self.cu) / self.fu\n        self.Y_cam = (self.V - self.cv) / self.fv\n\n        self.is_cuda = False\n\n    def cuda(self):\n        self.X_cam.data = self.X_cam.data.cuda()\n        self.Y_cam.data = self.Y_cam.data.cuda()\n        self.is_cuda = True\n        return self\n\n    def scale(self, height, width):\n        # return a new set of corresponding intrinsic parameters for the scaled image\n        ratio_u = float(width) / self.width\n        ratio_v = float(height) / self.height\n        fu = ratio_u * self.fu\n        fv = ratio_v * self.fv\n        cu = ratio_u * self.cu\n        cv = ratio_v * self.cv\n        new_intrinsics = Intrinsics(width, height, fu, fv, cu, cv)\n        if self.is_cuda:\n            new_intrinsics.cuda()\n        return new_intrinsics\n\n    def __print__(self):\n        print('size=({},{})\\nfocal length=({},{})\\noptical center=({},{})'.\n              format(self.height, self.width, self.fv, self.fu, self.cv,\n                     self.cu))\n\n\ndef image_to_pointcloud(depth, intrinsics):\n    assert depth.dim() == 4\n    assert depth.size(1) == 1\n\n    X = depth * intrinsics.X_cam\n    Y = depth * intrinsics.Y_cam\n    return torch.cat((X, Y, depth), dim=1)\n\n\ndef pointcloud_to_image(pointcloud, intrinsics):\n    assert pointcloud.dim() == 4\n\n    batch_size = pointcloud.size(0)\n    X = pointcloud[:, 0, :, :]  #.view(batch_size, -1)\n    Y = pointcloud[:, 1, :, :]  #.view(batch_size, -1)\n    Z = pointcloud[:, 2, :, :].clamp(min=1e-3)  #.view(batch_size, -1)\n\n    # compute pixel coordinates\n    U_proj = intrinsics.fu * X / Z + intrinsics.cu  # horizontal pixel coordinate\n    V_proj = intrinsics.fv * Y / Z + intrinsics.cv  # vertical pixel coordinate\n\n    # normalization to [-1, 1], required by torch.nn.functional.grid_sample\n    U_proj_normalized = (2 * U_proj / (intrinsics.width - 1) - 1).view(\n        batch_size, -1)\n    V_proj_normalized = (2 * V_proj / (intrinsics.height - 1) - 1).view(\n        batch_size, -1)\n\n    # This was important since PyTorch didn't do as it claimed for points out of boundary\n    # See https://github.com/ClementPinard/SfmLearner-Pytorch/blob/master/inverse_warp.py\n    # Might not be necessary any more\n    U_proj_mask = ((U_proj_normalized > 1) + (U_proj_normalized < -1)).detach()\n    U_proj_normalized[U_proj_mask] = 2\n    V_proj_mask = ((V_proj_normalized > 1) + (V_proj_normalized < -1)).detach()\n    V_proj_normalized[V_proj_mask] = 2\n\n    pixel_coords = torch.stack([U_proj_normalized, V_proj_normalized],\n                               dim=2)  # [B, H*W, 2]\n    return pixel_coords.view(batch_size, intrinsics.height, intrinsics.width,\n                             2)\n\n\ndef batch_multiply(batch_scalar, batch_matrix):\n    # input: batch_scalar of size b, batch_matrix of size b * 3 * 3\n    # output: batch_matrix of size b * 3 * 3\n    batch_size = batch_scalar.size(0)\n    output = batch_matrix.clone()\n    for i in range(batch_size):\n        output[i] = batch_scalar[i] * batch_matrix[i]\n    return output\n\n\ndef transform_curr_to_near(pointcloud_curr, r_mat, t_vec, intrinsics):\n    # translation and rotmat represent the transformation from tgt pose to src pose\n    batch_size = pointcloud_curr.size(0)\n    XYZ_ = torch.bmm(r_mat, pointcloud_curr.view(batch_size, 3, -1))\n\n    X = (XYZ_[:, 0, :] + t_vec[:, 0].unsqueeze(1)).view(\n        -1, 1, intrinsics.height, intrinsics.width)\n    Y = (XYZ_[:, 1, :] + t_vec[:, 1].unsqueeze(1)).view(\n        -1, 1, intrinsics.height, intrinsics.width)\n    Z = (XYZ_[:, 2, :] + t_vec[:, 2].unsqueeze(1)).view(\n        -1, 1, intrinsics.height, intrinsics.width)\n\n    pointcloud_near = torch.cat((X, Y, Z), dim=1)\n\n    return pointcloud_near\n\n\ndef homography_from(rgb_near, depth_curr, r_mat, t_vec, intrinsics):\n    # inverse warp the RGB image from the nearby frame to the current frame\n\n    # to ensure dimension consistency\n    r_mat = r_mat.view(-1, 3, 3)\n    t_vec = t_vec.view(-1, 3)\n\n    # compute source pixel coordinate\n    pointcloud_curr = image_to_pointcloud(depth_curr, intrinsics)\n    pointcloud_near = transform_curr_to_near(pointcloud_curr, r_mat, t_vec,\n                                             intrinsics)\n    pixel_coords_near = pointcloud_to_image(pointcloud_near, intrinsics)\n\n    # the warping\n    warped = F.grid_sample(rgb_near, pixel_coords_near)\n\n    return warped\n"""
main.py,14,"b'import argparse\nimport os\nimport time\n\nimport torch\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\n\nfrom dataloaders.kitti_loader import load_calib, oheight, owidth, input_options, KittiDepth\nfrom model import DepthCompletionNet\nfrom metrics import AverageMeter, Result\nimport criteria\nimport helper\nfrom inverse_warp import Intrinsics, homography_from\n\nparser = argparse.ArgumentParser(description=\'Sparse-to-Dense\')\nparser.add_argument(\'-w\',\n                    \'--workers\',\n                    default=4,\n                    type=int,\n                    metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\',\n                    default=11,\n                    type=int,\n                    metavar=\'N\',\n                    help=\'number of total epochs to run (default: 11)\')\nparser.add_argument(\'--start-epoch\',\n                    default=0,\n                    type=int,\n                    metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-c\',\n                    \'--criterion\',\n                    metavar=\'LOSS\',\n                    default=\'l2\',\n                    choices=criteria.loss_names,\n                    help=\'loss function: | \'.join(criteria.loss_names) +\n                    \' (default: l2)\')\nparser.add_argument(\'-b\',\n                    \'--batch-size\',\n                    default=1,\n                    type=int,\n                    help=\'mini-batch size (default: 1)\')\nparser.add_argument(\'--lr\',\n                    \'--learning-rate\',\n                    default=1e-5,\n                    type=float,\n                    metavar=\'LR\',\n                    help=\'initial learning rate (default 1e-5)\')\nparser.add_argument(\'--weight-decay\',\n                    \'--wd\',\n                    default=0,\n                    type=float,\n                    metavar=\'W\',\n                    help=\'weight decay (default: 0)\')\nparser.add_argument(\'--print-freq\',\n                    \'-p\',\n                    default=10,\n                    type=int,\n                    metavar=\'N\',\n                    help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\',\n                    default=\'\',\n                    type=str,\n                    metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--data-folder\',\n                    default=\'../data\',\n                    type=str,\n                    metavar=\'PATH\',\n                    help=\'data folder (default: none)\')\nparser.add_argument(\'-i\',\n                    \'--input\',\n                    type=str,\n                    default=\'gd\',\n                    choices=input_options,\n                    help=\'input: | \'.join(input_options))\nparser.add_argument(\'-l\',\n                    \'--layers\',\n                    type=int,\n                    default=34,\n                    help=\'use 16 for sparse_conv; use 18 or 34 for resnet\')\nparser.add_argument(\'--pretrained\',\n                    action=""store_true"",\n                    help=\'use ImageNet pre-trained weights\')\nparser.add_argument(\'--val\',\n                    type=str,\n                    default=""select"",\n                    choices=[""select"", ""full""],\n                    help=\'full or select validation set\')\nparser.add_argument(\'--jitter\',\n                    type=float,\n                    default=0.1,\n                    help=\'color jitter for images\')\nparser.add_argument(\n    \'--rank-metric\',\n    type=str,\n    default=\'rmse\',\n    choices=[m for m in dir(Result()) if not m.startswith(\'_\')],\n    help=\'metrics for which best result is sbatch_datacted\')\nparser.add_argument(\n    \'-m\',\n    \'--train-mode\',\n    type=str,\n    default=""dense"",\n    choices=[""dense"", ""sparse"", ""photo"", ""sparse+photo"", ""dense+photo""],\n    help=\'dense | sparse | photo | sparse+photo | dense+photo\')\nparser.add_argument(\'-e\', \'--evaluate\', default=\'\', type=str, metavar=\'PATH\')\nparser.add_argument(\'--cpu\', action=""store_true"", help=\'run on cpu\')\n\nargs = parser.parse_args()\nargs.use_pose = (""photo"" in args.train_mode)\n# args.pretrained = not args.no_pretrained\nargs.result = os.path.join(\'..\', \'results\')\nargs.use_rgb = (\'rgb\' in args.input) or args.use_pose\nargs.use_d = \'d\' in args.input\nargs.use_g = \'g\' in args.input\nif args.use_pose:\n    args.w1, args.w2 = 0.1, 0.1\nelse:\n    args.w1, args.w2 = 0, 0\nprint(args)\n\ncuda = torch.cuda.is_available() and not args.cpu\nif cuda:\n    import torch.backends.cudnn as cudnn\n    cudnn.benchmark = True\n    device = torch.device(""cuda"")\nelse:\n    device = torch.device(""cpu"")\nprint(""=> using \'{}\' for computation."".format(device))\n\n# define loss functions\ndepth_criterion = criteria.MaskedMSELoss() if (\n    args.criterion == \'l2\') else criteria.MaskedL1Loss()\nphotometric_criterion = criteria.PhotometricLoss()\nsmoothness_criterion = criteria.SmoothnessLoss()\n\nif args.use_pose:\n    # hard-coded KITTI camera intrinsics\n    K = load_calib()\n    fu, fv = float(K[0, 0]), float(K[1, 1])\n    cu, cv = float(K[0, 2]), float(K[1, 2])\n    kitti_intrinsics = Intrinsics(owidth, oheight, fu, fv, cu, cv)\n    if cuda:\n        kitti_intrinsics = kitti_intrinsics.cuda()\n\n\ndef iterate(mode, args, loader, model, optimizer, logger, epoch):\n    block_average_meter = AverageMeter()\n    average_meter = AverageMeter()\n    meters = [block_average_meter, average_meter]\n\n    # switch to appropriate mode\n    assert mode in [""train"", ""val"", ""eval"", ""test_prediction"", ""test_completion""], \\\n        ""unsupported mode: {}"".format(mode)\n    if mode == \'train\':\n        model.train()\n        lr = helper.adjust_learning_rate(args.lr, optimizer, epoch)\n    else:\n        model.eval()\n        lr = 0\n\n    for i, batch_data in enumerate(loader):\n        start = time.time()\n        batch_data = {\n            key: val.to(device)\n            for key, val in batch_data.items() if val is not None\n        }\n        gt = batch_data[\n            \'gt\'] if mode != \'test_prediction\' and mode != \'test_completion\' else None\n        data_time = time.time() - start\n\n        start = time.time()\n        pred = model(batch_data)\n        depth_loss, photometric_loss, smooth_loss, mask = 0, 0, 0, None\n        if mode == \'train\':\n            # Loss 1: the direct depth supervision from ground truth label\n            # mask=1 indicates that a pixel does not ground truth labels\n            if \'sparse\' in args.train_mode:\n                depth_loss = depth_criterion(pred, batch_data[\'d\'])\n                mask = (batch_data[\'d\'] < 1e-3).float()\n            elif \'dense\' in args.train_mode:\n                depth_loss = depth_criterion(pred, gt)\n                mask = (gt < 1e-3).float()\n\n            # Loss 2: the self-supervised photometric loss\n            if args.use_pose:\n                # create multi-scale pyramids\n                pred_array = helper.multiscale(pred)\n                rgb_curr_array = helper.multiscale(batch_data[\'rgb\'])\n                rgb_near_array = helper.multiscale(batch_data[\'rgb_near\'])\n                if mask is not None:\n                    mask_array = helper.multiscale(mask)\n                num_scales = len(pred_array)\n\n                # compute photometric loss at multiple scales\n                for scale in range(len(pred_array)):\n                    pred_ = pred_array[scale]\n                    rgb_curr_ = rgb_curr_array[scale]\n                    rgb_near_ = rgb_near_array[scale]\n                    mask_ = None\n                    if mask is not None:\n                        mask_ = mask_array[scale]\n\n                    # compute the corresponding intrinsic parameters\n                    height_, width_ = pred_.size(2), pred_.size(3)\n                    intrinsics_ = kitti_intrinsics.scale(height_, width_)\n\n                    # inverse warp from a nearby frame to the current frame\n                    warped_ = homography_from(rgb_near_, pred_,\n                                              batch_data[\'r_mat\'],\n                                              batch_data[\'t_vec\'], intrinsics_)\n                    photometric_loss += photometric_criterion(\n                        rgb_curr_, warped_, mask_) * (2**(scale - num_scales))\n\n            # Loss 3: the depth smoothness loss\n            smooth_loss = smoothness_criterion(pred) if args.w2 > 0 else 0\n\n            # backprop\n            loss = depth_loss + args.w1 * photometric_loss + args.w2 * smooth_loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        gpu_time = time.time() - start\n\n        # measure accuracy and record loss\n        with torch.no_grad():\n            mini_batch_size = next(iter(batch_data.values())).size(0)\n            result = Result()\n            if mode != \'test_prediction\' and mode != \'test_completion\':\n                result.evaluate(pred.data, gt.data, photometric_loss)\n            [\n                m.update(result, gpu_time, data_time, mini_batch_size)\n                for m in meters\n            ]\n            logger.conditional_print(mode, i, epoch, lr, len(loader),\n                                     block_average_meter, average_meter)\n            logger.conditional_save_img_comparison(mode, i, batch_data, pred,\n                                                   epoch)\n            logger.conditional_save_pred(mode, i, pred, epoch)\n\n    avg = logger.conditional_save_info(mode, average_meter, epoch)\n    is_best = logger.rank_conditional_save_best(mode, avg, epoch)\n    if is_best and not (mode == ""train""):\n        logger.save_img_comparison_as_best(mode, epoch)\n    logger.conditional_summarize(mode, avg, is_best)\n\n    return avg, is_best\n\n\ndef main():\n    global args\n    checkpoint = None\n    is_eval = False\n    if args.evaluate:\n        args_new = args\n        if os.path.isfile(args.evaluate):\n            print(""=> loading checkpoint \'{}\' ... "".format(args.evaluate),\n                  end=\'\')\n            checkpoint = torch.load(args.evaluate, map_location=device)\n            args = checkpoint[\'args\']\n            args.data_folder = args_new.data_folder\n            args.val = args_new.val\n            is_eval = True\n            print(""Completed."")\n        else:\n            print(""No model found at \'{}\'"".format(args.evaluate))\n            return\n    elif args.resume:  # optionally resume from a checkpoint\n        args_new = args\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\' ... "".format(args.resume),\n                  end=\'\')\n            checkpoint = torch.load(args.resume, map_location=device)\n            args.start_epoch = checkpoint[\'epoch\'] + 1\n            args.data_folder = args_new.data_folder\n            args.val = args_new.val\n            print(""Completed. Resuming from epoch {}."".format(\n                checkpoint[\'epoch\']))\n        else:\n            print(""No checkpoint found at \'{}\'"".format(args.resume))\n            return\n\n    print(""=> creating model and optimizer ... "", end=\'\')\n    model = DepthCompletionNet(args).to(device)\n    model_named_params = [\n        p for _, p in model.named_parameters() if p.requires_grad\n    ]\n    optimizer = torch.optim.Adam(model_named_params,\n                                 lr=args.lr,\n                                 weight_decay=args.weight_decay)\n    print(""completed."")\n    if checkpoint is not None:\n        model.load_state_dict(checkpoint[\'model\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        print(""=> checkpoint state loaded."")\n\n    model = torch.nn.DataParallel(model)\n\n    # Data loading code\n    print(""=> creating data loaders ... "")\n    if not is_eval:\n        train_dataset = KittiDepth(\'train\', args)\n        train_loader = torch.utils.data.DataLoader(train_dataset,\n                                                   batch_size=args.batch_size,\n                                                   shuffle=True,\n                                                   num_workers=args.workers,\n                                                   pin_memory=True,\n                                                   sampler=None)\n        print(""\\t==> train_loader size:{}"".format(len(train_loader)))\n    val_dataset = KittiDepth(\'val\', args)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=1,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True)  # set batch size to be 1 for validation\n    print(""\\t==> val_loader size:{}"".format(len(val_loader)))\n\n    # create backups and results folder\n    logger = helper.logger(args)\n    if checkpoint is not None:\n        logger.best_result = checkpoint[\'best_result\']\n    print(""=> logger created."")\n\n    if is_eval:\n        print(""=> starting model evaluation ..."")\n        result, is_best = iterate(""val"", args, val_loader, model, None, logger,\n                                  checkpoint[\'epoch\'])\n        return\n\n    # main loop\n    print(""=> starting main loop ..."")\n    for epoch in range(args.start_epoch, args.epochs):\n        print(""=> starting training epoch {} .."".format(epoch))\n        iterate(""train"", args, train_loader, model, optimizer, logger,\n                epoch)  # train for one epoch\n        result, is_best = iterate(""val"", args, val_loader, model, None, logger,\n                                  epoch)  # evaluate on validation set\n        helper.save_checkpoint({ # save checkpoint\n            \'epoch\': epoch,\n            \'model\': model.module.state_dict(),\n            \'best_result\': logger.best_result,\n            \'optimizer\' : optimizer.state_dict(),\n            \'args\' : args,\n        }, is_best, epoch, logger.output_directory)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
metrics.py,5,"b'import torch\nimport math\nimport numpy as np\n\nlg_e_10 = math.log(10)\n\n\ndef log10(x):\n    """"""Convert a new tensor with the base-10 logarithm of the elements of x. """"""\n    return torch.log(x) / lg_e_10\n\n\nclass Result(object):\n    def __init__(self):\n        self.irmse = 0\n        self.imae = 0\n        self.mse = 0\n        self.rmse = 0\n        self.mae = 0\n        self.absrel = 0\n        self.squared_rel = 0\n        self.lg10 = 0\n        self.delta1 = 0\n        self.delta2 = 0\n        self.delta3 = 0\n        self.data_time = 0\n        self.gpu_time = 0\n        self.silog = 0  # Scale invariant logarithmic error [log(m)*100]\n        self.photometric = 0\n\n    def set_to_worst(self):\n        self.irmse = np.inf\n        self.imae = np.inf\n        self.mse = np.inf\n        self.rmse = np.inf\n        self.mae = np.inf\n        self.absrel = np.inf\n        self.squared_rel = np.inf\n        self.lg10 = np.inf\n        self.silog = np.inf\n        self.delta1 = 0\n        self.delta2 = 0\n        self.delta3 = 0\n        self.data_time = 0\n        self.gpu_time = 0\n\n    def update(self, irmse, imae, mse, rmse, mae, absrel, squared_rel, lg10, \\\n            delta1, delta2, delta3, gpu_time, data_time, silog, photometric=0):\n        self.irmse = irmse\n        self.imae = imae\n        self.mse = mse\n        self.rmse = rmse\n        self.mae = mae\n        self.absrel = absrel\n        self.squared_rel = squared_rel\n        self.lg10 = lg10\n        self.delta1 = delta1\n        self.delta2 = delta2\n        self.delta3 = delta3\n        self.data_time = data_time\n        self.gpu_time = gpu_time\n        self.silog = silog\n        self.photometric = photometric\n\n    def evaluate(self, output, target, photometric=0):\n        valid_mask = target > 0.1\n\n        # convert from meters to mm\n        output_mm = 1e3 * output[valid_mask]\n        target_mm = 1e3 * target[valid_mask]\n\n        abs_diff = (output_mm - target_mm).abs()\n\n        self.mse = float((torch.pow(abs_diff, 2)).mean())\n        self.rmse = math.sqrt(self.mse)\n        self.mae = float(abs_diff.mean())\n        self.lg10 = float((log10(output_mm) - log10(target_mm)).abs().mean())\n        self.absrel = float((abs_diff / target_mm).mean())\n        self.squared_rel = float(((abs_diff / target_mm)**2).mean())\n\n        maxRatio = torch.max(output_mm / target_mm, target_mm / output_mm)\n        self.delta1 = float((maxRatio < 1.25).float().mean())\n        self.delta2 = float((maxRatio < 1.25**2).float().mean())\n        self.delta3 = float((maxRatio < 1.25**3).float().mean())\n        self.data_time = 0\n        self.gpu_time = 0\n\n        # silog uses meters\n        err_log = torch.log(target[valid_mask]) - torch.log(output[valid_mask])\n        normalized_squared_log = (err_log**2).mean()\n        log_mean = err_log.mean()\n        self.silog = math.sqrt(normalized_squared_log -\n                               log_mean * log_mean) * 100\n\n        # convert from meters to km\n        inv_output_km = (1e-3 * output[valid_mask])**(-1)\n        inv_target_km = (1e-3 * target[valid_mask])**(-1)\n        abs_inv_diff = (inv_output_km - inv_target_km).abs()\n        self.irmse = math.sqrt((torch.pow(abs_inv_diff, 2)).mean())\n        self.imae = float(abs_inv_diff.mean())\n\n        self.photometric = float(photometric)\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.count = 0.0\n        self.sum_irmse = 0\n        self.sum_imae = 0\n        self.sum_mse = 0\n        self.sum_rmse = 0\n        self.sum_mae = 0\n        self.sum_absrel = 0\n        self.sum_squared_rel = 0\n        self.sum_lg10 = 0\n        self.sum_delta1 = 0\n        self.sum_delta2 = 0\n        self.sum_delta3 = 0\n        self.sum_data_time = 0\n        self.sum_gpu_time = 0\n        self.sum_photometric = 0\n        self.sum_silog = 0\n\n    def update(self, result, gpu_time, data_time, n=1):\n        self.count += n\n        self.sum_irmse += n * result.irmse\n        self.sum_imae += n * result.imae\n        self.sum_mse += n * result.mse\n        self.sum_rmse += n * result.rmse\n        self.sum_mae += n * result.mae\n        self.sum_absrel += n * result.absrel\n        self.sum_squared_rel += n * result.squared_rel\n        self.sum_lg10 += n * result.lg10\n        self.sum_delta1 += n * result.delta1\n        self.sum_delta2 += n * result.delta2\n        self.sum_delta3 += n * result.delta3\n        self.sum_data_time += n * data_time\n        self.sum_gpu_time += n * gpu_time\n        self.sum_silog += n * result.silog\n        self.sum_photometric += n * result.photometric\n\n    def average(self):\n        avg = Result()\n        if self.count > 0:\n            avg.update(\n                self.sum_irmse / self.count, self.sum_imae / self.count,\n                self.sum_mse / self.count, self.sum_rmse / self.count,\n                self.sum_mae / self.count, self.sum_absrel / self.count,\n                self.sum_squared_rel / self.count, self.sum_lg10 / self.count,\n                self.sum_delta1 / self.count, self.sum_delta2 / self.count,\n                self.sum_delta3 / self.count, self.sum_gpu_time / self.count,\n                self.sum_data_time / self.count, self.sum_silog / self.count,\n                self.sum_photometric / self.count)\n        return avg\n'"
model.py,8,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models import resnet\n\n\ndef init_weights(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        m.weight.data.normal_(0, 1e-3)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.ConvTranspose2d):\n        m.weight.data.normal_(0, 1e-3)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n\ndef conv_bn_relu(in_channels, out_channels, kernel_size, \\\n        stride=1, padding=0, bn=True, relu=True):\n    bias = not bn\n    layers = []\n    layers.append(\n        nn.Conv2d(in_channels,\n                  out_channels,\n                  kernel_size,\n                  stride,\n                  padding,\n                  bias=bias))\n    if bn:\n        layers.append(nn.BatchNorm2d(out_channels))\n    if relu:\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n    layers = nn.Sequential(*layers)\n\n    # initialize the weights\n    for m in layers.modules():\n        init_weights(m)\n\n    return layers\n\ndef convt_bn_relu(in_channels, out_channels, kernel_size, \\\n        stride=1, padding=0, output_padding=0, bn=True, relu=True):\n    bias = not bn\n    layers = []\n    layers.append(\n        nn.ConvTranspose2d(in_channels,\n                           out_channels,\n                           kernel_size,\n                           stride,\n                           padding,\n                           output_padding,\n                           bias=bias))\n    if bn:\n        layers.append(nn.BatchNorm2d(out_channels))\n    if relu:\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n    layers = nn.Sequential(*layers)\n\n    # initialize the weights\n    for m in layers.modules():\n        init_weights(m)\n\n    return layers\n\n\nclass DepthCompletionNet(nn.Module):\n    def __init__(self, args):\n        assert (\n            args.layers in [18, 34, 50, 101, 152]\n        ), 'Only layers 18, 34, 50, 101, and 152 are defined, but got {}'.format(\n            layers)\n        super(DepthCompletionNet, self).__init__()\n        self.modality = args.input\n\n        if 'd' in self.modality:\n            channels = 64 // len(self.modality)\n            self.conv1_d = conv_bn_relu(1,\n                                        channels,\n                                        kernel_size=3,\n                                        stride=1,\n                                        padding=1)\n        if 'rgb' in self.modality:\n            channels = 64 * 3 // len(self.modality)\n            self.conv1_img = conv_bn_relu(3,\n                                          channels,\n                                          kernel_size=3,\n                                          stride=1,\n                                          padding=1)\n        elif 'g' in self.modality:\n            channels = 64 // len(self.modality)\n            self.conv1_img = conv_bn_relu(1,\n                                          channels,\n                                          kernel_size=3,\n                                          stride=1,\n                                          padding=1)\n\n        pretrained_model = resnet.__dict__['resnet{}'.format(\n            args.layers)](pretrained=args.pretrained)\n        if not args.pretrained:\n            pretrained_model.apply(init_weights)\n        #self.maxpool = pretrained_model._modules['maxpool']\n        self.conv2 = pretrained_model._modules['layer1']\n        self.conv3 = pretrained_model._modules['layer2']\n        self.conv4 = pretrained_model._modules['layer3']\n        self.conv5 = pretrained_model._modules['layer4']\n        del pretrained_model  # clear memory\n\n        # define number of intermediate channels\n        if args.layers <= 34:\n            num_channels = 512\n        elif args.layers >= 50:\n            num_channels = 2048\n        self.conv6 = conv_bn_relu(num_channels,\n                                  512,\n                                  kernel_size=3,\n                                  stride=2,\n                                  padding=1)\n\n        # decoding layers\n        kernel_size = 3\n        stride = 2\n        self.convt5 = convt_bn_relu(in_channels=512,\n                                    out_channels=256,\n                                    kernel_size=kernel_size,\n                                    stride=stride,\n                                    padding=1,\n                                    output_padding=1)\n        self.convt4 = convt_bn_relu(in_channels=768,\n                                    out_channels=128,\n                                    kernel_size=kernel_size,\n                                    stride=stride,\n                                    padding=1,\n                                    output_padding=1)\n        self.convt3 = convt_bn_relu(in_channels=(256 + 128),\n                                    out_channels=64,\n                                    kernel_size=kernel_size,\n                                    stride=stride,\n                                    padding=1,\n                                    output_padding=1)\n        self.convt2 = convt_bn_relu(in_channels=(128 + 64),\n                                    out_channels=64,\n                                    kernel_size=kernel_size,\n                                    stride=stride,\n                                    padding=1,\n                                    output_padding=1)\n        self.convt1 = convt_bn_relu(in_channels=128,\n                                    out_channels=64,\n                                    kernel_size=kernel_size,\n                                    stride=1,\n                                    padding=1)\n        self.convtf = conv_bn_relu(in_channels=128,\n                                   out_channels=1,\n                                   kernel_size=1,\n                                   stride=1,\n                                   bn=False,\n                                   relu=False)\n\n    def forward(self, x):\n        # first layer\n        if 'd' in self.modality:\n            conv1_d = self.conv1_d(x['d'])\n        if 'rgb' in self.modality:\n            conv1_img = self.conv1_img(x['rgb'])\n        elif 'g' in self.modality:\n            conv1_img = self.conv1_img(x['g'])\n\n        if self.modality == 'rgbd' or self.modality == 'gd':\n            conv1 = torch.cat((conv1_d, conv1_img), 1)\n        else:\n            conv1 = conv1_d if (self.modality == 'd') else conv1_img\n\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)  # batchsize * ? * 176 * 608\n        conv4 = self.conv4(conv3)  # batchsize * ? * 88 * 304\n        conv5 = self.conv5(conv4)  # batchsize * ? * 44 * 152\n        conv6 = self.conv6(conv5)  # batchsize * ? * 22 * 76\n\n        # decoder\n        convt5 = self.convt5(conv6)\n        y = torch.cat((convt5, conv5), 1)\n\n        convt4 = self.convt4(y)\n        y = torch.cat((convt4, conv4), 1)\n\n        convt3 = self.convt3(y)\n        y = torch.cat((convt3, conv3), 1)\n\n        convt2 = self.convt2(y)\n        y = torch.cat((convt2, conv2), 1)\n\n        convt1 = self.convt1(y)\n        y = torch.cat((convt1, conv1), 1)\n\n        y = self.convtf(y)\n\n        if self.training:\n            return 100 * y\n        else:\n            min_distance = 0.9\n            return F.relu(\n                100 * y - min_distance\n            ) + min_distance  # the minimum range of Velodyne is around 3 feet ~= 0.9m\n"""
vis_utils.py,0,"b'import os\nif not (""DISPLAY"" in os.environ):\n    import matplotlib as mpl\n    mpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\ncmap = plt.cm.jet\n\n\ndef depth_colorize(depth):\n    depth = (depth - np.min(depth)) / (np.max(depth) - np.min(depth))\n    depth = 255 * cmap(depth)[:, :, :3]  # H, W, C\n    return depth.astype(\'uint8\')\n\n\ndef merge_into_row(ele, pred):\n    def preprocess_depth(x):\n        y = np.squeeze(x.data.cpu().numpy())\n        return depth_colorize(y)\n\n    # if is gray, transforms to rgb\n    img_list = []\n    if \'rgb\' in ele:\n        rgb = np.squeeze(ele[\'rgb\'][0, ...].data.cpu().numpy())\n        rgb = np.transpose(rgb, (1, 2, 0))\n        img_list.append(rgb)\n    elif \'g\' in ele:\n        g = np.squeeze(ele[\'g\'][0, ...].data.cpu().numpy())\n        g = np.array(Image.fromarray(g).convert(\'RGB\'))\n        img_list.append(g)\n    if \'d\' in ele:\n        img_list.append(preprocess_depth(ele[\'d\'][0, ...]))\n    img_list.append(preprocess_depth(pred[0, ...]))\n    if \'gt\' in ele:\n        img_list.append(preprocess_depth(ele[\'gt\'][0, ...]))\n\n    img_merge = np.hstack(img_list)\n    return img_merge.astype(\'uint8\')\n\n\ndef add_row(img_merge, row):\n    return np.vstack([img_merge, row])\n\n\ndef save_image(img_merge, filename):\n    image_to_write = cv2.cvtColor(img_merge, cv2.COLOR_RGB2BGR)\n    cv2.imwrite(filename, image_to_write)\n\n\ndef save_depth_as_uint16png(img, filename):\n    img = (img * 256).astype(\'uint16\')\n    cv2.imwrite(filename, img)\n\n\nif (""DISPLAY"" in os.environ):\n    f, axarr = plt.subplots(4, 1)\n    plt.tight_layout()\n    plt.ion()\n\n\ndef display_warping(rgb_tgt, pred_tgt, warped):\n    def preprocess(rgb_tgt, pred_tgt, warped):\n        rgb_tgt = 255 * np.transpose(np.squeeze(rgb_tgt.data.cpu().numpy()),\n                                     (1, 2, 0))  # H, W, C\n        # depth = np.squeeze(depth.cpu().numpy())\n        # depth = depth_colorize(depth)\n\n        # convert to log-scale\n        pred_tgt = np.squeeze(pred_tgt.data.cpu().numpy())\n        # pred_tgt[pred_tgt<=0] = 0.9 # remove negative predictions\n        # pred_tgt = np.log10(pred_tgt)\n\n        pred_tgt = depth_colorize(pred_tgt)\n\n        warped = 255 * np.transpose(np.squeeze(warped.data.cpu().numpy()),\n                                    (1, 2, 0))  # H, W, C\n        recon_err = np.absolute(\n            warped.astype(\'float\') - rgb_tgt.astype(\'float\')) * (warped > 0)\n        recon_err = recon_err[:, :, 0] + recon_err[:, :, 1] + recon_err[:, :, 2]\n        recon_err = depth_colorize(recon_err)\n        return rgb_tgt.astype(\'uint8\'), warped.astype(\n            \'uint8\'), recon_err, pred_tgt\n\n    rgb_tgt, warped, recon_err, pred_tgt = preprocess(rgb_tgt, pred_tgt,\n                                                      warped)\n\n    # 1st column\n    column = 0\n    axarr[0].imshow(rgb_tgt)\n    axarr[0].axis(\'off\')\n    axarr[0].axis(\'equal\')\n    # axarr[0, column].set_title(\'rgb_tgt\')\n\n    axarr[1].imshow(warped)\n    axarr[1].axis(\'off\')\n    axarr[1].axis(\'equal\')\n    # axarr[1, column].set_title(\'warped\')\n\n    axarr[2].imshow(recon_err, \'hot\')\n    axarr[2].axis(\'off\')\n    axarr[2].axis(\'equal\')\n    # axarr[2, column].set_title(\'recon_err error\')\n\n    axarr[3].imshow(pred_tgt, \'hot\')\n    axarr[3].axis(\'off\')\n    axarr[3].axis(\'equal\')\n    # axarr[3, column].set_title(\'pred_tgt\')\n\n    # plt.show()\n    plt.pause(0.001)\n'"
dataloaders/kitti_loader.py,1,"b'import os\nimport os.path\nimport glob\nimport fnmatch  # pattern matching\nimport numpy as np\nfrom numpy import linalg as LA\nfrom random import choice\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport cv2\nfrom dataloaders import transforms\nfrom dataloaders.pose_estimator import get_pose_pnp\n\ninput_options = [\'d\', \'rgb\', \'rgbd\', \'g\', \'gd\']\n\n\ndef load_calib():\n    """"""\n    Temporarily hardcoding the calibration matrix using calib file from 2011_09_26\n    """"""\n    calib = open(""dataloaders/calib_cam_to_cam.txt"", ""r"")\n    lines = calib.readlines()\n    P_rect_line = lines[25]\n\n    Proj_str = P_rect_line.split("":"")[1].split("" "")[1:]\n    Proj = np.reshape(np.array([float(p) for p in Proj_str]),\n                      (3, 4)).astype(np.float32)\n    K = Proj[:3, :3]  # camera matrix\n\n    # note: we will take the center crop of the images during augmentation\n    # that changes the optical centers, but not focal lengths\n    K[0, 2] = K[\n        0,\n        2] - 13  # from width = 1242 to 1216, with a 13-pixel cut on both sides\n    K[1, 2] = K[\n        1,\n        2] - 11.5  # from width = 375 to 352, with a 11.5-pixel cut on both sides\n    return K\n\n\ndef get_paths_and_transform(split, args):\n    assert (args.use_d or args.use_rgb\n            or args.use_g), \'no proper input selected\'\n\n    if split == ""train"":\n        transform = train_transform\n        glob_d = os.path.join(\n            args.data_folder,\n            \'data_depth_velodyne/train/*_sync/proj_depth/velodyne_raw/image_0[2,3]/*.png\'\n        )\n        glob_gt = os.path.join(\n            args.data_folder,\n            \'data_depth_annotated/train/*_sync/proj_depth/groundtruth/image_0[2,3]/*.png\'\n        )\n\n        def get_rgb_paths(p):\n            ps = p.split(\'/\')\n            pnew = \'/\'.join([args.data_folder] + [\'data_rgb\'] + ps[-6:-4] +\n                            ps[-2:-1] + [\'data\'] + ps[-1:])\n            return pnew\n    elif split == ""val"":\n        if args.val == ""full"":\n            transform = val_transform\n            glob_d = os.path.join(\n                args.data_folder,\n                \'data_depth_velodyne/val/*_sync/proj_depth/velodyne_raw/image_0[2,3]/*.png\'\n            )\n            glob_gt = os.path.join(\n                args.data_folder,\n                \'data_depth_annotated/val/*_sync/proj_depth/groundtruth/image_0[2,3]/*.png\'\n            )\n            def get_rgb_paths(p):\n                ps = p.split(\'/\')\n                pnew = \'/\'.join(ps[:-7] +  \n                    [\'data_rgb\']+ps[-6:-4]+ps[-2:-1]+[\'data\']+ps[-1:])\n                return pnew\n        elif args.val == ""select"":\n            transform = no_transform\n            glob_d = os.path.join(\n                args.data_folder,\n                ""depth_selection/val_selection_cropped/velodyne_raw/*.png"")\n            glob_gt = os.path.join(\n                args.data_folder,\n                ""depth_selection/val_selection_cropped/groundtruth_depth/*.png""\n            )\n            def get_rgb_paths(p):\n                return p.replace(""groundtruth_depth"",""image"")\n    elif split == ""test_completion"":\n        transform = no_transform\n        glob_d = os.path.join(\n            args.data_folder,\n            ""depth_selection/test_depth_completion_anonymous/velodyne_raw/*.png""\n        )\n        glob_gt = None  #""test_depth_completion_anonymous/""\n        glob_rgb = os.path.join(\n            args.data_folder,\n            ""depth_selection/test_depth_completion_anonymous/image/*.png"")\n    elif split == ""test_prediction"":\n        transform = no_transform\n        glob_d = None\n        glob_gt = None  #""test_depth_completion_anonymous/""\n        glob_rgb = os.path.join(\n            args.data_folder,\n            ""depth_selection/test_depth_prediction_anonymous/image/*.png"")\n    else:\n        raise ValueError(""Unrecognized split "" + str(split))\n\n    if glob_gt is not None:\n        # train or val-full or val-select\n        paths_d = sorted(glob.glob(glob_d)) \n        paths_gt = sorted(glob.glob(glob_gt)) \n        paths_rgb = [get_rgb_paths(p) for p in paths_gt]\n    else:  \n        # test only has d or rgb\n        paths_rgb = sorted(glob.glob(glob_rgb))\n        paths_gt = [None] * len(paths_rgb)\n        if split == ""test_prediction"":\n            paths_d = [None] * len(\n                paths_rgb)  # test_prediction has no sparse depth\n        else:\n            paths_d = sorted(glob.glob(glob_d))\n\n    if len(paths_d) == 0 and len(paths_rgb) == 0 and len(paths_gt) == 0:\n        raise (RuntimeError(""Found 0 images under {}"".format(glob_gt)))\n    if len(paths_d) == 0 and args.use_d:\n        raise (RuntimeError(""Requested sparse depth but none was found""))\n    if len(paths_rgb) == 0 and args.use_rgb:\n        raise (RuntimeError(""Requested rgb images but none was found""))\n    if len(paths_rgb) == 0 and args.use_g:\n        raise (RuntimeError(""Requested gray images but no rgb was found""))\n    if len(paths_rgb) != len(paths_d) or len(paths_rgb) != len(paths_gt):\n        raise (RuntimeError(""Produced different sizes for datasets""))\n\n    paths = {""rgb"": paths_rgb, ""d"": paths_d, ""gt"": paths_gt}\n    return paths, transform\n\n\ndef rgb_read(filename):\n    assert os.path.exists(filename), ""file not found: {}"".format(filename)\n    img_file = Image.open(filename)\n    # rgb_png = np.array(img_file, dtype=float) / 255.0 # scale pixels to the range [0,1]\n    rgb_png = np.array(img_file, dtype=\'uint8\')  # in the range [0,255]\n    img_file.close()\n    return rgb_png\n\n\ndef depth_read(filename):\n    # loads depth map D from png file\n    # and returns it as a numpy array,\n    # for details see readme.txt\n    assert os.path.exists(filename), ""file not found: {}"".format(filename)\n    img_file = Image.open(filename)\n    depth_png = np.array(img_file, dtype=int)\n    img_file.close()\n    # make sure we have a proper 16bit depth map here.. not 8bit!\n    assert np.max(depth_png) > 255, \\\n        ""np.max(depth_png)={}, path={}"".format(np.max(depth_png),filename)\n\n    depth = depth_png.astype(np.float) / 256.\n    # depth[depth_png == 0] = -1.\n    depth = np.expand_dims(depth, -1)\n    return depth\n\n\noheight, owidth = 352, 1216\n\n\ndef drop_depth_measurements(depth, prob_keep):\n    mask = np.random.binomial(1, prob_keep, depth.shape)\n    depth *= mask\n    return depth\n\n\ndef train_transform(rgb, sparse, target, rgb_near, args):\n    # s = np.random.uniform(1.0, 1.5) # random scaling\n    # angle = np.random.uniform(-5.0, 5.0) # random rotation degrees\n    do_flip = np.random.uniform(0.0, 1.0) < 0.5  # random horizontal flip\n\n    transform_geometric = transforms.Compose([\n        # transforms.Rotate(angle),\n        # transforms.Resize(s),\n        transforms.BottomCrop((oheight, owidth)),\n        transforms.HorizontalFlip(do_flip)\n    ])\n    if sparse is not None:\n        sparse = transform_geometric(sparse)\n    target = transform_geometric(target)\n    if rgb is not None:\n        brightness = np.random.uniform(max(0, 1 - args.jitter),\n                                       1 + args.jitter)\n        contrast = np.random.uniform(max(0, 1 - args.jitter), 1 + args.jitter)\n        saturation = np.random.uniform(max(0, 1 - args.jitter),\n                                       1 + args.jitter)\n        transform_rgb = transforms.Compose([\n            transforms.ColorJitter(brightness, contrast, saturation, 0),\n            transform_geometric\n        ])\n        rgb = transform_rgb(rgb)\n        if rgb_near is not None:\n            rgb_near = transform_rgb(rgb_near)\n    # sparse = drop_depth_measurements(sparse, 0.9)\n\n    return rgb, sparse, target, rgb_near\n\n\ndef val_transform(rgb, sparse, target, rgb_near, args):\n    transform = transforms.Compose([\n        transforms.BottomCrop((oheight, owidth)),\n    ])\n    if rgb is not None:\n        rgb = transform(rgb)\n    if sparse is not None:\n        sparse = transform(sparse)\n    if target is not None:\n        target = transform(target)\n    if rgb_near is not None:\n        rgb_near = transform(rgb_near)\n    return rgb, sparse, target, rgb_near\n\n\ndef no_transform(rgb, sparse, target, rgb_near, args):\n    return rgb, sparse, target, rgb_near\n\n\nto_tensor = transforms.ToTensor()\nto_float_tensor = lambda x: to_tensor(x).float()\n\n\ndef handle_gray(rgb, args):\n    if rgb is None:\n        return None, None\n    if not args.use_g:\n        return rgb, None\n    else:\n        img = np.array(Image.fromarray(rgb).convert(\'L\'))\n        img = np.expand_dims(img, -1)\n        if not args.use_rgb:\n            rgb_ret = None\n        else:\n            rgb_ret = rgb\n        return rgb_ret, img\n\n\ndef get_rgb_near(path, args):\n    assert path is not None, ""path is None""\n\n    def extract_frame_id(filename):\n        head, tail = os.path.split(filename)\n        number_string = tail[0:tail.find(\'.\')]\n        number = int(number_string)\n        return head, number\n\n    def get_nearby_filename(filename, new_id):\n        head, _ = os.path.split(filename)\n        new_filename = os.path.join(head, \'%010d.png\' % new_id)\n        return new_filename\n\n    head, number = extract_frame_id(path)\n    count = 0\n    max_frame_diff = 3\n    candidates = [\n        i - max_frame_diff for i in range(max_frame_diff * 2 + 1)\n        if i - max_frame_diff != 0\n    ]\n    while True:\n        random_offset = choice(candidates)\n        path_near = get_nearby_filename(path, number + random_offset)\n        if os.path.exists(path_near):\n            break\n        assert count < 20, ""cannot find a nearby frame in 20 trials for {}"".format(\n            path_rgb_tgt)\n\n    return rgb_read(path_near)\n\n\nclass KittiDepth(data.Dataset):\n    """"""A data loader for the Kitti dataset\n    """"""\n    def __init__(self, split, args):\n        self.args = args\n        self.split = split\n        paths, transform = get_paths_and_transform(split, args)\n        self.paths = paths\n        self.transform = transform\n        self.K = load_calib()\n        self.threshold_translation = 0.1\n\n    def __getraw__(self, index):\n        rgb = rgb_read(self.paths[\'rgb\'][index]) if \\\n            (self.paths[\'rgb\'][index] is not None and (self.args.use_rgb or self.args.use_g)) else None\n        sparse = depth_read(self.paths[\'d\'][index]) if \\\n            (self.paths[\'d\'][index] is not None and self.args.use_d) else None\n        target = depth_read(self.paths[\'gt\'][index]) if \\\n            self.paths[\'gt\'][index] is not None else None\n        rgb_near = get_rgb_near(self.paths[\'rgb\'][index], self.args) if \\\n            self.split == \'train\' and self.args.use_pose else None\n        return rgb, sparse, target, rgb_near\n\n    def __getitem__(self, index):\n        rgb, sparse, target, rgb_near = self.__getraw__(index)\n        rgb, sparse, target, rgb_near = self.transform(rgb, sparse, target,\n                                                       rgb_near, self.args)\n        r_mat, t_vec = None, None\n        if self.split == \'train\' and self.args.use_pose:\n            success, r_vec, t_vec = get_pose_pnp(rgb, rgb_near, sparse, self.K)\n            # discard if translation is too small\n            success = success and LA.norm(t_vec) > self.threshold_translation\n            if success:\n                r_mat, _ = cv2.Rodrigues(r_vec)\n            else:\n                # return the same image and no motion when PnP fails\n                rgb_near = rgb\n                t_vec = np.zeros((3, 1))\n                r_mat = np.eye(3)\n\n        rgb, gray = handle_gray(rgb, self.args)\n        candidates = {""rgb"":rgb, ""d"":sparse, ""gt"":target, \\\n            ""g"":gray, ""r_mat"":r_mat, ""t_vec"":t_vec, ""rgb_near"":rgb_near}\n        items = {\n            key: to_float_tensor(val)\n            for key, val in candidates.items() if val is not None\n        }\n\n        return items\n\n    def __len__(self):\n        return len(self.paths[\'gt\'])\n'"
dataloaders/pose_estimator.py,0,"b""import cv2\nimport numpy as np\n\n\ndef rgb2gray(rgb):\n    return np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n\n\ndef convert_2d_to_3d(u, v, z, K):\n    v0 = K[1][2]\n    u0 = K[0][2]\n    fy = K[1][1]\n    fx = K[0][0]\n    x = (u - u0) * z / fx\n    y = (v - v0) * z / fy\n    return (x, y, z)\n\n\ndef feature_match(img1, img2):\n    r''' Find features on both images and match them pairwise\n   '''\n    max_n_features = 1000\n    # max_n_features = 500\n    use_flann = False  # better not use flann\n\n    detector = cv2.xfeatures2d.SIFT_create(max_n_features)\n\n    # find the keypoints and descriptors with SIFT\n    kp1, des1 = detector.detectAndCompute(img1, None)\n    kp2, des2 = detector.detectAndCompute(img2, None)\n    if (des1 is None) or (des2 is None):\n        return [], []\n    des1 = des1.astype(np.float32)\n    des2 = des2.astype(np.float32)\n\n    if use_flann:\n        # FLANN parameters\n        FLANN_INDEX_KDTREE = 0\n        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n        search_params = dict(checks=50)\n        flann = cv2.FlannBasedMatcher(index_params, search_params)\n        matches = flann.knnMatch(des1, des2, k=2)\n    else:\n        matcher = cv2.DescriptorMatcher().create('BruteForce')\n        matches = matcher.knnMatch(des1, des2, k=2)\n\n    good = []\n    pts1 = []\n    pts2 = []\n    # ratio test as per Lowe's paper\n    for i, (m, n) in enumerate(matches):\n        if m.distance < 0.8 * n.distance:\n            good.append(m)\n            pts2.append(kp2[m.trainIdx].pt)\n            pts1.append(kp1[m.queryIdx].pt)\n\n    pts1 = np.int32(pts1)\n    pts2 = np.int32(pts2)\n    return pts1, pts2\n\n\ndef get_pose_pnp(rgb_curr, rgb_near, depth_curr, K):\n    gray_curr = rgb2gray(rgb_curr).astype(np.uint8)\n    gray_near = rgb2gray(rgb_near).astype(np.uint8)\n    height, width = gray_curr.shape\n\n    pts2d_curr, pts2d_near = feature_match(gray_curr,\n                                           gray_near)  # feature matching\n\n    # dilation of depth\n    kernel = np.ones((4, 4), np.uint8)\n    depth_curr_dilated = cv2.dilate(depth_curr, kernel)\n\n    # extract 3d pts\n    pts3d_curr = []\n    pts2d_near_filtered = [\n    ]  # keep only feature points with depth in the current frame\n    for i, pt2d in enumerate(pts2d_curr):\n        # print(pt2d)\n        u, v = pt2d[0], pt2d[1]\n        z = depth_curr_dilated[v, u]\n        if z > 0:\n            xyz_curr = convert_2d_to_3d(u, v, z, K)\n            pts3d_curr.append(xyz_curr)\n            pts2d_near_filtered.append(pts2d_near[i])\n\n    # the minimal number of points accepted by solvePnP is 4:\n    if len(pts3d_curr) >= 4 and len(pts2d_near_filtered) >= 4:\n        pts3d_curr = np.expand_dims(np.array(pts3d_curr).astype(np.float32),\n                                    axis=1)\n        pts2d_near_filtered = np.expand_dims(\n            np.array(pts2d_near_filtered).astype(np.float32), axis=1)\n\n        # ransac\n        ret = cv2.solvePnPRansac(pts3d_curr,\n                                 pts2d_near_filtered,\n                                 K,\n                                 distCoeffs=None)\n        success = ret[0]\n        rotation_vector = ret[1]\n        translation_vector = ret[2]\n        return (success, rotation_vector, translation_vector)\n    else:\n        return (0, None, None)\n"""
dataloaders/transforms.py,5,"b'from __future__ import division\nimport torch\nimport math\nimport random\n\nfrom PIL import Image, ImageOps, ImageEnhance\ntry:\n    import accimage\nexcept ImportError:\n    accimage = None\n\nimport numpy as np\nimport numbers\nimport types\nimport collections\nimport warnings\n\nimport scipy.ndimage.interpolation as itpl\nimport skimage.transform\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\ndef _is_pil_image(img):\n    if accimage is not None:\n        return isinstance(img, (Image.Image, accimage.Image))\n    else:\n        return isinstance(img, Image.Image)\n\n\ndef _is_tensor_image(img):\n    return torch.is_tensor(img) and img.ndimension() == 3\n\n\ndef adjust_brightness(img, brightness_factor):\n    """"""Adjust brightness of an Image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        brightness_factor (float):  How much to adjust the brightness. Can be\n            any non negative number. 0 gives a black image, 1 gives the\n            original image while 2 increases the brightness by a factor of 2.\n\n    Returns:\n        PIL Image: Brightness adjusted image.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img\n\n\ndef adjust_contrast(img, contrast_factor):\n    """"""Adjust contrast of an Image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        contrast_factor (float): How much to adjust the contrast. Can be any\n            non negative number. 0 gives a solid gray image, 1 gives the\n            original image while 2 increases the contrast by a factor of 2.\n\n    Returns:\n        PIL Image: Contrast adjusted image.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img\n\n\ndef adjust_saturation(img, saturation_factor):\n    """"""Adjust color saturation of an image.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        saturation_factor (float):  How much to adjust the saturation. 0 will\n            give a black and white image, 1 will give the original image while\n            2 will enhance the saturation by a factor of 2.\n\n    Returns:\n        PIL Image: Saturation adjusted image.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img\n\n\ndef adjust_hue(img, hue_factor):\n    """"""Adjust hue of an image.\n\n    The image hue is adjusted by converting the image to HSV and\n    cyclically shifting the intensities in the hue channel (H).\n    The image is then converted back to original image mode.\n\n    `hue_factor` is the amount of shift in H channel and must be in the\n    interval `[-0.5, 0.5]`.\n\n    See https://en.wikipedia.org/wiki/Hue for more details on Hue.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        hue_factor (float):  How much to shift the hue channel. Should be in\n            [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in\n            HSV space in positive and negative direction respectively.\n            0 means no shift. Therefore, both -0.5 and 0.5 will give an image\n            with complementary colors while 0 gives the original image.\n\n    Returns:\n        PIL Image: Hue adjusted image.\n    """"""\n    if not (-0.5 <= hue_factor <= 0.5):\n        raise ValueError(\n            \'hue_factor is not in [-0.5, 0.5].\'.format(hue_factor))\n\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    input_mode = img.mode\n    if input_mode in {\'L\', \'1\', \'I\', \'F\'}:\n        return img\n\n    h, s, v = img.convert(\'HSV\').split()\n\n    np_h = np.array(h, dtype=np.uint8)\n    # uint8 addition take cares of rotation across boundaries\n    with np.errstate(over=\'ignore\'):\n        np_h += np.uint8(hue_factor * 255)\n    h = Image.fromarray(np_h, \'L\')\n\n    img = Image.merge(\'HSV\', (h, s, v)).convert(input_mode)\n    return img\n\n\ndef adjust_gamma(img, gamma, gain=1):\n    """"""Perform gamma correction on an image.\n\n    Also known as Power Law Transform. Intensities in RGB mode are adjusted\n    based on the following equation:\n\n        I_out = 255 * gain * ((I_in / 255) ** gamma)\n\n    See https://en.wikipedia.org/wiki/Gamma_correction for more details.\n\n    Args:\n        img (PIL Image): PIL Image to be adjusted.\n        gamma (float): Non negative real number. gamma larger than 1 make the\n            shadows darker, while gamma smaller than 1 make dark regions\n            lighter.\n        gain (float): The constant multiplier.\n    """"""\n    if not _is_pil_image(img):\n        raise TypeError(\'img should be PIL Image. Got {}\'.format(type(img)))\n\n    if gamma < 0:\n        raise ValueError(\'Gamma should be a non-negative real number\')\n\n    input_mode = img.mode\n    img = img.convert(\'RGB\')\n\n    np_img = np.array(img, dtype=np.float32)\n    np_img = 255 * gain * ((np_img / 255)**gamma)\n    np_img = np.uint8(np.clip(np_img, 0, 255))\n\n    img = Image.fromarray(np_img, \'RGB\').convert(input_mode)\n    return img\n\n\nclass Compose(object):\n    """"""Composes several transforms together.\n\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n\nclass ToTensor(object):\n    """"""Convert a ``numpy.ndarray`` to tensor.\n\n    Converts a numpy.ndarray (H x W x C) to a torch.FloatTensor of shape (C x H x W).\n    """"""\n    def __call__(self, img):\n        """"""Convert a ``numpy.ndarray`` to tensor.\n\n        Args:\n            img (numpy.ndarray): Image to be converted to tensor.\n\n        Returns:\n            Tensor: Converted image.\n        """"""\n        if not (_is_numpy_image(img)):\n            raise TypeError(\'img should be ndarray. Got {}\'.format(type(img)))\n\n        if isinstance(img, np.ndarray):\n            # handle numpy array\n            if img.ndim == 3:\n                img = torch.from_numpy(img.transpose((2, 0, 1)).copy())\n            elif img.ndim == 2:\n                img = torch.from_numpy(img.copy())\n            else:\n                raise RuntimeError(\n                    \'img should be ndarray with 2 or 3 dimensions. Got {}\'.\n                    format(img.ndim))\n\n            return img\n\n\nclass NormalizeNumpyArray(object):\n    """"""Normalize a ``numpy.ndarray`` with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``numpy.ndarray`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    """"""\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray): Image of size (H, W, C) to be normalized.\n\n        Returns:\n            Tensor: Normalized image.\n        """"""\n        if not (_is_numpy_image(img)):\n            raise TypeError(\'img should be ndarray. Got {}\'.format(type(img)))\n        # TODO: make efficient\n        print(img.shape)\n        for i in range(3):\n            img[:, :, i] = (img[:, :, i] - self.mean[i]) / self.std[i]\n        return img\n\n\nclass NormalizeTensor(object):\n    """"""Normalize an tensor image with mean and standard deviation.\n    Given mean: ``(M1,...,Mn)`` and std: ``(M1,..,Mn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n    Args:\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n    """"""\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n\n        Returns:\n            Tensor: Normalized Tensor image.\n        """"""\n        if not _is_tensor_image(tensor):\n            raise TypeError(\'tensor is not a torch image.\')\n        # TODO: make efficient\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return tensor\n\n\nclass Rotate(object):\n    """"""Rotates the given ``numpy.ndarray``.\n\n    Args:\n        angle (float): The rotation angle in degrees.\n    """"""\n    def __init__(self, angle):\n        self.angle = angle\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be rotated.\n\n        Returns:\n            img (numpy.ndarray (C x H x W)): Rotated image.\n        """"""\n\n        # order=0 means nearest-neighbor type interpolation\n        return skimage.transform.rotate(img, self.angle, resize=False, order=0)\n\n\nclass Resize(object):\n    """"""Resize the the given ``numpy.ndarray`` to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    """"""\n    def __init__(self, size, interpolation=\'nearest\'):\n        assert isinstance(size, float)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be scaled.\n        Returns:\n            img (numpy.ndarray (C x H x W)): Rescaled image.\n        """"""\n        if img.ndim == 3:\n            return skimage.transform.rescale(img, self.size, order=0)\n        elif img.ndim == 2:\n            return skimage.transform.rescale(img, self.size, order=0)\n        else:\n            RuntimeError(\n                \'img should be ndarray with 2 or 3 dimensions. Got {}\'.format(\n                    img.ndim))\n\n\nclass CenterCrop(object):\n    """"""Crops the given ``numpy.ndarray`` at the center.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    """"""\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    @staticmethod\n    def get_params(img, output_size):\n        """"""Get parameters for ``crop`` for center crop.\n\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for center crop.\n        """"""\n        h = img.shape[0]\n        w = img.shape[1]\n        th, tw = output_size\n        i = int(round((h - th) / 2.))\n        j = int(round((w - tw) / 2.))\n\n        # # randomized cropping\n        # i = np.random.randint(i-3, i+4)\n        # j = np.random.randint(j-3, j+4)\n\n        return i, j, th, tw\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n\n        Returns:\n            img (numpy.ndarray (C x H x W)): Cropped image.\n        """"""\n        i, j, h, w = self.get_params(img, self.size)\n        """"""\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n        """"""\n        if not (_is_numpy_image(img)):\n            raise TypeError(\'img should be ndarray. Got {}\'.format(type(img)))\n        if img.ndim == 3:\n            return img[i:i + h, j:j + w, :]\n        elif img.ndim == 2:\n            return img[i:i + h, j:j + w]\n        else:\n            raise RuntimeError(\n                \'img should be ndarray with 2 or 3 dimensions. Got {}\'.format(\n                    img.ndim))\n\n\nclass BottomCrop(object):\n    """"""Crops the given ``numpy.ndarray`` at the bottom.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    """"""\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    @staticmethod\n    def get_params(img, output_size):\n        """"""Get parameters for ``crop`` for bottom crop.\n\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for bottom crop.\n        """"""\n        h = img.shape[0]\n        w = img.shape[1]\n        th, tw = output_size\n        i = h - th\n        j = int(round((w - tw) / 2.))\n\n        # randomized left and right cropping\n        # i = np.random.randint(i-3, i+4)\n        # j = np.random.randint(j-1, j+1)\n\n        return i, j, th, tw\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n\n        Returns:\n            img (numpy.ndarray (C x H x W)): Cropped image.\n        """"""\n        i, j, h, w = self.get_params(img, self.size)\n        """"""\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n        """"""\n        if not (_is_numpy_image(img)):\n            raise TypeError(\'img should be ndarray. Got {}\'.format(type(img)))\n        if img.ndim == 3:\n            return img[i:i + h, j:j + w, :]\n        elif img.ndim == 2:\n            return img[i:i + h, j:j + w]\n        else:\n            raise RuntimeError(\n                \'img should be ndarray with 2 or 3 dimensions. Got {}\'.format(\n                    img.ndim))\n\n\nclass Crop(object):\n    """"""Crops the given ``numpy.ndarray`` at the center.\n\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    """"""\n    def __init__(self, crop):\n        self.crop = crop\n\n    @staticmethod\n    def get_params(img, crop):\n        """"""Get parameters for ``crop`` for center crop.\n\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for center crop.\n        """"""\n        x_l, x_r, y_b, y_t = crop\n        h = img.shape[0]\n        w = img.shape[1]\n        assert x_l >= 0 and x_l < w\n        assert x_r >= 0 and x_r < w\n        assert y_b >= 0 and y_b < h\n        assert y_t >= 0 and y_t < h\n        assert x_l < x_r and y_b < y_t\n\n        return x_l, x_r, y_b, y_t\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be cropped.\n\n        Returns:\n            img (numpy.ndarray (C x H x W)): Cropped image.\n        """"""\n        x_l, x_r, y_b, y_t = self.get_params(img, self.crop)\n        """"""\n        i: Upper pixel coordinate.\n        j: Left pixel coordinate.\n        h: Height of the cropped image.\n        w: Width of the cropped image.\n        """"""\n        if not (_is_numpy_image(img)):\n            raise TypeError(\'img should be ndarray. Got {}\'.format(type(img)))\n        if img.ndim == 3:\n            return img[y_b:y_t, x_l:x_r, :]\n        elif img.ndim == 2:\n            return img[y_b:y_t, x_l:x_r]\n        else:\n            raise RuntimeError(\n                \'img should be ndarray with 2 or 3 dimensions. Got {}\'.format(\n                    img.ndim))\n\n\nclass Lambda(object):\n    """"""Apply a user-defined lambda as a transform.\n\n    Args:\n        lambd (function): Lambda/function to be used for transform.\n    """"""\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img):\n        return self.lambd(img)\n\n\nclass HorizontalFlip(object):\n    """"""Horizontally flip the given ``numpy.ndarray``.\n\n    Args:\n        do_flip (boolean): whether or not do horizontal flip.\n\n    """"""\n    def __init__(self, do_flip):\n        self.do_flip = do_flip\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray (C x H x W)): Image to be flipped.\n\n        Returns:\n            img (numpy.ndarray (C x H x W)): flipped image.\n        """"""\n        if not (_is_numpy_image(img)):\n            raise TypeError(\'img should be ndarray. Got {}\'.format(type(img)))\n\n        if self.do_flip:\n            return np.fliplr(img)\n        else:\n            return img\n\n\nclass ColorJitter(object):\n    """"""Randomly change the brightness, contrast and saturation of an image.\n\n    Args:\n        brightness (float): How much to jitter brightness. brightness_factor\n            is chosen uniformly from [max(0, 1 - brightness), 1 + brightness].\n        contrast (float): How much to jitter contrast. contrast_factor\n            is chosen uniformly from [max(0, 1 - contrast), 1 + contrast].\n        saturation (float): How much to jitter saturation. saturation_factor\n            is chosen uniformly from [max(0, 1 - saturation), 1 + saturation].\n        hue(float): How much to jitter hue. hue_factor is chosen uniformly from\n            [-hue, hue]. Should be >=0 and <= 0.5.\n    """"""\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        transforms = []\n        transforms.append(\n            Lambda(lambda img: adjust_brightness(img, brightness)))\n        transforms.append(Lambda(lambda img: adjust_contrast(img, contrast)))\n        transforms.append(\n            Lambda(lambda img: adjust_saturation(img, saturation)))\n        transforms.append(Lambda(lambda img: adjust_hue(img, hue)))\n        np.random.shuffle(transforms)\n        self.transform = Compose(transforms)\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (numpy.ndarray (C x H x W)): Input image.\n\n        Returns:\n            img (numpy.ndarray (C x H x W)): Color jittered image.\n        """"""\n        if not (_is_numpy_image(img)):\n            raise TypeError(\'img should be ndarray. Got {}\'.format(type(img)))\n\n        pil = Image.fromarray(img)\n        return np.array(self.transform(pil))\n'"
