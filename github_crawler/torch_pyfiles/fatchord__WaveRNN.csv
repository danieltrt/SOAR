file_path,api_count,code
gen_tacotron.py,4,"b'import torch\nfrom models.fatchord_version import WaveRNN\nfrom utils import hparams as hp\nfrom utils.text.symbols import symbols\nfrom utils.paths import Paths\nfrom models.tacotron import Tacotron\nimport argparse\nfrom utils.text import text_to_sequence\nfrom utils.display import save_attention, simple_table\nfrom utils.dsp import reconstruct_waveform, save_wav\nimport numpy as np\n\nif __name__ == ""__main__"":\n\n    # Parse Arguments\n    parser = argparse.ArgumentParser(description=\'TTS Generator\')\n    parser.add_argument(\'--input_text\', \'-i\', type=str, help=\'[string] Type in something here and TTS will generate it!\')\n    parser.add_argument(\'--tts_weights\', type=str, help=\'[string/path] Load in different Tacotron weights\')\n    parser.add_argument(\'--save_attention\', \'-a\', dest=\'save_attn\', action=\'store_true\', help=\'Save Attention Plots\')\n    parser.add_argument(\'--force_cpu\', \'-c\', action=\'store_true\', help=\'Forces CPU-only training, even when in CUDA capable environment\')\n    parser.add_argument(\'--hp_file\', metavar=\'FILE\', default=\'hparams.py\', help=\'The file to use for the hyperparameters\')\n\n    parser.set_defaults(input_text=None)\n    parser.set_defaults(weights_path=None)\n\n    # name of subcommand goes to args.vocoder\n    subparsers = parser.add_subparsers(required=True, dest=\'vocoder\')\n\n    wr_parser = subparsers.add_parser(\'wavernn\', aliases=[\'wr\'])\n    wr_parser.add_argument(\'--batched\', \'-b\', dest=\'batched\', action=\'store_true\', help=\'Fast Batched Generation\')\n    wr_parser.add_argument(\'--unbatched\', \'-u\', dest=\'batched\', action=\'store_false\', help=\'Slow Unbatched Generation\')\n    wr_parser.add_argument(\'--overlap\', \'-o\', type=int, help=\'[int] number of crossover samples\')\n    wr_parser.add_argument(\'--target\', \'-t\', type=int, help=\'[int] number of samples in each batch index\')\n    wr_parser.add_argument(\'--voc_weights\', type=str, help=\'[string/path] Load in different WaveRNN weights\')\n    wr_parser.set_defaults(batched=None)\n\n    gl_parser = subparsers.add_parser(\'griffinlim\', aliases=[\'gl\'])\n    gl_parser.add_argument(\'--iters\', type=int, default=32, help=\'[int] number of griffinlim iterations\')\n\n    args = parser.parse_args()\n\n    if args.vocoder in [\'griffinlim\', \'gl\']:\n        args.vocoder = \'griffinlim\'\n    elif args.vocoder in [\'wavernn\', \'wr\']:\n        args.vocoder = \'wavernn\'\n    else:\n        raise argparse.ArgumentError(\'Must provide a valid vocoder type!\')\n\n    hp.configure(args.hp_file)  # Load hparams from file\n    # set defaults for any arguments that depend on hparams\n    if args.vocoder == \'wavernn\':\n        if args.target is None:\n            args.target = hp.voc_target\n        if args.overlap is None:\n            args.overlap = hp.voc_overlap\n        if args.batched is None:\n            args.batched = hp.voc_gen_batched\n\n        batched = args.batched\n        target = args.target\n        overlap = args.overlap\n\n    input_text = args.input_text\n    tts_weights = args.tts_weights\n    save_attn = args.save_attn\n\n    paths = Paths(hp.data_path, hp.voc_model_id, hp.tts_model_id)\n\n    if not args.force_cpu and torch.cuda.is_available():\n        device = torch.device(\'cuda\')\n    else:\n        device = torch.device(\'cpu\')\n    print(\'Using device:\', device)\n\n    if args.vocoder == \'wavernn\':\n        print(\'\\nInitialising WaveRNN Model...\\n\')\n        # Instantiate WaveRNN Model\n        voc_model = WaveRNN(rnn_dims=hp.voc_rnn_dims,\n                            fc_dims=hp.voc_fc_dims,\n                            bits=hp.bits,\n                            pad=hp.voc_pad,\n                            upsample_factors=hp.voc_upsample_factors,\n                            feat_dims=hp.num_mels,\n                            compute_dims=hp.voc_compute_dims,\n                            res_out_dims=hp.voc_res_out_dims,\n                            res_blocks=hp.voc_res_blocks,\n                            hop_length=hp.hop_length,\n                            sample_rate=hp.sample_rate,\n                            mode=hp.voc_mode).to(device)\n\n        voc_load_path = args.voc_weights if args.voc_weights else paths.voc_latest_weights\n        voc_model.load(voc_load_path)\n\n    print(\'\\nInitialising Tacotron Model...\\n\')\n\n    # Instantiate Tacotron Model\n    tts_model = Tacotron(embed_dims=hp.tts_embed_dims,\n                         num_chars=len(symbols),\n                         encoder_dims=hp.tts_encoder_dims,\n                         decoder_dims=hp.tts_decoder_dims,\n                         n_mels=hp.num_mels,\n                         fft_bins=hp.num_mels,\n                         postnet_dims=hp.tts_postnet_dims,\n                         encoder_K=hp.tts_encoder_K,\n                         lstm_dims=hp.tts_lstm_dims,\n                         postnet_K=hp.tts_postnet_K,\n                         num_highways=hp.tts_num_highways,\n                         dropout=hp.tts_dropout,\n                         stop_threshold=hp.tts_stop_threshold).to(device)\n\n    tts_load_path = tts_weights if tts_weights else paths.tts_latest_weights\n    tts_model.load(tts_load_path)\n\n    if input_text:\n        inputs = [text_to_sequence(input_text.strip(), hp.tts_cleaner_names)]\n    else:\n        with open(\'sentences.txt\') as f:\n            inputs = [text_to_sequence(l.strip(), hp.tts_cleaner_names) for l in f]\n\n    if args.vocoder == \'wavernn\':\n        voc_k = voc_model.get_step() // 1000\n        tts_k = tts_model.get_step() // 1000\n\n        simple_table([(\'Tacotron\', str(tts_k) + \'k\'),\n                    (\'r\', tts_model.r),\n                    (\'Vocoder Type\', \'WaveRNN\'),\n                    (\'WaveRNN\', str(voc_k) + \'k\'),\n                    (\'Generation Mode\', \'Batched\' if batched else \'Unbatched\'),\n                    (\'Target Samples\', target if batched else \'N/A\'),\n                    (\'Overlap Samples\', overlap if batched else \'N/A\')])\n\n    elif args.vocoder == \'griffinlim\':\n        tts_k = tts_model.get_step() // 1000\n        simple_table([(\'Tacotron\', str(tts_k) + \'k\'),\n                    (\'r\', tts_model.r),\n                    (\'Vocoder Type\', \'Griffin-Lim\'),\n                    (\'GL Iters\', args.iters)])\n\n    for i, x in enumerate(inputs, 1):\n\n        print(f\'\\n| Generating {i}/{len(inputs)}\')\n        _, m, attention = tts_model.generate(x)\n        # Fix mel spectrogram scaling to be from 0 to 1\n        m = (m + 4) / 8\n        np.clip(m, 0, 1, out=m)\n\n        if args.vocoder == \'griffinlim\':\n            v_type = args.vocoder\n        elif args.vocoder == \'wavernn\' and args.batched:\n            v_type = \'wavernn_batched\'\n        else:\n            v_type = \'wavernn_unbatched\'\n\n        if input_text:\n            save_path = paths.tts_output/f\'__input_{input_text[:10]}_{v_type}_{tts_k}k.wav\'\n        else:\n            save_path = paths.tts_output/f\'{i}_{v_type}_{tts_k}k.wav\'\n\n        if save_attn: save_attention(attention, save_path)\n\n        if args.vocoder == \'wavernn\':\n            m = torch.tensor(m).unsqueeze(0)\n            voc_model.generate(m, save_path, batched, hp.voc_target, hp.voc_overlap, hp.mu_law)\n        elif args.vocoder == \'griffinlim\':\n            wav = reconstruct_waveform(m, n_iter=args.iters)\n            save_wav(wav, save_path)\n\n    print(\'\\n\\nDone.\\n\')\n'"
gen_wavernn.py,4,"b'from utils.dataset import get_vocoder_datasets\nfrom utils.dsp import *\nfrom models.fatchord_version import WaveRNN\nfrom utils.paths import Paths\nfrom utils.display import simple_table\nimport torch\nimport argparse\nfrom pathlib import Path\n\n\ndef gen_testset(model: WaveRNN, test_set, samples, batched, target, overlap, save_path: Path):\n\n    k = model.get_step() // 1000\n\n    for i, (m, x) in enumerate(test_set, 1):\n\n        if i > samples: break\n\n        print(\'\\n| Generating: %i/%i\' % (i, samples))\n\n        x = x[0].numpy()\n\n        bits = 16 if hp.voc_mode == \'MOL\' else hp.bits\n\n        if hp.mu_law and hp.voc_mode != \'MOL\':\n            x = decode_mu_law(x, 2**bits, from_labels=True)\n        else:\n            x = label_2_float(x, bits)\n\n        save_wav(x, save_path/f\'{k}k_steps_{i}_target.wav\')\n\n        batch_str = f\'gen_batched_target{target}_overlap{overlap}\' if batched else \'gen_NOT_BATCHED\'\n        save_str = str(save_path/f\'{k}k_steps_{i}_{batch_str}.wav\')\n\n        _ = model.generate(m, save_str, batched, target, overlap, hp.mu_law)\n\n\ndef gen_from_file(model: WaveRNN, load_path: Path, save_path: Path, batched, target, overlap):\n\n    k = model.get_step() // 1000\n    file_name = load_path.stem\n\n    suffix = load_path.suffix\n    if suffix == "".wav"":\n        wav = load_wav(load_path)\n        save_wav(wav, save_path/f\'__{file_name}__{k}k_steps_target.wav\')\n        mel = melspectrogram(wav)\n    elif suffix == "".npy"":\n        mel = np.load(load_path)\n        if mel.ndim != 2 or mel.shape[0] != hp.num_mels:\n            raise ValueError(f\'Expected a numpy array shaped (n_mels, n_hops), but got {wav.shape}!\')\n        _max = np.max(mel)\n        _min = np.min(mel)\n        if _max >= 1.01 or _min <= -0.01:\n            raise ValueError(f\'Expected spectrogram range in [0,1] but was instead [{_min}, {_max}]\')\n    else:\n        raise ValueError(f""Expected an extension of .wav or .npy, but got {suffix}!"")\n\n\n    mel = torch.tensor(mel).unsqueeze(0)\n\n    batch_str = f\'gen_batched_target{target}_overlap{overlap}\' if batched else \'gen_NOT_BATCHED\'\n    save_str = save_path/f\'__{file_name}__{k}k_steps_{batch_str}.wav\'\n\n    _ = model.generate(mel, save_str, batched, target, overlap, hp.mu_law)\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser(description=\'Generate WaveRNN Samples\')\n    parser.add_argument(\'--batched\', \'-b\', dest=\'batched\', action=\'store_true\', help=\'Fast Batched Generation\')\n    parser.add_argument(\'--unbatched\', \'-u\', dest=\'batched\', action=\'store_false\', help=\'Slow Unbatched Generation\')\n    parser.add_argument(\'--samples\', \'-s\', type=int, help=\'[int] number of utterances to generate\')\n    parser.add_argument(\'--target\', \'-t\', type=int, help=\'[int] number of samples in each batch index\')\n    parser.add_argument(\'--overlap\', \'-o\', type=int, help=\'[int] number of crossover samples\')\n    parser.add_argument(\'--file\', \'-f\', type=str, help=\'[string/path] for testing a wav outside dataset\')\n    parser.add_argument(\'--voc_weights\', \'-w\', type=str, help=\'[string/path] Load in different WaveRNN weights\')\n    parser.add_argument(\'--gta\', \'-g\', dest=\'gta\', action=\'store_true\', help=\'Generate from GTA testset\')\n    parser.add_argument(\'--force_cpu\', \'-c\', action=\'store_true\', help=\'Forces CPU-only training, even when in CUDA capable environment\')\n    parser.add_argument(\'--hp_file\', metavar=\'FILE\', default=\'hparams.py\', help=\'The file to use for the hyperparameters\')\n\n    parser.set_defaults(batched=None)\n\n    args = parser.parse_args()\n\n    hp.configure(args.hp_file)  # Load hparams from file\n    # set defaults for any arguments that depend on hparams\n    if args.target is None:\n        args.target = hp.voc_target\n    if args.overlap is None:\n        args.overlap = hp.voc_overlap\n    if args.batched is None:\n        args.batched = hp.voc_gen_batched\n    if args.samples is None:\n        args.samples = hp.voc_gen_at_checkpoint\n\n    batched = args.batched\n    samples = args.samples\n    target = args.target\n    overlap = args.overlap\n    file = args.file\n    gta = args.gta\n\n    if not args.force_cpu and torch.cuda.is_available():\n        device = torch.device(\'cuda\')\n    else:\n        device = torch.device(\'cpu\')\n    print(\'Using device:\', device)\n\n    print(\'\\nInitialising Model...\\n\')\n\n    model = WaveRNN(rnn_dims=hp.voc_rnn_dims,\n                    fc_dims=hp.voc_fc_dims,\n                    bits=hp.bits,\n                    pad=hp.voc_pad,\n                    upsample_factors=hp.voc_upsample_factors,\n                    feat_dims=hp.num_mels,\n                    compute_dims=hp.voc_compute_dims,\n                    res_out_dims=hp.voc_res_out_dims,\n                    res_blocks=hp.voc_res_blocks,\n                    hop_length=hp.hop_length,\n                    sample_rate=hp.sample_rate,\n                    mode=hp.voc_mode).to(device)\n\n    paths = Paths(hp.data_path, hp.voc_model_id, hp.tts_model_id)\n\n    voc_weights = args.voc_weights if args.voc_weights else paths.voc_latest_weights\n\n    model.load(voc_weights)\n\n    simple_table([(\'Generation Mode\', \'Batched\' if batched else \'Unbatched\'),\n                  (\'Target Samples\', target if batched else \'N/A\'),\n                  (\'Overlap Samples\', overlap if batched else \'N/A\')])\n\n    if file:\n        file = Path(file).expanduser()\n        gen_from_file(model, file, paths.voc_output, batched, target, overlap)\n    else:\n        _, test_set = get_vocoder_datasets(paths.data, 1, gta)\n        gen_testset(model, test_set, samples, batched, target, overlap, paths.voc_output)\n\n    print(\'\\n\\nExiting...\\n\')\n'"
hparams.py,0,"b""\n# CONFIG -----------------------------------------------------------------------------------------------------------#\n\n# Here are the input and output data paths (Note: you can override wav_path in preprocess.py)\nwav_path = '/path/to/wav_files/'\ndata_path = 'data/'\n\n# model ids are separate - that way you can use a new tts with an old wavernn and vice versa\n# NB: expect undefined behaviour if models were trained on different DSP settings\nvoc_model_id = 'ljspeech_mol'\ntts_model_id = 'ljspeech_lsa_smooth_attention'\n\n# set this to True if you are only interested in WaveRNN\nignore_tts = False\n\n\n# DSP --------------------------------------------------------------------------------------------------------------#\n\n# Settings for all models\nsample_rate = 22050\nn_fft = 2048\nfft_bins = n_fft // 2 + 1\nnum_mels = 80\nhop_length = 275                    # 12.5ms - in line with Tacotron 2 paper\nwin_length = 1100                   # 50ms - same reason as above\nfmin = 40\nmin_level_db = -100\nref_level_db = 20\nbits = 9                            # bit depth of signal\nmu_law = True                       # Recommended to suppress noise if using raw bits in hp.voc_mode below\npeak_norm = False                   # Normalise to the peak of each wav file\n\n\n# WAVERNN / VOCODER ------------------------------------------------------------------------------------------------#\n\n\n# Model Hparams\nvoc_mode = 'MOL'                    # either 'RAW' (softmax on raw bits) or 'MOL' (sample from mixture of logistics)\nvoc_upsample_factors = (5, 5, 11)   # NB - this needs to correctly factorise hop_length\nvoc_rnn_dims = 512\nvoc_fc_dims = 512\nvoc_compute_dims = 128\nvoc_res_out_dims = 128\nvoc_res_blocks = 10\n\n# Training\nvoc_batch_size = 32\nvoc_lr = 1e-4\nvoc_checkpoint_every = 25_000\nvoc_gen_at_checkpoint = 5           # number of samples to generate at each checkpoint\nvoc_total_steps = 1_000_000         # Total number of training steps\nvoc_test_samples = 50               # How many unseen samples to put aside for testing\nvoc_pad = 2                         # this will pad the input so that the resnet can 'see' wider than input length\nvoc_seq_len = hop_length * 5        # must be a multiple of hop_length\nvoc_clip_grad_norm = 4              # set to None if no gradient clipping needed\n\n# Generating / Synthesizing\nvoc_gen_batched = True              # very fast (realtime+) single utterance batched generation\nvoc_target = 11_000                 # target number of samples to be generated in each batch entry\nvoc_overlap = 550                   # number of samples for crossfading between batches\n\n\n# TACOTRON/TTS -----------------------------------------------------------------------------------------------------#\n\n\n# Model Hparams\ntts_embed_dims = 256                # embedding dimension for the graphemes/phoneme inputs\ntts_encoder_dims = 128\ntts_decoder_dims = 256\ntts_postnet_dims = 128\ntts_encoder_K = 16\ntts_lstm_dims = 512\ntts_postnet_K = 8\ntts_num_highways = 4\ntts_dropout = 0.5\ntts_cleaner_names = ['english_cleaners']\ntts_stop_threshold = -3.4           # Value below which audio generation ends.\n                                    # For example, for a range of [-4, 4], this\n                                    # will terminate the sequence at the first\n                                    # frame that has all values < -3.4\n\n# Training\n\ntts_schedule = [(7,  1e-3,  10_000,  32),   # progressive training schedule\n                (5,  1e-4, 100_000,  32),   # (r, lr, step, batch_size)\n                (2,  1e-4, 180_000,  16),\n                (2,  1e-4, 350_000,  8)]\n\ntts_max_mel_len = 1250              # if you have a couple of extremely long spectrograms you might want to use this\ntts_bin_lengths = True              # bins the spectrogram lengths before sampling in data loader - speeds up training\ntts_clip_grad_norm = 1.0            # clips the gradient norm to prevent explosion - set to None if not needed\ntts_checkpoint_every = 2_000        # checkpoints the model every X steps\n# TODO: tts_phoneme_prob = 0.0              # [0 <-> 1] probability for feeding model phonemes vrs graphemes\n\n\n# ------------------------------------------------------------------------------------------------------------------#\n\n"""
preprocess.py,0,"b'import glob\nfrom utils.display import *\nfrom utils.dsp import *\nfrom utils import hparams as hp\nfrom multiprocessing import Pool, cpu_count\nfrom utils.paths import Paths\nimport pickle\nimport argparse\nfrom utils.text.recipes import ljspeech\nfrom utils.files import get_files\nfrom pathlib import Path\n\n\n# Helper functions for argument types\ndef valid_n_workers(num):\n    n = int(num)\n    if n < 1:\n        raise argparse.ArgumentTypeError(\'%r must be an integer greater than 0\' % num)\n    return n\n\nparser = argparse.ArgumentParser(description=\'Preprocessing for WaveRNN and Tacotron\')\nparser.add_argument(\'--path\', \'-p\', help=\'directly point to dataset path (overrides hparams.wav_path\')\nparser.add_argument(\'--extension\', \'-e\', metavar=\'EXT\', default=\'.wav\', help=\'file extension to search for in dataset folder\')\nparser.add_argument(\'--num_workers\', \'-w\', metavar=\'N\', type=valid_n_workers, default=cpu_count()-1, help=\'The number of worker threads to use for preprocessing\')\nparser.add_argument(\'--hp_file\', metavar=\'FILE\', default=\'hparams.py\', help=\'The file to use for the hyperparameters\')\nargs = parser.parse_args()\n\nhp.configure(args.hp_file)  # Load hparams from file\nif args.path is None:\n    args.path = hp.wav_path\n\nextension = args.extension\npath = args.path\n\n\ndef convert_file(path: Path):\n    y = load_wav(path)\n    peak = np.abs(y).max()\n    if hp.peak_norm or peak > 1.0:\n        y /= peak\n    mel = melspectrogram(y)\n    if hp.voc_mode == \'RAW\':\n        quant = encode_mu_law(y, mu=2**hp.bits) if hp.mu_law else float_2_label(y, bits=hp.bits)\n    elif hp.voc_mode == \'MOL\':\n        quant = float_2_label(y, bits=16)\n\n    return mel.astype(np.float32), quant.astype(np.int64)\n\n\ndef process_wav(path: Path):\n    wav_id = path.stem\n    m, x = convert_file(path)\n    np.save(paths.mel/f\'{wav_id}.npy\', m, allow_pickle=False)\n    np.save(paths.quant/f\'{wav_id}.npy\', x, allow_pickle=False)\n    return wav_id, m.shape[-1]\n\n\nwav_files = get_files(path, extension)\npaths = Paths(hp.data_path, hp.voc_model_id, hp.tts_model_id)\n\nprint(f\'\\n{len(wav_files)} {extension[1:]} files found in ""{path}""\\n\')\n\nif len(wav_files) == 0:\n\n    print(\'Please point wav_path in hparams.py to your dataset,\')\n    print(\'or use the --path option.\\n\')\n\nelse:\n\n    if not hp.ignore_tts:\n\n        text_dict = ljspeech(path)\n\n        with open(paths.data/\'text_dict.pkl\', \'wb\') as f:\n            pickle.dump(text_dict, f)\n\n    n_workers = max(1, args.num_workers)\n\n    simple_table([\n        (\'Sample Rate\', hp.sample_rate),\n        (\'Bit Depth\', hp.bits),\n        (\'Mu Law\', hp.mu_law),\n        (\'Hop Length\', hp.hop_length),\n        (\'CPU Usage\', f\'{n_workers}/{cpu_count()}\')\n    ])\n\n    pool = Pool(processes=n_workers)\n    dataset = []\n\n    for i, (item_id, length) in enumerate(pool.imap_unordered(process_wav, wav_files), 1):\n        dataset += [(item_id, length)]\n        bar = progbar(i, len(wav_files))\n        message = f\'{bar} {i}/{len(wav_files)} \'\n        stream(message)\n\n    with open(paths.data/\'dataset.pkl\', \'wb\') as f:\n        pickle.dump(dataset, f)\n\n    print(\'\\n\\nCompleted. Ready to run ""python train_tacotron.py"" or ""python train_wavernn.py"". \\n\')\n'"
quick_start.py,4,"b'import torch\nfrom models.fatchord_version import WaveRNN\nfrom utils import hparams as hp\nfrom utils.text.symbols import symbols\nfrom models.tacotron import Tacotron\nimport argparse\nfrom utils.text import text_to_sequence\nfrom utils.display import save_attention, simple_table\nimport zipfile, os\n\n\nos.makedirs(\'quick_start/tts_weights/\', exist_ok=True)\nos.makedirs(\'quick_start/voc_weights/\', exist_ok=True)\n\nzip_ref = zipfile.ZipFile(\'pretrained/ljspeech.wavernn.mol.800k.zip\', \'r\')\nzip_ref.extractall(\'quick_start/voc_weights/\')\nzip_ref.close()\n\nzip_ref = zipfile.ZipFile(\'pretrained/ljspeech.tacotron.r2.180k.zip\', \'r\')\nzip_ref.extractall(\'quick_start/tts_weights/\')\nzip_ref.close()\n\n\nif __name__ == ""__main__"":\n\n    # Parse Arguments\n    parser = argparse.ArgumentParser(description=\'TTS Generator\')\n    parser.add_argument(\'--input_text\', \'-i\', type=str, help=\'[string] Type in something here and TTS will generate it!\')\n    parser.add_argument(\'--batched\', \'-b\', dest=\'batched\', action=\'store_true\', help=\'Fast Batched Generation (lower quality)\')\n    parser.add_argument(\'--unbatched\', \'-u\', dest=\'batched\', action=\'store_false\', help=\'Slower Unbatched Generation (better quality)\')\n    parser.add_argument(\'--force_cpu\', \'-c\', action=\'store_true\', help=\'Forces CPU-only training, even when in CUDA capable environment\')\n    parser.add_argument(\'--hp_file\', metavar=\'FILE\', default=\'hparams.py\',\n                        help=\'The file to use for the hyperparameters\')\n    args = parser.parse_args()\n\n    hp.configure(args.hp_file)  # Load hparams from file\n\n    parser.set_defaults(batched=True)\n    parser.set_defaults(input_text=None)\n\n    batched = args.batched\n    input_text = args.input_text\n\n    if not args.force_cpu and torch.cuda.is_available():\n        device = torch.device(\'cuda\')\n    else:\n        device = torch.device(\'cpu\')\n    print(\'Using device:\', device)\n\n    print(\'\\nInitialising WaveRNN Model...\\n\')\n\n    # Instantiate WaveRNN Model\n    voc_model = WaveRNN(rnn_dims=hp.voc_rnn_dims,\n                        fc_dims=hp.voc_fc_dims,\n                        bits=hp.bits,\n                        pad=hp.voc_pad,\n                        upsample_factors=hp.voc_upsample_factors,\n                        feat_dims=hp.num_mels,\n                        compute_dims=hp.voc_compute_dims,\n                        res_out_dims=hp.voc_res_out_dims,\n                        res_blocks=hp.voc_res_blocks,\n                        hop_length=hp.hop_length,\n                        sample_rate=hp.sample_rate,\n                        mode=\'MOL\').to(device)\n\n    voc_model.load(\'quick_start/voc_weights/latest_weights.pyt\')\n\n    print(\'\\nInitialising Tacotron Model...\\n\')\n\n    # Instantiate Tacotron Model\n    tts_model = Tacotron(embed_dims=hp.tts_embed_dims,\n                         num_chars=len(symbols),\n                         encoder_dims=hp.tts_encoder_dims,\n                         decoder_dims=hp.tts_decoder_dims,\n                         n_mels=hp.num_mels,\n                         fft_bins=hp.num_mels,\n                         postnet_dims=hp.tts_postnet_dims,\n                         encoder_K=hp.tts_encoder_K,\n                         lstm_dims=hp.tts_lstm_dims,\n                         postnet_K=hp.tts_postnet_K,\n                         num_highways=hp.tts_num_highways,\n                         dropout=hp.tts_dropout,\n                         stop_threshold=hp.tts_stop_threshold).to(device)\n\n\n    tts_model.load(\'quick_start/tts_weights/latest_weights.pyt\')\n\n    if input_text:\n        inputs = [text_to_sequence(input_text.strip(), hp.tts_cleaner_names)]\n    else:\n        with open(\'sentences.txt\') as f:\n            inputs = [text_to_sequence(l.strip(), hp.tts_cleaner_names) for l in f]\n\n    voc_k = voc_model.get_step() // 1000\n    tts_k = tts_model.get_step() // 1000\n\n    r = tts_model.r\n\n    simple_table([(\'WaveRNN\', str(voc_k) + \'k\'),\n                  (f\'Tacotron(r={r})\', str(tts_k) + \'k\'),\n                  (\'Generation Mode\', \'Batched\' if batched else \'Unbatched\'),\n                  (\'Target Samples\', 11_000 if batched else \'N/A\'),\n                  (\'Overlap Samples\', 550 if batched else \'N/A\')])\n\n    for i, x in enumerate(inputs, 1):\n\n        print(f\'\\n| Generating {i}/{len(inputs)}\')\n        _, m, attention = tts_model.generate(x)\n\n        if input_text:\n            save_path = f\'quick_start/__input_{input_text[:10]}_{tts_k}k.wav\'\n        else:\n            save_path = f\'quick_start/{i}_batched{str(batched)}_{tts_k}k.wav\'\n\n        # save_attention(attention, save_path)\n\n        m = torch.tensor(m).unsqueeze(0)\n        m = (m + 4) / 8\n\n        voc_model.generate(m, save_path, batched, 11_000, 550, hp.mu_law)\n\n    print(\'\\n\\nDone.\\n\')\n'"
train_tacotron.py,9,"b'import torch\nfrom torch import optim\nimport torch.nn.functional as F\nfrom utils import hparams as hp\nfrom utils.display import *\nfrom utils.dataset import get_tts_datasets\nfrom utils.text.symbols import symbols\nfrom utils.paths import Paths\nfrom models.tacotron import Tacotron\nimport argparse\nfrom utils import data_parallel_workaround\nimport os\nfrom pathlib import Path\nimport time\nimport numpy as np\nimport sys\nfrom utils.checkpoints import save_checkpoint, restore_checkpoint\n\n\ndef np_now(x: torch.Tensor): return x.detach().cpu().numpy()\n\n\ndef main():\n    # Parse Arguments\n    parser = argparse.ArgumentParser(description=\'Train Tacotron TTS\')\n    parser.add_argument(\'--force_train\', \'-f\', action=\'store_true\', help=\'Forces the model to train past total steps\')\n    parser.add_argument(\'--force_gta\', \'-g\', action=\'store_true\', help=\'Force the model to create GTA features\')\n    parser.add_argument(\'--force_cpu\', \'-c\', action=\'store_true\', help=\'Forces CPU-only training, even when in CUDA capable environment\')\n    parser.add_argument(\'--hp_file\', metavar=\'FILE\', default=\'hparams.py\', help=\'The file to use for the hyperparameters\')\n    args = parser.parse_args()\n\n    hp.configure(args.hp_file)  # Load hparams from file\n    paths = Paths(hp.data_path, hp.voc_model_id, hp.tts_model_id)\n\n    force_train = args.force_train\n    force_gta = args.force_gta\n\n    if not args.force_cpu and torch.cuda.is_available():\n        device = torch.device(\'cuda\')\n        for session in hp.tts_schedule:\n            _, _, _, batch_size = session\n            if batch_size % torch.cuda.device_count() != 0:\n                raise ValueError(\'`batch_size` must be evenly divisible by n_gpus!\')\n    else:\n        device = torch.device(\'cpu\')\n    print(\'Using device:\', device)\n\n    # Instantiate Tacotron Model\n    print(\'\\nInitialising Tacotron Model...\\n\')\n    model = Tacotron(embed_dims=hp.tts_embed_dims,\n                     num_chars=len(symbols),\n                     encoder_dims=hp.tts_encoder_dims,\n                     decoder_dims=hp.tts_decoder_dims,\n                     n_mels=hp.num_mels,\n                     fft_bins=hp.num_mels,\n                     postnet_dims=hp.tts_postnet_dims,\n                     encoder_K=hp.tts_encoder_K,\n                     lstm_dims=hp.tts_lstm_dims,\n                     postnet_K=hp.tts_postnet_K,\n                     num_highways=hp.tts_num_highways,\n                     dropout=hp.tts_dropout,\n                     stop_threshold=hp.tts_stop_threshold).to(device)\n\n    optimizer = optim.Adam(model.parameters())\n    restore_checkpoint(\'tts\', paths, model, optimizer, create_if_missing=True)\n\n    if not force_gta:\n        for i, session in enumerate(hp.tts_schedule):\n            current_step = model.get_step()\n\n            r, lr, max_step, batch_size = session\n\n            training_steps = max_step - current_step\n\n            # Do we need to change to the next session?\n            if current_step >= max_step:\n                # Are there no further sessions than the current one?\n                if i == len(hp.tts_schedule)-1:\n                    # There are no more sessions. Check if we force training.\n                    if force_train:\n                        # Don\'t finish the loop - train forever\n                        training_steps = 999_999_999\n                    else:\n                        # We have completed training. Breaking is same as continue\n                        break\n                else:\n                    # There is a following session, go to it\n                    continue\n\n            model.r = r\n\n            simple_table([(f\'Steps with r={r}\', str(training_steps//1000) + \'k Steps\'),\n                            (\'Batch Size\', batch_size),\n                            (\'Learning Rate\', lr),\n                            (\'Outputs/Step (r)\', model.r)])\n\n            train_set, attn_example = get_tts_datasets(paths.data, batch_size, r)\n            tts_train_loop(paths, model, optimizer, train_set, lr, training_steps, attn_example)\n\n        print(\'Training Complete.\')\n        print(\'To continue training increase tts_total_steps in hparams.py or use --force_train\\n\')\n\n\n    print(\'Creating Ground Truth Aligned Dataset...\\n\')\n\n    train_set, attn_example = get_tts_datasets(paths.data, 8, model.r)\n    create_gta_features(model, train_set, paths.gta)\n\n    print(\'\\n\\nYou can now train WaveRNN on GTA features - use python train_wavernn.py --gta\\n\')\n\n\ndef tts_train_loop(paths: Paths, model: Tacotron, optimizer, train_set, lr, train_steps, attn_example):\n    device = next(model.parameters()).device  # use same device as model parameters\n\n    for g in optimizer.param_groups: g[\'lr\'] = lr\n\n    total_iters = len(train_set)\n    epochs = train_steps // total_iters + 1\n\n    for e in range(1, epochs+1):\n\n        start = time.time()\n        running_loss = 0\n\n        # Perform 1 epoch\n        for i, (x, m, ids, _) in enumerate(train_set, 1):\n\n            x, m = x.to(device), m.to(device)\n\n            # Parallelize model onto GPUS using workaround due to python bug\n            if device.type == \'cuda\' and torch.cuda.device_count() > 1:\n                m1_hat, m2_hat, attention = data_parallel_workaround(model, x, m)\n            else:\n                m1_hat, m2_hat, attention = model(x, m)\n\n            m1_loss = F.l1_loss(m1_hat, m)\n            m2_loss = F.l1_loss(m2_hat, m)\n\n            loss = m1_loss + m2_loss\n\n            optimizer.zero_grad()\n            loss.backward()\n            if hp.tts_clip_grad_norm is not None:\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hp.tts_clip_grad_norm)\n                if np.isnan(grad_norm):\n                    print(\'grad_norm was NaN!\')\n\n            optimizer.step()\n\n            running_loss += loss.item()\n            avg_loss = running_loss / i\n\n            speed = i / (time.time() - start)\n\n            step = model.get_step()\n            k = step // 1000\n\n            if step % hp.tts_checkpoint_every == 0:\n                ckpt_name = f\'taco_step{k}K\'\n                save_checkpoint(\'tts\', paths, model, optimizer,\n                                name=ckpt_name, is_silent=True)\n\n            if attn_example in ids:\n                idx = ids.index(attn_example)\n                save_attention(np_now(attention[idx][:, :160]), paths.tts_attention/f\'{step}\')\n                save_spectrogram(np_now(m2_hat[idx]), paths.tts_mel_plot/f\'{step}\', 600)\n\n            msg = f\'| Epoch: {e}/{epochs} ({i}/{total_iters}) | Loss: {avg_loss:#.4} | {speed:#.2} steps/s | Step: {k}k | \'\n            stream(msg)\n\n        # Must save latest optimizer state to ensure that resuming training\n        # doesn\'t produce artifacts\n        save_checkpoint(\'tts\', paths, model, optimizer, is_silent=True)\n        model.log(paths.tts_log, msg)\n        print(\' \')\n\n\ndef create_gta_features(model: Tacotron, train_set, save_path: Path):\n    device = next(model.parameters()).device  # use same device as model parameters\n\n    iters = len(train_set)\n\n    for i, (x, mels, ids, mel_lens) in enumerate(train_set, 1):\n\n        x, mels = x.to(device), mels.to(device)\n\n        with torch.no_grad(): _, gta, _ = model(x, mels)\n\n        gta = gta.cpu().numpy()\n\n        for j, item_id in enumerate(ids):\n            mel = gta[j][:, :mel_lens[j]]\n            mel = (mel + 4) / 8\n            np.save(save_path/f\'{item_id}.npy\', mel, allow_pickle=False)\n\n        bar = progbar(i, iters)\n        msg = f\'{bar} {i}/{iters} Batches \'\n        stream(msg)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
train_wavernn.py,7,"b'import time\nimport numpy as np\nimport torch\nfrom torch import optim\nimport torch.nn.functional as F\nfrom utils.display import stream, simple_table\nfrom utils.dataset import get_vocoder_datasets\nfrom utils.distribution import discretized_mix_logistic_loss\nfrom utils import hparams as hp\nfrom models.fatchord_version import WaveRNN\nfrom gen_wavernn import gen_testset\nfrom utils.paths import Paths\nimport argparse\nfrom utils import data_parallel_workaround\nfrom utils.checkpoints import save_checkpoint, restore_checkpoint\n\n\ndef main():\n\n    # Parse Arguments\n    parser = argparse.ArgumentParser(description=\'Train WaveRNN Vocoder\')\n    parser.add_argument(\'--lr\', \'-l\', type=float,  help=\'[float] override hparams.py learning rate\')\n    parser.add_argument(\'--batch_size\', \'-b\', type=int, help=\'[int] override hparams.py batch size\')\n    parser.add_argument(\'--force_train\', \'-f\', action=\'store_true\', help=\'Forces the model to train past total steps\')\n    parser.add_argument(\'--gta\', \'-g\', action=\'store_true\', help=\'train wavernn on GTA features\')\n    parser.add_argument(\'--force_cpu\', \'-c\', action=\'store_true\', help=\'Forces CPU-only training, even when in CUDA capable environment\')\n    parser.add_argument(\'--hp_file\', metavar=\'FILE\', default=\'hparams.py\', help=\'The file to use for the hyperparameters\')\n    args = parser.parse_args()\n\n    hp.configure(args.hp_file)  # load hparams from file\n    if args.lr is None:\n        args.lr = hp.voc_lr\n    if args.batch_size is None:\n        args.batch_size = hp.voc_batch_size\n\n    paths = Paths(hp.data_path, hp.voc_model_id, hp.tts_model_id)\n\n    batch_size = args.batch_size\n    force_train = args.force_train\n    train_gta = args.gta\n    lr = args.lr\n\n    if not args.force_cpu and torch.cuda.is_available():\n        device = torch.device(\'cuda\')\n        if batch_size % torch.cuda.device_count() != 0:\n            raise ValueError(\'`batch_size` must be evenly divisible by n_gpus!\')\n    else:\n        device = torch.device(\'cpu\')\n    print(\'Using device:\', device)\n\n    print(\'\\nInitialising Model...\\n\')\n\n    # Instantiate WaveRNN Model\n    voc_model = WaveRNN(rnn_dims=hp.voc_rnn_dims,\n                        fc_dims=hp.voc_fc_dims,\n                        bits=hp.bits,\n                        pad=hp.voc_pad,\n                        upsample_factors=hp.voc_upsample_factors,\n                        feat_dims=hp.num_mels,\n                        compute_dims=hp.voc_compute_dims,\n                        res_out_dims=hp.voc_res_out_dims,\n                        res_blocks=hp.voc_res_blocks,\n                        hop_length=hp.hop_length,\n                        sample_rate=hp.sample_rate,\n                        mode=hp.voc_mode).to(device)\n\n    # Check to make sure the hop length is correctly factorised\n    assert np.cumprod(hp.voc_upsample_factors)[-1] == hp.hop_length\n\n    optimizer = optim.Adam(voc_model.parameters())\n    restore_checkpoint(\'voc\', paths, voc_model, optimizer, create_if_missing=True)\n\n    train_set, test_set = get_vocoder_datasets(paths.data, batch_size, train_gta)\n\n    total_steps = 10_000_000 if force_train else hp.voc_total_steps\n\n    simple_table([(\'Remaining\', str((total_steps - voc_model.get_step())//1000) + \'k Steps\'),\n                  (\'Batch Size\', batch_size),\n                  (\'LR\', lr),\n                  (\'Sequence Len\', hp.voc_seq_len),\n                  (\'GTA Train\', train_gta)])\n\n    loss_func = F.cross_entropy if voc_model.mode == \'RAW\' else discretized_mix_logistic_loss\n\n    voc_train_loop(paths, voc_model, loss_func, optimizer, train_set, test_set, lr, total_steps)\n\n    print(\'Training Complete.\')\n    print(\'To continue training increase voc_total_steps in hparams.py or use --force_train\')\n\n\ndef voc_train_loop(paths: Paths, model: WaveRNN, loss_func, optimizer, train_set, test_set, lr, total_steps):\n    # Use same device as model parameters\n    device = next(model.parameters()).device\n\n    for g in optimizer.param_groups: g[\'lr\'] = lr\n\n    total_iters = len(train_set)\n    epochs = (total_steps - model.get_step()) // total_iters + 1\n\n    for e in range(1, epochs + 1):\n\n        start = time.time()\n        running_loss = 0.\n\n        for i, (x, y, m) in enumerate(train_set, 1):\n            x, m, y = x.to(device), m.to(device), y.to(device)\n\n            # Parallelize model onto GPUS using workaround due to python bug\n            if device.type == \'cuda\' and torch.cuda.device_count() > 1:\n                y_hat = data_parallel_workaround(model, x, m)\n            else:\n                y_hat = model(x, m)\n\n            if model.mode == \'RAW\':\n                y_hat = y_hat.transpose(1, 2).unsqueeze(-1)\n\n            elif model.mode == \'MOL\':\n                y = y.float()\n\n            y = y.unsqueeze(-1)\n\n\n            loss = loss_func(y_hat, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            if hp.voc_clip_grad_norm is not None:\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), hp.voc_clip_grad_norm)\n                if np.isnan(grad_norm):\n                    print(\'grad_norm was NaN!\')\n            optimizer.step()\n\n            running_loss += loss.item()\n            avg_loss = running_loss / i\n\n            speed = i / (time.time() - start)\n\n            step = model.get_step()\n            k = step // 1000\n\n            if step % hp.voc_checkpoint_every == 0:\n                gen_testset(model, test_set, hp.voc_gen_at_checkpoint, hp.voc_gen_batched,\n                            hp.voc_target, hp.voc_overlap, paths.voc_output)\n                ckpt_name = f\'wave_step{k}K\'\n                save_checkpoint(\'voc\', paths, model, optimizer,\n                                name=ckpt_name, is_silent=True)\n\n            msg = f\'| Epoch: {e}/{epochs} ({i}/{total_iters}) | Loss: {avg_loss:.4f} | {speed:.1f} steps/s | Step: {k}k | \'\n            stream(msg)\n\n        # Must save latest optimizer state to ensure that resuming training\n        # doesn\'t produce artifacts\n        save_checkpoint(\'voc\', paths, model, optimizer, is_silent=True)\n        model.log(paths.voc_log, msg)\n        print(\' \')\n\n\nif __name__ == ""__main__"":\n    main()\n'"
models/__init__.py,0,b''
models/deepmind_version.py,31,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.display import *\nfrom utils.dsp import *\nimport numpy as np\n\nclass WaveRNN(nn.Module):\n    def __init__(self, hidden_size=896, quantisation=256):\n        super(WaveRNN, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.split_size = hidden_size // 2\n        \n        # The main matmul\n        self.R = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)\n        \n        # Output fc layers\n        self.O1 = nn.Linear(self.split_size, self.split_size)\n        self.O2 = nn.Linear(self.split_size, quantisation)\n        self.O3 = nn.Linear(self.split_size, self.split_size)\n        self.O4 = nn.Linear(self.split_size, quantisation)\n        \n        # Input fc layers\n        self.I_coarse = nn.Linear(2, 3 * self.split_size, bias=False)\n        self.I_fine = nn.Linear(3, 3 * self.split_size, bias=False)\n\n        # biases for the gates\n        self.bias_u = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_r = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_e = nn.Parameter(torch.zeros(self.hidden_size))\n        \n        # display num params\n        self.num_params()\n\n        \n    def forward(self, prev_y, prev_hidden, current_coarse):\n        \n        # Main matmul - the projection is split 3 ways\n        R_hidden = self.R(prev_hidden)\n        R_u, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n        \n        # Project the prev input \n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n        \n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n        \n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n        \n        # Compute all gates for coarse and fine \n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n        \n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n        \n        # Compute outputs \n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n    \n        \n    def generate(self, seq_len):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        with torch.no_grad():\n            \n            # First split up the biases for the gates \n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.tensor([0], dtype=torch.long, device=device)\n            out_fine = torch.tensor([0], dtype=torch.long, device=device)\n\n            # We'll meed a hidden state\n            hidden = self.get_initial_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len):\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input \n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()        \n            output = combine_signal(coarse, fine)\n        \n        return output, coarse, fine\n\n    def get_initial_hidden(self, batch_size=1):\n        device = next(self.parameters()).device  # use same device as parameters\n        return torch.zeros(batch_size, self.hidden_size, device=device)\n    \n    def num_params(self, print_out=True):\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        if print_out:\n            print('Trainable Parameters: %.3f million' % parameters)\n        return parameters"""
models/fatchord_version.py,25,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.distribution import sample_from_discretized_mix_logistic\nfrom utils.display import *\nfrom utils.dsp import *\nimport os\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Union\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.batch_norm1 = nn.BatchNorm1d(dims)\n        self.batch_norm2 = nn.BatchNorm1d(dims)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        return x + residual\n\n\nclass MelResNet(nn.Module):\n    def __init__(self, res_blocks, in_dims, compute_dims, res_out_dims, pad):\n        super().__init__()\n        k_size = pad * 2 + 1\n        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n        self.batch_norm = nn.BatchNorm1d(compute_dims)\n        self.layers = nn.ModuleList()\n        for i in range(res_blocks):\n            self.layers.append(ResBlock(compute_dims))\n        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.batch_norm(x)\n        x = F.relu(x)\n        for f in self.layers: x = f(x)\n        x = self.conv_out(x)\n        return x\n\n\nclass Stretch2d(nn.Module):\n    def __init__(self, x_scale, y_scale):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        x = x.unsqueeze(-1).unsqueeze(3)\n        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n        return x.view(b, c, h * self.y_scale, w * self.x_scale)\n\n\nclass UpsampleNetwork(nn.Module):\n    def __init__(self, feat_dims, upsample_scales, compute_dims,\n                 res_blocks, res_out_dims, pad):\n        super().__init__()\n        total_scale = np.cumproduct(upsample_scales)[-1]\n        self.indent = pad * total_scale\n        self.resnet = MelResNet(res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n        self.resnet_stretch = Stretch2d(total_scale, 1)\n        self.up_layers = nn.ModuleList()\n        for scale in upsample_scales:\n            k_size = (1, scale * 2 + 1)\n            padding = (0, scale)\n            stretch = Stretch2d(scale, 1)\n            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n            conv.weight.data.fill_(1. / k_size[1])\n            self.up_layers.append(stretch)\n            self.up_layers.append(conv)\n\n    def forward(self, m):\n        aux = self.resnet(m).unsqueeze(1)\n        aux = self.resnet_stretch(aux)\n        aux = aux.squeeze(1)\n        m = m.unsqueeze(1)\n        for f in self.up_layers: m = f(m)\n        m = m.squeeze(1)[:, :, self.indent:-self.indent]\n        return m.transpose(1, 2), aux.transpose(1, 2)\n\n\nclass WaveRNN(nn.Module):\n    def __init__(self, rnn_dims, fc_dims, bits, pad, upsample_factors,\n                 feat_dims, compute_dims, res_out_dims, res_blocks,\n                 hop_length, sample_rate, mode=\'RAW\'):\n        super().__init__()\n        self.mode = mode\n        self.pad = pad\n        if self.mode == \'RAW\':\n            self.n_classes = 2 ** bits\n        elif self.mode == \'MOL\':\n            self.n_classes = 30\n        else:\n            RuntimeError(""Unknown model mode value - "", self.mode)\n\n        # List of rnns to call `flatten_parameters()` on\n        self._to_flatten = []\n\n        self.rnn_dims = rnn_dims\n        self.aux_dims = res_out_dims // 4\n        self.hop_length = hop_length\n        self.sample_rate = sample_rate\n\n        self.upsample = UpsampleNetwork(feat_dims, upsample_factors, compute_dims, res_blocks, res_out_dims, pad)\n        self.I = nn.Linear(feat_dims + self.aux_dims + 1, rnn_dims)\n\n        self.rnn1 = nn.GRU(rnn_dims, rnn_dims, batch_first=True)\n        self.rnn2 = nn.GRU(rnn_dims + self.aux_dims, rnn_dims, batch_first=True)\n        self._to_flatten += [self.rnn1, self.rnn2]\n\n        self.fc1 = nn.Linear(rnn_dims + self.aux_dims, fc_dims)\n        self.fc2 = nn.Linear(fc_dims + self.aux_dims, fc_dims)\n        self.fc3 = nn.Linear(fc_dims, self.n_classes)\n\n        self.register_buffer(\'step\', torch.zeros(1, dtype=torch.long))\n        self.num_params()\n\n        # Avoid fragmentation of RNN parameters and associated warning\n        self._flatten_parameters()\n\n    def forward(self, x, mels):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        # Although we `_flatten_parameters()` on init, when using DataParallel\n        # the model gets replicated, making it no longer guaranteed that the\n        # weights are contiguous in GPU memory. Hence, we must call it again\n        self._flatten_parameters()\n\n        self.step += 1\n        bsize = x.size(0)\n        h1 = torch.zeros(1, bsize, self.rnn_dims, device=device)\n        h2 = torch.zeros(1, bsize, self.rnn_dims, device=device)\n        mels, aux = self.upsample(mels)\n\n        aux_idx = [self.aux_dims * i for i in range(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, save_path: Union[str, Path], batched, target, overlap, mu_law):\n        self.eval()\n\n        device = next(self.parameters()).device  # use same device as parameters\n\n        mu_law = mu_law if self.mode == \'RAW\' else False\n\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n\n            mels = torch.as_tensor(mels, device=device)\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side=\'both\')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            h1 = torch.zeros(b_size, self.rnn_dims, device=device)\n            h2 = torch.zeros(b_size, self.rnn_dims, device=device)\n            x = torch.zeros(b_size, 1, device=device)\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = \\\n                    (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == \'MOL\':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    # x = torch.FloatTensor([[sample]]).cuda()\n                    x = sample.transpose(0, 1)\n\n                elif self.mode == \'RAW\':\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(""Unknown model mode value - "", self.mode)\n\n                if i % 100 == 0: self.gen_display(i, seq_len, b_size, start)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        save_wav(output, save_path)\n\n        self.train()\n\n        return output\n\n\n    def gen_display(self, i, seq_len, b_size, start):\n        gen_rate = (i + 1) / (time.time() - start) * b_size / 1000\n        pbar = progbar(i, seq_len)\n        msg = f\'| {pbar} {i*b_size}/{seq_len*b_size} | Batch Size: {b_size} | Gen Rate: {gen_rate:.1f}kHz | \'\n        stream(msg)\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n    def pad_tensor(self, x, pad, side=\'both\'):\n        # NB - this is just a quick method i need right now\n        # i.e., it won\'t generalise to other shapes/dims\n        b, t, c = x.size()\n        total = t + 2 * pad if side == \'both\' else t + pad\n        padded = torch.zeros(b, total, c, device=x.device)\n        if side == \'before\' or side == \'both\':\n            padded[:, pad:pad + t, :] = x\n        elif side == \'after\':\n            padded[:, :t, :] = x\n        return padded\n\n    def fold_with_overlap(self, x, target, overlap):\n\n        \'\'\' Fold the tensor with overlap for quick batched inference.\n            Overlap will be used for crossfading in xfade_and_unfold()\n\n        Args:\n            x (tensor)    : Upsampled conditioning features.\n                            shape=(1, timesteps, features)\n            target (int)  : Target timesteps for each index of batch\n            overlap (int) : Timesteps for both xfade and rnn warmup\n\n        Return:\n            (tensor) : shape=(num_folds, target + 2 * overlap, features)\n\n        Details:\n            x = [[h1, h2, ... hn]]\n\n            Where each h is a vector of conditioning features\n\n            Eg: target=2, overlap=1 with x.size(1)=10\n\n            folded = [[h1, h2, h3, h4],\n                      [h4, h5, h6, h7],\n                      [h7, h8, h9, h10]]\n        \'\'\'\n\n        _, total_len, features = x.size()\n\n        # Calculate variables needed\n        num_folds = (total_len - overlap) // (target + overlap)\n        extended_len = num_folds * (overlap + target) + overlap\n        remaining = total_len - extended_len\n\n        # Pad if some time steps poking out\n        if remaining != 0:\n            num_folds += 1\n            padding = target + 2 * overlap - remaining\n            x = self.pad_tensor(x, padding, side=\'after\')\n\n        folded = torch.zeros(num_folds, target + 2 * overlap, features, device=x.device)\n\n        # Get the values for the folded tensor\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            folded[i] = x[:, start:end, :]\n\n        return folded\n\n    def xfade_and_unfold(self, y, target, overlap):\n\n        \'\'\' Applies a crossfade and unfolds into a 1d array.\n\n        Args:\n            y (ndarry)    : Batched sequences of audio samples\n                            shape=(num_folds, target + 2 * overlap)\n                            dtype=np.float64\n            overlap (int) : Timesteps for both xfade and rnn warmup\n\n        Return:\n            (ndarry) : audio samples in a 1d array\n                       shape=(total_len)\n                       dtype=np.float64\n\n        Details:\n            y = [[seq1],\n                 [seq2],\n                 [seq3]]\n\n            Apply a gain envelope at both ends of the sequences\n\n            y = [[seq1_in, seq1_target, seq1_out],\n                 [seq2_in, seq2_target, seq2_out],\n                 [seq3_in, seq3_target, seq3_out]]\n\n            Stagger and add up the groups of samples:\n\n            [seq1_in, seq1_target, (seq1_out + seq2_in), seq2_target, ...]\n\n        \'\'\'\n\n        num_folds, length = y.shape\n        target = length - 2 * overlap\n        total_len = num_folds * (target + overlap) + overlap\n\n        # Need some silence for the rnn warmup\n        silence_len = overlap // 2\n        fade_len = overlap - silence_len\n        silence = np.zeros((silence_len), dtype=np.float64)\n        linear = np.ones((silence_len), dtype=np.float64)\n\n        # Equal power crossfade\n        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n        fade_in = np.sqrt(0.5 * (1 + t))\n        fade_out = np.sqrt(0.5 * (1 - t))\n\n        # Concat the silence to the fades\n        fade_in = np.concatenate([silence, fade_in])\n        fade_out = np.concatenate([linear, fade_out])\n\n        # Apply the gain to the overlap samples\n        y[:, :overlap] *= fade_in\n        y[:, -overlap:] *= fade_out\n\n        unfolded = np.zeros((total_len), dtype=np.float64)\n\n        # Loop to add up all the samples\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            unfolded[start:end] += y[i]\n\n        return unfolded\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def log(self, path, msg):\n        with open(path, \'a\') as f:\n            print(msg, file=f)\n\n    def load(self, path: Union[str, Path]):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        self.load_state_dict(torch.load(path, map_location=device), strict=False)\n\n    def save(self, path: Union[str, Path]):\n        # No optimizer argument because saving a model should not include data\n        # only relevant in the training process - it should only be properties\n        # of the model itself. Let caller take care of saving optimzier state.\n        torch.save(self.state_dict(), path)\n\n    def num_params(self, print_out=True):\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        if print_out:\n            print(\'Trainable Parameters: %.3fM\' % parameters)\n        return parameters\n\n    def _flatten_parameters(self):\n        """"""Calls `flatten_parameters` on all the rnns used by the WaveRNN. Used\n        to improve efficiency and avoid PyTorch yelling at us.""""""\n        [m.flatten_parameters() for m in self._to_flatten]\n'"
models/tacotron.py,37,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom typing import Union\n\n\nclass HighwayNetwork(nn.Module):\n    def __init__(self, size):\n        super().__init__()\n        self.W1 = nn.Linear(size, size)\n        self.W2 = nn.Linear(size, size)\n        self.W1.bias.data.fill_(0.)\n\n    def forward(self, x):\n        x1 = self.W1(x)\n        x2 = self.W2(x)\n        g = torch.sigmoid(x2)\n        y = g * F.relu(x1) + (1. - g) * x\n        return y\n\n\nclass Encoder(nn.Module):\n    def __init__(self, embed_dims, num_chars, cbhg_channels, K, num_highways, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(num_chars, embed_dims)\n        self.pre_net = PreNet(embed_dims)\n        self.cbhg = CBHG(K=K, in_channels=cbhg_channels, channels=cbhg_channels,\n                         proj_channels=[cbhg_channels, cbhg_channels],\n                         num_highways=num_highways)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.pre_net(x)\n        x.transpose_(1, 2)\n        x = self.cbhg(x)\n        return x\n\n\nclass BatchNormConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel, relu=True):\n        super().__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel, stride=1, padding=kernel // 2, bias=False)\n        self.bnorm = nn.BatchNorm1d(out_channels)\n        self.relu = relu\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = F.relu(x) if self.relu is True else x\n        return self.bnorm(x)\n\n\nclass CBHG(nn.Module):\n    def __init__(self, K, in_channels, channels, proj_channels, num_highways):\n        super().__init__()\n\n        # List of all rnns to call `flatten_parameters()` on\n        self._to_flatten = []\n\n        self.bank_kernels = [i for i in range(1, K + 1)]\n        self.conv1d_bank = nn.ModuleList()\n        for k in self.bank_kernels:\n            conv = BatchNormConv(in_channels, channels, k)\n            self.conv1d_bank.append(conv)\n\n        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1)\n\n        self.conv_project1 = BatchNormConv(len(self.bank_kernels) * channels, proj_channels[0], 3)\n        self.conv_project2 = BatchNormConv(proj_channels[0], proj_channels[1], 3, relu=False)\n\n        # Fix the highway input if necessary\n        if proj_channels[-1] != channels:\n            self.highway_mismatch = True\n            self.pre_highway = nn.Linear(proj_channels[-1], channels, bias=False)\n        else:\n            self.highway_mismatch = False\n\n        self.highways = nn.ModuleList()\n        for i in range(num_highways):\n            hn = HighwayNetwork(channels)\n            self.highways.append(hn)\n\n        self.rnn = nn.GRU(channels, channels, batch_first=True, bidirectional=True)\n        self._to_flatten.append(self.rnn)\n\n        # Avoid fragmentation of RNN parameters and associated warning\n        self._flatten_parameters()\n\n    def forward(self, x):\n        # Although we `_flatten_parameters()` on init, when using DataParallel\n        # the model gets replicated, making it no longer guaranteed that the\n        # weights are contiguous in GPU memory. Hence, we must call it again\n        self._flatten_parameters()\n\n        # Save these for later\n        residual = x\n        seq_len = x.size(-1)\n        conv_bank = []\n\n        # Convolution Bank\n        for conv in self.conv1d_bank:\n            c = conv(x) # Convolution\n            conv_bank.append(c[:, :, :seq_len])\n\n        # Stack along the channel axis\n        conv_bank = torch.cat(conv_bank, dim=1)\n\n        # dump the last padding to fit residual\n        x = self.maxpool(conv_bank)[:, :, :seq_len]\n\n        # Conv1d projections\n        x = self.conv_project1(x)\n        x = self.conv_project2(x)\n\n        # Residual Connect\n        x = x + residual\n\n        # Through the highways\n        x = x.transpose(1, 2)\n        if self.highway_mismatch is True:\n            x = self.pre_highway(x)\n        for h in self.highways: x = h(x)\n\n        # And then the RNN\n        x, _ = self.rnn(x)\n        return x\n\n    def _flatten_parameters(self):\n        """"""Calls `flatten_parameters` on all the rnns used by the WaveRNN. Used\n        to improve efficiency and avoid PyTorch yelling at us.""""""\n        [m.flatten_parameters() for m in self._to_flatten]\n\nclass PreNet(nn.Module):\n    def __init__(self, in_dims, fc1_dims=256, fc2_dims=128, dropout=0.5):\n        super().__init__()\n        self.fc1 = nn.Linear(in_dims, fc1_dims)\n        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n        self.p = dropout\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = F.dropout(x, self.p, training=self.training)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = F.dropout(x, self.p, training=self.training)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, attn_dims):\n        super().__init__()\n        self.W = nn.Linear(attn_dims, attn_dims, bias=False)\n        self.v = nn.Linear(attn_dims, 1, bias=False)\n\n    def forward(self, encoder_seq_proj, query, t):\n\n        # print(encoder_seq_proj.shape)\n        # Transform the query vector\n        query_proj = self.W(query).unsqueeze(1)\n\n        # Compute the scores\n        u = self.v(torch.tanh(encoder_seq_proj + query_proj))\n        scores = F.softmax(u, dim=1)\n\n        return scores.transpose(1, 2)\n\n\nclass LSA(nn.Module):\n    def __init__(self, attn_dim, kernel_size=31, filters=32):\n        super().__init__()\n        self.conv = nn.Conv1d(2, filters, padding=(kernel_size - 1) // 2, kernel_size=kernel_size, bias=False)\n        self.L = nn.Linear(filters, attn_dim, bias=True)\n        self.W = nn.Linear(attn_dim, attn_dim, bias=True)\n        self.v = nn.Linear(attn_dim, 1, bias=False)\n        self.cumulative = None\n        self.attention = None\n\n    def init_attention(self, encoder_seq_proj):\n        device = next(self.parameters()).device  # use same device as parameters\n        b, t, c = encoder_seq_proj.size()\n        self.cumulative = torch.zeros(b, t, device=device)\n        self.attention = torch.zeros(b, t, device=device)\n\n    def forward(self, encoder_seq_proj, query, t):\n\n        if t == 0: self.init_attention(encoder_seq_proj)\n\n        processed_query = self.W(query).unsqueeze(1)\n\n        location = torch.cat([self.cumulative.unsqueeze(1), self.attention.unsqueeze(1)], dim=1)\n        processed_loc = self.L(self.conv(location).transpose(1, 2))\n\n        u = self.v(torch.tanh(processed_query + encoder_seq_proj + processed_loc))\n        u = u.squeeze(-1)\n\n        # Smooth Attention\n        scores = torch.sigmoid(u) / torch.sigmoid(u).sum(dim=1, keepdim=True)\n        # scores = F.softmax(u, dim=1)\n        self.attention = scores\n        self.cumulative += self.attention\n\n        return scores.unsqueeze(-1).transpose(1, 2)\n\n\nclass Decoder(nn.Module):\n    # Class variable because its value doesn\'t change between classes\n    # yet ought to be scoped by class because its a property of a Decoder\n    max_r = 20\n    def __init__(self, n_mels, decoder_dims, lstm_dims):\n        super().__init__()\n        self.register_buffer(\'r\', torch.tensor(1, dtype=torch.int))\n        self.n_mels = n_mels\n        self.prenet = PreNet(n_mels)\n        self.attn_net = LSA(decoder_dims)\n        self.attn_rnn = nn.GRUCell(decoder_dims + decoder_dims // 2, decoder_dims)\n        self.rnn_input = nn.Linear(2 * decoder_dims, lstm_dims)\n        self.res_rnn1 = nn.LSTMCell(lstm_dims, lstm_dims)\n        self.res_rnn2 = nn.LSTMCell(lstm_dims, lstm_dims)\n        self.mel_proj = nn.Linear(lstm_dims, n_mels * self.max_r, bias=False)\n\n    def zoneout(self, prev, current, p=0.1):\n        device = next(self.parameters()).device  # Use same device as parameters\n        mask = torch.zeros(prev.size(), device=device).bernoulli_(p)\n        return prev * mask + current * (1 - mask)\n\n    def forward(self, encoder_seq, encoder_seq_proj, prenet_in,\n                hidden_states, cell_states, context_vec, t):\n\n        # Need this for reshaping mels\n        batch_size = encoder_seq.size(0)\n\n        # Unpack the hidden and cell states\n        attn_hidden, rnn1_hidden, rnn2_hidden = hidden_states\n        rnn1_cell, rnn2_cell = cell_states\n\n        # PreNet for the Attention RNN\n        prenet_out = self.prenet(prenet_in)\n\n        # Compute the Attention RNN hidden state\n        attn_rnn_in = torch.cat([context_vec, prenet_out], dim=-1)\n        attn_hidden = self.attn_rnn(attn_rnn_in.squeeze(1), attn_hidden)\n\n        # Compute the attention scores\n        scores = self.attn_net(encoder_seq_proj, attn_hidden, t)\n\n        # Dot product to create the context vector\n        context_vec = scores @ encoder_seq\n        context_vec = context_vec.squeeze(1)\n\n        # Concat Attention RNN output w. Context Vector & project\n        x = torch.cat([context_vec, attn_hidden], dim=1)\n        x = self.rnn_input(x)\n\n        # Compute first Residual RNN\n        rnn1_hidden_next, rnn1_cell = self.res_rnn1(x, (rnn1_hidden, rnn1_cell))\n        if self.training:\n            rnn1_hidden = self.zoneout(rnn1_hidden, rnn1_hidden_next)\n        else:\n            rnn1_hidden = rnn1_hidden_next\n        x = x + rnn1_hidden\n\n        # Compute second Residual RNN\n        rnn2_hidden_next, rnn2_cell = self.res_rnn2(x, (rnn2_hidden, rnn2_cell))\n        if self.training:\n            rnn2_hidden = self.zoneout(rnn2_hidden, rnn2_hidden_next)\n        else:\n            rnn2_hidden = rnn2_hidden_next\n        x = x + rnn2_hidden\n\n        # Project Mels\n        mels = self.mel_proj(x)\n        mels = mels.view(batch_size, self.n_mels, self.max_r)[:, :, :self.r]\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        return mels, scores, hidden_states, cell_states, context_vec\n\n\nclass Tacotron(nn.Module):\n    def __init__(self, embed_dims, num_chars, encoder_dims, decoder_dims, n_mels, fft_bins, postnet_dims,\n                 encoder_K, lstm_dims, postnet_K, num_highways, dropout, stop_threshold):\n        super().__init__()\n        self.n_mels = n_mels\n        self.lstm_dims = lstm_dims\n        self.decoder_dims = decoder_dims\n        self.encoder = Encoder(embed_dims, num_chars, encoder_dims,\n                               encoder_K, num_highways, dropout)\n        self.encoder_proj = nn.Linear(decoder_dims, decoder_dims, bias=False)\n        self.decoder = Decoder(n_mels, decoder_dims, lstm_dims)\n        self.postnet = CBHG(postnet_K, n_mels, postnet_dims, [256, 80], num_highways)\n        self.post_proj = nn.Linear(postnet_dims * 2, fft_bins, bias=False)\n\n        self.init_model()\n        self.num_params()\n\n        self.register_buffer(\'step\', torch.zeros(1, dtype=torch.long))\n        self.register_buffer(\'stop_threshold\', torch.tensor(stop_threshold, dtype=torch.float32))\n\n    @property\n    def r(self):\n        return self.decoder.r.item()\n\n    @r.setter\n    def r(self, value):\n        self.decoder.r = self.decoder.r.new_tensor(value, requires_grad=False)\n\n    def forward(self, x, m, generate_gta=False):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        self.step += 1\n\n        if generate_gta:\n            self.eval()\n        else:\n            self.train()\n\n        batch_size, _, steps  = m.size()\n\n        # Initialise all hidden states and pack into tuple\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Initialise all lstm cell states and pack into tuple\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.decoder_dims, device=device)\n\n        # Project the encoder outputs to avoid\n        # unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores = [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = m[:, :, t - 1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec = \\\n                self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                             hidden_states, cell_states, context_vec, t)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n        linear = linear.transpose(1, 2)\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        # attn_scores = attn_scores.cpu().data.numpy()\n\n        return mel_outputs, linear, attn_scores\n\n    def generate(self, x, steps=2000):\n        self.eval()\n        device = next(self.parameters()).device  # use same device as parameters\n\n        batch_size = 1\n        x = torch.as_tensor(x, dtype=torch.long, device=device).unsqueeze(0)\n\n        # Need to initialise all hidden states and pack into tuple for tidyness\n        attn_hidden = torch.zeros(batch_size, self.decoder_dims, device=device)\n        rnn1_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_hidden = torch.zeros(batch_size, self.lstm_dims, device=device)\n        hidden_states = (attn_hidden, rnn1_hidden, rnn2_hidden)\n\n        # Need to initialise all lstm cell states and pack into tuple for tidyness\n        rnn1_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        rnn2_cell = torch.zeros(batch_size, self.lstm_dims, device=device)\n        cell_states = (rnn1_cell, rnn2_cell)\n\n        # Need a <GO> Frame for start of decoder loop\n        go_frame = torch.zeros(batch_size, self.n_mels, device=device)\n\n        # Need an initial context vector\n        context_vec = torch.zeros(batch_size, self.decoder_dims, device=device)\n\n        # Project the encoder outputs to avoid\n        # unnecessary matmuls in the decoder loop\n        encoder_seq = self.encoder(x)\n        encoder_seq_proj = self.encoder_proj(encoder_seq)\n\n        # Need a couple of lists for outputs\n        mel_outputs, attn_scores = [], []\n\n        # Run the decoder loop\n        for t in range(0, steps, self.r):\n            prenet_in = mel_outputs[-1][:, :, -1] if t > 0 else go_frame\n            mel_frames, scores, hidden_states, cell_states, context_vec = \\\n            self.decoder(encoder_seq, encoder_seq_proj, prenet_in,\n                         hidden_states, cell_states, context_vec, t)\n            mel_outputs.append(mel_frames)\n            attn_scores.append(scores)\n            # Stop the loop if silent frames present\n            if (mel_frames < self.stop_threshold).all() and t > 10: break\n\n        # Concat the mel outputs into sequence\n        mel_outputs = torch.cat(mel_outputs, dim=2)\n\n        # Post-Process for Linear Spectrograms\n        postnet_out = self.postnet(mel_outputs)\n        linear = self.post_proj(postnet_out)\n\n\n        linear = linear.transpose(1, 2)[0].cpu().data.numpy()\n        mel_outputs = mel_outputs[0].cpu().data.numpy()\n\n        # For easy visualisation\n        attn_scores = torch.cat(attn_scores, 1)\n        attn_scores = attn_scores.cpu().data.numpy()[0]\n\n        self.train()\n\n        return mel_outputs, linear, attn_scores\n\n    def init_model(self):\n        for p in self.parameters():\n            if p.dim() > 1: nn.init.xavier_uniform_(p)\n\n    def get_step(self):\n        return self.step.data.item()\n\n    def reset_step(self):\n        # assignment to parameters or buffers is overloaded, updates internal dict entry\n        self.step = self.step.data.new_tensor(1)\n\n    def log(self, path, msg):\n        with open(path, \'a\') as f:\n            print(msg, file=f)\n\n    def load(self, path: Union[str, Path]):\n        # Use device of model params as location for loaded state\n        device = next(self.parameters()).device\n        state_dict = torch.load(path, map_location=device)\n\n        # Backwards compatibility with old saved models\n        if \'r\' in state_dict and not \'decoder.r\' in state_dict:\n            self.r = state_dict[\'r\']\n\n        self.load_state_dict(state_dict, strict=False)\n\n    def save(self, path: Union[str, Path]):\n        # No optimizer argument because saving a model should not include data\n        # only relevant in the training process - it should only be properties\n        # of the model itself. Let caller take care of saving optimzier state.\n        torch.save(self.state_dict(), path)\n\n    def num_params(self, print_out=True):\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        if print_out:\n            print(\'Trainable Parameters: %.3fM\' % parameters)\n        return parameters\n'"
notebooks/__init__.py,0,b''
utils/__init__.py,5,"b'# Make it explicit that we do it the Python 3 way\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nfrom builtins import *\n\nimport sys\nimport torch\nimport re\n\nfrom importlib.util import spec_from_file_location, module_from_spec\nfrom pathlib import Path\nfrom typing import Union\n\n# Credit: Ryuichi Yamamoto (https://github.com/r9y9/wavenet_vocoder/blob/1717f145c8f8c0f3f85ccdf346b5209fa2e1c920/train.py#L599)\n# Modified by: Ryan Butler (https://github.com/TheButlah)\n# workaround for https://github.com/pytorch/pytorch/issues/15716\n# the idea is to return outputs and replicas explicitly, so that making pytorch\n# not to release the nodes (this is a pytorch bug though)\n\n_output_ref = None\n_replicas_ref = None\n\ndef data_parallel_workaround(model, *input):\n    global _output_ref\n    global _replicas_ref\n    device_ids = list(range(torch.cuda.device_count()))\n    output_device = device_ids[0]\n    replicas = torch.nn.parallel.replicate(model, device_ids)\n    # input.shape = (num_args, batch, ...)\n    inputs = torch.nn.parallel.scatter(input, device_ids)\n    # inputs.shape = (num_gpus, num_args, batch/num_gpus, ...)\n    replicas = replicas[:len(inputs)]\n    outputs = torch.nn.parallel.parallel_apply(replicas, inputs)\n    y_hat = torch.nn.parallel.gather(outputs, output_device)\n    _output_ref = outputs\n    _replicas_ref = replicas\n    return y_hat\n\n\n###### Deal with hparams import that has to be configured at runtime ######\nclass __HParams:\n    """"""Manages the hyperparams pseudo-module""""""\n    def __init__(self, path: Union[str, Path]=None):\n        """"""Constructs the hyperparameters from a path to a python module. If\n        `path` is None, will raise an AttributeError whenever its attributes\n        are accessed. Otherwise, configures self based on `path`.""""""\n        if path is None:\n            self._configured = False\n        else:\n            self.configure(path)\n\n    def __getattr__(self, item):\n        if not self.is_configured():\n            raise AttributeError(""HParams not configured yet. Call self.configure()"")\n        else:\n            return super().__getattr__(item)\n\n    def configure(self, path: Union[str, Path]):\n        """"""Configures hparams by copying over atrributes from a module with the\n        given path. Raises an exception if already configured.""""""\n        if self.is_configured():\n            raise RuntimeError(""Cannot reconfigure hparams!"")\n\n        ###### Check for proper path ######\n        if not isinstance(path, Path):\n            path = Path(path).expanduser()\n        if not path.exists():\n            raise FileNotFoundError(f""Could not find hparams file {path}"")\n        elif path.suffix != "".py"":\n            raise ValueError(""`path` must be a python file"")\n\n        ###### Load in attributes from module ######\n        m = _import_from_file(""hparams"", path)\n\n        reg = re.compile(r""^__.+__$"")  # Matches magic methods\n        for name, value in m.__dict__.items():\n            if reg.match(name):\n                # Skip builtins\n                continue\n            if name in self.__dict__:\n                # Cannot overwrite already existing attributes\n                raise AttributeError(\n                    f""module at `path` cannot contain attribute {name} as it ""\n                    ""overwrites an attribute of the same name in utils.hparams"")\n            # Fair game to copy over the attribute\n            self.__setattr__(name, value)\n\n        self._configured = True\n\n    def is_configured(self):\n        return self._configured\n\nhparams = __HParams()\n\n\ndef _import_from_file(name, path: Path):\n    """"""Programmatically returns a module object from a filepath""""""\n    if not Path(path).exists():\n        raise FileNotFoundError(\'""%s"" doesn\\\'t exist!\' % path)\n    spec = spec_from_file_location(name, path)\n    if spec is None:\n        raise ValueError(\'could not load module from ""%s""\' % path)\n    m = module_from_spec(spec)\n    spec.loader.exec_module(m)\n    return m'"
utils/checkpoints.py,2,"b'import torch\nfrom utils.paths import Paths\nfrom models.tacotron import Tacotron\n\n\ndef get_checkpoint_paths(checkpoint_type: str, paths: Paths):\n    """"""\n    Returns the correct checkpointing paths\n    depending on whether model is Vocoder or TTS\n\n    Args:\n        checkpoint_type: Either \'voc\' or \'tts\'\n        paths: Paths object\n    """"""\n    if checkpoint_type is \'tts\':\n        weights_path = paths.tts_latest_weights\n        optim_path = paths.tts_latest_optim\n        checkpoint_path = paths.tts_checkpoints\n    elif checkpoint_type is \'voc\':\n        weights_path = paths.voc_latest_weights\n        optim_path = paths.voc_latest_optim\n        checkpoint_path = paths.voc_checkpoints\n    else:\n        raise NotImplementedError\n\n    return weights_path, optim_path, checkpoint_path\n\n\ndef save_checkpoint(checkpoint_type: str, paths: Paths, model, optimizer, *,\n        name=None, is_silent=False):\n    """"""Saves the training session to disk.\n\n    Args:\n        paths:  Provides information about the different paths to use.\n        model:  A `Tacotron` or `WaveRNN` model to save the parameters and buffers from.\n        optimizer:  An optmizer to save the state of (momentum, etc).\n        name:  If provided, will name to a checkpoint with the given name. Note\n            that regardless of whether this is provided or not, this function\n            will always update the files specified in `paths` that give the\n            location of the latest weights and optimizer state. Saving\n            a named checkpoint happens in addition to this update.\n    """"""\n    def helper(path_dict, is_named):\n        s = \'named\' if is_named else \'latest\'\n        num_exist = sum(p.exists() for p in path_dict.values())\n\n        if num_exist not in (0,2):\n            # Checkpoint broken\n            raise FileNotFoundError(\n                f\'We expected either both or no files in the {s} checkpoint to \'\n                \'exist, but instead we got exactly one!\')\n\n        if num_exist == 0:\n            if not is_silent: print(f\'Creating {s} checkpoint...\')\n            for p in path_dict.values():\n                p.parent.mkdir(parents=True, exist_ok=True)\n        else:\n            if not is_silent: print(f\'Saving to existing {s} checkpoint...\')\n\n        if not is_silent: print(f\'Saving {s} weights: {path_dict[""w""]}\')\n        model.save(path_dict[\'w\'])\n        if not is_silent: print(f\'Saving {s} optimizer state: {path_dict[""o""]}\')\n        torch.save(optimizer.state_dict(), path_dict[\'o\'])\n\n    weights_path, optim_path, checkpoint_path = \\\n        get_checkpoint_paths(checkpoint_type, paths)\n\n    latest_paths = {\'w\': weights_path, \'o\': optim_path}\n    helper(latest_paths, False)\n\n    if name:\n        named_paths = {\n            \'w\': checkpoint_path/f\'{name}_weights.pyt\',\n            \'o\': checkpoint_path/f\'{name}_optim.pyt\',\n        }\n        helper(named_paths, True)\n\n\ndef restore_checkpoint(checkpoint_type: str, paths: Paths, model, optimizer, *,\n        name=None, create_if_missing=False):\n    """"""Restores from a training session saved to disk.\n\n    NOTE: The optimizer\'s state is placed on the same device as it\'s model\n    parameters. Therefore, be sure you have done `model.to(device)` before\n    calling this method.\n\n    Args:\n        paths:  Provides information about the different paths to use.\n        model:  A `Tacotron` or `WaveRNN` model to save the parameters and buffers from.\n        optimizer:  An optmizer to save the state of (momentum, etc).\n        name:  If provided, will restore from a checkpoint with the given name.\n            Otherwise, will restore from the latest weights and optimizer state\n            as specified in `paths`.\n        create_if_missing:  If `True`, will create the checkpoint if it doesn\'t\n            yet exist, as well as update the files specified in `paths` that\n            give the location of the current latest weights and optimizer state.\n            If `False` and the checkpoint doesn\'t exist, will raise a\n            `FileNotFoundError`.\n    """"""\n\n    weights_path, optim_path, checkpoint_path = \\\n        get_checkpoint_paths(checkpoint_type, paths)\n\n    if name:\n        path_dict = {\n            \'w\': checkpoint_path/f\'{name}_weights.pyt\',\n            \'o\': checkpoint_path/f\'{name}_optim.pyt\',\n        }\n        s = \'named\'\n    else:\n        path_dict = {\n            \'w\': weights_path,\n            \'o\': optim_path\n        }\n        s = \'latest\'\n\n    num_exist = sum(p.exists() for p in path_dict.values())\n    if num_exist == 2:\n        # Checkpoint exists\n        print(f\'Restoring from {s} checkpoint...\')\n        print(f\'Loading {s} weights: {path_dict[""w""]}\')\n        model.load(path_dict[\'w\'])\n        print(f\'Loading {s} optimizer state: {path_dict[""o""]}\')\n        optimizer.load_state_dict(torch.load(path_dict[\'o\']))\n    elif create_if_missing:\n        save_checkpoint(checkpoint_type, paths, model, optimizer, name=name, is_silent=False)\n    else:\n        raise FileNotFoundError(f\'The {s} checkpoint could not be found!\')'"
utils/dataset.py,8,"b""import pickle\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import Sampler\nfrom utils.dsp import *\nfrom utils import hparams as hp\nfrom utils.text import text_to_sequence\nfrom utils.paths import Paths\nfrom pathlib import Path\n\n\n###################################################################################\n# WaveRNN/Vocoder Dataset #########################################################\n###################################################################################\n\n\nclass VocoderDataset(Dataset):\n    def __init__(self, path: Path, dataset_ids, train_gta=False):\n        self.metadata = dataset_ids\n        self.mel_path = path/'gta' if train_gta else path/'mel'\n        self.quant_path = path/'quant'\n\n\n    def __getitem__(self, index):\n        item_id = self.metadata[index]\n        m = np.load(self.mel_path/f'{item_id}.npy')\n        x = np.load(self.quant_path/f'{item_id}.npy')\n        return m, x\n\n    def __len__(self):\n        return len(self.metadata)\n\n\ndef get_vocoder_datasets(path: Path, batch_size, train_gta):\n\n    with open(path/'dataset.pkl', 'rb') as f:\n        dataset = pickle.load(f)\n\n    dataset_ids = [x[0] for x in dataset]\n\n    random.seed(1234)\n    random.shuffle(dataset_ids)\n\n    test_ids = dataset_ids[-hp.voc_test_samples:]\n    train_ids = dataset_ids[:-hp.voc_test_samples]\n\n    train_dataset = VocoderDataset(path, train_ids, train_gta)\n    test_dataset = VocoderDataset(path, test_ids, train_gta)\n\n    train_set = DataLoader(train_dataset,\n                           collate_fn=collate_vocoder,\n                           batch_size=batch_size,\n                           num_workers=2,\n                           shuffle=True,\n                           pin_memory=True)\n\n    test_set = DataLoader(test_dataset,\n                          batch_size=1,\n                          num_workers=1,\n                          shuffle=False,\n                          pin_memory=True)\n\n    return train_set, test_set\n\n\ndef collate_vocoder(batch):\n    mel_win = hp.voc_seq_len // hp.hop_length + 2 * hp.voc_pad\n    max_offsets = [x[0].shape[-1] -2 - (mel_win + 2 * hp.voc_pad) for x in batch]\n    mel_offsets = [np.random.randint(0, offset) for offset in max_offsets]\n    sig_offsets = [(offset + hp.voc_pad) * hp.hop_length for offset in mel_offsets]\n\n    mels = [x[0][:, mel_offsets[i]:mel_offsets[i] + mel_win] for i, x in enumerate(batch)]\n\n    labels = [x[1][sig_offsets[i]:sig_offsets[i] + hp.voc_seq_len + 1] for i, x in enumerate(batch)]\n\n    mels = np.stack(mels).astype(np.float32)\n    labels = np.stack(labels).astype(np.int64)\n\n    mels = torch.tensor(mels)\n    labels = torch.tensor(labels).long()\n\n    x = labels[:, :hp.voc_seq_len]\n    y = labels[:, 1:]\n\n    bits = 16 if hp.voc_mode == 'MOL' else hp.bits\n\n    x = label_2_float(x.float(), bits)\n\n    if hp.voc_mode == 'MOL':\n        y = label_2_float(y.float(), bits)\n\n    return x, y, mels\n\n\n###################################################################################\n# Tacotron/TTS Dataset ############################################################\n###################################################################################\n\n\ndef get_tts_datasets(path: Path, batch_size, r):\n\n    with open(path/'dataset.pkl', 'rb') as f:\n        dataset = pickle.load(f)\n\n    dataset_ids = []\n    mel_lengths = []\n\n    for (item_id, len) in dataset:\n        if len <= hp.tts_max_mel_len:\n            dataset_ids += [item_id]\n            mel_lengths += [len]\n\n    with open(path/'text_dict.pkl', 'rb') as f:\n        text_dict = pickle.load(f)\n\n    train_dataset = TTSDataset(path, dataset_ids, text_dict)\n\n    sampler = None\n\n    if hp.tts_bin_lengths:\n        sampler = BinnedLengthSampler(mel_lengths, batch_size, batch_size * 3)\n\n    train_set = DataLoader(train_dataset,\n                           collate_fn=lambda batch: collate_tts(batch, r),\n                           batch_size=batch_size,\n                           sampler=sampler,\n                           num_workers=1,\n                           pin_memory=True)\n\n    longest = mel_lengths.index(max(mel_lengths))\n\n    # Used to evaluate attention during training process\n    attn_example = dataset_ids[longest]\n\n    # print(attn_example)\n\n    return train_set, attn_example\n\n\nclass TTSDataset(Dataset):\n    def __init__(self, path: Path, dataset_ids, text_dict):\n        self.path = path\n        self.metadata = dataset_ids\n        self.text_dict = text_dict\n\n    def __getitem__(self, index):\n        item_id = self.metadata[index]\n        x = text_to_sequence(self.text_dict[item_id], hp.tts_cleaner_names)\n        mel = np.load(self.path/'mel'/f'{item_id}.npy')\n        mel_len = mel.shape[-1]\n        return x, mel, item_id, mel_len\n\n    def __len__(self):\n        return len(self.metadata)\n\n\ndef pad1d(x, max_len):\n    return np.pad(x, (0, max_len - len(x)), mode='constant')\n\n\ndef pad2d(x, max_len):\n    return np.pad(x, ((0, 0), (0, max_len - x.shape[-1])), mode='constant')\n\n\ndef collate_tts(batch, r):\n\n    x_lens = [len(x[0]) for x in batch]\n    max_x_len = max(x_lens)\n\n    chars = [pad1d(x[0], max_x_len) for x in batch]\n    chars = np.stack(chars)\n\n    spec_lens = [x[1].shape[-1] for x in batch]\n    max_spec_len = max(spec_lens) + 1\n    if max_spec_len % r != 0:\n        max_spec_len += r - max_spec_len % r\n\n    mel = [pad2d(x[1], max_spec_len) for x in batch]\n    mel = np.stack(mel)\n\n    ids = [x[2] for x in batch]\n    mel_lens = [x[3] for x in batch]\n\n    chars = torch.tensor(chars).long()\n    mel = torch.tensor(mel)\n\n    # scale spectrograms to -4 <--> 4\n    mel = (mel * 8.) - 4.\n    return chars, mel, ids, mel_lens\n\n\nclass BinnedLengthSampler(Sampler):\n    def __init__(self, lengths, batch_size, bin_size):\n        _, self.idx = torch.sort(torch.tensor(lengths).long())\n        self.batch_size = batch_size\n        self.bin_size = bin_size\n        assert self.bin_size % self.batch_size == 0\n\n    def __iter__(self):\n        # Need to change to numpy since there's a bug in random.shuffle(tensor)\n        # TODO: Post an issue on pytorch repo\n        idx = self.idx.numpy()\n        bins = []\n\n        for i in range(len(idx) // self.bin_size):\n            this_bin = idx[i * self.bin_size:(i + 1) * self.bin_size]\n            random.shuffle(this_bin)\n            bins += [this_bin]\n\n        random.shuffle(bins)\n        binned_idx = np.stack(bins).reshape(-1)\n\n        if len(binned_idx) < len(idx):\n            last_bin = idx[len(binned_idx):]\n            random.shuffle(last_bin)\n            binned_idx = np.concatenate([binned_idx, last_bin])\n\n        return iter(torch.tensor(binned_idx).long())\n\n    def __len__(self):\n        return len(self.idx)\n"""
utils/display.py,0,"b'import matplotlib as mpl\nmpl.use(\'agg\')  # Use non-interactive backend by default\nimport matplotlib.pyplot as plt\nimport time\nimport numpy as np\nimport sys\n\n\ndef progbar(i, n, size=16):\n    done = (i * size) // n\n    bar = \'\'\n    for i in range(size):\n        bar += \'\xe2\x96\x88\' if i <= done else \'\xe2\x96\x91\'\n    return bar\n\n\ndef stream(message):\n    sys.stdout.write(f""\\r{message}"")\n\n\ndef simple_table(item_tuples):\n\n    border_pattern = \'+---------------------------------------\'\n    whitespace = \'                                            \'\n\n    headings, cells, = [], []\n\n    for item in item_tuples:\n\n        heading, cell = str(item[0]), str(item[1])\n\n        pad_head = True if len(heading) < len(cell) else False\n\n        pad = abs(len(heading) - len(cell))\n        pad = whitespace[:pad]\n\n        pad_left = pad[:len(pad)//2]\n        pad_right = pad[len(pad)//2:]\n\n        if pad_head:\n            heading = pad_left + heading + pad_right\n        else:\n            cell = pad_left + cell + pad_right\n\n        headings += [heading]\n        cells += [cell]\n\n    border, head, body = \'\', \'\', \'\'\n\n    for i in range(len(item_tuples)):\n\n        temp_head = f\'| {headings[i]} \'\n        temp_body = f\'| {cells[i]} \'\n\n        border += border_pattern[:len(temp_head)]\n        head += temp_head\n        body += temp_body\n\n        if i == len(item_tuples) - 1:\n            head += \'|\'\n            body += \'|\'\n            border += \'+\'\n\n    print(border)\n    print(head)\n    print(border)\n    print(body)\n    print(border)\n    print(\' \')\n\n\ndef time_since(started):\n    elapsed = time.time() - started\n    m = int(elapsed // 60)\n    s = int(elapsed % 60)\n    if m >= 60:\n        h = int(m // 60)\n        m = m % 60\n        return f\'{h}h {m}m {s}s\'\n    else:\n        return f\'{m}m {s}s\'\n\n\ndef save_attention(attn, path):\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(attn.T, interpolation=\'nearest\', aspect=\'auto\')\n    fig.savefig(path.parent/f\'{path.stem}.png\', bbox_inches=\'tight\')\n    plt.close(fig)\n\n\ndef save_spectrogram(M, path, length=None):\n    M = np.flip(M, axis=0)\n    if length: M = M[:, :length]\n    fig = plt.figure(figsize=(12, 6))\n    plt.imshow(M, interpolation=\'nearest\', aspect=\'auto\')\n    fig.savefig(f\'{path}.png\', bbox_inches=\'tight\')\n    plt.close(fig)\n\n\ndef plot(array):\n    mpl.interactive(True)\n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color(\'grey\')\n    ax.yaxis.label.set_color(\'grey\')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis=\'x\', colors=\'grey\', labelsize=23)\n    ax.tick_params(axis=\'y\', colors=\'grey\', labelsize=23)\n    plt.plot(array)\n    mpl.interactive(False)\n\n\ndef plot_spec(M):\n    mpl.interactive(True)\n    M = np.flip(M, axis=0)\n    plt.figure(figsize=(18,4))\n    plt.imshow(M, interpolation=\'nearest\', aspect=\'auto\')\n    plt.show()\n    mpl.interactive(False)\n\n'"
utils/distribution.py,17,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef log_sum_exp(x):\n    """""" numerically stable log_sum_exp implementation that prevents overflow """"""\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=axis)\n    m2, _ = torch.max(x, dim=axis, keepdim=True)\n    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n\n\n# It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=65536,\n                                  log_scale_min=None, reduce=True):\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    y_hat = y_hat.permute(0,2,1)\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) % 3 == 0\n    nr_mix = y_hat.size(1) // 3\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    # unpack parameters. (B, T, num_mixtures) x 3\n    logit_probs = y_hat[:, :, :nr_mix]\n    means = y_hat[:, :, nr_mix:2 * nr_mix]\n    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n\n    # B x T x 1 -> B x T x num_mixtures\n    y = y.expand_as(means)\n\n    centered_y = y - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n    cdf_plus = torch.sigmoid(plus_in)\n    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n    cdf_min = torch.sigmoid(min_in)\n\n    # log probability for edge case of 0 (before scaling)\n    # equivalent: torch.log(F.sigmoid(plus_in))\n    log_cdf_plus = plus_in - F.softplus(plus_in)\n\n    # log probability for edge case of 255 (before scaling)\n    # equivalent: (1 - F.sigmoid(min_in)).log()\n    log_one_minus_cdf_min = -F.softplus(min_in)\n\n    # probability for all other cases\n    cdf_delta = cdf_plus - cdf_min\n\n    mid_in = inv_stdv * centered_y\n    # log probability in the center of the bin, to be used in extreme cases\n    # (not actually used in our code)\n    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n\n    # tf equivalent\n    """"""\n    log_probs = tf.where(x < -0.999, log_cdf_plus,\n                         tf.where(x > 0.999, log_one_minus_cdf_min,\n                                  tf.where(cdf_delta > 1e-5,\n                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n                                           log_pdf_mid - np.log(127.5))))\n    """"""\n    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n    # for num_classes=65536 case? 1e-7? not sure..\n    inner_inner_cond = (cdf_delta > 1e-5).float()\n\n    inner_inner_out = inner_inner_cond * \\\n        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n    inner_cond = (y > 0.999).float()\n    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n    cond = (y < -0.999).float()\n    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n\n    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n\n    if reduce:\n        return -torch.mean(log_sum_exp(log_probs))\n    else:\n        return -log_sum_exp(log_probs).unsqueeze(-1)\n\n\ndef sample_from_discretized_mix_logistic(y, log_scale_min=None):\n    """"""\n    Sample from discretized mixture of logistic distributions\n    Args:\n        y (Tensor): B x C x T\n        log_scale_min (float): Log scale minimum value\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    """"""\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    assert y.size(1) % 3 == 0\n    nr_mix = y.size(1) // 3\n\n    # B x T x C\n    y = y.transpose(1, 2)\n    logit_probs = y[:, :, :nr_mix]\n\n    # sample mixture indicator from softmax\n    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n    temp = logit_probs.data - torch.log(- torch.log(temp))\n    _, argmax = temp.max(dim=-1)\n\n    # (B, T) -> (B, T, nr_mix)\n    one_hot = F.one_hot(argmax, nr_mix).float()\n    # select logistic parameters\n    means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n    log_scales = torch.clamp(torch.sum(\n        y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1), min=log_scale_min)\n    # sample from logistic & clip to interval\n    # we don\'t actually round to the nearest 8bit value when sampling\n    u = means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5)\n    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n\n    x = torch.clamp(torch.clamp(x, min=-1.), max=1.)\n\n    return x\n\n\'\'\'\ndef to_one_hot(tensor, n, fill_with=1.):\n    # we perform one hot encore with respect to the last axis\n    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_()\n    if tensor.is_cuda:\n        one_hot = one_hot.cuda()\n    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n    return one_hot\'\'\'\n'"
utils/dsp.py,0,"b'import math\nimport numpy as np\nimport librosa\nfrom utils import hparams as hp\nfrom scipy.signal import lfilter\n\n\ndef label_2_float(x, bits):\n    return 2 * x / (2**bits - 1.) - 1.\n\n\ndef float_2_label(x, bits):\n    assert abs(x).max() <= 1.0\n    x = (x + 1.) * (2**bits - 1) / 2\n    return x.clip(0, 2**bits - 1)\n\n\ndef load_wav(path):\n    return librosa.load(path, sr=hp.sample_rate)[0]\n\n\ndef save_wav(x, path):\n    librosa.output.write_wav(path, x.astype(np.float32), sr=hp.sample_rate)\n\n\ndef split_signal(x):\n    unsigned = x + 2**15\n    coarse = unsigned // 256\n    fine = unsigned % 256\n    return coarse, fine\n\n\ndef combine_signal(coarse, fine):\n    return coarse * 256 + fine - 2**15\n\n\ndef encode_16bits(x):\n    return np.clip(x * 2**15, -2**15, 2**15 - 1).astype(np.int16)\n\n\ndef linear_to_mel(spectrogram):\n    return librosa.feature.melspectrogram(\n        S=spectrogram, sr=hp.sample_rate, n_fft=hp.n_fft, n_mels=hp.num_mels, fmin=hp.fmin)\n\n\'\'\'\ndef build_mel_basis():\n    return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels, fmin=hp.fmin)\n\'\'\'\n\ndef normalize(S):\n    return np.clip((S - hp.min_level_db) / -hp.min_level_db, 0, 1)\n\n\ndef denormalize(S):\n    return (np.clip(S, 0, 1) * -hp.min_level_db) + hp.min_level_db\n\n\ndef amp_to_db(x):\n    return 20 * np.log10(np.maximum(1e-5, x))\n\n\ndef db_to_amp(x):\n    return np.power(10.0, x * 0.05)\n\n\ndef spectrogram(y):\n    D = stft(y)\n    S = amp_to_db(np.abs(D)) - hp.ref_level_db\n    return normalize(S)\n\n\ndef melspectrogram(y):\n    D = stft(y)\n    S = amp_to_db(linear_to_mel(np.abs(D)))\n    return normalize(S)\n\n\ndef stft(y):\n    return librosa.stft(\n        y=y,\n        n_fft=hp.n_fft, hop_length=hp.hop_length, win_length=hp.win_length)\n\n\ndef pre_emphasis(x):\n    return lfilter([1, -hp.preemphasis], [1], x)\n\n\ndef de_emphasis(x):\n    return lfilter([1], [1, -hp.preemphasis], x)\n\n\ndef encode_mu_law(x, mu):\n    mu = mu - 1\n    fx = np.sign(x) * np.log(1 + mu * np.abs(x)) / np.log(1 + mu)\n    return np.floor((fx + 1) / 2 * mu + 0.5)\n\n\ndef decode_mu_law(y, mu, from_labels=True):\n    # TODO: get rid of log2 - makes no sense\n    if from_labels: y = label_2_float(y, math.log2(mu))\n    mu = mu - 1\n    x = np.sign(y) / mu * ((1 + mu) ** np.abs(y) - 1)\n    return x\n\ndef reconstruct_waveform(mel, n_iter=32):\n    """"""Uses Griffin-Lim phase reconstruction to convert from a normalized\n    mel spectrogram back into a waveform.""""""\n    denormalized = denormalize(mel)\n    amp_mel = db_to_amp(denormalized)\n    S = librosa.feature.inverse.mel_to_stft(\n        amp_mel, power=1, sr=hp.sample_rate,\n        n_fft=hp.n_fft, fmin=hp.fmin)\n    wav = librosa.core.griffinlim(\n        S, n_iter=n_iter,\n        hop_length=hp.hop_length, win_length=hp.win_length)\n    return wav\n\n'"
utils/files.py,0,"b""from pathlib import Path\nfrom typing import Union\n\ndef get_files(path: Union[str, Path], extension='.wav'):\n    if isinstance(path, str): path = Path(path).expanduser().resolve()\n    return list(path.rglob(f'*{extension}'))\n"""
utils/paths.py,0,"b'import os\nfrom pathlib import Path\n\n\nclass Paths:\n    """"""Manages and configures the paths used by WaveRNN, Tacotron, and the data.""""""\n    def __init__(self, data_path, voc_id, tts_id):\n        self.base = Path(__file__).parent.parent.expanduser().resolve()\n\n        # Data Paths\n        self.data = Path(data_path).expanduser().resolve()\n        self.quant = self.data/\'quant\'\n        self.mel = self.data/\'mel\'\n        self.gta = self.data/\'gta\'\n\n        # WaveRNN/Vocoder Paths\n        self.voc_checkpoints = self.base/\'checkpoints\'/f\'{voc_id}.wavernn\'\n        self.voc_latest_weights = self.voc_checkpoints/\'latest_weights.pyt\'\n        self.voc_latest_optim = self.voc_checkpoints/\'latest_optim.pyt\'\n        self.voc_output = self.base/\'model_outputs\'/f\'{voc_id}.wavernn\'\n        self.voc_step = self.voc_checkpoints/\'step.npy\'\n        self.voc_log = self.voc_checkpoints/\'log.txt\'\n\n        # Tactron/TTS Paths\n        self.tts_checkpoints = self.base/\'checkpoints\'/f\'{tts_id}.tacotron\'\n        self.tts_latest_weights = self.tts_checkpoints/\'latest_weights.pyt\'\n        self.tts_latest_optim = self.tts_checkpoints/\'latest_optim.pyt\'\n        self.tts_output = self.base/\'model_outputs\'/f\'{tts_id}.tacotron\'\n        self.tts_step = self.tts_checkpoints/\'step.npy\'\n        self.tts_log = self.tts_checkpoints/\'log.txt\'\n        self.tts_attention = self.tts_checkpoints/\'attention\'\n        self.tts_mel_plot = self.tts_checkpoints/\'mel_plots\'\n\n        self.create_paths()\n\n    def create_paths(self):\n        os.makedirs(self.data, exist_ok=True)\n        os.makedirs(self.quant, exist_ok=True)\n        os.makedirs(self.mel, exist_ok=True)\n        os.makedirs(self.gta, exist_ok=True)\n        os.makedirs(self.voc_checkpoints, exist_ok=True)\n        os.makedirs(self.voc_output, exist_ok=True)\n        os.makedirs(self.tts_checkpoints, exist_ok=True)\n        os.makedirs(self.tts_output, exist_ok=True)\n        os.makedirs(self.tts_attention, exist_ok=True)\n        os.makedirs(self.tts_mel_plot, exist_ok=True)\n\n    def get_tts_named_weights(self, name):\n        """"""Gets the path for the weights in a named tts checkpoint.""""""\n        return self.tts_checkpoints/f\'{name}_weights.pyt\'\n\n    def get_tts_named_optim(self, name):\n        """"""Gets the path for the optimizer state in a named tts checkpoint.""""""\n        return self.tts_checkpoints/f\'{name}_optim.pyt\'\n\n    def get_voc_named_weights(self, name):\n        """"""Gets the path for the weights in a named voc checkpoint.""""""\n        return self.voc_checkpoints/f\'{name}_weights.pyt\'\n\n    def get_voc_named_optim(self, name):\n        """"""Gets the path for the optimizer state in a named voc checkpoint.""""""\n        return self.voc_checkpoints/f\'{name}_optim.pyt\'\n\n\n'"
notebooks/models/wavernn.py,31,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass WaveRNN(nn.Module) :\n    def __init__(self, hidden_size=896, quantisation=256) :\n        super(WaveRNN, self).__init__()\n        \n        self.hidden_size = hidden_size\n        self.split_size = hidden_size // 2\n        \n        # The main matmul\n        self.R = nn.Linear(self.hidden_size, 3 * self.hidden_size, bias=False)\n        \n        # Output fc layers\n        self.O1 = nn.Linear(self.split_size, self.split_size)\n        self.O2 = nn.Linear(self.split_size, quantisation)\n        self.O3 = nn.Linear(self.split_size, self.split_size)\n        self.O4 = nn.Linear(self.split_size, quantisation)\n        \n        # Input fc layers\n        self.I_coarse = nn.Linear(2, 3 * self.split_size, bias=False)\n        self.I_fine = nn.Linear(3, 3 * self.split_size, bias=False)\n\n        # biases for the gates\n        self.bias_u = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_r = nn.Parameter(torch.zeros(self.hidden_size))\n        self.bias_e = nn.Parameter(torch.zeros(self.hidden_size))\n        \n        # display num params\n        self.num_params()\n\n        \n    def forward(self, prev_y, prev_hidden, current_coarse) :\n        \n        # Main matmul - the projection is split 3 ways\n        R_hidden = self.R(prev_hidden)\n        R_u, R_r, R_e, = torch.split(R_hidden, self.hidden_size, dim=1)\n        \n        # Project the prev input \n        coarse_input_proj = self.I_coarse(prev_y)\n        I_coarse_u, I_coarse_r, I_coarse_e = \\\n            torch.split(coarse_input_proj, self.split_size, dim=1)\n        \n        # Project the prev input and current coarse sample\n        fine_input = torch.cat([prev_y, current_coarse], dim=1)\n        fine_input_proj = self.I_fine(fine_input)\n        I_fine_u, I_fine_r, I_fine_e = \\\n            torch.split(fine_input_proj, self.split_size, dim=1)\n        \n        # concatenate for the gates\n        I_u = torch.cat([I_coarse_u, I_fine_u], dim=1)\n        I_r = torch.cat([I_coarse_r, I_fine_r], dim=1)\n        I_e = torch.cat([I_coarse_e, I_fine_e], dim=1)\n        \n        # Compute all gates for coarse and fine \n        u = F.sigmoid(R_u + I_u + self.bias_u)\n        r = F.sigmoid(R_r + I_r + self.bias_r)\n        e = F.tanh(r * R_e + I_e + self.bias_e)\n        hidden = u * prev_hidden + (1. - u) * e\n        \n        # Split the hidden state\n        hidden_coarse, hidden_fine = torch.split(hidden, self.split_size, dim=1)\n        \n        # Compute outputs \n        out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n        out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n\n        return out_coarse, out_fine, hidden\n    \n        \n    def generate(self, seq_len) :\n        \n        with torch.no_grad() :\n            \n            # First split up the biases for the gates \n            b_coarse_u, b_fine_u = torch.split(self.bias_u, self.split_size)\n            b_coarse_r, b_fine_r = torch.split(self.bias_r, self.split_size)\n            b_coarse_e, b_fine_e = torch.split(self.bias_e, self.split_size)\n\n            # Lists for the two output seqs\n            c_outputs, f_outputs = [], []\n\n            # Some initial inputs\n            out_coarse = torch.LongTensor([0]).cuda()\n            out_fine = torch.LongTensor([0]).cuda()\n\n            # We'll meed a hidden state\n            hidden = self.init_hidden()\n\n            # Need a clock for display\n            start = time.time()\n\n            # Loop for generation\n            for i in range(seq_len) :\n\n                # Split into two hidden states\n                hidden_coarse, hidden_fine = \\\n                    torch.split(hidden, self.split_size, dim=1)\n\n                # Scale and concat previous predictions\n                out_coarse = out_coarse.unsqueeze(0).float() / 127.5 - 1.\n                out_fine = out_fine.unsqueeze(0).float() / 127.5 - 1.\n                prev_outputs = torch.cat([out_coarse, out_fine], dim=1)\n\n                # Project input \n                coarse_input_proj = self.I_coarse(prev_outputs)\n                I_coarse_u, I_coarse_r, I_coarse_e = \\\n                    torch.split(coarse_input_proj, self.split_size, dim=1)\n\n                # Project hidden state and split 6 ways\n                R_hidden = self.R(hidden)\n                R_coarse_u , R_fine_u, \\\n                R_coarse_r, R_fine_r, \\\n                R_coarse_e, R_fine_e = torch.split(R_hidden, self.split_size, dim=1)\n\n                # Compute the coarse gates\n                u = F.sigmoid(R_coarse_u + I_coarse_u + b_coarse_u)\n                r = F.sigmoid(R_coarse_r + I_coarse_r + b_coarse_r)\n                e = F.tanh(r * R_coarse_e + I_coarse_e + b_coarse_e)\n                hidden_coarse = u * hidden_coarse + (1. - u) * e\n\n                # Compute the coarse output\n                out_coarse = self.O2(F.relu(self.O1(hidden_coarse)))\n                posterior = F.softmax(out_coarse, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_coarse = distrib.sample()\n                c_outputs.append(out_coarse)\n\n                # Project the [prev outputs and predicted coarse sample]\n                coarse_pred = out_coarse.float() / 127.5 - 1.\n                fine_input = torch.cat([prev_outputs, coarse_pred.unsqueeze(0)], dim=1)\n                fine_input_proj = self.I_fine(fine_input)\n                I_fine_u, I_fine_r, I_fine_e = \\\n                    torch.split(fine_input_proj, self.split_size, dim=1)\n\n                # Compute the fine gates\n                u = F.sigmoid(R_fine_u + I_fine_u + b_fine_u)\n                r = F.sigmoid(R_fine_r + I_fine_r + b_fine_r)\n                e = F.tanh(r * R_fine_e + I_fine_e + b_fine_e)\n                hidden_fine = u * hidden_fine + (1. - u) * e\n\n                # Compute the fine output\n                out_fine = self.O4(F.relu(self.O3(hidden_fine)))\n                posterior = F.softmax(out_fine, dim=1)\n                distrib = torch.distributions.Categorical(posterior)\n                out_fine = distrib.sample()\n                f_outputs.append(out_fine)\n\n                # Put the hidden state back together\n                hidden = torch.cat([hidden_coarse, hidden_fine], dim=1)\n\n                # Display progress\n                speed = (i + 1) / (time.time() - start)\n                stream('Gen: %i/%i -- Speed: %i',  (i + 1, seq_len, speed))\n\n            coarse = torch.stack(c_outputs).squeeze(1).cpu().data.numpy()\n            fine = torch.stack(f_outputs).squeeze(1).cpu().data.numpy()        \n            output = combine_signal(coarse, fine)\n        \n        return output, coarse, fine\n        \n             \n    def init_hidden(self, batch_size=1) :\n        return torch.zeros(batch_size, self.hidden_size).cuda()\n    \n    \n    def num_params(self) :\n        parameters = filter(lambda p: p.requires_grad, self.parameters())\n        parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n        print('Trainable Parameters: %.3f million' % parameters)"""
notebooks/utils/__init__.py,0,b''
notebooks/utils/display.py,0,"b""import matplotlib.pyplot as plt\nimport time, sys, math\nimport numpy as np\n\ndef stream(string, variables) :\n    sys.stdout.write(f'\\r{string}' % variables)\n    \ndef num_params(model) :\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n    print('Trainable Parameters: %.3f million' % parameters)\n\ndef time_since(started) :\n    elapsed = time.time() - started\n    m = int(elapsed // 60)\n    s = int(elapsed % 60)\n    if m >= 60 :\n        h = int(m // 60)\n        m = m % 60\n        return f'{h}h {m}m {s}s'\n    else :\n        return f'{m}m {s}s'\n\ndef plot(array) : \n    fig = plt.figure(figsize=(30, 5))\n    ax = fig.add_subplot(111)\n    ax.xaxis.label.set_color('grey')\n    ax.yaxis.label.set_color('grey')\n    ax.xaxis.label.set_fontsize(23)\n    ax.yaxis.label.set_fontsize(23)\n    ax.tick_params(axis='x', colors='grey', labelsize=23)\n    ax.tick_params(axis='y', colors='grey', labelsize=23)\n    plt.plot(array)\n\ndef plot_spec(M) :\n    M = np.flip(M, axis=0)\n    plt.figure(figsize=(18,4))\n    plt.imshow(M, interpolation='nearest', aspect='auto')\n    plt.show()\n\n"""
notebooks/utils/dsp.py,0,"b""import numpy as np\nimport librosa, math\n\nsample_rate = 22050\nn_fft = 2048\nfft_bins = n_fft // 2 + 1\nnum_mels = 80\nhop_length = int(sample_rate * 0.0125) # 12.5ms\nwin_length = int(sample_rate * 0.05)   # 50ms\nfmin = 40\nmin_level_db = -100\nref_level_db = 20\n\ndef load_wav(filename, encode=True) :\n    x = librosa.load(filename, sr=sample_rate)[0]\n    if encode == True : x = encode_16bits(x)\n    return x\n\ndef save_wav(y, filename) :\n    if y.dtype != 'int16' :\n        y = encode_16bits(y)\n    librosa.output.write_wav(filename, y.astype(np.int16), sample_rate)\n\ndef split_signal(x) :\n    unsigned = x + 2**15\n    coarse = unsigned // 256\n    fine = unsigned % 256\n    return coarse, fine\n\ndef combine_signal(coarse, fine) :\n    return coarse * 256 + fine - 2**15\n\ndef encode_16bits(x) :\n    return np.clip(x * 2**15, -2**15, 2**15 - 1).astype(np.int16)\n\nmel_basis = None\n\ndef linear_to_mel(spectrogram):\n    global mel_basis\n    if mel_basis is None:\n        mel_basis = build_mel_basis()\n    return np.dot(mel_basis, spectrogram)\n\ndef build_mel_basis():\n    return librosa.filters.mel(sample_rate, n_fft, n_mels=num_mels, fmin=fmin)\n\ndef normalize(S):\n    return np.clip((S - min_level_db) / -min_level_db, 0, 1)\n\ndef denormalize(S):\n    return (np.clip(S, 0, 1) * -min_level_db) + min_level_db\n\ndef amp_to_db(x):\n    return 20 * np.log10(np.maximum(1e-5, x))\n\ndef db_to_amp(x):\n    return np.power(10.0, x * 0.05)\n\ndef spectrogram(y):\n    D = stft(y)\n    S = amp_to_db(np.abs(D)) - ref_level_db\n    return normalize(S)\n\ndef melspectrogram(y):\n    D = stft(y)\n    S = amp_to_db(linear_to_mel(np.abs(D)))\n    return normalize(S)\n\ndef stft(y):\n    return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)"""
utils/text/__init__.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\nimport re\nfrom utils.text import cleaners\nfrom utils.text.symbols import symbols\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n  \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  \'\'\'\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  \'\'\'Converts a sequence of IDs back to a string\'\'\'\n  result = \'\'\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == \'@\':\n        s = \'{%s}\' % s[1:]\n      result += s\n  return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(\'Unknown cleaner: %s\' % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s is not \'_\' and s is not \'~\'\n'"
utils/text/cleaners.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\n\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n  (\'mrs\', \'misess\'),\n  (\'mr\', \'mister\'),\n  (\'dr\', \'doctor\'),\n  (\'st\', \'saint\'),\n  (\'co\', \'company\'),\n  (\'jr\', \'junior\'),\n  (\'maj\', \'major\'),\n  (\'gen\', \'general\'),\n  (\'drs\', \'doctors\'),\n  (\'rev\', \'reverend\'),\n  (\'lt\', \'lieutenant\'),\n  (\'hon\', \'honorable\'),\n  (\'sgt\', \'sergeant\'),\n  (\'capt\', \'captain\'),\n  (\'esq\', \'esquire\'),\n  (\'ltd\', \'limited\'),\n  (\'col\', \'colonel\'),\n  (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n'"
utils/text/cmudict.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\nimport re\n\n\nvalid_symbols = [\n  \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n  \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n  \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n  \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n  \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n  \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n  \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=\'latin-1\') as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n      parts = line.split(\'  \')\n      word = re.sub(_alt_re, \'\', parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split(\' \')\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return \' \'.join(parts)\n'"
utils/text/numbers.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\nimport inflect\nimport re\n\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r\'([0-9][0-9\\,]+[0-9])\')\n_decimal_number_re = re.compile(r\'([0-9]+\\.[0-9]+)\')\n_pounds_re = re.compile(r\'\xc2\xa3([0-9\\,]*[0-9]+)\')\n_dollars_re = re.compile(r\'\\$([0-9\\.\\,]*[0-9]+)\')\n_ordinal_re = re.compile(r\'[0-9]+(st|nd|rd|th)\')\n_number_re = re.compile(r\'[0-9]+\')\n\n\ndef _remove_commas(m):\n  return m.group(1).replace(\',\', \'\')\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace(\'.\', \' point \')\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split(\'.\')\n  if len(parts) > 2:\n    return match + \' dollars\'  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = \'dollar\' if dollars == 1 else \'dollars\'\n    cent_unit = \'cent\' if cents == 1 else \'cents\'\n    return \'%s %s, %s %s\' % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = \'dollar\' if dollars == 1 else \'dollars\'\n    return \'%s %s\' % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = \'cent\' if cents == 1 else \'cents\'\n    return \'%s %s\' % (cents, cent_unit)\n  else:\n    return \'zero dollars\'\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return \'two thousand\'\n    elif num > 2000 and num < 2010:\n      return \'two thousand \' + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + \' hundred\'\n    else:\n      return _inflect.number_to_words(num, andword=\'\', zero=\'oh\', group=2).replace(\', \', \' \')\n  else:\n    return _inflect.number_to_words(num, andword=\'\')\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r\'\\1 pounds\', text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n'"
utils/text/recipes.py,0,"b""from utils.files import get_files\nfrom pathlib import Path\nfrom typing import Union\n\n\ndef ljspeech(path: Union[str, Path]):\n    csv_file = get_files(path, extension='.csv')\n\n    assert len(csv_file) == 1\n\n    text_dict = {}\n\n    with open(csv_file[0], encoding='utf-8') as f :\n        for line in f :\n            split = line.split('|')\n            text_dict[split[0]] = split[-1]\n\n    return text_dict"""
utils/text/symbols.py,0,"b'"""""" from https://github.com/keithito/tacotron """"""\n\n\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run through Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details. \'\'\'\nfrom utils.text import cmudict\n\n_pad = \'_\'\n_punctuation = \'!\\\'(),.:;? \'\n_special = \'-\'\n_letters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\'@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad] + list(_special) + list(_punctuation) + list(_letters) + _arpabet\n'"
