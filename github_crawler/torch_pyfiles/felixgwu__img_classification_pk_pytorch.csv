file_path,api_count,code
args.py,0,"b'import os\nimport glob\nimport time\nimport argparse\n\nimport config\n\nmodel_names = list(map(lambda n: os.path.basename(n)[:-3],\n                       glob.glob(\'models/[A-Za-z]*.py\')))\n\narg_parser = argparse.ArgumentParser(\n                description=\'Image classification PK main script\')\n\nexp_group = arg_parser.add_argument_group(\'exp\', \'experiment setting\')\nexp_group.add_argument(\'--save\', default=\'save/default-{}\'.format(time.time()),\n                       type=str, metavar=\'SAVE\',\n                       help=\'path to the experiment logging directory\'\n                       \'(default: save/debug)\')\nexp_group.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                       help=\'path to latest checkpoint (default: none)\')\nexp_group.add_argument(\'--eval\', \'--evaluate\', dest=\'evaluate\', default=\'\',\n                       choices=[\'\', \'train\', \'val\', \'test\'],\n                       help=\'eval mode: evaluate model on train/val/test set\'\n                       \' (default: \\\'\\\' i.e. training mode)\')\nexp_group.add_argument(\'-f\', \'--force\', dest=\'force\', action=\'store_true\',\n                       help=\'force to overwrite existing save path\')\nexp_group.add_argument(\'--print-freq\', \'-p\', default=100, type=int,\n                       metavar=\'N\', help=\'print frequency (default: 100)\')\nexp_group.add_argument(\'--no_tensorboard\', dest=\'tensorboard\',\n                       action=\'store_false\',\n                       help=\'do not use tensorboard_logger for logging\')\nexp_group.add_argument(\'--seed\', default=0, type=int,\n                       help=\'random seed\')\n\n# dataset related\ndata_group = arg_parser.add_argument_group(\'data\', \'dataset setting\')\ndata_group.add_argument(\'--data\', metavar=\'D\', default=\'cifar10\',\n                        choices=config.datasets.keys(),\n                        help=\'datasets: \' +\n                        \' | \'.join(config.datasets.keys()) +\n                        \' (default: cifar10)\')\ndata_group.add_argument(\'--no_valid\', action=\'store_false\', dest=\'use_validset\',\n                        help=\'not hold out 10 percent of training data as validation\')\ndata_group.add_argument(\'--data_root\', metavar=\'DIR\', default=\'data\',\n                        help=\'path to dataset (default: data)\')\ndata_group.add_argument(\'-j\', \'--workers\', dest=\'num_workers\', default=4,\n                        type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 4)\')\ndata_group.add_argument(\'--normalized\', action=\'store_true\',\n                        help=\'normalize the data into zero mean and unit std\')\ndata_group.add_argument(\'--cutout\', action=\'store_true\',\n                        help=\'use cutout\')\ndata_group.add_argument(\'--n_holes\', type=int, default=1,\n                        help=\'number of holes to cut out from image\')\ndata_group.add_argument(\'--length\', type=int, default=16,\n                        help=\'length of the holes\')\ndata_group.add_argument(\'--data_aug\', action=\'store_true\',\n                        help=\'data augmentation\')\n\n# model arch related\narch_group = arg_parser.add_argument_group(\'arch\',\n                                           \'model architecture setting\')\narch_group.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet\',\n                        type=str, choices=model_names,\n                        help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet)\')\narch_group.add_argument(\'-d\', \'--depth\', default=56, type=int, metavar=\'D\',\n                        help=\'depth (default=56)\')\narch_group.add_argument(\'--drop-rate\', default=0.0, type=float,\n                        metavar=\'DROPRATE\', help=\'dropout rate (default: 0.2)\')\narch_group.add_argument(\'--death-mode\', default=\'none\',\n                        choices=[\'none\', \'linear\', \'uniform\'],\n                        help=\'death mode for stochastic depth (default: none)\')\narch_group.add_argument(\'--death-rate\', default=0.5, type=float,\n                        help=\'death rate rate (default: 0.5)\')\narch_group.add_argument(\'--growth-rate\', default=12, type=int,\n                        metavar=\'GR\', help=\'Growth rate of DenseNet\'\n                        \'(default: 12)\')\narch_group.add_argument(\'--bn-size\', default=4, type=int,\n                        metavar=\'B\', help=\'bottle neck ratio of DenseNet\'\n                        \' (0 means dot\\\'t use bottle necks) (default: 4)\')\narch_group.add_argument(\'--compression\', default=0.5, type=float,\n                        metavar=\'C\', help=\'compression ratio of DenseNet\'\n                        \' (1 means dot\\\'t use compression) (default: 0.5)\')\n# used to set the argument when to resume automatically\narch_resume_names = [\'arch\', \'depth\', \'death_mode\', \'death_rate\', \'death_rate\',\n                     \'growth_rate\', \'bn_size\', \'compression\']\n\n# training related\noptim_group = arg_parser.add_argument_group(\'optimization\',\n                                            \'optimization setting\')\noptim_group.add_argument(\'--trainer\', default=\'train\', type=str,\n                         help=\'trainer file name without "".py""\'\n                         \' (default: train)\')\noptim_group.add_argument(\'--epochs\', default=164, type=int, metavar=\'N\',\n                         help=\'number of total epochs to run (default: 164)\')\noptim_group.add_argument(\'--start-epoch\', default=1, type=int, metavar=\'N\',\n                         help=\'manual epoch number (useful on restarts)\')\noptim_group.add_argument(\'--patience\', default=0, type=int, metavar=\'N\',\n                         help=\'patience for early stopping\'\n                         \'(0 means no early stopping)\')\noptim_group.add_argument(\'-b\', \'--batch-size\', default=64, type=int,\n                         metavar=\'N\', help=\'mini-batch size (default: 64)\')\noptim_group.add_argument(\'--optimizer\', default=\'sgd\',\n                         choices=[\'sgd\', \'rmsprop\', \'adam\'], metavar=\'N\',\n                         help=\'optimizer (default=sgd)\')\noptim_group.add_argument(\'--lr\', \'--learning-rate\', default=0.1, type=float,\n                         metavar=\'LR\',\n                         help=\'initial learning rate (default: 0.1)\')\noptim_group.add_argument(\'--decay_rate\', default=0.1, type=float, metavar=\'N\',\n                         help=\'decay rate of learning rate (default: 0.1)\')\noptim_group.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                         help=\'momentum (default=0.9)\')\noptim_group.add_argument(\'--no_nesterov\', dest=\'nesterov\',\n                         action=\'store_false\',\n                         help=\'do not use Nesterov momentum\')\noptim_group.add_argument(\'--alpha\', default=0.99, type=float, metavar=\'M\',\n                         help=\'alpha for \')\noptim_group.add_argument(\'--beta1\', default=0.9, type=float, metavar=\'M\',\n                         help=\'beta1 for Adam (default: 0.9)\')\noptim_group.add_argument(\'--beta2\', default=0.999, type=float, metavar=\'M\',\n                         help=\'beta2 for Adam (default: 0.999)\')\noptim_group.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                         metavar=\'W\', help=\'weight decay (default: 1e-4)\')\n'"
config.py,0,"b""# this is used for storing configurations of datasets & models\n\ndatasets = {\n    'cifar10': {\n        'num_classes': 10,\n    },\n    'cifar100': {\n        'num_classes': 100,\n        'augmentation': False,\n    },\n}\n"""
dataloader.py,7,"b""import torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nfrom utils import Cutout\n\n\ndef getDataloaders(data, config_of_data, splits=['train', 'val', 'test'],\n                   aug=True, use_validset=True, data_root='data', batch_size=64, normalized=True,\n                   data_aug=False, cutout=False, n_holes=1, length=16,\n                   num_workers=3, **kwargs):\n    train_loader, val_loader, test_loader = None, None, None\n\n    if data.find('cifar10') >= 0:\n        print('loading ' + data)\n        print(config_of_data)\n        if data.find('cifar100') >= 0:\n            d_func = dset.CIFAR100\n            normalize = transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n                                             std=[0.2675, 0.2565, 0.2761])\n        else:\n            d_func = dset.CIFAR10\n            normalize = transforms.Normalize(mean=[0.4914, 0.4824, 0.4467],\n                                             std=[0.2471, 0.2435, 0.2616])\n        if data_aug:\n            print('with data augmentation')\n            aug_trans = [\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n            ]\n        else:\n            aug_trans = []\n        common_trans = [transforms.ToTensor()]\n        if normalized:\n            print('dataset is normalized')\n            common_trans.append(normalize)\n        train_compose = aug_trans + common_trans\n        if cutout:\n            train_compose.append(Cutout(n_holes=n_holes, length=length))\n        train_compose = transforms.Compose(train_compose)\n        test_compose = transforms.Compose(common_trans)\n\n        if use_validset:\n            # uses last 5000 images of the original training split as the\n            # validation set\n            if 'train' in splits:\n                train_set = d_func(data_root, train=True, transform=train_compose,\n                                   download=True)\n                train_loader = torch.utils.data.DataLoader(\n                    train_set, batch_size=batch_size,\n                    sampler=torch.utils.data.sampler.SubsetRandomSampler(\n                        range(45000)),\n                    num_workers=num_workers, pin_memory=True)\n            if 'val' in splits:\n                val_set = d_func(data_root, train=True, transform=test_compose)\n                val_loader = torch.utils.data.DataLoader(\n                    val_set, batch_size=batch_size,\n                    sampler=torch.utils.data.sampler.SubsetRandomSampler(\n                        range(45000, 50000)),\n                    num_workers=num_workers, pin_memory=True)\n\n            if 'test' in splits:\n                test_set = d_func(data_root, train=False, transform=test_compose)\n                test_loader = torch.utils.data.DataLoader(\n                    test_set, batch_size=batch_size, shuffle=True,\n                    num_workers=num_workers, pin_memory=True)\n        else:\n            if 'train' in splits:\n                train_set = d_func(data_root, train=True, transform=train_compose,\n                                   download=True)\n                train_loader = torch.utils.data.DataLoader(\n                    train_set, batch_size=batch_size, shuffle=True,\n                    num_workers=num_workers, pin_memory=True)\n            if 'val' in splits or 'test' in splits:\n                test_set = d_func(data_root, train=False, transform=test_compose)\n                test_loader = torch.utils.data.DataLoader(\n                    test_set, batch_size=batch_size, shuffle=True,\n                    num_workers=num_workers, pin_memory=True)\n                val_loader = test_loader\n\n\n    else:\n        raise NotImplemented\n    return train_loader, val_loader, test_loader\n"""
getbest.py,1,"b""#!/usr/bin/env python3\nimport json\nimport os\nimport sys\nimport torch\n\npath_len = max(0, *map(len, sys.argv[1:]))\npath_part = '{:' + str(path_len) + '}'\n\nprint((path_part + ' {:8} {:10} {:10} {:10}')\n      .format('Path', 'n_epochs', 'best_epoch', 'train_err1', 'val_err1'))\nfor i in range(1, len(sys.argv)):\n    try:\n        with open(os.path.join(sys.argv[i], 'scores.tsv')) as f:\n            names = f.readline().split()\n            scores = [list(map(float, line.split())) for line in f]\n        name2col = {n:i for i, n in enumerate(names)}\n        scores = torch.Tensor(scores)\n        argmax = scores.argmax(0)\n        best_rol = scores[argmax[name2col['val_err1']], :]\n        print((path_part + '{:8d} {:10d} {:10.2f} {:10.2f}')\n              .format(sys.argv[i], scores.size(0),\n                      int(best_rol[name2col['epoch']]),\n                      best_rol[name2col['train_err1']],\n                      best_rol[name2col['val_err1']],\n                      ))\n    except FileNotFoundError:\n        pass\n"""
main.py,9,"b'#!/usr/bin/env python3\n\nfrom __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport os\nimport sys\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nfrom colorama import Fore\nfrom importlib import import_module\n\nimport config\nfrom dataloader import getDataloaders\nfrom utils import save_checkpoint, get_optimizer, create_save_folder\nfrom args import arg_parser, arch_resume_names\n\ntry:\n    from tensorboard_logger import configure, log_value\nexcept BaseException:\n    configure = None\n\n\ndef getModel(arch, **kargs):\n    m = import_module(\'models.\' + arch)\n    model = m.createModel(**kargs)\n    if arch.startswith(\'alexnet\') or arch.startswith(\'vgg\'):\n        model.features = torch.nn.DataParallel(model.features)\n        model.cuda()\n    else:\n        model = torch.nn.DataParallel(model).cuda()\n    return model\n\n\ndef main():\n    # parse arg and start experiment\n    global args\n    best_err1 = 100.\n    best_epoch = 0\n\n    args = arg_parser.parse_args()\n    args.config_of_data = config.datasets[args.data]\n    args.num_classes = config.datasets[args.data][\'num_classes\']\n    if configure is None:\n        args.tensorboard = False\n        print(Fore.RED +\n              \'WARNING: you don\\\'t have tesnorboard_logger installed\' +\n              Fore.RESET)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if args.resume and os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            old_args = checkpoint[\'args\']\n            print(\'Old args:\')\n            print(old_args)\n            # set args based on checkpoint\n            if args.start_epoch <= 0:\n                args.start_epoch = checkpoint[\'epoch\'] + 1\n            best_epoch = args.start_epoch - 1\n            best_err1 = checkpoint[\'best_err1\']\n            for name in arch_resume_names:\n                if name in vars(args) and name in vars(old_args):\n                    setattr(args, name, getattr(old_args, name))\n            model = getModel(**vars(args))\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(\n                ""=> no checkpoint found at \'{}\'"".format(\n                    Fore.RED +\n                    args.resume +\n                    Fore.RESET),\n                file=sys.stderr)\n            return\n    else:\n        # create model\n        print(""=> creating model \'{}\'"".format(args.arch))\n        model = getModel(**vars(args))\n\n    cudnn.benchmark = True\n\n    # define loss function (criterion) and pptimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    # define optimizer\n    optimizer = get_optimizer(model, args)\n\n    # set random seed\n    torch.manual_seed(args.seed)\n\n    Trainer = import_module(args.trainer).Trainer\n    trainer = Trainer(model, criterion, optimizer, args)\n\n    # create dataloader\n    if args.evaluate == \'train\':\n        train_loader, _, _ = getDataloaders(\n            splits=(\'train\'), **vars(args))\n        trainer.test(train_loader, best_epoch)\n        return\n    elif args.evaluate == \'val\':\n        _, val_loader, _ = getDataloaders(\n            splits=(\'val\'), **vars(args))\n        trainer.test(val_loader, best_epoch)\n        return\n    elif args.evaluate == \'test\':\n        _, _, test_loader = getDataloaders(\n            splits=(\'test\'), **vars(args))\n        trainer.test(test_loader, best_epoch)\n        return\n    else:\n        train_loader, val_loader, _ = getDataloaders(\n            splits=(\'train\', \'val\'), **vars(args))\n\n    # check if the folder exists\n    create_save_folder(args.save, args.force)\n\n    # set up logging\n    global log_print, f_log\n    f_log = open(os.path.join(args.save, \'log.txt\'), \'w\')\n\n    def log_print(*args):\n        print(*args)\n        print(*args, file=f_log)\n    log_print(\'args:\')\n    log_print(args)\n    print(\'model:\', file=f_log)\n    print(model, file=f_log)\n    log_print(\'# of params:\',\n              str(sum([p.numel() for p in model.parameters()])))\n    f_log.flush()\n    torch.save(args, os.path.join(args.save, \'args.pth\'))\n    scores = [\'epoch\\tlr\\ttrain_loss\\tval_loss\\ttrain_err1\'\n              \'\\tval_err1\\ttrain_err5\\tval_err5\']\n    if args.tensorboard:\n        configure(args.save, flush_secs=5)\n\n    for epoch in range(args.start_epoch, args.epochs + 1):\n\n        # train for one epoch\n        train_loss, train_err1, train_err5, lr = trainer.train(\n            train_loader, epoch)\n\n        if args.tensorboard:\n            log_value(\'lr\', lr, epoch)\n            log_value(\'train_loss\', train_loss, epoch)\n            log_value(\'train_err1\', train_err1, epoch)\n            log_value(\'train_err5\', train_err5, epoch)\n\n        # evaluate on validation set\n        val_loss, val_err1, val_err5 = trainer.test(val_loader, epoch)\n\n        if args.tensorboard:\n            log_value(\'val_loss\', val_loss, epoch)\n            log_value(\'val_err1\', val_err1, epoch)\n            log_value(\'val_err5\', val_err5, epoch)\n\n        # save scores to a tsv file, rewrite the whole file to prevent\n        # accidental deletion\n        scores.append((\'{}\\t{}\' + \'\\t{:.4f}\' * 6)\n                      .format(epoch, lr, train_loss, val_loss,\n                              train_err1, val_err1, train_err5, val_err5))\n        with open(os.path.join(args.save, \'scores.tsv\'), \'w\') as f:\n            print(\'\\n\'.join(scores), file=f)\n\n        # remember best err@1 and save checkpoint\n        is_best = val_err1 < best_err1\n        if is_best:\n            best_err1 = val_err1\n            best_epoch = epoch\n            print(Fore.GREEN + \'Best var_err1 {}\'.format(best_err1) +\n                  Fore.RESET)\n            # test_loss, test_err1, test_err1 = validate(\n            #     test_loader, model, criterion, epoch, True)\n            # save test\n        save_checkpoint({\n            \'args\': args,\n            \'epoch\': epoch,\n            \'best_epoch\': best_epoch,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'best_err1\': best_err1,\n        }, is_best, args.save)\n        if not is_best and epoch - best_epoch >= args.patience > 0:\n            break\n    print(\'Best val_err1: {:.4f} at epoch {}\'.format(best_err1, best_epoch))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,1,"b""from __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import\n\nimport torch\nfrom utils import AverageMeter, adjust_learning_rate, error\nimport time\n\n\nclass Trainer(object):\n    def __init__(self, model, criterion=None, optimizer=None, args=None):\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.args = args\n\n    def train(self, train_loader, epoch):\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        # switch to train mode\n        self.model.train()\n\n        lr = adjust_learning_rate(self.optimizer, self.args.lr,\n                                  self.args.decay_rate, epoch,\n                                  self.args.epochs)  # TODO: add custom\n        print('Epoch {:3d} lr = {:.6e}'.format(epoch, lr))\n\n        end = time.time()\n        for i, (inputs, targets) in enumerate(train_loader):\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            inputs = inputs.cuda()\n            targets = targets.cuda()\n\n            # compute outputs\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, targets)\n\n            # measure error and record loss\n            err1, err5 = error(outputs, targets, topk=(1, 5))\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(err1.item(), inputs.size(0))\n            top5.update(err5.item(), inputs.size(0))\n\n            # compute gradient and do SGD step\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if self.args.print_freq > 0 and \\\n                    (i + 1) % self.args.print_freq == 0:\n                print('Epoch: [{0}][{1}/{2}]\\t'\n                      'Time {batch_time.avg:.3f}\\t'\n                      'Data {data_time.avg:.3f}\\t'\n                      'Loss {loss.val:.4f}\\t'\n                      'Err@1 {top1.val:.4f}\\t'\n                      'Err@5 {top5.val:.4f}'.format(\n                          epoch, i + 1, len(train_loader),\n                          batch_time=batch_time, data_time=data_time,\n                          loss=losses, top1=top1, top5=top5))\n\n        print('Epoch: {:3d} Train loss {loss.avg:.4f} '\n              'Err@1 {top1.avg:.4f}'\n              ' Err@5 {top5.avg:.4f}'\n              .format(epoch, loss=losses, top1=top1, top5=top5))\n        return losses.avg, top1.avg, top5.avg, lr\n\n    def test(self, val_loader, epoch, silence=False):\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        top1 = AverageMeter()\n        top5 = AverageMeter()\n\n        # switch to evaluate mode\n        self.model.eval()\n\n        end = time.time()\n        with torch.no_grad():\n            for i, (inputs, targets) in enumerate(val_loader):\n                inputs = inputs.cuda()\n                targets = targets.cuda()\n\n                # compute outputs\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, targets)\n\n                # measure error and record loss\n                err1, err5 = error(outputs.data, targets, topk=(1, 5))\n                losses.update(loss.item(), inputs.size(0))\n                top1.update(err1.item(), inputs.size(0))\n                top5.update(err5.item(), inputs.size(0))\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n        if not silence:\n            print('Epoch: {:3d} val   loss {loss.avg:.4f} Err@1 {top1.avg:.4f}'\n                  ' Err@5 {top5.avg:.4f}'.format(epoch, loss=losses,\n                                                 top1=top1, top5=top5))\n\n        return losses.avg, top1.avg, top5.avg\n"""
utils.py,6,"b'import sys\nimport time\nimport os\nimport shutil\nimport torch\n\nimport numpy as np\nfrom colorama import Fore\n\n\ndef create_save_folder(save_path, force=False, ignore_patterns=[]):\n    if os.path.exists(save_path):\n        print(Fore.RED + save_path + Fore.RESET\n              + \' already exists!\', file=sys.stderr)\n        if not force:\n            ans = input(\'Do you want to overwrite it? [y/N]:\')\n            if ans not in (\'y\', \'Y\', \'yes\', \'Yes\'):\n                os.exit(1)\n        from getpass import getuser\n        tmp_path = \'/tmp/{}-experiments/{}_{}\'.format(getuser(),\n                                                      os.path.basename(save_path),\n                                                      time.time())\n        print(\'move existing {} to {}\'.format(save_path, Fore.RED\n                                              + tmp_path + Fore.RESET))\n        shutil.copytree(save_path, tmp_path)\n        shutil.rmtree(save_path)\n    os.makedirs(save_path)\n    print(\'create folder: \' + Fore.GREEN + save_path + Fore.RESET)\n\n    # copy code to save folder\n    if save_path.find(\'debug\') < 0:\n        shutil.copytree(\'.\', os.path.join(save_path, \'src\'), symlinks=True,\n                        ignore=shutil.ignore_patterns(\'*.pyc\', \'__pycache__\',\n                                                      \'*.path.tar\', \'*.pth\',\n                                                      \'*.ipynb\', \'.*\', \'data\',\n                                                      \'save\', \'save_backup\',\n                                                      save_path,\n                                                      *ignore_patterns))\n\n\ndef adjust_learning_rate(optimizer, lr_init, decay_rate, epoch, num_epochs):\n    """"""Decay Learning rate at 1/2 and 3/4 of the num_epochs""""""\n    lr = lr_init\n    if epoch >= num_epochs * 0.75:\n        lr *= decay_rate**2\n    elif epoch >= num_epochs * 0.5:\n        lr *= decay_rate\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\n\ndef save_checkpoint(state, is_best, save_dir, filename=\'checkpoint.pth.tar\'):\n    filename = os.path.join(save_dir, filename)\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, os.path.join(save_dir, \'model_best.pth.tar\'))\n\n\ndef get_optimizer(model, args):\n    if args.optimizer == \'sgd\':\n        return torch.optim.SGD(model.parameters(), args.lr,\n                               momentum=args.momentum, nesterov=args.nesterov,\n                               weight_decay=args.weight_decay)\n    elif args.optimizer == \'rmsprop\':\n        return torch.optim.RMSprop(model.parameters(), args.lr,\n                                   alpha=args.alpha,\n                                   weight_decay=args.weight_decay)\n    elif args.optimizer == \'adam\':\n        return torch.optim.Adam(model.parameters(), args.lr,\n                                beta=(args.beta1, args.beta2),\n                                weight_decay=args.weight_decay)\n    else:\n        raise NotImplementedError\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef error(output, target, topk=(1,)):\n    """"""Computes the error@k for the specified values of k""""""\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0)\n            res.append(100.0 - correct_k.mul_(100.0 / batch_size))\n    return res\n\n\n###############################################################\n# Copied from  https://github.com/uoguelph-mlrg/Cutout\n# ECL v2.0 license https://github.com/uoguelph-mlrg/Cutout/blob/master/LICENSE.md\n\nclass Cutout(object):\n    """"""Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    """"""\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        """"""\n        h = img.size(1)\n        w = img.size(2)\n\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img = img * mask\n\n        return img\n###############################################################\n'"
models/__init__.py,0,b''
models/densenet.py,3,"b'# This implementation is based on the DenseNet-BC implementation in torchvision\n# https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py\n# This code supports original DenseNet as well\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom torchvision.models.densenet import _Transition\n\n\nclass _DenseLayer(nn.Sequential):\n\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm.1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu.1\', nn.ReLU(inplace=True)),\n        if bn_size > 0:\n            self.add_module(\'conv.1\', nn.Conv2d(num_input_features, bn_size *\n                            growth_rate, kernel_size=1, stride=1, bias=False)),\n            self.add_module(\'norm.2\', nn.BatchNorm2d(bn_size * growth_rate)),\n            self.add_module(\'relu.2\', nn.ReLU(inplace=True)),\n            self.add_module(\'conv.2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                            kernel_size=3, stride=1, padding=1, bias=False)),\n        else:\n            self.add_module(\'conv.1\', nn.Conv2d(num_input_features, growth_rate,\n                            kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n    def __init__(self, growth_rate=12, block_config=(16, 16, 16), compression=0.5,\n                 num_init_features=24, bn_size=4, drop_rate=0, avgpool_size=8,\n                 num_classes=10):\n\n        super(DenseNet, self).__init__()\n        assert 0 < compression <= 1, \'compression of densenet should be between \'\n        self.avgpool_size = avgpool_size\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=3, stride=1, padding=1, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers,\n                                num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate,\n                                drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=int(num_features\n                                                            * compression))\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = int(num_features * compression)\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # for m in self.modules():\n        #     if isinstance(m, nn.BatchNorm2d):\n        #         m.weight.fill_(1.)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=self.avgpool_size).view(\n            features.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n\ndef createModel(data, depth=100, growth_rate=12, num_classes=10, drop_rate=0,\n                num_init_features=24, compression=0.5, bn_size=4, **kwargs):\n    assert (depth - 4) % 3 == 0, \'depth should be one of 3N+4\'\n    avgpool_size = 7 if data == \'imagenet\' else 8\n    N = (depth - 4) // 3\n    suffix = \'-\'\n    if bn_size > 0:\n        N //= 2\n        suffix += \'B\'\n    block_config = (N, N, N)\n    if compression < 1.:\n        suffix += \'C\'\n\n    if suffix == \'-\':\n        suffix = \'\'\n    print(\'Create DenseNet{}-{:d} for {}\'.format(suffix, depth, data)) \n    return DenseNet(growth_rate=growth_rate, num_classes=num_classes,\n                    compression=compression, drop_rate=drop_rate, bn_size=bn_size,\n                    block_config=block_config, avgpool_size=avgpool_size)\n\n'"
models/preact_resnet.py,3,"b""# Copied from https://github.com/kuangliu/pytorch-cifar/blob/master/models/preact_resnet.py\n\n'''Pre-activation ResNet in PyTorch.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PreActBlock(nn.Module):\n    '''Pre-activation version of the BasicBlock.'''\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    '''Pre-activation version of the original Bottleneck module.'''\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(PreActResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef PreActResNet18():\n    return PreActResNet(PreActBlock, [2,2,2,2])\n\ndef PreActResNet34():\n    return PreActResNet(PreActBlock, [3,4,6,3])\n\ndef PreActResNet50():\n    return PreActResNet(PreActBottleneck, [3,4,6,3])\n\ndef PreActResNet101():\n    return PreActResNet(PreActBottleneck, [3,4,23,3])\n\ndef PreActResNet152():\n    return PreActResNet(PreActBottleneck, [3,8,36,3])\n\n\ndef test():\n    net = PreActResNet18()\n    y = net((torch.randn(1,3,32,32)))\n    print(y.size())\n\n# test()\ndef createModel(depth, data, num_classes, death_mode='none', death_rate=0.5,\n                **kwargs):\n    print('Create PreActResNet-{:d} for {}'.format(depth, data))\n    if depth == 18:\n        return PreActResNet(PreActBlock, [2,2,2,2], num_classes)\n    elif depth == 34:\n        return PreActResNet(PreActBlock, [3,4,6,3], num_classes)\n    elif depth == 50:\n        return PreActResNet(PreActBottleneck, [3,4,6,3], num_classes)\n    elif depth == 101:\n        return PreActResNet(PreActBottleneck, [3,4,23,3], num_classes)\n    elif depth == 152:\n        return PreActResNet(PreActBottleneck, [3,8,36,3], num_classes)\n"""
models/resnet.py,3,"b""# This implementation is based on the DenseNet implementation in torchvision\n# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\nimport math\nimport torch\nfrom torch import nn\nfrom torchvision.models.resnet import conv3x3\n\n\nclass BasicBlockWithDeathRate(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, death_rate=0.,\n                 downsample=None):\n        super(BasicBlockWithDeathRate, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.death_rate = death_rate\n\n    def forward(self, x):\n        residual = x\n        if self.downsample is not None:\n            x = self.downsample(x)\n        # TODO: fix the bug of original Stochatic depth\n        if not self.training or torch.rand(1)[0] >= self.death_rate:\n            residual = self.conv1(residual)\n            residual = self.bn1(residual)\n            residual = self.relu1(residual)\n            residual = self.conv2(residual)\n            residual = self.bn2(residual)\n            if self.training:\n                residual /= (1. - self.death_rate)\n            x = x + residual\n            x = self.relu2(x)\n\n        return x\n\n\n# class DropLayer(nn.Module):\n#     '''Drop the layer with probability p.\n#     It can be used for stochasitc depth'''\n\n#     def __init__(self, layer, death_rate=0.5):\n#         super(DropLayer, self).__init__()\n#         self.layer = layer\n#         self.death_rate = death_rate\n\n#     def forward(self, x):\n#         print(self.layer)\n#         if not self.training or torch.rand(1)[0] >= self.death_rate:\n#             print('pass')\n#             return self.layer(x)\n#         else:\n#             print('stop')\n#             return x.div_(1 - self.death_rate)\n\n#     def __str__(self):\n#         return 'DropLayer(death_rate={})'.format(self.death_rate)\n\n\nclass DownsampleB(nn.Module):\n\n    def __init__(self, nIn, nOut, stride):\n        super(DownsampleB, self).__init__()\n        self.avg = nn.AvgPool2d(stride)\n        self.expand_ratio = nOut // nIn\n\n    def forward(self, x):\n        x = self.avg(x)\n        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n\n\nclass ResNetCifar(nn.Module):\n    '''Small ResNet for CIFAR & SVHN\n    death_rates: death_rates of each block except for the first and\n                 the last block\n    '''\n\n    def __init__(self, depth, death_rates=None, block=BasicBlockWithDeathRate,\n                 num_classes=10):\n        assert (depth - 2) % 6 == 0, 'depth should be one of 6N+2'\n        super(ResNetCifar, self).__init__()\n        n = (depth - 2) // 6\n        assert death_rates is None or len(death_rates) == 3 * n\n        if death_rates is None:\n            death_rates = [0.] * (3 * n)\n        self.inplanes = 16\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, death_rates[:n])\n        self.layer2 = self._make_layer(block, 32, death_rates[n:2 * n],\n                                       stride=2)\n        self.layer3 = self._make_layer(block, 64, death_rates[2 * n:],\n                                       stride=2)\n        self.avgpool = nn.AvgPool2d(8)\n        self.fc = nn.Linear(64 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, death_rates, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = DownsampleB(self.inplanes, planes * block.expansion,\n                                     stride)\n            # downsample = nn.Sequential(\n            #     nn.Conv2d(self.inplanes, planes * block.expansion,\n            #               kernel_size=1, stride=stride, bias=False),\n            #     nn.BatchNorm2d(planes * block.expansion),\n            # )\n\n        layers = [block(self.inplanes, planes, stride, downsample=downsample,\n                        death_rate=death_rates[0])]\n        self.inplanes = planes * block.expansion\n        for death_rate in death_rates[1:]:\n            layers.append(block(self.inplanes, planes, death_rate=death_rate))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef createModel(depth, data, num_classes, death_mode='none', death_rate=0.5,\n                **kwargs):\n    assert (depth - 2) % 6 == 0, 'depth should be one of 6N+2'\n    print('Create ResNet-{:d} for {}'.format(depth, data))\n    nblocks = (depth - 2) // 2\n    if death_mode == 'uniform':\n        death_rates = [death_rate] * nblocks\n    elif death_mode == 'linear':\n        death_rates = [float(i + 1) * death_rate / float(nblocks)\n                       for i in range(nblocks)]\n    else:\n        death_rates = None\n    return ResNetCifar(depth, death_rates, BasicBlockWithDeathRate,\n                       num_classes)"""
models/resnet_kuangliu.py,3,"b""# Copied from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n'''ResNet in PyTorch.\n\nFor Pre-activation ResNet, see 'preact_resnet.py'.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2,2,2,2])\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3,4,6,3])\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3,4,6,3])\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3,4,23,3])\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3,8,36,3])\n\n\ndef test():\n    net = ResNet18()\n    y = net(torch.randn(1,3,32,32))\n    print(y.size())\n\n# test()\n\ndef createModel(depth, data, num_classes, death_mode='none', death_rate=0.5,\n                **kwargs):\n    print('Create ResNet-{:d} for {}'.format(depth, data))\n    if depth == 18:\n        return ResNet(BasicBlock, [2,2,2,2], num_classes)\n    elif depth == 34:\n        return ResNet(BasicBlock, [3,4,6,3], num_classes)\n    elif depth == 50:\n        return ResNet(Bottleneck, [3,4,6,3], num_classes)\n    elif depth == 101:\n        return ResNet(Bottleneck, [3,4,23,3], num_classes)\n    elif depth == 152:\n        return ResNet(Bottleneck, [3,8,36,3], num_classes)\n"""
