file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\n""""""The setup script.""""""\n\nfrom setuptools import setup, find_packages\n\nwith open(\'README.md\') as readme_file:\n    readme = readme_file.read()\n\nwith open(\'HISTORY.rst\') as history_file:\n    history = history_file.read()\n\nrequirements = [\n    \'Click>=7.0\',\n    \'nibabel\',\n    \'numpy\',\n    \'Python-Deprecated\',\n    \'scipy\',\n    \'SimpleITK\',\n    \'torch>=1.2\',  # for IterableDataset\n    \'torchvision\',\n    \'tqdm\',\n]\n\nsetup(\n    author=""Fernando Perez-Garcia"",\n    author_email=\'fernando.perezgarcia.17@ucl.ac.uk\',\n    python_requires=\'>=3.6\',\n    classifiers=[\n        \'Development Status :: 2 - Pre-Alpha\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Natural Language :: English\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n    description=(\n        ""Tools for loading, augmenting and writing 3D medical images""\n        "" on PyTorch.""\n    ),\n    entry_points={\n        \'console_scripts\': [\n            \'torchio-transform=torchio.cli:apply_transform\',\n        ],\n    },\n    install_requires=requirements,\n    license=""MIT license"",\n    long_description=readme + \'\\n\\n\' + history,\n    long_description_content_type=\'text/markdown\',\n    include_package_data=True,\n    keywords=\'torchio\',\n    name=\'torchio\',\n    packages=find_packages(include=[\'torchio\', \'torchio.*\']),\n    setup_requires=[],\n    test_suite=\'tests\',\n    tests_require=[],\n    url=\'https://github.com/fepegar/torchio\',\n    version=\'0.16.22\',\n    zip_safe=False,\n)\n'"
examples/example_heteromodal.py,2,"b'""""""\nThis is an example of a very particular case in which some modalities might be\nmissing for some of the subjects, as in\n\n    Dorent et al. 2019, Hetero-Modal Variational Encoder-Decoder\n    for Joint Modality Completion and Segmentation\n\n""""""\n\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchio\nfrom torchio import Image, Subject, ImagesDataset, Queue\nfrom torchio.data import UniformSampler\n\ndef main():\n    # Define training and patches sampling parameters\n    num_epochs = 20\n    patch_size = 128\n    queue_length = 100\n    samples_per_volume = 5\n    batch_size = 2\n\n    # Populate a list with images\n    one_subject = Subject(\n        T1=Image(\'../BRATS2018_crop_renamed/LGG75_T1.nii.gz\', torchio.INTENSITY),\n        T2=Image(\'../BRATS2018_crop_renamed/LGG75_T2.nii.gz\', torchio.INTENSITY),\n        label=Image(\'../BRATS2018_crop_renamed/LGG75_Label.nii.gz\', torchio.LABEL),\n    )\n\n    # This subject doesn\'t have a T2 MRI!\n    another_subject = Subject(\n        T1=Image(\'../BRATS2018_crop_renamed/LGG74_T1.nii.gz\', torchio.INTENSITY),\n        label=Image(\'../BRATS2018_crop_renamed/LGG74_Label.nii.gz\', torchio.LABEL),\n    )\n\n    subjects = [\n        one_subject,\n        another_subject,\n    ]\n\n    subjects_dataset = ImagesDataset(subjects)\n    queue_dataset = Queue(\n        subjects_dataset,\n        queue_length,\n        samples_per_volume,\n        UniformSampler(patch_size),\n    )\n\n    # This collate_fn is needed in the case of missing modalities\n    # In this case, the batch will be composed by a *list* of samples instead of\n    # the typical Python dictionary that is collated by default in Pytorch\n    batch_loader = DataLoader(\n        queue_dataset,\n        batch_size=batch_size,\n        collate_fn=lambda x: x,\n    )\n\n    # Mock PyTorch model\n    model = nn.Identity()\n\n    for epoch_index in range(num_epochs):\n        for batch in batch_loader:  # batch is a *list* here, not a dictionary\n            logits = model(batch)\n            print([batch[idx].keys() for idx in range(batch_size)])\n    print()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tests/__init__.py,0,"b'""""""Unit test package for torchio.""""""\n'"
tests/test_cli.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for CLI tool package.""""""\n\nfrom click.testing import CliRunner\nfrom torchio import cli\nfrom .utils import TorchioTestCase\n\n\nclass TestCLI(TorchioTestCase):\n    """"""Tests for CLI tool.""""""\n    def test_help(self):\n        """"""Test the CLI.""""""\n        runner = CliRunner()\n        help_result = runner.invoke(cli.apply_transform, [\'--help\'])\n        assert help_result.exit_code == 0\n        assert \'Show this message and exit.\' in help_result.output\n'"
tests/test_utils.py,1,"b'#!/usr/bin/env python\n\n""""""Tests for `utils` package.""""""\n\nimport unittest\nimport torch\nimport numpy as np\nfrom torchio import LABEL, INTENSITY\nfrom torchio.utils import (\n    to_tuple,\n    get_stem,\n    guess_type,\n)\nfrom .utils import TorchioTestCase\n\n\nclass TestUtils(TorchioTestCase):\n    """"""Tests for `utils` module.""""""\n\n    def get_sample(self, consistent):\n        shape = 1, 10, 20, 30\n        affine = np.diag((1, 2, 3, 1))\n        affine[:3, 3] = 40, 50, 60\n        shape2 = 1, 20, 10, 30\n        sample = {\n            \'t1\': dict(\n                data=self.getRandomData(shape),\n                affine=affine,\n                type=INTENSITY,\n            ),\n            \'t2\': dict(\n                data=self.getRandomData(shape if consistent else shape2),\n                affine=affine,\n                type=INTENSITY,\n            ),\n            \'label\': dict(\n                data=(self.getRandomData(shape) > 0.5).float(),\n                affine=affine,\n                type=LABEL,\n            ),\n        }\n        return sample\n\n    @staticmethod\n    def getRandomData(shape):\n        return torch.rand(*shape)\n\n    def test_to_tuple(self):\n        assert to_tuple(1) == (1,)\n        assert to_tuple((1,)) == (1,)\n        assert to_tuple(1, length=3) == (1, 1, 1)\n        assert to_tuple((1, 2)) == (1, 2)\n        assert to_tuple((1, 2), length=3) == (1, 2)\n        assert to_tuple([1, 2], length=3) == (1, 2)\n\n    def test_get_stem(self):\n        assert get_stem(\'/home/image.nii.gz\') == \'image\'\n        assert get_stem(\'/home/image.nii\') == \'image\'\n        assert get_stem(\'/home/image.nrrd\') == \'image\'\n\n    def test_guess_type(self):\n        assert guess_type(\'None\') is None\n        assert isinstance(guess_type(\'1\'), int)\n        assert isinstance(guess_type(\'1.5\'), float)\n        assert isinstance(guess_type(\'(1, 3, 5)\'), tuple)\n        assert isinstance(guess_type(\'(1,3,5)\'), tuple)\n        assert isinstance(guess_type(\'[1,3,5]\'), list)\n        assert isinstance(guess_type(\'test\'), str)\n\n    def test_check_consistent_shape(self):\n        good_sample = self.sample\n        bad_sample = self.get_inconsistent_sample()\n        good_sample.check_consistent_shape()\n        with self.assertRaises(ValueError):\n            bad_sample.check_consistent_shape()\n'"
tests/utils.py,0,"b'import copy\nimport shutil\nimport random\nimport tempfile\nimport unittest\nfrom pathlib import Path\nimport numpy as np\nimport nibabel as nib\nfrom torchio.datasets import IXITiny\nfrom torchio import INTENSITY, LABEL, DATA, Image, ImagesDataset, Subject\n\n\nclass TorchioTestCase(unittest.TestCase):\n\n    def setUp(self):\n        """"""Set up test fixtures, if any.""""""\n        self.dir = Path(tempfile.gettempdir()) / \'.torchio_tests\'\n        self.dir.mkdir(exist_ok=True)\n        random.seed(42)\n        np.random.seed(42)\n\n        registration_matrix = np.array([\n            [1, 0, 0, 10],\n            [0, 1, 0, 0],\n            [0, 0, 1.2, 0],\n            [0, 0, 0, 1]\n        ])\n\n        subject_a = Subject(\n            t1=Image(self.get_image_path(\'t1_a\'), INTENSITY),\n        )\n        subject_b = Subject(\n            t1=Image(self.get_image_path(\'t1_b\'), INTENSITY),\n            label=Image(self.get_image_path(\'label_b\', binary=True), LABEL),\n        )\n        subject_c = Subject(\n            label=Image(self.get_image_path(\'label_c\', binary=True), LABEL),\n        )\n        subject_d = Subject(\n            t1=Image(\n                self.get_image_path(\'t1_d\'),\n                INTENSITY,\n                pre_affine=registration_matrix,\n            ),\n            t2=Image(self.get_image_path(\'t2_d\'), INTENSITY),\n            label=Image(self.get_image_path(\'label_d\', binary=True), LABEL),\n        )\n        self.subjects_list = [\n            subject_a,\n            subject_b,\n            subject_c,\n            subject_d,\n        ]\n        self.dataset = ImagesDataset(self.subjects_list)\n        self.sample = self.dataset[-1]\n\n    def make_2d(self, sample):\n        sample = copy.deepcopy(sample)\n        for image in sample.get_images(intensity_only=False):\n            image[DATA] = image[DATA][:, 0:1, ...]\n        return sample\n\n    def get_inconsistent_sample(self):\n        """"""Return a sample containing images of different shape.""""""\n        subject = Subject(\n            t1=Image(self.get_image_path(\'t1_d\'), INTENSITY),\n            t2=Image(\n                self.get_image_path(\'t2_d\', shape=(10, 20, 31)), INTENSITY),\n            label=Image(\n                self.get_image_path(\n                    \'label_d\',\n                    shape=(8, 17, 25),\n                    binary=True,\n                ),\n                LABEL,\n            ),\n        )\n        subjects_list = [subject]\n        dataset = ImagesDataset(subjects_list)\n        return dataset[0]\n\n    def get_reference_image_and_path(self):\n        """"""Return a reference image and its path""""""\n        path = self.get_image_path(\'ref\', shape=(10, 20, 31))\n        image = Image(path, INTENSITY)\n        return image, path\n\n    def tearDown(self):\n        """"""Tear down test fixtures, if any.""""""\n        print(\'Deleting\', self.dir)\n        shutil.rmtree(self.dir)\n\n    def get_ixi_tiny(self):\n        root_dir = Path(tempfile.gettempdir()) / \'torchio\' / \'ixi_tiny\'\n        return IXITiny(root_dir, download=True)\n\n    def get_image_path(\n            self,\n            stem,\n            binary=False,\n            shape=(10, 20, 30),\n            spacing=(1, 1, 1),\n            ):\n        data = np.random.rand(*shape)\n        if binary:\n            data = (data > 0.5).astype(np.uint8)\n        affine = np.diag((*spacing, 1))\n        suffix = random.choice((\'.nii.gz\', \'.nii\'))\n        path = self.dir / f\'{stem}{suffix}\'\n        nib.Nifti1Image(data, affine).to_filename(str(path))\n        if np.random.rand() > 0.5:\n            path = str(path)\n        return path\n'"
torchio/__init__.py,0,"b'""""""Top-level package for torchio.""""""\n\n__author__ = """"""Fernando Perez-Garcia""""""\n__email__ = \'fernando.perezgarcia.17@ucl.ac.uk\'\n__version__ = \'0.16.22\'\n\nfrom . import utils\nfrom .torchio import *\nfrom .transforms import *\nfrom .data import io, sampler, inference, ImagesDataset, Image, Queue, Subject\nfrom . import datasets\nfrom . import reference\n\nprint(\'If you use TorchIO for your research, please cite the following paper:\')\nprint(\'P\xc3\xa9rez-Garc\xc3\xada et al., TorchIO: a Python library for efficient loading,\')\nprint(\'preprocessing, augmentation and patch-based sampling of medical images\')\nprint(\'in deep learning. Link: https://arxiv.org/abs/2003.04696\\n\')\n'"
torchio/cli.py,0,"b'# pylint: disable=import-outside-toplevel\n\n""""""Console script for torchio.""""""\nimport sys\nimport click\n\n\n@click.command()\n@click.argument(\'input-path\', type=click.Path(exists=True))\n@click.argument(\'transform-name\', type=str)\n@click.argument(\'output-path\', type=click.Path())\n@click.option(\n    \'--kwargs\', \'-k\',\n    type=str,\n    help=\'String of kwargs, e.g. ""degrees=(-5,15) num_transforms=3"".\',\n)\n@click.option(\n    \'--seed\', \'-s\',\n    type=int,\n    help=\'Seed for PyTorch random number generator.\',\n)\ndef apply_transform(\n        input_path,\n        transform_name,\n        output_path,\n        kwargs,\n        seed,\n        ):\n    """"""Apply transform to an image.\n\n    \\b\n    Example:\n    $ torchio-transform -k ""degrees=(-5,15) num_transforms=3"" input.nrrd RandomMotion output.nii\n    """"""\n    # Imports are placed here so that the tool loads faster if not being run\n    import torchio.transforms as transforms\n    from torchio.transforms.augmentation import RandomTransform\n    from torchio.utils import apply_transform_to_file\n\n    try:\n        transform_class = getattr(transforms, transform_name)\n    except AttributeError as error:\n        message = f\'Transform ""{transform_name}"" not found in torchio\'\n        raise ValueError(message) from error\n\n    params_dict = get_params_dict_from_kwargs(kwargs)\n    if issubclass(transform_class, RandomTransform):\n        params_dict[\'seed\'] = seed\n    transform = transform_class(**params_dict)\n    apply_transform_to_file(\n        input_path,\n        transform,\n        output_path,\n    )\n    return 0\n\n\ndef get_params_dict_from_kwargs(kwargs):\n    from torchio.utils import guess_type\n    params_dict = {}\n    if kwargs is not None:\n        for substring in kwargs.split():\n            try:\n                key, value_string = substring.split(\'=\')\n            except ValueError as error:\n                message = f\'Arguments string ""{kwargs}"" not valid\'\n                raise ValueError(message) from error\n\n            value = guess_type(value_string)\n            params_dict[key] = value\n    return params_dict\n\n\nif __name__ == ""__main__"":\n    # pylint: disable=no-value-for-parameter\n    sys.exit(apply_transform())  # pragma: no cover\n'"
torchio/reference.py,0,"b'# Use duecredit to provide a citation to relevant work to\n# be cited. This does nothing unless the user has duecredit installed\n# and calls this with duecredit (as in `python -m duecredit script.py`):\nfrom .external.due import due, Doi, BibTeX\n\nBIBTEX = r""""""@misc{fern2020torchio,\n   title={TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning},\n   author={Fernando P\xc3\xa9rez-Garc\xc3\xada and Rachel Sparks and Sebastien Ourselin},\n   year={2020},\n   eprint={2003.04696},\n   archivePrefix={arXiv},\n   primaryClass={eess.IV}\n} """"""\n\nTITLE = (\n    \'TorchIO: a Python library for efficient loading, preprocessing,\'\n    \' augmentation and patch-based sampling of medical images in deep learning\'\n)\n\nDESCRIPTION = (\n    \'Tools for loading, augmenting and writing 3D medical images\'\n    \' on PyTorch\'\n)\n\ndue.cite(\n    BibTeX(BIBTEX),\n    description=TITLE,\n    path=\'torchio\',\n    cite_module=True,\n)\n\ndue.cite(\n    Doi(\'10.5281/zenodo.3739230\'),\n    description=DESCRIPTION,\n    path=\'torchio\',\n    tags=[\'implementation\'],\n    cite_module=True,\n)\n'"
torchio/torchio.py,2,"b'""""""Main module.""""""\n\nfrom pathlib import Path\nfrom typing import Union, Tuple, Callable\nimport torch\nimport numpy as np\n\n\n# Image types\nINTENSITY = \'intensity\'\nLABEL = \'label\'\nSAMPLING_MAP = \'sampling_map\'\n\n# Keys for dataset samples\nPATH = \'path\'\nTYPE = \'type\'\nSTEM = \'stem\'\nDATA = \'data\'\nAFFINE = \'affine\'\n\n# For aggregator\nIMAGE = \'image\'\nLOCATION = \'location\'\n\n# In PyTorch convention\nCHANNELS_DIMENSION = 1\n\n# For typing hints\nTypePath = Union[Path, str]\nTypeNumber = Union[int, float]\nTypeData = Union[torch.Tensor, np.ndarray]\nTypeTripletInt = Tuple[int, int, int]\nTypeTripletFloat = Tuple[float, float, float]\nTypeTuple = Union[int, TypeTripletInt]\nTypeRangeInt = Union[int, Tuple[int, int]]\nTypePatchSize = Union[int, Tuple[int, int, int]]\nTypeRangeFloat = Union[float, Tuple[float, float]]\nTypeCallable = Callable[[torch.Tensor], torch.Tensor]\n'"
torchio/utils.py,2,"b'import ast\nimport shutil\nimport tempfile\nfrom pathlib import Path\nfrom typing import Union, Iterable, Tuple, Any, Optional, List\n\nimport torch\nimport numpy as np\nimport nibabel as nib\nimport SimpleITK as sitk\nfrom tqdm import trange\nfrom .torchio import (\n    INTENSITY,\n    LABEL,\n    TypeData,\n    TypeNumber,\n    TypePath,\n)\n\n\nFLIP_XY = np.diag((-1, -1, 1))  # used to switch between LPS and RAS\n\n\ndef to_tuple(\n        value: Union[TypeNumber, Iterable[TypeNumber]],\n        length: int = 1,\n        ) -> Tuple[TypeNumber, ...]:\n    """"""\n    to_tuple(1, length=1) -> (1,)\n    to_tuple(1, length=3) -> (1, 1, 1)\n\n    If value is an iterable, n is ignored and tuple(value) is returned\n    to_tuple((1,), length=1) -> (1,)\n    to_tuple((1, 2), length=1) -> (1, 2)\n    to_tuple([1, 2], length=3) -> (1, 2)\n    """"""\n    try:\n        iter(value)\n        value = tuple(value)\n    except TypeError:\n        value = length * (value,)\n    return value\n\n\ndef get_stem(path: TypePath) -> str:\n    """"""\n    \'/home/user/image.nii.gz\' -> \'image\'\n    """"""\n    path = Path(path)\n    return path.name.split(\'.\')[0]\n\n\ndef create_dummy_dataset(\n        num_images: int,\n        size_range: Tuple[int, int],\n        directory: Optional[TypePath] = None,\n        suffix: str = \'.nii.gz\',\n        force: bool = False,\n        verbose: bool = False,\n        ):\n    from .data import Image, Subject\n    output_dir = tempfile.gettempdir() if directory is None else directory\n    output_dir = Path(output_dir)\n    images_dir = output_dir / \'dummy_images\'\n    labels_dir = output_dir / \'dummy_labels\'\n\n    if force:\n        shutil.rmtree(images_dir)\n        shutil.rmtree(labels_dir)\n\n    subjects: List[Subject] = []\n    if images_dir.is_dir():\n        for i in trange(num_images):\n            image_path = images_dir / f\'image_{i}{suffix}\'\n            label_path = labels_dir / f\'label_{i}{suffix}\'\n            subject = Subject(\n                one_modality=Image(image_path, INTENSITY),\n                segmentation=Image(label_path, LABEL),\n            )\n            subjects.append(subject)\n    else:\n        images_dir.mkdir(exist_ok=True, parents=True)\n        labels_dir.mkdir(exist_ok=True, parents=True)\n        if verbose:\n            print(\'Creating dummy dataset...\')\n            iterable = trange(num_images)\n        else:\n            iterable = range(num_images)\n        for i in iterable:\n            shape = np.random.randint(*size_range, size=3)\n            affine = np.eye(4)\n            image = np.random.rand(*shape)\n            label = np.ones_like(image)\n            label[image < 0.33] = 0\n            label[image > 0.66] = 2\n            image *= 255\n\n            image_path = images_dir / f\'image_{i}{suffix}\'\n            nii = nib.Nifti1Image(image.astype(np.uint8), affine)\n            nii.to_filename(str(image_path))\n\n            label_path = labels_dir / f\'label_{i}{suffix}\'\n            nii = nib.Nifti1Image(label.astype(np.uint8), affine)\n            nii.to_filename(str(label_path))\n\n            subject = Subject(\n                one_modality=Image(image_path, INTENSITY),\n                segmentation=Image(label_path, LABEL),\n            )\n            subjects.append(subject)\n    return subjects\n\n\ndef apply_transform_to_file(\n        input_path: TypePath,\n        transform,  # : Transform seems to create a circular import (TODO)\n        output_path: TypePath,\n        type: str = INTENSITY,\n        ):\n    from . import Image, ImagesDataset, Subject\n    subject = Subject(image=Image(input_path, type))\n    dataset = ImagesDataset([subject], transform=transform)\n    transformed = dataset[0]\n    dataset.save_sample(transformed, dict(image=output_path))\n\n\ndef guess_type(string: str) -> Any:\n    # Adapted from\n    # https://www.reddit.com/r/learnpython/comments/4599hl/module_to_guess_type_from_a_string/czw3f5s\n    string = string.replace(\' \', \'\')\n    try:\n        value = ast.literal_eval(string)\n    except ValueError:\n        result_type = str\n    else:\n        result_type = type(value)\n    if result_type in (list, tuple):\n        string = string[1:-1]  # remove brackets\n        split = string.split(\',\')\n        list_result = [guess_type(n) for n in split]\n        value = tuple(list_result) if result_type is tuple else list_result\n        return value\n    try:\n        value = result_type(string)\n    except TypeError:\n        value = None\n    return value\n\n\ndef get_rotation_and_spacing_from_affine(\n        affine: np.ndarray,\n        ) -> Tuple[np.ndarray, np.ndarray]:\n    # From https://github.com/nipy/nibabel/blob/master/nibabel/orientations.py\n    rotation_zoom = affine[:3, :3]\n    spacing = np.sqrt(np.sum(rotation_zoom * rotation_zoom, axis=0))\n    rotation = rotation_zoom / spacing\n    return rotation, spacing\n\n\ndef nib_to_sitk(data: TypeData, affine: TypeData) -> sitk.Image:\n    array = data.numpy() if isinstance(data, torch.Tensor) else data\n    affine = affine.numpy() if isinstance(affine, torch.Tensor) else affine\n    origin = np.dot(FLIP_XY, affine[:3, 3]).astype(np.float64)\n    rotation, spacing = get_rotation_and_spacing_from_affine(affine)\n    direction = np.dot(FLIP_XY, rotation)\n    image = sitk.GetImageFromArray(array.transpose())\n    if array.ndim == 2:  # ignore first dimension if 2D (1, 1, H, W)\n        direction = direction[1:3, 1:3]\n    image.SetOrigin(origin)\n    image.SetSpacing(spacing)\n    image.SetDirection(direction.flatten())\n    return image\n\n\ndef sitk_to_nib(image: sitk.Image) -> Tuple[np.ndarray, np.ndarray]:\n    data = sitk.GetArrayFromImage(image).transpose()\n    spacing = np.array(image.GetSpacing())\n    direction = np.array(image.GetDirection())\n    origin = image.GetOrigin()\n    if len(direction) == 9:\n        rotation = direction.reshape(3, 3)\n    elif len(direction) == 4:  # ignore first dimension if 2D (1, 1, H, W)\n        rotation_2d = direction.reshape(2, 2)\n        rotation = np.eye(3)\n        rotation[1:3, 1:3] = rotation_2d\n        spacing = 1, *spacing\n        origin = 0, *origin\n    rotation = np.dot(FLIP_XY, rotation)\n    rotation_zoom = rotation * spacing\n    translation = np.dot(FLIP_XY, origin)\n    affine = np.eye(4)\n    affine[:3, :3] = rotation_zoom\n    affine[:3, 3] = translation\n    return data, affine\n\n\ndef get_torchio_cache_dir():\n    return Path(\'~/.cache/torchio\').expanduser()\n\n\ndef round_up(value: float) -> float:\n    """"""Round half towards infinity.\n\n    Args:\n        value: The value to round.\n\n    Example:\n\n        >>> round(2.5)\n        2\n        >>> round(3.5)\n        4\n        >>> round_up(2.5)\n        3\n        >>> round_up(3.5)\n        4\n\n    """"""\n    return np.floor(value + 0.5)\n'"
docs/source/conf.py,2,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\nfrom typing import List\n\n# -- Project information -----------------------------------------------------\n\nproject = \'TorchIO\'\ncopyright = \'2020, Fernando P\xc3\xa9rez-Garc\xc3\xada\'\nauthor = \'Fernando P\xc3\xa9rez-Garc\xc3\xada\'\n\n# version is the short X.Y version\n# release is the full version, including alpha/beta/rc tags\nversion = release = \'0.16.22\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx_rtd_theme\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.intersphinx\',\n]\n\n# Add mappings\n# https://kevin.burke.dev/kevin/sphinx-interlinks/\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3\', None),\n    \'torch\': (\'https://pytorch.org/docs/master/\', None),  # https://github.com/pytorch/fairseq/blob/adb5b9c71f7ef4fe2f258e0da102d819ab9920ef/docs/conf.py#L131\n    \'torchvision\': (\'https://pytorch.org/docs/master/\', None),\n    \'nibabel\': (\'https://nipy.org/nibabel/\', None),\n}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns: List[str] = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'navigation_depth\': 3,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'TorchIOdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'TorchIO.tex\', \'TorchIO Documentation\',\n     \'Fernando P\xc3\xa9rez-Garc\xc3\xada\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'torchio\', \'TorchIO Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'TorchIO\', \'TorchIO Documentation\',\n     author, \'TorchIO\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\ndef setup(app):\n    app.add_js_file(""copybutton.js"")\n\n\n# -- Extension configuration -------------------------------------------------\n'"
tests/data/__init__.py,0,b''
tests/data/test_image.py,2,"b'#!/usr/bin/env python\n\n""""""Tests for Image.""""""\n\nimport torch\nfrom torchio import INTENSITY, Image\nfrom ..utils import TorchioTestCase\nfrom torchio import RandomFlip, RandomAffine\n\n\nclass TestImage(TorchioTestCase):\n    """"""Tests for `Image`.""""""\n\n    def test_image_not_found(self):\n        with self.assertRaises(FileNotFoundError):\n            Image(\'nopath\', INTENSITY)\n\n    def test_wrong_path_type(self):\n        with self.assertRaises(TypeError):\n            Image(5, INTENSITY)\n\n    def test_incompatible_arguments(self):\n        with self.assertRaises(ValueError):\n            Image(5, INTENSITY, affine=1)\n\n    def test_tensor_flip(self):\n        sample_input = torch.ones((4, 30, 30, 30))\n        RandomFlip()(sample_input)\n\n    def test_tensor_affine(self):\n        sample_input = torch.ones((4, 10, 10, 10))\n        RandomAffine()(sample_input)'"
tests/data/test_images_dataset.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for ImagesDataset.""""""\n\nimport nibabel as nib\nfrom torchio import DATA, ImagesDataset\nfrom ..utils import TorchioTestCase\n\n\nclass TestImagesDataset(TorchioTestCase):\n    """"""Tests for `ImagesDataset`.""""""\n\n    def test_images(self):\n        self.iterate_dataset(self.subjects_list)\n\n    def test_empty_subjects_list(self):\n        with self.assertRaises(ValueError):\n            self.iterate_dataset([])\n\n    def test_empty_subjects_tuple(self):\n        with self.assertRaises(ValueError):\n            self.iterate_dataset(())\n\n    def test_wrong_subjects_type(self):\n        with self.assertRaises(TypeError):\n            self.iterate_dataset(0)\n\n    def test_wrong_subject_type_int(self):\n        with self.assertRaises(TypeError):\n            self.iterate_dataset([0])\n\n    def test_wrong_subject_type_dict(self):\n        with self.assertRaises(TypeError):\n            self.iterate_dataset([{}])\n\n    def test_wrong_index(self):\n        with self.assertRaises(ValueError):\n            self.dataset[:3]\n\n    def test_save_sample(self):\n        dataset = ImagesDataset(\n            self.subjects_list, transform=lambda x: x)\n        _ = len(dataset)  # for coverage\n        sample = dataset[0]\n        output_path = self.dir / \'test.nii.gz\'\n        paths_dict = {\'t1\': output_path}\n        dataset.save_sample(sample, paths_dict)\n        nii = nib.load(str(output_path))\n        ndims_output = len(nii.shape)\n        ndims_sample = len(sample[\'t1\'].shape)\n        assert ndims_sample == ndims_output + 1\n\n    def test_no_load(self):\n        dataset = ImagesDataset(\n            self.subjects_list, load_image_data=False)\n        for _ in dataset:\n            pass\n\n    def test_no_load_transform(self):\n        with self.assertRaises(ValueError):\n            ImagesDataset(\n                self.subjects_list,\n                load_image_data=False,\n                transform=lambda x: x,\n            )\n\n    def test_wrong_transform_init(self):\n        with self.assertRaises(ValueError):\n            ImagesDataset(\n                self.subjects_list,\n                transform=dict(),\n            )\n\n    def test_wrong_transform_arg(self):\n        with self.assertRaises(ValueError):\n            self.dataset.set_transform(1)\n\n    @staticmethod\n    def iterate_dataset(subjects_list):\n        dataset = ImagesDataset(subjects_list)\n        for _ in dataset:\n            pass\n'"
tests/data/test_io.py,0,"b'import tempfile\nimport unittest\nfrom pathlib import Path\nimport SimpleITK as sitk\nfrom ..utils import TorchioTestCase\nfrom torchio.data import io\n\n\nclass TestIO(TorchioTestCase):\n    """"""Tests for `io` module.""""""\n    def setUp(self):\n        super().setUp()\n        self.write_dicom()\n\n    def write_dicom(self):\n        self.dicom_dir = self.dir / \'dicom\'\n        self.dicom_dir.mkdir()\n        self.dicom_path = self.dicom_dir / \'dicom.dcm\'\n        self.nii_path = self.get_image_path(\'read_image\')\n        writer = sitk.ImageFileWriter()\n        writer.SetFileName(str(self.dicom_path))\n        image = sitk.ReadImage(str(self.nii_path))\n        image = sitk.Cast(image, sitk.sitkUInt16)\n        image = image[0]  # dicom reader supports 2D only\n        writer.Execute(image)\n\n    def test_read_image(self):\n        # I need to find something readable by nib but not sitk\n        io.read_image(self.nii_path)\n        io.read_image(self.nii_path, itk_first=True)\n\n    def test_read_dicom_file(self):\n        io.read_image(self.dicom_path)\n\n    def test_read_dicom_dir(self):\n        io.read_image(self.dicom_dir)\n\n    def test_dicom_dir_missing(self):\n        with self.assertRaises(FileNotFoundError):\n            io._read_dicom(\'missing\')\n\n    def test_dicom_dir_no_files(self):\n        empty = self.dir / \'empty\'\n        empty.mkdir()\n        with self.assertRaises(FileNotFoundError):\n            io._read_dicom(empty)\n'"
tests/data/test_queue.py,1,"b'from torch.utils.data import DataLoader\nfrom torchio.data import UniformSampler\nfrom torchio import ImagesDataset, Queue, DATA\nfrom torchio.utils import create_dummy_dataset\nfrom ..utils import TorchioTestCase\n\n\nclass TestQueue(TorchioTestCase):\n    """"""Tests for `queue` module.""""""\n    def setUp(self):\n        super().setUp()\n        self.subjects_list = create_dummy_dataset(\n            num_images=10,\n            size_range=(10, 20),\n            directory=self.dir,\n            suffix=\'.nii\',\n            force=False,\n        )\n\n    def test_queue(self):\n        subjects_dataset = ImagesDataset(self.subjects_list)\n        patch_size = 10\n        sampler = UniformSampler(patch_size)\n        queue_dataset = Queue(\n            subjects_dataset,\n            max_length=6,\n            samples_per_volume=2,\n            sampler=sampler,\n            num_workers=0,\n            verbose=True,\n        )\n        _ = str(queue_dataset)\n        batch_loader = DataLoader(queue_dataset, batch_size=4)\n        for batch in batch_loader:\n            _ = batch[\'one_modality\'][DATA]\n            _ = batch[\'segmentation\'][DATA]\n'"
tests/data/test_subject.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for Subject.""""""\n\nimport tempfile\nfrom torchio import INTENSITY, Subject, Image, RandomFlip\nfrom ..utils import TorchioTestCase\n\n\nclass TestSubject(TorchioTestCase):\n    """"""Tests for `Subject`.""""""\n    def test_positional_args(self):\n        with self.assertRaises(ValueError):\n            with tempfile.NamedTemporaryFile() as f:\n                Subject(Image(f.name, INTENSITY))\n\n    def test_input_dict(self):\n        with tempfile.NamedTemporaryFile() as f:\n            input_dict = {\'image\': Image(f.name, INTENSITY)}\n            Subject(input_dict)\n            Subject(**input_dict)\n\n    def test_no_sample(self):\n        with tempfile.NamedTemporaryFile() as f:\n            input_dict = {\'image\': Image(f.name, INTENSITY)}\n            subject = Subject(input_dict)\n            with self.assertRaises(RuntimeError):\n                RandomFlip()(subject)\n'"
tests/datasets/__init__.py,0,b''
tests/datasets/test_ixi.py,0,"b'from torchio.datasets import IXI, IXITiny\nfrom ..utils import TorchioTestCase\n\n\nclass TestIXI(TorchioTestCase):\n    """"""Tests for `ixi` module.""""""\n\n    def test_ixi(self):\n        self.get_ixi_tiny()\n\n    def test_not_downloaded(self):\n        with self.assertRaises(RuntimeError):\n            dataset = IXI(\'testing123\', download=False)\n\n    def test_tiny_not_downloaded(self):\n        with self.assertRaises(RuntimeError):\n            dataset = IXITiny(\'testing123\', download=False)\n'"
tests/transforms/__init__.py,0,b''
tests/transforms/test_collate.py,1,"b'from torch.utils.data import DataLoader\nfrom torchio.transforms import RandomElasticDeformation\nfrom ..utils import TorchioTestCase\n\n\nclass TestCollate(TorchioTestCase):\n    def test_collate(self):\n        # Keys missing in one of the samples will not be present in the batch\n        # This is relevant for the case in which a transform is applied to some\n        # samples only, according to its probability (p argument)\n        transform_no = RandomElasticDeformation(p=0, max_displacement=1)\n        transform_yes = RandomElasticDeformation(p=1, max_displacement=1)\n        sample_no = transform_no(self.sample)\n        sample_yes = transform_yes(self.sample)\n        data = sample_no, sample_yes\n        class Dataset:\n            def __init__(self, data):\n                self.data = data\n            def __len__(self):\n                return len(self.data)\n            def __getitem__(self, index):\n                return self.data[index]\n        loader = DataLoader(Dataset(data), batch_size=2)\n        batch = next(iter(loader))\n'"
tests/transforms/test_lambda_transform.py,8,"b'import torch\nfrom torchio import DATA, LABEL\nfrom torchio.transforms import Lambda\nfrom ..utils import TorchioTestCase\n\n\nclass TestLambda(TorchioTestCase):\n    """"""Tests for :py:class:`Lambda` class.""""""\n\n    def test_wrong_return_type(self):\n        transform = Lambda(lambda x: \'Not a tensor\')\n        with self.assertRaises(ValueError):\n            transform(self.sample)\n\n    def test_wrong_return_data_type(self):\n        transform = Lambda(lambda x: torch.rand(1) > 0)\n        with self.assertRaises(ValueError):\n            transform(self.sample)\n\n    def test_wrong_return_shape(self):\n        transform = Lambda(lambda x: torch.rand(1))\n        with self.assertRaises(ValueError):\n            transform(self.sample)\n\n    def test_lambda(self):\n        transform = Lambda(lambda x: x + 1)\n        transformed = transform(self.sample)\n        assert torch.all(torch.eq(\n            transformed[\'t1\'][DATA], self.sample[\'t1\'][DATA] + 1))\n        assert torch.all(torch.eq(\n            transformed[\'t2\'][DATA], self.sample[\'t2\'][DATA] + 1))\n        assert torch.all(torch.eq(\n            transformed[\'label\'][DATA], self.sample[\'label\'][DATA] + 1))\n\n    def test_image_types(self):\n        transform = Lambda(lambda x: x + 1, types_to_apply=[LABEL])\n        transformed = transform(self.sample)\n        assert torch.all(torch.eq(\n            transformed[\'t1\'][DATA], self.sample[\'t1\'][DATA]))\n        assert torch.all(torch.eq(\n            transformed[\'t2\'][DATA], self.sample[\'t2\'][DATA]))\n        assert torch.all(torch.eq(\n            transformed[\'label\'][DATA], self.sample[\'label\'][DATA] + 1))\n'"
tests/transforms/test_transforms.py,1,"b'#!/usr/bin/env python\n\nimport torch\nimport numpy as np\nimport torchio\nfrom ..utils import TorchioTestCase\n\n\nclass TestTransforms(TorchioTestCase):\n    """"""Tests for all transforms.""""""\n\n    def get_transform(self, channels, is_3d=True):\n        landmarks_dict = {\n            channel: np.linspace(0, 100, 13) for channel in channels\n        }\n        disp = 1 if is_3d else (0.01, 1, 1)\n        elastic = torchio.RandomElasticDeformation(max_displacement=disp)\n        cp_args = (9, 21, 30) if is_3d else (1, 21, 30)\n        flip_axes = (0, 1, 2) if is_3d else (0, 1)\n        swap_patch = (2, 3, 4) if is_3d else (1, 3, 4)\n        pad_args = (1, 2, 3, 0, 5, 6) if is_3d else (0, 0, 3, 0, 5, 6)\n        crop_args = (3, 2, 8, 0, 1, 4) if is_3d else (0, 0, 8, 0, 1, 4)\n        transforms = (\n            torchio.CropOrPad(cp_args),\n            torchio.ToCanonical(),\n            torchio.Resample((1, 1.1, 1.25)),\n            torchio.RandomFlip(axes=flip_axes, flip_probability=1),\n            torchio.RandomMotion(),\n            torchio.RandomGhosting(axes=(0, 1, 2)),\n            torchio.RandomSpike(),\n            torchio.RandomNoise(),\n            torchio.RandomBlur(),\n            torchio.RandomSwap(patch_size=swap_patch, num_iterations=5),\n            torchio.Lambda(lambda x: 2 * x, types_to_apply=torchio.INTENSITY),\n            torchio.RandomBiasField(),\n            torchio.RescaleIntensity((0, 1)),\n            torchio.ZNormalization(),\n            torchio.HistogramStandardization(landmarks_dict),\n            elastic,\n            torchio.RandomAffine(),\n            torchio.OneOf({\n                torchio.RandomAffine(): 3,\n                elastic: 1,\n            }),\n            torchio.Pad(pad_args, padding_mode=3),\n            torchio.Crop(crop_args),\n        )\n        return torchio.Compose(transforms)\n\n    def test_transforms_tensor(self):\n        tensor = torch.rand(2, 4, 5, 8)\n        transform = self.get_transform(channels=(\'channel_0\', \'channel_1\'))\n        transform(tensor)\n\n    def test_transforms_sample_3d(self):\n        transform = self.get_transform(channels=(\'t1\', \'t2\'), is_3d=True)\n        transform(self.sample)\n\n    def test_transforms_sample_2d(self):\n        transform = self.get_transform(channels=(\'t1\', \'t2\'), is_3d=False)\n        sample = self.make_2d(self.sample)\n        transform(sample)\n'"
torchio/data/__init__.py,0,"b'from .queue import Queue\nfrom .image import Image\nfrom .subject import Subject\nfrom .dataset import ImagesDataset\nfrom .inference import GridSampler, GridAggregator\nfrom .sampler import PatchSampler, LabelSampler, WeightedSampler, UniformSampler\n'"
torchio/data/dataset.py,4,"b'import copy\nimport collections\nfrom typing import (\n    Dict,\n    Sequence,\n    Optional,\n    Callable,\n)\nfrom torch.utils.data import Dataset\nfrom ..utils import get_stem\nfrom ..torchio import DATA, AFFINE, TYPE, PATH, STEM, TypePath\nfrom .image import Image\nfrom .io import write_image\nfrom .subject import Subject\n\n\nclass ImagesDataset(Dataset):\n    """"""Base TorchIO dataset.\n\n    :py:class:`~torchio.data.dataset.ImagesDataset`\n    is a reader of 3D medical images that directly\n    inherits from :class:`torch.utils.data.Dataset`.\n    It can be used with a :class:`torch.utils.data.DataLoader`\n    for efficient loading and augmentation.\n    It receives a list of subjects, where each subject is an instance of\n    :py:class:`~torchio.data.subject.Subject` containing instances of\n    :py:class:`~torchio.data.image.Image`.\n    The file format must be compatible with `NiBabel`_ or `SimpleITK`_ readers.\n    It can also be a directory containing\n    `DICOM`_ files.\n\n    Indexing an :py:class:`~torchio.data.dataset.ImagesDataset` returns a\n    Python dictionary with the data corresponding to the queried subject.\n    The keys in the dictionary are the names of the images passed to that\n    subject, for example ``(\'t1\', \'t2\', \'segmentation\')``.\n\n    The value corresponding to each image name is typically an instance of\n    :py:class:`~torchio.data.image.Image` with information about the image.\n    The data is stored in ``image[torchio.DATA]`` (or just ``image.data``),\n    and the corresponding `affine matrix`_\n    is in ``image[torchio.AFFINE]`` (or just ``image.affine``):\n\n        >>> sample = images_dataset[0]\n        >>> sample.keys()\n        dict_keys([\'image\', \'label\'])\n        >>> image = sample[\'image\']  # or sample.image\n        >>> image.shape\n        torch.Size([1, 176, 256, 256])\n        >>> image.affine\n        array([[   0.03,    1.13,   -0.08,  -88.54],\n               [   0.06,    0.08,    0.95, -129.66],\n               [   1.18,   -0.06,   -0.11,  -67.15],\n               [   0.  ,    0.  ,    0.  ,    1.  ]])\n\n    Args:\n        subjects: Sequence of instances of\n            :class:`~torchio.data.subject.Subject`.\n        transform: An instance of :py:class:`torchio.transforms.Transform`\n            that will be applied to each sample.\n        check_nans: If ``True``, issues a warning if NaNs are found\n            in the image.\n        load_image_data: If ``False``, image data and affine will not be loaded.\n            These fields will be set to ``None`` in the sample. This can be\n            used to quickly iterate over the samples to retrieve e.g. the\n            images paths. If ``True``, transform must be ``None``.\n\n    Example:\n        >>> import torchio\n        >>> from torchio import ImagesDataset, Image, Subject\n        >>> from torchio.transforms import RescaleIntensity, RandomAffine, Compose\n        >>> subject_a = Subject([\n        ...     t1=Image(\'~/Dropbox/MRI/t1.nrrd\', torchio.INTENSITY),\n        ...     t2=Image(\'~/Dropbox/MRI/t2.mha\', torchio.INTENSITY),\n        ...     label=Image(\'~/Dropbox/MRI/t1_seg.nii.gz\', torchio.LABEL),\n        ...     age=31,\n        ...     name=\'Fernando Perez\',\n        >>> ])\n        >>> subject_b = Subject(\n        ...     t1=Image(\'/tmp/colin27_t1_tal_lin.nii.gz\', torchio.INTENSITY),\n        ...     t2=Image(\'/tmp/colin27_t2_tal_lin.nii\', torchio.INTENSITY),\n        ...     label=Image(\'/tmp/colin27_seg1.nii.gz\', torchio.LABEL),\n        ...     age=56,\n        ...     name=\'Colin Holmes\',\n        ... )\n        >>> subjects_list = [subject_a, subject_b]\n        >>> transforms = [\n        ...     RescaleIntensity((0, 1)),\n        ...     RandomAffine(),\n        ... ]\n        >>> transform = Compose(transforms)\n        >>> subjects_dataset = ImagesDataset(subjects_list, transform=transform)\n        >>> subject_sample = subjects_dataset[0]\n\n    .. _NiBabel: https://nipy.org/nibabel/#nibabel\n    .. _SimpleITK: https://itk.org/Wiki/ITK/FAQ#What_3D_file_formats_can_ITK_import_and_export.3F\n    .. _DICOM: https://www.dicomstandard.org/\n    .. _affine matrix: https://nipy.org/nibabel/coordinate_systems.html\n\n    """"""\n\n    def __init__(\n            self,\n            subjects: Sequence[Subject],\n            transform: Optional[Callable] = None,\n            check_nans: bool = True,\n            load_image_data: bool = True,\n            ):\n        self._parse_subjects_list(subjects)\n        self.subjects = subjects\n        self._transform: Optional[Callable]\n        self.set_transform(transform)\n        self.check_nans = check_nans\n        self._load_image_data: bool\n        self.set_load_image_data(load_image_data)\n\n    def __len__(self):\n        return len(self.subjects)\n\n    def __getitem__(self, index: int) -> dict:\n        if not isinstance(index, int):\n            raise ValueError(f\'Index ""{index}"" must be int, not {type(index)}\')\n        subject = self.subjects[index]\n        sample = self._get_sample_dict_from_subject(subject)\n\n        # Apply transform (this is usually the bottleneck)\n        if self._transform is not None:\n            sample = self._transform(sample)\n        return sample\n\n    def _get_sample_dict_from_subject(self, subject: Subject):\n        """"""Create a dictionary of dictionaries with subject information.\n\n        Args:\n            subject: Instance of :py:class:`~torchio.data.subject.Subject`.\n        """"""\n        subject_sample = copy.deepcopy(subject)\n        for (key, value) in subject.items():\n            if isinstance(value, Image):\n                value = self._get_image_dict_from_image(value)\n            subject_sample[key] = value\n        # This allows me to do e.g. subject.t1\n        subject_sample.__dict__.update(subject_sample)\n        subject_sample.is_sample = True\n        return subject_sample\n\n    def _get_image_dict_from_image(self, image: Image):\n        """"""Create a dictionary with image information.\n\n        Args:\n            image: Instance of :py:class:`~torchio.data.dataset.Image`.\n\n        Return:\n            Dictionary with keys\n            :py:attr:`torchio.DATA`,\n            :py:attr:`torchio.AFFINE`,\n            :py:attr:`torchio.TYPE`,\n            :py:attr:`torchio.PATH` and\n            :py:attr:`torchio.STEM`.\n        """"""\n        if self._load_image_data:\n            tensor, affine = image.load(check_nans=self.check_nans)\n        else:\n            tensor = affine = None\n        path = \'\' if image.path is None else str(image.path)\n        stem = \'\' if image.path is None else get_stem(image.path)\n        image_dict = {\n            DATA: tensor,\n            AFFINE: affine,\n            TYPE: image.type,\n            PATH: path,\n            STEM: stem,\n        }\n        image = copy.deepcopy(image)\n        image.update(image_dict)\n        image.is_sample = True\n        return image\n\n    def set_transform(self, transform: Optional[Callable]) -> None:\n        """"""Set the :attr:`transform` attribute.\n\n        Args:\n            transform: An instance of :py:class:`torchio.transforms.Transform`.\n        """"""\n        if transform is not None and not callable(transform):\n            raise ValueError(\n                f\'The transform must be a callable object, not {transform}\')\n        self._transform = transform\n\n    @staticmethod\n    def _parse_subjects_list(subjects_list: Sequence[Subject]) -> None:\n        # Check that it\'s list or tuple\n        if not isinstance(subjects_list, collections.abc.Sequence):\n            raise TypeError(\n                f\'Subject list must be a sequence, not {type(subjects_list)}\')\n\n        # Check that it\'s not empty\n        if not subjects_list:\n            raise ValueError(\'Subjects list is empty\')\n\n        # Check each element\n        for subject in subjects_list:\n            if not isinstance(subject, Subject):\n                message = (\n                    \'Subjects list must contain instances of torchio.Subject,\'\n                    f\' not ""{type(subject)}""\'\n                )\n                raise TypeError(message)\n\n    @classmethod\n    def save_sample(\n            cls,\n            sample: Subject,\n            output_paths_dict: Dict[str, TypePath],\n            ) -> None:\n        for key, output_path in output_paths_dict.items():\n            tensor = sample[key][DATA].squeeze()  # assume 2D if (1, 1, H, W)\n            affine = sample[key][AFFINE]\n            write_image(tensor, affine, output_path)\n\n    def set_load_image_data(self, load_image_data: bool):\n        if not load_image_data and self._transform is not None:\n            message = (\n                \'Load data cannot be set to False if transform is not None.\'\n                f\'Current transform is {self._transform}\')\n            raise ValueError(message)\n        self._load_image_data = load_image_data\n'"
torchio/data/image.py,7,"b'import warnings\nfrom pathlib import Path\nfrom typing import Any, Dict, Tuple, Optional\n\nimport torch\nimport numpy as np\nimport nibabel as nib\nimport SimpleITK as sitk\n\nfrom ..utils import nib_to_sitk, get_rotation_and_spacing_from_affine\nfrom ..torchio import (\n    TypePath,\n    TypeTripletInt,\n    TypeTripletFloat,\n    DATA,\n    TYPE,\n    AFFINE,\n    PATH,\n    STEM,\n    INTENSITY,\n)\nfrom .io import read_image\n\n\nclass Image(dict):\n    r""""""Class to store information about an image.\n\n    Args:\n        path: Path to a file that can be read by\n            :mod:`SimpleITK` or :mod:`nibabel` or to a directory containing\n            DICOM files.\n        type: Type of image, such as :attr:`torchio.INTENSITY` or\n            :attr:`torchio.LABEL`. This will be used by the transforms to\n            decide whether to apply an operation, or which interpolation to use\n            when resampling.\n        tensor: If :attr:`path` is not given, :attr:`tensor` must be a 4D\n            :py:class:`torch.Tensor` with dimensions :math:`(C, D, H, W)`,\n            where :math:`C` is the number of channels and :math:`D, H, W`\n            are the spatial dimensions.\n        affine: If :attr:`path` is not given, :attr:`affine` must be a\n            :math:`4 \\times 4` NumPy array. If ``None``, :attr:`affine` is an\n            identity matrix.\n        **kwargs: Items that will be added to image dictionary within the\n            subject sample.\n    """"""\n    def __init__(\n            self,\n            path: Optional[TypePath] = None,\n            type: str = INTENSITY,\n            tensor: Optional[torch.Tensor] = None,\n            affine: Optional[torch.Tensor] = None,\n            **kwargs: Dict[str, Any],\n            ):\n        if path is None and tensor is None:\n            raise ValueError(\'A value for path or tensor must be given\')\n        if path is not None:\n            if tensor is not None or affine is not None:\n                message = \'If a path is given, tensor and affine must be None\'\n                raise ValueError(message)\n        self._tensor = self.parse_tensor(tensor)\n        self._affine = self.parse_affine(affine)\n        if self._affine is None:\n            self._affine = np.eye(4)\n        for key in (DATA, AFFINE, TYPE, PATH, STEM):\n            if key in kwargs:\n                raise ValueError(f\'Key {key} is reserved. Use a different one\')\n\n        super().__init__(**kwargs)\n        self.path = self._parse_path(path)\n        self.type = type\n        self.is_sample = False  # set to True by ImagesDataset\n\n    def __repr__(self):\n        properties = [\n            f\'shape: {self.shape}\',\n            f\'spacing: {self.spacing}\',\n            f\'orientation: {"""".join(self.orientation)}+\',\n        ]\n        properties = \'; \'.join(properties)\n        string = f\'{self.__class__.__name__}({properties})\'\n        return string\n\n    @property\n    def data(self):\n        return self[DATA]\n\n    @property\n    def affine(self):\n        return self[AFFINE]\n\n    @property\n    def shape(self) -> Tuple[int, int, int, int]:\n        return tuple(self[DATA].shape)\n\n    @property\n    def spatial_shape(self) -> TypeTripletInt:\n        return self.shape[1:]\n\n    @property\n    def orientation(self):\n        return nib.aff2axcodes(self[AFFINE])\n\n    @property\n    def spacing(self):\n        _, spacing = get_rotation_and_spacing_from_affine(self.affine)\n        return tuple(spacing)\n\n    @staticmethod\n    def _parse_path(path: TypePath) -> Path:\n        if path is None:\n            return None\n        try:\n            path = Path(path).expanduser()\n        except TypeError:\n            message = f\'Conversion to path not possible for variable: {path}\'\n            raise TypeError(message)\n        if not (path.is_file() or path.is_dir()):  # might be a dir with DICOM\n            raise FileNotFoundError(f\'File not found: {path}\')\n        return path\n\n    @staticmethod\n    def parse_tensor(tensor: torch.Tensor) -> torch.Tensor:\n        if tensor is None:\n            return None\n        num_dimensions = tensor.dim()\n        if num_dimensions != 3:\n            message = (\n                \'The input tensor must have 3 dimensions (D, H, W),\'\n                f\' but has {num_dimensions}: {tensor.shape}\'\n            )\n            raise RuntimeError(message)\n        tensor = tensor.unsqueeze(0)  # add channels dimension\n        return tensor\n\n    @staticmethod\n    def parse_affine(affine: np.ndarray) -> np.ndarray:\n        if affine is None:\n            return np.eye(4)\n        if not isinstance(affine, np.ndarray):\n            raise TypeError(f\'Affine must be a NumPy array, not {type(affine)}\')\n        if affine.shape != (4, 4):\n            raise ValueError(f\'Affine shape must be (4, 4), not {affine.shape}\')\n        return affine\n\n    def load(self, check_nans: bool = True) -> Tuple[torch.Tensor, np.ndarray]:\n        r""""""Load the image from disk.\n\n        The file is expected to be monomodal/grayscale and 2D or 3D.\n        A channels dimension is added to the tensor.\n\n        Args:\n            check_nans: If ``True``, issues a warning if NaNs are found\n                in the image\n\n        Returns:\n            Tuple containing a 4D data tensor of size\n            :math:`(1, D_{in}, H_{in}, W_{in})`\n            and a 2D 4x4 affine matrix\n        """"""\n        if self.path is None:\n            return self._tensor, self._affine\n        tensor, affine = read_image(self.path)\n        # https://github.com/pytorch/pytorch/issues/9410#issuecomment-404968513\n        tensor = tensor[(None,) * (3 - tensor.ndim)]  # force to be 3D\n        # Remove next line and uncomment the two following ones once/if this issue\n        # gets fixed:\n        # https://github.com/pytorch/pytorch/issues/29010\n        # See also https://discuss.pytorch.org/t/collating-named-tensors/78650/4\n        tensor = tensor.unsqueeze(0)  # add channels dimension\n        # name_dimensions(tensor, affine)\n        # tensor = tensor.align_to(\'channels\', ...)\n        if check_nans and torch.isnan(tensor).any():\n            warnings.warn(f\'NaNs found in file ""{self.path}""\')\n        return tensor, affine\n\n    def is_2d(self) -> bool:\n        return self.shape[-3] == 1\n\n    def numpy(self) -> np.ndarray:\n        return self[DATA].numpy()\n\n    def as_sitk(self) -> sitk.Image:\n        return nib_to_sitk(self[DATA], self[AFFINE])\n\n    def get_center(self, lps: bool = False) -> TypeTripletFloat:\n        """"""Get image center in RAS (default) or LPS coordinates.""""""\n        image = self.as_sitk()\n        size = np.array(image.GetSize())\n        center_index = (size - 1) / 2\n        l, p, s = image.TransformContinuousIndexToPhysicalPoint(center_index)\n        if lps:\n            return (l, p, s)\n        else:\n            return (-l, -p, s)\n'"
torchio/data/io.py,8,"b'from pathlib import Path\nfrom typing import Tuple\nimport torch\nimport numpy as np\nimport nibabel as nib\nimport SimpleITK as sitk\nfrom .. import TypePath, TypeData\nfrom ..utils import nib_to_sitk, sitk_to_nib\n\n\ndef read_image(\n        path: TypePath,\n        itk_first: bool = False,\n        ) -> Tuple[torch.Tensor, np.ndarray]:\n    if itk_first:\n        try:\n            result = _read_sitk(path)\n        except RuntimeError:  # try with NiBabel\n            result = _read_nibabel(path)\n    else:\n        try:\n            result = _read_nibabel(path)\n        except nib.loadsave.ImageFileError:  # try with ITK\n            result = _read_sitk(path)\n    return result\n\n\ndef _read_nibabel(path: TypePath) -> Tuple[torch.Tensor, np.ndarray]:\n    nii = nib.load(str(path), mmap=False)\n    data = nii.get_fdata(dtype=np.float32)\n    tensor = torch.from_numpy(data)\n    affine = nii.affine\n    return tensor, affine\n\n\ndef _read_sitk(path: TypePath) -> Tuple[torch.Tensor, np.ndarray]:\n    if Path(path).is_dir():  # assume DICOM\n        image = _read_dicom(path)\n    else:\n        image = sitk.ReadImage(str(path))\n    data, affine = sitk_to_nib(image)\n    if data.dtype != np.float32:\n        data = data.astype(np.float32)\n    tensor = torch.from_numpy(data)\n    return tensor, affine\n\n\ndef _read_dicom(directory: TypePath):\n    directory = Path(directory)\n    if not directory.is_dir():  # unreachable if called from _read_sitk\n        raise FileNotFoundError(f\'Directory ""{directory}"" not found\')\n    reader = sitk.ImageSeriesReader()\n    dicom_names = reader.GetGDCMSeriesFileNames(str(directory))\n    if not dicom_names:\n        message = (\n            f\'The directory ""{directory}""\'\n            \' does not seem to contain DICOM files\'\n        )\n        raise FileNotFoundError(message)\n    reader.SetFileNames(dicom_names)\n    image = reader.Execute()\n    return image\n\n\ndef write_image(\n        tensor: torch.Tensor,\n        affine: TypeData,\n        path: TypePath,\n        itk_first: bool = False,\n        ) -> None:\n    if itk_first:\n        try:\n            _write_sitk(tensor, affine, path)\n        except RuntimeError:  # try with NiBabel\n            _write_nibabel(tensor, affine, path)\n    else:\n        try:\n            _write_nibabel(tensor, affine, path)\n        except nib.loadsave.ImageFileError:  # try with ITK\n            _write_sitk(tensor, affine, path)\n\n\ndef _write_nibabel(\n        tensor: torch.Tensor,\n        affine: TypeData,\n        path: TypePath,\n        ) -> None:\n    """"""\n    Expects a path with an extension that can be used by nibabel.save\n    to write a NIfTI-1 image, such as \'.nii.gz\' or \'.img\'\n    """"""\n    nii = nib.Nifti1Image(tensor.numpy(), affine)\n    nii.header[\'qform_code\'] = 1\n    nii.header[\'sform_code\'] = 0\n    nii.to_filename(str(path))\n\n\ndef _write_sitk(\n        tensor: torch.Tensor,\n        affine: TypeData,\n        path: TypePath,\n        ) -> None:\n    image = nib_to_sitk(tensor, affine)\n    sitk.WriteImage(image, str(path))\n'"
torchio/data/orientation.py,0,"b""import warnings\nimport nibabel as nib\n\n\nAXCODES_TO_WORDS = {\n    'L': 'left',\n    'R': 'right',\n    'P': 'posterior',\n    'A': 'anterior',\n    'I': 'inferior',\n    'S': 'superior',\n    # 'C': 'caudal',\n    # 'R': 'rostral',  # conflic with right\n    # 'D': 'dorsal',\n    # 'V': 'ventral',\n}\n\n\ndef name_dimensions(tensor, affine):\n    axcodes = nib.aff2axcodes(affine)\n    names = [AXCODES_TO_WORDS[axcode] for axcode in axcodes]\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', UserWarning)\n        tensor.rename_(*names)\n"""
torchio/data/queue.py,4,"b'import random\nimport warnings\nfrom itertools import islice\nfrom typing import List, Iterator\n\nfrom tqdm import trange\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom .sampler import PatchSampler\nfrom .dataset import ImagesDataset\n\n\nclass Queue(Dataset):\n    r""""""Patches queue used for patch-based training.\n\n    Args:\n        subjects_dataset: Instance of\n            :class:`~torchio.data.dataset.ImagesDataset`.\n        max_length: Maximum number of patches that can be stored in the queue.\n            Using a large number means that the queue needs to be filled less\n            often, but more CPU memory is needed to store the patches.\n        samples_per_volume: Number of patches to extract from each volume.\n            A small number of patches ensures a large variability in the queue,\n            but training will be slower.\n        sampler: A sampler used to extract patches from the volumes.\n        num_workers: Number of subprocesses to use for data loading\n            (as in :class:`torch.utils.data.DataLoader`).\n            ``0`` means that the data will be loaded in the main process.\n        shuffle_subjects: If ``True``, the subjects dataset is shuffled at the\n            beginning of each epoch, i.e. when all patches from all subjects\n            have been processed.\n        shuffle_patches: If ``True``, patches are shuffled after filling the\n            queue.\n        verbose: If ``True``, some debugging messages are printed.\n\n    This sketch can be used to experiment and understand how the queue works.\n    In this case, :attr:`shuffle_subjects` is ``False``\n    and :attr:`shuffle_patches` is ``True``.\n\n    .. raw:: html\n\n        <embed>\n            <iframe style=""width: 640px; height: 360px; overflow: hidden;"" scrolling=""no"" frameborder=""0"" src=""https://editor.p5js.org/embed/DZwjZzkkV""></iframe>\n        </embed>\n\n    .. note:: :attr:`num_workers` refers to the number of workers used to\n        load and transform the volumes. Multiprocessing is not needed to pop\n        patches from the queue.\n\n    Example:\n\n    >>> from torch.utils.data import DataLoader\n    >>> import torchio\n    >>> patches_queue = torchio.Queue(\n    ...     subjects_dataset=subjects_dataset,  # instance of torchio.ImagesDataset\n    ...     max_length=300,\n    ...     samples_per_volume=10,\n    ...     patch_size=96,\n    ...     sampler=,\n    ...     num_workers=4,\n    ...     shuffle_subjects=True,\n    ...     shuffle_patches=True,\n    ... )\n    >>> patches_loader = DataLoader(patches_queue, batch_size=4)\n    >>> num_epochs = 20\n    >>> for epoch_index in range(num_epochs):\n    ...     for patches_batch in patches_loader:\n    ...         inputs = patches_batch[\'image_name\'][torchio.DATA]\n    ...         targets = patches_batch[\'targets_name\'][torchio.DATA]\n    ...         logits = model(inputs)  # model is some torch.nn.Module\n\n    """"""\n    def __init__(\n            self,\n            subjects_dataset: ImagesDataset,\n            max_length: int,\n            samples_per_volume: int,\n            sampler: PatchSampler,\n            num_workers: int = 0,\n            shuffle_subjects: bool = True,\n            shuffle_patches: bool = True,\n            verbose: bool = False,\n            ):\n        self.subjects_dataset = subjects_dataset\n        self.max_length = max_length\n        self.shuffle_subjects = shuffle_subjects\n        self.shuffle_patches = shuffle_patches\n        self.samples_per_volume = samples_per_volume\n        self.sampler = sampler\n        self.num_workers = num_workers\n        self.verbose = verbose\n        self.subjects_iterable = self.get_subjects_iterable()\n        self.patches_list: List[dict] = []\n        self.num_sampled_patches = 0\n\n    def __len__(self):\n        return self.iterations_per_epoch\n\n    def __getitem__(self, _):\n        # There are probably more elegant ways of doing this\n        if not self.patches_list:\n            self.print(\'Patches list is empty.\')\n            self.fill()\n        sample_patch = self.patches_list.pop()\n        self.num_sampled_patches += 1\n        return sample_patch\n\n    def __repr__(self):\n        attributes = [\n            f\'max_length={self.max_length}\',\n            f\'num_subjects={self.num_subjects}\',\n            f\'num_patches={self.num_patches}\',\n            f\'samples_per_volume={self.samples_per_volume}\',\n            f\'num_sampled_patches={self.num_sampled_patches}\',\n            f\'iterations_per_epoch={self.iterations_per_epoch}\',\n        ]\n        attributes_string = \', \'.join(attributes)\n        return f\'Queue({attributes_string})\'\n\n    def print(self, *args):\n        if self.verbose:\n            print(*args)\n\n    @property\n    def num_subjects(self) -> int:\n        return len(self.subjects_dataset)\n\n    @property\n    def num_patches(self) -> int:\n        return len(self.patches_list)\n\n    @property\n    def iterations_per_epoch(self) -> int:\n        return self.num_subjects * self.samples_per_volume\n\n    def fill(self) -> None:\n        assert self.sampler is not None\n        if self.max_length % self.samples_per_volume != 0:\n            message = (\n                f\'Queue length ({self.max_length})\'\n                \' not divisible by the number of\'\n                f\' patches per volume ({self.samples_per_volume})\'\n            )\n            warnings.warn(message)\n\n        # If there are e.g. 4 subjects and 1 sample per volume and max_length\n        # is 6, we just need to load 4 subjects, not 6\n        max_num_subjects_for_queue = self.max_length // self.samples_per_volume\n        num_subjects_for_queue = min(\n            self.num_subjects, max_num_subjects_for_queue)\n\n        self.print(f\'Filling queue from {num_subjects_for_queue} subjects...\')\n        if self.verbose:\n            iterable = trange(num_subjects_for_queue, leave=False)\n        else:\n            iterable = range(num_subjects_for_queue)\n        for _ in iterable:\n            subject_sample = self.get_next_subject_sample()\n            iterable = self.sampler(subject_sample)\n            patches = list(islice(iterable, self.samples_per_volume))\n            self.patches_list.extend(patches)\n        if self.shuffle_patches:\n            random.shuffle(self.patches_list)\n\n    def get_next_subject_sample(self) -> dict:\n        # A StopIteration exception is expected when the queue is empty\n        try:\n            subject_sample = next(self.subjects_iterable)\n        except StopIteration as exception:\n            self.print(\'Queue is empty:\', exception)\n            self.subjects_iterable = self.get_subjects_iterable()\n            subject_sample = next(self.subjects_iterable)\n        return subject_sample\n\n    def get_subjects_iterable(self) -> Iterator:\n        # I need a DataLoader to handle parallelism\n        # But this loader is always expected to yield single subject samples\n        self.print(\n            \'\\nCreating subjects loader with\', self.num_workers, \'workers\')\n        subjects_loader = DataLoader(\n            self.subjects_dataset,\n            num_workers=self.num_workers,\n            collate_fn=lambda x: x[0],\n            shuffle=self.shuffle_subjects,\n        )\n        return iter(subjects_loader)\n'"
torchio/data/subject.py,0,"b'import pprint\nfrom typing import (\n    Any,\n    Dict,\n    List,\n    Tuple,\n)\nfrom ..torchio import TYPE, INTENSITY\nfrom .image import Image\n\n\nclass Subject(dict):\n    """"""Class to store information about the images corresponding to a subject.\n\n    Args:\n        *args: If provided, a dictionary of items.\n        **kwargs: Items that will be added to the subject sample.\n\n    Example:\n\n        >>> import torchio\n        >>> from torchio import Image, Subject\n        >>> # One way:\n        >>> subject = Subject(\n        ...     one_image=Image(\'path_to_image.nii.gz\', torchio.INTENSITY),\n        ...     a_segmentation=Image(\'path_to_seg.nii.gz\', torchio.LABEL),\n        ...     age=45,\n        ...     name=\'John Doe\',\n        ...     hospital=\'Hospital Juan Negr\xc3\xadn\',\n        ... )\n        >>> # If you want to create the mapping before, or have spaces in the keys:\n        >>> subject_dict = {\n        ...     \'one image\': Image(\'path_to_image.nii.gz\', torchio.INTENSITY),\n        ...     \'a segmentation\': Image(\'path_to_seg.nii.gz\', torchio.LABEL),\n        ...     \'age\': 45,\n        ...     \'name\': \'John Doe\',\n        ...     \'hospital\': \'Hospital Juan Negr\xc3\xadn\',\n        ... }\n        >>> Subject(subject_dict)\n\n    """"""\n\n    def __init__(self, *args, **kwargs: Dict[str, Any]):\n        if args:\n            if len(args) == 1 and isinstance(args[0], dict):\n                kwargs.update(args[0])\n            else:\n                message = (\n                    \'Only one dictionary as positional argument is allowed\')\n                raise ValueError(message)\n        super().__init__(**kwargs)\n        self.images = [\n            (k, v) for (k, v) in self.items()\n            if isinstance(v, Image)\n        ]\n        self._parse_images(self.images)\n        self.is_sample = False  # set to True by ImagesDataset\n        self.history = []\n\n    def __repr__(self):\n        string = (\n            f\'{self.__class__.__name__}\'\n            f\'(Keys: {tuple(self.keys())}; images: {len(self.images)})\'\n        )\n        return string\n\n    @staticmethod\n    def _parse_images(images: List[Tuple[str, Image]]) -> None:\n        # Check that it\'s not empty\n        if not images:\n            raise ValueError(\'A subject without images cannot be created\')\n\n    @property\n    def shape(self):\n        """"""Return shape of first image in sample.\n\n        Consistency of shapes across images in the sample is checked first.\n        """"""\n        self.check_consistent_shape()\n        image = self.get_images(intensity_only=False)[0]\n        return image.shape\n\n    @property\n    def spatial_shape(self):\n        """"""Return spatial shape of first image in sample.\n\n        Consistency of shapes across images in the sample is checked first.\n        """"""\n        return self.shape[1:]\n\n    def get_images_dict(self, intensity_only=True):\n        images = {}\n        for image_name, image in self.items():\n            if not isinstance(image, Image):\n                continue\n            if intensity_only and not image[TYPE] == INTENSITY:\n                continue\n            images[image_name] = image\n        return images\n\n    def get_images(self, intensity_only=True):\n        images_dict = self.get_images_dict(intensity_only=intensity_only)\n        return list(images_dict.values())\n\n    def check_consistent_shape(self) -> None:\n        shapes_dict = {}\n        iterable = self.get_images_dict(intensity_only=False).items()\n        for image_name, image in iterable:\n            shapes_dict[image_name] = image.shape\n        num_unique_shapes = len(set(shapes_dict.values()))\n        if num_unique_shapes > 1:\n            message = (\n                \'Images in sample have inconsistent shapes:\'\n                f\'\\n{pprint.pformat(shapes_dict)}\'\n            )\n            raise ValueError(message)\n\n    def add_transform(\n            self,\n            transform: \'Transform\',\n            parameters_dict: dict,\n            ) -> None:\n        self.history.append((transform.name, parameters_dict))\n'"
torchio/datasets/__init__.py,0,"b'from .ixi import IXI, IXITiny\nfrom .slicer import SubjectSlicer\nfrom .mni import Colin27, Sheep, Pediatric\n'"
torchio/datasets/ixi.py,1,"b'""""""\nThe `Information eXtraction from Images (IXI)`_\ndataset contains ""nearly 600 MR images from normal, healthy subjects"",\nincluding ""T1, T2 and PD-weighted images,\nMRA images and Diffusion-weighted images (15 directions)"".\n\n\n.. note ::\n    This data is made available under the\n    Creative Commons CC BY-SA 3.0 license.\n    If you use it please acknowledge the source of the IXI data, e.g.\n    `the IXI website <https://brain-development.org/ixi-dataset/>`_.\n\n.. _Information eXtraction from Images (IXI): https://brain-development.org/ixi-dataset/\n""""""\n\n# Adapted from\n# https://pytorch.org/docs/stable/_modules/torchvision/datasets/mnist.html#MNIST\n\n\nimport shutil\nfrom pathlib import Path\nfrom typing import Optional, Sequence\nfrom tempfile import NamedTemporaryFile\nfrom torchvision.datasets.utils import download_and_extract_archive\nfrom ..transforms import Transform\nfrom .. import ImagesDataset, Subject, Image, INTENSITY, LABEL, TypePath\n\n\nclass IXI(ImagesDataset):\n    """"""\n    Full IXI dataset.\n\n    Args:\n        root: Root directory to which the dataset will be downloaded.\n        transform: An instance of\n            :class:`~torchio.transforms.transform.Transform`.\n        download: If set to ``True``, will download the data into :attr:`root`.\n        modalities: List of modalities to be downloaded. They must be in\n            ``(\'T1\', \'T2\', \'PD\', \'MRA\', \'DTI\')``.\n\n    .. warning:: The size of this dataset is multiple GB.\n        If you set :attr:`download` to ``True``, it will take some time\n        to be downloaded if it is not already present.\n\n    Example::\n\n        >>> import torchio\n        >>> transforms = [\n        ...     torchio.ToCanonical(),  # to RAS\n        ...     torchio.Resample((1, 1, 1)),  # to 1 mm iso\n        ... ]\n        >>> ixi_dataset = torchio.datasets.IXI(\n        ...     \'path/to/ixi_root/\',\n        ...     modalities=(\'T1\', \'T2\'),\n        ...     transform=torchio.Compose(transforms),\n        ...     download=True,\n        ... )\n        >>> print(\'Number of subjects in dataset:\', len(ixi_dataset))  # 577\n        >>> sample_subject = ixi_dataset[0]\n        >>> print(\'Keys in subject sample:\', tuple(sample_subject.keys()))  # (\'T1\', \'T2\')\n        >>> print(\'Shape of T1 data:\', sample_subject[\'T1\'].shape)  # [1, 180, 268, 268]\n        >>> print(\'Shape of T2 data:\', sample_subject[\'T2\'].shape)  # [1, 241, 257, 188]\n    """"""\n\n    base_url = \'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-{modality}.tar\'\n    md5_dict = {\n        \'T1\': \'34901a0593b41dd19c1a1f746eac2d58\',\n        \'T2\': \'e3140d78730ecdd32ba92da48c0a9aaa\',\n        \'PD\': \'88ecd9d1fa33cb4a2278183b42ffd749\',\n        \'MRA\': \'29be7d2fee3998f978a55a9bdaf3407e\',\n        \'DTI\': \'636573825b1c8b9e8c78f1877df3ee66\',\n    }\n\n    def __init__(\n            self,\n            root: TypePath,\n            transform: Optional[Transform] = None,\n            download: bool = False,\n            modalities: Sequence[str] = (\'T1\', \'T2\'),\n            **kwargs,\n            ):\n        root = Path(root)\n        for modality in modalities:\n            if modality not in self.md5_dict:\n                message = (\n                    f\'Modality ""{modality}"" must be\'\n                    f\' one of {tuple(self.md5_dict.keys())}\'\n                )\n                raise ValueError(message)\n        if download:\n            self._download(root, modalities)\n        if not self._check_exists(root, modalities):\n            message = (\n                \'Dataset not found.\'\n                \' You can use download=True to download it\'\n            )\n            raise RuntimeError(message)\n        subjects_list = self._get_subjects_list(root, modalities)\n        super().__init__(subjects_list, transform=transform, **kwargs)\n\n    @staticmethod\n    def _check_exists(root, modalities):\n        for modality in modalities:\n            modality_dir = root / modality\n            if not modality_dir.is_dir():\n                exists = False\n                break\n        else:\n            exists = True\n        return exists\n\n    @staticmethod\n    def _get_subjects_list(root, modalities):\n        # The number of files for each modality is not the same\n        # E.g. 581 for T1, 578 for T2\n        # Let\'s just use the first modality as reference for now\n        # I.e. only subjects with all modalities will be included\n        one_modality = modalities[0]\n        paths = sglob(root / one_modality, \'*.nii.gz\')\n        subjects = []\n        for filepath in paths:\n            subject_id = get_subject_id(filepath)\n            images_dict = dict(subject_id=subject_id)\n            images_dict[one_modality] = Image(filepath, INTENSITY)\n            for modality in modalities[1:]:\n                globbed = sglob(\n                    root / modality, f\'{subject_id}-{modality}.nii.gz\')\n                if globbed:\n                    assert len(globbed) == 1\n                    images_dict[modality] = Image(globbed[0], INTENSITY)\n                else:\n                    skip_subject = True\n                    break\n            else:\n                skip_subject = False\n            if skip_subject:\n                continue\n            subjects.append(Subject(**images_dict))\n        return subjects\n\n    def _download(self, root, modalities):\n        """"""Download the IXI data if it does not exist already.""""""\n\n        for modality in modalities:\n            modality_dir = root / modality\n            if modality_dir.is_dir():\n                continue\n            modality_dir.mkdir(exist_ok=True, parents=True)\n\n            # download files\n            url = self.base_url.format(modality=modality)\n            md5 = self.md5_dict[modality]\n\n            with NamedTemporaryFile(suffix=\'.tar\') as f:\n                download_and_extract_archive(\n                    url,\n                    download_root=modality_dir,\n                    filename=f.name,\n                    md5=md5,\n                )\n\n\nclass IXITiny(ImagesDataset):\n    r""""""\n    This is the dataset used in the `notebook`_.\n    It is a tiny version of IXI, containing 566 :math:`T_1`-weighted brain MR\n    images and their corresponding brain segmentations,\n    all with size :math:`83 \\times 44 \\times 55`.\n\n    It can be used as a medical image MNIST.\n\n    Args:\n        root: Root directory to which the dataset will be downloaded.\n        transform: An instance of\n            :class:`~torchio.transforms.transform.Transform`.\n        download: If set to ``True``, will download the data into :attr:`root`.\n\n    .. _notebook: https://colab.research.google.com/drive/112NTL8uJXzcMw4PQbUvMQN-WHlVwQS3i\n    """"""\n    url = \'https://www.dropbox.com/s/ogxjwjxdv5mieah/ixi_tiny.zip?dl=1\'\n    md5 = \'bfb60f4074283d78622760230bfa1f98\'\n\n    def __init__(\n            self,\n            root: TypePath,\n            transform: Optional[Transform] = None,\n            download: bool = False,\n            **kwargs,\n            ):\n        root = Path(root)\n        if download:\n            self._download(root)\n        if not root.is_dir():\n            message = (\n                \'Dataset not found.\'\n                \' You can use download=True to download it\'\n            )\n            raise RuntimeError(message)\n        subjects_list = self._get_subjects_list(root)\n        super().__init__(subjects_list, transform=transform, **kwargs)\n\n    @staticmethod\n    def _get_subjects_list(root):\n        image_paths = sglob(root / \'image\', \'*.nii.gz\')\n        label_paths = sglob(root / \'label\', \'*.nii.gz\')\n        assert image_paths and label_paths\n\n        subjects = []\n        for image_path, label_path in zip(image_paths, label_paths):\n            subject_id = get_subject_id(image_path)\n            subject_dict = {}\n            subject_dict[\'image\'] = Image(image_path, INTENSITY)\n            subject_dict[\'label\'] = Image(label_path, LABEL)\n            subject_dict[\'subject_id\'] = subject_id\n            subjects.append(Subject(**subject_dict))\n        return subjects\n\n    def _download(self, root):\n        """"""Download the tiny IXI data if it doesn\'t exist already.""""""\n        if root.is_dir():  # assume it\'s been downloaded\n            print(\'Root directory for IXITiny found:\', root)\n            return\n        print(\'Root directory for IXITiny not found:\', root)\n        print(\'Downloading...\')\n        with NamedTemporaryFile(suffix=\'.zip\') as f:\n            download_and_extract_archive(\n                self.url,\n                download_root=root,\n                filename=f.name,\n                md5=self.md5,\n            )\n        ixi_tiny_dir = root / \'ixi_tiny\'\n        (ixi_tiny_dir / \'image\').rename(root / \'image\')\n        (ixi_tiny_dir / \'label\').rename(root / \'label\')\n        shutil.rmtree(ixi_tiny_dir)\n\n\ndef sglob(directory, pattern):\n    return sorted(list(Path(directory).glob(pattern)))\n\n\ndef get_subject_id(path):\n    return \'-\'.join(path.name.split(\'-\')[:-1])\n'"
torchio/datasets/slicer.py,0,"b'import urllib.parse\nfrom torchvision.datasets.utils import download_url\nfrom .. import Subject, Image\nfrom ..utils import get_torchio_cache_dir\n\n\nSLICER_URL = \'https://github.com/Slicer/SlicerTestingData/releases/download/\'\nURLS_DICT = {\n    \'MRHead\': (\'MRHead.nrrd\', \'SHA256/cc211f0dfd9a05ca3841ce1141b292898b2dd2d3f08286affadf823a7e58df93\'),\n}\n\n\nclass SubjectSlicer(Subject):\n    """"""Sample data provided by 3D Slicer.\n\n    See `the website <https://www.slicer.org/wiki/SampleData>`_\n    for more information.\n    """"""\n    def __init__(self, name=\'MRHead\'):\n        filename, url_file = URLS_DICT[name]\n        url = urllib.parse.urljoin(SLICER_URL, url_file)\n        download_root = get_torchio_cache_dir() / \'slicer\'\n        stem = filename.split(\'.\')[0]\n        download_url(\n            url,\n            download_root,\n            filename=filename,\n        )\n        super().__init__({\n            stem: Image(download_root / filename),\n        })\n'"
torchio/external/__init__.py,0,b''
torchio/external/due.py,0,"b'# emacs: at the end of the file\n# ex: set sts=4 ts=4 sw=4 et:\n# ## ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### #\n""""""\n\nStub file for a guaranteed safe import of duecredit constructs:  if duecredit\nis not available.\n\nTo use it, place it into your project codebase to be imported, e.g. copy as\n\n    cp stub.py /path/tomodule/module/due.py\n\nNote that it might be better to avoid naming it duecredit.py to avoid shadowing\ninstalled duecredit.\n\nThen use in your code as\n\n    from .due import due, Doi, BibTeX, Text\n\nSee  https://github.com/duecredit/duecredit/blob/master/README.md for examples.\n\nOrigin:     Originally a part of the duecredit\nCopyright:  2015-2019  DueCredit developers\nLicense:    BSD-2\n""""""\n\n__version__ = \'0.0.8\'\n\n\nclass InactiveDueCreditCollector(object):\n    """"""Just a stub at the Collector which would not do anything""""""\n    def _donothing(self, *args, **kwargs):\n        """"""Perform no good and no bad""""""\n        pass\n\n    def dcite(self, *args, **kwargs):\n        """"""If I could cite I would""""""\n        def nondecorating_decorator(func):\n            return func\n        return nondecorating_decorator\n\n    active = False\n    activate = add = cite = dump = load = _donothing\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\n\ndef _donothing_func(*args, **kwargs):\n    """"""Perform no good and no bad""""""\n    pass\n\n\ntry:\n    from duecredit import due, BibTeX, Doi, Url, Text\n    if \'due\' in locals() and not hasattr(due, \'cite\'):\n        raise RuntimeError(\n            ""Imported due lacks .cite. DueCredit is now disabled"")\nexcept Exception as e:\n    if not isinstance(e, ImportError):\n        import logging\n        logging.getLogger(""duecredit"").error(\n            ""Failed to import duecredit due to %s"" % str(e))\n    # Initiate due stub\n    due = InactiveDueCreditCollector()\n    BibTeX = Doi = Url = Text = _donothing_func\n\n# Emacs mode definitions\n# Local Variables:\n# mode: python\n# py-indent-offset: 4\n# tab-width: 4\n# indent-tabs-mode: nil\n# End:\n'"
torchio/transforms/__init__.py,0,"b'from .transform import Transform\nfrom .interpolation import Interpolation, get_sitk_interpolator\n\n# Generic\nfrom .lambda_transform import Lambda\n\n# Augmentation\nfrom .augmentation.composition import OneOf\nfrom .augmentation.composition import Compose\n\nfrom .augmentation.spatial import RandomFlip\nfrom .augmentation.spatial import RandomAffine\nfrom .augmentation.spatial import RandomElasticDeformation\n\nfrom .augmentation.intensity import RandomSwap\nfrom .augmentation.intensity import RandomBlur\nfrom .augmentation.intensity import RandomNoise\nfrom .augmentation.intensity import RandomSpike\nfrom .augmentation.intensity import RandomMotion\nfrom .augmentation.intensity import RandomGhosting\nfrom .augmentation.intensity import RandomBiasField\n\n# Preprocessing\nfrom .preprocessing import Pad\nfrom .preprocessing import Crop\nfrom .preprocessing import Resample\nfrom .preprocessing import ToCanonical\nfrom .preprocessing import ZNormalization\nfrom .preprocessing import HistogramStandardization\nfrom .preprocessing import Rescale, RescaleIntensity\nfrom .preprocessing import CropOrPad, CenterCropOrPad\nfrom .preprocessing.intensity.histogram_standardization import train as train_histogram\n'"
torchio/transforms/interpolation.py,0,"b'import enum\nimport SimpleITK as sitk\n\n\n@enum.unique\nclass Interpolation(enum.Enum):\n    """"""Interpolation techniques available in ITK.\n\n    Example:\n        >>> from torchio.transforms import RandomAffine\n        >>> transform = RandomAffine(image_interpolation=\'nearest\')\n    """"""\n    #: Interpolates image intensity at a non-integer pixel position by copying the intensity for the nearest neighbor.\n    NEAREST: str = \'sitkNearestNeighbor\'\n\n    #: Linearly interpolates image intensity at a non-integer pixel position.\n    LINEAR: str = \'sitkLinear\'\n\n    #: Computes the B-spline interpolation weights over the support region of the B-spline.\n    BSPLINE: str = \'sitkBSpline\'\n\n    GAUSSIAN: str = \'sitkGaussian\'\n    LABEL_GAUSSIAN: str = \'sitkLabelGaussian\'\n\n    HAMMING: str = \'sitkHammingWindowedSinc\'\n    COSINE: str = \'sitkCosineWindowedSinc\'\n    WELCH: str = \'sitkWelchWindowedSinc\'\n    LANCZOS: str = \'sitkLanczosWindowedSinc\'\n    BLACKMAN: str = \'sitkBlackmanWindowedSinc\'\n\n\ndef get_sitk_interpolator(interpolation: Interpolation):\n    return getattr(sitk, interpolation.value)\n'"
torchio/transforms/lambda_transform.py,5,"b'from typing import Sequence, Optional\nimport torch\nfrom ..data.subject import Subject\nfrom ..torchio import DATA, TYPE, TypeCallable\nfrom .transform import Transform\n\n\nclass Lambda(Transform):\n    """"""Applies a user-defined function as transform.\n\n    Args:\n        function: Callable that receives and returns a\n            :py:class:`torch.Tensor`.\n        types_to_apply: List of strings corresponding to the image types to\n            which this transform should be applied. If ``None``, the transform\n            will be applied to all images in the sample.\n        p: Probability that this transform will be applied.\n\n    Example:\n        >>> import torchio\n        >>> from torchio.transforms import Lambda\n        >>> invert_intensity = Lambda(lambda x: -x, types_to_apply=[torchio.INTENSITY])\n        >>> invert_mask = Lambda(lambda x: 1 - x, types_to_apply=[torchio.LABEL])\n        >>> def double(x):\n        ...     return 2 * x\n        >>> double_transform = Lambda(double)\n    """"""\n    def __init__(\n            self,\n            function: TypeCallable,\n            types_to_apply: Optional[Sequence[str]] = None,\n            p: float = 1,\n            ):\n        super().__init__(p=p)\n        self.function = function\n        self.types_to_apply = types_to_apply\n\n    def apply_transform(self, sample: Subject) -> dict:\n        for image in sample.get_images(intensity_only=False):\n\n            image_type = image[TYPE]\n            if self.types_to_apply is not None:\n                if image_type not in self.types_to_apply:\n                    continue\n\n            function_arg = image[DATA][0]\n            result = self.function(function_arg)\n            if not isinstance(result, torch.Tensor):\n                message = (\n                    \'The returned value from the callable argument must be\'\n                    f\' of type {torch.Tensor}, not {type(result)}\'\n                )\n                raise ValueError(message)\n            if result.dtype != torch.float32:\n                message = (\n                    \'The data type of the returned value must be\'\n                    f\' of type {torch.float32}, not {result.dtype}\'\n                )\n                raise ValueError(message)\n            if result.ndim != function_arg.ndim:\n                message = (\n                    \'The number of dimensions of the returned value must\'\n                    f\' be {function_arg.ndim}, not {result.ndim}\'\n                )\n                raise ValueError(message)\n            image[DATA][0] = result\n        return sample\n'"
torchio/transforms/transform.py,7,"b'import numbers\nimport warnings\nfrom typing import Union\nfrom copy import deepcopy\nfrom abc import ABC, abstractmethod\n\nimport torch\nimport SimpleITK as sitk\n\nfrom .. import TypeData, INTENSITY, DATA\nfrom ..data.image import Image\nfrom ..data.subject import Subject\nfrom ..data.dataset import ImagesDataset\nfrom ..utils import nib_to_sitk, sitk_to_nib\nfrom .interpolation import Interpolation\n\n\nclass Transform(ABC):\n    """"""Abstract class for all TorchIO transforms.\n\n    All classes used to transform a sample from an\n    :py:class:`~torchio.ImagesDataset` should subclass it.\n    All subclasses should overwrite\n    :py:meth:`torchio.tranforms.Transform.apply_transform`,\n    which takes a sample, applies some transformation and returns the result.\n\n    Args:\n        p: Probability that this transform will be applied.\n    """"""\n    def __init__(self, p: float = 1):\n        self.probability = self.parse_probability(p)\n\n    def __call__(self, data: Union[Subject, torch.Tensor]):\n        """"""Transform a sample and return the result.\n\n        Args:\n            data: Instance of :py:class:`~torchio.Subject` or 4D\n                :py:class:`torch.Tensor` with dimensions :math:`(C, D, H, W)`,\n                where :math:`C` is the number of channels and :math:`D, H, W`\n                are the spatial dimensions. If the input is a tensor, the affine\n                matrix is an identity and a tensor will be also returned.\n        """"""\n        if isinstance(data, torch.Tensor):\n            is_tensor = True\n            sample = self.parse_tensor(data)\n        else:\n            is_tensor = False\n            sample = data\n        self.parse_sample(sample)\n        if torch.rand(1).item() > self.probability:\n            return sample\n        sample = deepcopy(sample)\n        transformed = self.apply_transform(sample)\n        if is_tensor:\n            num_channels = len(data)\n            images = [\n                transformed[f\'channel_{i}\'][DATA]\n                for i in range(num_channels)\n            ]\n            transformed = torch.cat(images)\n        return transformed\n\n    @abstractmethod\n    def apply_transform(self, sample: Subject):\n        raise NotImplementedError\n\n    @staticmethod\n    def parse_probability(probability: float) -> float:\n        is_number = isinstance(probability, numbers.Number)\n        if not (is_number and 0 <= probability <= 1):\n            message = (\n                \'Probability must be a number in [0, 1],\'\n                f\' not {probability}\'\n            )\n            raise ValueError(message)\n        return probability\n\n    @staticmethod\n    def parse_sample(sample: Subject) -> None:\n        if not isinstance(sample, Subject) or not sample.is_sample:\n            message = (\n                \'Input to a transform must be a PyTorch tensor or and instance\'\n                \' of torchio.Subject generated by a torchio.ImagesDataset,\'\n                f\' not ""{type(sample)}""\'\n            )\n            raise RuntimeError(message)\n\n    def parse_tensor(self, tensor: torch.Tensor) -> Subject:\n        num_dimensions = tensor.dim()\n        if num_dimensions != 4:\n            message = (\n                \'The input tensor must have 4 dimensions (channels, i, j, k),\'\n                f\' but has {num_dimensions}: {tensor.shape}\'\n            )\n            raise RuntimeError(message)\n        return self._get_subject_from_tensor(tensor)\n\n    @staticmethod\n    def parse_interpolation(interpolation: str) -> Interpolation:\n        if isinstance(interpolation, Interpolation):\n            message = (\n                \'Interpolation of type torchio.Interpolation\'\n                \' is deprecated, please use a string instead\'\n            )\n            warnings.warn(message, FutureWarning)\n        elif isinstance(interpolation, str):\n            interpolation = interpolation.lower()\n            supported_values = [key.name.lower() for key in Interpolation]\n            if interpolation in supported_values:\n                interpolation = getattr(Interpolation, interpolation.upper())\n            else:\n                message = (\n                    f\'Interpolation ""{interpolation}"" is not among\'\n                    f\' the supported values: {supported_values}\'\n                )\n                raise AttributeError(message)\n        else:\n            message = (\n                \'image_interpolation must be a string,\'\n                f\' not {type(interpolation)}\'\n            )\n            raise TypeError(message)\n        return interpolation\n\n    @staticmethod\n    def _get_subject_from_tensor(tensor: torch.Tensor) -> Subject:\n        subject_dict = {}\n        for channel_index, channel_tensor in enumerate(tensor):\n            name = f\'channel_{channel_index}\'\n            image = Image(tensor=channel_tensor, type=INTENSITY)\n            subject_dict[name] = image\n        subject = Subject(subject_dict)\n        dataset = ImagesDataset([subject])\n        sample = dataset[0]\n        return sample\n\n    @staticmethod\n    def nib_to_sitk(data: TypeData, affine: TypeData):\n        return nib_to_sitk(data, affine)\n\n    @staticmethod\n    def sitk_to_nib(image: sitk.Image):\n        return sitk_to_nib(image)\n\n    @property\n    def name(self):\n        return self.__class__.__name__\n'"
tests/data/inference/__init__.py,0,b''
tests/data/inference/test_grid_sampler.py,0,"b'#!/usr/bin/env python\n\nfrom torchio.data import GridSampler\nfrom ...utils import TorchioTestCase\n\n\nclass TestGridSampler(TorchioTestCase):\n    """"""Tests for `GridSampler`.""""""\n\n    def test_locations(self):\n        patch_size = 5, 20, 20\n        patch_overlap = 2, 4, 6\n        sampler = GridSampler(self.sample, patch_size, patch_overlap)\n        fixture = [\n            [0, 0, 0, 5, 20, 20],\n            [0, 0, 10, 5, 20, 30],\n            [3, 0, 0, 8, 20, 20],\n            [3, 0, 10, 8, 20, 30],\n            [5, 0, 0, 10, 20, 20],\n            [5, 0, 10, 10, 20, 30],\n        ]\n        locations = sampler.locations.tolist()\n        self.assertEqual(locations, fixture)\n\n    def test_large_patch(self):\n        with self.assertRaises(ValueError):\n            GridSampler(self.sample, (5, 21, 5), (0, 2, 0))\n\n    def test_large_overlap(self):\n        with self.assertRaises(ValueError):\n            GridSampler(self.sample, (5, 20, 5), (2, 4, 6))\n\n    def test_odd_overlap(self):\n        with self.assertRaises(ValueError):\n            GridSampler(self.sample, (5, 20, 5), (2, 4, 3))\n\n    def test_single_location(self):\n        sampler = GridSampler(self.sample, (10, 20, 30), 0)\n        fixture = [[0, 0, 0, 10, 20, 30]]\n        self.assertEqual(sampler.locations.tolist(), fixture)\n'"
tests/data/inference/test_inference.py,2,"b'import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchio import LOCATION, DATA\nfrom torchio.data.inference import GridSampler, GridAggregator\nfrom ...utils import TorchioTestCase\n\n\nclass TestInference(TorchioTestCase):\n    """"""Tests for `inference` module.""""""\n    def test_inference_no_padding(self):\n        self.try_inference(None)\n\n    def test_inference_padding(self):\n        self.try_inference(3)\n\n    def try_inference(self, padding_mode):\n        for n in 17, 27:\n            patch_size = 10, 15, n\n            patch_overlap = 4, 6, 8\n            batch_size = 6\n\n            grid_sampler = GridSampler(\n                self.sample,\n                patch_size,\n                patch_overlap,\n                padding_mode=padding_mode,\n            )\n            aggregator = GridAggregator(grid_sampler)\n            patch_loader = DataLoader(grid_sampler, batch_size=batch_size)\n            with torch.no_grad():\n                for patches_batch in tqdm(patch_loader):\n                    input_tensor = patches_batch[\'t1\'][DATA]\n                    locations = patches_batch[LOCATION]\n                    logits = model(input_tensor)  # some model\n                    outputs = logits\n                    aggregator.add_batch(outputs, locations)\n\n            output = aggregator.get_output_tensor()\n            assert (output == -5).all()\n            assert output.shape == self.sample.t1.shape\n\n\ndef model(tensor):\n    tensor[:] = -5\n    return tensor\n'"
tests/data/sampler/__init__.py,0,b''
tests/data/sampler/test_label_sampler.py,3,"b'import torch\nimport torchio\nfrom torchio.data import LabelSampler\nfrom ...utils import TorchioTestCase\n\n\nclass TestLabelSampler(TorchioTestCase):\n    """"""Tests for `LabelSampler` class.""""""\n\n    def test_label_sampler(self):\n        sampler = LabelSampler(5)\n        for patch in sampler(self.sample, num_patches=10):\n            patch_center = patch[\'label\'][torchio.DATA][0, 2, 2, 2]\n            self.assertEqual(patch_center, 1)\n\n    def test_label_probabilities(self):\n        labels = torch.Tensor((0, 0, 1, 1, 2, 1, 0)).reshape(1, 1, -1)\n        subject = torchio.Subject(\n            label=torchio.Image(tensor=labels, type=torchio.LABEL),\n        )\n        sample = torchio.ImagesDataset([subject])[0]\n        probs_dict = {0: 0, 1: 50, 2: 25, 3: 25}\n        sampler = LabelSampler(5, \'label\', label_probabilities=probs_dict)\n        probabilities = sampler.get_probability_map(sample)\n        fixture = torch.Tensor((0, 0, 2/12, 2/12, 3/12, 2/12, 0))\n        assert torch.all(probabilities.squeeze().eq(fixture))\n'"
tests/data/sampler/test_weighted_sampler.py,2,"b'import torch\nimport torchio\nfrom torchio.data import WeightedSampler\nfrom ...utils import TorchioTestCase\n\n\nclass TestWeightedSampler(TorchioTestCase):\n    """"""Tests for `WeightedSampler` class.""""""\n\n    def test_weighted_sampler(self):\n        sample = self.get_sample((7, 7, 7))\n        sampler = WeightedSampler(5, \'prob\')\n        patch = next(iter(sampler(sample)))\n        self.assertEqual(tuple(patch[\'index_ini\']), (1, 1, 1))\n\n    def get_sample(self, image_shape):\n        t1 = torch.rand(*image_shape)\n        prob = torch.zeros_like(t1)\n        prob[3, 3, 3] = 1\n        subject = torchio.Subject(\n            t1=torchio.Image(tensor=t1),\n            prob=torchio.Image(tensor=prob),\n        )\n        sample = torchio.ImagesDataset([subject])[0]\n        return sample\n'"
tests/transforms/augmentation/__init__.py,0,b''
tests/transforms/augmentation/test_oneof.py,0,"b'import torchio\nfrom torchio.transforms import OneOf, RandomAffine, RandomElasticDeformation\nfrom ...utils import TorchioTestCase\n\n\nclass TestOneOf(TorchioTestCase):\n    """"""Tests for `OneOf`.""""""\n    def test_wrong_input_type(self):\n        with self.assertRaises(ValueError):\n            OneOf(1)\n\n    def test_negative_probabilities(self):\n        with self.assertRaises(ValueError):\n            OneOf({RandomAffine(): -1, RandomElasticDeformation(): 1})\n\n    def test_zero_probabilities(self):\n        with self.assertRaises(ValueError):\n            OneOf({RandomAffine(): 0, RandomElasticDeformation(): 0})\n\n    def test_not_transform(self):\n        with self.assertRaises(ValueError):\n            OneOf({RandomAffine: 1, RandomElasticDeformation: 2})\n'"
tests/transforms/augmentation/test_random_affine.py,0,"b'from torchio.transforms import RandomAffine\nfrom ...utils import TorchioTestCase\n\n\nclass TestRandomAffine(TorchioTestCase):\n    """"""Tests for `RandomAffine`.""""""\n    def setUp(self):\n        # Set image origin far from center\n        super().setUp()\n        affine = self.sample.t1.affine\n        affine[:3, 3] = 1e5\n\n    def test_rotation_image(self):\n        # Rotation around image center\n        transform = RandomAffine(\n            degrees=(90, 90),\n            default_pad_value=0,\n            center=\'image\',\n        )\n        transformed = transform(self.sample)\n        total = transformed.t1.data.sum()\n        self.assertNotEqual(total, 0)\n\n    def test_rotation_origin(self):\n        # Rotation around far away point, image should be empty\n        transform = RandomAffine(\n            degrees=(90, 90),\n            default_pad_value=0,\n            center=\'origin\',\n        )\n        transformed = transform(self.sample)\n        total = transformed.t1.data.sum()\n        self.assertEqual(total, 0)\n'"
tests/transforms/augmentation/test_random_elastic_deformation.py,0,"b'import numpy as np\nfrom torchio import Interpolation\nfrom torchio.transforms import RandomElasticDeformation\nfrom ...utils import TorchioTestCase\n\n\nclass TestRandomElasticDeformation(TorchioTestCase):\n    """"""Tests for `RandomElasticDeformation`.""""""\n\n    def test_random_elastic_deformation(self):\n        transform = RandomElasticDeformation(\n            num_control_points=5,\n            max_displacement=(2, 3, 5),  # half grid spacing is (3.3, 3.3, 5)\n            seed=42,\n        )\n        keys = (\'t1\', \'t2\', \'label\')\n        fixtures = 2916.7192, 2955.1265, 2950\n        transformed = transform(self.sample)\n        for key, fixture in zip(keys, fixtures):\n            sample_data = self.sample[key].numpy()\n            transformed_data = transformed[key].numpy()\n            transformed_total = transformed_data.sum()\n            # Make sure that intensities have changed\n            assert not np.array_equal(sample_data, transformed_data)\n            self.assertAlmostEqual(transformed_total, fixture, places=4)\n\n    def test_inputs_pta_gt_one(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(p=1.5)\n\n    def test_inputs_pta_lt_zero(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(p=-1)\n\n    def test_inputs_interpolation_int(self):\n        with self.assertRaises(TypeError):\n            RandomElasticDeformation(image_interpolation=1)\n\n    def test_inputs_interpolation(self):\n        with self.assertWarns(FutureWarning):\n            RandomElasticDeformation(image_interpolation=Interpolation.LINEAR)\n\n    def test_num_control_points_noint(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(num_control_points=2.5)\n\n    def test_num_control_points_small(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(num_control_points=3)\n\n    def test_max_displacement_no_num(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(max_displacement=None)\n\n    def test_max_displacement_negative(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(max_displacement=-1)\n\n    def test_wrong_locked_borders(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(locked_borders=-1)\n\n    def test_coarse_grid_removed(self):\n        with self.assertRaises(ValueError):\n            RandomElasticDeformation(\n                num_control_points=(4, 5, 6),\n                locked_borders=2,\n            )\n\n    def test_folding(self):\n        # Assume shape is (10, 20, 30) and spacing is (1, 1, 1)\n        # Then grid spacing is (10/(12-2), 20/(5-2), 30/(5-2))\n        # or (1, 6.7, 10), and half is (0.5, 3.3, 5)\n        transform = RandomElasticDeformation(\n            num_control_points=(12, 5, 5),\n            max_displacement=6,\n        )\n        with self.assertWarns(RuntimeWarning):\n            transformed = transform(self.sample)\n\n    def test_num_control_points(self):\n        RandomElasticDeformation(num_control_points=5)\n        RandomElasticDeformation(num_control_points=(5, 6, 7))\n\n    def test_max_displacement(self):\n        RandomElasticDeformation(max_displacement=5)\n        RandomElasticDeformation(max_displacement=(5, 6, 7))\n'"
tests/transforms/augmentation/test_random_flip.py,0,"b'import torchio\nfrom ...utils import TorchioTestCase\n\n\nclass TestRandomFlip(TorchioTestCase):\n    """"""Tests for `RandomFlip`.""""""\n    def test_2d(self):\n        sample = self.make_2d(self.sample)\n        transform = torchio.transforms.RandomFlip(\n            axes=(0, 1), flip_probability=1)\n        transform(sample)\n\n    def test_wrong_axes(self):\n        sample = self.make_2d(self.sample)\n        transform = torchio.transforms.RandomFlip(axes=2, flip_probability=1)\n        with self.assertRaises(RuntimeError):\n            transform(sample)\n'"
tests/transforms/augmentation/test_random_motion.py,0,"b'import torchio\nfrom ...utils import TorchioTestCase\n\n\nclass TestRandomMotion(TorchioTestCase):\n    """"""Tests for `RandomMotion`.""""""\n    def test_random_motion(self):\n        transform = torchio.transforms.RandomMotion(\n            seed=42,\n        )\n        transformed = transform(self.sample)\n        self.sample[\'t2\'][torchio.DATA] = self.sample[\'t2\'][torchio.DATA] - 0.5\n        with self.assertWarns(UserWarning):\n            transformed = transform(self.sample)\n'"
tests/transforms/preprocessing/__init__.py,0,b''
tests/transforms/preprocessing/test_crop_pad.py,0,"b'import warnings\nimport numpy as np\nfrom numpy.testing import assert_array_equal\nfrom torchio.transforms import CropOrPad, CenterCropOrPad\nfrom torchio import DATA, AFFINE\nfrom ...utils import TorchioTestCase\n\n\nclass TestCropOrPad(TorchioTestCase):\n    """"""Tests for `CropOrPad`.""""""\n    def test_no_changes(self):\n        sample_t1 = self.sample[\'t1\']\n        shape = sample_t1.spatial_shape\n        transform = CropOrPad(shape)\n        transformed = transform(self.sample)\n        assert_array_equal(sample_t1[DATA], transformed[\'t1\'][DATA])\n        assert_array_equal(sample_t1[AFFINE], transformed[\'t1\'][AFFINE])\n\n    def test_no_changes_mask(self):\n        sample_t1 = self.sample[\'t1\']\n        sample_mask = self.sample[\'label\'][DATA]\n        sample_mask *= 0\n        shape = sample_t1.spatial_shape\n        transform = CropOrPad(shape, mask_name=\'label\')\n        with self.assertWarns(UserWarning):\n            transformed = transform(self.sample)\n        for key in transformed:\n            image_dict = self.sample[key]\n            assert_array_equal(image_dict[DATA], transformed[key][DATA])\n            assert_array_equal(image_dict[AFFINE], transformed[key][AFFINE])\n\n    def test_different_shape(self):\n        shape = self.sample[\'t1\'].spatial_shape\n        target_shape = 9, 21, 30\n        transform = CropOrPad(target_shape)\n        transformed = transform(self.sample)\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            self.assertNotEqual(shape, result_shape)\n\n    def test_shape_right(self):\n        target_shape = 9, 21, 30\n        transform = CropOrPad(target_shape)\n        transformed = transform(self.sample)\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            self.assertEqual(target_shape, result_shape)\n\n    def test_only_pad(self):\n        target_shape = 11, 22, 30\n        transform = CropOrPad(target_shape)\n        transformed = transform(self.sample)\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            self.assertEqual(target_shape, result_shape)\n\n    def test_only_crop(self):\n        target_shape = 9, 18, 30\n        transform = CropOrPad(target_shape)\n        transformed = transform(self.sample)\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            self.assertEqual(target_shape, result_shape)\n\n    def test_shape_negative(self):\n        with self.assertRaises(ValueError):\n            CropOrPad(-1)\n\n    def test_shape_float(self):\n        with self.assertRaises(ValueError):\n            CropOrPad(2.5)\n\n    def test_shape_string(self):\n        with self.assertRaises(ValueError):\n            CropOrPad(\'\')\n\n    def test_shape_one(self):\n        transform = CropOrPad(1)\n        transformed = transform(self.sample)\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            self.assertEqual((1, 1, 1), result_shape)\n\n    def test_wrong_mask_name(self):\n        cop = CropOrPad(1, mask_name=\'wrong\')\n        with self.assertWarns(UserWarning):\n            cop(self.sample)\n\n    def test_deprecation(self):\n        with self.assertWarns(DeprecationWarning):\n            CenterCropOrPad(1)\n\n    def test_empty_mask(self):\n        target_shape = 8, 22, 30\n        transform = CropOrPad(target_shape, mask_name=\'label\')\n        mask = self.sample[\'label\'][DATA]\n        mask *= 0\n        with self.assertWarns(UserWarning):\n            transform(self.sample)\n\n    def test_mask_only_pad(self):\n        target_shape = 11, 22, 30\n        transform = CropOrPad(target_shape, mask_name=\'label\')\n        mask = self.sample[\'label\'][DATA]\n        mask *= 0\n        mask [0, 4:6, 5:8, 3:7] = 1\n        transformed = transform(self.sample)\n        shapes = []\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            shapes.append(result_shape)\n        set_shapes = set(shapes)\n        message = f\'Images have different shapes: {set_shapes}\'\n        assert len(set_shapes) == 1, message\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            self.assertEqual(target_shape, result_shape,\n                f\'Wrong shape for image: {key}\',\n            )\n\n    def test_mask_only_crop(self):\n        target_shape = 9, 18, 30\n        transform = CropOrPad(target_shape, mask_name=\'label\')\n        mask = self.sample[\'label\'][DATA]\n        mask *= 0\n        mask [0, 4:6, 5:8, 3:7] = 1\n        transformed = transform(self.sample)\n        shapes = []\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            shapes.append(result_shape)\n        set_shapes = set(shapes)\n        message = f\'Images have different shapes: {set_shapes}\'\n        assert len(set_shapes) == 1, message\n        for key in transformed:\n            result_shape = transformed[key].spatial_shape\n            self.assertEqual(target_shape, result_shape,\n                f\'Wrong shape for image: {key}\',\n            )\n\n    def test_center_mask(self):\n        """"""The mask bounding box and the input image have the same center""""""\n        target_shape = 8, 22, 30\n        transform_center = CropOrPad(target_shape)\n        transform_mask = CropOrPad(target_shape, mask_name=\'label\')\n        mask = self.sample[\'label\'][DATA]\n        mask *= 0\n        mask[0, 4:6, 9:11, 14:16] = 1\n        transformed_center = transform_center(self.sample)\n        transformed_mask = transform_mask(self.sample)\n        zipped = zip(transformed_center.values(), transformed_mask.values())\n        for image_center, image_mask in zipped:\n            assert_array_equal(\n                image_center[DATA], image_mask[DATA],\n                \'Data is different after cropping\',\n            )\n            assert_array_equal(\n                image_center[AFFINE], image_mask[AFFINE],\n                \'Physical position is different after cropping\',\n            )\n\n    def test_mask_corners(self):\n        """"""The mask bounding box and the input image have the same center""""""\n        target_shape = 8, 22, 30\n        transform_center = CropOrPad(target_shape)\n        transform_mask = CropOrPad(\n            target_shape, mask_name=\'label\')\n        mask = self.sample[\'label\'][DATA]\n        mask *= 0\n        mask[0, 0, 0, 0] = 1\n        mask[0, -1, -1, -1] = 1\n        transformed_center = transform_center(self.sample)\n        transformed_mask = transform_mask(self.sample)\n        zipped = zip(transformed_center.values(), transformed_mask.values())\n        for image_center, image_mask in zipped:\n            assert_array_equal(\n                image_center[DATA], image_mask[DATA],\n                \'Data is different after cropping\',\n            )\n            assert_array_equal(\n                image_center[AFFINE], image_mask[AFFINE],\n                \'Physical position is different after cropping\',\n            )\n\n    def test_mask_origin(self):\n        target_shape = 7, 21, 29\n        center_voxel = np.floor(np.array(target_shape) / 2).astype(int)\n        transform_center = CropOrPad(target_shape)\n        transform_mask = CropOrPad(\n            target_shape, mask_name=\'label\')\n        mask = self.sample[\'label\'][DATA]\n        mask *= 0\n        mask[0, 0, 0, 0] = 1\n        transformed_center = transform_center(self.sample)\n        transformed_mask = transform_mask(self.sample)\n        zipped = zip(transformed_center.values(), transformed_mask.values())\n        for image_center, image_mask in zipped:\n            # Arrays are different\n            assert not np.array_equal(image_center[DATA], image_mask[DATA])\n            # Rotation matrix doesn\'t change\n            center_rotation = image_center[AFFINE][:3, :3]\n            mask_rotation = image_mask[AFFINE][:3, :3]\n            assert_array_equal(center_rotation, mask_rotation)\n            # Origin does change\n            center_origin = image_center[AFFINE][:3, 3]\n            mask_origin = image_mask[AFFINE][:3, 3]\n            assert not np.array_equal(center_origin, mask_origin)\n            # Voxel at origin is center of transformed image\n            origin_value = image_center[DATA][0, 0, 0, 0]\n            i, j, k = center_voxel\n            transformed_value = image_mask[DATA][0, i, j, k]\n            self.assertEqual(origin_value, transformed_value)\n'"
tests/transforms/preprocessing/test_histogram_standardization.py,2,"b'from copy import deepcopy\nimport numpy as np\nimport torch\nfrom torchio.transforms import HistogramStandardization\nfrom ...utils import TorchioTestCase\n\n\nclass TestHistogramStandardization(TorchioTestCase):\n    """"""Tests for :py:class:`HistogramStandardization` class.""""""\n\n    def setUp(self):\n        super().setUp()\n        self.dataset = self.get_ixi_tiny()\n\n    def test_train_histogram(self):\n        samples = [self.dataset[i] for i in range(3)]\n        paths = [sample[\'image\'][\'path\'] for sample in samples]\n        HistogramStandardization.train(\n            paths,\n            masking_function=HistogramStandardization.mean,\n            output_path=(self.dir / \'landmarks.txt\'),\n        )\n        HistogramStandardization.train(\n            paths,\n            mask_path=samples[0][\'label\'][\'path\'],\n            output_path=(self.dir / \'landmarks.npy\'),\n        )\n\n    def test_normalize(self):\n        landmarks = np.linspace(0, 100, 13)\n        landmarks_dict = {\'image\': landmarks}\n        transform = HistogramStandardization(landmarks_dict)\n        transform(self.dataset[0])\n\n    def test_wrong_image_key(self):\n        landmarks = np.linspace(0, 100, 13)\n        landmarks_dict = {\'wrong_key\': landmarks}\n        transform = HistogramStandardization(landmarks_dict)\n        with self.assertRaises(KeyError):\n            transform(self.dataset[0])\n\n    def test_with_saved_dict(self):\n        landmarks = np.linspace(0, 100, 13)\n        landmarks_dict = {\'image\': landmarks}\n        torch.save(landmarks_dict, self.dir / \'landmarks_dict.pth\')\n        landmarks_dict = torch.load(self.dir / \'landmarks_dict.pth\')\n        transform = HistogramStandardization(landmarks_dict)\n        transform(self.dataset[0])\n\n    def test_with_saved_array(self):\n        landmarks = np.linspace(0, 100, 13)\n        np.save(self.dir / \'landmarks.npy\', landmarks)\n        landmarks_dict = {\'image\': self.dir / \'landmarks.npy\'}\n        transform = HistogramStandardization(landmarks_dict)\n        transform(self.dataset[0])\n'"
tests/transforms/preprocessing/test_resample.py,0,"b'import numpy as np\nfrom numpy.testing import assert_array_equal\nfrom torchio import DATA, AFFINE\nfrom torchio.transforms import Resample\nfrom torchio.utils import nib_to_sitk\nfrom ...utils import TorchioTestCase\n\n\nclass TestResample(TorchioTestCase):\n    """"""Tests for `Resample`.""""""\n    def test_spacing(self):\n        # Should this raise an error if sizes are different?\n        spacing = 2\n        transform = Resample(spacing)\n        transformed = transform(self.sample)\n        for image_dict in transformed.values():\n            image = nib_to_sitk(image_dict[DATA], image_dict[AFFINE])\n            self.assertEqual(image.GetSpacing(), 3 * (spacing,))\n\n    def test_reference_name(self):\n        sample = self.get_inconsistent_sample()\n        reference_name = \'t1\'\n        transform = Resample(reference_name)\n        transformed = transform(sample)\n        ref_image_dict = sample[reference_name]\n        for image_dict in transformed.values():\n            self.assertEqual(\n                ref_image_dict.shape, image_dict.shape)\n            assert_array_equal(ref_image_dict[AFFINE], image_dict[AFFINE])\n\n    def test_affine(self):\n        spacing = 1\n        affine_name = \'pre_affine\'\n        transform = Resample(spacing, pre_affine_name=affine_name)\n        transformed = transform(self.sample)\n        for image_dict in transformed.values():\n            if affine_name in image_dict.keys():\n                new_affine = np.eye(4)\n                new_affine[0, 3] = 10\n                assert_array_equal(image_dict[AFFINE], new_affine)\n            else:\n                assert_array_equal(image_dict[AFFINE], np.eye(4))\n\n    def test_missing_affine(self):\n        transform = Resample(1, pre_affine_name=\'missing\')\n        with self.assertRaises(ValueError):\n            transform(self.sample)\n\n    def test_reference_path(self):\n        reference_image, reference_path = self.get_reference_image_and_path()\n        transform = Resample(reference_path)\n        transformed = transform(self.sample)\n        ref_data, ref_affine = reference_image.load()\n        for image_dict in transformed.values():\n            self.assertEqual(\n                ref_data.shape, image_dict.shape)\n            assert_array_equal(ref_affine, image_dict[AFFINE])\n\n    def test_wrong_spacing_length(self):\n        with self.assertRaises(ValueError):\n            Resample((1, 2))\n\n    def test_wrong_spacing_value(self):\n        with self.assertRaises(ValueError):\n            Resample(0)\n\n    def test_wrong_target_type(self):\n        with self.assertRaises(ValueError):\n            Resample(None)\n\n    def test_missing_reference(self):\n        transform = Resample(\'missing\')\n        with self.assertRaises(ValueError):\n            transform(self.sample)\n'"
torchio/data/inference/__init__.py,0,b'from .grid_sampler import GridSampler\nfrom .aggregator import GridAggregator\n'
torchio/data/inference/aggregator.py,8,"b'import warnings\nfrom typing import Tuple\nimport torch\nimport numpy as np\nfrom ...torchio import TypeData, CHANNELS_DIMENSION\nfrom .grid_sampler import GridSampler\n\n\nclass GridAggregator:\n    r""""""Aggregate patches for dense inference.\n\n    This class is typically used to build a volume made of patches after\n    inference of batches extracted by a :py:class:`~torchio.data.GridSampler`.\n\n    Args:\n        sampler: Instance of :py:class:`~torchio.data.GridSampler` used to\n            extract the patches.\n\n    .. note:: Adapted from NiftyNet. See `this NiftyNet tutorial\n        <https://niftynet.readthedocs.io/en/dev/window_sizes.html>`_ for more\n        information about patch based sampling.\n    """"""\n    def __init__(self, sampler: GridSampler):\n        sample = sampler.sample\n        self.volume_padded = sampler.padding_mode is not None\n        self.spatial_shape = sample.spatial_shape\n        self._output_tensor = None\n        self.patch_overlap = sampler.patch_overlap\n\n    def crop_batch(\n            self,\n            batch: torch.Tensor,\n            locations: np.ndarray,\n            overlap: np.ndarray,\n            ) -> Tuple[TypeData, np.ndarray]:\n        border = np.array(overlap) // 2  # overlap is even in grid sampler\n        crop_locations = locations.astype(int).copy()\n        indices_ini, indices_fin = crop_locations[:, :3], crop_locations[:, 3:]\n        num_locations = len(crop_locations)\n\n        border_ini = np.tile(border, (num_locations, 1))\n        border_fin = border_ini.copy()\n        # Do not crop patches at the border of the volume\n        # Unless we\'re padding the volume in the grid sampler. In that case,\n        # it doesn\'t matter if we don\'t crop patches at the border, because the\n        # output volume will be cropped\n        if not self.volume_padded:\n            mask_border_ini = indices_ini == 0\n            border_ini[mask_border_ini] = 0\n            for axis, size in enumerate(self.spatial_shape):\n                mask_border_fin = indices_fin[:, axis] == size\n                border_fin[mask_border_fin, axis] = 0\n\n        indices_ini += border_ini\n        indices_fin -= border_fin\n\n        crop_shapes = indices_fin - indices_ini\n        patch_shape = batch.shape[2:]  # ignore batch and channels dim\n        cropped_patches = []\n        for patch, crop_shape in zip(batch, crop_shapes):\n            diff = patch_shape - crop_shape\n            left = (diff / 2).astype(int)\n            i_ini, j_ini, k_ini = left\n            i_fin, j_fin, k_fin = left + crop_shape\n            cropped_patch = patch[:, i_ini:i_fin, j_ini:j_fin, k_ini:k_fin]\n            cropped_patches.append(cropped_patch)\n        return cropped_patches, crop_locations\n\n    def initialize_output_tensor(self, batch: torch.Tensor) -> None:\n        if self._output_tensor is not None:\n            return\n        num_channels = batch.shape[CHANNELS_DIMENSION]\n        self._output_tensor = torch.zeros(\n            num_channels,\n            *self.spatial_shape,\n            dtype=batch.dtype,\n        )\n\n    def add_batch(\n            self,\n            batch_tensor: torch.Tensor,\n            locations: torch.Tensor,\n            ) -> None:\n        """"""Add batch processed by a CNN to the output prediction volume.\n\n        Args:\n            batch_tensor: 5D tensor, typically the output of a convolutional\n                neural network, e.g. ``batch[\'image\'][torchio.DATA]``.\n            locations: 2D tensor with shape :math:`(B, 6)` representing the\n                patch indices in the original image. They are typically\n                extracted using ``batch[torchio.LOCATION]``.\n        """"""\n        batch = batch_tensor.cpu()\n        locations = locations.cpu().numpy()\n        self.initialize_output_tensor(batch)\n        cropped_patches, crop_locations = self.crop_batch(\n            batch,\n            locations,\n            self.patch_overlap,\n        )\n        for patch, crop_location in zip(cropped_patches, crop_locations):\n            i_ini, j_ini, k_ini, i_fin, j_fin, k_fin = crop_location\n            self._output_tensor[\n                :,\n                i_ini:i_fin,\n                j_ini:j_fin,\n                k_ini:k_fin] = patch\n\n    def get_output_tensor(self) -> torch.Tensor:\n        """"""Get the aggregated volume after dense inference.""""""\n        if self._output_tensor.dtype == torch.int64:\n            message = (\n                \'Medical image frameworks such as ITK do not support int64.\'\n                \' Casting to int32...\'\n            )\n            warnings.warn(message)\n            self._output_tensor = self._output_tensor.type(torch.int32)\n        if self.volume_padded:\n            from ...transforms import Crop\n            border = self.patch_overlap // 2\n            cropping = border.repeat(2)\n            crop = Crop(cropping)\n            return crop(self._output_tensor)\n        else:\n            return self._output_tensor\n'"
torchio/data/inference/grid_sampler.py,1,"b'from typing import Union\n\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom ...utils import to_tuple\nfrom ...torchio import LOCATION, TypeTuple, TypeTripletInt\nfrom ..subject import Subject\nfrom ..sampler.sampler import PatchSampler\n\n\nclass GridSampler(PatchSampler, Dataset):\n    r""""""Extract patches across a whole volume.\n\n    Grid samplers are useful to perform inference using all patches from a\n    volume. It is often used with a :py:class:`~torchio.data.GridAggregator`.\n\n    Args:\n        sample: Instance of :py:class:`~torchio.data.subject.Subject`\n            from which patches will be extracted.\n        patch_size: Tuple of integers :math:`(d, h, w)` to generate patches\n            of size :math:`d \\times h \\times w`.\n            If a single number :math:`n` is provided,\n            :math:`d = h = w = n`.\n        patch_overlap: Tuple of even integers :math:`(d_o, h_o, w_o)` specifying\n            the overlap between patches for dense inference. If a single number\n            :math:`n` is provided, :math:`d_o = h_o = w_o = n`.\n        padding_mode: Same as :attr:`padding_mode` in\n            :py:class:`~torchio.transforms.Pad`. If ``None``, the volume will\n            not be padded before sampling and patches at the border will not be\n            cropped by the aggregator. Otherwise, the volume will be padded with\n            :math:`\\left(\\frac{d_o}{2}, \\frac{h_o}{2}, \\frac{w_o}{2}\\right)`\n            on each side before sampling. If the sampler is passed to a\n            :py:class:`~torchio.data.GridAggregator`, it will crop the output\n            to its original size.\n\n    .. note:: Adapted from NiftyNet. See `this NiftyNet tutorial\n        <https://niftynet.readthedocs.io/en/dev/window_sizes.html>`_ for more\n        information about patch based sampling. Note that\n        :py:attr:`patch_overlap` is twice :py:attr:`border` in NiftyNet\n        tutorial.\n    """"""\n    def __init__(\n            self,\n            sample: Subject,\n            patch_size: TypeTuple,\n            patch_overlap: TypeTuple = (0, 0, 0),\n            padding_mode: Union[str, float, None] = None,\n            ):\n        self.sample = sample\n        self.patch_overlap = np.array(to_tuple(patch_overlap, length=3))\n        self.padding_mode = padding_mode\n        if padding_mode is not None:\n            from ...transforms import Pad\n            border = self.patch_overlap // 2\n            padding = border.repeat(2)\n            pad = Pad(padding, padding_mode=padding_mode)\n            self.sample = pad(self.sample)\n        PatchSampler.__init__(self, patch_size)\n        sizes = self.sample.spatial_shape, self.patch_size, self.patch_overlap\n        self.parse_sizes(*sizes)\n        self.locations = self.get_patches_locations(*sizes)\n\n    def __len__(self):\n        return len(self.locations)\n\n    def __getitem__(self, index):\n        # Assume 3D\n        location = self.locations[index]\n        index_ini = location[:3]\n        index_fin = location[3:]\n        cropped_sample = self.extract_patch(self.sample, index_ini, index_fin)\n        cropped_sample[LOCATION] = location\n        return cropped_sample\n\n    @staticmethod\n    def parse_sizes(\n            image_size: TypeTripletInt,\n            patch_size: TypeTripletInt,\n            patch_overlap: TypeTripletInt,\n            ) -> None:\n        image_size = np.array(image_size)\n        patch_size = np.array(patch_size)\n        patch_overlap = np.array(patch_overlap)\n        if np.any(patch_size > image_size):\n            message = (\n                f\'Patch size {tuple(patch_size)} cannot be\'\n                f\' larger than image size {tuple(image_size)}\'\n            )\n            raise ValueError(message)\n        if np.any(patch_overlap >= patch_size):\n            message = (\n                f\'Patch overlap {tuple(patch_overlap)} must be smaller\'\n                f\' than patch size {tuple(image_size)}\'\n            )\n            raise ValueError(message)\n        if np.any(patch_overlap % 2):\n            message = (\n                \'Patch overlap must be a tuple of even integers,\'\n                f\' not {tuple(patch_overlap)}\'\n            )\n            raise ValueError(message)\n\n    def extract_patch(\n            self,\n            sample: Subject,\n            index_ini: TypeTripletInt,\n            index_fin: TypeTripletInt,\n            ) -> Subject:\n        crop = self.get_crop_transform(\n            sample.spatial_shape,\n            index_ini,\n            index_fin - index_ini,\n        )\n        cropped_sample = crop(sample)\n        return cropped_sample\n\n    @staticmethod\n    def get_patches_locations(\n            image_size: TypeTripletInt,\n            patch_size: TypeTripletInt,\n            patch_overlap: TypeTripletInt,\n            ) -> np.ndarray:\n        # Example with image_size 10, patch_size 5, overlap 2:\n        # [0 1 2 3 4 5 6 7 8 9]\n        # [0 0 0 0 0]\n        #       [1 1 1 1 1]\n        #           [2 2 2 2 2]\n        # Locations:\n        # [[0, 5],\n        #  [3, 8],\n        #  [5, 10]]\n        indices = []\n        zipped = zip(image_size, patch_size, patch_overlap)\n        for im_size_dim, patch_size_dim, patch_overlap_dim in zipped:\n            end = im_size_dim + 1 - patch_size_dim\n            step = patch_size_dim - patch_overlap_dim\n            indices_dim = list(range(0, end, step))\n            if indices_dim[-1] != im_size_dim - patch_size_dim:\n                indices_dim.append(im_size_dim - patch_size_dim)\n            indices.append(indices_dim)\n        indices_ini = np.array(np.meshgrid(*indices)).reshape(3, -1).T\n        indices_ini = np.unique(indices_ini, axis=0)\n        indices_fin = indices_ini + np.array(patch_size)\n        locations = np.hstack((indices_ini, indices_fin))\n        return np.array(sorted(locations.tolist()))\n'"
torchio/data/sampler/__init__.py,0,"b'from .label import LabelSampler\nfrom .uniform import UniformSampler\nfrom .weighted import WeightedSampler\nfrom .sampler import PatchSampler, RandomSampler\n'"
torchio/data/sampler/label.py,5,"b'from typing import Dict, Optional\n\nimport torch\n\nfrom ...data.subject import Subject\nfrom ...torchio import TypePatchSize, DATA, TYPE, LABEL\nfrom .weighted import WeightedSampler\n\n\nclass LabelSampler(WeightedSampler):\n    r""""""Extract random patches with labeled voxels at their center.\n\n    This sampler yields patches whose center value is greater than 0\n    in the :py:attr:`label_name`.\n\n    Args:\n        patch_size: See :py:class:`~torchio.data.PatchSampler`.\n        label_name: Name of the label image in the sample that will be used to\n            generate the sampling probability map. If ``None``, the first image\n            of type :py:attr:`torchio.LABEL` found in the subject sample will be\n            used.\n        label_probabilities: Dictionary containing the probability that each\n            class will be sampled. Probabilities do not need to be normalized.\n            For example, a value of ``{0: 0, 1: 2, 2: 1, 3: 1}`` will create a\n            sampler whose patches centers will have 50% probability of being\n            labeled as ``1``, 25% of being ``2`` and 25% of being ``3``.\n            If ``None``, the label map is binarized and the value is set to\n            ``{0: 0, 1: 1}``.\n\n    Example:\n        >>> import torchio\n        >>> subject = torchio.datasets.Colin27()\n        >>> subject\n        Colin27(Keys: (\'t1\', \'head\', \'brain\'); images: 3)\n        >>> sample = torchio.ImagesDataset([subject])[0]\n        >>> sampler = torchio.data.LabelSampler(64, \'brain\')\n        >>> generator = sampler(sample)\n        >>> for patch in generator:\n        ...     print(patch.shape)\n\n    If you want a specific number of patches from a volume, e.g. 10:\n\n        >>> generator = sampler(sample, num_patches=10)\n        >>> for patch in iterator:\n        ...     print(patch.shape)\n\n    """"""\n    def __init__(\n            self,\n            patch_size: TypePatchSize,\n            label_name: Optional[str] = None,\n            label_probabilities: Optional[Dict[int, float]] = None,\n        ):\n        super().__init__(patch_size, probability_map=label_name)\n        self.label_probabilities_dict = label_probabilities\n\n    def get_probability_map(self, sample: Subject) -> torch.Tensor:\n        if self.probability_map_name is None:\n            for image in sample.get_images(intensity_only=False):\n                if image[TYPE] == LABEL:\n                    label_map_tensor = image[DATA]\n                    break\n        elif self.probability_map_name in sample:\n            label_map_tensor = sample[self.probability_map_name][DATA]\n        else:\n            message = (\n                f\'Image ""{self.probability_map_name}""\'\n                f\' not found in subject sample: {sample}\'\n            )\n            raise KeyError(message)\n        if self.label_probabilities_dict is None:\n            return label_map_tensor > 0\n        probability_map = self.get_probabilities_from_label_map(\n            label_map_tensor,\n            self.label_probabilities_dict,\n        )\n        return probability_map\n\n    @staticmethod\n    def get_probabilities_from_label_map(\n            label_map: torch.Tensor,\n            label_probabilities_dict: Dict[int, float],\n            ) -> torch.Tensor:\n        """"""Create probability map according to label map probabilities.""""""\n        probability_map = torch.zeros_like(label_map)\n        label_probs = torch.Tensor(list(label_probabilities_dict.values()))\n        normalized_probs = label_probs / label_probs.sum()\n        iterable = zip(label_probabilities_dict, normalized_probs)\n        for label, label_probability in iterable:\n            mask = label_map == label\n            label_size = mask.sum()\n            if not label_size: continue\n            prob_voxels = label_probability / label_size\n            probability_map[mask] = prob_voxels\n        return probability_map\n'"
torchio/data/sampler/sampler.py,0,"b'from typing import Tuple, Optional, Generator\n\nimport numpy as np\n\nfrom ... import TypePatchSize\nfrom ...data.subject import Subject\nfrom ...utils import to_tuple\n\n\nclass PatchSampler:\n    r""""""Base class for TorchIO samplers.\n\n    Args:\n        patch_size: Tuple of integers :math:`(d, h, w)` to generate patches\n            of size :math:`d \\times h \\times w`.\n            If a single number :math:`n` is provided, :math:`d = h = w = n`.\n    """"""\n    def __init__(self, patch_size: TypePatchSize):\n        patch_size_array = np.array(to_tuple(patch_size, length=3))\n        if np.any(patch_size_array < 1):\n            message = (\n                \'Patch dimensions must be positive integers,\'\n                f\' not {patch_size_array}\'\n            )\n            raise ValueError(message)\n        self.patch_size = patch_size_array.astype(np.uint16)\n\n    def extract_patch(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def get_crop_transform(\n            image_size,\n            index_ini,\n            patch_size: TypePatchSize,\n            ):\n        from ...transforms.preprocessing.spatial.crop import Crop\n        image_size = np.array(image_size, dtype=np.uint16)\n        index_ini = np.array(index_ini, dtype=np.uint16)\n        patch_size = np.array(patch_size, dtype=np.uint16)\n        index_fin = index_ini + patch_size\n        crop_ini = index_ini.tolist()\n        crop_fin = (image_size - index_fin).tolist()\n        TypeBounds = Tuple[int, int, int, int, int, int]\n        start = ()\n        cropping: TypeBounds = sum(zip(crop_ini, crop_fin), start)\n        return Crop(cropping)\n\n\nclass RandomSampler(PatchSampler):\n    r""""""Base class for TorchIO samplers.\n\n    Args:\n        patch_size: Tuple of integers :math:`(d, h, w)` to generate patches\n            of size :math:`d \\times h \\times w`.\n            If a single number :math:`n` is provided, :math:`d = h = w = n`.\n    """"""\n    def __call__(\n            self,\n            sample: Subject,\n            num_patches: Optional[int] = None,\n            ) -> Generator[Subject, None, None]:\n        raise NotImplementedError\n\n    def get_probability_map(self, sample: Subject):\n        raise NotImplementedError\n'"
torchio/data/sampler/uniform.py,2,"b'import torch\nfrom ...data.subject import Subject\nfrom ...torchio import TypePatchSize\nfrom .weighted import WeightedSampler\n\n\nclass UniformSampler(WeightedSampler):\n    """"""Randomly extract patches from a volume with uniform probability.\n\n    Args:\n        patch_size: See :py:class:`~torchio.data.PatchSampler`.\n    """"""\n    def __init__(self, patch_size: TypePatchSize):\n        super().__init__(patch_size)\n\n    def get_probability_map(self, sample: Subject) -> torch.Tensor:\n        return torch.ones(sample.shape)\n'"
torchio/data/sampler/weighted.py,4,"b'from typing import Optional, Tuple, Generator\n\nimport numpy as np\n\nimport torch\n\nfrom ...torchio import TypePatchSize\nfrom ..subject import Subject\nfrom .sampler import RandomSampler\n\n\n\nclass WeightedSampler(RandomSampler):\n    r""""""Randomly extract patches from a volume given a probability map.\n\n    The probability of sampling a patch centered on a specific voxel is the\n    value of that voxel in the probability map. The probabilities need not be\n    normalized. For example, voxels can have values 0, 1 and 5. Voxels with\n    value 0 will never be at the center of a patch. Voxels with value 5 will\n    have 5 times more chance of being at the center of a patch that voxels\n    with a value of 1.\n\n    Args:\n        sample: Sample generated by a\n            :py:class:`~torchio.data.dataset.ImagesDataset`, from which image\n            patches will be extracted.\n        patch_size: See :py:class:`~torchio.data.PatchSampler`.\n        probability_map: Name of the image in the sample that will be used\n            as a probability map.\n\n    Raises:\n        RuntimeError: If the probability map is empty.\n\n    Example:\n        >>> import torchio\n        >>> subject = torchio.Subject(\n        ...     t1=torchio.Image(\'t1_mri.nii.gz\', type=torchio.INTENSITY),\n        ...     sampling_map=torchio.Image(\'sampling.nii.gz\', type=torchio.SAMPLING_MAP),\n        ... )\n        >>> sample = torchio.ImagesDataset([subject])[0]\n        >>> patch_size = 64\n        >>> sampler = torchio.data.WeightedSampler(patch_size, probability_map=\'sampling_map\')\n        >>> for patch in sampler(sample):\n        ...     print(patch[\'index_ini\'])\n\n    .. note:: The index of the center of a patch with even size :math:`s` is\n        arbitrarily set to :math:`s/2`. This is an implementation detail that\n        will typically not make any difference in practice.\n\n    .. note:: Values of the probability map near the border will be set to 0 as\n        the center of the patch cannot be at the border (unless the patch has\n        size 1 or 2 along that axis).\n\n    """"""\n    def __init__(\n            self,\n            patch_size: TypePatchSize,\n            probability_map: Optional[str] = None,\n            ):\n        super().__init__(patch_size)\n        self.probability_map_name = probability_map\n        self.cdf = None\n        self.sort_indices = None\n\n    def __call__(\n            self,\n            sample: Subject,\n            num_patches: Optional[int] = None,\n            ) -> Generator[Subject, None, None]:\n        sample.check_consistent_shape()\n        if np.any(self.patch_size > sample.spatial_shape):\n            message = (\n                f\'Patch size {tuple(self.patch_size)} cannot be\'\n                f\' larger than image size {tuple(sample.spatial_shape)}\'\n            )\n            raise RuntimeError(message)\n        probability_map = self.get_probability_map(sample)\n        probability_map = self.process_probability_map(probability_map)\n        cdf, sort_indices = self.get_cumulative_distribution_function(\n            probability_map)\n\n        patches_left = num_patches if num_patches is not None else True\n        while patches_left:\n            yield self.extract_patch(sample, probability_map, cdf, sort_indices)\n            if num_patches is not None:\n                patches_left -= 1\n\n    def get_probability_map(self, sample: Subject) -> torch.Tensor:\n        if self.probability_map_name in sample:\n            data = sample[self.probability_map_name].data\n        else:\n            message = (\n                f\'Image ""{self.probability_map_name}""\'\n                f\' not found in subject sample: {sample}\'\n            )\n            raise KeyError(message)\n        if torch.any(data < 0):\n            message = (\n                \'Negative values found\'\n                f\' in probability map ""{self.probability_map_name}""\'\n            )\n            raise ValueError(message)\n        return data\n\n    def process_probability_map(\n            self,\n            probability_map: torch.Tensor,\n            ) -> np.ndarray:\n        # Using float32 can create cdf with maximum very far from 1, e.g. 0.92!\n        data = probability_map[0].numpy().astype(np.float64)\n        assert data.ndim == 3\n        self.clear_probability_borders(data, self.patch_size)\n        total = data.sum()\n        if total == 0:\n            message = (\n                \'Empty probability map found\'\n                f\' ({self.probability_map_name})\'\n            )\n            raise RuntimeError(message)\n        data /= total  # normalize probabilities\n        return data\n\n    @staticmethod\n    def clear_probability_borders(\n            probability_map: np.ndarray,\n            patch_size: TypePatchSize,\n            ) -> None:\n        # Set probability to 0 on voxels that wouldn\'t possibly be sampled given\n        # the current patch size\n        # We will arbitrarily define the center of an array with even length\n        # using the // Python operator\n        # For example, the center of an array (3, 4) will be on (1, 2)\n        #\n        #   Patch         center\n        #  . . . .        . . . .\n        #  . . . .   ->   . . x .\n        #  . . . .        . . . .\n        #\n        #\n        #    Prob. map      After preprocessing\n        #\n        #  x x x x x x x       . . . . . . .\n        #  x x x x x x x       . . x x x x .\n        #  x x x x x x x  -->  . . x x x x .\n        #  x x x x x x x  -->  . . x x x x .\n        #  x x x x x x x       . . x x x x .\n        #  x x x x x x x       . . . . . . .\n        #\n        # The dots represent removed probabilities, x mark possible locations\n        crop_ini = patch_size // 2\n        crop_fin = (patch_size - 1) // 2\n        crop_i, crop_j, crop_k = crop_ini\n        probability_map[:crop_i, :, :] = 0\n        probability_map[:, :crop_j, :] = 0\n        probability_map[:, :, :crop_k] = 0\n\n        # The call tolist() is very important. Using np.uint16 as negative index\n        # will not work because e.g. -np.uint16(2) == 65534\n        crop_i, crop_j, crop_k = crop_fin.tolist()\n        if crop_i:\n            probability_map[-crop_i:, :, :] = 0\n        if crop_j:\n            probability_map[:, -crop_j:, :] = 0\n        if crop_k:\n            probability_map[:, :, -crop_k:] = 0\n\n    @staticmethod\n    def get_cumulative_distribution_function(\n            probability_map: np.ndarray,\n            ) -> Tuple[np.ndarray, np.ndarray]:\n        """"""Return the CDF of a probability map.\n\n        The cumulative distribution function (CDF) is computed as follows:\n\n        1. Flatten probability map\n        2. Compute sorting indices\n        3. Sort flattened map\n        4. Compute cumulative sum\n\n        For example,\n        if the probability map is [0.0, 0.0, 0.1, 0.2, 0.5, 0.1, 0.1, 0.0],\n        the sorting indices are [0, 1, 7, 2, 5, 6, 3, 4],\n        the sorted map is [0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2, 0.5],\n        and the CDF is [0.0, 0.0, 0.0, 0.1, 0.2, 0.3, 0.5, 1.0].\n        """"""\n        flat_map = probability_map.flatten()\n        flat_map_normalized = flat_map / flat_map.sum()\n        # Get the sorting indices to that we can invert the sorting later on\n        sort_indices = np.argsort(flat_map_normalized)\n        flat_map_normalized_sorted = flat_map_normalized[sort_indices]\n        cdf = np.cumsum(flat_map_normalized_sorted)\n        return cdf, sort_indices\n\n    def extract_patch(\n            self,\n            sample: Subject,\n            probability_map: np.ndarray,\n            cdf: np.ndarray,\n            sort_indices: np.ndarray,\n            ) -> Subject:\n        index_ini = self.get_random_index_ini(probability_map, cdf, sort_indices)\n        crop = self.get_crop_transform(\n            sample.spatial_shape,\n            index_ini,\n            self.patch_size,\n        )\n        cropped_sample = crop(sample)\n        cropped_sample[\'index_ini\'] = index_ini.astype(int)\n        return cropped_sample\n\n    def get_random_index_ini(\n            self,\n            probability_map: np.ndarray,\n            cdf: np.ndarray,\n            sort_indices: np.ndarray,\n            ) -> np.ndarray:\n        center = self.sample_probability_map(probability_map, cdf, sort_indices)\n        assert np.all(center >= 0)\n        # See self.clear_probability_borders\n        index_ini = center - self.patch_size // 2\n        assert np.all(index_ini >= 0)\n        return index_ini\n\n    def sample_probability_map(\n            self,\n            probability_map: np.ndarray,\n            cdf: np.ndarray,\n            sort_indices: np.ndarray,\n            ) -> np.ndarray:\n        """"""Inverse transform sampling.\n\n        Example:\n            >>> probability_map = np.array(\n            ...    ((0,0,1,1,5,2,1,1,0),\n            ...     (2,2,2,2,2,2,2,2,2)))\n            >>> probability_map\n            array([[0, 0, 1, 1, 5, 2, 1, 1, 0],\n                   [2, 2, 2, 2, 2, 2, 2, 2, 2]])\n            >>> histogram = np.zeros_like(probability_map)\n            >>> for _ in range(100000):\n            ...     histogram[sample_probability_map(probability_map)] += 1\n            ...\n            >>> histogram\n            array([[    0,     0,  3479,  3478, 17121,  7023,  3355,  3378,     0],\n                   [ 6808,  6804,  6942,  6809,  6946,  6988,  7002,  6826,  7041]])\n\n        """"""\n        # Get first value larger than random number\n        random_number = torch.rand(1).item()\n        # If probability map is float32, cdf.max() can be far from 1, e.g. 0.92\n        if random_number > cdf.max():\n            cdf_index = -1\n        else:  # proceed as usual\n            cdf_index = np.argmax(random_number < cdf)\n\n        random_location_index = sort_indices[cdf_index]\n        center = np.unravel_index(\n            random_location_index,\n            probability_map.shape\n        )\n\n        i, j, k = center\n        probability = probability_map[i, j, k]\n        assert probability > 0\n\n        center = np.array(center).astype(int)\n        return center\n'"
torchio/datasets/mni/__init__.py,0,b'from .sheep import Sheep\nfrom .colin import Colin27\nfrom .pediatric import Pediatric\n'
torchio/datasets/mni/colin.py,0,"b'import urllib.parse\nfrom torchvision.datasets.utils import download_and_extract_archive\nfrom ...utils import get_torchio_cache_dir\nfrom ... import Image, LABEL\nfrom .mni import SubjectMNI\n\n\nclass Colin27(SubjectMNI):\n    """"""Colin27 MNI template.\n\n    Arguments:\n        version: Template year. It can ``1998`` or ``2008``.\n    """"""\n    def __init__(self, version=1998):\n        if version not in (1998, 2008):\n            raise ValueError(f\'Version must be 1998 or 2008, not ""{version}""\')\n        self.name = f\'mni_colin27_{version}_nifti\'\n        self.url_dir = urllib.parse.urljoin(self.url_base, \'colin27/\')\n        self.filename = f\'{self.name}.zip\'\n        self.url = urllib.parse.urljoin(self.url_dir, self.filename)\n        download_root = get_torchio_cache_dir() / self.name\n        if download_root.is_dir():\n            print(f\'Using cache found in {download_root}\')\n        else:\n            download_and_extract_archive(\n                self.url,\n                download_root=download_root,\n                filename=self.filename,\n            )\n\n        if version == 1998:\n            t1, head, mask = [\n                download_root / f\'colin27_t1_tal_lin{suffix}.nii\'\n                for suffix in (\'\', \'_headmask\', \'_mask\')\n            ]\n            super().__init__(\n                t1=Image(t1),\n                head=Image(head, type=LABEL),\n                brain=Image(mask, type=LABEL),\n            )\n        elif version == 2008:\n            t1, t2, pd, label = [\n                download_root / f\'colin27_{name}_tal_hires.nii\'\n                for name in (\'t1\', \'t2\', \'pd\', \'cls\')\n            ]\n            super().__init__(\n                t1=Image(t1),\n                t2=Image(t2),\n                pd=Image(pd),\n                cls=Image(label, type=LABEL),\n            )\n'"
torchio/datasets/mni/mni.py,0,"b'from ... import Subject\n\n\nclass SubjectMNI(Subject):\n    """"""Atlases from the Montreal Neurological Institute (MNI).\n\n    See `the website <http://nist.mni.mcgill.ca/?page_id=714>`_\n    for more information.\n    """"""\n    url_base = \'http://packages.bic.mni.mcgill.ca/mni-models/\'\n'"
torchio/datasets/mni/pediatric.py,0,"b'import urllib.parse\nfrom torchvision.datasets.utils import download_and_extract_archive\nfrom ...utils import get_torchio_cache_dir\nfrom ... import Image, LABEL\nfrom .mni import SubjectMNI\n\n\nSUPPORTED_YEARS = (\n    (4.5, 18.5),\n    (4.5, 8.5),\n    (7, 11),\n    (7.5, 13.5),\n    (10, 14),\n    (13, 18.5),\n)\n\ndef format_age(n):\n    integer = int(n)\n    decimal = int(10 * (n - integer))\n    return f\'{integer:02d}.{decimal}\'\n\n\nclass Pediatric(SubjectMNI):\n    """"""MNI pediatric atlases.\n\n    See `the website <http://nist.mni.mcgill.ca/?p=974>`_ for more information.\n\n    Arguments:\n        years: Tuple of 2 years. Possible values are: ``[(4.5, 18.5),\n            (4.5, 8.5),\n            (7, 11),\n            (7.5, 13.5),\n            (10, 14),\n            (13, 18.5)]``.\n        symmetric: If ``True`` the left-right symmetric templates will be used.\n            If ``False``, the asymmetric (natural) templates will be used.\n    """"""\n    def __init__(self, years, symmetric=False):\n        self.url_dir = \'http://www.bic.mni.mcgill.ca/~vfonov/nihpd/obj1/\'\n        sym_string = \'sym\' if symmetric else \'asym\'\n        if not isinstance(years, tuple) or years not in SUPPORTED_YEARS:\n            message = f\'Years must be a tuple in {SUPPORTED_YEARS}\'\n            raise ValueError(message)\n        a, b = years\n        file_id = f\'{sym_string}_{format_age(a)}-{format_age(b)}\'\n        self.name = f\'nihpd_{file_id}_nifti\'\n        self.filename = f\'{self.name}.zip\'\n        self.url = urllib.parse.urljoin(self.url_dir, self.filename)\n        download_root = get_torchio_cache_dir() / self.name\n        if download_root.is_dir():\n            print(f\'Using cache found in {download_root}\')\n        else:\n            download_and_extract_archive(\n                self.url,\n                download_root=download_root,\n                filename=self.filename,\n            )\n        super().__init__(\n            t1=Image(download_root / f\'nihpd_{file_id}_t1w.nii\'),\n            t2=Image(download_root / f\'nihpd_{file_id}_t2w.nii\'),\n            pd=Image(download_root / f\'nihpd_{file_id}_pdw.nii\'),\n            mask=Image(download_root / f\'nihpd_{file_id}_pdw.nii\', type=LABEL),\n        )\n'"
torchio/datasets/mni/sheep.py,0,"b""import urllib.parse\nfrom torchvision.datasets.utils import download_and_extract_archive\nfrom ...utils import get_torchio_cache_dir\nfrom ... import Image\nfrom .mni import SubjectMNI\n\n\nclass Sheep(SubjectMNI):\n    def __init__(self, version=1998):\n        self.name = 'NIFTI_ovine_05mm'\n        self.url_dir = urllib.parse.urljoin(self.url_base, 'sheep/')\n        self.filename = f'{self.name}.zip'\n        self.url = urllib.parse.urljoin(self.url_dir, self.filename)\n        download_root = get_torchio_cache_dir() / self.name\n        if download_root.is_dir():\n            print(f'Using cache found in {download_root}')\n        else:\n            download_and_extract_archive(\n                self.url,\n                download_root=download_root,\n                filename=self.filename,\n            )\n        t1_path = download_root / 'ovine_model_05.nii'\n        super().__init__(\n            t1=Image(t1_path)\n        )\n"""
torchio/transforms/augmentation/__init__.py,0,"b'from ..interpolation import Interpolation, get_sitk_interpolator\nfrom .random_transform import RandomTransform\n'"
torchio/transforms/augmentation/composition.py,2,"b'from typing import Union, Sequence\n\nimport torch\nimport numpy as np\nfrom torchvision.transforms import Compose as PyTorchCompose\n\nfrom ...data.subject import Subject\nfrom .. import Transform\nfrom . import RandomTransform\n\n\nclass Compose(Transform):\n    """"""Compose several transforms together.\n\n    Args:\n        transforms: Sequence of instances of\n            :py:class:`~torchio.transforms.transform.Transform`.\n        p: Probability that this transform will be applied.\n\n    .. note::\n        This is a thin wrapper of :py:class:`torchvision.transforms.Compose`.\n    """"""\n    def __init__(self, transforms: Sequence[Transform], p: float = 1):\n        super().__init__(p=p)\n        self.transform = PyTorchCompose(transforms)\n\n    def apply_transform(self, sample: Subject):\n        return self.transform(sample)\n\n\nclass OneOf(RandomTransform):\n    """"""Apply only one of the given transforms.\n\n    Args:\n        transforms: Dictionary with instances of\n            :py:class:`~torchio.transforms.transform.Transform` as keys and\n            probabilities as values. Probabilities are normalized so they sum\n            to one. If a sequence is given, the same probability will be\n            assigned to each transform.\n        p: Probability that this transform will be applied.\n\n    Example:\n        >>> import torchio\n        >>> ixi = torchio.datasets.ixi.IXITiny(\'ixi\', download=True)\n        >>> sample = ixi[0]\n        >>> transforms_dict = {\n        ...     torchio.transforms.RandomAffine(): 0.75,\n        ...     torchio.transforms.RandomElasticDeformation(): 0.25,\n        ... }  # Using 3 and 1 as probabilities would have the same effect\n        >>> transform = torchio.transforms.OneOf(transforms_dict)\n\n    """"""\n    def __init__(\n            self,\n            transforms: Union[dict, Sequence[Transform]],\n            p: float = 1,\n            ):\n        super().__init__(p=p)\n        self.transforms_dict = self._get_transforms_dict(transforms)\n\n    def apply_transform(self, sample: Subject):\n        weights = torch.Tensor(list(self.transforms_dict.values()))\n        index = torch.multinomial(weights, 1)\n        transforms = list(self.transforms_dict.keys())\n        transform = transforms[index]\n        transformed = transform(sample)\n        return transformed\n\n    def _get_transforms_dict(self, transforms: Union[dict, Sequence]):\n        if isinstance(transforms, dict):\n            transforms_dict = dict(transforms)\n            self._normalize_probabilities(transforms_dict)\n        else:\n            try:\n                p = 1 / len(transforms)\n            except TypeError as e:\n                message = (\n                    \'Transforms argument must be a dictionary or a sequence,\'\n                    f\' not {type(transforms)}\'\n                )\n                raise ValueError(message) from e\n            transforms_dict = {transform: p for transform in transforms}\n        for transform in transforms_dict:\n            if not isinstance(transform, Transform):\n                message = (\n                    \'All keys in transform_dict must be instances of\'\n                    f\'torchio.Transform, not ""{type(transform)}""\'\n                )\n                raise ValueError(message)\n        return transforms_dict\n\n    @staticmethod\n    def _normalize_probabilities(transforms_dict: dict):\n        probabilities = np.array(list(transforms_dict.values()), dtype=float)\n        if np.any(probabilities < 0):\n            message = (\n                \'Probabilities must be greater or equal to zero,\'\n                f\' not ""{probabilities}""\'\n            )\n            raise ValueError(message)\n        if np.all(probabilities == 0):\n            message = (\n                \'At least one probability must be greater than zero,\'\n                f\' but they are ""{probabilities}""\'\n            )\n            raise ValueError(message)\n        for transform, probability in transforms_dict.items():\n            transforms_dict[transform] = probability / probabilities.sum()\n'"
torchio/transforms/augmentation/random_transform.py,1,"b'""""""\nThis is the docstring of random transform module\n""""""\n\nimport numbers\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport numpy as np\n\nfrom ...data.subject import Subject\nfrom ... import TypeNumber, TypeRangeFloat\nfrom .. import Transform\n\n\nclass RandomTransform(Transform):\n    """"""Base class for stochastic augmentation transforms.\n\n    Args:\n        p: Probability that this transform will be applied.\n        seed: Seed for :py:mod:`torch` random number generator.\n    """"""\n    def __init__(\n            self,\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p)\n        self._seed = seed\n\n    def __call__(self, sample: Subject):\n        self.check_seed()\n        return super().__call__(sample)\n\n    @staticmethod\n    def parse_range(\n            nums_range: Union[TypeNumber, Tuple[TypeNumber, TypeNumber]],\n            name: str,\n            ) -> Tuple[TypeNumber, TypeNumber]:\n        r""""""Adapted from ``torchvision.transforms.RandomRotation``.\n\n        Args:\n            nums_range: Tuple of two numbers :math:`(n_{min}, n_{max})`,\n                where :math:`n_{min} \\leq n_{max}`.\n                If a single positive number :math:`n` is provided,\n                :math:`n_{min} = -n` and :math:`n_{max} = n`.\n            name: Name of the parameter, so that an informative error message\n                can be printed.\n\n        Returns:\n            A tuple of two numbers :math:`(n_{min}, n_{max})`.\n\n        Raises:\n            ValueError: if :attr:`nums_range` is negative\n            ValueError: if :math:`n_{max} \\lt n_{min}`.\n        """"""\n        if isinstance(nums_range, numbers.Number):\n            if nums_range < 0:\n                raise ValueError(\n                    f\'If {name} is a single number,\'\n                    f\' it must be positive, not {nums_range}\')\n            return (-nums_range, nums_range)\n\n        if len(nums_range) != 2:\n            raise ValueError(\n                f\'If {name} is a sequence,\'\n                f\' it must be of len 2, not {nums_range}\')\n        min_degree, max_degree = nums_range\n        if min_degree > max_degree:\n            raise ValueError(\n                f\'If {name} is a sequence, the second value must be\'\n                f\' equal or greater than the first, not {nums_range}\')\n        return nums_range\n\n    def parse_degrees(\n            self,\n            degrees: TypeRangeFloat,\n            ) -> Tuple[float, float]:\n        return self.parse_range(degrees, \'degrees\')\n\n    def parse_translation(\n            self,\n            translation: TypeRangeFloat,\n            ) -> Tuple[float, float]:\n        return self.parse_range(translation, \'translation\')\n\n    def check_seed(self) -> None:\n        if self._seed is not None:\n            torch.manual_seed(self._seed)\n\n    @staticmethod\n    def fourier_transform(array: np.ndarray):\n        transformed = np.fft.fftn(array)\n        fshift = np.fft.fftshift(transformed)\n        return fshift\n\n    @staticmethod\n    def inv_fourier_transform(fshift: np.ndarray):\n        f_ishift = np.fft.ifftshift(fshift)\n        img_back = np.fft.ifftn(f_ishift)\n        return np.abs(img_back)\n'"
torchio/transforms/preprocessing/__init__.py,0,"b'from .spatial.pad import Pad\nfrom .spatial.crop import Crop\nfrom .spatial.resample import Resample\nfrom .spatial.to_canonical import ToCanonical\nfrom .spatial.crop_or_pad import CropOrPad, CenterCropOrPad\n\nfrom .intensity.rescale import Rescale, RescaleIntensity\nfrom .intensity.z_normalization import ZNormalization\nfrom .intensity.histogram_standardization import HistogramStandardization\n'"
torchio/transforms/augmentation/intensity/__init__.py,0,b'from .random_swap import RandomSwap\nfrom .random_blur import RandomBlur\nfrom .random_noise import RandomNoise\nfrom .random_spike import RandomSpike\nfrom .random_motion import RandomMotion\nfrom .random_ghosting import RandomGhosting\nfrom .random_bias_field import RandomBiasField\n'
torchio/transforms/augmentation/intensity/random_bias_field.py,2,"b'\nfrom typing import Union, Tuple, Optional\nimport numpy as np\nimport torch\nfrom ....torchio import DATA, TypeData\nfrom ....data.subject import Subject\nfrom .. import RandomTransform\n\n\nclass RandomBiasField(RandomTransform):\n    r""""""Add random MRI bias field artifact.\n\n    Args:\n        coefficients: Magnitude :math:`n` of polynomial coefficients.\n            If a tuple :math:`(a, b)` is specified, then\n            :math:`n \\sim \\mathcal{U}(a, b)`.\n        order: Order of the basis polynomial functions.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n    """"""\n    def __init__(\n            self,\n            coefficients: Union[float, Tuple[float, float]] = 0.5,\n            order: int = 3,\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.coefficients_range = self.parse_range(\n            coefficients, \'coefficients_range\')\n        self.order = order\n\n    def apply_transform(self, sample: Subject) -> dict:\n        random_parameters_images_dict = {}\n        for image_name, image_dict in sample.get_images_dict().items():\n            coefficients = self.get_params(\n                self.order,\n                self.coefficients_range,\n            )\n            random_parameters_dict = {\'coefficients\': coefficients}\n            random_parameters_images_dict[image_name] = random_parameters_dict\n\n            bias_field = self.generate_bias_field(\n                image_dict[DATA], self.order, coefficients)\n            image_with_bias = image_dict[DATA] * torch.from_numpy(bias_field)\n            image_dict[DATA] = image_with_bias\n        sample.add_transform(self, random_parameters_images_dict)\n        return sample\n\n    @staticmethod\n    def get_params(\n            order: int,\n            coefficients_range: Tuple[float, float],\n            ) -> Tuple[bool, np.ndarray]:\n        # Sampling of the appropriate number of coefficients for the creation\n        # of the bias field map\n        random_coefficients = []\n        for x_order in range(0, order + 1):\n            for y_order in range(0, order + 1 - x_order):\n                for _ in range(0, order + 1 - (x_order + y_order)):\n                    number = torch.FloatTensor(1).uniform_(*coefficients_range)\n                    random_coefficients.append(number.item())\n        return np.array(random_coefficients)\n\n    @staticmethod\n    def generate_bias_field(\n            data: TypeData,\n            order: int,\n            coefficients: TypeData,\n            ) -> np.ndarray:\n        # Create the bias field map using a linear combination of polynomial\n        # functions and the coefficients previously sampled\n        shape = np.array(data.shape[1:])  # first axis is channels\n        half_shape = shape / 2\n\n        ranges = [np.arange(-n, n) for n in half_shape]\n\n        bias_field = np.zeros(shape)\n        x_mesh, y_mesh, z_mesh = np.asarray(np.meshgrid(*ranges))\n\n        x_mesh /= x_mesh.max()\n        y_mesh /= y_mesh.max()\n        z_mesh /= z_mesh.max()\n\n        i = 0\n        for x_order in range(order + 1):\n            for y_order in range(order + 1 - x_order):\n                for z_order in range(order + 1 - (x_order + y_order)):\n                    random_coefficient = coefficients[i]\n                    new_map = (\n                        random_coefficient\n                        * x_mesh ** x_order\n                        * y_mesh ** y_order\n                        * z_mesh ** z_order\n                    )\n                    bias_field += np.transpose(new_map, (1, 0, 2))  # why?\n                    i += 1\n        bias_field = np.exp(bias_field).astype(np.float32)\n        return bias_field\n'"
torchio/transforms/augmentation/intensity/random_blur.py,3,"b'from typing import Union, Tuple, Optional\nimport torch\nimport numpy as np\nimport SimpleITK as sitk\nfrom ....utils import nib_to_sitk, sitk_to_nib\nfrom ....torchio import DATA, AFFINE, TypeData\nfrom ....data.subject import Subject\nfrom .. import RandomTransform\n\n\nclass RandomBlur(RandomTransform):\n    r""""""Blur an image using a random-sized Gaussian filter.\n\n    Args:\n        std: Tuple :math:`(a, b)` to compute the standard deviations\n            :math:`(\\sigma_1, \\sigma_2, \\sigma_3)` of the Gaussian kernels used\n            to blur the image along each axis,\n            where :math:`\\sigma_i \\sim \\mathcal{U}(a, b)` mm.\n            If a single value :math:`n` is provided, then :math:`a = b = n`.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n    """"""\n    def __init__(\n            self,\n            std: Union[float, Tuple[float, float]] = (0, 4),\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.std_range = self.parse_range(std, \'std\')\n        if any(np.array(self.std_range) < 0):\n            message = (\n                \'Standard deviation std must greater or equal to zero,\'\n                f\' not ""{self.std_range}""\'\n            )\n            raise ValueError(message)\n\n    def apply_transform(self, sample: Subject) -> dict:\n        random_parameters_images_dict = {}\n        for image_name, image_dict in sample.get_images_dict().items():\n            std = self.get_params(self.std_range)\n            random_parameters_dict = {\'std\': std}\n            random_parameters_images_dict[image_name] = random_parameters_dict\n            image_dict[DATA][0] = blur(\n                image_dict[DATA][0],\n                image_dict[AFFINE],\n                std,\n            )\n        sample.add_transform(self, random_parameters_images_dict)\n        return sample\n\n    @staticmethod\n    def get_params(std_range: Tuple[float, float]) -> np.ndarray:\n        std = torch.FloatTensor(3).uniform_(*std_range).numpy()\n        return std\n\n\ndef blur(data: TypeData, affine: TypeData, std: np.ndarray) -> torch.Tensor:\n    image = nib_to_sitk(data, affine)\n    image = sitk.DiscreteGaussian(image, std.tolist())\n    array, _ = sitk_to_nib(image)\n    tensor = torch.from_numpy(array)\n    return tensor\n'"
torchio/transforms/augmentation/intensity/random_ghosting.py,4,"b'import warnings\nfrom typing import Tuple, Optional, Union\nimport torch\nimport numpy as np\nimport SimpleITK as sitk\nfrom ....torchio import DATA, AFFINE\nfrom ....data.subject import Subject\nfrom .. import RandomTransform\n\n\nclass RandomGhosting(RandomTransform):\n    r""""""Add random MRI ghosting artifact.\n\n    Args:\n        num_ghosts: Number of \'ghosts\' :math:`n` in the image.\n            If :py:attr:`num_ghosts` is a tuple :math:`(a, b)`, then\n            :math:`n \\sim \\mathcal{U}(a, b) \\cap \\mathbb{N}`.\n        axes: Axis along which the ghosts will be created. If\n            :py:attr:`axes` is a tuple, the axis will be randomly chosen\n            from the passed values.\n        intensity: Number between 0 and 1 representing the artifact strength\n            :math:`s`. If ``0``, the ghosts will not be visible. If a tuple\n            :math:`(a, b)`, is provided then\n            :math:`s \\sim \\mathcal{U}(a, b)`.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n\n    .. note:: The execution time of this transform does not depend on the\n        number of ghosts.\n    """"""\n    def __init__(\n            self,\n            num_ghosts: Union[int, Tuple[int, int]] = (4, 10),\n            axes: Union[int, Tuple[int, ...]] = (0, 1, 2),\n            intensity: Union[float, Tuple[float, float]] = (0.5, 1),\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        if not isinstance(axes, tuple):\n            try:\n                axes = tuple(axes)\n            except TypeError:\n                axes = (axes,)\n        for axis in axes:\n            if axis not in (0, 1, 2):\n                raise ValueError(f\'Axes must be in (0, 1, 2), not ""{axes}""\')\n        self.axes = axes\n        if isinstance(num_ghosts, int):\n            self.num_ghosts_range = num_ghosts, num_ghosts\n        elif isinstance(num_ghosts, tuple) and len(num_ghosts) == 2:\n            self.num_ghosts_range = num_ghosts\n        self.intensity_range = self.parse_range(intensity, \'intensity\')\n        for n in self.intensity_range:\n            if not 0 <= n <= 1:\n                message = (\n                    f\'Intensity must be a number between 0 and 1, not {n}\')\n                raise ValueError(message)\n\n    def apply_transform(self, sample: Subject) -> dict:\n        random_parameters_images_dict = {}\n        for image_name, image_dict in sample.get_images_dict().items():\n            data = image_dict[DATA]\n            is_2d = data.shape[-3] == 1\n            axes = [a for a in self.axes if a != 0] if is_2d else self.axes\n            params = self.get_params(\n                self.num_ghosts_range,\n                axes,\n                self.intensity_range,\n            )\n            num_ghosts_param, axis_param, intensity_param = params\n            random_parameters_dict = {\n                \'axis\': axis_param,\n                \'num_ghosts\': num_ghosts_param,\n                \'intensity\': intensity_param,\n            }\n            random_parameters_images_dict[image_name] = random_parameters_dict\n            if (data[0] < -0.1).any():\n                # I use -0.1 instead of 0 because Python was warning me when\n                # a value in a voxel was -7.191084e-35\n                # There must be a better way of solving this\n                message = (\n                    f\'Image ""{image_name}"" from ""{image_dict[""stem""]}""\'\n                    \' has negative values.\'\n                    \' Results can be unexpected because the transformed sample\'\n                    \' is computed as the absolute values\'\n                    \' of an inverse Fourier transform\'\n                )\n                warnings.warn(message)\n            image = self.nib_to_sitk(\n                data[0],\n                image_dict[AFFINE],\n            )\n            data = self.add_artifact(\n                image,\n                num_ghosts_param,\n                axis_param,\n                intensity_param,\n            )\n            # Add channels dimension\n            data = data[np.newaxis, ...]\n            image_dict[DATA] = torch.from_numpy(data)\n        sample.add_transform(self, random_parameters_images_dict)\n        return sample\n\n    @staticmethod\n    def get_params(\n            num_ghosts_range: Tuple[int, int],\n            axes: Tuple[int, ...],\n            intensity_range: Tuple[float, float],\n            ) -> Tuple:\n        ng_min, ng_max = num_ghosts_range\n        num_ghosts = torch.randint(ng_min, ng_max + 1, (1,)).item()\n        axis = axes[torch.randint(0, len(axes), (1,))]\n        intensity = torch.FloatTensor(1).uniform_(*intensity_range).item()\n        return num_ghosts, axis, intensity\n\n    @staticmethod\n    def get_axis_and_size(axis, array):\n        if axis == 1:\n            axis = 0\n            size = array.shape[0]\n        elif axis == 0:\n            axis = 1\n            size = array.shape[1]\n        elif axis == 2:  # we will also traverse in sagittal (if RAS)\n            size = array.shape[0]\n        else:\n            raise RuntimeError(f\'Axis ""{axis}"" is not valid\')\n        return axis, size\n\n    @staticmethod\n    def get_slice(axis, array, slice_idx):\n        # Comments apply if RAS\n        if axis == 0:  # sagittal (columns) - artifact AP\n            image_slice = array[slice_idx, ...]\n        elif axis == 1:  # coronal (columns) - artifact LR\n            image_slice = array[:, slice_idx, :]\n        elif axis == 2:  # sagittal (rows) - artifact IS\n            image_slice = array[slice_idx, ...].T\n        else:\n            raise RuntimeError(f\'Axis ""{axis}"" is not valid\')\n        return image_slice\n\n    def add_artifact(\n            self,\n            image: sitk.Image,\n            num_ghosts: int,\n            axis: int,\n            intensity: float,\n            ):\n        array = sitk.GetArrayFromImage(image).transpose()\n        # Leave first 5% of frequencies untouched. If the image is in RAS\n        # orientation, this helps applying the ghosting in the desired axis\n        # intuitively\n        # [Why? I forgot]\n        percentage_to_avoid = 0.05\n        axis, size = self.get_axis_and_size(axis, array)\n        for slice_idx in range(size):\n            image_slice = self.get_slice(axis, array, slice_idx)\n            spectrum = self.fourier_transform(image_slice)\n            for row_idx, row in enumerate(spectrum):\n                if row_idx % num_ghosts:\n                    continue\n                progress = row_idx / array.shape[0]\n                if np.abs(progress - 0.5) < percentage_to_avoid / 2:\n                    continue\n                row *= 1 - intensity\n            image_slice *= 0\n            image_slice += self.inv_fourier_transform(spectrum)\n        return array\n'"
torchio/transforms/augmentation/intensity/random_motion.py,4,"b'""""""\nCustom implementation of\n\n    Shaw et al., 2019\n    MRI k-Space Motion Artefact Augmentation:\n    Model Robustness and Task-Specific Uncertainty\n\n""""""\n\nimport warnings\nfrom typing import Tuple, Optional, List\nimport torch\nimport numpy as np\nimport SimpleITK as sitk\nfrom ....torchio import DATA, AFFINE\nfrom ....data.subject import Subject\nfrom .. import Interpolation, get_sitk_interpolator\nfrom .. import RandomTransform\n\n\nclass RandomMotion(RandomTransform):\n    r""""""Add random MRI motion artifact.\n\n    Custom implementation of `Shaw et al. 2019, MRI k-Space Motion Artefact\n    Augmentation: Model Robustness and Task-Specific\n    Uncertainty <http://proceedings.mlr.press/v102/shaw19a.html>`_.\n\n    Args:\n        degrees: Tuple :math:`(a, b)` defining the rotation range in degrees of\n            the simulated movements. The rotation angles around each axis are\n            :math:`(\\theta_1, \\theta_2, \\theta_3)`,\n            where :math:`\\theta_i \\sim \\mathcal{U}(a, b)`.\n            If only one value :math:`d` is provided,\n            :math:`\\theta_i \\sim \\mathcal{U}(-d, d)`.\n            Larger values generate more distorted images.\n        translation: Tuple :math:`(a, b)` defining the translation in mm of\n            the simulated movements. The translations along each axis are\n            :math:`(t_1, t_2, t_3)`,\n            where :math:`t_i \\sim \\mathcal{U}(a, b)`.\n            If only one value :math:`t` is provided,\n            :math:`t_i \\sim \\mathcal{U}(-t, t)`.\n            Larger values generate more distorted images.\n        num_transforms: Number of simulated movements.\n            Larger values generate more distorted images.\n        image_interpolation: See :ref:`Interpolation`.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n\n    .. warning:: Large numbers of movements lead to longer execution times for\n        3D images.\n    """"""\n    def __init__(\n            self,\n            degrees: float = 10,\n            translation: float = 10,  # in mm\n            num_transforms: int = 2,\n            image_interpolation: str = \'linear\',\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.degrees_range = self.parse_degrees(degrees)\n        self.translation_range = self.parse_translation(translation)\n        self.num_transforms = num_transforms\n        self.image_interpolation = self.parse_interpolation(image_interpolation)\n\n    def apply_transform(self, sample: Subject) -> dict:\n        random_parameters_images_dict = {}\n        for image_name, image_dict in sample.get_images_dict().items():\n            data = image_dict[DATA]\n            is_2d = data.shape[-3] == 1\n            params = self.get_params(\n                self.degrees_range,\n                self.translation_range,\n                self.num_transforms,\n                is_2d=is_2d,\n            )\n            times_params, degrees_params, translation_params = params\n            random_parameters_dict = {\n                \'times\': times_params,\n                \'degrees\': degrees_params,\n                \'translation\': translation_params,\n            }\n            random_parameters_images_dict[image_name] = random_parameters_dict\n            if (data[0] < -0.1).any():\n                # I use -0.1 instead of 0 because Python was warning me when\n                # a value in a voxel was -7.191084e-35\n                # There must be a better way of solving this\n                message = (\n                    f\'Image ""{image_name}"" from ""{image_dict[""stem""]}""\'\n                    \' has negative values.\'\n                    \' Results can be unexpected because the transformed sample\'\n                    \' is computed as the absolute values\'\n                    \' of an inverse Fourier transform\'\n                )\n                warnings.warn(message)\n            image = self.nib_to_sitk(\n                data[0],\n                image_dict[AFFINE],\n            )\n            transforms = self.get_rigid_transforms(\n                degrees_params,\n                translation_params,\n                image,\n            )\n            data = self.add_artifact(\n                image,\n                transforms,\n                times_params,\n                self.image_interpolation,\n            )\n            # Add channels dimension\n            data = data[np.newaxis, ...]\n            image_dict[DATA] = torch.from_numpy(data)\n        sample.add_transform(self, random_parameters_images_dict)\n        return sample\n\n    @staticmethod\n    def get_params(\n            degrees_range: Tuple[float, float],\n            translation_range: Tuple[float, float],\n            num_transforms: int,\n            perturbation: float = 0.3,\n            is_2d: bool = False,\n            ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, bool]:\n        # If perturbation is 0, time intervals between movements are constant\n        degrees_params = get_params_array(\n            degrees_range, num_transforms)\n        translation_params = get_params_array(\n            translation_range, num_transforms)\n        if is_2d:  # imagine sagittal (1, A, S)\n            degrees_params[:, -2:] = 0  # rotate around R axis only\n            translation_params[:, 0] = 0  # translate in AS plane only\n        step = 1 / (num_transforms + 1)\n        times = torch.arange(0, 1, step)[1:]\n        noise = torch.FloatTensor(num_transforms)\n        noise.uniform_(-step * perturbation, step * perturbation)\n        times += noise\n        times_params = times.numpy()\n        return times_params, degrees_params, translation_params\n\n    def get_rigid_transforms(\n            self,\n            degrees_params: np.ndarray,\n            translation_params: np.ndarray,\n            image: sitk.Image,\n            ) -> List[sitk.Euler3DTransform]:\n        center_ijk = np.array(image.GetSize()) / 2\n        center_lps = image.TransformContinuousIndexToPhysicalPoint(center_ijk)\n        identity = np.eye(4)\n        matrices = [identity]\n        for degrees, translation in zip(degrees_params, translation_params):\n            radians = np.radians(degrees).tolist()\n            motion = sitk.Euler3DTransform()\n            motion.SetCenter(center_lps)\n            motion.SetRotation(*radians)\n            motion.SetTranslation(translation.tolist())\n            motion_matrix = self.transform_to_matrix(motion)\n            matrices.append(motion_matrix)\n        transforms = [self.matrix_to_transform(m) for m in matrices]\n        return transforms\n\n    @staticmethod\n    def transform_to_matrix(transform: sitk.Euler3DTransform) -> np.ndarray:\n        matrix = np.eye(4)\n        rotation = np.array(transform.GetMatrix()).reshape(3, 3)\n        matrix[:3, :3] = rotation\n        matrix[:3, 3] = transform.GetTranslation()\n        return matrix\n\n    @staticmethod\n    def matrix_to_transform(matrix: np.ndarray) -> sitk.Euler3DTransform:\n        transform = sitk.Euler3DTransform()\n        rotation = matrix[:3, :3].flatten().tolist()\n        transform.SetMatrix(rotation)\n        transform.SetTranslation(matrix[:3, 3])\n        return transform\n\n    @staticmethod\n    def resample_images(\n            image: sitk.Image,\n            transforms: List[sitk.Euler3DTransform],\n            interpolation: Interpolation,\n            ) -> List[sitk.Image]:\n        floating = reference = image\n        default_value = np.float64(sitk.GetArrayViewFromImage(image).min())\n        transforms = transforms[1:]  # first is identity\n        images = [image]  # first is identity\n        for transform in transforms:\n            resampler = sitk.ResampleImageFilter()\n            resampler.SetInterpolator(get_sitk_interpolator(interpolation))\n            resampler.SetReferenceImage(reference)\n            resampler.SetOutputPixelType(sitk.sitkFloat32)\n            resampler.SetDefaultPixelValue(default_value)\n            resampler.SetTransform(transform)\n            resampled = resampler.Execute(floating)\n            images.append(resampled)\n        return images\n\n    @staticmethod\n    def sort_spectra(spectra: np.ndarray, times: np.ndarray):\n        """"""Use original spectrum to fill the center of k-space""""""\n        num_spectra = len(spectra)\n        if np.any(times > 0.5):\n            index = np.where(times > 0.5)[0].min()\n        else:\n            index = num_spectra - 1\n        spectra[0], spectra[index] = spectra[index], spectra[0]\n\n    def add_artifact(\n            self,\n            image: sitk.Image,\n            transforms: List[sitk.Euler3DTransform],\n            times: np.ndarray,\n            interpolation: Interpolation,\n            ):\n        images = self.resample_images(image, transforms, interpolation)\n        arrays = [sitk.GetArrayViewFromImage(im) for im in images]\n        arrays = [array.transpose() for array in arrays]  # ITK to NumPy\n        spectra = [self.fourier_transform(array) for array in arrays]\n        self.sort_spectra(spectra, times)\n        result_spectrum = np.empty_like(spectra[0])\n        last_index = result_spectrum.shape[2]\n        indices = (last_index * times).astype(int).tolist()\n        indices.append(last_index)\n        ini = 0\n        for spectrum, fin in zip(spectra, indices):\n            result_spectrum[..., ini:fin] = spectrum[..., ini:fin]\n            ini = fin\n        result_image = self.inv_fourier_transform(result_spectrum)\n        return result_image.astype(np.float32)\n\n\ndef get_params_array(nums_range: Tuple[float, float], num_transforms: int):\n    tensor = torch.FloatTensor(num_transforms, 3).uniform_(*nums_range)\n    return tensor.numpy()\n'"
torchio/transforms/augmentation/intensity/random_noise.py,4,"b'from typing import Tuple, Optional, Union\nimport torch\nimport numpy as np\nfrom ....torchio import DATA\nfrom ....data.subject import Subject\nfrom .. import RandomTransform\n\n\nclass RandomNoise(RandomTransform):\n    r""""""Add random Gaussian noise.\n\n    Args:\n        mean: Mean :math:`\\mu` of the Gaussian distribution\n            from which the noise is sampled.\n            If two values :math:`(a, b)` are provided,\n            then :math:`\\mu \\sim \\mathcal{U}(a, b)`.\n        std: Standard deviation :math:`\\sigma` of the Gaussian distribution\n            from which the noise is sampled.\n            If two values :math:`(a, b)` are provided,\n            then :math:`\\sigma \\sim \\mathcal{U}(a, b)`.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n    """"""\n    def __init__(\n            self,\n            mean: Union[float, Tuple[float, float]] = 0,\n            std: Union[float, Tuple[float, float]] = (0, 0.25),\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.mean_range = self.parse_range(mean, \'mean\')\n        self.std_range = self.parse_range(std, \'std\')\n        if any(np.array(self.std_range) < 0):\n            message = (\n                \'Standard deviation std must greater or equal to zero,\'\n                f\' not ""{self.std_range}""\'\n            )\n            raise ValueError(message)\n\n    def apply_transform(self, sample: Subject) -> dict:\n        random_parameters_images_dict = {}\n        for image_name, image_dict in sample.get_images_dict().items():\n            mean, std = self.get_params(self.mean_range, self.std_range)\n            random_parameters_dict = {\'std\': std}\n            random_parameters_images_dict[image_name] = random_parameters_dict\n            image_dict[DATA] = add_noise(image_dict[DATA], mean, std)\n        sample.add_transform(self, random_parameters_images_dict)\n        return sample\n\n    @staticmethod\n    def get_params(\n            mean_range: Tuple[float, float],\n            std_range: Tuple[float, float],\n            ) -> Tuple[float, float]:\n        mean = torch.FloatTensor(1).uniform_(*mean_range).item()\n        std = torch.FloatTensor(1).uniform_(*std_range).item()\n        return mean, std\n\n\ndef add_noise(tensor: torch.Tensor, mean: float, std: float) -> torch.Tensor:\n    noise = torch.FloatTensor(*tensor.shape).normal_(mean=mean, std=std)\n    tensor = tensor + noise\n    return tensor\n'"
torchio/transforms/augmentation/intensity/random_spike.py,4,"b'import warnings\nfrom typing import Tuple, Optional, Union\nimport torch\nimport numpy as np\nimport SimpleITK as sitk\nfrom ....torchio import DATA, AFFINE\nfrom ....data.subject import Subject\nfrom .. import RandomTransform\n\n\nclass RandomSpike(RandomTransform):\n    r""""""Add random MRI spike artifacts.\n\n    Args:\n        num_spikes: Number of spikes :math:`n` presnet in k-space.\n            If a tuple :math:`(a, b)` is provided, then\n            :math:`n \\sim \\mathcal{U}(a, b) \\cap \\mathbb{N}`.\n            Larger values generate more distorted images.\n        intensity: Ratio :math:`r` between the spike intensity and the maximum\n            of the spectrum.\n            Larger values generate more distorted images.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n\n    .. note:: The execution time of this transform does not depend on the\n        number of spikes.\n    """"""\n    def __init__(\n            self,\n            num_spikes: Union[int, Tuple[int, int]] = 1,\n            intensity: Union[float, Tuple[float, float]] = (0.1, 1),\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.intensity_range = self.parse_range(\n            intensity, \'intensity_range\')\n        if isinstance(num_spikes, int):\n            self.num_spikes_range = num_spikes, num_spikes\n        else:\n            self.num_spikes_range = num_spikes\n\n    def apply_transform(self, sample: Subject) -> dict:\n        random_parameters_images_dict = {}\n        for image_name, image_dict in sample.get_images_dict().items():\n            params = self.get_params(\n                self.num_spikes_range,\n                self.intensity_range,\n            )\n            spikes_positions_param, intensity_param = params\n            random_parameters_dict = {\n                \'intensity\': intensity_param,\n                \'spikes_positions\': spikes_positions_param,\n            }\n            random_parameters_images_dict[image_name] = random_parameters_dict\n            if (image_dict[DATA][0] < -0.1).any():\n                # I use -0.1 instead of 0 because Python was warning me when\n                # a value in a voxel was -7.191084e-35\n                # There must be a better way of solving this\n                message = (\n                    f\'Image ""{image_name}"" from ""{image_dict[""stem""]}""\'\n                    \' has negative values.\'\n                    \' Results can be unexpected because the transformed sample\'\n                    \' is computed as the absolute values\'\n                    \' of an inverse Fourier transform\'\n                )\n                warnings.warn(message)\n            image = self.nib_to_sitk(\n                image_dict[DATA][0],\n                image_dict[AFFINE],\n            )\n            image_dict[DATA] = self.add_artifact(\n                image,\n                spikes_positions_param,\n                intensity_param,\n            )\n            # Add channels dimension\n            image_dict[DATA] = image_dict[DATA][np.newaxis, ...]\n            image_dict[DATA] = torch.from_numpy(image_dict[DATA])\n        sample.add_transform(self, random_parameters_images_dict)\n        return sample\n\n    @staticmethod\n    def get_params(\n            num_spikes_range: Tuple[int, int],\n            intensity_range: Tuple[float, float],\n            ) -> Tuple:\n        ns_min, ns_max = num_spikes_range\n        num_spikes_param = torch.randint(ns_min, ns_max + 1, (1,)).item()\n        intensity_param = torch.FloatTensor(1).uniform_(*intensity_range)\n        spikes_positions = torch.rand(num_spikes_param).numpy()\n        return spikes_positions, intensity_param.item()\n\n    def add_artifact(\n            self,\n            image: sitk.Image,\n            spikes_positions: np.ndarray,\n            intensity_factor: float,\n            ):\n        array = sitk.GetArrayViewFromImage(image).transpose()\n        spectrum = self.fourier_transform(array).ravel()\n        indices = np.floor(spikes_positions * len(spectrum)).astype(int)\n        for index in indices:\n            spectrum[index] = spectrum.max() * intensity_factor\n        spectrum = spectrum.reshape(array.shape)\n        result = self.inv_fourier_transform(spectrum)\n        return result.astype(np.float32)\n'"
torchio/transforms/augmentation/intensity/random_swap.py,4,"b'from typing import Optional, Tuple, Union\nimport torch\nimport numpy as np\nfrom ....data.subject import Subject\nfrom ....utils import to_tuple\nfrom ....torchio import DATA, TypeTuple, TypeData\nfrom .. import RandomTransform\n\n\nclass RandomSwap(RandomTransform):\n    r""""""Randomly swap patches within an image.\n\n    Args:\n        patch_size: Tuple of integers :math:`(d, h, w)` to swap patches\n            of size :math:`d \\times h \\times w`.\n            If a single number :math:`n` is provided, :math:`d = h = w = n`.\n        num_iterations: Number of times that two patches will be swapped.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n    """"""\n    def __init__(\n            self,\n            patch_size: TypeTuple = 15,\n            num_iterations: int = 100,\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.patch_size = to_tuple(patch_size)\n        self.num_iterations = num_iterations\n\n    @staticmethod\n    def get_params():\n        # TODO: return locations?\n        return\n\n    def apply_transform(self, sample: Subject) -> dict:\n        for image_dict in sample.get_images():\n            swap(image_dict[DATA][0], self.patch_size, self.num_iterations)\n        return sample\n\n\ndef swap(\n        tensor: torch.Tensor,\n        patch_size: TypeTuple,\n        num_iterations: int,\n        ) -> None:\n    patch_size = to_tuple(patch_size)\n    for _ in range(num_iterations):\n        first_ini, first_fin = get_random_indices_from_shape(\n            tensor.shape,\n            patch_size,\n        )\n        while True:\n            second_ini, second_fin = get_random_indices_from_shape(\n                tensor.shape,\n                patch_size,\n            )\n            larger_than_initial = np.all(second_ini >= first_ini)\n            less_than_final = np.all(second_fin <= first_fin)\n            if larger_than_initial and less_than_final:\n                continue  # patches overlap\n            else:\n                break  # patches don\'t overlap\n        first_patch = crop(tensor, first_ini, first_fin)\n        second_patch = crop(tensor, second_ini, second_fin).clone()\n        insert(tensor, first_patch, second_ini)\n        insert(tensor, second_patch, first_ini)\n\n\ndef insert(tensor: TypeData, patch: TypeData, index_ini: np.ndarray) -> None:\n    index_fin = index_ini + np.array(patch.shape)\n    i_ini, j_ini, k_ini = index_ini\n    i_fin, j_fin, k_fin = index_fin\n    tensor[i_ini:i_fin, j_ini:j_fin, k_ini:k_fin] = patch\n\n\ndef crop(\n        image: Union[np.ndarray, torch.Tensor],\n        index_ini: np.ndarray,\n        index_fin: np.ndarray,\n        ) -> Union[np.ndarray, torch.Tensor]:\n    i_ini, j_ini, k_ini = index_ini\n    i_fin, j_fin, k_fin = index_fin\n    return image[..., i_ini:i_fin, j_ini:j_fin, k_ini:k_fin]\n\n\ndef get_random_indices_from_shape(\n        shape: Tuple[int, int, int],\n        patch_size: Tuple[int, int, int],\n        ) -> Tuple[np.ndarray, np.ndarray]:\n    shape_array = np.array(shape)\n    patch_size_array = np.array(patch_size)\n    max_index_ini = shape_array - patch_size_array\n    if (max_index_ini < 0).any():\n        message = (\n            f\'Patch size {patch_size} must not be\'\n            f\' larger than image size {shape}\'\n        )\n        raise ValueError(message)\n    max_index_ini = max_index_ini.astype(np.uint16)\n    coordinates = []\n    for max_coordinate in max_index_ini.tolist():\n        if max_coordinate == 0:\n            coordinate = 0\n        else:\n            coordinate = torch.randint(max_coordinate, size=(1,)).item()\n        coordinates.append(coordinate)\n    index_ini = np.array(coordinates, np.uint16)\n    index_fin = index_ini + patch_size_array\n    return index_ini, index_fin\n'"
torchio/transforms/augmentation/spatial/__init__.py,0,b'from .random_flip import RandomFlip\nfrom .random_affine import RandomAffine\nfrom .random_elastic_deformation import RandomElasticDeformation\n'
torchio/transforms/augmentation/spatial/random_affine.py,6,"b'from numbers import Number\nfrom typing import Tuple, Optional, List, Union\nimport torch\nimport numpy as np\nimport SimpleITK as sitk\nfrom ....data.subject import Subject\nfrom ....torchio import (\n    INTENSITY,\n    DATA,\n    AFFINE,\n    TYPE,\n    TypeRangeFloat,\n    TypeTripletFloat,\n)\nfrom .. import Interpolation, get_sitk_interpolator\nfrom .. import RandomTransform\n\n\nclass RandomAffine(RandomTransform):\n    r""""""Random affine transformation.\n\n    Args:\n        scales: Tuple :math:`(a, b)` defining the scaling\n            magnitude. The scaling values along each dimension are\n            :math:`(s_1, s_2, s_3)`, where :math:`s_i \\sim \\mathcal{U}(a, b)`.\n            For example, using ``scales=(0.5, 0.5)`` will zoom out the image,\n            making the objects inside look twice as small while preserving\n            the physical size and position of the image.\n        degrees: Tuple :math:`(a, b)` defining the rotation range in degrees.\n            The rotation angles around each axis are\n            :math:`(\\theta_1, \\theta_2, \\theta_3)`,\n            where :math:`\\theta_i \\sim \\mathcal{U}(a, b)`.\n            If only one value :math:`d` is provided,\n            :math:`\\theta_i \\sim \\mathcal{U}(-d, d)`.\n        translation: Tuple :math:`(a, b)` defining the translation in mm.\n            Translation along each axis is\n            :math:`(x_1, x_2, x_3)`,\n            where :math:`x_i \\sim \\mathcal{U}(a, b)`.\n            If only one value :math:`d` is provided,\n            :math:`x_i \\sim \\mathcal{U}(-d, d)`.\n        isotropic: If ``True``, the scaling factor along all dimensions is the\n            same, i.e. :math:`s_1 = s_2 = s_3`.\n        center: If ``\'image\'``, rotations and scaling will be performed around\n            the image center. If ``\'origin\'``, rotations and scaling will be\n            performed around the origin in world coordinates.\n        default_pad_value: As the image is rotated, some values near the\n            borders will be undefined.\n            If ``\'minimum\'``, the fill value will be the image minimum.\n            If ``\'mean\'``, the fill value is the mean of the border values.\n            If ``\'otsu\'``, the fill value is the mean of the values at the\n            border that lie under an\n            `Otsu threshold <https://ieeexplore.ieee.org/document/4310076>`_.\n        image_interpolation: See :ref:`Interpolation`.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n\n    Example:\n        >>> from torchio.transforms import RandomAffine, Interpolation\n        >>> sample = images_dataset[0]  # instance of torchio.ImagesDataset\n        >>> transform = RandomAffine(\n        ...     scales=(0.9, 1.2),\n        ...     degrees=(10),\n        ...     isotropic=False,\n        ...     default_pad_value=\'otsu\',\n        ...     image_interpolation=\'bspline\',\n        ... )\n        >>> transformed = transform(sample)\n\n    From the command line::\n\n        $ torchio-transform t1.nii.gz RandomAffine --kwargs ""degrees=30 default_pad_value=minimum"" --seed 42 affine_min.nii.gz\n\n    """"""\n    def __init__(\n            self,\n            scales: Tuple[float, float] = (0.9, 1.1),\n            degrees: TypeRangeFloat = 10,\n            translation: TypeRangeFloat = 0,\n            isotropic: bool = False,\n            center: str = \'image\',\n            default_pad_value: Union[str, float] = \'otsu\',\n            image_interpolation: str = \'linear\',\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.scales = scales\n        self.degrees = self.parse_degrees(degrees)\n        self.translation = self.parse_range(translation, \'translation\')\n        self.isotropic = isotropic\n        if center not in (\'image\', \'origin\'):\n            message = (\n                \'Center argument must be ""image"" or ""origin"",\'\n                f\' not ""{center}""\'\n            )\n            raise ValueError(message)\n        self.use_image_center = center == \'image\'\n        self.default_pad_value = self.parse_default_value(default_pad_value)\n        self.interpolation = self.parse_interpolation(image_interpolation)\n\n    @staticmethod\n    def parse_default_value(value: Union[str, float]) -> Union[str, float]:\n        if isinstance(value, Number) or value in (\'minimum\', \'otsu\', \'mean\'):\n            return value\n        message = (\n            \'Value for default_pad_value must be ""minimum"", ""otsu"", ""mean""\'\n            \' or a number\'\n        )\n        raise ValueError(message)\n\n    @staticmethod\n    def get_params(\n            scales: Tuple[float, float],\n            degrees: Tuple[float, float],\n            translation: Tuple[float, float],\n            isotropic: bool,\n            ) -> Tuple[np.ndarray, np.ndarray]:\n        scaling_params = torch.FloatTensor(3).uniform_(*scales)\n        if isotropic:\n            scaling_params.fill_(scaling_params[0])\n        rotation_params = torch.FloatTensor(3).uniform_(*degrees).numpy()\n        translation_params = torch.FloatTensor(3).uniform_(*translation).numpy()\n        return scaling_params.numpy(), rotation_params, translation_params\n\n    @staticmethod\n    def get_scaling_transform(\n            scaling_params: List[float],\n            center_lps: Optional[TypeTripletFloat] = None,\n            ) -> sitk.ScaleTransform:\n        # scaling_params are inverted so that they are more intuitive\n        # For example, 1.5 means the objects look 1.5 times larger\n        transform = sitk.ScaleTransform(3)\n        scaling_params = 1 / np.array(scaling_params)\n        transform.SetScale(scaling_params)\n        if center_lps is not None:\n            transform.SetCenter(center_lps)\n        return transform\n\n    @staticmethod\n    def get_rotation_transform(\n            degrees: List[float],\n            translation: List[float],\n            center_lps: Optional[TypeTripletFloat] = None,\n            ) -> sitk.Euler3DTransform:\n        transform = sitk.Euler3DTransform()\n        radians = np.radians(degrees)\n        transform.SetRotation(*radians)\n        transform.SetTranslation(translation)\n        if center_lps is not None:\n            transform.SetCenter(center_lps)\n        return transform\n\n    def apply_transform(self, sample: Subject) -> dict:\n        sample.check_consistent_shape()\n        params = self.get_params(\n            self.scales,\n            self.degrees,\n            self.translation,\n            self.isotropic,\n        )\n        scaling_params, rotation_params, translation_params = params\n        for image in sample.get_images(intensity_only=False):\n            if image[TYPE] != INTENSITY:\n                interpolation = Interpolation.NEAREST\n            else:\n                interpolation = self.interpolation\n\n            if image.is_2d():\n                scaling_params[0] = 1\n                rotation_params[-2:] = 0\n\n            if self.use_image_center:\n                center = image.get_center(lps=True)\n            else:\n                center = None\n\n            image[DATA] = self.apply_affine_transform(\n                image[DATA],\n                image[AFFINE],\n                scaling_params.tolist(),\n                rotation_params.tolist(),\n                translation_params.tolist(),\n                interpolation,\n                center_lps=center,\n            )\n        random_parameters_dict = {\n            \'scaling\': scaling_params,\n            \'rotation\': rotation_params,\n            \'translation\': translation_params,\n        }\n        sample.add_transform(self, random_parameters_dict)\n        return sample\n\n    def apply_affine_transform(\n            self,\n            tensor: torch.Tensor,\n            affine: np.ndarray,\n            scaling_params: List[float],\n            rotation_params: List[float],\n            translation_params: List[float],\n            interpolation: Interpolation,\n            center_lps: Optional[TypeTripletFloat] = None,\n            ) -> torch.Tensor:\n        assert tensor.ndim == 4\n        assert len(tensor) == 1\n\n        image = self.nib_to_sitk(tensor[0], affine)\n        floating = reference = image\n\n        scaling_transform = self.get_scaling_transform(\n            scaling_params,\n            center_lps=center_lps,\n        )\n        rotation_transform = self.get_rotation_transform(\n            rotation_params,\n            translation_params,\n            center_lps=center_lps,\n        )\n        transform = sitk.Transform(3, sitk.sitkComposite)\n        transform.AddTransform(scaling_transform)\n        transform.AddTransform(rotation_transform)\n\n        if self.default_pad_value == \'minimum\':\n            default_value = tensor.min().item()\n        elif self.default_pad_value == \'mean\':\n            default_value = get_borders_mean(image, filter_otsu=False)\n        elif self.default_pad_value == \'otsu\':\n            default_value = get_borders_mean(image, filter_otsu=True)\n        else:\n            default_value = self.default_pad_value\n\n        resampler = sitk.ResampleImageFilter()\n        resampler.SetInterpolator(get_sitk_interpolator(interpolation))\n        resampler.SetReferenceImage(reference)\n        resampler.SetDefaultPixelValue(float(default_value))\n        resampler.SetOutputPixelType(sitk.sitkFloat32)\n        resampler.SetTransform(transform)\n        resampled = resampler.Execute(floating)\n\n        np_array = sitk.GetArrayFromImage(resampled)\n        np_array = np_array.transpose()  # ITK to NumPy\n        tensor[0] = torch.from_numpy(np_array)\n        return tensor\n\n\ndef get_borders_mean(image, filter_otsu=True):\n    # pylint: disable=bad-whitespace\n    array = sitk.GetArrayViewFromImage(image)\n    borders_tuple = (\n        array[ 0,  :,  :],\n        array[-1,  :,  :],\n        array[ :,  0,  :],\n        array[ :, -1,  :],\n        array[ :,  :,  0],\n        array[ :,  :, -1],\n    )\n    borders_flat = np.hstack([border.ravel() for border in borders_tuple])\n    if not filter_otsu:\n        return borders_flat.mean()\n    borders_reshaped = borders_flat.reshape(1, 1, -1)\n    borders_image = sitk.GetImageFromArray(borders_reshaped)\n    otsu = sitk.OtsuThresholdImageFilter()\n    otsu.Execute(borders_image)\n    threshold = otsu.GetThreshold()\n    values = borders_flat[borders_flat < threshold]\n    if values.any():\n        default_value = values.mean()\n    else:\n        default_value = borders_flat.mean()\n    return default_value\n'"
torchio/transforms/augmentation/spatial/random_elastic_deformation.py,4,"b'import warnings\nfrom numbers import Number\nfrom typing import Tuple, Optional, Union\nimport torch\nimport numpy as np\nimport SimpleITK as sitk\nfrom ....data.subject import Subject\nfrom ....utils import to_tuple\nfrom ....torchio import INTENSITY, DATA, AFFINE, TYPE\nfrom .. import Interpolation, get_sitk_interpolator\nfrom .. import RandomTransform\n\n\nSPLINE_ORDER = 3\n\n\nclass RandomElasticDeformation(RandomTransform):\n    r""""""Apply dense random elastic deformation.\n\n    A random displacement is assigned to a coarse grid of control points around\n    and inside the image. The displacement at each voxel is interpolated from\n    the coarse grid using cubic B-splines.\n\n    The `\'Deformable Registration\' <https://www.sciencedirect.com/topics/computer-science/deformable-registration>`_\n    topic on ScienceDirect contains useful articles explaining interpolation of\n    displacement fields using cubic B-splines.\n\n    Args:\n        num_control_points: Number of control points along each dimension of\n            the coarse grid :math:`(n_x, n_y, n_z)`.\n            If a single value :math:`n` is passed,\n            then :math:`n_x = n_y = n_z = n`.\n            Smaller numbers generate smoother deformations.\n            The minimum number of control points is ``4`` as this transform\n            uses cubic B-splines to interpolate displacement.\n        max_displacement: Maximum displacement along each dimension at each\n            control point :math:`(D_x, D_y, D_z)`.\n            The displacement along dimension :math:`i` at each control point is\n            :math:`d_i \\sim \\mathcal{U}(0, D_i)`.\n            If a single value :math:`D` is passed,\n            then :math:`D_x = D_y = D_z = D`.\n            Note that the total maximum displacement would actually be\n            :math:`D_{max} = \\sqrt{D_x^2 + D_y^2 + D_z^2}`.\n        locked_borders: If ``0``, all displacement vectors are kept.\n            If ``1``, displacement of control points at the\n            border of the coarse grid will also be set to ``0``.\n            If ``2``, displacement of control points at the border of the image\n            will also be set to ``0``.\n        image_interpolation: See :ref:`Interpolation`.\n            Note that this is the interpolation used to compute voxel\n            intensities when resampling using the dense displacement field.\n            The value of the dense displacement at each voxel is always\n            interpolated with cubic B-splines from the values at the control\n            points of the coarse grid.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n\n    `This gist <https://gist.github.com/fepegar/b723d15de620cd2a3a4dbd71e491b59d>`_\n    can also be used to better understand the meaning of the parameters.\n\n    This is an example from the\n    `3D Slicer registration FAQ <https://www.slicer.org/wiki/Documentation/4.10/FAQ/Registration#What.27s_the_BSpline_Grid_Size.3F>`_.\n\n    .. image:: https://www.slicer.org/w/img_auth.php/6/6f/RegLib_BSplineGridModel.png\n        :alt: B-spline example from 3D Slicer documentation\n\n    To generate a similar grid of control points with TorchIO,\n    the transform can be instantiated as follows::\n\n        >>> from torchio import RandomElasticDeformation\n        >>> transform = RandomElasticDeformation(\n        ...     num_control_points=(7, 7, 7),  # or just 7\n        ...     locked_borders=2,\n        ... )\n\n    Note that control points outside the image bounds are not showed in the\n    example image (they would also be red as we set :py:attr:`locked_borders`\n    to ``2``).\n\n    .. warning:: Image folding may occur if the maximum displacement is larger\n        than half the coarse grid spacing. The grid spacing can be computed\n        using the image bounds in physical space [#]_ and the number of control\n        points::\n\n            >>> import numpy as np\n            >>> import SimpleITK as sitk\n            >>> image = sitk.ReadImage(\'my_image.nii.gz\')\n            >>> image.GetSize()\n            (512, 512, 139)  # voxels\n            >>> image.GetSpacing()\n            (0.76, 0.76, 2.50)  # mm\n            >>> bounds = np.array(image.GetSize()) * np.array(image.GetSpacing())\n            array([390.0, 390.0, 347.5])  # mm\n            >>> num_control_points = np.array((7, 7, 6))\n            >>> grid_spacing = bounds / (num_control_points - 2)\n            >>> grid_spacing\n            array([78.0, 78.0, 86.9])  # mm\n            >>> potential_folding = grid_spacing / 2\n            >>> potential_folding\n            array([39.0, 39.0, 43.4])  # mm\n\n        Using a :py:attr:`max_displacement` larger than the computed\n        :py:attr:`potential_folding` will raise a :py:class:`RuntimeWarning`.\n\n        .. [#] Technically, :math:`2 \\epsilon` should be added to the\n            image bounds, where :math:`\\epsilon = 2^{-3}` `according to ITK\n            source code <https://github.com/InsightSoftwareConsortium/ITK/blob/633f84548311600845d54ab2463d3412194690a8/Modules/Core/Transform/include/itkBSplineTransformInitializer.hxx#L116-L138>`_.\n    """"""\n\n    def __init__(\n            self,\n            num_control_points: Union[int, Tuple[int, int, int]] = 7,\n            max_displacement: Union[float, Tuple[float, float, float]] = 7.5,\n            locked_borders: int = 2,\n            image_interpolation: str = \'linear\',\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self._bspline_transformation = None\n        self.num_control_points = to_tuple(num_control_points, length=3)\n        self.parse_control_points(self.num_control_points)\n        self.max_displacement = to_tuple(max_displacement, length=3)\n        self.parse_max_displacement(self.max_displacement)\n        self.num_locked_borders = locked_borders\n        if locked_borders not in (0, 1, 2):\n            raise ValueError(\'locked_borders must be 0, 1, or 2\')\n        if locked_borders == 2 and 4 in self.num_control_points:\n            message = (\n                \'Setting locked_borders to 2 and using less than 5 control\'\n                \'points results in an identity transform. Lock fewer borders\'\n                \' or use more control points.\'\n            )\n            raise ValueError(message)\n        self.interpolation = self.parse_interpolation(image_interpolation)\n\n    @staticmethod\n    def parse_control_points(\n            num_control_points: Tuple[int, int, int],\n            ) -> None:\n        for axis, number in enumerate(num_control_points):\n            if not isinstance(number, int) or number < 4:\n                message = (\n                    f\'The number of control points for axis {axis} must be\'\n                    f\' an integer greater than 3, not {number}\'\n                )\n                raise ValueError(message)\n\n    @staticmethod\n    def parse_max_displacement(\n            max_displacement: Tuple[float, float, float],\n            ) -> None:\n        for axis, number in enumerate(max_displacement):\n            if not isinstance(number, Number) or number < 0:\n                message = (\n                    \'The maximum displacement at each control point\'\n                    f\' for axis {axis} must be\'\n                    f\' a number greater or equal to 0, not {number}\'\n                )\n                raise ValueError(message)\n\n    @staticmethod\n    def get_params(\n            num_control_points: Tuple[int, int, int],\n            max_displacement: Tuple[float, float, float],\n            num_locked_borders: int,\n            ) -> Tuple:\n        grid_shape = num_control_points\n        num_dimensions = 3\n        coarse_field = torch.rand(*grid_shape, num_dimensions)  # [0, 1)\n        coarse_field -= 0.5  # [-0.5, 0.5)\n        coarse_field *= 2  # [-1, 1]\n        for dimension in range(3):\n            # [-max_displacement, max_displacement)\n            coarse_field[..., dimension] *= max_displacement[dimension]\n\n        # Set displacement to 0 at the borders\n        for i in range(num_locked_borders):\n            coarse_field[i, :] = 0\n            coarse_field[-1 - i, :] = 0\n            coarse_field[:, i] = 0\n            coarse_field[:, -1 - i] = 0\n\n        return coarse_field.numpy()\n\n    @staticmethod\n    def get_bspline_transform(\n            image: sitk.Image,\n            num_control_points: Tuple[int, int, int],\n            coarse_field: np.ndarray,\n            ) -> sitk.BSplineTransformInitializer:\n        mesh_shape = [n - SPLINE_ORDER for n in num_control_points]\n        bspline_transform = sitk.BSplineTransformInitializer(image, mesh_shape)\n        parameters = coarse_field.flatten(order=\'F\').tolist()\n        bspline_transform.SetParameters(parameters)\n        return bspline_transform\n\n    @staticmethod\n    def parse_free_form_transform(transform, max_displacement):\n        """"""Issue a warning is possible folding is detected.""""""\n        coefficient_images = transform.GetCoefficientImages()\n        grid_spacing = coefficient_images[0].GetSpacing()\n        conflicts = np.array(max_displacement) > np.array(grid_spacing) / 2\n        if np.any(conflicts):\n            where, = np.where(conflicts)\n            message = (\n                \'The maximum displacement is larger than the coarse grid\'\n                f\' spacing for dimensions: {where.tolist()}, so folding may\'\n                \' occur. Choose fewer control points or a smaller\'\n                \' maximum displacement\'\n            )\n            warnings.warn(message, RuntimeWarning)\n\n    def apply_transform(self, sample: Subject) -> dict:\n        sample.check_consistent_shape()\n        bspline_params = self.get_params(\n            self.num_control_points,\n            self.max_displacement,\n            self.num_locked_borders,\n        )\n        for image in sample.get_images(intensity_only=False):\n            if image[TYPE] != INTENSITY:\n                interpolation = Interpolation.NEAREST\n            else:\n                interpolation = self.interpolation\n            if image.is_2d():\n                bspline_params[..., -3] = 0  # no displacement in LR axis\n            image[DATA] = self.apply_bspline_transform(\n                image[DATA],\n                image[AFFINE],\n                bspline_params,\n                interpolation,\n            )\n        random_parameters_dict = {\'coarse_grid\': bspline_params}\n        sample.add_transform(self, random_parameters_dict)\n        return sample\n\n    def apply_bspline_transform(\n            self,\n            tensor: torch.Tensor,\n            affine: np.ndarray,\n            bspline_params: np.ndarray,\n            interpolation: Interpolation,\n            ) -> torch.Tensor:\n        assert tensor.ndim == 4\n        assert len(tensor) == 1\n        image = self.nib_to_sitk(tensor[0], affine)\n        floating = reference = image\n        bspline_transform = self.get_bspline_transform(\n            image,\n            self.num_control_points,\n            bspline_params,\n        )\n        self.parse_free_form_transform(\n            bspline_transform, self.max_displacement)\n        resampler = sitk.ResampleImageFilter()\n        resampler.SetReferenceImage(reference)\n        resampler.SetTransform(bspline_transform)\n        resampler.SetInterpolator(get_sitk_interpolator(interpolation))\n        resampler.SetDefaultPixelValue(tensor.min().item())\n        resampler.SetOutputPixelType(sitk.sitkFloat32)\n        resampled = resampler.Execute(floating)\n\n        np_array = sitk.GetArrayFromImage(resampled)\n        np_array = np_array.transpose()  # ITK to NumPy\n        tensor[0] = torch.from_numpy(np_array)\n        return tensor\n'"
torchio/transforms/augmentation/spatial/random_flip.py,2,"b'from typing import Union, Tuple, Optional, List\nimport torch\nfrom ....torchio import DATA\nfrom ....data.subject import Subject\nfrom ....utils import to_tuple\nfrom .. import RandomTransform\n\n\nclass RandomFlip(RandomTransform):\n    """"""Reverse the order of elements in an image along the given axes.\n\n    Args:\n        axes: Axis or tuple of axes along which the image will be flipped.\n        flip_probability: Probability that the image will be flipped. This is\n            computed on a per-axis basis.\n        p: Probability that this transform will be applied.\n        seed: See :py:class:`~torchio.transforms.augmentation.RandomTransform`.\n\n    .. note:: If the input image is 2D, all axes should be in ``(0, 1)``.\n    """"""\n\n    def __init__(\n            self,\n            axes: Union[int, Tuple[int, ...]] = 0,\n            flip_probability: float = 0.5,\n            p: float = 1,\n            seed: Optional[int] = None,\n            ):\n        super().__init__(p=p, seed=seed)\n        self.axes = self.parse_axes(axes)\n        self.flip_probability = self.parse_probability(\n            flip_probability,\n        )\n\n    def apply_transform(self, sample: Subject) -> dict:\n        axes_to_flip_hot = self.get_params(self.axes, self.flip_probability)\n        random_parameters_dict = {\'axes\': axes_to_flip_hot}\n        items = sample.get_images_dict(intensity_only=False).items()\n        for image_name, image_dict in items:\n            data = image_dict[DATA]\n            is_2d = data.shape[-3] == 1\n            dims = []\n            for dim, flip_this in enumerate(axes_to_flip_hot):\n                if not flip_this:\n                    continue\n                actual_dim = dim + 1  # images are 4D\n                # If the user is using 2D images and they use (0, 1) for axes,\n                # they probably mean (1, 2). This should make this transform\n                # more user-friendly.\n                if is_2d:\n                    actual_dim += 1\n                if actual_dim > 3:\n                    message = (\n                        f\'Image ""{image_name}"" with shape {data.shape} seems to\'\n                        \' be 2D, so all axes must be in (0, 1),\'\n                        f\' but they are {self.axes}\'\n                    )\n                    raise RuntimeError(message)\n                dims.append(actual_dim)\n            data = torch.flip(data, dims=dims)\n            image_dict[DATA] = data\n        sample.add_transform(self, random_parameters_dict)\n        return sample\n\n    @staticmethod\n    def get_params(axes: Tuple[int, ...], probability: float) -> List[bool]:\n        axes_hot = [False, False, False]\n        for axis in axes:\n            random_number = torch.rand(1)\n            flip_this = bool(probability > random_number)\n            axes_hot[axis] = flip_this\n        return axes_hot\n\n    @staticmethod\n    def parse_axes(axes: Union[int, Tuple[int, ...]]):\n        axes_tuple = to_tuple(axes)\n        for axis in axes_tuple:\n            is_int = isinstance(axis, int)\n            if not is_int or axis not in (0, 1, 2):\n                raise ValueError(\'All axes must be 0, 1 or 2\')\n        return axes_tuple\n'"
torchio/transforms/preprocessing/intensity/__init__.py,0,b'from .normalization_transform import NormalizationTransform\n'
torchio/transforms/preprocessing/intensity/histogram_standardization.py,8,"b'from pathlib import Path\nfrom typing import Dict, Callable, Tuple, Sequence, Union, Optional\nimport torch\nimport numpy as np\nimport nibabel as nib\nfrom tqdm import tqdm\nfrom ....torchio import DATA, TypePath\nfrom ....data.io import read_image\nfrom ....data.subject import Subject\nfrom .normalization_transform import NormalizationTransform, TypeMaskingMethod\n\nDEFAULT_CUTOFF = 0.01, 0.99\nSTANDARD_RANGE = 0, 100\nTypeLandmarks = Union[TypePath, Dict[str, Union[TypePath, np.ndarray]]]\n\n\nclass HistogramStandardization(NormalizationTransform):\n    """"""Perform histogram standardization of intensity values.\n\n    See example in :py:func:`torchio.transforms.HistogramStandardization.train`.\n\n    Args:\n        landmarks: Dictionary (or path to a PyTorch file with ``.pt`` or ``.pth``\n            extension in which a dictionary has been saved) whose keys are\n            image names in the sample and values are NumPy arrays or paths to\n            NumPy arrays defining the landmarks after training with\n            :py:meth:`torchio.transforms.HistogramStandardization.train`.\n        masking_method: See\n            :py:class:`~torchio.transforms.preprocessing.normalization_transform.NormalizationTransform`.\n        p: Probability that this transform will be applied.\n\n    Example:\n        >>> import torch\n        >>> from pathlib import Path\n        >>> from torchio.transforms import HistogramStandardization\n        >>>\n        >>> landmarks = {\n        ...     \'t1\': \'t1_landmarks.npy\',\n        ...     \'t2\': \'t2_landmarks.npy\',\n        ... }\n        >>> transform = HistogramStandardization(landmarks)\n        >>>\n        >>> torch.save(landmarks, \'path_to_landmarks.pth\')\n        >>> transform = HistogramStandardization(\'path_to_landmarks.pth\')\n    """"""\n    def __init__(\n            self,\n            landmarks: TypeLandmarks,\n            masking_method: TypeMaskingMethod = None,\n            p: float = 1,\n            ):\n        super().__init__(masking_method=masking_method, p=p)\n        self.landmarks_dict = self.parse_landmarks(landmarks)\n\n    @staticmethod\n    def parse_landmarks(landmarks: TypeLandmarks) -> Dict[str, np.ndarray]:\n        if isinstance(landmarks, (str, Path)):\n            path = Path(landmarks)\n            if not path.suffix in (\'.pt\', \'.pth\'):\n                message = (\n                    \'The landmarks file must have extension .pt or .pth,\'\n                    f\' not ""{path.suffix}""\'\n                )\n                raise ValueError(message)\n            landmarks_dict = torch.load(path)\n        else:\n            landmarks_dict = landmarks\n        for key, value in landmarks_dict.items():\n            if isinstance(value, (str, Path)):\n                landmarks_dict[key] = np.load(value)\n        return landmarks_dict\n\n    def apply_normalization(\n            self,\n            sample: Subject,\n            image_name: str,\n            mask: torch.Tensor,\n            ) -> None:\n        if image_name not in self.landmarks_dict:\n            keys = tuple(self.landmarks_dict.keys())\n            message = (\n                f\'Image name ""{image_name}"" should be a key in the\'\n                f\' landmarks dictionary, whose keys are {keys}\'\n            )\n            raise KeyError(message)\n        image_dict = sample[image_name]\n        landmarks = self.landmarks_dict[image_name]\n        image_dict[DATA] = normalize(\n            image_dict[DATA],\n            landmarks,\n            mask=mask,\n        )\n\n    @classmethod\n    def train(\n            cls,\n            images_paths: Sequence[TypePath],\n            cutoff: Optional[Tuple[float, float]] = None,\n            mask_path: Optional[TypePath] = None,\n            masking_function: Optional[Callable] = None,\n            output_path: Optional[TypePath] = None,\n            ) -> np.ndarray:\n        """"""Extract average histogram landmarks from images used for training.\n\n        Args:\n            images_paths: List of image paths used to train.\n            cutoff: Optional minimum and maximum quantile values,\n                respectively, that are used to select a range of intensity of\n                interest. Equivalent to :math:`pc_1` and :math:`pc_2` in\n                `Ny\xc3\xbal and Udupa\'s paper <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.204.102&rep=rep1&type=pdf>`_.\n            mask_path: Optional path to a mask image to extract voxels used for\n                training.\n            masking_function: Optional function used to extract voxels used for\n                training.\n            output_path: Optional file path with extension ``.txt`` or\n                ``.npy``, where the landmarks will be saved.\n\n        Example:\n\n            >>> import torch\n            >>> import numpy as np\n            >>> from pathlib import Path\n            >>> from torchio.transforms import HistogramStandardization\n            >>>\n            >>> t1_paths = [\'subject_a_t1.nii\', \'subject_b_t1.nii.gz\']\n            >>> t2_paths = [\'subject_a_t2.nii\', \'subject_b_t2.nii.gz\']\n            >>>\n            >>> t1_landmarks_path = Path(\'t1_landmarks.npy\')\n            >>> t2_landmarks_path = Path(\'t2_landmarks.npy\')\n            >>>\n            >>> t1_landmarks = (\n            ...     t1_landmarks_path\n            ...     if t1_landmarks_path.is_file()\n            ...     else HistogramStandardization.train(t1_paths)\n            ... )\n            >>> torch.save(t1_landmarks, t1_landmarks_path)\n            >>>\n            >>> t2_landmarks = (\n            ...     t2_landmarks_path\n            ...     if t2_landmarks_path.is_file()\n            ...     else HistogramStandardization.train(t2_paths)\n            ... )\n            >>> torch.save(t2_landmarks, t2_landmarks_path)\n            >>>\n            >>> landmarks_dict = {\n            ...     \'t1\': t1_landmarks,\n            ...     \'t2\': t2_landmarks,\n            ... }\n            >>>\n            >>> transform = HistogramStandardization(landmarks_dict)\n        """"""\n        quantiles_cutoff = DEFAULT_CUTOFF if cutoff is None else cutoff\n        percentiles_cutoff = 100 * np.array(quantiles_cutoff)\n        percentiles_database = []\n        percentiles = _get_percentiles(percentiles_cutoff)\n        for image_file_path in tqdm(images_paths):\n            tensor, _ = read_image(image_file_path)\n            data = tensor.numpy()\n            if masking_function is not None:\n                mask = masking_function(data)\n            else:\n                if mask_path is not None:\n                    mask = nib.load(str(mask_path)).get_fdata()\n                    mask = mask > 0\n                else:\n                    mask = np.ones_like(data, dtype=np.bool)\n            percentile_values = np.percentile(data[mask], percentiles)\n            percentiles_database.append(percentile_values)\n        percentiles_database = np.vstack(percentiles_database)\n        mapping = _get_average_mapping(percentiles_database)\n\n        if output_path is not None:\n            output_path = Path(output_path).expanduser()\n            extension = output_path.suffix\n            if extension == \'.txt\':\n                modality = \'image\'\n                text = f\'{modality} {"" "".join(map(str, mapping))}\'\n                output_path.write_text(text)\n            elif extension == \'.npy\':\n                np.save(output_path, mapping)\n        return mapping\n\n\ndef _standardize_cutoff(cutoff: np.ndarray) -> np.ndarray:\n    """"""Standardize the cutoff values given in the configuration.\n\n    Computes percentile landmark normalization by default.\n\n    """"""\n    cutoff = np.asarray(cutoff)\n    cutoff[0] = max(0., cutoff[0])\n    cutoff[1] = min(1., cutoff[1])\n    cutoff[0] = np.min([cutoff[0], 0.09])\n    cutoff[1] = np.max([cutoff[1], 0.91])\n    return cutoff\n\n\ndef _get_average_mapping(percentiles_database: np.ndarray) -> np.ndarray:\n    """"""Map the landmarks of the database to the chosen range.\n\n    Args:\n        percentiles_database: Percentiles database over which to perform the\n            averaging.\n    """"""\n    # Assuming percentiles_database.shape == (num_data_points, num_percentiles)\n    pc1 = percentiles_database[:, 0]\n    pc2 = percentiles_database[:, -1]\n    s1, s2 = STANDARD_RANGE\n    slopes = (s2 - s1) / (pc2 - pc1)\n    slopes = np.nan_to_num(slopes)\n    intercepts = np.mean(s1 - slopes * pc1)\n    num_images = len(percentiles_database)\n    final_map = slopes.dot(percentiles_database) / num_images + intercepts\n    return final_map\n\n\ndef _get_percentiles(percentiles_cutoff: Tuple[float, float]) -> np.ndarray:\n    quartiles = np.arange(25, 100, 25).tolist()\n    deciles = np.arange(10, 100, 10).tolist()\n    all_percentiles = list(percentiles_cutoff) + quartiles + deciles\n    percentiles = sorted(set(all_percentiles))\n    return np.array(percentiles)\n\n\ndef normalize(\n        tensor: torch.Tensor,\n        landmarks: np.ndarray,\n        mask: Optional[np.ndarray],\n        cutoff: Optional[Tuple[float, float]] = None,\n        epsilon: float = 1e-5,\n        ) -> torch.Tensor:\n    cutoff_ = DEFAULT_CUTOFF if cutoff is None else cutoff\n    array = tensor.numpy()\n    mapping = landmarks\n\n    data = array\n    shape = data.shape\n    data = data.reshape(-1).astype(np.float32)\n\n    if mask is None:\n        mask = np.ones_like(data, np.bool)\n    mask = mask.reshape(-1)\n\n    range_to_use = [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12]\n\n    quantiles_cutoff = _standardize_cutoff(cutoff_)\n    percentiles_cutoff = 100 * np.array(quantiles_cutoff)\n    percentiles = _get_percentiles(percentiles_cutoff)\n    percentile_values = np.percentile(data[mask], percentiles)\n\n    # Apply linear histogram standardization\n    range_mapping = mapping[range_to_use]\n    range_perc = percentile_values[range_to_use]\n    diff_mapping = np.diff(range_mapping)\n    diff_perc = np.diff(range_perc)\n\n    # Handling the case where two landmarks are the same\n    # for a given input image. This usually happens when\n    # image background is not removed from the image.\n    diff_perc[diff_perc < epsilon] = np.inf\n\n    affine_map = np.zeros([2, len(range_to_use) - 1])\n\n    # Compute slopes of the linear models\n    affine_map[0] = diff_mapping / diff_perc\n\n    # Compute intercepts of the linear models\n    affine_map[1] = range_mapping[:-1] - affine_map[0] * range_perc[:-1]\n\n    bin_id = np.digitize(data, range_perc[1:-1], right=False)\n    lin_img = affine_map[0, bin_id]\n    aff_img = affine_map[1, bin_id]\n    new_img = lin_img * data + aff_img\n    new_img = new_img.reshape(shape)\n    new_img = new_img.astype(np.float32)\n    new_img = torch.from_numpy(new_img)\n    return new_img\n\n\ntrain = train_histogram = HistogramStandardization.train\n'"
torchio/transforms/preprocessing/intensity/normalization_transform.py,6,"b'from typing import Union\nimport torch\nfrom ....data.subject import Subject\nfrom ....torchio import DATA, TypeCallable\nfrom ... import Transform\n\n\nTypeMaskingMethod = Union[str, TypeCallable, None]\n\n\nclass NormalizationTransform(Transform):\n    """"""Base class for intensity preprocessing transforms.\n\n    Args:\n        masking_method: Defines the mask used to compute the normalization statistics. It can be one of:\n\n            - ``None``: the mask image is all ones, i.e. all values in the image are used\n\n            - A string: the mask image is retrieved from the sample, which is expected the string as a key\n\n            - A function: the mask image is computed as a function of the intensity image. The function must receive and return a :py:class:`torch.Tensor`\n\n    Example:\n        >>> from torchio.datasets import IXITiny\n        >>> from torchio.transforms import ZNormalization\n        >>> dataset = IXITiny(\'ixi_root\', download=True)\n        >>> sample = dataset[0]\n        >>> sample.keys()  # image is the MRI, label is a brain segmentation\n        dict_keys([\'image\', \'label\'])\n        >>> transform = ZNormalization()  # ZNormalization is a subclass of NormalizationTransform\n        >>> transformed = transform(sample)  # use all values to compute mean and std\n        >>> transform = ZNormalization(masking_method=\'label\')\n        >>> transformed = transform(sample)  # use only values within the brain\n        >>> transform = ZNormalization(masking_method=lambda x: x > x.mean())\n        >>> transformed = transform(sample)  # use values above the image mean\n\n    """"""\n    def __init__(\n            self,\n            masking_method: TypeMaskingMethod = None,\n            p: float = 1,\n            ):\n        """"""\n        masking_method is used to choose the values used for normalization.\n        It can be:\n         - A string: the mask will be retrieved from the sample\n         - A function: the mask will be computed using the function\n         - None: all values are used\n        """"""\n        super().__init__(p=p)\n        self.mask_name = None\n        if masking_method is None:\n            self.masking_method = self.ones\n        elif callable(masking_method):\n            self.masking_method = masking_method\n        elif isinstance(masking_method, str):\n            self.mask_name = masking_method\n\n    def get_mask(self, sample: Subject, tensor: torch.Tensor) -> torch.Tensor:\n        if self.mask_name is None:\n            return self.masking_method(tensor)\n        else:\n            return sample[self.mask_name][DATA].bool()\n\n    def apply_transform(self, sample: Subject) -> dict:\n        for image_name, image_dict in sample.get_images_dict().items():\n            mask = self.get_mask(sample, image_dict[DATA])\n            self.apply_normalization(sample, image_name, mask)\n        return sample\n\n    def apply_normalization(\n            self,\n            sample: Subject,\n            image_name: str,\n            mask: torch.Tensor,\n            ) -> None:\n        # There must be a nicer way of doing this\n        raise NotImplementedError\n\n    @staticmethod\n    def ones(tensor: torch.Tensor) -> torch.Tensor:\n        return torch.ones_like(tensor, dtype=torch.bool)\n\n    @staticmethod\n    def mean(tensor: torch.Tensor) -> torch.Tensor:\n        mask = tensor > tensor.mean()\n        return mask\n'"
torchio/transforms/preprocessing/intensity/rescale.py,5,"b'import warnings\nfrom typing import Tuple\n\nimport torch\nimport numpy as np\nfrom deprecated import deprecated\n\nfrom ....data.subject import Subject\nfrom ....torchio import DATA\nfrom .normalization_transform import NormalizationTransform, TypeMaskingMethod\n\n\nclass RescaleIntensity(NormalizationTransform):\n    """"""Rescale intensity values to a certain range.\n\n    Args:\n        out_min_max: Range :math:`(n_{min}, n_{max})` of output intensities.\n        percentiles: Percentile values of the input image that will be mapped\n            to :math:`(n_{min}, n_{max})`. They can be used for contrast\n            stretching, as in `this scikit-image example`_. For example,\n            Isensee et al. use ``(0.05, 99.5)`` in their `nn-UNet paper`_.\n        masking_method: See\n            :py:class:`~torchio.transforms.preprocessing.normalization_transform.NormalizationTransform`.\n        p: Probability that this transform will be applied.\n\n    .. _this scikit-image example: https://scikit-image.org/docs/dev/auto_examples/color_exposure/plot_equalize.html#sphx-glr-auto-examples-color-exposure-plot-equalize-py\n    .. _nn-UNet paper: https://arxiv.org/abs/1809.10486\n    """"""\n    def __init__(\n            self,\n            out_min_max: Tuple[float, float],\n            percentiles: Tuple[int, int] = (0, 100),\n            masking_method: TypeMaskingMethod = None,\n            p: float = 1,\n            ):\n        super().__init__(masking_method=masking_method, p=p)\n        self.out_min, self.out_max = out_min_max\n        self.percentiles = percentiles\n\n    def apply_normalization(\n            self,\n            sample: Subject,\n            image_name: str,\n            mask: torch.Tensor,\n            ) -> None:\n        image_dict = sample[image_name]\n        image_dict[DATA] = self.rescale(image_dict[DATA], mask, image_name)\n\n    def rescale(\n            self,\n            tensor: torch.Tensor,\n            mask: torch.Tensor,\n            image_name: str,\n            ) -> torch.Tensor:\n        array = tensor.numpy()\n        mask = mask.numpy()\n        values = array[mask]\n        cutoff = np.percentile(values, self.percentiles)\n        np.clip(array, *cutoff, out=array)\n        array -= array.min()  # [0, max]\n        array_max = array.max()\n        if array_max == 0:\n            message = (\n                f\'Rescaling image ""{image_name}"" not possible\'\n                \' due to division by zero\'\n            )\n            warnings.warn(message)\n            return tensor\n        array /= array.max()  # [0, 1]\n        out_range = self.out_max - self.out_min\n        array *= out_range  # [0, out_range]\n        array += self.out_min  # [out_min, out_max]\n        return torch.from_numpy(array)\n\n\n@deprecated(\'Rescale is deprecated. Use RescaleIntensity instead.\')\nclass Rescale(RescaleIntensity):\n    pass\n'"
torchio/transforms/preprocessing/intensity/z_normalization.py,2,"b'import torch\nfrom ....data.subject import Subject\nfrom ....torchio import DATA\nfrom .normalization_transform import NormalizationTransform, TypeMaskingMethod\n\n\nclass ZNormalization(NormalizationTransform):\n    """"""Subtract mean and divide by standard deviation.\n\n    Args:\n        masking_method: See\n            :py:class:`~torchio.transforms.preprocessing.normalization_transform.NormalizationTransform`.\n        p: Probability that this transform will be applied.\n    """"""\n    def __init__(\n            self,\n            masking_method: TypeMaskingMethod = None,\n            p: float = 1,\n            ):\n        super().__init__(masking_method=masking_method, p=p)\n\n    def apply_normalization(\n            self,\n            sample: Subject,\n            image_name: str,\n            mask: torch.Tensor,\n            ) -> None:\n        image_dict = sample[image_name]\n        image_dict[DATA] = self.znorm(\n            image_dict[DATA],\n            mask,\n        )\n\n    @staticmethod\n    def znorm(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        values = tensor[mask]\n        mean, std = values.mean(), values.std()\n        tensor = tensor - mean\n        tensor = tensor / std\n        return tensor\n'"
torchio/transforms/preprocessing/spatial/__init__.py,0,b''
torchio/transforms/preprocessing/spatial/bounds_transform.py,1,"b'from typing import Union, Tuple\nimport torch\nimport numpy as np\nfrom ....data.subject import Subject\nfrom ....torchio import DATA, AFFINE\nfrom ... import Transform\n\n\nTypeShape = Tuple[int, int, int]\nTypeSixBounds = Tuple[int, int, int, int, int, int]\nTypeBounds = Union[\n    int,\n    TypeShape,\n    TypeSixBounds,\n]\n\n\nclass BoundsTransform(Transform):\n    """"""Base class for transforms that change image bounds.\n\n    Args:\n        bounds_parameters: The meaning of this argument varies according to the\n            child class.\n        p: Probability that this transform will be applied.\n\n    """"""\n    def __init__(\n            self,\n            bounds_parameters: TypeBounds,\n            p: float = 1,\n            ):\n        super().__init__(p=p)\n        self.bounds_parameters = self.parse_bounds(bounds_parameters)\n\n    @property\n    def bounds_function(self):\n        raise NotImplementedError\n\n    @staticmethod\n    def parse_bounds(bounds_parameters: TypeBounds) -> Tuple[int, ...]:\n        try:\n            bounds_parameters = tuple(bounds_parameters)\n        except TypeError:\n            bounds_parameters = (bounds_parameters,)\n\n        # Check that numbers are integers\n        for number in bounds_parameters:\n            if not isinstance(number, (int, np.integer)) or number < 0:\n                message = (\n                    \'Bounds values must be integers greater or equal to zero,\'\n                    f\' not ""{bounds_parameters}"" of type {type(number)}\'\n                )\n                raise ValueError(message)\n        bounds_parameters = tuple(int(n) for n in bounds_parameters)\n        bounds_parameters_length = len(bounds_parameters)\n        if bounds_parameters_length == 6:\n            return bounds_parameters\n        if bounds_parameters_length == 1:\n            return 6 * bounds_parameters\n        if bounds_parameters_length == 3:\n            return tuple(np.repeat(bounds_parameters, 2).tolist())\n        message = (\n            \'Bounds parameter must be an integer or a tuple of\'\n            f\' 3 or 6 integers, not {bounds_parameters}\'\n        )\n        raise ValueError(message)\n\n    def apply_transform(self, sample: Subject) -> dict:\n        low = self.bounds_parameters[::2]\n        high = self.bounds_parameters[1::2]\n        for image_dict in sample.get_images(intensity_only=False):\n            image = self.nib_to_sitk(image_dict[DATA][0], image_dict[AFFINE])\n            result = self.bounds_function(image, low, high)\n            data, affine = self.sitk_to_nib(result)\n            tensor = torch.from_numpy(data).unsqueeze(0)\n            image_dict[DATA] = tensor\n            image_dict[AFFINE] = affine\n        return sample\n'"
torchio/transforms/preprocessing/spatial/crop.py,0,"b'from typing import Callable\nimport SimpleITK as sitk\nfrom .bounds_transform import BoundsTransform\n\n\nclass Crop(BoundsTransform):\n    r""""""Crop an image.\n\n    Args:\n        cropping: Tuple\n            :math:`(d_{ini}, d_{fin}, h_{ini}, h_{fin}, w_{ini}, w_{fin})`\n            defining the number of values cropped from the edges of each axis.\n            If the initial shape of the image is\n            :math:`D \\times H \\times W`, the final shape will be\n            :math:`(- d_{ini} + D - d_{fin}) \\times (- h_{ini} + H - h_{fin}) \\times (- w_{ini} + W - w_{fin})`.\n            If only three values :math:`(d, h, w)` are provided, then\n            :math:`d_{ini} = d_{fin} = d`,\n            :math:`h_{ini} = h_{fin} = h` and\n            :math:`w_{ini} = w_{fin} = w`.\n            If only one value :math:`n` is provided, then\n            :math:`d_{ini} = d_{fin} = h_{ini} = h_{fin} = w_{ini} = w_{fin} = n`.\n\n    """"""\n    @property\n    def bounds_function(self) -> Callable:\n        return sitk.Crop\n'"
torchio/transforms/preprocessing/spatial/crop_or_pad.py,2,"b'import warnings\nfrom typing import Union, Tuple, Optional\nimport numpy as np\nfrom deprecated import deprecated\nfrom .pad import Pad\nfrom .crop import Crop\nfrom .bounds_transform import BoundsTransform, TypeShape, TypeSixBounds\nfrom ....data.subject import Subject\nfrom ....utils import round_up\n\n\nclass CropOrPad(BoundsTransform):\n    """"""Crop and/or pad an image to a target shape.\n\n    This transform modifies the affine matrix associated to the volume so that\n    physical positions of the voxels are maintained.\n\n    Args:\n        target_shape: Tuple :math:`(D, H, W)`. If a single value :math:`N` is\n            provided, then :math:`D = H = W = N`.\n        padding_mode: Same as :attr:`padding_mode` in\n            :py:class:`~torchio.transforms.Pad`.\n        mask_name: If ``None``, the centers of the input and output volumes\n            will be the same.\n            If a string is given, the output volume center will be the center\n            of the bounding box of non-zero values in the image named\n            :py:attr:`mask_name`.\n        p: Probability that this transform will be applied.\n\n    Example:\n        >>> import torchio\n        >>> from torchio.tranforms import CropOrPad\n        >>> subject = torchio.Subject(\n        ...     torchio.Image(\'chest_ct\', \'subject_a_ct.nii.gz\', torchio.INTENSITY),\n        ...     torchio.Image(\'heart_mask\', \'subject_a_heart_seg.nii.gz\', torchio.LABEL),\n        ... )\n        >>> sample = torchio.ImagesDataset([subject])[0]\n        >>> sample[\'chest_ct\'].shape\n        torch.Size([1, 512, 512, 289])\n        >>> transform = CropOrPad(\n        ...     (120, 80, 180),\n        ...     mask_name=\'heart_mask\',\n        ... )\n        >>> transformed = transform(sample)\n        >>> transformed[\'chest_ct\'].shape\n        torch.Size([1, 120, 80, 180])\n    """"""\n    def __init__(\n            self,\n            target_shape: Union[int, TypeShape],\n            padding_mode: Union[str, float] = 0,\n            mask_name: Optional[str] = None,\n            p: float = 1,\n            ):\n        super().__init__(target_shape, p=p)\n        self.padding_mode = padding_mode\n        if mask_name is not None and not isinstance(mask_name, str):\n            message = (\n                \'If mask_name is not None, it must be a string,\'\n                f\' not {type(mask_name)}\'\n            )\n            raise ValueError(message)\n        self.mask_name = mask_name\n        if self.mask_name is None:\n            self.compute_crop_or_pad = self._compute_center_crop_or_pad\n        else:\n            if not isinstance(mask_name, str):\n                message = (\n                    \'If mask_name is not None, it must be a string,\'\n                    f\' not {type(mask_name)}\'\n                )\n                raise ValueError(message)\n            self.compute_crop_or_pad = self._compute_mask_center_crop_or_pad\n\n    @staticmethod\n    def _bbox_mask(\n            mask_volume: np.ndarray,\n            ) -> Tuple[np.ndarray, np.ndarray]:\n        """"""Return 6 coordinates of a 3D bounding box from a given mask.\n\n        Taken from `this SO question <https://stackoverflow.com/questions/31400769/bounding-box-of-numpy-array>`_.\n\n        Args:\n            mask_volume: 3D NumPy array.\n        """"""\n        i_any = np.any(mask_volume, axis=(1, 2))\n        j_any = np.any(mask_volume, axis=(0, 2))\n        k_any = np.any(mask_volume, axis=(0, 1))\n        i_min, i_max = np.where(i_any)[0][[0, -1]]\n        j_min, j_max = np.where(j_any)[0][[0, -1]]\n        k_min, k_max = np.where(k_any)[0][[0, -1]]\n        bb_min = np.array([i_min, j_min, k_min])\n        bb_max = np.array([i_max, j_max, k_max])\n        return bb_min, bb_max\n\n    @staticmethod\n    def _get_sample_shape(sample: Subject) -> TypeShape:\n        """"""Return the shape of the first image in the sample.""""""\n        sample.check_consistent_shape()\n        for image_dict in sample.get_images(intensity_only=False):\n            data = image_dict.spatial_shape  # remove channels dimension\n            break\n        return data\n\n    @staticmethod\n    def _get_six_bounds_parameters(\n            parameters: np.ndarray,\n            ) -> TypeSixBounds:\n        r""""""Compute bounds parameters for ITK filters.\n\n        Args:\n            parameters: Tuple :math:`(d, h, w)` with the number of voxels to be\n                cropped or padded.\n\n        Returns:\n            Tuple :math:`(d_{ini}, d_{fin}, h_{ini}, h_{fin}, w_{ini}, w_{fin})`,\n            where :math:`n_{ini} = \\left \\lceil \\frac{n}{2} \\right \\rceil` and\n            :math:`n_{fin} = \\left \\lfloor \\frac{n}{2} \\right \\rfloor`.\n\n        Example:\n            >>> p = np.array((4, 0, 7))\n            >>> _get_six_bounds_parameters(p)\n            (2, 2, 0, 0, 4, 3)\n\n        """"""\n        parameters = parameters / 2\n        result = []\n        for number in parameters:\n            ini, fin = int(np.ceil(number)), int(np.floor(number))\n            result.extend([ini, fin])\n        return tuple(result)\n\n    def _compute_cropping_padding_from_shapes(\n            self,\n            source_shape: TypeShape,\n            target_shape: TypeShape,\n            ) -> Tuple[Optional[TypeSixBounds], Optional[TypeSixBounds]]:\n        diff_shape = target_shape - source_shape\n\n        cropping = -np.minimum(diff_shape, 0)\n        if cropping.any():\n            cropping_params = self._get_six_bounds_parameters(cropping)\n        else:\n            cropping_params = None\n\n        padding = np.maximum(diff_shape, 0)\n        if padding.any():\n            padding_params = self._get_six_bounds_parameters(padding)\n        else:\n            padding_params = None\n\n        return padding_params, cropping_params\n\n    def _compute_center_crop_or_pad(\n            self,\n            sample: Subject,\n            ) -> Tuple[Optional[TypeSixBounds], Optional[TypeSixBounds]]:\n        source_shape = self._get_sample_shape(sample)\n        # The parent class turns the 3-element shape tuple (d, h, w)\n        # into a 6-element bounds tuple (d, d, h, h, w, w)\n        target_shape = np.array(self.bounds_parameters[::2])\n        parameters = self._compute_cropping_padding_from_shapes(\n            source_shape, target_shape)\n        padding_params, cropping_params = parameters\n        return padding_params, cropping_params\n\n    def _compute_mask_center_crop_or_pad(\n            self,\n            sample: Subject,\n            ) -> Tuple[Optional[TypeSixBounds], Optional[TypeSixBounds]]:\n        if self.mask_name not in sample:\n            message = (\n                f\'Mask name ""{self.mask_name}""\'\n                f\' not found in sample keys ""{tuple(sample.keys())}"".\'\n                \' Using volume center instead\'\n            )\n            warnings.warn(message)\n            return self._compute_center_crop_or_pad(sample=sample)\n\n        mask = sample[self.mask_name].numpy()\n\n        if not np.any(mask):\n            message = (\n                f\'All values found in the mask ""{self.mask_name}""\'\n                \' are zero. Using volume center instead\'\n            )\n            warnings.warn(message)\n            return self._compute_center_crop_or_pad(sample=sample)\n\n        # Original sample shape (from mask shape)\n        sample_shape = self._get_sample_shape(sample)  # remove channels dimension\n        # Calculate bounding box of the mask center\n        bb_min, bb_max = self._bbox_mask(mask[0])\n        # Coordinates of the mask center\n        center_mask = (bb_max - bb_min) / 2 + bb_min\n        # List of padding to do\n        padding = []\n        # Final cropping (after padding)\n        cropping = []\n        for dim, center_dimension in enumerate(center_mask):\n            # Compute coordinates of the target shape taken from the center of\n            # the mask\n            center_dim = round_up(center_dimension)\n            begin = center_dim - (self.bounds_parameters[2 * dim] / 2)\n            end = center_dim + (self.bounds_parameters[2 * dim + 1] / 2)\n            # Check if dimension needs padding (before or after)\n            begin_pad = round_up(abs(min(begin, 0)))\n            end_pad = round(max(end - sample_shape[dim], 0))\n            # Check if cropping is needed\n            begin_crop = round_up(max(begin, 0))\n            end_crop = abs(round(min(end - sample_shape[dim], 0)))\n            # Add padding values of the dim to the list\n            padding.append(begin_pad)\n            padding.append(end_pad)\n            # Add the slice of the dimension to take\n            cropping.append(begin_crop)\n            cropping.append(end_crop)\n        # Conversion for SimpleITK compatibility\n        padding = np.asarray(padding, dtype=int)\n        cropping = np.asarray(cropping, dtype=int)\n        padding_params = tuple(padding.tolist()) if padding.any() else None\n        cropping_params = tuple(cropping.tolist()) if cropping.any() else None\n        return padding_params, cropping_params\n\n    def apply_transform(self, sample: Subject) -> dict:\n        padding_params, cropping_params = self.compute_crop_or_pad(sample)\n        padding_kwargs = dict(\n            padding_mode=self.padding_mode)\n        if padding_params is not None:\n            sample = Pad(padding_params, **padding_kwargs)(sample)\n        if cropping_params is not None:\n            sample = Crop(cropping_params)(sample)\n        return sample\n\n\n@deprecated(\'CenterCropOrPad is deprecated. Use CropOrPad instead.\')\nclass CenterCropOrPad(CropOrPad):\n    """"""Crop or pad around image center.""""""\n'"
torchio/transforms/preprocessing/spatial/pad.py,1,"b'from numbers import Number\nfrom typing import Callable, Union\nimport SimpleITK as sitk\nfrom .bounds_transform import BoundsTransform, TypeBounds\n\n\nclass Pad(BoundsTransform):\n    r""""""Pad an image.\n\n    Args:\n        padding: Tuple\n            :math:`(d_{ini}, d_{fin}, h_{ini}, h_{fin}, w_{ini}, w_{fin})`\n            defining the number of values padded to the edges of each axis.\n            If the initial shape of the image is\n            :math:`D \\times H \\times W`, the final shape will be\n            :math:`(d_{ini} + D + d_{fin}) \\times (h_{ini} + H + h_{fin}) \\times (w_{ini} + W + w_{fin})`.\n            If only three values :math:`(d, h, w)` are provided, then\n            :math:`d_{ini} = d_{fin} = d`,\n            :math:`h_{ini} = h_{fin} = h` and\n            :math:`w_{ini} = w_{fin} = w`.\n            If only one value :math:`n` is provided, then\n            :math:`d_{ini} = d_{fin} = h_{ini} = h_{fin} = w_{ini} = w_{fin} = n`.\n        padding_mode:\n            Type of padding. Should be one of:\n\n            - A number. Pad with a constant value.\n\n            - ``reflect`` Pad with reflection of image without repeating the last value on the edge.\n\n            - ``mirror`` Same as ``reflect``.\n\n            - ``edge`` Pad with the last value at the edge of the image.\n\n            - ``replicate`` Same as ``edge``.\n\n            - ``circular`` Pad with the wrap of the vector along the axis. The first values are used to pad the end and the end values are used to pad the beginning.\n\n            - ``wrap`` Same as ``circular``.\n\n        p: Probability that this transform will be applied.\n\n    """"""\n\n    PADDING_FUNCTIONS = {\n        \'reflect\': sitk.MirrorPad,\n        \'mirror\': sitk.MirrorPad,\n        \'edge\': sitk.ZeroFluxNeumannPad,\n        \'replicate\': sitk.ZeroFluxNeumannPad,\n        \'circular\': sitk.WrapPad,\n        \'wrap\': sitk.WrapPad,\n    }\n\n    def __init__(\n            self,\n            padding: TypeBounds,\n            padding_mode: Union[str, float] = 0,\n            p: float = 1,\n            ):\n        """"""\n        padding_mode can be \'constant\', \'reflect\', \'replicate\' or \'circular\'.\n        See https://pytorch.org/docs/stable/nn.functional.html#pad for more\n        information about this transform.\n        """"""\n        super().__init__(padding, p=p)\n        self.padding_mode, self.fill = self.parse_padding_mode(padding_mode)\n\n    @classmethod\n    def parse_padding_mode(cls, padding_mode):\n        if padding_mode in cls.PADDING_FUNCTIONS:\n            fill = None\n        elif isinstance(padding_mode, Number):\n            fill = padding_mode\n            padding_mode = \'constant\'\n        else:\n            message = (\n                f\'Padding mode ""{padding_mode}"" not valid. Valid options are\'\n                f\' {list(cls.PADDING_FUNCTIONS.keys())} or a number\'\n            )\n            raise KeyError(message)\n        return padding_mode, fill\n\n    @property\n    def bounds_function(self) -> Callable:\n        if self.fill is not None:\n            function = _pad_with_fill(self.fill)\n        else:\n            function = self.PADDING_FUNCTIONS[self.padding_mode]\n        return function\n\n\ndef _pad_with_fill(fill):\n    def wrapped(image, bounds1, bounds2):\n        return sitk.ConstantPad(image, bounds1, bounds2, fill)\n    return wrapped\n'"
torchio/transforms/preprocessing/spatial/resample.py,4,"b'from numbers import Number\nfrom typing import Union, Tuple, Optional\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\nimport nibabel as nib\nfrom nibabel.processing import resample_to_output, resample_from_to\n\nfrom ....data.subject import Subject\nfrom ....data.image import Image\nfrom ....torchio import DATA, AFFINE, TYPE, INTENSITY\nfrom ... import Interpolation\nfrom ... import Transform\n\n\nTypeSpacing = Union[float, Tuple[float, float, float]]\nTypeTarget = Tuple[\n    Optional[Union[Image, str]],\n    Optional[Tuple[float, float, float]],\n]\n\n\nclass Resample(Transform):\n    """"""Change voxel spacing by resampling.\n\n    Args:\n        target: Tuple :math:`(s_d, s_h, s_w)`. If only one value\n            :math:`n` is specified, then :math:`s_d = s_h = s_w = n`.\n            If a string or :py:class:`~pathlib.Path` is given,\n            all images will be resampled using the image\n            with that name as reference or found at the path.\n        pre_affine_name: Name of the *image key* (not subject key) storing an\n            affine matrix that will be applied to the image header before\n            resampling. If ``None``, the image is resampled with an identity\n            transform. See usage in the example below.\n        image_interpolation: String that defines the interpolation technique.\n            Supported interpolation techniques for resampling\n            are \'nearest\',\'linear\' and \'bspline\'.\n            Using a member of :py:class:`torchio.Interpolation` is still\n            supported for backward compatibility,\n            but will be removed in a future version.\n        p: Probability that this transform will be applied.\n\n\n    .. note:: Resampling is performed using\n        :py:meth:`nibabel.processing.resample_to_output` or\n        :py:meth:`nibabel.processing.resample_from_to`, depending on whether\n        the target is a spacing or a reference image.\n\n    Example:\n        >>> import torchio\n        >>> from torchio.transforms import Resample\n        >>> from pathlib import Path\n        >>> transform = Resample(1)                     # resample all images to 1mm iso\n        >>> transform = Resample((1, 1, 1))             # resample all images to 1mm iso\n        >>> transform = Resample(\'t1\')                  # resample all images to \'t1\' image space\n        >>> transform = Resample(\'path/to/ref.nii.gz\')  # resample all images to space of image at this path\n        >>>\n        >>> # Affine matrices are added to each image\n        >>> matrix_to_mni = some_4_by_4_array  # e.g. result of registration to MNI space\n        >>> subject = torchio.Subject(\n        ...     t1=Image(\'t1.nii.gz\', torchio.INTENSITY, to_mni=matrix_to_mni),\n        ...     mni=Image(\'mni_152_lin.nii.gz\', torchio.INTENSITY),\n        ... )\n        >>> resample = Resample(\n        ...     \'mni\',  # this is a subject key\n        ...     affine_name=\'to_mni\',  # this is an image key\n        ... )\n        >>> dataset = torchio.ImagesDataset([subject], transform=resample)\n        >>> sample = dataset[0]  # sample[\'t1\'] is now in MNI space\n    """"""\n    def __init__(\n            self,\n            target: Union[TypeSpacing, str, Path],\n            image_interpolation: str = \'linear\',\n            pre_affine_name: Optional[str] = None,\n            p: float = 1,\n            ):\n        super().__init__(p=p)\n        self.reference_image, self.target_spacing = self.parse_target(target)\n        self.interpolation_order = self.parse_interpolation(image_interpolation)\n        self.affine_name = pre_affine_name\n\n    def parse_target(\n            self,\n            target: Union[TypeSpacing, str],\n            ) -> TypeTarget:\n        if isinstance(target, (str, Path)):\n            if Path(target).is_file():\n                reference_image = Image(target, INTENSITY).load()\n            else:\n                reference_image = target\n            target_spacing = None\n        else:\n            reference_image = None\n            target_spacing = self.parse_spacing(target)\n        return reference_image, target_spacing\n\n    @staticmethod\n    def parse_spacing(spacing: TypeSpacing) -> Tuple[float, float, float]:\n        if isinstance(spacing, tuple) and len(spacing) == 3:\n            result = spacing\n        elif isinstance(spacing, Number):\n            result = 3 * (spacing,)\n        else:\n            message = (\n                \'Target must be a string, a positive number\'\n                f\' or a tuple of positive numbers, not {type(spacing)}\'\n            )\n            raise ValueError(message)\n        if np.any(np.array(spacing) <= 0):\n            raise ValueError(f\'Spacing must be positive, not ""{spacing}""\')\n        return result\n\n    def parse_interpolation(self, interpolation: str) -> int:\n        interpolation = super().parse_interpolation(interpolation)\n\n        if interpolation in (Interpolation.NEAREST, \'nearest\'):\n            order = 0\n        elif interpolation in (Interpolation.LINEAR, \'linear\'):\n            order = 1\n        elif interpolation in (Interpolation.BSPLINE, \'bspline\'):\n            order = 3\n        else:\n            message = f\'Interpolation not implemented yet: {interpolation}\'\n            raise NotImplementedError(message)\n        return order\n\n    @staticmethod\n    def check_affine(affine_name: str, image_dict: dict):\n        if not isinstance(affine_name, str):\n            message = (\n                \'Affine name argument must be a string,\'\n                f\' not {type(affine_name)}\'\n            )\n            raise TypeError(message)\n        if affine_name in image_dict:\n            matrix = image_dict[affine_name]\n            if not isinstance(matrix, np.ndarray):\n                message = (\n                    \'The affine matrix must be a NumPy array,\'\n                    f\' not {type(matrix)}\'\n                )\n                raise TypeError(message)\n            if matrix.shape != (4, 4):\n                message = (\n                    \'The affine matrix shape must be (4, 4),\'\n                    f\' not {matrix.shape}\'\n                )\n                raise ValueError(message)\n\n    @staticmethod\n    def check_affine_key_presence(affine_name: str, sample: Subject):\n        for image_dict in sample.get_images(intensity_only=False):\n            if affine_name in image_dict:\n                return\n        message = (\n            f\'An affine name was given (""{affine_name}""), but it was not found\'\n            \' in any image in the sample\'\n        )\n        raise ValueError(message)\n\n    def apply_transform(self, sample: Subject) -> dict:\n        use_reference = self.reference_image is not None\n        use_pre_affine = self.affine_name is not None\n        if use_pre_affine:\n            self.check_affine_key_presence(self.affine_name, sample)\n        images_dict = sample.get_images_dict(intensity_only=False).items()\n        for image_name, image_dict in images_dict:\n            # Do not resample the reference image if there is one\n            if use_reference and image_name == self.reference_image:\n                continue\n\n            # Choose interpolator\n            if image_dict[TYPE] != INTENSITY:\n                interpolation_order = 0  # nearest neighbor\n            else:\n                interpolation_order = self.interpolation_order\n\n            # Apply given affine matrix if found in image\n            if use_pre_affine and self.affine_name in image_dict:\n                self.check_affine(self.affine_name, image_dict)\n                matrix = image_dict[self.affine_name]\n                image_dict[AFFINE] = matrix @ image_dict[AFFINE]\n\n            # Resample\n            args = image_dict[DATA], image_dict[AFFINE], interpolation_order\n            if use_reference:\n                if isinstance(self.reference_image, str):\n                    try:\n                        ref_image_dict = sample[self.reference_image]\n                    except KeyError as error:\n                        message = (\n                            f\'Reference name ""{self.reference_image}""\'\n                            \' not found in sample\'\n                        )\n                        raise ValueError(message) from error\n                    reference = ref_image_dict[DATA], ref_image_dict[AFFINE]\n                else:\n                    reference = self.reference_image\n                kwargs = dict(reference=reference)\n            else:\n                kwargs = dict(target_spacing=self.target_spacing)\n            image_dict[DATA], image_dict[AFFINE] = self.apply_resample(\n                *args,\n                **kwargs,\n            )\n        return sample\n\n    @staticmethod\n    def apply_resample(\n            tensor: torch.Tensor,\n            affine: np.ndarray,\n            interpolation_order: int,\n            target_spacing: Optional[Tuple[float, float, float]] = None,\n            reference: Optional[Tuple[torch.Tensor, np.ndarray]] = None,\n            ) -> Tuple[torch.Tensor, np.ndarray]:\n        array = tensor.numpy()[0]\n        if reference is None:\n            nii = resample_to_output(\n                nib.Nifti1Image(array, affine),\n                voxel_sizes=target_spacing,\n                order=interpolation_order,\n            )\n        else:\n            reference_tensor, reference_affine = reference\n            reference_array = reference_tensor.numpy()[0]\n            nii = resample_from_to(\n                nib.Nifti1Image(array, affine),\n                nib.Nifti1Image(reference_array, reference_affine),\n                order=interpolation_order,\n            )\n        tensor = torch.from_numpy(nii.get_fdata(dtype=np.float32))\n        tensor = tensor.unsqueeze(dim=0)\n        return tensor, nii.affine\n'"
torchio/transforms/preprocessing/spatial/to_canonical.py,1,"b'import torch\nimport numpy as np\nimport nibabel as nib\nfrom ....data.subject import Subject\nfrom ....torchio import DATA, AFFINE\nfrom ... import Transform\n\n\nclass ToCanonical(Transform):\n    """"""Reorder the data to be closest to canonical (RAS+) orientation.\n\n    This transform reorders the voxels and modifies the affine matrix so that\n    the voxel orientations are nearest to:\n\n        1. First voxel axis goes from left to Right\n        2. Second voxel axis goes from posterior to Anterior\n        3. Third voxel axis goes from inferior to Superior\n\n    See `NiBabel docs about image orientation`_ for more information.\n\n    Args:\n        p: Probability that this transform will be applied.\n\n    .. note:: The reorientation is performed using\n        :py:meth:`nibabel.as_closest_canonical`.\n\n    .. _NiBabel docs about image orientation: https://nipy.org/nibabel/image_orientation.html\n\n    """"""\n\n    def apply_transform(self, sample: Subject) -> dict:\n        for image_dict in sample.get_images(intensity_only=False):\n            affine = image_dict[AFFINE]\n            if nib.aff2axcodes(affine) == tuple(\'RAS\'):\n                continue\n            array = image_dict[DATA][0].numpy()\n            nii = nib.Nifti1Image(array, affine)\n            reoriented = nib.as_closest_canonical(nii)\n            array = reoriented.get_fdata(dtype=np.float32)\n            # https://github.com/facebookresearch/InferSent/issues/99#issuecomment-446175325\n            array = array.copy()[np.newaxis, ...]\n            image_dict[DATA] = torch.from_numpy(array)\n            image_dict[AFFINE] = reoriented.affine\n        return sample\n'"
