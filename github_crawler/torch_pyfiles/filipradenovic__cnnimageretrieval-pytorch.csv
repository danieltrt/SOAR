file_path,api_count,code
cirtorch/__init__.py,0,"b'from . import datasets, examples, layers, networks, utils\n\nfrom .datasets import datahelpers, genericdataset, testdataset, traindataset\nfrom .layers import functional, loss, normalization, pooling\nfrom .networks import imageretrievalnet\nfrom .utils import general, download, evaluate, whiten'"
cirtorch/datasets/__init__.py,0,b''
cirtorch/datasets/datahelpers.py,1,"b'import os\nfrom PIL import Image\n\nimport torch\n\ndef cid2filename(cid, prefix):\n    """"""\n    Creates a training image path out of its CID name\n    \n    Arguments\n    ---------\n    cid      : name of the image\n    prefix   : root directory where images are saved\n    \n    Returns\n    -------\n    filename : full image filename\n    """"""\n    return os.path.join(prefix, cid[-2:], cid[-4:-2], cid[-6:-4], cid)\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\ndef imresize(img, imsize):\n    img.thumbnail((imsize, imsize), Image.ANTIALIAS)\n    return img\n\ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\ndef collate_tuples(batch):\n    if len(batch) == 1:\n        return [batch[0][0]], [batch[0][1]]\n    return [batch[i][0] for i in range(len(batch))], [batch[i][1] for i in range(len(batch))]'"
cirtorch/datasets/genericdataset.py,2,"b'import os\nimport pdb\n\nimport torch\nimport torch.utils.data as data\n\nfrom cirtorch.datasets.datahelpers import default_loader, imresize\n\n\nclass ImagesFromList(data.Dataset):\n    """"""A generic data loader that loads images from a list \n        (Based on ImageFolder from pytorch)\n\n    Args:\n        root (string): Root directory path.\n        images (list): Relative image paths as strings.\n        imsize (int, Default: None): Defines the maximum size of longer image side\n        bbxs (list): List of (x1,y1,x2,y2) tuples to crop the query images\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        loader (callable, optional): A function to load an image given its path.\n\n     Attributes:\n        images_fn (list): List of full image filename\n    """"""\n\n    def __init__(self, root, images, imsize=None, bbxs=None, transform=None, loader=default_loader):\n\n        images_fn = [os.path.join(root,images[i]) for i in range(len(images))]\n\n        if len(images_fn) == 0:\n            raise(RuntimeError(""Dataset contains 0 images!""))\n\n        self.root = root\n        self.images = images\n        self.imsize = imsize\n        self.images_fn = images_fn\n        self.bbxs = bbxs\n        self.transform = transform\n        self.loader = loader\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            image (PIL): Loaded image\n        """"""\n        path = self.images_fn[index]\n        img = self.loader(path)\n        imfullsize = max(img.size)\n\n        if self.bbxs is not None:\n            img = img.crop(self.bbxs[index])\n\n        if self.imsize is not None:\n            if self.bbxs is not None:\n                img = imresize(img, self.imsize * max(img.size) / imfullsize)\n            else:\n                img = imresize(img, self.imsize)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img\n\n    def __len__(self):\n        return len(self.images_fn)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of images: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\nclass ImagesFromDataList(data.Dataset):\n    """"""A generic data loader that loads images given as an array of pytorch tensors\n        (Based on ImageFolder from pytorch)\n\n    Args:\n        images (list): Images as tensors.\n        transform (callable, optional): A function/transform that image as a tensors\n            and returns a transformed version. E.g, ``normalize`` with mean and std\n    """"""\n\n    def __init__(self, images, transform=None):\n\n        if len(images) == 0:\n            raise(RuntimeError(""Dataset contains 0 images!""))\n\n        self.images = images\n        self.transform = transform\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            image (Tensor): Loaded image\n        """"""\n        img = self.images[index]\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if len(img.size()):\n            img = img.unsqueeze(0)\n\n        return img\n\n    def __len__(self):\n        return len(self.images)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of images: {}\\n\'.format(self.__len__())\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str'"
cirtorch/datasets/testdataset.py,0,"b""import os\nimport pickle\n\nDATASETS = ['oxford5k', 'paris6k', 'roxford5k', 'rparis6k']\n\ndef configdataset(dataset, dir_main):\n\n    dataset = dataset.lower()\n\n    if dataset not in DATASETS:    \n        raise ValueError('Unknown dataset: {}!'.format(dataset))\n\n    # loading imlist, qimlist, and gnd, in cfg as a dict\n    gnd_fname = os.path.join(dir_main, dataset, 'gnd_{}.pkl'.format(dataset))\n    with open(gnd_fname, 'rb') as f:\n        cfg = pickle.load(f)\n    cfg['gnd_fname'] = gnd_fname\n\n    cfg['ext'] = '.jpg'\n    cfg['qext'] = '.jpg'\n    cfg['dir_data'] = os.path.join(dir_main, dataset)\n    cfg['dir_images'] = os.path.join(cfg['dir_data'], 'jpg')\n\n    cfg['n'] = len(cfg['imlist'])\n    cfg['nq'] = len(cfg['qimlist'])\n\n    cfg['im_fname'] = config_imname\n    cfg['qim_fname'] = config_qimname\n\n    cfg['dataset'] = dataset\n\n    return cfg\n\ndef config_imname(cfg, i):\n    return os.path.join(cfg['dir_images'], cfg['imlist'][i] + cfg['ext'])\n\ndef config_qimname(cfg, i):\n    return os.path.join(cfg['dir_images'], cfg['qimlist'][i] + cfg['qext'])\n"""
cirtorch/datasets/traindataset.py,17,"b'import os\nimport pickle\nimport pdb\n\nimport torch\nimport torch.utils.data as data\n\nfrom cirtorch.datasets.datahelpers import default_loader, imresize, cid2filename\nfrom cirtorch.datasets.genericdataset import ImagesFromList\nfrom cirtorch.utils.general import get_data_root\n\nclass TuplesDataset(data.Dataset):\n    """"""Data loader that loads training and validation tuples of \n        Radenovic etal ECCV16: CNN image retrieval learns from BoW\n\n    Args:\n        name (string): dataset name: \'retrieval-sfm-120k\'\n        mode (string): \'train\' or \'val\' for training and validation parts of dataset\n        imsize (int, Default: None): Defines the maximum size of longer image side\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        loader (callable, optional): A function to load an image given its path.\n        nnum (int, Default:5): Number of negatives for a query image in a training tuple\n        qsize (int, Default:1000): Number of query images, ie number of (q,p,n1,...nN) tuples, to be processed in one epoch\n        poolsize (int, Default:10000): Pool size for negative images re-mining\n\n     Attributes:\n        images (list): List of full filenames for each image\n        clusters (list): List of clusterID per image\n        qpool (list): List of all query image indexes\n        ppool (list): List of positive image indexes, each corresponding to query at the same position in qpool\n\n        qidxs (list): List of qsize query image indexes to be processed in an epoch\n        pidxs (list): List of qsize positive image indexes, each corresponding to query at the same position in qidxs\n        nidxs (list): List of qsize tuples of negative images\n                        Each nidxs tuple contains nnum images corresponding to query image at the same position in qidxs\n\n        Lists qidxs, pidxs, nidxs are refreshed by calling the ``create_epoch_tuples()`` method, \n            ie new q-p pairs are picked and negative images are remined\n    """"""\n\n    def __init__(self, name, mode, imsize=None, nnum=5, qsize=2000, poolsize=20000, transform=None, loader=default_loader):\n\n        if not (mode == \'train\' or mode == \'val\'):\n            raise(RuntimeError(""MODE should be either train or val, passed as string""))\n\n        if name.startswith(\'retrieval-SfM\'):\n            # setting up paths\n            data_root = get_data_root()\n            db_root = os.path.join(data_root, \'train\', name)\n            ims_root = os.path.join(db_root, \'ims\')\n    \n            # loading db\n            db_fn = os.path.join(db_root, \'{}.pkl\'.format(name))\n            with open(db_fn, \'rb\') as f:\n                db = pickle.load(f)[mode]\n    \n            # setting fullpath for images\n            self.images = [cid2filename(db[\'cids\'][i], ims_root) for i in range(len(db[\'cids\']))]\n\n        elif name.startswith(\'gl\'):\n            ## TODO: NOT IMPLEMENTED YET PROPOERLY (WITH AUTOMATIC DOWNLOAD)\n\n            # setting up paths\n            db_root = \'/mnt/fry2/users/datasets/landmarkscvprw18/recognition/\'\n            ims_root = os.path.join(db_root, \'images\', \'train\')\n    \n            # loading db\n            db_fn = os.path.join(db_root, \'{}.pkl\'.format(name))\n            with open(db_fn, \'rb\') as f:\n                db = pickle.load(f)[mode]\n    \n            # setting fullpath for images\n            self.images = [os.path.join(ims_root, db[\'cids\'][i]+\'.jpg\') for i in range(len(db[\'cids\']))]\n        else:\n            raise(RuntimeError(""Unknown dataset name!""))\n\n        # initializing tuples dataset\n        self.name = name\n        self.mode = mode\n        self.imsize = imsize\n        self.clusters = db[\'cluster\']\n        self.qpool = db[\'qidxs\']\n        self.ppool = db[\'pidxs\']\n\n        ## If we want to keep only unique q-p pairs \n        ## However, ordering of pairs will change, although that is not important\n        # qpidxs = list(set([(self.qidxs[i], self.pidxs[i]) for i in range(len(self.qidxs))]))\n        # self.qidxs = [qpidxs[i][0] for i in range(len(qpidxs))]\n        # self.pidxs = [qpidxs[i][1] for i in range(len(qpidxs))]\n\n        # size of training subset for an epoch\n        self.nnum = nnum\n        self.qsize = min(qsize, len(self.qpool))\n        self.poolsize = min(poolsize, len(self.images))\n        self.qidxs = None\n        self.pidxs = None\n        self.nidxs = None\n\n        self.transform = transform\n        self.loader = loader\n\n        self.print_freq = 10\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            images tuple (q,p,n1,...,nN): Loaded train/val tuple at index of self.qidxs\n        """"""\n        if self.__len__() == 0:\n            raise(RuntimeError(""List qidxs is empty. Run ``dataset.create_epoch_tuples(net)`` method to create subset for train/val!""))\n\n        output = []\n        # query image\n        output.append(self.loader(self.images[self.qidxs[index]]))\n        # positive image\n        output.append(self.loader(self.images[self.pidxs[index]]))\n        # negative images\n        for i in range(len(self.nidxs[index])):\n            output.append(self.loader(self.images[self.nidxs[index][i]]))\n\n        if self.imsize is not None:\n            output = [imresize(img, self.imsize) for img in output]\n        \n        if self.transform is not None:\n            output = [self.transform(output[i]).unsqueeze_(0) for i in range(len(output))]\n\n        target = torch.Tensor([-1, 1] + [0]*len(self.nidxs[index]))\n\n        return output, target\n\n    def __len__(self):\n        # if not self.qidxs:\n        #     return 0\n        # return len(self.qidxs)\n        return self.qsize\n\n    def __repr__(self):\n        fmt_str = self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Name and mode: {} {}\\n\'.format(self.name, self.mode)\n        fmt_str += \'    Number of images: {}\\n\'.format(len(self.images))\n        fmt_str += \'    Number of training tuples: {}\\n\'.format(len(self.qpool))\n        fmt_str += \'    Number of negatives per tuple: {}\\n\'.format(self.nnum)\n        fmt_str += \'    Number of tuples processed in an epoch: {}\\n\'.format(self.qsize)\n        fmt_str += \'    Pool size for negative remining: {}\\n\'.format(self.poolsize)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\n    def create_epoch_tuples(self, net):\n\n        print(\'>> Creating tuples for an epoch of {}-{}...\'.format(self.name, self.mode))\n        print("">>>> used network: "")\n        print(net.meta_repr())\n\n        ## ------------------------\n        ## SELECTING POSITIVE PAIRS\n        ## ------------------------\n\n        # draw qsize random queries for tuples\n        idxs2qpool = torch.randperm(len(self.qpool))[:self.qsize]\n        self.qidxs = [self.qpool[i] for i in idxs2qpool]\n        self.pidxs = [self.ppool[i] for i in idxs2qpool]\n\n        ## ------------------------\n        ## SELECTING NEGATIVE PAIRS\n        ## ------------------------\n\n        # if nnum = 0 create dummy nidxs\n        # useful when only positives used for training\n        if self.nnum == 0:\n            self.nidxs = [[] for _ in range(len(self.qidxs))]\n            return 0\n\n        # draw poolsize random images for pool of negatives images\n        idxs2images = torch.randperm(len(self.images))[:self.poolsize]\n\n        # prepare network\n        net.cuda()\n        net.eval()\n\n        # no gradients computed, to reduce memory and increase speed\n        with torch.no_grad():\n\n            print(\'>> Extracting descriptors for query images...\')\n            # prepare query loader\n            loader = torch.utils.data.DataLoader(\n                ImagesFromList(root=\'\', images=[self.images[i] for i in self.qidxs], imsize=self.imsize, transform=self.transform),\n                batch_size=1, shuffle=False, num_workers=8, pin_memory=True\n            )\n            # extract query vectors\n            qvecs = torch.zeros(net.meta[\'outputdim\'], len(self.qidxs)).cuda()\n            for i, input in enumerate(loader):\n                qvecs[:, i] = net(input.cuda()).data.squeeze()\n                if (i+1) % self.print_freq == 0 or (i+1) == len(self.qidxs):\n                    print(\'\\r>>>> {}/{} done...\'.format(i+1, len(self.qidxs)), end=\'\')\n            print(\'\')\n\n            print(\'>> Extracting descriptors for negative pool...\')\n            # prepare negative pool data loader\n            loader = torch.utils.data.DataLoader(\n                ImagesFromList(root=\'\', images=[self.images[i] for i in idxs2images], imsize=self.imsize, transform=self.transform),\n                batch_size=1, shuffle=False, num_workers=8, pin_memory=True\n            )\n            # extract negative pool vectors\n            poolvecs = torch.zeros(net.meta[\'outputdim\'], len(idxs2images)).cuda()\n            for i, input in enumerate(loader):\n                poolvecs[:, i] = net(input.cuda()).data.squeeze()\n                if (i+1) % self.print_freq == 0 or (i+1) == len(idxs2images):\n                    print(\'\\r>>>> {}/{} done...\'.format(i+1, len(idxs2images)), end=\'\')\n            print(\'\')\n\n            print(\'>> Searching for hard negatives...\')\n            # compute dot product scores and ranks on GPU\n            scores = torch.mm(poolvecs.t(), qvecs)\n            scores, ranks = torch.sort(scores, dim=0, descending=True)\n            avg_ndist = torch.tensor(0).float().cuda()  # for statistics\n            n_ndist = torch.tensor(0).float().cuda()  # for statistics\n            # selection of negative examples\n            self.nidxs = []\n            for q in range(len(self.qidxs)):\n                # do not use query cluster,\n                # those images are potentially positive\n                qcluster = self.clusters[self.qidxs[q]]\n                clusters = [qcluster]\n                nidxs = []\n                r = 0\n                while len(nidxs) < self.nnum:\n                    potential = idxs2images[ranks[r, q]]\n                    # take at most one image from the same cluster\n                    if not self.clusters[potential] in clusters:\n                        nidxs.append(potential)\n                        clusters.append(self.clusters[potential])\n                        avg_ndist += torch.pow(qvecs[:,q]-poolvecs[:,ranks[r, q]]+1e-6, 2).sum(dim=0).sqrt()\n                        n_ndist += 1\n                    r += 1\n                self.nidxs.append(nidxs)\n            print(\'>>>> Average negative l2-distance: {:.2f}\'.format(avg_ndist/n_ndist))\n            print(\'>>>> Done\')\n\n        return (avg_ndist/n_ndist).item()  # return average negative l2-distance\n'"
cirtorch/examples/__init__.py,0,b''
cirtorch/examples/test.py,11,"b'import argparse\nimport os\nimport time\nimport pickle\nimport pdb\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.model_zoo import load_url\nfrom torchvision import transforms\n\nfrom cirtorch.networks.imageretrievalnet import init_network, extract_vectors\nfrom cirtorch.datasets.datahelpers import cid2filename\nfrom cirtorch.datasets.testdataset import configdataset\nfrom cirtorch.utils.download import download_train, download_test\nfrom cirtorch.utils.whiten import whitenlearn, whitenapply\nfrom cirtorch.utils.evaluate import compute_map_and_print\nfrom cirtorch.utils.general import get_data_root, htime\n\nPRETRAINED = {\n    \'retrievalSfM120k-vgg16-gem\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/retrievalSfM120k-vgg16-gem-b4dcdc6.pth\',\n    \'retrievalSfM120k-resnet101-gem\'    : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/retrievalSfM120k-resnet101-gem-b80fb85.pth\',\n    # new networks with whitening learned end-to-end\n    \'rSfM120k-tl-resnet50-gem-w\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/rSfM120k-tl-resnet50-gem-w-97bf910.pth\',\n    \'rSfM120k-tl-resnet101-gem-w\'       : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/rSfM120k-tl-resnet101-gem-w-a155e54.pth\',\n    \'rSfM120k-tl-resnet152-gem-w\'       : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/rSfM120k-tl-resnet152-gem-w-f39cada.pth\',\n    \'gl18-tl-resnet50-gem-w\'            : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/gl18/gl18-tl-resnet50-gem-w-83fdc30.pth\',\n    \'gl18-tl-resnet101-gem-w\'           : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/gl18/gl18-tl-resnet101-gem-w-a4d43db.pth\',\n    \'gl18-tl-resnet152-gem-w\'           : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/gl18/gl18-tl-resnet152-gem-w-21278d5.pth\',\n}\n\ndatasets_names = [\'oxford5k\', \'paris6k\', \'roxford5k\', \'rparis6k\']\nwhitening_names = [\'retrieval-SfM-30k\', \'retrieval-SfM-120k\']\n\nparser = argparse.ArgumentParser(description=\'PyTorch CNN Image Retrieval Testing\')\n\n# network\ngroup = parser.add_mutually_exclusive_group(required=True)\ngroup.add_argument(\'--network-path\', \'-npath\', metavar=\'NETWORK\',\n                    help=""pretrained network or network path (destination where network is saved)"")\ngroup.add_argument(\'--network-offtheshelf\', \'-noff\', metavar=\'NETWORK\',\n                    help=""off-the-shelf network, in the format \'ARCHITECTURE-POOLING\' or \'ARCHITECTURE-POOLING-{reg-lwhiten-whiten}\',"" + \n                        "" examples: \'resnet101-gem\' | \'resnet101-gem-reg\' | \'resnet101-gem-whiten\' | \'resnet101-gem-lwhiten\' | \'resnet101-gem-reg-whiten\'"")\n\n# test options\nparser.add_argument(\'--datasets\', \'-d\', metavar=\'DATASETS\', default=\'oxford5k,paris6k\',\n                    help=""comma separated list of test datasets: "" + \n                        "" | "".join(datasets_names) + \n                        "" (default: \'oxford5k,paris6k\')"")\nparser.add_argument(\'--image-size\', \'-imsize\', default=1024, type=int, metavar=\'N\',\n                    help=""maximum size of longer image side used for testing (default: 1024)"")\nparser.add_argument(\'--multiscale\', \'-ms\', metavar=\'MULTISCALE\', default=\'[1]\', \n                    help=""use multiscale vectors for testing, "" + \n                    "" examples: \'[1]\' | \'[1, 1/2**(1/2), 1/2]\' | \'[1, 2**(1/2), 1/2**(1/2)]\' (default: \'[1]\')"")\nparser.add_argument(\'--whitening\', \'-w\', metavar=\'WHITENING\', default=None, choices=whitening_names,\n                    help=""dataset used to learn whitening for testing: "" + \n                        "" | "".join(whitening_names) + \n                        "" (default: None)"")\n\n# GPU ID\nparser.add_argument(\'--gpu-id\', \'-g\', default=\'0\', metavar=\'N\',\n                    help=""gpu id used for testing (default: \'0\')"")\n\ndef main():\n    args = parser.parse_args()\n\n    # check if there are unknown datasets\n    for dataset in args.datasets.split(\',\'):\n        if dataset not in datasets_names:\n            raise ValueError(\'Unsupported or unknown dataset: {}!\'.format(dataset))\n\n    # check if test dataset are downloaded\n    # and download if they are not\n    download_train(get_data_root())\n    download_test(get_data_root())\n\n    # setting up the visible GPU\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\n    # loading network from path\n    if args.network_path is not None:\n\n        print("">> Loading network:\\n>>>> \'{}\'"".format(args.network_path))\n        if args.network_path in PRETRAINED:\n            # pretrained networks (downloaded automatically)\n            state = load_url(PRETRAINED[args.network_path], model_dir=os.path.join(get_data_root(), \'networks\'))\n        else:\n            # fine-tuned network from path\n            state = torch.load(args.network_path)\n\n        # parsing net params from meta\n        # architecture, pooling, mean, std required\n        # the rest has default values, in case that is doesnt exist\n        net_params = {}\n        net_params[\'architecture\'] = state[\'meta\'][\'architecture\']\n        net_params[\'pooling\'] = state[\'meta\'][\'pooling\']\n        net_params[\'local_whitening\'] = state[\'meta\'].get(\'local_whitening\', False)\n        net_params[\'regional\'] = state[\'meta\'].get(\'regional\', False)\n        net_params[\'whitening\'] = state[\'meta\'].get(\'whitening\', False)\n        net_params[\'mean\'] = state[\'meta\'][\'mean\']\n        net_params[\'std\'] = state[\'meta\'][\'std\']\n        net_params[\'pretrained\'] = False\n\n        # load network\n        net = init_network(net_params)\n        net.load_state_dict(state[\'state_dict\'])\n        \n        # if whitening is precomputed\n        if \'Lw\' in state[\'meta\']:\n            net.meta[\'Lw\'] = state[\'meta\'][\'Lw\']\n        \n        print("">>>> loaded network: "")\n        print(net.meta_repr())\n\n    # loading offtheshelf network\n    elif args.network_offtheshelf is not None:\n        \n        # parse off-the-shelf parameters\n        offtheshelf = args.network_offtheshelf.split(\'-\')\n        net_params = {}\n        net_params[\'architecture\'] = offtheshelf[0]\n        net_params[\'pooling\'] = offtheshelf[1]\n        net_params[\'local_whitening\'] = \'lwhiten\' in offtheshelf[2:]\n        net_params[\'regional\'] = \'reg\' in offtheshelf[2:]\n        net_params[\'whitening\'] = \'whiten\' in offtheshelf[2:]\n        net_params[\'pretrained\'] = True\n\n        # load off-the-shelf network\n        print("">> Loading off-the-shelf network:\\n>>>> \'{}\'"".format(args.network_offtheshelf))\n        net = init_network(net_params)\n        print("">>>> loaded network: "")\n        print(net.meta_repr())\n\n    # setting up the multi-scale parameters\n    ms = list(eval(args.multiscale))\n    if len(ms)>1 and net.meta[\'pooling\'] == \'gem\' and not net.meta[\'regional\'] and not net.meta[\'whitening\']:\n        msp = net.pool.p.item()\n        print("">> Set-up multiscale:"")\n        print("">>>> ms: {}"".format(ms))            \n        print("">>>> msp: {}"".format(msp))\n    else:\n        msp = 1\n\n    # moving network to gpu and eval mode\n    net.cuda()\n    net.eval()\n\n    # set up the transform\n    normalize = transforms.Normalize(\n        mean=net.meta[\'mean\'],\n        std=net.meta[\'std\']\n    )\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        normalize\n    ])\n\n    # compute whitening\n    if args.whitening is not None:\n        start = time.time()\n\n        if \'Lw\' in net.meta and args.whitening in net.meta[\'Lw\']:\n            \n            print(\'>> {}: Whitening is precomputed, loading it...\'.format(args.whitening))\n            \n            if len(ms)>1:\n                Lw = net.meta[\'Lw\'][args.whitening][\'ms\']\n            else:\n                Lw = net.meta[\'Lw\'][args.whitening][\'ss\']\n\n        else:\n\n            # if we evaluate networks from path we should save/load whitening\n            # not to compute it every time\n            if args.network_path is not None:\n                whiten_fn = args.network_path + \'_{}_whiten\'.format(args.whitening)\n                if len(ms) > 1:\n                    whiten_fn += \'_ms\'\n                whiten_fn += \'.pth\'\n            else:\n                whiten_fn = None\n\n            if whiten_fn is not None and os.path.isfile(whiten_fn):\n                print(\'>> {}: Whitening is precomputed, loading it...\'.format(args.whitening))\n                Lw = torch.load(whiten_fn)\n\n            else:\n                print(\'>> {}: Learning whitening...\'.format(args.whitening))\n                \n                # loading db\n                db_root = os.path.join(get_data_root(), \'train\', args.whitening)\n                ims_root = os.path.join(db_root, \'ims\')\n                db_fn = os.path.join(db_root, \'{}-whiten.pkl\'.format(args.whitening))\n                with open(db_fn, \'rb\') as f:\n                    db = pickle.load(f)\n                images = [cid2filename(db[\'cids\'][i], ims_root) for i in range(len(db[\'cids\']))]\n\n                # extract whitening vectors\n                print(\'>> {}: Extracting...\'.format(args.whitening))\n                wvecs = extract_vectors(net, images, args.image_size, transform, ms=ms, msp=msp)\n                \n                # learning whitening \n                print(\'>> {}: Learning...\'.format(args.whitening))\n                wvecs = wvecs.numpy()\n                m, P = whitenlearn(wvecs, db[\'qidxs\'], db[\'pidxs\'])\n                Lw = {\'m\': m, \'P\': P}\n\n                # saving whitening if whiten_fn exists\n                if whiten_fn is not None:\n                    print(\'>> {}: Saving to {}...\'.format(args.whitening, whiten_fn))\n                    torch.save(Lw, whiten_fn)\n\n        print(\'>> {}: elapsed time: {}\'.format(args.whitening, htime(time.time()-start)))\n\n    else:\n        Lw = None\n\n    # evaluate on test datasets\n    datasets = args.datasets.split(\',\')\n    for dataset in datasets: \n        start = time.time()\n\n        print(\'>> {}: Extracting...\'.format(dataset))\n\n        # prepare config structure for the test dataset\n        cfg = configdataset(dataset, os.path.join(get_data_root(), \'test\'))\n        images = [cfg[\'im_fname\'](cfg,i) for i in range(cfg[\'n\'])]\n        qimages = [cfg[\'qim_fname\'](cfg,i) for i in range(cfg[\'nq\'])]\n        try:\n            bbxs = [tuple(cfg[\'gnd\'][i][\'bbx\']) for i in range(cfg[\'nq\'])]\n        except:\n            bbxs = None  # for holidaysmanrot and copydays\n        \n        # extract database and query vectors\n        print(\'>> {}: database images...\'.format(dataset))\n        vecs = extract_vectors(net, images, args.image_size, transform, ms=ms, msp=msp)\n        print(\'>> {}: query images...\'.format(dataset))\n        qvecs = extract_vectors(net, qimages, args.image_size, transform, bbxs=bbxs, ms=ms, msp=msp)\n        \n        print(\'>> {}: Evaluating...\'.format(dataset))\n\n        # convert to numpy\n        vecs = vecs.numpy()\n        qvecs = qvecs.numpy()\n\n        # search, rank, and print\n        scores = np.dot(vecs.T, qvecs)\n        ranks = np.argsort(-scores, axis=0)\n        compute_map_and_print(dataset, ranks, cfg[\'gnd\'])\n    \n        if Lw is not None:\n            # whiten the vectors\n            vecs_lw  = whitenapply(vecs, Lw[\'m\'], Lw[\'P\'])\n            qvecs_lw = whitenapply(qvecs, Lw[\'m\'], Lw[\'P\'])\n\n            # search, rank, and print\n            scores = np.dot(vecs_lw.T, qvecs_lw)\n            ranks = np.argsort(-scores, axis=0)\n            compute_map_and_print(dataset + \' + whiten\', ranks, cfg[\'gnd\'])\n        \n        print(\'>> {}: elapsed time: {}\'.format(dataset, htime(time.time()-start)))\n\n\nif __name__ == \'__main__\':\n    main()'"
cirtorch/examples/test_e2e.py,6,"b'import argparse\nimport os\nimport time\nimport pickle\nimport pdb\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.model_zoo import load_url\nfrom torchvision import transforms\n\nfrom cirtorch.networks.imageretrievalnet import init_network, extract_vectors\nfrom cirtorch.datasets.testdataset import configdataset\nfrom cirtorch.utils.download import download_train, download_test\nfrom cirtorch.utils.evaluate import compute_map_and_print\nfrom cirtorch.utils.general import get_data_root, htime\n\nPRETRAINED = {\n    \'rSfM120k-tl-resnet50-gem-w\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/rSfM120k-tl-resnet50-gem-w-97bf910.pth\',\n    \'rSfM120k-tl-resnet101-gem-w\'       : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/rSfM120k-tl-resnet101-gem-w-a155e54.pth\',\n    \'rSfM120k-tl-resnet152-gem-w\'       : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/rSfM120k-tl-resnet152-gem-w-f39cada.pth\',\n    \'gl18-tl-resnet50-gem-w\'            : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/gl18/gl18-tl-resnet50-gem-w-83fdc30.pth\',\n    \'gl18-tl-resnet101-gem-w\'           : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/gl18/gl18-tl-resnet101-gem-w-a4d43db.pth\',\n    \'gl18-tl-resnet152-gem-w\'           : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/gl18/gl18-tl-resnet152-gem-w-21278d5.pth\',\n}\n\ndatasets_names = [\'oxford5k\', \'paris6k\', \'roxford5k\', \'rparis6k\']\n\nparser = argparse.ArgumentParser(description=\'PyTorch CNN Image Retrieval Testing End-to-End\')\n\n# test options\nparser.add_argument(\'--network\', \'-n\', metavar=\'NETWORK\',\n                    help=""network to be evaluated: "" +\n                        "" | "".join(PRETRAINED.keys()))\nparser.add_argument(\'--datasets\', \'-d\', metavar=\'DATASETS\', default=\'roxford5k,rparis6k\',\n                    help=""comma separated list of test datasets: "" + \n                        "" | "".join(datasets_names) + \n                        "" (default: \'roxford5k,rparis6k\')"")\nparser.add_argument(\'--image-size\', \'-imsize\', default=1024, type=int, metavar=\'N\',\n                    help=""maximum size of longer image side used for testing (default: 1024)"")\nparser.add_argument(\'--multiscale\', \'-ms\', metavar=\'MULTISCALE\', default=\'[1]\', \n                    help=""use multiscale vectors for testing, "" + \n                    "" examples: \'[1]\' | \'[1, 1/2**(1/2), 1/2]\' | \'[1, 2**(1/2), 1/2**(1/2)]\' (default: \'[1]\')"")\n\n# GPU ID\nparser.add_argument(\'--gpu-id\', \'-g\', default=\'0\', metavar=\'N\',\n                    help=""gpu id used for testing (default: \'0\')"")\n\ndef main():\n    args = parser.parse_args()\n\n    # check if there are unknown datasets\n    for dataset in args.datasets.split(\',\'):\n        if dataset not in datasets_names:\n            raise ValueError(\'Unsupported or unknown dataset: {}!\'.format(dataset))\n\n    # check if test dataset are downloaded\n    # and download if they are not\n    download_train(get_data_root())\n    download_test(get_data_root())\n\n    # setting up the visible GPU\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n\n    # loading network\n    # pretrained networks (downloaded automatically)\n    print("">> Loading network:\\n>>>> \'{}\'"".format(args.network))\n    state = load_url(PRETRAINED[args.network], model_dir=os.path.join(get_data_root(), \'networks\'))\n    # parsing net params from meta\n    # architecture, pooling, mean, std required\n    # the rest has default values, in case that is doesnt exist\n    net_params = {}\n    net_params[\'architecture\'] = state[\'meta\'][\'architecture\']\n    net_params[\'pooling\'] = state[\'meta\'][\'pooling\']\n    net_params[\'local_whitening\'] = state[\'meta\'].get(\'local_whitening\', False)\n    net_params[\'regional\'] = state[\'meta\'].get(\'regional\', False)\n    net_params[\'whitening\'] = state[\'meta\'].get(\'whitening\', False)\n    net_params[\'mean\'] = state[\'meta\'][\'mean\']\n    net_params[\'std\'] = state[\'meta\'][\'std\']\n    net_params[\'pretrained\'] = False\n    # network initialization\n    net = init_network(net_params)\n    net.load_state_dict(state[\'state_dict\'])\n        \n    print("">>>> loaded network: "")\n    print(net.meta_repr())\n\n    # setting up the multi-scale parameters\n    ms = list(eval(args.multiscale))\n    print("">>>> Evaluating scales: {}"".format(ms))\n\n    # moving network to gpu and eval mode\n    net.cuda()\n    net.eval()\n\n    # set up the transform\n    normalize = transforms.Normalize(\n        mean=net.meta[\'mean\'],\n        std=net.meta[\'std\']\n    )\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        normalize\n    ])\n\n    # evaluate on test datasets\n    datasets = args.datasets.split(\',\')\n    for dataset in datasets: \n        start = time.time()\n\n        print(\'>> {}: Extracting...\'.format(dataset))\n\n        # prepare config structure for the test dataset\n        cfg = configdataset(dataset, os.path.join(get_data_root(), \'test\'))\n        images = [cfg[\'im_fname\'](cfg,i) for i in range(cfg[\'n\'])]\n        qimages = [cfg[\'qim_fname\'](cfg,i) for i in range(cfg[\'nq\'])]\n        try:\n            bbxs = [tuple(cfg[\'gnd\'][i][\'bbx\']) for i in range(cfg[\'nq\'])]\n        except:\n            bbxs = None  # for holidaysmanrot and copydays\n        \n        # extract database and query vectors\n        print(\'>> {}: database images...\'.format(dataset))\n        vecs = extract_vectors(net, images, args.image_size, transform, ms=ms)\n        print(\'>> {}: query images...\'.format(dataset))\n        qvecs = extract_vectors(net, qimages, args.image_size, transform, bbxs=bbxs, ms=ms)\n        \n        print(\'>> {}: Evaluating...\'.format(dataset))\n\n        # convert to numpy\n        vecs = vecs.numpy()\n        qvecs = qvecs.numpy()\n\n        # search, rank, and print\n        scores = np.dot(vecs.T, qvecs)\n        ranks = np.argsort(-scores, axis=0)\n        compute_map_and_print(dataset, ranks, cfg[\'gnd\'])\n        \n        print(\'>> {}: elapsed time: {}\'.format(dataset, htime(time.time()-start)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
cirtorch/examples/train.py,32,"b'import argparse\nimport os\nimport shutil\nimport time\nimport math\nimport pickle\nimport pdb\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim\nimport torch.utils.data\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom cirtorch.networks.imageretrievalnet import init_network, extract_vectors\nfrom cirtorch.layers.loss import ContrastiveLoss, TripletLoss\nfrom cirtorch.datasets.datahelpers import collate_tuples, cid2filename\nfrom cirtorch.datasets.traindataset import TuplesDataset\nfrom cirtorch.datasets.testdataset import configdataset\nfrom cirtorch.utils.download import download_train, download_test\nfrom cirtorch.utils.whiten import whitenlearn, whitenapply\nfrom cirtorch.utils.evaluate import compute_map_and_print\nfrom cirtorch.utils.general import get_data_root, htime\n\ntraining_dataset_names = [\'retrieval-SfM-120k\']\ntest_datasets_names = [\'oxford5k\', \'paris6k\', \'roxford5k\', \'rparis6k\']\ntest_whiten_names = [\'retrieval-SfM-30k\', \'retrieval-SfM-120k\']\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\npool_names = [\'mac\', \'spoc\', \'gem\', \'gemmp\']\nloss_names = [\'contrastive\', \'triplet\']\noptimizer_names = [\'sgd\', \'adam\']\n\nparser = argparse.ArgumentParser(description=\'PyTorch CNN Image Retrieval Training\')\n\n# export directory, training and val datasets, test datasets\nparser.add_argument(\'directory\', metavar=\'EXPORT_DIR\',\n                    help=\'destination where trained network should be saved\')\nparser.add_argument(\'--training-dataset\', \'-d\', metavar=\'DATASET\', default=\'retrieval-SfM-120k\', choices=training_dataset_names,\n                    help=\'training dataset: \' + \n                        \' | \'.join(training_dataset_names) +\n                        \' (default: retrieval-SfM-120k)\')\nparser.add_argument(\'--no-val\', dest=\'val\', action=\'store_false\',\n                    help=\'do not run validation\')\nparser.add_argument(\'--test-datasets\', \'-td\', metavar=\'DATASETS\', default=\'roxford5k,rparis6k\',\n                    help=\'comma separated list of test datasets: \' + \n                        \' | \'.join(test_datasets_names) + \n                        \' (default: roxford5k,rparis6k)\')\nparser.add_argument(\'--test-whiten\', metavar=\'DATASET\', default=\'\', choices=test_whiten_names,\n                    help=\'dataset used to learn whitening for testing: \' + \n                        \' | \'.join(test_whiten_names) + \n                        \' (default: None)\')\nparser.add_argument(\'--test-freq\', default=1, type=int, metavar=\'N\', \n                    help=\'run test evaluation every N epochs (default: 1)\')\n\n# network architecture and initialization options\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'resnet101\', choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: resnet101)\')\nparser.add_argument(\'--pool\', \'-p\', metavar=\'POOL\', default=\'gem\', choices=pool_names,\n                    help=\'pooling options: \' +\n                        \' | \'.join(pool_names) +\n                        \' (default: gem)\')\nparser.add_argument(\'--local-whitening\', \'-lw\', dest=\'local_whitening\', action=\'store_true\',\n                    help=\'train model with learnable local whitening (linear layer) before the pooling\')\nparser.add_argument(\'--regional\', \'-r\', dest=\'regional\', action=\'store_true\',\n                    help=\'train model with regional pooling using fixed grid\')\nparser.add_argument(\'--whitening\', \'-w\', dest=\'whitening\', action=\'store_true\',\n                    help=\'train model with learnable whitening (linear layer) after the pooling\')\nparser.add_argument(\'--not-pretrained\', dest=\'pretrained\', action=\'store_false\',\n                    help=\'initialize model with random weights (default: pretrained on imagenet)\')\nparser.add_argument(\'--loss\', \'-l\', metavar=\'LOSS\', default=\'contrastive\',\n                    choices=loss_names,\n                    help=\'training loss options: \' +\n                        \' | \'.join(loss_names) +\n                        \' (default: contrastive)\')\nparser.add_argument(\'--loss-margin\', \'-lm\', metavar=\'LM\', default=0.7, type=float,\n                    help=\'loss margin: (default: 0.7)\')\n\n# train/val options specific for image retrieval learning\nparser.add_argument(\'--image-size\', default=1024, type=int, metavar=\'N\',\n                    help=\'maximum size of longer image side used for training (default: 1024)\')\nparser.add_argument(\'--neg-num\', \'-nn\', default=5, type=int, metavar=\'N\',\n                    help=\'number of negative image per train/val tuple (default: 5)\')\nparser.add_argument(\'--query-size\', \'-qs\', default=2000, type=int, metavar=\'N\',\n                    help=\'number of queries randomly drawn per one train epoch (default: 2000)\')\nparser.add_argument(\'--pool-size\', \'-ps\', default=20000, type=int, metavar=\'N\',\n                    help=\'size of the pool for hard negative mining (default: 20000)\')\n\n# standard train/val options\nparser.add_argument(\'--gpu-id\', \'-g\', default=\'0\', metavar=\'N\',\n                    help=\'gpu id used for training (default: 0)\')\nparser.add_argument(\'--workers\', \'-j\', default=8, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 8)\')\nparser.add_argument(\'--epochs\', default=100, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run (default: 100)\')\nparser.add_argument(\'--batch-size\', \'-b\', default=5, type=int, metavar=\'N\', \n                    help=\'number of (q,p,n1,...,nN) tuples in a mini-batch (default: 5)\')\nparser.add_argument(\'--update-every\', \'-u\', default=1, type=int, metavar=\'N\',\n                    help=\'update model weights every N batches, used to handle really large batches, \' + \n                        \'batch_size effectively becomes update_every x batch_size (default: 1)\')\nparser.add_argument(\'--optimizer\', \'-o\', metavar=\'OPTIMIZER\', default=\'adam\',\n                    choices=optimizer_names,\n                    help=\'optimizer options: \' +\n                        \' | \'.join(optimizer_names) +\n                        \' (default: adam)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-6, type=float,\n                    metavar=\'LR\', help=\'initial learning rate (default: 1e-6)\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-6, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-6)\')\nparser.add_argument(\'--print-freq\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'FILENAME\',\n                    help=\'name of the latest checkpoint (default: None)\')\n\nmin_loss = float(\'inf\')\n\ndef main():\n    global args, min_loss\n    args = parser.parse_args()\n\n    # manually check if there are unknown test datasets\n    for dataset in args.test_datasets.split(\',\'):\n        if dataset not in test_datasets_names:\n            raise ValueError(\'Unsupported or unknown test dataset: {}!\'.format(dataset))\n\n    # check if test dataset are downloaded\n    # and download if they are not\n    download_train(get_data_root())\n    download_test(get_data_root())\n\n    # create export dir if it doesnt exist\n    directory = ""{}"".format(args.training_dataset)\n    directory += ""_{}"".format(args.arch)\n    directory += ""_{}"".format(args.pool)\n    if args.local_whitening:\n        directory += ""_lwhiten""\n    if args.regional:\n        directory += ""_r""\n    if args.whitening:\n        directory += ""_whiten""\n    if not args.pretrained:\n        directory += ""_notpretrained""\n    directory += ""_{}_m{:.2f}"".format(args.loss, args.loss_margin)\n    directory += ""_{}_lr{:.1e}_wd{:.1e}"".format(args.optimizer, args.lr, args.weight_decay)\n    directory += ""_nnum{}_qsize{}_psize{}"".format(args.neg_num, args.query_size, args.pool_size)\n    directory += ""_bsize{}_uevery{}_imsize{}"".format(args.batch_size, args.update_every, args.image_size)\n\n    args.directory = os.path.join(args.directory, directory)\n    print("">> Creating directory if it does not exist:\\n>> \'{}\'"".format(args.directory))\n    if not os.path.exists(args.directory):\n        os.makedirs(args.directory)\n\n    # set cuda visible device\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpu_id\n    \n    # set random seeds\n    # TODO: maybe pass as argument in future implementation?\n    torch.manual_seed(0)\n    torch.cuda.manual_seed_all(0)\n    np.random.seed(0)\n\n    # initialize model\n    if args.pretrained:\n        print("">> Using pre-trained model \'{}\'"".format(args.arch))\n    else:\n        print("">> Using model from scratch (random weights) \'{}\'"".format(args.arch))\n    model_params = {}\n    model_params[\'architecture\'] = args.arch\n    model_params[\'pooling\'] = args.pool\n    model_params[\'local_whitening\'] = args.local_whitening\n    model_params[\'regional\'] = args.regional\n    model_params[\'whitening\'] = args.whitening\n    # model_params[\'mean\'] = ...  # will use default\n    # model_params[\'std\'] = ...  # will use default\n    model_params[\'pretrained\'] = args.pretrained\n    model = init_network(model_params)\n\n    # move network to gpu\n    model.cuda()\n\n    # define loss function (criterion) and optimizer\n    if args.loss == \'contrastive\':\n        criterion = ContrastiveLoss(margin=args.loss_margin).cuda()\n    elif args.loss == \'triplet\':\n        criterion = TripletLoss(margin=args.loss_margin).cuda()\n    else:\n        raise(RuntimeError(""Loss {} not available!"".format(args.loss)))\n\n    # parameters split into features, pool, whitening \n    # IMPORTANT: no weight decay for pooling parameter p in GeM or regional-GeM\n    parameters = []\n    # add feature parameters\n    parameters.append({\'params\': model.features.parameters()})\n    # add local whitening if exists\n    if model.lwhiten is not None:\n        parameters.append({\'params\': model.lwhiten.parameters()})\n    # add pooling parameters (or regional whitening which is part of the pooling layer!)\n    if not args.regional:\n        # global, only pooling parameter p weight decay should be 0\n        if args.pool == \'gem\':\n            parameters.append({\'params\': model.pool.parameters(), \'lr\': args.lr*10, \'weight_decay\': 0})\n        elif args.pool == \'gemmp\':\n            parameters.append({\'params\': model.pool.parameters(), \'lr\': args.lr*100, \'weight_decay\': 0})\n    else:\n        # regional, pooling parameter p weight decay should be 0, \n        # and we want to add regional whitening if it is there\n        if args.pool == \'gem\':\n            parameters.append({\'params\': model.pool.rpool.parameters(), \'lr\': args.lr*10, \'weight_decay\': 0})\n        elif args.pool == \'gemmp\':\n            parameters.append({\'params\': model.pool.rpool.parameters(), \'lr\': args.lr*100, \'weight_decay\': 0})\n        if model.pool.whiten is not None:\n            parameters.append({\'params\': model.pool.whiten.parameters()})\n    # add final whitening if exists\n    if model.whiten is not None:\n        parameters.append({\'params\': model.whiten.parameters()})\n\n    # define optimizer\n    if args.optimizer == \'sgd\':\n        optimizer = torch.optim.SGD(parameters, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    elif args.optimizer == \'adam\':\n        optimizer = torch.optim.Adam(parameters, args.lr, weight_decay=args.weight_decay)\n\n    # define learning rate decay schedule\n    # TODO: maybe pass as argument in future implementation?\n    exp_decay = math.exp(-0.01)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=exp_decay)\n\n    # optionally resume from a checkpoint\n    start_epoch = 0\n    if args.resume:\n        args.resume = os.path.join(args.directory, args.resume)\n        if os.path.isfile(args.resume):\n            # load checkpoint weights and update model and optimizer\n            print("">> Loading checkpoint:\\n>> \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            start_epoch = checkpoint[\'epoch\']\n            min_loss = checkpoint[\'min_loss\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print("">>>> loaded checkpoint:\\n>>>> \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            # important not to forget scheduler updating\n            scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=exp_decay, last_epoch=checkpoint[\'epoch\']-1)\n        else:\n            print("">> No checkpoint found at \'{}\'"".format(args.resume))\n\n    # Data loading code\n    normalize = transforms.Normalize(mean=model.meta[\'mean\'], std=model.meta[\'std\'])\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        normalize,\n    ])\n    train_dataset = TuplesDataset(\n        name=args.training_dataset,\n        mode=\'train\',\n        imsize=args.image_size,\n        nnum=args.neg_num,\n        qsize=args.query_size,\n        poolsize=args.pool_size,\n        transform=transform\n    )\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True, sampler=None,\n        drop_last=True, collate_fn=collate_tuples\n    )\n    if args.val:\n        val_dataset = TuplesDataset(\n            name=args.training_dataset,\n            mode=\'val\',\n            imsize=args.image_size,\n            nnum=args.neg_num,\n            qsize=float(\'Inf\'),\n            poolsize=float(\'Inf\'),\n            transform=transform\n        )\n        val_loader = torch.utils.data.DataLoader(\n            val_dataset, batch_size=args.batch_size, shuffle=False,\n            num_workers=args.workers, pin_memory=True,\n            drop_last=True, collate_fn=collate_tuples\n        )\n\n    # evaluate the network before starting\n    # this might not be necessary?\n    test(args.test_datasets, model)\n\n    for epoch in range(start_epoch, args.epochs):\n\n        # set manual seeds per epoch\n        np.random.seed(epoch)\n        torch.manual_seed(epoch)\n        torch.cuda.manual_seed_all(epoch)\n\n        # adjust learning rate for each epoch\n        scheduler.step()\n        # # debug printing to check if everything ok\n        # lr_feat = optimizer.param_groups[0][\'lr\']\n        # lr_pool = optimizer.param_groups[1][\'lr\']\n        # print(\'>> Features lr: {:.2e}; Pooling lr: {:.2e}\'.format(lr_feat, lr_pool))\n\n        # train for one epoch on train set\n        loss = train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        if args.val:\n            with torch.no_grad():\n                loss = validate(val_loader, model, criterion, epoch)\n\n        # evaluate on test datasets every test_freq epochs\n        if (epoch + 1) % args.test_freq == 0:\n            with torch.no_grad():\n                test(args.test_datasets, model)\n\n        # remember best loss and save checkpoint\n        is_best = loss < min_loss\n        min_loss = min(loss, min_loss)\n\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'meta\': model.meta,\n            \'state_dict\': model.state_dict(),\n            \'min_loss\': min_loss,\n            \'optimizer\' : optimizer.state_dict(),\n        }, is_best, args.directory)\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    # create tuples for training\n    avg_neg_distance = train_loader.dataset.create_epoch_tuples(model)\n\n    # switch to train mode\n    model.train()\n    model.apply(set_batchnorm_eval)\n\n    # zero out gradients\n    optimizer.zero_grad()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n\n        nq = len(input) # number of training tuples\n        ni = len(input[0]) # number of images per tuple\n\n        for q in range(nq):\n            output = torch.zeros(model.meta[\'outputdim\'], ni).cuda()\n            for imi in range(ni):\n\n                # compute output vector for image imi\n                output[:, imi] = model(input[q][imi].cuda()).squeeze()\n\n            # reducing memory consumption:\n            # compute loss for this query tuple only\n            # then, do backward pass for one tuple only\n            # each backward pass gradients will be accumulated\n            # the optimization step is performed for the full batch later\n            loss = criterion(output, target[q].cuda())\n            losses.update(loss.item())\n            loss.backward()\n\n        if (i + 1) % args.update_every == 0:\n            # do one step for multiple batches\n            # accumulated gradients are used\n            optimizer.step()\n            # zero out gradients so we can \n            # accumulate new ones over batches\n            optimizer.zero_grad()\n            # print(\'>> Train: [{0}][{1}/{2}]\\t\'\n            #       \'Weight update performed\'.format(\n            #        epoch+1, i+1, len(train_loader)))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(train_loader):\n            print(\'>> Train: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(\n                   epoch+1, i+1, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses))\n\n    return losses.avg\n\n\ndef validate(val_loader, model, criterion, epoch):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n\n    # create tuples for validation\n    avg_neg_distance = val_loader.dataset.create_epoch_tuples(model)\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n\n        nq = len(input) # number of training tuples\n        ni = len(input[0]) # number of images per tuple\n        output = torch.zeros(model.meta[\'outputdim\'], nq*ni).cuda()\n\n        for q in range(nq):\n            for imi in range(ni):\n\n                # compute output vector for image imi of query q\n                output[:, q*ni + imi] = model(input[q][imi].cuda()).squeeze()\n\n        # no need to reduce memory consumption (no backward pass):\n        # compute loss for the full batch\n        loss = criterion(output, torch.cat(target).cuda())\n\n        # record loss\n        losses.update(loss.item()/nq, nq)\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if (i+1) % args.print_freq == 0 or i == 0 or (i+1) == len(val_loader):\n            print(\'>> Val: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(\n                   epoch+1, i+1, len(val_loader), batch_time=batch_time, loss=losses))\n\n    return losses.avg\n\ndef test(datasets, net):\n\n    print(\'>> Evaluating network on test datasets...\')\n\n    # for testing we use image size of max 1024\n    image_size = 1024\n\n    # moving network to gpu and eval mode\n    net.cuda()\n    net.eval()\n    # set up the transform\n    normalize = transforms.Normalize(\n        mean=net.meta[\'mean\'],\n        std=net.meta[\'std\']\n    )\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        normalize\n    ])\n\n    # compute whitening\n    if args.test_whiten:\n        start = time.time()\n\n        print(\'>> {}: Learning whitening...\'.format(args.test_whiten))\n\n        # loading db\n        db_root = os.path.join(get_data_root(), \'train\', args.test_whiten)\n        ims_root = os.path.join(db_root, \'ims\')\n        db_fn = os.path.join(db_root, \'{}-whiten.pkl\'.format(args.test_whiten))\n        with open(db_fn, \'rb\') as f:\n            db = pickle.load(f)\n        images = [cid2filename(db[\'cids\'][i], ims_root) for i in range(len(db[\'cids\']))]\n\n        # extract whitening vectors\n        print(\'>> {}: Extracting...\'.format(args.test_whiten))\n        wvecs = extract_vectors(net, images, image_size, transform)  # implemented with torch.no_grad\n        \n        # learning whitening \n        print(\'>> {}: Learning...\'.format(args.test_whiten))\n        wvecs = wvecs.numpy()\n        m, P = whitenlearn(wvecs, db[\'qidxs\'], db[\'pidxs\'])\n        Lw = {\'m\': m, \'P\': P}\n\n        print(\'>> {}: elapsed time: {}\'.format(args.test_whiten, htime(time.time()-start)))\n    else:\n        Lw = None\n\n    # evaluate on test datasets\n    datasets = args.test_datasets.split(\',\')\n    for dataset in datasets: \n        start = time.time()\n\n        print(\'>> {}: Extracting...\'.format(dataset))\n\n        # prepare config structure for the test dataset\n        cfg = configdataset(dataset, os.path.join(get_data_root(), \'test\'))\n        images = [cfg[\'im_fname\'](cfg,i) for i in range(cfg[\'n\'])]\n        qimages = [cfg[\'qim_fname\'](cfg,i) for i in range(cfg[\'nq\'])]\n        bbxs = [tuple(cfg[\'gnd\'][i][\'bbx\']) for i in range(cfg[\'nq\'])]\n        \n        # extract database and query vectors\n        print(\'>> {}: database images...\'.format(dataset))\n        vecs = extract_vectors(net, images, image_size, transform)  # implemented with torch.no_grad\n        print(\'>> {}: query images...\'.format(dataset))\n        qvecs = extract_vectors(net, qimages, image_size, transform, bbxs)  # implemented with torch.no_grad\n        \n        print(\'>> {}: Evaluating...\'.format(dataset))\n\n        # convert to numpy\n        vecs = vecs.numpy()\n        qvecs = qvecs.numpy()\n\n        # search, rank, and print\n        scores = np.dot(vecs.T, qvecs)\n        ranks = np.argsort(-scores, axis=0)\n        compute_map_and_print(dataset, ranks, cfg[\'gnd\'])\n    \n        if Lw is not None:\n            # whiten the vectors\n            vecs_lw  = whitenapply(vecs, Lw[\'m\'], Lw[\'P\'])\n            qvecs_lw = whitenapply(qvecs, Lw[\'m\'], Lw[\'P\'])\n\n            # search, rank, and print\n            scores = np.dot(vecs_lw.T, qvecs_lw)\n            ranks = np.argsort(-scores, axis=0)\n            compute_map_and_print(dataset + \' + whiten\', ranks, cfg[\'gnd\'])\n        \n        print(\'>> {}: elapsed time: {}\'.format(dataset, htime(time.time()-start)))\n\n\ndef save_checkpoint(state, is_best, directory):\n    filename = os.path.join(directory, \'model_epoch%d.pth.tar\' % state[\'epoch\'])\n    torch.save(state, filename)\n    if is_best:\n        filename_best = os.path.join(directory, \'model_best.pth.tar\')\n        shutil.copyfile(filename, filename_best)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef set_batchnorm_eval(m):\n    classname = m.__class__.__name__\n    if classname.find(\'BatchNorm\') != -1:\n        # freeze running mean and std:\n        # we do training one image at a time\n        # so the statistics would not be per batch\n        # hence we choose freezing (ie using imagenet statistics)\n        m.eval()\n        # # freeze parameters:\n        # # in fact no need to freeze scale and bias\n        # # they can be learned\n        # # that is why next two lines are commented\n        # for p in m.parameters():\n            # p.requires_grad = False\n\n\nif __name__ == \'__main__\':\n    main()'"
cirtorch/layers/__init__.py,0,b''
cirtorch/layers/functional.py,23,"b'import math\nimport pdb\n\nimport torch\nimport torch.nn.functional as F\n\n# --------------------------------------\n# pooling\n# --------------------------------------\n\ndef mac(x):\n    return F.max_pool2d(x, (x.size(-2), x.size(-1)))\n    # return F.adaptive_max_pool2d(x, (1,1)) # alternative\n\n\ndef spoc(x):\n    return F.avg_pool2d(x, (x.size(-2), x.size(-1)))\n    # return F.adaptive_avg_pool2d(x, (1,1)) # alternative\n\n\ndef gem(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n    # return F.lp_pool2d(F.threshold(x, eps, eps), p, (x.size(-2), x.size(-1))) # alternative\n\n\ndef rmac(x, L=3, eps=1e-6):\n    ovr = 0.4 # desired overlap of neighboring regions\n    steps = torch.Tensor([2, 3, 4, 5, 6, 7]) # possible regions for the long dimension\n\n    W = x.size(3)\n    H = x.size(2)\n\n    w = min(W, H)\n    w2 = math.floor(w/2.0 - 1)\n\n    b = (max(H, W)-w)/(steps-1)\n    (tmp, idx) = torch.min(torch.abs(((w**2 - w*b)/w**2)-ovr), 0) # steps(idx) regions for long dimension\n\n    # region overplus per dimension\n    Wd = 0;\n    Hd = 0;\n    if H < W:  \n        Wd = idx.item() + 1\n    elif H > W:\n        Hd = idx.item() + 1\n\n    v = F.max_pool2d(x, (x.size(-2), x.size(-1)))\n    v = v / (torch.norm(v, p=2, dim=1, keepdim=True) + eps).expand_as(v)\n\n    for l in range(1, L+1):\n        wl = math.floor(2*w/(l+1))\n        wl2 = math.floor(wl/2 - 1)\n\n        if l+Wd == 1:\n            b = 0\n        else:\n            b = (W-wl)/(l+Wd-1)\n        cenW = torch.floor(wl2 + torch.Tensor(range(l-1+Wd+1))*b) - wl2 # center coordinates\n        if l+Hd == 1:\n            b = 0\n        else:\n            b = (H-wl)/(l+Hd-1)\n        cenH = torch.floor(wl2 + torch.Tensor(range(l-1+Hd+1))*b) - wl2 # center coordinates\n            \n        for i_ in cenH.tolist():\n            for j_ in cenW.tolist():\n                if wl == 0:\n                    continue\n                R = x[:,:,(int(i_)+torch.Tensor(range(wl)).long()).tolist(),:]\n                R = R[:,:,:,(int(j_)+torch.Tensor(range(wl)).long()).tolist()]\n                vt = F.max_pool2d(R, (R.size(-2), R.size(-1)))\n                vt = vt / (torch.norm(vt, p=2, dim=1, keepdim=True) + eps).expand_as(vt)\n                v += vt\n\n    return v\n\n\ndef roipool(x, rpool, L=3, eps=1e-6):\n    ovr = 0.4 # desired overlap of neighboring regions\n    steps = torch.Tensor([2, 3, 4, 5, 6, 7]) # possible regions for the long dimension\n\n    W = x.size(3)\n    H = x.size(2)\n\n    w = min(W, H)\n    w2 = math.floor(w/2.0 - 1)\n\n    b = (max(H, W)-w)/(steps-1)\n    _, idx = torch.min(torch.abs(((w**2 - w*b)/w**2)-ovr), 0) # steps(idx) regions for long dimension\n\n    # region overplus per dimension\n    Wd = 0;\n    Hd = 0;\n    if H < W:  \n        Wd = idx.item() + 1\n    elif H > W:\n        Hd = idx.item() + 1\n\n    vecs = []\n    vecs.append(rpool(x).unsqueeze(1))\n\n    for l in range(1, L+1):\n        wl = math.floor(2*w/(l+1))\n        wl2 = math.floor(wl/2 - 1)\n\n        if l+Wd == 1:\n            b = 0\n        else:\n            b = (W-wl)/(l+Wd-1)\n        cenW = torch.floor(wl2 + torch.Tensor(range(l-1+Wd+1))*b).int() - wl2 # center coordinates\n        if l+Hd == 1:\n            b = 0\n        else:\n            b = (H-wl)/(l+Hd-1)\n        cenH = torch.floor(wl2 + torch.Tensor(range(l-1+Hd+1))*b).int() - wl2 # center coordinates\n            \n        for i_ in cenH.tolist():\n            for j_ in cenW.tolist():\n                if wl == 0:\n                    continue\n                vecs.append(rpool(x.narrow(2,i_,wl).narrow(3,j_,wl)).unsqueeze(1))\n\n    return torch.cat(vecs, dim=1)\n\n\n# --------------------------------------\n# normalization\n# --------------------------------------\n\ndef l2n(x, eps=1e-6):\n    return x / (torch.norm(x, p=2, dim=1, keepdim=True) + eps).expand_as(x)\n\ndef powerlaw(x, eps=1e-6):\n    x = x + self.eps\n    return x.abs().sqrt().mul(x.sign())\n\n# --------------------------------------\n# loss\n# --------------------------------------\n\ndef contrastive_loss(x, label, margin=0.7, eps=1e-6):\n    # x is D x N\n    dim = x.size(0) # D\n    nq = torch.sum(label.data==-1) # number of tuples\n    S = x.size(1) // nq # number of images per tuple including query: 1+1+n\n\n    x1 = x[:, ::S].permute(1,0).repeat(1,S-1).view((S-1)*nq,dim).permute(1,0)\n    idx = [i for i in range(len(label)) if label.data[i] != -1]\n    x2 = x[:, idx]\n    lbl = label[label!=-1]\n\n    dif = x1 - x2\n    D = torch.pow(dif+eps, 2).sum(dim=0).sqrt()\n\n    y = 0.5*lbl*torch.pow(D,2) + 0.5*(1-lbl)*torch.pow(torch.clamp(margin-D, min=0),2)\n    y = torch.sum(y)\n    return y\n\ndef triplet_loss(x, label, margin=0.1):\n    # x is D x N\n    dim = x.size(0) # D\n    nq = torch.sum(label.data==-1).item() # number of tuples\n    S = x.size(1) // nq # number of images per tuple including query: 1+1+n\n\n    xa = x[:, label.data==-1].permute(1,0).repeat(1,S-2).view((S-2)*nq,dim).permute(1,0)\n    xp = x[:, label.data==1].permute(1,0).repeat(1,S-2).view((S-2)*nq,dim).permute(1,0)\n    xn = x[:, label.data==0]\n\n    dist_pos = torch.sum(torch.pow(xa - xp, 2), dim=0)\n    dist_neg = torch.sum(torch.pow(xa - xn, 2), dim=0)\n\n    return torch.sum(torch.clamp(dist_pos - dist_neg + margin, min=0))\n'"
cirtorch/layers/loss.py,4,"b'import torch\nimport torch.nn as nn\n\nimport cirtorch.layers.functional as LF\n\n# --------------------------------------\n# Loss/Error layers\n# --------------------------------------\n\nclass ContrastiveLoss(nn.Module):\n    r""""""CONTRASTIVELOSS layer that computes contrastive loss for a batch of images:\n        Q query tuples, each packed in the form of (q,p,n1,..nN)\n\n    Args:\n        x: tuples arranges in columns as [q,p,n1,nN, ... ]\n        label: -1 for query, 1 for corresponding positive, 0 for corresponding negative\n        margin: contrastive loss margin. Default: 0.7\n\n    >>> contrastive_loss = ContrastiveLoss(margin=0.7)\n    >>> input = torch.randn(128, 35, requires_grad=True)\n    >>> label = torch.Tensor([-1, 1, 0, 0, 0, 0, 0] * 5)\n    >>> output = contrastive_loss(input, label)\n    >>> output.backward()\n    """"""\n\n    def __init__(self, margin=0.7, eps=1e-6):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n        self.eps = eps\n\n    def forward(self, x, label):\n        return LF.contrastive_loss(x, label, margin=self.margin, eps=self.eps)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' + \'margin=\' + \'{:.4f}\'.format(self.margin) + \')\'\n\n\nclass TripletLoss(nn.Module):\n\n    def __init__(self, margin=0.1):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, x, label):\n        return LF.triplet_loss(x, label, margin=self.margin)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' + \'margin=\' + \'{:.4f}\'.format(self.margin) + \')\'\n'"
cirtorch/layers/normalization.py,2,"b""import torch\nimport torch.nn as nn\n\nimport cirtorch.layers.functional as LF\n\n# --------------------------------------\n# Normalization layers\n# --------------------------------------\n\nclass L2N(nn.Module):\n\n    def __init__(self, eps=1e-6):\n        super(L2N,self).__init__()\n        self.eps = eps\n\n    def forward(self, x):\n        return LF.l2n(x, eps=self.eps)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'eps=' + str(self.eps) + ')'\n\n\nclass PowerLaw(nn.Module):\n\n    def __init__(self, eps=1e-6):\n        super(PowerLaw, self).__init__()\n        self.eps = eps\n\n    def forward(self, x):\n        return LF.powerlaw(x, eps=self.eps)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'eps=' + str(self.eps) + ')'"""
cirtorch/layers/pooling.py,6,"b""import torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\n\nimport cirtorch.layers.functional as LF\nfrom cirtorch.layers.normalization import L2N\n\n# --------------------------------------\n# Pooling layers\n# --------------------------------------\n\nclass MAC(nn.Module):\n\n    def __init__(self):\n        super(MAC,self).__init__()\n\n    def forward(self, x):\n        return LF.mac(x)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nclass SPoC(nn.Module):\n\n    def __init__(self):\n        super(SPoC,self).__init__()\n\n    def forward(self, x):\n        return LF.spoc(x)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '()'\n\n\nclass GeM(nn.Module):\n\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM,self).__init__()\n        self.p = Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return LF.gem(x, p=self.p, eps=self.eps)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'\n\nclass GeMmp(nn.Module):\n\n    def __init__(self, p=3, mp=1, eps=1e-6):\n        super(GeMmp,self).__init__()\n        self.p = Parameter(torch.ones(mp)*p)\n        self.mp = mp\n        self.eps = eps\n\n    def forward(self, x):\n        return LF.gem(x, p=self.p.unsqueeze(-1).unsqueeze(-1), eps=self.eps)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'p=' + '[{}]'.format(self.mp) + ', ' + 'eps=' + str(self.eps) + ')'\n\nclass RMAC(nn.Module):\n\n    def __init__(self, L=3, eps=1e-6):\n        super(RMAC,self).__init__()\n        self.L = L\n        self.eps = eps\n\n    def forward(self, x):\n        return LF.rmac(x, L=self.L, eps=self.eps)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + '(' + 'L=' + '{}'.format(self.L) + ')'\n\n\nclass Rpool(nn.Module):\n\n    def __init__(self, rpool, whiten=None, L=3, eps=1e-6):\n        super(Rpool,self).__init__()\n        self.rpool = rpool\n        self.L = L\n        self.whiten = whiten\n        self.norm = L2N()\n        self.eps = eps\n\n    def forward(self, x, aggregate=True):\n        # features -> roipool\n        o = LF.roipool(x, self.rpool, self.L, self.eps) # size: #im, #reg, D, 1, 1\n\n        # concatenate regions from all images in the batch\n        s = o.size()\n        o = o.view(s[0]*s[1], s[2], s[3], s[4]) # size: #im x #reg, D, 1, 1\n\n        # rvecs -> norm\n        o = self.norm(o)\n\n        # rvecs -> whiten -> norm\n        if self.whiten is not None:\n            o = self.norm(self.whiten(o.squeeze(-1).squeeze(-1)))\n\n        # reshape back to regions per image\n        o = o.view(s[0], s[1], s[2], s[3], s[4]) # size: #im, #reg, D, 1, 1\n\n        # aggregate regions into a single global vector per image\n        if aggregate:\n            # rvecs -> sumpool -> norm\n            o = self.norm(o.sum(1, keepdim=False)) # size: #im, D, 1, 1\n\n        return o\n\n    def __repr__(self):\n        return super(Rpool, self).__repr__() + '(' + 'L=' + '{}'.format(self.L) + ')'"""
cirtorch/networks/__init__.py,0,b''
cirtorch/networks/imageretrievalnet.py,14,"b'import os\nimport pdb\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\nimport torchvision\n\nfrom cirtorch.layers.pooling import MAC, SPoC, GeM, GeMmp, RMAC, Rpool\nfrom cirtorch.layers.normalization import L2N, PowerLaw\nfrom cirtorch.datasets.genericdataset import ImagesFromList\nfrom cirtorch.utils.general import get_data_root\n\n# for some models, we have imported features (convolutions) from caffe because the image retrieval performance is higher for them\nFEATURES = {\n    \'vgg16\'         : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/imagenet/imagenet-caffe-vgg16-features-d369c8e.pth\',\n    \'resnet50\'      : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/imagenet/imagenet-caffe-resnet50-features-ac468af.pth\',\n    \'resnet101\'     : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/imagenet/imagenet-caffe-resnet101-features-10a101d.pth\',\n    \'resnet152\'     : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/imagenet/imagenet-caffe-resnet152-features-1011020.pth\',\n}\n\n# TODO: pre-compute for more architectures and properly test variations (pre l2norm, post l2norm)\n# pre-computed local pca whitening that can be applied before the pooling layer\nL_WHITENING = {\n    \'resnet101\' : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-lwhiten-9f830ef.pth\', # no pre l2 norm\n    # \'resnet101\' : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-lwhiten-da5c935.pth\', # with pre l2 norm\n}\n\n# possible global pooling layers, each on of these can be made regional\nPOOLING = {\n    \'mac\'   : MAC,\n    \'spoc\'  : SPoC,\n    \'gem\'   : GeM,\n    \'gemmp\' : GeMmp,\n    \'rmac\'  : RMAC,\n}\n\n# TODO: pre-compute for: resnet50-gem-r, resnet50-mac-r, vgg16-mac-r, alexnet-mac-r\n# pre-computed regional whitening, for most commonly used architectures and pooling methods\nR_WHITENING = {\n    \'alexnet-gem-r\'   : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-alexnet-gem-r-rwhiten-c8cf7e2.pth\',\n    \'vgg16-gem-r\'     : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-vgg16-gem-r-rwhiten-19b204e.pth\',\n    \'resnet101-mac-r\' : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-mac-r-rwhiten-7f1ed8c.pth\',\n    \'resnet101-gem-r\' : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-gem-r-rwhiten-adace84.pth\',\n}\n\n# TODO: pre-compute for more architectures\n# pre-computed final (global) whitening, for most commonly used architectures and pooling methods\nWHITENING = {\n    \'alexnet-gem\'            : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-alexnet-gem-whiten-454ad53.pth\',\n    \'alexnet-gem-r\'          : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-alexnet-gem-r-whiten-4c9126b.pth\',\n    \'vgg16-gem\'              : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-vgg16-gem-whiten-eaa6695.pth\',\n    \'vgg16-gem-r\'            : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-vgg16-gem-r-whiten-83582df.pth\',\n    \'resnet50-gem\'           : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet50-gem-whiten-f15da7b.pth\',\n    \'resnet101-mac-r\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-mac-r-whiten-9df41d3.pth\',\n    \'resnet101-gem\'          : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-gem-whiten-22ab0c1.pth\',\n    \'resnet101-gem-r\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-gem-r-whiten-b379c0a.pth\',\n    \'resnet101-gemmp\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet101-gemmp-whiten-770f53c.pth\',\n    \'resnet152-gem\'          : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-resnet152-gem-whiten-abe7b93.pth\',\n    \'densenet121-gem\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-densenet121-gem-whiten-79e3eea.pth\',\n    \'densenet169-gem\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-densenet169-gem-whiten-6b2a76a.pth\',\n    \'densenet201-gem\'        : \'http://cmp.felk.cvut.cz/cnnimageretrieval/data/whiten/retrieval-SfM-120k/retrieval-SfM-120k-densenet201-gem-whiten-22ea45c.pth\',\n}\n\n# output dimensionality for supported architectures\nOUTPUT_DIM = {\n    \'alexnet\'               :  256,\n    \'vgg11\'                 :  512,\n    \'vgg13\'                 :  512,\n    \'vgg16\'                 :  512,\n    \'vgg19\'                 :  512,\n    \'resnet18\'              :  512,\n    \'resnet34\'              :  512,\n    \'resnet50\'              : 2048,\n    \'resnet101\'             : 2048,\n    \'resnet152\'             : 2048,\n    \'densenet121\'           : 1024,\n    \'densenet169\'           : 1664,\n    \'densenet201\'           : 1920,\n    \'densenet161\'           : 2208, # largest densenet\n    \'squeezenet1_0\'         :  512,\n    \'squeezenet1_1\'         :  512,\n}\n\n\nclass ImageRetrievalNet(nn.Module):\n    \n    def __init__(self, features, lwhiten, pool, whiten, meta):\n        super(ImageRetrievalNet, self).__init__()\n        self.features = nn.Sequential(*features)\n        self.lwhiten = lwhiten\n        self.pool = pool\n        self.whiten = whiten\n        self.norm = L2N()\n        self.meta = meta\n    \n    def forward(self, x):\n        # x -> features\n        o = self.features(x)\n\n        # TODO: properly test (with pre-l2norm and/or post-l2norm)\n        # if lwhiten exist: features -> local whiten\n        if self.lwhiten is not None:\n            # o = self.norm(o)\n            s = o.size()\n            o = o.permute(0,2,3,1).contiguous().view(-1, s[1])\n            o = self.lwhiten(o)\n            o = o.view(s[0],s[2],s[3],self.lwhiten.out_features).permute(0,3,1,2)\n            # o = self.norm(o)\n\n        # features -> pool -> norm\n        o = self.norm(self.pool(o)).squeeze(-1).squeeze(-1)\n\n        # if whiten exist: pooled features -> whiten -> norm\n        if self.whiten is not None:\n            o = self.norm(self.whiten(o))\n\n        # permute so that it is Dx1 column vector per image (DxN if many images)\n        return o.permute(1,0)\n\n    def __repr__(self):\n        tmpstr = super(ImageRetrievalNet, self).__repr__()[:-1]\n        tmpstr += self.meta_repr()\n        tmpstr = tmpstr + \')\'\n        return tmpstr\n\n    def meta_repr(self):\n        tmpstr = \'  (\' + \'meta\' + \'): dict( \\n\' # + self.meta.__repr__() + \'\\n\'\n        tmpstr += \'     architecture: {}\\n\'.format(self.meta[\'architecture\'])\n        tmpstr += \'     local_whitening: {}\\n\'.format(self.meta[\'local_whitening\'])\n        tmpstr += \'     pooling: {}\\n\'.format(self.meta[\'pooling\'])\n        tmpstr += \'     regional: {}\\n\'.format(self.meta[\'regional\'])\n        tmpstr += \'     whitening: {}\\n\'.format(self.meta[\'whitening\'])\n        tmpstr += \'     outputdim: {}\\n\'.format(self.meta[\'outputdim\'])\n        tmpstr += \'     mean: {}\\n\'.format(self.meta[\'mean\'])\n        tmpstr += \'     std: {}\\n\'.format(self.meta[\'std\'])\n        tmpstr = tmpstr + \'  )\\n\'\n        return tmpstr\n\n\ndef init_network(params):\n\n    # parse params with default values\n    architecture = params.get(\'architecture\', \'resnet101\')\n    local_whitening = params.get(\'local_whitening\', False)\n    pooling = params.get(\'pooling\', \'gem\')\n    regional = params.get(\'regional\', False)\n    whitening = params.get(\'whitening\', False)\n    mean = params.get(\'mean\', [0.485, 0.456, 0.406])\n    std = params.get(\'std\', [0.229, 0.224, 0.225])\n    pretrained = params.get(\'pretrained\', True)\n\n    # get output dimensionality size\n    dim = OUTPUT_DIM[architecture]\n\n    # loading network from torchvision\n    if pretrained:\n        if architecture not in FEATURES:\n            # initialize with network pretrained on imagenet in pytorch\n            net_in = getattr(torchvision.models, architecture)(pretrained=True)\n        else:\n            # initialize with random weights, later on we will fill features with custom pretrained network\n            net_in = getattr(torchvision.models, architecture)(pretrained=False)\n    else:\n        # initialize with random weights\n        net_in = getattr(torchvision.models, architecture)(pretrained=False)\n\n    # initialize features\n    # take only convolutions for features,\n    # always ends with ReLU to make last activations non-negative\n    if architecture.startswith(\'alexnet\'):\n        features = list(net_in.features.children())[:-1]\n    elif architecture.startswith(\'vgg\'):\n        features = list(net_in.features.children())[:-1]\n    elif architecture.startswith(\'resnet\'):\n        features = list(net_in.children())[:-2]\n    elif architecture.startswith(\'densenet\'):\n        features = list(net_in.features.children())\n        features.append(nn.ReLU(inplace=True))\n    elif architecture.startswith(\'squeezenet\'):\n        features = list(net_in.features.children())\n    else:\n        raise ValueError(\'Unsupported or unknown architecture: {}!\'.format(architecture))\n\n    # initialize local whitening\n    if local_whitening:\n        lwhiten = nn.Linear(dim, dim, bias=True)\n        # TODO: lwhiten with possible dimensionality reduce\n\n        if pretrained:\n            lw = architecture\n            if lw in L_WHITENING:\n                print("">> {}: for \'{}\' custom computed local whitening \'{}\' is used""\n                    .format(os.path.basename(__file__), lw, os.path.basename(L_WHITENING[lw])))\n                whiten_dir = os.path.join(get_data_root(), \'whiten\')\n                lwhiten.load_state_dict(model_zoo.load_url(L_WHITENING[lw], model_dir=whiten_dir))\n            else:\n                print("">> {}: for \'{}\' there is no local whitening computed, random weights are used""\n                    .format(os.path.basename(__file__), lw))\n\n    else:\n        lwhiten = None\n    \n    # initialize pooling\n    if pooling == \'gemmp\':\n        pool = POOLING[pooling](mp=dim)\n    else:\n        pool = POOLING[pooling]()\n    \n    # initialize regional pooling\n    if regional:\n        rpool = pool\n        rwhiten = nn.Linear(dim, dim, bias=True)\n        # TODO: rwhiten with possible dimensionality reduce\n\n        if pretrained:\n            rw = \'{}-{}-r\'.format(architecture, pooling)\n            if rw in R_WHITENING:\n                print("">> {}: for \'{}\' custom computed regional whitening \'{}\' is used""\n                    .format(os.path.basename(__file__), rw, os.path.basename(R_WHITENING[rw])))\n                whiten_dir = os.path.join(get_data_root(), \'whiten\')\n                rwhiten.load_state_dict(model_zoo.load_url(R_WHITENING[rw], model_dir=whiten_dir))\n            else:\n                print("">> {}: for \'{}\' there is no regional whitening computed, random weights are used""\n                    .format(os.path.basename(__file__), rw))\n\n        pool = Rpool(rpool, rwhiten)\n\n    # initialize whitening\n    if whitening:\n        whiten = nn.Linear(dim, dim, bias=True)\n        # TODO: whiten with possible dimensionality reduce\n\n        if pretrained:\n            w = architecture\n            if local_whitening:\n                w += \'-lw\'\n            w += \'-\' + pooling\n            if regional:\n                w += \'-r\'\n            if w in WHITENING:\n                print("">> {}: for \'{}\' custom computed whitening \'{}\' is used""\n                    .format(os.path.basename(__file__), w, os.path.basename(WHITENING[w])))\n                whiten_dir = os.path.join(get_data_root(), \'whiten\')\n                whiten.load_state_dict(model_zoo.load_url(WHITENING[w], model_dir=whiten_dir))\n            else:\n                print("">> {}: for \'{}\' there is no whitening computed, random weights are used""\n                    .format(os.path.basename(__file__), w))\n    else:\n        whiten = None\n\n    # create meta information to be stored in the network\n    meta = {\n        \'architecture\' : architecture, \n        \'local_whitening\' : local_whitening, \n        \'pooling\' : pooling, \n        \'regional\' : regional, \n        \'whitening\' : whitening, \n        \'mean\' : mean, \n        \'std\' : std,\n        \'outputdim\' : dim,\n    }\n\n    # create a generic image retrieval network\n    net = ImageRetrievalNet(features, lwhiten, pool, whiten, meta)\n\n    # initialize features with custom pretrained network if needed\n    if pretrained and architecture in FEATURES:\n        print("">> {}: for \'{}\' custom pretrained features \'{}\' are used""\n            .format(os.path.basename(__file__), architecture, os.path.basename(FEATURES[architecture])))\n        model_dir = os.path.join(get_data_root(), \'networks\')\n        net.features.load_state_dict(model_zoo.load_url(FEATURES[architecture], model_dir=model_dir))\n\n    return net\n\n\ndef extract_vectors(net, images, image_size, transform, bbxs=None, ms=[1], msp=1, print_freq=10):\n    # moving network to gpu and eval mode\n    net.cuda()\n    net.eval()\n\n    # creating dataset loader\n    loader = torch.utils.data.DataLoader(\n        ImagesFromList(root=\'\', images=images, imsize=image_size, bbxs=bbxs, transform=transform),\n        batch_size=1, shuffle=False, num_workers=8, pin_memory=True\n    )\n\n    # extracting vectors\n    with torch.no_grad():\n        vecs = torch.zeros(net.meta[\'outputdim\'], len(images))\n        for i, input in enumerate(loader):\n            input = input.cuda()\n\n            if len(ms) == 1 and ms[0] == 1:\n                vecs[:, i] = extract_ss(net, input)\n            else:\n                vecs[:, i] = extract_ms(net, input, ms, msp)\n\n            if (i+1) % print_freq == 0 or (i+1) == len(images):\n                print(\'\\r>>>> {}/{} done...\'.format((i+1), len(images)), end=\'\')\n        print(\'\')\n\n    return vecs\n\ndef extract_ss(net, input):\n    return net(input).cpu().data.squeeze()\n\ndef extract_ms(net, input, ms, msp):\n    \n    v = torch.zeros(net.meta[\'outputdim\'])\n    \n    for s in ms: \n        if s == 1:\n            input_t = input.clone()\n        else:    \n            input_t = nn.functional.interpolate(input, scale_factor=s, mode=\'bilinear\', align_corners=False)\n        v += net(input_t).pow(msp).cpu().data.squeeze()\n        \n    v /= len(ms)\n    v = v.pow(1./msp)\n    v /= v.norm()\n\n    return v\n\n\ndef extract_regional_vectors(net, images, image_size, transform, bbxs=None, ms=[1], msp=1, print_freq=10):\n    # moving network to gpu and eval mode\n    net.cuda()\n    net.eval()\n\n    # creating dataset loader\n    loader = torch.utils.data.DataLoader(\n        ImagesFromList(root=\'\', images=images, imsize=image_size, bbxs=bbxs, transform=transform),\n        batch_size=1, shuffle=False, num_workers=8, pin_memory=True\n    )\n\n    # extracting vectors\n    with torch.no_grad():\n        vecs = []\n        for i, input in enumerate(loader):\n            input = input.cuda()\n\n            if len(ms) == 1:\n                vecs.append(extract_ssr(net, input))\n            else:\n                # TODO: not implemented yet\n                # vecs.append(extract_msr(net, input, ms, msp))\n                raise NotImplementedError\n\n            if (i+1) % print_freq == 0 or (i+1) == len(images):\n                print(\'\\r>>>> {}/{} done...\'.format((i+1), len(images)), end=\'\')\n        print(\'\')\n\n    return vecs\n\ndef extract_ssr(net, input):\n    return net.pool(net.features(input), aggregate=False).squeeze(0).squeeze(-1).squeeze(-1).permute(1,0).cpu().data\n\n\ndef extract_local_vectors(net, images, image_size, transform, bbxs=None, ms=[1], msp=1, print_freq=10):\n    # moving network to gpu and eval mode\n    net.cuda()\n    net.eval()\n\n    # creating dataset loader\n    loader = torch.utils.data.DataLoader(\n        ImagesFromList(root=\'\', images=images, imsize=image_size, bbxs=bbxs, transform=transform),\n        batch_size=1, shuffle=False, num_workers=8, pin_memory=True\n    )\n\n    # extracting vectors\n    with torch.no_grad():\n        vecs = []\n        for i, input in enumerate(loader):\n            input = input.cuda()\n\n            if len(ms) == 1:\n                vecs.append(extract_ssl(net, input))\n            else:\n                # TODO: not implemented yet\n                # vecs.append(extract_msl(net, input, ms, msp))\n                raise NotImplementedError\n\n            if (i+1) % print_freq == 0 or (i+1) == len(images):\n                print(\'\\r>>>> {}/{} done...\'.format((i+1), len(images)), end=\'\')\n        print(\'\')\n\n    return vecs\n\ndef extract_ssl(net, input):\n    return net.norm(net.features(input)).squeeze(0).view(net.meta[\'outputdim\'], -1).cpu().data'"
cirtorch/utils/__init__.py,0,b''
cirtorch/utils/download.py,0,"b'import os\n\ndef download_test(data_dir):\n    """"""\n    DOWNLOAD_TEST Checks, and, if required, downloads the necessary datasets for the testing.\n      \n        download_test(DATA_ROOT) checks if the data necessary for running the example script exist.\n        If not it downloads it in the folder structure:\n            DATA_ROOT/test/oxford5k/  : folder with Oxford images and ground truth file\n            DATA_ROOT/test/paris6k/   : folder with Paris images and ground truth file\n            DATA_ROOT/test/roxford5k/ : folder with Oxford images and revisited ground truth file\n            DATA_ROOT/test/rparis6k/  : folder with Paris images and revisited ground truth file\n    """"""\n\n    # Create data folder if it does not exist\n    if not os.path.isdir(data_dir):\n        os.mkdir(data_dir)\n    \n    # Create datasets folder if it does not exist\n    datasets_dir = os.path.join(data_dir, \'test\')\n    if not os.path.isdir(datasets_dir):\n        os.mkdir(datasets_dir)\n\n    # Download datasets folders test/DATASETNAME/\n    datasets = [\'oxford5k\', \'paris6k\', \'roxford5k\', \'rparis6k\']\n    for di in range(len(datasets)):\n        dataset = datasets[di]\n\n        if dataset == \'oxford5k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/oxbuildings\'\n            dl_files = [\'oxbuild_images.tgz\']\n        elif dataset == \'paris6k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/parisbuildings\'\n            dl_files = [\'paris_1.tgz\', \'paris_2.tgz\']\n        elif dataset == \'roxford5k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/oxbuildings\'\n            dl_files = [\'oxbuild_images.tgz\']\n        elif dataset == \'rparis6k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/parisbuildings\'\n            dl_files = [\'paris_1.tgz\', \'paris_2.tgz\']\n        else:\n            raise ValueError(\'Unknown dataset: {}!\'.format(dataset))\n\n        dst_dir = os.path.join(datasets_dir, dataset, \'jpg\')\n        if not os.path.isdir(dst_dir):\n\n            # for oxford and paris download images\n            if dataset == \'oxford5k\' or dataset == \'paris6k\':\n                print(\'>> Dataset {} directory does not exist. Creating: {}\'.format(dataset, dst_dir))\n                os.makedirs(dst_dir)\n                for dli in range(len(dl_files)):\n                    dl_file = dl_files[dli]\n                    src_file = os.path.join(src_dir, dl_file)\n                    dst_file = os.path.join(dst_dir, dl_file)\n                    print(\'>> Downloading dataset {} archive {}...\'.format(dataset, dl_file))\n                    os.system(\'wget {} -O {}\'.format(src_file, dst_file))\n                    print(\'>> Extracting dataset {} archive {}...\'.format(dataset, dl_file))\n                    # create tmp folder\n                    dst_dir_tmp = os.path.join(dst_dir, \'tmp\')\n                    os.system(\'mkdir {}\'.format(dst_dir_tmp))\n                    # extract in tmp folder\n                    os.system(\'tar -zxf {} -C {}\'.format(dst_file, dst_dir_tmp))\n                    # remove all (possible) subfolders by moving only files in dst_dir\n                    os.system(\'find {} -type f -exec mv -i {{}} {} \\\\;\'.format(dst_dir_tmp, dst_dir))\n                    # remove tmp folder\n                    os.system(\'rm -rf {}\'.format(dst_dir_tmp))\n                    print(\'>> Extracted, deleting dataset {} archive {}...\'.format(dataset, dl_file))\n                    os.system(\'rm {}\'.format(dst_file))\n\n            # for roxford and rparis just make sym links\n            elif dataset == \'roxford5k\' or dataset == \'rparis6k\':\n                print(\'>> Dataset {} directory does not exist. Creating: {}\'.format(dataset, dst_dir))\n                dataset_old = dataset[1:]\n                dst_dir_old = os.path.join(datasets_dir, dataset_old, \'jpg\')\n                os.mkdir(os.path.join(datasets_dir, dataset))\n                os.system(\'ln -s {} {}\'.format(dst_dir_old, dst_dir))\n                print(\'>> Created symbolic link from {} jpg to {} jpg\'.format(dataset_old, dataset))\n\n\n        gnd_src_dir = os.path.join(\'http://cmp.felk.cvut.cz/cnnimageretrieval/data\', \'test\', dataset)\n        gnd_dst_dir = os.path.join(datasets_dir, dataset)\n        gnd_dl_file = \'gnd_{}.pkl\'.format(dataset)\n        gnd_src_file = os.path.join(gnd_src_dir, gnd_dl_file)\n        gnd_dst_file = os.path.join(gnd_dst_dir, gnd_dl_file)\n        if not os.path.exists(gnd_dst_file):\n            print(\'>> Downloading dataset {} ground truth file...\'.format(dataset))\n            os.system(\'wget {} -O {}\'.format(gnd_src_file, gnd_dst_file))\n\n\ndef download_train(data_dir):\n    """"""\n    DOWNLOAD_TRAIN Checks, and, if required, downloads the necessary datasets for the training.\n      \n        download_train(DATA_ROOT) checks if the data necessary for running the example script exist.\n        If not it downloads it in the folder structure:\n            DATA_ROOT/train/retrieval-SfM-120k/  : folder with rsfm120k images and db files\n            DATA_ROOT/train/retrieval-SfM-30k/   : folder with rsfm30k images and db files\n    """"""\n\n    # Create data folder if it does not exist\n    if not os.path.isdir(data_dir):\n        os.mkdir(data_dir)\n    \n    # Create datasets folder if it does not exist\n    datasets_dir = os.path.join(data_dir, \'train\')\n    if not os.path.isdir(datasets_dir):\n        os.mkdir(datasets_dir)\n\n    # Download folder train/retrieval-SfM-120k/\n    src_dir = os.path.join(\'http://cmp.felk.cvut.cz/cnnimageretrieval/data\', \'train\', \'ims\')\n    dst_dir = os.path.join(datasets_dir, \'retrieval-SfM-120k\', \'ims\')\n    dl_file = \'ims.tar.gz\'\n    if not os.path.isdir(dst_dir):\n        src_file = os.path.join(src_dir, dl_file)\n        dst_file = os.path.join(dst_dir, dl_file)\n        print(\'>> Image directory does not exist. Creating: {}\'.format(dst_dir))\n        os.makedirs(dst_dir)\n        print(\'>> Downloading ims.tar.gz...\')\n        os.system(\'wget {} -O {}\'.format(src_file, dst_file))\n        print(\'>> Extracting {}...\'.format(dst_file))\n        os.system(\'tar -zxf {} -C {}\'.format(dst_file, dst_dir))\n        print(\'>> Extracted, deleting {}...\'.format(dst_file))\n        os.system(\'rm {}\'.format(dst_file))\n\n    # Create symlink for train/retrieval-SfM-30k/ \n    dst_dir_old = os.path.join(datasets_dir, \'retrieval-SfM-120k\', \'ims\')\n    dst_dir = os.path.join(datasets_dir, \'retrieval-SfM-30k\', \'ims\')\n    if not os.path.isdir(dst_dir):\n        os.makedirs(os.path.join(datasets_dir, \'retrieval-SfM-30k\'))\n        os.system(\'ln -s {} {}\'.format(dst_dir_old, dst_dir))\n        print(\'>> Created symbolic link from retrieval-SfM-120k/ims to retrieval-SfM-30k/ims\')\n\n    # Download db files\n    src_dir = os.path.join(\'http://cmp.felk.cvut.cz/cnnimageretrieval/data\', \'train\', \'dbs\')\n    datasets = [\'retrieval-SfM-120k\', \'retrieval-SfM-30k\']\n    for dataset in datasets:\n        dst_dir = os.path.join(datasets_dir, dataset)\n        if dataset == \'retrieval-SfM-120k\':\n            dl_files = [\'{}.pkl\'.format(dataset), \'{}-whiten.pkl\'.format(dataset)]\n        elif dataset == \'retrieval-SfM-30k\':\n            dl_files = [\'{}-whiten.pkl\'.format(dataset)]\n\n        if not os.path.isdir(dst_dir):\n            print(\'>> Dataset directory does not exist. Creating: {}\'.format(dst_dir))\n            os.mkdir(dst_dir)\n\n        for i in range(len(dl_files)):\n            src_file = os.path.join(src_dir, dl_files[i])\n            dst_file = os.path.join(dst_dir, dl_files[i])\n            if not os.path.isfile(dst_file):\n                print(\'>> DB file {} does not exist. Downloading...\'.format(dl_files[i]))\n                os.system(\'wget {} -O {}\'.format(src_file, dst_file))\n'"
cirtorch/utils/download_win.py,0,"b'import os\n\ndef download_test(data_dir):\n    """"""\n    DOWNLOAD_TEST Checks, and, if required, downloads the necessary datasets for the testing.\n      \n        download_test(DATA_ROOT) checks if the data necessary for running the example script exist.\n        If not it downloads it in the folder structure:\n            DATA_ROOT/test/oxford5k/  : folder with Oxford images and ground truth file\n            DATA_ROOT/test/paris6k/   : folder with Paris images and ground truth file\n            DATA_ROOT/test/roxford5k/ : folder with Oxford images and revisited ground truth file\n            DATA_ROOT/test/rparis6k/  : folder with Paris images and revisited ground truth file\n    """"""\n\n    # Create data folder if it does not exist\n    if not os.path.isdir(data_dir):\n        os.mkdir(data_dir)\n    \n    # Create datasets folder if it does not exist\n    datasets_dir = os.path.join(data_dir, \'test\')\n    if not os.path.isdir(datasets_dir):\n        os.mkdir(datasets_dir)\n\n    # Download datasets folders test/DATASETNAME/\n    datasets = [\'oxford5k\', \'paris6k\', \'roxford5k\', \'rparis6k\']\n    for di in range(len(datasets)):\n        dataset = datasets[di]\n\n        if dataset == \'oxford5k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/oxbuildings\'\n            dl_files = [\'oxbuild_images.tgz\']\n        elif dataset == \'paris6k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/parisbuildings\'\n            dl_files = [\'paris_1.tgz\', \'paris_2.tgz\']\n        elif dataset == \'roxford5k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/oxbuildings\'\n            dl_files = [\'oxbuild_images.tgz\']\n        elif dataset == \'rparis6k\':\n            src_dir = \'http://www.robots.ox.ac.uk/~vgg/data/parisbuildings\'\n            dl_files = [\'paris_1.tgz\', \'paris_2.tgz\']\n        else:\n            raise ValueError(\'Unknown dataset: {}!\'.format(dataset))\n\n        dst_dir = os.path.join(datasets_dir, dataset, \'jpg\')\n        if not os.path.isdir(dst_dir):\n\n            # for oxford and paris download images\n            if dataset == \'oxford5k\' or dataset == \'paris6k\':\n                print(\'>> Dataset {} directory does not exist. Creating: {}\'.format(dataset, dst_dir))\n                os.makedirs(dst_dir)\n                for dli in range(len(dl_files)):\n                    dl_file = dl_files[dli]\n                    src_file = os.path.join(src_dir, dl_file)\n                    dst_file = os.path.join(dst_dir, dl_file)\n                    print(\'>> Downloading dataset {} archive {}...\'.format(dataset, dl_file))\n                    os.system(\'wget {} -O {}\'.format(src_file, dst_file))\n                    print(\'>> Extracting dataset {} archive {}...\'.format(dataset, dl_file))\n                    # create tmp folder\n                    dst_dir_tmp = os.path.join(dst_dir, \'tmp\')\n                    os.system(\'mkdir {}\'.format(dst_dir_tmp))\n                    # extract in tmp folder\n                    os.system(\'tar -zxf {} -C {}\'.format(dst_file, dst_dir_tmp))\n                    # remove all (possible) subfolders by moving only files in dst_dir\n                    os.system(\'find {} -type f -exec mv -i {{}} {} \\\\;\'.format(dst_dir_tmp, dst_dir))\n                    # remove tmp folder\n                    os.system(\'rd {}\'.format(dst_dir_tmp))\n                    print(\'>> Extracted, deleting dataset {} archive {}...\'.format(dataset, dl_file))\n                    os.system(\'del {}\'.format(dst_file))\n\n            # for roxford and rparis just make sym links\n            elif dataset == \'roxford5k\' or dataset == \'rparis6k\':\n                print(\'>> Dataset {} directory does not exist. Creating: {}\'.format(dataset, dst_dir))\n                dataset_old = dataset[1:]\n                dst_dir_old = os.path.join(datasets_dir, dataset_old, \'jpg\')\n                os.mkdir(os.path.join(datasets_dir, dataset))\n                os.system(\'cmd /c mklink /d {} {}\'.format(dst_dir_old, dst_dir))\n                print(\'>> Created symbolic link from {} jpg to {} jpg\'.format(dataset_old, dataset))\n\n\n        gnd_src_dir = os.path.join(\'http://cmp.felk.cvut.cz/cnnimageretrieval/data\', \'test\', dataset)\n        gnd_dst_dir = os.path.join(datasets_dir, dataset)\n        gnd_dl_file = \'gnd_{}.pkl\'.format(dataset)\n        gnd_src_file = os.path.join(gnd_src_dir, gnd_dl_file)\n        gnd_dst_file = os.path.join(gnd_dst_dir, gnd_dl_file)\n        if not os.path.exists(gnd_dst_file):\n            print(\'>> Downloading dataset {} ground truth file...\'.format(dataset))\n            os.system(\'wget {} -O {}\'.format(gnd_src_file, gnd_dst_file))\n\n\ndef download_train(data_dir):\n    """"""\n    DOWNLOAD_TRAIN Checks, and, if required, downloads the necessary datasets for the training.\n      \n        download_train(DATA_ROOT) checks if the data necessary for running the example script exist.\n        If not it downloads it in the folder structure:\n            DATA_ROOT/train/retrieval-SfM-120k/  : folder with rsfm120k images and db files\n            DATA_ROOT/train/retrieval-SfM-30k/   : folder with rsfm30k images and db files\n    """"""\n\n    # Create data folder if it does not exist\n    if not os.path.isdir(data_dir):\n        os.mkdir(data_dir)\n    print(data_dir)\n    # Create datasets folder if it does not exist\n    datasets_dir = os.path.join(data_dir, \'train\')\n    if not os.path.isdir(datasets_dir):\n        os.mkdir(datasets_dir)\n\n    # Download folder train/retrieval-SfM-120k/\n    src_dir = os.path.join(\'http://cmp.felk.cvut.cz/cnnimageretrieval/data\', \'train\', \'ims\')\n    dst_dir = os.path.join(datasets_dir, \'retrieval-SfM-120k\', \'ims\')\n    dl_file = \'ims.tar.gz\'\n    if not os.path.isdir(dst_dir):\n        src_file = os.path.join(src_dir, dl_file)\n        dst_file = os.path.join(dst_dir, dl_file)\n        print(\'>> Image directory does not exist. Creating: {}\'.format(dst_dir))\n        os.makedirs(dst_dir)\n        print(\'>> Downloading ims.tar.gz...\')\n        # os.system(\'wget {} -O {}\'.format(src_file, dst_file))\n        print(\'>> Extracting {}...\'.format(dst_file))\n        os.system(\'tar -zxf {} -C {}\'.format(dst_file, dst_dir))\n        print(\'>> Extracted, deleting {}...\'.format(dst_file))\n        os.system(\'del {}\'.format(dst_file))\n\n    # Create symlink for train/retrieval-SfM-30k/ \n    dst_dir_old = os.path.join(datasets_dir, \'retrieval-SfM-120k\', \'ims\')\n    dst_dir = os.path.join(datasets_dir, \'retrieval-SfM-30k\', \'ims\')\n    if not os.path.isdir(dst_dir):\n        os.makedirs(os.path.join(datasets_dir, \'retrieval-SfM-30k\',\'ims\'))\n        os.system(\'mklink {} {}\'.format(dst_dir_old, dst_dir))\n        print(\'>> Created symbolic link from retrieval-SfM-120k/ims to retrieval-SfM-30k/ims\')\n\n    # Download db files\n    src_dir = os.path.join(\'http://cmp.felk.cvut.cz/cnnimageretrieval/data\', \'train\', \'dbs\')\n    datasets = [\'retrieval-SfM-120k\', \'retrieval-SfM-30k\']\n    for dataset in datasets:\n        dst_dir = os.path.join(datasets_dir, dataset)\n        if dataset == \'retrieval-SfM-120k\':\n            dl_files = [\'{}.pkl\'.format(dataset), \'{}-whiten.pkl\'.format(dataset)]\n        elif dataset == \'retrieval-SfM-30k\':\n            dl_files = [\'{}-whiten.pkl\'.format(dataset)]\n\n        if not os.path.isdir(dst_dir):\n            print(\'>> Dataset directory does not exist. Creating: {}\'.format(dst_dir))\n            os.mkdir(dst_dir)\n\n        for i in range(len(dl_files)):\n            src_file = os.path.join(src_dir, dl_files[i])\n            dst_file = os.path.join(dst_dir, dl_files[i])\n            if not os.path.isfile(dst_file):\n                print(\'>> DB file {} does not exist. Downloading...\'.format(dl_files[i]))\n                os.system(\'wget {} -O {}\'.format(src_file, dst_file))\n'"
cirtorch/utils/evaluate.py,0,"b'import numpy as np\n\ndef compute_ap(ranks, nres):\n    """"""\n    Computes average precision for given ranked indexes.\n    \n    Arguments\n    ---------\n    ranks : zerro-based ranks of positive images\n    nres  : number of positive images\n    \n    Returns\n    -------\n    ap    : average precision\n    """"""\n\n    # number of images ranked by the system\n    nimgranks = len(ranks)\n\n    # accumulate trapezoids in PR-plot\n    ap = 0\n\n    recall_step = 1. / nres\n\n    for j in np.arange(nimgranks):\n        rank = ranks[j]\n\n        if rank == 0:\n            precision_0 = 1.\n        else:\n            precision_0 = float(j) / rank\n\n        precision_1 = float(j + 1) / (rank + 1)\n\n        ap += (precision_0 + precision_1) * recall_step / 2.\n\n    return ap\n\ndef compute_map(ranks, gnd, kappas=[]):\n    """"""\n    Computes the mAP for a given set of returned results.\n\n         Usage: \n           map = compute_map (ranks, gnd) \n                 computes mean average precsion (map) only\n        \n           map, aps, pr, prs = compute_map (ranks, gnd, kappas) \n                 computes mean average precision (map), average precision (aps) for each query\n                 computes mean precision at kappas (pr), precision at kappas (prs) for each query\n        \n         Notes:\n         1) ranks starts from 0, ranks.shape = db_size X #queries\n         2) The junk results (e.g., the query itself) should be declared in the gnd stuct array\n         3) If there are no positive images for some query, that query is excluded from the evaluation\n    """"""\n\n    map = 0.\n    nq = len(gnd) # number of queries\n    aps = np.zeros(nq)\n    pr = np.zeros(len(kappas))\n    prs = np.zeros((nq, len(kappas)))\n    nempty = 0\n\n    for i in np.arange(nq):\n        qgnd = np.array(gnd[i][\'ok\'])\n\n        # no positive images, skip from the average\n        if qgnd.shape[0] == 0:\n            aps[i] = float(\'nan\')\n            prs[i, :] = float(\'nan\')\n            nempty += 1\n            continue\n\n        try:\n            qgndj = np.array(gnd[i][\'junk\'])\n        except:\n            qgndj = np.empty(0)\n\n        # sorted positions of positive and junk images (0 based)\n        pos  = np.arange(ranks.shape[0])[np.in1d(ranks[:,i], qgnd)]\n        junk = np.arange(ranks.shape[0])[np.in1d(ranks[:,i], qgndj)]\n\n        k = 0;\n        ij = 0;\n        if len(junk):\n            # decrease positions of positives based on the number of\n            # junk images appearing before them\n            ip = 0\n            while (ip < len(pos)):\n                while (ij < len(junk) and pos[ip] > junk[ij]):\n                    k += 1\n                    ij += 1\n                pos[ip] = pos[ip] - k\n                ip += 1\n\n        # compute ap\n        ap = compute_ap(pos, len(qgnd))\n        map = map + ap\n        aps[i] = ap\n\n        # compute precision @ k\n        pos += 1 # get it to 1-based\n        for j in np.arange(len(kappas)):\n            kq = min(max(pos), kappas[j]); \n            prs[i, j] = (pos <= kq).sum() / kq\n        pr = pr + prs[i, :]\n\n    map = map / (nq - nempty)\n    pr = pr / (nq - nempty)\n\n    return map, aps, pr, prs\n\n\ndef compute_map_and_print(dataset, ranks, gnd, kappas=[1, 5, 10]):\n    \n    # old evaluation protocol\n    if dataset.startswith(\'oxford5k\') or dataset.startswith(\'paris6k\'):\n        map, aps, _, _ = compute_map(ranks, gnd)\n        print(\'>> {}: mAP {:.2f}\'.format(dataset, np.around(map*100, decimals=2)))\n\n    # new evaluation protocol\n    elif dataset.startswith(\'roxford5k\') or dataset.startswith(\'rparis6k\'):\n        \n        gnd_t = []\n        for i in range(len(gnd)):\n            g = {}\n            g[\'ok\'] = np.concatenate([gnd[i][\'easy\']])\n            g[\'junk\'] = np.concatenate([gnd[i][\'junk\'], gnd[i][\'hard\']])\n            gnd_t.append(g)\n        mapE, apsE, mprE, prsE = compute_map(ranks, gnd_t, kappas)\n\n        gnd_t = []\n        for i in range(len(gnd)):\n            g = {}\n            g[\'ok\'] = np.concatenate([gnd[i][\'easy\'], gnd[i][\'hard\']])\n            g[\'junk\'] = np.concatenate([gnd[i][\'junk\']])\n            gnd_t.append(g)\n        mapM, apsM, mprM, prsM = compute_map(ranks, gnd_t, kappas)\n\n        gnd_t = []\n        for i in range(len(gnd)):\n            g = {}\n            g[\'ok\'] = np.concatenate([gnd[i][\'hard\']])\n            g[\'junk\'] = np.concatenate([gnd[i][\'junk\'], gnd[i][\'easy\']])\n            gnd_t.append(g)\n        mapH, apsH, mprH, prsH = compute_map(ranks, gnd_t, kappas)\n\n        print(\'>> {}: mAP E: {}, M: {}, H: {}\'.format(dataset, np.around(mapE*100, decimals=2), np.around(mapM*100, decimals=2), np.around(mapH*100, decimals=2)))\n        print(\'>> {}: mP@k{} E: {}, M: {}, H: {}\'.format(dataset, kappas, np.around(mprE*100, decimals=2), np.around(mprM*100, decimals=2), np.around(mprH*100, decimals=2)))'"
cirtorch/utils/general.py,0,"b""import os\nimport hashlib\n\ndef get_root():\n    return os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))))\n\n\ndef get_data_root():\n    return os.path.join(get_root(), 'data')\n\n\ndef htime(c):\n    c = round(c)\n    \n    days = c // 86400\n    hours = c // 3600 % 24\n    minutes = c // 60 % 60\n    seconds = c % 60\n\n    if days > 0:\n        return '{:d}d {:d}h {:d}m {:d}s'.format(days, hours, minutes, seconds)\n    if hours > 0:\n        return '{:d}h {:d}m {:d}s'.format(hours, minutes, seconds)\n    if minutes > 0:\n        return '{:d}m {:d}s'.format(minutes, seconds)\n    return '{:d}s'.format(seconds)\n\n\ndef sha256_hash(filename, block_size=65536, length=8):\n    sha256 = hashlib.sha256()\n    with open(filename, 'rb') as f:\n        for block in iter(lambda: f.read(block_size), b''):\n            sha256.update(block)\n    return sha256.hexdigest()[:length-1]"""
cirtorch/utils/whiten.py,0,"b'import os\nimport numpy as np\n\ndef whitenapply(X, m, P, dimensions=None):\n    \n    if not dimensions:\n        dimensions = P.shape[0]\n\n    X = np.dot(P[:dimensions, :], X-m)\n    X = X / (np.linalg.norm(X, ord=2, axis=0, keepdims=True) + 1e-6)\n\n    return X\n\ndef pcawhitenlearn(X):\n\n    N = X.shape[1]\n\n    # Learning PCA w/o annotations\n    m = X.mean(axis=1, keepdims=True)\n    Xc = X - m\n    Xcov = np.dot(Xc, Xc.T)\n    Xcov = (Xcov + Xcov.T) / (2*N)\n    eigval, eigvec = np.linalg.eig(Xcov)\n    order = eigval.argsort()[::-1]\n    eigval = eigval[order]\n    eigvec = eigvec[:, order]\n\n    P = np.dot(np.linalg.inv(np.sqrt(np.diag(eigval))), eigvec.T)\n    \n    return m, P\n\ndef whitenlearn(X, qidxs, pidxs):\n\n    # Learning Lw w annotations\n    m = X[:, qidxs].mean(axis=1, keepdims=True)\n    df = X[:, qidxs] - X[:, pidxs]\n    S = np.dot(df, df.T) / df.shape[1]\n    P = np.linalg.inv(cholesky(S))\n    df = np.dot(P, X-m)\n    D = np.dot(df, df.T)\n    eigval, eigvec = np.linalg.eig(D)\n    order = eigval.argsort()[::-1]\n    eigval = eigval[order]\n    eigvec = eigvec[:, order]\n\n    P = np.dot(eigvec.T, P)\n\n    return m, P\n\ndef cholesky(S):\n    # Cholesky decomposition\n    # with adding a small value on the diagonal\n    # until matrix is positive definite\n    alpha = 0\n    while 1:\n        try:\n            L = np.linalg.cholesky(S + alpha*np.eye(*S.shape))\n            return L\n        except:\n            if alpha == 0:\n                alpha = 1e-10\n            else:\n                alpha *= 10\n            print("">>>> {}::cholesky: Matrix is not positive definite, adding {:.0e} on the diagonal""\n                .format(os.path.basename(__file__), alpha))\n'"
