file_path,api_count,code
predict.py,0,"b'from flair.data import Sentence\nfrom flair.models import SequenceTagger\n\ntagger: SequenceTagger = SequenceTagger.load(""ner"")\n\nsentence: Sentence = Sentence(""George Washington went to Washington ."")\ntagger.predict(sentence)\n\nprint(""Analysing %s"" % sentence)\nprint(""\\nThe following NER tags are found: \\n"")\nprint(sentence.to_tagged_string())\n'"
setup.py,0,"b'from setuptools import setup, find_packages\n\nwith open(""requirements.txt"") as f:\n    required = f.read().splitlines()\n\nsetup(\n    name=""flair"",\n    version=""0.5"",\n    description=""A very simple framework for state-of-the-art NLP"",\n    long_description=open(""README.md"", encoding=""utf-8"").read(),\n    long_description_content_type=""text/markdown"",\n    author=""Alan Akbik"",\n    author_email=""alan.akbik@gmail.com"",\n    url=""https://github.com/flairNLP/flair"",\n    packages=find_packages(exclude=""tests""),  # same as name\n    license=""MIT"",\n    install_requires=required,\n    include_package_data=True,\n    python_requires="">=3.6"",\n)\n'"
train.py,0,"b'from typing import List\n\nimport flair.datasets\nfrom flair.data import Corpus\nfrom flair.embeddings import (\n    TokenEmbeddings,\n    WordEmbeddings,\n    StackedEmbeddings,\n    FlairEmbeddings,\n    CharacterEmbeddings,\n)\nfrom flair.training_utils import EvaluationMetric\nfrom flair.visual.training_curves import Plotter\n\n# 1. get the corpus\ncorpus: Corpus = flair.datasets.UD_ENGLISH()\nprint(corpus)\n\n# 2. what tag do we want to predict?\ntag_type = ""upos""\n\n# 3. make the tag dictionary from the corpus\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\nprint(tag_dictionary.idx2item)\n\n# initialize embeddings\nembedding_types: List[TokenEmbeddings] = [\n    WordEmbeddings(""glove""),\n    # comment in this line to use character embeddings\n    # CharacterEmbeddings(),\n    # comment in these lines to use contextual string embeddings\n    #\n    # FlairEmbeddings(\'news-forward\'),\n    #\n    # FlairEmbeddings(\'news-backward\'),\n]\n\nembeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n\n# initialize sequence tagger\nfrom flair.models import SequenceTagger\n\ntagger: SequenceTagger = SequenceTagger(\n    hidden_size=256,\n    embeddings=embeddings,\n    tag_dictionary=tag_dictionary,\n    tag_type=tag_type,\n    use_crf=True,\n)\n\n# initialize trainer\nfrom flair.trainers import ModelTrainer\n\ntrainer: ModelTrainer = ModelTrainer(tagger, corpus)\n\ntrainer.train(\n    ""resources/taggers/example-ner"",\n    learning_rate=0.1,\n    mini_batch_size=32,\n    max_epochs=20,\n    shuffle=False,\n)\n\nplotter = Plotter()\nplotter.plot_training_curves(""resources/taggers/example-ner/loss.tsv"")\nplotter.plot_weights(""resources/taggers/example-ner/weights.txt"")\n'"
flair/__init__.py,3,"b'import os\nimport torch\nfrom pathlib import Path\n\n# global variable: cache_root\ncache_root = os.getenv(\'FLAIR_CACHE_ROOT\', Path(Path.home(), "".flair""))\n\n# global variable: device\ndevice = None\nif torch.cuda.is_available():\n    device = torch.device(""cuda:0"")\nelse:\n    device = torch.device(""cpu"")\n\n# global variable: embedding_storage_mode\nembedding_storage_mode = ""default""\n\nfrom . import data\nfrom . import models\nfrom . import visual\nfrom . import trainers\nfrom . import nn\nfrom .training_utils import AnnealOnPlateau\n\nimport logging.config\n\n__version__ = ""0.5""\n\nlogging.config.dictConfig(\n    {\n        ""version"": 1,\n        ""disable_existing_loggers"": False,\n        ""formatters"": {""standard"": {""format"": ""%(asctime)-15s %(message)s""}},\n        ""handlers"": {\n            ""console"": {\n                ""level"": ""INFO"",\n                ""class"": ""logging.StreamHandler"",\n                ""formatter"": ""standard"",\n                ""stream"": ""ext://sys.stdout"",\n            }\n        },\n        ""loggers"": {\n            ""flair"": {""handlers"": [""console""], ""level"": ""INFO"", ""propagate"": False}\n        },\n    }\n)\n\nlogger = logging.getLogger(""flair"")\n'"
flair/data.py,16,"b'from abc import abstractmethod\nfrom operator import itemgetter\nfrom typing import List, Dict, Union, Callable, Optional\nimport re\n\nimport torch, flair\nimport logging\n\nfrom collections import Counter\nfrom collections import defaultdict\n\nfrom segtok.segmenter import split_single\nfrom segtok.tokenizer import split_contractions\nfrom segtok.tokenizer import word_tokenizer\nfrom torch.utils.data import Dataset, random_split\nfrom torch.utils.data.dataset import ConcatDataset, Subset\n\nfrom flair.file_utils import Tqdm\n\nlog = logging.getLogger(""flair"")\n\n\nclass Dictionary:\n    """"""\n    This class holds a dictionary that maps strings to IDs, used to generate one-hot encodings of strings.\n    """"""\n\n    def __init__(self, add_unk=True):\n        # init dictionaries\n        self.item2idx: Dict[str, int] = {}\n        self.idx2item: List[str] = []\n        self.multi_label: bool = False\n\n        # in order to deal with unknown tokens, add <unk>\n        if add_unk:\n            self.add_item(""<unk>"")\n\n    def add_item(self, item: str) -> int:\n        """"""\n        add string - if already in dictionary returns its ID. if not in dictionary, it will get a new ID.\n        :param item: a string for which to assign an id.\n        :return: ID of string\n        """"""\n        item = item.encode(""utf-8"")\n        if item not in self.item2idx:\n            self.idx2item.append(item)\n            self.item2idx[item] = len(self.idx2item) - 1\n        return self.item2idx[item]\n\n    def get_idx_for_item(self, item: str) -> int:\n        """"""\n        returns the ID of the string, otherwise 0\n        :param item: string for which ID is requested\n        :return: ID of string, otherwise 0\n        """"""\n        item = item.encode(""utf-8"")\n        if item in self.item2idx.keys():\n            return self.item2idx[item]\n        else:\n            return 0\n\n    def get_idx_for_items(self, items: List[str]) -> List[int]:\n        """"""\n        returns the IDs for each item of the list of string, otherwise 0 if not found\n        :param items: List of string for which IDs are requested\n        :return: List of ID of strings\n        """"""\n        if not hasattr(self, ""item2idx_not_encoded""):\n            d = dict(\n                [(key.decode(""UTF-8""), value) for key, value in self.item2idx.items()]\n            )\n            self.item2idx_not_encoded = defaultdict(int, d)\n\n        if not items:\n            return []\n        results = itemgetter(*items)(self.item2idx_not_encoded)\n        if isinstance(results, int):\n            return [results]\n        return list(results)\n\n    def get_items(self) -> List[str]:\n        items = []\n        for item in self.idx2item:\n            items.append(item.decode(""UTF-8""))\n        return items\n\n    def __len__(self) -> int:\n        return len(self.idx2item)\n\n    def get_item_for_index(self, idx):\n        return self.idx2item[idx].decode(""UTF-8"")\n\n    def save(self, savefile):\n        import pickle\n\n        with open(savefile, ""wb"") as f:\n            mappings = {""idx2item"": self.idx2item, ""item2idx"": self.item2idx}\n            pickle.dump(mappings, f)\n\n    @classmethod\n    def load_from_file(cls, filename: str):\n        import pickle\n\n        dictionary: Dictionary = Dictionary()\n        with open(filename, ""rb"") as f:\n            mappings = pickle.load(f, encoding=""latin1"")\n            idx2item = mappings[""idx2item""]\n            item2idx = mappings[""item2idx""]\n            dictionary.item2idx = item2idx\n            dictionary.idx2item = idx2item\n        return dictionary\n\n    @classmethod\n    def load(cls, name: str):\n        from flair.file_utils import cached_path\n\n        if name == ""chars"" or name == ""common-chars"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters""\n            char_dict = cached_path(base_path, cache_dir=""datasets"")\n            return Dictionary.load_from_file(char_dict)\n\n        if name == ""chars-large"" or name == ""common-chars-large"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters_large""\n            char_dict = cached_path(base_path, cache_dir=""datasets"")\n            return Dictionary.load_from_file(char_dict)\n\n        if name == ""chars-xl"" or name == ""common-chars-xl"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters_xl""\n            char_dict = cached_path(base_path, cache_dir=""datasets"")\n            return Dictionary.load_from_file(char_dict)\n\n        return Dictionary.load_from_file(name)\n\n    def __str__(self):\n        tags = \', \'.join(self.get_item_for_index(i) for i in range(min(len(self), 30)))\n        return f""Dictionary with {len(self)} tags: {tags}""\n\n\nclass Label:\n    """"""\n    This class represents a label. Each label has a value and optionally a confidence score. The\n    score needs to be between 0.0 and 1.0. Default value for the score is 1.0.\n    """"""\n\n    def __init__(self, value: str, score: float = 1.0):\n        self.value = value\n        self.score = score\n        super().__init__()\n\n    @property\n    def value(self):\n        return self._value\n\n    @value.setter\n    def value(self, value):\n        if not value and value != """":\n            raise ValueError(\n                ""Incorrect label value provided. Label value needs to be set.""\n            )\n        else:\n            self._value = value\n\n    @property\n    def score(self):\n        return self._score\n\n    @score.setter\n    def score(self, score):\n        if 0.0 <= score <= 1.0:\n            self._score = score\n        else:\n            self._score = 1.0\n\n    def to_dict(self):\n        return {""value"": self.value, ""confidence"": self.score}\n\n    def __str__(self):\n        return f""{self._value} ({round(self._score, 4)})""\n\n    def __repr__(self):\n        return f""{self._value} ({round(self._score, 4)})""\n\n\nclass DataPoint:\n    """"""\n    This is the parent class of all data points in Flair (including Token, Sentence, Image, etc.). Each DataPoint\n    must be embeddable (hence the abstract property embedding() and methods to() and clear_embeddings()). Also,\n    each DataPoint may have Labels in several layers of annotation (hence the functions add_label(), get_labels()\n    and the property \'label\')\n    """"""\n\n    def __init__(self):\n        self.annotation_layers = {}\n\n    @property\n    @abstractmethod\n    def embedding(self):\n        pass\n\n    @abstractmethod\n    def to(self, device: str, pin_memory: bool = False):\n        pass\n\n    @abstractmethod\n    def clear_embeddings(self, embedding_names: List[str] = None):\n        pass\n\n    def add_label(self, label_type: str, value: str, score: float = 1.):\n\n        if label_type not in self.annotation_layers:\n            self.annotation_layers[label_type] = [Label(value, score)]\n        else:\n            self.annotation_layers[label_type].append(Label(value, score))\n\n        return self\n\n    def set_label(self, label_type: str, value: str, score: float = 1.):\n        self.annotation_layers[label_type] = [Label(value, score)]\n\n        return self\n\n    def get_labels(self, label_type: str = None):\n        if label_type is None:\n            return self.labels\n\n        return self.annotation_layers[label_type] if label_type in self.annotation_layers else []\n\n    @property\n    def labels(self) -> List[Label]:\n        all_labels = []\n        for key in self.annotation_layers.keys():\n            all_labels.extend(self.annotation_layers[key])\n        return all_labels\n\n\nclass DataPair(DataPoint):\n    def __init__(self, first: DataPoint, second: DataPoint):\n        super().__init__()\n        self.first = first\n        self.second = second\n\n    def to(self, device: str, pin_memory: bool = False):\n        self.first.to(device, pin_memory)\n        self.second.to(device, pin_memory)\n\n    def clear_embeddings(self, embedding_names: List[str] = None):\n        self.first.clear_embeddings(embedding_names)\n        self.second.clear_embeddings(embedding_names)\n\n    @property\n    def embedding(self):\n        return torch.cat([self.first.embedding, self.second.embedding])\n\n    def __str__(self):\n        return f""DataPair:\\n \xe2\x88\x92 First {self.first}\\n \xe2\x88\x92 Second {self.second}\\n \xe2\x88\x92 Labels: {self.labels}""\n\n    def to_plain_string(self):\n        return f""DataPair: First {self.first}  ||  Second {self.second}""\n\n    def __len__(self):\n        return len(self.first) + len(self.second)\n\n\nclass Token(DataPoint):\n    """"""\n    This class represents one word in a tokenized sentence. Each token may have any number of tags. It may also point\n    to its head in a dependency tree.\n    """"""\n\n    def __init__(\n            self,\n            text: str,\n            idx: int = None,\n            head_id: int = None,\n            whitespace_after: bool = True,\n            start_position: int = None,\n    ):\n        super().__init__()\n\n        self.text: str = text\n        self.idx: int = idx\n        self.head_id: int = head_id\n        self.whitespace_after: bool = whitespace_after\n\n        self.start_pos = start_position\n        self.end_pos = (\n            start_position + len(text) if start_position is not None else None\n        )\n\n        self.sentence: Sentence = None\n        self._embeddings: Dict = {}\n        self.tags_proba_dist: Dict[str, List[Label]] = {}\n\n    def add_tag_label(self, tag_type: str, tag: Label):\n        self.set_label(tag_type, tag.value, tag.score)\n\n    def add_tags_proba_dist(self, tag_type: str, tags: List[Label]):\n        self.tags_proba_dist[tag_type] = tags\n\n    def add_tag(self, tag_type: str, tag_value: str, confidence=1.0):\n        self.set_label(tag_type, tag_value, confidence)\n\n    def get_tag(self, label_type):\n        if len(self.get_labels(label_type)) == 0: return Label(\'\')\n        return self.get_labels(label_type)[0]\n\n    def get_tags_proba_dist(self, tag_type: str) -> List[Label]:\n        if tag_type in self.tags_proba_dist:\n            return self.tags_proba_dist[tag_type]\n        return []\n\n    def get_head(self):\n        return self.sentence.get_token(self.head_id)\n\n    def set_embedding(self, name: str, vector: torch.tensor):\n        device = flair.device\n        if (flair.embedding_storage_mode == ""cpu"") and len(self._embeddings.keys()) > 0:\n            device = next(iter(self._embeddings.values())).device\n        if device != vector.device:\n            vector = vector.to(device)\n        self._embeddings[name] = vector\n\n    def to(self, device: str, pin_memory: bool = False):\n        for name, vector in self._embeddings.items():\n            if str(vector.device) != str(device):\n                if pin_memory:\n                    self._embeddings[name] = vector.to(\n                        device, non_blocking=True\n                    ).pin_memory()\n                else:\n                    self._embeddings[name] = vector.to(device, non_blocking=True)\n\n    def clear_embeddings(self, embedding_names: List[str] = None):\n        if embedding_names is None:\n            self._embeddings: Dict = {}\n        else:\n            for name in embedding_names:\n                if name in self._embeddings.keys():\n                    del self._embeddings[name]\n\n    def get_each_embedding(self, embedding_names: Optional[List[str]] = None) -> torch.tensor:\n        embeddings = []\n        for embed in sorted(self._embeddings.keys()):\n            if embedding_names and embed not in embedding_names: continue\n            embed = self._embeddings[embed].to(flair.device)\n            if (flair.embedding_storage_mode == ""cpu"") and embed.device != flair.device:\n                embed = embed.to(flair.device)\n            embeddings.append(embed)\n        return embeddings\n\n    def get_embedding(self, names: Optional[List[str]] = None) -> torch.tensor:\n        embeddings = self.get_each_embedding(names)\n\n        if embeddings:\n            return torch.cat(embeddings, dim=0)\n\n        return torch.tensor([], device=flair.device)\n\n    @property\n    def start_position(self) -> int:\n        return self.start_pos\n\n    @property\n    def end_position(self) -> int:\n        return self.end_pos\n\n    @property\n    def embedding(self):\n        return self.get_embedding()\n\n    def __str__(self) -> str:\n        return (\n            ""Token: {} {}"".format(self.idx, self.text)\n            if self.idx is not None\n            else ""Token: {}"".format(self.text)\n        )\n\n    def __repr__(self) -> str:\n        return (\n            ""Token: {} {}"".format(self.idx, self.text)\n            if self.idx is not None\n            else ""Token: {}"".format(self.text)\n        )\n\n\nclass Span(DataPoint):\n    """"""\n    This class represents one textual span consisting of Tokens.\n    """"""\n\n    def __init__(self, tokens: List[Token]):\n\n        super().__init__()\n\n        self.tokens = tokens\n        self.start_pos = None\n        self.end_pos = None\n\n        if tokens:\n            self.start_pos = tokens[0].start_position\n            self.end_pos = tokens[len(tokens) - 1].end_position\n\n    @property\n    def text(self) -> str:\n        return "" "".join([t.text for t in self.tokens])\n\n    def to_original_text(self) -> str:\n        pos = self.tokens[0].start_pos\n        if pos is None:\n            return "" "".join([t.text for t in self.tokens])\n        str = """"\n        for t in self.tokens:\n            while t.start_pos != pos:\n                str += "" ""\n                pos += 1\n\n            str += t.text\n            pos += len(t.text)\n\n        return str\n\n    def to_dict(self):\n        return {\n            ""text"": self.to_original_text(),\n            ""start_pos"": self.start_pos,\n            ""end_pos"": self.end_pos,\n            ""labels"": self.labels,\n        }\n\n    def __str__(self) -> str:\n        ids = "","".join([str(t.idx) for t in self.tokens])\n        label_string = "" "".join([str(label) for label in self.labels])\n        labels = f\'   [\xe2\x88\x92 Labels: {label_string}]\' if self.labels is not None else """"\n        return (\n            \'Span [{}]: ""{}""{}\'.format(ids, self.text, labels)\n        )\n\n    def __repr__(self) -> str:\n        ids = "","".join([str(t.idx) for t in self.tokens])\n        return (\n            \'<{}-span ({}): ""{}"">\'.format(self.tag, ids, self.text)\n            if self.tag is not None\n            else \'<span ({}): ""{}"">\'.format(ids, self.text)\n        )\n\n    def __getitem__(self, idx: int) -> Token:\n        return self.tokens[idx]\n\n    def __iter__(self):\n        return iter(self.tokens)\n\n    def __len__(self) -> int:\n        return len(self.tokens)\n\n    @property\n    def tag(self):\n        return self.labels[0].value\n\n    @property\n    def score(self):\n        return self.labels[0].score\n\n\nclass Sentence(DataPoint):\n    """"""\n       A Sentence is a list of Tokens and is used to represent a sentence or text fragment.\n    """"""\n\n    def __init__(\n        self,\n        text: str = None,\n        use_tokenizer: Union[bool, Callable[[str], List[Token]]] = False,\n        language_code: str = None,\n    ):\n        """"""\n        Class to hold all meta related to a text (tokens, predictions, language code, ...)\n        :param text: original string\n        :param use_tokenizer: a custom tokenizer (default is space based tokenizer,\n        more advanced options are segtok_tokenizer to use segtok or build_spacy_tokenizer to use Spacy library\n        if available). Check the code of space_tokenizer to implement your own (if you need it).\n        If instead of providing a function, this parameter is just set to True, segtok will be used.\n        :param labels:\n        :param language_code:\n        """"""\n        super().__init__()\n\n        self.tokens: List[Token] = []\n\n        self._embeddings: Dict = {}\n\n        self.language_code: str = language_code\n\n        tokenizer = use_tokenizer\n        if type(use_tokenizer) == bool:\n            tokenizer = segtok_tokenizer if use_tokenizer else space_tokenizer\n\n        # if text is passed, instantiate sentence with tokens (words)\n        if text is not None:\n            text = self._restore_windows_1252_characters(text)\n            [self.add_token(token) for token in tokenizer(text)]\n\n        # log a warning if the dataset is empty\n        if text == """":\n            log.warning(\n                ""ACHTUNG: An empty Sentence was created! Are there empty strings in your dataset?""\n            )\n\n        self.tokenized = None\n\n    def get_token(self, token_id: int) -> Token:\n        for token in self.tokens:\n            if token.idx == token_id:\n                return token\n\n    def add_token(self, token: Union[Token, str]):\n\n        if type(token) is str:\n            token = Token(token)\n\n        token.text = token.text.replace(\'\\u200c\', \'\')\n        token.text = token.text.replace(\'\\u200b\', \'\')\n        token.text = token.text.replace(\'\\ufe0f\', \'\')\n        token.text = token.text.replace(\'\\ufeff\', \'\')\n\n        # data with zero-width characters cannot be handled\n        if token.text.strip() == \'\':\n            return\n\n        self.tokens.append(token)\n\n        # set token idx if not set\n        token.sentence = self\n        if token.idx is None:\n            token.idx = len(self.tokens)\n\n    def get_label_names(self):\n        label_names = []\n        for label in self.labels:\n            label_names.append(label.value)\n        return label_names\n\n    def get_spans(self, label_type: str, min_score=-1) -> List[Span]:\n\n        spans: List[Span] = []\n\n        current_span = []\n\n        tags = defaultdict(lambda: 0.0)\n\n        previous_tag_value: str = ""O""\n        for token in self:\n\n            tag: Label = token.get_tag(label_type)\n            tag_value = tag.value\n\n            # non-set tags are OUT tags\n            if tag_value == """" or tag_value == ""O"":\n                tag_value = ""O-""\n\n            # anything that is not a BIOES tag is a SINGLE tag\n            if tag_value[0:2] not in [""B-"", ""I-"", ""O-"", ""E-"", ""S-""]:\n                tag_value = ""S-"" + tag_value\n\n            # anything that is not OUT is IN\n            in_span = False\n            if tag_value[0:2] not in [""O-""]:\n                in_span = True\n\n            # single and begin tags start a new span\n            starts_new_span = False\n            if tag_value[0:2] in [""B-"", ""S-""]:\n                starts_new_span = True\n\n            if (\n                    previous_tag_value[0:2] in [""S-""]\n                    and previous_tag_value[2:] != tag_value[2:]\n                    and in_span\n            ):\n                starts_new_span = True\n\n            if (starts_new_span or not in_span) and len(current_span) > 0:\n                scores = [t.get_labels(label_type)[0].score for t in current_span]\n                span_score = sum(scores) / len(scores)\n                if span_score > min_score:\n                    span = Span(current_span)\n                    span.add_label(\n                        label_type=label_type,\n                        value=sorted(tags.items(), key=lambda k_v: k_v[1], reverse=True)[0][0],\n                        score=span_score)\n                    spans.append(span)\n\n                current_span = []\n                tags = defaultdict(lambda: 0.0)\n\n            if in_span:\n                current_span.append(token)\n                weight = 1.1 if starts_new_span else 1.0\n                tags[tag_value[2:]] += weight\n\n            # remember previous tag\n            previous_tag_value = tag_value\n\n        if len(current_span) > 0:\n            scores = [t.get_labels(label_type)[0].score for t in current_span]\n            span_score = sum(scores) / len(scores)\n            if span_score > min_score:\n                span = Span(current_span)\n                span.add_label(\n                    label_type=label_type,\n                    value=sorted(tags.items(), key=lambda k_v: k_v[1], reverse=True)[0][0],\n                    score=span_score)\n                spans.append(span)\n\n        return spans\n\n    @property\n    def embedding(self):\n        return self.get_embedding()\n\n    def set_embedding(self, name: str, vector: torch.tensor):\n        device = flair.device\n        if (flair.embedding_storage_mode == ""cpu"") and len(self._embeddings.keys()) > 0:\n            device = next(iter(self._embeddings.values())).device\n        if device != vector.device:\n            vector = vector.to(device)\n        self._embeddings[name] = vector\n\n    def get_embedding(self, names: Optional[List[str]] = None) -> torch.tensor:\n        embeddings = []\n        for embed in sorted(self._embeddings.keys()):\n            if names and embed not in names: continue\n            embedding = self._embeddings[embed]\n            embeddings.append(embedding)\n\n        if embeddings:\n            return torch.cat(embeddings, dim=0)\n\n        return torch.Tensor()\n\n    def to(self, device: str, pin_memory: bool = False):\n\n        # move sentence embeddings to device\n        for name, vector in self._embeddings.items():\n            if str(vector.device) != str(device):\n                if pin_memory:\n                    self._embeddings[name] = vector.to(\n                        device, non_blocking=True\n                    ).pin_memory()\n                else:\n                    self._embeddings[name] = vector.to(device, non_blocking=True)\n\n        # move token embeddings to device\n        for token in self:\n            token.to(device, pin_memory)\n\n    def clear_embeddings(self, embedding_names: List[str] = None):\n\n        # clear sentence embeddings\n        if embedding_names is None:\n            self._embeddings: Dict = {}\n        else:\n            for name in embedding_names:\n                if name in self._embeddings.keys():\n                    del self._embeddings[name]\n\n        # clear token embeddings\n        for token in self:\n            token.clear_embeddings(embedding_names)\n\n    def to_tagged_string(self, main_tag=None) -> str:\n        list = []\n        for token in self.tokens:\n            list.append(token.text)\n\n            tags: List[str] = []\n            for label_type in token.annotation_layers.keys():\n\n                if main_tag is not None and main_tag != label_type:\n                    continue\n\n                if token.get_labels(label_type)[0].value == ""O"":\n                    continue\n\n                tags.append(token.get_labels(label_type)[0].value)\n            all_tags = ""<"" + ""/"".join(tags) + "">""\n            if all_tags != ""<>"":\n                list.append(all_tags)\n        return "" "".join(list)\n\n    def to_tokenized_string(self) -> str:\n\n        if self.tokenized is None:\n            self.tokenized = "" "".join([t.text for t in self.tokens])\n\n        return self.tokenized\n\n    def to_plain_string(self):\n        plain = """"\n        for token in self.tokens:\n            plain += token.text\n            if token.whitespace_after:\n                plain += "" ""\n        return plain.rstrip()\n\n    def convert_tag_scheme(self, tag_type: str = ""ner"", target_scheme: str = ""iob""):\n\n        tags: List[Label] = []\n        for token in self.tokens:\n            tags.append(token.get_tag(tag_type))\n\n        if target_scheme == ""iob"":\n            iob2(tags)\n\n        if target_scheme == ""iobes"":\n            iob2(tags)\n            tags = iob_iobes(tags)\n\n        for index, tag in enumerate(tags):\n            self.tokens[index].set_label(tag_type, tag)\n\n    def infer_space_after(self):\n        """"""\n        Heuristics in case you wish to infer whitespace_after values for tokenized text. This is useful for some old NLP\n        tasks (such as CoNLL-03 and CoNLL-2000) that provide only tokenized data with no info of original whitespacing.\n        :return:\n        """"""\n        last_token = None\n        quote_count: int = 0\n        # infer whitespace after field\n\n        for token in self.tokens:\n            if token.text == \'""\':\n                quote_count += 1\n                if quote_count % 2 != 0:\n                    token.whitespace_after = False\n                elif last_token is not None:\n                    last_token.whitespace_after = False\n\n            if last_token is not None:\n\n                if token.text in [""."", "":"", "","", "";"", "")"", ""n\'t"", ""!"", ""?""]:\n                    last_token.whitespace_after = False\n\n                if token.text.startswith(""\'""):\n                    last_token.whitespace_after = False\n\n            if token.text in [""(""]:\n                token.whitespace_after = False\n\n            last_token = token\n        return self\n\n    def to_original_text(self) -> str:\n        if len(self.tokens) > 0 and (self.tokens[0].start_pos is None):\n            return "" "".join([t.text for t in self.tokens])\n        str = """"\n        pos = 0\n        for t in self.tokens:\n            while t.start_pos > pos:\n                str += "" ""\n                pos += 1\n\n            str += t.text\n            pos += len(t.text)\n\n        return str\n\n    def to_dict(self, tag_type: str = None):\n        labels = []\n        entities = []\n\n        if tag_type:\n            entities = [span.to_dict() for span in self.get_spans(tag_type)]\n        if self.labels:\n            labels = [l.to_dict() for l in self.labels]\n\n        return {""text"": self.to_original_text(), ""labels"": labels, ""entities"": entities}\n\n    def __getitem__(self, idx: int) -> Token:\n        return self.tokens[idx]\n\n    def __iter__(self):\n        return iter(self.tokens)\n\n    def __len__(self) -> int:\n        return len(self.tokens)\n\n    def __repr__(self):\n        tagged_string = self.to_tagged_string()\n        tokenized_string = self.to_tokenized_string()\n\n        # add Sentence labels to output if they exist\n        sentence_labels = f""  \xe2\x88\x92 Sentence-Labels: {self.annotation_layers}"" if self.annotation_layers != {} else """"\n\n        # add Token labels to output if they exist\n        token_labels = f\'  \xe2\x88\x92 Token-Labels: ""{tagged_string}""\' if tokenized_string != tagged_string else """"\n\n        return f\'Sentence: ""{tokenized_string}""   [\xe2\x88\x92 Tokens: {len(self)}{token_labels}{sentence_labels}]\'\n\n    def __copy__(self):\n        s = Sentence()\n        for token in self.tokens:\n            nt = Token(token.text)\n            for tag_type in token.tags:\n                nt.add_label(\n                    tag_type,\n                    token.get_tag(tag_type).value,\n                    token.get_tag(tag_type).score,\n                )\n\n            s.add_token(nt)\n        return s\n\n    def __str__(self) -> str:\n\n        tagged_string = self.to_tagged_string()\n        tokenized_string = self.to_tokenized_string()\n\n        # add Sentence labels to output if they exist\n        sentence_labels = f""  \xe2\x88\x92 Sentence-Labels: {self.annotation_layers}"" if self.annotation_layers != {} else """"\n\n        # add Token labels to output if they exist\n        token_labels = f\'  \xe2\x88\x92 Token-Labels: ""{tagged_string}""\' if tokenized_string != tagged_string else """"\n\n        return f\'Sentence: ""{tokenized_string}""   [\xe2\x88\x92 Tokens: {len(self)}{token_labels}{sentence_labels}]\'\n\n    def get_language_code(self) -> str:\n        if self.language_code is None:\n            import langdetect\n\n            try:\n                self.language_code = langdetect.detect(self.to_plain_string())\n            except:\n                self.language_code = ""en""\n\n        return self.language_code\n\n    @staticmethod\n    def _restore_windows_1252_characters(text: str) -> str:\n        def to_windows_1252(match):\n            try:\n                return bytes([ord(match.group(0))]).decode(""windows-1252"")\n            except UnicodeDecodeError:\n                # No character at the corresponding code point: remove it\n                return """"\n\n        return re.sub(r""[\\u0080-\\u0099]"", to_windows_1252, text)\n\n\nclass Image(DataPoint):\n\n    def __init__(self, data=None, imageURL=None):\n        super().__init__()\n\n        self.data = data\n        self._embeddings: Dict = {}\n        self.imageURL = imageURL\n\n    @property\n    def embedding(self):\n        return self.get_embedding()\n\n    def __str__(self):\n\n        image_repr = self.data.size() if self.data else """"\n        image_url = self.imageURL if self.imageURL else """"\n\n        return f""Image: {image_repr} {image_url}""\n\n    def get_embedding(self) -> torch.tensor:\n        embeddings = [\n            self._embeddings[embed] for embed in sorted(self._embeddings.keys())\n        ]\n\n        if embeddings:\n            return torch.cat(embeddings, dim=0)\n\n        return torch.tensor([], device=flair.device)\n\n    def set_embedding(self, name: str, vector: torch.tensor):\n        device = flair.device\n        if (flair.embedding_storage_mode == ""cpu"") and len(self._embeddings.keys()) > 0:\n            device = next(iter(self._embeddings.values())).device\n        if device != vector.device:\n            vector = vector.to(device)\n        self._embeddings[name] = vector\n\n    def to(self, device: str, pin_memory: bool = False):\n        for name, vector in self._embeddings.items():\n            if str(vector.device) != str(device):\n                if pin_memory:\n                    self._embeddings[name] = vector.to(\n                        device, non_blocking=True\n                    ).pin_memory()\n                else:\n                    self._embeddings[name] = vector.to(device, non_blocking=True)\n\n    def clear_embeddings(self, embedding_names: List[str] = None):\n        if embedding_names is None:\n            self._embeddings: Dict = {}\n        else:\n            for name in embedding_names:\n                if name in self._embeddings.keys():\n                    del self._embeddings[name]\n\n\nclass FlairDataset(Dataset):\n    @abstractmethod\n    def is_in_memory(self) -> bool:\n        pass\n\n\nclass Corpus:\n    def __init__(\n            self,\n            train: FlairDataset,\n            dev: FlairDataset = None,\n            test: FlairDataset = None,\n            name: str = ""corpus"",\n    ):\n        # set name\n        self.name: str = name\n\n        # sample test data if none is provided\n        if test is None:\n            train_length = len(train)\n            test_size: int = round(train_length / 10)\n            splits = randomly_split_into_two_datasets(train, test_size)\n            test = splits[0]\n            train = splits[1]\n\n        # sample dev data if none is provided\n        if dev is None:\n            train_length = len(train)\n            dev_size: int = round(train_length / 10)\n            splits = randomly_split_into_two_datasets(train, dev_size)\n            dev = splits[0]\n            train = splits[1]\n\n        # set train dev and test data\n        self._train: FlairDataset = train\n        self._test: FlairDataset = test\n        self._dev: FlairDataset = dev\n\n    @property\n    def train(self) -> FlairDataset:\n        return self._train\n\n    @property\n    def dev(self) -> FlairDataset:\n        return self._dev\n\n    @property\n    def test(self) -> FlairDataset:\n        return self._test\n\n    def downsample(self, percentage: float = 0.1, downsample_train=True, downsample_dev=True, downsample_test=True):\n\n        if downsample_train:\n            self._train = self._downsample_to_proportion(self.train, percentage)\n\n        if downsample_dev:\n            self._dev = self._downsample_to_proportion(self.dev, percentage)\n\n        if downsample_test:\n            self._test = self._downsample_to_proportion(self.test, percentage)\n\n        return self\n\n    def filter_empty_sentences(self):\n        log.info(""Filtering empty sentences"")\n        self._train = Corpus._filter_empty_sentences(self._train)\n        self._test = Corpus._filter_empty_sentences(self._test)\n        self._dev = Corpus._filter_empty_sentences(self._dev)\n        log.info(self)\n\n    @staticmethod\n    def _filter_empty_sentences(dataset) -> Dataset:\n\n        # find out empty sentence indices\n        empty_sentence_indices = []\n        non_empty_sentence_indices = []\n        index = 0\n\n        from flair.datasets import DataLoader\n\n        for batch in DataLoader(dataset):\n            for sentence in batch:\n                if len(sentence) == 0:\n                    empty_sentence_indices.append(index)\n                else:\n                    non_empty_sentence_indices.append(index)\n                index += 1\n\n        # create subset of non-empty sentence indices\n        subset = Subset(dataset, non_empty_sentence_indices)\n\n        return subset\n\n    def make_vocab_dictionary(self, max_tokens=-1, min_freq=1) -> Dictionary:\n        """"""\n        Creates a dictionary of all tokens contained in the corpus.\n        By defining `max_tokens` you can set the maximum number of tokens that should be contained in the dictionary.\n        If there are more than `max_tokens` tokens in the corpus, the most frequent tokens are added first.\n        If `min_freq` is set the a value greater than 1 only tokens occurring more than `min_freq` times are considered\n        to be added to the dictionary.\n        :param max_tokens: the maximum number of tokens that should be added to the dictionary (-1 = take all tokens)\n        :param min_freq: a token needs to occur at least `min_freq` times to be added to the dictionary (-1 = there is no limitation)\n        :return: dictionary of tokens\n        """"""\n        tokens = self._get_most_common_tokens(max_tokens, min_freq)\n\n        vocab_dictionary: Dictionary = Dictionary()\n        for token in tokens:\n            vocab_dictionary.add_item(token)\n\n        return vocab_dictionary\n\n    def _get_most_common_tokens(self, max_tokens, min_freq) -> List[str]:\n        tokens_and_frequencies = Counter(self._get_all_tokens())\n        tokens_and_frequencies = tokens_and_frequencies.most_common()\n\n        tokens = []\n        for token, freq in tokens_and_frequencies:\n            if (min_freq != -1 and freq < min_freq) or (\n                    max_tokens != -1 and len(tokens) == max_tokens\n            ):\n                break\n            tokens.append(token)\n        return tokens\n\n    def _get_all_tokens(self) -> List[str]:\n        tokens = list(map((lambda s: s.tokens), self.train))\n        tokens = [token for sublist in tokens for token in sublist]\n        return list(map((lambda t: t.text), tokens))\n\n    @staticmethod\n    def _downsample_to_proportion(dataset: Dataset, proportion: float):\n\n        sampled_size: int = round(len(dataset) * proportion)\n        splits = randomly_split_into_two_datasets(dataset, sampled_size)\n        return splits[0]\n\n    def obtain_statistics(\n            self, label_type: str = None, pretty_print: bool = True\n    ) -> dict:\n        """"""\n        Print statistics about the class distribution (only labels of sentences are taken into account) and sentence\n        sizes.\n        """"""\n        json_string = {\n            ""TRAIN"": self._obtain_statistics_for(self.train, ""TRAIN"", label_type),\n            ""TEST"": self._obtain_statistics_for(self.test, ""TEST"", label_type),\n            ""DEV"": self._obtain_statistics_for(self.dev, ""DEV"", label_type),\n        }\n        if pretty_print:\n            import json\n\n            json_string = json.dumps(json_string, indent=4)\n        return json_string\n\n    @staticmethod\n    def _obtain_statistics_for(sentences, name, tag_type) -> dict:\n        if len(sentences) == 0:\n            return {}\n\n        classes_to_count = Corpus._count_sentence_labels(sentences)\n        tags_to_count = Corpus._count_token_labels(sentences, tag_type)\n        tokens_per_sentence = Corpus._get_tokens_per_sentence(sentences)\n\n        label_size_dict = {}\n        for l, c in classes_to_count.items():\n            label_size_dict[l] = c\n\n        tag_size_dict = {}\n        for l, c in tags_to_count.items():\n            tag_size_dict[l] = c\n\n        return {\n            ""dataset"": name,\n            ""total_number_of_documents"": len(sentences),\n            ""number_of_documents_per_class"": label_size_dict,\n            ""number_of_tokens_per_tag"": tag_size_dict,\n            ""number_of_tokens"": {\n                ""total"": sum(tokens_per_sentence),\n                ""min"": min(tokens_per_sentence),\n                ""max"": max(tokens_per_sentence),\n                ""avg"": sum(tokens_per_sentence) / len(sentences),\n            },\n        }\n\n    @staticmethod\n    def _get_tokens_per_sentence(sentences):\n        return list(map(lambda x: len(x.tokens), sentences))\n\n    @staticmethod\n    def _count_sentence_labels(sentences):\n        label_count = defaultdict(lambda: 0)\n        for sent in sentences:\n            for label in sent.labels:\n                label_count[label.value] += 1\n        return label_count\n\n    @staticmethod\n    def _count_token_labels(sentences, label_type):\n        label_count = defaultdict(lambda: 0)\n        for sent in sentences:\n            for token in sent.tokens:\n                if label_type in token.annotation_layers.keys():\n                    label = token.get_tag(label_type)\n                    label_count[label.value] += 1\n        return label_count\n\n    def __str__(self) -> str:\n        return ""Corpus: %d train + %d dev + %d test sentences"" % (\n            len(self.train),\n            len(self.dev),\n            len(self.test),\n        )\n\n    def make_label_dictionary(self, label_type: str = None) -> Dictionary:\n        """"""\n        Creates a dictionary of all labels assigned to the sentences in the corpus.\n        :return: dictionary of labels\n        """"""\n        label_dictionary: Dictionary = Dictionary(add_unk=False)\n        label_dictionary.multi_label = False\n\n        from flair.datasets import DataLoader\n\n        data = ConcatDataset([self.train, self.test])\n        loader = DataLoader(data, batch_size=1)\n\n        log.info(""Computing label dictionary. Progress:"")\n        for batch in Tqdm.tqdm(iter(loader)):\n\n            for sentence in batch:\n\n                # check if sentence itself has labels\n                labels = sentence.get_labels(label_type) if label_type is not None else sentence.labels\n\n                for label in labels:\n                    label_dictionary.add_item(label.value)\n\n                # check for labels of words\n                if isinstance(sentence, Sentence):\n                    for token in sentence.tokens:\n                        for label in token.get_labels(label_type):\n                            label_dictionary.add_item(label.value)\n\n                if not label_dictionary.multi_label:\n                    if len(labels) > 1:\n                        label_dictionary.multi_label = True\n\n        log.info(label_dictionary.idx2item)\n\n        return label_dictionary\n\n    def get_label_distribution(self):\n        class_to_count = defaultdict(lambda: 0)\n        for sent in self.train:\n            for label in sent.labels:\n                class_to_count[label.value] += 1\n        return class_to_count\n\n    def get_all_sentences(self) -> Dataset:\n        return ConcatDataset([self.train, self.dev, self.test])\n\n    def make_tag_dictionary(self, tag_type: str) -> Dictionary:\n\n        # Make the tag dictionary\n        tag_dictionary: Dictionary = Dictionary()\n        tag_dictionary.add_item(""O"")\n        for sentence in self.get_all_sentences():\n            for token in sentence.tokens:\n                tag_dictionary.add_item(token.get_tag(tag_type).value)\n        tag_dictionary.add_item(""<START>"")\n        tag_dictionary.add_item(""<STOP>"")\n        return tag_dictionary\n\n\nclass MultiCorpus(Corpus):\n    def __init__(self, corpora: List[Corpus], name: str = ""multicorpus""):\n        self.corpora: List[Corpus] = corpora\n\n        super(MultiCorpus, self).__init__(\n            ConcatDataset([corpus.train for corpus in self.corpora]),\n            ConcatDataset([corpus.dev for corpus in self.corpora]),\n            ConcatDataset([corpus.test for corpus in self.corpora]),\n            name=name,\n        )\n\n    def __str__(self):\n        return ""\\n"".join([str(corpus) for corpus in self.corpora])\n\n\ndef iob2(tags):\n    """"""\n    Check that tags have a valid IOB format.\n    Tags in IOB1 format are converted to IOB2.\n    """"""\n    for i, tag in enumerate(tags):\n        if tag.value == ""O"":\n            continue\n        split = tag.value.split(""-"")\n        if len(split) != 2 or split[0] not in [""I"", ""B""]:\n            return False\n        if split[0] == ""B"":\n            continue\n        elif i == 0 or tags[i - 1].value == ""O"":  # conversion IOB1 to IOB2\n            tags[i].value = ""B"" + tag.value[1:]\n        elif tags[i - 1].value[1:] == tag.value[1:]:\n            continue\n        else:  # conversion IOB1 to IOB2\n            tags[i].value = ""B"" + tag.value[1:]\n    return True\n\n\ndef iob_iobes(tags):\n    """"""\n    IOB -> IOBES\n    """"""\n    new_tags = []\n    for i, tag in enumerate(tags):\n        if tag.value == ""O"":\n            new_tags.append(tag.value)\n        elif tag.value.split(""-"")[0] == ""B"":\n            if i + 1 != len(tags) and tags[i + 1].value.split(""-"")[0] == ""I"":\n                new_tags.append(tag.value)\n            else:\n                new_tags.append(tag.value.replace(""B-"", ""S-""))\n        elif tag.value.split(""-"")[0] == ""I"":\n            if i + 1 < len(tags) and tags[i + 1].value.split(""-"")[0] == ""I"":\n                new_tags.append(tag.value)\n            else:\n                new_tags.append(tag.value.replace(""I-"", ""E-""))\n        else:\n            raise Exception(""Invalid IOB format!"")\n    return new_tags\n\n\ndef space_tokenizer(text: str) -> List[Token]:\n    """"""\n    Tokenizer based on space character only.\n    """"""\n    tokens: List[Token] = []\n    word = """"\n    index = -1\n    for index, char in enumerate(text):\n        if char == "" "":\n            if len(word) > 0:\n                start_position = index - len(word)\n                tokens.append(\n                    Token(\n                        text=word, start_position=start_position, whitespace_after=True\n                    )\n                )\n\n            word = """"\n        else:\n            word += char\n    # increment for last token in sentence if not followed by whitespace\n    index += 1\n    if len(word) > 0:\n        start_position = index - len(word)\n        tokens.append(\n            Token(text=word, start_position=start_position, whitespace_after=False)\n        )\n    return tokens\n\n\ndef build_japanese_tokenizer(tokenizer: str = ""MeCab""):\n    if tokenizer.lower() != ""mecab"":\n        raise NotImplementedError(""Currently, MeCab is only supported."")\n\n    try:\n        import konoha\n    except ModuleNotFoundError:\n        log.warning(""-"" * 100)\n        log.warning(\'ATTENTION! The library ""konoha"" is not installed!\')\n        log.warning(\n            \'To use Japanese tokenizer, please first install with the following steps:\'\n        )\n        log.warning(\n            \'- Install mecab with ""sudo apt install mecab libmecab-dev mecab-ipadic""\'\n        )\n        log.warning(\'- Install konoha with ""pip install konoha[mecab]""\')\n        log.warning(""-"" * 100)\n        pass\n\n    sentence_tokenizer = konoha.SentenceTokenizer()\n    word_tokenizer = konoha.WordTokenizer(tokenizer)\n\n    def tokenizer(text: str) -> List[Token]:\n        """"""\n        Tokenizer using konoha, a third party library which supports\n        multiple Japanese tokenizer such as MeCab, KyTea and SudachiPy.\n        """"""\n        tokens: List[Token] = []\n        words: List[str] = []\n\n        sentences = sentence_tokenizer.tokenize(text)\n        for sentence in sentences:\n            konoha_tokens = word_tokenizer.tokenize(sentence)\n            words.extend(list(map(str, konoha_tokens)))\n\n        # determine offsets for whitespace_after field\n        index = text.index\n        current_offset = 0\n        previous_word_offset = -1\n        previous_token = None\n        for word in words:\n            try:\n                word_offset = index(word, current_offset)\n                start_position = word_offset\n            except:\n                word_offset = previous_word_offset + 1\n                start_position = (\n                    current_offset + 1 if current_offset > 0 else current_offset\n                )\n\n            token = Token(\n                text=word, start_position=start_position, whitespace_after=True\n            )\n            tokens.append(token)\n\n            if (previous_token is not None) and word_offset - 1 == previous_word_offset:\n                previous_token.whitespace_after = False\n\n            current_offset = word_offset + len(word)\n            previous_word_offset = current_offset - 1\n            previous_token = token\n\n        return tokens\n\n    return tokenizer\n\n\ndef segtok_tokenizer(text: str) -> List[Token]:\n    """"""\n    Tokenizer using segtok, a third party library dedicated to rules-based Indo-European languages.\n    https://github.com/fnl/segtok\n    """"""\n    tokens: List[Token] = []\n\n    words: List[str] = []\n    sentences = split_single(text)\n    for sentence in sentences:\n        contractions = split_contractions(word_tokenizer(sentence))\n        words.extend(contractions)\n\n    words = list(filter(None, words))\n\n    # determine offsets for whitespace_after field\n    index = text.index\n    current_offset = 0\n    previous_word_offset = -1\n    previous_token = None\n    for word in words:\n        try:\n            word_offset = index(word, current_offset)\n            start_position = word_offset\n        except:\n            word_offset = previous_word_offset + 1\n            start_position = (\n                current_offset + 1 if current_offset > 0 else current_offset\n            )\n\n        if word:\n            token = Token(\n                text=word, start_position=start_position, whitespace_after=True\n            )\n            tokens.append(token)\n\n        if (previous_token is not None) and word_offset - 1 == previous_word_offset:\n            previous_token.whitespace_after = False\n\n        current_offset = word_offset + len(word)\n        previous_word_offset = current_offset - 1\n        previous_token = token\n\n    return tokens\n\n\ndef build_spacy_tokenizer(model) -> Callable[[str], List[Token]]:\n    """"""\n    Wrap Spacy model to build a tokenizer for the Sentence class.\n    :param model a Spacy V2 model\n    :return a tokenizer function to provide to Sentence class constructor\n    """"""\n    try:\n        from spacy.language import Language\n        from spacy.tokens.doc import Doc\n        from spacy.tokens.token import Token as SpacyToken\n    except ImportError:\n        raise ImportError(\n            ""Please install Spacy v2.0 or better before using the Spacy tokenizer, otherwise you can use segtok_tokenizer as advanced tokenizer.""\n        )\n\n    model: Language = model\n\n    def tokenizer(text: str) -> List[Token]:\n        doc: Doc = model.make_doc(text)\n        previous_token = None\n        tokens: List[Token] = []\n        for word in doc:\n            word: SpacyToken = word\n            token = Token(\n                text=word.text, start_position=word.idx, whitespace_after=True\n            )\n            tokens.append(token)\n\n            if (previous_token is not None) and (\n                    token.start_pos - 1\n                    == previous_token.start_pos + len(previous_token.text)\n            ):\n                previous_token.whitespace_after = False\n\n            previous_token = token\n        return tokens\n\n    return tokenizer\n\n\ndef randomly_split_into_two_datasets(dataset, length_of_first):\n\n    import random\n    indices = [i for i in range(len(dataset))]\n    random.shuffle(indices)\n\n    first_dataset = indices[:length_of_first]\n    second_dataset = indices[length_of_first:]\n    first_dataset.sort()\n    second_dataset.sort()\n\n    return [Subset(dataset, first_dataset), Subset(dataset, second_dataset)]\n'"
flair/data_fetcher.py,0,"b'import os\nfrom typing import List, Dict, Union, Callable\nimport re\nimport logging\nfrom enum import Enum\nfrom pathlib import Path\n\nfrom deprecated import deprecated\n\nimport flair\nfrom flair.data import (\n    Sentence,\n    Corpus,\n    Token,\n    MultiCorpus,\n    space_tokenizer,\n    segtok_tokenizer,\n)\nfrom flair.file_utils import cached_path\n\nlog = logging.getLogger(""flair"")\n\n\nclass NLPTask(Enum):\n    # conll 2000 column format\n    CONLL_2000 = ""conll_2000""\n\n    # conll 03 NER column format\n    CONLL_03 = ""conll_03""\n    CONLL_03_GERMAN = ""conll_03_german""\n    CONLL_03_DUTCH = ""conll_03_dutch""\n    CONLL_03_SPANISH = ""conll_03_spanish""\n\n    # WNUT-17\n    WNUT_17 = ""wnut_17""\n\n    # -- WikiNER datasets\n    WIKINER_ENGLISH = ""wikiner_english""\n    WIKINER_GERMAN = ""wikiner_german""\n    WIKINER_FRENCH = ""wikiner_french""\n    WIKINER_SPANISH = ""wikiner_spanish""\n    WIKINER_ITALIAN = ""wikiner_italian""\n    WIKINER_DUTCH = ""wikiner_dutch""\n    WIKINER_POLISH = ""wikiner_polish""\n    WIKINER_PORTUGUESE = ""wikiner_portuguese""\n    WIKINER_RUSSIAN = ""wikiner_russian""\n\n    # -- Universal Dependencies\n    # Germanic\n    UD_ENGLISH = ""ud_english""\n    UD_GERMAN = ""ud_german""\n    UD_DUTCH = ""ud_dutch""\n    # Romance\n    UD_FRENCH = ""ud_french""\n    UD_ITALIAN = ""ud_italian""\n    UD_SPANISH = ""ud_spanish""\n    UD_PORTUGUESE = ""ud_portuguese""\n    UD_ROMANIAN = ""ud_romanian""\n    UD_CATALAN = ""ud_catalan""\n    # West-Slavic\n    UD_POLISH = ""ud_polish""\n    UD_CZECH = ""ud_czech""\n    UD_SLOVAK = ""ud_slovak""\n    # South-Slavic\n    UD_SLOVENIAN = ""ud_slovenian""\n    UD_CROATIAN = ""ud_croatian""\n    UD_SERBIAN = ""ud_serbian""\n    UD_BULGARIAN = ""ud_bulgarian""\n    # East-Slavic\n    UD_RUSSIAN = ""ud_russian""\n    # Scandinavian\n    UD_SWEDISH = ""ud_swedish""\n    UD_DANISH = ""ud_danish""\n    UD_NORWEGIAN = ""ud_norwegian""\n    UD_FINNISH = ""ud_finnish""\n    # Asian\n    UD_ARABIC = ""ud_arabic""\n    UD_HEBREW = ""ud_hebrew""\n    UD_TURKISH = ""ud_turkish""\n    UD_PERSIAN = ""ud_persian""\n    UD_HINDI = ""ud_hindi""\n    UD_INDONESIAN = ""ud_indonesian""\n    UD_JAPANESE = ""ud_japanese""\n    UD_CHINESE = ""ud_chinese""\n    UD_KOREAN = ""ud_korean""\n\n    # Language isolates\n    UD_BASQUE = ""ud_basque""\n\n    # recent Universal Dependencies\n    UD_GERMAN_HDT = ""ud_german_hdt""\n\n    # other datasets\n    ONTONER = ""ontoner""\n    FASHION = ""fashion""\n    GERMEVAL = ""germeval""\n    SRL = ""srl""\n    WSD = ""wsd""\n    CONLL_12 = ""conll_12""\n    PENN = ""penn""\n    ONTONOTES = ""ontonotes""\n    NER_BASQUE = ""eiec""\n\n    # text classification format\n    IMDB = ""imdb""\n    AG_NEWS = ""ag_news""\n    TREC_6 = ""trec-6""\n    TREC_50 = ""trec-50""\n\n    # text regression format\n    REGRESSION = ""regression""\n    WASSA_ANGER = ""wassa-anger""\n    WASSA_FEAR = ""wassa-fear""\n    WASSA_JOY = ""wassa-joy""\n    WASSA_SADNESS = ""wassa-sadness""\n\n\nclass NLPTaskDataFetcher:\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def load_corpora(\n        tasks: List[Union[NLPTask, str]], base_path: Union[str, Path] = None\n    ) -> MultiCorpus:\n        return MultiCorpus(\n            [NLPTaskDataFetcher.load_corpus(task, Path(base_path)) for task in tasks]\n        )\n\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def load_corpus(task: Union[NLPTask, str], base_path: Union[str, Path] = None) -> Corpus:\n        """"""\n        Helper function to fetch a Corpus for a specific NLPTask. For this to work you need to first download\n        and put into the appropriate folder structure the corresponding NLP task data. The tutorials on\n        https://github.com/zalandoresearch/flair give more info on how to do this. Alternatively, you can use this\n        code to create your own data fetchers.\n        :param task: specification of the NLPTask you wish to get\n        :param base_path: path to data folder containing tasks sub folders\n        :return: a Corpus consisting of train, dev and test data\n        """"""\n\n        # first, try to fetch dataset online\n        if type(task) is NLPTask:\n            NLPTaskDataFetcher.download_dataset(task)\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # get string value if enum is passed\n        task = task.value if type(task) is NLPTask else task\n\n        data_folder = base_path / task.lower()\n\n        # the CoNLL 2000 task on chunking has three columns: text, pos and np (chunk)\n        if task == NLPTask.CONLL_2000.value:\n            columns = {0: ""text"", 1: ""pos"", 2: ""np""}\n\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder, columns, tag_to_biloes=""np""\n            )\n\n        # many NER tasks follow the CoNLL 03 format with four colulms: text, pos, np and ner tag\n        if (\n            task == NLPTask.CONLL_03.value\n            or task == NLPTask.ONTONER.value\n            or task == NLPTask.FASHION.value\n        ):\n            columns = {0: ""text"", 1: ""pos"", 2: ""np"", 3: ""ner""}\n\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder, columns, tag_to_biloes=""ner""\n            )\n\n        # the CoNLL 03 task for German has an additional lemma column\n        if task == NLPTask.CONLL_03_GERMAN.value:\n            columns = {0: ""text"", 1: ""lemma"", 2: ""pos"", 3: ""np"", 4: ""ner""}\n\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder, columns, tag_to_biloes=""ner""\n            )\n\n        # the CoNLL 03 task for Dutch has no NP column\n        if task == NLPTask.CONLL_03_DUTCH.value or task.startswith(""wikiner""):\n            columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder, columns, tag_to_biloes=""ner""\n            )\n\n        # the CoNLL 03 task for Spanish only has two columns\n        if task == NLPTask.CONLL_03_SPANISH.value or task == NLPTask.WNUT_17.value:\n            columns = {0: ""text"", 1: ""ner""}\n\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder, columns, tag_to_biloes=""ner""\n            )\n\n        # the GERMEVAL task only has two columns: text and ner\n        if task == NLPTask.GERMEVAL.value:\n            columns = {1: ""text"", 2: ""ner""}\n\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder, columns, tag_to_biloes=""ner""\n            )\n\n        # WSD tasks may be put into this column format\n        if task == NLPTask.WSD.value:\n            columns = {0: ""text"", 1: ""lemma"", 2: ""pos"", 3: ""sense""}\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder,\n                columns,\n                train_file=""semcor.tsv"",\n                test_file=""semeval2015.tsv"",\n            )\n\n        # the UD corpora follow the CoNLL-U format, for which we have a special reader\n        if task.startswith(""ud_"") or task in [\n            NLPTask.ONTONOTES.value,\n            NLPTask.CONLL_12.value,\n            NLPTask.PENN.value,\n        ]:\n            return NLPTaskDataFetcher.load_ud_corpus(data_folder)\n\n        # for text classifiers, we use our own special format\n        if task in [\n            NLPTask.IMDB.value,\n            NLPTask.AG_NEWS.value,\n            NLPTask.TREC_6.value,\n            NLPTask.TREC_50.value,\n            NLPTask.REGRESSION.value,\n        ]:\n            tokenizer: Callable[[str], List[Token]] = space_tokenizer if task in [\n                NLPTask.TREC_6.value,\n                NLPTask.TREC_50.value,\n            ] else segtok_tokenizer\n\n            return NLPTaskDataFetcher.load_classification_corpus(\n                data_folder, tokenizer=tokenizer\n            )\n\n        # NER corpus for Basque\n        if task == NLPTask.NER_BASQUE.value:\n            columns = {0: ""text"", 1: ""ner""}\n            return NLPTaskDataFetcher.load_column_corpus(\n                data_folder, columns, tag_to_biloes=""ner""\n            )\n\n        if task.startswith(""wassa""):\n            return NLPTaskDataFetcher.load_classification_corpus(\n                data_folder, tokenizer=segtok_tokenizer\n            )\n\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def load_column_corpus(\n        data_folder: Union[str, Path],\n        column_format: Dict[int, str],\n        train_file=None,\n        test_file=None,\n        dev_file=None,\n        tag_to_biloes=None,\n    ) -> Corpus:\n        """"""\n        Helper function to get a Corpus from CoNLL column-formatted task data such as CoNLL03 or CoNLL2000.\n\n        :param data_folder: base folder with the task data\n        :param column_format: a map specifying the column format\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param tag_to_biloes: whether to convert to BILOES tagging scheme\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        if type(data_folder) == str:\n            data_folder: Path = Path(data_folder)\n\n        if train_file is not None:\n            train_file = data_folder / train_file\n        if test_file is not None:\n            test_file = data_folder / test_file\n        if dev_file is not None:\n            dev_file = data_folder / dev_file\n\n        # automatically identify train / test / dev files\n        if train_file is None:\n            for file in data_folder.iterdir():\n                file_name = file.name\n                if file_name.endswith("".gz""):\n                    continue\n                if ""train"" in file_name and not ""54019"" in file_name:\n                    train_file = file\n                if ""dev"" in file_name:\n                    dev_file = file\n                if ""testa"" in file_name:\n                    dev_file = file\n                if ""testb"" in file_name:\n                    test_file = file\n\n            # if no test file is found, take any file with \'test\' in name\n            if test_file is None:\n                for file in data_folder.iterdir():\n                    file_name = file.name\n                    if file_name.endswith("".gz""):\n                        continue\n                    if ""test"" in file_name:\n                        test_file = file\n\n        log.info(""Reading data from {}"".format(data_folder))\n        log.info(""Train: {}"".format(train_file))\n        log.info(""Dev: {}"".format(dev_file))\n        log.info(""Test: {}"".format(test_file))\n\n        # get train and test data\n        sentences_train: List[Sentence] = NLPTaskDataFetcher.read_column_data(\n            train_file, column_format\n        )\n\n        # read in test file if exists, otherwise sample 10% of train data as test dataset\n        if test_file is not None:\n            sentences_test: List[Sentence] = NLPTaskDataFetcher.read_column_data(\n                test_file, column_format\n            )\n        else:\n            sentences_test: List[Sentence] = [\n                sentences_train[i]\n                for i in NLPTaskDataFetcher.__sample(len(sentences_train), 0.1)\n            ]\n            sentences_train = [x for x in sentences_train if x not in sentences_test]\n\n        # read in dev file if exists, otherwise sample 10% of train data as dev dataset\n        if dev_file is not None:\n            sentences_dev: List[Sentence] = NLPTaskDataFetcher.read_column_data(\n                dev_file, column_format\n            )\n        else:\n            sentences_dev: List[Sentence] = [\n                sentences_train[i]\n                for i in NLPTaskDataFetcher.__sample(len(sentences_train), 0.1)\n            ]\n            sentences_train = [x for x in sentences_train if x not in sentences_dev]\n\n        if tag_to_biloes is not None:\n            # convert tag scheme to iobes\n            for sentence in sentences_train + sentences_test + sentences_dev:\n                sentence.convert_tag_scheme(\n                    tag_type=tag_to_biloes, target_scheme=""iobes""\n                )\n\n        return Corpus(\n            sentences_train, sentences_dev, sentences_test, name=data_folder.name\n        )\n\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def load_ud_corpus(\n        data_folder: Union[str, Path], train_file=None, test_file=None, dev_file=None\n    ) -> Corpus:\n        """"""\n        Helper function to get a Corpus from CoNLL-U column-formatted task data such as the UD corpora\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n        # automatically identify train / test / dev files\n        if train_file is None:\n            for file in data_folder.iterdir():\n                file_name = file.name\n                if ""train"" in file_name:\n                    train_file = file\n                if ""test"" in file_name:\n                    test_file = file\n                if ""dev"" in file_name:\n                    dev_file = file\n                if ""testa"" in file_name:\n                    dev_file = file\n                if ""testb"" in file_name:\n                    test_file = file\n\n        log.info(""Reading data from {}"".format(data_folder))\n        log.info(""Train: {}"".format(train_file))\n        log.info(""Dev: {}"".format(dev_file))\n        log.info(""Test: {}"".format(test_file))\n\n        sentences_train: List[Sentence] = NLPTaskDataFetcher.read_conll_ud(train_file)\n        sentences_test: List[Sentence] = NLPTaskDataFetcher.read_conll_ud(test_file)\n        sentences_dev: List[Sentence] = NLPTaskDataFetcher.read_conll_ud(dev_file)\n\n        return Corpus(\n            sentences_train, sentences_dev, sentences_test, name=data_folder.name\n        )\n\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def load_classification_corpus(\n        data_folder: Union[str, Path],\n        train_file=None,\n        test_file=None,\n        dev_file=None,\n        tokenizer: Callable[[str], List[Token]] = segtok_tokenizer,\n        max_tokens_per_doc=-1,\n    ) -> Corpus:\n        """"""\n        Helper function to get a Corpus from text classification-formatted task data\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        if type(data_folder) == str:\n            data_folder: Path = Path(data_folder)\n\n        if train_file is not None:\n            train_file = data_folder / train_file\n        if test_file is not None:\n            test_file = data_folder / test_file\n        if dev_file is not None:\n            dev_file = data_folder / dev_file\n\n        # automatically identify train / test / dev files\n        if train_file is None:\n            for file in data_folder.iterdir():\n                file_name = file.name\n                if ""train"" in file_name:\n                    train_file = file\n                if ""test"" in file_name:\n                    test_file = file\n                if ""dev"" in file_name:\n                    dev_file = file\n                if ""testa"" in file_name:\n                    dev_file = file\n                if ""testb"" in file_name:\n                    test_file = file\n\n        log.info(""Reading data from {}"".format(data_folder))\n        log.info(""Train: {}"".format(train_file))\n        log.info(""Dev: {}"".format(dev_file))\n        log.info(""Test: {}"".format(test_file))\n\n        sentences_train: List[\n            Sentence\n        ] = NLPTaskDataFetcher.read_text_classification_file(\n            train_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n        )\n        sentences_test: List[\n            Sentence\n        ] = NLPTaskDataFetcher.read_text_classification_file(\n            test_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n        )\n\n        if dev_file is not None:\n            sentences_dev: List[\n                Sentence\n            ] = NLPTaskDataFetcher.read_text_classification_file(\n                dev_file, tokenizer=tokenizer, max_tokens_per_doc=max_tokens_per_doc\n            )\n        else:\n            sentences_dev: List[Sentence] = [\n                sentences_train[i]\n                for i in NLPTaskDataFetcher.__sample(len(sentences_train), 0.1)\n            ]\n            sentences_train = [x for x in sentences_train if x not in sentences_dev]\n\n        return Corpus(sentences_train, sentences_dev, sentences_test)\n\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def read_text_classification_file(\n        path_to_file: Union[str, Path],\n        max_tokens_per_doc=-1,\n        tokenizer=segtok_tokenizer,\n    ) -> List[Sentence]:\n        """"""\n        Reads a data file for text classification. The file should contain one document/text per line.\n        The line should have the following format:\n        __label__<class_name> <text>\n        If you have a multi class task, you can have as many labels as you want at the beginning of the line, e.g.,\n        __label__<class_name_1> __label__<class_name_2> <text>\n        :param path_to_file: the path to the data file\n        :param max_tokens_per_doc: Takes at most this amount of tokens per document. If set to -1 all documents are taken as is.\n        :return: list of sentences\n        """"""\n        label_prefix = ""__label__""\n        sentences = []\n\n        with open(str(path_to_file), encoding=""utf-8"") as f:\n            for line in f:\n                words = line.split()\n\n                labels = []\n                l_len = 0\n\n                for i in range(len(words)):\n                    if words[i].startswith(label_prefix):\n                        l_len += len(words[i]) + 1\n                        label = words[i].replace(label_prefix, """")\n                        labels.append(label)\n                    else:\n                        break\n\n                text = line[l_len:].strip()\n\n                if text and labels:\n                    sentence = Sentence(text, labels=labels, use_tokenizer=tokenizer)\n                    if len(sentence) > max_tokens_per_doc and max_tokens_per_doc > 0:\n                        sentence.tokens = sentence.tokens[:max_tokens_per_doc]\n                    if len(sentence.tokens) > 0:\n                        sentences.append(sentence)\n\n        return sentences\n\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def read_column_data(\n        path_to_column_file: Union[str, Path],\n        column_name_map: Dict[int, str],\n        infer_whitespace_after: bool = True,\n    ):\n        """"""\n        Reads a file in column format and produces a list of Sentence with tokenlevel annotation as specified in the\n        column_name_map. For instance, by passing ""{0: \'text\', 1: \'pos\', 2: \'np\', 3: \'ner\'}"" as column_name_map you\n        specify that the first column is the text (lexical value) of the token, the second the PoS tag, the third\n        the chunk and the forth the NER tag.\n        :param path_to_column_file: the path to the column file\n        :param column_name_map: a map of column number to token annotation name\n        :param infer_whitespace_after: if True, tries to infer whitespace_after field for Token\n        :return: list of sentences\n        """"""\n        sentences: List[Sentence] = []\n\n        try:\n            lines: List[str] = open(\n                str(path_to_column_file), encoding=""utf-8""\n            ).read().strip().split(""\\n"")\n        except:\n            log.info(\n                \'UTF-8 can\\\'t read: {} ... using ""latin-1"" instead.\'.format(\n                    path_to_column_file\n                )\n            )\n            lines: List[str] = open(\n                str(path_to_column_file), encoding=""latin1""\n            ).read().strip().split(""\\n"")\n\n        # most data sets have the token text in the first column, if not, pass \'text\' as column\n        text_column: int = 0\n        for column in column_name_map:\n            if column_name_map[column] == ""text"":\n                text_column = column\n\n        sentence: Sentence = Sentence()\n        for line in lines:\n\n            if line.startswith(""#""):\n                continue\n\n            if line.strip().replace(""\xef\xbb\xbf"", """") == """":\n                if len(sentence) > 0:\n                    sentence.infer_space_after()\n                    sentences.append(sentence)\n                sentence: Sentence = Sentence()\n\n            else:\n                fields: List[str] = re.split(r""\\s+"", line)\n                token = Token(fields[text_column])\n                for column in column_name_map:\n                    if len(fields) > column:\n                        if column != text_column:\n                            token.add_tag(column_name_map[column], fields[column])\n\n                sentence.add_token(token)\n\n        if len(sentence.tokens) > 0:\n            sentence.infer_space_after()\n            sentences.append(sentence)\n\n        return sentences\n\n    @staticmethod\n    @deprecated(version=""0.4.1"", reason=""Use \'flair.datasets\' instead."")\n    def read_conll_ud(path_to_conll_file: Union[str, Path]) -> List[Sentence]:\n        """"""\n       Reads a file in CoNLL-U format and produces a list of Sentence with full morphosyntactic annotation\n       :param path_to_conll_file: the path to the conll-u file\n       :return: list of sentences\n       """"""\n        sentences: List[Sentence] = []\n\n        lines: List[str] = open(\n            path_to_conll_file, encoding=""utf-8""\n        ).read().strip().split(""\\n"")\n\n        sentence: Sentence = Sentence()\n        for line in lines:\n\n            fields: List[str] = re.split(""\\t+"", line)\n            if line == """":\n                if len(sentence) > 0:\n                    sentences.append(sentence)\n                sentence: Sentence = Sentence()\n\n            elif line.startswith(""#""):\n                continue\n            elif ""."" in fields[0]:\n                continue\n            elif ""-"" in fields[0]:\n                continue\n            else:\n                token = Token(fields[1], head_id=int(fields[6]))\n                token.add_tag(""lemma"", str(fields[2]))\n                token.add_tag(""upos"", str(fields[3]))\n                token.add_tag(""pos"", str(fields[4]))\n                token.add_tag(""dependency"", str(fields[7]))\n\n                for morph in str(fields[5]).split(""|""):\n                    if not ""="" in morph:\n                        continue\n                    token.add_tag(morph.split(""="")[0].lower(), morph.split(""="")[1])\n\n                if len(fields) > 10 and str(fields[10]) == ""Y"":\n                    token.add_tag(""frame"", str(fields[11]))\n\n                sentence.add_token(token)\n\n        if len(sentence.tokens) > 0:\n            sentences.append(sentence)\n\n        return sentences\n\n    @staticmethod\n    def __sample(total_number_of_sentences: int, percentage: float = 0.1) -> List[int]:\n        import random\n\n        sample_size: int = round(total_number_of_sentences * percentage)\n        sample = random.sample(range(1, total_number_of_sentences), sample_size)\n        return sample\n\n    @staticmethod\n    def download_dataset(task: NLPTask):\n\n        # conll 2000 chunking task\n        if task == NLPTask.CONLL_2000:\n            conll_2000_path = ""https://www.clips.uantwerpen.be/conll2000/chunking/""\n            data_file = Path(flair.cache_root) / ""datasets"" / task.value / ""train.txt""\n            if not data_file.is_file():\n                cached_path(\n                    f""{conll_2000_path}train.txt.gz"", Path(""datasets"") / task.value\n                )\n                cached_path(\n                    f""{conll_2000_path}test.txt.gz"", Path(""datasets"") / task.value\n                )\n                import gzip, shutil\n\n                with gzip.open(\n                    Path(flair.cache_root) / ""datasets"" / task.value / ""train.txt.gz"",\n                    ""rb"",\n                ) as f_in:\n                    with open(\n                        Path(flair.cache_root) / ""datasets"" / task.value / ""train.txt"",\n                        ""wb"",\n                    ) as f_out:\n                        shutil.copyfileobj(f_in, f_out)\n                with gzip.open(\n                    Path(flair.cache_root) / ""datasets"" / task.value / ""test.txt.gz"",\n                    ""rb"",\n                ) as f_in:\n                    with open(\n                        Path(flair.cache_root) / ""datasets"" / task.value / ""test.txt"",\n                        ""wb"",\n                    ) as f_out:\n                        shutil.copyfileobj(f_in, f_out)\n\n        if task == NLPTask.NER_BASQUE:\n            ner_basque_path = ""http://ixa2.si.ehu.eus/eiec/""\n            data_path = Path(flair.cache_root) / ""datasets"" / task.value\n            data_file = data_path / ""named_ent_eu.train""\n            if not data_file.is_file():\n                cached_path(\n                    f""{ner_basque_path}/eiec_v1.0.tgz"", Path(""datasets"") / task.value\n                )\n                import tarfile, shutil\n\n                with tarfile.open(\n                    Path(flair.cache_root) / ""datasets"" / task.value / ""eiec_v1.0.tgz"",\n                    ""r:gz"",\n                ) as f_in:\n                    corpus_files = (\n                        ""eiec_v1.0/named_ent_eu.train"",\n                        ""eiec_v1.0/named_ent_eu.test"",\n                    )\n                    for corpus_file in corpus_files:\n                        f_in.extract(corpus_file, data_path)\n                        shutil.move(f""{data_path}/{corpus_file}"", data_path)\n\n        if task == NLPTask.IMDB:\n            imdb_acl_path = (\n                ""http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz""\n            )\n            data_path = Path(flair.cache_root) / ""datasets"" / task.value\n            data_file = data_path / ""train.txt""\n            if not data_file.is_file():\n                cached_path(imdb_acl_path, Path(""datasets"") / task.value)\n                import tarfile\n\n                with tarfile.open(\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / task.value\n                    / ""aclImdb_v1.tar.gz"",\n                    ""r:gz"",\n                ) as f_in:\n                    datasets = [""train"", ""test""]\n                    labels = [""pos"", ""neg""]\n\n                    for label in labels:\n                        for dataset in datasets:\n                            f_in.extractall(\n                                data_path,\n                                members=[\n                                    m\n                                    for m in f_in.getmembers()\n                                    if f""{dataset}/{label}"" in m.name\n                                ],\n                            )\n                            with open(f""{data_path}/{dataset}.txt"", ""at"") as f_p:\n                                current_path = data_path / ""aclImdb"" / dataset / label\n                                for file_name in current_path.iterdir():\n                                    if file_name.is_file() and file_name.name.endswith(\n                                        "".txt""\n                                    ):\n                                        f_p.write(\n                                            f""__label__{label} ""\n                                            + file_name.open(\n                                                ""rt"", encoding=""utf-8""\n                                            ).read()\n                                            + ""\\n""\n                                        )\n\n        # Support both TREC-6 and TREC-50\n        if task.value.startswith(""trec""):\n            trec_path = ""http://cogcomp.org/Data/QA/QC/""\n\n            original_filenames = [""train_5500.label"", ""TREC_10.label""]\n            new_filenames = [""train.txt"", ""test.txt""]\n            for original_filename in original_filenames:\n                cached_path(\n                    f""{trec_path}{original_filename}"",\n                    Path(""datasets"") / task.value / ""original"",\n                )\n\n            data_path = Path(flair.cache_root) / ""datasets"" / task.value\n            data_file = data_path / new_filenames[0]\n\n            if not data_file.is_file():\n                for original_filename, new_filename in zip(\n                    original_filenames, new_filenames\n                ):\n                    with open(\n                        data_path / ""original"" / original_filename,\n                        ""rt"",\n                        encoding=""latin1"",\n                    ) as open_fp:\n                        with open(\n                            data_path / new_filename, ""wt"", encoding=""utf-8""\n                        ) as write_fp:\n                            for line in open_fp:\n                                line = line.rstrip()\n                                fields = line.split()\n                                old_label = fields[0]\n                                question = "" "".join(fields[1:])\n\n                                # Create flair compatible labels\n                                # TREC-6 : NUM:dist -> __label__NUM\n                                # TREC-50: NUM:dist -> __label__NUM:dist\n                                new_label = ""__label__""\n                                new_label += (\n                                    old_label.split("":"")[0]\n                                    if task.value == ""trec-6""\n                                    else old_label\n                                )\n\n                                write_fp.write(f""{new_label} {question}\\n"")\n\n        if task == NLPTask.WNUT_17:\n            wnut_path = ""https://noisy-text.github.io/2017/files/""\n            cached_path(f""{wnut_path}wnut17train.conll"", Path(""datasets"") / task.value)\n            cached_path(f""{wnut_path}emerging.dev.conll"", Path(""datasets"") / task.value)\n            cached_path(\n                f""{wnut_path}emerging.test.annotated"", Path(""datasets"") / task.value\n            )\n\n        # Wikiner NER task\n        wikiner_path = (\n            ""https://raw.githubusercontent.com/dice-group/FOX/master/input/Wikiner/""\n        )\n        if task.value.startswith(""wikiner""):\n            lc = """"\n            if task == NLPTask.WIKINER_ENGLISH:\n                lc = ""en""\n            if task == NLPTask.WIKINER_GERMAN:\n                lc = ""de""\n            if task == NLPTask.WIKINER_DUTCH:\n                lc = ""nl""\n            if task == NLPTask.WIKINER_FRENCH:\n                lc = ""fr""\n            if task == NLPTask.WIKINER_ITALIAN:\n                lc = ""it""\n            if task == NLPTask.WIKINER_SPANISH:\n                lc = ""es""\n            if task == NLPTask.WIKINER_PORTUGUESE:\n                lc = ""pt""\n            if task == NLPTask.WIKINER_POLISH:\n                lc = ""pl""\n            if task == NLPTask.WIKINER_RUSSIAN:\n                lc = ""ru""\n\n            data_file = (\n                Path(flair.cache_root)\n                / ""datasets""\n                / task.value\n                / f""aij-wikiner-{lc}-wp3.train""\n            )\n            if not data_file.is_file():\n\n                cached_path(\n                    f""{wikiner_path}aij-wikiner-{lc}-wp3.bz2"",\n                    Path(""datasets"") / task.value,\n                )\n                import bz2, shutil\n\n                # unpack and write out in CoNLL column-like format\n                bz_file = bz2.BZ2File(\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / task.value\n                    / f""aij-wikiner-{lc}-wp3.bz2"",\n                    ""rb"",\n                )\n                with bz_file as f, open(\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / task.value\n                    / f""aij-wikiner-{lc}-wp3.train"",\n                    ""w"",\n                ) as out:\n                    for line in f:\n                        line = line.decode(""utf-8"")\n                        words = line.split("" "")\n                        for word in words:\n                            out.write(""\\t"".join(word.split(""|"")) + ""\\n"")\n\n        # CoNLL 02/03 NER\n        conll_02_path = ""https://www.clips.uantwerpen.be/conll2002/ner/data/""\n        if task == NLPTask.CONLL_03_DUTCH:\n            cached_path(f""{conll_02_path}ned.testa"", Path(""datasets"") / task.value)\n            cached_path(f""{conll_02_path}ned.testb"", Path(""datasets"") / task.value)\n            cached_path(f""{conll_02_path}ned.train"", Path(""datasets"") / task.value)\n        if task == NLPTask.CONLL_03_SPANISH:\n            cached_path(f""{conll_02_path}esp.testa"", Path(""datasets"") / task.value)\n            cached_path(f""{conll_02_path}esp.testb"", Path(""datasets"") / task.value)\n            cached_path(f""{conll_02_path}esp.train"", Path(""datasets"") / task.value)\n\n        # universal dependencies\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/""\n        # --- UD Germanic\n        if task == NLPTask.UD_ENGLISH:\n            cached_path(\n                f""{ud_path}UD_English-EWT/master/en_ewt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_English-EWT/master/en_ewt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_English-EWT/master/en_ewt-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_GERMAN:\n            cached_path(\n                f""{ud_path}UD_German-GSD/master/de_gsd-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_German-GSD/master/de_gsd-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_German-GSD/master/de_gsd-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_DUTCH:\n            cached_path(\n                f""{ud_path}UD_Dutch-Alpino/master/nl_alpino-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Dutch-Alpino/master/nl_alpino-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Dutch-Alpino/master/nl_alpino-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        # --- UD Romance\n        if task == NLPTask.UD_FRENCH:\n            cached_path(\n                f""{ud_path}UD_French-GSD/master/fr_gsd-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_French-GSD/master/fr_gsd-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_French-GSD/master/fr_gsd-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_ITALIAN:\n            cached_path(\n                f""{ud_path}UD_Italian-ISDT/master/it_isdt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Italian-ISDT/master/it_isdt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Italian-ISDT/master/it_isdt-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_SPANISH:\n            cached_path(\n                f""{ud_path}UD_Spanish-GSD/master/es_gsd-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Spanish-GSD/master/es_gsd-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Spanish-GSD/master/es_gsd-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_PORTUGUESE:\n            cached_path(\n                f""{ud_path}UD_Portuguese-Bosque/blob/master/pt_bosque-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Portuguese-Bosque/blob/master/pt_bosque-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Portuguese-Bosque/blob/master/pt_bosque-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_ROMANIAN:\n            cached_path(\n                f""{ud_path}UD_Romanian-RRT/master/ro_rrt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Romanian-RRT/master/ro_rrt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Romanian-RRT/master/ro_rrt-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_CATALAN:\n            cached_path(\n                f""{ud_path}UD_Catalan-AnCora/master/ca_ancora-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Catalan-AnCora/master/ca_ancora-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Catalan-AnCora/master/ca_ancora-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        # --- UD West-Slavic\n        if task == NLPTask.UD_POLISH:\n            cached_path(\n                f""{ud_path}UD_Polish-LFG/master/pl_lfg-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Polish-LFG/master/pl_lfg-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Polish-LFG/master/pl_lfg-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_CZECH:\n            cached_path(\n                f""{ud_path}UD_Czech-PDT/master/cs_pdt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Czech-PDT/master/cs_pdt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Czech-PDT/master/cs_pdt-ud-train-l.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_SLOVAK:\n            cached_path(\n                f""{ud_path}UD_Slovak-SNK/master/sk_snk-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Slovak-SNK/master/sk_snk-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Slovak-SNK/master/sk_snk-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        # --- UD Scandinavian\n        if task == NLPTask.UD_SWEDISH:\n            cached_path(\n                f""{ud_path}UD_Swedish-Talbanken/master/sv_talbanken-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Swedish-Talbanken/master/sv_talbanken-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Swedish-Talbanken/master/sv_talbanken-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_DANISH:\n            cached_path(\n                f""{ud_path}UD_Danish-DDT/master/da_ddt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Danish-DDT/master/da_ddt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Danish-DDT/master/da_ddt-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_NORWEGIAN:\n            cached_path(\n                f""{ud_path}UD_Norwegian-Bokmaal/master/no_bokmaal-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Norwegian-Bokmaal/master/no_bokmaal-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Norwegian-Bokmaal/master/no_bokmaal-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_FINNISH:\n            cached_path(\n                f""{ud_path}UD_Finnish-TDT/master/fi_tdt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Finnish-TDT/master/fi_tdt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Finnish-TDT/master/fi_tdt-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        # --- UD South-Slavic\n        if task == NLPTask.UD_SLOVENIAN:\n            cached_path(\n                f""{ud_path}UD_Slovenian-SSJ/master/sl_ssj-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Slovenian-SSJ/master/sl_ssj-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Slovenian-SSJ/master/sl_ssj-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_CROATIAN:\n            cached_path(\n                f""{ud_path}UD_Croatian-SET/master/hr_set-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Croatian-SET/master/hr_set-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Croatian-SET/master/hr_set-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_SERBIAN:\n            cached_path(\n                f""{ud_path}UD_Serbian-SET/master/sr_set-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Serbian-SET/master/sr_set-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Serbian-SET/master/sr_set-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_BULGARIAN:\n            cached_path(\n                f""{ud_path}UD_Bulgarian-BTB/master/bg_btb-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Bulgarian-BTB/master/bg_btb-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Bulgarian-BTB/master/bg_btb-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        # --- UD Asian\n        if task == NLPTask.UD_ARABIC:\n            cached_path(\n                f""{ud_path}UD_Arabic-PADT/master/ar_padt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Arabic-PADT/master/ar_padt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Arabic-PADT/master/ar_padt-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_HEBREW:\n            cached_path(\n                f""{ud_path}UD_Hebrew-HTB/master/he_htb-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Hebrew-HTB/master/he_htb-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Hebrew-HTB/master/he_htb-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_TURKISH:\n            cached_path(\n                f""{ud_path}UD_Turkish-IMST/master/tr_imst-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Turkish-IMST/master/tr_imst-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Turkish-IMST/master/tr_imst-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_PERSIAN:\n            cached_path(\n                f""{ud_path}UD_Persian-Seraji/master/fa_seraji-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Persian-Seraji/master/fa_seraji-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Persian-Seraji/master/fa_seraji-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_RUSSIAN:\n            cached_path(\n                f""{ud_path}UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Russian-SynTagRus/master/ru_syntagrus-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_HINDI:\n            cached_path(\n                f""{ud_path}UD_Hindi-HDTB/master/hi_hdtb-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Hindi-HDTB/master/hi_hdtb-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Hindi-HDTB/master/hi_hdtb-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_INDONESIAN:\n            cached_path(\n                f""{ud_path}UD_Indonesian-GSD/master/id_gsd-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Indonesian-GSD/master/id_gsd-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Indonesian-GSD/master/id_gsd-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_JAPANESE:\n            cached_path(\n                f""{ud_path}UD_Japanese-GSD/master/ja_gsd-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Japanese-GSD/master/ja_gsd-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Japanese-GSD/master/ja_gsd-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_CHINESE:\n            cached_path(\n                f""{ud_path}UD_Chinese-GSD/master/zh_gsd-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Chinese-GSD/master/zh_gsd-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Chinese-GSD/master/zh_gsd-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_KOREAN:\n            cached_path(\n                f""{ud_path}UD_Korean-Kaist/master/ko_kaist-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Korean-Kaist/master/ko_kaist-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Korean-Kaist/master/ko_kaist-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task == NLPTask.UD_BASQUE:\n            cached_path(\n                f""{ud_path}UD_Basque-BDT/master/eu_bdt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Basque-BDT/master/eu_bdt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_Basque-BDT/master/eu_bdt-ud-train.conllu"",\n                Path(""datasets"") / task.value,\n            )\n\n        if task.value.startswith(""wassa""):\n\n            emotion = task.value[6:]\n\n            for split in [""train"", ""dev"", ""test""]:\n\n                data_file = (\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / task.value\n                    / f""{emotion}-{split}.txt""\n                )\n\n                if not data_file.is_file():\n\n                    if split == ""train"":\n                        url = f""http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/{emotion}-ratings-0to1.train.txt""\n                    if split == ""dev"":\n                        url = f""http://saifmohammad.com/WebDocs/EmoInt%20Dev%20Data%20With%20Gold/{emotion}-ratings-0to1.dev.gold.txt""\n                    if split == ""test"":\n                        url = f""http://saifmohammad.com/WebDocs/EmoInt%20Test%20Gold%20Data/{emotion}-ratings-0to1.test.gold.txt""\n\n                    path = cached_path(url, Path(""datasets"") / task.value)\n\n                    with open(path, ""r"") as f:\n                        with open(data_file, ""w"") as out:\n                            next(f)\n                            for line in f:\n                                fields = line.split(""\\t"")\n                                out.write(\n                                    f""__label__{fields[3].rstrip()} {fields[1]}\\n""\n                                )\n\n                    os.remove(path)\n\n        if task == NLPTask.UD_GERMAN_HDT:\n            cached_path(\n                f""{ud_path}UD_German-HDT/dev/de_hdt-ud-dev.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_German-HDT/dev/de_hdt-ud-test.conllu"",\n                Path(""datasets"") / task.value,\n            )\n            cached_path(\n                f""{ud_path}UD_German-HDT/dev/de_hdt-ud-train-a.conllu"",\n                Path(""datasets"") / task.value / ""original"",\n            )\n            cached_path(\n                f""{ud_path}UD_German-HDT/dev/de_hdt-ud-train-b.conllu"",\n                Path(""datasets"") / task.value / ""original"",\n            )\n            data_path = Path(flair.cache_root) / ""datasets"" / task.value\n\n            train_filenames = [""de_hdt-ud-train-a.conllu"", ""de_hdt-ud-train-b.conllu""]\n\n            new_train_file: Path = data_path / ""de_hdt-ud-train-all.conllu""\n\n            if not new_train_file.is_file():\n                with open(new_train_file, ""wt"") as f_out:\n                    for train_filename in train_filenames:\n                        with open(\n                            data_path / ""original"" / train_filename, ""rt""\n                        ) as f_in:\n                            f_out.write(f_in.read())\n'"
flair/file_utils.py,0,"b'""""""\nUtilities for working with the local dataset cache. Copied from AllenNLP\n""""""\nfrom pathlib import Path\nfrom typing import Tuple, Union, Optional, Sequence, cast\nimport os\nimport base64\nimport logging\nimport shutil\nimport tempfile\nimport re\nfrom urllib.parse import urlparse\n\nimport mmap\nimport requests\nimport zipfile\nimport io\n\n# from allennlp.common.tqdm import Tqdm\nimport flair\n\nlogger = logging.getLogger(""flair"")\n\n\ndef load_big_file(f: str) -> mmap.mmap:\n    """"""\n    Workaround for loading a big pickle file. Files over 2GB cause pickle errors on certin Mac and Windows distributions.\n    :param f:\n    :return:\n    """"""\n    logger.info(f""loading file {f}"")\n    with open(f, ""rb"") as f_in:\n        # mmap seems to be much more memory efficient\n        bf = mmap.mmap(f_in.fileno(), 0, access=mmap.ACCESS_READ)\n        f_in.close()\n    return bf\n\n\ndef url_to_filename(url: str, etag: str = None) -> str:\n    """"""\n    Converts a url into a filename in a reversible way.\n    If `etag` is specified, add it on the end, separated by a period\n    (which necessarily won\'t appear in the base64-encoded filename).\n    Get rid of the quotes in the etag, since Windows doesn\'t like them.\n    """"""\n    url_bytes = url.encode(""utf-8"")\n    b64_bytes = base64.b64encode(url_bytes)\n    decoded = b64_bytes.decode(""utf-8"")\n\n    if etag:\n        # Remove quotes from etag\n        etag = etag.replace(\'""\', """")\n        return f""{decoded}.{etag}""\n    else:\n        return decoded\n\n\ndef filename_to_url(filename: str) -> Tuple[str, str]:\n    """"""\n    Recovers the the url from the encoded filename. Returns it and the ETag\n    (which may be ``None``)\n    """"""\n    try:\n        # If there is an etag, it\'s everything after the first period\n        decoded, etag = filename.split(""."", 1)\n    except ValueError:\n        # Otherwise, use None\n        decoded, etag = filename, None\n\n    filename_bytes = decoded.encode(""utf-8"")\n    url_bytes = base64.b64decode(filename_bytes)\n    return url_bytes.decode(""utf-8""), etag\n\n\ndef cached_path(url_or_filename: str, cache_dir: Union[str, Path]) -> Path:\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if type(cache_dir) is str:\n        cache_dir = Path(cache_dir)\n    dataset_cache = Path(flair.cache_root) / cache_dir\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (""http"", ""https""):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, dataset_cache)\n    elif parsed.scheme == """" and Path(url_or_filename).exists():\n        # File, and it exists.\n        return Path(url_or_filename)\n    elif parsed.scheme == """":\n        # File, but it doesn\'t exist.\n        raise FileNotFoundError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(\n            ""unable to parse {} as a URL or as a local path"".format(url_or_filename)\n        )\n\n\ndef unzip_file(file: Union[str, Path], unzip_to: Union[str, Path]):\n    from zipfile import ZipFile\n\n    with ZipFile(Path(file), ""r"") as zipObj:\n        # Extract all the contents of zip file in current directory\n        zipObj.extractall(Path(unzip_to))\n\n\ndef download_file(url: str, cache_dir: Union[str, Path]):\n    if type(cache_dir) is str:\n        cache_dir = Path(cache_dir)\n    cache_dir.mkdir(parents=True, exist_ok=True)\n\n    filename = re.sub(r"".+/"", """", url)\n    # get cache path to put the file\n    cache_path = cache_dir / filename\n    print(cache_path)\n\n    # Download to temporary file, then copy to cache dir once finished.\n    # Otherwise you get corrupt cache entries if the download gets interrupted.\n    fd, temp_filename = tempfile.mkstemp()\n    logger.info(""%s not found in cache, downloading to %s"", url, temp_filename)\n\n    # GET file object\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(""Content-Length"")\n    total = int(content_length) if content_length is not None else None\n    progress = Tqdm.tqdm(unit=""B"", total=total)\n    with open(temp_filename, ""wb"") as temp_file:\n        for chunk in req.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                progress.update(len(chunk))\n                temp_file.write(chunk)\n\n    progress.close()\n\n    logger.info(""copying %s to cache at %s"", temp_filename, cache_path)\n    shutil.copyfile(temp_filename, str(cache_path))\n    logger.info(""removing temp file %s"", temp_filename)\n    os.close(fd)\n    os.remove(temp_filename)\n\n    progress.close()\n\n\n# TODO(joelgrus): do we want to do checksums or anything like that?\ndef get_from_cache(url: str, cache_dir: Path = None) -> Path:\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    cache_dir.mkdir(parents=True, exist_ok=True)\n\n    filename = re.sub(r"".+/"", """", url)\n    # get cache path to put the file\n    cache_path = cache_dir / filename\n    if cache_path.exists():\n        return cache_path\n\n    # make HEAD request to check ETag\n    response = requests.head(url, headers={""User-Agent"": ""Flair""})\n    if response.status_code != 200:\n        raise IOError(\n            f""HEAD request failed for url {url} with status code {response.status_code}.""\n        )\n\n    # add ETag to filename if it exists\n    # etag = response.headers.get(""ETag"")\n\n    if not cache_path.exists():\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        fd, temp_filename = tempfile.mkstemp()\n        logger.info(""%s not found in cache, downloading to %s"", url, temp_filename)\n\n        # GET file object\n        req = requests.get(url, stream=True, headers={""User-Agent"": ""Flair""})\n        content_length = req.headers.get(""Content-Length"")\n        total = int(content_length) if content_length is not None else None\n        progress = Tqdm.tqdm(unit=""B"", total=total)\n        with open(temp_filename, ""wb"") as temp_file:\n            for chunk in req.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    progress.update(len(chunk))\n                    temp_file.write(chunk)\n\n        progress.close()\n\n        logger.info(""copying %s to cache at %s"", temp_filename, cache_path)\n        shutil.copyfile(temp_filename, str(cache_path))\n        logger.info(""removing temp file %s"", temp_filename)\n        os.close(fd)\n        os.remove(temp_filename)\n\n    return cache_path\n\n\ndef open_inside_zip(\n    archive_path: str,\n    cache_dir: Union[str, Path],\n    member_path: Optional[str] = None,\n    encoding: str = ""utf8"",\n) -> iter:\n    cached_archive_path = cached_path(archive_path, cache_dir=Path(cache_dir))\n    archive = zipfile.ZipFile(cached_archive_path, ""r"")\n    if member_path is None:\n        members_list = archive.namelist()\n        member_path = get_the_only_file_in_the_archive(members_list, archive_path)\n    member_path = cast(str, member_path)\n    member_file = archive.open(member_path, ""r"")\n    return io.TextIOWrapper(member_file, encoding=encoding)\n\n\ndef get_the_only_file_in_the_archive(\n    members_list: Sequence[str], archive_path: str\n) -> str:\n    if len(members_list) > 1:\n        raise ValueError(\n            ""The archive %s contains multiple files, so you must select ""\n            ""one of the files inside providing a uri of the type: %s""\n            % (\n                archive_path,\n                format_embeddings_file_uri(\n                    ""path_or_url_to_archive"", ""path_inside_archive""\n                ),\n            )\n        )\n    return members_list[0]\n\n\ndef format_embeddings_file_uri(\n    main_file_path_or_url: str, path_inside_archive: Optional[str] = None\n) -> str:\n    if path_inside_archive:\n        return ""({})#{}"".format(main_file_path_or_url, path_inside_archive)\n    return main_file_path_or_url\n\n\nfrom tqdm import tqdm as _tqdm\n\n\nclass Tqdm:\n    # These defaults are the same as the argument defaults in tqdm.\n    default_mininterval: float = 0.1\n\n    @staticmethod\n    def set_default_mininterval(value: float) -> None:\n        Tqdm.default_mininterval = value\n\n    @staticmethod\n    def set_slower_interval(use_slower_interval: bool) -> None:\n        """"""\n        If ``use_slower_interval`` is ``True``, we will dramatically slow down ``tqdm\'s`` default\n        output rate.  ``tqdm\'s`` default output rate is great for interactively watching progress,\n        but it is not great for log files.  You might want to set this if you are primarily going\n        to be looking at output through log files, not the terminal.\n        """"""\n        if use_slower_interval:\n            Tqdm.default_mininterval = 10.0\n        else:\n            Tqdm.default_mininterval = 0.1\n\n    @staticmethod\n    def tqdm(*args, **kwargs):\n        new_kwargs = {""mininterval"": Tqdm.default_mininterval, **kwargs}\n\n        return _tqdm(*args, **new_kwargs)\n'"
flair/inference_utils.py,1,"b'import logging\nimport pickle\nimport re\nimport shutil\nimport sqlite3\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nimport flair\nfrom flair.embeddings import WordEmbeddings\n\n# this is the default init size of a lmdb database for embeddings\nDEFAULT_MAP_SIZE = 100 * 1024 * 1024 * 1024\n\nlogger = logging.getLogger(""flair"")\n\n\nclass WordEmbeddingsStore:\n    """"""\n    class to simulate a WordEmbeddings class from flair.\n\n    Run this to generate a headless (without word embeddings) model as well a stored word embeddings:\n\n    >>> from flair.inference_utils import WordEmbeddingsStore\n    >>> from flair.models import SequenceTagger\n    >>> import pickle\n    >>> tagger = SequenceTagger.load(""multi-ner-fast"")\n    >>> WordEmbeddingsStore.create_stores(tagger)\n    >>> pickle.dump(tagger, open(""multi-ner-fast-headless.pickle"", ""wb""))\n\n    The same but using LMDB as memory database:\n\n    >>> from flair.inference_utils import WordEmbeddingsStore\n    >>> from flair.models import SequenceTagger\n    >>> import pickle\n    >>> tagger = SequenceTagger.load(""multi-ner-fast"")\n    >>> WordEmbeddingsStore.create_stores(tagger, backend=\'lmdb\')\n    >>> pickle.dump(tagger, open(""multi-ner-fast-headless.pickle"", ""wb""))\n\n    Then this can be used as follows:\n\n    >>> from flair.data import Sentence\n    >>> tagger = pickle.load(open(""multi-ner-fast-headless.pickle"", ""rb""))\n    >>> WordEmbeddingsStore.load_stores(tagger)\n    >>> text = ""Schade um den Ameisenb\xc3\xa4ren. Lukas B\xc3\xa4rfuss ver\xc3\xb6ffentlicht Erz\xc3\xa4hlungen aus zwanzig Jahren.""\n    >>> sentence = Sentence(text)\n    >>> tagger.predict(sentence)\n    >>> print(sentence.get_spans(\'ner\'))\n\n    The same but using LMDB as memory database:\n\n    >>> from flair.data import Sentence\n    >>> tagger = pickle.load(open(""multi-ner-fast-headless.pickle"", ""rb""))\n    >>> WordEmbeddingsStore.load_stores(tagger, backend=\'lmdb\')\n    >>> text = ""Schade um den Ameisenb\xc3\xa4ren. Lukas B\xc3\xa4rfuss ver\xc3\xb6ffentlicht Erz\xc3\xa4hlungen aus zwanzig Jahren.""\n    >>> sentence = Sentence(text)\n    >>> tagger.predict(sentence)\n    >>> print(sentence.get_spans(\'ner\'))\n    """"""\n\n    def __init__(self, embedding: WordEmbeddings, backend=\'sqlite\', verbose=True):\n        """"""\n        :param embedding: Flair WordEmbeddings instance.\n        :param backend: cache database backend name e.g ``\'sqlite\'``, ``\'lmdb\'``.\n                        Default value is ``\'sqlite\'``.\n        :param verbose: If `True` print information on standard output\n        """"""\n        # some non-used parameter to allow print\n        self._modules = dict()\n        self.items = """"\n\n        # get db filename from embedding name\n        self.name = embedding.name\n        self.store_path: Path = WordEmbeddingsStore._get_store_path(embedding, backend)\n        if verbose:\n            logger.info(f""store filename: {str(self.store_path)}"")\n\n        if backend == \'sqlite\':\n            self.backend = SqliteWordEmbeddingsStoreBackend(embedding, verbose)\n        elif backend == \'lmdb\':\n            self.backend = LmdbWordEmbeddingsStoreBackend(embedding, verbose)\n        else:\n            raise ValueError(\n                f\'The given backend ""{backend}"" is not available.\'\n            )\n        # In case initialization of cached version failed, just fallback to the original WordEmbeddings\n        if not self.backend.is_ok:\n            self.backend = WordEmbeddings(embedding.embeddings)\n\n    def _get_vector(self, word=""house""):\n        return self.backend._get_vector(word)\n\n    def embed(self, sentences):\n        for sentence in sentences:\n            for token in sentence:\n                t = torch.tensor(self._get_vector(word=token.text.lower()))\n                token.set_embedding(self.name, t)\n\n    @staticmethod\n    def _get_store_path(embedding, backend=\'sqlite\'):\n        """"""\n        get the filename of the store\n        """"""\n        cache_dir = Path(flair.cache_root)\n        embedding_filename = re.findall(""/(embeddings/.*)"", embedding.name)[0]\n        store_path = cache_dir / (embedding_filename + ""."" + backend)\n        return store_path\n\n    @staticmethod\n    def _word_embeddings(model):\n        # SequenceTagger\n        if hasattr(model, \'embeddings\'):\n            embeds = model.embeddings.embeddings\n        # TextClassifier\n        elif hasattr(model, \'document_embeddings\') and hasattr(model.document_embeddings, \'embeddings\'):\n            embeds = model.document_embeddings.embeddings.embeddings\n        else:\n            embeds = []\n        return embeds\n\n    @staticmethod\n    def create_stores(model, backend=\'sqlite\'):\n        """"""\n        creates database versions of all word embeddings in the model and\n        deletes the original vectors to save memory\n        """"""\n        for embedding in WordEmbeddingsStore._word_embeddings(model):\n            if type(embedding) == WordEmbeddings:\n                WordEmbeddingsStore(embedding, backend)\n                del embedding.precomputed_word_embeddings\n\n    @staticmethod\n    def load_stores(model, backend=\'sqlite\'):\n        """"""\n        loads the db versions of all word embeddings in the model\n        """"""\n        embeds = WordEmbeddingsStore._word_embeddings(model)\n        for i, embedding in enumerate(embeds):\n            if type(embedding) == WordEmbeddings:\n                embeds[i] = WordEmbeddingsStore(embedding, backend)\n\n    @staticmethod\n    def delete_stores(model, backend=\'sqlite\'):\n        """"""\n        deletes the db versions of all word embeddings\n        """"""\n        for embedding in WordEmbeddingsStore._word_embeddings(model):\n            store_path: Path = WordEmbeddingsStore._get_store_path(embedding)\n            logger.info(f""delete store: {str(store_path)}"")\n            if store_path.is_file():\n                store_path.unlink()\n            elif store_path.is_dir():\n                shutil.rmtree(store_path, ignore_errors=False, onerror=None)\n\n\nclass WordEmbeddingsStoreBackend:\n    def __init__(self, embedding, backend, verbose=True):\n        # get db filename from embedding name\n        self.name = embedding.name\n        self.store_path: Path = WordEmbeddingsStore._get_store_path(embedding, backend)\n\n    @property\n    def is_ok(self):\n        return hasattr(self, \'k\')\n\n    def _get_vector(self, word=""house""):\n        pass\n\n\nclass SqliteWordEmbeddingsStoreBackend(WordEmbeddingsStoreBackend):\n    def __init__(self, embedding, verbose):\n        super().__init__(embedding, \'sqlite\', verbose)\n        # if embedding database already exists\n        if self.store_path.exists() and self.store_path.is_file():\n            try:\n                self.db = sqlite3.connect(str(self.store_path))\n                cursor = self.db.cursor()\n                cursor.execute(""SELECT * FROM embedding LIMIT 1;"")\n                result = list(cursor)\n                self.k = len(result[0]) - 1\n                return\n            except sqlite3.Error as err:\n                logger.exception(f""Fail to open sqlite database {str(self.store_path)}: {str(err)}"")\n        # otherwise, push embedding to database\n        if hasattr(embedding, \'precomputed_word_embeddings\'):\n            self.db = sqlite3.connect(str(self.store_path))\n            pwe = embedding.precomputed_word_embeddings\n            self.k = pwe.vector_size\n            self.db.execute(f""DROP TABLE IF EXISTS embedding;"")\n            self.db.execute(\n                f""CREATE TABLE embedding(word text,{\',\'.join(\'v\' + str(i) + \' float\' for i in range(self.k))});""\n            )\n            vectors_it = (\n                [word] + pwe.get_vector(word).tolist() for word in pwe.vocab.keys()\n            )\n            if verbose:\n                logger.info(""load vectors to store"")\n            self.db.executemany(\n                f""INSERT INTO embedding(word,{\',\'.join(\'v\' + str(i) for i in range(self.k))}) \\\n            values ({\',\'.join([\'?\'] * (1 + self.k))})"",\n                tqdm(vectors_it),\n            )\n            self.db.execute(f""DROP INDEX IF EXISTS embedding_index;"")\n            self.db.execute(f""CREATE INDEX embedding_index ON embedding(word);"")\n            self.db.commit()\n            self.db.close()\n\n    def _get_vector(self, word=""house""):\n        db = sqlite3.connect(str(self.store_path))\n        cursor = db.cursor()\n        word = word.replace(\'""\', \'\')\n        cursor.execute(f\'SELECT * FROM embedding WHERE word=""{word}"";\')\n        result = list(cursor)\n        db.close()\n        if not result:\n            return [0.0] * self.k\n        return result[0][1:]\n\n\nclass LmdbWordEmbeddingsStoreBackend(WordEmbeddingsStoreBackend):\n    def __init__(self, embedding, verbose):\n        super().__init__(embedding, \'lmdb\', verbose)\n        try:\n            import lmdb\n            # if embedding database already exists\n            if self.store_path.exists() and self.store_path.is_dir():\n                # open the database in read mode\n                try:\n                    self.env = lmdb.open(str(self.store_path), readonly=True, max_readers=2048, max_spare_txns=4)\n                    if self.env:\n                        # we need to set self.k\n                        with self.env.begin() as txn:\n                            cursor = txn.cursor()\n                            for key, value in cursor:\n                                vector = pickle.loads(value)\n                                self.k = vector.shape[0]\n                                break\n                            cursor.close()\n                        return\n                except lmdb.Error as err:\n                    logger.exception(f""Fail to open lmdb database {str(self.store_path)}: {str(err)}"")\n            # create and load the database in write mode\n            if hasattr(embedding, \'precomputed_word_embeddings\'):\n                pwe = embedding.precomputed_word_embeddings\n                self.k = pwe.vector_size\n                self.store_path.mkdir(parents=True, exist_ok=True)\n                self.env = lmdb.open(str(self.store_path), map_size=DEFAULT_MAP_SIZE)\n                if verbose:\n                    logger.info(""load vectors to store"")\n\n                txn = self.env.begin(write=True)\n                for word in tqdm(pwe.vocab.keys()):\n                    vector = pwe.get_vector(word)\n                    if len(word.encode(encoding=\'UTF-8\')) < self.env.max_key_size():\n                        txn.put(word.encode(encoding=\'UTF-8\'), pickle.dumps(vector))\n                txn.commit()\n                return\n        except ModuleNotFoundError:\n            logger.warning(""-"" * 100)\n            logger.warning(\'ATTENTION! The library ""lmdb"" is not installed!\')\n            logger.warning(\n                \'To use LMDB, please first install with ""pip install lmdb""\'\n            )\n            logger.warning(""-"" * 100)\n\n    def _get_vector(self, word=""house""):\n        try:\n            import lmdb\n            with self.env.begin() as txn:\n                vector = txn.get(word.encode(encoding=\'UTF-8\'))\n                if vector:\n                    word_vector = pickle.loads(vector)\n                    vector = None\n                else:\n                    word_vector = np.zeros((self.k,), dtype=np.float32)\n        except lmdb.Error:\n            # no idea why, but we need to close and reopen the environment to avoid\n            # mdb_txn_begin: MDB_BAD_RSLOT: Invalid reuse of reader locktable slot\n            # when opening new transaction !\n            self.env.close()\n            self.env = lmdb.open(self.store_path, readonly=True, max_readers=2048, max_spare_txns=2, lock=False)\n            return self._get_vector(word)\n        except ModuleNotFoundError:\n            logger.warning(""-"" * 100)\n            logger.warning(\'ATTENTION! The library ""lmdb"" is not installed!\')\n            logger.warning(\n                \'To use LMDB, please first install with ""pip install lmdb""\'\n            )\n            logger.warning(""-"" * 100)\n            word_vector = np.zeros((self.k,), dtype=np.float32)\n        return word_vector\n'"
flair/nn.py,10,"b'import warnings\nfrom pathlib import Path\n\nimport torch.nn\n\nfrom abc import abstractmethod\n\nfrom typing import Union, List\n\nfrom torch.utils.data.dataset import Dataset\n\nimport flair\nfrom flair import file_utils\nfrom flair.data import DataPoint, Sentence\nfrom flair.datasets import DataLoader\nfrom flair.training_utils import Result\n\n\nclass Model(torch.nn.Module):\n    """"""Abstract base class for all downstream task models in Flair, such as SequenceTagger and TextClassifier.\n    Every new type of model must implement these methods.""""""\n\n    @abstractmethod\n    def forward_loss(\n        self, data_points: Union[List[DataPoint], DataPoint]\n    ) -> torch.tensor:\n        """"""Performs a forward pass and returns a loss tensor for backpropagation. Implement this to enable training.""""""\n        pass\n\n    @abstractmethod\n    def evaluate(\n        self,\n        sentences: Union[List[DataPoint], Dataset],\n        out_path: Path = None,\n        embedding_storage_mode: str = ""none"",\n    ) -> (Result, float):\n        """"""Evaluates the model. Returns a Result object containing evaluation\n        results and a loss value. Implement this to enable evaluation.\n        :param data_loader: DataLoader that iterates over dataset to be evaluated\n        :param out_path: Optional output path to store predictions\n        :param embedding_storage_mode: One of \'none\', \'cpu\' or \'gpu\'. \'none\' means all embeddings are deleted and\n        freshly recomputed, \'cpu\' means all embeddings are stored on CPU, or \'gpu\' means all embeddings are stored on GPU\n        :return: Returns a Tuple consisting of a Result object and a loss float value\n        """"""\n        pass\n\n    @abstractmethod\n    def _get_state_dict(self):\n        """"""Returns the state dictionary for this model. Implementing this enables the save() and save_checkpoint()\n        functionality.""""""\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def _init_model_with_state_dict(state):\n        """"""Initialize the model from a state dictionary. Implementing this enables the load() and load_checkpoint()\n        functionality.""""""\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def _fetch_model(model_name) -> str:\n        return model_name\n\n    def save(self, model_file: Union[str, Path]):\n        """"""\n        Saves the current model to the provided file.\n        :param model_file: the model file\n        """"""\n        model_state = self._get_state_dict()\n\n        torch.save(model_state, str(model_file), pickle_protocol=4)\n\n    @classmethod\n    def load(cls, model: Union[str, Path]):\n        """"""\n        Loads the model from the given file.\n        :param model: the model file\n        :return: the loaded text classifier model\n        """"""\n        model_file = cls._fetch_model(str(model))\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"")\n            # load_big_file is a workaround by https://github.com/highway11git to load models on some Mac/Windows setups\n            # see https://github.com/zalandoresearch/flair/issues/351\n            f = file_utils.load_big_file(str(model_file))\n            state = torch.load(f, map_location=flair.device)\n\n        model = cls._init_model_with_state_dict(state)\n\n        model.eval()\n        model.to(flair.device)\n\n        return model\n\n\nclass LockedDropout(torch.nn.Module):\n    """"""\n    Implementation of locked (or variational) dropout. Randomly drops out entire parameters in embedding space.\n    """"""\n\n    def __init__(self, dropout_rate=0.5, batch_first=True, inplace=False):\n        super(LockedDropout, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.batch_first = batch_first\n        self.inplace = inplace\n\n    def forward(self, x):\n        if not self.training or not self.dropout_rate:\n            return x\n\n        if not self.batch_first:\n            m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.dropout_rate)\n        else:\n            m = x.data.new(x.size(0), 1, x.size(2)).bernoulli_(1 - self.dropout_rate)\n\n        mask = torch.autograd.Variable(m, requires_grad=False) / (1 - self.dropout_rate)\n        mask = mask.expand_as(x)\n        return mask * x\n\n    def extra_repr(self):\n        inplace_str = "", inplace"" if self.inplace else """"\n        return ""p={}{}"".format(self.dropout_rate, inplace_str)\n\n\nclass WordDropout(torch.nn.Module):\n    """"""\n    Implementation of word dropout. Randomly drops out entire words (or characters) in embedding space.\n    """"""\n\n    def __init__(self, dropout_rate=0.05, inplace=False):\n        super(WordDropout, self).__init__()\n        self.dropout_rate = dropout_rate\n        self.inplace = inplace\n\n    def forward(self, x):\n        if not self.training or not self.dropout_rate:\n            return x\n\n        m = x.data.new(x.size(0), x.size(1), 1).bernoulli_(1 - self.dropout_rate)\n\n        mask = torch.autograd.Variable(m, requires_grad=False)\n        return mask * x\n\n    def extra_repr(self):\n        inplace_str = "", inplace"" if self.inplace else """"\n        return ""p={}{}"".format(self.dropout_rate, inplace_str)\n'"
flair/optim.py,9,"b'import logging\nimport math\n\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau\n\n\nlog = logging.getLogger(""flair"")\n\n\nclass SGDW(Optimizer):\n    r""""""Implements stochastic gradient descent (optionally with momentum) with\n    weight decay from the paper `Fixing Weight Decay Regularization in Adam`_.\n\n    Nesterov momentum is based on the formula from\n    `On the importance of initialization and momentum in deep learning`__.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float): learning rate\n        momentum (float, optional): momentum factor (default: 0)\n        weight_decay (float, optional): weight decay factor (default: 0)\n        dampening (float, optional): dampening for momentum (default: 0)\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\n\n    .. _Fixing Weight Decay Regularization in Adam:\n        https://arxiv.org/abs/1711.05101\n\n    Example:\n        >>> optimizer = torch.optim.SGDW(model.parameters(), lr=0.1, momentum=0.9,\n                                         weight_decay=1e-5)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n\n    .. note::\n        The implementation of SGD with Momentum/Nesterov subtly differs from\n        Sutskever et. al. and implementations in some other frameworks.\n\n        Considering the specific case of Momentum, the update can be written as\n\n        .. math::\n                  v = \\rho * v + g \\\\\n                  p = p - lr * v\n\n        where p, g, v and :math:`\\rho` denote the parameters, gradient,\n        velocity, and momentum respectively.\n\n        This is in contrast to Sutskever et. al. and\n        other frameworks which employ an update of the form\n\n        .. math::\n             v = \\rho * v + lr * g \\\\\n             p = p - v\n\n        The Nesterov version is analogously modified.\n    """"""\n\n    def __init__(\n        self,\n        params,\n        lr=required,\n        momentum=0,\n        dampening=0,\n        weight_decay=0,\n        nesterov=False,\n    ):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if momentum < 0.0:\n            raise ValueError(""Invalid momentum value: {}"".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n        )\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\n        super(SGDW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(SGDW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(""nesterov"", False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[""weight_decay""]\n            momentum = group[""momentum""]\n            dampening = group[""dampening""]\n            nesterov = group[""nesterov""]\n\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if ""momentum_buffer"" not in param_state:\n                        buf = param_state[""momentum_buffer""] = torch.zeros_like(p.data)\n                        buf.mul_(momentum).add_(d_p)\n                    else:\n                        buf = param_state[""momentum_buffer""]\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n\n                if weight_decay != 0:\n                    p.data.add_(-weight_decay, p.data)\n\n                p.data.add_(-group[""lr""], d_p)\n\n        return loss\n\n\nclass AdamW(Optimizer):\n    r""""""Implements AdamW optimizer.\n\n    Adam has been proposed in `Adam\\: A Method for Stochastic Optimization`_.\n    AdamW uses the weight decay method from the paper\n    `Fixing Weight Decay Regularization in Adam`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay factor (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _Fixing Weight Decay Regularization in Adam:\n        https://arxiv.org/abs/1711.05101\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        amsgrad=False,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(\n            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad\n        )\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(""amsgrad"", False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        ""Adam does not support sparse gradients, please consider SparseAdam instead""\n                    )\n                amsgrad = group[""amsgrad""]\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[""step""] = 0\n                    # Exponential moving average of gradient values\n                    state[""exp_avg""] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[""exp_avg_sq""] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[""max_exp_avg_sq""] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                if amsgrad:\n                    max_exp_avg_sq = state[""max_exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                state[""step""] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[""eps""])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[""eps""])\n\n                bias_correction1 = 1 - beta1 ** state[""step""]\n                bias_correction2 = 1 - beta2 ** state[""step""]\n                step_size = group[""lr""] * math.sqrt(bias_correction2) / bias_correction1\n\n                if group[""weight_decay""] != 0:\n                    p.data.add_(-group[""weight_decay""], p.data)\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n\n\nclass ExpAnnealLR(_LRScheduler):\n    """"""Exponentially anneal the learning rate of each parameter group\n    from the initial lr to end_lr over a number of iterations.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        end_lr (float): The final learning rate.\n        iterations (int): The number of iterations over which to increase the\n            learning rate.\n        last_epoch (int): The index of the last iteration. Default: -1.\n    """"""\n\n    def __init__(self, optimizer, end_lr, iterations, last_epoch=-1):\n        self.end_lr = end_lr\n        self.iterations = iterations\n        super(ExpAnnealLR, self).__init__(optimizer, last_epoch=last_epoch)\n\n    def get_lr(self):\n        iteration = self.last_epoch + 1\n        pct = iteration / self.iterations\n        return [base_lr * (self.end_lr / base_lr) ** pct for base_lr in self.base_lrs]\n\n\nclass ReduceLRWDOnPlateau(ReduceLROnPlateau):\n    """"""Reduce learning rate and weight decay when a metric has stopped\n    improving. Models often benefit from reducing the learning rate by\n    a factor of 2-10 once learning stagnates. This scheduler reads a metric\n    quantity and if no improvement is seen for a \'patience\' number\n    of epochs, the learning rate and weight decay factor is reduced for\n    optimizers that implement the the weight decay method from the paper\n    `Fixing Weight Decay Regularization in Adam`_.\n\n    .. _Fixing Weight Decay Regularization in Adam:\n        https://arxiv.org/abs/1711.05101\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        mode (str): One of `min`, `max`. In `min` mode, lr will\n            be reduced when the quantity monitored has stopped\n            decreasing; in `max` mode it will be reduced when the\n            quantity monitored has stopped increasing. Default: \'min\'.\n        factor (float): Factor by which the learning rate will be\n            reduced. new_lr = lr * factor. Default: 0.1.\n        patience (int): Number of epochs with no improvement after\n            which learning rate will be reduced. For example, if\n            `patience = 2`, then we will ignore the first 2 epochs\n            with no improvement, and will only decrease the LR after the\n            3rd epoch if the loss still hasn\'t improved then.\n            Default: 10.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n        threshold (float): Threshold for measuring the new optimum,\n            to only focus on significant changes. Default: 1e-4.\n        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n            dynamic_threshold = best * ( 1 + threshold ) in \'max\'\n            mode or best * ( 1 - threshold ) in `min` mode.\n            In `abs` mode, dynamic_threshold = best + threshold in\n            `max` mode or best - threshold in `min` mode. Default: \'rel\'.\n        cooldown (int): Number of epochs to wait before resuming\n            normal operation after lr has been reduced. Default: 0.\n        min_lr (float or list): A scalar or a list of scalars. A\n            lower bound on the learning rate of all param groups\n            or each group respectively. Default: 0.\n        eps (float): Minimal decay applied to lr. If the difference\n            between new and old lr is smaller than eps, the update is\n            ignored. Default: 1e-8.\n\n    Example:\n        >>> optimizer = AdamW(model.parameters(), lr=0.1, weight_decay=1e-3)\n        >>> scheduler = ReduceLRWDOnPlateau(optimizer, \'min\')\n        >>> for epoch in range(10):\n        >>>     train(...)\n        >>>     val_loss = validate(...)\n        >>>     # Note that step should be called after validate()\n        >>>     scheduler.step(val_loss)\n    """"""\n\n    def step(self, metrics, epoch=None):\n        current = metrics\n        if epoch is None:\n            epoch = self.last_epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n\n        if self.is_better(current, self.best):\n            self.best = current\n            self.num_bad_epochs = 0\n        else:\n            self.num_bad_epochs += 1\n\n        if self.in_cooldown:\n            self.cooldown_counter -= 1\n            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n\n        if self.num_bad_epochs > self.patience:\n            self._reduce_lr(epoch)\n            self._reduce_weight_decay(epoch)\n            self.cooldown_counter = self.cooldown\n            self.num_bad_epochs = 0\n\n    def _reduce_weight_decay(self, epoch):\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            if param_group[""weight_decay""] != 0:\n                old_weight_decay = float(param_group[""weight_decay""])\n                new_weight_decay = max(old_weight_decay * self.factor, self.min_lrs[i])\n                if old_weight_decay - new_weight_decay > self.eps:\n                    param_group[""weight_decay""] = new_weight_decay\n                    if self.verbose:\n                        log.info(\n                            f""Epoch {epoch}: reducing weight decay factor of group {i} to {new_weight_decay:.4e}.""\n                        )\n'"
flair/samplers.py,3,"b'import logging\nfrom collections import defaultdict\n\nfrom torch.utils.data.sampler import Sampler\nimport random, torch\n\nfrom flair.data import FlairDataset\n\nlog = logging.getLogger(""flair"")\n\n\nclass FlairSampler(Sampler):\n    def set_dataset(self, data_source):\n        """"""Initialize by passing a block_size and a plus_window parameter.\n        :param data_source: dataset to sample from\n        """"""\n        self.data_source = data_source\n        self.num_samples = len(self.data_source)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass ImbalancedClassificationDatasetSampler(FlairSampler):\n    """"""Use this to upsample rare classes and downsample common classes in your unbalanced classification dataset.\n    """"""\n\n    def __init__(self):\n        super(ImbalancedClassificationDatasetSampler, self).__init__(None)\n\n    def set_dataset(self, data_source: FlairDataset):\n        """"""\n        Initialize by passing a classification dataset with labels, i.e. either TextClassificationDataSet or\n        :param data_source:\n        """"""\n        self.data_source = data_source\n        self.num_samples = len(self.data_source)\n        self.indices = list(range(len(data_source)))\n\n        # first determine the distribution of classes in the dataset\n        label_count = defaultdict(int)\n        for sentence in data_source:\n            for label in sentence.labels:\n                label_count[label.value] += 1\n\n        # weight for each sample\n        offset = 0\n        weights = [\n            1.0 / (offset + label_count[data_source[idx].labels[0].value])\n            for idx in self.indices\n        ]\n\n        self.weights = torch.DoubleTensor(weights)\n\n    def __iter__(self):\n        return (\n            self.indices[i]\n            for i in torch.multinomial(self.weights, self.num_samples, replacement=True)\n        )\n\n\nclass ChunkSampler(FlairSampler):\n    """"""Splits data into blocks and randomizes them before sampling. This causes some order of the data to be preserved,\n    while still shuffling the data.\n    """"""\n\n    def __init__(self, block_size=5, plus_window=5):\n        super(ChunkSampler, self).__init__(None)\n        self.block_size = block_size\n        self.plus_window = plus_window\n        self.data_source = None\n\n    def __iter__(self):\n        data = list(range(len(self.data_source)))\n\n        blocksize = self.block_size + random.randint(0, self.plus_window)\n\n        log.info(\n            f""Chunk sampling with blocksize = {blocksize} ({self.block_size} + {self.plus_window})""\n        )\n\n        # Create blocks\n        blocks = [data[i : i + blocksize] for i in range(0, len(data), blocksize)]\n        # shuffle the blocks\n        random.shuffle(blocks)\n        # concatenate the shuffled blocks\n        data[:] = [b for bs in blocks for b in bs]\n        return iter(data)\n\n\nclass ExpandingChunkSampler(FlairSampler):\n    """"""Splits data into blocks and randomizes them before sampling. Block size grows with each epoch.\n    This causes some order of the data to be preserved, while still shuffling the data.\n    """"""\n\n    def __init__(self, step=3):\n        """"""Initialize by passing a block_size and a plus_window parameter.\n        :param data_source: dataset to sample from\n        """"""\n        super(ExpandingChunkSampler, self).__init__(None)\n        self.block_size = 1\n        self.epoch_count = 0\n        self.step = step\n\n    def __iter__(self):\n        self.epoch_count += 1\n\n        data = list(range(len(self.data_source)))\n\n        log.info(f""Chunk sampling with blocksize = {self.block_size}"")\n\n        # Create blocks\n        blocks = [\n            data[i : i + self.block_size] for i in range(0, len(data), self.block_size)\n        ]\n        # shuffle the blocks\n        random.shuffle(blocks)\n        # concatenate the shuffled blocks\n        data[:] = [b for bs in blocks for b in bs]\n\n        if self.epoch_count % self.step == 0:\n            self.block_size += 1\n\n        return iter(data)\n'"
flair/training_utils.py,3,"b'import itertools\nimport random\nimport logging\nfrom collections import defaultdict\nfrom enum import Enum\nfrom math import inf\nfrom pathlib import Path\nfrom typing import Union, List\n\nfrom torch.optim import Optimizer\n\nimport flair\nfrom flair.data import Dictionary, Sentence\nfrom functools import reduce\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr, spearmanr\n\n\nclass Result(object):\n    def __init__(\n        self, main_score: float, log_header: str, log_line: str, detailed_results: str\n    ):\n        self.main_score: float = main_score\n        self.log_header: str = log_header\n        self.log_line: str = log_line\n        self.detailed_results: str = detailed_results\n\n\nclass Metric(object):\n    def __init__(self, name, beta=1):\n        self.name = name\n        self.beta = beta\n\n        self._tps = defaultdict(int)\n        self._fps = defaultdict(int)\n        self._tns = defaultdict(int)\n        self._fns = defaultdict(int)\n\n    def add_tp(self, class_name):\n        self._tps[class_name] += 1\n\n    def add_tn(self, class_name):\n        self._tns[class_name] += 1\n\n    def add_fp(self, class_name):\n        self._fps[class_name] += 1\n\n    def add_fn(self, class_name):\n        self._fns[class_name] += 1\n\n    def get_tp(self, class_name=None):\n        if class_name is None:\n            return sum([self._tps[class_name] for class_name in self.get_classes()])\n        return self._tps[class_name]\n\n    def get_tn(self, class_name=None):\n        if class_name is None:\n            return sum([self._tns[class_name] for class_name in self.get_classes()])\n        return self._tns[class_name]\n\n    def get_fp(self, class_name=None):\n        if class_name is None:\n            return sum([self._fps[class_name] for class_name in self.get_classes()])\n        return self._fps[class_name]\n\n    def get_fn(self, class_name=None):\n        if class_name is None:\n            return sum([self._fns[class_name] for class_name in self.get_classes()])\n        return self._fns[class_name]\n\n    def precision(self, class_name=None):\n        if self.get_tp(class_name) + self.get_fp(class_name) > 0:\n            return (\n                self.get_tp(class_name)\n                / (self.get_tp(class_name) + self.get_fp(class_name))\n            )\n        return 0.0\n\n    def recall(self, class_name=None):\n        if self.get_tp(class_name) + self.get_fn(class_name) > 0:\n            return (\n                self.get_tp(class_name)\n                / (self.get_tp(class_name) + self.get_fn(class_name))\n            )\n        return 0.0\n\n    def f_score(self, class_name=None):\n        if self.precision(class_name) + self.recall(class_name) > 0:\n            return (\n                (1 + self.beta*self.beta)\n                * (self.precision(class_name) * self.recall(class_name))\n                / (self.precision(class_name) * self.beta*self.beta + self.recall(class_name))\n            )\n        return 0.0\n\n    def accuracy(self, class_name=None):\n        if (\n            self.get_tp(class_name) + self.get_fp(class_name) + self.get_fn(class_name) + self.get_tn(class_name)\n            > 0\n        ):\n            return (\n                (self.get_tp(class_name) + self.get_tn(class_name))\n                / (\n                    self.get_tp(class_name)\n                    + self.get_fp(class_name)\n                    + self.get_fn(class_name)\n                    + self.get_tn(class_name)\n                )\n            )\n        return 0.0\n\n    def micro_avg_f_score(self):\n        return self.f_score(None)\n\n    def macro_avg_f_score(self):\n        class_f_scores = [self.f_score(class_name) for class_name in self.get_classes()]\n        if len(class_f_scores) == 0:\n            return 0.0\n        macro_f_score = sum(class_f_scores) / len(class_f_scores)\n        return macro_f_score\n\n    def micro_avg_accuracy(self):\n        return self.accuracy(None)\n\n    def macro_avg_accuracy(self):\n        class_accuracy = [\n            self.accuracy(class_name) for class_name in self.get_classes()\n        ]\n\n        if len(class_accuracy) > 0:\n            return sum(class_accuracy) / len(class_accuracy)\n\n        return 0.0\n\n    def get_classes(self) -> List:\n        all_classes = set(\n            itertools.chain(\n                *[\n                    list(keys)\n                    for keys in [\n                        self._tps.keys(),\n                        self._fps.keys(),\n                        self._tns.keys(),\n                        self._fns.keys(),\n                    ]\n                ]\n            )\n        )\n        all_classes = [\n            class_name for class_name in all_classes if class_name is not None\n        ]\n        all_classes.sort()\n        return all_classes\n\n    def to_tsv(self):\n        return ""{}\\t{}\\t{}\\t{}"".format(\n            self.precision(), self.recall(), self.accuracy(), self.micro_avg_f_score()\n        )\n\n    @staticmethod\n    def tsv_header(prefix=None):\n        if prefix:\n            return ""{0}_PRECISION\\t{0}_RECALL\\t{0}_ACCURACY\\t{0}_F-SCORE"".format(prefix)\n\n        return ""PRECISION\\tRECALL\\tACCURACY\\tF-SCORE""\n\n    @staticmethod\n    def to_empty_tsv():\n        return ""\\t_\\t_\\t_\\t_""\n\n    def __str__(self):\n        all_classes = self.get_classes()\n        all_classes = [None] + all_classes\n        all_lines = [\n            ""{0:<10}\\ttp: {1} - fp: {2} - fn: {3} - tn: {4} - precision: {5:.4f} - recall: {6:.4f} - accuracy: {7:.4f} - f1-score: {8:.4f}"".format(\n                self.name if class_name is None else class_name,\n                self.get_tp(class_name),\n                self.get_fp(class_name),\n                self.get_fn(class_name),\n                self.get_tn(class_name),\n                self.precision(class_name),\n                self.recall(class_name),\n                self.accuracy(class_name),\n                self.f_score(class_name),\n            )\n            for class_name in all_classes\n        ]\n        return ""\\n"".join(all_lines)\n\n\nclass MetricRegression(object):\n    def __init__(self, name):\n        self.name = name\n\n        self.true = []\n        self.pred = []\n\n    def mean_squared_error(self):\n        return mean_squared_error(self.true, self.pred)\n\n    def mean_absolute_error(self):\n        return mean_absolute_error(self.true, self.pred)\n\n    def pearsonr(self):\n        return pearsonr(self.true, self.pred)[0]\n\n    def spearmanr(self):\n        return spearmanr(self.true, self.pred)[0]\n\n    ## dummy return to fulfill trainer.train() needs\n    def micro_avg_f_score(self):\n        return self.mean_squared_error()\n\n    def to_tsv(self):\n        return ""{}\\t{}\\t{}\\t{}"".format(\n            self.mean_squared_error(),\n            self.mean_absolute_error(),\n            self.pearsonr(),\n            self.spearmanr(),\n        )\n\n    @staticmethod\n    def tsv_header(prefix=None):\n        if prefix:\n            return ""{0}_MEAN_SQUARED_ERROR\\t{0}_MEAN_ABSOLUTE_ERROR\\t{0}_PEARSON\\t{0}_SPEARMAN"".format(\n                prefix\n            )\n\n        return ""MEAN_SQUARED_ERROR\\tMEAN_ABSOLUTE_ERROR\\tPEARSON\\tSPEARMAN""\n\n    @staticmethod\n    def to_empty_tsv():\n        return ""\\t_\\t_\\t_\\t_""\n\n    def __str__(self):\n        line = ""mean squared error: {0:.4f} - mean absolute error: {1:.4f} - pearson: {2:.4f} - spearman: {3:.4f}"".format(\n            self.mean_squared_error(),\n            self.mean_absolute_error(),\n            self.pearsonr(),\n            self.spearmanr(),\n        )\n        return line\n\n\nclass EvaluationMetric(Enum):\n    MICRO_ACCURACY = ""micro-average accuracy""\n    MICRO_F1_SCORE = ""micro-average f1-score""\n    MACRO_ACCURACY = ""macro-average accuracy""\n    MACRO_F1_SCORE = ""macro-average f1-score""\n    MEAN_SQUARED_ERROR = ""mean squared error""\n\n\nclass WeightExtractor(object):\n    def __init__(self, directory: Union[str, Path], number_of_weights: int = 10):\n        if type(directory) is str:\n            directory = Path(directory)\n        self.weights_file = init_output_file(directory, ""weights.txt"")\n        self.weights_dict = defaultdict(lambda: defaultdict(lambda: list()))\n        self.number_of_weights = number_of_weights\n\n    def extract_weights(self, state_dict, iteration):\n        for key in state_dict.keys():\n\n            vec = state_dict[key]\n            weights_to_watch = min(\n                self.number_of_weights, reduce(lambda x, y: x * y, list(vec.size()))\n            )\n\n            if key not in self.weights_dict:\n                self._init_weights_index(key, state_dict, weights_to_watch)\n\n            for i in range(weights_to_watch):\n                vec = state_dict[key]\n                for index in self.weights_dict[key][i]:\n                    vec = vec[index]\n\n                value = vec.item()\n\n                with open(self.weights_file, ""a"") as f:\n                    f.write(""{}\\t{}\\t{}\\t{}\\n"".format(iteration, key, i, float(value)))\n\n    def _init_weights_index(self, key, state_dict, weights_to_watch):\n        indices = {}\n\n        i = 0\n        while len(indices) < weights_to_watch:\n            vec = state_dict[key]\n            cur_indices = []\n\n            for x in range(len(vec.size())):\n                index = random.randint(0, len(vec) - 1)\n                vec = vec[index]\n                cur_indices.append(index)\n\n            if cur_indices not in list(indices.values()):\n                indices[i] = cur_indices\n                i += 1\n\n        self.weights_dict[key] = indices\n\n\nclass AnnealOnPlateau(object):\n    """"""This class is a modification of\n    torch.optim.lr_scheduler.ReduceLROnPlateau that enables\n    setting an ""auxiliary metric"" to break ties.\n\n    Reduce learning rate when a metric has stopped improving.\n    Models often benefit from reducing the learning rate by a factor\n    of 2-10 once learning stagnates. This scheduler reads a metrics\n    quantity and if no improvement is seen for a \'patience\' number\n    of epochs, the learning rate is reduced.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        mode (str): One of `min`, `max`. In `min` mode, lr will\n            be reduced when the quantity monitored has stopped\n            decreasing; in `max` mode it will be reduced when the\n            quantity monitored has stopped increasing. Default: \'min\'.\n        factor (float): Factor by which the learning rate will be\n            reduced. new_lr = lr * factor. Default: 0.1.\n        patience (int): Number of epochs with no improvement after\n            which learning rate will be reduced. For example, if\n            `patience = 2`, then we will ignore the first 2 epochs\n            with no improvement, and will only decrease the LR after the\n            3rd epoch if the loss still hasn\'t improved then.\n            Default: 10.\n        verbose (bool): If ``True``, prints a message to stdout for\n            each update. Default: ``False``.\n        cooldown (int): Number of epochs to wait before resuming\n            normal operation after lr has been reduced. Default: 0.\n        min_lr (float or list): A scalar or a list of scalars. A\n            lower bound on the learning rate of all param groups\n            or each group respectively. Default: 0.\n        eps (float): Minimal decay applied to lr. If the difference\n            between new and old lr is smaller than eps, the update is\n            ignored. Default: 1e-8.\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = ReduceLROnPlateau(optimizer, \'min\')\n        >>> for epoch in range(10):\n        >>>     train(...)\n        >>>     val_loss = validate(...)\n        >>>     # Note that step should be called after validate()\n        >>>     scheduler.step(val_loss)\n    """"""\n\n    def __init__(self, optimizer, mode=\'min\', aux_mode=\'min\', factor=0.1, patience=10, initial_extra_patience=0,\n                 verbose=False, cooldown=0, min_lr=0, eps=1e-8):\n\n        if factor >= 1.0:\n            raise ValueError(\'Factor should be < 1.0.\')\n        self.factor = factor\n\n        # Attach optimizer\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError(\'{} is not an Optimizer\'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(min_lr, list) or isinstance(min_lr, tuple):\n            if len(min_lr) != len(optimizer.param_groups):\n                raise ValueError(""expected {} min_lrs, got {}"".format(\n                    len(optimizer.param_groups), len(min_lr)))\n            self.min_lrs = list(min_lr)\n        else:\n            self.min_lrs = [min_lr] * len(optimizer.param_groups)\n\n        self.default_patience = patience\n        self.effective_patience = patience + initial_extra_patience\n        self.verbose = verbose\n        self.cooldown = cooldown\n        self.cooldown_counter = 0\n        self.mode = mode\n        self.aux_mode = aux_mode\n        self.best = None\n        self.best_aux = None\n        self.num_bad_epochs = None\n        self.mode_worse = None  # the worse value for the chosen mode\n        self.eps = eps\n        self.last_epoch = 0\n        self._init_is_better(mode=mode)\n        self._reset()\n\n    def _reset(self):\n        """"""Resets num_bad_epochs counter and cooldown counter.""""""\n        self.best = self.mode_worse\n        self.cooldown_counter = 0\n        self.num_bad_epochs = 0\n\n    def step(self, metric, auxiliary_metric = None):\n        # convert `metrics` to float, in case it\'s a zero-dim Tensor\n        current = float(metric)\n        epoch = self.last_epoch + 1\n        self.last_epoch = epoch\n\n        is_better = False\n\n        if self.mode == \'min\':\n            if current < self.best:\n                is_better = True\n\n        if self.mode == \'max\':\n            if current > self.best:\n                is_better = True\n\n        if current == self.best and auxiliary_metric:\n            current_aux = float(auxiliary_metric)\n            if self.aux_mode == \'min\':\n                if current_aux < self.best_aux:\n                    is_better = True\n\n            if self.aux_mode == \'max\':\n                if current_aux > self.best_aux:\n                    is_better = True\n\n        if is_better:\n            self.best = current\n            if auxiliary_metric:\n                self.best_aux = auxiliary_metric\n            self.num_bad_epochs = 0\n        else:\n            self.num_bad_epochs += 1\n\n        if self.in_cooldown:\n            self.cooldown_counter -= 1\n            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n\n        if self.num_bad_epochs > self.effective_patience:\n            self._reduce_lr(epoch)\n            self.cooldown_counter = self.cooldown\n            self.num_bad_epochs = 0\n            self.effective_patience = self.default_patience\n\n        self._last_lr = [group[\'lr\'] for group in self.optimizer.param_groups]\n\n    def _reduce_lr(self, epoch):\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            old_lr = float(param_group[\'lr\'])\n            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n            if old_lr - new_lr > self.eps:\n                param_group[\'lr\'] = new_lr\n                if self.verbose:\n                    print(\'Epoch {:5d}: reducing learning rate\'\n                          \' of group {} to {:.4e}.\'.format(epoch, i, new_lr))\n\n    @property\n    def in_cooldown(self):\n        return self.cooldown_counter > 0\n\n    def _init_is_better(self, mode):\n        if mode not in {\'min\', \'max\'}:\n            raise ValueError(\'mode \' + mode + \' is unknown!\')\n\n        if mode == \'min\':\n            self.mode_worse = inf\n        else:  # mode == \'max\':\n            self.mode_worse = -inf\n\n        self.mode = mode\n\n    def state_dict(self):\n        return {key: value for key, value in self.__dict__.items() if key != \'optimizer\'}\n\n    def load_state_dict(self, state_dict):\n        self.__dict__.update(state_dict)\n        self._init_is_better(mode=self.mode, threshold=self.threshold, threshold_mode=self.threshold_mode)\n\n\ndef init_output_file(base_path: Union[str, Path], file_name: str) -> Path:\n    """"""\n    Creates a local file.\n    :param base_path: the path to the directory\n    :param file_name: the file name\n    :return: the created file\n    """"""\n    if type(base_path) is str:\n        base_path = Path(base_path)\n    base_path.mkdir(parents=True, exist_ok=True)\n\n    file = base_path / file_name\n    open(file, ""w"", encoding=""utf-8"").close()\n    return file\n\n\ndef convert_labels_to_one_hot(\n    label_list: List[List[str]], label_dict: Dictionary\n) -> List[List[int]]:\n    """"""\n    Convert list of labels (strings) to a one hot list.\n    :param label_list: list of labels\n    :param label_dict: label dictionary\n    :return: converted label list\n    """"""\n    return [\n        [1 if l in labels else 0 for l in label_dict.get_items()]\n        for labels in label_list\n    ]\n\n\ndef log_line(log):\n    log.info(""-"" * 100)\n\n\ndef add_file_handler(log, output_file):\n    init_output_file(output_file.parents[0], output_file.name)\n    fh = logging.FileHandler(output_file, mode=""w"", encoding=""utf-8"")\n    fh.setLevel(logging.INFO)\n    formatter = logging.Formatter(""%(asctime)-15s %(message)s"")\n    fh.setFormatter(formatter)\n    log.addHandler(fh)\n    return fh\n\n\ndef store_embeddings(sentences: List[Sentence], storage_mode: str):\n\n    # if memory mode option \'none\' delete everything\n    if storage_mode == ""none"":\n        for sentence in sentences:\n            sentence.clear_embeddings()\n\n    # else delete only dynamic embeddings (otherwise autograd will keep everything in memory)\n    else:\n        # find out which ones are dynamic embeddings\n        delete_keys = []\n        if type(sentences[0]) == Sentence:\n            for name, vector in sentences[0][0]._embeddings.items():\n                if sentences[0][0]._embeddings[name].requires_grad:\n                    delete_keys.append(name)\n\n        # find out which ones are dynamic embeddings\n        for sentence in sentences:\n            sentence.clear_embeddings(delete_keys)\n\n    # memory management - option 1: send everything to CPU (pin to memory if we train on GPU)\n    if storage_mode == ""cpu"":\n        pin_memory = False if str(flair.device) == ""cpu"" else True\n        for sentence in sentences:\n            sentence.to(""cpu"", pin_memory=pin_memory)\n\n    # record current embedding storage mode to allow optimization (for instance in FlairEmbeddings class)\n    flair.embedding_storage_mode = storage_mode\n'"
tests/conftest.py,0,"b'import pytest\nfrom pathlib import Path\n\n\n@pytest.fixture(scope=""module"")\ndef resources_path():\n    return Path(__file__).parent / ""resources""\n\n\n@pytest.fixture(scope=""module"")\ndef tasks_base_path(resources_path):\n    return resources_path / ""tasks""\n\n\n@pytest.fixture(scope=""module"")\ndef results_base_path(resources_path):\n    return resources_path / ""results""\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        ""--runslow"", action=""store_true"", default=False, help=""run slow tests""\n    )\n    parser.addoption(\n        ""--runintegration"",\n        action=""store_true"",\n        default=False,\n        help=""run integration tests"",\n    )\n\n\ndef pytest_collection_modifyitems(config, items):\n    if config.getoption(""--runslow"") and config.getoption(""--runintegration""):\n        return\n\n    if not config.getoption(""--runslow""):\n        skip_slow = pytest.mark.skip(reason=""need --runslow option to run"")\n        for item in items:\n            if ""slow"" in item.keywords:\n                item.add_marker(skip_slow)\n\n    if not config.getoption(""--runintegration""):\n        skip_integration = pytest.mark.skip(\n            reason=""need --runintegration option to run""\n        )\n        for item in items:\n            if ""integration"" in item.keywords:\n                item.add_marker(skip_integration)\n'"
tests/test_data.py,0,"b'import os\nfrom typing import List\n\nimport pytest\n\nimport flair.datasets\nfrom flair.data import (\n    Sentence,\n    Label,\n    Token,\n    Dictionary,\n    Corpus,\n    Span,\n    segtok_tokenizer,\n    build_japanese_tokenizer\n)\n\n\ndef test_get_head():\n    token1 = Token(""I"", 0)\n    token2 = Token(""love"", 1, 0)\n    token3 = Token(""Berlin"", 2, 1)\n\n    sentence: Sentence = Sentence()\n    sentence.add_token(token1)\n    sentence.add_token(token2)\n    sentence.add_token(token3)\n\n    assert token2 == token3.get_head()\n    assert token1 == token2.get_head()\n    assert None == token1.get_head()\n\n\ndef test_create_sentence_on_empty_string():\n\n    sentence: Sentence = Sentence("""")\n    assert 0 == len(sentence.tokens)\n\n\ndef test_create_sentence_without_tokenizer():\n    sentence: Sentence = Sentence(""I love Berlin."")\n\n    assert 3 == len(sentence.tokens)\n    assert ""I"" == sentence.tokens[0].text\n    assert ""love"" == sentence.tokens[1].text\n    assert ""Berlin."" == sentence.tokens[2].text\n\n\n# skip because it is optional https://github.com/flairNLP/flair/pull/1296\n# def test_create_sentence_using_japanese_tokenizer():\n#     sentence: Sentence = Sentence(""\xe7\xa7\x81\xe3\x81\xaf\xe3\x83\x99\xe3\x83\xab\xe3\x83\xaa\xe3\x83\xb3\xe3\x81\x8c\xe5\xa5\xbd\xe3\x81\x8d"", use_tokenizer=build_japanese_tokenizer())\n#\n#     assert 5 == len(sentence.tokens)\n#     assert ""\xe7\xa7\x81"" == sentence.tokens[0].text\n#     assert ""\xe3\x81\xaf"" == sentence.tokens[1].text\n#     assert ""\xe3\x83\x99\xe3\x83\xab\xe3\x83\xaa\xe3\x83\xb3"" == sentence.tokens[2].text\n#     assert ""\xe3\x81\x8c"" == sentence.tokens[3].text\n#     assert ""\xe5\xa5\xbd\xe3\x81\x8d"" == sentence.tokens[4].text\n\n\ndef test_problem_sentences():\n    text = ""so out of the norm \xe2\x9d\xa4 \xef\xb8\x8f enjoyed every moment\xef\xb8\x8f""\n    sentence = Sentence(text)\n    assert len(sentence) == 9\n\n    text= ""equivalently , accumulating the logs as :( 6 ) sl = 1N \xe2\x88\x91 t = 1Nlogp ( Ll | xt \xe2\x80\x8b , \xce\xb8 ) where "" \\\n          ""p ( Ll | xt \xe2\x80\x8b , \xce\xb8 ) represents the class probability output""\n    sentence = Sentence(text)\n    assert len(sentence) == 37\n\n    text = ""This guy needs his own show on Discivery Channel ! \xef\xbb\xbf""\n    sentence = Sentence(text)\n    assert len(sentence) == 10\n\n    text = ""n\'t have new vintages.""\n    sentence = Sentence(text, use_tokenizer=True)\n    assert len(sentence) == 5\n\n\ndef test_token_indices():\n\n    text = "":    nation on""\n    sentence = Sentence(text)\n    assert text == sentence.to_original_text()\n\n    text = "":    nation on""\n    sentence = Sentence(text, use_tokenizer=segtok_tokenizer)\n    assert text == sentence.to_original_text()\n\n    text = ""I love Berlin.""\n    sentence = Sentence(text)\n    assert text == sentence.to_original_text()\n\n    text = \'Schartau sagte dem "" Tagesspiegel "" vom Freitag , Fischer sei "" in einer Weise aufgetreten , die alles andere als \xc3\xbcberzeugend war "" .\'\n    sentence = Sentence(text)\n    assert text == sentence.to_original_text()\n\n    text = \'Schartau sagte dem "" Tagesspiegel "" vom Freitag , Fischer sei "" in einer Weise aufgetreten , die alles andere als \xc3\xbcberzeugend war "" .\'\n    sentence = Sentence(text, use_tokenizer=segtok_tokenizer)\n    assert text == sentence.to_original_text()\n\n\ndef test_create_sentence_with_tokenizer():\n    sentence: Sentence = Sentence(""I love Berlin."", use_tokenizer=segtok_tokenizer)\n\n    assert 4 == len(sentence.tokens)\n    assert ""I"" == sentence.tokens[0].text\n    assert ""love"" == sentence.tokens[1].text\n    assert ""Berlin"" == sentence.tokens[2].text\n    assert ""."" == sentence.tokens[3].text\n\n\ndef test_sentence_to_plain_string():\n    sentence: Sentence = Sentence(""I love Berlin."", use_tokenizer=segtok_tokenizer)\n\n    assert ""I love Berlin ."" == sentence.to_tokenized_string()\n\n\ndef test_sentence_to_real_string(tasks_base_path):\n    sentence: Sentence = Sentence(""I love Berlin."", use_tokenizer=segtok_tokenizer)\n    assert ""I love Berlin."" == sentence.to_plain_string()\n\n    corpus = flair.datasets.GERMEVAL_14(base_path=tasks_base_path)\n\n    sentence = corpus.train[0]\n    sentence.infer_space_after()\n    assert (\n        \'Schartau sagte dem "" Tagesspiegel "" vom Freitag , Fischer sei "" in einer Weise aufgetreten , die alles andere als \xc3\xbcberzeugend war "" .\'\n        == sentence.to_tokenized_string()\n    )\n    assert (\n        \'Schartau sagte dem ""Tagesspiegel"" vom Freitag, Fischer sei ""in einer Weise aufgetreten, die alles andere als \xc3\xbcberzeugend war"".\'\n        == sentence.to_plain_string()\n    )\n\n    sentence = corpus.train[1]\n    sentence.infer_space_after()\n    assert (\n        ""Firmengr\xc3\xbcnder Wolf Peter Bree arbeitete Anfang der siebziger Jahre als M\xc3\xb6belvertreter , als er einen fliegenden H\xc3\xa4ndler aus dem Libanon traf .""\n        == sentence.to_tokenized_string()\n    )\n    assert (\n        ""Firmengr\xc3\xbcnder Wolf Peter Bree arbeitete Anfang der siebziger Jahre als M\xc3\xb6belvertreter, als er einen fliegenden H\xc3\xa4ndler aus dem Libanon traf.""\n        == sentence.to_plain_string()\n    )\n\n\ndef test_sentence_infer_tokenization():\n    sentence: Sentence = Sentence()\n    sentence.add_token(Token(""xyz""))\n    sentence.add_token(Token(\'""\'))\n    sentence.add_token(Token(""abc""))\n    sentence.add_token(Token(\'""\'))\n    sentence.infer_space_after()\n\n    assert \'xyz "" abc ""\' == sentence.to_tokenized_string()\n    assert \'xyz ""abc""\' == sentence.to_plain_string()\n\n    sentence: Sentence = Sentence(\'xyz "" abc ""\')\n    sentence.infer_space_after()\n    assert \'xyz "" abc ""\' == sentence.to_tokenized_string()\n    assert \'xyz ""abc""\' == sentence.to_plain_string()\n\n\ndef test_sentence_get_item():\n    sentence: Sentence = Sentence(""I love Berlin."", use_tokenizer=segtok_tokenizer)\n\n    assert sentence.get_token(1) == sentence[0]\n    assert sentence.get_token(3) == sentence[2]\n\n    with pytest.raises(IndexError):\n        token = sentence[4]\n\n\ndef test_sentence_whitespace_tokenization():\n    sentence: Sentence = Sentence(""I  love Berlin ."")\n\n    assert 4 == len(sentence.tokens)\n    assert ""I"" == sentence.get_token(1).text\n    assert ""love"" == sentence.get_token(2).text\n    assert ""Berlin"" == sentence.get_token(3).text\n    assert ""."" == sentence.get_token(4).text\n\n\ndef test_sentence_to_tagged_string():\n    token1 = Token(""I"", 0)\n    token2 = Token(""love"", 1, 0)\n    token3 = Token(""Berlin"", 2, 1)\n    token3.add_tag(""ner"", ""LOC"")\n\n    sentence: Sentence = Sentence()\n    sentence.add_token(token1)\n    sentence.add_token(token2)\n    sentence.add_token(token3)\n\n    assert ""I love Berlin <LOC>"" == sentence.to_tagged_string()\n\n\ndef test_sentence_add_token():\n    token1: Token = Token(""Munich"")\n    token2: Token = Token(""and"")\n    token3: Token = Token(""Berlin"")\n    token4: Token = Token(""are"")\n    token5: Token = Token(""nice"")\n\n    sentence: Sentence = Sentence()\n\n    sentence.add_token(token1)\n    sentence.add_token(token2)\n    sentence.add_token(token3)\n    sentence.add_token(token4)\n    sentence.add_token(token5)\n\n    sentence.add_token(""cities"")\n    sentence.add_token(Token("".""))\n\n    assert ""Munich and Berlin are nice cities ."" == sentence.to_tokenized_string()\n\n\ndef test_dictionary_get_items_with_unk():\n    dictionary: Dictionary = Dictionary()\n\n    dictionary.add_item(""class_1"")\n    dictionary.add_item(""class_2"")\n    dictionary.add_item(""class_3"")\n\n    items = dictionary.get_items()\n\n    assert 4 == len(items)\n    assert ""<unk>"" == items[0]\n    assert ""class_1"" == items[1]\n    assert ""class_2"" == items[2]\n    assert ""class_3"" == items[3]\n\n\ndef test_dictionary_get_items_without_unk():\n    dictionary: Dictionary = Dictionary(add_unk=False)\n\n    dictionary.add_item(""class_1"")\n    dictionary.add_item(""class_2"")\n    dictionary.add_item(""class_3"")\n\n    items = dictionary.get_items()\n\n    assert 3 == len(items)\n    assert ""class_1"" == items[0]\n    assert ""class_2"" == items[1]\n    assert ""class_3"" == items[2]\n\n\ndef test_dictionary_get_idx_for_item():\n    dictionary: Dictionary = Dictionary(add_unk=False)\n\n    dictionary.add_item(""class_1"")\n    dictionary.add_item(""class_2"")\n    dictionary.add_item(""class_3"")\n\n    idx = dictionary.get_idx_for_item(""class_2"")\n\n    assert 1 == idx\n\n\ndef test_dictionary_get_item_for_index():\n    dictionary: Dictionary = Dictionary(add_unk=False)\n\n    dictionary.add_item(""class_1"")\n    dictionary.add_item(""class_2"")\n    dictionary.add_item(""class_3"")\n\n    item = dictionary.get_item_for_index(0)\n\n    assert ""class_1"" == item\n\n\ndef test_dictionary_save_and_load():\n    dictionary: Dictionary = Dictionary(add_unk=False)\n\n    dictionary.add_item(""class_1"")\n    dictionary.add_item(""class_2"")\n    dictionary.add_item(""class_3"")\n\n    file_path = ""dictionary.txt""\n\n    dictionary.save(file_path)\n    loaded_dictionary = dictionary.load_from_file(file_path)\n\n    assert len(dictionary) == len(loaded_dictionary)\n    assert len(dictionary.get_items()) == len(loaded_dictionary.get_items())\n\n    # clean up file\n    os.remove(file_path)\n\n\ndef test_tagged_corpus_get_all_sentences():\n    train_sentence = Sentence(""I\'m used in training."", use_tokenizer=segtok_tokenizer)\n    dev_sentence = Sentence(""I\'m a dev sentence."", use_tokenizer=segtok_tokenizer)\n    test_sentence = Sentence(\n        ""I will be only used for testing."", use_tokenizer=segtok_tokenizer\n    )\n\n    corpus: Corpus = Corpus([train_sentence], [dev_sentence], [test_sentence])\n\n    all_sentences = corpus.get_all_sentences()\n\n    assert 3 == len(all_sentences)\n\n\ndef test_tagged_corpus_make_vocab_dictionary():\n    train_sentence = Sentence(\n        ""used in training. training is cool."", use_tokenizer=segtok_tokenizer\n    )\n\n    corpus: Corpus = Corpus([train_sentence], [], [])\n\n    vocab = corpus.make_vocab_dictionary(max_tokens=2, min_freq=-1)\n\n    assert 3 == len(vocab)\n    assert ""<unk>"" in vocab.get_items()\n    assert ""training"" in vocab.get_items()\n    assert ""."" in vocab.get_items()\n\n    vocab = corpus.make_vocab_dictionary(max_tokens=-1, min_freq=-1)\n\n    assert 7 == len(vocab)\n\n    vocab = corpus.make_vocab_dictionary(max_tokens=-1, min_freq=2)\n\n    assert 3 == len(vocab)\n    assert ""<unk>"" in vocab.get_items()\n    assert ""training"" in vocab.get_items()\n    assert ""."" in vocab.get_items()\n\n\ndef test_label_set_confidence():\n    label = Label(""class_1"", 3.2)\n\n    assert 1.0 == label.score\n    assert ""class_1"" == label.value\n\n    label.score = 0.2\n\n    assert 0.2 == label.score\n\n\ndef test_tagged_corpus_make_label_dictionary():\n    sentence_1 = Sentence(""sentence 1"").add_label(\'label\', \'class_1\')\n\n    sentence_2 = Sentence(""sentence 2"").add_label(\'label\', \'class_2\')\n\n    sentence_3 = Sentence(""sentence 3"").add_label(\'label\', \'class_1\')\n\n    corpus: Corpus = Corpus([sentence_1, sentence_2, sentence_3], [], [])\n\n    label_dict = corpus.make_label_dictionary(\'label\')\n\n    assert 2 == len(label_dict)\n    assert ""<unk>"" not in label_dict.get_items()\n    assert ""class_1"" in label_dict.get_items()\n    assert ""class_2"" in label_dict.get_items()\n\n\ndef test_tagged_corpus_statistics():\n\n    train_sentence = Sentence(""I love Berlin."", use_tokenizer=True).add_label(\'label\', \'class_1\')\n\n    dev_sentence = Sentence(""The sun is shining."", use_tokenizer=True).add_label(\'label\', \'class_2\')\n\n    test_sentence = Sentence(""Berlin is sunny."", use_tokenizer=True).add_label(\'label\', \'class_1\')\n\n    class_to_count_dict = Corpus._count_sentence_labels(\n        [train_sentence, dev_sentence, test_sentence]\n    )\n\n    assert ""class_1"" in class_to_count_dict\n    assert ""class_2"" in class_to_count_dict\n    assert 2 == class_to_count_dict[""class_1""]\n    assert 1 == class_to_count_dict[""class_2""]\n\n    tokens_in_sentences = Corpus._get_tokens_per_sentence(\n        [train_sentence, dev_sentence, test_sentence]\n    )\n\n    assert 3 == len(tokens_in_sentences)\n    assert 4 == tokens_in_sentences[0]\n    assert 5 == tokens_in_sentences[1]\n    assert 4 == tokens_in_sentences[2]\n\n\ndef test_tagged_corpus_statistics_multi_label():\n\n    train_sentence = Sentence(""I love Berlin."", use_tokenizer=True).add_label(\'label\', \'class_1\')\n\n    dev_sentence = Sentence(""The sun is shining."", use_tokenizer=True).add_label(\'label\', \'class_2\')\n\n    test_sentence = Sentence(""Berlin is sunny."", use_tokenizer=True)\n    test_sentence.add_label(\'label\', \'class_1\')\n    test_sentence.add_label(\'label\', \'class_2\')\n\n    class_to_count_dict = Corpus._count_sentence_labels(\n        [train_sentence, dev_sentence, test_sentence]\n    )\n\n    assert ""class_1"" in class_to_count_dict\n    assert ""class_2"" in class_to_count_dict\n    assert 2 == class_to_count_dict[""class_1""]\n    assert 2 == class_to_count_dict[""class_2""]\n\n    tokens_in_sentences = Corpus._get_tokens_per_sentence(\n        [train_sentence, dev_sentence, test_sentence]\n    )\n\n    assert 3 == len(tokens_in_sentences)\n    assert 4 == tokens_in_sentences[0]\n    assert 5 == tokens_in_sentences[1]\n    assert 4 == tokens_in_sentences[2]\n\n\ndef test_tagged_corpus_get_tag_statistic():\n    train_sentence = Sentence(""Zalando Research is located in Berlin ."")\n    train_sentence[0].add_tag(""ner"", ""B-ORG"")\n    train_sentence[1].add_tag(""ner"", ""E-ORG"")\n    train_sentence[5].add_tag(""ner"", ""S-LOC"")\n\n    dev_sentence = Sentence(\n        ""Facebook, Inc. is a company, and Google is one as well."",\n        use_tokenizer=segtok_tokenizer,\n    )\n    dev_sentence[0].add_tag(""ner"", ""B-ORG"")\n    dev_sentence[1].add_tag(""ner"", ""I-ORG"")\n    dev_sentence[2].add_tag(""ner"", ""E-ORG"")\n    dev_sentence[8].add_tag(""ner"", ""S-ORG"")\n\n    test_sentence = Sentence(""Nothing to do with companies."")\n\n    tag_to_count_dict = Corpus._count_token_labels(\n        [train_sentence, dev_sentence, test_sentence], ""ner""\n    )\n\n    assert 1 == tag_to_count_dict[""S-ORG""]\n    assert 1 == tag_to_count_dict[""S-LOC""]\n    assert 2 == tag_to_count_dict[""B-ORG""]\n    assert 2 == tag_to_count_dict[""E-ORG""]\n    assert 1 == tag_to_count_dict[""I-ORG""]\n\n\ndef test_tagged_corpus_downsample():\n\n    sentence = Sentence(""I love Berlin."", use_tokenizer=True).add_label(\'label\', \'class_1\')\n\n    corpus: Corpus = Corpus(\n        [\n            sentence,\n            sentence,\n            sentence,\n            sentence,\n            sentence,\n            sentence,\n            sentence,\n            sentence,\n            sentence,\n            sentence,\n        ],\n        [],\n        [],\n    )\n\n    assert 10 == len(corpus.train)\n\n    corpus.downsample(percentage=0.3, downsample_dev=False, downsample_test=False)\n\n    assert 3 == len(corpus.train)\n\n\ndef test_spans():\n    sentence = Sentence(""Zalando Research is located in Berlin ."")\n\n    # bioes tags\n    sentence[0].add_tag(""ner"", ""B-ORG"")\n    sentence[1].add_tag(""ner"", ""E-ORG"")\n    sentence[5].add_tag(""ner"", ""S-LOC"")\n\n    spans: List[Span] = sentence.get_spans(""ner"")\n\n    assert 2 == len(spans)\n    assert ""Zalando Research"" == spans[0].text\n    assert ""ORG"" == spans[0].tag\n    assert ""Berlin"" == spans[1].text\n    assert ""LOC"" == spans[1].tag\n\n    # bio tags\n    sentence[0].add_tag(""ner"", ""B-ORG"")\n    sentence[1].add_tag(""ner"", ""I-ORG"")\n    sentence[5].add_tag(""ner"", ""B-LOC"")\n\n    spans: List[Span] = sentence.get_spans(""ner"")\n\n    assert ""Zalando Research"" == spans[0].text\n    assert ""ORG"" == spans[0].tag\n    assert ""Berlin"" == spans[1].text\n    assert ""LOC"" == spans[1].tag\n\n    # broken tags\n    sentence[0].add_tag(""ner"", ""I-ORG"")\n    sentence[1].add_tag(""ner"", ""E-ORG"")\n    sentence[5].add_tag(""ner"", ""I-LOC"")\n\n    spans: List[Span] = sentence.get_spans(""ner"")\n\n    assert ""Zalando Research"" == spans[0].text\n    assert ""ORG"" == spans[0].tag\n    assert ""Berlin"" == spans[1].text\n    assert ""LOC"" == spans[1].tag\n\n    # all tags\n    sentence[0].add_tag(""ner"", ""I-ORG"")\n    sentence[1].add_tag(""ner"", ""E-ORG"")\n    sentence[2].add_tag(""ner"", ""aux"")\n    sentence[3].add_tag(""ner"", ""verb"")\n    sentence[4].add_tag(""ner"", ""preposition"")\n    sentence[5].add_tag(""ner"", ""I-LOC"")\n\n    spans: List[Span] = sentence.get_spans(""ner"")\n    assert 5 == len(spans)\n    assert ""Zalando Research"" == spans[0].text\n    assert ""ORG"" == spans[0].tag\n    assert ""Berlin"" == spans[4].text\n    assert ""LOC"" == spans[4].tag\n\n    # all weird tags\n    sentence[0].add_tag(""ner"", ""I-ORG"")\n    sentence[1].add_tag(""ner"", ""S-LOC"")\n    sentence[2].add_tag(""ner"", ""aux"")\n    sentence[3].add_tag(""ner"", ""B-relation"")\n    sentence[4].add_tag(""ner"", ""E-preposition"")\n    sentence[5].add_tag(""ner"", ""S-LOC"")\n\n    spans: List[Span] = sentence.get_spans(""ner"")\n    assert 5 == len(spans)\n    assert ""Zalando"" == spans[0].text\n    assert ""ORG"" == spans[0].tag\n    assert ""Research"" == spans[1].text\n    assert ""LOC"" == spans[1].tag\n    assert ""located in"" == spans[3].text\n    assert ""relation"" == spans[3].tag\n\n    sentence = Sentence(\n        ""A woman was charged on Friday with terrorist offences after three Irish Republican Army mortar bombs were found in a Belfast house , police said . ""\n    )\n    sentence[11].add_tag(""ner"", ""S-MISC"")\n    sentence[12].add_tag(""ner"", ""B-MISC"")\n    sentence[13].add_tag(""ner"", ""E-MISC"")\n    spans: List[Span] = sentence.get_spans(""ner"")\n    assert 2 == len(spans)\n    assert ""Irish"" == spans[0].text\n    assert ""Republican Army"" == spans[1].text\n\n    sentence = Sentence(""Zalando Research is located in Berlin ."")\n\n    # tags with confidence\n    sentence[0].add_tag(""ner"", ""B-ORG"", 1.0)\n    sentence[1].add_tag(""ner"", ""E-ORG"", 0.9)\n    sentence[5].add_tag(""ner"", ""S-LOC"", 0.5)\n\n    spans: List[Span] = sentence.get_spans(""ner"", min_score=0.0)\n\n    assert 2 == len(spans)\n    assert ""Zalando Research"" == spans[0].text\n    assert ""ORG"" == spans[0].tag\n    assert 0.95 == spans[0].score\n\n    assert ""Berlin"" == spans[1].text\n    assert ""LOC"" == spans[1].tag\n    assert 0.5 == spans[1].score\n\n    spans: List[Span] = sentence.get_spans(""ner"", min_score=0.6)\n    assert 1 == len(spans)\n\n    spans: List[Span] = sentence.get_spans(""ner"", min_score=0.99)\n    assert 0 == len(spans)\n\n\ndef test_token_position_in_sentence():\n    sentence = Sentence(""I love Berlin ."")\n\n    assert 0 == sentence.tokens[0].start_position\n    assert 1 == sentence.tokens[0].end_position\n    assert 2 == sentence.tokens[1].start_position\n    assert 6 == sentence.tokens[1].end_position\n    assert 7 == sentence.tokens[2].start_position\n    assert 13 == sentence.tokens[2].end_position\n\n    sentence = Sentence("" I love  Berlin."", use_tokenizer=segtok_tokenizer)\n\n    assert 1 == sentence.tokens[0].start_position\n    assert 2 == sentence.tokens[0].end_position\n    assert 3 == sentence.tokens[1].start_position\n    assert 7 == sentence.tokens[1].end_position\n    assert 9 == sentence.tokens[2].start_position\n    assert 15 == sentence.tokens[2].end_position\n\n\ndef test_sentence_to_dict():\n    sentence = Sentence(\n        ""Zalando Research is   located in Berlin, the capital of Germany."",\n        use_tokenizer=True,\n    ).add_label(\'class\', \'business\')\n\n    # bioes tags\n    sentence[0].add_tag(""ner"", ""B-ORG"")\n    sentence[1].add_tag(""ner"", ""E-ORG"")\n    sentence[5].add_tag(""ner"", ""S-LOC"")\n    sentence[10].add_tag(""ner"", ""S-LOC"")\n\n    dict = sentence.to_dict(""ner"")\n\n    assert (\n        ""Zalando Research is   located in Berlin, the capital of Germany.""\n        == dict[""text""]\n    )\n    assert ""Zalando Research"" == dict[""entities""][0][""text""]\n    assert ""Berlin"" == dict[""entities""][1][""text""]\n    assert ""Germany"" == dict[""entities""][2][""text""]\n    assert 1 == len(dict[""labels""])\n\n    sentence = Sentence(\n        ""Facebook, Inc. is a company, and Google is one as well."",\n        use_tokenizer=True,\n    )\n\n    # bioes tags\n    sentence[0].add_tag(""ner"", ""B-ORG"")\n    sentence[1].add_tag(""ner"", ""I-ORG"")\n    sentence[2].add_tag(""ner"", ""E-ORG"")\n    sentence[8].add_tag(""ner"", ""S-ORG"")\n\n    dict = sentence.to_dict(""ner"")\n\n    assert ""Facebook, Inc. is a company, and Google is one as well."" == dict[""text""]\n    assert ""Facebook, Inc."" == dict[""entities""][0][""text""]\n    assert ""Google"" == dict[""entities""][1][""text""]\n    assert 0 == len(dict[""labels""])\n'"
tests/test_datasets.py,0,"b'import shutil\nfrom pathlib import Path\n\nimport flair\nimport flair.datasets\nfrom flair.data import MultiCorpus\n\n\ndef test_load_imdb_data(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ClassificationCorpus(\n        tasks_base_path / ""imdb"", memory_mode=\'full\',\n    )\n\n    assert len(corpus.train) == 5\n    assert len(corpus.dev) == 5\n    assert len(corpus.test) == 5\n\n\ndef test_load_imdb_data_streaming(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ClassificationCorpus(\n        tasks_base_path / ""imdb"", memory_mode=\'disk\',\n    )\n\n    assert len(corpus.train) == 5\n    assert len(corpus.dev) == 5\n    assert len(corpus.test) == 5\n\n\ndef test_load_imdb_data_max_tokens(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ClassificationCorpus(\n        tasks_base_path / ""imdb"", memory_mode=\'full\', truncate_to_max_tokens=3\n    )\n\n    assert len(corpus.train[0]) <= 3\n    assert len(corpus.dev[0]) <= 3\n    assert len(corpus.test[0]) <= 3\n\n\ndef test_load_imdb_data_streaming_max_tokens(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ClassificationCorpus(\n        tasks_base_path / ""imdb"", memory_mode=\'full\', truncate_to_max_tokens=3\n    )\n\n    assert len(corpus.train[0]) <= 3\n    assert len(corpus.dev[0]) <= 3\n    assert len(corpus.test[0]) <= 3\n\n\ndef test_load_ag_news_data(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"")\n\n    assert len(corpus.train) == 10\n    assert len(corpus.dev) == 10\n    assert len(corpus.test) == 10\n\n\ndef test_load_sequence_labeling_data(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ColumnCorpus(\n        tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n\n    assert len(corpus.train) == 6\n    assert len(corpus.dev) == 1\n    assert len(corpus.test) == 1\n\n\ndef test_load_sequence_labeling_whitespace_after(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ColumnCorpus(\n        tasks_base_path / ""column_with_whitespaces"", column_format={0: \'text\', 1: \'ner\', 2: \'space-after\'}\n    )\n\n    assert len(corpus.train) == 1\n    assert len(corpus.dev) == 1\n    assert len(corpus.test) == 1\n\n    assert corpus.train[0].to_tokenized_string() == ""It is a German - owned firm .""\n    assert corpus.train[0].to_plain_string() == ""It is a German-owned firm.""\n\n\ndef test_load_column_corpus_options(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ColumnCorpus(\n        tasks_base_path / ""column_corpus_options"",\n        column_format={0: \'text\', 1: \'ner\'},\n        column_delimiter=\'\\t\',\n        skip_first_line=True,\n    )\n\n    assert len(corpus.train) == 1\n    assert len(corpus.dev) == 1\n    assert len(corpus.test) == 1\n\n    assert corpus.train[0].to_tokenized_string() == ""This is New Berlin""\n\ndef test_load_germeval_data(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.GERMEVAL_14(tasks_base_path)\n\n    assert len(corpus.train) == 2\n    assert len(corpus.dev) == 1\n    assert len(corpus.test) == 1\n\n\ndef test_load_ud_english_data(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.UD_ENGLISH(tasks_base_path)\n\n    assert len(corpus.train) == 6\n    assert len(corpus.test) == 4\n    assert len(corpus.dev) == 2\n\n\ndef test_load_no_dev_data(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ColumnCorpus(\n        tasks_base_path / ""fashion_nodev"", column_format={0: ""text"", 2: ""ner""}\n    )\n\n    assert len(corpus.train) == 5\n    assert len(corpus.dev) == 1\n    assert len(corpus.test) == 1\n\n\ndef test_load_no_dev_data_explicit(tasks_base_path):\n    # get training, test and dev data\n    corpus = flair.datasets.ColumnCorpus(\n        tasks_base_path / ""fashion_nodev"",\n        column_format={0: ""text"", 2: ""ner""},\n        train_file=""train.tsv"",\n        test_file=""test.tsv"",\n    )\n\n    assert len(corpus.train) == 5\n    assert len(corpus.dev) == 1\n    assert len(corpus.test) == 1\n\n\ndef test_multi_corpus(tasks_base_path):\n\n    corpus_1 = flair.datasets.GERMEVAL_14(tasks_base_path)\n\n    corpus_2 = flair.datasets.ColumnCorpus(\n        tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n    # get two corpora as one\n    corpus = MultiCorpus([corpus_1, corpus_2])\n\n    assert len(corpus.train) == 8\n    assert len(corpus.dev) == 2\n    assert len(corpus.test) == 2\n\n\ndef test_download_load_data(tasks_base_path):\n    # get training, test and dev data for full English UD corpus from web\n    corpus = flair.datasets.UD_ENGLISH()\n\n    assert len(corpus.train) == 12543\n    assert len(corpus.dev) == 2002\n    assert len(corpus.test) == 2077\n\n    # clean up data directory\n    shutil.rmtree(Path(flair.cache_root) / ""datasets"" / ""ud_english"")\n'"
tests/test_embeddings.py,2,"b'import pytest\nimport torch\n\nfrom flair.embeddings import (\n    WordEmbeddings,\n    TokenEmbeddings,\n    StackedEmbeddings,\n    DocumentPoolEmbeddings,\n    FlairEmbeddings,\n    DocumentRNNEmbeddings,\n    DocumentLMEmbeddings, TransformerWordEmbeddings, TransformerDocumentEmbeddings,\n)\n\nfrom flair.data import Sentence, Dictionary\nfrom flair.models import LanguageModel\n\nglove: TokenEmbeddings = WordEmbeddings(""turian"")\nflair_embedding: TokenEmbeddings = FlairEmbeddings(""news-forward-fast"")\n\n\ndef test_load_non_existing_embedding():\n    with pytest.raises(ValueError):\n        WordEmbeddings(""other"")\n\n    with pytest.raises(ValueError):\n        WordEmbeddings(""not/existing/path/to/embeddings"")\n\n\ndef test_load_non_existing_flair_embedding():\n    with pytest.raises(ValueError):\n        FlairEmbeddings(""other"")\n\n\ndef test_keep_batch_order():\n\n    embeddings = DocumentRNNEmbeddings([glove])\n    sentences_1 = [Sentence(""First sentence""), Sentence(""This is second sentence"")]\n    sentences_2 = [Sentence(""This is second sentence""), Sentence(""First sentence"")]\n\n    embeddings.embed(sentences_1)\n    embeddings.embed(sentences_2)\n\n    assert sentences_1[0].to_original_text() == ""First sentence""\n    assert sentences_1[1].to_original_text() == ""This is second sentence""\n\n    assert torch.norm(sentences_1[0].embedding - sentences_2[1].embedding) == 0.0\n    assert torch.norm(sentences_1[0].embedding - sentences_2[1].embedding) == 0.0\n    del embeddings\n\n\ndef test_stacked_embeddings():\n\n    embeddings: StackedEmbeddings = StackedEmbeddings([glove, flair_embedding])\n\n    sentence: Sentence = Sentence(""I love Berlin. Berlin is a great place to live."")\n    embeddings.embed(sentence)\n\n    for token in sentence.tokens:\n        assert len(token.get_embedding()) == 1074\n\n        token.clear_embeddings()\n\n        assert len(token.get_embedding()) == 0\n    del embeddings\n\n\ndef test_transformer_word_embeddings():\n\n    embeddings = TransformerWordEmbeddings(\'distilbert-base-uncased\')\n\n    sentence: Sentence = Sentence(""I love Berlin"")\n    embeddings.embed(sentence)\n\n    for token in sentence.tokens:\n        assert len(token.get_embedding()) == 3072\n\n        token.clear_embeddings()\n\n        assert len(token.get_embedding()) == 0\n\n    embeddings = TransformerWordEmbeddings(\'distilbert-base-uncased\', layers=\'all\')\n\n    embeddings.embed(sentence)\n\n    for token in sentence.tokens:\n        assert len(token.get_embedding()) == 5376\n\n        token.clear_embeddings()\n\n        assert len(token.get_embedding()) == 0\n    del embeddings\n\n    embeddings = TransformerWordEmbeddings(\'distilbert-base-uncased\', layers=\'all\', use_scalar_mix=True)\n\n    embeddings.embed(sentence)\n\n    for token in sentence.tokens:\n        assert len(token.get_embedding()) == 768\n\n        token.clear_embeddings()\n\n        assert len(token.get_embedding()) == 0\n    del embeddings\n\n\ndef test_transformer_weird_sentences():\n\n    embeddings = TransformerWordEmbeddings(\'distilbert-base-uncased\', layers=\'all\', use_scalar_mix=True)\n\n    sentence = Sentence(""Hybrid mesons , qq \xcc\x84 states with an admixture"")\n    embeddings.embed(sentence)\n    for token in sentence:\n        assert len(token.get_embedding()) == 768\n\n    sentence = Sentence(""typical proportionalities of \xe2\x88\xbc 1nmV \xe2\x88\x92 1 [ 3,4 ] ."")\n    embeddings.embed(sentence)\n    for token in sentence:\n        assert len(token.get_embedding()) == 768\n\n\ndef test_fine_tunable_flair_embedding():\n    language_model_forward = LanguageModel(\n        Dictionary.load(""chars""), is_forward_lm=True, hidden_size=32, nlayers=1\n    )\n\n    embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(\n        [FlairEmbeddings(language_model_forward, fine_tune=True)],\n        hidden_size=128,\n        bidirectional=False,\n    )\n\n    sentence: Sentence = Sentence(""I love Berlin."")\n\n    embeddings.embed(sentence)\n\n    assert len(sentence.get_embedding()) == 128\n    assert len(sentence.get_embedding()) == embeddings.embedding_length\n\n    sentence.clear_embeddings()\n\n    assert len(sentence.get_embedding()) == 0\n\n    embeddings: DocumentLMEmbeddings = DocumentLMEmbeddings(\n        [FlairEmbeddings(language_model_forward, fine_tune=True)]\n    )\n\n    sentence: Sentence = Sentence(""I love Berlin."")\n\n    embeddings.embed(sentence)\n\n    assert len(sentence.get_embedding()) == 32\n    assert len(sentence.get_embedding()) == embeddings.embedding_length\n\n    sentence.clear_embeddings()\n\n    assert len(sentence.get_embedding()) == 0\n    del embeddings\n\n\ndef test_document_lstm_embeddings():\n    sentence: Sentence = Sentence(""I love Berlin. Berlin is a great place to live."")\n\n    embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(\n        [glove, flair_embedding], hidden_size=128, bidirectional=False\n    )\n\n    embeddings.embed(sentence)\n\n    assert len(sentence.get_embedding()) == 128\n    assert len(sentence.get_embedding()) == embeddings.embedding_length\n\n    sentence.clear_embeddings()\n\n    assert len(sentence.get_embedding()) == 0\n    del embeddings\n\n\ndef test_document_bidirectional_lstm_embeddings():\n    sentence: Sentence = Sentence(""I love Berlin. Berlin is a great place to live."")\n\n    embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(\n        [glove, flair_embedding], hidden_size=128, bidirectional=True\n    )\n\n    embeddings.embed(sentence)\n\n    assert len(sentence.get_embedding()) == 512\n    assert len(sentence.get_embedding()) == embeddings.embedding_length\n\n    sentence.clear_embeddings()\n\n    assert len(sentence.get_embedding()) == 0\n    del embeddings\n\n\ndef test_document_pool_embeddings():\n    sentence: Sentence = Sentence(""I love Berlin. Berlin is a great place to live."")\n\n    for mode in [""mean"", ""max"", ""min""]:\n        embeddings: DocumentPoolEmbeddings = DocumentPoolEmbeddings(\n            [glove, flair_embedding], pooling=mode, fine_tune_mode=""none""\n        )\n\n        embeddings.embed(sentence)\n\n        assert len(sentence.get_embedding()) == 1074\n\n        sentence.clear_embeddings()\n\n        assert len(sentence.get_embedding()) == 0\n        del embeddings\n\n\ndef test_document_pool_embeddings_nonlinear():\n    sentence: Sentence = Sentence(""I love Berlin. Berlin is a great place to live."")\n\n    for mode in [""mean"", ""max"", ""min""]:\n        embeddings: DocumentPoolEmbeddings = DocumentPoolEmbeddings(\n            [glove, flair_embedding], pooling=mode, fine_tune_mode=""nonlinear""\n        )\n\n        embeddings.embed(sentence)\n\n        assert len(sentence.get_embedding()) == 1074\n\n        sentence.clear_embeddings()\n\n        assert len(sentence.get_embedding()) == 0\n        del embeddings\n\n\ndef test_transformer_document_embeddings():\n\n    embeddings = TransformerDocumentEmbeddings(\'distilbert-base-uncased\')\n\n    sentence: Sentence = Sentence(""I love Berlin"")\n    embeddings.embed(sentence)\n\n    assert len(sentence.get_embedding()) == 768\n\n    sentence.clear_embeddings()\n\n    assert len(sentence.get_embedding()) == 0\n\n    embeddings = TransformerDocumentEmbeddings(\'distilbert-base-uncased\', layers=\'all\')\n\n    embeddings.embed(sentence)\n\n    assert len(sentence.get_embedding()) == 5376\n\n    sentence.clear_embeddings()\n\n    assert len(sentence.get_embedding()) == 0\n\n    embeddings = TransformerDocumentEmbeddings(\'distilbert-base-uncased\', layers=\'all\', use_scalar_mix=True)\n\n    embeddings.embed(sentence)\n\n    assert len(sentence.get_embedding()) == 768\n\n    sentence.clear_embeddings()\n\n    del embeddings'"
tests/test_hyperparameter.py,1,"b'import shutil\n\nimport pytest\nfrom hyperopt import hp\nfrom torch.optim import SGD\n\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, FlairEmbeddings\nfrom flair.hyperparameter import (\n    SearchSpace,\n    Parameter,\n    SequenceTaggerParamSelector,\n    TextClassifierParamSelector,\n)\nimport flair.datasets\n\nglove_embedding: WordEmbeddings = WordEmbeddings(""glove"")\n\n\ndef test_sequence_tagger_param_selector(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ColumnCorpus(\n        data_folder=tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n\n    # define search space\n    search_space = SearchSpace()\n\n    # sequence tagger parameter\n    search_space.add(\n        Parameter.EMBEDDINGS,\n        hp.choice,\n        options=[StackedEmbeddings([glove_embedding])],\n    )\n    search_space.add(Parameter.USE_CRF, hp.choice, options=[True, False])\n    search_space.add(Parameter.DROPOUT, hp.uniform, low=0.25, high=0.75)\n    search_space.add(Parameter.WORD_DROPOUT, hp.uniform, low=0.0, high=0.25)\n    search_space.add(Parameter.LOCKED_DROPOUT, hp.uniform, low=0.0, high=0.5)\n    search_space.add(Parameter.HIDDEN_SIZE, hp.choice, options=[64, 128])\n    search_space.add(Parameter.RNN_LAYERS, hp.choice, options=[1, 2])\n\n    # model trainer parameter\n    search_space.add(Parameter.OPTIMIZER, hp.choice, options=[SGD])\n\n    # training parameter\n    search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[4, 8, 32])\n    search_space.add(Parameter.LEARNING_RATE, hp.uniform, low=0.01, high=1)\n    search_space.add(Parameter.ANNEAL_FACTOR, hp.uniform, low=0.3, high=0.75)\n    search_space.add(Parameter.PATIENCE, hp.choice, options=[3, 5])\n    search_space.add(Parameter.WEIGHT_DECAY, hp.uniform, low=0.01, high=1)\n\n    # find best parameter settings\n    optimizer = SequenceTaggerParamSelector(\n        corpus, ""ner"", results_base_path, max_epochs=2\n    )\n    optimizer.optimize(search_space, max_evals=2)\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del optimizer, search_space\n\n\n@pytest.mark.integration\ndef test_text_classifier_param_selector(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""imdb"")\n\n    search_space = SearchSpace()\n\n    # document embeddings parameter\n    search_space.add(Parameter.EMBEDDINGS, hp.choice, options=[[glove_embedding]])\n    search_space.add(Parameter.HIDDEN_SIZE, hp.choice, options=[64, 128, 256, 512])\n    search_space.add(Parameter.RNN_LAYERS, hp.choice, options=[1, 2])\n    search_space.add(Parameter.REPROJECT_WORDS, hp.choice, options=[True, False])\n    search_space.add(Parameter.REPROJECT_WORD_DIMENSION, hp.choice, options=[64, 128])\n    search_space.add(Parameter.BIDIRECTIONAL, hp.choice, options=[True, False])\n    search_space.add(Parameter.DROPOUT, hp.uniform, low=0.25, high=0.75)\n    search_space.add(Parameter.WORD_DROPOUT, hp.uniform, low=0.25, high=0.75)\n    search_space.add(Parameter.LOCKED_DROPOUT, hp.uniform, low=0.25, high=0.75)\n\n    # training parameter\n    search_space.add(Parameter.LEARNING_RATE, hp.uniform, low=0, high=1)\n    search_space.add(Parameter.MINI_BATCH_SIZE, hp.choice, options=[4, 8, 16, 32])\n    search_space.add(Parameter.ANNEAL_FACTOR, hp.uniform, low=0, high=0.75)\n    search_space.add(Parameter.PATIENCE, hp.choice, options=[3, 5])\n\n    param_selector = TextClassifierParamSelector(\n        corpus, False, results_base_path, document_embedding_type=""lstm"", max_epochs=2\n    )\n    param_selector.optimize(search_space, max_evals=2)\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del param_selector, search_space\n'"
tests/test_language_model.py,0,"b'import shutil, pytest\n\nfrom flair.data import Dictionary, Sentence\nfrom flair.embeddings import TokenEmbeddings, FlairEmbeddings\nfrom flair.models import LanguageModel\nfrom flair.trainers.language_model_trainer import TextCorpus, LanguageModelTrainer\n\n\n@pytest.mark.integration\ndef test_train_language_model(results_base_path, resources_path):\n    # get default dictionary\n    dictionary: Dictionary = Dictionary.load(""chars"")\n\n    # init forward LM with 128 hidden states and 1 layer\n    language_model: LanguageModel = LanguageModel(\n        dictionary, is_forward_lm=True, hidden_size=128, nlayers=1\n    )\n\n    # get the example corpus and process at character level in forward direction\n    corpus: TextCorpus = TextCorpus(\n        resources_path / ""corpora/lorem_ipsum"",\n        dictionary,\n        language_model.is_forward_lm,\n        character_level=True,\n    )\n\n    # train the language model\n    trainer: LanguageModelTrainer = LanguageModelTrainer(\n        language_model, corpus, test_mode=True\n    )\n    trainer.train(\n        results_base_path, sequence_length=10, mini_batch_size=10, max_epochs=2\n    )\n\n    # use the character LM as embeddings to embed the example sentence \'I love Berlin\'\n    char_lm_embeddings: TokenEmbeddings = FlairEmbeddings(\n        str(results_base_path / ""best-lm.pt"")\n    )\n    sentence = Sentence(""I love Berlin"")\n    char_lm_embeddings.embed(sentence)\n\n    text, likelihood = language_model.generate_text(number_of_characters=100)\n    assert text is not None\n    assert len(text) >= 100\n\n    # clean up results directory\n    shutil.rmtree(results_base_path, ignore_errors=True)\n    del trainer, language_model, corpus, char_lm_embeddings\n\n\n@pytest.mark.integration\ndef test_train_resume_language_model(\n    resources_path, results_base_path, tasks_base_path\n):\n    # get default dictionary\n    dictionary: Dictionary = Dictionary.load(""chars"")\n\n    # init forward LM with 128 hidden states and 1 layer\n    language_model: LanguageModel = LanguageModel(\n        dictionary, is_forward_lm=True, hidden_size=128, nlayers=1\n    )\n\n    # get the example corpus and process at character level in forward direction\n    corpus: TextCorpus = TextCorpus(\n        resources_path / ""corpora/lorem_ipsum"",\n        dictionary,\n        language_model.is_forward_lm,\n        character_level=True,\n    )\n\n    # train the language model\n    trainer: LanguageModelTrainer = LanguageModelTrainer(\n        language_model, corpus, test_mode=True\n    )\n    trainer.train(\n        results_base_path,\n        sequence_length=10,\n        mini_batch_size=10,\n        max_epochs=2,\n        checkpoint=True,\n    )\n    del trainer, language_model\n\n    trainer = LanguageModelTrainer.load_from_checkpoint(\n        results_base_path / ""checkpoint.pt"", corpus\n    )\n    trainer.train(\n        results_base_path, sequence_length=10, mini_batch_size=10, max_epochs=2\n    )\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del trainer\n\n\ndef test_generate_text_with_small_temperatures():\n\n    from flair.embeddings import FlairEmbeddings\n\n    language_model = FlairEmbeddings(""news-forward-fast"").lm\n\n    text, likelihood = language_model.generate_text(\n        temperature=0.01, number_of_characters=100\n    )\n    assert text is not None\n    assert len(text) >= 100\n    del language_model\n\n\ndef test_compute_perplexity():\n\n    from flair.embeddings import FlairEmbeddings\n\n    language_model = FlairEmbeddings(""news-forward-fast"").lm\n\n    grammatical = ""The company made a profit""\n    perplexity_gramamtical_sentence = language_model.calculate_perplexity(grammatical)\n\n    ungrammatical = ""Nook negh qapla!""\n    perplexity_ungramamtical_sentence = language_model.calculate_perplexity(\n        ungrammatical\n    )\n\n    print(f\'""{grammatical}"" - perplexity is {perplexity_gramamtical_sentence}\')\n    print(f\'""{ungrammatical}"" - perplexity is {perplexity_ungramamtical_sentence}\')\n\n    assert perplexity_gramamtical_sentence < perplexity_ungramamtical_sentence\n\n    language_model = FlairEmbeddings(""news-backward-fast"").lm\n\n    grammatical = ""The company made a profit""\n    perplexity_gramamtical_sentence = language_model.calculate_perplexity(grammatical)\n\n    ungrammatical = ""Nook negh qapla!""\n    perplexity_ungramamtical_sentence = language_model.calculate_perplexity(\n        ungrammatical\n    )\n\n    print(f\'""{grammatical}"" - perplexity is {perplexity_gramamtical_sentence}\')\n    print(f\'""{ungrammatical}"" - perplexity is {perplexity_ungramamtical_sentence}\')\n\n    assert perplexity_gramamtical_sentence < perplexity_ungramamtical_sentence\n    del language_model\n\n'"
tests/test_sequence_tagger.py,2,"b'import shutil\n\nimport pytest\nfrom torch.optim import SGD\nfrom torch.optim.adam import Adam\n\nimport flair.datasets\nfrom flair.data import Sentence, MultiCorpus\nfrom flair.embeddings import (\n    WordEmbeddings,\n    FlairEmbeddings,\n)\nfrom flair.models import SequenceTagger\nfrom flair.trainers import ModelTrainer\n\nturian_embeddings = WordEmbeddings(""turian"")\nflair_embeddings = FlairEmbeddings(""news-forward-fast"")\n\n\n@pytest.mark.integration\ndef test_load_use_tagger():\n    loaded_model: SequenceTagger = SequenceTagger.load(""ner"")\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n    del loaded_model\n\n    sentence.clear_embeddings()\n    sentence_empty.clear_embeddings()\n\n    loaded_model: SequenceTagger = SequenceTagger.load(""pos"")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_load_use_tagger_keep_embedding():\n    loaded_model: SequenceTagger = SequenceTagger.load(""ner"")\n\n    sentence = Sentence(""I love Berlin"")\n    loaded_model.predict(sentence)\n    for token in sentence:\n        assert len(token.embedding.cpu().numpy()) == 0\n\n    loaded_model.predict(sentence, embedding_storage_mode=""cpu"")\n    for token in sentence:\n        assert len(token.embedding.cpu().numpy()) > 0\n\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_tagger(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ColumnCorpus(\n        data_folder=tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n    tag_dictionary = corpus.make_tag_dictionary(""ner"")\n\n    tagger: SequenceTagger = SequenceTagger(\n        hidden_size=64,\n        embeddings=turian_embeddings,\n        tag_dictionary=tag_dictionary,\n        tag_type=""ner"",\n        use_crf=False,\n    )\n\n    # initialize trainer\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n\n    trainer.train(\n        results_base_path,\n        learning_rate=0.1,\n        mini_batch_size=2,\n        max_epochs=2,\n        shuffle=False,\n    )\n\n    del trainer, tagger, tag_dictionary, corpus\n    loaded_model: SequenceTagger = SequenceTagger.load(\n        results_base_path / ""final-model.pt""\n    )\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_tagger_large(results_base_path, tasks_base_path):\n    corpus = flair.datasets.UD_ENGLISH().downsample(0.05)\n    tag_dictionary = corpus.make_tag_dictionary(""pos"")\n\n    tagger: SequenceTagger = SequenceTagger(\n        hidden_size=64,\n        embeddings=turian_embeddings,\n        tag_dictionary=tag_dictionary,\n        tag_type=""pos"",\n        use_crf=False,\n    )\n\n    # initialize trainer\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n\n    trainer.train(\n        results_base_path,\n        learning_rate=0.1,\n        mini_batch_size=32,\n        max_epochs=2,\n        shuffle=False,\n    )\n\n    del trainer, tagger, tag_dictionary, corpus\n    loaded_model: SequenceTagger = SequenceTagger.load(\n        results_base_path / ""final-model.pt""\n    )\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_tagger_flair_embeddings(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ColumnCorpus(\n        data_folder=tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n    tag_dictionary = corpus.make_tag_dictionary(""ner"")\n\n    tagger: SequenceTagger = SequenceTagger(\n        hidden_size=64,\n        embeddings=flair_embeddings,\n        tag_dictionary=tag_dictionary,\n        tag_type=""ner"",\n        use_crf=False,\n    )\n\n    # initialize trainer\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n\n    trainer.train(\n        results_base_path,\n        learning_rate=0.1,\n        mini_batch_size=2,\n        max_epochs=2,\n        shuffle=False,\n    )\n\n    del trainer, tagger, tag_dictionary, corpus\n    loaded_model: SequenceTagger = SequenceTagger.load(\n        results_base_path / ""final-model.pt""\n    )\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_tagger_adam(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ColumnCorpus(\n        data_folder=tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n    tag_dictionary = corpus.make_tag_dictionary(""ner"")\n\n    tagger: SequenceTagger = SequenceTagger(\n        hidden_size=64,\n        embeddings=turian_embeddings,\n        tag_dictionary=tag_dictionary,\n        tag_type=""ner"",\n        use_crf=False,\n    )\n\n    # initialize trainer\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=Adam)\n\n    trainer.train(\n        results_base_path,\n        learning_rate=0.1,\n        mini_batch_size=2,\n        max_epochs=2,\n        shuffle=False,\n    )\n\n    del trainer, tagger, tag_dictionary, corpus\n    loaded_model: SequenceTagger = SequenceTagger.load(\n        results_base_path / ""final-model.pt""\n    )\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_tagger_multicorpus(results_base_path, tasks_base_path):\n    corpus_1 = flair.datasets.ColumnCorpus(\n        data_folder=tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n    corpus_2 = flair.datasets.GERMEVAL_14(base_path=tasks_base_path)\n\n    corpus = MultiCorpus([corpus_1, corpus_2])\n    tag_dictionary = corpus.make_tag_dictionary(""ner"")\n\n    tagger: SequenceTagger = SequenceTagger(\n        hidden_size=64,\n        embeddings=turian_embeddings,\n        tag_dictionary=tag_dictionary,\n        tag_type=""ner"",\n        use_crf=False,\n    )\n\n    # initialize trainer\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n\n    trainer.train(\n        results_base_path,\n        learning_rate=0.1,\n        mini_batch_size=2,\n        max_epochs=2,\n        shuffle=False,\n    )\n\n    del trainer, tagger, corpus\n    loaded_model: SequenceTagger = SequenceTagger.load(\n        results_base_path / ""final-model.pt""\n    )\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_resume_tagger(results_base_path, tasks_base_path):\n    corpus_1 = flair.datasets.ColumnCorpus(\n        data_folder=tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n    corpus_2 = flair.datasets.GERMEVAL_14(base_path=tasks_base_path)\n\n    corpus = MultiCorpus([corpus_1, corpus_2])\n    tag_dictionary = corpus.make_tag_dictionary(""ner"")\n\n    model: SequenceTagger = SequenceTagger(\n        hidden_size=64,\n        embeddings=turian_embeddings,\n        tag_dictionary=tag_dictionary,\n        tag_type=""ner"",\n        use_crf=False,\n    )\n\n    trainer = ModelTrainer(model, corpus)\n    trainer.train(results_base_path, max_epochs=2, shuffle=False, checkpoint=True)\n\n    del trainer, model\n    trainer = ModelTrainer.load_checkpoint(results_base_path / ""checkpoint.pt"", corpus)\n\n    trainer.train(results_base_path, max_epochs=2, shuffle=False, checkpoint=True)\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del trainer\n\n\n@pytest.mark.integration\ndef test_find_learning_rate(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ColumnCorpus(\n        data_folder=tasks_base_path / ""fashion"", column_format={0: ""text"", 2: ""ner""}\n    )\n    tag_dictionary = corpus.make_tag_dictionary(""ner"")\n\n    tagger: SequenceTagger = SequenceTagger(\n        hidden_size=64,\n        embeddings=turian_embeddings,\n        tag_dictionary=tag_dictionary,\n        tag_type=""ner"",\n        use_crf=False,\n    )\n\n    # initialize trainer\n    trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=SGD)\n\n    trainer.find_learning_rate(results_base_path, iterations=5)\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del trainer, tagger, tag_dictionary, corpus\n\n'"
tests/test_text_classifier.py,0,"b'import flair.datasets\nimport shutil\n\nimport pytest\n\nimport flair.datasets\nfrom flair.data import Sentence\nfrom flair.embeddings import (\n    WordEmbeddings,\n    FlairEmbeddings,\n    DocumentRNNEmbeddings,\n)\nfrom flair.models import TextClassifier\nfrom flair.samplers import ImbalancedClassificationDatasetSampler\nfrom flair.trainers import ModelTrainer\n\nturian_embeddings = WordEmbeddings(""turian"")\nflair_embeddings = FlairEmbeddings(""news-forward-fast"")\ndocument_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(\n    [turian_embeddings], 128, 1, False, 64, False, False\n)\n\n\n@pytest.mark.integration\ndef test_load_use_classifier():\n    loaded_model: TextClassifier = TextClassifier.load(""sentiment"")\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n    del loaded_model\n\n    sentence.clear_embeddings()\n    sentence_empty.clear_embeddings()\n\n\n@pytest.mark.integration\ndef test_train_load_use_classifier(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""imdb"")\n    label_dict = corpus.make_label_dictionary()\n\n    model: TextClassifier = TextClassifier(document_embeddings, label_dict, multi_label=False)\n\n    trainer = ModelTrainer(model, corpus)\n    trainer.train(results_base_path, max_epochs=2, shuffle=False)\n\n    sentence = Sentence(""Berlin is a really nice city."")\n\n    model.predict(sentence)\n\n    for label in sentence.labels:\n        assert label.value is not None\n        assert 0.0 <= label.score <= 1.0\n        assert type(label.score) is float\n\n    del trainer, model, corpus\n    loaded_model = TextClassifier.load(results_base_path / ""final-model.pt"")\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_classifier_with_sampler(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""imdb"")\n    label_dict = corpus.make_label_dictionary()\n\n    model: TextClassifier = TextClassifier(document_embeddings, label_dict, multi_label=False)\n\n    trainer = ModelTrainer(model, corpus)\n    trainer.train(\n        results_base_path,\n        max_epochs=2,\n        shuffle=False,\n        sampler=ImbalancedClassificationDatasetSampler,\n    )\n\n    sentence = Sentence(""Berlin is a really nice city."")\n    model.predict(sentence)\n\n    for label in sentence.labels:\n        assert label.value is not None\n        assert 0.0 <= label.score <= 1.0\n        assert type(label.score) is float\n\n    del trainer, model, corpus\n    loaded_model = TextClassifier.load(results_base_path / ""final-model.pt"")\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_classifier_with_prob(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""imdb"")\n    label_dict = corpus.make_label_dictionary()\n\n    model: TextClassifier = TextClassifier(document_embeddings, label_dict, multi_label=False)\n\n    trainer = ModelTrainer(model, corpus)\n    trainer.train(results_base_path, max_epochs=2, shuffle=False)\n\n    sentence = Sentence(""Berlin is a really nice city."")\n\n    model.predict(sentence, multi_class_prob=True)\n\n    for label in sentence.labels:\n        assert label.value is not None\n        assert 0.0 <= label.score <= 1.0\n        assert type(label.score) is float\n\n    del trainer, model, corpus\n    loaded_model = TextClassifier.load(results_base_path / ""final-model.pt"")\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence, multi_class_prob=True)\n    loaded_model.predict([sentence, sentence_empty], multi_class_prob=True)\n    loaded_model.predict([sentence_empty], multi_class_prob=True)\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_classifier_multi_label(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""multi_class"")\n    label_dict = corpus.make_label_dictionary()\n\n    model: TextClassifier = TextClassifier(\n        document_embeddings, label_dict, multi_label=True\n    )\n\n    trainer = ModelTrainer(model, corpus)\n    trainer.train(\n        results_base_path,\n        mini_batch_size=1,\n        max_epochs=100,\n        shuffle=False,\n        checkpoint=False,\n    )\n\n    sentence = Sentence(""apple tv"")\n\n    model.predict(sentence)\n\n    for label in sentence.labels:\n        print(label)\n        assert label.value is not None\n        assert 0.0 <= label.score <= 1.0\n        assert type(label.score) is float\n\n    sentence = Sentence(""apple tv"")\n\n    model.predict(sentence)\n\n    assert ""apple"" in sentence.get_label_names()\n    assert ""tv"" in sentence.get_label_names()\n\n    for label in sentence.labels:\n        assert label.value is not None\n        assert 0.0 <= label.score <= 1.0\n        assert type(label.score) is float\n\n    del trainer, model, corpus\n    loaded_model = TextClassifier.load(results_base_path / ""final-model.pt"")\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_load_use_classifier_flair(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""imdb"")\n    label_dict = corpus.make_label_dictionary()\n\n    flair_document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(\n       [flair_embeddings], 128, 1, False, 64, False, False\n    )\n\n    model: TextClassifier = TextClassifier(flair_document_embeddings, label_dict, multi_label=False)\n\n    trainer = ModelTrainer(model, corpus)\n    trainer.train(results_base_path, max_epochs=2, shuffle=False)\n\n    sentence = Sentence(""Berlin is a really nice city."")\n\n    model.predict(sentence)\n\n    for label in sentence.labels:\n        assert label.value is not None\n        assert 0.0 <= label.score <= 1.0\n        assert type(label.score) is float\n\n    del trainer, model, corpus, flair_document_embeddings\n    loaded_model = TextClassifier.load(results_base_path / ""final-model.pt"")\n\n    sentence = Sentence(""I love Berlin"")\n    sentence_empty = Sentence(""       "")\n\n    loaded_model.predict(sentence)\n    loaded_model.predict([sentence, sentence_empty])\n    loaded_model.predict([sentence_empty])\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del loaded_model\n\n\n@pytest.mark.integration\ndef test_train_resume_classifier(results_base_path, tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""imdb"")\n    label_dict = corpus.make_label_dictionary()\n\n    model = TextClassifier(document_embeddings, label_dict, multi_label=False)\n\n    trainer = ModelTrainer(model, corpus)\n    trainer.train(results_base_path, max_epochs=2, shuffle=False, checkpoint=True)\n\n    del trainer, model\n    trainer = ModelTrainer.load_checkpoint(results_base_path / ""checkpoint.pt"", corpus)\n    trainer.train(results_base_path, max_epochs=2, shuffle=False, checkpoint=True)\n\n    # clean up results directory\n    shutil.rmtree(results_base_path)\n    del trainer\n\n\ndef test_labels_to_indices(tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"")\n    label_dict = corpus.make_label_dictionary()\n    model = TextClassifier(document_embeddings, label_dict, multi_label=False)\n\n    result = model._labels_to_indices(corpus.train)\n\n    for i in range(len(corpus.train)):\n        expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value)\n        actual = result[i].item()\n\n        assert expected == actual\n\n\ndef test_labels_to_one_hot(tasks_base_path):\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / ""ag_news"")\n    label_dict = corpus.make_label_dictionary()\n    model = TextClassifier(document_embeddings, label_dict, multi_label=False)\n\n    result = model._labels_to_one_hot(corpus.train)\n\n    for i in range(len(corpus.train)):\n        expected = label_dict.get_idx_for_item(corpus.train[i].labels[0].value)\n        actual = result[i]\n\n        for idx in range(len(label_dict)):\n            if idx == expected:\n                assert actual[idx] == 1\n            else:\n                assert actual[idx] == 0'"
tests/test_text_regressor.py,0,"b'import pytest\nfrom typing import Tuple\nimport flair.datasets\nfrom flair.data import Dictionary, Corpus\nfrom flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\nfrom flair.models.text_regression_model import TextRegressor\n\n# from flair.trainers.trainer_regression import RegressorTrainer\nfrom flair.trainers import ModelTrainer\n\n\ndef init(tasks_base_path) -> Tuple[Corpus, TextRegressor, ModelTrainer]:\n    corpus = flair.datasets.ClassificationCorpus(tasks_base_path / \'regression\')\n\n    glove_embedding: WordEmbeddings = WordEmbeddings(""glove"")\n    document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(\n        [glove_embedding], 128, 1, False, 64, False, False\n    )\n\n    model = TextRegressor(document_embeddings)\n\n    trainer = ModelTrainer(model, corpus)\n\n    return corpus, model, trainer\n\n\ndef test_labels_to_indices(tasks_base_path):\n    corpus, model, trainer = init(tasks_base_path)\n\n    result = model._labels_to_indices(corpus.train)\n\n    for i in range(len(corpus.train)):\n        expected = round(float(corpus.train[i].labels[0].value), 3)\n        actual = round(float(result[i].item()), 3)\n\n        assert expected == actual\n\n\ndef test_trainer_evaluation(tasks_base_path):\n    corpus, model, trainer = init(tasks_base_path)\n\n    expected = model.evaluate(corpus.dev)\n\n    assert expected is not None\n\n\n# def test_trainer_results(tasks_base_path):\n#    corpus, model, trainer = init(tasks_base_path)\n\n#    results = trainer.train(""regression_train/"", max_epochs=1)\n\n#    assert results[""test_score""] > 0\n#    assert len(results[""dev_loss_history""]) == 1\n#    assert len(results[""dev_score_history""]) == 1\n#    assert len(results[""train_loss_history""]) == 1\n'"
tests/test_utils.py,0,"b'from flair.data import Dictionary\nfrom flair.models import TextClassifier\nfrom flair.trainers import ModelTrainer\nfrom flair.training_utils import convert_labels_to_one_hot, Metric\n\n\ndef test_metric_get_classes():\n    metric = Metric(""Test"")\n\n    metric.add_fn(""class-1"")\n    metric.add_fn(""class-3"")\n    metric.add_tn(""class-1"")\n    metric.add_tp(""class-2"")\n\n    assert 3 == len(metric.get_classes())\n    assert ""class-1"" in metric.get_classes()\n    assert ""class-2"" in metric.get_classes()\n    assert ""class-3"" in metric.get_classes()\n\n\n# def test_multiclass_metrics():\n#\n#     metric = Metric(""Test"")\n#     available_labels = [""A"", ""B"", ""C""]\n#\n#     predictions = [""A"", ""B""]\n#     true_values = [""A""]\n#     TextClassifier._evaluate_sentence_for_text_classification(\n#         metric, available_labels, predictions, true_values\n#     )\n#\n#     predictions = [""C"", ""B""]\n#     true_values = [""A"", ""B""]\n#     TextClassifier._evaluate_sentence_for_text_classification(\n#         metric, available_labels, predictions, true_values\n#     )\n#\n#     print(metric)\n\n\ndef test_metric_with_classes():\n    metric = Metric(""Test"")\n\n    metric.add_tp(""class-1"")\n    metric.add_tn(""class-1"")\n    metric.add_tn(""class-1"")\n    metric.add_fp(""class-1"")\n\n    metric.add_tp(""class-2"")\n    metric.add_tn(""class-2"")\n    metric.add_tn(""class-2"")\n    metric.add_fp(""class-2"")\n\n    for i in range(0, 10):\n        metric.add_tp(""class-3"")\n    for i in range(0, 90):\n        metric.add_fp(""class-3"")\n\n    metric.add_tp(""class-4"")\n    metric.add_tn(""class-4"")\n    metric.add_tn(""class-4"")\n    metric.add_fp(""class-4"")\n\n    print(metric)\n\n    assert metric.precision(""class-1"") == 0.5\n    assert metric.precision(""class-2"") == 0.5\n    assert metric.precision(""class-3"") == 0.1\n    assert metric.precision(""class-4"") == 0.5\n\n    assert metric.recall(""class-1"") == 1\n    assert metric.recall(""class-2"") == 1\n    assert metric.recall(""class-3"") == 1\n    assert metric.recall(""class-4"") == 1\n\n    assert metric.accuracy() == metric.micro_avg_accuracy()\n    assert metric.f_score() == metric.micro_avg_f_score()\n\n    assert metric.f_score(""class-1"") == 0.6666666666666666\n    assert metric.f_score(""class-2"") == 0.6666666666666666\n    assert metric.f_score(""class-3"") == 0.18181818181818182\n    assert metric.f_score(""class-4"") == 0.6666666666666666\n\n    assert metric.accuracy(""class-1"") == 0.75\n    assert metric.accuracy(""class-2"") == 0.75\n    assert metric.accuracy(""class-3"") == 0.1\n    assert metric.accuracy(""class-4"") == 0.75\n\n    assert metric.micro_avg_f_score() == 0.21848739495798317\n    assert metric.macro_avg_f_score() == 0.5454545454545454\n\n    assert metric.micro_avg_accuracy() == 0.16964285714285715\n    assert metric.macro_avg_accuracy() == 0.5875\n\n    assert metric.precision() == 0.12264150943396226\n    assert metric.recall() == 1\n\n\ndef test_convert_labels_to_one_hot():\n    label_dict = Dictionary(add_unk=False)\n    label_dict.add_item(""class-1"")\n    label_dict.add_item(""class-2"")\n    label_dict.add_item(""class-3"")\n\n    one_hot = convert_labels_to_one_hot([[""class-2""]], label_dict)\n\n    assert one_hot[0][0] == 0\n    assert one_hot[0][1] == 1\n    assert one_hot[0][2] == 0\n'"
tests/test_visual.py,0,"b'from unittest.mock import MagicMock\n\nfrom flair.data import Sentence, Span, Token\nfrom flair.embeddings import FlairEmbeddings\nfrom flair.visual import *\nfrom flair.visual.ner_html import render_ner_html, HTML_PAGE, TAGGED_ENTITY, PARAGRAPH\nfrom flair.visual.training_curves import Plotter\n\n\ndef test_highlighter(resources_path):\n    with (resources_path / ""visual/snippet.txt"").open() as f:\n        sentences = [x for x in f.read().split(""\\n"") if x]\n\n    embeddings = FlairEmbeddings(""news-forward"")\n\n    features = embeddings.lm.get_representation(sentences[0], """", """").squeeze()\n\n    Highlighter().highlight_selection(\n        features,\n        sentences[0],\n        n=1000,\n        file_=str(resources_path / ""visual/highligh.html""),\n    )\n\n    # clean up directory\n    (resources_path / ""visual/highligh.html"").unlink()\n\n\ndef test_plotting_training_curves_and_weights(resources_path):\n    plotter = Plotter()\n    plotter.plot_training_curves(resources_path / ""visual/loss.tsv"")\n    plotter.plot_weights(resources_path / ""visual/weights.txt"")\n\n    # clean up directory\n    (resources_path / ""visual/weights.png"").unlink()\n    (resources_path / ""visual/training.png"").unlink()\n\n\ndef mock_ner_span(text, tag, start, end):\n    span = Span([]).set_label(\'class\', tag)\n    span.start_pos = start\n    span.end_pos = end\n    span.tokens = [Token(text[start:end])]\n    return span\n\n\ndef test_html_rendering():\n    text = (\n        ""Boris Johnson has been elected new Conservative leader in a ballot of party members and will become the ""\n        ""next UK prime minister. &""\n    )\n    sent = Sentence()\n    sent.get_spans = MagicMock()\n    sent.get_spans.return_value = [\n        mock_ner_span(text, ""PER"", 0, 13),\n        mock_ner_span(text, ""MISC"", 35, 47),\n        mock_ner_span(text, ""LOC"", 109, 111),\n    ]\n    sent.to_original_text = MagicMock()\n    sent.to_original_text.return_value = text\n    colors = {\n        ""PER"": ""#F7FF53"",\n        ""ORG"": ""#E8902E"",\n        ""LOC"": ""yellow"",\n        ""MISC"": ""#4647EB"",\n        ""O"": ""#ddd"",\n    }\n    actual = render_ner_html([sent], colors=colors)\n\n    expected_res = HTML_PAGE.format(\n        text=PARAGRAPH.format(\n            sentence=TAGGED_ENTITY.format(\n                color=""#F7FF53"", entity=""Boris Johnson"", label=""PER""\n            )\n            + "" has been elected new ""\n            + TAGGED_ENTITY.format(color=""#4647EB"", entity=""Conservative"", label=""MISC"")\n            + "" leader in a ballot of party members and will become the next ""\n            + TAGGED_ENTITY.format(color=""yellow"", entity=""UK"", label=""LOC"")\n            + "" prime minister. &amp;""\n        ),\n        title=""Flair"",\n    )\n\n    assert expected_res == actual\n'"
flair/datasets/__init__.py,0,b'# Expose base classses\nfrom .base import DataLoader\nfrom .base import SentenceDataset\nfrom .base import StringDataset\nfrom .base import MongoDataset\n\n# Expose all sequence labeling datasets\nfrom .sequence_labeling import ColumnCorpus\nfrom .sequence_labeling import ColumnDataset\nfrom .sequence_labeling import BIOFID\nfrom .sequence_labeling import CONLL_03\nfrom .sequence_labeling import CONLL_03_GERMAN\nfrom .sequence_labeling import CONLL_03_DUTCH\nfrom .sequence_labeling import CONLL_03_SPANISH\nfrom .sequence_labeling import CONLL_2000\nfrom .sequence_labeling import DANE\nfrom .sequence_labeling import GERMEVAL_14\nfrom .sequence_labeling import INSPEC\nfrom .sequence_labeling import NER_BASQUE\nfrom .sequence_labeling import NER_FINNISH\nfrom .sequence_labeling import NER_SWEDISH\nfrom .sequence_labeling import SEMEVAL2010\nfrom .sequence_labeling import SEMEVAL2017\nfrom .sequence_labeling import WIKINER_ENGLISH\nfrom .sequence_labeling import WIKINER_GERMAN\nfrom .sequence_labeling import WIKINER_DUTCH\nfrom .sequence_labeling import WIKINER_FRENCH\nfrom .sequence_labeling import WIKINER_ITALIAN\nfrom .sequence_labeling import WIKINER_SPANISH\nfrom .sequence_labeling import WIKINER_PORTUGUESE\nfrom .sequence_labeling import WIKINER_POLISH\nfrom .sequence_labeling import WIKINER_RUSSIAN\nfrom .sequence_labeling import WNUT_17\n\n# Expose all document classification datasets\nfrom .document_classification import ClassificationCorpus\nfrom .document_classification import ClassificationDataset\nfrom .document_classification import CSVClassificationCorpus\nfrom .document_classification import CSVClassificationDataset\nfrom .document_classification import AMAZON_REVIEWS\nfrom .document_classification import IMDB\nfrom .document_classification import NEWSGROUPS\nfrom .document_classification import SENTIMENT_140\nfrom .document_classification import SENTEVAL_CR\nfrom .document_classification import SENTEVAL_MR\nfrom .document_classification import SENTEVAL_MPQA\nfrom .document_classification import SENTEVAL_SUBJ\nfrom .document_classification import SENTEVAL_SST_BINARY\nfrom .document_classification import SENTEVAL_SST_GRANULAR\nfrom .document_classification import TREC_50\nfrom .document_classification import TREC_6\nfrom .document_classification import WASSA_ANGER\nfrom .document_classification import WASSA_FEAR\nfrom .document_classification import WASSA_JOY\nfrom .document_classification import WASSA_SADNESS\n\n# Expose all treebanks\nfrom .treebanks import UniversalDependenciesCorpus\nfrom .treebanks import UniversalDependenciesDataset\nfrom .treebanks import UD_ENGLISH\nfrom .treebanks import UD_GERMAN\nfrom .treebanks import UD_GERMAN_HDT\nfrom .treebanks import UD_DUTCH\nfrom .treebanks import UD_FRENCH\nfrom .treebanks import UD_ITALIAN\nfrom .treebanks import UD_SPANISH\nfrom .treebanks import UD_PORTUGUESE\nfrom .treebanks import UD_ROMANIAN\nfrom .treebanks import UD_CATALAN\nfrom .treebanks import UD_POLISH\nfrom .treebanks import UD_CZECH\nfrom .treebanks import UD_SLOVAK\nfrom .treebanks import UD_SWEDISH\nfrom .treebanks import UD_DANISH\nfrom .treebanks import UD_NORWEGIAN\nfrom .treebanks import UD_FINNISH\nfrom .treebanks import UD_SLOVENIAN\nfrom .treebanks import UD_CROATIAN\nfrom .treebanks import UD_SERBIAN\nfrom .treebanks import UD_BULGARIAN\nfrom .treebanks import UD_ARABIC\nfrom .treebanks import UD_HEBREW\nfrom .treebanks import UD_TURKISH\nfrom .treebanks import UD_PERSIAN\nfrom .treebanks import UD_RUSSIAN\nfrom .treebanks import UD_HINDI\nfrom .treebanks import UD_INDONESIAN\nfrom .treebanks import UD_JAPANESE\nfrom .treebanks import UD_CHINESE\nfrom .treebanks import UD_KOREAN\nfrom .treebanks import UD_BASQUE\n\n# Expose all text-text datasets\nfrom .text_text import ParallelTextCorpus\nfrom .text_text import ParallelTextDataset\nfrom .text_text import OpusParallelCorpus\n\n# Expose all text-image datasets\nfrom .text_image import FeideggerCorpus\nfrom .text_image import FeideggerDataset\n'
flair/datasets/base.py,3,"b'import logging\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import List, Union, Callable\n\nimport torch.utils.data.dataloader\nfrom torch.utils.data.dataset import Subset, ConcatDataset\n\nfrom flair.data import (\n    Sentence,\n    Token,\n    FlairDataset,\n    space_tokenizer,\n    segtok_tokenizer,\n)\n\n\nlog = logging.getLogger(""flair"")\n\n\nclass DataLoader(torch.utils.data.dataloader.DataLoader):\n    def __init__(\n            self,\n            dataset,\n            batch_size=1,\n            shuffle=False,\n            sampler=None,\n            batch_sampler=None,\n            num_workers=8,\n            drop_last=False,\n            timeout=0,\n            worker_init_fn=None,\n    ):\n\n        # in certain cases, multi-CPU data loading makes no sense and slows\n        # everything down. For this reason, we detect if a dataset is in-memory:\n        # if so, num_workers is set to 0 for faster processing\n        flair_dataset = dataset\n        while True:\n            if type(flair_dataset) is Subset:\n                flair_dataset = flair_dataset.dataset\n            elif type(flair_dataset) is ConcatDataset:\n                flair_dataset = flair_dataset.datasets[0]\n            else:\n                break\n\n        if type(flair_dataset) is list:\n            num_workers = 0\n        elif isinstance(flair_dataset, FlairDataset) and flair_dataset.is_in_memory():\n            num_workers = 0\n\n        super(DataLoader, self).__init__(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            sampler=sampler,\n            batch_sampler=batch_sampler,\n            num_workers=num_workers,\n            collate_fn=list,\n            drop_last=drop_last,\n            timeout=timeout,\n            worker_init_fn=worker_init_fn,\n        )\n\n\nclass SentenceDataset(FlairDataset):\n    """"""\n    A simple Dataset object to wrap a List of Sentence\n    """"""\n\n    def __init__(self, sentences: Union[Sentence, List[Sentence]]):\n        """"""\n        Instantiate SentenceDataset\n        :param sentences: Sentence or List of Sentence that make up SentenceDataset\n        """"""\n        # cast to list if necessary\n        if type(sentences) == Sentence:\n            sentences = [sentences]\n        self.sentences = sentences\n\n    @abstractmethod\n    def is_in_memory(self) -> bool:\n        return True\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n        return self.sentences[index]\n\n\nclass StringDataset(FlairDataset):\n    """"""\n    A Dataset taking string as input and returning Sentence during iteration\n    """"""\n\n    def __init__(\n            self,\n            texts: Union[str, List[str]],\n            use_tokenizer: Union[bool, Callable[[str], List[Token]]] = space_tokenizer,\n    ):\n        """"""\n        Instantiate StringDataset\n        :param texts: a string or List of string that make up StringDataset\n        :param use_tokenizer: a custom tokenizer (default is space based tokenizer,\n        more advanced options are segtok_tokenizer to use segtok or build_spacy_tokenizer to use Spacy library\n        if available). Check the code of space_tokenizer to implement your own (if you need it).\n        If instead of providing a function, this parameter is just set to True, segtok will be used.\n        """"""\n        # cast to list if necessary\n        if type(texts) == Sentence:\n            texts = [texts]\n        self.texts = texts\n        self.use_tokenizer = use_tokenizer\n\n    @abstractmethod\n    def is_in_memory(self) -> bool:\n        return True\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n        text = self.texts[index]\n        return Sentence(text, use_tokenizer=self.use_tokenizer)\n\n\nclass MongoDataset(FlairDataset):\n    def __init__(\n            self,\n            query: str,\n            host: str,\n            port: int,\n            database: str,\n            collection: str,\n            text_field: str,\n            categories_field: List[str] = None,\n            max_tokens_per_doc: int = -1,\n            max_chars_per_doc: int = -1,\n            tokenizer=segtok_tokenizer,\n            in_memory: bool = True,\n    ):\n        """"""\n        Reads Mongo collections. Each collection should contain one document/text per item.\n\n        Each item should have the following format:\n        {\n        \'Beskrivning\': \'Abrahamsby. G\xc3\xa5rd i Gottr\xc3\xb6ra sn, L\xc3\xa5nghundra hd, Stockholms l\xc3\xa4n, n\xc3\xa4ra L\xc3\xa5ngsj\xc3\xb6n.\',\n        \'L\xc3\xa4n\':\'Stockholms l\xc3\xa4n\',\n        \'H\xc3\xa4rad\': \'L\xc3\xa5nghundra\',\n        \'F\xc3\xb6rsamling\': \'Gottr\xc3\xb6ra\',\n        \'Plats\': \'Abrahamsby\'\n        }\n\n        :param query: Query, e.g. {\'L\xc3\xa4n\': \'Stockholms l\xc3\xa4n\'}\n        :param host: Host, e.g. \'localhost\',\n        :param port: Port, e.g. 27017\n        :param database: Database, e.g. \'rosenberg\',\n        :param collection: Collection, e.g. \'book\',\n        :param text_field: Text field, e.g. \'Beskrivning\',\n        :param categories_field: List of category fields, e.g [\'L\xc3\xa4n\', \'H\xc3\xa4rad\', \'Tingslag\', \'F\xc3\xb6rsamling\', \'Plats\'],\n        :param max_tokens_per_doc: Takes at most this amount of tokens per document. If set to -1 all documents are taken as is.\n        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens\n        :param max_chars_per_doc: If set, truncates each Sentence to a maximum number of chars\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :return: list of sentences\n        """"""\n\n        # first, check if pymongo is installed\n        try:\n            import pymongo\n        except ModuleNotFoundError:\n            log.warning(""-"" * 100)\n            log.warning(\'ATTENTION! The library ""pymongo"" is not installed!\')\n            log.warning(\n                \'To use MongoDataset, please first install with ""pip install pymongo""\'\n            )\n            log.warning(""-"" * 100)\n            pass\n\n        self.in_memory = in_memory\n        self.tokenizer = tokenizer\n\n        if self.in_memory:\n            self.sentences = []\n        else:\n            self.indices = []\n\n        self.total_sentence_count: int = 0\n        self.max_chars_per_doc = max_chars_per_doc\n        self.max_tokens_per_doc = max_tokens_per_doc\n\n        self.__connection = pymongo.MongoClient(host, port)\n        self.__cursor = self.__connection[database][collection]\n\n        self.text = text_field\n        self.categories = categories_field if categories_field is not None else []\n\n        start = 0\n\n        kwargs = lambda start: {""filter"": query, ""skip"": start, ""limit"": 0}\n\n        if self.in_memory:\n            for document in self.__cursor.find(**kwargs(start)):\n                sentence = self._parse_document_to_sentence(\n                    document[self.text],\n                    [document[_] if _ in document else """" for _ in self.categories],\n                    tokenizer,\n                )\n                if sentence is not None and len(sentence.tokens) > 0:\n                    self.sentences.append(sentence)\n                    self.total_sentence_count += 1\n        else:\n            self.indices = self.__cursor.find().distinct(""_id"")\n            self.total_sentence_count = self.__cursor.count_documents()\n\n    def _parse_document_to_sentence(\n            self, text: str, labels: List[str], tokenizer: Callable[[str], List[Token]]\n    ):\n        if self.max_chars_per_doc > 0:\n            text = text[: self.max_chars_per_doc]\n\n        if text and labels:\n            sentence = Sentence(text, labels=labels, use_tokenizer=tokenizer)\n\n            if self.max_tokens_per_doc > 0:\n                sentence.tokens = sentence.tokens[\n                                  : min(len(sentence), self.max_tokens_per_doc)\n                                  ]\n\n            return sentence\n        return None\n\n    def is_in_memory(self) -> bool:\n        return self.in_memory\n\n    def __len__(self):\n        return self.total_sentence_count\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n        if self.in_memory:\n            return self.sentences[index]\n        else:\n            document = self.__cursor.find_one({""_id"": index})\n            sentence = self._parse_document_to_sentence(\n                document[self.text],\n                [document[_] if _ in document else """" for _ in self.categories],\n                self.tokenizer,\n            )\n            return sentence\n\n\ndef find_train_dev_test_files(data_folder, dev_file, test_file, train_file):\n    if type(data_folder) == str:\n        data_folder: Path = Path(data_folder)\n\n    if train_file is not None:\n        train_file = data_folder / train_file\n    if test_file is not None:\n        test_file = data_folder / test_file\n    if dev_file is not None:\n        dev_file = data_folder / dev_file\n\n    suffixes_to_ignore = {"".gz"", "".swp""}\n\n    # automatically identify train / test / dev files\n    if train_file is None:\n        for file in data_folder.iterdir():\n            file_name = file.name\n            if not suffixes_to_ignore.isdisjoint(file.suffixes):\n                continue\n            if ""train"" in file_name and not ""54019"" in file_name:\n                train_file = file\n            if ""dev"" in file_name:\n                dev_file = file\n            if ""testa"" in file_name:\n                dev_file = file\n            if ""testb"" in file_name:\n                test_file = file\n\n        # if no test file is found, take any file with \'test\' in name\n        if test_file is None:\n            for file in data_folder.iterdir():\n                file_name = file.name\n                if not suffixes_to_ignore.isdisjoint(file.suffixes):\n                    continue\n                if ""test"" in file_name:\n                    test_file = file\n\n    log.info(""Reading data from {}"".format(data_folder))\n    log.info(""Train: {}"".format(train_file))\n    log.info(""Dev: {}"".format(dev_file))\n    log.info(""Test: {}"".format(test_file))\n\n    return dev_file, test_file, train_file\n'"
flair/datasets/document_classification.py,0,"b'import csv\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import List, Dict, Union, Callable\n\n\nimport flair\nfrom flair.data import (\n    Sentence,\n    Corpus,\n    Token,\n    FlairDataset,\n    space_tokenizer,\n    segtok_tokenizer,\n)\nfrom flair.datasets.base import find_train_dev_test_files\nfrom flair.file_utils import cached_path, unzip_file\n\n\nclass ClassificationCorpus(Corpus):\n    """"""\n    A classification corpus from FastText-formatted text files.\n    """"""\n    def __init__(\n            self,\n            data_folder: Union[str, Path],\n            label_type: str = \'class\',\n            train_file=None,\n            test_file=None,\n            dev_file=None,\n            truncate_to_max_tokens: int = -1,\n            truncate_to_max_chars: int = -1,\n            filter_if_longer_than: int = -1,\n            tokenizer: Callable[[str], List[Token]] = segtok_tokenizer,\n            memory_mode: str = ""partial"",\n            label_name_map: Dict[str, str] = None,\n            skip_labels: List[str] = None,\n            encoding: str = \'utf-8\',\n    ):\n        """"""\n        Instantiates a Corpus from text classification-formatted task data\n\n        :param data_folder: base folder with the task data\n        :param label_type: name of the label\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param truncate_to_max_tokens: If set, truncates each Sentence to a maximum number of tokens\n        :param truncate_to_max_chars: If set, truncates each Sentence to a maximum number of chars\n        :param filter_if_longer_than: If set, filters documents that are longer that the specified number of tokens.\n        :param tokenizer: Tokenizer for dataset, default is segtok\n        :param memory_mode: Set to what degree to keep corpus in memory (\'full\', \'partial\' or \'disk\'). Use \'full\'\n        if full corpus and all embeddings fits into memory for speedups during training. Otherwise use \'partial\' and if\n        even this is too much for your memory, use \'disk\'.\n        :param label_name_map: Optionally map label names to different schema.\n        :param encoding: Default is \'uft-8\' but some datasets are in \'latin-1\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        # find train, dev and test files if not specified\n        dev_file, test_file, train_file = \\\n            find_train_dev_test_files(data_folder, dev_file, test_file, train_file)\n\n        train: FlairDataset = ClassificationDataset(\n            train_file,\n            label_type=label_type,\n            tokenizer=tokenizer,\n            truncate_to_max_tokens=truncate_to_max_tokens,\n            truncate_to_max_chars=truncate_to_max_chars,\n            filter_if_longer_than=filter_if_longer_than,\n            memory_mode=memory_mode,\n            label_name_map=label_name_map,\n            skip_labels=skip_labels,\n            encoding=encoding,\n        )\n\n        # use test_file to create test split if available\n        test: FlairDataset = ClassificationDataset(\n            test_file,\n            label_type=label_type,\n            tokenizer=tokenizer,\n            truncate_to_max_tokens=truncate_to_max_tokens,\n            truncate_to_max_chars=truncate_to_max_chars,\n            filter_if_longer_than=filter_if_longer_than,\n            memory_mode=memory_mode,\n            label_name_map=label_name_map,\n            skip_labels=skip_labels,\n            encoding=encoding,\n        ) if test_file is not None else None\n\n        # use dev_file to create test split if available\n        dev: FlairDataset = ClassificationDataset(\n            dev_file,\n            label_type=label_type,\n            tokenizer=tokenizer,\n            truncate_to_max_tokens=truncate_to_max_tokens,\n            truncate_to_max_chars=truncate_to_max_chars,\n            filter_if_longer_than=filter_if_longer_than,\n            memory_mode=memory_mode,\n            label_name_map=label_name_map,\n            skip_labels=skip_labels,\n            encoding=encoding,\n        ) if dev_file is not None else None\n\n        super(ClassificationCorpus, self).__init__(\n            train, dev, test, name=str(data_folder)\n        )\n\n\nclass ClassificationDataset(FlairDataset):\n    """"""\n    Dataset for classification instantiated from a single FastText-formatted file.\n    """"""\n    def __init__(\n            self,\n            path_to_file: Union[str, Path],\n            label_type: str = \'class\',\n            truncate_to_max_tokens=-1,\n            truncate_to_max_chars=-1,\n            filter_if_longer_than: int = -1,\n            tokenizer=segtok_tokenizer,\n            memory_mode: str = ""partial"",\n            label_name_map: Dict[str, str] = None,\n            skip_labels: List[str] = None,\n            encoding: str = \'utf-8\',\n    ):\n        """"""\n        Reads a data file for text classification. The file should contain one document/text per line.\n        The line should have the following format:\n        __label__<class_name> <text>\n        If you have a multi class task, you can have as many labels as you want at the beginning of the line, e.g.,\n        __label__<class_name_1> __label__<class_name_2> <text>\n        :param path_to_file: the path to the data file\n        :param label_type: name of the label\n        :param truncate_to_max_tokens: If set, truncates each Sentence to a maximum number of tokens\n        :param truncate_to_max_chars: If set, truncates each Sentence to a maximum number of chars\n        :param filter_if_longer_than: If set, filters documents that are longer that the specified number of tokens.\n        :param tokenizer: Tokenizer for dataset, default is segtok\n        :param memory_mode: Set to what degree to keep corpus in memory (\'full\', \'partial\' or \'disk\'). Use \'full\'\n        if full corpus and all embeddings fits into memory for speedups during training. Otherwise use \'partial\' and if\n        even this is too much for your memory, use \'disk\'.\n        :param label_name_map: Optionally map label names to different schema.\n        :param encoding: Default is \'uft-8\' but some datasets are in \'latin-1\n        :return: list of sentences\n        """"""\n        if type(path_to_file) == str:\n            path_to_file: Path = Path(path_to_file)\n\n        assert path_to_file.exists()\n\n        self.label_prefix = ""__label__""\n        self.label_type = label_type\n\n        self.memory_mode = memory_mode\n        self.tokenizer = tokenizer\n\n        if self.memory_mode == \'full\':\n            self.sentences = []\n        if self.memory_mode == \'partial\':\n            self.lines = []\n        if self.memory_mode == \'disk\':\n            self.indices = []\n\n        self.total_sentence_count: int = 0\n        self.truncate_to_max_chars = truncate_to_max_chars\n        self.truncate_to_max_tokens = truncate_to_max_tokens\n        self.filter_if_longer_than = filter_if_longer_than\n        self.label_name_map = label_name_map\n\n        self.path_to_file = path_to_file\n\n        with open(str(path_to_file), encoding=encoding) as f:\n            line = f.readline()\n            position = 0\n            while line:\n                if ""__label__"" not in line or ("" "" not in line and ""\\t"" not in line):\n                    position = f.tell()\n                    line = f.readline()\n                    continue\n\n                if 0 < self.filter_if_longer_than < len(line.split(\' \')):\n                    position = f.tell()\n                    line = f.readline()\n                    continue\n\n                # if data point contains black-listed label, do not use\n                if skip_labels:\n                    skip = False\n                    for skip_label in skip_labels:\n                        if ""__label__"" + skip_label in line:\n                            skip = True\n                    if skip:\n                        line = f.readline()\n                        continue\n\n                if self.memory_mode == \'full\':\n                    sentence = self._parse_line_to_sentence(\n                        line, self.label_prefix, tokenizer\n                    )\n                    if sentence is not None and len(sentence.tokens) > 0:\n                        self.sentences.append(sentence)\n                        self.total_sentence_count += 1\n\n                if self.memory_mode == \'partial\' or self.memory_mode == \'disk\':\n\n                    # first check if valid sentence\n                    words = line.split()\n                    l_len = 0\n                    label = False\n                    for i in range(len(words)):\n                        if words[i].startswith(self.label_prefix):\n                            l_len += len(words[i]) + 1\n                            label = True\n                        else:\n                            break\n                    text = line[l_len:].strip()\n\n                    # if so, add to indices\n                    if text and label:\n\n                        if self.memory_mode == \'partial\':\n                            self.lines.append(line)\n                            self.total_sentence_count += 1\n\n                        if self.memory_mode == \'disk\':\n                            self.indices.append(position)\n                            self.total_sentence_count += 1\n\n                position = f.tell()\n                line = f.readline()\n\n    def _parse_line_to_sentence(\n            self, line: str, label_prefix: str, tokenizer: Callable[[str], List[Token]]\n    ):\n        words = line.split()\n\n        labels = []\n        l_len = 0\n\n        for i in range(len(words)):\n            if words[i].startswith(label_prefix):\n                l_len += len(words[i]) + 1\n                label = words[i].replace(label_prefix, """")\n\n                if self.label_name_map and label in self.label_name_map.keys():\n                    label = self.label_name_map[label]\n\n                labels.append(label)\n            else:\n                break\n\n        text = line[l_len:].strip()\n\n        if self.truncate_to_max_chars > 0:\n            text = text[: self.truncate_to_max_chars]\n\n        if text and labels:\n            sentence = Sentence(text, use_tokenizer=tokenizer)\n\n            for label in labels:\n                sentence.add_label(self.label_type, label)\n\n            if (\n                    sentence is not None\n                    and 0 < self.truncate_to_max_tokens < len(sentence)\n            ):\n                sentence.tokens = sentence.tokens[: self.truncate_to_max_tokens]\n\n            return sentence\n        return None\n\n    def is_in_memory(self) -> bool:\n        if self.memory_mode == \'disk\': return False\n        if self.memory_mode == \'partial\': return False\n        return True\n\n    def __len__(self):\n        return self.total_sentence_count\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n\n        if self.memory_mode == \'full\':\n            return self.sentences[index]\n\n        if self.memory_mode == \'partial\':\n            sentence = self._parse_line_to_sentence(\n                self.lines[index], self.label_prefix, self.tokenizer\n            )\n            return sentence\n\n        if self.memory_mode == \'disk\':\n            with open(str(self.path_to_file), encoding=""utf-8"") as file:\n                file.seek(self.indices[index])\n                line = file.readline()\n                sentence = self._parse_line_to_sentence(\n                    line, self.label_prefix, self.tokenizer\n                )\n                return sentence\n\n\nclass CSVClassificationCorpus(Corpus):\n    """"""\n    Classification corpus instantiated from CSV data files.\n    """"""\n    def __init__(\n            self,\n            data_folder: Union[str, Path],\n            column_name_map: Dict[int, str],\n            label_type: str = \'class\',\n            train_file=None,\n            test_file=None,\n            dev_file=None,\n            max_tokens_per_doc=-1,\n            max_chars_per_doc=-1,\n            tokenizer: Callable[[str], List[Token]] = segtok_tokenizer,\n            in_memory: bool = False,\n            skip_header: bool = False,\n            encoding: str = \'utf-8\',\n            **fmtparams,\n    ):\n        """"""\n        Instantiates a Corpus for text classification from CSV column formatted data\n\n        :param data_folder: base folder with the task data\n        :param column_name_map: a column name map that indicates which column is text and which the label(s)\n        :param label_type: name of the label\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens\n        :param max_chars_per_doc: If set, truncates each Sentence to a maximum number of chars\n        :param tokenizer: Tokenizer for dataset, default is segtok\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :param skip_header: If True, skips first line because it is header\n        :param encoding: Default is \'uft-8\' but some datasets are in \'latin-1\n        :param fmtparams: additional parameters for the CSV file reader\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        # find train, dev and test files if not specified\n        dev_file, test_file, train_file = \\\n            find_train_dev_test_files(data_folder, dev_file, test_file, train_file)\n\n        train: FlairDataset = CSVClassificationDataset(\n            train_file,\n            column_name_map,\n            label_type=label_type,\n            tokenizer=tokenizer,\n            max_tokens_per_doc=max_tokens_per_doc,\n            max_chars_per_doc=max_chars_per_doc,\n            in_memory=in_memory,\n            skip_header=skip_header,\n            encoding=encoding,\n            **fmtparams,\n        )\n\n        test: FlairDataset = CSVClassificationDataset(\n            test_file,\n            column_name_map,\n            label_type=label_type,\n            tokenizer=tokenizer,\n            max_tokens_per_doc=max_tokens_per_doc,\n            max_chars_per_doc=max_chars_per_doc,\n            in_memory=in_memory,\n            skip_header=skip_header,\n            encoding=encoding,\n            **fmtparams,\n        ) if test_file is not None else None\n\n        dev: FlairDataset = CSVClassificationDataset(\n            dev_file,\n            column_name_map,\n            label_type=label_type,\n            tokenizer=tokenizer,\n            max_tokens_per_doc=max_tokens_per_doc,\n            max_chars_per_doc=max_chars_per_doc,\n            in_memory=in_memory,\n            skip_header=skip_header,\n            encoding=encoding,\n            **fmtparams,\n        ) if dev_file is not None else None\n\n        super(CSVClassificationCorpus, self).__init__(\n            train, dev, test, name=str(data_folder)\n        )\n\n\nclass CSVClassificationDataset(FlairDataset):\n    """"""\n    Dataset for text classification from CSV column formatted data.\n    """"""\n    def __init__(\n            self,\n            path_to_file: Union[str, Path],\n            column_name_map: Dict[int, str],\n            label_type: str = ""class"",\n            max_tokens_per_doc: int = -1,\n            max_chars_per_doc: int = -1,\n            tokenizer=segtok_tokenizer,\n            in_memory: bool = True,\n            skip_header: bool = False,\n            encoding: str = \'utf-8\',\n            **fmtparams,\n    ):\n        """"""\n        Instantiates a Dataset for text classification from CSV column formatted data\n\n        :param path_to_file: path to the file with the CSV data\n        :param column_name_map: a column name map that indicates which column is text and which the label(s)\n        :param label_type: name of the label\n        :param max_tokens_per_doc: If set, truncates each Sentence to a maximum number of Tokens\n        :param max_chars_per_doc: If set, truncates each Sentence to a maximum number of chars\n        :param tokenizer: Tokenizer for dataset, default is segtok\n        :param in_memory: If True, keeps dataset as Sentences in memory, otherwise only keeps strings\n        :param skip_header: If True, skips first line because it is header\n        :param encoding: Most datasets are \'utf-8\' but some are \'latin-1\'\n        :param fmtparams: additional parameters for the CSV file reader\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        if type(path_to_file) == str:\n            path_to_file: Path = Path(path_to_file)\n\n        assert path_to_file.exists()\n\n        # variables\n        self.path_to_file = path_to_file\n        self.in_memory = in_memory\n        self.tokenizer = tokenizer\n        self.column_name_map = column_name_map\n        self.max_tokens_per_doc = max_tokens_per_doc\n        self.max_chars_per_doc = max_chars_per_doc\n\n        self.label_type = label_type\n\n        # different handling of in_memory data than streaming data\n        if self.in_memory:\n            self.sentences = []\n        else:\n            self.raw_data = []\n\n        self.total_sentence_count: int = 0\n\n        # most data sets have the token text in the first column, if not, pass \'text\' as column\n        self.text_columns: List[int] = []\n        for column in column_name_map:\n            if column_name_map[column] == ""text"":\n                self.text_columns.append(column)\n\n        with open(self.path_to_file, encoding=encoding) as csv_file:\n\n            csv_reader = csv.reader(csv_file, **fmtparams)\n\n            if skip_header:\n                next(csv_reader, None)  # skip the headers\n\n            for row in csv_reader:\n\n                # test if format is OK\n                wrong_format = False\n                for text_column in self.text_columns:\n                    if text_column >= len(row):\n                        wrong_format = True\n\n                if wrong_format:\n                    continue\n\n                # test if at least one label given\n                has_label = False\n                for column in self.column_name_map:\n                    if self.column_name_map[column].startswith(""label"") and row[column]:\n                        has_label = True\n                        break\n\n                if not has_label:\n                    continue\n\n                if self.in_memory:\n\n                    text = "" "".join(\n                        [row[text_column] for text_column in self.text_columns]\n                    )\n\n                    if self.max_chars_per_doc > 0:\n                        text = text[: self.max_chars_per_doc]\n\n                    sentence = Sentence(text, use_tokenizer=self.tokenizer)\n\n                    for column in self.column_name_map:\n                        if (\n                                self.column_name_map[column].startswith(""label"")\n                                and row[column]\n                        ):\n                            sentence.add_label(label_type, row[column])\n\n                    if 0 < self.max_tokens_per_doc < len(sentence):\n                        sentence.tokens = sentence.tokens[: self.max_tokens_per_doc]\n                    self.sentences.append(sentence)\n\n                else:\n                    self.raw_data.append(row)\n\n                self.total_sentence_count += 1\n\n    def is_in_memory(self) -> bool:\n        return self.in_memory\n\n    def __len__(self):\n        return self.total_sentence_count\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n        if self.in_memory:\n            return self.sentences[index]\n        else:\n            row = self.raw_data[index]\n\n            text = "" "".join([row[text_column] for text_column in self.text_columns])\n\n            if self.max_chars_per_doc > 0:\n                text = text[: self.max_chars_per_doc]\n\n            sentence = Sentence(text, use_tokenizer=self.tokenizer)\n            for column in self.column_name_map:\n                if self.column_name_map[column].startswith(""label"") and row[column]:\n                    sentence.add_label(self.label_type, row[column])\n\n            if 0 < self.max_tokens_per_doc < len(sentence):\n                sentence.tokens = sentence.tokens[: self.max_tokens_per_doc]\n\n            return sentence\n\n\nclass AMAZON_REVIEWS(ClassificationCorpus):\n    """"""\n    A very large corpus of Amazon reviews with positivity ratings. Corpus is downloaded from and documented at\n    https://nijianmo.github.io/amazon/index.html. We download the 5-core subset which is still tens of millions of\n    reviews.\n    """"""\n\n    # noinspection PyDefaultArgument\n    def __init__(\n            self,\n            split_max: int = 30000,\n            label_name_map: Dict[str, str] = {\n                \'1.0\': \'NEGATIVE\',\n                \'2.0\': \'NEGATIVE\',\n                \'3.0\': \'NEGATIVE\',\n                \'4.0\': \'POSITIVE\',\n                \'5.0\': \'POSITIVE\',\n            },\n            skip_labels = [\'3.0\', \'4.0\'],\n            fraction_of_5_star_reviews: int = 10,\n            tokenizer=segtok_tokenizer,\n            **corpusargs\n    ):\n        """"""\n        Constructs corpus object. Split_max indicates how many data points from each of the 28 splits are used, so\n        set this higher or lower to increase/decrease corpus size.\n        :param label_name_map: Map label names to different schema. By default, the 5-star rating is mapped onto 3\n        classes (POSITIVE, NEGATIVE, NEUTRAL)\n        :param split_max: Split_max indicates how many data points from each of the 28 splits are used, so\n        set this higher or lower to increase/decrease corpus size.\n        :param memory_mode: Set to what degree to keep corpus in memory (\'full\', \'partial\' or \'disk\'). Use \'full\'\n        if full corpus and all embeddings fits into memory for speedups during training. Otherwise use \'partial\' and if\n        even this is too much for your memory, use \'disk\'.\n        :param corpusargs: Arguments for ClassificationCorpus\n        """"""\n\n        # dataset name includes the split size\n        dataset_name = self.__class__.__name__.lower() + \'_\' + str(split_max) + \'_\' + str(fraction_of_5_star_reviews)\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if not (data_folder / ""train.txt"").is_file():\n\n            # download each of the 28 splits\n            self.download_and_prepare_amazon_product_file(data_folder, ""AMAZON_FASHION_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""All_Beauty_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Appliances_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Arts_Crafts_and_Sewing_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Arts_Crafts_and_Sewing_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Automotive_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Books_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""CDs_and_Vinyl_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Cell_Phones_and_Accessories_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Clothing_Shoes_and_Jewelry_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Digital_Music_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Electronics_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Gift_Cards_5.json.gz"", split_max, fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Grocery_and_Gourmet_Food_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Home_and_Kitchen_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Industrial_and_Scientific_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Kindle_Store_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Luxury_Beauty_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Magazine_Subscriptions_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Movies_and_TV_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Musical_Instruments_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Office_Products_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Patio_Lawn_and_Garden_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Pet_Supplies_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Prime_Pantry_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Software_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Sports_and_Outdoors_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Tools_and_Home_Improvement_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Toys_and_Games_5.json.gz"", split_max , fraction_of_5_star_reviews)\n            self.download_and_prepare_amazon_product_file(data_folder, ""Video_Games_5.json.gz"", split_max , fraction_of_5_star_reviews)\n\n        super(AMAZON_REVIEWS, self).__init__(\n            data_folder,\n            label_type=\'sentiment\',\n            label_name_map=label_name_map,\n            skip_labels=skip_labels,\n            tokenizer=tokenizer,\n            **corpusargs\n        )\n\n    def download_and_prepare_amazon_product_file(self, data_folder, part_name, max_data_points=None, fraction_of_5_star_reviews=None):\n        amazon__path = ""http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall""\n        cached_path(f""{amazon__path}/{part_name}"", Path(""datasets"") / \'Amazon_Product_Reviews\')\n        import gzip\n        # create dataset directory if necessary\n        if not os.path.exists(data_folder):\n            os.makedirs(data_folder)\n        with open(data_folder / ""train.txt"", ""a"") as train_file:\n\n            write_count = 0\n            review_5_count = 0\n            # download senteval datasets if necessary und unzip\n            with gzip.open(Path(flair.cache_root) / ""datasets"" / \'Amazon_Product_Reviews\' / part_name, ""rb"", ) as f_in:\n                for line in f_in:\n                    parsed_json = json.loads(line)\n                    if \'reviewText\' not in parsed_json:\n                        continue\n                    if parsed_json[\'reviewText\'].strip() == \'\':\n                        continue\n                    text = parsed_json[\'reviewText\'].replace(\'\\n\', \'\')\n\n                    if fraction_of_5_star_reviews and str(parsed_json[\'overall\']) == \'5.0\':\n                        review_5_count += 1\n                        if review_5_count != fraction_of_5_star_reviews:\n                            continue\n                        else:\n                            review_5_count = 0\n\n                    train_file.write(f""__label__{parsed_json[\'overall\']} {text}\\n"")\n\n                    write_count += 1\n                    if max_data_points and write_count >= max_data_points:\n                        break\n\n\nclass IMDB(ClassificationCorpus):\n    """"""\n    Corpus of IMDB movie reviews labeled by sentiment (POSITIVE, NEGATIVE). Downloaded from and documented at\n    http://ai.stanford.edu/~amaas/data/sentiment/.\n    """"""\n    def __init__(self, base_path: Union[str, Path] = None, rebalance_corpus: bool = True, tokenizer=segtok_tokenizer, **corpusargs):\n        """"""\n\n        :param base_path: Provide this only if you store the IMDB corpus in a specific folder, otherwise use default.\n        :param rebalance_corpus: Default splits for this corpus have a strange 50/50 train/test split that are impractical.\n        With rebalance_corpus=True (default setting), corpus is rebalanced to a 80/10/10 train/dev/test split. If you\n        want to use original splits, set this to False.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower() + \'_v2\'\n\n        if rebalance_corpus:\n            dataset_name = dataset_name + \'-rebalanced\'\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        imdb_acl_path = ""http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz""\n        data_path = Path(flair.cache_root) / ""datasets"" / dataset_name\n        data_file = data_path / ""train.txt""\n        if not data_file.is_file():\n            cached_path(imdb_acl_path, Path(""datasets"") / dataset_name)\n            import tarfile\n\n            with tarfile.open(\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / dataset_name\n                    / ""aclImdb_v1.tar.gz"",\n                    ""r:gz"",\n            ) as f_in:\n                datasets = [""train"", ""test""]\n                labels = [""pos"", ""neg""]\n\n                for label in labels:\n                    for dataset in datasets:\n                        f_in.extractall(\n                            data_path,\n                            members=[\n                                m\n                                for m in f_in.getmembers()\n                                if f""{dataset}/{label}"" in m.name\n                            ],\n                        )\n                        with open(f""{data_path}/train-all.txt"", ""at"") as f_p:\n                            current_path = data_path / ""aclImdb"" / dataset / label\n                            for file_name in current_path.iterdir():\n                                if file_name.is_file() and file_name.name.endswith(\n                                        "".txt""\n                                ):\n                                    if label == ""pos"": sentiment_label = \'POSITIVE\'\n                                    if label == ""neg"": sentiment_label = \'NEGATIVE\'\n                                    f_p.write(\n                                        f""__label__{sentiment_label} ""\n                                        + file_name.open(""rt"", encoding=""utf-8"").read()\n                                        + ""\\n""\n                                    )\n\n        super(IMDB, self).__init__(\n            data_folder, tokenizer=tokenizer, **corpusargs\n        )\n\n\nclass NEWSGROUPS(ClassificationCorpus):\n    """"""\n    20 newsgroups corpus available at ""http://qwone.com/~jason/20Newsgroups"", classifying\n    news items into one of 20 categories. Each data point is a full news article so documents may be very long.\n    """"""\n    def __init__(self, base_path: Union[str, Path] = None, tokenizer=segtok_tokenizer, **corpusargs):\n        """"""\n        Instantiates 20 newsgroups corpus.\n        :param base_path: Provide this only if you store the IMDB corpus in a specific folder, otherwise use default.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        twenty_newsgroups_path = (\n            ""http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz""\n        )\n        data_path = Path(flair.cache_root) / ""datasets"" / dataset_name\n        data_file = data_path / ""20news-bydate-train.txt""\n        if not data_file.is_file():\n            cached_path(\n                twenty_newsgroups_path, Path(""datasets"") / dataset_name / ""original""\n            )\n\n            import tarfile\n\n            with tarfile.open(\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / dataset_name\n                    / ""original""\n                    / ""20news-bydate.tar.gz"",\n                    ""r:gz"",\n            ) as f_in:\n                datasets = [""20news-bydate-test"", ""20news-bydate-train""]\n                labels = [\n                    ""alt.atheism"",\n                    ""comp.graphics"",\n                    ""comp.os.ms-windows.misc"",\n                    ""comp.sys.ibm.pc.hardware"",\n                    ""comp.sys.mac.hardware"",\n                    ""comp.windows.x"",\n                    ""misc.forsale"",\n                    ""rec.autos"",\n                    ""rec.motorcycles"",\n                    ""rec.sport.baseball"",\n                    ""rec.sport.hockey"",\n                    ""sci.crypt"",\n                    ""sci.electronics"",\n                    ""sci.med"",\n                    ""sci.space"",\n                    ""soc.religion.christian"",\n                    ""talk.politics.guns"",\n                    ""talk.politics.mideast"",\n                    ""talk.politics.misc"",\n                    ""talk.religion.misc"",\n                ]\n\n                for label in labels:\n                    for dataset in datasets:\n                        f_in.extractall(\n                            data_path / ""original"",\n                            members=[\n                                m\n                                for m in f_in.getmembers()\n                                if f""{dataset}/{label}"" in m.name\n                            ],\n                        )\n                        with open(\n                                f""{data_path}/{dataset}.txt"", ""at"", encoding=""utf-8""\n                        ) as f_p:\n                            current_path = data_path / ""original"" / dataset / label\n                            for file_name in current_path.iterdir():\n                                if file_name.is_file():\n                                    f_p.write(\n                                        f""__label__{label} ""\n                                        + file_name.open(""rt"", encoding=""latin1"")\n                                        .read()\n                                        .replace(""\\n"", "" <n> "")\n                                        + ""\\n""\n                                    )\n\n        super(NEWSGROUPS, self).__init__(\n            data_folder, tokenizer=tokenizer, **corpusargs,\n        )\n\n\nclass SENTIMENT_140(ClassificationCorpus):\n    """"""\n    Twitter sentiment corpus downloaded from and documented at http://help.sentiment140.com/for-students. Two sentiments\n    in train data (POSITIVE, NEGATIVE) and three sentiments in test data (POSITIVE, NEGATIVE, NEUTRAL).\n    """"""\n    def __init__(\n            self,\n            label_name_map=None,\n            tokenizer=segtok_tokenizer,\n            **corpusargs,\n    ):\n        """"""\n        Instantiates twitter sentiment corpus.\n        :param label_name_map: By default, the numeric values are mapped to (\'NEGATIVE\', \'POSITIVE\' and \'NEUTRAL\')\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        # by defaut, map point score to POSITIVE / NEGATIVE values\n        if label_name_map is None:\n            label_name_map = {\'0\': \'NEGATIVE\',\n                              \'2\': \'NEUTRAL\',\n                              \'4\': \'POSITIVE\'}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if True or not (data_folder / ""train.txt"").is_file():\n\n            # download senteval datasets if necessary und unzip\n            sentiment_url = ""https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip""\n            cached_path(sentiment_url, Path(""datasets"") / dataset_name / \'raw\')\n            senteval_folder = Path(flair.cache_root) / ""datasets"" / dataset_name / \'raw\'\n            unzip_file(senteval_folder / ""trainingandtestdata.zip"", senteval_folder)\n\n            # create dataset directory if necessary\n            if not os.path.exists(data_folder):\n                os.makedirs(data_folder)\n\n            # create train.txt file from CSV\n            with open(data_folder / ""train.txt"", ""w"") as train_file:\n\n                with open(senteval_folder / ""training.1600000.processed.noemoticon.csv"", encoding=\'latin-1\') as csv_train:\n\n                    csv_reader = csv.reader(csv_train)\n\n                    for row in csv_reader:\n\n                        label = row[0]\n                        text = row[5]\n                        train_file.write(f""__label__{label} {text}\\n"")\n\n            # create test.txt file from CSV\n            with open(data_folder / ""test.txt"", ""w"") as train_file:\n\n                with open(senteval_folder / ""testdata.manual.2009.06.14.csv"", encoding=\'latin-1\') as csv_train:\n\n                    csv_reader = csv.reader(csv_train)\n\n                    for row in csv_reader:\n\n                        label = row[0]\n                        text = row[5]\n                        train_file.write(f""__label__{label} {text}\\n"")\n\n        super(SENTIMENT_140, self).__init__(\n            data_folder, label_type=\'sentiment\', tokenizer=tokenizer, label_name_map=label_name_map, **corpusargs,\n        )\n\n\nclass SENTEVAL_CR(ClassificationCorpus):\n    """"""\n    The customer reviews dataset of SentEval, see https://github.com/facebookresearch/SentEval, classified into\n    NEGATIVE or POSITIVE sentiment.\n    """"""\n    def __init__(\n            self,\n            tokenizer=space_tokenizer,\n            **corpusargs,\n    ):\n        """"""\n        Instantiates SentEval customer reviews dataset.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if not (data_folder / ""train.txt"").is_file():\n\n            # download senteval datasets if necessary und unzip\n            senteval_path = ""https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip""\n            cached_path(senteval_path, Path(""datasets"") / ""senteval"")\n            senteval_folder = Path(flair.cache_root) / ""datasets"" / ""senteval""\n            unzip_file(senteval_folder / ""datasmall_NB_ACL12.zip"", senteval_folder)\n\n            # create dataset directory if necessary\n            if not os.path.exists(data_folder):\n                os.makedirs(data_folder)\n\n            # create train.txt file by iterating over pos and neg file\n            with open(data_folder / ""train.txt"", ""a"") as train_file:\n\n                with open(senteval_folder / ""data"" / ""customerr"" / ""custrev.pos"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__POSITIVE {line}"")\n\n                with open(senteval_folder / ""data"" / ""customerr"" / ""custrev.neg"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__NEGATIVE {line}"")\n\n        super(SENTEVAL_CR, self).__init__(\n            data_folder, label_type=\'sentiment\', tokenizer=tokenizer, **corpusargs,\n        )\n\n\nclass SENTEVAL_MR(ClassificationCorpus):\n    """"""\n    The movie reviews dataset of SentEval, see https://github.com/facebookresearch/SentEval, classified into\n    NEGATIVE or POSITIVE sentiment.\n    """"""\n    def __init__(\n            self,\n            tokenizer=space_tokenizer,\n            **corpusargs\n    ):\n        """"""\n        Instantiates SentEval movie reviews dataset.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if not (data_folder / ""train.txt"").is_file():\n\n            # download senteval datasets if necessary und unzip\n            senteval_path = ""https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip""\n            cached_path(senteval_path, Path(""datasets"") / ""senteval"")\n            senteval_folder = Path(flair.cache_root) / ""datasets"" / ""senteval""\n            unzip_file(senteval_folder / ""datasmall_NB_ACL12.zip"", senteval_folder)\n\n            # create dataset directory if necessary\n            if not os.path.exists(data_folder):\n                os.makedirs(data_folder)\n\n            # create train.txt file by iterating over pos and neg file\n            with open(data_folder / ""train.txt"", ""a"") as train_file:\n\n                with open(senteval_folder / ""data"" / ""rt10662"" / ""rt-polarity.pos"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__POSITIVE {line}"")\n\n                with open(senteval_folder / ""data"" / ""rt10662"" / ""rt-polarity.neg"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__NEGATIVE {line}"")\n\n        super(SENTEVAL_MR, self).__init__(\n            data_folder, label_type=\'sentiment\', tokenizer=tokenizer, **corpusargs\n        )\n\n\nclass SENTEVAL_SUBJ(ClassificationCorpus):\n    """"""\n    The subjectivity dataset of SentEval, see https://github.com/facebookresearch/SentEval, classified into\n    SUBJECTIVE or OBJECTIVE sentiment.\n    """"""\n    def __init__(\n            self,\n            tokenizer=space_tokenizer,\n            **corpusargs,\n    ):\n        """"""\n        Instantiates SentEval subjectivity dataset.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if not (data_folder / ""train.txt"").is_file():\n\n            # download senteval datasets if necessary und unzip\n            senteval_path = ""https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip""\n            cached_path(senteval_path, Path(""datasets"") / ""senteval"")\n            senteval_folder = Path(flair.cache_root) / ""datasets"" / ""senteval""\n            unzip_file(senteval_folder / ""datasmall_NB_ACL12.zip"", senteval_folder)\n\n            # create dataset directory if necessary\n            if not os.path.exists(data_folder):\n                os.makedirs(data_folder)\n\n            # create train.txt file by iterating over pos and neg file\n            with open(data_folder / ""train.txt"", ""a"") as train_file:\n\n                with open(senteval_folder / ""data"" / ""subj"" / ""subj.subjective"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__SUBJECTIVE {line}"")\n\n                with open(senteval_folder / ""data"" / ""subj"" / ""subj.objective"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__OBJECTIVE {line}"")\n\n        super(SENTEVAL_SUBJ, self).__init__(\n            data_folder, label_type=\'objectivity\', tokenizer=tokenizer, **corpusargs,\n        )\n\n\nclass SENTEVAL_MPQA(ClassificationCorpus):\n    """"""\n    The opinion-polarity dataset of SentEval, see https://github.com/facebookresearch/SentEval, classified into\n    NEGATIVE or POSITIVE polarity.\n    """"""\n\n    def __init__(\n            self,\n            tokenizer=space_tokenizer,\n            **corpusargs,\n    ):\n        """"""\n        Instantiates SentEval opinion polarity dataset.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if not (data_folder / ""train.txt"").is_file():\n\n            # download senteval datasets if necessary und unzip\n            senteval_path = ""https://dl.fbaipublicfiles.com/senteval/senteval_data/datasmall_NB_ACL12.zip""\n            cached_path(senteval_path, Path(""datasets"") / ""senteval"")\n            senteval_folder = Path(flair.cache_root) / ""datasets"" / ""senteval""\n            unzip_file(senteval_folder / ""datasmall_NB_ACL12.zip"", senteval_folder)\n\n            # create dataset directory if necessary\n            if not os.path.exists(data_folder):\n                os.makedirs(data_folder)\n\n            # create train.txt file by iterating over pos and neg file\n            with open(data_folder / ""train.txt"", ""a"") as train_file:\n\n                with open(senteval_folder / ""data"" / ""mpqa"" / ""mpqa.pos"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__POSITIVE {line}"")\n\n                with open(senteval_folder / ""data"" / ""mpqa"" / ""mpqa.neg"", encoding=""latin1"") as file:\n                    for line in file:\n                        train_file.write(f""__label__NEGATIVE {line}"")\n\n        super(SENTEVAL_MPQA, self).__init__(\n            data_folder, label_type=\'sentiment\', tokenizer=tokenizer, **corpusargs,\n        )\n\n\nclass SENTEVAL_SST_BINARY(ClassificationCorpus):\n    """"""\n    The Stanford sentiment treebank dataset of SentEval, see https://github.com/facebookresearch/SentEval, classified\n    into NEGATIVE or POSITIVE sentiment.\n    """"""\n\n    def __init__(\n            self,\n            tokenizer=space_tokenizer,\n            **corpusargs,\n    ):\n        """"""\n        Instantiates SentEval Stanford sentiment treebank dataset.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower() + \'_v2\'\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if not (data_folder / ""train.txt"").is_file():\n\n            # download senteval datasets if necessary und unzip\n            cached_path(\'https://raw.githubusercontent.com/PrincetonML/SIF/master/data/sentiment-train\', Path(""datasets"") / dataset_name / \'raw\')\n            cached_path(\'https://raw.githubusercontent.com/PrincetonML/SIF/master/data/sentiment-test\', Path(""datasets"") / dataset_name / \'raw\')\n            cached_path(\'https://raw.githubusercontent.com/PrincetonML/SIF/master/data/sentiment-dev\', Path(""datasets"") / dataset_name / \'raw\')\n\n            # create train.txt file by iterating over pos and neg file\n            with open(data_folder / ""train.txt"", ""a"") as out_file, open(data_folder / \'raw\' / ""sentiment-train"") as in_file:\n                for line in in_file:\n                    fields = line.split(\'\\t\')\n                    label = \'POSITIVE\' if fields[1].rstrip() == \'1\' else \'NEGATIVE\'\n                    out_file.write(f""__label__{label} {fields[0]}\\n"")\n\n        super(SENTEVAL_SST_BINARY, self).__init__(\n            data_folder,\n            tokenizer=tokenizer,\n            **corpusargs,\n        )\n\n\nclass SENTEVAL_SST_GRANULAR(ClassificationCorpus):\n    """"""\n    The Stanford sentiment treebank dataset of SentEval, see https://github.com/facebookresearch/SentEval, classified\n    into 5 sentiment classes.\n    """"""\n\n    def __init__(\n            self,\n            tokenizer=space_tokenizer,\n            **corpusargs,\n    ):\n        """"""\n        Instantiates SentEval Stanford sentiment treebank dataset.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        data_folder = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        # download data if necessary\n        if not (data_folder / ""train.txt"").is_file():\n\n            # download senteval datasets if necessary und unzip\n            cached_path(\'https://raw.githubusercontent.com/AcademiaSinicaNLPLab/sentiment_dataset/master/data/stsa.fine.train\', Path(""datasets"") / dataset_name / \'raw\')\n            cached_path(\'https://raw.githubusercontent.com/AcademiaSinicaNLPLab/sentiment_dataset/master/data/stsa.fine.test\', Path(""datasets"") / dataset_name / \'raw\')\n            cached_path(\'https://raw.githubusercontent.com/AcademiaSinicaNLPLab/sentiment_dataset/master/data/stsa.fine.dev\', Path(""datasets"") / dataset_name / \'raw\')\n\n            # convert to FastText format\n            for split in [\'train\', \'dev\', \'test\']:\n                with open(data_folder / f""{split}.txt"", ""w"") as train_file:\n\n                    with open(data_folder / \'raw\' / f\'stsa.fine.{split}\', encoding=""latin1"") as file:\n                        for line in file:\n                            train_file.write(f""__label__{line[0]} {line[2:]}"")\n\n        super(SENTEVAL_SST_GRANULAR, self).__init__(\n            data_folder,\n            tokenizer=tokenizer,\n            **corpusargs,\n        )\n\n\nclass TREC_50(ClassificationCorpus):\n    """"""\n    The TREC Question Classification Corpus, classifying questions into 50 fine-grained answer types.\n    """"""\n\n    def __init__(self, base_path: Union[str, Path] = None, tokenizer=space_tokenizer, **corpusargs):\n        """"""\n        Instantiates TREC Question Classification Corpus with 50 classes.\n        :param base_path: Provide this only if you store the TREC corpus in a specific folder, otherwise use default.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        trec_path = ""https://cogcomp.seas.upenn.edu/Data/QA/QC/""\n\n        original_filenames = [""train_5500.label"", ""TREC_10.label""]\n        new_filenames = [""train.txt"", ""test.txt""]\n        for original_filename in original_filenames:\n            cached_path(\n                f""{trec_path}{original_filename}"",\n                Path(""datasets"") / dataset_name / ""original"",\n            )\n\n        data_file = data_folder / new_filenames[0]\n\n        if not data_file.is_file():\n            for original_filename, new_filename in zip(\n                    original_filenames, new_filenames\n            ):\n                with open(\n                        data_folder / ""original"" / original_filename,\n                        ""rt"",\n                        encoding=""latin1"",\n                ) as open_fp:\n                    with open(\n                            data_folder / new_filename, ""wt"", encoding=""utf-8""\n                    ) as write_fp:\n                        for line in open_fp:\n                            line = line.rstrip()\n                            fields = line.split()\n                            old_label = fields[0]\n                            question = "" "".join(fields[1:])\n\n                            # Create flair compatible labels\n                            # TREC-6 : NUM:dist -> __label__NUM\n                            # TREC-50: NUM:dist -> __label__NUM:dist\n                            new_label = ""__label__""\n                            new_label += old_label\n\n                            write_fp.write(f""{new_label} {question}\\n"")\n\n        super(TREC_50, self).__init__(\n            data_folder, tokenizer=tokenizer, **corpusargs,\n        )\n\n\nclass TREC_6(ClassificationCorpus):\n    """"""\n    The TREC Question Classification Corpus, classifying questions into 6 coarse-grained answer types\n    (DESC, HUM, LOC, ENTY, NUM, ABBR).\n    """"""\n\n    def __init__(self, base_path: Union[str, Path] = None,  tokenizer=space_tokenizer, **corpusargs):\n        """"""\n        Instantiates TREC Question Classification Corpus with 6 classes.\n        :param base_path: Provide this only if you store the TREC corpus in a specific folder, otherwise use default.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        trec_path = ""https://cogcomp.seas.upenn.edu/Data/QA/QC/""\n\n        original_filenames = [""train_5500.label"", ""TREC_10.label""]\n        new_filenames = [""train.txt"", ""test.txt""]\n        for original_filename in original_filenames:\n            cached_path(\n                f""{trec_path}{original_filename}"",\n                Path(""datasets"") / dataset_name / ""original"",\n            )\n\n        data_file = data_folder / new_filenames[0]\n\n        if not data_file.is_file():\n            for original_filename, new_filename in zip(\n                    original_filenames, new_filenames\n            ):\n                with open(\n                        data_folder / ""original"" / original_filename,\n                        ""rt"",\n                        encoding=""latin1"",\n                ) as open_fp:\n                    with open(\n                            data_folder / new_filename, ""wt"", encoding=""utf-8""\n                    ) as write_fp:\n                        for line in open_fp:\n                            line = line.rstrip()\n                            fields = line.split()\n                            old_label = fields[0]\n                            question = "" "".join(fields[1:])\n\n                            # Create flair compatible labels\n                            # TREC-6 : NUM:dist -> __label__NUM\n                            # TREC-50: NUM:dist -> __label__NUM:dist\n                            new_label = ""__label__""\n                            new_label += old_label.split("":"")[0]\n\n                            write_fp.write(f""{new_label} {question}\\n"")\n\n        super(TREC_6, self).__init__(\n            data_folder, label_type=\'question_type\', tokenizer=tokenizer, **corpusargs,\n        )\n\n\ndef _download_wassa_if_not_there(emotion, data_folder, dataset_name):\n    for split in [""train"", ""dev"", ""test""]:\n\n        data_file = data_folder / f""{emotion}-{split}.txt""\n\n        if not data_file.is_file():\n\n            if split == ""train"":\n                url = f""http://saifmohammad.com/WebDocs/EmoInt%20Train%20Data/{emotion}-ratings-0to1.train.txt""\n            if split == ""dev"":\n                url = f""http://saifmohammad.com/WebDocs/EmoInt%20Dev%20Data%20With%20Gold/{emotion}-ratings-0to1.dev.gold.txt""\n            if split == ""test"":\n                url = f""http://saifmohammad.com/WebDocs/EmoInt%20Test%20Gold%20Data/{emotion}-ratings-0to1.test.gold.txt""\n\n            path = cached_path(url, Path(""datasets"") / dataset_name)\n\n            with open(path, ""r"") as f:\n                with open(data_file, ""w"") as out:\n                    next(f)\n                    for line in f:\n                        fields = line.split(""\\t"")\n                        out.write(f""__label__{fields[3].rstrip()} {fields[1]}\\n"")\n\n            os.remove(path)\n\n\nclass WASSA_ANGER(ClassificationCorpus):\n    """"""\n    WASSA-2017 anger emotion-intensity dataset downloaded from and documented at\n     https://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html\n    """"""\n    def __init__(self, base_path: Union[str, Path] = None, tokenizer=segtok_tokenizer, **corpusargs):\n        """"""\n        Instantiates WASSA-2017 anger emotion-intensity dataset\n        :param base_path: Provide this only if you store the WASSA corpus in a specific folder, otherwise use default.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wassa_if_not_there(""anger"", data_folder, dataset_name)\n\n        super(WASSA_ANGER, self).__init__(\n            data_folder, tokenizer=tokenizer, **corpusargs,\n        )\n\n\nclass WASSA_FEAR(ClassificationCorpus):\n    """"""\n    WASSA-2017 fear emotion-intensity dataset downloaded from and documented at\n     https://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html\n    """"""\n    def __init__(self, base_path: Union[str, Path] = None, tokenizer=segtok_tokenizer, **corpusargs):\n        """"""\n        Instantiates WASSA-2017 fear emotion-intensity dataset\n        :param base_path: Provide this only if you store the WASSA corpus in a specific folder, otherwise use default.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wassa_if_not_there(""fear"", data_folder, dataset_name)\n\n        super(WASSA_FEAR, self).__init__(\n            data_folder, tokenizer=tokenizer, **corpusargs\n        )\n\n\nclass WASSA_JOY(ClassificationCorpus):\n    """"""\n    WASSA-2017 joy emotion-intensity dataset downloaded from and documented at\n     https://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html\n    """"""\n    def __init__(self, base_path: Union[str, Path] = None, tokenizer=segtok_tokenizer, **corpusargs):\n        """"""\n        Instantiates WASSA-2017 joy emotion-intensity dataset\n        :param base_path: Provide this only if you store the WASSA corpus in a specific folder, otherwise use default.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wassa_if_not_there(""joy"", data_folder, dataset_name)\n\n        super(WASSA_JOY, self).__init__(\n            data_folder, tokenizer=tokenizer, **corpusargs,\n        )\n\n\nclass WASSA_SADNESS(ClassificationCorpus):\n    """"""\n    WASSA-2017 sadness emotion-intensity dataset downloaded from and documented at\n     https://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html\n    """"""\n    def __init__(self, base_path: Union[str, Path] = None, tokenizer=segtok_tokenizer, **corpusargs):\n        """"""\n        Instantiates WASSA-2017 sadness emotion-intensity dataset\n        :param base_path: Provide this only if you store the WASSA corpus in a specific folder, otherwise use default.\n        :param corpusargs: Other args for ClassificationCorpus.\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wassa_if_not_there(""sadness"", data_folder, dataset_name)\n\n        super(WASSA_SADNESS, self).__init__(\n            data_folder, tokenizer=tokenizer, **corpusargs,\n        )\n'"
flair/datasets/sequence_labeling.py,0,"b'import logging\nimport re\nimport os\nfrom pathlib import Path\nfrom typing import Union, Dict, List\n\nimport flair\nfrom flair.data import Corpus, FlairDataset, Sentence, Token\nfrom flair.datasets.base import find_train_dev_test_files\nfrom flair.file_utils import cached_path\n\nlog = logging.getLogger(""flair"")\n\n\nclass ColumnCorpus(Corpus):\n    def __init__(\n            self,\n            data_folder: Union[str, Path],\n            column_format: Dict[int, str],\n            train_file=None,\n            test_file=None,\n            dev_file=None,\n            tag_to_bioes=None,\n            column_delimiter: str = r""\\s+"",\n            comment_symbol: str = None,\n            encoding: str = ""utf-8"",\n            document_separator_token: str = None,\n            skip_first_line: bool = False,\n            in_memory: bool = True,\n    ):\n        """"""\n        Instantiates a Corpus from CoNLL column-formatted task data such as CoNLL03 or CoNLL2000.\n\n        :param data_folder: base folder with the task data\n        :param column_format: a map specifying the column format\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param tag_to_bioes: whether to convert to BIOES tagging scheme\n        :param column_delimiter: default is to split on any separatator, but you can overwrite for instance with ""\\t""\n        to split only on tabs\n        :param comment_symbol: if set, lines that begin with this symbol are treated as comments\n        :param document_separator_token: If provided, multiple sentences are read into one object. Provide the string token\n        that indicates that a new document begins\n        :param skip_first_line: set to True if your dataset has a header line\n        :param in_memory: If set to True, the dataset is kept in memory as Sentence objects, otherwise does disk reads\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        # find train, dev and test files if not specified\n        dev_file, test_file, train_file = \\\n            find_train_dev_test_files(data_folder, dev_file, test_file, train_file)\n\n        # get train data\n        train = ColumnDataset(\n            train_file,\n            column_format,\n            tag_to_bioes,\n            encoding=encoding,\n            comment_symbol=comment_symbol,\n            column_delimiter=column_delimiter,\n            in_memory=in_memory,\n            document_separator_token=document_separator_token,\n            skip_first_line=skip_first_line,\n        )\n\n        # read in test file if exists\n        test = ColumnDataset(\n            test_file,\n            column_format,\n            tag_to_bioes,\n            encoding=encoding,\n            comment_symbol=comment_symbol,\n            column_delimiter=column_delimiter,\n            in_memory=in_memory,\n            document_separator_token=document_separator_token,\n            skip_first_line=skip_first_line,\n        ) if test_file is not None else None\n\n        # read in dev file if exists\n        dev = ColumnDataset(\n            dev_file,\n            column_format,\n            tag_to_bioes,\n            encoding=encoding,\n            comment_symbol=comment_symbol,\n            column_delimiter=column_delimiter,\n            in_memory=in_memory,\n            document_separator_token=document_separator_token,\n            skip_first_line=skip_first_line,\n        ) if dev_file is not None else None\n\n        super(ColumnCorpus, self).__init__(train, dev, test, name=str(data_folder))\n\n\nclass ColumnDataset(FlairDataset):\n\n    # special key for space after\n    SPACE_AFTER_KEY = ""space-after""\n\n    def __init__(\n            self,\n            path_to_column_file: Union[str, Path],\n            column_name_map: Dict[int, str],\n            tag_to_bioes: str = None,\n            column_delimiter: str = r""\\s+"",\n            comment_symbol: str = None,\n            in_memory: bool = True,\n            document_separator_token: str = None,\n            encoding: str = ""utf-8"",\n            skip_first_line: bool = False,\n    ):\n        """"""\n        Instantiates a column dataset (typically used for sequence labeling or word-level prediction).\n\n        :param path_to_column_file: path to the file with the column-formatted data\n        :param column_name_map: a map specifying the column format\n        :param tag_to_bioes: whether to convert to BIOES tagging scheme\n        :param column_delimiter: default is to split on any separatator, but you can overwrite for instance with ""\\t""\n        to split only on tabs\n        :param comment_symbol: if set, lines that begin with this symbol are treated as comments\n        :param in_memory: If set to True, the dataset is kept in memory as Sentence objects, otherwise does disk reads\n        :param document_separator_token: If provided, multiple sentences are read into one object. Provide the string token\n        that indicates that a new document begins\n        :param skip_first_line: set to True if your dataset has a header line\n        """"""\n        if type(path_to_column_file) is str:\n            path_to_column_file = Path(path_to_column_file)\n        assert path_to_column_file.exists()\n        self.path_to_column_file = path_to_column_file\n        self.tag_to_bioes = tag_to_bioes\n        self.column_name_map = column_name_map\n        self.column_delimiter = column_delimiter\n        self.comment_symbol = comment_symbol\n        self.document_separator_token = document_separator_token\n\n        # store either Sentence objects in memory, or only file offsets\n        self.in_memory = in_memory\n        if self.in_memory:\n            self.sentences: List[Sentence] = []\n        else:\n            self.indices: List[int] = []\n\n        self.total_sentence_count: int = 0\n\n        # most data sets have the token text in the first column, if not, pass \'text\' as column\n        self.text_column: int = 0\n        for column in self.column_name_map:\n            if column_name_map[column] == ""text"":\n                self.text_column = column\n\n        # determine encoding of text file\n        self.encoding = encoding\n\n        sentence: Sentence = Sentence()\n        sentence_started: bool = False\n        with open(str(self.path_to_column_file), encoding=self.encoding) as f:\n\n            # skip first line if to selected\n            if skip_first_line:\n                f.readline()\n\n            line = f.readline()\n            position = 0\n\n            while line:\n\n                if self.comment_symbol is not None and line.startswith(comment_symbol):\n                    line = f.readline()\n                    continue\n\n                if self.__line_completes_sentence(line):\n\n                    if sentence_started:\n\n                        if self.in_memory:\n                            if self.tag_to_bioes is not None:\n                                sentence.convert_tag_scheme(\n                                    tag_type=self.tag_to_bioes, target_scheme=""iobes""\n                                )\n                            self.sentences.append(sentence)\n                        else:\n                            self.indices.append(position)\n                            position = f.tell()\n                        self.total_sentence_count += 1\n                    sentence: Sentence = Sentence()\n                    sentence_started = False\n\n                elif self.in_memory:\n                    token = self._parse_token(line)\n                    if not line.isspace():\n                        sentence.add_token(token)\n                        sentence_started = True\n\n                elif not line.isspace():\n                    sentence_started = True\n\n                line = f.readline()\n\n        if sentence_started:\n            if self.in_memory:\n                self.sentences.append(sentence)\n            else:\n                self.indices.append(position)\n            self.total_sentence_count += 1\n\n    def _parse_token(self, line: str) -> Token:\n        fields: List[str] = re.split(self.column_delimiter, line)\n        token = Token(fields[self.text_column])\n        for column in self.column_name_map:\n            if len(fields) > column:\n                if column != self.text_column and self.column_name_map[column] != self.SPACE_AFTER_KEY:\n                    token.add_label(\n                        self.column_name_map[column], fields[column]\n                    )\n                if self.column_name_map[column] == self.SPACE_AFTER_KEY and fields[column] == \'-\':\n                    token.whitespace_after = False\n        return token\n\n    def __line_completes_sentence(self, line: str) -> bool:\n        sentence_completed = line.isspace()\n        if self.document_separator_token:\n            sentence_completed = False\n            fields: List[str] = re.split(self.column_delimiter, line)\n            if len(fields) >= self.text_column:\n                if fields[self.text_column] == self.document_separator_token:\n                    sentence_completed = True\n        return sentence_completed\n\n    def is_in_memory(self) -> bool:\n        return self.in_memory\n\n    def __len__(self):\n        return self.total_sentence_count\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n\n        if self.in_memory:\n            sentence = self.sentences[index]\n\n        else:\n            with open(str(self.path_to_column_file), encoding=self.encoding) as file:\n                file.seek(self.indices[index])\n                line = file.readline()\n                sentence: Sentence = Sentence()\n                while line:\n                    if self.comment_symbol is not None and line.startswith(\n                            self.comment_symbol\n                    ):\n                        line = file.readline()\n                        continue\n\n                    if self.__line_completes_sentence(line):\n                        if len(sentence) > 0:\n                            if self.tag_to_bioes is not None:\n                                sentence.convert_tag_scheme(\n                                    tag_type=self.tag_to_bioes, target_scheme=""iobes""\n                                )\n                            return sentence\n\n                    else:\n                        token = self._parse_token(line)\n                        if not line.isspace():\n                            sentence.add_token(token)\n\n                    line = file.readline()\n        return sentence\n\n\nclass BIOFID(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""lemma"", 2: ""pos"", 3: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        biofid_path = ""https://raw.githubusercontent.com/texttechnologylab/BIOfid/master/BIOfid-Dataset-NER/""\n        cached_path(f""{biofid_path}train.conll"", Path(""datasets"") / dataset_name)\n        cached_path(f""{biofid_path}dev.conll"", Path(""datasets"") / dataset_name)\n        cached_path(f""{biofid_path}test.conll"", Path(""datasets"") / dataset_name)\n\n        super(BIOFID, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass CONLL_03(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n            document_as_sequence: bool = False,\n    ):\n        """"""\n        Initialize the CoNLL-03 corpus. This is only possible if you\'ve manually downloaded it to your machine.\n        Obtain the corpus from https://www.clips.uantwerpen.be/conll2003/ner/ and put the eng.testa, .testb, .train\n        files in a folder called \'conll_03\'. Then set the base_path parameter in the constructor to the path to the\n        parent directory where the conll_03 folder resides.\n        :param base_path: Path to the CoNLL-03 corpus (i.e. \'conll_03\' folder) on your machine\n        :param tag_to_bioes: NER by default, need not be changed, but you could also select \'pos\' or \'np\' to predict\n        POS tags or chunks respectively\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        """"""\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""np"", 3: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # check if data there\n        if not data_folder.exists():\n            log.warning(""-"" * 100)\n            log.warning(f\'WARNING: CoNLL-03 dataset not found at ""{data_folder}"".\')\n            log.warning(\n                \'Instructions for obtaining the data can be found here: https://www.clips.uantwerpen.be/conll2003/ner/""\'\n            )\n            log.warning(""-"" * 100)\n\n        super(CONLL_03, self).__init__(\n            data_folder,\n            columns,\n            tag_to_bioes=tag_to_bioes,\n            in_memory=in_memory,\n            document_separator_token=None if not document_as_sequence else ""-DOCSTART-"",\n        )\n\n\nclass CONLL_03_GERMAN(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n            document_as_sequence: bool = False,\n    ):\n        """"""\n        Initialize the CoNLL-03 corpus for German. This is only possible if you\'ve manually downloaded it to your machine.\n        Obtain the corpus from https://www.clips.uantwerpen.be/conll2003/ner/ and put the respective files in a folder called\n        \'conll_03_german\'. Then set the base_path parameter in the constructor to the path to the parent directory where\n        the conll_03_german folder resides.\n        :param base_path: Path to the CoNLL-03 corpus (i.e. \'conll_03_german\' folder) on your machine\n        :param tag_to_bioes: NER by default, need not be changed, but you could also select \'lemma\', \'pos\' or \'np\' to predict\n        word lemmas, POS tags or chunks respectively\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        """"""\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""lemma"", 2: ""pos"", 3: ""np"", 4: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # check if data there\n        if not data_folder.exists():\n            log.warning(""-"" * 100)\n            log.warning(f\'WARNING: CoNLL-03 dataset not found at ""{data_folder}"".\')\n            log.warning(\n                \'Instructions for obtaining the data can be found here: https://www.clips.uantwerpen.be/conll2003/ner/""\'\n            )\n            log.warning(""-"" * 100)\n\n        super(CONLL_03_GERMAN, self).__init__(\n            data_folder,\n            columns,\n            tag_to_bioes=tag_to_bioes,\n            in_memory=in_memory,\n            document_separator_token=None if not document_as_sequence else ""-DOCSTART-"",\n        )\n\n\nclass CONLL_03_DUTCH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n            document_as_sequence: bool = False,\n    ):\n        """"""\n        Initialize the CoNLL-03 corpus for Dutch. The first time you call this constructor it will automatically\n        download the dataset.\n        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n        to point to a different folder but typically this should not be necessary.\n        :param tag_to_bioes: NER by default, need not be changed, but you could also select \'pos\' to predict\n        POS tags instead\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        """"""\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        conll_02_path = ""https://www.clips.uantwerpen.be/conll2002/ner/data/""\n        cached_path(f""{conll_02_path}ned.testa"", Path(""datasets"") / dataset_name)\n        cached_path(f""{conll_02_path}ned.testb"", Path(""datasets"") / dataset_name)\n        cached_path(f""{conll_02_path}ned.train"", Path(""datasets"") / dataset_name)\n\n        super(CONLL_03_DUTCH, self).__init__(\n            data_folder,\n            columns,\n            tag_to_bioes=tag_to_bioes,\n            encoding=""latin-1"",\n            in_memory=in_memory,\n            document_separator_token=None if not document_as_sequence else ""-DOCSTART-"",\n        )\n        \n        \ndef add_IOB2_tags(data_file: Union[str, Path], encoding: str = ""utf8""):\n        """"""\n    Function that adds IOB2 tags if only chunk names are provided (e.g. words are tagged PER instead\n    of B-PER or I-PER). Replaces \'0\' with \'O\' as the no-chunk tag since ColumnCorpus expects\n    the letter \'O\'. Additionaly it removes lines with no tags in the data file and can also\n    be used if the data is only partialy IOB tagged.\n    Parameters\n    ----------\n    data_file : Union[str, Path]\n        Path to the data file. \n    encoding : str, optional\n        Encoding used in open function. The default is ""utf8"".\n\n    """"""\n        with open(file=data_file, mode=\'r\', encoding=encoding) as f:\n            lines = f.readlines()\n        with open(file=data_file, mode=\'w\', encoding=encoding) as f:\n            pred = \'O\'  #remembers tag of predecessing line\n            for line in lines:\n                line_list = line.split()\n                if len(line_list) == 2: # word with tag\n                    word = line_list[0]\n                    tag = line_list[1]\n                    if tag in [\'0\',\'O\']: #no chunk\n                        f.write(word + \' O\\n\')\n                        pred = \'O\'\n                    elif \'-\' not in tag: #no IOB tags\n                        if pred == \'O\': #found a new chunk\n                            f.write(word + \' B-\'+ tag +\'\\n\')\n                            pred = tag\n                        else: #found further part of chunk or new chunk directly after old chunk\n                            if pred == tag:\n                                f.write(word + \' I-\'+ tag +\'\\n\')\n                            else:\n                                f.write(word + \' B-\'+ tag +\'\\n\')\n                                pred = tag\n                    else: #line already has IOB tag (tag contains \'-\')\n                        f.write(line)\n                        pred = tag.split(\'-\')[1]\n                elif len(line_list) == 0: #empty line\n                    f.write(\'\\n\')\n                    pred = \'O\'\n\n                \n\n\nclass CONLL_03_SPANISH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        """"""\n        Initialize the CoNLL-03 corpus for Spanish. The first time you call this constructor it will automatically\n        download the dataset.\n        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n        to point to a different folder but typically this should not be necessary.\n        :param tag_to_bioes: NER by default, should not be changed\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        """"""\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        conll_02_path = ""https://www.clips.uantwerpen.be/conll2002/ner/data/""\n        cached_path(f""{conll_02_path}esp.testa"", Path(""datasets"") / dataset_name)\n        cached_path(f""{conll_02_path}esp.testb"", Path(""datasets"") / dataset_name)\n        cached_path(f""{conll_02_path}esp.train"", Path(""datasets"") / dataset_name)\n\n        super(CONLL_03_SPANISH, self).__init__(\n            data_folder,\n            columns,\n            tag_to_bioes=tag_to_bioes,\n            encoding=""latin-1"",\n            in_memory=in_memory,\n        )\n\n\nclass CONLL_2000(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""np"",\n            in_memory: bool = True,\n    ):\n        """"""\n        Initialize the CoNLL-2000 corpus for English chunking.\n        The first time you call this constructor it will automatically download the dataset.\n        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n        to point to a different folder but typically this should not be necessary.\n        :param tag_to_bioes: \'np\' by default, should not be changed, but you can set \'pos\' instead to predict POS tags\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        """"""\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""np""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        conll_2000_path = ""https://www.clips.uantwerpen.be/conll2000/chunking/""\n        data_file = Path(flair.cache_root) / ""datasets"" / dataset_name / ""train.txt""\n        if not data_file.is_file():\n            cached_path(\n                f""{conll_2000_path}train.txt.gz"", Path(""datasets"") / dataset_name\n            )\n            cached_path(\n                f""{conll_2000_path}test.txt.gz"", Path(""datasets"") / dataset_name\n            )\n            import gzip, shutil\n\n            with gzip.open(\n                    Path(flair.cache_root) / ""datasets"" / dataset_name / ""train.txt.gz"",\n                    ""rb"",\n            ) as f_in:\n                with open(\n                        Path(flair.cache_root) / ""datasets"" / dataset_name / ""train.txt"",\n                        ""wb"",\n                ) as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n            with gzip.open(\n                    Path(flair.cache_root) / ""datasets"" / dataset_name / ""test.txt.gz"", ""rb""\n            ) as f_in:\n                with open(\n                        Path(flair.cache_root) / ""datasets"" / dataset_name / ""test.txt"",\n                        ""wb"",\n                ) as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n\n        super(CONLL_2000, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass DANE(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {1: \'text\', 3: \'pos\', 9: \'ner\'}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        data_path = Path(flair.cache_root) / ""datasets"" / dataset_name\n        train_data_file = data_path / ""ddt.train.conllu""\n        if not train_data_file.is_file():\n            temp_file = cached_path(\n                \'https://danlp.s3.eu-central-1.amazonaws.com/datasets/ddt.zip\',\n                Path(""datasets"") / dataset_name\n            )\n            from zipfile import ZipFile\n\n            with ZipFile(temp_file, \'r\') as zip_file:\n                zip_file.extractall(path=data_path)\n\n            # Remove CoNLL-U meta information in the last column\n            for part in [\'train\', \'dev\', \'test\']:\n                lines = []\n                data_file = ""ddt.{}.conllu"".format(part)\n                with open(data_path / data_file, \'r\') as file:\n                    for line in file:\n                        if line.startswith(""#"") or line == ""\\n"":\n                            lines.append(line)\n                        lines.append(line.replace(""name="", """").replace(""|SpaceAfter=No"", """"))\n\n                with open(data_path / data_file, \'w\') as file:\n                    file.writelines(lines)\n\n                print(data_path / data_file)\n\n        super(DANE, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes,\n            in_memory=in_memory, comment_symbol=""#""\n        )\n\n\nclass GERMEVAL_14(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        """"""\n        Initialize the GermEval NER corpus for German. This is only possible if you\'ve manually downloaded it to your\n        machine. Obtain the corpus from https://sites.google.com/site/germeval2014ner/home/ and put it into some folder.\n        Then point the base_path parameter in the constructor to this folder\n        :param base_path: Path to the GermEval corpus on your machine\n        :param tag_to_bioes: \'ner\' by default, should not be changed.\n        :param in_memory:If True, keeps dataset in memory giving speedups in training.\n        """"""\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {1: ""text"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # check if data there\n        if not data_folder.exists():\n\n            log.warning(""-"" * 100)\n            log.warning(f\'WARNING: GermEval-14 dataset not found at ""{data_folder}"".\')\n            log.warning(\n                \'Instructions for obtaining the data can be found here: https://sites.google.com/site/germeval2014ner/home/""\'\n            )\n            log.warning(""-"" * 100)\n        super(GERMEVAL_14, self).__init__(\n            data_folder,\n            columns,\n            tag_to_bioes=tag_to_bioes,\n            comment_symbol=""#"",\n            in_memory=in_memory,\n        )\n\n\nclass INSPEC(ColumnCorpus):\n    def __init__(\n        self,\n        base_path: Union[str, Path] = None,\n        tag_to_bioes: str = ""keyword"",\n        in_memory: bool = True,\n    ):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""keyword""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        inspec_path = ""https://raw.githubusercontent.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/master/Inspec""\n        cached_path(f""{inspec_path}/train.txt"", Path(""datasets"") / dataset_name)\n        cached_path(f""{inspec_path}/test.txt"", Path(""datasets"") / dataset_name)\n        if not ""dev.txt"" in os.listdir(data_folder):\n            cached_path(f""{inspec_path}/valid.txt"", Path(""datasets"") / dataset_name)\n            #rename according to train - test - dev - convention\n            os.rename(data_folder / ""valid.txt"", data_folder / ""dev.txt"")\n\n        super(INSPEC, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass NER_BASQUE(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ner_basque_path = ""http://ixa2.si.ehu.eus/eiec/""\n        data_path = Path(flair.cache_root) / ""datasets"" / dataset_name\n        data_file = data_path / ""named_ent_eu.train""\n        if not data_file.is_file():\n            cached_path(\n                f""{ner_basque_path}/eiec_v1.0.tgz"", Path(""datasets"") / dataset_name\n            )\n            import tarfile, shutil\n\n            with tarfile.open(\n                    Path(flair.cache_root) / ""datasets"" / dataset_name / ""eiec_v1.0.tgz"",\n                    ""r:gz"",\n            ) as f_in:\n                corpus_files = (\n                    ""eiec_v1.0/named_ent_eu.train"",\n                    ""eiec_v1.0/named_ent_eu.test"",\n                )\n                for corpus_file in corpus_files:\n                    f_in.extract(corpus_file, data_path)\n                    shutil.move(f""{data_path}/{corpus_file}"", data_path)\n\n        super(NER_BASQUE, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass NER_FINNISH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ner_finnish_path = ""https://raw.githubusercontent.com/mpsilfve/finer-data/master/data/digitoday.""\n        cached_path(f""{ner_finnish_path}2014.train.csv"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ner_finnish_path}2014.dev.csv"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ner_finnish_path}2015.test.csv"", Path(""datasets"") / dataset_name)\n\n        _remove_lines_without_annotations(data_file=Path(data_folder / ""digitoday.2015.test.csv""))\n\n        super(NER_FINNISH, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory, skip_first_line=True\n        )\n\ndef _remove_lines_without_annotations(data_file: Union[str, Path] = None):\n        with open(data_file, \'r\') as f:\n            lines = f.readlines()\n        with open(data_file, \'w\') as f:\n            for line in lines:\n                if len(line.split()) != 1:\n                    f.write(line)\n\n\nclass NER_SWEDISH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        """"""\n        Initialize the NER_SWEDISH corpus for Swedish. The first time you call this constructor it will automatically\n        download the dataset.\n        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n        to point to a different folder but typically this should not be necessary.\n        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n        """"""\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ner_spraakbanken_path = ""https://raw.githubusercontent.com/klintan/swedish-ner-corpus/master/""\n        cached_path(f""{ner_spraakbanken_path}test_corpus.txt"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ner_spraakbanken_path}train_corpus.txt"", Path(""datasets"") / dataset_name)\n\n        # data is not in IOB2 format. Thus we transform it to IOB2\n        add_IOB2_tags(data_file=Path(data_folder / ""test_corpus.txt""))\n        add_IOB2_tags(data_file=Path(data_folder / ""train_corpus.txt""))\n\n        super(NER_SWEDISH, self).__init__(\n            data_folder,\n            columns,\n            tag_to_bioes=tag_to_bioes,\n            in_memory=in_memory,\n        )\n\n\nclass SEMEVAL2017(ColumnCorpus):\n    def __init__(\n        self,\n        base_path: Union[str, Path] = None,\n        tag_to_bioes: str = ""keyword"",\n        in_memory: bool = True,\n    ):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""keyword""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        semeval2017_path = ""https://raw.githubusercontent.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/master/SemEval-2017""\n        cached_path(f""{semeval2017_path}/train.txt"", Path(""datasets"") / dataset_name)\n        cached_path(f""{semeval2017_path}/test.txt"", Path(""datasets"") / dataset_name)\n        cached_path(f""{semeval2017_path}/dev.txt"", Path(""datasets"") / dataset_name)\n\n        super(SEMEVAL2017, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass SEMEVAL2010(ColumnCorpus):\n    def __init__(\n        self,\n        base_path: Union[str, Path] = None,\n        tag_to_bioes: str = ""keyword"",\n        in_memory: bool = True,\n    ):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""keyword""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        semeval2010_path = ""https://raw.githubusercontent.com/midas-research/keyphrase-extraction-as-sequence-labeling-data/master/processed_semeval-2010""\n        cached_path(f""{semeval2010_path}/train.txt"", Path(""datasets"") / dataset_name)\n        cached_path(f""{semeval2010_path}/test.txt"", Path(""datasets"") / dataset_name)\n\n        super(SEMEVAL2010, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_ENGLISH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""en"", dataset_name)\n\n        super(WIKINER_ENGLISH, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_GERMAN(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""de"", dataset_name)\n\n        super(WIKINER_GERMAN, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_DUTCH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""nl"", dataset_name)\n\n        super(WIKINER_DUTCH, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_FRENCH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""fr"", dataset_name)\n\n        super(WIKINER_FRENCH, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_ITALIAN(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""it"", dataset_name)\n\n        super(WIKINER_ITALIAN, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_SPANISH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""es"", dataset_name)\n\n        super(WIKINER_SPANISH, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_PORTUGUESE(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""pt"", dataset_name)\n\n        super(WIKINER_PORTUGUESE, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_POLISH(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""pl"", dataset_name)\n\n        super(WIKINER_POLISH, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WIKINER_RUSSIAN(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = False,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""pos"", 2: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        _download_wikiner(""ru"", dataset_name)\n\n        super(WIKINER_RUSSIAN, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\nclass WNUT_17(ColumnCorpus):\n    def __init__(\n            self,\n            base_path: Union[str, Path] = None,\n            tag_to_bioes: str = ""ner"",\n            in_memory: bool = True,\n    ):\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # column format\n        columns = {0: ""text"", 1: ""ner""}\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        wnut_path = ""https://noisy-text.github.io/2017/files/""\n        cached_path(f""{wnut_path}wnut17train.conll"", Path(""datasets"") / dataset_name)\n        cached_path(f""{wnut_path}emerging.dev.conll"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{wnut_path}emerging.test.annotated"", Path(""datasets"") / dataset_name\n        )\n\n        super(WNUT_17, self).__init__(\n            data_folder, columns, tag_to_bioes=tag_to_bioes, in_memory=in_memory\n        )\n\n\ndef _download_wikiner(language_code: str, dataset_name: str):\n    # download data if necessary\n    wikiner_path = (\n        ""https://raw.githubusercontent.com/dice-group/FOX/master/input/Wikiner/""\n    )\n    lc = language_code\n\n    data_file = (\n            Path(flair.cache_root)\n            / ""datasets""\n            / dataset_name\n            / f""aij-wikiner-{lc}-wp3.train""\n    )\n    if not data_file.is_file():\n\n        cached_path(\n            f""{wikiner_path}aij-wikiner-{lc}-wp3.bz2"", Path(""datasets"") / dataset_name\n        )\n        import bz2, shutil\n\n        # unpack and write out in CoNLL column-like format\n        bz_file = bz2.BZ2File(\n            Path(flair.cache_root)\n            / ""datasets""\n            / dataset_name\n            / f""aij-wikiner-{lc}-wp3.bz2"",\n            ""rb"",\n        )\n        with bz_file as f, open(\n                Path(flair.cache_root)\n                / ""datasets""\n                / dataset_name\n                / f""aij-wikiner-{lc}-wp3.train"",\n                ""w"",\n        ) as out:\n            for line in f:\n                line = line.decode(""utf-8"")\n                words = line.split("" "")\n                for word in words:\n                    out.write(""\\t"".join(word.split(""|"")) + ""\\n"")\n'"
flair/datasets/text_image.py,5,"b'import logging\nimport os\nimport numpy as np\nimport json\nimport urllib\n\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom typing import List\n\nimport torch.utils.data.dataloader\nfrom torch.utils.data import Dataset\n\nfrom flair.data import (\n    Sentence,\n    Corpus,\n    FlairDataset,\n    DataPair,\n    Image,\n)\nfrom flair.file_utils import cached_path\n\nlog = logging.getLogger(""flair"")\n\n\nclass FeideggerCorpus(Corpus):\n    def __init__(self, **kwargs):\n        dataset = ""feidegger""\n\n        # cache Feidegger config file\n        json_link = ""https://raw.githubusercontent.com/zalandoresearch/feidegger/master/data/FEIDEGGER_release_1.1.json""\n        json_local_path = cached_path(json_link, Path(""datasets"") / dataset)\n\n        # cache Feidegger images\n        dataset_info = json.load(open(json_local_path, ""r""))\n        images_cache_folder = os.path.join(os.path.dirname(json_local_path), ""images"")\n        if not os.path.isdir(images_cache_folder):\n            os.mkdir(images_cache_folder)\n        for image_info in tqdm(dataset_info):\n            name = os.path.basename(image_info[""url""])\n            filename = os.path.join(images_cache_folder, name)\n            if not os.path.isfile(filename):\n                urllib.request.urlretrieve(image_info[""url""], filename)\n            # replace image URL with local cached file\n            image_info[""url""] = filename\n\n        feidegger_dataset: Dataset = FeideggerDataset(dataset_info, **kwargs)\n\n        train_indices = list(\n            np.where(np.in1d(feidegger_dataset.split, list(range(8))))[0]\n        )\n        train = torch.utils.data.dataset.Subset(feidegger_dataset, train_indices)\n\n        dev_indices = list(np.where(np.in1d(feidegger_dataset.split, [8]))[0])\n        dev = torch.utils.data.dataset.Subset(feidegger_dataset, dev_indices)\n\n        test_indices = list(np.where(np.in1d(feidegger_dataset.split, [9]))[0])\n        test = torch.utils.data.dataset.Subset(feidegger_dataset, test_indices)\n\n        super(FeideggerCorpus, self).__init__(train, dev, test, name=""feidegger"")\n\n\nclass FeideggerDataset(FlairDataset):\n    def __init__(self, dataset_info, in_memory: bool = True, **kwargs):\n        super(FeideggerDataset, self).__init__()\n\n        self.data_points: List[DataPair] = []\n        self.split: List[int] = []\n\n        preprocessor = lambda x: x\n        if ""lowercase"" in kwargs and kwargs[""lowercase""]:\n            preprocessor = lambda x: x.lower()\n\n        for image_info in dataset_info:\n            image = Image(imageURL=image_info[""url""])\n            for caption in image_info[""descriptions""]:\n                # append Sentence-Image data point\n                self.data_points.append(\n                    DataPair(Sentence(preprocessor(caption), use_tokenizer=True), image)\n                )\n                self.split.append(int(image_info[""split""]))\n\n    def __len__(self):\n        return len(self.data_points)\n\n    def __getitem__(self, index: int = 0) -> DataPair:\n        return self.data_points[index]'"
flair/datasets/text_text.py,0,"b'import logging\nfrom pathlib import Path\nfrom typing import List, Union\n\nimport flair\nfrom flair.data import (\n    Sentence,\n    Corpus,\n    FlairDataset,\n    DataPair,\n)\nfrom flair.file_utils import cached_path, unzip_file\n\nlog = logging.getLogger(""flair"")\n\n\nclass ParallelTextCorpus(Corpus):\n    def __init__(\n            self,\n            source_file: Union[str, Path],\n            target_file: Union[str, Path],\n            name: str = None,\n            use_tokenizer: bool = True,\n            max_tokens_per_doc=-1,\n            max_chars_per_doc=-1,\n            in_memory: bool = True,\n    ):\n        """"""\n        Instantiates a Corpus for text classification from CSV column formatted data\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        train: FlairDataset = ParallelTextDataset(\n            source_file,\n            target_file,\n            use_tokenizer=use_tokenizer,\n            max_tokens_per_doc=max_tokens_per_doc,\n            max_chars_per_doc=max_chars_per_doc,\n            in_memory=in_memory,\n        )\n\n        super(ParallelTextCorpus, self).__init__(train, name=name)\n\n\nclass OpusParallelCorpus(ParallelTextCorpus):\n    def __init__(\n            self,\n            dataset: str,\n            l1: str,\n            l2: str,\n            use_tokenizer: bool = True,\n            max_tokens_per_doc=-1,\n            max_chars_per_doc=-1,\n            in_memory: bool = True,\n    ):\n        """"""\n        Instantiates a Parallel Corpus from OPUS (http://opus.nlpl.eu/)\n        :param dataset: Name of the dataset (one of ""tatoeba"")\n        :param l1: Language code of first language in pair (""en"", ""de"", etc.)\n        :param l2: Language code of second language in pair (""en"", ""de"", etc.)\n        :param use_tokenizer: Whether or not to use in-built tokenizer\n        :param max_tokens_per_doc: If set, shortens sentences to this maximum number of tokens\n        :param max_chars_per_doc: If set, shortens sentences to this maximum number of characters\n        :param in_memory: If True, keeps dataset fully in memory\n        """"""\n\n        if l1 > l2:\n            l1, l2 = l2, l1\n\n        # check if dataset is supported\n        supported_datasets = [""tatoeba""]\n        if dataset not in supported_datasets:\n            log.error(f""Dataset must be one of: {supported_datasets}"")\n\n        # set file names\n        if dataset == ""tatoeba"":\n            link = f""https://object.pouta.csc.fi/OPUS-Tatoeba/v20190709/moses/{l1}-{l2}.txt.zip""\n\n            l1_file = (\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / dataset\n                    / f""{l1}-{l2}""\n                    / f""Tatoeba.{l1}-{l2}.{l1}""\n            )\n            l2_file = (\n                    Path(flair.cache_root)\n                    / ""datasets""\n                    / dataset\n                    / f""{l1}-{l2}""\n                    / f""Tatoeba.{l1}-{l2}.{l2}""\n            )\n\n        # download and unzip in file structure if necessary\n        if not l1_file.exists():\n            path = cached_path(link, Path(""datasets"") / dataset / f""{l1}-{l2}"")\n            unzip_file(\n                path, Path(flair.cache_root) / Path(""datasets"") / dataset / f""{l1}-{l2}""\n            )\n\n        # instantiate corpus\n        super(OpusParallelCorpus, self).__init__(\n            l1_file,\n            l2_file,\n            name=f""{dataset}-{l1_file}-{l2_file}"",\n            use_tokenizer=use_tokenizer,\n            max_tokens_per_doc=max_tokens_per_doc,\n            max_chars_per_doc=max_chars_per_doc,\n            in_memory=in_memory,\n        )\n\n\nclass ParallelTextDataset(FlairDataset):\n    def __init__(\n            self,\n            path_to_source: Union[str, Path],\n            path_to_target: Union[str, Path],\n            max_tokens_per_doc=-1,\n            max_chars_per_doc=-1,\n            use_tokenizer=True,\n            in_memory: bool = True,\n    ):\n        if type(path_to_source) == str:\n            path_to_source: Path = Path(path_to_source)\n        if type(path_to_target) == str:\n            path_to_target: Path = Path(path_to_target)\n\n        assert path_to_source.exists()\n        assert path_to_target.exists()\n\n        self.in_memory = in_memory\n\n        self.use_tokenizer = use_tokenizer\n        self.max_tokens_per_doc = max_tokens_per_doc\n\n        self.total_sentence_count: int = 0\n\n        if self.in_memory:\n            self.bi_sentences: List[DataPair] = []\n        else:\n            self.source_lines: List[str] = []\n            self.target_lines: List[str] = []\n\n        with open(str(path_to_source), encoding=""utf-8"") as source_file, open(\n                str(path_to_target), encoding=""utf-8""\n        ) as target_file:\n\n            source_line = source_file.readline()\n            target_line = target_file.readline()\n\n            while source_line and target_line:\n\n                source_line = source_file.readline()\n                target_line = target_file.readline()\n\n                if source_line.strip() == """":\n                    continue\n                if target_line.strip() == """":\n                    continue\n\n                if max_chars_per_doc > 0:\n                    source_line = source_line[:max_chars_per_doc]\n                    target_line = target_line[:max_chars_per_doc]\n\n                if self.in_memory:\n                    bi_sentence = self._make_bi_sentence(source_line, target_line)\n                    self.bi_sentences.append(bi_sentence)\n                else:\n                    self.source_lines.append(source_line)\n                    self.target_lines.append(target_line)\n\n                self.total_sentence_count += 1\n\n    def _make_bi_sentence(self, source_line: str, target_line: str):\n\n        source_sentence = Sentence(source_line, use_tokenizer=self.use_tokenizer)\n        target_sentence = Sentence(target_line, use_tokenizer=self.use_tokenizer)\n\n        if self.max_tokens_per_doc > 0:\n            source_sentence.tokens = source_sentence.tokens[: self.max_tokens_per_doc]\n            target_sentence.tokens = target_sentence.tokens[: self.max_tokens_per_doc]\n\n        return DataPair(source_sentence, target_sentence)\n\n    def __len__(self):\n        return self.total_sentence_count\n\n    def __getitem__(self, index: int = 0) -> DataPair:\n        if self.in_memory:\n            return self.bi_sentences[index]\n        else:\n            return self._make_bi_sentence(\n                self.source_lines[index], self.target_lines[index]\n            )'"
flair/datasets/treebanks.py,0,"b'import logging\nimport re\nfrom pathlib import Path\nfrom typing import List, Union\n\nimport flair\nfrom flair.data import (\n    Sentence,\n    Corpus,\n    Token,\n    FlairDataset,\n)\nfrom flair.datasets.base import find_train_dev_test_files\nfrom flair.file_utils import cached_path, unzip_file\n\nlog = logging.getLogger(""flair"")\n\n\nclass UniversalDependenciesCorpus(Corpus):\n    def __init__(\n            self,\n            data_folder: Union[str, Path],\n            train_file=None,\n            test_file=None,\n            dev_file=None,\n            in_memory: bool = True,\n    ):\n        """"""\n        Instantiates a Corpus from CoNLL-U column-formatted task data such as the UD corpora\n\n        :param data_folder: base folder with the task data\n        :param train_file: the name of the train file\n        :param test_file: the name of the test file\n        :param dev_file: the name of the dev file, if None, dev data is sampled from train\n        :param in_memory: If set to True, keeps full dataset in memory, otherwise does disk reads\n        :return: a Corpus with annotated train, dev and test data\n        """"""\n\n        # find train, dev and test files if not specified\n        dev_file, test_file, train_file = \\\n            find_train_dev_test_files(data_folder, dev_file, test_file, train_file)\n\n        # get train data\n        train = UniversalDependenciesDataset(train_file, in_memory=in_memory)\n\n        # get test data\n        test = UniversalDependenciesDataset(test_file, in_memory=in_memory)\n\n        # get dev data\n        dev = UniversalDependenciesDataset(dev_file, in_memory=in_memory)\n\n        super(UniversalDependenciesCorpus, self).__init__(\n            train, dev, test, name=str(data_folder)\n        )\n\n\nclass UniversalDependenciesDataset(FlairDataset):\n    def __init__(self, path_to_conll_file: Union[str, Path], in_memory: bool = True):\n        """"""\n        Instantiates a column dataset in CoNLL-U format.\n\n        :param path_to_conll_file: Path to the CoNLL-U formatted file\n        :param in_memory: If set to True, keeps full dataset in memory, otherwise does disk reads\n        """"""\n        if type(path_to_conll_file) is str:\n            path_to_conll_file = Path(path_to_conll_file)\n        assert path_to_conll_file.exists()\n\n        self.in_memory = in_memory\n        self.path_to_conll_file = path_to_conll_file\n        self.total_sentence_count: int = 0\n\n        if self.in_memory:\n            self.sentences: List[Sentence] = []\n        else:\n            self.indices: List[int] = []\n\n        with open(str(self.path_to_conll_file), encoding=""utf-8"") as file:\n\n            line = file.readline()\n            position = 0\n            sentence: Sentence = Sentence()\n            while line:\n\n                line = line.strip()\n                fields: List[str] = re.split(""\\t+"", line)\n                if line == """":\n                    if len(sentence) > 0:\n                        self.total_sentence_count += 1\n                        if self.in_memory:\n                            self.sentences.append(sentence)\n                        else:\n                            self.indices.append(position)\n                            position = file.tell()\n                    sentence: Sentence = Sentence()\n\n                elif line.startswith(""#""):\n                    line = file.readline()\n                    continue\n                elif ""."" in fields[0]:\n                    line = file.readline()\n                    continue\n                elif ""-"" in fields[0]:\n                    line = file.readline()\n                    continue\n                else:\n                    token = Token(fields[1], head_id=int(fields[6]))\n                    token.add_label(""lemma"", str(fields[2]))\n                    token.add_label(""upos"", str(fields[3]))\n                    token.add_label(""pos"", str(fields[4]))\n                    token.add_label(""dependency"", str(fields[7]))\n\n                    if len(fields) > 9 and \'SpaceAfter=No\' in fields[9]:\n                        token.whitespace_after = False\n\n                    for morph in str(fields[5]).split(""|""):\n                        if ""="" not in morph:\n                            continue\n                        token.add_label(morph.split(""="")[0].lower(), morph.split(""="")[1])\n\n                    if len(fields) > 10 and str(fields[10]) == ""Y"":\n                        token.add_label(""frame"", str(fields[11]))\n\n                    sentence.add_token(token)\n\n                line = file.readline()\n            if len(sentence.tokens) > 0:\n                self.total_sentence_count += 1\n                if self.in_memory:\n                    self.sentences.append(sentence)\n                else:\n                    self.indices.append(position)\n\n    def is_in_memory(self) -> bool:\n        return self.in_memory\n\n    def __len__(self):\n        return self.total_sentence_count\n\n    def __getitem__(self, index: int = 0) -> Sentence:\n\n        if self.in_memory:\n            sentence = self.sentences[index]\n        else:\n            with open(str(self.path_to_conll_file), encoding=""utf-8"") as file:\n                file.seek(self.indices[index])\n                line = file.readline()\n                sentence: Sentence = Sentence()\n                while line:\n\n                    line = line.strip()\n                    fields: List[str] = re.split(""\\t+"", line)\n                    if line == """":\n                        if len(sentence) > 0:\n                            break\n\n                    elif line.startswith(""#""):\n                        line = file.readline()\n                        continue\n                    elif ""."" in fields[0]:\n                        line = file.readline()\n                        continue\n                    elif ""-"" in fields[0]:\n                        line = file.readline()\n                        continue\n                    else:\n                        token = Token(fields[1], head_id=int(fields[6]))\n                        token.add_label(""lemma"", str(fields[2]))\n                        token.add_label(""upos"", str(fields[3]))\n                        token.add_label(""pos"", str(fields[4]))\n                        token.add_label(""dependency"", str(fields[7]))\n\n                        if len(fields) > 9 and \'SpaceAfter=No\' in fields[9]:\n                            token.whitespace_after = False\n\n                        for morph in str(fields[5]).split(""|""):\n                            if ""="" not in morph:\n                                continue\n                            token.add_label(\n                                morph.split(""="")[0].lower(), morph.split(""="")[1]\n                            )\n\n                        if len(fields) > 10 and str(fields[10]) == ""Y"":\n                            token.add_label(""frame"", str(fields[11]))\n\n                        sentence.add_token(token)\n\n                    line = file.readline()\n        return sentence\n\n\nclass UD_ENGLISH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        web_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master""\n        cached_path(f""{web_path}/en_ewt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{web_path}/en_ewt-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{web_path}/en_ewt-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_ENGLISH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_GERMAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/master""\n        cached_path(f""{ud_path}/de_gsd-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/de_gsd-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/de_gsd-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_GERMAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_GERMAN_HDT(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = False):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = (\n            ""https://raw.githubusercontent.com/UniversalDependencies/UD_German-HDT/dev""\n        )\n        cached_path(f""{ud_path}/de_hdt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/de_hdt-ud-test.conllu"", Path(""datasets"") / dataset_name)\n\n        train_filenames = [\n            ""de_hdt-ud-train-a-1.conllu"",\n            ""de_hdt-ud-train-a-2.conllu"",\n            ""de_hdt-ud-train-b-1.conllu"",\n            ""de_hdt-ud-train-b-2.conllu"",\n        ]\n\n        for train_file in train_filenames:\n            cached_path(\n                f""{ud_path}/{train_file}"", Path(""datasets"") / dataset_name / ""original""\n            )\n\n        data_path = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        new_train_file: Path = data_path / ""de_hdt-ud-train-all.conllu""\n\n        if not new_train_file.is_file():\n            with open(new_train_file, ""wt"") as f_out:\n                for train_filename in train_filenames:\n                    with open(data_path / ""original"" / train_filename, ""rt"") as f_in:\n                        f_out.write(f_in.read())\n\n        super(UD_GERMAN_HDT, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_DUTCH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Dutch-Alpino/master""\n        cached_path(\n            f""{ud_path}/nl_alpino-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/nl_alpino-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/nl_alpino-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_DUTCH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_FRENCH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_French-GSD/master""\n        cached_path(f""{ud_path}/fr_gsd-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/fr_gsd-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/fr_gsd-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_FRENCH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_ITALIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Italian-ISDT/master""\n        cached_path(f""{ud_path}/it_isdt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/it_isdt-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/it_isdt-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_ITALIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_SPANISH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Spanish-GSD/master""\n        cached_path(f""{ud_path}/es_gsd-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/es_gsd-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/es_gsd-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_SPANISH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_PORTUGUESE(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Portuguese-Bosque/master""\n        cached_path(\n            f""{ud_path}/pt_bosque-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/pt_bosque-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/pt_bosque-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_PORTUGUESE, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_ROMANIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Romanian-RRT/master""\n        cached_path(f""{ud_path}/ro_rrt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/ro_rrt-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/ro_rrt-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_ROMANIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_CATALAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Catalan-AnCora/master""\n        cached_path(\n            f""{ud_path}/ca_ancora-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/ca_ancora-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/ca_ancora-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_CATALAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_POLISH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Polish-LFG/master""\n        cached_path(f""{ud_path}/pl_lfg-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/pl_lfg-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/pl_lfg-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_POLISH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_CZECH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = False):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Czech-PDT/master""\n        cached_path(f""{ud_path}/cs_pdt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/cs_pdt-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/cs_pdt-ud-train-c.conllu"",\n            Path(""datasets"") / dataset_name / ""original"",\n        )\n        cached_path(\n            f""{ud_path}/cs_pdt-ud-train-l.conllu"",\n            Path(""datasets"") / dataset_name / ""original"",\n        )\n        cached_path(\n            f""{ud_path}/cs_pdt-ud-train-m.conllu"",\n            Path(""datasets"") / dataset_name / ""original"",\n        )\n        cached_path(\n            f""{ud_path}/cs_pdt-ud-train-v.conllu"",\n            Path(""datasets"") / dataset_name / ""original"",\n        )\n        data_path = Path(flair.cache_root) / ""datasets"" / dataset_name\n\n        train_filenames = [\n            ""cs_pdt-ud-train-c.conllu"",\n            ""cs_pdt-ud-train-l.conllu"",\n            ""cs_pdt-ud-train-m.conllu"",\n            ""cs_pdt-ud-train-v.conllu"",\n        ]\n\n        new_train_file: Path = data_path / ""cs_pdt-ud-train-all.conllu""\n\n        if not new_train_file.is_file():\n            with open(new_train_file, ""wt"") as f_out:\n                for train_filename in train_filenames:\n                    with open(data_path / ""original"" / train_filename, ""rt"") as f_in:\n                        f_out.write(f_in.read())\n        super(UD_CZECH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_SLOVAK(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Slovak-SNK/master""\n        cached_path(f""{ud_path}/sk_snk-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/sk_snk-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/sk_snk-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_SLOVAK, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_SWEDISH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Swedish-Talbanken/master""\n        cached_path(\n            f""{ud_path}/sv_talbanken-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/sv_talbanken-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/sv_talbanken-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_SWEDISH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_DANISH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Danish-DDT/master""\n        cached_path(f""{ud_path}/da_ddt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/da_ddt-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/da_ddt-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_DANISH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_NORWEGIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Norwegian-Bokmaal/master""\n        cached_path(\n            f""{ud_path}/no_bokmaal-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/no_bokmaal-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/no_bokmaal-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_NORWEGIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_FINNISH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Finnish-TDT/master""\n        cached_path(f""{ud_path}/fi_tdt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/fi_tdt-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/fi_tdt-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_FINNISH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_SLOVENIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Slovenian-SSJ/master""\n        cached_path(f""{ud_path}/sl_ssj-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/sl_ssj-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/sl_ssj-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_SLOVENIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_CROATIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Croatian-SET/master""\n        cached_path(f""{ud_path}/hr_set-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/hr_set-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/hr_set-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_CROATIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_SERBIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Serbian-SET/master""\n        cached_path(f""{ud_path}/sr_set-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/sr_set-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/sr_set-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_SERBIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_BULGARIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Bulgarian-BTB/master""\n        cached_path(f""{ud_path}/bg_btb-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/bg_btb-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/bg_btb-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_BULGARIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_ARABIC(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Arabic-PADT/master""\n        cached_path(f""{ud_path}/ar_padt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/ar_padt-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/ar_padt-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_ARABIC, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_HEBREW(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Hebrew-HTB/master""\n        cached_path(f""{ud_path}/he_htb-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/he_htb-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/he_htb-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n        super(UD_HEBREW, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_TURKISH(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Turkish-IMST/master""\n        cached_path(f""{ud_path}/tr_imst-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/tr_imst-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/tr_imst-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_TURKISH, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_PERSIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Persian-Seraji/master""\n        cached_path(\n            f""{ud_path}/fa_seraji-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/fa_seraji-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/fa_seraji-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_PERSIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_RUSSIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master""\n        cached_path(\n            f""{ud_path}/ru_syntagrus-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/ru_syntagrus-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/ru_syntagrus-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_RUSSIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_HINDI(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Hindi-HDTB/master""\n        cached_path(f""{ud_path}/hi_hdtb-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/hi_hdtb-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/hi_hdtb-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_HINDI, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_INDONESIAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Indonesian-GSD/master""\n        cached_path(f""{ud_path}/id_gsd-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/id_gsd-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/id_gsd-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_INDONESIAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_JAPANESE(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Japanese-GSD/master""\n        cached_path(f""{ud_path}/ja_gsd-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/ja_gsd-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/ja_gsd-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_JAPANESE, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_CHINESE(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master""\n        cached_path(f""{ud_path}/zh_gsd-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/zh_gsd-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/zh_gsd-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_CHINESE, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_KOREAN(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Korean-Kaist/master""\n        cached_path(\n            f""{ud_path}/ko_kaist-ud-dev.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/ko_kaist-ud-test.conllu"", Path(""datasets"") / dataset_name\n        )\n        cached_path(\n            f""{ud_path}/ko_kaist-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_KOREAN, self).__init__(data_folder, in_memory=in_memory)\n\n\nclass UD_BASQUE(UniversalDependenciesCorpus):\n    def __init__(self, base_path: Union[str, Path] = None, in_memory: bool = True):\n\n        if type(base_path) == str:\n            base_path: Path = Path(base_path)\n\n        # this dataset name\n        dataset_name = self.__class__.__name__.lower()\n\n        # default dataset folder is the cache root\n        if not base_path:\n            base_path = Path(flair.cache_root) / ""datasets""\n        data_folder = base_path / dataset_name\n\n        # download data if necessary\n        ud_path = ""https://raw.githubusercontent.com/UniversalDependencies/UD_Basque-BDT/master""\n        cached_path(f""{ud_path}/eu_bdt-ud-dev.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(f""{ud_path}/eu_bdt-ud-test.conllu"", Path(""datasets"") / dataset_name)\n        cached_path(\n            f""{ud_path}/eu_bdt-ud-train.conllu"", Path(""datasets"") / dataset_name\n        )\n\n        super(UD_BASQUE, self).__init__(data_folder, in_memory=in_memory)\n'"
flair/embeddings/__init__.py,0,b'# Expose base classses\nfrom .base import Embeddings\nfrom .base import ScalarMix\n\n# Expose token embedding classes\nfrom .token import TokenEmbeddings\nfrom .token import StackedEmbeddings\nfrom .token import WordEmbeddings\nfrom .token import CharacterEmbeddings\nfrom .token import FlairEmbeddings\nfrom .token import PooledFlairEmbeddings\nfrom .token import TransformerWordEmbeddings\nfrom .token import BPEmbSerializable\nfrom .token import BytePairEmbeddings\nfrom .token import ELMoEmbeddings\nfrom .token import OneHotEmbeddings\nfrom .token import FastTextEmbeddings\nfrom .token import HashEmbeddings\nfrom .token import MuseCrosslingualEmbeddings\nfrom .token import NILCEmbeddings\n\n# Expose document embedding classes\nfrom .document import DocumentEmbeddings\nfrom .document import TransformerDocumentEmbeddings\nfrom .document import DocumentPoolEmbeddings\nfrom .document import DocumentRNNEmbeddings\nfrom .document import DocumentLMEmbeddings\n\n# Expose image embedding classes\nfrom .image import ImageEmbeddings\nfrom .image import IdentityImageEmbeddings\nfrom .image import PrecomputedImageEmbeddings\nfrom .image import NetworkImageEmbeddings\nfrom .image import ConvTransformNetworkImageEmbeddings\n\n# Expose legacy embedding classes\nfrom .legacy import CharLMEmbeddings\nfrom .legacy import TransformerXLEmbeddings\nfrom .legacy import XLNetEmbeddings\nfrom .legacy import XLMEmbeddings\nfrom .legacy import OpenAIGPTEmbeddings\nfrom .legacy import OpenAIGPT2Embeddings\nfrom .legacy import RoBERTaEmbeddings\nfrom .legacy import CamembertEmbeddings\nfrom .legacy import XLMRobertaEmbeddings\nfrom .legacy import BertEmbeddings\nfrom .legacy import DocumentMeanEmbeddings\nfrom .legacy import DocumentLSTMEmbeddings\nfrom .legacy import ELMoTransformerEmbeddings'
flair/embeddings/base.py,11,"b'from abc import abstractmethod\nfrom typing import Union, List\nfrom torch.nn import ParameterList, Parameter\n\nimport torch\nimport logging\n\nimport flair\nfrom flair.data import Sentence, Image\n\n\nlog = logging.getLogger(""flair"")\n\n\nclass Embeddings(torch.nn.Module):\n    """"""Abstract base class for all embeddings. Every new type of embedding must implement these methods.""""""\n\n    def __init__(self):\n        """"""Set some attributes that would otherwise result in errors. Overwrite these in your embedding class.""""""\n        if not hasattr(self, ""name""):\n            self.name: str = ""unnamed_embedding""\n        if not hasattr(self, ""static_embeddings""):\n            # if the embeddings for a sentence are the same in each epoch, set this to True for improved efficiency\n            self.static_embeddings = False\n        super().__init__()\n\n    @property\n    @abstractmethod\n    def embedding_length(self) -> int:\n        """"""Returns the length of the embedding vector.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def embedding_type(self) -> str:\n        pass\n\n    def embed(self, sentences: Union[Sentence, List[Sentence]]) -> List[Sentence]:\n        """"""Add embeddings to all words in a list of sentences. If embeddings are already added, updates only if embeddings\n        are non-static.""""""\n\n        # if only one sentence is passed, convert to list of sentence\n        if (type(sentences) is Sentence) or (type(sentences) is Image):\n            sentences = [sentences]\n\n        everything_embedded: bool = True\n\n        if self.embedding_type == ""word-level"":\n            for sentence in sentences:\n                for token in sentence.tokens:\n                    if self.name not in token._embeddings.keys():\n                        everything_embedded = False\n        else:\n            for sentence in sentences:\n                if self.name not in sentence._embeddings.keys():\n                    everything_embedded = False\n\n        if not everything_embedded or not self.static_embeddings:\n            self._add_embeddings_internal(sentences)\n\n        return sentences\n\n    @abstractmethod\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        """"""Private method for adding embeddings to all words in a list of sentences.""""""\n        pass\n\n    def get_names(self) -> List[str]:\n        """"""Returns a list of embedding names. In most cases, it is just a list with one item, namely the name of\n        this embedding. But in some cases, the embedding is made up by different embeddings (StackedEmbedding).\n        Then, the list contains the names of all embeddings in the stack.""""""\n        return [self.name]\n\n\nclass ScalarMix(torch.nn.Module):\n    """"""\n    Computes a parameterised scalar mixture of N tensors.\n    This method was proposed by Liu et al. (2019) in the paper:\n    ""Linguistic Knowledge and Transferability of Contextual Representations"" (https://arxiv.org/abs/1903.08855)\n\n    The implementation is copied and slightly modified from the allennlp repository and is licensed under Apache 2.0.\n    It can be found under:\n    https://github.com/allenai/allennlp/blob/master/allennlp/modules/scalar_mix.py.\n    """"""\n\n    def __init__(self, mixture_size: int, trainable: bool = False) -> None:\n        """"""\n        Inits scalar mix implementation.\n        ``mixture = gamma * sum(s_k * tensor_k)`` where ``s = softmax(w)``, with ``w`` and ``gamma`` scalar parameters.\n        :param mixture_size: size of mixtures (usually the number of layers)\n        """"""\n        super(ScalarMix, self).__init__()\n        self.mixture_size = mixture_size\n\n        initial_scalar_parameters = [0.0] * mixture_size\n\n        self.scalar_parameters = ParameterList(\n            [\n                Parameter(\n                    torch.tensor(\n                        [initial_scalar_parameters[i]],\n                        dtype=torch.float,\n                        device=flair.device,\n                    ),\n                    requires_grad=trainable,\n\n                )\n                for i in range(mixture_size)\n            ]\n        )\n        self.gamma = Parameter(\n            torch.tensor(\n                [1.0],\n                dtype=torch.float,\n                device=flair.device,\n            ), requires_grad=trainable\n        )\n\n    def forward(self, tensors: List[torch.Tensor]) -> torch.Tensor:\n        """"""\n        Computes a weighted average of the ``tensors``.  The input tensors an be any shape\n        with at least two dimensions, but must all be the same shape.\n        :param tensors: list of input tensors\n        :return: computed weighted average of input tensors\n        """"""\n        if len(tensors) != self.mixture_size:\n            log.error(\n                ""{} tensors were passed, but the module was initialized to mix {} tensors."".format(\n                    len(tensors), self.mixture_size\n                )\n            )\n\n        normed_weights = torch.nn.functional.softmax(\n            torch.cat([parameter for parameter in self.scalar_parameters]), dim=0\n        )\n        normed_weights = torch.split(normed_weights, split_size_or_sections=1)\n\n        pieces = []\n        for weight, tensor in zip(normed_weights, tensors):\n            pieces.append(weight * tensor)\n        return self.gamma * sum(pieces)\n'"
flair/embeddings/document.py,31,"b'from abc import abstractmethod\nimport logging\nfrom typing import List, Union\n\nimport torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom transformers import BertTokenizer, AlbertTokenizer, AutoTokenizer, AutoConfig, AutoModel\n\nimport flair\nfrom flair.data import Sentence\nfrom flair.embeddings.base import Embeddings, ScalarMix\nfrom flair.embeddings.token import TokenEmbeddings, StackedEmbeddings, FlairEmbeddings\nfrom flair.nn import LockedDropout, WordDropout\n\nlog = logging.getLogger(""flair"")\n\n\nclass DocumentEmbeddings(Embeddings):\n    """"""Abstract base class for all document-level embeddings. Ever new type of document embedding must implement these methods.""""""\n\n    @property\n    @abstractmethod\n    def embedding_length(self) -> int:\n        """"""Returns the length of the embedding vector.""""""\n        pass\n\n    @property\n    def embedding_type(self) -> str:\n        return ""sentence-level""\n\n\nclass TransformerDocumentEmbeddings(DocumentEmbeddings):\n    def __init__(\n        self,\n        model: str = ""bert-base-uncased"",\n        fine_tune: bool = True,\n        batch_size: int = 1,\n        layers: str = ""-1"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""\n        Bidirectional transformer embeddings of words from various transformer architectures.\n        :param model: name of transformer model (see https://huggingface.co/transformers/pretrained_models.html for\n        options)\n        :param fine_tune: If True, allows transformers to be fine-tuned during training\n        :param batch_size: How many sentence to push through transformer at once. Set to 1 by default since transformer\n        models tend to be huge.\n        :param layers: string indicating which layers to take for embedding (-1 is topmost layer)\n        :param use_scalar_mix: If True, uses a scalar mix of layers as embedding\n        """"""\n        super().__init__()\n\n        # load tokenizer and transformer model\n        self.tokenizer = AutoTokenizer.from_pretrained(model)\n        config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(model, config=config)\n\n        # model name\n        self.name = \'transformer-document-\' + str(model)\n\n        # when initializing, embeddings are in eval mode by default\n        self.model.eval()\n        self.model.to(flair.device)\n\n        # embedding parameters\n        if layers == \'all\':\n            # send mini-token through to check how many layers the model has\n            hidden_states = self.model(torch.tensor([1], device=flair.device).unsqueeze(0))[-1]\n            self.layer_indexes = [int(x) for x in range(len(hidden_states))]\n        else:\n            self.layer_indexes = [int(x) for x in layers.split("","")]\n\n        self.use_scalar_mix = use_scalar_mix\n        self.fine_tune = fine_tune\n        self.static_embeddings = not self.fine_tune\n        self.batch_size = batch_size\n\n        # most models have CLS token as last token (GPT-1, GPT-2, TransfoXL, XLNet, XLM), but BERT is initial\n        self.initial_cls_token: bool = False\n        if isinstance(self.tokenizer, BertTokenizer) or isinstance(self.tokenizer, AlbertTokenizer):\n            self.initial_cls_token = True\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        """"""Add embeddings to all words in a list of sentences.""""""\n\n        # using list comprehension\n        sentence_batches = [sentences[i * self.batch_size:(i + 1) * self.batch_size]\n                            for i in range((len(sentences) + self.batch_size - 1) // self.batch_size)]\n\n        for batch in sentence_batches:\n            self._add_embeddings_to_sentences(batch)\n\n        return sentences\n\n    def _add_embeddings_to_sentences(self, sentences: List[Sentence]):\n        """"""Extract sentence embedding from CLS token or similar and add to Sentence object.""""""\n\n        # gradients are enabled if fine-tuning is enabled\n        gradient_context = torch.enable_grad() if (self.fine_tune and self.training) else torch.no_grad()\n\n        with gradient_context:\n\n            # first, subtokenize each sentence and find out into how many subtokens each token was divided\n            subtokenized_sentences = []\n\n            # subtokenize sentences\n            for sentence in sentences:\n                # tokenize and truncate to 512 subtokens (TODO: check better truncation strategies)\n                subtokenized_sentence = self.tokenizer.encode(sentence.to_tokenized_string(),\n                                                              add_special_tokens=True,\n                                                              max_length=512)\n                subtokenized_sentences.append(\n                    torch.tensor(subtokenized_sentence, dtype=torch.long, device=flair.device))\n\n            # find longest sentence in batch\n            longest_sequence_in_batch: int = len(max(subtokenized_sentences, key=len))\n\n            # initialize batch tensors and mask\n            input_ids = torch.zeros(\n                [len(sentences), longest_sequence_in_batch],\n                dtype=torch.long,\n                device=flair.device,\n            )\n            mask = torch.zeros(\n                [len(sentences), longest_sequence_in_batch],\n                dtype=torch.long,\n                device=flair.device,\n            )\n            for s_id, sentence in enumerate(subtokenized_sentences):\n                sequence_length = len(sentence)\n                input_ids[s_id][:sequence_length] = sentence\n                mask[s_id][:sequence_length] = torch.ones(sequence_length)\n\n            # put encoded batch through transformer model to get all hidden states of all encoder layers\n            hidden_states = self.model(input_ids, attention_mask=mask)[-1] if len(sentences) > 1 \\\n                else self.model(input_ids)[-1]\n\n            # iterate over all subtokenized sentences\n            for sentence_idx, (sentence, subtokens) in enumerate(zip(sentences, subtokenized_sentences)):\n\n                index_of_CLS_token = 0 if self.initial_cls_token else len(subtokens) -1\n\n                cls_embeddings_all_layers: List[torch.FloatTensor] = \\\n                    [hidden_states[layer][sentence_idx][index_of_CLS_token] for layer in self.layer_indexes]\n\n                # use scalar mix of embeddings if so selected\n                if self.use_scalar_mix:\n                    sm = ScalarMix(mixture_size=len(cls_embeddings_all_layers))\n                    sm_embeddings = sm(cls_embeddings_all_layers)\n\n                    cls_embeddings_all_layers = [sm_embeddings]\n\n                # set the extracted embedding for the token\n                sentence.set_embedding(self.name, torch.cat(cls_embeddings_all_layers))\n\n    @property\n    @abstractmethod\n    def embedding_length(self) -> int:\n        """"""Returns the length of the embedding vector.""""""\n        return (\n            len(self.layer_indexes) * self.model.config.hidden_size\n            if not self.use_scalar_mix\n            else self.model.config.hidden_size\n        )\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n\n        # reload tokenizer to get around serialization issues\n        model_name = self.name.split(\'transformer-document-\')[-1]\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nclass DocumentPoolEmbeddings(DocumentEmbeddings):\n    def __init__(\n        self,\n        embeddings: List[TokenEmbeddings],\n        fine_tune_mode: str = ""none"",\n        pooling: str = ""mean"",\n    ):\n        """"""The constructor takes a list of embeddings to be combined.\n        :param embeddings: a list of token embeddings\n        :param fine_tune_mode: if set to ""linear"" a trainable layer is added, if set to\n        ""nonlinear"", a nonlinearity is added as well. Set this to make the pooling trainable.\n        :param pooling: a string which can any value from [\'mean\', \'max\', \'min\']\n        """"""\n        super().__init__()\n\n        self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embeddings)\n        self.__embedding_length = self.embeddings.embedding_length\n\n        # optional fine-tuning on top of embedding layer\n        self.fine_tune_mode = fine_tune_mode\n        if self.fine_tune_mode in [""nonlinear"", ""linear""]:\n            self.embedding_flex = torch.nn.Linear(\n                self.embedding_length, self.embedding_length, bias=False\n            )\n            self.embedding_flex.weight.data.copy_(torch.eye(self.embedding_length))\n\n        if self.fine_tune_mode in [""nonlinear""]:\n            self.embedding_flex_nonlinear = torch.nn.ReLU(self.embedding_length)\n            self.embedding_flex_nonlinear_map = torch.nn.Linear(\n                self.embedding_length, self.embedding_length\n            )\n\n        self.__embedding_length: int = self.embeddings.embedding_length\n\n        self.to(flair.device)\n\n        if pooling not in [\'min\', \'max\', \'mean\']:\n            raise ValueError(f""Pooling operation for {self.mode!r} is not defined"")\n\n        self.pooling = pooling\n        self.name: str = f""document_{self.pooling}""\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def embed(self, sentences: Union[List[Sentence], Sentence]):\n        """"""Add embeddings to every sentence in the given list of sentences. If embeddings are already added, updates\n        only if embeddings are non-static.""""""\n\n        # if only one sentence is passed, convert to list of sentence\n        if isinstance(sentences, Sentence):\n            sentences = [sentences]\n\n        self.embeddings.embed(sentences)\n\n        for sentence in sentences:\n            word_embeddings = []\n            for token in sentence.tokens:\n                word_embeddings.append(token.get_embedding().unsqueeze(0))\n\n            word_embeddings = torch.cat(word_embeddings, dim=0).to(flair.device)\n\n            if self.fine_tune_mode in [""nonlinear"", ""linear""]:\n                word_embeddings = self.embedding_flex(word_embeddings)\n\n            if self.fine_tune_mode in [""nonlinear""]:\n                word_embeddings = self.embedding_flex_nonlinear(word_embeddings)\n                word_embeddings = self.embedding_flex_nonlinear_map(word_embeddings)\n\n            if self.pooling == ""mean"":\n                pooled_embedding = torch.mean(word_embeddings, 0)\n            elif self.pooling == ""max"":\n                pooled_embedding, _ = torch.max(word_embeddings, 0)\n            elif self.pooling == ""min"":\n                pooled_embedding, _ = torch.min(word_embeddings, 0)\n\n            sentence.set_embedding(self.name, pooled_embedding)\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]):\n        pass\n\n    def extra_repr(self):\n        return f""fine_tune_mode={self.fine_tune_mode}, pooling={self.pooling}""\n\n\nclass DocumentRNNEmbeddings(DocumentEmbeddings):\n    def __init__(\n        self,\n        embeddings: List[TokenEmbeddings],\n        hidden_size=128,\n        rnn_layers=1,\n        reproject_words: bool = True,\n        reproject_words_dimension: int = None,\n        bidirectional: bool = False,\n        dropout: float = 0.5,\n        word_dropout: float = 0.0,\n        locked_dropout: float = 0.0,\n        rnn_type=""GRU"",\n        fine_tune: bool = True,\n    ):\n        """"""The constructor takes a list of embeddings to be combined.\n        :param embeddings: a list of token embeddings\n        :param hidden_size: the number of hidden states in the rnn\n        :param rnn_layers: the number of layers for the rnn\n        :param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n        layer before putting them into the rnn or not\n        :param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n        dimension as before will be taken.\n        :param bidirectional: boolean value, indicating whether to use a bidirectional rnn or not\n        :param dropout: the dropout value to be used\n        :param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used\n        :param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used\n        :param rnn_type: \'GRU\' or \'LSTM\'\n        """"""\n        super().__init__()\n\n        self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embeddings)\n\n        self.rnn_type = rnn_type\n\n        self.reproject_words = reproject_words\n        self.bidirectional = bidirectional\n\n        self.length_of_all_token_embeddings: int = self.embeddings.embedding_length\n\n        self.static_embeddings = False if fine_tune else True\n\n        self.__embedding_length: int = hidden_size\n        if self.bidirectional:\n            self.__embedding_length *= 4\n\n        self.embeddings_dimension: int = self.length_of_all_token_embeddings\n        if self.reproject_words and reproject_words_dimension is not None:\n            self.embeddings_dimension = reproject_words_dimension\n\n        self.word_reprojection_map = torch.nn.Linear(\n            self.length_of_all_token_embeddings, self.embeddings_dimension\n        )\n\n        # bidirectional RNN on top of embedding layer\n        if rnn_type == ""LSTM"":\n            self.rnn = torch.nn.LSTM(\n                self.embeddings_dimension,\n                hidden_size,\n                num_layers=rnn_layers,\n                bidirectional=self.bidirectional,\n                batch_first=True,\n            )\n        else:\n            self.rnn = torch.nn.GRU(\n                self.embeddings_dimension,\n                hidden_size,\n                num_layers=rnn_layers,\n                bidirectional=self.bidirectional,\n                batch_first=True,\n            )\n\n        self.name = ""document_"" + self.rnn._get_name()\n\n        # dropouts\n        self.dropout = torch.nn.Dropout(dropout) if dropout > 0.0 else None\n        self.locked_dropout = (\n            LockedDropout(locked_dropout) if locked_dropout > 0.0 else None\n        )\n        self.word_dropout = WordDropout(word_dropout) if word_dropout > 0.0 else None\n\n        torch.nn.init.xavier_uniform_(self.word_reprojection_map.weight)\n\n        self.to(flair.device)\n\n        self.eval()\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: Union[List[Sentence], Sentence]):\n        """"""Add embeddings to all sentences in the given list of sentences. If embeddings are already added, update\n         only if embeddings are non-static.""""""\n\n        # TODO: remove in future versions\n        if not hasattr(self, ""locked_dropout""):\n            self.locked_dropout = None\n        if not hasattr(self, ""word_dropout""):\n            self.word_dropout = None\n\n        if type(sentences) is Sentence:\n            sentences = [sentences]\n\n        self.rnn.zero_grad()\n\n        # embed words in the sentence\n        self.embeddings.embed(sentences)\n\n        lengths: List[int] = [len(sentence.tokens) for sentence in sentences]\n        longest_token_sequence_in_batch: int = max(lengths)\n\n        pre_allocated_zero_tensor = torch.zeros(\n            self.embeddings.embedding_length * longest_token_sequence_in_batch,\n            dtype=torch.float,\n            device=flair.device,\n        )\n\n        all_embs: List[torch.Tensor] = list()\n        for sentence in sentences:\n            all_embs += [\n                emb for token in sentence for emb in token.get_each_embedding()\n            ]\n            nb_padding_tokens = longest_token_sequence_in_batch - len(sentence)\n\n            if nb_padding_tokens > 0:\n                t = pre_allocated_zero_tensor[\n                    : self.embeddings.embedding_length * nb_padding_tokens\n                ]\n                all_embs.append(t)\n\n        sentence_tensor = torch.cat(all_embs).view(\n            [\n                len(sentences),\n                longest_token_sequence_in_batch,\n                self.embeddings.embedding_length,\n            ]\n        )\n\n        # before-RNN dropout\n        if self.dropout:\n            sentence_tensor = self.dropout(sentence_tensor)\n        if self.locked_dropout:\n            sentence_tensor = self.locked_dropout(sentence_tensor)\n        if self.word_dropout:\n            sentence_tensor = self.word_dropout(sentence_tensor)\n\n        # reproject if set\n        if self.reproject_words:\n            sentence_tensor = self.word_reprojection_map(sentence_tensor)\n\n        # push through RNN\n        packed = pack_padded_sequence(\n            sentence_tensor, lengths, enforce_sorted=False, batch_first=True\n        )\n        rnn_out, hidden = self.rnn(packed)\n        outputs, output_lengths = pad_packed_sequence(rnn_out, batch_first=True)\n\n        # after-RNN dropout\n        if self.dropout:\n            outputs = self.dropout(outputs)\n        if self.locked_dropout:\n            outputs = self.locked_dropout(outputs)\n\n        # extract embeddings from RNN\n        for sentence_no, length in enumerate(lengths):\n            last_rep = outputs[sentence_no, length - 1]\n\n            embedding = last_rep\n            if self.bidirectional:\n                first_rep = outputs[sentence_no, 0]\n                embedding = torch.cat([first_rep, last_rep], 0)\n\n            if self.static_embeddings:\n                embedding = embedding.detach()\n\n            sentence = sentences[sentence_no]\n            sentence.set_embedding(self.name, embedding)\n\n    def _apply(self, fn):\n        major, minor, build, *_ = (int(info)\n                                for info in torch.__version__.replace(""+"",""."").split(\'.\') if info.isdigit())\n\n        # fixed RNN change format for torch 1.4.0\n        if major >= 1 and minor >= 4:\n            for child_module in self.children():\n                if isinstance(child_module, torch.nn.RNNBase):\n                    _flat_weights_names = []\n                    num_direction = None\n\n                    if child_module.__dict__[""bidirectional""]:\n                        num_direction = 2\n                    else:\n                        num_direction = 1\n                    for layer in range(child_module.__dict__[""num_layers""]):\n                        for direction in range(num_direction):\n                            suffix = ""_reverse"" if direction == 1 else """"\n                            param_names = [""weight_ih_l{}{}"", ""weight_hh_l{}{}""]\n                            if child_module.__dict__[""bias""]:\n                                param_names += [""bias_ih_l{}{}"", ""bias_hh_l{}{}""]\n                            param_names = [\n                                x.format(layer, suffix) for x in param_names\n                            ]\n                            _flat_weights_names.extend(param_names)\n\n                    setattr(child_module, ""_flat_weights_names"",\n                            _flat_weights_names)\n\n                child_module._apply(fn)\n\n        else:\n            super()._apply(fn)\n\n\nclass DocumentLMEmbeddings(DocumentEmbeddings):\n    def __init__(self, flair_embeddings: List[FlairEmbeddings]):\n        super().__init__()\n\n        self.embeddings = flair_embeddings\n        self.name = ""document_lm""\n\n        # IMPORTANT: add embeddings as torch modules\n        for i, embedding in enumerate(flair_embeddings):\n            self.add_module(""lm_embedding_{}"".format(i), embedding)\n            if not embedding.static_embeddings:\n                self.static_embeddings = False\n\n        self._embedding_length: int = sum(\n            embedding.embedding_length for embedding in flair_embeddings\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self._embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]):\n        if type(sentences) is Sentence:\n            sentences = [sentences]\n\n        for embedding in self.embeddings:\n            embedding.embed(sentences)\n\n            # iterate over sentences\n            for sentence in sentences:\n                sentence: Sentence = sentence\n\n                # if its a forward LM, take last state\n                if embedding.is_forward_lm:\n                    sentence.set_embedding(\n                        embedding.name,\n                        sentence[len(sentence) - 1]._embeddings[embedding.name],\n                    )\n                else:\n                    sentence.set_embedding(\n                        embedding.name, sentence[0]._embeddings[embedding.name]\n                    )\n\n        return sentences'"
flair/embeddings/image.py,17,"b'from abc import abstractmethod\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\nimport flair\nfrom flair.data import Image\nfrom flair.embeddings.base import Embeddings\n\nimport logging\n\nfrom torch.nn import Sequential, Linear, Conv2d, ReLU, MaxPool2d, Dropout2d\nfrom torch.nn import AdaptiveAvgPool2d, AdaptiveMaxPool2d\nfrom torch.nn import TransformerEncoderLayer, TransformerEncoder\n\n\nlog = logging.getLogger(""flair"")\n\n\nclass ImageEmbeddings(Embeddings):\n    @property\n    @abstractmethod\n    def embedding_length(self) -> int:\n        """"""Returns the length of the embedding vector.""""""\n        pass\n\n    @property\n    def embedding_type(self) -> str:\n        return ""image-level""\n\n\nclass IdentityImageEmbeddings(ImageEmbeddings):\n    def __init__(self, transforms):\n        import PIL as pythonimagelib\n\n        self.PIL = pythonimagelib\n        self.name = ""Identity""\n        self.transforms = transforms\n        self.__embedding_length = None\n        self.static_embeddings = True\n        super().__init__()\n\n    def _add_embeddings_internal(self, images: List[Image]) -> List[Image]:\n        for image in images:\n            image_data = self.PIL.Image.open(image.imageURL)\n            image_data.load()\n            image.set_embedding(self.name, self.transforms(image_data))\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def __str__(self):\n        return self.name\n\n\nclass PrecomputedImageEmbeddings(ImageEmbeddings):\n    def __init__(self, url2tensor_dict, name):\n        self.url2tensor_dict = url2tensor_dict\n        self.name = name\n        self.__embedding_length = len(list(self.url2tensor_dict.values())[0])\n        self.static_embeddings = True\n        super().__init__()\n\n    def _add_embeddings_internal(self, images: List[Image]) -> List[Image]:\n        for image in images:\n            if image.imageURL in self.url2tensor_dict:\n                image.set_embedding(self.name, self.url2tensor_dict[image.imageURL])\n            else:\n                image.set_embedding(\n                    self.name, torch.zeros(self.__embedding_length, device=flair.device)\n                )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def __str__(self):\n        return self.name\n\n\nclass NetworkImageEmbeddings(ImageEmbeddings):\n    def __init__(self, name, pretrained=True, transforms=None):\n        super().__init__()\n\n        try:\n            import torchvision as torchvision\n        except ModuleNotFoundError:\n            log.warning(""-"" * 100)\n            log.warning(\'ATTENTION! The library ""torchvision"" is not installed!\')\n            log.warning(\n                \'To use convnets pretraned on ImageNet, please first install with ""pip install torchvision""\'\n            )\n            log.warning(""-"" * 100)\n            pass\n\n        model_info = {\n            ""resnet50"": (torchvision.models.resnet50, lambda x: list(x)[:-1], 2048),\n            ""mobilenet_v2"": (\n                torchvision.models.mobilenet_v2,\n                lambda x: list(x)[:-1] + [torch.nn.AdaptiveAvgPool2d((1, 1))],\n                1280,\n            ),\n        }\n\n        transforms = [] if transforms is None else transforms\n        transforms += [torchvision.transforms.ToTensor()]\n        if pretrained:\n            imagenet_mean = [0.485, 0.456, 0.406]\n            imagenet_std = [0.229, 0.224, 0.225]\n            transforms += [\n                torchvision.transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n            ]\n        self.transforms = torchvision.transforms.Compose(transforms)\n\n        if name in model_info:\n            model_constructor = model_info[name][0]\n            model_features = model_info[name][1]\n            embedding_length = model_info[name][2]\n\n            net = model_constructor(pretrained=pretrained)\n            modules = model_features(net.children())\n            self.features = torch.nn.Sequential(*modules)\n\n            self.__embedding_length = embedding_length\n\n            self.name = name\n        else:\n            raise Exception(f""Image embeddings {name} not available."")\n\n    def _add_embeddings_internal(self, images: List[Image]) -> List[Image]:\n        image_tensor = torch.stack([self.transforms(image.data) for image in images])\n        image_embeddings = self.features(image_tensor)\n        image_embeddings = (\n            image_embeddings.view(image_embeddings.shape[:2])\n            if image_embeddings.dim() == 4\n            else image_embeddings\n        )\n        if image_embeddings.dim() != 2:\n            raise Exception(\n                f""Unknown embedding shape of length {image_embeddings.dim()}""\n            )\n        for image_id, image in enumerate(images):\n            image.set_embedding(self.name, image_embeddings[image_id])\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def __str__(self):\n        return self.name\n\n\nclass ConvTransformNetworkImageEmbeddings(ImageEmbeddings):\n    def __init__(self, feats_in, convnet_parms, posnet_parms, transformer_parms):\n        super(ConvTransformNetworkImageEmbeddings, self).__init__()\n\n        adaptive_pool_func_map = {""max"": AdaptiveMaxPool2d, ""avg"": AdaptiveAvgPool2d}\n\n        convnet_arch = (\n            []\n            if convnet_parms[""dropout""][0] <= 0\n            else [Dropout2d(convnet_parms[""dropout""][0])]\n        )\n        convnet_arch.extend(\n            [\n                Conv2d(\n                    in_channels=feats_in,\n                    out_channels=convnet_parms[""n_feats_out""][0],\n                    kernel_size=convnet_parms[""kernel_sizes""][0],\n                    padding=convnet_parms[""kernel_sizes""][0][0] // 2,\n                    stride=convnet_parms[""strides""][0],\n                    groups=convnet_parms[""groups""][0],\n                ),\n                ReLU(),\n            ]\n        )\n        if ""0"" in convnet_parms[""pool_layers_map""]:\n            convnet_arch.append(\n                MaxPool2d(kernel_size=convnet_parms[""pool_layers_map""][""0""])\n            )\n        for layer_id, (kernel_size, n_in, n_out, groups, stride, dropout) in enumerate(\n            zip(\n                convnet_parms[""kernel_sizes""][1:],\n                convnet_parms[""n_feats_out""][:-1],\n                convnet_parms[""n_feats_out""][1:],\n                convnet_parms[""groups""][1:],\n                convnet_parms[""strides""][1:],\n                convnet_parms[""dropout""][1:],\n            )\n        ):\n            if dropout > 0:\n                convnet_arch.append(Dropout2d(dropout))\n            convnet_arch.append(\n                Conv2d(\n                    in_channels=n_in,\n                    out_channels=n_out,\n                    kernel_size=kernel_size,\n                    padding=kernel_size[0] // 2,\n                    stride=stride,\n                    groups=groups,\n                )\n            )\n            convnet_arch.append(ReLU())\n            if str(layer_id + 1) in convnet_parms[""pool_layers_map""]:\n                convnet_arch.append(\n                    MaxPool2d(\n                        kernel_size=convnet_parms[""pool_layers_map""][str(layer_id + 1)]\n                    )\n                )\n        convnet_arch.append(\n            adaptive_pool_func_map[convnet_parms[""adaptive_pool_func""]](\n                output_size=convnet_parms[""output_size""]\n            )\n        )\n        self.conv_features = Sequential(*convnet_arch)\n        conv_feat_dim = convnet_parms[""n_feats_out""][-1]\n        if posnet_parms is not None and transformer_parms is not None:\n            self.use_transformer = True\n            if posnet_parms[""nonlinear""]:\n                posnet_arch = [\n                    Linear(2, posnet_parms[""n_hidden""]),\n                    ReLU(),\n                    Linear(posnet_parms[""n_hidden""], conv_feat_dim),\n                ]\n            else:\n                posnet_arch = [Linear(2, conv_feat_dim)]\n            self.position_features = Sequential(*posnet_arch)\n            transformer_layer = TransformerEncoderLayer(\n                d_model=conv_feat_dim, **transformer_parms[""transformer_encoder_parms""]\n            )\n            self.transformer = TransformerEncoder(\n                transformer_layer, num_layers=transformer_parms[""n_blocks""]\n            )\n            # <cls> token initially set to 1/D, so it attends to all image features equally\n            self.cls_token = Parameter(torch.ones(conv_feat_dim, 1) / conv_feat_dim)\n            self._feat_dim = conv_feat_dim\n        else:\n            self.use_transformer = False\n            self._feat_dim = (\n                convnet_parms[""output_size""][0]\n                * convnet_parms[""output_size""][1]\n                * conv_feat_dim\n            )\n\n    def forward(self, x):\n        x = self.conv_features(x)  # [b, d, h, w]\n        b, d, h, w = x.shape\n        if self.use_transformer:\n            # add positional encodings\n            y = torch.stack(\n                [\n                    torch.cat([torch.arange(h).unsqueeze(1)] * w, dim=1),\n                    torch.cat([torch.arange(w).unsqueeze(0)] * h, dim=0),\n                ]\n            )  # [2, h, w\n            y = y.view([2, h * w]).transpose(1, 0)  # [h*w, 2]\n            y = y.type(torch.float32).to(flair.device)\n            y = (\n                self.position_features(y).transpose(1, 0).view([d, h, w])\n            )  # [h*w, d] => [d, h, w]\n            y = y.unsqueeze(dim=0)  # [1, d, h, w]\n            x = x + y  # [b, d, h, w] + [1, d, h, w] => [b, d, h, w]\n            # reshape the pixels into the sequence\n            x = x.view([b, d, h * w])  # [b, d, h*w]\n            # layer norm after convolution and positional encodings\n            x = F.layer_norm(x.permute([0, 2, 1]), (d,)).permute([0, 2, 1])\n            # add <cls> token\n            x = torch.cat(\n                [x, torch.stack([self.cls_token] * b)], dim=2\n            )  # [b, d, h*w+1]\n            # transformer requires input in the shape [h*w+1, b, d]\n            x = (\n                x.view([b * d, h * w + 1]).transpose(1, 0).view([h * w + 1, b, d])\n            )  # [b, d, h*w+1] => [b*d, h*w+1] => [h*w+1, b*d] => [h*w+1, b*d]\n            x = self.transformer(x)  # [h*w+1, b, d]\n            # the output is an embedding of <cls> token\n            x = x[-1, :, :]  # [b, d]\n        else:\n            x = x.view([-1, self._feat_dim])\n            x = F.layer_norm(x, (self._feat_dim,))\n\n        return x\n\n    def _add_embeddings_internal(self, images: List[Image]) -> List[Image]:\n        image_tensor = torch.stack([image.data for image in images])\n        image_embeddings = self.forward(image_tensor)\n        for image_id, image in enumerate(images):\n            image.set_embedding(self.name, image_embeddings[image_id])\n\n    @property\n    def embedding_length(self):\n        return self._feat_dim\n\n    def __str__(self):\n        return self.name\n'"
flair/embeddings/legacy.py,36,"b'from pathlib import Path\nfrom deprecated import deprecated\nfrom abc import abstractmethod\nfrom typing import List, Union, Tuple, Dict\n\nimport torch\nimport logging\nimport flair\n\nfrom flair.data import Sentence, Token\nfrom flair.embeddings.base import ScalarMix\nfrom flair.embeddings.document import DocumentEmbeddings\nfrom flair.embeddings.token import TokenEmbeddings, StackedEmbeddings\nfrom flair.file_utils import cached_path\n\nfrom transformers import (\n    AlbertTokenizer,\n    AlbertModel,\n    BertTokenizer,\n    BertModel,\n    CamembertTokenizer,\n    CamembertModel,\n    RobertaTokenizer,\n    RobertaModel,\n    TransfoXLTokenizer,\n    TransfoXLModel,\n    OpenAIGPTModel,\n    OpenAIGPTTokenizer,\n    GPT2Model,\n    GPT2Tokenizer,\n    XLNetTokenizer,\n    XLMTokenizer,\n    XLNetModel,\n    XLMModel,\n    XLMRobertaTokenizer,\n    XLMRobertaModel,\n    PreTrainedTokenizer,\n    PreTrainedModel,\n    AutoTokenizer, AutoConfig, AutoModel, T5Tokenizer)\n\nfrom flair.nn import LockedDropout, WordDropout\n\nlog = logging.getLogger(""flair"")\n\n\nclass CharLMEmbeddings(TokenEmbeddings):\n    """"""Contextual string embeddings of words, as proposed in Akbik et al., 2018. """"""\n\n    @deprecated(version=""0.4"", reason=""Use \'FlairEmbeddings\' instead."")\n    def __init__(\n        self,\n        model: str,\n        detach: bool = True,\n        use_cache: bool = False,\n        cache_directory: Path = None,\n    ):\n        """"""\n        initializes contextual string embeddings using a character-level language model.\n        :param model: model string, one of \'news-forward\', \'news-backward\', \'news-forward-fast\', \'news-backward-fast\',\n                \'mix-forward\', \'mix-backward\', \'german-forward\', \'german-backward\', \'polish-backward\', \'polish-forward\'\n                depending on which character language model is desired.\n        :param detach: if set to False, the gradient will propagate into the language model. this dramatically slows down\n                training and often leads to worse results, so not recommended.\n        :param use_cache: if set to False, will not write embeddings to file for later retrieval. this saves disk space but will\n                not allow re-use of once computed embeddings that do not fit into memory\n        :param cache_directory: if cache_directory is not set, the cache will be written to ~/.flair/embeddings. otherwise the cache\n                is written to the provided directory.\n        """"""\n        super().__init__()\n\n        cache_dir = Path(""embeddings"")\n\n        # multilingual forward (English, German, French, Italian, Dutch, Polish)\n        if model.lower() == ""multi-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # multilingual backward  (English, German, French, Italian, Dutch, Polish)\n        elif model.lower() == ""multi-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # news-english-forward\n        elif model.lower() == ""news-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # news-english-backward\n        elif model.lower() == ""news-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # news-english-forward\n        elif model.lower() == ""news-forward-fast"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # news-english-backward\n        elif model.lower() == ""news-backward-fast"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # mix-english-forward\n        elif model.lower() == ""mix-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-forward-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # mix-english-backward\n        elif model.lower() == ""mix-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-backward-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # mix-german-forward\n        elif model.lower() == ""german-forward"" or model.lower() == ""de-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-forward-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # mix-german-backward\n        elif model.lower() == ""german-backward"" or model.lower() == ""de-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-backward-v0.2rc.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # common crawl Polish forward\n        elif model.lower() == ""polish-forward"" or model.lower() == ""pl-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-forward-v0.2.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # common crawl Polish backward\n        elif model.lower() == ""polish-backward"" or model.lower() == ""pl-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-backward-v0.2.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # Slovenian forward\n        elif model.lower() == ""slovenian-forward"" or model.lower() == ""sl-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-forward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # Slovenian backward\n        elif model.lower() == ""slovenian-backward"" or model.lower() == ""sl-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-backward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # Bulgarian forward\n        elif model.lower() == ""bulgarian-forward"" or model.lower() == ""bg-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-forward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # Bulgarian backward\n        elif model.lower() == ""bulgarian-backward"" or model.lower() == ""bg-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-backward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # Dutch forward\n        elif model.lower() == ""dutch-forward"" or model.lower() == ""nl-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-nl-large-forward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # Dutch backward\n        elif model.lower() == ""dutch-backward"" or model.lower() == ""nl-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-nl-large-backward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # Swedish forward\n        elif model.lower() == ""swedish-forward"" or model.lower() == ""sv-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-sv-large-forward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # Swedish backward\n        elif model.lower() == ""swedish-backward"" or model.lower() == ""sv-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-sv-large-backward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # French forward\n        elif model.lower() == ""french-forward"" or model.lower() == ""fr-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-fr-charlm-forward.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # French backward\n        elif model.lower() == ""french-backward"" or model.lower() == ""fr-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-fr-charlm-backward.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # Czech forward\n        elif model.lower() == ""czech-forward"" or model.lower() == ""cs-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-cs-large-forward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # Czech backward\n        elif model.lower() == ""czech-backward"" or model.lower() == ""cs-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-cs-large-backward-v0.1.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        # Portuguese forward\n        elif model.lower() == ""portuguese-forward"" or model.lower() == ""pt-forward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-pt-forward.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n        # Portuguese backward\n        elif model.lower() == ""portuguese-backward"" or model.lower() == ""pt-backward"":\n            base_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-pt-backward.pt""\n            model = cached_path(base_path, cache_dir=cache_dir)\n\n        elif not Path(model).exists():\n            raise ValueError(\n                f\'The given model ""{model}"" is not available or is not a valid path.\'\n            )\n\n        self.name = str(model)\n        self.static_embeddings = detach\n\n        from flair.models import LanguageModel\n\n        self.lm = LanguageModel.load_language_model(model)\n        self.detach = detach\n\n        self.is_forward_lm: bool = self.lm.is_forward_lm\n\n        # initialize cache if use_cache set\n        self.cache = None\n        if use_cache:\n            cache_path = (\n                Path(f""{self.name}-tmp-cache.sqllite"")\n                if not cache_directory\n                else cache_directory / f""{self.name}-tmp-cache.sqllite""\n            )\n            from sqlitedict import SqliteDict\n\n            self.cache = SqliteDict(str(cache_path), autocommit=True)\n\n        # embed a dummy sentence to determine embedding_length\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n        # set to eval mode\n        self.eval()\n\n    def train(self, mode=True):\n        pass\n\n    def __getstate__(self):\n        # Copy the object\'s state from self.__dict__ which contains\n        # all our instance attributes. Always use the dict.copy()\n        # method to avoid modifying the original state.\n        state = self.__dict__.copy()\n        # Remove the unpicklable entries.\n        state[""cache""] = None\n        return state\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        # if cache is used, try setting embeddings from cache first\n        if ""cache"" in self.__dict__ and self.cache is not None:\n\n            # try populating embeddings from cache\n            all_embeddings_retrieved_from_cache: bool = True\n            for sentence in sentences:\n                key = sentence.to_tokenized_string()\n                embeddings = self.cache.get(key)\n\n                if not embeddings:\n                    all_embeddings_retrieved_from_cache = False\n                    break\n                else:\n                    for token, embedding in zip(sentence, embeddings):\n                        token.set_embedding(self.name, torch.FloatTensor(embedding))\n\n            if all_embeddings_retrieved_from_cache:\n                return sentences\n\n        # if this is not possible, use LM to generate embedding. First, get text sentences\n        text_sentences = [sentence.to_tokenized_string() for sentence in sentences]\n\n        start_marker = ""\\n""\n        end_marker = "" ""\n\n        # get hidden states from language model\n        all_hidden_states_in_lm = self.lm.get_representation(\n            text_sentences, start_marker, end_marker, self.chars_per_chunk\n        )\n\n        # take first or last hidden states from language model as word representation\n        for i, sentence in enumerate(sentences):\n            sentence_text = sentence.to_tokenized_string()\n\n            offset_forward: int = len(start_marker)\n            offset_backward: int = len(sentence_text) + len(start_marker)\n\n            for token in sentence.tokens:\n\n                offset_forward += len(token.text)\n\n                if self.is_forward_lm:\n                    offset = offset_forward\n                else:\n                    offset = offset_backward\n\n                embedding = all_hidden_states_in_lm[offset, i, :]\n\n                # if self.tokenized_lm or token.whitespace_after:\n                offset_forward += 1\n                offset_backward -= 1\n\n                offset_backward -= len(token.text)\n\n                token.set_embedding(self.name, embedding)\n\n        if ""cache"" in self.__dict__ and self.cache is not None:\n            for sentence in sentences:\n                self.cache[sentence.to_tokenized_string()] = [\n                    token._embeddings[self.name].tolist() for token in sentence\n                ]\n\n        return sentences\n\n    def __str__(self):\n        return self.name\n\n\nclass TransformerXLEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""transfo-xl-wt103"",\n        layers: str = ""1,2,3"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""Transformer-XL embeddings, as proposed in Dai et al., 2019.\n        :param pretrained_model_name_or_path: name or path of Transformer-XL model\n        :param layers: comma-separated list of layers\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = TransfoXLTokenizer.from_pretrained(\n            pretrained_model_name_or_path\n        )\n        self.model = TransfoXLModel.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=""first"",\n            use_scalar_mix=self.use_scalar_mix,\n            eos_token=""<eos>"",\n        )\n\n        return sentences\n\n    def extra_repr(self):\n        return ""model={}"".format(self.name)\n\n    def __str__(self):\n        return self.name\n\n\nclass XLNetEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""xlnet-large-cased"",\n        layers: str = ""1"",\n        pooling_operation: str = ""first_last"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""XLNet embeddings, as proposed in Yang et al., 2019.\n        :param pretrained_model_name_or_path: name or path of XLNet model\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = XLNetTokenizer.from_pretrained(pretrained_model_name_or_path)\n        self.model = XLNetModel.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=self.pooling_operation,\n            use_scalar_mix=self.use_scalar_mix,\n            bos_token=""<s>"",\n            eos_token=""</s>"",\n        )\n\n        return sentences\n\n    def extra_repr(self):\n        return ""model={}"".format(self.name)\n\n    def __str__(self):\n        return self.name\n\n\nclass XLMEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""xlm-mlm-en-2048"",\n        layers: str = ""1"",\n        pooling_operation: str = ""first_last"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""\n        XLM embeddings, as proposed in Guillaume et al., 2019.\n        :param pretrained_model_name_or_path: name or path of XLM model\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = XLMTokenizer.from_pretrained(pretrained_model_name_or_path)\n        self.model = XLMModel.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=self.pooling_operation,\n            use_scalar_mix=self.use_scalar_mix,\n            bos_token=""<s>"",\n            eos_token=""</s>"",\n        )\n\n        return sentences\n\n    def extra_repr(self):\n        return ""model={}"".format(self.name)\n\n    def __str__(self):\n        return self.name\n\n\nclass OpenAIGPTEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""openai-gpt"",\n        layers: str = ""1"",\n        pooling_operation: str = ""first_last"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""OpenAI GPT embeddings, as proposed in Radford et al. 2018.\n        :param pretrained_model_name_or_path: name or path of OpenAI GPT model\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = OpenAIGPTTokenizer.from_pretrained(\n            pretrained_model_name_or_path\n        )\n        self.model = OpenAIGPTModel.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=self.pooling_operation,\n            use_scalar_mix=self.use_scalar_mix,\n        )\n\n        return sentences\n\n    def extra_repr(self):\n        return ""model={}"".format(self.name)\n\n    def __str__(self):\n        return self.name\n\n\nclass OpenAIGPT2Embeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""gpt2-medium"",\n        layers: str = ""1"",\n        pooling_operation: str = ""first_last"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""OpenAI GPT-2 embeddings, as proposed in Radford et al. 2019.\n        :param pretrained_model_name_or_path: name or path of OpenAI GPT-2 model\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path)\n        self.model = GPT2Model.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=self.pooling_operation,\n            use_scalar_mix=self.use_scalar_mix,\n            bos_token=""<|endoftext|>"",\n            eos_token=""<|endoftext|>"",\n        )\n\n        return sentences\n\n\nclass RoBERTaEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""roberta-base"",\n        layers: str = ""-1"",\n        pooling_operation: str = ""first"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""RoBERTa, as proposed by Liu et al. 2019.\n        :param pretrained_model_name_or_path: name or path of RoBERTa model\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path)\n        self.model = RobertaModel.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=self.pooling_operation,\n            use_scalar_mix=self.use_scalar_mix,\n            bos_token=""<s>"",\n            eos_token=""</s>"",\n        )\n\n        return sentences\n\n\nclass CamembertEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""camembert-base"",\n        layers: str = ""-1"",\n        pooling_operation: str = ""first"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""CamemBERT, a Tasty French Language Model, as proposed by Martin et al. 2019.\n        :param pretrained_model_name_or_path: name or path of RoBERTa model\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = CamembertTokenizer.from_pretrained(\n            pretrained_model_name_or_path\n        )\n        self.model = CamembertModel.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[""tokenizer""] = None\n        return state\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n\n        # 1-camembert-base -> camembert-base\n        if any(char.isdigit() for char in self.name):\n            self.tokenizer = CamembertTokenizer.from_pretrained(\n                ""-"".join(self.name.split(""-"")[1:]))\n        else:\n            self.tokenizer = CamembertTokenizer.from_pretrained(self.name)\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=self.pooling_operation,\n            use_scalar_mix=self.use_scalar_mix,\n            bos_token=""<s>"",\n            eos_token=""</s>"",\n        )\n\n        return sentences\n\n\nclass XLMRobertaEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        pretrained_model_name_or_path: str = ""xlm-roberta-large"",\n        layers: str = ""-1"",\n        pooling_operation: str = ""first"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""XLM-RoBERTa as proposed by Conneau et al. 2019.\n        :param pretrained_model_name_or_path: name or path of XLM-R model\n        :param layers: comma-separated list of layers\n        :param pooling_operation: defines pooling operation for subwords\n        :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n        """"""\n        super().__init__()\n\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained(\n            pretrained_model_name_or_path\n        )\n        self.model = XLMRobertaModel.from_pretrained(\n            pretrained_model_name_or_path=pretrained_model_name_or_path,\n            output_hidden_states=True,\n        )\n        self.name = pretrained_model_name_or_path\n        self.layers: List[int] = [int(layer) for layer in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.static_embeddings = True\n\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[""tokenizer""] = None\n        return state\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n\n        # 1-xlm-roberta-large -> xlm-roberta-large\n        self.tokenizer = self.tokenizer = XLMRobertaTokenizer.from_pretrained(\n            ""-"".join(self.name.split(""-"")[1:])\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        self.model.to(flair.device)\n        self.model.eval()\n\n        sentences = _get_transformer_sentence_embeddings(\n            sentences=sentences,\n            tokenizer=self.tokenizer,\n            model=self.model,\n            name=self.name,\n            layers=self.layers,\n            pooling_operation=self.pooling_operation,\n            use_scalar_mix=self.use_scalar_mix,\n            bos_token=""<s>"",\n            eos_token=""</s>"",\n        )\n\n        return sentences\n\ndef _extract_embeddings(\n    hidden_states: List[torch.FloatTensor],\n    layers: List[int],\n    pooling_operation: str,\n    subword_start_idx: int,\n    subword_end_idx: int,\n    use_scalar_mix: bool = False,\n) -> List[torch.FloatTensor]:\n    """"""\n    Extracts subword embeddings from specified layers from hidden states.\n    :param hidden_states: list of hidden states from model\n    :param layers: list of layers\n    :param pooling_operation: pooling operation for subword embeddings (supported: first, last, first_last and mean)\n    :param subword_start_idx: defines start index for subword\n    :param subword_end_idx: defines end index for subword\n    :param use_scalar_mix: determines, if scalar mix should be used\n    :return: list of extracted subword embeddings\n    """"""\n    subtoken_embeddings: List[torch.FloatTensor] = []\n\n    for layer in layers:\n        current_embeddings = hidden_states[layer][0][subword_start_idx:subword_end_idx]\n\n        first_embedding: torch.FloatTensor = current_embeddings[0]\n        if pooling_operation == ""first_last"":\n            last_embedding: torch.FloatTensor = current_embeddings[-1]\n            final_embedding: torch.FloatTensor = torch.cat(\n                [first_embedding, last_embedding]\n            )\n        elif pooling_operation == ""last"":\n            final_embedding: torch.FloatTensor = current_embeddings[-1]\n        elif pooling_operation == ""mean"":\n            all_embeddings: List[torch.FloatTensor] = [\n                embedding.unsqueeze(0) for embedding in current_embeddings\n            ]\n            final_embedding: torch.FloatTensor = torch.mean(\n                torch.cat(all_embeddings, dim=0), dim=0\n            )\n        else:\n            final_embedding: torch.FloatTensor = first_embedding\n\n        subtoken_embeddings.append(final_embedding)\n\n    if use_scalar_mix:\n        sm = ScalarMix(mixture_size=len(subtoken_embeddings))\n        sm_embeddings = sm(subtoken_embeddings)\n\n        subtoken_embeddings = [sm_embeddings]\n\n    return subtoken_embeddings\n\n\ndef _build_token_subwords_mapping(\n    sentence: Sentence, tokenizer: PreTrainedTokenizer\n) -> Tuple[Dict[int, int], str]:\n    """""" Builds a dictionary that stores the following information:\n    Token index (key) and number of corresponding subwords (value) for a sentence.\n\n    :param sentence: input sentence\n    :param tokenizer: Transformers tokenization object\n    :return: dictionary of token index to corresponding number of subwords, tokenized string\n    """"""\n    token_subwords_mapping: Dict[int, int] = {}\n\n    tokens = []\n\n    for token in sentence.tokens:\n        token_text = token.text\n\n        subwords = tokenizer.tokenize(token_text)\n\n        tokens.append(token.text if subwords else tokenizer.unk_token)\n\n        token_subwords_mapping[token.idx] = len(subwords) if subwords else 1\n\n    return token_subwords_mapping, "" "".join(tokens)\n\n\ndef _build_token_subwords_mapping_gpt2(\n    sentence: Sentence, tokenizer: PreTrainedTokenizer\n) -> Tuple[Dict[int, int], str]:\n    """""" Builds a dictionary that stores the following information:\n    Token index (key) and number of corresponding subwords (value) for a sentence.\n\n    :param sentence: input sentence\n    :param tokenizer: Transformers tokenization object\n    :return: dictionary of token index to corresponding number of subwords, tokenized string\n    """"""\n    token_subwords_mapping: Dict[int, int] = {}\n\n    tokens = []\n\n    for token in sentence.tokens:\n        # Dummy token is needed to get the actually token tokenized correctly with special ``\xc4\xa0`` symbol\n\n        if token.idx == 1:\n            token_text = token.text\n            subwords = tokenizer.tokenize(token_text)\n        else:\n            token_text = ""X "" + token.text\n            subwords = tokenizer.tokenize(token_text)[1:]\n\n        tokens.append(token.text if subwords else tokenizer.unk_token)\n\n        token_subwords_mapping[token.idx] = len(subwords) if subwords else 1\n\n    return token_subwords_mapping, "" "".join(tokens)\n\n\ndef _get_transformer_sentence_embeddings(\n    sentences: List[Sentence],\n    tokenizer: PreTrainedTokenizer,\n    model: PreTrainedModel,\n    name: str,\n    layers: List[int],\n    pooling_operation: str,\n    use_scalar_mix: bool,\n    bos_token: str = None,\n    eos_token: str = None,\n) -> List[Sentence]:\n    """"""\n    Builds sentence embeddings for Transformer-based architectures.\n    :param sentences: input sentences\n    :param tokenizer: tokenization object\n    :param model: model object\n    :param name: name of the Transformer-based model\n    :param layers: list of layers\n    :param pooling_operation: defines pooling operation for subword extraction\n    :param use_scalar_mix: defines the usage of scalar mix for specified layer(s)\n    :param bos_token: defines begin of sentence token (used for left padding)\n    :param eos_token: defines end of sentence token (used for right padding)\n    :return: list of sentences (each token of a sentence is now embedded)\n    """"""\n    with torch.no_grad():\n        for sentence in sentences:\n            token_subwords_mapping: Dict[int, int] = {}\n\n            if (""gpt2"" in name or ""roberta"" in name) and ""xlm"" not in name:\n                (\n                    token_subwords_mapping,\n                    tokenized_string,\n                ) = _build_token_subwords_mapping_gpt2(\n                    sentence=sentence, tokenizer=tokenizer\n                )\n            else:\n                (\n                    token_subwords_mapping,\n                    tokenized_string,\n                ) = _build_token_subwords_mapping(\n                    sentence=sentence, tokenizer=tokenizer\n                )\n\n            subwords = tokenizer.tokenize(tokenized_string)\n\n            offset = 0\n\n            if bos_token:\n                subwords = [bos_token] + subwords\n                offset = 1\n\n            if eos_token:\n                subwords = subwords + [eos_token]\n\n            indexed_tokens = tokenizer.convert_tokens_to_ids(subwords)\n            tokens_tensor = torch.tensor([indexed_tokens])\n            tokens_tensor = tokens_tensor.to(flair.device)\n\n            hidden_states = model(tokens_tensor)[-1]\n\n            for token in sentence.tokens:\n                len_subwords = token_subwords_mapping[token.idx]\n\n                subtoken_embeddings = _extract_embeddings(\n                    hidden_states=hidden_states,\n                    layers=layers,\n                    pooling_operation=pooling_operation,\n                    subword_start_idx=offset,\n                    subword_end_idx=offset + len_subwords,\n                    use_scalar_mix=use_scalar_mix,\n                )\n\n                offset += len_subwords\n\n                final_subtoken_embedding = torch.cat(subtoken_embeddings)\n                token.set_embedding(name, final_subtoken_embedding)\n\n    return sentences\n\n\nclass BertEmbeddings(TokenEmbeddings):\n\n    @deprecated(\n        version=""0.4.5"",\n        reason=""Use \'TransformerWordEmbeddings\' for all transformer-based word embeddings"",\n    )\n    def __init__(\n        self,\n        bert_model_or_path: str = ""bert-base-uncased"",\n        layers: str = ""-1,-2,-3,-4"",\n        pooling_operation: str = ""first"",\n        use_scalar_mix: bool = False,\n    ):\n        """"""\n        Bidirectional transformer embeddings of words, as proposed in Devlin et al., 2018.\n        :param bert_model_or_path: name of BERT model (\'\') or directory path containing custom model, configuration file\n        and vocab file (names of three files should be - config.json, pytorch_model.bin/model.chkpt, vocab.txt)\n        :param layers: string indicating which layers to take for embedding\n        :param pooling_operation: how to get from token piece embeddings to token embedding. Either pool them and take\n        the average (\'mean\') or use first word piece embedding as token embedding (\'first)\n        """"""\n        super().__init__()\n\n        if ""distilbert"" in bert_model_or_path:\n            try:\n                from transformers import DistilBertTokenizer, DistilBertModel\n            except ImportError:\n                log.warning(""-"" * 100)\n                log.warning(\n                    ""ATTENTION! To use DistilBert, please first install a recent version of transformers!""\n                )\n                log.warning(""-"" * 100)\n                pass\n\n            self.tokenizer = DistilBertTokenizer.from_pretrained(bert_model_or_path)\n            self.model = DistilBertModel.from_pretrained(\n                pretrained_model_name_or_path=bert_model_or_path,\n                output_hidden_states=True,\n            )\n        elif ""albert"" in bert_model_or_path:\n            self.tokenizer = AlbertTokenizer.from_pretrained(bert_model_or_path)\n            self.model = AlbertModel.from_pretrained(\n                pretrained_model_name_or_path=bert_model_or_path,\n                output_hidden_states=True,\n            )\n        else:\n            self.tokenizer = BertTokenizer.from_pretrained(bert_model_or_path)\n            self.model = BertModel.from_pretrained(\n                pretrained_model_name_or_path=bert_model_or_path,\n                output_hidden_states=True,\n            )\n        self.layer_indexes = [int(x) for x in layers.split("","")]\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.name = str(bert_model_or_path)\n        self.static_embeddings = True\n\n    class BertInputFeatures(object):\n        """"""Private helper class for holding BERT-formatted features""""""\n\n        def __init__(\n            self,\n            unique_id,\n            tokens,\n            input_ids,\n            input_mask,\n            input_type_ids,\n            token_subtoken_count,\n        ):\n            self.unique_id = unique_id\n            self.tokens = tokens\n            self.input_ids = input_ids\n            self.input_mask = input_mask\n            self.input_type_ids = input_type_ids\n            self.token_subtoken_count = token_subtoken_count\n\n    def _convert_sentences_to_features(\n        self, sentences, max_sequence_length: int\n    ) -> [BertInputFeatures]:\n\n        max_sequence_length = max_sequence_length + 2\n\n        features: List[BertEmbeddings.BertInputFeatures] = []\n        for (sentence_index, sentence) in enumerate(sentences):\n\n            bert_tokenization: List[str] = []\n            token_subtoken_count: Dict[int, int] = {}\n\n            for token in sentence:\n                subtokens = self.tokenizer.tokenize(token.text)\n                bert_tokenization.extend(subtokens)\n                token_subtoken_count[token.idx] = len(subtokens)\n\n            if len(bert_tokenization) > max_sequence_length - 2:\n                bert_tokenization = bert_tokenization[0 : (max_sequence_length - 2)]\n\n            tokens = []\n            input_type_ids = []\n            tokens.append(""[CLS]"")\n            input_type_ids.append(0)\n            for token in bert_tokenization:\n                tokens.append(token)\n                input_type_ids.append(0)\n            tokens.append(""[SEP]"")\n            input_type_ids.append(0)\n\n            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n            # tokens are attended to.\n            input_mask = [1] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            while len(input_ids) < max_sequence_length:\n                input_ids.append(0)\n                input_mask.append(0)\n                input_type_ids.append(0)\n\n            features.append(\n                BertEmbeddings.BertInputFeatures(\n                    unique_id=sentence_index,\n                    tokens=tokens,\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    input_type_ids=input_type_ids,\n                    token_subtoken_count=token_subtoken_count,\n                )\n            )\n\n        return features\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        """"""Add embeddings to all words in a list of sentences. If embeddings are already added,\n        updates only if embeddings are non-static.""""""\n\n        # first, find longest sentence in batch\n        longest_sentence_in_batch: int = len(\n            max(\n                [\n                    self.tokenizer.tokenize(sentence.to_tokenized_string())\n                    for sentence in sentences\n                ],\n                key=len,\n            )\n        )\n\n        # prepare id maps for BERT model\n        features = self._convert_sentences_to_features(\n            sentences, longest_sentence_in_batch\n        )\n        all_input_ids = torch.LongTensor([f.input_ids for f in features]).to(\n            flair.device\n        )\n        all_input_masks = torch.LongTensor([f.input_mask for f in features]).to(\n            flair.device\n        )\n\n        # put encoded batch through BERT model to get all hidden states of all encoder layers\n        self.model.to(flair.device)\n        self.model.eval()\n        all_encoder_layers = self.model(all_input_ids, attention_mask=all_input_masks)[\n            -1\n        ]\n\n        with torch.no_grad():\n\n            for sentence_index, sentence in enumerate(sentences):\n\n                feature = features[sentence_index]\n\n                # get aggregated embeddings for each BERT-subtoken in sentence\n                subtoken_embeddings = []\n                for token_index, _ in enumerate(feature.tokens):\n                    all_layers = []\n                    for layer_index in self.layer_indexes:\n                        layer_output = all_encoder_layers[int(layer_index)][\n                            sentence_index\n                        ]\n                        all_layers.append(layer_output[token_index])\n\n                    if self.use_scalar_mix:\n                        sm = ScalarMix(mixture_size=len(all_layers))\n                        sm_embeddings = sm(all_layers)\n                        all_layers = [sm_embeddings]\n\n                    subtoken_embeddings.append(torch.cat(all_layers))\n\n                # get the current sentence object\n                token_idx = 0\n                for token in sentence:\n                    # add concatenated embedding to sentence\n                    token_idx += 1\n\n                    if self.pooling_operation == ""first"":\n                        # use first subword embedding if pooling operation is \'first\'\n                        token.set_embedding(self.name, subtoken_embeddings[token_idx])\n                    else:\n                        # otherwise, do a mean over all subwords in token\n                        embeddings = subtoken_embeddings[\n                            token_idx : token_idx\n                            + feature.token_subtoken_count[token.idx]\n                        ]\n                        embeddings = [\n                            embedding.unsqueeze(0) for embedding in embeddings\n                        ]\n                        mean = torch.mean(torch.cat(embeddings, dim=0), dim=0)\n                        token.set_embedding(self.name, mean)\n\n                    token_idx += feature.token_subtoken_count[token.idx] - 1\n\n        return sentences\n\n    @property\n    @abstractmethod\n    def embedding_length(self) -> int:\n        """"""Returns the length of the embedding vector.""""""\n        return (\n            len(self.layer_indexes) * self.model.config.hidden_size\n            if not self.use_scalar_mix\n            else self.model.config.hidden_size\n        )\n\n\nclass DocumentMeanEmbeddings(DocumentEmbeddings):\n    @deprecated(\n        version=""0.3.1"",\n        reason=""The functionality of this class is moved to \'DocumentPoolEmbeddings\'"",\n    )\n    def __init__(self, token_embeddings: List[TokenEmbeddings]):\n        """"""The constructor takes a list of embeddings to be combined.""""""\n        super().__init__()\n\n        self.embeddings: StackedEmbeddings = StackedEmbeddings(\n            embeddings=token_embeddings\n        )\n        self.name: str = ""document_mean""\n\n        self.__embedding_length: int = self.embeddings.embedding_length\n\n        self.to(flair.device)\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def embed(self, sentences: Union[List[Sentence], Sentence]):\n        """"""Add embeddings to every sentence in the given list of sentences. If embeddings are already added, updates\n        only if embeddings are non-static.""""""\n\n        everything_embedded: bool = True\n\n        # if only one sentence is passed, convert to list of sentence\n        if type(sentences) is Sentence:\n            sentences = [sentences]\n\n        for sentence in sentences:\n            if self.name not in sentence._embeddings.keys():\n                everything_embedded = False\n\n        if not everything_embedded:\n\n            self.embeddings.embed(sentences)\n\n            for sentence in sentences:\n                word_embeddings = []\n                for token in sentence.tokens:\n                    word_embeddings.append(token.get_embedding().unsqueeze(0))\n\n                word_embeddings = torch.cat(word_embeddings, dim=0).to(flair.device)\n\n                mean_embedding = torch.mean(word_embeddings, 0)\n\n                sentence.set_embedding(self.name, mean_embedding)\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]):\n        pass\n\n\nclass DocumentLSTMEmbeddings(DocumentEmbeddings):\n    @deprecated(\n        version=""0.4"",\n        reason=""The functionality of this class is moved to \'DocumentRNNEmbeddings\'"",\n    )\n    def __init__(\n        self,\n        embeddings: List[TokenEmbeddings],\n        hidden_size=128,\n        rnn_layers=1,\n        reproject_words: bool = True,\n        reproject_words_dimension: int = None,\n        bidirectional: bool = False,\n        dropout: float = 0.5,\n        word_dropout: float = 0.0,\n        locked_dropout: float = 0.0,\n    ):\n        """"""The constructor takes a list of embeddings to be combined.\n        :param embeddings: a list of token embeddings\n        :param hidden_size: the number of hidden states in the lstm\n        :param rnn_layers: the number of layers for the lstm\n        :param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n        layer before putting them into the lstm or not\n        :param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n        dimension as before will be taken.\n        :param bidirectional: boolean value, indicating whether to use a bidirectional lstm or not\n        :param dropout: the dropout value to be used\n        :param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used\n        :param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used\n        """"""\n        super().__init__()\n\n        self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embeddings)\n\n        self.reproject_words = reproject_words\n        self.bidirectional = bidirectional\n\n        self.length_of_all_token_embeddings: int = self.embeddings.embedding_length\n\n        self.name = ""document_lstm""\n        self.static_embeddings = False\n\n        self.__embedding_length: int = hidden_size\n        if self.bidirectional:\n            self.__embedding_length *= 4\n\n        self.embeddings_dimension: int = self.length_of_all_token_embeddings\n        if self.reproject_words and reproject_words_dimension is not None:\n            self.embeddings_dimension = reproject_words_dimension\n\n        # bidirectional LSTM on top of embedding layer\n        self.word_reprojection_map = torch.nn.Linear(\n            self.length_of_all_token_embeddings, self.embeddings_dimension\n        )\n        self.rnn = torch.nn.GRU(\n            self.embeddings_dimension,\n            hidden_size,\n            num_layers=rnn_layers,\n            bidirectional=self.bidirectional,\n        )\n\n        # dropouts\n        if locked_dropout > 0.0:\n            self.dropout: torch.nn.Module = LockedDropout(locked_dropout)\n        else:\n            self.dropout = torch.nn.Dropout(dropout)\n\n        self.use_word_dropout: bool = word_dropout > 0.0\n        if self.use_word_dropout:\n            self.word_dropout = WordDropout(word_dropout)\n\n        torch.nn.init.xavier_uniform_(self.word_reprojection_map.weight)\n\n        self.to(flair.device)\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def embed(self, sentences: Union[List[Sentence], Sentence]):\n        """"""Add embeddings to all sentences in the given list of sentences. If embeddings are already added, update\n         only if embeddings are non-static.""""""\n\n        if type(sentences) is Sentence:\n            sentences = [sentences]\n\n        self.rnn.zero_grad()\n\n        sentences.sort(key=lambda x: len(x), reverse=True)\n\n        self.embeddings.embed(sentences)\n\n        # first, sort sentences by number of tokens\n        longest_token_sequence_in_batch: int = len(sentences[0])\n\n        all_sentence_tensors = []\n        lengths: List[int] = []\n\n        # go through each sentence in batch\n        for i, sentence in enumerate(sentences):\n\n            lengths.append(len(sentence.tokens))\n\n            word_embeddings = []\n\n            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n                word_embeddings.append(token.get_embedding().unsqueeze(0))\n\n            # PADDING: pad shorter sentences out\n            for add in range(longest_token_sequence_in_batch - len(sentence.tokens)):\n                word_embeddings.append(\n                    torch.zeros(\n                        self.length_of_all_token_embeddings, dtype=torch.float\n                    ).unsqueeze(0).to(flair.device)\n                )\n\n            word_embeddings_tensor = torch.cat(word_embeddings, 0).to(flair.device)\n\n            sentence_states = word_embeddings_tensor\n\n            # ADD TO SENTENCE LIST: add the representation\n            all_sentence_tensors.append(sentence_states.unsqueeze(1))\n\n        # --------------------------------------------------------------------\n        # GET REPRESENTATION FOR ENTIRE BATCH\n        # --------------------------------------------------------------------\n        sentence_tensor = torch.cat(all_sentence_tensors, 1)\n\n        # --------------------------------------------------------------------\n        # FF PART\n        # --------------------------------------------------------------------\n        # use word dropout if set\n        if self.use_word_dropout:\n            sentence_tensor = self.word_dropout(sentence_tensor)\n\n        if self.reproject_words:\n            sentence_tensor = self.word_reprojection_map(sentence_tensor)\n\n        sentence_tensor = self.dropout(sentence_tensor)\n\n        packed = torch.nn.utils.rnn.pack_padded_sequence(sentence_tensor, lengths)\n\n        self.rnn.flatten_parameters()\n\n        lstm_out, hidden = self.rnn(packed)\n\n        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n\n        outputs = self.dropout(outputs)\n\n        # --------------------------------------------------------------------\n        # EXTRACT EMBEDDINGS FROM LSTM\n        # --------------------------------------------------------------------\n        for sentence_no, length in enumerate(lengths):\n            last_rep = outputs[length - 1, sentence_no]\n\n            embedding = last_rep\n            if self.bidirectional:\n                first_rep = outputs[0, sentence_no]\n                embedding = torch.cat([first_rep, last_rep], 0)\n\n            sentence = sentences[sentence_no]\n            sentence.set_embedding(self.name, embedding)\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]):\n        pass\n\n\nclass ELMoTransformerEmbeddings(TokenEmbeddings):\n    """"""Contextual word embeddings using word-level Transformer-based LM, as proposed in Peters et al., 2018.""""""\n\n    @deprecated(\n        version=""0.4.2"",\n        reason=""Not possible to load or save ELMo Transformer models. @stefan-it is working on it."",\n    )\n    def __init__(self, model_file: str):\n        super().__init__()\n\n        try:\n            from allennlp.modules.token_embedders.bidirectional_language_model_token_embedder import (\n                BidirectionalLanguageModelTokenEmbedder,\n            )\n            from allennlp.data.token_indexers.elmo_indexer import (\n                ELMoTokenCharactersIndexer,\n            )\n        except ModuleNotFoundError:\n            log.warning(""-"" * 100)\n            log.warning(\'ATTENTION! The library ""allennlp"" is not installed!\')\n            log.warning(\n                ""To use ELMoTransformerEmbeddings, please first install a recent version from https://github.com/allenai/allennlp""\n            )\n            log.warning(""-"" * 100)\n            pass\n\n        self.name = ""elmo-transformer""\n        self.static_embeddings = True\n        self.lm_embedder = BidirectionalLanguageModelTokenEmbedder(\n            archive_file=model_file,\n            dropout=0.2,\n            bos_eos_tokens=(""<S>"", ""</S>""),\n            remove_bos_eos=True,\n            requires_grad=False,\n        )\n        self.lm_embedder = self.lm_embedder.to(device=flair.device)\n        self.vocab = self.lm_embedder._lm.vocab\n        self.indexer = ELMoTokenCharactersIndexer()\n\n        # embed a dummy sentence to determine embedding_length\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        # Avoid conflicts with flair\'s Token class\n        import allennlp.data.tokenizers.token as allen_nlp_token\n\n        indexer = self.indexer\n        vocab = self.vocab\n\n        for sentence in sentences:\n            character_indices = indexer.tokens_to_indices(\n                [allen_nlp_token.Token(token.text) for token in sentence], vocab, ""elmo""\n            )[""elmo""]\n\n            indices_tensor = torch.LongTensor([character_indices])\n            indices_tensor = indices_tensor.to(device=flair.device)\n            embeddings = self.lm_embedder(indices_tensor)[0].detach().cpu().numpy()\n\n            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n                embedding = embeddings[token_idx]\n                word_embedding = torch.FloatTensor(embedding)\n                token.set_embedding(self.name, word_embedding)\n\n        return sentences\n\n    def extra_repr(self):\n        return ""model={}"".format(self.name)\n\n    def __str__(self):\n        return self.name'"
flair/embeddings/token.py,54,"b'import hashlib\nfrom abc import abstractmethod\nfrom pathlib import Path\nfrom typing import List, Union\nfrom collections import Counter\nfrom functools import lru_cache\n\nimport torch\nfrom bpemb import BPEmb\nfrom transformers import XLNetTokenizer, T5Tokenizer, GPT2Tokenizer, AutoTokenizer, AutoConfig, AutoModel\n\nimport flair\nimport gensim\nimport os\nimport re\nimport logging\nimport numpy as np\n\nfrom flair.data import Sentence, Token, Corpus, Dictionary\nfrom flair.embeddings.base import Embeddings, ScalarMix\nfrom flair.file_utils import cached_path, open_inside_zip\n\nlog = logging.getLogger(""flair"")\n\n\nclass TokenEmbeddings(Embeddings):\n    """"""Abstract base class for all token-level embeddings. Ever new type of word embedding must implement these methods.""""""\n\n    @property\n    @abstractmethod\n    def embedding_length(self) -> int:\n        """"""Returns the length of the embedding vector.""""""\n        pass\n\n    @property\n    def embedding_type(self) -> str:\n        return ""word-level""\n\n\nclass StackedEmbeddings(TokenEmbeddings):\n    """"""A stack of embeddings, used if you need to combine several different embedding types.""""""\n\n    def __init__(self, embeddings: List[TokenEmbeddings]):\n        """"""The constructor takes a list of embeddings to be combined.""""""\n        super().__init__()\n\n        self.embeddings = embeddings\n\n        # IMPORTANT: add embeddings as torch modules\n        for i, embedding in enumerate(embeddings):\n            embedding.name = f""{str(i)}-{embedding.name}""\n            self.add_module(f""list_embedding_{str(i)}"", embedding)\n\n        self.name: str = ""Stack""\n        self.static_embeddings: bool = True\n\n        self.__embedding_type: str = embeddings[0].embedding_type\n\n        self.__embedding_length: int = 0\n        for embedding in embeddings:\n            self.__embedding_length += embedding.embedding_length\n\n    def embed(\n        self, sentences: Union[Sentence, List[Sentence]], static_embeddings: bool = True\n    ):\n        # if only one sentence is passed, convert to list of sentence\n        if type(sentences) is Sentence:\n            sentences = [sentences]\n\n        for embedding in self.embeddings:\n            embedding.embed(sentences)\n\n    @property\n    def embedding_type(self) -> str:\n        return self.__embedding_type\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        for embedding in self.embeddings:\n            embedding._add_embeddings_internal(sentences)\n\n        return sentences\n\n    def __str__(self):\n        return f\'StackedEmbeddings [{"","".join([str(e) for e in self.embeddings])}]\'\n\n    def get_names(self) -> List[str]:\n        """"""Returns a list of embedding names. In most cases, it is just a list with one item, namely the name of\n        this embedding. But in some cases, the embedding is made up by different embeddings (StackedEmbedding).\n        Then, the list contains the names of all embeddings in the stack.""""""\n        names = []\n        for embedding in self.embeddings:\n            names.extend(embedding.get_names())\n        return names\n\nclass WordEmbeddings(TokenEmbeddings):\n    """"""Standard static word embeddings, such as GloVe or FastText.""""""\n\n    def __init__(self, embeddings: str, field: str = None):\n        """"""\n        Initializes classic word embeddings. Constructor downloads required files if not there.\n        :param embeddings: one of: \'glove\', \'extvec\', \'crawl\' or two-letter language code or custom\n        If you want to use a custom embedding file, just pass the path to the embeddings as embeddings variable.\n        """"""\n        self.embeddings = embeddings\n\n        old_base_path = (\n            ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/""\n        )\n        base_path = (\n            ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/""\n        )\n        embeddings_path_v4 = (\n            ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/""\n        )\n        embeddings_path_v4_1 = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/""\n\n        cache_dir = Path(""embeddings"")\n\n        # GLOVE embeddings\n        if embeddings.lower() == ""glove"" or embeddings.lower() == ""en-glove"":\n            cached_path(f""{old_base_path}glove.gensim.vectors.npy"", cache_dir=cache_dir)\n            embeddings = cached_path(\n                f""{old_base_path}glove.gensim"", cache_dir=cache_dir\n            )\n\n        # TURIAN embeddings\n        elif embeddings.lower() == ""turian"" or embeddings.lower() == ""en-turian"":\n            cached_path(\n                f""{embeddings_path_v4_1}turian.vectors.npy"", cache_dir=cache_dir\n            )\n            embeddings = cached_path(\n                f""{embeddings_path_v4_1}turian"", cache_dir=cache_dir\n            )\n\n        # KOMNINOS embeddings\n        elif embeddings.lower() == ""extvec"" or embeddings.lower() == ""en-extvec"":\n            cached_path(\n                f""{old_base_path}extvec.gensim.vectors.npy"", cache_dir=cache_dir\n            )\n            embeddings = cached_path(\n                f""{old_base_path}extvec.gensim"", cache_dir=cache_dir\n            )\n\n        # FT-CRAWL embeddings\n        elif embeddings.lower() == ""crawl"" or embeddings.lower() == ""en-crawl"":\n            cached_path(\n                f""{base_path}en-fasttext-crawl-300d-1M.vectors.npy"", cache_dir=cache_dir\n            )\n            embeddings = cached_path(\n                f""{base_path}en-fasttext-crawl-300d-1M"", cache_dir=cache_dir\n            )\n\n        # FT-CRAWL embeddings\n        elif (\n            embeddings.lower() == ""news""\n            or embeddings.lower() == ""en-news""\n            or embeddings.lower() == ""en""\n        ):\n            cached_path(\n                f""{base_path}en-fasttext-news-300d-1M.vectors.npy"", cache_dir=cache_dir\n            )\n            embeddings = cached_path(\n                f""{base_path}en-fasttext-news-300d-1M"", cache_dir=cache_dir\n            )\n\n        # twitter embeddings\n        elif embeddings.lower() == ""twitter"" or embeddings.lower() == ""en-twitter"":\n            cached_path(\n                f""{old_base_path}twitter.gensim.vectors.npy"", cache_dir=cache_dir\n            )\n            embeddings = cached_path(\n                f""{old_base_path}twitter.gensim"", cache_dir=cache_dir\n            )\n\n        # two-letter language code wiki embeddings\n        elif len(embeddings.lower()) == 2:\n            cached_path(\n                f""{embeddings_path_v4}{embeddings}-wiki-fasttext-300d-1M.vectors.npy"",\n                cache_dir=cache_dir,\n            )\n            embeddings = cached_path(\n                f""{embeddings_path_v4}{embeddings}-wiki-fasttext-300d-1M"",\n                cache_dir=cache_dir,\n            )\n\n        # two-letter language code wiki embeddings\n        elif len(embeddings.lower()) == 7 and embeddings.endswith(""-wiki""):\n            cached_path(\n                f""{embeddings_path_v4}{embeddings[:2]}-wiki-fasttext-300d-1M.vectors.npy"",\n                cache_dir=cache_dir,\n            )\n            embeddings = cached_path(\n                f""{embeddings_path_v4}{embeddings[:2]}-wiki-fasttext-300d-1M"",\n                cache_dir=cache_dir,\n            )\n\n        # two-letter language code crawl embeddings\n        elif len(embeddings.lower()) == 8 and embeddings.endswith(""-crawl""):\n            cached_path(\n                f""{embeddings_path_v4}{embeddings[:2]}-crawl-fasttext-300d-1M.vectors.npy"",\n                cache_dir=cache_dir,\n            )\n            embeddings = cached_path(\n                f""{embeddings_path_v4}{embeddings[:2]}-crawl-fasttext-300d-1M"",\n                cache_dir=cache_dir,\n            )\n\n        elif not Path(embeddings).exists():\n            raise ValueError(\n                f\'The given embeddings ""{embeddings}"" is not available or is not a valid path.\'\n            )\n\n        self.name: str = str(embeddings)\n        self.static_embeddings = True\n\n        if str(embeddings).endswith("".bin""):\n            self.precomputed_word_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n                str(embeddings), binary=True\n            )\n        else:\n            self.precomputed_word_embeddings = gensim.models.KeyedVectors.load(\n                str(embeddings)\n            )\n\n        self.field = field\n\n        self.__embedding_length: int = self.precomputed_word_embeddings.vector_size\n        super().__init__()\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    @lru_cache(maxsize=10000, typed=False)\n    def get_cached_vec(self, word: str) -> torch.Tensor:\n        if word in self.precomputed_word_embeddings:\n            word_embedding = self.precomputed_word_embeddings[word]\n        elif word.lower() in self.precomputed_word_embeddings:\n            word_embedding = self.precomputed_word_embeddings[word.lower()]\n        elif re.sub(r""\\d"", ""#"", word.lower()) in self.precomputed_word_embeddings:\n            word_embedding = self.precomputed_word_embeddings[\n                re.sub(r""\\d"", ""#"", word.lower())\n            ]\n        elif re.sub(r""\\d"", ""0"", word.lower()) in self.precomputed_word_embeddings:\n            word_embedding = self.precomputed_word_embeddings[\n                re.sub(r""\\d"", ""0"", word.lower())\n            ]\n        else:\n            word_embedding = np.zeros(self.embedding_length, dtype=""float"")\n\n        word_embedding = torch.tensor(\n            word_embedding.tolist(), device=flair.device, dtype=torch.float\n        )\n        return word_embedding\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        for i, sentence in enumerate(sentences):\n\n            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n\n                if ""field"" not in self.__dict__ or self.field is None:\n                    word = token.text\n                else:\n                    word = token.get_tag(self.field).value\n\n                word_embedding = self.get_cached_vec(word=word)\n\n                token.set_embedding(self.name, word_embedding)\n\n        return sentences\n\n    def __str__(self):\n        return self.name\n\n    def extra_repr(self):\n        # fix serialized models\n        if ""embeddings"" not in self.__dict__:\n            self.embeddings = self.name\n\n        return f""\'{self.embeddings}\'""\n\nclass CharacterEmbeddings(TokenEmbeddings):\n    """"""Character embeddings of words, as proposed in Lample et al., 2016.""""""\n\n    def __init__(\n        self,\n        path_to_char_dict: str = None,\n        char_embedding_dim: int = 25,\n        hidden_size_char: int = 25,\n    ):\n        """"""Uses the default character dictionary if none provided.""""""\n\n        super().__init__()\n        self.name = ""Char""\n        self.static_embeddings = False\n\n        # use list of common characters if none provided\n        if path_to_char_dict is None:\n            self.char_dictionary: Dictionary = Dictionary.load(""common-chars"")\n        else:\n            self.char_dictionary: Dictionary = Dictionary.load_from_file(\n                path_to_char_dict\n            )\n\n        self.char_embedding_dim: int = char_embedding_dim\n        self.hidden_size_char: int = hidden_size_char\n        self.char_embedding = torch.nn.Embedding(\n            len(self.char_dictionary.item2idx), self.char_embedding_dim\n        )\n        self.char_rnn = torch.nn.LSTM(\n            self.char_embedding_dim,\n            self.hidden_size_char,\n            num_layers=1,\n            bidirectional=True,\n        )\n\n        self.__embedding_length = self.hidden_size_char * 2\n\n        self.to(flair.device)\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]):\n\n        for sentence in sentences:\n\n            tokens_char_indices = []\n\n            # translate words in sentence into ints using dictionary\n            for token in sentence.tokens:\n                char_indices = [\n                    self.char_dictionary.get_idx_for_item(char) for char in token.text\n                ]\n                tokens_char_indices.append(char_indices)\n\n            # sort words by length, for batching and masking\n            tokens_sorted_by_length = sorted(\n                tokens_char_indices, key=lambda p: len(p), reverse=True\n            )\n            d = {}\n            for i, ci in enumerate(tokens_char_indices):\n                for j, cj in enumerate(tokens_sorted_by_length):\n                    if ci == cj:\n                        d[j] = i\n                        continue\n            chars2_length = [len(c) for c in tokens_sorted_by_length]\n            longest_token_in_sentence = max(chars2_length)\n            tokens_mask = torch.zeros(\n                (len(tokens_sorted_by_length), longest_token_in_sentence),\n                dtype=torch.long,\n                device=flair.device,\n            )\n\n            for i, c in enumerate(tokens_sorted_by_length):\n                tokens_mask[i, : chars2_length[i]] = torch.tensor(\n                    c, dtype=torch.long, device=flair.device\n                )\n\n            # chars for rnn processing\n            chars = tokens_mask\n\n            character_embeddings = self.char_embedding(chars).transpose(0, 1)\n\n            packed = torch.nn.utils.rnn.pack_padded_sequence(\n                character_embeddings, chars2_length\n            )\n\n            lstm_out, self.hidden = self.char_rnn(packed)\n\n            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n            outputs = outputs.transpose(0, 1)\n            chars_embeds_temp = torch.zeros(\n                (outputs.size(0), outputs.size(2)),\n                dtype=torch.float,\n                device=flair.device,\n            )\n            for i, index in enumerate(output_lengths):\n                chars_embeds_temp[i] = outputs[i, index - 1]\n            character_embeddings = chars_embeds_temp.clone()\n            for i in range(character_embeddings.size(0)):\n                character_embeddings[d[i]] = chars_embeds_temp[i]\n\n            for token_number, token in enumerate(sentence.tokens):\n                token.set_embedding(self.name, character_embeddings[token_number])\n\n    def __str__(self):\n        return self.name\n\n\nclass FlairEmbeddings(TokenEmbeddings):\n    """"""Contextual string embeddings of words, as proposed in Akbik et al., 2018.""""""\n\n    def __init__(self,\n                 model,\n                 fine_tune: bool = False,\n                 chars_per_chunk: int = 512,\n                 with_whitespace: bool = True,\n                 tokenized_lm: bool = True,\n                 ):\n        """"""\n        initializes contextual string embeddings using a character-level language model.\n        :param model: model string, one of \'news-forward\', \'news-backward\', \'news-forward-fast\', \'news-backward-fast\',\n                \'mix-forward\', \'mix-backward\', \'german-forward\', \'german-backward\', \'polish-backward\', \'polish-forward\',\n                etc (see https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md)\n                depending on which character language model is desired.\n        :param fine_tune: if set to True, the gradient will propagate into the language model. This dramatically slows\n                down training and often leads to overfitting, so use with caution.\n        :param chars_per_chunk: max number of chars per rnn pass to control speed/memory tradeoff. Higher means faster\n                but requires more memory. Lower means slower but less memory.\n        :param with_whitespace: If True, use hidden state after whitespace after word. If False, use hidden\n                 state at last character of word.\n        :param tokenized_lm: Whether this lm is tokenized. Default is True, but for LMs trained over unprocessed text\n                False might be better.\n        """"""\n        super().__init__()\n\n        cache_dir = Path(""embeddings"")\n\n        aws_path: str = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources""\n        hu_path: str = ""https://flair.informatik.hu-berlin.de/resources""\n        clef_hipe_path: str = ""https://files.ifi.uzh.ch/cl/siclemat/impresso/clef-hipe-2020/flair""\n\n        self.PRETRAINED_MODEL_ARCHIVE_MAP = {\n            # multilingual models\n            ""multi-forward"": f""{aws_path}/embeddings-v0.4.3/lm-jw300-forward-v0.1.pt"",\n            ""multi-backward"": f""{aws_path}/embeddings-v0.4.3/lm-jw300-backward-v0.1.pt"",\n            ""multi-v0-forward"": f""{aws_path}/embeddings-v0.4/lm-multi-forward-v0.1.pt"",\n            ""multi-v0-backward"": f""{aws_path}/embeddings-v0.4/lm-multi-backward-v0.1.pt"",\n            ""multi-v0-forward-fast"": f""{aws_path}/embeddings-v0.4/lm-multi-forward-fast-v0.1.pt"",\n            ""multi-v0-backward-fast"": f""{aws_path}/embeddings-v0.4/lm-multi-backward-fast-v0.1.pt"",\n            # English models\n            ""en-forward"": f""{aws_path}/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt"",\n            ""en-backward"": f""{aws_path}/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt"",\n            ""en-forward-fast"": f""{aws_path}/embeddings/lm-news-english-forward-1024-v0.2rc.pt"",\n            ""en-backward-fast"": f""{aws_path}/embeddings/lm-news-english-backward-1024-v0.2rc.pt"",\n            ""news-forward"": f""{aws_path}/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt"",\n            ""news-backward"": f""{aws_path}/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt"",\n            ""news-forward-fast"": f""{aws_path}/embeddings/lm-news-english-forward-1024-v0.2rc.pt"",\n            ""news-backward-fast"": f""{aws_path}/embeddings/lm-news-english-backward-1024-v0.2rc.pt"",\n            ""mix-forward"": f""{aws_path}/embeddings/lm-mix-english-forward-v0.2rc.pt"",\n            ""mix-backward"": f""{aws_path}/embeddings/lm-mix-english-backward-v0.2rc.pt"",\n            # Arabic\n            ""ar-forward"": f""{aws_path}/embeddings-stefan-it/lm-ar-opus-large-forward-v0.1.pt"",\n            ""ar-backward"": f""{aws_path}/embeddings-stefan-it/lm-ar-opus-large-backward-v0.1.pt"",\n            # Bulgarian\n            ""bg-forward-fast"": f""{aws_path}/embeddings-v0.3/lm-bg-small-forward-v0.1.pt"",\n            ""bg-backward-fast"": f""{aws_path}/embeddings-v0.3/lm-bg-small-backward-v0.1.pt"",\n            ""bg-forward"": f""{aws_path}/embeddings-stefan-it/lm-bg-opus-large-forward-v0.1.pt"",\n            ""bg-backward"": f""{aws_path}/embeddings-stefan-it/lm-bg-opus-large-backward-v0.1.pt"",\n            # Czech\n            ""cs-forward"": f""{aws_path}/embeddings-stefan-it/lm-cs-opus-large-forward-v0.1.pt"",\n            ""cs-backward"": f""{aws_path}/embeddings-stefan-it/lm-cs-opus-large-backward-v0.1.pt"",\n            ""cs-v0-forward"": f""{aws_path}/embeddings-v0.4/lm-cs-large-forward-v0.1.pt"",\n            ""cs-v0-backward"": f""{aws_path}/embeddings-v0.4/lm-cs-large-backward-v0.1.pt"",\n            # Danish\n            ""da-forward"": f""{aws_path}/embeddings-stefan-it/lm-da-opus-large-forward-v0.1.pt"",\n            ""da-backward"": f""{aws_path}/embeddings-stefan-it/lm-da-opus-large-backward-v0.1.pt"",\n            # German\n            ""de-forward"": f""{aws_path}/embeddings/lm-mix-german-forward-v0.2rc.pt"",\n            ""de-backward"": f""{aws_path}/embeddings/lm-mix-german-backward-v0.2rc.pt"",\n            ""de-historic-ha-forward"": f""{aws_path}/embeddings-stefan-it/lm-historic-hamburger-anzeiger-forward-v0.1.pt"",\n            ""de-historic-ha-backward"": f""{aws_path}/embeddings-stefan-it/lm-historic-hamburger-anzeiger-backward-v0.1.pt"",\n            ""de-historic-wz-forward"": f""{aws_path}/embeddings-stefan-it/lm-historic-wiener-zeitung-forward-v0.1.pt"",\n            ""de-historic-wz-backward"": f""{aws_path}/embeddings-stefan-it/lm-historic-wiener-zeitung-backward-v0.1.pt"",\n            ""de-historic-rw-forward"": f""{hu_path}/embeddings/redewiedergabe_lm_forward.pt"",\n            ""de-historic-rw-backward"": f""{hu_path}/embeddings/redewiedergabe_lm_backward.pt"",\n            # Spanish\n            ""es-forward"": f""{aws_path}/embeddings-v0.4/language_model_es_forward_long/lm-es-forward.pt"",\n            ""es-backward"": f""{aws_path}/embeddings-v0.4/language_model_es_backward_long/lm-es-backward.pt"",\n            ""es-forward-fast"": f""{aws_path}/embeddings-v0.4/language_model_es_forward/lm-es-forward-fast.pt"",\n            ""es-backward-fast"": f""{aws_path}/embeddings-v0.4/language_model_es_backward/lm-es-backward-fast.pt"",\n            # Basque\n            ""eu-forward"": f""{aws_path}/embeddings-stefan-it/lm-eu-opus-large-forward-v0.2.pt"",\n            ""eu-backward"": f""{aws_path}/embeddings-stefan-it/lm-eu-opus-large-backward-v0.2.pt"",\n            ""eu-v1-forward"": f""{aws_path}/embeddings-stefan-it/lm-eu-opus-large-forward-v0.1.pt"",\n            ""eu-v1-backward"": f""{aws_path}/embeddings-stefan-it/lm-eu-opus-large-backward-v0.1.pt"",\n            ""eu-v0-forward"": f""{aws_path}/embeddings-v0.4/lm-eu-large-forward-v0.1.pt"",\n            ""eu-v0-backward"": f""{aws_path}/embeddings-v0.4/lm-eu-large-backward-v0.1.pt"",\n            # Persian\n            ""fa-forward"": f""{aws_path}/embeddings-stefan-it/lm-fa-opus-large-forward-v0.1.pt"",\n            ""fa-backward"": f""{aws_path}/embeddings-stefan-it/lm-fa-opus-large-backward-v0.1.pt"",\n            # Finnish\n            ""fi-forward"": f""{aws_path}/embeddings-stefan-it/lm-fi-opus-large-forward-v0.1.pt"",\n            ""fi-backward"": f""{aws_path}/embeddings-stefan-it/lm-fi-opus-large-backward-v0.1.pt"",\n            # French\n            ""fr-forward"": f""{aws_path}/embeddings/lm-fr-charlm-forward.pt"",\n            ""fr-backward"": f""{aws_path}/embeddings/lm-fr-charlm-backward.pt"",\n            # Hebrew\n            ""he-forward"": f""{aws_path}/embeddings-stefan-it/lm-he-opus-large-forward-v0.1.pt"",\n            ""he-backward"": f""{aws_path}/embeddings-stefan-it/lm-he-opus-large-backward-v0.1.pt"",\n            # Hindi\n            ""hi-forward"": f""{aws_path}/embeddings-stefan-it/lm-hi-opus-large-forward-v0.1.pt"",\n            ""hi-backward"": f""{aws_path}/embeddings-stefan-it/lm-hi-opus-large-backward-v0.1.pt"",\n            # Croatian\n            ""hr-forward"": f""{aws_path}/embeddings-stefan-it/lm-hr-opus-large-forward-v0.1.pt"",\n            ""hr-backward"": f""{aws_path}/embeddings-stefan-it/lm-hr-opus-large-backward-v0.1.pt"",\n            # Indonesian\n            ""id-forward"": f""{aws_path}/embeddings-stefan-it/lm-id-opus-large-forward-v0.1.pt"",\n            ""id-backward"": f""{aws_path}/embeddings-stefan-it/lm-id-opus-large-backward-v0.1.pt"",\n            # Italian\n            ""it-forward"": f""{aws_path}/embeddings-stefan-it/lm-it-opus-large-forward-v0.1.pt"",\n            ""it-backward"": f""{aws_path}/embeddings-stefan-it/lm-it-opus-large-backward-v0.1.pt"",\n            # Japanese\n            ""ja-forward"": f""{aws_path}/embeddings-v0.4.1/lm__char-forward__ja-wikipedia-3GB/japanese-forward.pt"",\n            ""ja-backward"": f""{aws_path}/embeddings-v0.4.1/lm__char-backward__ja-wikipedia-3GB/japanese-backward.pt"",\n            # Malayalam\n            ""ml-forward"": f""https://raw.githubusercontent.com/qburst/models-repository/master/FlairMalayalamModels/ml-forward.pt"",\n            ""ml-backward"": f""https://raw.githubusercontent.com/qburst/models-repository/master/FlairMalayalamModels/ml-backward.pt"",\n            # Dutch\n            ""nl-forward"": f""{aws_path}/embeddings-stefan-it/lm-nl-opus-large-forward-v0.1.pt"",\n            ""nl-backward"": f""{aws_path}/embeddings-stefan-it/lm-nl-opus-large-backward-v0.1.pt"",\n            ""nl-v0-forward"": f""{aws_path}/embeddings-v0.4/lm-nl-large-forward-v0.1.pt"",\n            ""nl-v0-backward"": f""{aws_path}/embeddings-v0.4/lm-nl-large-backward-v0.1.pt"",\n            # Norwegian\n            ""no-forward"": f""{aws_path}/embeddings-stefan-it/lm-no-opus-large-forward-v0.1.pt"",\n            ""no-backward"": f""{aws_path}/embeddings-stefan-it/lm-no-opus-large-backward-v0.1.pt"",\n            # Polish\n            ""pl-forward"": f""{aws_path}/embeddings/lm-polish-forward-v0.2.pt"",\n            ""pl-backward"": f""{aws_path}/embeddings/lm-polish-backward-v0.2.pt"",\n            ""pl-opus-forward"": f""{aws_path}/embeddings-stefan-it/lm-pl-opus-large-forward-v0.1.pt"",\n            ""pl-opus-backward"": f""{aws_path}/embeddings-stefan-it/lm-pl-opus-large-backward-v0.1.pt"",\n            # Portuguese\n            ""pt-forward"": f""{aws_path}/embeddings-v0.4/lm-pt-forward.pt"",\n            ""pt-backward"": f""{aws_path}/embeddings-v0.4/lm-pt-backward.pt"",\n            # Pubmed\n            ""pubmed-forward"": f""{aws_path}/embeddings-v0.4.1/pubmed-2015-fw-lm.pt"",\n            ""pubmed-backward"": f""{aws_path}/embeddings-v0.4.1/pubmed-2015-bw-lm.pt"",\n            # Slovenian\n            ""sl-forward"": f""{aws_path}/embeddings-stefan-it/lm-sl-opus-large-forward-v0.1.pt"",\n            ""sl-backward"": f""{aws_path}/embeddings-stefan-it/lm-sl-opus-large-backward-v0.1.pt"",\n            ""sl-v0-forward"": f""{aws_path}/embeddings-v0.3/lm-sl-large-forward-v0.1.pt"",\n            ""sl-v0-backward"": f""{aws_path}/embeddings-v0.3/lm-sl-large-backward-v0.1.pt"",\n            # Swedish\n            ""sv-forward"": f""{aws_path}/embeddings-stefan-it/lm-sv-opus-large-forward-v0.1.pt"",\n            ""sv-backward"": f""{aws_path}/embeddings-stefan-it/lm-sv-opus-large-backward-v0.1.pt"",\n            ""sv-v0-forward"": f""{aws_path}/embeddings-v0.4/lm-sv-large-forward-v0.1.pt"",\n            ""sv-v0-backward"": f""{aws_path}/embeddings-v0.4/lm-sv-large-backward-v0.1.pt"",\n            # Tamil\n            ""ta-forward"": f""{aws_path}/embeddings-stefan-it/lm-ta-opus-large-forward-v0.1.pt"",\n            ""ta-backward"": f""{aws_path}/embeddings-stefan-it/lm-ta-opus-large-backward-v0.1.pt"",\n            # CLEF HIPE Shared task\n            ""de-impresso-hipe-v1-forward"": f""{clef_hipe_path}/de-hipe-flair-v1-forward/best-lm.pt"",\n            ""de-impresso-hipe-v1-backward"": f""{clef_hipe_path}/de-hipe-flair-v1-backward/best-lm.pt"",\n            ""en-impresso-hipe-v1-forward"": f""{clef_hipe_path}/en-flair-v1-forward/best-lm.pt"",\n            ""en-impresso-hipe-v1-backward"": f""{clef_hipe_path}/en-flair-v1-backward/best-lm.pt"",\n            ""fr-impresso-hipe-v1-forward"": f""{clef_hipe_path}/fr-hipe-flair-v1-forward/best-lm.pt"",\n            ""fr-impresso-hipe-v1-backward"": f""{clef_hipe_path}/fr-hipe-flair-v1-backward/best-lm.pt"",\n        }\n\n        if type(model) == str:\n\n            # load model if in pretrained model map\n            if model.lower() in self.PRETRAINED_MODEL_ARCHIVE_MAP:\n                base_path = self.PRETRAINED_MODEL_ARCHIVE_MAP[model.lower()]\n\n                # Fix for CLEF HIPE models (avoid overwriting best-lm.pt in cache_dir)\n                if ""impresso-hipe"" in model.lower():\n                    cache_dir = cache_dir / model.lower()\n                model = cached_path(base_path, cache_dir=cache_dir)\n\n            elif replace_with_language_code(model) in self.PRETRAINED_MODEL_ARCHIVE_MAP:\n                base_path = self.PRETRAINED_MODEL_ARCHIVE_MAP[\n                    replace_with_language_code(model)\n                ]\n                model = cached_path(base_path, cache_dir=cache_dir)\n\n            elif not Path(model).exists():\n                raise ValueError(\n                    f\'The given model ""{model}"" is not available or is not a valid path.\'\n                )\n\n        from flair.models import LanguageModel\n\n        if type(model) == LanguageModel:\n            self.lm: LanguageModel = model\n            self.name = f""Task-LSTM-{self.lm.hidden_size}-{self.lm.nlayers}-{self.lm.is_forward_lm}""\n        else:\n            self.lm: LanguageModel = LanguageModel.load_language_model(model)\n            self.name = str(model)\n\n        # embeddings are static if we don\'t do finetuning\n        self.fine_tune = fine_tune\n        self.static_embeddings = not fine_tune\n\n        self.is_forward_lm: bool = self.lm.is_forward_lm\n        self.with_whitespace: bool = with_whitespace\n        self.tokenized_lm: bool = tokenized_lm\n        self.chars_per_chunk: int = chars_per_chunk\n\n        # embed a dummy sentence to determine embedding_length\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n        # set to eval mode\n        self.eval()\n\n    def train(self, mode=True):\n\n        # make compatible with serialized models (TODO: remove)\n        if ""fine_tune"" not in self.__dict__:\n            self.fine_tune = False\n        if ""chars_per_chunk"" not in self.__dict__:\n            self.chars_per_chunk = 512\n\n        if not self.fine_tune:\n            pass\n        else:\n            super(FlairEmbeddings, self).train(mode)\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        # make compatible with serialized models (TODO: remove)\n        if ""with_whitespace"" not in self.__dict__:\n            self.with_whitespace = True\n        if ""tokenized_lm"" not in self.__dict__:\n            self.tokenized_lm = True\n\n        # gradients are enable if fine-tuning is enabled\n        gradient_context = torch.enable_grad() if self.fine_tune else torch.no_grad()\n\n        with gradient_context:\n\n            # if this is not possible, use LM to generate embedding. First, get text sentences\n            text_sentences = [sentence.to_tokenized_string() for sentence in sentences] if self.tokenized_lm \\\n                else [sentence.to_plain_string() for sentence in sentences]\n\n            start_marker = self.lm.document_delimiter if ""document_delimiter"" in self.lm.__dict__ else \'\\n\'\n            end_marker = "" ""\n\n            # get hidden states from language model\n            all_hidden_states_in_lm = self.lm.get_representation(\n                text_sentences, start_marker, end_marker, self.chars_per_chunk\n            )\n\n            if not self.fine_tune:\n                all_hidden_states_in_lm = all_hidden_states_in_lm.detach()\n\n            # take first or last hidden states from language model as word representation\n            for i, sentence in enumerate(sentences):\n                sentence_text = sentence.to_tokenized_string() if self.tokenized_lm else sentence.to_plain_string()\n\n                offset_forward: int = len(start_marker)\n                offset_backward: int = len(sentence_text) + len(start_marker)\n\n                for token in sentence.tokens:\n\n                    offset_forward += len(token.text)\n                    if self.is_forward_lm:\n                        offset_with_whitespace = offset_forward\n                        offset_without_whitespace = offset_forward - 1\n                    else:\n                        offset_with_whitespace = offset_backward\n                        offset_without_whitespace = offset_backward - 1\n\n                    # offset mode that extracts at whitespace after last character\n                    if self.with_whitespace:\n                        embedding = all_hidden_states_in_lm[offset_with_whitespace, i, :]\n                    # offset mode that extracts at last character\n                    else:\n                        embedding = all_hidden_states_in_lm[offset_without_whitespace, i, :]\n\n                    if self.tokenized_lm or token.whitespace_after:\n                        offset_forward += 1\n                        offset_backward -= 1\n\n                    offset_backward -= len(token.text)\n\n                    # only clone if optimization mode is \'gpu\'\n                    if flair.embedding_storage_mode == ""gpu"":\n                        embedding = embedding.clone()\n\n                    token.set_embedding(self.name, embedding)\n\n            del all_hidden_states_in_lm\n\n        return sentences\n\n    def __str__(self):\n        return self.name\n\n\nclass PooledFlairEmbeddings(TokenEmbeddings):\n    def __init__(\n        self,\n        contextual_embeddings: Union[str, FlairEmbeddings],\n        pooling: str = ""min"",\n        only_capitalized: bool = False,\n        **kwargs,\n    ):\n\n        super().__init__()\n\n        # use the character language model embeddings as basis\n        if type(contextual_embeddings) is str:\n            self.context_embeddings: FlairEmbeddings = FlairEmbeddings(\n                contextual_embeddings, **kwargs\n            )\n        else:\n            self.context_embeddings: FlairEmbeddings = contextual_embeddings\n\n        # length is twice the original character LM embedding length\n        self.embedding_length = self.context_embeddings.embedding_length * 2\n        self.name = self.context_embeddings.name + ""-context""\n\n        # these fields are for the embedding memory\n        self.word_embeddings = {}\n        self.word_count = {}\n\n        # whether to add only capitalized words to memory (faster runtime and lower memory consumption)\n        self.only_capitalized = only_capitalized\n\n        # we re-compute embeddings dynamically at each epoch\n        self.static_embeddings = False\n\n        # set the memory method\n        self.pooling = pooling\n        if pooling == ""mean"":\n            self.aggregate_op = torch.add\n        elif pooling == ""fade"":\n            self.aggregate_op = torch.add\n        elif pooling == ""max"":\n            self.aggregate_op = torch.max\n        elif pooling == ""min"":\n            self.aggregate_op = torch.min\n\n    def train(self, mode=True):\n        super().train(mode=mode)\n        if mode:\n            # memory is wiped each time we do a training run\n            print(""train mode resetting embeddings"")\n            self.word_embeddings = {}\n            self.word_count = {}\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        self.context_embeddings.embed(sentences)\n\n        # if we keep a pooling, it needs to be updated continuously\n        for sentence in sentences:\n            for token in sentence.tokens:\n\n                # update embedding\n                local_embedding = token._embeddings[self.context_embeddings.name].cpu()\n\n                # check token.text is empty or not\n                if token.text:\n                    if token.text[0].isupper() or not self.only_capitalized:\n\n                        if token.text not in self.word_embeddings:\n                            self.word_embeddings[token.text] = local_embedding\n                            self.word_count[token.text] = 1\n                        else:\n                            aggregated_embedding = self.aggregate_op(\n                                self.word_embeddings[token.text], local_embedding\n                            )\n                            if self.pooling == ""fade"":\n                                aggregated_embedding /= 2\n                            self.word_embeddings[token.text] = aggregated_embedding\n                            self.word_count[token.text] += 1\n\n        # add embeddings after updating\n        for sentence in sentences:\n            for token in sentence.tokens:\n                if token.text in self.word_embeddings:\n                    base = (\n                        self.word_embeddings[token.text] / self.word_count[token.text]\n                        if self.pooling == ""mean""\n                        else self.word_embeddings[token.text]\n                    )\n                else:\n                    base = token._embeddings[self.context_embeddings.name]\n\n                token.set_embedding(self.name, base)\n\n        return sentences\n\n    def embedding_length(self) -> int:\n        return self.embedding_length\n\n    def get_names(self) -> List[str]:\n        return [self.name, self.context_embeddings.name]\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n\n        if flair.device != \'cpu\':\n            for key in self.word_embeddings:\n                self.word_embeddings[key] = self.word_embeddings[key].cpu()\n\n\nclass TransformerWordEmbeddings(TokenEmbeddings):\n    def __init__(\n        self,\n        model: str = ""bert-base-uncased"",\n        layers: str = ""-1,-2,-3,-4"",\n        pooling_operation: str = ""first"",\n        batch_size: int = 1,\n        use_scalar_mix: bool = False,\n        fine_tune: bool = False\n    ):\n        """"""\n        Bidirectional transformer embeddings of words from various transformer architectures.\n        :param model: name of transformer model (see https://huggingface.co/transformers/pretrained_models.html for\n        options)\n        :param layers: string indicating which layers to take for embedding (-1 is topmost layer)\n        :param pooling_operation: how to get from token piece embeddings to token embedding. Either take the first\n        subtoken (\'first\'), the last subtoken (\'last\'), both first and last (\'first_last\') or a mean over all (\'mean\')\n        :param batch_size: How many sentence to push through transformer at once. Set to 1 by default since transformer\n        models tend to be huge.\n        :param use_scalar_mix: If True, uses a scalar mix of layers as embedding\n        :param fine_tune: If True, allows transformers to be fine-tuned during training\n        """"""\n        super().__init__()\n\n        # load tokenizer and transformer model\n        self.tokenizer = AutoTokenizer.from_pretrained(model)\n        config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n        self.model = AutoModel.from_pretrained(model, config=config)\n\n        # model name\n        self.name = \'transformer-word-\' + str(model)\n\n        # when initializing, embeddings are in eval mode by default\n        self.model.eval()\n        self.model.to(flair.device)\n\n        # embedding parameters\n        if layers == \'all\':\n            # send mini-token through to check how many layers the model has\n            hidden_states = self.model(torch.tensor([1], device=flair.device).unsqueeze(0))[-1]\n            self.layer_indexes = [int(x) for x in range(len(hidden_states))]\n        else:\n            self.layer_indexes = [int(x) for x in layers.split("","")]\n        # self.mix = ScalarMix(mixture_size=len(self.layer_indexes), trainable=False)\n        self.pooling_operation = pooling_operation\n        self.use_scalar_mix = use_scalar_mix\n        self.fine_tune = fine_tune\n        self.static_embeddings = not self.fine_tune\n        self.batch_size = batch_size\n\n        self.special_tokens = []\n        # check if special tokens exist to circumvent error message\n        if self.tokenizer._bos_token:\n            self.special_tokens.append(self.tokenizer.bos_token)\n        if self.tokenizer._cls_token:\n            self.special_tokens.append(self.tokenizer.cls_token)\n\n        # most models have an intial BOS token, except for XLNet, T5 and GPT2\n        self.begin_offset = 1\n        if type(self.tokenizer) == XLNetTokenizer:\n            self.begin_offset = 0\n        if type(self.tokenizer) == T5Tokenizer:\n            self.begin_offset = 0\n        if type(self.tokenizer) == GPT2Tokenizer:\n            self.begin_offset = 0\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        """"""Add embeddings to all words in a list of sentences.""""""\n\n        # split into micro batches of size self.batch_size before pushing through transformer\n        sentence_batches = [sentences[i * self.batch_size:(i + 1) * self.batch_size]\n                            for i in range((len(sentences) + self.batch_size - 1) // self.batch_size)]\n\n        # embed each micro-batch\n        for batch in sentence_batches:\n            self._add_embeddings_to_sentences(batch)\n\n        return sentences\n\n    @staticmethod\n    def _remove_special_markup(text: str):\n        # remove special markup\n        text = re.sub(\'^\xc4\xa0\', \'\', text)  # RoBERTa models\n        text = re.sub(\'^##\', \'\', text)  # BERT models\n        text = re.sub(\'^\xe2\x96\x81\', \'\', text)  # XLNet models\n        text = re.sub(\'</w>$\', \'\', text)  # XLM models\n        return text\n\n    def _get_processed_token_text(self, token: Token) -> str:\n        pieces = self.tokenizer.convert_ids_to_tokens(\n            self.tokenizer.encode(token.text, add_special_tokens=False))\n        token_text = \'\'\n        for piece in pieces:\n            token_text += self._remove_special_markup(piece)\n        token_text = token_text.lower()\n        return token_text\n\n    def _add_embeddings_to_sentences(self, sentences: List[Sentence]):\n        """"""Match subtokenization to Flair tokenization and extract embeddings from transformers for each token.""""""\n\n        # first, subtokenize each sentence and find out into how many subtokens each token was divided\n        subtokenized_sentences = []\n        subtokenized_sentences_token_lengths = []\n\n        for sentence in sentences:\n\n            tokenized_string = sentence.to_tokenized_string()\n\n            # method 1: subtokenize sentence\n            # subtokenized_sentence = self.tokenizer.encode(tokenized_string, add_special_tokens=True)\n\n            # method 2:\n            ids = self.tokenizer.encode(tokenized_string, add_special_tokens=False)\n            subtokenized_sentence = self.tokenizer.build_inputs_with_special_tokens(ids)\n\n            subtokenized_sentences.append(torch.tensor(subtokenized_sentence, dtype=torch.long))\n            subtokens = self.tokenizer.convert_ids_to_tokens(subtokenized_sentence)\n\n            word_iterator = iter(sentence)\n            token = next(word_iterator)\n            token_text = self._get_processed_token_text(token)\n\n            token_subtoken_lengths = []\n            reconstructed_token = \'\'\n            subtoken_count = 0\n\n            # iterate over subtokens and reconstruct tokens\n            for subtoken_id, subtoken in enumerate(subtokens):\n\n                # remove special markup\n                subtoken = self._remove_special_markup(subtoken)\n\n                # check if subtoken is special begin token ([CLS] or similar)\n                if subtoken in self.special_tokens and subtoken_id == 0:\n                    continue\n\n                # some BERT tokenizers somehow omit words - in such cases skip to next token\n                if subtoken_count == 0 and not token_text.startswith(subtoken.lower()):\n                    token_subtoken_lengths.append(0)\n                    token = next(word_iterator)\n                    token_text = self._get_processed_token_text(token)\n\n                subtoken_count += 1\n\n                # append subtoken to reconstruct token\n                reconstructed_token = reconstructed_token + subtoken\n\n                # check if reconstructed token is the same as current token\n                if reconstructed_token.lower() == token_text:\n\n                    # if so, add subtoken count\n                    token_subtoken_lengths.append(subtoken_count)\n\n                    # reset subtoken count and reconstructed token\n                    reconstructed_token = \'\'\n                    subtoken_count = 0\n\n                    # get next token\n                    if len(token_subtoken_lengths) < len(sentence):\n                        token = next(word_iterator)\n                        token_text = self._get_processed_token_text(token)\n                    # break from loop if all tokens are accounted for\n                    else:\n                        break\n\n            # check if all tokens were matched to subtokens\n            if token != sentence[-1]:\n                log.error(f""Tokenization MISMATCH in sentence \'{sentence.to_tokenized_string()}\'"")\n                log.error(f""Last matched: \'{token}\'"")\n                log.error(f""Last sentence: \'{sentence[-1]}\'"")\n                log.error(f""subtokenized: \'{subtokens}\'"")\n\n            subtokenized_sentences_token_lengths.append(token_subtoken_lengths)\n\n        # find longest sentence in batch\n        longest_sequence_in_batch: int = len(max(subtokenized_sentences, key=len))\n\n        # initialize batch tensors and mask\n        input_ids = torch.zeros(\n            [len(sentences), longest_sequence_in_batch],\n            dtype=torch.long,\n            device=flair.device,\n        )\n        mask = torch.zeros(\n            [len(sentences), longest_sequence_in_batch],\n            dtype=torch.long,\n            device=flair.device,\n        )\n        for s_id, sentence in enumerate(subtokenized_sentences):\n            sequence_length = len(sentence)\n            input_ids[s_id][:sequence_length] = sentence\n            mask[s_id][:sequence_length] = torch.ones(sequence_length)\n\n        # put encoded batch through transformer model to get all hidden states of all encoder layers\n        hidden_states = self.model(input_ids, attention_mask=mask)[-1]\n\n        # gradients are enabled if fine-tuning is enabled\n        gradient_context = torch.enable_grad() if (self.fine_tune and self.training) else torch.no_grad()\n\n        with gradient_context:\n\n            # iterate over all subtokenized sentences\n            for sentence_idx, (sentence, subtoken_lengths) in enumerate(zip(sentences, subtokenized_sentences_token_lengths)):\n\n                subword_start_idx = self.begin_offset\n\n                # for each token, get embedding\n                for token_idx, (token, number_of_subtokens) in enumerate(zip(sentence, subtoken_lengths)):\n\n                    # some tokens have no subtokens at all (if omitted by BERT tokenizer) so return zero vector\n                    if number_of_subtokens == 0:\n                        token.set_embedding(self.name, torch.zeros(self.embedding_length))\n                        continue\n\n                    subword_end_idx = subword_start_idx + number_of_subtokens\n\n                    subtoken_embeddings: List[torch.FloatTensor] = []\n\n                    # get states from all selected layers, aggregate with pooling operation\n                    for layer in self.layer_indexes:\n                        current_embeddings = hidden_states[layer][sentence_idx][subword_start_idx:subword_end_idx]\n\n                        if self.pooling_operation == ""first"":\n                            final_embedding: torch.FloatTensor = current_embeddings[0]\n\n                        if self.pooling_operation == ""last"":\n                            final_embedding: torch.FloatTensor = current_embeddings[-1]\n\n                        if self.pooling_operation == ""first_last"":\n                            final_embedding: torch.Tensor = torch.cat([current_embeddings[0], current_embeddings[-1]])\n\n                        if self.pooling_operation == ""mean"":\n                            all_embeddings: List[torch.FloatTensor] = [\n                                embedding.unsqueeze(0) for embedding in current_embeddings\n                            ]\n                            final_embedding: torch.Tensor = torch.mean(torch.cat(all_embeddings, dim=0), dim=0)\n\n                        subtoken_embeddings.append(final_embedding)\n\n                    # use scalar mix of embeddings if so selected\n                    if self.use_scalar_mix:\n                        sm_embeddings = torch.mean(torch.stack(subtoken_embeddings, dim=1), dim=1)\n                        # sm_embeddings = self.mix(subtoken_embeddings)\n\n                        subtoken_embeddings = [sm_embeddings]\n\n                    # set the extracted embedding for the token\n                    token.set_embedding(self.name, torch.cat(subtoken_embeddings))\n\n                    subword_start_idx += number_of_subtokens\n\n    def train(self, mode=True):\n        # if fine-tuning is not enabled (i.e. a ""feature-based approach"" used), this\n        # module should never be in training mode\n        if not self.fine_tune:\n            pass\n        else:\n            super().train(mode)\n\n    @property\n    @abstractmethod\n    def embedding_length(self) -> int:\n        """"""Returns the length of the embedding vector.""""""\n\n        if not self.use_scalar_mix:\n            length = len(self.layer_indexes) * self.model.config.hidden_size\n        else:\n            length = self.model.config.hidden_size\n\n        if self.pooling_operation == \'first_last\': length *= 2\n\n        return length\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n\n        # reload tokenizer to get around serialization issues\n        model_name = self.name.split(\'transformer-word-\')[-1]\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\nclass FastTextEmbeddings(TokenEmbeddings):\n    """"""FastText Embeddings with oov functionality""""""\n\n    def __init__(self, embeddings: str, use_local: bool = True, field: str = None):\n        """"""\n        Initializes fasttext word embeddings. Constructor downloads required embedding file and stores in cache\n        if use_local is False.\n\n        :param embeddings: path to your embeddings \'.bin\' file\n        :param use_local: set this to False if you are using embeddings from a remote source\n        """"""\n\n        cache_dir = Path(""embeddings"")\n\n        if use_local:\n            if not Path(embeddings).exists():\n                raise ValueError(\n                    f\'The given embeddings ""{embeddings}"" is not available or is not a valid path.\'\n                )\n        else:\n            embeddings = cached_path(f""{embeddings}"", cache_dir=cache_dir)\n\n        self.embeddings = embeddings\n\n        self.name: str = str(embeddings)\n\n        self.static_embeddings = True\n\n        self.precomputed_word_embeddings = gensim.models.FastText.load_fasttext_format(\n            str(embeddings)\n        )\n\n        self.__embedding_length: int = self.precomputed_word_embeddings.vector_size\n\n        self.field = field\n        super().__init__()\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    @lru_cache(maxsize=10000, typed=False)\n    def get_cached_vec(self, word: str) -> torch.Tensor:\n        try:\n            word_embedding = self.precomputed_word_embeddings[word]\n        except:\n            word_embedding = np.zeros(self.embedding_length, dtype=""float"")\n\n        word_embedding = torch.tensor(\n            word_embedding.tolist(), device=flair.device, dtype=torch.float\n        )\n        return word_embedding\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        for i, sentence in enumerate(sentences):\n\n            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n\n                if ""field"" not in self.__dict__ or self.field is None:\n                    word = token.text\n                else:\n                    word = token.get_tag(self.field).value\n\n                word_embedding = self.get_cached_vec(word)\n\n                token.set_embedding(self.name, word_embedding)\n\n        return sentences\n\n    def __str__(self):\n        return self.name\n\n    def extra_repr(self):\n        return f""\'{self.embeddings}\'""\n\n\nclass OneHotEmbeddings(TokenEmbeddings):\n    """"""One-hot encoded embeddings. """"""\n\n    def __init__(\n        self,\n        corpus: Corpus,\n        field: str = ""text"",\n        embedding_length: int = 300,\n        min_freq: int = 3,\n    ):\n        """"""\n        Initializes one-hot encoded word embeddings and a trainable embedding layer\n        :param corpus: you need to pass a Corpus in order to construct the vocabulary\n        :param field: by default, the \'text\' of tokens is embedded, but you can also embed tags such as \'pos\'\n        :param embedding_length: dimensionality of the trainable embedding layer\n        :param min_freq: minimum frequency of a word to become part of the vocabulary\n        """"""\n        super().__init__()\n        self.name = ""one-hot""\n        self.static_embeddings = False\n        self.min_freq = min_freq\n        self.field = field\n\n        tokens = list(map((lambda s: s.tokens), corpus.train))\n        tokens = [token for sublist in tokens for token in sublist]\n\n        if field == ""text"":\n            most_common = Counter(list(map((lambda t: t.text), tokens))).most_common()\n        else:\n            most_common = Counter(\n                list(map((lambda t: t.get_tag(field).value), tokens))\n            ).most_common()\n\n        tokens = []\n        for token, freq in most_common:\n            if freq < min_freq:\n                break\n            tokens.append(token)\n\n        self.vocab_dictionary: Dictionary = Dictionary()\n        for token in tokens:\n            self.vocab_dictionary.add_item(token)\n\n        # max_tokens = 500\n        self.__embedding_length = embedding_length\n\n        print(self.vocab_dictionary.idx2item)\n        print(f""vocabulary size of {len(self.vocab_dictionary)}"")\n\n        # model architecture\n        self.embedding_layer = torch.nn.Embedding(\n            len(self.vocab_dictionary), self.__embedding_length\n        )\n        torch.nn.init.xavier_uniform_(self.embedding_layer.weight)\n\n        self.to(flair.device)\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        one_hot_sentences = []\n        for i, sentence in enumerate(sentences):\n\n            if self.field == ""text"":\n                context_idxs = [\n                    self.vocab_dictionary.get_idx_for_item(t.text)\n                    for t in sentence.tokens\n                ]\n            else:\n                context_idxs = [\n                    self.vocab_dictionary.get_idx_for_item(t.get_tag(self.field).value)\n                    for t in sentence.tokens\n                ]\n\n            one_hot_sentences.extend(context_idxs)\n\n        one_hot_sentences = torch.tensor(one_hot_sentences, dtype=torch.long).to(\n            flair.device\n        )\n\n        embedded = self.embedding_layer.forward(one_hot_sentences)\n\n        index = 0\n        for sentence in sentences:\n            for token in sentence:\n                embedding = embedded[index]\n                token.set_embedding(self.name, embedding)\n                index += 1\n\n        return sentences\n\n    def __str__(self):\n        return self.name\n\n    def extra_repr(self):\n        return ""min_freq={}"".format(self.min_freq)\n\n\nclass HashEmbeddings(TokenEmbeddings):\n    """"""Standard embeddings with Hashing Trick.""""""\n\n    def __init__(\n        self, num_embeddings: int = 1000, embedding_length: int = 300, hash_method=""md5""\n    ):\n\n        super().__init__()\n        self.name = ""hash""\n        self.static_embeddings = False\n\n        self.__num_embeddings = num_embeddings\n        self.__embedding_length = embedding_length\n\n        self.__hash_method = hash_method\n\n        # model architecture\n        self.embedding_layer = torch.nn.Embedding(\n            self.__num_embeddings, self.__embedding_length\n        )\n        torch.nn.init.xavier_uniform_(self.embedding_layer.weight)\n\n        self.to(flair.device)\n\n    @property\n    def num_embeddings(self) -> int:\n        return self.__num_embeddings\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n        def get_idx_for_item(text):\n            hash_function = hashlib.new(self.__hash_method)\n            hash_function.update(bytes(str(text), ""utf-8""))\n            return int(hash_function.hexdigest(), 16) % self.__num_embeddings\n\n        hash_sentences = []\n        for i, sentence in enumerate(sentences):\n            context_idxs = [get_idx_for_item(t.text) for t in sentence.tokens]\n\n            hash_sentences.extend(context_idxs)\n\n        hash_sentences = torch.tensor(hash_sentences, dtype=torch.long).to(flair.device)\n\n        embedded = self.embedding_layer.forward(hash_sentences)\n\n        index = 0\n        for sentence in sentences:\n            for token in sentence:\n                embedding = embedded[index]\n                token.set_embedding(self.name, embedding)\n                index += 1\n\n        return sentences\n\n    def __str__(self):\n        return self.name\n\n\nclass MuseCrosslingualEmbeddings(TokenEmbeddings):\n    def __init__(self,):\n        self.name: str = f""muse-crosslingual""\n        self.static_embeddings = True\n        self.__embedding_length: int = 300\n        self.language_embeddings = {}\n        super().__init__()\n\n    @lru_cache(maxsize=10000, typed=False)\n    def get_cached_vec(self, language_code: str, word: str) -> torch.Tensor:\n        current_embedding_model = self.language_embeddings[language_code]\n        if word in current_embedding_model:\n            word_embedding = current_embedding_model[word]\n        elif word.lower() in current_embedding_model:\n            word_embedding = current_embedding_model[word.lower()]\n        elif re.sub(r""\\d"", ""#"", word.lower()) in current_embedding_model:\n            word_embedding = current_embedding_model[re.sub(r""\\d"", ""#"", word.lower())]\n        elif re.sub(r""\\d"", ""0"", word.lower()) in current_embedding_model:\n            word_embedding = current_embedding_model[re.sub(r""\\d"", ""0"", word.lower())]\n        else:\n            word_embedding = np.zeros(self.embedding_length, dtype=""float"")\n        word_embedding = torch.tensor(\n            word_embedding, device=flair.device, dtype=torch.float\n        )\n        return word_embedding\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        for i, sentence in enumerate(sentences):\n\n            language_code = sentence.get_language_code()\n            supported = [\n                ""en"",\n                ""de"",\n                ""bg"",\n                ""ca"",\n                ""hr"",\n                ""cs"",\n                ""da"",\n                ""nl"",\n                ""et"",\n                ""fi"",\n                ""fr"",\n                ""el"",\n                ""he"",\n                ""hu"",\n                ""id"",\n                ""it"",\n                ""mk"",\n                ""no"",\n                ""pl"",\n                ""pt"",\n                ""ro"",\n                ""ru"",\n                ""sk"",\n            ]\n            if language_code not in supported:\n                language_code = ""en""\n\n            if language_code not in self.language_embeddings:\n                log.info(f""Loading up MUSE embeddings for \'{language_code}\'!"")\n                # download if necessary\n                webpath = ""https://alan-nlp.s3.eu-central-1.amazonaws.com/resources/embeddings-muse""\n                cache_dir = Path(""embeddings"") / ""MUSE""\n                cached_path(\n                    f""{webpath}/muse.{language_code}.vec.gensim.vectors.npy"",\n                    cache_dir=cache_dir,\n                )\n                embeddings_file = cached_path(\n                    f""{webpath}/muse.{language_code}.vec.gensim"", cache_dir=cache_dir\n                )\n\n                # load the model\n                self.language_embeddings[\n                    language_code\n                ] = gensim.models.KeyedVectors.load(str(embeddings_file))\n\n            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n\n                if ""field"" not in self.__dict__ or self.field is None:\n                    word = token.text\n                else:\n                    word = token.get_tag(self.field).value\n\n                word_embedding = self.get_cached_vec(\n                    language_code=language_code, word=word\n                )\n\n                token.set_embedding(self.name, word_embedding)\n\n        return sentences\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def __str__(self):\n        return self.name\n\n\n# TODO: keep for backwards compatibility, but remove in future\nclass BPEmbSerializable(BPEmb):\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        # save the sentence piece model as binary file (not as path which may change)\n        state[""spm_model_binary""] = open(self.model_file, mode=""rb"").read()\n        state[""spm""] = None\n        return state\n\n    def __setstate__(self, state):\n        from bpemb.util import sentencepiece_load\n\n        model_file = self.model_tpl.format(lang=state[""lang""], vs=state[""vs""])\n        self.__dict__ = state\n\n        # write out the binary sentence piece model into the expected directory\n        self.cache_dir: Path = Path(flair.cache_root) / ""embeddings""\n        if ""spm_model_binary"" in self.__dict__:\n            # if the model was saved as binary and it is not found on disk, write to appropriate path\n            if not os.path.exists(self.cache_dir / state[""lang""]):\n                os.makedirs(self.cache_dir / state[""lang""])\n            self.model_file = self.cache_dir / model_file\n            with open(self.model_file, ""wb"") as out:\n                out.write(self.__dict__[""spm_model_binary""])\n        else:\n            # otherwise, use normal process and potentially trigger another download\n            self.model_file = self._load_file(model_file)\n\n        # once the modes if there, load it with sentence piece\n        state[""spm""] = sentencepiece_load(self.model_file)\n\n\nclass BytePairEmbeddings(TokenEmbeddings):\n    def __init__(\n        self,\n        language: str,\n        dim: int = 50,\n        syllables: int = 100000,\n        cache_dir=Path(flair.cache_root) / ""embeddings"",\n    ):\n        """"""\n        Initializes BP embeddings. Constructor downloads required files if not there.\n        """"""\n\n        self.name: str = f""bpe-{language}-{syllables}-{dim}""\n        self.static_embeddings = True\n        self.embedder = BPEmb(lang=language, vs=syllables, dim=dim, cache_dir=cache_dir)\n\n        self.__embedding_length: int = self.embedder.emb.vector_size * 2\n        super().__init__()\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        for i, sentence in enumerate(sentences):\n\n            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n\n                if ""field"" not in self.__dict__ or self.field is None:\n                    word = token.text\n                else:\n                    word = token.get_tag(self.field).value\n\n                if word.strip() == """":\n                    # empty words get no embedding\n                    token.set_embedding(\n                        self.name, torch.zeros(self.embedding_length, dtype=torch.float)\n                    )\n                else:\n                    # all other words get embedded\n                    embeddings = self.embedder.embed(word.lower())\n                    embedding = np.concatenate(\n                        (embeddings[0], embeddings[len(embeddings) - 1])\n                    )\n                    token.set_embedding(\n                        self.name, torch.tensor(embedding, dtype=torch.float)\n                    )\n\n        return sentences\n\n    def __str__(self):\n        return self.name\n\n    def extra_repr(self):\n        return ""model={}"".format(self.name)\n\n\nclass ELMoEmbeddings(TokenEmbeddings):\n    """"""Contextual word embeddings using word-level LM, as proposed in Peters et al., 2018.\n    ELMo word vectors can be constructed by combining layers in different ways.\n    Default is to concatene the top 3 layers in the LM.""""""\n\n    def __init__(\n        self, model: str = ""original"", options_file: str = None, weight_file: str = None, embedding_mode: str = ""all""\n    ):\n        super().__init__()\n\n        try:\n            import allennlp.commands.elmo\n        except ModuleNotFoundError:\n            log.warning(""-"" * 100)\n            log.warning(\'ATTENTION! The library ""allennlp"" is not installed!\')\n            log.warning(\n                \'To use ELMoEmbeddings, please first install with ""pip install allennlp""\'\n            )\n            log.warning(""-"" * 100)\n            pass\n\n        assert embedding_mode in [""all"", ""top"", ""average""]\n\n        self.name = f""elmo-{model}-{embedding_mode}""\n        self.static_embeddings = True\n\n        if not options_file or not weight_file:\n            # the default model for ELMo is the \'original\' model, which is very large\n            options_file = allennlp.commands.elmo.DEFAULT_OPTIONS_FILE\n            weight_file = allennlp.commands.elmo.DEFAULT_WEIGHT_FILE\n            # alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name\n            if model == ""small"":\n                options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json""\n                weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5""\n            if model == ""medium"":\n                options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_options.json""\n                weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5""\n            if model in [""large"", ""5.5B""]:\n                options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json""\n                weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5""\n            if model == ""pt"" or model == ""portuguese"":\n                options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pt/elmo_pt_options.json""\n                weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pt/elmo_pt_weights.hdf5""\n            if model == ""pubmed"":\n                options_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pubmed/elmo_2x4096_512_2048cnn_2xhighway_options.json""\n                weight_file = ""https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pubmed/elmo_2x4096_512_2048cnn_2xhighway_weights_PubMed_only.hdf5""\n\n        if embedding_mode == ""all"":\n            self.embedding_mode_fn = lambda x: torch.cat(x, 0)\n        elif embedding_mode == ""top"":\n            self.embedding_mode_fn = lambda x: x[-1]\n        elif embedding_mode == ""average"":\n            self.embedding_mode_fn = lambda x: torch.mean(torch.stack(x), 0)\n\n        # put on Cuda if available\n        from flair import device\n\n        if re.fullmatch(r""cuda:[0-9]+"", str(device)):\n            cuda_device = int(str(device).split("":"")[-1])\n        elif str(device) == ""cpu"":\n            cuda_device = -1\n        else:\n            cuda_device = 0\n\n        self.ee = allennlp.commands.elmo.ElmoEmbedder(\n            options_file=options_file, weight_file=weight_file, cuda_device=cuda_device\n        )\n\n        # embed a dummy sentence to determine embedding_length\n        dummy_sentence: Sentence = Sentence()\n        dummy_sentence.add_token(Token(""hello""))\n        embedded_dummy = self.embed(dummy_sentence)\n        self.__embedding_length: int = len(\n            embedded_dummy[0].get_token(1).get_embedding()\n        )\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n\n        sentence_words: List[List[str]] = []\n        for sentence in sentences:\n            sentence_words.append([token.text for token in sentence])\n\n        embeddings = self.ee.embed_batch(sentence_words)\n\n        for i, sentence in enumerate(sentences):\n\n            sentence_embeddings = embeddings[i]\n\n            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n                elmo_embedding_layers = [\n                    torch.FloatTensor(sentence_embeddings[0, token_idx, :]),\n                    torch.FloatTensor(sentence_embeddings[1, token_idx, :]),\n                    torch.FloatTensor(sentence_embeddings[2, token_idx, :])\n                ]\n                word_embedding = self.embedding_mode_fn(elmo_embedding_layers)\n                token.set_embedding(self.name, word_embedding)\n\n        return sentences\n\n    def extra_repr(self):\n        return ""model={}"".format(self.name)\n\n    def __str__(self):\n        return self.name\n\n\nclass NILCEmbeddings(WordEmbeddings):\n    def __init__(self, embeddings: str, model: str = ""skip"", size: int = 100):\n        """"""\n        Initializes portuguese classic word embeddings trained by NILC Lab (http://www.nilc.icmc.usp.br/embeddings).\n        Constructor downloads required files if not there.\n        :param embeddings: one of: \'fasttext\', \'glove\', \'wang2vec\' or \'word2vec\'\n        :param model: one of: \'skip\' or \'cbow\'. This is not applicable to glove.\n        :param size: one of: 50, 100, 300, 600 or 1000.\n        """"""\n\n        base_path = ""http://143.107.183.175:22980/download.php?file=embeddings/""\n\n        cache_dir = Path(""embeddings"") / embeddings.lower()\n\n        # GLOVE embeddings\n        if embeddings.lower() == ""glove"":\n            cached_path(\n                f""{base_path}{embeddings}/{embeddings}_s{size}.zip"", cache_dir=cache_dir\n            )\n            embeddings = cached_path(\n                f""{base_path}{embeddings}/{embeddings}_s{size}.zip"", cache_dir=cache_dir\n            )\n\n        elif embeddings.lower() in [""fasttext"", ""wang2vec"", ""word2vec""]:\n            cached_path(\n                f""{base_path}{embeddings}/{model}_s{size}.zip"", cache_dir=cache_dir\n            )\n            embeddings = cached_path(\n                f""{base_path}{embeddings}/{model}_s{size}.zip"", cache_dir=cache_dir\n            )\n\n        elif not Path(embeddings).exists():\n            raise ValueError(\n                f\'The given embeddings ""{embeddings}"" is not available or is not a valid path.\'\n            )\n\n        self.name: str = str(embeddings)\n        self.static_embeddings = True\n\n        log.info(""Reading embeddings from %s"" % embeddings)\n        self.precomputed_word_embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n            open_inside_zip(str(embeddings), cache_dir=cache_dir)\n        )\n\n        self.__embedding_length: int = self.precomputed_word_embeddings.vector_size\n        super(TokenEmbeddings, self).__init__()\n\n    @property\n    def embedding_length(self) -> int:\n        return self.__embedding_length\n\n    def __str__(self):\n        return self.name\n\n\ndef replace_with_language_code(string: str):\n    string = string.replace(""arabic-"", ""ar-"")\n    string = string.replace(""basque-"", ""eu-"")\n    string = string.replace(""bulgarian-"", ""bg-"")\n    string = string.replace(""croatian-"", ""hr-"")\n    string = string.replace(""czech-"", ""cs-"")\n    string = string.replace(""danish-"", ""da-"")\n    string = string.replace(""dutch-"", ""nl-"")\n    string = string.replace(""farsi-"", ""fa-"")\n    string = string.replace(""persian-"", ""fa-"")\n    string = string.replace(""finnish-"", ""fi-"")\n    string = string.replace(""french-"", ""fr-"")\n    string = string.replace(""german-"", ""de-"")\n    string = string.replace(""hebrew-"", ""he-"")\n    string = string.replace(""hindi-"", ""hi-"")\n    string = string.replace(""indonesian-"", ""id-"")\n    string = string.replace(""italian-"", ""it-"")\n    string = string.replace(""japanese-"", ""ja-"")\n    string = string.replace(""norwegian-"", ""no"")\n    string = string.replace(""polish-"", ""pl-"")\n    string = string.replace(""portuguese-"", ""pt-"")\n    string = string.replace(""slovenian-"", ""sl-"")\n    string = string.replace(""spanish-"", ""es-"")\n    string = string.replace(""swedish-"", ""sv-"")\n    return string\n'"
flair/hyperparameter/__init__.py,0,"b'from .parameter import (\n    Parameter,\n    SEQUENCE_TAGGER_PARAMETERS,\n    TRAINING_PARAMETERS,\n    DOCUMENT_EMBEDDING_PARAMETERS,\n)\nfrom .param_selection import (\n    SequenceTaggerParamSelector,\n    TextClassifierParamSelector,\n    SearchSpace,\n)\n'"
flair/hyperparameter/param_selection.py,0,"b'import logging\nfrom abc import abstractmethod\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Tuple, Union\nimport numpy as np\n\nfrom hyperopt import hp, fmin, tpe\n\nimport flair.nn\nfrom flair.data import Corpus\nfrom flair.embeddings import DocumentPoolEmbeddings, DocumentRNNEmbeddings\nfrom flair.hyperparameter import Parameter\nfrom flair.hyperparameter.parameter import (\n    SEQUENCE_TAGGER_PARAMETERS,\n    TRAINING_PARAMETERS,\n    DOCUMENT_EMBEDDING_PARAMETERS,\n    MODEL_TRAINER_PARAMETERS,\n)\nfrom flair.models import SequenceTagger, TextClassifier\nfrom flair.trainers import ModelTrainer\nfrom flair.training_utils import (\n    EvaluationMetric,\n    log_line,\n    init_output_file,\n    add_file_handler,\n)\n\nlog = logging.getLogger(""flair"")\n\n\nclass OptimizationValue(Enum):\n    DEV_LOSS = ""loss""\n    DEV_SCORE = ""score""\n\n\nclass SearchSpace(object):\n    def __init__(self):\n        self.search_space = {}\n\n    def add(self, parameter: Parameter, func, **kwargs):\n        self.search_space[parameter.value] = func(parameter.value, **kwargs)\n\n    def get_search_space(self):\n        return hp.choice(""parameters"", [self.search_space])\n\n\nclass ParamSelector(object):\n    def __init__(\n        self,\n        corpus: Corpus,\n        base_path: Union[str, Path],\n        max_epochs: int,\n        evaluation_metric: EvaluationMetric,\n        training_runs: int,\n        optimization_value: OptimizationValue,\n    ):\n        if type(base_path) is str:\n            base_path = Path(base_path)\n\n        self.corpus = corpus\n        self.max_epochs = max_epochs\n        self.base_path = base_path\n        self.evaluation_metric = evaluation_metric\n        self.run = 1\n        self.training_runs = training_runs\n        self.optimization_value = optimization_value\n\n        self.param_selection_file = init_output_file(base_path, ""param_selection.txt"")\n\n    @abstractmethod\n    def _set_up_model(self, params: dict) -> flair.nn.Model:\n        pass\n\n    def _objective(self, params: dict):\n        log_line(log)\n        log.info(f""Evaluation run: {self.run}"")\n        log.info(f""Evaluating parameter combination:"")\n        for k, v in params.items():\n            if isinstance(v, Tuple):\n                v = "","".join([str(x) for x in v])\n            log.info(f""\\t{k}: {str(v)}"")\n        log_line(log)\n\n        for sent in self.corpus.get_all_sentences():\n            sent.clear_embeddings()\n\n        scores = []\n        vars = []\n\n        for i in range(0, self.training_runs):\n            log_line(log)\n            log.info(f""Training run: {i + 1}"")\n\n            model = self._set_up_model(params)\n\n            training_params = {\n                key: params[key] for key in params if key in TRAINING_PARAMETERS\n            }\n            model_trainer_parameters = {\n                key: params[key] for key in params if key in MODEL_TRAINER_PARAMETERS\n            }\n\n            trainer: ModelTrainer = ModelTrainer(\n                model, self.corpus, **model_trainer_parameters\n            )\n\n            result = trainer.train(\n                self.base_path,\n                max_epochs=self.max_epochs,\n                param_selection_mode=True,\n                **training_params,\n            )\n\n            # take the average over the last three scores of training\n            if self.optimization_value == OptimizationValue.DEV_LOSS:\n                curr_scores = result[""dev_loss_history""][-3:]\n            else:\n                curr_scores = list(\n                    map(lambda s: 1 - s, result[""dev_score_history""][-3:])\n                )\n\n            score = sum(curr_scores) / float(len(curr_scores))\n            var = np.var(curr_scores)\n            scores.append(score)\n            vars.append(var)\n\n        # take average over the scores from the different training runs\n        final_score = sum(scores) / float(len(scores))\n        final_var = sum(vars) / float(len(vars))\n\n        test_score = result[""test_score""]\n        log_line(log)\n        log.info(f""Done evaluating parameter combination:"")\n        for k, v in params.items():\n            if isinstance(v, Tuple):\n                v = "","".join([str(x) for x in v])\n            log.info(f""\\t{k}: {v}"")\n        log.info(f""{self.optimization_value.value}: {final_score}"")\n        log.info(f""variance: {final_var}"")\n        log.info(f""test_score: {test_score}\\n"")\n        log_line(log)\n\n        with open(self.param_selection_file, ""a"") as f:\n            f.write(f""evaluation run {self.run}\\n"")\n            for k, v in params.items():\n                if isinstance(v, Tuple):\n                    v = "","".join([str(x) for x in v])\n                f.write(f""\\t{k}: {str(v)}\\n"")\n            f.write(f""{self.optimization_value.value}: {final_score}\\n"")\n            f.write(f""variance: {final_var}\\n"")\n            f.write(f""test_score: {test_score}\\n"")\n            f.write(""-"" * 100 + ""\\n"")\n\n        self.run += 1\n\n        return {""status"": ""ok"", ""loss"": final_score, ""loss_variance"": final_var}\n\n    def optimize(self, space: SearchSpace, max_evals=100):\n        search_space = space.search_space\n        best = fmin(\n            self._objective, search_space, algo=tpe.suggest, max_evals=max_evals\n        )\n\n        log_line(log)\n        log.info(""Optimizing parameter configuration done."")\n        log.info(""Best parameter configuration found:"")\n        for k, v in best.items():\n            log.info(f""\\t{k}: {v}"")\n        log_line(log)\n\n        with open(self.param_selection_file, ""a"") as f:\n            f.write(""best parameter combination\\n"")\n            for k, v in best.items():\n                if isinstance(v, Tuple):\n                    v = "","".join([str(x) for x in v])\n                f.write(f""\\t{k}: {str(v)}\\n"")\n\n\nclass SequenceTaggerParamSelector(ParamSelector):\n    def __init__(\n        self,\n        corpus: Corpus,\n        tag_type: str,\n        base_path: Union[str, Path],\n        max_epochs: int = 50,\n        evaluation_metric: EvaluationMetric = EvaluationMetric.MICRO_F1_SCORE,\n        training_runs: int = 1,\n        optimization_value: OptimizationValue = OptimizationValue.DEV_LOSS,\n    ):\n        """"""\n        :param corpus: the corpus\n        :param tag_type: tag type to use\n        :param base_path: the path to the result folder (results will be written to that folder)\n        :param max_epochs: number of epochs to perform on every evaluation run\n        :param evaluation_metric: evaluation metric used during training\n        :param training_runs: number of training runs per evaluation run\n        :param optimization_value: value to optimize\n        """"""\n        super().__init__(\n            corpus,\n            base_path,\n            max_epochs,\n            evaluation_metric,\n            training_runs,\n            optimization_value,\n        )\n\n        self.tag_type = tag_type\n        self.tag_dictionary = self.corpus.make_tag_dictionary(self.tag_type)\n\n    def _set_up_model(self, params: dict):\n        sequence_tagger_params = {\n            key: params[key] for key in params if key in SEQUENCE_TAGGER_PARAMETERS\n        }\n\n        tagger: SequenceTagger = SequenceTagger(\n            tag_dictionary=self.tag_dictionary,\n            tag_type=self.tag_type,\n            **sequence_tagger_params,\n        )\n        return tagger\n\n\nclass TextClassifierParamSelector(ParamSelector):\n    def __init__(\n        self,\n        corpus: Corpus,\n        multi_label: bool,\n        base_path: Union[str, Path],\n        document_embedding_type: str,\n        max_epochs: int = 50,\n        evaluation_metric: EvaluationMetric = EvaluationMetric.MICRO_F1_SCORE,\n        training_runs: int = 1,\n        optimization_value: OptimizationValue = OptimizationValue.DEV_LOSS,\n    ):\n        """"""\n        :param corpus: the corpus\n        :param multi_label: true, if the dataset is multi label, false otherwise\n        :param base_path: the path to the result folder (results will be written to that folder)\n        :param document_embedding_type: either \'lstm\', \'mean\', \'min\', or \'max\'\n        :param max_epochs: number of epochs to perform on every evaluation run\n        :param evaluation_metric: evaluation metric used during training\n        :param training_runs: number of training runs per evaluation run\n        :param optimization_value: value to optimize\n        """"""\n        super().__init__(\n            corpus,\n            base_path,\n            max_epochs,\n            evaluation_metric,\n            training_runs,\n            optimization_value,\n        )\n\n        self.multi_label = multi_label\n        self.document_embedding_type = document_embedding_type\n\n        self.label_dictionary = self.corpus.make_label_dictionary()\n\n    def _set_up_model(self, params: dict):\n        embdding_params = {\n            key: params[key] for key in params if key in DOCUMENT_EMBEDDING_PARAMETERS\n        }\n\n        if self.document_embedding_type == ""lstm"":\n            document_embedding = DocumentRNNEmbeddings(**embdding_params)\n        else:\n            document_embedding = DocumentPoolEmbeddings(**embdding_params)\n\n        text_classifier: TextClassifier = TextClassifier(\n            label_dictionary=self.label_dictionary,\n            multi_label=self.multi_label,\n            document_embeddings=document_embedding,\n        )\n\n        return text_classifier\n'"
flair/hyperparameter/parameter.py,0,"b'from enum import Enum\n\n\nclass Parameter(Enum):\n    EMBEDDINGS = ""embeddings""\n    HIDDEN_SIZE = ""hidden_size""\n    USE_CRF = ""use_crf""\n    USE_RNN = ""use_rnn""\n    RNN_LAYERS = ""rnn_layers""\n    DROPOUT = ""dropout""\n    WORD_DROPOUT = ""word_dropout""\n    LOCKED_DROPOUT = ""locked_dropout""\n    LEARNING_RATE = ""learning_rate""\n    MINI_BATCH_SIZE = ""mini_batch_size""\n    ANNEAL_FACTOR = ""anneal_factor""\n    ANNEAL_WITH_RESTARTS = ""anneal_with_restarts""\n    PATIENCE = ""patience""\n    REPROJECT_WORDS = ""reproject_words""\n    REPROJECT_WORD_DIMENSION = ""reproject_words_dimension""\n    BIDIRECTIONAL = ""bidirectional""\n    OPTIMIZER = ""optimizer""\n    MOMENTUM = ""momentum""\n    DAMPENING = ""dampening""\n    WEIGHT_DECAY = ""weight_decay""\n    NESTEROV = ""nesterov""\n    AMSGRAD = ""amsgrad""\n    BETAS = ""betas""\n    EPS = ""eps""\n\n\nTRAINING_PARAMETERS = [\n    Parameter.LEARNING_RATE.value,\n    Parameter.MINI_BATCH_SIZE.value,\n    Parameter.ANNEAL_FACTOR.value,\n    Parameter.PATIENCE.value,\n    Parameter.ANNEAL_WITH_RESTARTS.value,\n    Parameter.MOMENTUM.value,\n    Parameter.DAMPENING.value,\n    Parameter.WEIGHT_DECAY.value,\n    Parameter.NESTEROV.value,\n    Parameter.AMSGRAD.value,\n    Parameter.BETAS.value,\n    Parameter.EPS.value,\n]\nSEQUENCE_TAGGER_PARAMETERS = [\n    Parameter.EMBEDDINGS.value,\n    Parameter.HIDDEN_SIZE.value,\n    Parameter.RNN_LAYERS.value,\n    Parameter.USE_CRF.value,\n    Parameter.USE_RNN.value,\n    Parameter.DROPOUT.value,\n    Parameter.LOCKED_DROPOUT.value,\n    Parameter.WORD_DROPOUT.value,\n]\nMODEL_TRAINER_PARAMETERS = [Parameter.OPTIMIZER.value]\nDOCUMENT_EMBEDDING_PARAMETERS = [\n    Parameter.EMBEDDINGS.value,\n    Parameter.HIDDEN_SIZE.value,\n    Parameter.RNN_LAYERS.value,\n    Parameter.REPROJECT_WORDS.value,\n    Parameter.REPROJECT_WORD_DIMENSION.value,\n    Parameter.BIDIRECTIONAL.value,\n    Parameter.DROPOUT.value,\n    Parameter.LOCKED_DROPOUT.value,\n    Parameter.WORD_DROPOUT.value,\n]\n'"
flair/models/__init__.py,0,b'from .sequence_tagger_model import SequenceTagger\nfrom .language_model import LanguageModel\nfrom .text_classification_model import TextClassifier\n'
flair/models/language_model.py,23,"b'from pathlib import Path\n\nimport torch.nn as nn\nimport torch\nimport math\nfrom typing import Union, Tuple\nfrom typing import List\n\nfrom torch.optim import Optimizer\n\nimport flair\nfrom flair.data import Dictionary\n\n\nclass LanguageModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a decoder.""""""\n\n    def __init__(\n        self,\n        dictionary: Dictionary,\n        is_forward_lm: bool,\n        hidden_size: int,\n        nlayers: int,\n        embedding_size: int = 100,\n        nout=None,\n        document_delimiter: str = \'\\n\',\n        dropout=0.1,\n    ):\n\n        super(LanguageModel, self).__init__()\n\n        self.dictionary = dictionary\n        self.document_delimiter = document_delimiter\n        self.is_forward_lm: bool = is_forward_lm\n\n        self.dropout = dropout\n        self.hidden_size = hidden_size\n        self.embedding_size = embedding_size\n        self.nlayers = nlayers\n\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(len(dictionary), embedding_size)\n\n        if nlayers == 1:\n            self.rnn = nn.LSTM(embedding_size, hidden_size, nlayers)\n        else:\n            self.rnn = nn.LSTM(embedding_size, hidden_size, nlayers, dropout=dropout)\n\n        self.hidden = None\n\n        self.nout = nout\n        if nout is not None:\n            self.proj = nn.Linear(hidden_size, nout)\n            self.initialize(self.proj.weight)\n            self.decoder = nn.Linear(nout, len(dictionary))\n        else:\n            self.proj = None\n            self.decoder = nn.Linear(hidden_size, len(dictionary))\n\n        self.init_weights()\n\n        # auto-spawn on GPU if available\n        self.to(flair.device)\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.detach().uniform_(-initrange, initrange)\n        self.decoder.bias.detach().fill_(0)\n        self.decoder.weight.detach().uniform_(-initrange, initrange)\n\n    def set_hidden(self, hidden):\n        self.hidden = hidden\n\n    def forward(self, input, hidden, ordered_sequence_lengths=None):\n        encoded = self.encoder(input)\n        emb = self.drop(encoded)\n\n        self.rnn.flatten_parameters()\n\n        output, hidden = self.rnn(emb, hidden)\n\n        if self.proj is not None:\n            output = self.proj(output)\n\n        output = self.drop(output)\n\n        decoded = self.decoder(\n            output.view(output.size(0) * output.size(1), output.size(2))\n        )\n\n        return (\n            decoded.view(output.size(0), output.size(1), decoded.size(1)),\n            output,\n            hidden,\n        )\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).detach()\n        return (\n            weight.new(self.nlayers, bsz, self.hidden_size).zero_().clone().detach(),\n            weight.new(self.nlayers, bsz, self.hidden_size).zero_().clone().detach(),\n        )\n\n    def get_representation(\n        self,\n        strings: List[str],\n        start_marker: str,\n        end_marker: str,\n        chars_per_chunk: int = 512,\n    ):\n\n        len_longest_str: int = len(max(strings, key=len))\n\n        # pad strings with whitespaces to longest sentence\n        padded_strings: List[str] = []\n\n        for string in strings:\n            if not self.is_forward_lm:\n                string = string[::-1]\n\n            padded = f""{start_marker}{string}{end_marker}""\n            padded_strings.append(padded)\n\n        # cut up the input into chunks of max charlength = chunk_size\n        chunks = []\n        splice_begin = 0\n        longest_padded_str: int = len_longest_str + len(start_marker) + len(end_marker)\n        for splice_end in range(chars_per_chunk, longest_padded_str, chars_per_chunk):\n            chunks.append([text[splice_begin:splice_end] for text in padded_strings])\n            splice_begin = splice_end\n\n        chunks.append(\n            [text[splice_begin:longest_padded_str] for text in padded_strings]\n        )\n        hidden = self.init_hidden(len(chunks[0]))\n\n        padding_char_index = self.dictionary.get_idx_for_item("" "")\n\n        batches: List[torch.Tensor] = []\n        # push each chunk through the RNN language model\n        for chunk in chunks:\n            len_longest_chunk: int = len(max(chunk, key=len))\n            sequences_as_char_indices: List[List[int]] = []\n            for string in chunk:\n                char_indices = self.dictionary.get_idx_for_items(list(string))\n                char_indices += [padding_char_index] * (len_longest_chunk - len(string))\n\n                sequences_as_char_indices.append(char_indices)\n            t = torch.tensor(sequences_as_char_indices, dtype=torch.long).to(\n                device=flair.device, non_blocking=True\n            )\n            batches.append(t)\n\n        output_parts = []\n        for batch in batches:\n            batch = batch.transpose(0, 1)\n            _, rnn_output, hidden = self.forward(batch, hidden)\n            output_parts.append(rnn_output)\n\n        # concatenate all chunks to make final output\n        output = torch.cat(output_parts)\n\n        return output\n\n    def get_output(self, text: str):\n        char_indices = [self.dictionary.get_idx_for_item(char) for char in text]\n        input_vector = torch.LongTensor([char_indices]).transpose(0, 1)\n\n        hidden = self.init_hidden(1)\n        prediction, rnn_output, hidden = self.forward(input_vector, hidden)\n\n        return self.repackage_hidden(hidden)\n\n    def repackage_hidden(self, h):\n        """"""Wraps hidden states in new Variables, to detach them from their history.""""""\n        if type(h) == torch.Tensor:\n            return h.clone().detach()\n        else:\n            return tuple(self.repackage_hidden(v) for v in h)\n\n    @staticmethod\n    def initialize(matrix):\n        in_, out_ = matrix.size()\n        stdv = math.sqrt(3.0 / (in_ + out_))\n        matrix.detach().uniform_(-stdv, stdv)\n\n    @classmethod\n    def load_language_model(cls, model_file: Union[Path, str]):\n\n        state = torch.load(str(model_file), map_location=flair.device)\n\n        document_delimiter = state[""document_delimiter""] if ""document_delimiter"" in state else \'\\n\'\n\n        model = LanguageModel(\n            dictionary=state[""dictionary""],\n            is_forward_lm=state[""is_forward_lm""],\n            hidden_size=state[""hidden_size""],\n            nlayers=state[""nlayers""],\n            embedding_size=state[""embedding_size""],\n            nout=state[""nout""],\n            document_delimiter=document_delimiter,\n            dropout=state[""dropout""],\n        )\n        model.load_state_dict(state[""state_dict""])\n        model.eval()\n        model.to(flair.device)\n\n        return model\n\n    @classmethod\n    def load_checkpoint(cls, model_file: Union[Path, str]):\n        state = torch.load(str(model_file), map_location=flair.device)\n\n        epoch = state[""epoch""] if ""epoch"" in state else None\n        split = state[""split""] if ""split"" in state else None\n        loss = state[""loss""] if ""loss"" in state else None\n        document_delimiter = state[""document_delimiter""] if ""document_delimiter"" in state else \'\\n\'\n\n        optimizer_state_dict = (\n            state[""optimizer_state_dict""] if ""optimizer_state_dict"" in state else None\n        )\n\n        model = LanguageModel(\n            dictionary=state[""dictionary""],\n            is_forward_lm=state[""is_forward_lm""],\n            hidden_size=state[""hidden_size""],\n            nlayers=state[""nlayers""],\n            embedding_size=state[""embedding_size""],\n            nout=state[""nout""],\n            document_delimiter=document_delimiter,\n            dropout=state[""dropout""],\n        )\n        model.load_state_dict(state[""state_dict""])\n        model.eval()\n        model.to(flair.device)\n\n        return {\n            ""model"": model,\n            ""epoch"": epoch,\n            ""split"": split,\n            ""loss"": loss,\n            ""optimizer_state_dict"": optimizer_state_dict,\n        }\n\n    def save_checkpoint(\n        self, file: Union[Path, str], optimizer: Optimizer, epoch: int, split: int, loss: float\n    ):\n        model_state = {\n            ""state_dict"": self.state_dict(),\n            ""dictionary"": self.dictionary,\n            ""is_forward_lm"": self.is_forward_lm,\n            ""hidden_size"": self.hidden_size,\n            ""nlayers"": self.nlayers,\n            ""embedding_size"": self.embedding_size,\n            ""nout"": self.nout,\n            ""document_delimiter"": self.document_delimiter,\n            ""dropout"": self.dropout,\n            ""optimizer_state_dict"": optimizer.state_dict(),\n            ""epoch"": epoch,\n            ""split"": split,\n            ""loss"": loss,\n        }\n\n        torch.save(model_state, str(file), pickle_protocol=4)\n\n    def save(self, file: Union[Path, str]):\n        model_state = {\n            ""state_dict"": self.state_dict(),\n            ""dictionary"": self.dictionary,\n            ""is_forward_lm"": self.is_forward_lm,\n            ""hidden_size"": self.hidden_size,\n            ""nlayers"": self.nlayers,\n            ""embedding_size"": self.embedding_size,\n            ""nout"": self.nout,\n            ""document_delimiter"": self.document_delimiter,\n            ""dropout"": self.dropout,\n        }\n\n        torch.save(model_state, str(file), pickle_protocol=4)\n\n    def generate_text(\n        self,\n        prefix: str = ""\\n"",\n        number_of_characters: int = 1000,\n        temperature: float = 1.0,\n        break_on_suffix=None,\n    ) -> Tuple[str, float]:\n\n        if prefix == """":\n            prefix = ""\\n""\n\n        with torch.no_grad():\n            characters = []\n\n            idx2item = self.dictionary.idx2item\n\n            # initial hidden state\n            hidden = self.init_hidden(1)\n\n            if len(prefix) > 1:\n\n                char_tensors = []\n                for character in prefix[:-1]:\n                    char_tensors.append(\n                        torch.tensor(self.dictionary.get_idx_for_item(character))\n                        .unsqueeze(0)\n                        .unsqueeze(0)\n                    )\n\n                input = torch.cat(char_tensors).to(flair.device)\n\n                prediction, _, hidden = self.forward(input, hidden)\n\n            input = (\n                torch.tensor(self.dictionary.get_idx_for_item(prefix[-1]))\n                .unsqueeze(0)\n                .unsqueeze(0)\n            )\n\n            log_prob = 0.0\n\n            for i in range(number_of_characters):\n\n                input = input.to(flair.device)\n\n                # get predicted weights\n                prediction, _, hidden = self.forward(input, hidden)\n                prediction = prediction.squeeze().detach()\n                decoder_output = prediction\n\n                # divide by temperature\n                prediction = prediction.div(temperature)\n\n                # to prevent overflow problem with small temperature values, substract largest value from all\n                # this makes a vector in which the largest value is 0\n                max = torch.max(prediction)\n                prediction -= max\n\n                # compute word weights with exponential function\n                word_weights = prediction.exp().cpu()\n\n                # try sampling multinomial distribution for next character\n                try:\n                    word_idx = torch.multinomial(word_weights, 1)[0]\n                except:\n                    word_idx = torch.tensor(0)\n\n                # print(word_idx)\n                prob = decoder_output[word_idx]\n                log_prob += prob\n\n                input = word_idx.detach().unsqueeze(0).unsqueeze(0)\n                word = idx2item[word_idx].decode(""UTF-8"")\n                characters.append(word)\n\n                if break_on_suffix is not None:\n                    if """".join(characters).endswith(break_on_suffix):\n                        break\n\n            text = prefix + """".join(characters)\n\n            log_prob = log_prob.item()\n            log_prob /= len(characters)\n\n            if not self.is_forward_lm:\n                text = text[::-1]\n\n            return text, log_prob\n\n    def calculate_perplexity(self, text: str) -> float:\n\n        if not self.is_forward_lm:\n            text = text[::-1]\n\n        # input ids\n        input = torch.tensor(\n            [self.dictionary.get_idx_for_item(char) for char in text[:-1]]\n        ).unsqueeze(1)\n        input = input.to(flair.device)\n\n        # push list of character IDs through model\n        hidden = self.init_hidden(1)\n        prediction, _, hidden = self.forward(input, hidden)\n\n        # the target is always the next character\n        targets = torch.tensor(\n            [self.dictionary.get_idx_for_item(char) for char in text[1:]]\n        )\n        targets = targets.to(flair.device)\n\n        # use cross entropy loss to compare output of forward pass with targets\n        cross_entroy_loss = torch.nn.CrossEntropyLoss()\n        loss = cross_entroy_loss(\n            prediction.view(-1, len(self.dictionary)), targets\n        ).item()\n\n        # exponentiate cross-entropy loss to calculate perplexity\n        perplexity = math.exp(loss)\n\n        return perplexity\n\n    def _apply(self, fn):\n        major, minor, build, *_ = (int(info)\n                                for info in torch.__version__.replace(""+"",""."").split(\'.\') if info.isdigit())\n\n        # fixed RNN change format for torch 1.4.0\n        if major >= 1 and minor >= 4:\n            for child_module in self.children():\n                if isinstance(child_module, torch.nn.RNNBase):\n                    _flat_weights_names = []\n                    num_direction = None\n\n                    if child_module.__dict__[""bidirectional""]:\n                        num_direction = 2\n                    else:\n                        num_direction = 1\n                    for layer in range(child_module.__dict__[""num_layers""]):\n                        for direction in range(num_direction):\n                            suffix = ""_reverse"" if direction == 1 else """"\n                            param_names = [""weight_ih_l{}{}"", ""weight_hh_l{}{}""]\n                            if child_module.__dict__[""bias""]:\n                                param_names += [""bias_ih_l{}{}"", ""bias_hh_l{}{}""]\n                            param_names = [\n                                x.format(layer, suffix) for x in param_names\n                            ]\n                            _flat_weights_names.extend(param_names)\n\n                    setattr(child_module, ""_flat_weights_names"",\n                            _flat_weights_names)\n\n                child_module._apply(fn)\n\n        else:\n            super()._apply(fn)\n'"
flair/models/sequence_tagger_model.py,46,"b'import logging\nfrom pathlib import Path\nfrom typing import List, Union, Optional, Callable, Dict\n\nimport numpy as np\nimport torch\nimport torch.nn\nimport torch.nn.functional as F\nfrom tabulate import tabulate\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data.dataset import Dataset\nfrom tqdm import tqdm\n\nimport flair.nn\nfrom flair.data import Dictionary, Sentence, Token, Label, space_tokenizer, DataPoint\nfrom flair.datasets import SentenceDataset, StringDataset, DataLoader\nfrom flair.embeddings import TokenEmbeddings, StackedEmbeddings\nfrom flair.file_utils import cached_path, unzip_file\nfrom flair.training_utils import Metric, Result, store_embeddings\n\nlog = logging.getLogger(""flair"")\n\nSTART_TAG: str = ""<START>""\nSTOP_TAG: str = ""<STOP>""\n\n\ndef to_scalar(var):\n    return var.view(-1).detach().tolist()[0]\n\n\ndef argmax(vec):\n    _, idx = torch.max(vec, 1)\n    return to_scalar(idx)\n\n\ndef log_sum_exp(vec):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n\n\ndef argmax_batch(vecs):\n    _, idx = torch.max(vecs, 1)\n    return idx\n\n\ndef log_sum_exp_batch(vecs):\n    maxi = torch.max(vecs, 1)[0]\n    maxi_bc = maxi[:, None].repeat(1, vecs.shape[1])\n    recti_ = torch.log(torch.sum(torch.exp(vecs - maxi_bc), 1))\n    return maxi + recti_\n\n\ndef pad_tensors(tensor_list):\n    ml = max([x.shape[0] for x in tensor_list])\n    shape = [len(tensor_list), ml] + list(tensor_list[0].shape[1:])\n    template = torch.zeros(*shape, dtype=torch.long, device=flair.device)\n    lens_ = [x.shape[0] for x in tensor_list]\n    for i, tensor in enumerate(tensor_list):\n        template[i, : lens_[i]] = tensor\n\n    return template, lens_\n\n\nclass SequenceTagger(flair.nn.Model):\n    def __init__(\n        self,\n        hidden_size: int,\n        embeddings: TokenEmbeddings,\n        tag_dictionary: Dictionary,\n        tag_type: str,\n        use_crf: bool = True,\n        use_rnn: bool = True,\n        rnn_layers: int = 1,\n        dropout: float = 0.0,\n        word_dropout: float = 0.05,\n        locked_dropout: float = 0.5,\n        reproject_embeddings: Union[bool,int] = True,\n        train_initial_hidden_state: bool = False,\n        rnn_type: str = ""LSTM"",\n        pickle_module: str = ""pickle"",\n        beta: float = 1.0,\n        loss_weights: Dict[str, float] = None,\n    ):\n        """"""\n        Initializes a SequenceTagger\n        :param hidden_size: number of hidden states in RNN\n        :param embeddings: word embeddings used in tagger\n        :param tag_dictionary: dictionary of tags you want to predict\n        :param tag_type: string identifier for tag type\n        :param use_crf: if True use CRF decoder, else project directly to tag space\n        :param use_rnn: if True use RNN layer, otherwise use word embeddings directly\n        :param rnn_layers: number of RNN layers\n        :param dropout: dropout probability\n        :param word_dropout: word dropout probability\n        :param reproject_embeddings: if True, adds trainable linear map on top of embedding layer. If False, no map.\n        If you set this to an integer, you can control the dimensionality of the reprojection layer\n        :param locked_dropout: locked dropout probability\n        :param train_initial_hidden_state: if True, trains initial hidden state of RNN\n        :param beta: Parameter for F-beta score for evaluation and training annealing\n        :param loss_weights: Dictionary of weights for classes (tags) for the loss function\n        (if any tag\'s weight is unspecified it will default to 1.0)\n\n        """"""\n\n        super(SequenceTagger, self).__init__()\n        self.use_rnn = use_rnn\n        self.hidden_size = hidden_size\n        self.use_crf: bool = use_crf\n        self.rnn_layers: int = rnn_layers\n\n        self.trained_epochs: int = 0\n\n        self.embeddings = embeddings\n\n        # set the dictionaries\n        self.tag_dictionary: Dictionary = tag_dictionary\n        # if we use a CRF, we must add special START and STOP tags to the dictionary\n        if use_crf:\n            self.tag_dictionary.add_item(START_TAG)\n            self.tag_dictionary.add_item(STOP_TAG)\n\n        self.tag_type: str = tag_type\n        self.tagset_size: int = len(tag_dictionary)\n\n        self.beta = beta\n\n        self.weight_dict = loss_weights\n        # Initialize the weight tensor\n        if loss_weights is not None:\n            n_classes = len(self.tag_dictionary)\n            weight_list = [1. for i in range(n_classes)]\n            for i, tag in enumerate(self.tag_dictionary.get_items()):\n                if tag in loss_weights.keys():\n                    weight_list[i] = loss_weights[tag]\n            self.loss_weights = torch.FloatTensor(weight_list).to(flair.device)\n        else:\n            self.loss_weights = None\n\n        # initialize the network architecture\n        self.nlayers: int = rnn_layers\n        self.hidden_word = None\n\n        # dropouts\n        self.use_dropout: float = dropout\n        self.use_word_dropout: float = word_dropout\n        self.use_locked_dropout: float = locked_dropout\n\n        self.pickle_module = pickle_module\n\n        if dropout > 0.0:\n            self.dropout = torch.nn.Dropout(dropout)\n\n        if word_dropout > 0.0:\n            self.word_dropout = flair.nn.WordDropout(word_dropout)\n\n        if locked_dropout > 0.0:\n            self.locked_dropout = flair.nn.LockedDropout(locked_dropout)\n\n        embedding_dim: int = self.embeddings.embedding_length\n        rnn_input_dim: int = embedding_dim\n\n        # optional reprojection layer on top of word embeddings\n        self.reproject_embeddings = reproject_embeddings\n        if self.reproject_embeddings:\n            if type(self.reproject_embeddings) == int:\n                rnn_input_dim = self.reproject_embeddings\n\n            self.embedding2nn = torch.nn.Linear(embedding_dim, rnn_input_dim)\n\n        self.train_initial_hidden_state = train_initial_hidden_state\n        self.bidirectional = True\n        self.rnn_type = rnn_type\n\n        # bidirectional LSTM on top of embedding layer\n        if self.use_rnn:\n            num_directions = 2 if self.bidirectional else 1\n\n            if self.rnn_type in [""LSTM"", ""GRU""]:\n\n                self.rnn = getattr(torch.nn, self.rnn_type)(\n                    rnn_input_dim,\n                    hidden_size,\n                    num_layers=self.nlayers,\n                    dropout=0.0 if self.nlayers == 1 else 0.5,\n                    bidirectional=True,\n                    batch_first=True,\n                )\n                # Create initial hidden state and initialize it\n                if self.train_initial_hidden_state:\n                    self.hs_initializer = torch.nn.init.xavier_normal_\n\n                    self.lstm_init_h = Parameter(\n                        torch.randn(self.nlayers * num_directions, self.hidden_size),\n                        requires_grad=True,\n                    )\n\n                    self.lstm_init_c = Parameter(\n                        torch.randn(self.nlayers * num_directions, self.hidden_size),\n                        requires_grad=True,\n                    )\n\n                    # TODO: Decide how to initialize the hidden state variables\n                    # self.hs_initializer(self.lstm_init_h)\n                    # self.hs_initializer(self.lstm_init_c)\n\n            # final linear map to tag space\n            self.linear = torch.nn.Linear(\n                hidden_size * num_directions, len(tag_dictionary)\n            )\n        else:\n            self.linear = torch.nn.Linear(\n                self.embeddings.embedding_length, len(tag_dictionary)\n            )\n\n        if self.use_crf:\n            self.transitions = torch.nn.Parameter(\n                torch.randn(self.tagset_size, self.tagset_size)\n            )\n\n            self.transitions.detach()[\n                self.tag_dictionary.get_idx_for_item(START_TAG), :\n            ] = -10000\n\n            self.transitions.detach()[\n                :, self.tag_dictionary.get_idx_for_item(STOP_TAG)\n            ] = -10000\n\n        self.to(flair.device)\n\n    def _get_state_dict(self):\n        model_state = {\n            ""state_dict"": self.state_dict(),\n            ""embeddings"": self.embeddings,\n            ""hidden_size"": self.hidden_size,\n            ""train_initial_hidden_state"": self.train_initial_hidden_state,\n            ""tag_dictionary"": self.tag_dictionary,\n            ""tag_type"": self.tag_type,\n            ""use_crf"": self.use_crf,\n            ""use_rnn"": self.use_rnn,\n            ""rnn_layers"": self.rnn_layers,\n            ""use_dropout"": self.use_dropout,\n            ""use_word_dropout"": self.use_word_dropout,\n            ""use_locked_dropout"": self.use_locked_dropout,\n            ""rnn_type"": self.rnn_type,\n            ""beta"": self.beta,\n            ""weight_dict"": self.weight_dict,\n            ""reproject_embeddings"": self.reproject_embeddings,\n        }\n        return model_state\n\n    @staticmethod\n    def _init_model_with_state_dict(state):\n\n        rnn_type = ""LSTM"" if ""rnn_type"" not in state.keys() else state[""rnn_type""]\n        use_dropout = 0.0 if ""use_dropout"" not in state.keys() else state[""use_dropout""]\n        use_word_dropout = (\n            0.0 if ""use_word_dropout"" not in state.keys() else state[""use_word_dropout""]\n        )\n        use_locked_dropout = (\n            0.0\n            if ""use_locked_dropout"" not in state.keys()\n            else state[""use_locked_dropout""]\n        )\n        train_initial_hidden_state = (\n            False\n            if ""train_initial_hidden_state"" not in state.keys()\n            else state[""train_initial_hidden_state""]\n        )\n        beta = 1.0 if ""beta"" not in state.keys() else state[""beta""]\n        weights = None if ""weight_dict"" not in state.keys() else state[""weight_dict""]\n        reproject_embeddings = True if ""reproject_embeddings"" not in state.keys() else state[""reproject_embeddings""]\n        if ""reproject_to"" in state.keys():\n            reproject_embeddings = state[""reproject_to""]\n\n        model = SequenceTagger(\n            hidden_size=state[""hidden_size""],\n            embeddings=state[""embeddings""],\n            tag_dictionary=state[""tag_dictionary""],\n            tag_type=state[""tag_type""],\n            use_crf=state[""use_crf""],\n            use_rnn=state[""use_rnn""],\n            rnn_layers=state[""rnn_layers""],\n            dropout=use_dropout,\n            word_dropout=use_word_dropout,\n            locked_dropout=use_locked_dropout,\n            train_initial_hidden_state=train_initial_hidden_state,\n            rnn_type=rnn_type,\n            beta=beta,\n            loss_weights=weights,\n            reproject_embeddings=reproject_embeddings,\n        )\n        model.load_state_dict(state[""state_dict""])\n        return model\n\n    def predict(\n        self,\n        sentences: Union[List[Sentence], Sentence],\n        mini_batch_size=32,\n        all_tag_prob: bool = False,\n        verbose: bool = False,\n        label_name: Optional[str] = None,\n        return_loss = False,\n        embedding_storage_mode=""none"",\n    ):\n        """"""\n        Predict sequence tags for Named Entity Recognition task\n        :param sentences: a Sentence or a List of Sentence\n        :param mini_batch_size: size of the minibatch, usually bigger is more rapid but consume more memory,\n        up to a point when it has no more effect.\n        :param all_tag_prob: True to compute the score for each tag on each token,\n        otherwise only the score of the best tag is returned\n        :param verbose: set to True to display a progress bar\n        :param return_loss: set to True to return loss\n        :param label_name: set this to change the name of the label type that is predicted\n        :param embedding_storage_mode: default is \'none\' which is always best. Only set to \'cpu\' or \'gpu\' if\n        you wish to not only predict, but also keep the generated embeddings in CPU or GPU memory respectively.\n        \'gpu\' to store embeddings in GPU memory.\n        """"""\n        if label_name == None:\n            label_name = self.tag_type\n\n        with torch.no_grad():\n            if not sentences:\n                return sentences\n\n            if isinstance(sentences, Sentence):\n                sentences = [sentences]\n\n            # reverse sort all sequences by their length\n            rev_order_len_index = sorted(\n                range(len(sentences)), key=lambda k: len(sentences[k]), reverse=True\n            )\n\n            reordered_sentences: List[Union[Sentence, str]] = [\n                sentences[index] for index in rev_order_len_index\n            ]\n\n            dataloader = DataLoader(\n                dataset=SentenceDataset(reordered_sentences), batch_size=mini_batch_size\n            )\n\n            if self.use_crf:\n                transitions = self.transitions.detach().cpu().numpy()\n            else:\n                transitions = None\n\n            # progress bar for verbosity\n            if verbose:\n                dataloader = tqdm(dataloader)\n\n            overall_loss = 0\n            batch_no = 0\n            for batch in dataloader:\n\n                batch_no += 1\n\n                if verbose:\n                    dataloader.set_description(f""Inferencing on batch {batch_no}"")\n\n                batch = self._filter_empty_sentences(batch)\n                # stop if all sentences are empty\n                if not batch:\n                    continue\n\n                feature = self.forward(batch)\n\n                if return_loss:\n                    overall_loss += self._calculate_loss(feature, batch)\n\n                tags, all_tags = self._obtain_labels(\n                    feature=feature,\n                    batch_sentences=batch,\n                    transitions=transitions,\n                    get_all_tags=all_tag_prob,\n                )\n\n                for (sentence, sent_tags) in zip(batch, tags):\n                    for (token, tag) in zip(sentence.tokens, sent_tags):\n                        token.add_tag_label(label_name, tag)\n\n                # all_tags will be empty if all_tag_prob is set to False, so the for loop will be avoided\n                for (sentence, sent_all_tags) in zip(batch, all_tags):\n                    for (token, token_all_tags) in zip(sentence.tokens, sent_all_tags):\n                        token.add_tags_proba_dist(label_name, token_all_tags)\n\n                # clearing token embeddings to save memory\n                store_embeddings(batch, storage_mode=embedding_storage_mode)\n\n            if return_loss:\n                return overall_loss / batch_no\n\n    def _requires_span_F1_evaluation(self) -> bool:\n        span_F1 = False\n        for item in self.tag_dictionary.get_items():\n            if item.startswith(\'B-\'):\n                span_F1 = True\n        return span_F1\n\n    def _evaluate_with_span_F1(self, data_loader, embedding_storage_mode, mini_batch_size, out_path):\n        eval_loss = 0\n\n        batch_no: int = 0\n\n        metric = Metric(""Evaluation"", beta=self.beta)\n\n        lines: List[str] = []\n\n        y_true = []\n        y_pred = []\n\n        for batch in data_loader:\n\n            # predict for batch\n            loss = self.predict(batch,\n                                embedding_storage_mode=embedding_storage_mode,\n                                mini_batch_size=mini_batch_size,\n                                label_name=\'predicted\',\n                                return_loss=True)\n            eval_loss += loss\n            batch_no += 1\n\n            for sentence in batch:\n\n                # make list of gold tags\n                gold_spans = sentence.get_spans(self.tag_type)\n                gold_tags = [(span.tag, repr(span)) for span in gold_spans]\n\n                # make list of predicted tags\n                predicted_spans = sentence.get_spans(""predicted"")\n                predicted_tags = [(span.tag, repr(span)) for span in predicted_spans]\n\n                # check for true positives, false positives and false negatives\n                for tag, prediction in predicted_tags:\n                    if (tag, prediction) in gold_tags:\n                        metric.add_tp(tag)\n                    else:\n                        metric.add_fp(tag)\n\n                for tag, gold in gold_tags:\n                    if (tag, gold) not in predicted_tags:\n                        metric.add_fn(tag)\n\n                tags_gold = []\n                tags_pred = []\n\n                # also write to file in BIO format to use old conlleval script\n                if out_path:\n                    for token in sentence:\n                        # check if in gold spans\n                        gold_tag = \'O\'\n                        for span in gold_spans:\n                            if token in span:\n                                gold_tag = \'B-\' + span.tag if token == span[0] else \'I-\' + span.tag\n                        tags_gold.append(gold_tag)\n\n                        predicted_tag = \'O\'\n                        # check if in predicted spans\n                        for span in predicted_spans:\n                            if token in span:\n                                predicted_tag = \'B-\' + span.tag if token == span[0] else \'I-\' + span.tag\n                        tags_pred.append(predicted_tag)\n\n                        lines.append(f\'{token.text} {gold_tag} {predicted_tag}\\n\')\n                    lines.append(\'\\n\')\n\n                y_true.append(tags_gold)\n                y_pred.append(tags_pred)\n\n        if out_path:\n            with open(Path(out_path), ""w"", encoding=""utf-8"") as outfile:\n                outfile.write("""".join(lines))\n\n        eval_loss /= batch_no\n\n        detailed_result = (\n            ""\\nResults:""\n            f""\\n- F1-score (micro) {metric.micro_avg_f_score():.4f}""\n            f""\\n- F1-score (macro) {metric.macro_avg_f_score():.4f}""\n            \'\\n\\nBy class:\'\n        )\n\n        for class_name in metric.get_classes():\n            detailed_result += (\n                f""\\n{class_name:<10} tp: {metric.get_tp(class_name)} - fp: {metric.get_fp(class_name)} - ""\n                f""fn: {metric.get_fn(class_name)} - precision: ""\n                f""{metric.precision(class_name):.4f} - recall: {metric.recall(class_name):.4f} - ""\n                f""f1-score: ""\n                f""{metric.f_score(class_name):.4f}""\n            )\n\n        result = Result(\n            main_score=metric.micro_avg_f_score(),\n            log_line=f""{metric.precision():.4f}\\t{metric.recall():.4f}\\t{metric.micro_avg_f_score():.4f}"",\n            log_header=""PRECISION\\tRECALL\\tF1"",\n            detailed_results=detailed_result,\n        )\n\n        return result, eval_loss\n\n    def evaluate(\n        self,\n        sentences: Union[List[Sentence], Dataset],\n        out_path: Union[str, Path] = None,\n        embedding_storage_mode: str = ""none"",\n        mini_batch_size: int = 32,\n        num_workers: int = 8,\n    ) -> (Result, float):\n\n        # read Dataset into data loader (if list of sentences passed, make Dataset first)\n        if not isinstance(sentences, Dataset):\n            sentences = SentenceDataset(sentences)\n        data_loader = DataLoader(sentences, batch_size=mini_batch_size, num_workers=num_workers)\n\n        # if span F1 needs to be used, use separate eval method\n        if self._requires_span_F1_evaluation():\n            return self._evaluate_with_span_F1(data_loader, embedding_storage_mode, mini_batch_size, out_path)\n\n        # else, use scikit-learn to evaluate\n        y_true = []\n        y_pred = []\n        labels = Dictionary(add_unk=False)\n\n        eval_loss = 0\n        batch_no: int = 0\n\n        lines: List[str] = []\n\n        for batch in data_loader:\n\n            # predict for batch\n            loss = self.predict(batch,\n                                embedding_storage_mode=embedding_storage_mode,\n                                mini_batch_size=mini_batch_size,\n                                label_name=\'predicted\',\n                                return_loss=True)\n            eval_loss += loss\n            batch_no += 1\n\n            for sentence in batch:\n\n                for token in sentence:\n                    # add gold tag\n                    gold_tag = token.get_tag(self.tag_type).value\n                    y_true.append(labels.add_item(gold_tag))\n\n                    # add predicted tag\n                    predicted_tag = token.get_tag(\'predicted\').value\n                    y_pred.append(labels.add_item(predicted_tag))\n\n                    # for file output\n                    lines.append(f\'{token.text} {gold_tag} {predicted_tag}\\n\')\n\n                lines.append(\'\\n\')\n\n        if out_path:\n            with open(Path(out_path), ""w"", encoding=""utf-8"") as outfile:\n                outfile.write("""".join(lines))\n\n        eval_loss /= batch_no\n\n        # use sklearn\n        from sklearn import metrics\n\n        # make ""classification report""\n        target_names = []\n        for i in range(len(labels)):\n            target_names.append(labels.get_item_for_index(i))\n        classification_report = metrics.classification_report(y_true, y_pred, digits=4, target_names=target_names, zero_division=1)\n\n        # get scores\n        macro_f_score = round(metrics.fbeta_score(y_true, y_pred, beta=self.beta, average=\'micro\'), 4)\n        micro_f_score = round(metrics.fbeta_score(y_true, y_pred, beta=self.beta, average=\'macro\'), 4)\n        accuracy_score = round(metrics.accuracy_score(y_true, y_pred), 4)\n\n        detailed_result = (\n            ""\\nResults:""\n            f""\\n- F-score (micro) {macro_f_score}""\n            f""\\n- F-score (macro) {micro_f_score}""\n            f""\\n- Accuracy {accuracy_score}""\n            \'\\n\\nBy class:\\n\' + classification_report\n        )\n\n        # line for log file\n        log_header = ""ACCURACY""\n        log_line = f""\\t{accuracy_score}""\n\n        result = Result(\n            main_score=macro_f_score,\n            log_line=log_line,\n            log_header=log_header,\n            detailed_results=detailed_result,\n        )\n        return result, eval_loss\n\n    def forward_loss(\n        self, data_points: Union[List[Sentence], Sentence], sort=True\n    ) -> torch.tensor:\n        features = self.forward(data_points)\n        return self._calculate_loss(features, data_points)\n\n    def forward(self, sentences: List[Sentence]):\n\n        self.embeddings.embed(sentences)\n\n        names = self.embeddings.get_names()\n\n        lengths: List[int] = [len(sentence.tokens) for sentence in sentences]\n        longest_token_sequence_in_batch: int = max(lengths)\n\n        pre_allocated_zero_tensor = torch.zeros(\n            self.embeddings.embedding_length * longest_token_sequence_in_batch,\n            dtype=torch.float,\n            device=flair.device,\n        )\n\n        all_embs = list()\n        for sentence in sentences:\n            all_embs += [\n                emb for token in sentence for emb in token.get_each_embedding(names)\n            ]\n            nb_padding_tokens = longest_token_sequence_in_batch - len(sentence)\n\n            if nb_padding_tokens > 0:\n                t = pre_allocated_zero_tensor[\n                    : self.embeddings.embedding_length * nb_padding_tokens\n                ]\n                all_embs.append(t)\n\n        sentence_tensor = torch.cat(all_embs).view(\n            [\n                len(sentences),\n                longest_token_sequence_in_batch,\n                self.embeddings.embedding_length,\n            ]\n        )\n\n        # --------------------------------------------------------------------\n        # FF PART\n        # --------------------------------------------------------------------\n        if self.use_dropout > 0.0:\n            sentence_tensor = self.dropout(sentence_tensor)\n        if self.use_word_dropout > 0.0:\n            sentence_tensor = self.word_dropout(sentence_tensor)\n        if self.use_locked_dropout > 0.0:\n            sentence_tensor = self.locked_dropout(sentence_tensor)\n\n        if self.reproject_embeddings:\n            sentence_tensor = self.embedding2nn(sentence_tensor)\n\n        if self.use_rnn:\n            packed = torch.nn.utils.rnn.pack_padded_sequence(\n                sentence_tensor, lengths, enforce_sorted=False, batch_first=True\n            )\n\n            # if initial hidden state is trainable, use this state\n            if self.train_initial_hidden_state:\n                initial_hidden_state = [\n                    self.lstm_init_h.unsqueeze(1).repeat(1, len(sentences), 1),\n                    self.lstm_init_c.unsqueeze(1).repeat(1, len(sentences), 1),\n                ]\n                rnn_output, hidden = self.rnn(packed, initial_hidden_state)\n            else:\n                rnn_output, hidden = self.rnn(packed)\n\n            sentence_tensor, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(\n                rnn_output, batch_first=True\n            )\n\n            if self.use_dropout > 0.0:\n                sentence_tensor = self.dropout(sentence_tensor)\n            # word dropout only before LSTM - TODO: more experimentation needed\n            # if self.use_word_dropout > 0.0:\n            #     sentence_tensor = self.word_dropout(sentence_tensor)\n            if self.use_locked_dropout > 0.0:\n                sentence_tensor = self.locked_dropout(sentence_tensor)\n\n        features = self.linear(sentence_tensor)\n\n        return features\n\n    def _score_sentence(self, feats, tags, lens_):\n\n        start = torch.tensor(\n            [self.tag_dictionary.get_idx_for_item(START_TAG)], device=flair.device\n        )\n        start = start[None, :].repeat(tags.shape[0], 1)\n\n        stop = torch.tensor(\n            [self.tag_dictionary.get_idx_for_item(STOP_TAG)], device=flair.device\n        )\n        stop = stop[None, :].repeat(tags.shape[0], 1)\n\n        pad_start_tags = torch.cat([start, tags], 1)\n        pad_stop_tags = torch.cat([tags, stop], 1)\n\n        for i in range(len(lens_)):\n            pad_stop_tags[i, lens_[i] :] = self.tag_dictionary.get_idx_for_item(\n                STOP_TAG\n            )\n\n        score = torch.FloatTensor(feats.shape[0]).to(flair.device)\n\n        for i in range(feats.shape[0]):\n            r = torch.LongTensor(range(lens_[i])).to(flair.device)\n\n            score[i] = torch.sum(\n                self.transitions[\n                    pad_stop_tags[i, : lens_[i] + 1], pad_start_tags[i, : lens_[i] + 1]\n                ]\n            ) + torch.sum(feats[i, r, tags[i, : lens_[i]]])\n\n        return score\n\n    def _calculate_loss(\n        self, features: torch.tensor, sentences: List[Sentence]\n    ) -> float:\n\n        lengths: List[int] = [len(sentence.tokens) for sentence in sentences]\n\n        tag_list: List = []\n        for s_id, sentence in enumerate(sentences):\n            # get the tags in this sentence\n            tag_idx: List[int] = [\n                self.tag_dictionary.get_idx_for_item(token.get_tag(self.tag_type).value)\n                for token in sentence\n            ]\n            # add tags as tensor\n            tag = torch.tensor(tag_idx, device=flair.device)\n            tag_list.append(tag)\n\n        if self.use_crf:\n            # pad tags if using batch-CRF decoder\n            tags, _ = pad_tensors(tag_list)\n\n            forward_score = self._forward_alg(features, lengths)\n            gold_score = self._score_sentence(features, tags, lengths)\n\n            score = forward_score - gold_score\n\n            return score.mean()\n\n        else:\n            score = 0\n            for sentence_feats, sentence_tags, sentence_length in zip(\n                features, tag_list, lengths\n            ):\n                sentence_feats = sentence_feats[:sentence_length]\n                score += torch.nn.functional.cross_entropy(\n                    sentence_feats, sentence_tags, weight=self.loss_weights\n                )\n            score /= len(features)\n            return score\n\n    def _obtain_labels(\n        self,\n        feature: torch.Tensor,\n        batch_sentences: List[Sentence],\n        transitions: Optional[np.ndarray],\n        get_all_tags: bool,\n    ) -> (List[List[Label]], List[List[List[Label]]]):\n        """"""\n        Returns a tuple of two lists:\n         - The first list corresponds to the most likely `Label` per token in each sentence.\n         - The second list contains a probability distribution over all `Labels` for each token\n           in a sentence for all sentences.\n        """"""\n\n        lengths: List[int] = [len(sentence.tokens) for sentence in batch_sentences]\n\n        tags = []\n        all_tags = []\n        feature = feature.cpu()\n        if self.use_crf:\n            feature = feature.numpy()\n        else:\n            for index, length in enumerate(lengths):\n                feature[index, length:] = 0\n            softmax_batch = F.softmax(feature, dim=2).cpu()\n            scores_batch, prediction_batch = torch.max(softmax_batch, dim=2)\n            feature = zip(softmax_batch, scores_batch, prediction_batch)\n\n        for feats, length in zip(feature, lengths):\n            if self.use_crf:\n                confidences, tag_seq, scores = self._viterbi_decode(\n                    feats=feats[:length],\n                    transitions=transitions,\n                    all_scores=get_all_tags,\n                )\n            else:\n                softmax, score, prediction = feats\n                confidences = score[:length].tolist()\n                tag_seq = prediction[:length].tolist()\n                scores = softmax[:length].tolist()\n\n            tags.append(\n                [\n                    Label(self.tag_dictionary.get_item_for_index(tag), conf)\n                    for conf, tag in zip(confidences, tag_seq)\n                ]\n            )\n\n            if get_all_tags:\n                all_tags.append(\n                    [\n                        [\n                            Label(\n                                self.tag_dictionary.get_item_for_index(score_id), score\n                            )\n                            for score_id, score in enumerate(score_dist)\n                        ]\n                        for score_dist in scores\n                    ]\n                )\n\n        return tags, all_tags\n\n    @staticmethod\n    def _softmax(x, axis):\n        # reduce raw values to avoid NaN during exp\n        x_norm = x - x.max(axis=axis, keepdims=True)\n        y = np.exp(x_norm)\n        return y / y.sum(axis=axis, keepdims=True)\n\n    def _viterbi_decode(\n        self, feats: np.ndarray, transitions: np.ndarray, all_scores: bool\n    ):\n        id_start = self.tag_dictionary.get_idx_for_item(START_TAG)\n        id_stop = self.tag_dictionary.get_idx_for_item(STOP_TAG)\n\n        backpointers = np.empty(shape=(feats.shape[0], self.tagset_size), dtype=np.int_)\n        backscores = np.empty(\n            shape=(feats.shape[0], self.tagset_size), dtype=np.float32\n        )\n\n        init_vvars = np.expand_dims(\n            np.repeat(-10000.0, self.tagset_size), axis=0\n        ).astype(np.float32)\n        init_vvars[0][id_start] = 0\n\n        forward_var = init_vvars\n        for index, feat in enumerate(feats):\n            # broadcasting will do the job of reshaping and is more efficient than calling repeat\n            next_tag_var = forward_var + transitions\n            bptrs_t = next_tag_var.argmax(axis=1)\n            viterbivars_t = next_tag_var[np.arange(bptrs_t.shape[0]), bptrs_t]\n            forward_var = viterbivars_t + feat\n            backscores[index] = forward_var\n            forward_var = forward_var[np.newaxis, :]\n            backpointers[index] = bptrs_t\n\n        terminal_var = forward_var.squeeze() + transitions[id_stop]\n        terminal_var[id_stop] = -10000.0\n        terminal_var[id_start] = -10000.0\n        best_tag_id = terminal_var.argmax()\n\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n\n        start = best_path.pop()\n        assert start == id_start\n        best_path.reverse()\n\n        best_scores_softmax = self._softmax(backscores, axis=1)\n        best_scores_np = np.max(best_scores_softmax, axis=1)\n\n        # default value\n        all_scores_np = np.zeros(0, dtype=np.float64)\n        if all_scores:\n            all_scores_np = best_scores_softmax\n            for index, (tag_id, tag_scores) in enumerate(zip(best_path, all_scores_np)):\n                if type(tag_id) != int and tag_id.item() != tag_scores.argmax():\n                    swap_index_score = tag_scores.argmax()\n                    (\n                        all_scores_np[index][tag_id.item()],\n                        all_scores_np[index][swap_index_score],\n                    ) = (\n                        all_scores_np[index][swap_index_score],\n                        all_scores_np[index][tag_id.item()],\n                    )\n                elif type(tag_id) == int and tag_id != tag_scores.argmax():\n                    swap_index_score = tag_scores.argmax()\n                    (\n                        all_scores_np[index][tag_id],\n                        all_scores_np[index][swap_index_score],\n                    ) = (\n                        all_scores_np[index][swap_index_score],\n                        all_scores_np[index][tag_id],\n                    )\n\n        return best_scores_np.tolist(), best_path, all_scores_np.tolist()\n\n    def _forward_alg(self, feats, lens_):\n\n        init_alphas = torch.FloatTensor(self.tagset_size).fill_(-10000.0)\n        init_alphas[self.tag_dictionary.get_idx_for_item(START_TAG)] = 0.0\n\n        forward_var = torch.zeros(\n            feats.shape[0],\n            feats.shape[1] + 1,\n            feats.shape[2],\n            dtype=torch.float,\n            device=flair.device,\n        )\n\n        forward_var[:, 0, :] = init_alphas[None, :].repeat(feats.shape[0], 1)\n\n        transitions = self.transitions.view(\n            1, self.transitions.shape[0], self.transitions.shape[1]\n        ).repeat(feats.shape[0], 1, 1)\n\n        for i in range(feats.shape[1]):\n            emit_score = feats[:, i, :]\n\n            tag_var = (\n                emit_score[:, :, None].repeat(1, 1, transitions.shape[2])\n                + transitions\n                + forward_var[:, i, :][:, :, None]\n                .repeat(1, 1, transitions.shape[2])\n                .transpose(2, 1)\n            )\n\n            max_tag_var, _ = torch.max(tag_var, dim=2)\n\n            tag_var = tag_var - max_tag_var[:, :, None].repeat(\n                1, 1, transitions.shape[2]\n            )\n\n            agg_ = torch.log(torch.sum(torch.exp(tag_var), dim=2))\n\n            cloned = forward_var.clone()\n            cloned[:, i + 1, :] = max_tag_var + agg_\n\n            forward_var = cloned\n\n        forward_var = forward_var[range(forward_var.shape[0]), lens_, :]\n\n        terminal_var = forward_var + self.transitions[\n            self.tag_dictionary.get_idx_for_item(STOP_TAG)\n        ][None, :].repeat(forward_var.shape[0], 1)\n\n        alpha = log_sum_exp_batch(terminal_var)\n\n        return alpha\n\n    @staticmethod\n    def _filter_empty_sentences(sentences: List[Sentence]) -> List[Sentence]:\n        filtered_sentences = [sentence for sentence in sentences if sentence.tokens]\n        if len(sentences) != len(filtered_sentences):\n            log.warning(\n                f""Ignore {len(sentences) - len(filtered_sentences)} sentence(s) with no tokens.""\n            )\n        return filtered_sentences\n\n    @staticmethod\n    def _filter_empty_string(texts: List[str]) -> List[str]:\n        filtered_texts = [text for text in texts if text]\n        if len(texts) != len(filtered_texts):\n            log.warning(\n                f""Ignore {len(texts) - len(filtered_texts)} string(s) with no tokens.""\n            )\n        return filtered_texts\n\n    @staticmethod\n    def _fetch_model(model_name) -> str:\n\n        model_map = {}\n\n        aws_resource_path_v04 = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4""\n        hu_path: str = ""https://nlp.informatik.hu-berlin.de/resources/models""\n\n        model_map[""ner""] = ""/"".join(\n            [aws_resource_path_v04, ""NER-conll03-english"", ""en-ner-conll03-v0.4.pt""]\n        )\n\n        model_map[""ner-fast""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""NER-conll03--h256-l1-b32-p3-0.5-%2Bglove%2Bnews-forward-fast%2Bnews-backward-fast-normal-locked0.5-word0.05--release_4"",\n                ""en-ner-fast-conll03-v0.4.pt"",\n            ]\n        )\n\n        model_map[""ner-ontonotes""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""release-ner-ontonotes-0"",\n                ""en-ner-ontonotes-v0.4.pt"",\n            ]\n        )\n\n        model_map[""ner-ontonotes-fast""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""release-ner-ontonotes-fast-0"",\n                ""en-ner-ontonotes-fast-v0.4.pt"",\n            ]\n        )\n\n        for key in [""ner-multi"", ""multi-ner""]:\n            model_map[key] = ""/"".join(\n                [\n                    aws_resource_path_v04,\n                    ""release-quadner-512-l2-multi-embed"",\n                    ""quadner-large.pt"",\n                ]\n            )\n\n        for key in [""ner-multi-fast"", ""multi-ner-fast""]:\n            model_map[key] = ""/"".join(\n                [aws_resource_path_v04, ""NER-multi-fast"", ""ner-multi-fast.pt""]\n            )\n\n        for key in [""ner-multi-fast-learn"", ""multi-ner-fast-learn""]:\n            model_map[key] = ""/"".join(\n                [\n                    aws_resource_path_v04,\n                    ""NER-multi-fast-evolve"",\n                    ""ner-multi-fast-learn.pt"",\n                ]\n            )\n\n        model_map[""upos""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""POS-ontonotes--h256-l1-b32-p3-0.5-%2Bglove%2Bnews-forward%2Bnews-backward-normal-locked0.5-word0.05--v0.4_0"",\n                ""en-pos-ontonotes-v0.4.pt"",\n            ]\n        )\n\n        model_map[""pos""] = ""/"".join(\n            [\n                hu_path,\n                ""release-pos-0"",\n                ""en-pos-ontonotes-v0.5.pt"",\n            ]\n        )\n\n        model_map[""upos-fast""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""release-pos-fast-0"",\n                ""en-pos-ontonotes-fast-v0.4.pt"",\n            ]\n        )\n\n        model_map[""pos-fast""] = ""/"".join(\n            [\n                hu_path,\n                ""release-pos-fast-0"",\n                ""en-pos-ontonotes-fast-v0.5.pt"",\n            ]\n        )\n\n        for key in [""pos-multi"", ""multi-pos""]:\n            model_map[key] = ""/"".join(\n                [\n                    aws_resource_path_v04,\n                    ""release-dodekapos-512-l2-multi"",\n                    ""pos-multi-v0.1.pt"",\n                ]\n            )\n\n        for key in [""pos-multi-fast"", ""multi-pos-fast""]:\n            model_map[key] = ""/"".join(\n                [aws_resource_path_v04, ""UPOS-multi-fast"", ""pos-multi-fast.pt""]\n            )\n\n        model_map[""frame""] = ""/"".join(\n            [aws_resource_path_v04, ""release-frame-1"", ""en-frame-ontonotes-v0.4.pt""]\n        )\n\n        model_map[""frame-fast""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""release-frame-fast-0"",\n                ""en-frame-ontonotes-fast-v0.4.pt"",\n            ]\n        )\n\n        model_map[""chunk""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""NP-conll2000--h256-l1-b32-p3-0.5-%2Bnews-forward%2Bnews-backward-normal-locked0.5-word0.05--v0.4_0"",\n                ""en-chunk-conll2000-v0.4.pt"",\n            ]\n        )\n\n        model_map[""chunk-fast""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""release-chunk-fast-0"",\n                ""en-chunk-conll2000-fast-v0.4.pt"",\n            ]\n        )\n\n        model_map[""da-pos""] = ""/"".join(\n            [aws_resource_path_v04, ""POS-danish"", ""da-pos-v0.1.pt""]\n        )\n\n        model_map[""da-ner""] = ""/"".join(\n            [aws_resource_path_v04, ""NER-danish"", ""da-ner-v0.1.pt""]\n        )\n\n        model_map[""de-pos""] = ""/"".join(\n            [hu_path, ""release-de-pos-0"", ""de-pos-ud-hdt-v0.5.pt""]\n        )\n\n        model_map[""de-pos-tweets""] = ""/"".join(\n            [\n                aws_resource_path_v04,\n                ""POS-fine-grained-german-tweets"",\n                ""de-pos-twitter-v0.1.pt"",\n            ]\n        )\n\n        model_map[""de-ner""] = ""/"".join(\n            [aws_resource_path_v04, ""release-de-ner-0"", ""de-ner-conll03-v0.4.pt""]\n        )\n\n        model_map[""de-ner-germeval""] = ""/"".join(\n            [aws_resource_path_v04, ""NER-germeval"", ""de-ner-germeval-0.4.1.pt""]\n        )\n\n        model_map[""fr-ner""] = ""/"".join(\n            [aws_resource_path_v04, ""release-fr-ner-0"", ""fr-ner-wikiner-0.4.pt""]\n        )\n        model_map[""nl-ner""] = ""/"".join(\n            [aws_resource_path_v04, ""NER-conll2002-dutch"", ""nl-ner-conll02-v0.1.pt""]\n        )\n        model_map[""ml-pos""] = ""https://raw.githubusercontent.com/qburst/models-repository/master/FlairMalayalamModels/malayalam-xpos-model.pt""\n        model_map[""ml-upos""] = ""https://raw.githubusercontent.com/qburst/models-repository/master/FlairMalayalamModels/malayalam-upos-model.pt""\n\n        cache_dir = Path(""models"")\n        if model_name in model_map:\n            model_name = cached_path(model_map[model_name], cache_dir=cache_dir)\n\n        # the historical German taggers by the @redewiegergabe project\n        if model_name == ""de-historic-indirect"":\n            model_file = Path(flair.cache_root)  / cache_dir / \'indirect\' / \'final-model.pt\'\n            if not model_file.exists():\n                cached_path(\'http://www.redewiedergabe.de/models/indirect.zip\', cache_dir=cache_dir)\n                unzip_file(Path(flair.cache_root)  / cache_dir / \'indirect.zip\', Path(flair.cache_root)  / cache_dir)\n            model_name = str(Path(flair.cache_root)  / cache_dir / \'indirect\' / \'final-model.pt\')\n\n        if model_name == ""de-historic-direct"":\n            model_file = Path(flair.cache_root)  / cache_dir / \'direct\' / \'final-model.pt\'\n            if not model_file.exists():\n                cached_path(\'http://www.redewiedergabe.de/models/direct.zip\', cache_dir=cache_dir)\n                unzip_file(Path(flair.cache_root)  / cache_dir / \'direct.zip\', Path(flair.cache_root)  / cache_dir)\n            model_name = str(Path(flair.cache_root)  / cache_dir / \'direct\' / \'final-model.pt\')\n\n        if model_name == ""de-historic-reported"":\n            model_file = Path(flair.cache_root)  / cache_dir / \'reported\' / \'final-model.pt\'\n            if not model_file.exists():\n                cached_path(\'http://www.redewiedergabe.de/models/reported.zip\', cache_dir=cache_dir)\n                unzip_file(Path(flair.cache_root)  / cache_dir / \'reported.zip\', Path(flair.cache_root)  / cache_dir)\n            model_name = str(Path(flair.cache_root)  / cache_dir / \'reported\' / \'final-model.pt\')\n\n        if model_name == ""de-historic-free-indirect"":\n            model_file = Path(flair.cache_root)  / cache_dir / \'freeIndirect\' / \'final-model.pt\'\n            if not model_file.exists():\n                cached_path(\'http://www.redewiedergabe.de/models/freeIndirect.zip\', cache_dir=cache_dir)\n                unzip_file(Path(flair.cache_root)  / cache_dir / \'freeIndirect.zip\', Path(flair.cache_root)  / cache_dir)\n            model_name = str(Path(flair.cache_root)  / cache_dir / \'freeIndirect\' / \'final-model.pt\')\n\n        return model_name\n\n    def get_transition_matrix(self):\n        data = []\n        for to_idx, row in enumerate(self.transitions):\n            for from_idx, column in enumerate(row):\n                row = [\n                    self.tag_dictionary.get_item_for_index(from_idx),\n                    self.tag_dictionary.get_item_for_index(to_idx),\n                    column.item(),\n                ]\n                data.append(row)\n            data.append([""----""])\n        print(tabulate(data, headers=[""FROM"", ""TO"", ""SCORE""]))\n\n    def __str__(self):\n        return super(flair.nn.Model, self).__str__().rstrip(\')\') + \\\n               f\'  (beta): {self.beta}\\n\' + \\\n               f\'  (weights): {self.weight_dict}\\n\' + \\\n               f\'  (weight_tensor) {self.loss_weights}\\n)\'\n'"
flair/models/similarity_learning_model.py,29,"b'from abc import abstractmethod\n\nimport flair\nfrom flair.data import DataPoint, DataPair\nfrom flair.embeddings import Embeddings\nfrom flair.datasets import DataLoader\nfrom flair.training_utils import Result\nfrom flair.training_utils import store_embeddings\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\nimport itertools\n\nfrom typing import Union, List\nfrom pathlib import Path\n\n\n# == similarity measures ==\nclass SimilarityMeasure:\n    @abstractmethod\n    def forward(self, x):\n        pass\n\n\n# helper class for ModelSimilarity\nclass SliceReshaper(flair.nn.Model):\n    def __init__(self, begin, end=None, shape=None):\n        super(SliceReshaper, self).__init__()\n        self.begin = begin\n        self.end = end\n        self.shape = shape\n\n    def forward(self, x):\n        x = x[:, self.begin] if self.end is None else x[:, self.begin : self.end]\n        x = x.view(-1, *self.shape) if self.shape is not None else x\n        return x\n\n\n# -- works with binary cross entropy loss --\nclass ModelSimilarity(SimilarityMeasure):\n    """"""\n    Similarity defined by the model. The model parameters are given by the first element of the pair.\n    The similarity is evaluated by doing the forward pass (inference) on the parametrized model with\n    the second element of the pair as input.\n    """"""\n\n    def __init__(self, model):\n        # model is a list of tuples (function, parameters), where parameters is a dict {param_name: param_extract_model}\n        self.model = model\n\n    def forward(self, x):\n\n        model_parameters = x[0]\n        model_inputs = x[1]\n\n        cur_outputs = model_inputs\n        for layer_model, parameter_map in self.model:\n            param_dict = {}\n            for param_name, param_slice_reshape in parameter_map.items():\n                if isinstance(param_slice_reshape, SliceReshaper):\n                    val = param_slice_reshape(model_parameters)\n                else:\n                    val = param_slice_reshape\n                param_dict[param_name] = val\n            cur_outputs = layer_model(cur_outputs, **param_dict)\n\n        return cur_outputs\n\n\n# -- works with ranking/triplet loss --\nclass CosineSimilarity(SimilarityMeasure):\n    """"""\n    Similarity defined by the cosine distance.\n    """"""\n\n    def forward(self, x):\n        input_modality_0 = x[0]\n        input_modality_1 = x[1]\n\n        # normalize the embeddings\n        input_modality_0_norms = torch.norm(input_modality_0, dim=-1, keepdim=True)\n        input_modality_1_norms = torch.norm(input_modality_1, dim=-1, keepdim=True)\n\n        return torch.matmul(\n            input_modality_0 / input_modality_0_norms,\n            (input_modality_1 / input_modality_1_norms).t(),\n        )\n\n\n# == similarity losses ==\nclass SimilarityLoss(nn.Module):\n    def __init__(self):\n        super(SimilarityLoss, self).__init__()\n\n    @abstractmethod\n    def forward(self, inputs, targets):\n        pass\n\n\nclass PairwiseBCELoss(SimilarityLoss):\n    """"""\n    Binary cross entropy between pair similarities and pair labels.\n    """"""\n\n    def __init__(self, balanced=False):\n        super(PairwiseBCELoss, self).__init__()\n        self.balanced = balanced\n\n    def forward(self, inputs, targets):\n        n = inputs.shape[0]\n        neg_targets = torch.ones_like(targets).to(flair.device) - targets\n        # we want that logits for corresponding pairs are high, and for non-corresponding low\n        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=""none"")\n        if self.balanced:\n            # TODO: this assumes eye matrix\n            weight_matrix = n * (targets / 2.0 + neg_targets / (2.0 * (n - 1)))\n            bce_loss *= weight_matrix\n        loss = bce_loss.mean()\n\n        return loss\n\n\nclass RankingLoss(SimilarityLoss):\n    """"""\n    Triplet ranking loss between pair similarities and pair labels.\n    """"""\n\n    def __init__(self, margin=0.1, direction_weights=[0.5, 0.5]):\n        super(RankingLoss, self).__init__()\n        self.margin = margin\n        self.direction_weights = direction_weights\n\n    def forward(self, inputs, targets):\n        n = inputs.shape[0]\n        neg_targets = torch.ones_like(targets) - targets\n        # loss matrices for two directions of alignment, from modality 0 => modality 1 and vice versa\n        ranking_loss_matrix_01 = neg_targets * F.relu(\n            self.margin + inputs - torch.diag(inputs).view(n, 1)\n        )\n        ranking_loss_matrix_10 = neg_targets * F.relu(\n            self.margin + inputs - torch.diag(inputs).view(1, n)\n        )\n        neg_targets_01_sum = torch.sum(neg_targets, dim=1)\n        neg_targets_10_sum = torch.sum(neg_targets, dim=0)\n        loss = self.direction_weights[0] * torch.mean(\n            torch.sum(ranking_loss_matrix_01 / neg_targets_01_sum, dim=1)\n        ) + self.direction_weights[1] * torch.mean(\n            torch.sum(ranking_loss_matrix_10 / neg_targets_10_sum, dim=0)\n        )\n\n        return loss\n\n\n# == similarity learner ==\nclass SimilarityLearner(flair.nn.Model):\n    def __init__(\n        self,\n        source_embeddings: Embeddings,\n        target_embeddings: Embeddings,\n        similarity_measure: SimilarityMeasure,\n        similarity_loss: SimilarityLoss,\n        eval_device=flair.device,\n        source_mapping: torch.nn.Module = None,\n        target_mapping: torch.nn.Module = None,\n        recall_at_points: List[int] = [1, 5, 10, 20],\n        recall_at_points_weights: List[float] = [0.4, 0.3, 0.2, 0.1],\n        interleave_embedding_updates: bool = False,\n    ):\n        super(SimilarityLearner, self).__init__()\n        self.source_embeddings: Embeddings = source_embeddings\n        self.target_embeddings: Embeddings = target_embeddings\n        self.source_mapping: torch.nn.Module = source_mapping\n        self.target_mapping: torch.nn.Module = target_mapping\n        self.similarity_measure: SimilarityMeasure = similarity_measure\n        self.similarity_loss: SimilarityLoss = similarity_loss\n        self.eval_device = eval_device\n        self.recall_at_points: List[int] = recall_at_points\n        self.recall_at_points_weights: List[float] = recall_at_points_weights\n        self.interleave_embedding_updates = interleave_embedding_updates\n\n        self.to(flair.device)\n\n    def _embed_source(self, data_points):\n\n        if type(data_points[0]) == DataPair:\n            data_points = [point.first for point in data_points]\n\n        self.source_embeddings.embed(data_points)\n\n        source_embedding_tensor = torch.stack(\n            [point.embedding for point in data_points]\n        ).to(flair.device)\n\n        if self.source_mapping is not None:\n            source_embedding_tensor = self.source_mapping(source_embedding_tensor)\n\n        return source_embedding_tensor\n\n    def _embed_target(self, data_points):\n\n        if type(data_points[0]) == DataPair:\n            data_points = [point.second for point in data_points]\n\n        self.target_embeddings.embed(data_points)\n\n        target_embedding_tensor = torch.stack(\n            [point.embedding for point in data_points]\n        ).to(flair.device)\n\n        if self.target_mapping is not None:\n            target_embedding_tensor = self.target_mapping(target_embedding_tensor)\n\n        return target_embedding_tensor\n\n    def get_similarity(self, modality_0_embeddings, modality_1_embeddings):\n        """"""\n        :param modality_0_embeddings: embeddings of first modality, a tensor of shape [n0, d0]\n        :param modality_1_embeddings: embeddings of second modality, a tensor of shape [n1, d1]\n        :return: a similarity matrix of shape [n0, n1]\n        """"""\n        return self.similarity_measure.forward(\n            [modality_0_embeddings, modality_1_embeddings]\n        )\n\n    def forward_loss(\n        self, data_points: Union[List[DataPoint], DataPoint]\n    ) -> torch.tensor:\n        mapped_source_embeddings = self._embed_source(data_points)\n        mapped_target_embeddings = self._embed_target(data_points)\n\n        if self.interleave_embedding_updates:\n            # 1/3 only source branch of model, 1/3 only target branch of model, 1/3 both\n            detach_modality_id = torch.randint(0, 3, (1,)).item()\n            if detach_modality_id == 0:\n                mapped_source_embeddings.detach()\n            elif detach_modality_id == 1:\n                mapped_target_embeddings.detach()\n\n        similarity_matrix = self.similarity_measure.forward(\n            (mapped_source_embeddings, mapped_target_embeddings)\n        )\n\n        def add_to_index_map(hashmap, key, val):\n            if key not in hashmap:\n                hashmap[key] = [val]\n            else:\n                hashmap[key] += [val]\n\n        index_map = {""first"": {}, ""second"": {}}\n        for data_point_id, data_point in enumerate(data_points):\n            add_to_index_map(index_map[""first""], str(data_point.first), data_point_id)\n            add_to_index_map(index_map[""second""], str(data_point.second), data_point_id)\n\n        targets = torch.zeros_like(similarity_matrix).to(flair.device)\n\n        for data_point in data_points:\n            first_indices = index_map[""first""][str(data_point.first)]\n            second_indices = index_map[""second""][str(data_point.second)]\n            for first_index, second_index in itertools.product(\n                first_indices, second_indices\n            ):\n                targets[first_index, second_index] = 1.0\n\n        loss = self.similarity_loss(similarity_matrix, targets)\n\n        return loss\n\n    def evaluate(\n        self,\n        data_loader: DataLoader,\n        out_path: Path = None,\n        embedding_storage_mode=""none"",\n    ) -> (Result, float):\n        # assumes that for each data pair there\'s at least one embedding per modality\n\n        with torch.no_grad():\n            # pre-compute embeddings for all targets in evaluation dataset\n            target_index = {}\n            all_target_embeddings = []\n            for data_points in data_loader:\n                target_inputs = []\n                for data_point in data_points:\n                    if str(data_point.second) not in target_index:\n                        target_index[str(data_point.second)] = len(target_index)\n                        target_inputs.append(data_point)\n                if target_inputs:\n                    all_target_embeddings.append(\n                        self._embed_target(target_inputs).to(self.eval_device)\n                    )\n                store_embeddings(data_points, embedding_storage_mode)\n            all_target_embeddings = torch.cat(all_target_embeddings, dim=0)  # [n0, d0]\n            assert len(target_index) == all_target_embeddings.shape[0]\n\n            ranks = []\n            for data_points in data_loader:\n                batch_embeddings = self._embed_source(data_points)\n\n                batch_source_embeddings = batch_embeddings.to(self.eval_device)\n                # compute the similarity\n                batch_similarity_matrix = self.similarity_measure.forward(\n                    [batch_source_embeddings, all_target_embeddings]\n                )\n\n                # sort the similarity matrix across modality 1\n                batch_modality_1_argsort = torch.argsort(\n                    batch_similarity_matrix, descending=True, dim=1\n                )\n\n                # get the ranks, so +1 to start counting ranks from 1\n                batch_modality_1_ranks = (\n                    torch.argsort(batch_modality_1_argsort, dim=1) + 1\n                )\n\n                batch_target_indices = [\n                    target_index[str(data_point.second)] for data_point in data_points\n                ]\n\n                batch_gt_ranks = batch_modality_1_ranks[\n                    torch.arange(batch_similarity_matrix.shape[0]),\n                    torch.tensor(batch_target_indices),\n                ]\n                ranks.extend(batch_gt_ranks.tolist())\n\n                store_embeddings(data_points, embedding_storage_mode)\n\n        ranks = np.array(ranks)\n        median_rank = np.median(ranks)\n        recall_at = {k: np.mean(ranks <= k) for k in self.recall_at_points}\n\n        results_header = [""Median rank""] + [\n            ""Recall@top"" + str(r) for r in self.recall_at_points\n        ]\n        results_header_str = ""\\t"".join(results_header)\n        epoch_results = [str(median_rank)] + [\n            str(recall_at[k]) for k in self.recall_at_points\n        ]\n        epoch_results_str = ""\\t"".join(epoch_results)\n        detailed_results = "", "".join(\n            [f""{h}={v}"" for h, v in zip(results_header, epoch_results)]\n        )\n\n        validated_measure = sum(\n            [\n                recall_at[r] * w\n                for r, w in zip(self.recall_at_points, self.recall_at_points_weights)\n            ]\n        )\n\n        return (\n            Result(\n                validated_measure,\n                results_header_str,\n                epoch_results_str,\n                detailed_results,\n            ),\n            0,\n        )\n\n    def _get_state_dict(self):\n        model_state = {\n            ""state_dict"": self.state_dict(),\n            ""input_modality_0_embedding"": self.source_embeddings,\n            ""input_modality_1_embedding"": self.target_embeddings,\n            ""similarity_measure"": self.similarity_measure,\n            ""similarity_loss"": self.similarity_loss,\n            ""source_mapping"": self.source_mapping,\n            ""target_mapping"": self.target_mapping,\n            ""eval_device"": self.eval_device,\n            ""recall_at_points"": self.recall_at_points,\n            ""recall_at_points_weights"": self.recall_at_points_weights,\n        }\n        return model_state\n\n    @staticmethod\n    def _init_model_with_state_dict(state):\n        # The conversion from old model\'s constructor interface\n        if ""input_embeddings"" in state:\n            state[""input_modality_0_embedding""] = state[""input_embeddings""][0]\n            state[""input_modality_1_embedding""] = state[""input_embeddings""][1]\n        model = SimilarityLearner(\n            source_embeddings=state[""input_modality_0_embedding""],\n            target_embeddings=state[""input_modality_1_embedding""],\n            source_mapping=state[""source_mapping""],\n            target_mapping=state[""target_mapping""],\n            similarity_measure=state[""similarity_measure""],\n            similarity_loss=state[""similarity_loss""],\n            eval_device=state[""eval_device""],\n            recall_at_points=state[""recall_at_points""],\n            recall_at_points_weights=state[""recall_at_points_weights""],\n        )\n\n        model.load_state_dict(state[""state_dict""])\n        return model\n'"
flair/models/text_classification_model.py,15,"b'import logging\nfrom pathlib import Path\nfrom typing import List, Union, Callable, Dict, Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data.dataset import Dataset\nfrom tqdm import tqdm\nimport numpy as np\n\nimport sklearn.metrics as metrics\nimport flair.nn\nimport flair.embeddings\nfrom flair.data import Dictionary, Sentence, Label, Token, space_tokenizer, DataPoint\nfrom flair.datasets import SentenceDataset, StringDataset, DataLoader\nfrom flair.file_utils import cached_path\nfrom flair.training_utils import (\n    convert_labels_to_one_hot,\n    Metric,\n    Result,\n    store_embeddings,\n)\n\nlog = logging.getLogger(""flair"")\n\n\nclass TextClassifier(flair.nn.Model):\n    """"""\n    Text Classification Model\n    The model takes word embeddings, puts them into an RNN to obtain a text representation, and puts the\n    text representation in the end into a linear layer to get the actual class label.\n    The model can handle single and multi class data sets.\n    """"""\n\n    def __init__(\n        self,\n        document_embeddings: flair.embeddings.DocumentEmbeddings,\n        label_dictionary: Dictionary,\n        label_type: str = None,\n        multi_label: bool = None,\n        multi_label_threshold: float = 0.5,\n        beta: float = 1.0,\n        loss_weights: Dict[str, float] = None,\n    ):\n        """"""\n        Initializes a TextClassifier\n        :param document_embeddings: embeddings used to embed each data point\n        :param label_dictionary: dictionary of labels you want to predict\n        :param multi_label: auto-detected by default, but you can set this to True to force multi-label prediction\n        or False to force single-label prediction\n        :param multi_label_threshold: If multi-label you can set the threshold to make predictions\n        :param beta: Parameter for F-beta score for evaluation and training annealing\n        :param loss_weights: Dictionary of weights for labels for the loss function\n        (if any label\'s weight is unspecified it will default to 1.0)\n        """"""\n\n        super(TextClassifier, self).__init__()\n\n        self.document_embeddings: flair.embeddings.DocumentRNNEmbeddings = document_embeddings\n        self.label_dictionary: Dictionary = label_dictionary\n        self.label_type = label_type\n\n        if multi_label is not None:\n            self.multi_label = multi_label\n        else:\n            self.multi_label = self.label_dictionary.multi_label\n\n        self.multi_label_threshold = multi_label_threshold\n\n        self.beta = beta\n\n        self.weight_dict = loss_weights\n        # Initialize the weight tensor\n        if loss_weights is not None:\n            n_classes = len(self.label_dictionary)\n            weight_list = [1. for i in range(n_classes)]\n            for i, tag in enumerate(self.label_dictionary.get_items()):\n                if tag in loss_weights.keys():\n                    weight_list[i] = loss_weights[tag]\n            self.loss_weights = torch.FloatTensor(weight_list).to(flair.device)\n        else:\n            self.loss_weights = None\n\n        self.decoder = nn.Linear(\n            self.document_embeddings.embedding_length, len(self.label_dictionary)\n        )\n\n        nn.init.xavier_uniform_(self.decoder.weight)\n\n        if self.multi_label:\n            self.loss_function = nn.BCEWithLogitsLoss(weight=self.loss_weights)\n        else:\n            self.loss_function = nn.CrossEntropyLoss(weight=self.loss_weights)\n\n        # auto-spawn on GPU if available\n        self.to(flair.device)\n\n    def forward(self, sentences):\n\n        self.document_embeddings.embed(sentences)\n\n        embedding_names = self.document_embeddings.get_names()\n\n        text_embedding_list = [\n            sentence.get_embedding(embedding_names).unsqueeze(0) for sentence in sentences\n        ]\n        text_embedding_tensor = torch.cat(text_embedding_list, 0).to(flair.device)\n\n        label_scores = self.decoder(text_embedding_tensor)\n\n        return label_scores\n\n    def _get_state_dict(self):\n        model_state = {\n            ""state_dict"": self.state_dict(),\n            ""document_embeddings"": self.document_embeddings,\n            ""label_dictionary"": self.label_dictionary,\n            ""multi_label"": self.multi_label,\n            ""beta"": self.beta,\n            ""weight_dict"": self.weight_dict,\n        }\n        return model_state\n\n    @staticmethod\n    def _init_model_with_state_dict(state):\n        beta = 1.0 if ""beta"" not in state.keys() else state[""beta""]\n        weights = None if ""weight_dict"" not in state.keys() else state[""weight_dict""]\n        label_type = None if ""label_type"" not in state.keys() else state[""label_type""]\n\n        model = TextClassifier(\n            document_embeddings=state[""document_embeddings""],\n            label_dictionary=state[""label_dictionary""],\n            label_type=label_type,\n            multi_label=state[""multi_label""],\n            beta=beta,\n            loss_weights=weights,\n        )\n\n        model.load_state_dict(state[""state_dict""])\n        return model\n\n    def forward_loss(\n        self, data_points: Union[List[Sentence], Sentence]\n    ) -> torch.tensor:\n\n        scores = self.forward(data_points)\n\n        return self._calculate_loss(scores, data_points)\n\n    def _calculate_loss(self, scores, data_points):\n\n        labels = self._labels_to_one_hot(data_points) if self.multi_label \\\n            else self._labels_to_indices(data_points)\n\n        return self.loss_function(scores, labels)\n\n    def predict(\n        self,\n        sentences: Union[List[Sentence], Sentence],\n        mini_batch_size: int = 32,\n        multi_class_prob: bool = False,\n        verbose: bool = False,\n        label_name: Optional[str] = None,\n        return_loss = False,\n        embedding_storage_mode=""none"",\n    ):\n        """"""\n        Predicts the class labels for the given sentences. The labels are directly added to the sentences.\n        :param sentences: list of sentences\n        :param mini_batch_size: mini batch size to use\n        :param multi_class_prob : return probability for all class for multiclass\n        :param verbose: set to True to display a progress bar\n        :param return_loss: set to True to return loss\n        :param label_name: set this to change the name of the label type that is predicted\n        :param embedding_storage_mode: default is \'none\' which is always best. Only set to \'cpu\' or \'gpu\' if\n        you wish to not only predict, but also keep the generated embeddings in CPU or GPU memory respectively.\n        \'gpu\' to store embeddings in GPU memory.\n        """"""\n        if label_name == None:\n            label_name = self.label_type if self.label_type is not None else \'class\'\n\n        with torch.no_grad():\n            if not sentences:\n                return sentences\n\n            if isinstance(sentences, DataPoint):\n                sentences = [sentences]\n\n            # filter empty sentences\n            if isinstance(sentences[0], Sentence):\n                sentences = [sentence for sentence in sentences if len(sentence) > 0]\n            if len(sentences) == 0: return sentences\n\n            # reverse sort all sequences by their length\n            rev_order_len_index = sorted(\n                range(len(sentences)), key=lambda k: len(sentences[k]), reverse=True\n            )\n\n            reordered_sentences: List[Union[DataPoint, str]] = [\n                sentences[index] for index in rev_order_len_index\n            ]\n\n            dataloader = DataLoader(\n                dataset=SentenceDataset(reordered_sentences), batch_size=mini_batch_size\n            )\n            # progress bar for verbosity\n            if verbose:\n                dataloader = tqdm(dataloader)\n\n            overall_loss = 0\n            batch_no = 0\n            for batch in dataloader:\n\n                batch_no += 1\n\n                if verbose:\n                    dataloader.set_description(f""Inferencing on batch {batch_no}"")\n\n                # stop if all sentences are empty\n                if not batch:\n                    continue\n\n                scores = self.forward(batch)\n\n                if return_loss:\n                    overall_loss += self._calculate_loss(scores, batch)\n\n                predicted_labels = self._obtain_labels(\n                    scores, predict_prob=multi_class_prob\n                )\n\n                for (sentence, labels) in zip(batch, predicted_labels):\n                    for label in labels:\n                        if self.multi_label:\n                            sentence.add_label(label_name, label.value, label.score)\n                        else:\n                            sentence.set_label(label_name, label.value, label.score)\n\n                # clearing token embeddings to save memory\n                store_embeddings(batch, storage_mode=embedding_storage_mode)\n\n            if return_loss:\n                return overall_loss / batch_no\n\n    def evaluate(\n        self,\n        sentences: Union[List[DataPoint], Dataset],\n        out_path: Union[str, Path] = None,\n        embedding_storage_mode: str = ""none"",\n        mini_batch_size: int = 32,\n        num_workers: int = 8,\n    ) -> (Result, float):\n\n        # read Dataset into data loader (if list of sentences passed, make Dataset first)\n        if not isinstance(sentences, Dataset):\n            sentences = SentenceDataset(sentences)\n        data_loader = DataLoader(sentences, batch_size=mini_batch_size, num_workers=num_workers)\n\n        # use scikit-learn to evaluate\n        y_true = []\n        y_pred = []\n\n        with torch.no_grad():\n            eval_loss = 0\n\n            lines: List[str] = []\n            batch_count: int = 0\n            for batch in data_loader:\n\n                batch_count += 1\n\n                # predict for batch\n                loss = self.predict(batch,\n                                    embedding_storage_mode=embedding_storage_mode,\n                                    mini_batch_size=mini_batch_size,\n                                    label_name=\'predicted\',\n                                    return_loss=True)\n\n                eval_loss += loss\n\n                sentences_for_batch = [sent.to_plain_string() for sent in batch]\n\n                true_values_for_batch = [sentence.get_labels(self.label_type) for sentence in batch]\n                predictions = [sentence.get_labels(\'predicted\') for sentence in batch]\n\n                for sentence, prediction, true_value in zip(\n                    sentences_for_batch,\n                    predictions,\n                    true_values_for_batch,\n                ):\n                    eval_line = ""{}\\t{}\\t{}\\n"".format(\n                        sentence, true_value, prediction\n                    )\n                    lines.append(eval_line)\n\n                for predictions_for_sentence, true_values_for_sentence in zip(\n                    predictions, true_values_for_batch\n                ):\n\n                    true_values_for_sentence = [label.value for label in true_values_for_sentence]\n                    predictions_for_sentence = [label.value for label in predictions_for_sentence]\n\n                    y_true_instance = np.zeros(len(self.label_dictionary), dtype=int)\n                    for i in range(len(self.label_dictionary)):\n                        if self.label_dictionary.get_item_for_index(i) in true_values_for_sentence:\n                            y_true_instance[i] = 1\n                    y_true.append(y_true_instance.tolist())\n\n                    y_pred_instance = np.zeros(len(self.label_dictionary), dtype=int)\n                    for i in range(len(self.label_dictionary)):\n                        if self.label_dictionary.get_item_for_index(i) in predictions_for_sentence:\n                            y_pred_instance[i] = 1\n                    y_pred.append(y_pred_instance.tolist())\n\n                store_embeddings(batch, embedding_storage_mode)\n\n            if out_path is not None:\n                with open(out_path, ""w"", encoding=""utf-8"") as outfile:\n                    outfile.write("""".join(lines))\n\n            # make ""classification report""\n            target_names = []\n            for i in range(len(self.label_dictionary)):\n                target_names.append(self.label_dictionary.get_item_for_index(i))\n            classification_report = metrics.classification_report(y_true, y_pred, digits=4,\n                                                                  target_names=target_names, zero_division=1)\n\n            # get scores\n            micro_f_score = round(metrics.fbeta_score(y_true, y_pred, beta=self.beta, average=\'micro\'), 4)\n            accuracy_score = round(metrics.accuracy_score(y_true, y_pred), 4)\n            macro_f_score = round(metrics.fbeta_score(y_true, y_pred, beta=self.beta, average=\'macro\'), 4)\n            precision_score = round(metrics.precision_score(y_true, y_pred, average=\'macro\'), 4)\n            recall_score = round(metrics.recall_score(y_true, y_pred, average=\'macro\'), 4)\n\n            detailed_result = (\n                    ""\\nResults:""\n                    f""\\n- F-score (micro) {micro_f_score}""\n                    f""\\n- F-score (macro) {macro_f_score}""\n                    f""\\n- Accuracy {accuracy_score}""\n                    \'\\n\\nBy class:\\n\' + classification_report\n            )\n\n            # line for log file\n            if not self.multi_label:\n                log_header = ""ACCURACY""\n                log_line = f""\\t{accuracy_score}""\n            else:\n                log_header = ""PRECISION\\tRECALL\\tF1\\tACCURACY""\n                log_line = f""{precision_score}\\t"" \\\n                           f""{recall_score}\\t"" \\\n                           f""{macro_f_score}\\t"" \\\n                           f""{accuracy_score}""\n\n            result = Result(\n                main_score=micro_f_score,\n                log_line=log_line,\n                log_header=log_header,\n                detailed_results=detailed_result,\n            )\n\n            eval_loss /= batch_count\n\n            return result, eval_loss\n\n    @staticmethod\n    def _filter_empty_sentences(sentences: List[Sentence]) -> List[Sentence]:\n        filtered_sentences = [sentence for sentence in sentences if sentence.tokens]\n        if len(sentences) != len(filtered_sentences):\n            log.warning(\n                ""Ignore {} sentence(s) with no tokens."".format(\n                    len(sentences) - len(filtered_sentences)\n                )\n            )\n        return filtered_sentences\n\n    def _obtain_labels(\n        self, scores: List[List[float]], predict_prob: bool = False\n    ) -> List[List[Label]]:\n        """"""\n        Predicts the labels of sentences.\n        :param scores: the prediction scores from the model\n        :return: list of predicted labels\n        """"""\n\n        if self.multi_label:\n            return [self._get_multi_label(s) for s in scores]\n\n        elif predict_prob:\n            return [self._predict_label_prob(s) for s in scores]\n\n        return [self._get_single_label(s) for s in scores]\n\n    def _get_multi_label(self, label_scores) -> List[Label]:\n        labels = []\n\n        sigmoid = torch.nn.Sigmoid()\n\n        results = list(map(lambda x: sigmoid(x), label_scores))\n        for idx, conf in enumerate(results):\n            if conf > self.multi_label_threshold:\n                label = self.label_dictionary.get_item_for_index(idx)\n                labels.append(Label(label, conf.item()))\n\n        return labels\n\n    def _get_single_label(self, label_scores) -> List[Label]:\n        softmax = torch.nn.functional.softmax(label_scores, dim=0)\n        conf, idx = torch.max(softmax, 0)\n        label = self.label_dictionary.get_item_for_index(idx.item())\n\n        return [Label(label, conf.item())]\n\n    def _predict_label_prob(self, label_scores) -> List[Label]:\n        softmax = torch.nn.functional.softmax(label_scores, dim=0)\n        label_probs = []\n        for idx, conf in enumerate(softmax):\n            label = self.label_dictionary.get_item_for_index(idx)\n            label_probs.append(Label(label, conf.item()))\n        return label_probs\n\n    def _labels_to_one_hot(self, sentences: List[Sentence]):\n\n        label_list = []\n        for sentence in sentences:\n            label_list.append([label.value for label in sentence.get_labels(self.label_type)])\n\n        one_hot = convert_labels_to_one_hot(label_list, self.label_dictionary)\n        one_hot = [torch.FloatTensor(l).unsqueeze(0) for l in one_hot]\n        one_hot = torch.cat(one_hot, 0).to(flair.device)\n        return one_hot\n\n    def _labels_to_indices(self, sentences: List[Sentence]):\n\n        indices = [\n            torch.LongTensor(\n                [\n                    self.label_dictionary.get_idx_for_item(label.value)\n                    for label in sentence.get_labels(self.label_type)\n                ]\n            )\n            for sentence in sentences\n        ]\n\n        vec = torch.cat(indices, 0).to(flair.device)\n\n        return vec\n\n    @staticmethod\n    def _fetch_model(model_name) -> str:\n\n        model_map = {}\n        aws_resource_path = ""https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4""\n        hu_path: str = ""https://nlp.informatik.hu-berlin.de/resources/models""\n\n        model_map[""de-offensive-language""] = ""/"".join(\n            [\n                aws_resource_path,\n                ""classy-offensive-de-rnn-cuda%3A0"",\n                ""germ-eval-2018-task-1-v0.4.pt"",\n            ]\n        )\n\n        # English sentiment models\n        model_map[""sentiment""] = ""/"".join(\n            [hu_path, ""sentiment-curated-distilbert"", ""sentiment-en-mix-distillbert.pt""]\n        )\n        model_map[""en-sentiment""] = ""/"".join(\n            [hu_path, ""sentiment-curated-distilbert"", ""sentiment-en-mix-distillbert.pt""]\n        )\n        model_map[""sentiment-fast""] = ""/"".join(\n            [hu_path, ""sentiment-curated-fasttext-rnn"", ""sentiment-en-mix-ft-rnn.pt""]\n        )\n\n        cache_dir = Path(""models"")\n        if model_name in model_map:\n            model_name = cached_path(model_map[model_name], cache_dir=cache_dir)\n\n        return model_name\n\n    def __str__(self):\n        return super(flair.nn.Model, self).__str__().rstrip(\')\') + \\\n               f\'  (beta): {self.beta}\\n\' + \\\n               f\'  (weights): {self.weight_dict}\\n\' + \\\n               f\'  (weight_tensor) {self.loss_weights}\\n)\'\n'"
flair/models/text_regression_model.py,10,"b'from pathlib import Path\n\nfrom torch.utils.data.dataset import Dataset\n\nimport flair\nimport flair.embeddings\nimport torch\nimport torch.nn as nn\nfrom typing import List, Union\n\nfrom flair.datasets import DataLoader, SentenceDataset\nfrom flair.training_utils import MetricRegression, Result, store_embeddings\nfrom flair.data import Sentence, Label, DataPoint\nimport logging\n\nlog = logging.getLogger(""flair"")\n\n\nclass TextRegressor(flair.models.TextClassifier):\n    def __init__(self, document_embeddings: flair.embeddings.DocumentEmbeddings):\n\n        super(TextRegressor, self).__init__(\n            document_embeddings=document_embeddings,\n            label_dictionary=flair.data.Dictionary(),\n            multi_label=False,\n        )\n\n        log.info(""Using REGRESSION - experimental"")\n\n        self.loss_function = nn.MSELoss()\n\n    def _labels_to_indices(self, sentences: List[Sentence]):\n        indices = [\n            torch.tensor(\n                [float(label.value) for label in sentence.labels], dtype=torch.float\n            )\n            for sentence in sentences\n        ]\n\n        vec = torch.cat(indices, 0).to(flair.device)\n\n        return vec\n\n    def predict(\n        self,\n        sentences: Union[Sentence, List[Sentence]],\n        mini_batch_size: int = 32,\n        embedding_storage_mode=""none"",\n    ) -> List[Sentence]:\n\n        with torch.no_grad():\n            if type(sentences) is Sentence:\n                sentences = [sentences]\n\n            filtered_sentences = self._filter_empty_sentences(sentences)\n\n            # remove previous embeddings\n            store_embeddings(filtered_sentences, ""none"")\n\n            batches = [\n                filtered_sentences[x : x + mini_batch_size]\n                for x in range(0, len(filtered_sentences), mini_batch_size)\n            ]\n\n            for batch in batches:\n                scores = self.forward(batch)\n\n                for (sentence, score) in zip(batch, scores.tolist()):\n                    sentence.labels = [Label(value=str(score[0]))]\n\n                # clearing token embeddings to save memory\n                store_embeddings(batch, storage_mode=embedding_storage_mode)\n\n            return sentences\n\n    def _calculate_loss(\n        self, scores: torch.tensor, sentences: List[Sentence]\n    ) -> torch.tensor:\n        """"""\n        Calculates the loss.\n        :param scores: the prediction scores from the model\n        :param sentences: list of sentences\n        :return: loss value\n        """"""\n        return self.loss_function(scores.squeeze(1), self._labels_to_indices(sentences))\n\n    def forward_labels_and_loss(\n        self, sentences: Union[Sentence, List[Sentence]]\n    ) -> (List[List[float]], torch.tensor):\n\n        scores = self.forward(sentences)\n        loss = self._calculate_loss(scores, sentences)\n        return scores, loss\n\n    def evaluate(\n        self,\n        sentences: Union[List[DataPoint], Dataset],\n        out_path: Union[str, Path] = None,\n        embedding_storage_mode: str = ""none"",\n        mini_batch_size: int = 32,\n        num_workers: int = 8,\n    ) -> (Result, float):\n\n        # read Dataset into data loader (if list of sentences passed, make Dataset first)\n        if not isinstance(sentences, Dataset):\n            sentences = SentenceDataset(sentences)\n        data_loader = DataLoader(sentences, batch_size=mini_batch_size, num_workers=num_workers)\n\n        with torch.no_grad():\n            eval_loss = 0\n\n            metric = MetricRegression(""Evaluation"")\n\n            lines: List[str] = []\n            total_count = 0\n            for batch_nr, batch in enumerate(data_loader):\n\n                if isinstance(batch, Sentence):\n                    batch = [batch]\n\n                scores, loss = self.forward_labels_and_loss(batch)\n\n                true_values = []\n                for sentence in batch:\n                    total_count += 1\n                    for label in sentence.labels:\n                        true_values.append(float(label.value))\n\n                results = []\n                for score in scores:\n                    if type(score[0]) is Label:\n                        results.append(float(score[0].score))\n                    else:\n                        results.append(float(score[0]))\n\n                eval_loss += loss\n\n                metric.true.extend(true_values)\n                metric.pred.extend(results)\n\n                for sentence, prediction, true_value in zip(\n                    batch, results, true_values\n                ):\n                    eval_line = ""{}\\t{}\\t{}\\n"".format(\n                        sentence.to_original_text(), true_value, prediction\n                    )\n                    lines.append(eval_line)\n\n                store_embeddings(batch, embedding_storage_mode)\n\n            eval_loss /= total_count\n\n            ##TODO: not saving lines yet\n            if out_path is not None:\n                with open(out_path, ""w"", encoding=""utf-8"") as outfile:\n                    outfile.write("""".join(lines))\n\n            log_line = f""{metric.mean_squared_error()}\\t{metric.spearmanr()}\\t{metric.pearsonr()}""\n            log_header = ""MSE\\tSPEARMAN\\tPEARSON""\n\n            detailed_result = (\n                f""AVG: mse: {metric.mean_squared_error():.4f} - ""\n                f""mae: {metric.mean_absolute_error():.4f} - ""\n                f""pearson: {metric.pearsonr():.4f} - ""\n                f""spearman: {metric.spearmanr():.4f}""\n            )\n\n            result: Result = Result(\n                metric.pearsonr(), log_header, log_line, detailed_result\n            )\n\n            return result, eval_loss\n\n    def _get_state_dict(self):\n        model_state = {\n            ""state_dict"": self.state_dict(),\n            ""document_embeddings"": self.document_embeddings,\n        }\n        return model_state\n\n    @staticmethod\n    def _init_model_with_state_dict(state):\n\n        model = TextRegressor(document_embeddings=state[""document_embeddings""])\n\n        model.load_state_dict(state[""state_dict""])\n        return model\n'"
flair/trainers/__init__.py,0,b'from .trainer import ModelTrainer\n'
flair/trainers/language_model_trainer.py,9,"b'import time, datetime\nimport random\nimport sys\nfrom pathlib import Path\nfrom typing import Union\n\nfrom torch import cuda\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.sgd import SGD\n\ntry:\n    from apex import amp\nexcept ImportError:\n    amp = None\n\nimport flair\nfrom flair.data import Dictionary\nfrom flair.models import LanguageModel\nfrom flair.optim import *\nfrom flair.training_utils import add_file_handler\n\nlog = logging.getLogger(""flair"")\n\n\nclass TextDataset(Dataset):\n    def __init__(\n        self,\n        path: Union[str, Path],\n        dictionary: Dictionary,\n        expand_vocab: bool = False,\n        forward: bool = True,\n        split_on_char: bool = True,\n        random_case_flip: bool = True,\n        document_delimiter: str = \'\\n\',\n        shuffle: bool = True,\n    ):\n        if type(path) is str:\n            path = Path(path)\n        assert path.exists()\n\n        self.files = None\n        self.path = path\n        self.dictionary = dictionary\n        self.split_on_char = split_on_char\n        self.forward = forward\n        self.random_case_flip = random_case_flip\n        self.expand_vocab = expand_vocab\n        self.document_delimiter = document_delimiter\n        self.shuffle = shuffle\n\n        if path.is_dir():\n            self.files = sorted([f for f in path.iterdir() if f.exists()])\n        else:\n            self.files = [path]\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index=0) -> torch.tensor:\n        return self.charsplit(\n            self.files[index],\n            self.expand_vocab,\n            self.forward,\n            self.split_on_char,\n            self.random_case_flip,\n        )\n\n    def charsplit(\n        self,\n        path: Union[str, Path],\n        expand_vocab=False,\n        forward=True,\n        split_on_char=True,\n        random_case_flip=True,\n    ) -> torch.tensor:\n\n        """"""Tokenizes a text file on character basis.""""""\n        if type(path) is str:\n            path = Path(path)\n        assert path.exists()\n\n        lines = [doc + self.document_delimiter\n                 for doc in open(path, ""r"", encoding=""utf-8"").read().split(self.document_delimiter) if doc]\n\n        log.info(f""read text file with {len(lines)} lines"")\n        if self.shuffle:\n            random.shuffle(lines)\n            log.info(f""shuffled"")\n\n        tokens = 0\n        for line in lines:\n\n            if split_on_char:\n                chars = list(line)\n            else:\n                chars = line.split()\n\n            tokens += len(chars)\n\n            # Add chars to the dictionary\n            if expand_vocab:\n                for char in chars:\n                    self.dictionary.add_item(char)\n\n        ids = torch.zeros(tokens, dtype=torch.long)\n        if forward:\n            # charsplit file content\n            token = 0\n            for line in lines:\n                if random_case_flip:\n                    line = self.random_casechange(line)\n\n                if split_on_char:\n                    chars = list(line)\n                else:\n                    chars = line.split()\n\n                for char in chars:\n                    if token >= tokens:\n                        break\n                    ids[token] = self.dictionary.get_idx_for_item(char)\n                    token += 1\n        else:\n            # charsplit file content\n            token = tokens - 1\n            for line in lines:\n                if random_case_flip:\n                    line = self.random_casechange(line)\n\n                if split_on_char:\n                    chars = list(line)\n                else:\n                    chars = line.split()\n\n                for char in chars:\n                    if token >= tokens:\n                        break\n                    ids[token] = self.dictionary.get_idx_for_item(char)\n                    token -= 1\n\n        return ids\n\n    @staticmethod\n    def random_casechange(line: str) -> str:\n        no = random.randint(0, 99)\n        if no == 0:\n            line = line.lower()\n        if no == 1:\n            line = line.upper()\n        return line\n\n    def tokenize(self, path: Union[str, Path]):\n        """"""Tokenizes a text file.""""""\n        if type(path) is str:\n            path = Path(path)\n        assert path.exists()\n        # Add words to the dictionary\n        with open(path, ""r"") as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + [""<eos>""]\n                tokens += len(words)\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, ""r"") as f:\n            ids = torch.zeros(tokens, dtype=torch.long, device=flair.device)\n            token = 0\n            for line in f:\n                words = line.split() + [""<eos>""]\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return ids\n\n\nclass TextCorpus(object):\n    def __init__(\n        self,\n        path: Union[Path, str],\n        dictionary: Dictionary,\n        forward: bool = True,\n        character_level: bool = True,\n        random_case_flip: bool = True,\n        document_delimiter: str = \'\\n\',\n    ):\n        self.dictionary: Dictionary = dictionary\n        self.forward = forward\n        self.split_on_char = character_level\n        self.random_case_flip = random_case_flip\n        self.document_delimiter: str = document_delimiter\n\n        if type(path) == str:\n            path = Path(path)\n\n        self.train = TextDataset(\n            path / ""train"",\n            dictionary,\n            False,\n            self.forward,\n            self.split_on_char,\n            self.random_case_flip,\n            document_delimiter=self.document_delimiter,\n            shuffle=True,\n        )\n\n        # TextDataset returns a list. valid and test are only one file, so return the first element\n        self.valid = TextDataset(\n            path / ""valid.txt"",\n            dictionary,\n            False,\n            self.forward,\n            self.split_on_char,\n            self.random_case_flip,\n            document_delimiter=document_delimiter,\n            shuffle=False,\n        )[0]\n        self.test = TextDataset(\n            path / ""test.txt"",\n            dictionary,\n            False,\n            self.forward,\n            self.split_on_char,\n            self.random_case_flip,\n            document_delimiter=document_delimiter,\n            shuffle=False,\n        )[0]\n\n\nclass LanguageModelTrainer:\n    def __init__(\n        self,\n        model: LanguageModel,\n        corpus: TextCorpus,\n        optimizer: Optimizer = SGD,\n        test_mode: bool = False,\n        epoch: int = 0,\n        split: int = 0,\n        loss: float = 10000,\n        optimizer_state: dict = None,\n    ):\n        self.model: LanguageModel = model\n        self.optimizer: Optimizer = optimizer\n        self.corpus: TextCorpus = corpus\n        self.test_mode: bool = test_mode\n\n        self.loss_function = torch.nn.CrossEntropyLoss()\n        self.log_interval = 100\n        self.epoch = epoch\n        self.split = split\n        self.loss = loss\n        self.optimizer_state = optimizer_state\n\n    def train(\n        self,\n        base_path: Union[Path, str],\n        sequence_length: int,\n        learning_rate: float = 20,\n        mini_batch_size: int = 100,\n        anneal_factor: float = 0.25,\n        patience: int = 10,\n        clip=0.25,\n        max_epochs: int = 1000,\n        checkpoint: bool = False,\n        grow_to_sequence_length: int = 0,\n        num_workers: int = 2,\n        use_amp: bool = False,\n        amp_opt_level: str = ""O1"",\n        **kwargs,\n    ):\n\n        if use_amp:\n            if sys.version_info < (3, 0):\n                raise RuntimeError(""Apex currently only supports Python 3. Aborting."")\n            if amp is None:\n                raise RuntimeError(\n                    ""Failed to import apex. Please install apex from https://www.github.com/nvidia/apex ""\n                    ""to enable mixed-precision training.""\n                )\n\n        # cast string to Path\n        if type(base_path) is str:\n            base_path = Path(base_path)\n\n        add_file_handler(log, base_path / ""training.log"")\n\n        number_of_splits: int = len(self.corpus.train)\n\n        val_data = self._batchify(self.corpus.valid, mini_batch_size)\n\n        # error message if the validation dataset is too small\n        if val_data.size(0) == 1:\n            raise RuntimeError(\n                f""ERROR: Your validation dataset is too small. For your mini_batch_size, the data needs to ""\n                f""consist of at least {mini_batch_size * 2} characters!""\n            )\n\n        base_path.mkdir(parents=True, exist_ok=True)\n        loss_txt = base_path / ""loss.txt""\n        savefile = base_path / ""best-lm.pt""\n\n        try:\n            best_val_loss = self.loss\n            optimizer = self.optimizer(\n                self.model.parameters(), lr=learning_rate, **kwargs\n            )\n            if self.optimizer_state is not None:\n                optimizer.load_state_dict(self.optimizer_state)\n\n            if isinstance(optimizer, (AdamW, SGDW)):\n                scheduler: ReduceLRWDOnPlateau = ReduceLRWDOnPlateau(\n                    optimizer, verbose=True, factor=anneal_factor, patience=patience\n                )\n            else:\n                scheduler: ReduceLROnPlateau = ReduceLROnPlateau(\n                    optimizer, verbose=True, factor=anneal_factor, patience=patience\n                )\n\n            if use_amp:\n                self.model, optimizer = amp.initialize(\n                    self.model, optimizer, opt_level=amp_opt_level\n                )\n\n            training_generator = DataLoader(\n                self.corpus.train, shuffle=False, num_workers=num_workers\n            )\n\n            for epoch in range(self.epoch, max_epochs):\n                epoch_start_time = time.time()\n                # Shuffle training files randomly after serially iterating through corpus one\n                if epoch > 0:\n                    training_generator = DataLoader(\n                        self.corpus.train, shuffle=True, num_workers=num_workers\n                    )\n                    self.model.save_checkpoint(\n                        base_path / f""epoch_{epoch}.pt"",\n                        optimizer,\n                        epoch,\n                        0,\n                        best_val_loss,\n                    )\n\n                # iterate through training data, starting at self.split (for checkpointing)\n                for curr_split, train_slice in enumerate(\n                    training_generator, self.split\n                ):\n\n                    if sequence_length < grow_to_sequence_length:\n                        sequence_length += 1\n                    log.info(f""Sequence length is {sequence_length}"")\n\n                    split_start_time = time.time()\n                    # off by one for printing\n                    curr_split += 1\n                    train_data = self._batchify(train_slice.flatten(), mini_batch_size)\n\n                    log.info(\n                        ""Split %d"" % curr_split\n                        + ""\\t - ({:%H:%M:%S})"".format(datetime.datetime.now())\n                    )\n\n                    for group in optimizer.param_groups:\n                        learning_rate = group[""lr""]\n\n                    # go into train mode\n                    self.model.train()\n\n                    # reset variables\n                    hidden = self.model.init_hidden(mini_batch_size)\n\n                    # not really sure what this does\n                    ntokens = len(self.corpus.dictionary)\n\n                    total_loss = 0\n                    start_time = time.time()\n\n                    for batch, i in enumerate(\n                        range(0, train_data.size(0) - 1, sequence_length)\n                    ):\n                        data, targets = self._get_batch(train_data, i, sequence_length)\n\n                        if not data.is_cuda and cuda.is_available():\n                            log.info(\n                                ""Batch %d is not on CUDA, training will be very slow""\n                                % (batch)\n                            )\n                            raise Exception(""data isnt on cuda"")\n\n                        self.model.zero_grad()\n                        optimizer.zero_grad()\n\n                        # do the forward pass in the model\n                        output, rnn_output, hidden = self.model.forward(data, hidden)\n\n                        # try to predict the targets\n                        loss = self.loss_function(output.view(-1, ntokens), targets)\n                        # Backward\n                        if use_amp:\n                            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                                scaled_loss.backward()\n                        else:\n                            loss.backward()\n\n                        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip)\n\n                        optimizer.step()\n\n                        total_loss += loss.data\n\n                        # We detach the hidden state from how it was previously produced.\n                        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n                        hidden = self._repackage_hidden(hidden)\n\n                        # explicitly remove loss to clear up memory\n                        del loss, output, rnn_output\n\n                        if batch % self.log_interval == 0 and batch > 0:\n                            cur_loss = total_loss.item() / self.log_interval\n                            elapsed = time.time() - start_time\n                            log.info(\n                                ""| split {:3d} /{:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | ""\n                                ""loss {:5.2f} | ppl {:8.2f}"".format(\n                                    curr_split,\n                                    number_of_splits,\n                                    batch,\n                                    len(train_data) // sequence_length,\n                                    elapsed * 1000 / self.log_interval,\n                                    cur_loss,\n                                    math.exp(cur_loss),\n                                )\n                            )\n                            total_loss = 0\n                            start_time = time.time()\n\n                    log.info(\n                        ""%d seconds for train split %d""\n                        % (time.time() - split_start_time, curr_split)\n                    )\n\n                    ###############################################################################\n                    self.model.eval()\n\n                    val_loss = self.evaluate(val_data, mini_batch_size, sequence_length)\n                    scheduler.step(val_loss)\n\n                    log.info(""best loss so far {:5.2f}"".format(best_val_loss))\n\n                    log.info(self.model.generate_text())\n\n                    if checkpoint:\n                        self.model.save_checkpoint(\n                            base_path / ""checkpoint.pt"",\n                            optimizer,\n                            epoch,\n                            curr_split,\n                            best_val_loss,\n                        )\n\n                    # Save the model if the validation loss is the best we\'ve seen so far.\n                    if val_loss < best_val_loss:\n                        self.model.best_score = best_val_loss\n                        self.model.save(savefile)\n                        best_val_loss = val_loss\n\n                    ###############################################################################\n                    # print info\n                    ###############################################################################\n                    log.info(""-"" * 89)\n\n                    summary = (\n                        ""| end of split {:3d} /{:3d} | epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | ""\n                        ""valid ppl {:8.2f} | learning rate {:3.4f}"".format(\n                            curr_split,\n                            number_of_splits,\n                            epoch + 1,\n                            (time.time() - split_start_time),\n                            val_loss,\n                            math.exp(val_loss),\n                            learning_rate,\n                        )\n                    )\n\n                    with open(loss_txt, ""a"") as myfile:\n                        myfile.write(""%s\\n"" % summary)\n\n                    log.info(summary)\n                    log.info(""-"" * 89)\n\n                log.info(""Epoch time: %.2f"" % (time.time() - epoch_start_time))\n\n        except KeyboardInterrupt:\n            log.info(""-"" * 89)\n            log.info(""Exiting from training early"")\n\n        ###############################################################################\n        # final testing\n        ###############################################################################\n        test_data = self._batchify(self.corpus.test, mini_batch_size)\n        test_loss = self.evaluate(test_data, mini_batch_size, sequence_length)\n\n        summary = ""TEST: valid loss {:5.2f} | valid ppl {:8.2f}"".format(\n            test_loss, math.exp(test_loss)\n        )\n        with open(loss_txt, ""a"") as myfile:\n            myfile.write(""%s\\n"" % summary)\n\n        log.info(summary)\n        log.info(""-"" * 89)\n\n    def evaluate(self, data_source, eval_batch_size, sequence_length):\n        # Turn on evaluation mode which disables dropout.\n        self.model.eval()\n\n        with torch.no_grad():\n            total_loss = 0\n            ntokens = len(self.corpus.dictionary)\n\n            hidden = self.model.init_hidden(eval_batch_size)\n\n            for i in range(0, data_source.size(0) - 1, sequence_length):\n                data, targets = self._get_batch(data_source, i, sequence_length)\n                prediction, rnn_output, hidden = self.model.forward(data, hidden)\n                output_flat = prediction.view(-1, ntokens)\n                total_loss += len(data) * self.loss_function(output_flat, targets).data\n                hidden = self._repackage_hidden(hidden)\n            return total_loss.item() / len(data_source)\n\n    @staticmethod\n    def _batchify(data, batch_size):\n        # Work out how cleanly we can divide the dataset into bsz parts.\n        nbatch = data.size(0) // batch_size\n        # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n        data = data.narrow(0, 0, nbatch * batch_size)\n        # Evenly divide the data across the bsz batches.\n        data = data.view(batch_size, -1).t().contiguous()\n        return data\n\n    @staticmethod\n    def _get_batch(source, i, sequence_length):\n        seq_len = min(sequence_length, len(source) - 1 - i)\n\n        data = source[i : i + seq_len].clone().detach()\n        target = source[i + 1 : i + 1 + seq_len].view(-1).clone().detach()\n\n        data = data.to(flair.device)\n        target = target.to(flair.device)\n\n        return data, target\n\n    @staticmethod\n    def _repackage_hidden(h):\n        """"""Wraps hidden states in new tensors, to detach them from their history.""""""\n        return tuple(v.clone().detach() for v in h)\n\n    @staticmethod\n    def load_from_checkpoint(\n        checkpoint_file: Union[str, Path], corpus: TextCorpus, optimizer: Optimizer = SGD\n    ):\n        if type(checkpoint_file) is str:\n            checkpoint_file = Path(checkpoint_file)\n\n        checkpoint = LanguageModel.load_checkpoint(checkpoint_file)\n        return LanguageModelTrainer(\n            checkpoint[""model""],\n            corpus,\n            optimizer,\n            epoch=checkpoint[""epoch""],\n            split=checkpoint[""split""],\n            loss=checkpoint[""loss""],\n            optimizer_state=checkpoint[""optimizer_state_dict""],\n        )\n'"
flair/trainers/trainer.py,13,"b'import copy\nimport logging\nfrom pathlib import Path\nfrom typing import Union\nimport time\nimport datetime\nimport sys\nimport inspect\n\nimport torch\nfrom torch.optim.sgd import SGD\nfrom torch.utils.data.dataset import ConcatDataset\n\ntry:\n    from apex import amp\nexcept ImportError:\n    amp = None\n\nimport flair\nimport flair.nn\nfrom flair.data import MultiCorpus, Corpus\nfrom flair.datasets import DataLoader\nfrom flair.optim import ExpAnnealLR\nfrom flair.training_utils import (\n    init_output_file,\n    WeightExtractor,\n    log_line,\n    add_file_handler,\n    Result,\n    store_embeddings,\n    AnnealOnPlateau,\n)\nfrom flair.models import SequenceTagger\nimport random\n\nlog = logging.getLogger(""flair"")\n\n\nclass ModelTrainer:\n    def __init__(\n        self,\n        model: flair.nn.Model,\n        corpus: Corpus,\n        optimizer: torch.optim.Optimizer = SGD,\n        epoch: int = 0,\n        use_tensorboard: bool = False,\n    ):\n        """"""\n        Initialize a model trainer\n        :param model: The model that you want to train. The model should inherit from flair.nn.Model\n        :param corpus: The dataset used to train the model, should be of type Corpus\n        :param optimizer: The optimizer to use (typically SGD or Adam)\n        :param epoch: The starting epoch (normally 0 but could be higher if you continue training model)\n        :param use_tensorboard: If True, writes out tensorboard information\n        """"""\n        self.model: flair.nn.Model = model\n        self.corpus: Corpus = corpus\n        self.optimizer: torch.optim.Optimizer = optimizer\n        self.epoch: int = epoch\n        self.use_tensorboard: bool = use_tensorboard\n\n    def train(\n        self,\n        base_path: Union[Path, str],\n        learning_rate: float = 0.1,\n        mini_batch_size: int = 32,\n        mini_batch_chunk_size: int = None,\n        max_epochs: int = 100,\n        scheduler = AnnealOnPlateau,\n        anneal_factor: float = 0.5,\n        patience: int = 3,\n        initial_extra_patience = 0,\n        min_learning_rate: float = 0.0001,\n        train_with_dev: bool = False,\n        monitor_train: bool = False,\n        monitor_test: bool = False,\n        embeddings_storage_mode: str = ""cpu"",\n        checkpoint: bool = False,\n        save_final_model: bool = True,\n        anneal_with_restarts: bool = False,\n        anneal_with_prestarts: bool = False,\n        batch_growth_annealing: bool = False,\n        shuffle: bool = True,\n        param_selection_mode: bool = False,\n        num_workers: int = 6,\n        sampler=None,\n        use_amp: bool = False,\n        amp_opt_level: str = ""O1"",\n        eval_on_train_fraction=0.0,\n        eval_on_train_shuffle=False,\n        **kwargs,\n    ) -> dict:\n        """"""\n        Trains any class that implements the flair.nn.Model interface.\n        :param base_path: Main path to which all output during training is logged and models are saved\n        :param learning_rate: Initial learning rate\n        :param mini_batch_size: Size of mini-batches during training\n        :param mini_batch_chunk_size: If mini-batches are larger than this number, they get broken down into chunks of this size for processing purposes\n        :param max_epochs: Maximum number of epochs to train. Terminates training if this number is surpassed.\n        :param anneal_factor: The factor by which the learning rate is annealed\n        :param patience: Patience is the number of epochs with no improvement the Trainer waits\n         until annealing the learning rate\n        :param min_learning_rate: If the learning rate falls below this threshold, training terminates\n        :param train_with_dev: If True, training is performed using both train+dev data\n        :param monitor_train: If True, training data is evaluated at end of each epoch\n        :param monitor_test: If True, test data is evaluated at end of each epoch\n        :param embeddings_storage_mode: One of \'none\' (all embeddings are deleted and freshly recomputed),\n        \'cpu\' (embeddings are stored on CPU) or \'gpu\' (embeddings are stored on GPU)\n        :param checkpoint: If True, a full checkpoint is saved at end of each epoch\n        :param save_final_model: If True, final model is saved\n        :param anneal_with_restarts: If True, the last best model is restored when annealing the learning rate\n        :param shuffle: If True, data is shuffled during training\n        :param param_selection_mode: If True, testing is performed against dev data. Use this mode when doing\n        parameter selection.\n        :param num_workers: Number of workers in your data loader.\n        :param sampler: You can pass a data sampler here for special sampling of data.\n        :param eval_on_train_fraction: the fraction of train data to do the evaluation on,\n        if 0. the evaluation is not performed on fraction of training data,\n        if \'dev\' the size is determined from dev set size\n        :param eval_on_train_shuffle: if True the train data fraction is determined on the start of training\n        and kept fixed during training, otherwise it\'s sampled at beginning of each epoch\n        :param kwargs: Other arguments for the Optimizer\n        :return:\n        """"""\n\n        if self.use_tensorboard:\n            try:\n                from torch.utils.tensorboard import SummaryWriter\n\n                writer = SummaryWriter()\n            except:\n                log_line(log)\n                log.warning(\n                    ""ATTENTION! PyTorch >= 1.1.0 and pillow are required for TensorBoard support!""\n                )\n                log_line(log)\n                self.use_tensorboard = False\n                pass\n\n        if use_amp:\n            if sys.version_info < (3, 0):\n                raise RuntimeError(""Apex currently only supports Python 3. Aborting."")\n            if amp is None:\n                raise RuntimeError(\n                    ""Failed to import apex. Please install apex from https://www.github.com/nvidia/apex ""\n                    ""to enable mixed-precision training.""\n                )\n\n        if mini_batch_chunk_size is None:\n            mini_batch_chunk_size = mini_batch_size\n        if learning_rate < min_learning_rate:\n            min_learning_rate = learning_rate / 10\n\n        initial_learning_rate = learning_rate\n\n        # cast string to Path\n        if type(base_path) is str:\n            base_path = Path(base_path)\n\n        log_handler = add_file_handler(log, base_path / ""training.log"")\n\n        log_line(log)\n        log.info(f\'Model: ""{self.model}""\')\n        log_line(log)\n        log.info(f\'Corpus: ""{self.corpus}""\')\n        log_line(log)\n        log.info(""Parameters:"")\n        log.info(f\' - learning_rate: ""{learning_rate}""\')\n        log.info(f\' - mini_batch_size: ""{mini_batch_size}""\')\n        log.info(f\' - patience: ""{patience}""\')\n        log.info(f\' - anneal_factor: ""{anneal_factor}""\')\n        log.info(f\' - max_epochs: ""{max_epochs}""\')\n        log.info(f\' - shuffle: ""{shuffle}""\')\n        log.info(f\' - train_with_dev: ""{train_with_dev}""\')\n        log.info(f\' - batch_growth_annealing: ""{batch_growth_annealing}""\')\n        log_line(log)\n        log.info(f\'Model training base path: ""{base_path}""\')\n        log_line(log)\n        log.info(f""Device: {flair.device}"")\n        log_line(log)\n        log.info(f""Embeddings storage mode: {embeddings_storage_mode}"")\n        if isinstance(self.model, SequenceTagger) and self.model.weight_dict and self.model.use_crf:\n            log_line(log)\n            log.warning(f\'WARNING: Specified class weights will not take effect when using CRF\')\n\n        # determine what splits (train, dev, test) to evaluate and log\n        log_train = True if monitor_train else False\n        log_test = (\n            True\n            if (not param_selection_mode and self.corpus.test and monitor_test)\n            else False\n        )\n        log_dev = True if not train_with_dev else False\n        log_train_part = (\n            True\n            if (eval_on_train_fraction == ""dev"" or eval_on_train_fraction > 0.0)\n            else False\n        )\n\n        if log_train_part:\n            train_part_size = (\n                len(self.corpus.dev)\n                if eval_on_train_fraction == ""dev""\n                else int(len(self.corpus.train) * eval_on_train_fraction)\n            )\n            assert train_part_size > 0\n            if not eval_on_train_shuffle:\n                train_part_indices = list(range(train_part_size))\n                train_part = torch.utils.data.dataset.Subset(\n                    self.corpus.train, train_part_indices\n                )\n\n        # prepare loss logging file and set up header\n        loss_txt = init_output_file(base_path, ""loss.tsv"")\n\n        weight_extractor = WeightExtractor(base_path)\n\n        optimizer: torch.optim.Optimizer = self.optimizer(\n            self.model.parameters(), lr=learning_rate, **kwargs\n        )\n\n        if use_amp:\n            self.model, optimizer = amp.initialize(\n                self.model, optimizer, opt_level=amp_opt_level\n            )\n\n        # minimize training loss if training with dev data, else maximize dev score\n        anneal_mode = ""min"" if train_with_dev else ""max""\n\n        lr_scheduler = scheduler(\n            optimizer,\n            factor=anneal_factor,\n            patience=patience,\n            initial_extra_patience=initial_extra_patience,\n            mode=anneal_mode,\n            verbose=True,\n        )\n\n        train_data = self.corpus.train\n\n        # if training also uses dev data, include in training set\n        if train_with_dev:\n            train_data = ConcatDataset([self.corpus.train, self.corpus.dev])\n\n        # initialize sampler if provided\n        if sampler is not None:\n            # init with default values if only class is provided\n            if inspect.isclass(sampler):\n                sampler = sampler()\n            # set dataset to sample from\n            sampler.set_dataset(train_data)\n            shuffle = False\n\n        dev_score_history = []\n        dev_loss_history = []\n        train_loss_history = []\n\n        micro_batch_size = mini_batch_chunk_size\n\n        # At any point you can hit Ctrl + C to break out of training early.\n        try:\n            previous_learning_rate = learning_rate\n\n            for self.epoch in range(self.epoch + 1, max_epochs + 1):\n                log_line(log)\n\n                if anneal_with_prestarts:\n                    last_epoch_model_state_dict = copy.deepcopy(self.model.state_dict())\n\n                if eval_on_train_shuffle:\n                    train_part_indices = list(range(self.corpus.train))\n                    random.shuffle(train_part_indices)\n                    train_part_indices = train_part_indices[:train_part_size]\n                    train_part = torch.utils.data.dataset.Subset(\n                        self.corpus.train, train_part_indices\n                    )\n\n                # get new learning rate\n                for group in optimizer.param_groups:\n                    learning_rate = group[""lr""]\n\n                if learning_rate != previous_learning_rate and batch_growth_annealing:\n                    mini_batch_size *= 2\n\n                # reload last best model if annealing with restarts is enabled\n                if (\n                    (anneal_with_restarts or anneal_with_prestarts)\n                    and learning_rate != previous_learning_rate\n                    and (base_path / ""best-model.pt"").exists()\n                ):\n                    if anneal_with_restarts:\n                        log.info(""resetting to best model"")\n                        self.model.load_state_dict(\n                            self.model.load(base_path / ""best-model.pt"").state_dict()\n                        )\n                    if anneal_with_prestarts:\n                        log.info(""resetting to pre-best model"")\n                        self.model.load_state_dict(\n                            self.model.load(base_path / ""pre-best-model.pt"").state_dict()\n                        )\n\n                previous_learning_rate = learning_rate\n\n                # stop training if learning rate becomes too small\n                if learning_rate < min_learning_rate:\n                    log_line(log)\n                    log.info(""learning rate too small - quitting training!"")\n                    log_line(log)\n                    break\n\n                batch_loader = DataLoader(\n                    train_data,\n                    batch_size=mini_batch_size,\n                    shuffle=shuffle,\n                    num_workers=num_workers,\n                    sampler=sampler,\n                )\n\n                self.model.train()\n\n                train_loss: float = 0\n\n                seen_batches = 0\n                total_number_of_batches = len(batch_loader)\n\n                modulo = max(1, int(total_number_of_batches / 10))\n\n                # process mini-batches\n                batch_time = 0\n                for batch_no, batch in enumerate(batch_loader):\n                    start_time = time.time()\n\n                    # zero the gradients on the model and optimizer\n                    self.model.zero_grad()\n                    optimizer.zero_grad()\n\n                    # if necessary, make batch_steps\n                    batch_steps = [batch]\n                    if len(batch) > micro_batch_size:\n                        batch_steps = [\n                            batch[x : x + micro_batch_size]\n                            for x in range(0, len(batch), micro_batch_size)\n                        ]\n\n                    # forward and backward for batch\n                    for batch_step in batch_steps:\n\n                        # forward pass\n                        loss = self.model.forward_loss(batch_step)\n\n                        # Backward\n                        if use_amp:\n                            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                                scaled_loss.backward()\n                        else:\n                            loss.backward()\n\n                    # do the optimizer step\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n                    optimizer.step()\n\n                    seen_batches += 1\n                    train_loss += loss.item()\n\n                    # depending on memory mode, embeddings are moved to CPU, GPU or deleted\n                    store_embeddings(batch, embeddings_storage_mode)\n\n                    batch_time += time.time() - start_time\n                    if seen_batches % modulo == 0:\n                        log.info(\n                            f""epoch {self.epoch} - iter {seen_batches}/{total_number_of_batches} - loss ""\n                            f""{train_loss / seen_batches:.8f} - samples/sec: {mini_batch_size * modulo / batch_time:.2f}""\n                        )\n                        batch_time = 0\n                        iteration = self.epoch * total_number_of_batches + batch_no\n                        if not param_selection_mode:\n                            weight_extractor.extract_weights(\n                                self.model.state_dict(), iteration\n                            )\n\n                train_loss /= seen_batches\n\n                self.model.eval()\n\n                log_line(log)\n                log.info(\n                    f""EPOCH {self.epoch} done: loss {train_loss:.4f} - lr {learning_rate:.7f}""\n                )\n\n                if self.use_tensorboard:\n                    writer.add_scalar(""train_loss"", train_loss, self.epoch)\n\n                # anneal against train loss if training with dev, otherwise anneal against dev score\n                current_score = train_loss\n\n                # evaluate on train / dev / test split depending on training settings\n                result_line: str = """"\n\n                if log_train:\n                    train_eval_result, train_loss = self.model.evaluate(\n                        self.corpus.train,\n                        mini_batch_size=mini_batch_chunk_size,\n                        num_workers=num_workers,\n                        embedding_storage_mode=embeddings_storage_mode,\n                    )\n                    result_line += f""\\t{train_eval_result.log_line}""\n\n                    # depending on memory mode, embeddings are moved to CPU, GPU or deleted\n                    store_embeddings(self.corpus.train, embeddings_storage_mode)\n\n                if log_train_part:\n                    train_part_eval_result, train_part_loss = self.model.evaluate(\n                        train_part,\n                        mini_batch_size=mini_batch_chunk_size,\n                        num_workers=num_workers,\n                        embedding_storage_mode=embeddings_storage_mode,\n                    )\n                    result_line += (\n                        f""\\t{train_part_loss}\\t{train_part_eval_result.log_line}""\n                    )\n                    log.info(\n                        f""TRAIN_SPLIT : loss {train_part_loss} - score {round(train_part_eval_result.main_score, 4)}""\n                    )\n\n                if log_dev:\n                    dev_eval_result, dev_loss = self.model.evaluate(\n                        self.corpus.dev,\n                        mini_batch_size=mini_batch_chunk_size,\n                        num_workers=num_workers,\n                        embedding_storage_mode=embeddings_storage_mode,\n                    )\n                    result_line += f""\\t{dev_loss}\\t{dev_eval_result.log_line}""\n                    log.info(\n                        f""DEV : loss {dev_loss} - score {round(dev_eval_result.main_score, 4)}""\n                    )\n                    # calculate scores using dev data if available\n                    # append dev score to score history\n                    dev_score_history.append(dev_eval_result.main_score)\n                    dev_loss_history.append(dev_loss.item())\n\n                    current_score = dev_eval_result.main_score\n\n                    # depending on memory mode, embeddings are moved to CPU, GPU or deleted\n                    store_embeddings(self.corpus.dev, embeddings_storage_mode)\n\n                    if self.use_tensorboard:\n                        writer.add_scalar(""dev_loss"", dev_loss, self.epoch)\n                        writer.add_scalar(\n                            ""dev_score"", dev_eval_result.main_score, self.epoch\n                        )\n\n                if log_test:\n                    test_eval_result, test_loss = self.model.evaluate(\n                        self.corpus.test,\n                        mini_batch_size=mini_batch_chunk_size,\n                        num_workers=num_workers,\n                        out_path=base_path / ""test.tsv"",\n                        embedding_storage_mode=embeddings_storage_mode,\n                    )\n                    result_line += f""\\t{test_loss}\\t{test_eval_result.log_line}""\n                    log.info(\n                        f""TEST : loss {test_loss} - score {round(test_eval_result.main_score, 4)}""\n                    )\n\n                    # depending on memory mode, embeddings are moved to CPU, GPU or deleted\n                    store_embeddings(self.corpus.test, embeddings_storage_mode)\n\n                    if self.use_tensorboard:\n                        writer.add_scalar(""test_loss"", test_loss, self.epoch)\n                        writer.add_scalar(\n                            ""test_score"", test_eval_result.main_score, self.epoch\n                        )\n\n                # determine learning rate annealing through scheduler. Use auxiliary metric for AnnealOnPlateau\n                if not train_with_dev and isinstance(lr_scheduler, AnnealOnPlateau):\n                    lr_scheduler.step(current_score, dev_loss)\n                else:\n                    lr_scheduler.step(current_score)\n\n                train_loss_history.append(train_loss)\n\n                # determine bad epoch number\n                try:\n                    bad_epochs = lr_scheduler.num_bad_epochs\n                except:\n                    bad_epochs = 0\n                for group in optimizer.param_groups:\n                    new_learning_rate = group[""lr""]\n                if new_learning_rate != previous_learning_rate:\n                    bad_epochs = patience + 1\n                    if previous_learning_rate == initial_learning_rate: bad_epochs += initial_extra_patience\n\n                # log bad epochs\n                log.info(f""BAD EPOCHS (no improvement): {bad_epochs}"")\n\n                # output log file\n                with open(loss_txt, ""a"") as f:\n\n                    # make headers on first epoch\n                    if self.epoch == 1:\n                        f.write(\n                            f""EPOCH\\tTIMESTAMP\\tBAD_EPOCHS\\tLEARNING_RATE\\tTRAIN_LOSS""\n                        )\n\n                        if log_train:\n                            f.write(\n                                ""\\tTRAIN_""\n                                + ""\\tTRAIN_"".join(\n                                    train_eval_result.log_header.split(""\\t"")\n                                )\n                            )\n                        if log_train_part:\n                            f.write(\n                                ""\\tTRAIN_PART_LOSS\\tTRAIN_PART_""\n                                + ""\\tTRAIN_PART_"".join(\n                                    train_part_eval_result.log_header.split(""\\t"")\n                                )\n                            )\n                        if log_dev:\n                            f.write(\n                                ""\\tDEV_LOSS\\tDEV_""\n                                + ""\\tDEV_"".join(dev_eval_result.log_header.split(""\\t""))\n                            )\n                        if log_test:\n                            f.write(\n                                ""\\tTEST_LOSS\\tTEST_""\n                                + ""\\tTEST_"".join(\n                                    test_eval_result.log_header.split(""\\t"")\n                                )\n                            )\n\n                    f.write(\n                        f""\\n{self.epoch}\\t{datetime.datetime.now():%H:%M:%S}\\t{bad_epochs}\\t{learning_rate:.4f}\\t{train_loss}""\n                    )\n                    f.write(result_line)\n\n                # if checkpoint is enabled, save model at each epoch\n                if checkpoint and not param_selection_mode:\n                    self.save_checkpoint(base_path / ""checkpoint.pt"")\n\n                # if we use dev data, remember best model based on dev evaluation score\n                if (\n                    (not train_with_dev or anneal_with_restarts or anneal_with_prestarts)\n                    and not param_selection_mode\n                    and current_score == lr_scheduler.best\n                    and bad_epochs == 0\n                ):\n                    print(""saving best model"")\n                    self.model.save(base_path / ""best-model.pt"")\n\n                    if anneal_with_prestarts:\n                        current_state_dict = self.model.state_dict()\n                        self.model.load_state_dict(last_epoch_model_state_dict)\n                        self.model.save(base_path / ""pre-best-model.pt"")\n                        self.model.load_state_dict(current_state_dict)\n\n            # if we do not use dev data for model selection, save final model\n            if save_final_model and not param_selection_mode:\n                self.model.save(base_path / ""final-model.pt"")\n\n        except KeyboardInterrupt:\n            log_line(log)\n            log.info(""Exiting from training early."")\n\n            if self.use_tensorboard:\n                writer.close()\n\n            if not param_selection_mode:\n                log.info(""Saving model ..."")\n                self.model.save(base_path / ""final-model.pt"")\n                log.info(""Done."")\n\n        # test best model if test data is present\n        if self.corpus.test:\n            final_score = self.final_test(base_path, mini_batch_chunk_size, num_workers)\n        else:\n            final_score = 0\n            log.info(""Test data not provided setting final score to 0"")\n\n        log.removeHandler(log_handler)\n\n        if self.use_tensorboard:\n            writer.close()\n\n        return {\n            ""test_score"": final_score,\n            ""dev_score_history"": dev_score_history,\n            ""train_loss_history"": train_loss_history,\n            ""dev_loss_history"": dev_loss_history,\n        }\n\n    def save_checkpoint(self, model_file: Union[str, Path]):\n        corpus = self.corpus\n        self.corpus = None\n        torch.save(self, str(model_file), pickle_protocol=4)\n        self.corpus = corpus\n\n    @classmethod\n    def load_checkpoint(cls, checkpoint: Union[Path, str], corpus: Corpus):\n        model: ModelTrainer = torch.load(checkpoint, map_location=flair.device)\n        model.corpus = corpus\n        return model\n\n    def final_test(\n        self, base_path: Union[Path, str], eval_mini_batch_size: int, num_workers: int = 8\n    ):\n        if type(base_path) is str:\n            base_path = Path(base_path)\n\n        log_line(log)\n        log.info(""Testing using best model ..."")\n\n        self.model.eval()\n\n        if (base_path / ""best-model.pt"").exists():\n            self.model = self.model.load(base_path / ""best-model.pt"")\n\n        test_results, test_loss = self.model.evaluate(\n            self.corpus.test,\n            mini_batch_size=eval_mini_batch_size,\n            num_workers=num_workers,\n            out_path=base_path / ""test.tsv"",\n            embedding_storage_mode=""none"",\n        )\n\n        test_results: Result = test_results\n        log.info(test_results.log_line)\n        log.info(test_results.detailed_results)\n        log_line(log)\n\n        # if we are training over multiple datasets, do evaluation for each\n        if type(self.corpus) is MultiCorpus:\n            for subcorpus in self.corpus.corpora:\n                log_line(log)\n                self.model.evaluate(\n                    subcorpus.test,\n                    mini_batch_size=eval_mini_batch_size,\n                    num_workers=num_workers,\n                    out_path=base_path / f""{subcorpus.name}-test.tsv"",\n                    embedding_storage_mode=""none"",\n                )\n\n        # get and return the final test score of best model\n        final_score = test_results.main_score\n\n        return final_score\n\n    def find_learning_rate(\n        self,\n        base_path: Union[Path, str],\n        file_name: str = ""learning_rate.tsv"",\n        start_learning_rate: float = 1e-7,\n        end_learning_rate: float = 10,\n        iterations: int = 100,\n        mini_batch_size: int = 32,\n        stop_early: bool = True,\n        smoothing_factor: float = 0.98,\n        **kwargs,\n    ) -> Path:\n        best_loss = None\n        moving_avg_loss = 0\n\n        # cast string to Path\n        if type(base_path) is str:\n            base_path = Path(base_path)\n        learning_rate_tsv = init_output_file(base_path, file_name)\n\n        with open(learning_rate_tsv, ""a"") as f:\n            f.write(""ITERATION\\tTIMESTAMP\\tLEARNING_RATE\\tTRAIN_LOSS\\n"")\n\n        optimizer = self.optimizer(\n            self.model.parameters(), lr=start_learning_rate, **kwargs\n        )\n\n        train_data = self.corpus.train\n\n        scheduler = ExpAnnealLR(optimizer, end_learning_rate, iterations)\n\n        model_state = self.model.state_dict()\n        self.model.train()\n\n        step = 0\n        while step < iterations:\n            batch_loader = DataLoader(\n                train_data, batch_size=mini_batch_size, shuffle=True\n            )\n            for batch in batch_loader:\n                step += 1\n\n                # forward pass\n                loss = self.model.forward_loss(batch)\n\n                # update optimizer and scheduler\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0)\n                optimizer.step()\n                scheduler.step(step)\n\n                print(scheduler.get_lr())\n                learning_rate = scheduler.get_lr()[0]\n\n                loss_item = loss.item()\n                if step == 1:\n                    best_loss = loss_item\n                else:\n                    if smoothing_factor > 0:\n                        moving_avg_loss = (\n                            smoothing_factor * moving_avg_loss\n                            + (1 - smoothing_factor) * loss_item\n                        )\n                        loss_item = moving_avg_loss / (\n                            1 - smoothing_factor ** (step + 1)\n                        )\n                    if loss_item < best_loss:\n                        best_loss = loss\n\n                if step > iterations:\n                    break\n\n                if stop_early and (loss_item > 4 * best_loss or torch.isnan(loss)):\n                    log_line(log)\n                    log.info(""loss diverged - stopping early!"")\n                    step = iterations\n                    break\n\n                with open(str(learning_rate_tsv), ""a"") as f:\n                    f.write(\n                        f""{step}\\t{datetime.datetime.now():%H:%M:%S}\\t{learning_rate}\\t{loss_item}\\n""\n                    )\n\n            self.model.load_state_dict(model_state)\n            self.model.to(flair.device)\n\n        log_line(log)\n        log.info(f""learning rate finder finished - plot {learning_rate_tsv}"")\n        log_line(log)\n\n        return Path(learning_rate_tsv)\n'"
flair/visual/__init__.py,0,b'from .manifold import Visualizer\nfrom .activations import Highlighter\n'
flair/visual/activations.py,0,"b'import numpy\n\n\nclass Highlighter(object):\n    def __init__(self):\n        self.color_map = [\n            ""#ff0000"",\n            ""#ff4000"",\n            ""#ff8000"",\n            ""#ffbf00"",\n            ""#ffff00"",\n            ""#bfff00"",\n            ""#80ff00"",\n            ""#40ff00"",\n            ""#00ff00"",\n            ""#00ff40"",\n            ""#00ff80"",\n            ""#00ffbf"",\n            ""#00ffff"",\n            ""#00bfff"",\n            ""#0080ff"",\n            ""#0040ff"",\n            ""#0000ff"",\n            ""#4000ff"",\n            ""#8000ff"",\n            ""#bf00ff"",\n            ""#ff00ff"",\n            ""#ff00bf"",\n            ""#ff0080"",\n            ""#ff0040"",\n            ""#ff0000"",\n        ]\n\n    def highlight(self, activation, text):\n        activation = activation.detach().cpu().numpy()\n\n        step_size = (max(activation) - min(activation)) / len(self.color_map)\n\n        lookup = numpy.array(\n            list(numpy.arange(min(activation), max(activation), step_size))\n        )\n\n        colors = []\n\n        for i, act in enumerate(activation):\n\n            try:\n                colors.append(self.color_map[numpy.where(act > lookup)[0][-1]])\n            except IndexError:\n                colors.append(len(self.color_map) - 1)\n\n        str_ = ""<br><br>""\n\n        for i, (char, color) in enumerate(zip(list(text), colors)):\n            str_ += self._render(char, color)\n\n            if i % 100 == 0 and i > 0:\n                str_ += ""<br>""\n\n        return str_\n\n    def highlight_selection(\n        self, activations, text, file_=""resources/data/highlight.html"", n=10\n    ):\n\n        ix = numpy.random.choice(activations.shape[1], size=n)\n\n        rendered = """"\n\n        for i in ix:\n\n            rendered += self.highlight(activations[:, i], text)\n\n        with open(file_, ""w"") as f:\n            f.write(rendered)\n\n    @staticmethod\n    def _render(char, color):\n        return \'<span style=""background-color: {}"">{}</span>\'.format(color, char)\n'"
flair/visual/manifold.py,0,"b'import numpy\nimport tqdm\nfrom sklearn.manifold import TSNE\n\n\nclass _Transform:\n    def __init__(self):\n        pass\n\n    def fit(self, X):\n        return self.transform.fit_transform(X)\n\n\nclass tSNE(_Transform):\n    def __init__(self):\n        super().__init__()\n\n        self.transform = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n\n\nclass Visualizer(object):\n    def visualize_word_emeddings(self, embeddings, sentences, output_file):\n        X = self.prepare_word_embeddings(embeddings, sentences)\n        contexts = self.word_contexts(sentences)\n\n        trans_ = tSNE()\n        reduced = trans_.fit(X)\n\n        self.visualize(reduced, contexts, output_file)\n\n    def visualize_char_emeddings(self, embeddings, sentences, output_file):\n        X = self.prepare_char_embeddings(embeddings, sentences)\n        contexts = self.char_contexts(sentences)\n\n        trans_ = tSNE()\n        reduced = trans_.fit(X)\n\n        self.visualize(reduced, contexts, output_file)\n\n    @staticmethod\n    def prepare_word_embeddings(embeddings, sentences):\n        X = []\n\n        for sentence in tqdm.tqdm(sentences):\n            embeddings.embed(sentence)\n\n            for i, token in enumerate(sentence):\n                X.append(token.embedding.detach().numpy()[None, :])\n\n        X = numpy.concatenate(X, 0)\n\n        return X\n\n    @staticmethod\n    def word_contexts(sentences):\n        contexts = []\n\n        for sentence in sentences:\n\n            strs = [x.text for x in sentence.tokens]\n\n            for i, token in enumerate(strs):\n                prop = \'<b><font color=""red""> {token} </font></b>\'.format(token=token)\n\n                prop = "" "".join(strs[max(i - 4, 0) : i]) + prop\n                prop = prop + "" "".join(strs[i + 1 : min(len(strs), i + 5)])\n\n                contexts.append(""<p>"" + prop + ""</p>"")\n\n        return contexts\n\n    @staticmethod\n    def prepare_char_embeddings(embeddings, sentences):\n        X = []\n\n        for sentence in tqdm.tqdm(sentences):\n            sentence = "" "".join([x.text for x in sentence])\n\n            hidden = embeddings.lm.get_representation([sentence], """", """")\n            X.append(hidden.squeeze().detach().numpy())\n\n        X = numpy.concatenate(X, 0)\n\n        return X\n\n    @staticmethod\n    def char_contexts(sentences):\n        contexts = []\n\n        for sentence in sentences:\n            sentence = "" "".join([token.text for token in sentence])\n\n            for i, char in enumerate(sentence):\n                context = \'<span style=""background-color: yellow""><b>{}</b></span>\'.format(\n                    char\n                )\n                context = """".join(sentence[max(i - 30, 0) : i]) + context\n                context = context + """".join(\n                    sentence[i + 1 : min(len(sentence), i + 30)]\n                )\n\n                contexts.append(context)\n\n        return contexts\n\n    @staticmethod\n    def visualize(X, contexts, file):\n        import matplotlib.pyplot\n        import mpld3\n\n        fig, ax = matplotlib.pyplot.subplots()\n\n        ax.grid(True, alpha=0.3)\n\n        points = ax.plot(\n            X[:, 0], X[:, 1], ""o"", color=""b"", mec=""k"", ms=5, mew=1, alpha=0.6\n        )\n\n        ax.set_xlabel(""x"")\n        ax.set_ylabel(""y"")\n        ax.set_title(""Hover mouse to reveal context"", size=20)\n\n        tooltip = mpld3.plugins.PointHTMLTooltip(\n            points[0], contexts, voffset=10, hoffset=10\n        )\n\n        mpld3.plugins.connect(fig, tooltip)\n\n        mpld3.save_html(fig, file)\n'"
flair/visual/ner_html.py,0,"b'import html\nfrom typing import Union, List\n\nfrom flair.data import Sentence\n\nTAGGED_ENTITY = """"""\n<mark class=""entity"" style=""background: {color}; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 3; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone"">\n    {entity}\n    <span style=""font-size: 0.8em; font-weight: bold; line-height: 3; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem"">{label}</span>\n</mark>\n""""""\n\nPARAGRAPH = """"""<p>{sentence}</p>""""""\n\nHTML_PAGE = """"""\n<!DOCTYPE html>\n<html lang=""en"">\n    <head>\n        <title>{title}</title>\n    </head>\n\n    <body style=""font-size: 16px; font-family: \'Segoe UI\'; padding: 4rem 2rem"">{text}</body>\n</html>\n""""""\n\n\ndef split_to_spans(s: Sentence):\n    orig = s.to_original_text()\n    last_idx = 0\n    spans = []\n    tagged_ents = s.get_spans(""ner"")\n    for ent in tagged_ents:\n        if last_idx != ent.start_pos:\n            spans.append((orig[last_idx : ent.start_pos], None))\n        spans.append((ent.text, ent.tag))\n        last_idx = ent.end_pos\n    if last_idx < len(orig) - 1:\n        spans.append((orig[last_idx : len(orig)], None))\n    return spans\n\n\ndef render_ner_html(\n    sentences: Union[List[Sentence], Sentence],\n    title: str = ""Flair"",\n    colors={\n        ""PER"": ""#F7FF53"",\n        ""ORG"": ""#E8902E"",\n        ""LOC"": ""#FF40A3"",\n        ""MISC"": ""#4647EB"",\n        ""O"": ""#ddd"",\n    },\n    default_color: str = ""#ddd"",\n    wrap_page=True,\n) -> str:\n    """"""\n    :param sentences: single sentence or list of sentences to convert to HTML\n    :param title: title of the HTML page\n    :param colors: dict where keys are tags and values are color HTML codes\n    :param default_color: color to use if colors parameter is missing a tag\n    :param wrap_page: if True method returns result of processing sentences wrapped by &lt;html&gt; and &lt;body&gt; tags, otherwise - without these tags\n    :return: HTML as a string\n    """"""\n    if isinstance(sentences, Sentence):\n        sentences = [sentences]\n    sentences_html = []\n    for s in sentences:\n        spans = split_to_spans(s)\n        spans_html = list()\n        for fragment, tag in spans:\n            escaped_fragment = html.escape(fragment).replace(""\\n"", ""<br/>"")\n            if tag:\n                escaped_fragment = TAGGED_ENTITY.format(\n                    entity=escaped_fragment,\n                    label=tag,\n                    color=colors.get(tag, default_color),\n                )\n            spans_html.append(escaped_fragment)\n        line = PARAGRAPH.format(sentence="""".join(spans_html))\n        sentences_html.append(line)\n\n    final_text = """".join(sentences_html)\n\n    if wrap_page:\n        return HTML_PAGE.format(text=final_text, title=title)\n    else:\n        return final_text\n'"
flair/visual/training_curves.py,0,"b'import logging\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Union, List\n\nimport numpy as np\nimport csv\n\nimport math\n\nimport matplotlib.pyplot as plt\n\n\n# header for \'weights.txt\'\nWEIGHT_NAME = 1\nWEIGHT_NUMBER = 2\nWEIGHT_VALUE = 3\n\nlog = logging.getLogger(""flair"")\n\n\nclass Plotter(object):\n    """"""\n    Plots training parameters (loss, f-score, and accuracy) and training weights over time.\n    Input files are the output files \'loss.tsv\' and \'weights.txt\' from training either a sequence tagger or text\n    classification model.\n    """"""\n\n    @staticmethod\n    def _extract_evaluation_data(file_name: Union[str, Path], score: str = ""F1"") -> dict:\n        if type(file_name) is str:\n            file_name = Path(file_name)\n\n        training_curves = {\n            ""train"": {""loss"": [], ""score"": []},\n            ""test"": {""loss"": [], ""score"": []},\n            ""dev"": {""loss"": [], ""score"": []},\n        }\n\n        with open(file_name, ""r"") as tsvin:\n            tsvin = csv.reader(tsvin, delimiter=""\\t"")\n\n            # determine the column index of loss, f-score and accuracy for train, dev and test split\n            row = next(tsvin, None)\n\n            score = score.upper()\n\n            if f""TEST_{score}"" not in row:\n                log.warning(""-"" * 100)\n                log.warning(f""WARNING: No {score} found for test split in this data."")\n                log.warning(\n                    f""Are you sure you want to plot {score} and not another value?""\n                )\n                log.warning(""-"" * 100)\n\n            TRAIN_SCORE = (\n                row.index(f""TRAIN_{score}"") if f""TRAIN_{score}"" in row else None\n            )\n            DEV_SCORE = row.index(f""DEV_{score}"") if f""DEV_{score}"" in row else None\n            TEST_SCORE = row.index(f""TEST_{score}"") if f""TEST_{score}"" in row else None\n\n            # then get all relevant values from the tsv\n            for row in tsvin:\n\n                if TRAIN_SCORE is not None:\n                    if row[TRAIN_SCORE] != ""_"":\n                        training_curves[""train""][""score""].append(\n                            float(row[TRAIN_SCORE])\n                        )\n\n                if DEV_SCORE is not None:\n                    if row[DEV_SCORE] != ""_"":\n                        training_curves[""dev""][""score""].append(float(row[DEV_SCORE]))\n\n                if TEST_SCORE is not None:\n                    if row[TEST_SCORE] != ""_"":\n                        training_curves[""test""][""score""].append(float(row[TEST_SCORE]))\n\n        return training_curves\n\n    @staticmethod\n    def _extract_weight_data(file_name: Union[str, Path]) -> dict:\n        if type(file_name) is str:\n            file_name = Path(file_name)\n\n        weights = defaultdict(lambda: defaultdict(lambda: list()))\n\n        with open(file_name, ""r"") as tsvin:\n            tsvin = csv.reader(tsvin, delimiter=""\\t"")\n\n            for row in tsvin:\n                name = row[WEIGHT_NAME]\n                param = row[WEIGHT_NUMBER]\n                value = float(row[WEIGHT_VALUE])\n\n                weights[name][param].append(value)\n\n        return weights\n\n    @staticmethod\n    def _extract_learning_rate(file_name: Union[str, Path]):\n        if type(file_name) is str:\n            file_name = Path(file_name)\n\n        lrs = []\n        losses = []\n\n        with open(file_name, ""r"") as tsvin:\n            tsvin = csv.reader(tsvin, delimiter=""\\t"")\n            row = next(tsvin, None)\n            LEARNING_RATE = row.index(""LEARNING_RATE"")\n            TRAIN_LOSS = row.index(""TRAIN_LOSS"")\n\n            # then get all relevant values from the tsv\n            for row in tsvin:\n                if row[TRAIN_LOSS] != ""_"":\n                    losses.append(float(row[TRAIN_LOSS]))\n                if row[LEARNING_RATE] != ""_"":\n                    lrs.append(float(row[LEARNING_RATE]))\n\n        return lrs, losses\n\n    def plot_weights(self, file_name: Union[str, Path]):\n        if type(file_name) is str:\n            file_name = Path(file_name)\n\n        weights = self._extract_weight_data(file_name)\n\n        total = len(weights)\n        columns = 2\n        rows = max(2, int(math.ceil(total / columns)))\n\n        figsize = (4*columns, 3*rows)\n\n        fig = plt.figure()\n        f, axarr = plt.subplots(rows, columns, figsize=figsize)\n\n        c = 0\n        r = 0\n        for name, values in weights.items():\n            # plot i\n            axarr[r, c].set_title(name, fontsize=6)\n            for _, v in values.items():\n                axarr[r, c].plot(np.arange(0, len(v)), v, linewidth=0.35)\n            axarr[r, c].set_yticks([])\n            axarr[r, c].set_xticks([])\n            c += 1\n            if c == columns:\n                c = 0\n                r += 1\n\n        while r != rows and c != columns:\n            axarr[r, c].set_yticks([])\n            axarr[r, c].set_xticks([])\n            c += 1\n            if c == columns:\n                c = 0\n                r += 1\n\n        # save plots\n        f.subplots_adjust(hspace=0.5)\n        plt.tight_layout(pad=1.0)\n        path = file_name.parent / ""weights.png""\n        plt.savefig(path, dpi=300)\n        print(\n            f""Weights plots are saved in {path}""\n        )  # to let user know the path of the save plots\n        plt.close(fig)\n\n    def plot_training_curves(\n        self, file_name: Union[str, Path], plot_values: List[str] = [""loss"", ""F1""]\n    ):\n        if type(file_name) is str:\n            file_name = Path(file_name)\n\n        fig = plt.figure(figsize=(15, 10))\n\n        for plot_no, plot_value in enumerate(plot_values):\n\n            training_curves = self._extract_evaluation_data(file_name, plot_value)\n\n            plt.subplot(len(plot_values), 1, plot_no + 1)\n            if training_curves[""train""][""score""]:\n                x = np.arange(0, len(training_curves[""train""][""score""]))\n                plt.plot(\n                    x, training_curves[""train""][""score""], label=f""training {plot_value}""\n                )\n            if training_curves[""dev""][""score""]:\n                x = np.arange(0, len(training_curves[""dev""][""score""]))\n                plt.plot(\n                    x, training_curves[""dev""][""score""], label=f""validation {plot_value}""\n                )\n            if training_curves[""test""][""score""]:\n                x = np.arange(0, len(training_curves[""test""][""score""]))\n                plt.plot(\n                    x, training_curves[""test""][""score""], label=f""test {plot_value}""\n                )\n            plt.legend(bbox_to_anchor=(1.04, 0), loc=""lower left"", borderaxespad=0)\n            plt.ylabel(plot_value)\n            plt.xlabel(""epochs"")\n\n        # save plots\n        plt.tight_layout(pad=1.0)\n        path = file_name.parent / ""training.png""\n        plt.savefig(path, dpi=300)\n        print(\n            f""Loss and F1 plots are saved in {path}""\n        )  # to let user know the path of the save plots\n        plt.show(block=False)  # to have the plots displayed when user run this module\n        plt.close(fig)\n\n    def plot_learning_rate(\n        self, file_name: Union[str, Path], skip_first: int = 10, skip_last: int = 5\n    ):\n        if type(file_name) is str:\n            file_name = Path(file_name)\n\n        lrs, losses = self._extract_learning_rate(file_name)\n        lrs = lrs[skip_first:-skip_last] if skip_last > 0 else lrs[skip_first:]\n        losses = losses[skip_first:-skip_last] if skip_last > 0 else losses[skip_first:]\n\n        fig, ax = plt.subplots(1, 1)\n        ax.plot(lrs, losses)\n        ax.set_ylabel(""Loss"")\n        ax.set_xlabel(""Learning Rate"")\n        ax.set_xscale(""log"")\n        ax.xaxis.set_major_formatter(plt.FormatStrFormatter(""%.0e""))\n\n        # plt.show()\n\n        # save plot\n        plt.tight_layout(pad=1.0)\n        path = file_name.parent / ""learning_rate.png""\n        plt.savefig(path, dpi=300)\n        print(\n            f""Learning_rate plots are saved in {path}""\n        )  # to let user know the path of the save plots\n        plt.show(block=True)  # to have the plots displayed when user run this module\n        plt.close(fig)\n'"
