file_path,api_count,code
track/DCFNet.py,11,"b""from os.path import join, isdir\nfrom os import makedirs\nimport argparse\nimport json\nimport numpy as np\nimport torch\n\nimport cv2\nimport time as time\nfrom util import crop_chw, gaussian_shaped_labels, cxy_wh_2_rect1, rect1_2_cxy_wh, cxy_wh_2_bbox\nfrom net import DCFNet\nfrom eval_otb import eval_auc\n\n\nclass TrackerConfig(object):\n    # These are the default hyper-params for DCFNet\n    # OTB2013 / AUC(0.665)\n    feature_path = 'param.pth'\n    crop_sz = 125\n\n    lambda0 = 1e-4\n    padding = 2\n    output_sigma_factor = 0.1\n    interp_factor = 0.01\n    num_scale = 3\n    scale_step = 1.0275\n    scale_factor = scale_step ** (np.arange(num_scale) - num_scale / 2)\n    min_scale_factor = 0.2\n    max_scale_factor = 5\n    scale_penalty = 0.9925\n    scale_penalties = scale_penalty ** (np.abs((np.arange(num_scale) - num_scale / 2)))\n\n    net_input_size = [crop_sz, crop_sz]\n    net_average_image = np.array([104, 117, 123]).reshape(-1, 1, 1).astype(np.float32)\n    output_sigma = crop_sz / (1 + padding) * output_sigma_factor\n    y = gaussian_shaped_labels(output_sigma, net_input_size)\n    yf = torch.rfft(torch.Tensor(y).view(1, 1, crop_sz, crop_sz).cuda(), signal_ndim=2)\n    cos_window = torch.Tensor(np.outer(np.hanning(crop_sz), np.hanning(crop_sz))).cuda()\n\n\nclass DCFNetTraker(object):\n    def __init__(self, im, init_rect, config=TrackerConfig(), gpu=True):\n        self.gpu = gpu\n        self.config = config\n        self.net = DCFNet(config)\n        self.net.load_param(config.feature_path)\n        self.net.eval()\n        if gpu:\n            self.net.cuda()\n\n        # confine results\n        target_pos, target_sz = rect1_2_cxy_wh(init_rect)\n        self.min_sz = np.maximum(config.min_scale_factor * target_sz, 4)\n        self.max_sz = np.minimum(im.shape[:2], config.max_scale_factor * target_sz)\n\n        # crop template\n        window_sz = target_sz * (1 + config.padding)\n        bbox = cxy_wh_2_bbox(target_pos, window_sz)\n        patch = crop_chw(im, bbox, self.config.crop_sz)\n\n        target = patch - config.net_average_image\n        self.net.update(torch.Tensor(np.expand_dims(target, axis=0)).cuda())\n        self.target_pos, self.target_sz = target_pos, target_sz\n        self.patch_crop = np.zeros((config.num_scale, patch.shape[0], patch.shape[1], patch.shape[2]), np.float32)  # buff\n\n    def track(self, im):\n        for i in range(self.config.num_scale):  # crop multi-scale search region\n            window_sz = self.target_sz * (self.config.scale_factor[i] * (1 + self.config.padding))\n            bbox = cxy_wh_2_bbox(self.target_pos, window_sz)\n            self.patch_crop[i, :] = crop_chw(im, bbox, self.config.crop_sz)\n\n        search = self.patch_crop - self.config.net_average_image\n\n        if self.gpu:\n            response = self.net(torch.Tensor(search).cuda())\n        else:\n            response = self.net(torch.Tensor(search))\n        peak, idx = torch.max(response.view(self.config.num_scale, -1), 1)\n        peak = peak.data.cpu().numpy() * self.config.scale_penalties\n        best_scale = np.argmax(peak)\n        r_max, c_max = np.unravel_index(idx[best_scale], self.config.net_input_size)\n\n        if r_max > self.config.net_input_size[0] / 2:\n            r_max = r_max - self.config.net_input_size[0]\n        if c_max > self.config.net_input_size[1] / 2:\n            c_max = c_max - self.config.net_input_size[1]\n        window_sz = self.target_sz * (self.config.scale_factor[best_scale] * (1 + self.config.padding))\n\n        self.target_pos = self.target_pos + np.array([c_max, r_max]) * window_sz / self.config.net_input_size\n        self.target_sz = np.minimum(np.maximum(window_sz / (1 + self.config.padding), self.min_sz), self.max_sz)\n\n        # model update\n        window_sz = self.target_sz * (1 + self.config.padding)\n        bbox = cxy_wh_2_bbox(self.target_pos, window_sz)\n        patch = crop_chw(im, bbox, self.config.crop_sz)\n        target = patch - self.config.net_average_image\n        self.net.update(torch.Tensor(np.expand_dims(target, axis=0)).cuda(), lr=self.config.interp_factor)\n\n        return cxy_wh_2_rect1(self.target_pos, self.target_sz)  # 1-index\n\n\nif __name__ == '__main__':\n    # base dataset path and setting\n    parser = argparse.ArgumentParser(description='Test DCFNet on OTB')\n    parser.add_argument('--dataset', metavar='SET', default='OTB2013',\n                        choices=['OTB2013', 'OTB2015'], help='tune on which dataset')\n    parser.add_argument('--model', metavar='PATH', default='param.pth')\n    args = parser.parse_args()\n\n    dataset = args.dataset\n    base_path = join('dataset', dataset)\n    json_path = join('dataset', dataset + '.json')\n    annos = json.load(open(json_path, 'r'))\n    videos = sorted(annos.keys())\n\n    use_gpu = True\n    visualization = False\n\n    # default parameter and load feature extractor network\n    config = TrackerConfig()\n    net = DCFNet(config)\n    net.load_param(args.model)\n    net.eval().cuda()\n\n    speed = []\n    # loop videos\n    for video_id, video in enumerate(videos):  # run without resetting\n        video_path_name = annos[video]['name']\n        init_rect = np.array(annos[video]['init_rect']).astype(np.float)\n        image_files = [join(base_path, video_path_name, 'img', im_f) for im_f in annos[video]['image_files']]\n        n_images = len(image_files)\n\n        tic = time.time()  # time start\n\n        target_pos, target_sz = rect1_2_cxy_wh(init_rect)  # OTB label is 1-indexed\n\n        im = cv2.imread(image_files[0])  # HxWxC\n\n        # confine results\n        min_sz = np.maximum(config.min_scale_factor * target_sz, 4)\n        max_sz = np.minimum(im.shape[:2], config.max_scale_factor * target_sz)\n\n        # crop template\n        window_sz = target_sz * (1 + config.padding)\n        bbox = cxy_wh_2_bbox(target_pos, window_sz)\n        patch = crop_chw(im, bbox, config.crop_sz)\n\n        target = patch - config.net_average_image\n        net.update(torch.Tensor(np.expand_dims(target, axis=0)).cuda())\n\n        res = [cxy_wh_2_rect1(target_pos, target_sz)]  # save in .txt\n        patch_crop = np.zeros((config.num_scale, patch.shape[0], patch.shape[1], patch.shape[2]), np.float32)\n        for f in range(1, n_images):  # track\n            im = cv2.imread(image_files[f])\n\n            for i in range(config.num_scale):  # crop multi-scale search region\n                window_sz = target_sz * (config.scale_factor[i] * (1 + config.padding))\n                bbox = cxy_wh_2_bbox(target_pos, window_sz)\n                patch_crop[i, :] = crop_chw(im, bbox, config.crop_sz)\n\n            search = patch_crop - config.net_average_image\n            response = net(torch.Tensor(search).cuda())\n            peak, idx = torch.max(response.view(config.num_scale, -1), 1)\n            peak = peak.data.cpu().numpy() * config.scale_penalties\n            best_scale = np.argmax(peak)\n            r_max, c_max = np.unravel_index(idx[best_scale], config.net_input_size)\n\n            if r_max > config.net_input_size[0] / 2:\n                r_max = r_max - config.net_input_size[0]\n            if c_max > config.net_input_size[1] / 2:\n                c_max = c_max - config.net_input_size[1]\n            window_sz = target_sz * (config.scale_factor[best_scale] * (1 + config.padding))\n\n            target_pos = target_pos + np.array([c_max, r_max]) * window_sz / config.net_input_size\n            target_sz = np.minimum(np.maximum(window_sz / (1 + config.padding), min_sz), max_sz)\n\n            # model update\n            window_sz = target_sz * (1 + config.padding)\n            bbox = cxy_wh_2_bbox(target_pos, window_sz)\n            patch = crop_chw(im, bbox, config.crop_sz)\n            target = patch - config.net_average_image\n            net.update(torch.Tensor(np.expand_dims(target, axis=0)).cuda(), lr=config.interp_factor)\n\n            res.append(cxy_wh_2_rect1(target_pos, target_sz))  # 1-index\n\n            if visualization:\n                im_show = cv2.cvtColor(im, cv2.COLOR_RGB2BGR)\n                cv2.rectangle(im_show, (int(target_pos[0] - target_sz[0] / 2), int(target_pos[1] - target_sz[1] / 2)),\n                              (int(target_pos[0] + target_sz[0] / 2), int(target_pos[1] + target_sz[1] / 2)),\n                              (0, 255, 0), 3)\n                cv2.putText(im_show, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2, cv2.LINE_AA)\n                cv2.imshow(video, im_show)\n                cv2.waitKey(1)\n\n        toc = time.time() - tic\n        fps = n_images / toc\n        speed.append(fps)\n        print('{:3d} Video: {:12s} Time: {:3.1f}s\\tSpeed: {:3.1f}fps'.format(video_id, video, toc, fps))\n\n        # save result\n        test_path = join('result', dataset, 'DCFNet_test')\n        if not isdir(test_path): makedirs(test_path)\n        result_path = join(test_path, video + '.txt')\n        with open(result_path, 'w') as f:\n            for x in res:\n                f.write(','.join(['{:.2f}'.format(i) for i in x]) + '\\n')\n\n    print('***Total Mean Speed: {:3.1f} (FPS)***'.format(np.mean(speed)))\n\n    eval_auc(dataset, 'DCFNet_test', 0, 1)\n"""
track/eval_otb.py,0,"b'import sys\nimport json\nimport os\nimport glob\nfrom os.path import join as fullfile\nimport numpy as np\n\n\ndef overlap_ratio(rect1, rect2):\n    \'\'\'\n    Compute overlap ratio between two rects\n    - rect: 1d array of [x,y,w,h] or\n            2d array of N x [x,y,w,h]\n    \'\'\'\n\n    if rect1.ndim==1:\n        rect1 = rect1[None,:]\n    if rect2.ndim==1:\n        rect2 = rect2[None,:]\n\n    left = np.maximum(rect1[:,0], rect2[:,0])\n    right = np.minimum(rect1[:,0]+rect1[:,2], rect2[:,0]+rect2[:,2])\n    top = np.maximum(rect1[:,1], rect2[:,1])\n    bottom = np.minimum(rect1[:,1]+rect1[:,3], rect2[:,1]+rect2[:,3])\n\n    intersect = np.maximum(0,right - left) * np.maximum(0,bottom - top)\n    union = rect1[:,2]*rect1[:,3] + rect2[:,2]*rect2[:,3] - intersect\n    iou = np.clip(intersect / union, 0, 1)\n    return iou\n\n\ndef compute_success_overlap(gt_bb, result_bb):\n    thresholds_overlap = np.arange(0, 1.05, 0.05)\n    n_frame = len(gt_bb)\n    success = np.zeros(len(thresholds_overlap))\n    iou = overlap_ratio(gt_bb, result_bb)\n    for i in range(len(thresholds_overlap)):\n        success[i] = sum(iou > thresholds_overlap[i]) / float(n_frame)\n    return success\n\n\ndef compute_success_error(gt_center, result_center):\n    thresholds_error = np.arange(0, 51, 1)\n    n_frame = len(gt_center)\n    success = np.zeros(len(thresholds_error))\n    dist = np.sqrt(np.sum(np.power(gt_center - result_center, 2), axis=1))\n    for i in range(len(thresholds_error)):\n        success[i] = sum(dist <= thresholds_error[i]) / float(n_frame)\n    return success\n\n\ndef get_result_bb(arch, seq):\n    result_path = fullfile(arch, seq + \'.txt\')\n    temp = np.loadtxt(result_path, delimiter=\',\').astype(np.float)\n    return np.array(temp)\n\n\ndef convert_bb_to_center(bboxes):\n    return np.array([(bboxes[:, 0] + (bboxes[:, 2] - 1) / 2),\n                     (bboxes[:, 1] + (bboxes[:, 3] - 1) / 2)]).T\n\n\ndef eval_auc(dataset=\'OTB2015\', tracker_reg=\'S*\', start=0, end=1e6):\n    list_path = os.path.join(\'dataset\', dataset + \'.json\')\n    annos = json.load(open(list_path, \'r\'))\n    seqs = annos.keys()\n\n    OTB2013 = [\'carDark\', \'car4\', \'david\', \'david2\', \'sylvester\', \'trellis\', \'fish\', \'mhyang\', \'soccer\', \'matrix\',\n               \'ironman\', \'deer\', \'skating1\', \'shaking\', \'singer1\', \'singer2\', \'coke\', \'bolt\', \'boy\', \'dudek\',\n               \'crossing\', \'couple\', \'football1\', \'jogging_1\', \'jogging_2\', \'doll\', \'girl\', \'walking2\', \'walking\',\n               \'fleetface\', \'freeman1\', \'freeman3\', \'freeman4\', \'david3\', \'jumping\', \'carScale\', \'skiing\', \'dog1\',\n               \'suv\', \'motorRolling\', \'mountainBike\', \'lemming\', \'liquor\', \'woman\', \'faceocc1\', \'faceocc2\',\n               \'basketball\', \'football\', \'subway\', \'tiger1\', \'tiger2\']\n\n    OTB2015 = [\'carDark\', \'car4\', \'david\', \'david2\', \'sylvester\', \'trellis\', \'fish\', \'mhyang\', \'soccer\', \'matrix\',\n               \'ironman\', \'deer\', \'skating1\', \'shaking\', \'singer1\', \'singer2\', \'coke\', \'bolt\', \'boy\', \'dudek\',\n               \'crossing\', \'couple\', \'football1\', \'jogging_1\', \'jogging_2\', \'doll\', \'girl\', \'walking2\', \'walking\',\n               \'fleetface\', \'freeman1\', \'freeman3\', \'freeman4\', \'david3\', \'jumping\', \'carScale\', \'skiing\', \'dog1\',\n               \'suv\', \'motorRolling\', \'mountainBike\', \'lemming\', \'liquor\', \'woman\', \'faceocc1\', \'faceocc2\',\n               \'basketball\', \'football\', \'subway\', \'tiger1\', \'tiger2\', \'clifBar\', \'biker\', \'bird1\', \'blurBody\',\n               \'blurCar2\', \'blurFace\', \'blurOwl\', \'box\', \'car1\', \'crowds\', \'diving\', \'dragonBaby\', \'human3\', \'human4_2\',\n               \'human6\', \'human9\', \'jump\', \'panda\', \'redTeam\', \'skating2_1\', \'skating2_2\', \'surfer\', \'bird2\',\n               \'blurCar1\', \'blurCar3\', \'blurCar4\', \'board\', \'bolt2\', \'car2\', \'car24\', \'coupon\', \'dancer\', \'dancer2\',\n               \'dog\', \'girl2\', \'gym\', \'human2\', \'human5\', \'human7\', \'human8\', \'kiteSurf\', \'man\', \'rubik\', \'skater\',\n               \'skater2\', \'toy\', \'trans\', \'twinnings\', \'vase\']\n\n    trackers = glob.glob(fullfile(\'result\', dataset, tracker_reg))\n    trackers = trackers[start:min(end, len(trackers))]\n\n    n_seq = len(seqs)\n    thresholds_overlap = np.arange(0, 1.05, 0.05)\n    # thresholds_error = np.arange(0, 51, 1)\n\n    success_overlap = np.zeros((n_seq, len(trackers), len(thresholds_overlap)))\n    # success_error = np.zeros((n_seq, len(trackers), len(thresholds_error)))\n    for i in range(n_seq):\n        seq = seqs[i]\n        gt_rect = np.array(annos[seq][\'gt_rect\']).astype(np.float)\n        gt_center = convert_bb_to_center(gt_rect)\n        for j in range(len(trackers)):\n            tracker = trackers[j]\n            print(\'{:d} processing:{} tracker: {}\'.format(i, seq, tracker))\n            bb = get_result_bb(tracker, seq)\n            center = convert_bb_to_center(bb)\n            success_overlap[i][j] = compute_success_overlap(gt_rect, bb)\n            # success_error[i][j] = compute_success_error(gt_center, center)\n\n    print(\'Success Overlap\')\n\n    if \'OTB2015\' == dataset:\n        OTB2013_id = []\n        for i in range(n_seq):\n            if seqs[i] in OTB2013:\n                OTB2013_id.append(i)\n        max_auc_OTB2013 = 0.\n        max_name_OTB2013 = \'\'\n        for i in range(len(trackers)):\n            auc = success_overlap[OTB2013_id, i, :].mean()\n            if auc > max_auc_OTB2013:\n                max_auc_OTB2013 = auc\n                max_name_OTB2013 = trackers[i]\n            print(\'%s(%.4f)\' % (trackers[i], auc))\n\n        max_auc = 0.\n        max_name = \'\'\n        for i in range(len(trackers)):\n            auc = success_overlap[:, i, :].mean()\n            if auc > max_auc:\n                max_auc = auc\n                max_name = trackers[i]\n            print(\'%s(%.4f)\' % (trackers[i], auc))\n\n        print(\'\\nOTB2013 Best: %s(%.4f)\' % (max_name_OTB2013, max_auc_OTB2013))\n        print(\'\\nOTB2015 Best: %s(%.4f)\' % (max_name, max_auc))\n    else:\n        max_auc = 0.\n        max_name = \'\'\n        for i in range(len(trackers)):\n            auc = success_overlap[:, i, :].mean()\n            if auc > max_auc:\n                max_auc = auc\n                max_name = trackers[i]\n            print(\'%s(%.4f)\' % (trackers[i], auc))\n\n        print(\'\\n%s Best: %s(%.4f)\' % (dataset, max_name, max_auc))\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) < 5:\n        print(\'python eval_otb.py OTB2015 DCFNet_test* 0 10\')\n        exit()\n    dataset = sys.argv[1]\n    tracker_reg = sys.argv[2]\n    start = int(sys.argv[3])\n    end = int(sys.argv[4])\n    eval_auc(dataset, tracker_reg, start, end)\n'"
track/net.py,17,"b""import torch.nn as nn\nimport torch  # pytorch 0.4.0! fft\nimport numpy as np\nimport cv2\n\n\ndef complex_mul(x, z):\n    out_real = x[..., 0] * z[..., 0] - x[..., 1] * z[..., 1]\n    out_imag = x[..., 0] * z[..., 1] + x[..., 1] * z[..., 0]\n    return torch.stack((out_real, out_imag), -1)\n\n\ndef complex_mulconj(x, z):\n    out_real = x[..., 0] * z[..., 0] + x[..., 1] * z[..., 1]\n    out_imag = x[..., 1] * z[..., 0] - x[..., 0] * z[..., 1]\n    return torch.stack((out_real, out_imag), -1)\n\n\nclass DCFNetFeature(nn.Module):\n    def __init__(self):\n        super(DCFNetFeature, self).__init__()\n        self.feature = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 3, padding=1),\n            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=1),\n        )\n\n    def forward(self, x):\n        return self.feature(x)\n\n\nclass DCFNet(nn.Module):\n    def __init__(self, config=None):\n        super(DCFNet, self).__init__()\n        self.feature = DCFNetFeature()\n        self.model_alphaf = []\n        self.model_xf = []\n        self.config = config\n\n    def forward(self, x):\n        x = self.feature(x) * self.config.cos_window\n        xf = torch.rfft(x, signal_ndim=2)\n        kxzf = torch.sum(complex_mulconj(xf, self.model_zf), dim=1, keepdim=True)\n        response = torch.irfft(complex_mul(kxzf, self.model_alphaf), signal_ndim=2)\n        # r_max = torch.max(response)\n        # cv2.imshow('response', response[0, 0].data.cpu().numpy())\n        # cv2.waitKey(0)\n        return response\n\n    def update(self, z, lr=1.):\n        z = self.feature(z) * self.config.cos_window\n        zf = torch.rfft(z, signal_ndim=2)\n        kzzf = torch.sum(torch.sum(zf ** 2, dim=4, keepdim=True), dim=1, keepdim=True)\n        alphaf = self.config.yf / (kzzf + self.config.lambda0)\n        if lr > 0.99:\n            self.model_alphaf = alphaf\n            self.model_zf = zf\n        else:\n            self.model_alphaf = (1 - lr) * self.model_alphaf.data + lr * alphaf.data\n            self.model_zf = (1 - lr) * self.model_zf.data + lr * zf.data\n\n    def load_param(self, path='param.pth'):\n        checkpoint = torch.load(path)\n        if 'state_dict' in checkpoint.keys():  # from training result\n            state_dict = checkpoint['state_dict'] \n            if 'module' in state_dict.keys()[0]:  # train with nn.DataParallel\n                from collections import OrderedDict\n                new_state_dict = OrderedDict()\n                for k, v in state_dict.items():\n                    name = k[7:]  # remove `module.`\n                    new_state_dict[name] = v\n                self.load_state_dict(new_state_dict)\n            else:\n                self.load_state_dict(state_dict)\n        else:\n            self.feature.load_state_dict(checkpoint)\n\n\nif __name__ == '__main__':\n\n    # network test\n    net = DCFNetFeature()\n    net.eval()\n    for idx, m in enumerate(net.modules()):\n        print(idx, '->', m)\n    for name, param in net.named_parameters():\n        if 'bias' in name or 'weight' in name:\n            print(param.size())\n    from scipy import io\n    import numpy as np\n    p = io.loadmat('net_param.mat')\n    x = p['res'][0][0][:,:,::-1].copy()\n    x_out = p['res'][0][-1]\n    from collections import OrderedDict\n    pth_state_dict = OrderedDict()\n\n    match_dict = dict()\n    match_dict['feature.0.weight'] = 'conv1_w'\n    match_dict['feature.0.bias'] = 'conv1_b'\n    match_dict['feature.2.weight'] = 'conv2_w'\n    match_dict['feature.2.bias'] = 'conv2_b'\n\n    for var_name in net.state_dict().keys():\n        print var_name\n        key_in_model = match_dict[var_name]\n        param_in_model = var_name.rsplit('.', 1)[1]\n        if 'weight' in var_name:\n            pth_state_dict[var_name] = torch.Tensor(np.transpose(p[key_in_model],(3,2,0,1)))\n        elif 'bias' in var_name:\n            pth_state_dict[var_name] = torch.Tensor(np.squeeze(p[key_in_model]))\n        if var_name == 'feature.0.weight':\n            weight = pth_state_dict[var_name].data.numpy()\n            weight = weight[:, ::-1, :, :].copy()  # cv2 bgr input\n            pth_state_dict[var_name] = torch.Tensor(weight)\n\n\n    torch.save(pth_state_dict, 'param.pth')\n    net.load_state_dict(torch.load('param.pth'))\n    x_t = torch.Tensor(np.expand_dims(np.transpose(x,(2,0,1)), axis=0))\n    x_pred = net(x_t).data.numpy()\n    pred_error = np.sum(np.abs(np.transpose(x_pred,(0,2,3,1)).reshape(-1) - x_out.reshape(-1)))\n\n    x_fft = torch.rfft(x_t, signal_ndim=2, onesided=False)\n\n\n    print('model_transfer_error:{:.5f}'.format(pred_error))\n\n\n"""
track/tune_otb.py,0,"b""import argparse\nimport cv2\nimport numpy as np\nfrom os import makedirs\nfrom os.path import isfile, isdir, join\nfrom util import cxy_wh_2_rect1\nimport torch\nimport json\nfrom DCFNet import *\n\nparser = argparse.ArgumentParser(description='Tune parameters for DCFNet tracker on OTB2015')\nparser.add_argument('-v', '--visualization', dest='visualization', action='store_true',\n                    help='whether visualize result')\n\nargs = parser.parse_args()\n\n\ndef tune_otb(param):\n    regions = []  # result and states[1 init / 2 lost / 0 skip]\n    # save result\n    benchmark_result_path = join('result', param['dataset'])\n    tracker_path = join(benchmark_result_path, (param['network_name'] +\n                        '_scale_step_{:.3f}'.format(param['config'].scale_step) +\n                        '_scale_penalty_{:.3f}'.format(param['config'].scale_penalty) +\n                        '_interp_factor_{:.3f}'.format(param['config'].interp_factor)))\n    result_path = join(tracker_path, '{:s}.txt'.format(param['video']))\n    if isfile(result_path):\n        return\n    if not isdir(tracker_path): makedirs(tracker_path)\n    with open(result_path, 'w') as f:  # Occupation\n        for x in regions:\n            f.write('')\n\n    ims = param['ims']\n    toc = 0\n    for f, im in enumerate(ims):\n        tic = cv2.getTickCount()\n        if f == 0:  # init\n            init_rect = p['init_rect']\n            tracker = DCFNetTraker(ims[f], init_rect, config=param['config'])\n            regions.append(init_rect)\n        else:  # tracking\n            rect = tracker.track(ims[f])\n            regions.append(rect)\n        toc += cv2.getTickCount() - tic\n\n        if args.visualization:  # visualization (skip lost frame)\n            if f == 0: cv2.destroyAllWindows()\n            location = [int(l) for l in location]  # int\n            cv2.rectangle(im, (location[0], location[1]), (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n            cv2.putText(im, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n\n            cv2.imshow(video, im)\n            cv2.waitKey(1)\n    toc /= cv2.getTickFrequency()\n    print('{:2d} Video: {:12s} Time: {:2.1f}s Speed: {:3.1f}fps'.format(v, video, toc, f / toc))\n    regions = np.array(regions)\n    regions[:,:2] += 1  # 1-index\n    with open(result_path, 'w') as f:\n        for x in regions:\n            f.write(','.join(['{:.2f}'.format(i) for i in x]) + '\\n')\n\n\nparams = {'dataset':['OTB2013'], 'network':['param.pth'],\n          'scale_step':np.arange(1.01, 1.05, 0.005, np.float32),\n          'scale_penalty':np.arange(0.98, 1.0, 0.025, np.float32),\n          'interp_factor':np.arange(0.001, 0.015, 0.001, np.float32)}\n\np = dict()\np['config'] = TrackerConfig()\nfor network in params['network']:\n    p['network_name'] = network\n    np.random.shuffle(params['dataset'])\n    for dataset in params['dataset']:\n        base_path = join('dataset', dataset)\n        json_path = join('dataset', dataset+'.json')\n        annos = json.load(open(json_path, 'r'))\n        videos = annos.keys()\n        p['dataset'] = dataset\n        np.random.shuffle(videos)\n        for v, video in enumerate(videos):\n            p['v'] = v\n            p['video'] = video\n            video_path_name = annos[video]['name']\n            init_rect = np.array(annos[video]['init_rect']).astype(np.float)\n            image_files = [join(base_path, video_path_name, 'img', im_f) for im_f in annos[video]['image_files']]\n            target_pos = np.array([init_rect[0] + init_rect[2] / 2 -1 , init_rect[1] + init_rect[3] / 2 -1])  # 0-index\n            target_sz = np.array([init_rect[2], init_rect[3]])\n            ims = []\n            for image_file in image_files:\n                im = cv2.imread(image_file)\n                if im.shape[2] == 1:\n                    cv2.cvtColor(im, im, cv2.COLOR_GRAY2RGB)\n                ims.append(im)\n            p['ims'] = ims\n            p['init_rect'] = init_rect\n\n            np.random.shuffle(params['scale_step'])\n            np.random.shuffle(params['scale_penalty'])\n            np.random.shuffle(params['interp_factor'])\n            for scale_step in params['scale_step']:\n                for scale_penalty in params['scale_penalty']:\n                    for interp_factor in params['interp_factor']:\n                        p['config'].scale_step = float(scale_step)\n                        p['config'].scale_penalty = float(scale_penalty)\n                        p['config'].interp_factor = float(interp_factor)\n                        tune_otb(p)\n"""
track/util.py,0,"b""import numpy as np\nimport cv2\n\n\ndef cxy_wh_2_rect1(pos, sz):\n    return np.array([pos[0]-sz[0]/2+1, pos[1]-sz[1]/2+1, sz[0], sz[1]])  # 1-index\n\n\ndef rect1_2_cxy_wh(rect):\n    return np.array([rect[0]+rect[2]/2-1, rect[1]+rect[3]/2-1]), np.array([rect[2], rect[3]])  # 0-index\n\n\ndef cxy_wh_2_bbox(cxy, wh):\n    return np.array([cxy[0]-wh[0]/2, cxy[1]-wh[1]/2, cxy[0]+wh[0]/2, cxy[1]+wh[1]/2])  # 0-index\n\n\ndef gaussian_shaped_labels(sigma, sz):\n    x, y = np.meshgrid(np.arange(1, sz[0]+1) - np.floor(float(sz[0]) / 2), np.arange(1, sz[1]+1) - np.floor(float(sz[1]) / 2))\n    d = x ** 2 + y ** 2\n    g = np.exp(-0.5 / (sigma ** 2) * d)\n    g = np.roll(g, int(-np.floor(float(sz[0]) / 2.) + 1), axis=0)\n    g = np.roll(g, int(-np.floor(float(sz[1]) / 2.) + 1), axis=1)\n    return g\n\n\ndef crop_chw(image, bbox, out_sz, padding=(0, 0, 0)):\n    a = (out_sz-1) / (bbox[2]-bbox[0])\n    b = (out_sz-1) / (bbox[3]-bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz), borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return np.transpose(crop, (2, 0, 1))\n\n\nif __name__ == '__main__':\n    a = gaussian_shaped_labels(10, [5,5])\n    print a"""
train/dataset.py,1,"b""import torch.utils.data as data\nfrom os.path import join\nimport cv2\nimport json\nimport numpy as np\n\n\nclass VID(data.Dataset):\n    def __init__(self, file='dataset/dataset.json', root='dataset/crop_125_2.0', range=10, train=True):\n        self.imdb = json.load(open(file, 'r'))\n        self.root = root\n        self.range = range\n        self.train = train\n        self.mean = np.expand_dims(np.expand_dims(np.array([109, 120, 119]), axis=1), axis=1).astype(np.float32)\n\n    def __getitem__(self, item):\n        if self.train:\n            target_id = self.imdb['train_set'][item]\n        else:\n            target_id = self.imdb['val_set'][item]\n\n        # range_down = self.imdb['down_index'][target_id]\n        range_up = self.imdb['up_index'][target_id]\n        # search_id = np.random.randint(-min(range_down, self.range), min(range_up, self.range)) + target_id\n        search_id = np.random.randint(1, min(range_up, self.range+1)) + target_id\n\n        target = cv2.imread(join(self.root, '{:08d}.jpg'.format(target_id)))\n        search = cv2.imread(join(self.root, '{:08d}.jpg'.format(search_id)))\n\n        target = np.transpose(target, (2, 0, 1)).astype(np.float32) - self.mean\n        search = np.transpose(search, (2, 0, 1)).astype(np.float32) - self.mean\n\n        return target, search\n\n    def __len__(self):\n        if self.train:\n            return len(self.imdb['train_set'])\n        else:\n            return len(self.imdb['val_set'])\n\n\nif __name__ == '__main__':\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    data = VID(train=True)\n    n = len(data)\n    fig = plt.figure(1)\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    for i in range(n):\n        z, x = data[i]\n        z, x = np.transpose(z, (1, 2, 0)).astype(np.uint8), np.transpose(x, (1, 2, 0)).astype(np.uint8)\n        zx = np.concatenate((z, x), axis=1)\n\n        ax.imshow(cv2.cvtColor(zx, cv2.COLOR_BGR2RGB))\n        p = patches.Rectangle(\n            (125/3, 125/3), 125/3, 125/3, fill=False, clip_on=False, linewidth=2, edgecolor='g')\n        ax.add_patch(p)\n        p = patches.Rectangle(\n            (125 / 3+125, 125 / 3), 125 / 3, 125 / 3, fill=False, clip_on=False, linewidth=2, edgecolor='r')\n        ax.add_patch(p)\n        plt.pause(0.5)\n"""
train/net.py,8,"b""import torch  # pytorch 0.4.0! fft\nimport torch.nn as nn\n\n\ndef complex_mul(x, z):\n    out_real = x[..., 0] * z[..., 0] - x[..., 1] * z[..., 1]\n    out_imag = x[..., 0] * z[..., 1] + x[..., 1] * z[..., 0]\n    return torch.stack((out_real, out_imag), -1)\n\n\ndef complex_mulconj(x, z):\n    out_real = x[..., 0] * z[..., 0] + x[..., 1] * z[..., 1]\n    out_imag = x[..., 1] * z[..., 0] - x[..., 0] * z[..., 1]\n    return torch.stack((out_real, out_imag), -1)\n\n\nclass DCFNetFeature(nn.Module):\n    def __init__(self):\n        super(DCFNetFeature, self).__init__()\n        self.feature = nn.Sequential(\n            nn.Conv2d(3, 32, 3),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 3),\n            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=1),\n        )\n\n    def forward(self, x):\n        return self.feature(x)\n\n\nclass DCFNet(nn.Module):\n    def __init__(self, config=None):\n        super(DCFNet, self).__init__()\n        self.feature = DCFNetFeature()\n        self.yf = config.yf.clone()\n        self.lambda0 = config.lambda0\n\n    def forward(self, z, x):\n        z = self.feature(z)\n        x = self.feature(x)\n        zf = torch.rfft(z, signal_ndim=2)\n        xf = torch.rfft(x, signal_ndim=2)\n\n        kzzf = torch.sum(torch.sum(zf ** 2, dim=4, keepdim=True), dim=1, keepdim=True)\n        kxzf = torch.sum(complex_mulconj(xf, zf), dim=1, keepdim=True)\n        alphaf = self.yf.to(device=z.device) / (kzzf + self.lambda0)  # very Ugly\n        response = torch.irfft(complex_mul(kxzf, alphaf), signal_ndim=2)\n        return response\n\n\nif __name__ == '__main__':\n\n    # network test\n    net = DCFNet()\n    net.eval()\n\n\n\n"""
train/train_DCFNet.py,14,"b'import argparse\nimport shutil\nfrom os.path import join, isdir, isfile\nfrom os import makedirs\n\nfrom dataset import VID\nfrom net import DCFNet\nimport torch\nfrom torch.utils.data import dataloader\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport time\n\n\nparser = argparse.ArgumentParser(description=\'Training DCFNet in Pytorch 0.4.0\')\nparser.add_argument(\'--input_sz\', dest=\'input_sz\', default=125, type=int, help=\'crop input size\')\nparser.add_argument(\'--padding\', dest=\'padding\', default=2.0, type=float, help=\'crop padding size\')\nparser.add_argument(\'--range\', dest=\'range\', default=10, type=int, help=\'select range\')\nparser.add_argument(\'--epochs\', default=50, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'-j\', \'--workers\', default=8, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 8)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=32, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 32)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.01, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=5e-5, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 5e-5)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--save\', \'-s\', default=\'./work\', type=str, help=\'directory for saving\')\n\nargs = parser.parse_args()\n\nprint args\nbest_loss = 1e6\n\n\ndef gaussian_shaped_labels(sigma, sz):\n    x, y = np.meshgrid(np.arange(1, sz[0]+1) - np.floor(float(sz[0]) / 2), np.arange(1, sz[1]+1) - np.floor(float(sz[1]) / 2))\n    d = x ** 2 + y ** 2\n    g = np.exp(-0.5 / (sigma ** 2) * d)\n    g = np.roll(g, int(-np.floor(float(sz[0]) / 2.) + 1), axis=0)\n    g = np.roll(g, int(-np.floor(float(sz[1]) / 2.) + 1), axis=1)\n    return g.astype(np.float32)\n\n\nclass TrackerConfig(object):\n    crop_sz = 125\n    output_sz = 121\n\n    lambda0 = 1e-4\n    padding = 2.0\n    output_sigma_factor = 0.1\n\n    output_sigma = crop_sz / (1 + padding) * output_sigma_factor\n    y = gaussian_shaped_labels(output_sigma, [output_sz, output_sz])\n    yf = torch.rfft(torch.Tensor(y).view(1, 1, output_sz, output_sz).cuda(), signal_ndim=2)\n    # cos_window = torch.Tensor(np.outer(np.hanning(crop_sz), np.hanning(crop_sz))).cuda()  # train without cos window\n\n\nconfig = TrackerConfig()\n\nmodel = DCFNet(config=config)\nmodel.cuda()\ngpu_num = torch.cuda.device_count()\nprint(\'GPU NUM: {:2d}\'.format(gpu_num))\nif gpu_num > 1:\n    model = torch.nn.DataParallel(model, list(range(gpu_num))).cuda()\n\ncriterion = nn.MSELoss(size_average=False).cuda()\n\noptimizer = torch.optim.SGD(model.parameters(), args.lr,\n                            momentum=args.momentum,\n                            weight_decay=args.weight_decay)\n\ntarget = torch.Tensor(config.y).cuda().unsqueeze(0).unsqueeze(0).repeat(args.batch_size * gpu_num, 1, 1, 1)  # for training\n# optionally resume from a checkpoint\nif args.resume:\n    if isfile(args.resume):\n        print(""=> loading checkpoint \'{}\'"".format(args.resume))\n        checkpoint = torch.load(args.resume)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_loss = checkpoint[\'best_loss\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {})""\n              .format(args.resume, checkpoint[\'epoch\']))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\ncudnn.benchmark = True\n\n# training data\ncrop_base_path = join(\'dataset\', \'crop_{:d}_{:1.1f}\'.format(args.input_sz, args.padding))\nif not isdir(crop_base_path):\n    print(\'please run gen_training_data.py --output_size {:d} --padding {:.1f}!\'.format(args.input_sz, args.padding))\n    exit()\n\nsave_path = join(args.save, \'crop_{:d}_{:1.1f}\'.format(args.input_sz, args.padding))\nif not isdir(save_path):\n    makedirs(save_path)\n\ntrain_dataset = VID(root=crop_base_path, train=True, range=args.range)\nval_dataset = VID(root=crop_base_path, train=False, range=args.range)\n\ntrain_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size*gpu_num, shuffle=True,\n        num_workers=args.workers, pin_memory=True, drop_last=True)\n\nval_loader = torch.utils.data.DataLoader(\n        val_dataset, batch_size=args.batch_size*gpu_num, shuffle=False,\n        num_workers=args.workers, pin_memory=True, drop_last=True)\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    lr = np.logspace(-2, -5, num=args.epochs)[epoch]\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef save_checkpoint(state, is_best, filename=join(save_path, \'checkpoint.pth.tar\')):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, join(save_path, \'model_best.pth.tar\'))\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (template, search) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        template = template.cuda(non_blocking=True)\n        search = search.cuda(non_blocking=True)\n\n        # compute output\n        output = model(template, search)\n        loss = criterion(output, target)/template.size(0)\n\n        # measure accuracy and record loss\n        losses.update(loss.item())\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses))\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (template, search) in enumerate(val_loader):\n\n            # compute output\n            template = template.cuda(non_blocking=True)\n            search = search.cuda(non_blocking=True)\n\n            # compute output\n            output = model(template, search)\n            loss = criterion(output, target)/(args.batch_size * gpu_num)\n\n            # measure accuracy and record loss\n            losses.update(loss.item())\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print(\'Test: [{0}/{1}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'.format(\n                       i, len(val_loader), batch_time=batch_time, loss=losses))\n\n        print(\' * Loss {loss.val:.4f} ({loss.avg:.4f})\'.format(loss=losses))\n\n    return losses.avg\n\n\nfor epoch in range(args.start_epoch, args.epochs):\n    adjust_learning_rate(optimizer, epoch)\n\n    # train for one epoch\n    train(train_loader, model, criterion, optimizer, epoch)\n\n    # evaluate on validation set\n    loss = validate(val_loader, model, criterion)\n\n    # remember best loss and save checkpoint\n    is_best = loss < best_loss\n    best_loss = min(best_loss, loss)\n    save_checkpoint({\n        \'epoch\': epoch + 1,\n        \'state_dict\': model.state_dict(),\n        \'best_loss\': best_loss,\n        \'optimizer\': optimizer.state_dict(),\n    }, is_best)\n'"
track/dataset/gen_otb2013.py,0,"b""import json\n\nOTB2015 = json.load(open('OTB2015.json', 'r'))\nvideos = OTB2015.keys()\n\nOTB2013 = dict()\nfor v in videos:\n   if v in ['carDark', 'car4', 'david', 'david2', 'sylvester', 'trellis', 'fish', 'mhyang', 'soccer', 'matrix',\n            'ironman', 'deer', 'skating1', 'shaking', 'singer1', 'singer2', 'coke', 'bolt', 'boy', 'dudek',\n            'crossing', 'couple', 'football1', 'jogging_1', 'jogging_2', 'doll', 'girl', 'walking2', 'walking',\n            'fleetface', 'freeman1', 'freeman3', 'freeman4', 'david3', 'jumping', 'carScale', 'skiing', 'dog1',\n            'suv', 'motorRolling', 'mountainBike', 'lemming', 'liquor', 'woman', 'faceocc1', 'faceocc2',\n            'basketball', 'football', 'subway', 'tiger1', 'tiger2']:\n        OTB2013[v] = OTB2015[v]\n\n\njson.dump(OTB2013, open('OTB2013.json', 'w'), indent=2) \n\n"""
train/dataset/compute-image-mean.py,0,"b'import argparse\nimport numpy as np\nimport os\nimport time\nimport glob\n\nfrom skimage import io\nimport cv2\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--meanPrefix\', default=\'mean_img\', type=str, help=""Prefix of the mean file."")\n    parser.add_argument(\'--imageDir\', default=\'crop_125_2.0\', type=str, help=""Directory of images to read."")\n    args = parser.parse_args()\n\n    mean = np.zeros((1, 3, 125, 125))\n    N = 0\n    opencv_backend = True\n    beginTime = time.time()\n    files = glob.glob(os.path.join(args.imageDir, \'*.jpg\'))\n    for file in files:\n        if opencv_backend:\n            img = cv2.imread(file)\n        else:\n            img = io.imread(file)\n        if img.shape == (125, 125, 3):\n            mean[0][0] += img[:, :, 0]\n            mean[0][1] += img[:, :, 1]\n            mean[0][2] += img[:, :, 2]\n            N += 1\n            if N % 1000 == 0:\n                elapsed = time.time() - beginTime\n                print(""Processed {} images in {:.2f} seconds. ""\n                      ""{:.2f} images/second."".format(N, elapsed, N / elapsed))\n    mean[0] /= N\n\n    meanImg = np.transpose(mean[0].astype(np.uint8), (1, 2, 0))\n    if opencv_backend:\n        cv2.imwrite(""{}.png"".format(args.meanPrefix), meanImg)\n    else:\n        io.imsave(""{}.png"".format(args.meanPrefix), meanImg)\n\n    avg_chans = np.mean(meanImg, axis=(0, 1))\n    if opencv_backend:\n        print(""image BGR mean: {}"".format(avg_chans))\n    else:\n        print(""image RGB mean: {}"".format(avg_chans))'"
train/dataset/crop_image.py,0,"b'from os.path import join, isdir\nfrom os import mkdir\nimport argparse\nimport numpy as np\nimport json\nimport cv2\nimport time\n\nparse = argparse.ArgumentParser(description=\'Generate training data (cropped) for DCFNet_pytorch\')\nparse.add_argument(\'-v\', \'--visual\', dest=\'visual\', action=\'store_true\', help=\'whether visualise crop\')\nparse.add_argument(\'-o\', \'--output_size\', dest=\'output_size\', default=125, type=int, help=\'crop output size\')\nparse.add_argument(\'-p\', \'--padding\', dest=\'padding\', default=2, type=float, help=\'crop padding size\')\n\nargs = parse.parse_args()\n\nprint args\n\n\ndef crop_hwc(image, bbox, out_sz, padding=(0, 0, 0)):\n    bbox = [float(x) for x in bbox]\n    a = (out_sz-1) / (bbox[2]-bbox[0])\n    b = (out_sz-1) / (bbox[3]-bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz), borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return crop\n\n\ndef cxy_wh_2_bbox(cxy, wh):\n    return np.array([cxy[0] - wh[0] / 2, cxy[1] - wh[1] / 2, cxy[0] + wh[0] / 2, cxy[1] + wh[1] / 2])  # 0-index\n\n\nsnaps = json.load(open(\'snippet.json\', \'r\'))\n\nnum_all_frame = 546315  # cat snippet.json | grep bbox |wc -l\nnum_val = 1000\n# crop image\nlmdb = dict()\nlmdb[\'down_index\'] = np.zeros(num_all_frame, np.int)  # buff\nlmdb[\'up_index\'] = np.zeros(num_all_frame, np.int)\n\ncrop_base_path = \'crop_{:d}_{:1.1f}\'.format(args.output_size, args.padding)\nif not isdir(crop_base_path):\n    mkdir(crop_base_path)\n\ncount = 0\nbegin_time = time.time()\nfor snap in snaps:\n    frames = snap[\'frame\']\n    n_frames = len(frames)\n    for f, frame in enumerate(frames):\n        img_path = join(snap[\'base_path\'], frame[\'img_path\'])\n        im = cv2.imread(img_path)\n        avg_chans = np.mean(im, axis=(0, 1))\n        bbox = frame[\'obj\'][\'bbox\']\n\n        target_pos = [(bbox[2] + bbox[0])/2, (bbox[3] + bbox[1])/2]\n        target_sz = np.array([bbox[2] - bbox[0], bbox[3] - bbox[1]])\n        window_sz = target_sz * (1 + args.padding)\n        crop_bbox = cxy_wh_2_bbox(target_pos, window_sz)\n        patch = crop_hwc(im, crop_bbox, args.output_size)\n        cv2.imwrite(join(crop_base_path, \'{:08d}.jpg\'.format(count)), patch)\n        # cv2.imwrite(\'crop.jpg\'.format(count), patch)\n\n        lmdb[\'down_index\'][count] = f\n        lmdb[\'up_index\'][count] = n_frames - f\n        count += 1\n        if count % 100 == 0:\n            elapsed = time.time() - begin_time\n            print(""Processed {} images in {:.2f} seconds. ""\n                  ""{:.2f} images/second."".format(count, elapsed, count / elapsed))\n\ntemplate_id = np.where(lmdb[\'up_index\'] > 1)[0]  # NEVER use the last frame as template! I do not like bidirectional.\nrand_split = np.random.choice(len(template_id), len(template_id))\nlmdb[\'train_set\'] = template_id[rand_split[:(len(template_id)-num_val)]]\nlmdb[\'val_set\'] = template_id[rand_split[(len(template_id)-num_val):]]\nprint len(lmdb[\'train_set\'])\nprint len(lmdb[\'val_set\'])\n\n# to list for json\nlmdb[\'train_set\'] = lmdb[\'train_set\'].tolist()\nlmdb[\'val_set\'] = lmdb[\'val_set\'].tolist()\nlmdb[\'down_index\'] = lmdb[\'down_index\'].tolist()\nlmdb[\'up_index\'] = lmdb[\'up_index\'].tolist()\n\nprint(\'lmdb json, please wait 5 seconds~\')\njson.dump(lmdb, open(\'dataset.json\', \'w\'), indent=2)\nprint(\'done!\')\n'"
train/dataset/gen_snippet.py,0,"b""import numpy as np\nimport json\n\n\ndef check_size(frame_sz, bbox):\n    min_ratio = 0.1\n    max_ratio = 0.75\n    # only accept objects >10% and <75% of the total frame\n    area_ratio = np.sqrt((bbox[2]-bbox[0])*(bbox[3]-bbox[1])/float(np.prod(frame_sz)))\n    ok = (area_ratio > min_ratio) and (area_ratio < max_ratio)\n    return ok\n\n\ndef check_borders(frame_sz, bbox):\n    dist_from_border = 0.05 * (bbox[2] - bbox[0] + bbox[3] - bbox[1])/2\n    ok = (bbox[0] > dist_from_border) and (bbox[1] > dist_from_border) and \\\n         ((frame_sz[0] - bbox[2]) > dist_from_border) and \\\n         ((frame_sz[1] - bbox[3]) > dist_from_border)\n    return ok\n\n\n# Filter out snippets\nprint('load json (raw vid info), please wait 20 seconds~')\nvid = json.load(open('vid.json', 'r'))\nsnippets = []\nn_snippets = 0\nn_videos = 0\nfor subset in vid:\n    for video in subset:\n        n_videos += 1\n        frames = video['frame']\n        id_set = []\n        id_frames = [[]] * 60  # at most 60 objects\n        for f, frame in enumerate(frames):\n            objs = frame['objs']\n            frame_sz = frame['frame_sz']\n            for obj in objs:\n                trackid = obj['trackid']\n                occluded = obj['occ']\n                bbox = obj['bbox']\n                if occluded:\n                    continue\n\n                if not(check_size(frame_sz, bbox) and check_borders(frame_sz, bbox)):\n                    continue\n\n                if obj['c'] in ['n01674464', 'n01726692', 'n04468005', 'n02062744']:\n                    continue\n\n                if trackid not in id_set:\n                    id_set.append(trackid)\n                    id_frames[trackid] = []\n                id_frames[trackid].append(f)\n\n        for selected in id_set:\n            frame_ids = sorted(id_frames[selected])\n            sequences = np.split(frame_ids, np.array(np.where(np.diff(frame_ids) > 1)[0]) + 1)\n            sequences = [s for s in sequences if len(s) > 1]  # remove isolated frame.\n            for seq in sequences:\n                snippet = dict()\n                snippet['base_path'] = video['base_path']\n                snippet['frame'] = []\n                for frame_id in seq:\n                    frame = frames[frame_id]\n                    f = dict()\n                    f['frame_sz'] = frame['frame_sz']\n                    f['img_path'] = frame['img_path']\n                    for obj in frame['objs']:\n                        if obj['trackid'] == selected:\n                            o = obj\n                            continue\n                    f['obj'] = o\n                    snippet['frame'].append(f)\n                snippets.append(snippet)\n                n_snippets += 1\n        print('video: {:d} snippets_num: {:d}'.format(n_videos, n_snippets))\n\nprint('save json (snippets), please wait 20 seconds~')\njson.dump(snippets, open('snippet.json', 'w'), indent=2)\nprint('done!')\n"""
train/dataset/parse_vid.py,0,"b""from os.path import join, isdir\nfrom os import listdir\nimport argparse\nimport json\nimport glob\nimport xml.etree.ElementTree as ET\n\nparser = argparse.ArgumentParser(description='Parse the VID Annotations for training DCFNet')\nparser.add_argument('data', metavar='DIR', help='path to VID')\nargs = parser.parse_args()\n\nprint('VID2015 Data:')\nVID_base_path = args.data\nann_base_path = join(VID_base_path, 'Annotations/VID/train/')\nimg_base_path = join(VID_base_path, 'Data/VID/train/')\nsub_sets = sorted({'a', 'b', 'c', 'd', 'e'})\n\nvid = []\nfor sub_set in sub_sets:\n    sub_set_base_path = join(ann_base_path, sub_set)\n    videos = sorted(listdir(sub_set_base_path))\n    s = []\n    for vi, video in enumerate(videos):\n        print('subset: {} video id: {:04d} / {:04d}'.format(sub_set, vi, len(videos)))\n        v = dict()\n        v['base_path'] = join(img_base_path, sub_set, video)\n        v['frame'] = []\n        video_base_path = join(sub_set_base_path, video)\n        xmls = sorted(glob.glob(join(video_base_path, '*.xml')))\n        for xml in xmls:\n            f = dict()\n            xmltree = ET.parse(xml)\n            size = xmltree.findall('size')[0]\n            frame_sz = [int(it.text) for it in size]\n            objects = xmltree.findall('object')\n            objs = []\n            for object_iter in objects:\n                trackid = int(object_iter.find('trackid').text)\n                name = (object_iter.find('name')).text\n                bndbox = object_iter.find('bndbox')\n                occluded = int(object_iter.find('occluded').text)\n                o = dict()\n                o['c'] = name\n                o['bbox'] = [int(bndbox.find('xmin').text), int(bndbox.find('ymin').text),\n                             int(bndbox.find('xmax').text), int(bndbox.find('ymax').text)]\n                o['trackid'] = trackid\n                o['occ'] = occluded\n                objs.append(o)\n            f['frame_sz'] = frame_sz\n            f['img_path'] = xml.split('/')[-1].replace('xml', 'JPEG')\n            f['objs'] = objs\n            v['frame'].append(f)\n        s.append(v)\n    vid.append(s)\nprint('save json (raw vid info), please wait 1 min~')\njson.dump(vid, open('vid.json', 'w'), indent=2)\nprint('done!')\n\n"""
