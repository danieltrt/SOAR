file_path,api_count,code
data/create_json.py,0,"b'import json\nimport os\nimport re\nimport numpy as np\nimport cv2\n\nfrom glob import glob\nfrom fire import Fire\n\ndef process(dataset_name):\n    with open(os.path.join(dataset_name, \'list.txt\'), \'r\') as f:\n        lines = f.readlines()\n    videos = [x.strip() for x in lines]\n\n    # if dataset_name == \'VOT2016\':\n    meta_data = {}\n    tags = []\n    for video in videos:\n        with open(os.path.join(dataset_name, video, ""groundtruth.txt""),\'r\') as f:\n            gt_traj = [list(map(float, x.strip().split(\',\'))) for x in f.readlines()]\n\n        img_names = sorted(glob(os.path.join(dataset_name, video, \'color\', \'*.jpg\')))\n        if len(img_names) == 0:\n            img_names = sorted(glob(os.path.join(dataset_name, video, \'*.jpg\')))\n        im = cv2.imread(img_names[0])\n        img_names = [x.split(\'/\', 1)[1] for x in img_names]\n        # tag\n        if dataset_name in [\'VOT2018\', \'VOT2019\']:\n            tag_file = os.path.join(dataset_name, video, \'camera_motion.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    camera_motion = [int(x.strip()) for x in f.readlines()]\n                    camera_motion += [0] * (len(gt_traj) - len(camera_motion))\n            else:\n                print(""File not exists: "", tag_file)\n                camera_motion = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'illum_change.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    illum_change = [int(x.strip()) for x in f.readlines()]\n                    illum_change += [0] * (len(gt_traj) - len(illum_change))\n            else:\n                print(""File not exists: "", tag_file)\n                illum_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'motion_change.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    motion_change = [int(x.strip()) for x in f.readlines()]\n                    motion_change  += [0] * (len(gt_traj) - len(motion_change))\n            else:\n                print(""File not exists: "", tag_file)\n                motion_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'size_change.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    size_change = [int(x.strip()) for x in f.readlines()]\n                    size_change  += [0] * (len(gt_traj) - len(size_change))\n            else:\n                print(""File not exists: "", tag_file)\n                size_change  = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'occlusion.tag\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    occlusion = [int(x.strip()) for x in f.readlines()]\n                    occlusion  += [0] * (len(gt_traj) - len(occlusion))\n            else:\n                print(""File not exists: "", tag_file)\n                occlusion  = [] # [0] * len(gt_traj)\n            img_files = os.path.join(\'VOT2019\', )\n            meta_data[video] = {\'video_dir\': video,\n                                \'init_rect\': gt_traj[0],\n                                \'img_names\': img_names,\n                                \'width\': im.shape[1],\n                                \'height\': im.shape[0],\n                                \'gt_rect\': gt_traj,\n                                \'camera_motion\': camera_motion,\n                                \'illum_change\': illum_change,\n                                \'motion_change\': motion_change,\n                                \'size_change\': size_change,\n                                \'occlusion\': occlusion}\n        elif \'VOT2016\' == dataset_name:\n            tag_file = os.path.join(dataset_name, video, \'camera_motion.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    camera_motion = [int(x.strip()) for x in f.readlines()]\n                    camera_motion += [0] * (len(gt_traj) - len(camera_motion))\n            else:\n                print(""File not exists: "", tag_file)\n                camera_motion = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'illum_change.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    illum_change = [int(x.strip()) for x in f.readlines()]\n                    illum_change += [0] * (len(gt_traj) - len(illum_change))\n            else:\n                print(""File not exists: "", tag_file)\n                illum_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'motion_change.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    motion_change = [int(x.strip()) for x in f.readlines()]\n                    motion_change  += [0] * (len(gt_traj) - len(motion_change))\n            else:\n                print(""File not exists: "", tag_file)\n                motion_change = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'size_change.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    size_change = [int(x.strip()) for x in f.readlines()]\n                    size_change  += [0] * (len(gt_traj) - len(size_change))\n            else:\n                print(""File not exists: "", tag_file)\n                size_change  = [] # [0] * len(gt_traj)\n\n            tag_file = os.path.join(dataset_name, video, \'occlusion.label\')\n            if os.path.exists(tag_file):\n                with open(tag_file, \'r\') as f:\n                    occlusion = [int(x.strip()) for x in f.readlines()]\n                    occlusion  += [0] * (len(gt_traj) - len(occlusion))\n            else:\n                print(""File not exists: "", tag_file)\n                occlusion  = [] # [0] * len(gt_traj)\n\n            meta_data[video] = {\'video_dir\': video,\n                                \'init_rect\': gt_traj[0],\n                                \'img_names\': img_names,\n                                \'gt_rect\': gt_traj,\n                                \'width\': im.shape[1],\n                                \'height\': im.shape[0],\n                                \'camera_motion\': camera_motion,\n                                \'illum_change\': illum_change,\n                                \'motion_change\': motion_change,\n                                \'size_change\': size_change,\n                                \'occlusion\': occlusion}\n        else:\n            meta_data[video] = {\'video_dir\': video,\n                                \'init_rect\': gt_traj[0],\n                                \'img_names\': img_names,\n                                \'gt_rect\': gt_traj,\n                                \'width\': im.shape[1],\n                                \'height\': im.shape[0]}\n\n\n    json.dump(meta_data, open(dataset_name+\'.json\', \'w\'))\n\nif __name__ == \'__main__\':\n    Fire(process)\n\n'"
datasets/__init__.py,0,b''
datasets/siam_mask_dataset.py,1,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport json\nimport random\nimport logging\nfrom os.path import join\nfrom utils.bbox_helper import *\nfrom utils.anchors import Anchors\nimport math\nimport sys\npyv = sys.version[0]\nimport cv2\nif pyv[0] == \'3\':\n    cv2.ocl.setUseOpenCL(False)\n\nlogger = logging.getLogger(\'global\')\n\n\nsample_random = random.Random()\nsample_random.seed(123456)\n\n\nclass SubDataSet(object):\n    def __init__(self, cfg):\n        for string in [\'root\', \'anno\']:\n            if string not in cfg:\n                raise Exception(\'SubDataSet need ""{}""\'.format(string))\n\n        with open(cfg[\'anno\']) as fin:\n            logger.info(""loading "" + cfg[\'anno\'])\n            self.labels = self.filter_zero(json.load(fin), cfg)\n\n            def isint(x):\n                try:\n                    int(x)\n                    return True\n                except:\n                    return False\n\n            # add frames args into labels\n            to_del = []\n            for video in self.labels:\n                for track in self.labels[video]:\n                    frames = self.labels[video][track]\n                    frames = list(map(int, filter(lambda x: isint(x), frames.keys())))\n                    frames.sort()\n                    self.labels[video][track][\'frames\'] = frames\n                    if len(frames) <= 0:\n                        logger.info(""warning {}/{} has no frames."".format(video, track))\n                        to_del.append((video, track))\n\n            # delete tracks with no frames\n            for video, track in to_del:\n                del self.labels[video][track]\n\n            # delete videos with no valid track\n            to_del = []\n            for video in self.labels:\n                if len(self.labels[video]) <= 0:\n                    logger.info(""warning {} has no tracks"".format(video))\n                    to_del.append(video)\n\n            for video in to_del:\n                del self.labels[video]\n\n            self.videos = list(self.labels.keys())\n\n            logger.info(cfg[\'anno\'] + "" loaded."")\n\n        # default args\n        self.root = ""/""\n        self.start = 0\n        self.num = len(self.labels)\n        self.num_use = self.num\n        self.frame_range = 100\n        self.mark = ""vid""\n        self.path_format = ""{}.{}.{}.jpg""\n        self.mask_format = ""{}.{}.m.png""\n\n        self.pick = []\n\n        # input args\n        self.__dict__.update(cfg)\n\n        self.has_mask = self.mark in [\'coco\', \'ytb_vos\']\n\n        self.num_use = int(self.num_use)\n\n        # shuffle\n        self.shuffle()\n\n    def filter_zero(self, anno, cfg):\n        name = cfg.get(\'mark\', \'\')\n\n        out = {}\n        tot = 0\n        new = 0\n        zero = 0\n\n        for video, tracks in anno.items():\n            new_tracks = {}\n            for trk, frames in tracks.items():\n                new_frames = {}\n                for frm, bbox in frames.items():\n                    tot += 1\n                    if len(bbox) == 4:\n                        x1, y1, x2, y2 = bbox\n                        w, h = x2 - x1, y2 - y1\n                    else:\n                        w, h = bbox\n                    if w == 0 or h == 0:\n                        logger.info(\'Error, {name} {video} {trk} {bbox}\'.format(**locals()))\n                        zero += 1\n                        continue\n                    new += 1\n                    new_frames[frm] = bbox\n\n                if len(new_frames) > 0:\n                    new_tracks[trk] = new_frames\n\n            if len(new_tracks) > 0:\n                out[video] = new_tracks\n\n        return out\n\n    def log(self):\n        logger.info(\'SubDataSet {name} start-index {start} select [{select}/{num}] path {format}\'.format(\n            name=self.mark, start=self.start, select=self.num_use, num=self.num, format=self.path_format\n        ))\n\n    def shuffle(self):\n        lists = list(range(self.start, self.start + self.num))\n\n        m = 0\n        pick = []\n        while m < self.num_use:\n            sample_random.shuffle(lists)\n            pick += lists\n            m += self.num\n\n        self.pick = pick[:self.num_use]\n        return self.pick\n\n    def get_image_anno(self, video, track, frame):\n        frame = ""{:06d}"".format(frame)\n        image_path = join(self.root, video, self.path_format.format(frame, track, \'x\'))\n        image_anno = self.labels[video][track][frame]\n\n        mask_path = join(self.root, video, self.mask_format.format(frame, track))\n\n        return image_path, image_anno, mask_path\n\n    def get_positive_pair(self, index):\n        video_name = self.videos[index]\n        video = self.labels[video_name]\n        track = random.choice(list(video.keys()))\n        track_info = video[track]\n\n        frames = track_info[\'frames\']\n\n        if \'hard\' not in track_info:\n            template_frame = random.randint(0, len(frames)-1)\n\n            left = max(template_frame - self.frame_range, 0)\n            right = min(template_frame + self.frame_range, len(frames)-1) + 1\n            search_range = frames[left:right]\n            template_frame = frames[template_frame]\n            search_frame = random.choice(search_range)\n        else:\n            search_frame = random.choice(track_info[\'hard\'])\n            left = max(search_frame - self.frame_range, 0)\n            right = min(search_frame + self.frame_range, len(frames)-1) + 1  # python [left:right+1) = [left:right]\n            template_range = frames[left:right]\n            template_frame = random.choice(template_range)\n            search_frame = frames[search_frame]\n\n        return self.get_image_anno(video_name, track, template_frame), \\\n               self.get_image_anno(video_name, track, search_frame)\n\n    def get_random_target(self, index=-1):\n        if index == -1:\n            index = random.randint(0, self.num-1)\n        video_name = self.videos[index]\n        video = self.labels[video_name]\n        track = random.choice(list(video.keys()))\n        track_info = video[track]\n\n        frames = track_info[\'frames\']\n        frame = random.choice(frames)\n\n        return self.get_image_anno(video_name, track, frame)\n\n\ndef crop_hwc(image, bbox, out_sz, padding=(0, 0, 0)):\n    bbox = [float(x) for x in bbox]\n    a = (out_sz-1) / (bbox[2]-bbox[0])\n    b = (out_sz-1) / (bbox[3]-bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz), borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return crop\n\n\nclass Augmentation:\n    def __init__(self, cfg):\n        # default args\n        self.shift = 0\n        self.scale = 0\n        self.blur = 0  # False\n        self.resize = False\n        self.rgbVar = np.array([[-0.55919361,  0.98062831, - 0.41940627],\n            [1.72091413,  0.19879334, - 1.82968581],\n            [4.64467907,  4.73710203, 4.88324118]], dtype=np.float32)\n        self.flip = 0\n\n        self.eig_vec = np.array([\n            [0.4009, 0.7192, -0.5675],\n            [-0.8140, -0.0045, -0.5808],\n            [0.4203, -0.6948, -0.5836],\n        ], dtype=np.float32)\n\n        self.eig_val = np.array([[0.2175, 0.0188, 0.0045]], np.float32)\n\n        self.__dict__.update(cfg)\n\n    @staticmethod\n    def random():\n        return random.random() * 2 - 1.0\n\n    def blur_image(self, image):\n        def rand_kernel():\n            size = np.random.randn(1)\n            size = int(np.round(size)) * 2 + 1\n            if size < 0: return None\n            if random.random() < 0.5: return None\n            size = min(size, 45)\n            kernel = np.zeros((size, size))\n            c = int(size/2)\n            wx = random.random()\n            kernel[:, c] += 1. / size * wx\n            kernel[c, :] += 1. / size * (1-wx)\n            return kernel\n\n        kernel = rand_kernel()\n\n        if kernel is not None:\n            image = cv2.filter2D(image, -1, kernel)\n        return image\n\n    def __call__(self, image, bbox, size, gray=False, mask=None):\n        if gray:\n            grayed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            image = np.zeros((grayed.shape[0], grayed.shape[1], 3), np.uint8)\n            image[:, :, 0] = image[:, :, 1] = image[:, :, 2] = grayed\n\n        shape = image.shape\n\n        crop_bbox = center2corner((shape[0]//2, shape[1]//2, size-1, size-1))\n\n        param = {}\n        if self.shift:\n            param[\'shift\'] = (Augmentation.random() * self.shift, Augmentation.random() * self.shift)\n\n        if self.scale:\n            param[\'scale\'] = ((1.0 + Augmentation.random() * self.scale), (1.0 + Augmentation.random() * self.scale))\n\n        crop_bbox, _ = aug_apply(Corner(*crop_bbox), param, shape)\n\n        x1 = crop_bbox.x1\n        y1 = crop_bbox.y1\n\n        bbox = BBox(bbox.x1 - x1, bbox.y1 - y1,\n                    bbox.x2 - x1, bbox.y2 - y1)\n\n        if self.scale:\n            scale_x, scale_y = param[\'scale\']\n            bbox = Corner(bbox.x1 / scale_x, bbox.y1 / scale_y, bbox.x2 / scale_x, bbox.y2 / scale_y)\n\n        image = crop_hwc(image, crop_bbox, size)\n        if not mask is None:\n            mask = crop_hwc(mask, crop_bbox, size)\n\n        offset = np.dot(self.rgbVar, np.random.randn(3, 1))\n        offset = offset[::-1]  # bgr 2 rgb\n        offset = offset.reshape(3)\n        image = image - offset\n\n        if self.blur > random.random():\n            image = self.blur_image(image)\n\n        if self.resize:\n            imageSize = image.shape[:2]\n            ratio = max(math.pow(random.random(), 0.5), 0.2)  # 25 ~ 255\n            rand_size = (int(round(ratio*imageSize[0])), int(round(ratio*imageSize[1])))\n            image = cv2.resize(image, rand_size)\n            image = cv2.resize(image, tuple(imageSize))\n\n        if self.flip and self.flip > Augmentation.random():\n            image = cv2.flip(image, 1)\n            mask = cv2.flip(mask, 1)\n            width = image.shape[1]\n            bbox = Corner(width - 1 - bbox.x2, bbox.y1, width - 1 - bbox.x1, bbox.y2)\n\n        return image, bbox, mask\n\n\nclass AnchorTargetLayer:\n    def __init__(self, cfg):\n        self.thr_high = 0.6\n        self.thr_low = 0.3\n        self.negative = 16\n        self.rpn_batch = 64\n        self.positive = 16\n\n        self.__dict__.update(cfg)\n\n    def __call__(self, anchor, target, size, neg=False, need_iou=False):\n        anchor_num = anchor.anchors.shape[0]\n\n        cls = np.zeros((anchor_num, size, size), dtype=np.int64)\n        cls[...] = -1  # -1 ignore 0 negative 1 positive\n        delta = np.zeros((4, anchor_num, size, size), dtype=np.float32)\n        delta_weight = np.zeros((anchor_num, size, size), dtype=np.float32)\n\n        def select(position, keep_num=16):\n            num = position[0].shape[0]\n            if num <= keep_num:\n                return position, num\n            slt = np.arange(num)\n            np.random.shuffle(slt)\n            slt = slt[:keep_num]\n            return tuple(p[slt] for p in position), keep_num\n\n        if neg:\n            l = size // 2 - 3\n            r = size // 2 + 3 + 1\n\n            cls[:, l:r, l:r] = 0\n\n            neg, neg_num = select(np.where(cls == 0), self.negative)\n            cls[:] = -1\n            cls[neg] = 0\n\n            if not need_iou:\n                return cls, delta, delta_weight\n            else:\n                overlap = np.zeros((anchor_num, size, size), dtype=np.float32)\n                return cls, delta, delta_weight, overlap\n\n        tcx, tcy, tw, th = corner2center(target)\n\n        anchor_box = anchor.all_anchors[0]\n        anchor_center = anchor.all_anchors[1]\n        x1, y1, x2, y2 = anchor_box[0], anchor_box[1], anchor_box[2], anchor_box[3]\n        cx, cy, w, h = anchor_center[0], anchor_center[1], anchor_center[2], anchor_center[3]\n\n        # delta\n        delta[0] = (tcx - cx) / w\n        delta[1] = (tcy - cy) / h\n        delta[2] = np.log(tw / w)\n        delta[3] = np.log(th / h)\n\n        # IoU\n        overlap = IoU([x1, y1, x2, y2], target)\n\n        pos = np.where(overlap > self.thr_high)\n        neg = np.where(overlap < self.thr_low)\n\n        pos, pos_num = select(pos, self.positive)\n        neg, neg_num = select(neg, self.rpn_batch - pos_num)\n\n        cls[pos] = 1\n        delta_weight[pos] = 1. / (pos_num + 1e-6)\n\n        cls[neg] = 0\n\n        if not need_iou:\n            return cls, delta, delta_weight\n        else:\n            return cls, delta, delta_weight, overlap\n\n\nclass DataSets(Dataset):\n    def __init__(self, cfg, anchor_cfg, num_epoch=1):\n        super(DataSets, self).__init__()\n        global logger\n        logger = logging.getLogger(\'global\')\n\n        # anchors\n        self.anchors = Anchors(anchor_cfg)\n\n        # size\n        self.template_size = 127\n        self.origin_size = 127\n        self.search_size = 255\n        self.size = 17\n        self.base_size = 0\n        self.crop_size = 0\n\n        if \'template_size\' in cfg:\n            self.template_size = cfg[\'template_size\']\n        if \'origin_size\' in cfg:\n            self.origin_size = cfg[\'origin_size\']\n        if \'search_size\' in cfg:\n            self.search_size = cfg[\'search_size\']\n        if \'base_size\' in cfg:\n            self.base_size = cfg[\'base_size\']\n        if \'size\' in cfg:\n            self.size = cfg[\'size\']\n\n        if (self.search_size - self.template_size) / self.anchors.stride + 1 + self.base_size != self.size:\n            raise Exception(""size not match!"")  # TODO: calculate size online\n        if \'crop_size\' in cfg:\n            self.crop_size = cfg[\'crop_size\']\n        self.template_small = False\n        if \'template_small\' in cfg and cfg[\'template_small\']:\n            self.template_small = True\n\n        self.anchors.generate_all_anchors(im_c=self.search_size//2, size=self.size)\n\n        if \'anchor_target\' not in cfg:\n            cfg[\'anchor_target\'] = {}\n        self.anchor_target = AnchorTargetLayer(cfg[\'anchor_target\'])\n\n        # data sets\n        if \'datasets\' not in cfg:\n            raise(Exception(\'DataSet need ""{}""\'.format(\'datasets\')))\n\n        self.all_data = []\n        start = 0\n        self.num = 0\n        for name in cfg[\'datasets\']:\n            dataset = cfg[\'datasets\'][name]\n            dataset[\'mark\'] = name\n            dataset[\'start\'] = start\n\n            dataset = SubDataSet(dataset)\n            dataset.log()\n            self.all_data.append(dataset)\n\n            start += dataset.num  # real video number\n            self.num += dataset.num_use  # the number used for subset shuffle\n\n        # data augmentation\n        aug_cfg = cfg[\'augmentation\']\n        self.template_aug = Augmentation(aug_cfg[\'template\'])\n        self.search_aug = Augmentation(aug_cfg[\'search\'])\n        self.gray = aug_cfg[\'gray\']\n        self.neg = aug_cfg[\'neg\']\n        self.inner_neg = 0 if \'inner_neg\' not in aug_cfg else aug_cfg[\'inner_neg\']\n\n        self.pick = None  # list to save id for each img\n        if \'num\' in cfg:  # number used in training for all dataset\n            self.num = int(cfg[\'num\'])\n        self.num *= num_epoch\n        self.shuffle()\n\n        self.infos = {\n                \'template\': self.template_size,\n                \'search\': self.search_size,\n                \'template_small\': self.template_small,\n                \'gray\': self.gray,\n                \'neg\': self.neg,\n                \'inner_neg\': self.inner_neg,\n                \'crop_size\': self.crop_size,\n                \'anchor_target\': self.anchor_target.__dict__,\n                \'num\': self.num // num_epoch\n                }\n        logger.info(\'dataset informations: \\n{}\'.format(json.dumps(self.infos, indent=4)))\n\n    def imread(self, path):\n        img = cv2.imread(path)\n\n        if self.origin_size == self.template_size:\n            return img, 1.0\n\n        def map_size(exe, size):\n            return int(round(((exe + 1) / (self.origin_size + 1) * (size+1) - 1)))\n\n        nsize = map_size(self.template_size, img.shape[1])\n\n        img = cv2.resize(img, (nsize, nsize))\n\n        return img, nsize / img.shape[1]\n\n    def shuffle(self):\n        pick = []\n        m = 0\n        while m < self.num:\n            p = []\n            for subset in self.all_data:\n                sub_p = subset.shuffle()\n                p += sub_p\n\n            sample_random.shuffle(p)\n\n            pick += p\n            m = len(pick)\n        self.pick = pick\n        logger.info(""shuffle done!"")\n        logger.info(""dataset length {}"".format(self.num))\n\n    def __len__(self):\n        return self.num\n\n    def find_dataset(self, index):\n        for dataset in self.all_data:\n            if dataset.start + dataset.num > index:\n                return dataset, index - dataset.start\n\n    def __getitem__(self, index, debug=False):\n        index = self.pick[index]\n        dataset, index = self.find_dataset(index)\n\n        gray = self.gray and self.gray > random.random()\n        neg = self.neg and self.neg > random.random()\n\n        if neg:\n            template = dataset.get_random_target(index)\n            if self.inner_neg and self.inner_neg > random.random():\n                search = dataset.get_random_target()\n            else:\n                search = random.choice(self.all_data).get_random_target()\n        else:\n            template, search = dataset.get_positive_pair(index)\n\n        def center_crop(img, size):\n            shape = img.shape[1]\n            if shape == size: return img\n            c = shape // 2\n            l = c - size // 2\n            r = c + size // 2 + 1\n            return img[l:r, l:r]\n\n        template_image, scale_z = self.imread(template[0])\n\n        if self.template_small:\n            template_image = center_crop(template_image, self.template_size)\n\n        search_image, scale_x = self.imread(search[0])\n\n        if dataset.has_mask and not neg:\n            search_mask = (cv2.imread(search[2], 0) > 0).astype(np.float32)\n        else:\n            search_mask = np.zeros(search_image.shape[:2], dtype=np.float32)\n\n        if self.crop_size > 0:\n            search_image = center_crop(search_image, self.crop_size)\n            search_mask = center_crop(search_mask, self.crop_size)\n\n        def toBBox(image, shape):\n            imh, imw = image.shape[:2]\n            if len(shape) == 4:\n                w, h = shape[2]-shape[0], shape[3]-shape[1]\n            else:\n                w, h = shape\n            context_amount = 0.5\n            exemplar_size = self.template_size  # 127\n            wc_z = w + context_amount * (w+h)\n            hc_z = h + context_amount * (w+h)\n            s_z = np.sqrt(wc_z * hc_z)\n            scale_z = exemplar_size / s_z\n            w = w*scale_z\n            h = h*scale_z\n            cx, cy = imw//2, imh//2\n            bbox = center2corner(Center(cx, cy, w, h))\n            return bbox\n\n        template_box = toBBox(template_image, template[1])\n        search_box = toBBox(search_image, search[1])\n\n        template, _, _ = self.template_aug(template_image, template_box, self.template_size, gray=gray)\n        search, bbox, mask = self.search_aug(search_image, search_box, self.search_size, gray=gray, mask=search_mask)\n\n        def draw(image, box, name):\n            image = image.copy()\n            x1, y1, x2, y2 = map(lambda x: int(round(x)), box)\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0))\n            cv2.imwrite(name, image)\n\n        if debug:\n            draw(template_image, template_box, ""debug/{:06d}_ot.jpg"".format(index))\n            draw(search_image, search_box, ""debug/{:06d}_os.jpg"".format(index))\n            draw(template, _, ""debug/{:06d}_t.jpg"".format(index))\n            draw(search, bbox, ""debug/{:06d}_s.jpg"".format(index))\n\n        cls, delta, delta_weight = self.anchor_target(self.anchors, bbox, self.size, neg)\n        if dataset.has_mask and not neg:\n            mask_weight = cls.max(axis=0, keepdims=True)\n        else:\n            mask_weight = np.zeros([1, cls.shape[1], cls.shape[2]], dtype=np.float32)\n\n        template, search = map(lambda x: np.transpose(x, (2, 0, 1)).astype(np.float32), [template, search])\n        \n        mask = (np.expand_dims(mask, axis=0) > 0.5) * 2 - 1  # 1*H*W\n\n        return template, search, cls, delta, delta_weight, np.array(bbox, np.float32), \\\n               np.array(mask, np.float32), np.array(mask_weight, np.float32)\n\n'"
datasets/siam_rpn_dataset.py,1,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport json\nimport random\nimport logging\nfrom os.path import join\nfrom utils.bbox_helper import *\nfrom utils.anchors import Anchors\nimport math\nimport sys\npyv = sys.version[0]\nimport cv2\nif pyv[0] == \'3\':\n    cv2.ocl.setUseOpenCL(False)\n\nlogger = logging.getLogger(\'global\')\n\n\nsample_random = random.Random()\nsample_random.seed(123456)\n\n\nclass SubDataSet(object):\n    def __init__(self, cfg):\n        for string in [\'root\', \'anno\']:\n            if string not in cfg:\n                raise Exception(\'SubDataSet need ""{}""\'.format(string))\n\n        with open(cfg[\'anno\']) as fin:\n            logger.info(""loading "" + cfg[\'anno\'])\n            self.labels = self.filter_zero(json.load(fin), cfg)\n\n            def isint(x):\n                try:\n                    int(x)\n                    return True\n                except:\n                    return False\n\n            # add frames args into labels\n            to_del = []\n            for video in self.labels:\n                for track in self.labels[video]:\n                    frames = self.labels[video][track]\n                    frames = list(map(int, filter(lambda x: isint(x), frames.keys())))\n                    frames.sort()\n                    self.labels[video][track][\'frames\'] = frames\n                    if len(frames) <= 0:\n                        logger.info(""warning {}/{} has no frames."".format(video, track))\n                        to_del.append((video, track))\n\n            # delete tracks with no frames\n            for video, track in to_del:\n                del self.labels[video][track]\n\n            # delete videos with no valid track\n            to_del = []\n            for video in self.labels:\n                if len(self.labels[video]) <= 0:\n                    logger.info(""warning {} has no tracks"".format(video))\n                    to_del.append(video)\n\n            for video in to_del:\n                del self.labels[video]\n\n            self.videos = list(self.labels.keys())\n\n            logger.info(cfg[\'anno\'] + "" loaded."")\n\n        # default args\n        self.root = ""/""\n        self.start = 0\n        self.num = len(self.labels)\n        self.num_use = self.num\n        self.frame_range = 100\n        self.mark = ""vid""\n        self.path_format = ""{}.{}.{}.jpg""\n\n        self.pick = []\n\n        # input args\n        self.__dict__.update(cfg)\n\n        self.num_use = int(self.num_use)\n\n        # shuffle\n        self.shuffle()\n\n    def filter_zero(self, anno, cfg):\n        name = cfg.get(\'mark\', \'\')\n\n        out = {}\n        tot = 0\n        new = 0\n        zero = 0\n\n        for video, tracks in anno.items():\n            new_tracks = {}\n            for trk, frames in tracks.items():\n                new_frames = {}\n                for frm, bbox in frames.items():\n                    tot += 1\n                    if len(bbox) == 4:\n                        x1, y1, x2, y2 = bbox\n                        w, h = x2 - x1, y2 -y1\n                    else:\n                        w, h= bbox\n                    if w == 0 or h == 0:\n                        logger.info(\'Error, {name} {video} {trk} {bbox}\'.format(**locals()))\n                        zero += 1\n                        continue\n                    new += 1\n                    new_frames[frm] = bbox\n\n                if len(new_frames) > 0:\n                    new_tracks[trk] = new_frames\n\n            if len(new_tracks) > 0:\n                out[video] = new_tracks\n\n        return out\n\n    def log(self):\n        logger.info(\'SubDataSet {name} start-index {start} select [{select}/{num}] path {format}\'.format(\n            name=self.mark, start=self.start, select=self.num_use, num=self.num, format=self.path_format\n        ))\n\n    def shuffle(self):\n        lists = list(range(self.start, self.start + self.num))\n\n        m = 0\n        pick = []\n        while m < self.num_use:\n            sample_random.shuffle(lists)\n            pick += lists\n            m += self.num\n\n        self.pick = pick[:self.num_use]\n        return self.pick\n\n    def get_image_anno(self, video, track, frame):\n        frame = ""{:06d}"".format(frame)\n        image_path = join(self.root, video, self.path_format.format(frame, track, \'x\'))\n        image_anno = self.labels[video][track][frame]\n\n        return image_path, image_anno\n\n    def get_positive_pair(self, index):\n        video_name = self.videos[index]\n        video = self.labels[video_name]\n        track = random.choice(list(video.keys()))\n        track_info = video[track]\n\n        frames = track_info[\'frames\']\n\n        if \'hard\' not in track_info:\n            template_frame = random.randint(0, len(frames)-1)\n\n            left = max(template_frame - self.frame_range, 0)\n            right = min(template_frame + self.frame_range, len(frames)-1) + 1\n            search_range = frames[left:right]\n            template_frame = frames[template_frame]\n            search_frame = random.choice(search_range)\n        else:\n            search_frame = random.choice(track_info[\'hard\'])\n            left = max(search_frame - self.frame_range, 0)\n            right = min(search_frame + self.frame_range, len(frames)-1) + 1  # python [left:right+1) = [left:right]\n            template_range = frames[left:right]\n            template_frame = random.choice(template_range)\n            search_frame = frames[search_frame]\n\n        return self.get_image_anno(video_name, track, template_frame), \\\n               self.get_image_anno(video_name, track, search_frame)\n\n    def get_random_target(self, index=-1):\n        if index == -1:\n            index = random.randint(0, self.num-1)\n        video_name = self.videos[index]\n        video = self.labels[video_name]\n        track = random.choice(list(video.keys()))\n        track_info = video[track]\n\n        frames = track_info[\'frames\']\n        frame = random.choice(frames)\n\n        return self.get_image_anno(video_name, track, frame)\n\n\ndef crop_hwc(image, bbox, out_sz, padding=(0, 0, 0)):\n    bbox = [float(x) for x in bbox]\n    a = (out_sz-1) / (bbox[2]-bbox[0])\n    b = (out_sz-1) / (bbox[3]-bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz), borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return crop\n\n\nclass Augmentation:\n    def __init__(self, cfg):\n        # default args\n        self.shift = 0\n        self.scale = 0\n        self.blur = 0 #False\n        self.resize = False\n        self.rgbVar = np.array([[-0.55919361,  0.98062831, - 0.41940627],\n            [1.72091413,  0.19879334, - 1.82968581],\n            [4.64467907,  4.73710203, 4.88324118]], dtype=np.float32)\n        self.flip = 0\n\n        self.eig_vec = np.array([\n            [0.4009, 0.7192, -0.5675],\n            [-0.8140, -0.0045, -0.5808],\n            [0.4203, -0.6948, -0.5836],\n        ], dtype=np.float32)\n\n        self.eig_val = np.array([[0.2175, 0.0188, 0.0045]], np.float32)\n\n        self.__dict__.update(cfg)\n\n    @staticmethod\n    def random():\n        return random.random() * 2 - 1.0\n\n    def blur_image(self, image):\n        def rand_kernel():\n            size = np.random.randn(1)\n            size = int(np.round(size)) * 2 + 1\n            if size < 0: return None\n            if random.random() < 0.5: return None\n            size = min(size, 45)\n            kernel = np.zeros((size, size))\n            c = int(size/2)\n            wx = random.random()\n            kernel[:, c] += 1. / size * wx\n            kernel[c, :] += 1. / size * (1-wx)\n            return kernel\n\n        kernel = rand_kernel()\n\n        if kernel is not None:\n            image = cv2.filter2D(image, -1, kernel)\n        return image\n\n    def __call__(self, image, bbox, size, gray=False):\n        if gray:\n            grayed = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            image = np.zeros((grayed.shape[0], grayed.shape[1], 3), np.uint8)\n            image[:, :, 0] = image[:, :, 1] = image[:, :, 2] = grayed\n\n        shape = image.shape\n\n        crop_bbox = center2corner((shape[0]//2, shape[1]//2, size-1, size-1))\n\n        param = {}\n        if self.shift:\n            param[\'shift\'] = (Augmentation.random() * self.shift, Augmentation.random() * self.shift)\n\n        if self.scale:\n            param[\'scale\'] = ((1.0 + Augmentation.random() * self.scale), (1.0 + Augmentation.random() * self.scale))\n\n        crop_bbox, _ = aug_apply(Corner(*crop_bbox), param, shape)\n\n        x1 = crop_bbox.x1\n        y1 = crop_bbox.y1\n\n        bbox = BBox(bbox.x1 - x1, bbox.y1 - y1,\n                    bbox.x2 - x1, bbox.y2 - y1)\n\n        if self.scale:\n            scale_x, scale_y = param[\'scale\']\n            bbox = Corner(bbox.x1 / scale_x, bbox.y1 / scale_y, bbox.x2 / scale_x, bbox.y2 / scale_y)\n\n        image = crop_hwc(image, crop_bbox, size)\n\n        offset = np.dot(self.rgbVar, np.random.randn(3, 1))\n        offset = offset[::-1]  # bgr 2 rgb\n        offset = offset.reshape(3)\n        image = image - offset\n\n        if self.blur > random.random():\n            image = self.blur_image(image)\n\n        if self.resize:\n            imageSize = image.shape[:2]\n            ratio = max(math.pow(random.random(), 0.5), 0.2)  # 25 ~ 255\n            rand_size = (int(round(ratio*imageSize[0])), int(round(ratio*imageSize[1])))\n            image = cv2.resize(image, rand_size)\n            image = cv2.resize(image, tuple(imageSize))\n\n        if self.flip and self.flip > Augmentation.random():\n            image = cv2.flip(image, 1)\n            width = image.shape[1]\n            bbox = Corner(width - 1 - bbox.x2, bbox.y1, width - 1 - bbox.x1, bbox.y2)\n\n        return image, bbox\n\n\nclass AnchorTargetLayer:\n    def __init__(self, cfg):\n        self.thr_high = 0.6\n        self.thr_low = 0.3\n        self.negative = 16\n        self.rpn_batch = 64\n        self.positive = 16\n\n        self.__dict__.update(cfg)\n\n    def __call__(self, anchor, target, size, neg=False, need_iou=False):\n        anchor_num = anchor.anchors.shape[0]\n\n        cls = np.zeros((anchor_num, size, size), dtype=np.int64)\n        cls[...] = -1  # -1 ignore 0 negative 1 positive\n        delta = np.zeros((4, anchor_num, size, size), dtype=np.float32)\n        delta_weight = np.zeros((anchor_num, size, size), dtype=np.float32)\n\n        def select(position, keep_num=16):\n            num = position[0].shape[0]\n            if num <= keep_num:\n                return position, num\n            slt = np.arange(num)\n            np.random.shuffle(slt)\n            slt = slt[:keep_num]\n            return tuple(p[slt] for p in position), keep_num\n\n        if neg:\n            l = size // 2 - 3\n            r = size // 2 + 3 + 1\n\n            cls[:, l:r, l:r] = 0\n\n            neg, neg_num = select(np.where(cls == 0), self.negative)\n            cls[:] = -1\n            cls[neg] = 0\n\n            if not need_iou:\n                return cls, delta, delta_weight\n            else:\n                overlap = np.zeros((anchor_num, size, size), dtype=np.float32)\n                return cls, delta, delta_weight, overlap\n\n        tcx, tcy, tw, th = corner2center(target)\n\n        anchor_box = anchor.all_anchors[0]\n        anchor_center = anchor.all_anchors[1]\n        x1, y1, x2, y2 = anchor_box[0], anchor_box[1], anchor_box[2], anchor_box[3]\n        cx, cy, w, h = anchor_center[0], anchor_center[1], anchor_center[2], anchor_center[3]\n\n        # delta\n        delta[0] = (tcx - cx) / w\n        delta[1] = (tcy - cy) / h\n        delta[2] = np.log(tw / w)\n        delta[3] = np.log(th / h)\n\n        # IoU\n        overlap = IoU([x1, y1, x2, y2], target)\n\n        pos = np.where(overlap > self.thr_high)\n        neg = np.where(overlap < self.thr_low)\n\n        pos, pos_num = select(pos, self.positive)\n        neg, neg_num = select(neg, self.rpn_batch - pos_num)\n\n        cls[pos] = 1\n        delta_weight[pos] = 1. / (pos_num + 1e-6)\n\n        cls[neg] = 0\n\n        if not need_iou:\n            return cls, delta, delta_weight\n        else:\n            return cls, delta, delta_weight, overlap\n\n\nclass DataSets(Dataset):\n    def __init__(self, cfg, anchor_cfg, num_epoch=1):\n        super(DataSets, self).__init__()\n        global logger\n        logger = logging.getLogger(\'global\')\n\n        # anchors\n        self.anchors = Anchors(anchor_cfg)\n\n        # size\n        self.template_size = 127\n        self.origin_size = 127\n        self.search_size = 255\n        self.size = 17\n        self.base_size = 0\n        self.crop_size = 0\n\n        if \'template_size\' in cfg:\n            self.template_size = cfg[\'template_size\']\n        if \'origin_size\' in cfg:\n            self.origin_size = cfg[\'origin_size\']\n        if \'search_size\' in cfg:\n            self.search_size = cfg[\'search_size\']\n        if \'base_size\' in cfg:\n            self.base_size = cfg[\'base_size\']\n        if \'size\' in cfg:\n            self.size = cfg[\'size\']\n\n        if (self.search_size - self.template_size) / self.anchors.stride + 1 + self.base_size != self.size:\n            raise Exception(""size not match!"")  # TODO: calculate size online\n        if \'crop_size\' in cfg:\n            self.crop_size = cfg[\'crop_size\']\n        self.template_small = False\n        if \'template_small\' in cfg and cfg[\'template_small\']:\n            self.template_small = True\n\n        self.anchors.generate_all_anchors(im_c=self.search_size//2, size=self.size)\n\n        if \'anchor_target\' not in cfg:\n            cfg[\'anchor_target\'] = {}\n        self.anchor_target = AnchorTargetLayer(cfg[\'anchor_target\'])\n\n        # data sets\n        if \'datasets\' not in cfg:\n            raise(Exception(\'DataSet need ""{}""\'.format(\'datasets\')))\n\n        self.all_data = []\n        start = 0\n        self.num = 0\n        for name in cfg[\'datasets\']:\n            dataset = cfg[\'datasets\'][name]\n            dataset[\'mark\'] = name\n            dataset[\'start\'] = start\n\n            dataset = SubDataSet(dataset)\n            dataset.log()\n            self.all_data.append(dataset)\n\n            start += dataset.num  # real video number\n            self.num += dataset.num_use  # the number used for subset shuffle\n\n        # data augmentation\n        aug_cfg = cfg[\'augmentation\']\n        self.template_aug = Augmentation(aug_cfg[\'template\'])\n        self.search_aug = Augmentation(aug_cfg[\'search\'])\n        self.gray = aug_cfg[\'gray\']\n        self.neg = aug_cfg[\'neg\']\n        self.inner_neg = 0 if \'inner_neg\' not in aug_cfg else aug_cfg[\'inner_neg\']\n\n        self.pick = None  # list to save id for each img\n        if \'num\' in cfg:  # number used in training for all dataset\n            self.num = int(cfg[\'num\'])\n        self.num *= num_epoch\n        self.shuffle()\n\n        self.infos = {\n                \'template\': self.template_size,\n                \'search\': self.search_size,\n                \'template_small\': self.template_small,\n                \'gray\': self.gray,\n                \'neg\': self.neg,\n                \'inner_neg\': self.inner_neg,\n                \'crop_size\': self.crop_size,\n                \'anchor_target\': self.anchor_target.__dict__,\n                \'num\': self.num // num_epoch\n                }\n        logger.info(\'dataset informations: \\n{}\'.format(json.dumps(self.infos, indent=4)))\n\n    def imread(self, path):\n        img = cv2.imread(path)\n\n        if self.origin_size == self.template_size:\n            return img, 1.0\n\n        def map_size(exe, size):\n            return int(round(((exe + 1) / (self.origin_size + 1) * (size+1) - 1)))\n\n        nsize = map_size(self.template_size, img.shape[1])\n\n        img = cv2.resize(img, (nsize, nsize))\n\n        return img, nsize / img.shape[1]\n\n    def shuffle(self):\n        pick = []\n        m = 0\n        while m < self.num:\n            p = []\n            for subset in self.all_data:\n                sub_p = subset.shuffle()\n                p += sub_p\n\n            sample_random.shuffle(p)\n\n            pick += p\n            m = len(pick)\n        self.pick = pick\n        logger.info(""shuffle done!"")\n        logger.info(""dataset length {}"".format(self.num))\n\n    def __len__(self):\n        return self.num\n\n    def find_dataset(self, index):\n        for dataset in self.all_data:\n            if dataset.start + dataset.num > index:\n                return dataset, index - dataset.start\n\n    def __getitem__(self, index, debug=False):\n        index = self.pick[index]\n        dataset, index = self.find_dataset(index)\n\n        gray = self.gray and self.gray > random.random()\n        neg = self.neg and self.neg > random.random()\n\n        if neg:\n            template = dataset.get_random_target(index)\n            if self.inner_neg and self.inner_neg > random.random():\n                search = dataset.get_random_target()\n            else:\n                search = random.choice(self.all_data).get_random_target()\n        else:\n            template, search = dataset.get_positive_pair(index)\n\n        def center_crop(img, size):\n            shape = img.shape[1]\n            if shape == size: return img\n            c = shape // 2\n            l = c - size // 2\n            r = c + size // 2 + 1\n            return img[l:r, l:r]\n\n        template_image, scale_z = self.imread(template[0])\n\n        if self.template_small:\n            template_image = center_crop(template_image, self.template_size)\n\n        search_image, scale_x = self.imread(search[0])\n        if self.crop_size > 0:\n            search_image = center_crop(search_image, self.crop_size)\n\n        def toBBox(image, shape):\n            imh, imw = image.shape[:2]\n            if len(shape) == 4:\n                w, h = shape[2]-shape[0], shape[3]-shape[1]\n            else:\n                w, h = shape\n            context_amount = 0.5\n            exemplar_size = self.template_size  # 127\n            wc_z = w + context_amount * (w+h)\n            hc_z = h + context_amount * (w+h)\n            s_z = np.sqrt(wc_z * hc_z)\n            scale_z = exemplar_size / s_z\n            w = w*scale_z\n            h = h*scale_z\n            cx, cy = imw//2, imh//2\n            bbox = center2corner(Center(cx, cy, w, h))\n            return bbox\n\n        template_box = toBBox(template_image, template[1])\n        search_box = toBBox(search_image, search[1])\n\n        template, _ = self.template_aug(template_image, template_box, self.template_size, gray=gray)\n        search, bbox = self.search_aug(search_image, search_box, self.search_size, gray=gray)\n\n        def draw(image, box, name):\n            image = image.copy()\n            x1, y1, x2, y2 = map(lambda x: int(round(x)), box)\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0))\n            cv2.imwrite(name, image)\n\n        if debug:\n            draw(template_image, template_box, ""debug/{:06d}_ot.jpg"".format(index))\n            draw(search_image, search_box, ""debug/{:06d}_os.jpg"".format(index))\n            draw(template, _, ""debug/{:06d}_t.jpg"".format(index))\n            draw(search, bbox, ""debug/{:06d}_s.jpg"".format(index))\n\n        cls, delta, delta_weight = self.anchor_target(self.anchors, bbox, self.size, neg)\n\n        template, search = map(lambda x: np.transpose(x, (2, 0, 1)).astype(np.float32), [template, search])\n\n        return template, search, cls, delta, delta_weight, np.array(bbox, np.float32)\n\n'"
models/__init__.py,0,b''
models/features.py,2,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch.nn as nn\nimport logging\n\nlogger = logging.getLogger('global')\n\n\nclass Features(nn.Module):\n    def __init__(self):\n        super(Features, self).__init__()\n        self.feature_size = -1\n\n    def forward(self, x):\n        raise NotImplementedError\n\n    def param_groups(self, start_lr, feature_mult=1):\n        params = filter(lambda x:x.requires_grad, self.parameters())\n        params = [{'params': params, 'lr': start_lr * feature_mult}]\n        return params\n\n    def load_model(self, f='pretrain.model'):\n        with open(f) as f:\n            pretrained_dict = torch.load(f)\n            model_dict = self.state_dict()\n            print(pretrained_dict.keys())\n            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n            print(pretrained_dict.keys())\n            model_dict.update(pretrained_dict)\n            self.load_state_dict(model_dict)\n\n\nclass MultiStageFeature(Features):\n    def __init__(self):\n        super(MultiStageFeature, self).__init__()\n\n        self.layers = []\n        self.train_num = -1\n        self.change_point = []\n        self.train_nums = []\n\n    def unfix(self, ratio=0.0):\n        if self.train_num == -1:\n            self.train_num = 0\n            self.unlock()\n            self.eval()\n        for p, t in reversed(list(zip(self.change_point, self.train_nums))):\n            if ratio >= p:\n                if self.train_num != t:\n                    self.train_num = t\n                    self.unlock()\n                    return True\n                break\n        return False\n\n    def train_layers(self):\n        return self.layers[:self.train_num]\n\n    def unlock(self):\n        for p in self.parameters():\n            p.requires_grad = False\n\n        logger.info('Current training {} layers:\\n\\t'.format(self.train_num, self.train_layers()))\n        for m in self.train_layers():\n            for p in m.parameters():\n                p.requires_grad = True\n\n    def train(self, mode):\n        self.training = mode\n        if mode == False:\n            super(MultiStageFeature,self).train(False)\n        else:\n            for m in self.train_layers():\n                m.train(True)\n\n        return self\n"""
models/mask.py,1,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch.nn as nn\n\n\nclass Mask(nn.Module):\n    def __init__(self):\n        super(Mask, self).__init__()\n\n    def forward(self, z_f, x_f):\n        raise NotImplementedError\n\n    def template(self, template):\n        raise NotImplementedError\n\n    def track(self, search):\n        raise NotImplementedError\n\n    def param_groups(self, start_lr, feature_mult=1):\n        params = filter(lambda x:x.requires_grad, self.parameters())\n        params = [{'params': params, 'lr': start_lr * feature_mult}]\n        return params\n"""
models/rpn.py,2,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RPN(nn.Module):\n    def __init__(self):\n        super(RPN, self).__init__()\n\n    def forward(self, z_f, x_f):\n        raise NotImplementedError\n\n    def template(self, template):\n        raise NotImplementedError\n\n    def track(self, search):\n        raise NotImplementedError\n\n    def param_groups(self, start_lr, feature_mult=1, key=None):\n        if key is None:\n            params = filter(lambda x:x.requires_grad, self.parameters())\n        else:\n            params = [v for k, v in self.named_parameters() if (key in k) and v.requires_grad]\n        params = [{'params': params, 'lr': start_lr * feature_mult}]\n        return params\n\n\ndef conv2d_dw_group(x, kernel):\n    batch, channel = kernel.shape[:2]\n    x = x.view(1, batch*channel, x.size(2), x.size(3))  # 1 * (b*c) * k * k\n    kernel = kernel.view(batch*channel, 1, kernel.size(2), kernel.size(3))  # (b*c) * 1 * H * W\n    out = F.conv2d(x, kernel, groups=batch*channel)\n    out = out.view(batch, channel, out.size(2), out.size(3))\n    return out\n\n\nclass DepthCorr(nn.Module):\n    def __init__(self, in_channels, hidden, out_channels, kernel_size=3):\n        super(DepthCorr, self).__init__()\n        # adjust layer for asymmetrical features\n        self.conv_kernel = nn.Sequential(\n                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=False),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU(inplace=True),\n                )\n        self.conv_search = nn.Sequential(\n                nn.Conv2d(in_channels, hidden, kernel_size=kernel_size, bias=False),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU(inplace=True),\n                )\n\n        self.head = nn.Sequential(\n                nn.Conv2d(hidden, hidden, kernel_size=1, bias=False),\n                nn.BatchNorm2d(hidden),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(hidden, out_channels, kernel_size=1)\n                )\n\n    def forward_corr(self, kernel, input):\n        kernel = self.conv_kernel(kernel)\n        input = self.conv_search(input)\n        feature = conv2d_dw_group(input, kernel)\n        return feature\n\n    def forward(self, kernel, search):\n        feature = self.forward_corr(kernel, search)\n        out = self.head(feature)\n        return out\n"""
models/siammask.py,15,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom utils.anchors import Anchors\n\n\nclass SiamMask(nn.Module):\n    def __init__(self, anchors=None, o_sz=63, g_sz=127):\n        super(SiamMask, self).__init__()\n        self.anchors = anchors  # anchor_cfg\n        self.anchor_num = len(self.anchors[""ratios""]) * len(self.anchors[""scales""])\n        self.anchor = Anchors(anchors)\n        self.features = None\n        self.rpn_model = None\n        self.mask_model = None\n        self.o_sz = o_sz\n        self.g_sz = g_sz\n        self.upSample = nn.UpsamplingBilinear2d(size=[g_sz, g_sz])\n\n        self.all_anchors = None\n\n    def set_all_anchors(self, image_center, size):\n        # cx,cy,w,h\n        if not self.anchor.generate_all_anchors(image_center, size):\n            return\n        all_anchors = self.anchor.all_anchors[1]  # cx, cy, w, h\n        self.all_anchors = torch.from_numpy(all_anchors).float().cuda()\n        self.all_anchors = [self.all_anchors[i] for i in range(4)]\n\n    def feature_extractor(self, x):\n        return self.features(x)\n\n    def rpn(self, template, search):\n        pred_cls, pred_loc = self.rpn_model(template, search)\n        return pred_cls, pred_loc\n\n    def mask(self, template, search):\n        pred_mask = self.mask_model(template, search)\n        return pred_mask\n\n    def _add_rpn_loss(self, label_cls, label_loc, lable_loc_weight, label_mask, label_mask_weight,\n                      rpn_pred_cls, rpn_pred_loc, rpn_pred_mask):\n        rpn_loss_cls = select_cross_entropy_loss(rpn_pred_cls, label_cls)\n\n        rpn_loss_loc = weight_l1_loss(rpn_pred_loc, label_loc, lable_loc_weight)\n\n        rpn_loss_mask, iou_m, iou_5, iou_7 = select_mask_logistic_loss(rpn_pred_mask, label_mask, label_mask_weight)\n\n        return rpn_loss_cls, rpn_loss_loc, rpn_loss_mask, iou_m, iou_5, iou_7\n\n    def run(self, template, search, softmax=False):\n        """"""\n        run network\n        """"""\n        template_feature = self.feature_extractor(template)\n        search_feature = self.feature_extractor(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(template_feature, search_feature)\n        rpn_pred_mask = self.mask(template_feature, search_feature)  # (b, 63*63, w, h)\n\n        if softmax:\n            rpn_pred_cls = self.softmax(rpn_pred_cls)\n        return rpn_pred_cls, rpn_pred_loc, rpn_pred_mask, template_feature, search_feature\n\n    def softmax(self, cls):\n        b, a2, h, w = cls.size()\n        cls = cls.view(b, 2, a2//2, h, w)\n        cls = cls.permute(0, 2, 3, 4, 1).contiguous()\n        cls = F.log_softmax(cls, dim=4)\n        return cls\n\n    def forward(self, input):\n        """"""\n        :param input: dict of input with keys of:\n                \'template\': [b, 3, h1, w1], input template image.\n                \'search\': [b, 3, h2, w2], input search image.\n                \'label_cls\':[b, max_num_gts, 5] or None(self.training==False),\n                                     each gt contains x1,y1,x2,y2,class.\n        :return: dict of loss, predict, accuracy\n        """"""\n        template = input[\'template\']\n        search = input[\'search\']\n        if self.training:\n            label_cls = input[\'label_cls\']\n            label_loc = input[\'label_loc\']\n            lable_loc_weight = input[\'label_loc_weight\']\n            label_mask = input[\'label_mask\']\n            label_mask_weight = input[\'label_mask_weight\']\n\n        rpn_pred_cls, rpn_pred_loc, rpn_pred_mask, template_feature, search_feature = \\\n            self.run(template, search, softmax=self.training)\n\n        outputs = dict()\n\n        outputs[\'predict\'] = [rpn_pred_loc, rpn_pred_cls, rpn_pred_mask, template_feature, search_feature]\n\n        if self.training:\n            rpn_loss_cls, rpn_loss_loc, rpn_loss_mask, iou_acc_mean, iou_acc_5, iou_acc_7 = \\\n                self._add_rpn_loss(label_cls, label_loc, lable_loc_weight, label_mask, label_mask_weight,\n                                   rpn_pred_cls, rpn_pred_loc, rpn_pred_mask)\n            outputs[\'losses\'] = [rpn_loss_cls, rpn_loss_loc, rpn_loss_mask]\n            outputs[\'accuracy\'] = [iou_acc_mean, iou_acc_5, iou_acc_7]\n\n        return outputs\n\n    def template(self, z):\n        self.zf = self.feature_extractor(z)\n        cls_kernel, loc_kernel = self.rpn_model.template(self.zf)\n        return cls_kernel, loc_kernel\n\n    def track(self, x, cls_kernel=None, loc_kernel=None, softmax=False):\n        xf = self.feature_extractor(x)\n        rpn_pred_cls, rpn_pred_loc = self.rpn_model.track(xf, cls_kernel, loc_kernel)\n        if softmax:\n            rpn_pred_cls = self.softmax(rpn_pred_cls)\n        return rpn_pred_cls, rpn_pred_loc\n\n\ndef get_cls_loss(pred, label, select):\n    if select.nelement() == 0: return pred.sum()*0.\n    pred = torch.index_select(pred, 0, select)\n    label = torch.index_select(label, 0, select)\n\n    return F.nll_loss(pred, label)\n\n\ndef select_cross_entropy_loss(pred, label):\n    pred = pred.view(-1, 2)\n    label = label.view(-1)\n    pos = Variable(label.data.eq(1).nonzero().squeeze()).cuda()\n    neg = Variable(label.data.eq(0).nonzero().squeeze()).cuda()\n\n    loss_pos = get_cls_loss(pred, label, pos)\n    loss_neg = get_cls_loss(pred, label, neg)\n    return loss_pos * 0.5 + loss_neg * 0.5\n\n\ndef weight_l1_loss(pred_loc, label_loc, loss_weight):\n    """"""\n    :param pred_loc: [b, 4k, h, w]\n    :param label_loc: [b, 4k, h, w]\n    :param loss_weight:  [b, k, h, w]\n    :return: loc loss value\n    """"""\n    b, _, sh, sw = pred_loc.size()\n    pred_loc = pred_loc.view(b, 4, -1, sh, sw)\n    diff = (pred_loc - label_loc).abs()\n    diff = diff.sum(dim=1).view(b, -1, sh, sw)\n    loss = diff * loss_weight\n    return loss.sum().div(b)\n\n\ndef select_mask_logistic_loss(p_m, mask, weight, o_sz=63, g_sz=127):\n    weight = weight.view(-1)\n    pos = Variable(weight.data.eq(1).nonzero().squeeze())\n    if pos.nelement() == 0: return p_m.sum() * 0, p_m.sum() * 0, p_m.sum() * 0, p_m.sum() * 0\n\n    p_m = p_m.permute(0, 2, 3, 1).contiguous().view(-1, 1, o_sz, o_sz)\n    p_m = torch.index_select(p_m, 0, pos)\n    p_m = nn.UpsamplingBilinear2d(size=[g_sz, g_sz])(p_m)\n    p_m = p_m.view(-1, g_sz * g_sz)\n\n    mask_uf = F.unfold(mask, (g_sz, g_sz), padding=32, stride=8)\n    mask_uf = torch.transpose(mask_uf, 1, 2).contiguous().view(-1, g_sz * g_sz)\n\n    mask_uf = torch.index_select(mask_uf, 0, pos)\n    loss = F.soft_margin_loss(p_m, mask_uf)\n    iou_m, iou_5, iou_7 = iou_measure(p_m, mask_uf)\n    return loss, iou_m, iou_5, iou_7\n\n\ndef iou_measure(pred, label):\n    pred = pred.ge(0)\n    mask_sum = pred.eq(1).add(label.eq(1))\n    intxn = torch.sum(mask_sum == 2, dim=1).float()\n    union = torch.sum(mask_sum > 0, dim=1).float()\n    iou = intxn/union\n    return torch.mean(iou), (torch.sum(iou > 0.5).float()/iou.shape[0]), (torch.sum(iou > 0.7).float()/iou.shape[0])\n    \n\nif __name__ == ""__main__"":\n    p_m = torch.randn(4, 63*63, 25, 25)\n    cls = torch.randn(4, 1, 25, 25) > 0.9\n    mask = torch.randn(4, 1, 255, 255) * 2 - 1\n\n    loss = select_mask_logistic_loss(p_m, mask, cls)\n    print(loss)\n'"
models/siammask_sharp.py,16,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom utils.anchors import Anchors\n\n\nclass SiamMask(nn.Module):\n    def __init__(self, anchors=None, o_sz=127, g_sz=127):\n        super(SiamMask, self).__init__()\n        self.anchors = anchors  # anchor_cfg\n        self.anchor_num = len(self.anchors[""ratios""]) * len(self.anchors[""scales""])\n        self.anchor = Anchors(anchors)\n        self.features = None\n        self.rpn_model = None\n        self.mask_model = None\n        self.o_sz = o_sz\n        self.g_sz = g_sz\n        self.upSample = nn.UpsamplingBilinear2d(size=[g_sz, g_sz])\n\n        self.all_anchors = None\n\n    def set_all_anchors(self, image_center, size):\n        # cx,cy,w,h\n        if not self.anchor.generate_all_anchors(image_center, size):\n            return\n        all_anchors = self.anchor.all_anchors[1]  # cx, cy, w, h\n        self.all_anchors = torch.from_numpy(all_anchors).float().cuda()\n        self.all_anchors = [self.all_anchors[i] for i in range(4)]\n\n    def feature_extractor(self, x):\n        return self.features(x)\n\n    def rpn(self, template, search):\n        pred_cls, pred_loc = self.rpn_model(template, search)\n        return pred_cls, pred_loc\n\n    def mask(self, template, search):\n        pred_mask = self.mask_model(template, search)\n        return pred_mask\n\n    def _add_rpn_loss(self, label_cls, label_loc, lable_loc_weight, label_mask, label_mask_weight,\n                      rpn_pred_cls, rpn_pred_loc, rpn_pred_mask):\n        rpn_loss_cls = select_cross_entropy_loss(rpn_pred_cls, label_cls)\n\n        rpn_loss_loc = weight_l1_loss(rpn_pred_loc, label_loc, lable_loc_weight)\n\n        rpn_loss_mask, iou_m, iou_5, iou_7 = select_mask_logistic_loss(rpn_pred_mask, label_mask, label_mask_weight)\n\n        return rpn_loss_cls, rpn_loss_loc, rpn_loss_mask, iou_m, iou_5, iou_7\n\n    def run(self, template, search, softmax=False):\n        """"""\n        run network\n        """"""\n        template_feature = self.feature_extractor(template)\n        feature, search_feature = self.features.forward_all(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(template_feature, search_feature)\n        corr_feature = self.mask_model.mask.forward_corr(template_feature, search_feature)  # (b, 256, w, h)\n        rpn_pred_mask = self.refine_model(feature, corr_feature)\n\n        if softmax:\n            rpn_pred_cls = self.softmax(rpn_pred_cls)\n        return rpn_pred_cls, rpn_pred_loc, rpn_pred_mask, template_feature, search_feature\n\n    def softmax(self, cls):\n        b, a2, h, w = cls.size()\n        cls = cls.view(b, 2, a2//2, h, w)\n        cls = cls.permute(0, 2, 3, 4, 1).contiguous()\n        cls = F.log_softmax(cls, dim=4)\n        return cls\n\n    def forward(self, input):\n        """"""\n        :param input: dict of input with keys of:\n                \'template\': [b, 3, h1, w1], input template image.\n                \'search\': [b, 3, h2, w2], input search image.\n                \'label_cls\':[b, max_num_gts, 5] or None(self.training==False),\n                                     each gt contains x1,y1,x2,y2,class.\n        :return: dict of loss, predict, accuracy\n        """"""\n        template = input[\'template\']\n        search = input[\'search\']\n        if self.training:\n            label_cls = input[\'label_cls\']\n            label_loc = input[\'label_loc\']\n            lable_loc_weight = input[\'label_loc_weight\']\n            label_mask = input[\'label_mask\']\n            label_mask_weight = input[\'label_mask_weight\']\n\n        rpn_pred_cls, rpn_pred_loc, rpn_pred_mask, template_feature, search_feature = \\\n            self.run(template, search, softmax=self.training)\n\n        outputs = dict()\n\n        outputs[\'predict\'] = [rpn_pred_loc, rpn_pred_cls, rpn_pred_mask, template_feature, search_feature]\n\n        if self.training:\n            rpn_loss_cls, rpn_loss_loc, rpn_loss_mask, iou_acc_mean, iou_acc_5, iou_acc_7 = \\\n                self._add_rpn_loss(label_cls, label_loc, lable_loc_weight, label_mask, label_mask_weight,\n                                   rpn_pred_cls, rpn_pred_loc, rpn_pred_mask)\n            outputs[\'losses\'] = [rpn_loss_cls, rpn_loss_loc, rpn_loss_mask]\n            outputs[\'accuracy\'] = [iou_acc_mean, iou_acc_5, iou_acc_7]\n\n        return outputs\n\n    def template(self, z):\n        self.zf = self.feature_extractor(z)\n        cls_kernel, loc_kernel = self.rpn_model.template(self.zf)\n        return cls_kernel, loc_kernel\n\n    def track(self, x, cls_kernel=None, loc_kernel=None, softmax=False):\n        xf = self.feature_extractor(x)\n        rpn_pred_cls, rpn_pred_loc = self.rpn_model.track(xf, cls_kernel, loc_kernel)\n        if softmax:\n            rpn_pred_cls = self.softmax(rpn_pred_cls)\n        return rpn_pred_cls, rpn_pred_loc\n\n\ndef get_cls_loss(pred, label, select):\n    if select.nelement() == 0: return pred.sum()*0.\n    pred = torch.index_select(pred, 0, select)\n    label = torch.index_select(label, 0, select)\n\n    return F.nll_loss(pred, label)\n\n\ndef select_cross_entropy_loss(pred, label):\n    pred = pred.view(-1, 2)\n    label = label.view(-1)\n    pos = Variable(label.data.eq(1).nonzero().squeeze()).cuda()\n    neg = Variable(label.data.eq(0).nonzero().squeeze()).cuda()\n\n    loss_pos = get_cls_loss(pred, label, pos)\n    loss_neg = get_cls_loss(pred, label, neg)\n    return loss_pos * 0.5 + loss_neg * 0.5\n\n\ndef weight_l1_loss(pred_loc, label_loc, loss_weight):\n    """"""\n    :param pred_loc: [b, 4k, h, w]\n    :param label_loc: [b, 4k, h, w]\n    :param loss_weight:  [b, k, h, w]\n    :return: loc loss value\n    """"""\n    b, _, sh, sw = pred_loc.size()\n    pred_loc = pred_loc.view(b, 4, -1, sh, sw)\n    diff = (pred_loc - label_loc).abs()\n    diff = diff.sum(dim=1).view(b, -1, sh, sw)\n    loss = diff * loss_weight\n    return loss.sum().div(b)\n\n\ndef select_mask_logistic_loss(p_m, mask, weight, o_sz=63, g_sz=127):\n    weight = weight.view(-1)\n    pos = Variable(weight.data.eq(1).nonzero().squeeze())\n    if pos.nelement() == 0: return p_m.sum() * 0, p_m.sum() * 0, p_m.sum() * 0, p_m.sum() * 0\n\n    if len(p_m.shape) == 4:\n        p_m = p_m.permute(0, 2, 3, 1).contiguous().view(-1, 1, o_sz, o_sz)\n        p_m = torch.index_select(p_m, 0, pos)\n        p_m = nn.UpsamplingBilinear2d(size=[g_sz, g_sz])(p_m)\n        p_m = p_m.view(-1, g_sz * g_sz)\n    else:\n        p_m = torch.index_select(p_m, 0, pos)\n\n    mask_uf = F.unfold(mask, (g_sz, g_sz), padding=0, stride=8)\n    mask_uf = torch.transpose(mask_uf, 1, 2).contiguous().view(-1, g_sz * g_sz)\n\n    mask_uf = torch.index_select(mask_uf, 0, pos)\n    loss = F.soft_margin_loss(p_m, mask_uf)\n    iou_m, iou_5, iou_7 = iou_measure(p_m, mask_uf)\n    return loss, iou_m, iou_5, iou_7\n\n\ndef iou_measure(pred, label):\n    pred = pred.ge(0)\n    mask_sum = pred.eq(1).add(label.eq(1))\n    intxn = torch.sum(mask_sum == 2, dim=1).float()\n    union = torch.sum(mask_sum > 0, dim=1).float()\n    iou = intxn/union\n    return torch.mean(iou), (torch.sum(iou > 0.5).float()/iou.shape[0]), (torch.sum(iou > 0.7).float()/iou.shape[0])\n    \n\nif __name__ == ""__main__"":\n    p_m = torch.randn(4, 63*63, 25, 25)\n    cls = torch.randn(4, 1, 25, 25) > 0.9\n    mask = torch.randn(4, 1, 255, 255) * 2 - 1\n\n    loss = select_mask_logistic_loss(p_m, mask, cls)\n    print(loss)\n'"
models/siamrpn.py,7,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.bbox_helper import center2corner\nfrom torch.autograd import Variable\nfrom utils.anchors import Anchors\n\n\nclass SiamRPN(nn.Module):\n    def __init__(self, anchors=None):\n        super(SiamRPN, self).__init__()\n        self.anchors = anchors  # anchor_cfg\n        self.anchor = Anchors(anchors)\n        self.anchor_num = self.anchor.anchor_num\n        self.features = None\n        self.rpn_model = None\n\n        self.all_anchors = None\n\n    def set_all_anchors(self, image_center, size):\n        # cx,cy,w,h\n        if not self.anchor.generate_all_anchors(image_center, size):\n            return\n        all_anchors = self.anchor.all_anchors[1] # cx, cy, w, h\n        self.all_anchors = torch.from_numpy(all_anchors).float().cuda()\n        self.all_anchors = [self.all_anchors[i] for i in range(4)]\n\n    def feature_extractor(self, x):\n        return self.features(x)\n\n    def rpn(self, template, search):\n        pred_cls, pred_loc = self.rpn_model(template, search)\n        return pred_cls, pred_loc\n\n    def _add_rpn_loss(self, label_cls, label_loc, lable_loc_weight, rpn_pred_cls,\n                      rpn_pred_loc):\n        \'\'\'\n        :param compute_anchor_targets_fn: functions to produce anchors\' learning targets.\n        :param rpn_pred_cls: [B, num_anchors * 2, h, w], output of rpn for classification.\n        :param rpn_pred_loc: [B, num_anchors * 4, h, w], output of rpn for localization.\n        :return: loss of classification and localization, respectively.\n        \'\'\'\n        rpn_loss_cls = select_cross_entropy_loss(rpn_pred_cls, label_cls)\n\n        rpn_loss_loc = weight_l1_loss(rpn_pred_loc, label_loc, lable_loc_weight)\n\n        # classification accuracy, top1\n        acc = torch.zeros(1)  # TODO\n        return rpn_loss_cls, rpn_loss_loc, acc\n\n    def run(self, template, search, softmax=False):\n        """"""\n        run network\n        """"""\n        template_feature = self.feature_extractor(template)\n        search_feature = self.feature_extractor(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(template_feature, search_feature)\n        if softmax:\n            rpn_pred_cls = self.softmax(rpn_pred_cls)\n        return rpn_pred_cls, rpn_pred_loc, template_feature, search_feature\n\n    def softmax(self, cls):\n        b, a2, h, w = cls.size()\n        cls = cls.view(b, 2, a2//2, h, w)\n        cls = cls.permute(0, 2, 3, 4, 1).contiguous()\n        cls = F.log_softmax(cls, dim=4)\n        return cls\n\n    def forward(self, input):\n        """"""\n        :param input: dict of input with keys of:\n                \'template\': [b, 3, h1, w1], input template image.\n                \'search\': [b, 3, h2, w2], input search image.\n                \'label_cls\':[b, max_num_gts, 5] or None(self.training==False),\n                                     each gt contains x1,y1,x2,y2,class.\n        :return: dict of loss, predict, accuracy\n        """"""\n        template = input[\'template\']\n        search = input[\'search\']\n        if self.training:\n            label_cls = input[\'label_cls\']\n            label_loc = input[\'label_loc\']\n            lable_loc_weight = input[\'label_loc_weight\']\n\n        rpn_pred_cls, rpn_pred_loc, template_feature, search_feature = self.run(template, search, softmax=self.training)\n\n        outputs = dict(predict=[], losses=[], accuracy=[])\n\n        outputs[\'predict\'] = [rpn_pred_loc, rpn_pred_cls, template_feature, search_feature]\n        if self.training:\n            rpn_loss_cls, rpn_loss_loc, rpn_acc = self._add_rpn_loss(label_cls, label_loc, lable_loc_weight,\n                                                                     rpn_pred_cls, rpn_pred_loc)\n            outputs[\'losses\'] = [rpn_loss_cls, rpn_loss_loc]\n        return outputs\n\n    def template(self, z):\n        self.zf = self.feature_extractor(z)\n        cls_kernel, loc_kernel = self.rpn_model.template(self.zf)\n        return cls_kernel, loc_kernel\n\n    def track(self, x, cls_kernel=None, loc_kernel=None, softmax=False):\n        xf = self.feature_extractor(x)\n        rpn_pred_cls, rpn_pred_loc = self.rpn_model.track(xf, cls_kernel, loc_kernel)\n        if softmax:\n            rpn_pred_cls = self.softmax(rpn_pred_cls)\n        return rpn_pred_cls, rpn_pred_loc\n\n\ndef get_cls_loss(pred, label, select):\n    if len(select.size()) == 0: return 0\n    pred = torch.index_select(pred, 0, select)\n    label = torch.index_select(label, 0, select)\n    return F.nll_loss(pred, label)\n\n\ndef select_cross_entropy_loss(pred, label):\n    pred = pred.view(-1, 2)\n    label = label.view(-1)\n    pos = Variable(label.data.eq(1).nonzero().squeeze()).cuda()\n    neg = Variable(label.data.eq(0).nonzero().squeeze()).cuda()\n\n    loss_pos = get_cls_loss(pred, label, pos)\n    loss_neg = get_cls_loss(pred, label, neg)\n    return loss_pos * 0.5 + loss_neg * 0.5\n\n\ndef weight_l1_loss(pred_loc, label_loc, loss_weight):\n    """"""\n    :param pred_loc: [b, 4k, h, w]\n    :param label_loc: [b, 4k, h, w]\n    :param loss_weight:  [b, k, h, w]\n    :return: loc loss value\n    """"""\n    b, _, sh, sw = pred_loc.size()\n    pred_loc = pred_loc.view(b, 4, -1, sh, sw)\n    diff = (pred_loc - label_loc).abs()\n    diff = diff.sum(dim=1).view(b, -1, sh, sw)\n    loss = diff * loss_weight\n    return loss.sum().div(b)\n'"
tools/__init__.py,0,b''
tools/demo.py,2,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport glob\nfrom tools.test import *\n\nparser = argparse.ArgumentParser(description=\'PyTorch Tracking Demo\')\n\nparser.add_argument(\'--resume\', default=\'\', type=str, required=True,\n                    metavar=\'PATH\',help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--config\', dest=\'config\', default=\'config_davis.json\',\n                    help=\'hyper-parameter of SiamMask in json format\')\nparser.add_argument(\'--base_path\', default=\'../../data/tennis\', help=\'datasets\')\nparser.add_argument(\'--cpu\', action=\'store_true\', help=\'cpu mode\')\nargs = parser.parse_args()\n\nif __name__ == \'__main__\':\n    # Setup device\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    torch.backends.cudnn.benchmark = True\n\n    # Setup Model\n    cfg = load_config(args)\n    from custom import Custom\n    siammask = Custom(anchors=cfg[\'anchors\'])\n    if args.resume:\n        assert isfile(args.resume), \'Please download {} first.\'.format(args.resume)\n        siammask = load_pretrain(siammask, args.resume)\n\n    siammask.eval().to(device)\n\n    # Parse Image file\n    img_files = sorted(glob.glob(join(args.base_path, \'*.jp*\')))\n    ims = [cv2.imread(imf) for imf in img_files]\n\n    # Select ROI\n    cv2.namedWindow(""SiamMask"", cv2.WND_PROP_FULLSCREEN)\n    # cv2.setWindowProperty(""SiamMask"", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n    try:\n        init_rect = cv2.selectROI(\'SiamMask\', ims[0], False, False)\n        x, y, w, h = init_rect\n    except:\n        exit()\n\n    toc = 0\n    for f, im in enumerate(ims):\n        tic = cv2.getTickCount()\n        if f == 0:  # init\n            target_pos = np.array([x + w / 2, y + h / 2])\n            target_sz = np.array([w, h])\n            state = siamese_init(im, target_pos, target_sz, siammask, cfg[\'hp\'], device=device)  # init tracker\n        elif f > 0:  # tracking\n            state = siamese_track(state, im, mask_enable=True, refine_enable=True, device=device)  # track\n            location = state[\'ploygon\'].flatten()\n            mask = state[\'mask\'] > state[\'p\'].seg_thr\n\n            im[:, :, 2] = (mask > 0) * 255 + (mask == 0) * im[:, :, 2]\n            cv2.polylines(im, [np.int0(location).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n            cv2.imshow(\'SiamMask\', im)\n            key = cv2.waitKey(1)\n            if key > 0:\n                break\n\n        toc += cv2.getTickCount() - tic\n    toc /= cv2.getTickFrequency()\n    fps = f / toc\n    print(\'SiamMask Time: {:02.1f}s Speed: {:3.1f}fps (with visulization!)\'.format(toc, fps))\n'"
tools/eval.py,0,"b""# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport argparse\nimport glob\nfrom os.path import join, realpath, dirname\n\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nfrom utils.pysot.datasets import VOTDataset\nfrom utils.pysot.evaluation import AccuracyRobustnessBenchmark, EAOBenchmark\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='VOT Evaluation')\n    parser.add_argument('--dataset', type=str, help='dataset name')\n    parser.add_argument('--result_dir', type=str, help='tracker result root')\n    parser.add_argument('--tracker_prefix', type=str, help='tracker prefix')\n    parser.add_argument('--show_video_level', action='store_true')\n    parser.add_argument('--num', type=int, help='number of processes to eval', default=1)\n    args = parser.parse_args()\n\n    root = join(realpath(dirname(__file__)), '../data')\n    tracker_dir = args.result_dir\n    trackers = glob.glob(join(tracker_dir, args.tracker_prefix+'*'))\n    trackers = [t.split('/')[-1] for t in trackers]\n\n    assert len(trackers) > 0\n    args.num = min(args.num, len(trackers))\n\n    if args.dataset in ['VOT2016', 'VOT2018', 'VOT2019']:\n        dataset = VOTDataset(args.dataset, root)\n        dataset.set_tracker(tracker_dir, trackers)\n        ar_benchmark = AccuracyRobustnessBenchmark(dataset)\n        ar_result = {}\n        with Pool(processes=args.num) as pool:\n            for ret in tqdm(pool.imap_unordered(ar_benchmark.eval,\n                                                trackers), desc='eval ar', total=len(trackers), ncols=100):\n                ar_result.update(ret)\n\n        benchmark = EAOBenchmark(dataset)\n        eao_result = {}\n        with Pool(processes=args.num) as pool:\n            for ret in tqdm(pool.imap_unordered(benchmark.eval,\n                                                trackers), desc='eval eao', total=len(trackers), ncols=100):\n                eao_result.update(ret)\n        ar_benchmark.show_result(ar_result, eao_result, show_video_level=args.show_video_level)\n"""
tools/test.py,5,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\nimport argparse\nimport logging\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom os import makedirs\nfrom os.path import join, isdir, isfile\n\nfrom utils.log_helper import init_log, add_file_handler\nfrom utils.load_helper import load_pretrain\nfrom utils.bbox_helper import get_axis_aligned_bbox, cxy_wh_2_rect\nfrom utils.benchmark_helper import load_dataset, dataset_zoo\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom utils.anchors import Anchors\nfrom utils.tracker_config import TrackerConfig\n\nfrom utils.config_helper import load_config\nfrom utils.pyvotkit.region import vot_overlap, vot_float2str\n\nthrs = np.arange(0.3, 0.5, 0.05)\n\nparser = argparse.ArgumentParser(description=\'Test SiamMask\')\nparser.add_argument(\'--arch\', dest=\'arch\', default=\'\', choices=[\'Custom\',],\n                    help=\'architecture of pretrained model\')\nparser.add_argument(\'--config\', dest=\'config\', required=True, help=\'hyper-parameter for SiamMask\')\nparser.add_argument(\'--resume\', default=\'\', type=str, required=True,\n                    metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--mask\', action=\'store_true\', help=\'whether use mask output\')\nparser.add_argument(\'--refine\', action=\'store_true\', help=\'whether use mask refine output\')\nparser.add_argument(\'--dataset\', dest=\'dataset\', default=\'VOT2018\', choices=dataset_zoo,\n                    help=\'datasets\')\nparser.add_argument(\'-l\', \'--log\', default=""log_test.txt"", type=str, help=\'log file\')\nparser.add_argument(\'-v\', \'--visualization\', dest=\'visualization\', action=\'store_true\',\n                    help=\'whether visualize result\')\nparser.add_argument(\'--save_mask\', action=\'store_true\', help=\'whether use save mask for davis\')\nparser.add_argument(\'--gt\', action=\'store_true\', help=\'whether use gt rect for davis (Oracle)\')\nparser.add_argument(\'--video\', default=\'\', type=str, help=\'test special video\')\nparser.add_argument(\'--cpu\', action=\'store_true\', help=\'cpu mode\')\nparser.add_argument(\'--debug\', action=\'store_true\', help=\'debug mode\')\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n\n\ndef im_to_torch(img):\n    img = np.transpose(img, (2, 0, 1))  # C*H*W\n    img = to_torch(img).float()\n    return img\n\n\ndef get_subwindow_tracking(im, pos, model_sz, original_sz, avg_chans, out_mode=\'torch\'):\n    if isinstance(pos, float):\n        pos = [pos, pos]\n    sz = original_sz\n    im_sz = im.shape\n    c = (original_sz + 1) / 2\n    context_xmin = round(pos[0] - c)\n    context_xmax = context_xmin + sz - 1\n    context_ymin = round(pos[1] - c)\n    context_ymax = context_ymin + sz - 1\n    left_pad = int(max(0., -context_xmin))\n    top_pad = int(max(0., -context_ymin))\n    right_pad = int(max(0., context_xmax - im_sz[1] + 1))\n    bottom_pad = int(max(0., context_ymax - im_sz[0] + 1))\n\n    context_xmin = context_xmin + left_pad\n    context_xmax = context_xmax + left_pad\n    context_ymin = context_ymin + top_pad\n    context_ymax = context_ymax + top_pad\n\n    # zzp: a more easy speed version\n    r, c, k = im.shape\n    if any([top_pad, bottom_pad, left_pad, right_pad]):\n        te_im = np.zeros((r + top_pad + bottom_pad, c + left_pad + right_pad, k), np.uint8)\n        te_im[top_pad:top_pad + r, left_pad:left_pad + c, :] = im\n        if top_pad:\n            te_im[0:top_pad, left_pad:left_pad + c, :] = avg_chans\n        if bottom_pad:\n            te_im[r + top_pad:, left_pad:left_pad + c, :] = avg_chans\n        if left_pad:\n            te_im[:, 0:left_pad, :] = avg_chans\n        if right_pad:\n            te_im[:, c + left_pad:, :] = avg_chans\n        im_patch_original = te_im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n    else:\n        im_patch_original = im[int(context_ymin):int(context_ymax + 1), int(context_xmin):int(context_xmax + 1), :]\n\n    if not np.array_equal(model_sz, original_sz):\n        im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))\n    else:\n        im_patch = im_patch_original\n    # cv2.imshow(\'crop\', im_patch)\n    # cv2.waitKey(0)\n    return im_to_torch(im_patch) if out_mode in \'torch\' else im_patch\n\n\ndef generate_anchor(cfg, score_size):\n    anchors = Anchors(cfg)\n    anchor = anchors.anchors\n    x1, y1, x2, y2 = anchor[:, 0], anchor[:, 1], anchor[:, 2], anchor[:, 3]\n    anchor = np.stack([(x1+x2)*0.5, (y1+y2)*0.5, x2-x1, y2-y1], 1)\n\n    total_stride = anchors.stride\n    anchor_num = anchor.shape[0]\n\n    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n    ori = - (score_size // 2) * total_stride\n    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n                         [ori + total_stride * dy for dy in range(score_size)])\n    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n    return anchor\n\n\ndef siamese_init(im, target_pos, target_sz, model, hp=None, device=\'cpu\'):\n    state = dict()\n    state[\'im_h\'] = im.shape[0]\n    state[\'im_w\'] = im.shape[1]\n    p = TrackerConfig()\n    p.update(hp, model.anchors)\n\n    p.renew()\n\n    net = model\n    p.scales = model.anchors[\'scales\']\n    p.ratios = model.anchors[\'ratios\']\n    p.anchor_num = model.anchor_num\n    p.anchor = generate_anchor(model.anchors, p.score_size)\n    avg_chans = np.mean(im, axis=(0, 1))\n\n    wc_z = target_sz[0] + p.context_amount * sum(target_sz)\n    hc_z = target_sz[1] + p.context_amount * sum(target_sz)\n    s_z = round(np.sqrt(wc_z * hc_z))\n    # initialize the exemplar\n    z_crop = get_subwindow_tracking(im, target_pos, p.exemplar_size, s_z, avg_chans)\n\n    z = Variable(z_crop.unsqueeze(0))\n    net.template(z.to(device))\n\n    if p.windowing == \'cosine\':\n        window = np.outer(np.hanning(p.score_size), np.hanning(p.score_size))\n    elif p.windowing == \'uniform\':\n        window = np.ones((p.score_size, p.score_size))\n    window = np.tile(window.flatten(), p.anchor_num)\n\n    state[\'p\'] = p\n    state[\'net\'] = net\n    state[\'avg_chans\'] = avg_chans\n    state[\'window\'] = window\n    state[\'target_pos\'] = target_pos\n    state[\'target_sz\'] = target_sz\n    return state\n\n\ndef siamese_track(state, im, mask_enable=False, refine_enable=False, device=\'cpu\', debug=False):\n    p = state[\'p\']\n    net = state[\'net\']\n    avg_chans = state[\'avg_chans\']\n    window = state[\'window\']\n    target_pos = state[\'target_pos\']\n    target_sz = state[\'target_sz\']\n\n    wc_x = target_sz[1] + p.context_amount * sum(target_sz)\n    hc_x = target_sz[0] + p.context_amount * sum(target_sz)\n    s_x = np.sqrt(wc_x * hc_x)\n    scale_x = p.exemplar_size / s_x\n    d_search = (p.instance_size - p.exemplar_size) / 2\n    pad = d_search / scale_x\n    s_x = s_x + 2 * pad\n    crop_box = [target_pos[0] - round(s_x) / 2, target_pos[1] - round(s_x) / 2, round(s_x), round(s_x)]\n\n    if debug:\n        im_debug = im.copy()\n        crop_box_int = np.int0(crop_box)\n        cv2.rectangle(im_debug, (crop_box_int[0], crop_box_int[1]),\n                      (crop_box_int[0] + crop_box_int[2], crop_box_int[1] + crop_box_int[3]), (255, 0, 0), 2)\n        cv2.imshow(\'search area\', im_debug)\n        cv2.waitKey(0)\n\n    # extract scaled crops for search region x at previous target position\n    x_crop = Variable(get_subwindow_tracking(im, target_pos, p.instance_size, round(s_x), avg_chans).unsqueeze(0))\n\n    if mask_enable:\n        score, delta, mask = net.track_mask(x_crop.to(device))\n    else:\n        score, delta = net.track(x_crop.to(device))\n\n    delta = delta.permute(1, 2, 3, 0).contiguous().view(4, -1).data.cpu().numpy()\n    score = F.softmax(score.permute(1, 2, 3, 0).contiguous().view(2, -1).permute(1, 0), dim=1).data[:,\n            1].cpu().numpy()\n\n    delta[0, :] = delta[0, :] * p.anchor[:, 2] + p.anchor[:, 0]\n    delta[1, :] = delta[1, :] * p.anchor[:, 3] + p.anchor[:, 1]\n    delta[2, :] = np.exp(delta[2, :]) * p.anchor[:, 2]\n    delta[3, :] = np.exp(delta[3, :]) * p.anchor[:, 3]\n\n    def change(r):\n        return np.maximum(r, 1. / r)\n\n    def sz(w, h):\n        pad = (w + h) * 0.5\n        sz2 = (w + pad) * (h + pad)\n        return np.sqrt(sz2)\n\n    def sz_wh(wh):\n        pad = (wh[0] + wh[1]) * 0.5\n        sz2 = (wh[0] + pad) * (wh[1] + pad)\n        return np.sqrt(sz2)\n\n    # size penalty\n    target_sz_in_crop = target_sz*scale_x\n    s_c = change(sz(delta[2, :], delta[3, :]) / (sz_wh(target_sz_in_crop)))  # scale penalty\n    r_c = change((target_sz_in_crop[0] / target_sz_in_crop[1]) / (delta[2, :] / delta[3, :]))  # ratio penalty\n\n    penalty = np.exp(-(r_c * s_c - 1) * p.penalty_k)\n    pscore = penalty * score\n\n    # cos window (motion model)\n    pscore = pscore * (1 - p.window_influence) + window * p.window_influence\n    best_pscore_id = np.argmax(pscore)\n\n    pred_in_crop = delta[:, best_pscore_id] / scale_x\n    lr = penalty[best_pscore_id] * score[best_pscore_id] * p.lr  # lr for OTB\n\n    res_x = pred_in_crop[0] + target_pos[0]\n    res_y = pred_in_crop[1] + target_pos[1]\n\n    res_w = target_sz[0] * (1 - lr) + pred_in_crop[2] * lr\n    res_h = target_sz[1] * (1 - lr) + pred_in_crop[3] * lr\n\n    target_pos = np.array([res_x, res_y])\n    target_sz = np.array([res_w, res_h])\n\n    # for Mask Branch\n    if mask_enable:\n        best_pscore_id_mask = np.unravel_index(best_pscore_id, (5, p.score_size, p.score_size))\n        delta_x, delta_y = best_pscore_id_mask[2], best_pscore_id_mask[1]\n\n        if refine_enable:\n            mask = net.track_refine((delta_y, delta_x)).to(device).sigmoid().squeeze().view(\n                p.out_size, p.out_size).cpu().data.numpy()\n        else:\n            mask = mask[0, :, delta_y, delta_x].sigmoid(). \\\n                squeeze().view(p.out_size, p.out_size).cpu().data.numpy()\n\n        def crop_back(image, bbox, out_sz, padding=-1):\n            a = (out_sz[0] - 1) / bbox[2]\n            b = (out_sz[1] - 1) / bbox[3]\n            c = -a * bbox[0]\n            d = -b * bbox[1]\n            mapping = np.array([[a, 0, c],\n                                [0, b, d]]).astype(np.float)\n            crop = cv2.warpAffine(image, mapping, (out_sz[0], out_sz[1]),\n                                  flags=cv2.INTER_LINEAR,\n                                  borderMode=cv2.BORDER_CONSTANT,\n                                  borderValue=padding)\n            return crop\n\n        s = crop_box[2] / p.instance_size\n        sub_box = [crop_box[0] + (delta_x - p.base_size / 2) * p.total_stride * s,\n                   crop_box[1] + (delta_y - p.base_size / 2) * p.total_stride * s,\n                   s * p.exemplar_size, s * p.exemplar_size]\n        s = p.out_size / sub_box[2]\n        back_box = [-sub_box[0] * s, -sub_box[1] * s, state[\'im_w\'] * s, state[\'im_h\'] * s]\n        mask_in_img = crop_back(mask, back_box, (state[\'im_w\'], state[\'im_h\']))\n\n        target_mask = (mask_in_img > p.seg_thr).astype(np.uint8)\n        if cv2.__version__[-5] == \'4\':\n            contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        else:\n            _, contours, _ = cv2.findContours(target_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        cnt_area = [cv2.contourArea(cnt) for cnt in contours]\n        if len(contours) != 0 and np.max(cnt_area) > 100:\n            contour = contours[np.argmax(cnt_area)]  # use max area polygon\n            polygon = contour.reshape(-1, 2)\n            # pbox = cv2.boundingRect(polygon)  # Min Max Rectangle\n            prbox = cv2.boxPoints(cv2.minAreaRect(polygon))  # Rotated Rectangle\n\n            # box_in_img = pbox\n            rbox_in_img = prbox\n        else:  # empty mask\n            location = cxy_wh_2_rect(target_pos, target_sz)\n            rbox_in_img = np.array([[location[0], location[1]],\n                                    [location[0] + location[2], location[1]],\n                                    [location[0] + location[2], location[1] + location[3]],\n                                    [location[0], location[1] + location[3]]])\n\n    target_pos[0] = max(0, min(state[\'im_w\'], target_pos[0]))\n    target_pos[1] = max(0, min(state[\'im_h\'], target_pos[1]))\n    target_sz[0] = max(10, min(state[\'im_w\'], target_sz[0]))\n    target_sz[1] = max(10, min(state[\'im_h\'], target_sz[1]))\n\n    state[\'target_pos\'] = target_pos\n    state[\'target_sz\'] = target_sz\n    state[\'score\'] = score[best_pscore_id]\n    state[\'mask\'] = mask_in_img if mask_enable else []\n    state[\'ploygon\'] = rbox_in_img if mask_enable else []\n    return state\n\n\ndef track_vot(model, video, hp=None, mask_enable=False, refine_enable=False, device=\'cpu\'):\n    regions = []  # result and states[1 init / 2 lost / 0 skip]\n    image_files, gt = video[\'image_files\'], video[\'gt\']\n\n    start_frame, end_frame, lost_times, toc = 0, len(image_files), 0, 0\n\n    for f, image_file in enumerate(image_files):\n        im = cv2.imread(image_file)\n        tic = cv2.getTickCount()\n        if f == start_frame:  # init\n            cx, cy, w, h = get_axis_aligned_bbox(gt[f])\n            target_pos = np.array([cx, cy])\n            target_sz = np.array([w, h])\n            state = siamese_init(im, target_pos, target_sz, model, hp, device)  # init tracker\n            location = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n            regions.append(1 if \'VOT\' in args.dataset else gt[f])\n        elif f > start_frame:  # tracking\n            state = siamese_track(state, im, mask_enable, refine_enable, device, args.debug)  # track\n            if mask_enable:\n                location = state[\'ploygon\'].flatten()\n                mask = state[\'mask\']\n            else:\n                location = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n                mask = []\n\n            if \'VOT\' in args.dataset:\n                gt_polygon = ((gt[f][0], gt[f][1]), (gt[f][2], gt[f][3]),\n                              (gt[f][4], gt[f][5]), (gt[f][6], gt[f][7]))\n                if mask_enable:\n                    pred_polygon = ((location[0], location[1]), (location[2], location[3]),\n                                    (location[4], location[5]), (location[6], location[7]))\n                else:\n                    pred_polygon = ((location[0], location[1]),\n                                    (location[0] + location[2], location[1]),\n                                    (location[0] + location[2], location[1] + location[3]),\n                                    (location[0], location[1] + location[3]))\n                b_overlap = vot_overlap(gt_polygon, pred_polygon, (im.shape[1], im.shape[0]))\n            else:\n                b_overlap = 1\n\n            if b_overlap:\n                regions.append(location)\n            else:  # lost\n                regions.append(2)\n                lost_times += 1\n                start_frame = f + 5  # skip 5 frames\n        else:  # skip\n            regions.append(0)\n        toc += cv2.getTickCount() - tic\n\n        if args.visualization and f >= start_frame:  # visualization (skip lost frame)\n            im_show = im.copy()\n            if f == 0: cv2.destroyAllWindows()\n            if gt.shape[0] > f:\n                if len(gt[f]) == 8:\n                    cv2.polylines(im_show, [np.array(gt[f], np.int).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n                else:\n                    cv2.rectangle(im_show, (gt[f, 0], gt[f, 1]), (gt[f, 0] + gt[f, 2], gt[f, 1] + gt[f, 3]), (0, 255, 0), 3)\n            if len(location) == 8:\n                if mask_enable:\n                    mask = mask > state[\'p\'].seg_thr\n                    im_show[:, :, 2] = mask * 255 + (1 - mask) * im_show[:, :, 2]\n                location_int = np.int0(location)\n                cv2.polylines(im_show, [location_int.reshape((-1, 1, 2))], True, (0, 255, 255), 3)\n            else:\n                location = [int(l) for l in location]\n                cv2.rectangle(im_show, (location[0], location[1]),\n                              (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n            cv2.putText(im_show, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n            cv2.putText(im_show, str(lost_times), (40, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n            cv2.putText(im_show, str(state[\'score\']) if \'score\' in state else \'\', (40, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n\n            cv2.imshow(video[\'name\'], im_show)\n            cv2.waitKey(1)\n    toc /= cv2.getTickFrequency()\n\n    # save result\n    name = args.arch.split(\'.\')[0] + \'_\' + (\'mask_\' if mask_enable else \'\') + (\'refine_\' if refine_enable else \'\') +\\\n           args.resume.split(\'/\')[-1].split(\'.\')[0]\n\n    if \'VOT\' in args.dataset:\n        video_path = join(\'test\', args.dataset, name,\n                          \'baseline\', video[\'name\'])\n        if not isdir(video_path): makedirs(video_path)\n        result_path = join(video_path, \'{:s}_001.txt\'.format(video[\'name\']))\n        with open(result_path, ""w"") as fin:\n            for x in regions:\n                fin.write(""{:d}\\n"".format(x)) if isinstance(x, int) else \\\n                        fin.write(\',\'.join([vot_float2str(""%.4f"", i) for i in x]) + \'\\n\')\n    else:  # OTB\n        video_path = join(\'test\', args.dataset, name)\n        if not isdir(video_path): makedirs(video_path)\n        result_path = join(video_path, \'{:s}.txt\'.format(video[\'name\']))\n        with open(result_path, ""w"") as fin:\n            for x in regions:\n                fin.write(\',\'.join([str(i) for i in x])+\'\\n\')\n\n    logger.info(\'({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps Lost: {:d}\'.format(\n        v_id, video[\'name\'], toc, f / toc, lost_times))\n\n    return lost_times, f / toc\n\n\ndef MultiBatchIouMeter(thrs, outputs, targets, start=None, end=None):\n    targets = np.array(targets)\n    outputs = np.array(outputs)\n\n    num_frame = targets.shape[0]\n    if start is None:\n        object_ids = np.array(list(range(outputs.shape[0]))) + 1\n    else:\n        object_ids = [int(id) for id in start]\n\n    num_object = len(object_ids)\n    res = np.zeros((num_object, len(thrs)), dtype=np.float32)\n\n    output_max_id = np.argmax(outputs, axis=0).astype(\'uint8\')+1\n    outputs_max = np.max(outputs, axis=0)\n    for k, thr in enumerate(thrs):\n        output_thr = outputs_max > thr\n        for j in range(num_object):\n            target_j = targets == object_ids[j]\n\n            if start is None:\n                start_frame, end_frame = 1, num_frame - 1\n            else:\n                start_frame, end_frame = start[str(object_ids[j])] + 1, end[str(object_ids[j])] - 1\n            iou = []\n            for i in range(start_frame, end_frame):\n                pred = (output_thr[i] * output_max_id[i]) == (j+1)\n                mask_sum = (pred == 1).astype(np.uint8) + (target_j[i] > 0).astype(np.uint8)\n                intxn = np.sum(mask_sum == 2)\n                union = np.sum(mask_sum > 0)\n                if union > 0:\n                    iou.append(intxn / union)\n                elif union == 0 and intxn == 0:\n                    iou.append(1)\n            res[j, k] = np.mean(iou)\n    return res\n\n\ndef track_vos(model, video, hp=None, mask_enable=False, refine_enable=False, mot_enable=False, device=\'cpu\'):\n    image_files = video[\'image_files\']\n\n    annos = [np.array(Image.open(x)) for x in video[\'anno_files\']]\n    if \'anno_init_files\' in video:\n        annos_init = [np.array(Image.open(x)) for x in video[\'anno_init_files\']]\n    else:\n        annos_init = [annos[0]]\n\n    if not mot_enable:\n        annos = [(anno > 0).astype(np.uint8) for anno in annos]\n        annos_init = [(anno_init > 0).astype(np.uint8) for anno_init in annos_init]\n\n    if \'start_frame\' in video:\n        object_ids = [int(id) for id in video[\'start_frame\']]\n    else:\n        object_ids = [o_id for o_id in np.unique(annos[0]) if o_id != 0]\n        if len(object_ids) != len(annos_init):\n            annos_init = annos_init*len(object_ids)\n    object_num = len(object_ids)\n    toc = 0\n    pred_masks = np.zeros((object_num, len(image_files), annos[0].shape[0], annos[0].shape[1]))-1\n    for obj_id, o_id in enumerate(object_ids):\n\n        if \'start_frame\' in video:\n            start_frame = video[\'start_frame\'][str(o_id)]\n            end_frame = video[\'end_frame\'][str(o_id)]\n        else:\n            start_frame, end_frame = 0, len(image_files)\n\n        for f, image_file in enumerate(image_files):\n            im = cv2.imread(image_file)\n            tic = cv2.getTickCount()\n            if f == start_frame:  # init\n                mask = annos_init[obj_id] == o_id\n                x, y, w, h = cv2.boundingRect((mask).astype(np.uint8))\n                cx, cy = x + w/2, y + h/2\n                target_pos = np.array([cx, cy])\n                target_sz = np.array([w, h])\n                state = siamese_init(im, target_pos, target_sz, model, hp, device=device)  # init tracker\n            elif end_frame >= f > start_frame:  # tracking\n                state = siamese_track(state, im, mask_enable, refine_enable, device=device)  # track\n                mask = state[\'mask\']\n            toc += cv2.getTickCount() - tic\n            if end_frame >= f >= start_frame:\n                pred_masks[obj_id, f, :, :] = mask\n    toc /= cv2.getTickFrequency()\n\n    if len(annos) == len(image_files):\n        multi_mean_iou = MultiBatchIouMeter(thrs, pred_masks, annos,\n                                            start=video[\'start_frame\'] if \'start_frame\' in video else None,\n                                            end=video[\'end_frame\'] if \'end_frame\' in video else None)\n        for i in range(object_num):\n            for j, thr in enumerate(thrs):\n                logger.info(\'Fusion Multi Object{:20s} IOU at {:.2f}: {:.4f}\'.format(video[\'name\'] + \'_\' + str(i + 1), thr,\n                                                                           multi_mean_iou[i, j]))\n    else:\n        multi_mean_iou = []\n\n    if args.save_mask:\n        video_path = join(\'test\', args.dataset, \'SiamMask\', video[\'name\'])\n        if not isdir(video_path): makedirs(video_path)\n        pred_mask_final = np.array(pred_masks)\n        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype(\'uint8\') + 1) * (\n                np.max(pred_mask_final, axis=0) > state[\'p\'].seg_thr).astype(\'uint8\')\n        for i in range(pred_mask_final.shape[0]):\n            cv2.imwrite(join(video_path, image_files[i].split(\'/\')[-1].split(\'.\')[0] + \'.png\'), pred_mask_final[i].astype(np.uint8))\n\n    if args.visualization:\n        pred_mask_final = np.array(pred_masks)\n        pred_mask_final = (np.argmax(pred_mask_final, axis=0).astype(\'uint8\') + 1) * (\n                np.max(pred_mask_final, axis=0) > state[\'p\'].seg_thr).astype(\'uint8\')\n        COLORS = np.random.randint(128, 255, size=(object_num, 3), dtype=""uint8"")\n        COLORS = np.vstack([[0, 0, 0], COLORS]).astype(""uint8"")\n        mask = COLORS[pred_mask_final]\n        for f, image_file in enumerate(image_files):\n            output = ((0.4 * cv2.imread(image_file)) + (0.6 * mask[f,:,:,:])).astype(""uint8"")\n            cv2.imshow(""mask"", output)\n            cv2.waitKey(1)\n\n    logger.info(\'({:d}) Video: {:12s} Time: {:02.1f}s Speed: {:3.1f}fps\'.format(\n        v_id, video[\'name\'], toc, f*len(object_ids) / toc))\n\n    return multi_mean_iou, f*len(object_ids) / toc\n\n\ndef main():\n    global args, logger, v_id\n    args = parser.parse_args()\n    cfg = load_config(args)\n\n    init_log(\'global\', logging.INFO)\n    if args.log != """":\n        add_file_handler(\'global\', args.log, logging.INFO)\n\n    logger = logging.getLogger(\'global\')\n    logger.info(args)\n\n    # setup model\n    if args.arch == \'Custom\':\n        from custom import Custom\n        model = Custom(anchors=cfg[\'anchors\'])\n    else:\n        parser.error(\'invalid architecture: {}\'.format(args.arch))\n\n    if args.resume:\n        assert isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        model = load_pretrain(model, args.resume)\n    model.eval()\n    device = torch.device(\'cuda\' if (torch.cuda.is_available() and not args.cpu) else \'cpu\')\n    model = model.to(device)\n    # setup dataset\n    dataset = load_dataset(args.dataset)\n\n    # VOS or VOT?\n    if args.dataset in [\'DAVIS2016\', \'DAVIS2017\', \'ytb_vos\'] and args.mask:\n        vos_enable = True  # enable Mask output\n    else:\n        vos_enable = False\n\n    total_lost = 0  # VOT\n    iou_lists = []  # VOS\n    speed_list = []\n\n    for v_id, video in enumerate(dataset.keys(), start=1):\n        if args.video != \'\' and video != args.video:\n            continue\n\n        if vos_enable:\n            iou_list, speed = track_vos(model, dataset[video], cfg[\'hp\'] if \'hp\' in cfg.keys() else None,\n                                 args.mask, args.refine, args.dataset in [\'DAVIS2017\', \'ytb_vos\'], device=device)\n            iou_lists.append(iou_list)\n        else:\n            lost, speed = track_vot(model, dataset[video], cfg[\'hp\'] if \'hp\' in cfg.keys() else None,\n                             args.mask, args.refine, device=device)\n            total_lost += lost\n        speed_list.append(speed)\n\n    # report final result\n    if vos_enable:\n        for thr, iou in zip(thrs, np.mean(np.concatenate(iou_lists), axis=0)):\n            logger.info(\'Segmentation Threshold {:.2f} mIoU: {:.3f}\'.format(thr, iou))\n    else:\n        logger.info(\'Total Lost: {:d}\'.format(total_lost))\n\n    logger.info(\'Mean Speed: {:.2f} FPS\'.format(np.mean(speed_list)))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train_siammask.py,20,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport argparse\nimport logging\nimport os\nimport cv2\nimport shutil\nimport time\nimport json\nimport math\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom utils.log_helper import init_log, print_speed, add_file_handler, Dummy\nfrom utils.load_helper import load_pretrain, restore_from\nfrom utils.average_meter_helper import AverageMeter\n\nfrom datasets.siam_mask_dataset import DataSets\n\nfrom utils.lr_helper import build_lr_scheduler\nfrom tensorboardX import SummaryWriter\n\nfrom utils.config_helper import load_config\nfrom torch.utils.collect_env import get_pretty_env_info\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description=\'PyTorch Tracking SiamMask Training\')\n\nparser.add_argument(\'-j\', \'--workers\', default=16, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 16)\')\nparser.add_argument(\'--epochs\', default=50, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch\', default=64, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 64)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--clip\', default=10.0, type=float,\n                    help=\'gradient clip value\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', default=\'\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--config\', dest=\'config\', required=True,\n                    help=\'hyperparameter of SiamMask in json format\')\nparser.add_argument(\'--arch\', dest=\'arch\', default=\'\', choices=[\'Custom\',],\n                    help=\'architecture of pretrained model\')\nparser.add_argument(\'-l\', \'--log\', default=""log.txt"", type=str,\n                    help=\'log file\')\nparser.add_argument(\'-s\', \'--save_dir\', default=\'snapshot\', type=str,\n                    help=\'save dir\')\nparser.add_argument(\'--log-dir\', default=\'board\', help=\'TensorBoard log dir\')\n\n\nbest_acc = 0.\n\n\ndef collect_env_info():\n    env_str = get_pretty_env_info()\n    env_str += ""\\n        OpenCV ({})"".format(cv2.__version__)\n    return env_str\n\n\ndef build_data_loader(cfg):\n    logger = logging.getLogger(\'global\')\n\n    logger.info(""build train dataset"")  # train_dataset\n    train_set = DataSets(cfg[\'train_datasets\'], cfg[\'anchors\'], args.epochs)\n    train_set.shuffle()\n\n    logger.info(""build val dataset"")  # val_dataset\n    if not \'val_datasets\' in cfg.keys():\n        cfg[\'val_datasets\'] = cfg[\'train_datasets\']\n    val_set = DataSets(cfg[\'val_datasets\'], cfg[\'anchors\'])\n    val_set.shuffle()\n\n    train_loader = DataLoader(train_set, batch_size=args.batch, num_workers=args.workers,\n                              pin_memory=True, sampler=None)\n    val_loader = DataLoader(val_set, batch_size=args.batch, num_workers=args.workers,\n                            pin_memory=True, sampler=None)\n\n    logger.info(\'build dataset done\')\n    return train_loader, val_loader\n\n\ndef build_opt_lr(model, cfg, args, epoch):\n    backbone_feature = model.features.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'feature_lr_mult\'])\n    if len(backbone_feature) == 0:\n        trainable_params = model.rpn_model.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'rpn_lr_mult\'], \'mask\')\n    else:\n        trainable_params = backbone_feature + \\\n                           model.rpn_model.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'rpn_lr_mult\']) + \\\n                           model.mask_model.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'mask_lr_mult\'])\n\n    optimizer = torch.optim.SGD(trainable_params, args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    lr_scheduler = build_lr_scheduler(optimizer, cfg[\'lr\'], epochs=args.epochs)\n\n    lr_scheduler.step(epoch)\n\n    return optimizer, lr_scheduler\n\n\ndef main():\n    global args, best_acc, tb_writer, logger\n    args = parser.parse_args()\n\n    init_log(\'global\', logging.INFO)\n\n    if args.log != """":\n        add_file_handler(\'global\', args.log, logging.INFO)\n\n    logger = logging.getLogger(\'global\')\n    logger.info(""\\n"" + collect_env_info())\n    logger.info(args)\n\n    cfg = load_config(args)\n    logger.info(""config \\n{}"".format(json.dumps(cfg, indent=4)))\n\n    if args.log_dir:\n        tb_writer = SummaryWriter(args.log_dir)\n    else:\n        tb_writer = Dummy()\n\n    # build dataset\n    train_loader, val_loader = build_data_loader(cfg)\n\n    if args.arch == \'Custom\':\n        from custom import Custom\n        model = Custom(pretrain=True, anchors=cfg[\'anchors\'])\n    else:\n        exit()\n    logger.info(model)\n\n    if args.pretrained:\n        model = load_pretrain(model, args.pretrained)\n\n    model = model.cuda()\n    dist_model = torch.nn.DataParallel(model, list(range(torch.cuda.device_count()))).cuda()\n\n    if args.resume and args.start_epoch != 0:\n        model.features.unfix((args.start_epoch - 1) / args.epochs)\n\n    optimizer, lr_scheduler = build_opt_lr(model, cfg, args, args.start_epoch)\n    # optionally resume from a checkpoint\n    if args.resume:\n        assert os.path.isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        model, optimizer, args.start_epoch, best_acc, arch = restore_from(model, optimizer, args.resume)\n        dist_model = torch.nn.DataParallel(model, list(range(torch.cuda.device_count()))).cuda()\n\n    logger.info(lr_scheduler)\n\n    logger.info(\'model prepare done\')\n\n    train(train_loader, dist_model, optimizer, lr_scheduler, args.start_epoch, cfg)\n\n\ndef train(train_loader, model, optimizer, lr_scheduler, epoch, cfg):\n    global tb_index, best_acc, cur_lr, logger\n    cur_lr = lr_scheduler.get_cur_lr()\n    logger = logging.getLogger(\'global\')\n    avg = AverageMeter()\n    model.train()\n    model = model.cuda()\n    end = time.time()\n\n    def is_valid_number(x):\n        return not(math.isnan(x) or math.isinf(x) or x > 1e4)\n\n    num_per_epoch = len(train_loader.dataset) // args.epochs // args.batch\n    start_epoch = epoch\n    epoch = epoch\n    for iter, input in enumerate(train_loader):\n\n        if epoch != iter // num_per_epoch + start_epoch:  # next epoch\n            epoch = iter // num_per_epoch + start_epoch\n\n            if not os.path.exists(args.save_dir):  # makedir/save model\n                os.makedirs(args.save_dir)\n\n            save_checkpoint({\n                    \'epoch\': epoch,\n                    \'arch\': args.arch,\n                    \'state_dict\': model.module.state_dict(),\n                    \'best_acc\': best_acc,\n                    \'optimizer\': optimizer.state_dict(),\n                    \'anchor_cfg\': cfg[\'anchors\']\n                }, False,\n                os.path.join(args.save_dir, \'checkpoint_e%d.pth\' % (epoch)),\n                os.path.join(args.save_dir, \'best.pth\'))\n\n            if epoch == args.epochs:\n                return\n\n            if model.module.features.unfix(epoch/args.epochs):\n                logger.info(\'unfix part model.\')\n                optimizer, lr_scheduler = build_opt_lr(model.module, cfg, args, epoch)\n\n            lr_scheduler.step(epoch)\n            cur_lr = lr_scheduler.get_cur_lr()\n\n            logger.info(\'epoch:{}\'.format(epoch))\n\n        tb_index = iter\n        if iter % num_per_epoch == 0 and iter != 0:\n            for idx, pg in enumerate(optimizer.param_groups):\n                logger.info(""epoch {} lr {}"".format(epoch, pg[\'lr\']))\n                tb_writer.add_scalar(\'lr/group%d\' % (idx+1), pg[\'lr\'], tb_index)\n\n        data_time = time.time() - end\n        avg.update(data_time=data_time)\n        x = {\n            \'cfg\': cfg,\n            \'template\': torch.autograd.Variable(input[0]).cuda(),\n            \'search\': torch.autograd.Variable(input[1]).cuda(),\n            \'label_cls\': torch.autograd.Variable(input[2]).cuda(),\n            \'label_loc\': torch.autograd.Variable(input[3]).cuda(),\n            \'label_loc_weight\': torch.autograd.Variable(input[4]).cuda(),\n            \'label_mask\': torch.autograd.Variable(input[6]).cuda(),\n            \'label_mask_weight\': torch.autograd.Variable(input[7]).cuda(),\n        }\n\n        outputs = model(x)\n\n        rpn_cls_loss, rpn_loc_loss, rpn_mask_loss = torch.mean(outputs[\'losses\'][0]), torch.mean(outputs[\'losses\'][1]), torch.mean(outputs[\'losses\'][2])\n        mask_iou_mean, mask_iou_at_5, mask_iou_at_7 = torch.mean(outputs[\'accuracy\'][0]), torch.mean(outputs[\'accuracy\'][1]), torch.mean(outputs[\'accuracy\'][2])\n\n        cls_weight, reg_weight, mask_weight = cfg[\'loss\'][\'weight\']\n\n        loss = rpn_cls_loss * cls_weight + rpn_loc_loss * reg_weight + rpn_mask_loss * mask_weight\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        if cfg[\'clip\'][\'split\']:\n            torch.nn.utils.clip_grad_norm_(model.module.features.parameters(), cfg[\'clip\'][\'feature\'])\n            torch.nn.utils.clip_grad_norm_(model.module.rpn_model.parameters(), cfg[\'clip\'][\'rpn\'])\n            torch.nn.utils.clip_grad_norm_(model.module.mask_model.parameters(), cfg[\'clip\'][\'mask\'])\n        else:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)  # gradient clip\n\n        if is_valid_number(loss.item()):\n            optimizer.step()\n\n        siammask_loss = loss.item()\n\n        batch_time = time.time() - end\n\n        avg.update(batch_time=batch_time, rpn_cls_loss=rpn_cls_loss, rpn_loc_loss=rpn_loc_loss,\n                   rpn_mask_loss=rpn_mask_loss, siammask_loss=siammask_loss,\n                   mask_iou_mean=mask_iou_mean, mask_iou_at_5=mask_iou_at_5, mask_iou_at_7=mask_iou_at_7)\n\n        tb_writer.add_scalar(\'loss/cls\', rpn_cls_loss, tb_index)\n        tb_writer.add_scalar(\'loss/loc\', rpn_loc_loss, tb_index)\n        tb_writer.add_scalar(\'loss/mask\', rpn_mask_loss, tb_index)\n        tb_writer.add_scalar(\'mask/mIoU\', mask_iou_mean, tb_index)\n        tb_writer.add_scalar(\'mask/AP@.5\', mask_iou_at_5, tb_index)\n        tb_writer.add_scalar(\'mask/AP@.7\', mask_iou_at_7, tb_index)\n        end = time.time()\n\n        if (iter + 1) % args.print_freq == 0:\n            logger.info(\'Epoch: [{0}][{1}/{2}] lr: {lr:.6f}\\t{batch_time:s}\\t{data_time:s}\'\n                        \'\\t{rpn_cls_loss:s}\\t{rpn_loc_loss:s}\\t{rpn_mask_loss:s}\\t{siammask_loss:s}\'\n                        \'\\t{mask_iou_mean:s}\\t{mask_iou_at_5:s}\\t{mask_iou_at_7:s}\'.format(\n                        epoch+1, (iter + 1) % num_per_epoch, num_per_epoch, lr=cur_lr, batch_time=avg.batch_time,\n                        data_time=avg.data_time, rpn_cls_loss=avg.rpn_cls_loss, rpn_loc_loss=avg.rpn_loc_loss,\n                        rpn_mask_loss=avg.rpn_mask_loss, siammask_loss=avg.siammask_loss, mask_iou_mean=avg.mask_iou_mean,\n                        mask_iou_at_5=avg.mask_iou_at_5,mask_iou_at_7=avg.mask_iou_at_7))\n            print_speed(iter + 1, avg.batch_time.avg, args.epochs * num_per_epoch)\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth\', best_file=\'model_best.pth\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, best_file)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train_siammask_refine.py,21,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport argparse\nimport logging\nimport os\nimport cv2\nimport shutil\nimport time\nimport json\nimport math\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom utils.log_helper import init_log, print_speed, add_file_handler, Dummy\nfrom utils.load_helper import load_pretrain, restore_from\nfrom utils.average_meter_helper import AverageMeter\n\nfrom datasets.siam_mask_dataset import DataSets\n\nfrom utils.lr_helper import build_lr_scheduler\nfrom tensorboardX import SummaryWriter\n\nfrom utils.config_helper import load_config\nfrom torch.utils.collect_env import get_pretty_env_info\n\ntorch.backends.cudnn.benchmark = True\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch Tracking Training\')\n\nparser.add_argument(\'-j\', \'--workers\', default=16, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 16)\')\nparser.add_argument(\'--epochs\', default=50, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch\', default=64, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 64)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--clip\', default=10.0, type=float,\n                    help=\'gradient clip value\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', default=\'\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--config\', dest=\'config\', required=True,\n                    help=\'hyperparameter of SiamRPN in json format\')\nparser.add_argument(\'--arch\', dest=\'arch\', default=\'\', choices=[\'Custom\',\'\'],\n                    help=\'architecture of pretrained model\')\nparser.add_argument(\'-l\', \'--log\', default=""log.txt"", type=str,\n                    help=\'log file\')\nparser.add_argument(\'-s\', \'--save_dir\', default=\'snapshot\', type=str,\n                    help=\'save dir\')\nparser.add_argument(\'--log-dir\', default=\'board\', help=\'TensorBoard log dir\')\n\n\nbest_acc = 0.\n\n\ndef collect_env_info():\n    env_str = get_pretty_env_info()\n    env_str += ""\\n        OpenCV ({})"".format(cv2.__version__)\n    return env_str\n\n\ndef build_data_loader(cfg):\n    logger = logging.getLogger(\'global\')\n\n    logger.info(""build train dataset"")  # train_dataset\n    train_set = DataSets(cfg[\'train_datasets\'], cfg[\'anchors\'], args.epochs)\n    train_set.shuffle()\n\n    logger.info(""build val dataset"")  # val_dataset\n    if not \'val_datasets\' in cfg.keys():\n        cfg[\'val_datasets\'] = cfg[\'train_datasets\']\n    val_set = DataSets(cfg[\'val_datasets\'], cfg[\'anchors\'])\n    val_set.shuffle()\n\n    train_loader = DataLoader(train_set, batch_size=args.batch, num_workers=args.workers,\n                              pin_memory=True, sampler=None)\n    val_loader = DataLoader(val_set, batch_size=args.batch, num_workers=args.workers,\n                            pin_memory=True, sampler=None)\n\n    logger.info(\'build dataset done\')\n    return train_loader, val_loader\n\n\ndef build_opt_lr(model, cfg, args, epoch):\n    trainable_params = model.mask_model.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'mask_lr_mult\']) + \\\n                       model.refine_model.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'mask_lr_mult\'])\n\n    optimizer = torch.optim.SGD(trainable_params, args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    lr_scheduler = build_lr_scheduler(optimizer, cfg[\'lr\'], epochs=args.epochs)\n\n    lr_scheduler.step(epoch)\n\n    return optimizer, lr_scheduler\n\n\ndef main():\n    global args, best_acc, tb_writer, logger\n    args = parser.parse_args()\n\n    init_log(\'global\', logging.INFO)\n\n    if args.log != """":\n        add_file_handler(\'global\', args.log, logging.INFO)\n\n    logger = logging.getLogger(\'global\')\n    logger.info(""\\n"" + collect_env_info())\n    logger.info(args)\n\n    cfg = load_config(args)\n    logger.info(""config \\n{}"".format(json.dumps(cfg, indent=4)))\n\n    if args.log_dir:\n        tb_writer = SummaryWriter(args.log_dir)\n    else:\n        tb_writer = Dummy()\n\n    # build dataset\n    train_loader, val_loader = build_data_loader(cfg)\n\n    if args.arch == \'Custom\':\n        from custom import Custom\n        model = Custom(anchors=cfg[\'anchors\'])\n    else:\n        model = models.__dict__[args.arch](anchors=cfg[\'anchors\'])\n\n    logger.info(model)\n\n    if args.pretrained:\n        model = load_pretrain(model, args.pretrained)\n\n    model = model.cuda()\n    dist_model = torch.nn.DataParallel(model, list(range(torch.cuda.device_count()))).cuda()\n\n    if args.resume and args.start_epoch != 0:\n        model.features.unfix((args.start_epoch - 1) / args.epochs)\n\n    optimizer, lr_scheduler = build_opt_lr(model, cfg, args, args.start_epoch)\n    # optionally resume from a checkpoint\n    if args.resume:\n        assert os.path.isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        model, optimizer, args.start_epoch, best_acc, arch = restore_from(model, optimizer, args.resume)\n        dist_model = torch.nn.DataParallel(model, list(range(torch.cuda.device_count()))).cuda()\n\n    logger.info(lr_scheduler)\n\n    logger.info(\'model prepare done\')\n\n    train(train_loader, dist_model, optimizer, lr_scheduler, args.start_epoch, cfg)\n\n\ndef BNtoFixed(m):\n    class_name = m.__class__.__name__\n    if class_name.find(\'BatchNorm\') != -1:\n        m.eval()\n\n\ndef train(train_loader, model, optimizer, lr_scheduler, epoch, cfg):\n    global tb_index, best_acc, cur_lr, logger\n    cur_lr = lr_scheduler.get_cur_lr()\n    logger = logging.getLogger(\'global\')\n    avg = AverageMeter()\n    model.train()\n    model.module.features.eval()\n    model.module.rpn_model.eval()\n    model.module.features.apply(BNtoFixed)\n    model.module.rpn_model.apply(BNtoFixed)\n\n    model.module.mask_model.train()\n    model.module.refine_model.train()\n    model = model.cuda()\n    end = time.time()\n\n    def is_valid_number(x):\n        return not(math.isnan(x) or math.isinf(x) or x > 1e4)\n\n    num_per_epoch = len(train_loader.dataset) // args.epochs // args.batch\n    start_epoch = epoch\n    epoch = epoch\n    for iter, input in enumerate(train_loader):\n\n        if epoch != iter // num_per_epoch + start_epoch:  # next epoch\n            epoch = iter // num_per_epoch + start_epoch\n\n            if not os.path.exists(args.save_dir):  # makedir/save model\n                os.makedirs(args.save_dir)\n\n            save_checkpoint({\n                    \'epoch\': epoch,\n                    \'arch\': args.arch,\n                    \'state_dict\': model.module.state_dict(),\n                    \'best_acc\': best_acc,\n                    \'optimizer\': optimizer.state_dict(),\n                    \'anchor_cfg\': cfg[\'anchors\']\n                }, False,\n                os.path.join(args.save_dir, \'checkpoint_e%d.pth\' % (epoch)),\n                os.path.join(args.save_dir, \'best.pth\'))\n\n            if epoch == args.epochs:\n                return\n\n            optimizer, lr_scheduler = build_opt_lr(model.module, cfg, args, epoch)\n\n            lr_scheduler.step(epoch)\n            cur_lr = lr_scheduler.get_cur_lr()\n\n            logger.info(\'epoch:{}\'.format(epoch))\n\n        tb_index = iter\n        if iter % num_per_epoch == 0 and iter != 0:\n            for idx, pg in enumerate(optimizer.param_groups):\n                logger.info(""epoch {} lr {}"".format(epoch, pg[\'lr\']))\n                tb_writer.add_scalar(\'lr/group%d\' % (idx+1), pg[\'lr\'], tb_index)\n\n        data_time = time.time() - end\n        avg.update(data_time=data_time)\n        x = {\n            \'cfg\': cfg,\n            \'template\': torch.autograd.Variable(input[0]).cuda(),\n            \'search\': torch.autograd.Variable(input[1]).cuda(),\n            \'label_cls\': torch.autograd.Variable(input[2]).cuda(),\n            \'label_loc\': torch.autograd.Variable(input[3]).cuda(),\n            \'label_loc_weight\': torch.autograd.Variable(input[4]).cuda(),\n            \'label_mask\': torch.autograd.Variable(input[6]).cuda(),\n            \'label_mask_weight\': torch.autograd.Variable(input[7]).cuda(),\n        }\n\n        outputs = model(x)\n\n        rpn_cls_loss, rpn_loc_loss, rpn_mask_loss = torch.mean(outputs[\'losses\'][0]), torch.mean(outputs[\'losses\'][1]), torch.mean(outputs[\'losses\'][2])\n        mask_iou_mean, mask_iou_at_5, mask_iou_at_7 = torch.mean(outputs[\'accuracy\'][0]), torch.mean(outputs[\'accuracy\'][1]), torch.mean(outputs[\'accuracy\'][2])\n\n        cls_weight, reg_weight, mask_weight = cfg[\'loss\'][\'weight\']\n\n        loss = rpn_cls_loss * cls_weight + rpn_loc_loss * reg_weight + rpn_mask_loss * mask_weight\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        if cfg[\'clip\'][\'split\']:\n            torch.nn.utils.clip_grad_norm_(model.module.features.parameters(), cfg[\'clip\'][\'feature\'])\n            torch.nn.utils.clip_grad_norm_(model.module.rpn_model.parameters(), cfg[\'clip\'][\'rpn\'])\n            torch.nn.utils.clip_grad_norm_(model.module.mask_model.parameters(), cfg[\'clip\'][\'mask\'])\n            torch.nn.utils.clip_grad_norm_(model.module.refine_model.parameters(), cfg[\'clip\'][\'mask\'])\n        else:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)  # gradient clip\n\n        if is_valid_number(loss.item()):\n            optimizer.step()\n\n        siammask_loss = loss.item()\n\n        batch_time = time.time() - end\n\n        avg.update(batch_time=batch_time, rpn_cls_loss=rpn_cls_loss, rpn_loc_loss=rpn_loc_loss,\n                   rpn_mask_loss=rpn_mask_loss, siammask_loss=siammask_loss,\n                   mask_iou_mean=mask_iou_mean, mask_iou_at_5=mask_iou_at_5, mask_iou_at_7=mask_iou_at_7)\n\n        tb_writer.add_scalar(\'loss/cls\', rpn_cls_loss, tb_index)\n        tb_writer.add_scalar(\'loss/loc\', rpn_loc_loss, tb_index)\n        tb_writer.add_scalar(\'loss/mask\', rpn_mask_loss, tb_index)\n        tb_writer.add_scalar(\'mask/mIoU\', mask_iou_mean, tb_index)\n        tb_writer.add_scalar(\'mask/AP@.5\', mask_iou_at_5, tb_index)\n        tb_writer.add_scalar(\'mask/AP@.7\', mask_iou_at_7, tb_index)\n        end = time.time()\n\n        if (iter + 1) % args.print_freq == 0:\n            logger.info(\'Epoch: [{0}][{1}/{2}] lr: {lr:.6f}\\t{batch_time:s}\\t{data_time:s}\'\n                        \'\\t{rpn_cls_loss:s}\\t{rpn_loc_loss:s}\\t{rpn_mask_loss:s}\\t{siammask_loss:s}\'\n                        \'\\t{mask_iou_mean:s}\\t{mask_iou_at_5:s}\\t{mask_iou_at_7:s}\'.format(\n                        epoch+1, (iter + 1) % num_per_epoch, num_per_epoch, lr=cur_lr, batch_time=avg.batch_time,\n                        data_time=avg.data_time, rpn_cls_loss=avg.rpn_cls_loss, rpn_loc_loss=avg.rpn_loc_loss,\n                        rpn_mask_loss=avg.rpn_mask_loss, siammask_loss=avg.siammask_loss, mask_iou_mean=avg.mask_iou_mean,\n                        mask_iou_at_5=avg.mask_iou_at_5,mask_iou_at_7=avg.mask_iou_at_7))\n            print_speed(iter + 1, avg.batch_time.avg, args.epochs * num_per_epoch)\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth\', best_file=\'model_best.pth\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, best_file)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train_siamrpn.py,16,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport argparse\nimport logging\nimport os\nimport shutil\nimport time\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom utils.log_helper import init_log, print_speed, add_file_handler, Dummy\nfrom utils.load_helper import load_pretrain, restore_from\nfrom utils.average_meter_helper import AverageMeter\n\nfrom datasets.siam_rpn_dataset import DataSets\nimport models as models\nimport math\n\nfrom utils.lr_helper import build_lr_scheduler\nfrom tensorboardX import SummaryWriter\n\nfrom utils.config_helper import load_config\nimport json\nimport cv2\nfrom torch.utils.collect_env import get_pretty_env_info\n\ntorch.backends.cudnn.benchmark = True\n\nmodel_zoo = sorted(name for name in models.__dict__\n            if not name.startswith(""__"")\n            and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch Tracking Training\')\n\nparser.add_argument(\'-j\', \'--workers\', default=16, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 16)\')\nparser.add_argument(\'--epochs\', default=50, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch\', default=64, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 64)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--clip\', default=10.0, type=float,\n                    help=\'gradient clip value\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', default=\'\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--config\', dest=\'config\', required=True,\n                    help=\'hyperparameter of SiamRPN in json format\')\nparser.add_argument(\'--arch\', dest=\'arch\', default=\'\', choices=model_zoo + [\'Custom\',\'\'],\n                    help=\'architecture of pretrained model\')\nparser.add_argument(\'-l\', \'--log\', default=""log.txt"", type=str,\n                    help=\'log file\')\nparser.add_argument(\'-s\', \'--save_dir\', default=\'snapshot\', type=str,\n                    help=\'save dir\')\nparser.add_argument(\'--log-dir\', default=\'board\', help=\'TensorBoard log dir\')\n\nbest_acc = 0.\n\n\ndef collect_env_info():\n    env_str = get_pretty_env_info()\n    env_str += ""\\n        OpenCV ({})"".format(cv2.__version__)\n    return env_str\n\n\ndef build_data_loader(cfg):\n    logger = logging.getLogger(\'global\')\n\n    logger.info(""build train dataset"")  # train_dataset\n    train_set = DataSets(cfg[\'train_datasets\'], cfg[\'anchors\'], args.epochs)\n    train_set.shuffle()\n\n    logger.info(""build val dataset"")  # val_dataset\n    if not \'val_datasets\' in cfg.keys():\n        cfg[\'val_datasets\'] = cfg[\'train_datasets\']\n    val_set = DataSets(cfg[\'val_datasets\'], cfg[\'anchors\'])\n    val_set.shuffle()\n\n    train_loader = DataLoader(train_set, batch_size=args.batch, num_workers=args.workers,\n                              pin_memory=True, sampler=None)\n    val_loader = DataLoader(val_set, batch_size=args.batch, num_workers=args.workers,\n                            pin_memory=True, sampler=None)\n\n    logger.info(\'build dataset done\')\n    return train_loader, val_loader\n\n\ndef build_opt_lr(model, cfg, args, epoch):\n    trainable_params = model.features.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'feature_lr_mult\']) + \\\n            model.rpn_model.param_groups(cfg[\'lr\'][\'start_lr\'], cfg[\'lr\'][\'rpn_lr_mult\'])\n\n    optimizer = torch.optim.SGD(trainable_params, args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    lr_scheduler = build_lr_scheduler(optimizer, cfg[\'lr\'], epochs=args.epochs)\n\n    lr_scheduler.step(epoch)\n\n    return optimizer, lr_scheduler\n\n\ndef main():\n    global args, best_acc, tb_writer, logger\n    args = parser.parse_args()\n\n    init_log(\'global\', logging.INFO)\n\n    if args.log != """":\n        add_file_handler(\'global\', args.log, logging.INFO)\n\n    logger = logging.getLogger(\'global\')\n    logger.info(args)\n\n    cfg = load_config(args)\n\n    logger.info(""config \\n{}"".format(json.dumps(cfg, indent=4)))\n    \n    logger.info(""\\n"" + collect_env_info())\n\n    if args.log_dir:\n        tb_writer = SummaryWriter(args.log_dir)\n    else:\n        tb_writer = Dummy()\n\n    # build dataset\n    train_loader, val_loader = build_data_loader(cfg)\n\n    if args.arch == \'Custom\':\n        from custom import Custom\n        model = Custom(pretrain=True, anchors=cfg[\'anchors\'])\n    else:\n        model = models.__dict__[args.arch](anchors=cfg[\'anchors\'])\n\n    logger.info(model)\n\n    if args.pretrained:\n        model = load_pretrain(model, args.pretrained)\n\n    model = model.cuda()\n    dist_model = torch.nn.DataParallel(model, list(range(torch.cuda.device_count()))).cuda()\n\n    if args.resume and args.start_epoch != 0:\n        model.features.unfix((args.start_epoch - 1) / args.epochs)\n\n    optimizer, lr_scheduler = build_opt_lr(model, cfg, args, args.start_epoch)\n    logger.info(lr_scheduler)\n    # optionally resume from a checkpoint\n    if args.resume:\n        assert os.path.isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        model, optimizer, args.start_epoch, best_acc, arch = restore_from(model, optimizer, args.resume)\n        dist_model = torch.nn.DataParallel(model, list(range(torch.cuda.device_count()))).cuda()\n        epoch = args.start_epoch\n        if dist_model.module.features.unfix(epoch/args.epochs):\n            logger.info(\'unfix part model.\')\n            optimizer, lr_scheduler = build_opt_lr(dist_model.module, cfg, args, epoch)\n        lr_scheduler.step(epoch)\n        cur_lr = lr_scheduler.get_cur_lr()\n        logger.info(\'epoch:{} resume lr {}\'.format(epoch, cur_lr))\n\n    logger.info(\'model prepare done\')\n\n    train(train_loader, dist_model, optimizer, lr_scheduler, args.start_epoch, cfg)\n\n\ndef train(train_loader, model, optimizer, lr_scheduler, epoch, cfg):\n    global tb_index, best_acc, cur_lr\n    cur_lr = lr_scheduler.get_cur_lr()\n    logger = logging.getLogger(\'global\')\n    avg = AverageMeter()\n    model.train()\n    model = model.cuda()\n    end = time.time()\n\n    def is_valid_number(x):\n        return not(math.isnan(x) or math.isinf(x) or x > 1e4)\n\n    num_per_epoch = len(train_loader.dataset) // args.epochs // args.batch\n    start_epoch = epoch\n    epoch = epoch\n    for iter, input in enumerate(train_loader):\n        # next epoch\n        if epoch != iter // num_per_epoch + start_epoch:\n            epoch = iter // num_per_epoch + start_epoch\n\n            if not os.path.exists(args.save_dir):  # makedir/save model\n                os.makedirs(args.save_dir)\n\n            save_checkpoint({\n                    \'epoch\': epoch,\n                    \'arch\': args.arch,\n                    \'state_dict\': model.module.state_dict(),\n                    \'best_acc\': best_acc,\n                    \'optimizer\': optimizer.state_dict(),\n                    \'anchor_cfg\': cfg[\'anchors\']\n                }, False,\n                os.path.join(args.save_dir, \'checkpoint_e%d.pth\' % (epoch)),\n                os.path.join(args.save_dir, \'best.pth\'))\n\n            if epoch == args.epochs:\n                return\n\n            if model.module.features.unfix(epoch/args.epochs):\n                logger.info(\'unfix part model.\')\n                optimizer, lr_scheduler = build_opt_lr(model.module, cfg, args, epoch)\n\n            lr_scheduler.step(epoch)\n            cur_lr = lr_scheduler.get_cur_lr()\n\n            logger.info(\'epoch:{}\'.format(epoch))\n\n        tb_index = iter\n        if iter % num_per_epoch == 0 and iter != 0:\n            for idx, pg in enumerate(optimizer.param_groups):\n                logger.info(""epoch {} lr {}"".format(epoch, pg[\'lr\']))\n                tb_writer.add_scalar(\'lr/group%d\'%(idx+1), pg[\'lr\'], tb_index)\n\n        data_time = time.time() - end\n        avg.update(data_time=data_time)\n        x = {\n            \'cfg\': cfg,\n            \'template\': torch.autograd.Variable(input[0]).cuda(),\n            \'search\': torch.autograd.Variable(input[1]).cuda(),\n            \'label_cls\': torch.autograd.Variable(input[2]).cuda(),\n            \'label_loc\': torch.autograd.Variable(input[3]).cuda(),\n            \'label_loc_weight\': torch.autograd.Variable(input[4]).cuda(),\n        }\n\n        optimizer.zero_grad()\n        outputs = model(x)\n\n        rpn_cls_loss, rpn_loc_loss = outputs[\'losses\']\n\n        rpn_cls_loss, rpn_loc_loss = torch.mean(rpn_cls_loss), torch.mean(rpn_loc_loss)\n\n        cls_weight, reg_weight = cfg[\'loss\'][\'weight\']\n\n        loss = rpn_cls_loss * cls_weight + rpn_loc_loss * reg_weight\n\n        loss.backward()\n\n        if cfg[\'clip\'][\'split\']:\n            torch.nn.utils.clip_grad_norm_(model.module.features.parameters(), cfg[\'clip\'][\'feature\'])\n            torch.nn.utils.clip_grad_norm_(model.module.rpn_model.parameters(), cfg[\'clip\'][\'rpn\'])\n        else:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)  # gradient clip\n\n        siamrpn_loss = loss.item()\n\n        if is_valid_number(siamrpn_loss):\n            optimizer.step()\n\n        batch_time = time.time() - end\n\n        avg.update(batch_time=batch_time, rpn_cls_loss=rpn_cls_loss,\n                   rpn_loc_loss=rpn_loc_loss, siamrpn_loss=siamrpn_loss)\n\n        tb_writer.add_scalar(\'loss/cls\', rpn_cls_loss, tb_index)\n        tb_writer.add_scalar(\'loss/loc\', rpn_loc_loss, tb_index)\n\n        end = time.time()\n\n        if (iter + 1) % args.print_freq == 0:\n            logger.info(\'Epoch: [{0}][{1}/{2}] lr: {lr:.6f}\\t{batch_time:s}\\t{data_time:s}\'\n                        \'\\t{rpn_cls_loss:s}\\t{rpn_loc_loss:s}\\t{siamrpn_loss:s}\'.format(\n                        epoch+1, (iter + 1) % num_per_epoch, num_per_epoch, lr=cur_lr,\n                        batch_time=avg.batch_time, data_time=avg.data_time, rpn_cls_loss=avg.rpn_cls_loss,\n                        rpn_loc_loss=avg.rpn_loc_loss, siamrpn_loss=avg.siamrpn_loss))\n            print_speed(iter + 1, avg.batch_time.avg, args.epochs * num_per_epoch)\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth\', best_file=\'model_best.pth\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, best_file)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/tune_vos.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport argparse\nimport logging\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom os import makedirs\nfrom os.path import isfile, isdir, join\n\nfrom utils.log_helper import init_log, add_file_handler\nfrom utils.bbox_helper import cxy_wh_2_rect\nfrom utils.load_helper import load_pretrain\nfrom utils.benchmark_helper import load_dataset\nfrom utils.average_meter_helper import IouMeter\n\nimport models as models\nfrom tools.test import siamese_init, siamese_track\nfrom utils.config_helper import load_config\n\nthrs = np.arange(0.3, 0.81, 0.05)\n\nmodel_zoo = sorted(name for name in models.__dict__\n            if not name.startswith(""__"")\n            and callable(models.__dict__[name]))\n\n\ndef parse_range(arg):\n    param = map(float, arg.split(\',\'))\n    return np.arange(*param)\n\n\ndef parse_range_int(arg):\n    param = map(int, arg.split(\',\'))\n    return np.arange(*param)\n\n\nparser = argparse.ArgumentParser(description=\'Finetune parameters for SiamMask tracker on DAVIS\')\nparser.add_argument(\'--arch\', dest=\'arch\', default=\'SiamRPNA\', choices=model_zoo + [\'Custom\'],\n                    help=\'architecture of pretrained model\')\nparser.add_argument(\'--resume\', default=\'\', type=str, required=True,\n                    metavar=\'PATH\',help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--config\', dest=\'config\',help=\'hyperparameter of SiamMask in json format\')\nparser.add_argument(\'--mask\', action=\'store_true\', help=\'whether use mask output\')\nparser.add_argument(\'--refine\', action=\'store_true\', help=\'whether use mask refine output\')\nparser.add_argument(\'--dataset\', default=\'DAVIS2016\', type=str,\n                    metavar=\'DATASET\', help=\'dataset\')\nparser.add_argument(\'-l\', \'--log\', default=""log_tune_davis.txt"", type=str,\n                    help=\'log file\')\nparser.add_argument(\'--penalty-k\', default=\'0.0,0.1,0.03\', type=parse_range,\n                    help=\'penalty_k range\')\nparser.add_argument(\'--lr\', default=\'0.8,1.01,0.05\', type=parse_range,\n                    help=\'lr range\')\nparser.add_argument(\'--window-influence\', default=\'0.3,0.5,0.04\', type=parse_range,\n                    help=\'window influence range\')\nparser.add_argument(\'--search-region\', default=\'255,256,8\', type=parse_range_int,\n                    help=\'search region size\')\nparser.add_argument(\'-v\', \'--visualization\', dest=\'visualization\', action=\'store_true\',\n                    help=\'whether visualize result\')\n\nargs = parser.parse_args()\n\n\ndef tune(param):\n    regions = []  # result and states[1 init / 2 lost / 0 skip]\n    # save result\n    benchmark_result_path = join(\'result\', param[\'dataset\'])\n    tracker_path = join(benchmark_result_path, (param[\'network_name\'] + (\'_refine\' if args.refine else \'\') +\n                                                \'_r{}\'.format(param[\'hp\'][\'instance_size\']) +\n                                                \'_penalty_k_{:.3f}\'.format(param[\'hp\'][\'penalty_k\']) +\n                                                \'_window_influence_{:.3f}\'.format(param[\'hp\'][\'window_influence\']) +\n                                                \'_lr_{:.3f}\'.format(param[\'hp\'][\'lr\'])).replace(\'.\', \'_\'))  # no .\n    video_path = tracker_path\n    result_path = join(video_path, param[\'video\'] + \'.txt\')\n\n    if isfile(result_path):\n        return\n\n    try:\n        if not isdir(video_path):\n            makedirs(video_path)\n    except OSError as err:\n        print(err)\n\n    with open(result_path, \'w\') as f:  # Occupation\n        f.write(\'Occ\')\n    \n    global ims, gt, annos, image_files, anno_files\n    if ims is None:\n        print(param[\'video\'] + \'  Only load image once and if needed\')\n        ims = [cv2.imread(x) for x in image_files]\n        annos = [np.array(Image.open(x)) for x in anno_files]\n\n    iou = IouMeter(thrs, len(ims) - 2)\n    start_frame, end_frame, toc = 0, len(ims) - 1, 0\n    for f, (im, anno) in enumerate(zip(ims, annos)):\n        tic = cv2.getTickCount()\n        if f == start_frame:  # init\n            target_pos = np.array([gt[f, 0]+gt[f, 2]/2, gt[f, 1]+gt[f, 3]/2])\n            target_sz = np.array([gt[f, 2], gt[f, 3]])\n            state = siamese_init(im, target_pos, target_sz, param[\'network\'], param[\'hp\'])  # init tracker\n            location = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n            regions.append(gt[f])\n        elif f > start_frame:  # tracking\n            state = siamese_track(state, im, args.mask, args.refine)  # track\n            location = state[\'ploygon\'].flatten()\n            mask = state[\'mask\']\n\n            regions.append(location)\n        if start_frame < f < end_frame: iou.add(mask, anno)\n\n        toc += cv2.getTickCount() - tic\n\n        if args.visualization and f >= start_frame:  # visualization (skip lost frame)\n            im_show = im.copy()\n            if f == 0: cv2.destroyAllWindows()\n            if len(gt[f]) == 8:\n                cv2.polylines(im_show, [np.array(gt[f], np.int).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n            else:\n                cv2.rectangle(im_show, (gt[f, 0], gt[f, 1]), (gt[f, 0] + gt[f, 2], gt[f, 1] + gt[f, 3]), (0, 255, 0), 3)\n            if len(location) == 8:\n                im_show[:,:,2] = mask*255 + (1-mask)*im_show[:,:,2]\n                cv2.polylines(im_show, [np.int0(location).reshape((-1, 1, 2))], True, (0, 255, 255), 3)\n            else:\n                location = [int(l) for l in location]  # bad support for OPENCV\n                cv2.rectangle(im_show, (location[0], location[1]),\n                              (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n            cv2.putText(im_show, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)  # frame id\n\n            cv2.imshow(param[\'video\'], im_show)\n            cv2.waitKey(1)\n    toc /= cv2.getTickFrequency()\n    iou_list = iou.value(\'mean\')\n    print(\'Video: {:12s} Time: {:2.1f}s Speed: {:3.1f}fps IOU: {:.3f}\'.format(param[\'video\'], toc, f / toc, iou_list.max()))\n\n    with open(result_path, \'w\') as f:\n        f.write(\',\'.join([""%.5f"" % i for i in iou_list]) + \'\\n\')\n\n    return iou_list\n\n\ndef main():\n    init_log(\'global\', logging.INFO)\n    if args.log != """":\n        add_file_handler(\'global\', args.log, logging.INFO)\n    \n    params = {\'penalty_k\': args.penalty_k,\n              \'window_influence\': args.window_influence,\n              \'lr\': args.lr,\n              \'instance_size\': args.search_region}\n\n    num_search = len(params[\'penalty_k\']) * len(params[\'window_influence\']) * \\\n        len(params[\'lr\']) * len(params[\'instance_size\'])\n\n    print(params)\n    print(num_search)\n\n    cfg = load_config(args)\n    if args.arch == \'Custom\':\n        from custom import Custom\n        model = Custom(anchors=cfg[\'anchors\'])\n    else:\n        model = models.__dict__[args.arch](anchors=cfg[\'anchors\'])\n\n    if args.resume:\n        assert isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        model = load_pretrain(model, args.resume)\n    model.eval()\n    model = model.cuda()\n\n    default_hp = cfg.get(\'hp\', {})\n\n    p = dict()\n\n    p[\'network\'] = model\n    p[\'network_name\'] = args.arch+\'_mask_\'+args.resume.split(\'/\')[-1].split(\'.\')[0]\n    p[\'dataset\'] = args.dataset\n\n    global ims, gt, annos, image_files, anno_files\n\n    dataset_info = load_dataset(args.dataset)\n    videos = list(dataset_info.keys())\n    np.random.shuffle(videos)\n    for video in videos:\n        print(video)\n        p[\'video\'] = video\n        ims = None\n        annos = None\n        image_files = dataset_info[video][\'image_files\']\n        anno_files = dataset_info[video][\'anno_files\']\n        gt = dataset_info[video][\'gt\']\n\n        np.random.shuffle(params[\'penalty_k\'])\n        np.random.shuffle(params[\'window_influence\'])\n        np.random.shuffle(params[\'lr\'])\n        for penalty_k in params[\'penalty_k\']:\n            for window_influence in params[\'window_influence\']:\n                for lr in params[\'lr\']:\n                    for instance_size in params[\'instance_size\']:\n                        p[\'hp\'] = default_hp.copy()\n                        p[\'hp\'].update({\'penalty_k\':penalty_k,\n                                \'window_influence\':window_influence,\n                                \'lr\':lr,\n                                \'instance_size\': instance_size,\n                                        })\n                        tune(p)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/tune_vot.py,1,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport argparse\nimport logging\nimport numpy as np\nimport cv2\nimport torch\nfrom os import makedirs\nfrom os.path import isfile, isdir, join\n\nfrom utils.log_helper import init_log, add_file_handler\nfrom utils.bbox_helper import get_axis_aligned_bbox, cxy_wh_2_rect\nfrom utils.load_helper import load_pretrain\nfrom utils.benchmark_helper import load_dataset\n\nfrom tools.test import siamese_init, siamese_track\nfrom utils.config_helper import load_config\nfrom utils.pyvotkit.region import vot_overlap, vot_float2str\n\ndef parse_range(arg):\n    param = map(float, arg.split(\',\'))\n    return np.arange(*param)\n\n\ndef parse_range_int(arg):\n    param = map(int, arg.split(\',\'))\n    return np.arange(*param)\n\n\nparser = argparse.ArgumentParser(description=\'Finetune parameters for SiamMask tracker on VOT\')\nparser.add_argument(\'--arch\', dest=\'arch\', default=\'Custom\', choices=[\'Custom\', ],\n                    help=\'architecture of pretrained model\')\nparser.add_argument(\'--resume\', default=\'\', type=str, required=True,\n                    metavar=\'PATH\',help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--config\', dest=\'config\',help=\'hyperparameter of SiamRPN in json format\')\nparser.add_argument(\'--mask\', action=\'store_true\', help=\'whether use mask output\')\nparser.add_argument(\'--refine\', action=\'store_true\', help=\'whether use mask refine output\')\nparser.add_argument(\'-v\', \'--visualization\', dest=\'visualization\', action=\'store_true\',\n                    help=\'whether visualize result\')\nparser.add_argument(\'--dataset\', default=\'VOT2018\', type=str,\n                    metavar=\'DATASET\', help=\'dataset\')\nparser.add_argument(\'-l\', \'--log\', default=""log_tune.txt"", type=str,\n                    help=\'log file\')\nparser.add_argument(\'--penalty-k\', default=\'0.05,0.5,0.05\', type=parse_range,\n                    help=\'penalty_k range\')\nparser.add_argument(\'--lr\', default=\'0.35,0.5,0.05\', type=parse_range,\n                    help=\'lr range\')\nparser.add_argument(\'--window-influence\', default=\'0.1,0.8,0.05\', type=parse_range,\n                    help=\'window influence range\')\nparser.add_argument(\'--search-region\', default=\'255,256,8\', type=parse_range_int,\n                    help=\'search region size\')\n\nargs = parser.parse_args()\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n\ndef tune(param):\n    regions = []  # result and states[1 init / 2 lost / 0 skip]\n    # save result\n    benchmark_result_path = join(\'result\', param[\'dataset\'])\n    tracker_path = join(benchmark_result_path, (param[\'network_name\'] +\n                                                \'_r{}\'.format(param[\'hp\'][\'instance_size\']) +\n                                                \'_penalty_k_{:.3f}\'.format(param[\'hp\'][\'penalty_k\']) +\n                                                \'_window_influence_{:.3f}\'.format(param[\'hp\'][\'window_influence\']) +\n                                                \'_lr_{:.3f}\'.format(param[\'hp\'][\'lr\'])).replace(\'.\', \'_\'))  # no .\n    if param[\'dataset\'].startswith(\'VOT\'):\n        baseline_path = join(tracker_path, \'baseline\')\n        video_path = join(baseline_path, param[\'video\'])\n        result_path = join(video_path, param[\'video\'] + \'_001.txt\')\n    elif param[\'dataset\'].startswith(\'OTB\') or param[\'dataset\'].startswith(\'DAVIS\'):\n        video_path = tracker_path\n        result_path = join(video_path, param[\'video\']+\'.txt\')\n\n    if isfile(result_path):\n        return\n\n    try:\n        if not isdir(video_path):\n            makedirs(video_path)\n    except OSError as err:\n        print(err)\n\n    with open(result_path, \'w\') as f:  # Occupation\n        f.write(\'Occ\')\n    \n    global ims, gt, image_files\n    if ims is None:\n        print(param[\'video\'] + \'  Only load image once and if needed\')\n        ims = [cv2.imread(x) for x in image_files]\n    start_frame, lost_times, toc = 0, 0, 0\n    for f, im in enumerate(ims):\n        tic = cv2.getTickCount()\n        if f == start_frame:  # init\n            cx, cy, w, h = get_axis_aligned_bbox(gt[f])\n            target_pos = np.array([cx, cy])\n            target_sz = np.array([w, h])\n            state = siamese_init(im, target_pos, target_sz, param[\'network\'], param[\'hp\'], device=device)  # init tracker\n            location = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n            if param[\'dataset\'].startswith(\'VOT\'):\n                regions.append(1)\n            elif param[\'dataset\'].startswith(\'OTB\') or param[\'dataset\'].startswith(\'DAVIS\'):\n                regions.append(gt[f])\n        elif f > start_frame:  # tracking\n            state = siamese_track(state, im, args.mask, args.refine, device=device)\n            if args.mask:\n                location = state[\'ploygon\'].flatten()\n            else:\n                location = cxy_wh_2_rect(state[\'target_pos\'], state[\'target_sz\'])\n            if param[\'dataset\'].startswith(\'VOT\'):\n                if \'VOT\' in args.dataset:\n                    gt_polygon = ((gt[f][0], gt[f][1]), \n                                  (gt[f][2], gt[f][3]),\n                                  (gt[f][4], gt[f][5]), \n                                  (gt[f][6], gt[f][7]))\n                    if args.mask:\n                        pred_polygon = ((location[0], location[1]), (location[2], location[3]),\n                                        (location[4], location[5]), (location[6], location[7]))\n                    else:\n                        pred_polygon = ((location[0], location[1]),\n                                        (location[0] + location[2], location[1]),\n                                        (location[0] + location[2], location[1] + location[3]),\n                                        (location[0], location[1] + location[3]))\n                    b_overlap = vot_overlap(gt_polygon, pred_polygon, (im.shape[1], im.shape[0]))\n                else:\n                    b_overlap = 1\n\n                if b_overlap:  # continue to track\n                    regions.append(location)\n                else:  # lost\n                    regions.append(2)\n                    lost_times += 1\n                    start_frame = f + 5  # skip 5 frames\n            else:\n                regions.append(location)\n        else:  # skip\n            regions.append(0)\n        toc += cv2.getTickCount() - tic\n\n        if args.visualization and f >= start_frame:  # visualization (skip lost frame)\n            if f == 0: cv2.destroyAllWindows()\n            if len(gt[f]) == 8:\n                cv2.polylines(im, [np.array(gt[f], np.int).reshape((-1, 1, 2))], True, (0, 255, 0), 3)\n            else:\n                cv2.rectangle(im, (gt[f, 0], gt[f, 1]), (gt[f, 0] + gt[f, 2], gt[f, 1] + gt[f, 3]), (0, 255, 0), 3)\n            if len(location) == 8:\n                location = np.int0(location)\n                cv2.polylines(im, [location.reshape((-1, 1, 2))], True, (0, 255, 255), 3)\n            else:\n                location = [int(l) for l in location]  # bad support for OPENCV\n                cv2.rectangle(im, (location[0], location[1]),\n                              (location[0] + location[2], location[1] + location[3]), (0, 255, 255), 3)\n            cv2.putText(im, str(f), (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)  # frame id\n            cv2.putText(im, str(lost_times), (40, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)  # lost time\n\n            cv2.imshow(param[\'video\'], im)\n            cv2.waitKey(1)\n    toc /= cv2.getTickFrequency()\n    print(\'Video: {:12s} Time: {:2.1f}s Speed: {:3.1f}fps Lost: {:d}\'.format(param[\'video\'], toc, f / toc, lost_times))\n\n    with open(result_path, \'w\') as f:\n        for x in regions:\n            f.write(\'{:d}\\n\'.format(x)) if isinstance(x, int) else \\\n                    f.write(\',\'.join([vot_float2str(""%.4f"", i) for i in x]) + \'\\n\')\n\n\ndef main():\n    init_log(\'global\', logging.INFO)\n    if args.log != """":\n        add_file_handler(\'global\', args.log, logging.INFO)\n\n    params = {\'penalty_k\': args.penalty_k,\n              \'window_influence\': args.window_influence,\n              \'lr\': args.lr,\n              \'instance_size\': args.search_region}\n\n    num_search = len(params[\'penalty_k\']) * len(params[\'window_influence\']) * \\\n        len(params[\'lr\']) * len(params[\'instance_size\'])\n\n    print(params)\n    print(num_search)\n\n    cfg = load_config(args)\n    if args.arch == \'Custom\':\n        from custom import Custom\n        model = Custom(anchors=cfg[\'anchors\'])\n    else:\n        model = models.__dict__[args.arch](anchors=cfg[\'anchors\'])\n\n    if args.resume:\n        assert isfile(args.resume), \'{} is not a valid file\'.format(args.resume)\n        model = load_pretrain(model, args.resume)\n    model.eval()\n    model = model.to(device)\n\n    default_hp = cfg.get(\'hp\', {})\n\n    p = dict()\n\n    p[\'network\'] = model\n    p[\'network_name\'] = args.arch+\'_\'+args.resume.split(\'/\')[-1].split(\'.\')[0]\n    p[\'dataset\'] = args.dataset\n\n    global ims, gt, image_files\n\n    dataset_info = load_dataset(args.dataset)\n    videos = list(dataset_info.keys())\n    np.random.shuffle(videos)\n\n    for video in videos:\n        print(video)\n        if isfile(\'finish.flag\'):\n            return\n\n        p[\'video\'] = video\n        ims = None\n        image_files = dataset_info[video][\'image_files\']\n        gt = dataset_info[video][\'gt\']\n\n        np.random.shuffle(params[\'penalty_k\'])\n        np.random.shuffle(params[\'window_influence\'])\n        np.random.shuffle(params[\'lr\'])\n        for penalty_k in params[\'penalty_k\']:\n            for window_influence in params[\'window_influence\']:\n                for lr in params[\'lr\']:\n                    for instance_size in params[\'instance_size\']:\n                        p[\'hp\'] = default_hp.copy()\n                        p[\'hp\'].update({\'penalty_k\':penalty_k,\n                                \'window_influence\':window_influence,\n                                \'lr\':lr,\n                                \'instance_size\': instance_size,\n                                })\n                        tune(p)\n\n\nif __name__ == \'__main__\':\n    main()\n    with open(\'finish.flag\', \'w\') as f:  # Occupation\n        f.write(\'finish\')\n'"
utils/__init__.py,0,b''
utils/anchors.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport numpy as np\nimport math\nfrom utils.bbox_helper import center2corner, corner2center\n\n\nclass Anchors:\n    def __init__(self, cfg):\n        self.stride = 8\n        self.ratios = [0.33, 0.5, 1, 2, 3]\n        self.scales = [8]\n        self.round_dight = 0\n        self.image_center = 0\n        self.size = 0\n        self.anchor_density = 1\n\n        self.__dict__.update(cfg)\n\n        self.anchor_num = len(self.scales) * len(self.ratios) * (self.anchor_density**2)\n        self.anchors = None  # in single position (anchor_num*4)\n        self.all_anchors = None  # in all position 2*(4*anchor_num*h*w)\n        self.generate_anchors()\n\n    def generate_anchors(self):\n        self.anchors = np.zeros((self.anchor_num, 4), dtype=np.float32)\n\n        size = self.stride * self.stride\n        count = 0\n        anchors_offset = self.stride / self.anchor_density\n        anchors_offset = np.arange(self.anchor_density)*anchors_offset\n        anchors_offset = anchors_offset - np.mean(anchors_offset)\n        x_offsets, y_offsets = np.meshgrid(anchors_offset, anchors_offset)\n\n        for x_offset, y_offset in zip(x_offsets.flatten(), y_offsets.flatten()):\n            for r in self.ratios:\n                if self.round_dight > 0:\n                    ws = round(math.sqrt(size*1. / r), self.round_dight)\n                    hs = round(ws * r, self.round_dight)\n                else:\n                    ws = int(math.sqrt(size*1. / r))\n                    hs = int(ws * r)\n\n                for s in self.scales:\n                    w = ws * s\n                    h = hs * s\n                    self.anchors[count][:] = [-w*0.5+x_offset, -h*0.5+y_offset, w*0.5+x_offset, h*0.5+y_offset][:]\n                    count += 1\n\n    def generate_all_anchors(self, im_c, size):\n        if self.image_center == im_c and self.size == size:\n            return False\n        self.image_center = im_c\n        self.size = size\n\n        a0x = im_c - size // 2 * self.stride\n        ori = np.array([a0x] * 4, dtype=np.float32)\n        zero_anchors = self.anchors + ori\n\n        x1 = zero_anchors[:, 0]\n        y1 = zero_anchors[:, 1]\n        x2 = zero_anchors[:, 2]\n        y2 = zero_anchors[:, 3]\n\n        x1, y1, x2, y2 = map(lambda x: x.reshape(self.anchor_num, 1, 1), [x1, y1, x2, y2])\n        cx, cy, w, h = corner2center([x1, y1, x2, y2])\n\n        disp_x = np.arange(0, size).reshape(1, 1, -1) * self.stride\n        disp_y = np.arange(0, size).reshape(1, -1, 1) * self.stride\n\n        cx = cx + disp_x\n        cy = cy + disp_y\n\n        # broadcast\n        zero = np.zeros((self.anchor_num, size, size), dtype=np.float32)\n        cx, cy, w, h = map(lambda x: x + zero, [cx, cy, w, h])\n        x1, y1, x2, y2 = center2corner([cx, cy, w, h])\n\n        self.all_anchors = np.stack([x1, y1, x2, y2]), np.stack([cx, cy, w, h])\n        return True\n\n\n# if __name__ == '__main__':\n#     anchors = Anchors(cfg={'stride':16, 'anchor_density': 2})\n#     anchors.generate_all_anchors(im_c=255//2, size=(255-127)//16+1+8)\n#     a = 1\n\n"""
utils/average_meter_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport numpy as np\n\n\nclass Meter(object):\n    def __init__(self, name, val, avg):\n        self.name = name\n        self.val = val\n        self.avg = avg\n\n    def __repr__(self):\n        return ""{name}: {val:.6f} ({avg:.6f})"".format(\n            name=self.name, val=self.val, avg=self.avg\n        )\n\n    def __format__(self, *tuples, **kwargs):\n        return self.__repr__()\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = {}\n        self.sum = {}\n        self.count = {}\n\n    def update(self, batch=1, **kwargs):\n        val = {}\n        for k in kwargs:\n            val[k] = kwargs[k] / float(batch)\n        self.val.update(val)\n        for k in kwargs:\n            if k not in self.sum:\n                self.sum[k] = 0\n                self.count[k] = 0\n            self.sum[k] += kwargs[k]\n            self.count[k] += batch\n\n    def __repr__(self):\n        s = \'\'\n        for k in self.sum:\n            s += self.format_str(k)\n        return s\n\n    def format_str(self, attr):\n        return ""{name}: {val:.6f} ({avg:.6f}) "".format(\n                    name=attr,\n                    val=float(self.val[attr]),\n                    avg=float(self.sum[attr]) / self.count[attr])\n\n    def __getattr__(self, attr):\n        if attr in self.__dict__:\n            return super(AverageMeter, self).__getattr__(attr)\n        if attr not in self.sum:\n            # logger.warn(""invalid key \'{}\'"".format(attr))\n            print(""invalid key \'{}\'"".format(attr))\n            return Meter(attr, 0, 0)\n        return Meter(attr, self.val[attr], self.avg(attr))\n\n    def avg(self, attr):\n        return float(self.sum[attr]) / self.count[attr]\n\n\nclass IouMeter(object):\n    def __init__(self, thrs, sz):\n        self.sz = sz\n        self.iou = np.zeros((sz, len(thrs)), dtype=np.float32)\n        self.thrs = thrs\n        self.reset()\n\n    def reset(self):\n        self.iou.fill(0.)\n        self.n = 0\n\n    def add(self, output, target):\n        if self.n >= len(self.iou):\n            return\n        target, output = target.squeeze(), output.squeeze()\n        for i, thr in enumerate(self.thrs):\n            pred = output > thr\n            mask_sum = (pred == 1).astype(np.uint8) + (target > 0).astype(np.uint8)\n            intxn = np.sum(mask_sum == 2)\n            union = np.sum(mask_sum > 0)\n            if union > 0:\n                self.iou[self.n, i] = intxn / union\n            elif union == 0 and intxn == 0:\n                self.iou[self.n, i] = 1\n        self.n += 1\n\n    def value(self, s):\n        nb = max(int(np.sum(self.iou > 0)), 1)\n        iou = self.iou[:nb]\n\n        def is_number(s):\n            try:\n                float(s)\n                return True\n            except ValueError:\n                return False\n        if s == \'mean\':\n            res = np.mean(iou, axis=0)\n        elif s == \'median\':\n            res = np.median(iou, axis=0)\n        elif is_number(s):\n            res = np.sum(iou > float(s), axis=0) / float(nb)\n        return res\n\n\nif __name__ == \'__main__\':\n    avg = AverageMeter()\n    avg.update(time=1.1, accuracy=.99)\n    avg.update(time=1.0, accuracy=.90)\n\n    print(avg)\n\n    print(avg.time)\n    print(avg.time.avg)\n    print(avg.time.val)\n    print(avg.SS)\n\n\n\n'"
utils/bbox_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport numpy as np\nfrom collections import namedtuple\n\nCorner = namedtuple(\'Corner\', \'x1 y1 x2 y2\')\nBBox = Corner\nCenter = namedtuple(\'Center\', \'x y w h\')\n\n\ndef corner2center(corner):\n    """"""\n    :param corner: Corner or np.array 4*N\n    :return: Center or 4 np.array N\n    """"""\n    if isinstance(corner, Corner):\n        x1, y1, x2, y2 = corner\n        return Center((x1 + x2) * 0.5, (y1 + y2) * 0.5, (x2 - x1), (y2 - y1))\n    else:\n        x1, y1, x2, y2 = corner[0], corner[1], corner[2], corner[3]\n        x = (x1 + x2) * 0.5\n        y = (y1 + y2) * 0.5\n        w = x2 - x1\n        h = y2 - y1\n        return x, y, w, h\n\n\ndef center2corner(center):\n    """"""\n    :param center: Center or np.array 4*N\n    :return: Corner or np.array 4*N\n    """"""\n    if isinstance(center, Center):\n        x, y, w, h = center\n        return Corner(x - w * 0.5, y - h * 0.5, x + w * 0.5, y + h * 0.5)\n    else:\n        x, y, w, h = center[0], center[1], center[2], center[3]\n        x1 = x - w * 0.5\n        y1 = y - h * 0.5\n        x2 = x + w * 0.5\n        y2 = y + h * 0.5\n        return x1, y1, x2, y2\n\n\ndef cxy_wh_2_rect(pos, sz):\n    return np.array([pos[0]-sz[0]/2, pos[1]-sz[1]/2, sz[0], sz[1]])  # 0-index\n\n\ndef get_axis_aligned_bbox(region):\n    nv = region.size\n    if nv == 8:\n        cx = np.mean(region[0::2])\n        cy = np.mean(region[1::2])\n        x1 = min(region[0::2])\n        x2 = max(region[0::2])\n        y1 = min(region[1::2])\n        y2 = max(region[1::2])\n        A1 = np.linalg.norm(region[0:2] - region[2:4]) * np.linalg.norm(region[2:4] - region[4:6])\n        A2 = (x2 - x1) * (y2 - y1)\n        s = np.sqrt(A1 / A2)\n        w = s * (x2 - x1) + 1\n        h = s * (y2 - y1) + 1\n    else:\n        x = region[0]\n        y = region[1]\n        w = region[2]\n        h = region[3]\n        cx = x+w/2\n        cy = y+h/2\n\n    return cx, cy, w, h\n\n\ndef aug_apply(bbox, param, shape, inv=False, rd=False):\n    """"""\n    apply augmentation\n    :param bbox: original bbox in image\n    :param param: augmentation param, shift/scale\n    :param shape: image shape, h, w, (c)\n    :param inv: inverse\n    :param rd: round bbox\n    :return: bbox(, param)\n        bbox: augmented bbox\n        param: real augmentation param\n    """"""\n    if not inv:\n        center = corner2center(bbox)\n        original_center = center\n\n        real_param = {}\n        if \'scale\' in param:\n            scale_x, scale_y = param[\'scale\']\n            imh, imw = shape[:2]\n            h, w = center.h, center.w\n\n            scale_x = min(scale_x, float(imw) / w)\n            scale_y = min(scale_y, float(imh) / h)\n\n            # center.w *= scale_x\n            # center.h *= scale_y\n            center = Center(center.x, center.y, center.w * scale_x, center.h * scale_y)\n\n        bbox = center2corner(center)\n\n        if \'shift\' in param:\n            tx, ty = param[\'shift\']\n            x1, y1, x2, y2 = bbox\n            imh, imw = shape[:2]\n\n            tx = max(-x1, min(imw - 1 - x2, tx))\n            ty = max(-y1, min(imh - 1 - y2, ty))\n\n            bbox = Corner(x1 + tx, y1 + ty, x2 + tx, y2 + ty)\n\n        if rd:\n            bbox = Corner(*map(round, bbox))\n\n        current_center = corner2center(bbox)\n\n        real_param[\'scale\'] = current_center.w / original_center.w, current_center.h / original_center.h\n        real_param[\'shift\'] = current_center.x - original_center.x, current_center.y - original_center.y\n\n        return bbox, real_param\n    else:\n        if \'scale\' in param:\n            scale_x, scale_y = param[\'scale\']\n        else:\n            scale_x, scale_y = 1., 1.\n\n        if \'shift\' in param:\n            tx, ty = param[\'shift\']\n        else:\n            tx, ty = 0, 0\n\n        center = corner2center(bbox)\n\n        center = Center(center.x - tx, center.y - ty, center.w / scale_x, center.h / scale_y)\n\n        return center2corner(center)\n\n\ndef IoU(rect1, rect2):\n    # overlap\n    x1, y1, x2, y2 = rect1[0], rect1[1], rect1[2], rect1[3]\n    tx1, ty1, tx2, ty2 = rect2[0], rect2[1], rect2[2], rect2[3]\n\n    xx1 = np.maximum(tx1, x1)\n    yy1 = np.maximum(ty1, y1)\n    xx2 = np.minimum(tx2, x2)\n    yy2 = np.minimum(ty2, y2)\n\n    ww = np.maximum(0, xx2 - xx1)\n    hh = np.maximum(0, yy2 - yy1)\n\n    area = (x2-x1) * (y2-y1)\n\n    target_a = (tx2-tx1) * (ty2 - ty1)\n\n    inter = ww * hh\n    overlap = inter / (area + target_a - inter)\n\n    return overlap\n'"
utils/benchmark_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join, realpath, dirname, exists, isdir\nfrom os import listdir\nimport logging\nimport glob\nimport numpy as np\nimport json\nfrom collections import OrderedDict\n\n\ndef get_dataset_zoo():\n    root = realpath(join(dirname(__file__), \'../data\'))\n    zoos = listdir(root)\n\n    def valid(x):\n        y = join(root, x)\n        if not isdir(y): return False\n\n        return exists(join(y, \'list.txt\')) \\\n               or exists(join(y, \'train\', \'meta.json\')) \\\n               or exists(join(y, \'ImageSets\', \'2016\', \'val.txt\')) \\\n               or exists(join(y, \'ImageSets\', \'2017\', \'test-dev.txt\'))\n\n    zoos = list(filter(valid, zoos))\n    return zoos\n\n\ndataset_zoo = get_dataset_zoo()\n\n\ndef load_dataset(dataset):\n    info = OrderedDict()\n    if \'VOT\' in dataset:\n        base_path = join(realpath(dirname(__file__)), \'../data\', dataset)\n        if not exists(base_path):\n            logging.error(""Please download test dataset!!!"")\n            exit()\n        list_path = join(base_path, \'list.txt\')\n        with open(list_path) as f:\n            videos = [v.strip() for v in f.readlines()]\n        for video in videos:\n            video_path = join(base_path, video)\n            image_path = join(video_path, \'*.jpg\')\n            image_files = sorted(glob.glob(image_path))\n            if len(image_files) == 0:  # VOT2018\n                image_path = join(video_path, \'color\', \'*.jpg\')\n                image_files = sorted(glob.glob(image_path))\n            gt_path = join(video_path, \'groundtruth.txt\')\n            gt = np.loadtxt(gt_path, delimiter=\',\').astype(np.float64)\n            if gt.shape[1] == 4:\n                gt = np.column_stack((gt[:, 0], gt[:, 1], gt[:, 0], gt[:, 1] + gt[:, 3]-1,\n                                      gt[:, 0] + gt[:, 2]-1, gt[:, 1] + gt[:, 3]-1, gt[:, 0] + gt[:, 2]-1, gt[:, 1]))\n            info[video] = {\'image_files\': image_files, \'gt\': gt, \'name\': video}\n    elif \'DAVIS\' in dataset and \'TEST\' not in dataset:\n        base_path = join(realpath(dirname(__file__)), \'../data\', \'DAVIS\')\n        list_path = join(realpath(dirname(__file__)), \'../data\', \'DAVIS\', \'ImageSets\', dataset[-4:], \'val.txt\')\n        with open(list_path) as f:\n            videos = [v.strip() for v in f.readlines()]\n        for video in videos:\n            info[video] = {}\n            info[video][\'anno_files\'] = sorted(glob.glob(join(base_path, \'Annotations/480p\', video, \'*.png\')))\n            info[video][\'image_files\'] = sorted(glob.glob(join(base_path, \'JPEGImages/480p\', video, \'*.jpg\')))\n            info[video][\'name\'] = video\n    elif \'ytb_vos\' in dataset:\n        base_path = join(realpath(dirname(__file__)), \'../data\', \'ytb_vos\', \'valid\')\n        json_path = join(realpath(dirname(__file__)), \'../data\', \'ytb_vos\', \'valid\', \'meta.json\')\n        meta = json.load(open(json_path, \'r\'))\n        meta = meta[\'videos\']\n        info = dict()\n        for v in meta.keys():\n            objects = meta[v][\'objects\']\n            frames = []\n            anno_frames = []\n            info[v] = dict()\n            for obj in objects:\n                frames += objects[obj][\'frames\']\n                anno_frames += [objects[obj][\'frames\'][0]]\n            frames = sorted(np.unique(frames))\n            info[v][\'anno_files\'] = [join(base_path, \'Annotations\', v, im_f+\'.png\') for im_f in frames]\n            info[v][\'anno_init_files\'] = [join(base_path, \'Annotations\', v, im_f + \'.png\') for im_f in anno_frames]\n            info[v][\'image_files\'] = [join(base_path, \'JPEGImages\', v, im_f+\'.jpg\') for im_f in frames]\n            info[v][\'name\'] = v\n\n            info[v][\'start_frame\'] = dict()\n            info[v][\'end_frame\'] = dict()\n            for obj in objects:\n                start_file = objects[obj][\'frames\'][0]\n                end_file = objects[obj][\'frames\'][-1]\n                info[v][\'start_frame\'][obj] = frames.index(start_file)\n                info[v][\'end_frame\'][obj] = frames.index(end_file)\n    elif \'TEST\' in dataset:\n        base_path = join(realpath(dirname(__file__)), \'../data\', \'DAVIS2017TEST\')\n        list_path = join(realpath(dirname(__file__)), \'../data\', \'DAVIS2017TEST\', \'ImageSets\', \'2017\', \'test-dev.txt\')\n        with open(list_path) as f:\n            videos = [v.strip() for v in f.readlines()]\n        for video in videos:\n            info[video] = {}\n            info[video][\'anno_files\'] = sorted(glob.glob(join(base_path, \'Annotations/480p\', video, \'*.png\')))\n            info[video][\'image_files\'] = sorted(glob.glob(join(base_path, \'JPEGImages/480p\', video, \'*.jpg\')))\n            info[video][\'name\'] = video\n    else:\n        logging.error(\'Not support\')\n        exit()\n    return info\n'"
utils/config_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport json\nfrom os.path import exists\n\n\ndef proccess_loss(cfg):\n    if \'reg\' not in cfg:\n        cfg[\'reg\'] = {\'loss\': \'L1Loss\'}\n    else:\n        if \'loss\' not in cfg[\'reg\']:\n            cfg[\'reg\'][\'loss\'] = \'L1Loss\'\n\n    if \'cls\' not in cfg:\n        cfg[\'cls\'] = {\'split\': True}\n\n    cfg[\'weight\'] = cfg.get(\'weight\', [1, 1, 36])  # cls, reg, mask\n\n\ndef add_default(conf, default):\n    default.update(conf)\n    return default\n\n\ndef load_config(args):\n    assert exists(args.config), \'""{}"" not exists\'.format(args.config)\n    config = json.load(open(args.config))\n\n    # deal with network\n    if \'network\' not in config:\n        print(\'Warning: network lost in config. This will be error in next version\')\n\n        config[\'network\'] = {}\n\n        if not args.arch:\n            raise Exception(\'no arch provided\')\n    args.arch = config[\'network\'][\'arch\']\n\n    # deal with loss\n    if \'loss\' not in config:\n        config[\'loss\'] = {}\n\n    proccess_loss(config[\'loss\'])\n\n    # deal with lr\n    if \'lr\' not in config:\n        config[\'lr\'] = {}\n    default = {\n            \'feature_lr_mult\': 1.0,\n            \'rpn_lr_mult\': 1.0,\n            \'mask_lr_mult\': 1.0,\n            \'type\': \'log\',\n            \'start_lr\': 0.03\n            }\n    default.update(config[\'lr\'])\n    config[\'lr\'] = default\n\n    # clip\n    if \'clip\' in config or \'clip\' in args.__dict__:\n        if \'clip\' not in config:\n            config[\'clip\'] = {}\n        config[\'clip\'] = add_default(config[\'clip\'],\n                {\'feature\': args.clip, \'rpn\': args.clip, \'split\': False})\n        if config[\'clip\'][\'feature\'] != config[\'clip\'][\'rpn\']:\n            config[\'clip\'][\'split\'] = True\n        if not config[\'clip\'][\'split\']:\n            args.clip = config[\'clip\'][\'feature\']\n\n    return config\n\n'"
utils/load_helper.py,6,"b'import torch\nimport logging\nlogger = logging.getLogger(\'global\')\n\n\ndef check_keys(model, pretrained_state_dict):\n    ckpt_keys = set(pretrained_state_dict.keys())\n    model_keys = set(model.state_dict().keys())\n    used_pretrained_keys = model_keys & ckpt_keys\n    unused_pretrained_keys = ckpt_keys - model_keys\n    missing_keys = model_keys - ckpt_keys\n    if len(missing_keys) > 0:\n        logger.info(\'[Warning] missing keys: {}\'.format(missing_keys))\n        logger.info(\'missing keys:{}\'.format(len(missing_keys)))\n    if len(unused_pretrained_keys) > 0:\n        logger.info(\'[Warning] unused_pretrained_keys: {}\'.format(unused_pretrained_keys))\n        logger.info(\'unused checkpoint keys:{}\'.format(len(unused_pretrained_keys)))\n    logger.info(\'used keys:{}\'.format(len(used_pretrained_keys)))\n    assert len(used_pretrained_keys) > 0, \'load NONE from pretrained checkpoint\'\n    return True\n\n\ndef remove_prefix(state_dict, prefix):\n    \'\'\' Old style model is stored with all names of parameters share common prefix \'module.\' \'\'\'\n    logger.info(\'remove prefix \\\'{}\\\'\'.format(prefix))\n    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n    return {f(key): value for key, value in state_dict.items()}\n\n\ndef load_pretrain(model, pretrained_path):\n    logger.info(\'load pretrained model from {}\'.format(pretrained_path))\n    if not torch.cuda.is_available():\n        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n    else:\n        device = torch.cuda.current_device()\n        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n\n    if ""state_dict"" in pretrained_dict.keys():\n        pretrained_dict = remove_prefix(pretrained_dict[\'state_dict\'], \'module.\')\n    else:\n        pretrained_dict = remove_prefix(pretrained_dict, \'module.\')\n\n    try:\n        check_keys(model, pretrained_dict)\n    except:\n        logger.info(\'[Warning]: using pretrain as features. Adding ""features."" as prefix\')\n        new_dict = {}\n        for k, v in pretrained_dict.items():\n            k = \'features.\' + k\n            new_dict[k] = v\n        pretrained_dict = new_dict\n        check_keys(model, pretrained_dict)\n    model.load_state_dict(pretrained_dict, strict=False)\n    return model\n\n\ndef restore_from(model, optimizer, ckpt_path):\n    logger.info(\'restore from {}\'.format(ckpt_path))\n    device = torch.cuda.current_device()\n    ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage.cuda(device))\n    epoch = ckpt[\'epoch\']\n    best_acc = ckpt[\'best_acc\']\n    arch = ckpt[\'arch\']\n    ckpt_model_dict = remove_prefix(ckpt[\'state_dict\'], \'module.\')\n    check_keys(model, ckpt_model_dict)\n    model.load_state_dict(ckpt_model_dict, strict=False)\n\n    check_keys(optimizer, ckpt[\'optimizer\'])\n    optimizer.load_state_dict(ckpt[\'optimizer\'])\n    return model, optimizer, epoch, best_acc, arch\n'"
utils/log_helper.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\n\nimport os\nimport logging\nimport sys\nimport math\n\nif hasattr(sys, \'frozen\'):  # support for py2exe\n    _srcfile = ""logging%s__init__%s"" % (os.sep, __file__[-4:])\nelif __file__[-4:].lower() in [\'.pyc\', \'.pyo\']:\n    _srcfile = __file__[:-4] + \'.py\'\nelse:\n    _srcfile = __file__\n_srcfile = os.path.normcase(_srcfile)\n\n\nlogs = set()\n\n\nclass Filter:\n    def __init__(self, flag):\n        self.flag = flag\n\n    def filter(self, x): return self.flag\n\n\nclass Dummy:\n    def __init__(self, *arg, **kwargs):\n        pass\n\n    def __getattr__(self, arg):\n        def dummy(*args, **kwargs): pass\n        return dummy\n\n\ndef get_format(logger, level):\n    if \'SLURM_PROCID\' in os.environ:\n        rank = int(os.environ[\'SLURM_PROCID\'])\n\n        if level == logging.INFO:\n            logger.addFilter(Filter(rank == 0))\n    else:\n        rank = 0\n    format_str = \'[%(asctime)s-rk{}-%(filename)s#%(lineno)3d] %(message)s\'.format(rank)\n    formatter = logging.Formatter(format_str)\n    return formatter\n\n\ndef get_format_custom(logger, level):\n    if \'SLURM_PROCID\' in os.environ:\n        rank = int(os.environ[\'SLURM_PROCID\'])\n        if level == logging.INFO:\n            logger.addFilter(Filter(rank == 0))\n    else:\n        rank = 0\n    format_str = \'[%(asctime)s-rk{}-%(message)s\'.format(rank)\n    formatter = logging.Formatter(format_str)\n    return formatter\n\n\ndef init_log(name, level = logging.INFO, format_func=get_format):\n    if (name, level) in logs: return\n    logs.add((name, level))\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    formatter = format_func(logger, level)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    return logger\n\n\ndef add_file_handler(name, log_file, level = logging.INFO):\n    logger = logging.getLogger(name)\n    fh = logging.FileHandler(log_file)\n    fh.setFormatter(get_format(logger, level))\n    logger.addHandler(fh)\n\n\ninit_log(\'global\')\n\n\ndef print_speed(i, i_time, n):\n    """"""print_speed(index, index_time, total_iteration)""""""\n    logger = logging.getLogger(\'global\')\n    average_time = i_time\n    remaining_time = (n - i) * average_time\n    remaining_day = math.floor(remaining_time / 86400)\n    remaining_hour = math.floor(remaining_time / 3600 - remaining_day * 24)\n    remaining_min = math.floor(remaining_time / 60 - remaining_day * 1440 - remaining_hour * 60)\n    logger.info(\'Progress: %d / %d [%d%%], Speed: %.3f s/iter, ETA %d:%02d:%02d (D:H:M)\\n\' % (i, n, i/n*100, average_time, remaining_day, remaining_hour, remaining_min))\n\n\ndef find_caller():\n    def current_frame():\n        try:\n            raise Exception\n        except:\n            return sys.exc_info()[2].tb_frame.f_back\n\n    f = current_frame()\n    if f is not None:\n        f = f.f_back\n    rv = ""(unknown file)"", 0, ""(unknown function)""\n    while hasattr(f, ""f_code""):\n        co = f.f_code\n        filename = os.path.normcase(co.co_filename)\n        rv = (co.co_filename, f.f_lineno, co.co_name)\n        if filename == _srcfile:\n            f = f.f_back\n            continue\n        break\n    rv = list(rv)\n    rv[0] = os.path.basename(rv[0])\n    return rv\n\n\nclass LogOnce:\n    def __init__(self):\n        self.logged = set()\n        self.logger = init_log(\'log_once\', format_func=get_format_custom)\n\n    def log(self, strings):\n        fn, lineno, caller = find_caller()\n        key = (fn, lineno, caller, strings)\n        if key in self.logged:\n            return\n        self.logged.add(key)\n        message = ""{filename:s}<{caller}>#{lineno:3d}] {strings}"".format(filename=fn, lineno=lineno, strings=strings, caller=caller)\n        self.logger.info(message)\n\n\nonce_logger = LogOnce()\n\n\ndef log_once(strings):\n    once_logger.log(strings)\n'"
utils/lr_helper.py,3,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\nimport numpy as np\nimport math\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass LRScheduler(_LRScheduler):\n    def __init__(self, optimizer, last_epoch=-1):\n        if \'lr_spaces\' not in self.__dict__:\n            raise Exception(\'lr_spaces must be set in ""LRSchduler""\')\n        super(LRScheduler, self).__init__(optimizer, last_epoch)\n\n    def get_cur_lr(self):\n        return self.lr_spaces[self.last_epoch]\n\n    def get_lr(self):\n        epoch = self.last_epoch\n        return [self.lr_spaces[epoch] * pg[\'initial_lr\'] / self.start_lr for pg in self.optimizer.param_groups]\n\n    def __repr__(self):\n        return ""({}) lr spaces: \\n{}"".format(self.__class__.__name__, self.lr_spaces)\n\n\nclass LogScheduler(LRScheduler):\n    def __init__(self, optimizer, start_lr=0.03, end_lr=5e-4, epochs=50, last_epoch=-1, **kwargs):\n        self.start_lr = start_lr\n        self.end_lr = end_lr\n        self.epochs = epochs\n        self.lr_spaces = np.logspace(math.log10(start_lr), math.log10(end_lr), epochs)\n\n        super(LogScheduler, self).__init__(optimizer, last_epoch)\n\n\nclass StepScheduler(LRScheduler):\n    def __init__(self, optimizer, start_lr=0.01, end_lr=None, step=10, mult=0.1, epochs=50, last_epoch=-1, **kwargs):\n        if end_lr is not None:\n            if start_lr is None:\n                start_lr = end_lr / (mult ** (epochs // step))\n            else:  # for warm up policy\n                mult = math.pow(end_lr/start_lr, 1. / (epochs // step))\n        self.start_lr = start_lr\n        self.lr_spaces = self.start_lr * (mult**(np.arange(epochs) // step))\n        self.mult = mult\n        self._step = step\n\n        super(StepScheduler, self).__init__(optimizer, last_epoch)\n\n\nclass MultiStepScheduler(LRScheduler):\n    def __init__(self, optimizer, start_lr=0.01, end_lr=None, steps=[10,20,30,40], mult=0.5, epochs=50, last_epoch=-1, **kwargs):\n        if end_lr is not None:\n            if start_lr is None:\n                start_lr = end_lr / (mult ** (len(steps)))\n            else:\n                mult = math.pow(end_lr/start_lr, 1. / len(steps))\n        self.start_lr = start_lr\n        self.lr_spaces = self._build_lr(start_lr, steps, mult, epochs)\n        self.mult = mult\n        self.steps = steps\n\n        super(MultiStepScheduler, self).__init__(optimizer, last_epoch)\n\n    def _build_lr(self, start_lr, steps, mult, epochs):\n        lr = [0] * epochs\n        lr[0] = start_lr\n        for i in range(1, epochs):\n            lr[i] = lr[i-1]\n            if i in steps:\n                lr[i] *= mult\n        return np.array(lr, dtype=np.float32)\n\n\nclass LinearStepScheduler(LRScheduler):\n    def __init__(self, optimizer, start_lr=0.01, end_lr=0.005, epochs=50, last_epoch=-1, **kwargs):\n        self.start_lr = start_lr\n        self.end_lr = end_lr\n        self.lr_spaces = np.linspace(start_lr, end_lr, epochs)\n\n        super(LinearStepScheduler, self).__init__(optimizer, last_epoch)\n\n\nclass CosStepScheduler(LRScheduler):\n    def __init__(self, optimizer, start_lr=0.01, end_lr=0.005, epochs=50, last_epoch=-1, **kwargs):\n        self.start_lr = start_lr\n        self.end_lr = end_lr\n        self.lr_spaces = self._build_lr(start_lr, end_lr, epochs)\n\n        super(CosStepScheduler, self).__init__(optimizer, last_epoch)\n\n    def _build_lr(self, start_lr, end_lr, epochs):\n        index = np.arange(epochs).astype(np.float32)\n        lr = end_lr + (start_lr - end_lr) * (1. + np.cos(index * np.pi/ epochs)) * 0.5\n        return lr.astype(np.float32)\n\n\nclass WarmUPScheduler(LRScheduler):\n    def __init__(self, optimizer, warmup, normal, epochs=50, last_epoch=-1):\n        warmup = warmup.lr_spaces # [::-1]\n        normal = normal.lr_spaces\n        self.lr_spaces = np.concatenate([warmup, normal])\n        self.start_lr = normal[0]\n\n        super(WarmUPScheduler, self).__init__(optimizer, last_epoch)\n\n\nLRs = {\n    \'log\': LogScheduler,\n    \'step\': StepScheduler,\n    \'multi-step\': MultiStepScheduler,\n    \'linear\': LinearStepScheduler,\n    \'cos\': CosStepScheduler}\n\n\ndef _build_lr_scheduler(optimizer, cfg, epochs=50, last_epoch=-1):\n    if \'type\' not in cfg:\n        # return LogScheduler(optimizer, last_epoch=last_epoch, epochs=epochs)\n        cfg[\'type\'] = \'log\'\n\n    if cfg[\'type\'] not in LRs:\n        raise Exception(\'Unknown type of LR Scheduler ""%s""\'%cfg[\'type\'])\n\n    return LRs[cfg[\'type\']](optimizer, last_epoch=last_epoch, epochs=epochs, **cfg)\n\n\ndef _build_warm_up_scheduler(optimizer, cfg, epochs=50, last_epoch=-1):\n    warmup_epoch = cfg[\'warmup\'][\'epoch\']\n    sc1 = _build_lr_scheduler(optimizer, cfg[\'warmup\'], warmup_epoch, last_epoch)\n    sc2 = _build_lr_scheduler(optimizer, cfg, epochs - warmup_epoch, last_epoch)\n    return WarmUPScheduler(optimizer, sc1, sc2, epochs, last_epoch)\n\n\ndef build_lr_scheduler(optimizer, cfg, epochs=50, last_epoch=-1):\n    if \'warmup\' in cfg:\n        return _build_warm_up_scheduler(optimizer, cfg, epochs, last_epoch)\n    else:\n        return _build_lr_scheduler(optimizer, cfg, epochs, last_epoch)\n\n\nif __name__ == \'__main__\':\n    import torch.nn as nn\n    from torch.optim import SGD\n\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv = nn.Conv2d(10, 10, kernel_size=3)\n    net = Net().parameters()\n    optimizer = SGD(net, lr=0.01)\n\n    # test1\n    step = {\n            \'type\': \'step\',\n            \'start_lr\': 0.01,\n            \'step\': 10,\n            \'mult\': 0.1\n            }\n    lr = build_lr_scheduler(optimizer, step)\n    print(lr)\n\n    log = {\n            \'type\': \'log\',\n            \'start_lr\': 0.03,\n            \'end_lr\': 5e-4,\n            }\n    lr = build_lr_scheduler(optimizer, log)\n\n    print(lr)\n\n    log = {\n            \'type\': \'multi-step\',\n            ""start_lr"": 0.01,\n            ""mult"": 0.1,\n            ""steps"": [10, 15, 20]\n            }\n    lr = build_lr_scheduler(optimizer, log)\n    print(lr)\n\n    cos = {\n            ""type"": \'cos\',\n            \'start_lr\': 0.01,\n            \'end_lr\': 0.0005,\n            }\n    lr = build_lr_scheduler(optimizer, cos)\n    print(lr)\n\n    step = {\n            \'type\': \'step\',\n            \'start_lr\': 0.001,\n            \'end_lr\': 0.03,\n            \'step\': 1,\n            }\n\n    warmup = log.copy()\n    warmup[\'warmup\'] = step\n    warmup[\'warmup\'][\'epoch\'] = 5\n    lr = build_lr_scheduler(optimizer, warmup, epochs=55)\n    print(lr)\n\n    lr.step()\n    print(lr.last_epoch)\n\n    lr.step(5)\n    print(lr.last_epoch)\n\n\n'"
utils/tracker_config.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import division\nfrom utils.anchors import Anchors\n\n\nclass TrackerConfig(object):\n    # These are the default hyper-params for SiamMask\n    penalty_k = 0.09\n    window_influence = 0.39\n    lr = 0.38\n    seg_thr = 0.3  # for mask\n    windowing = 'cosine'  # to penalize large displacements [cosine/uniform]\n    # Params from the network architecture, have to be consistent with the training\n    exemplar_size = 127  # input z size\n    instance_size = 255  # input x size (search region)\n    total_stride = 8\n    out_size = 63  # for mask\n    base_size = 8\n    score_size = (instance_size-exemplar_size)//total_stride+1+base_size\n    context_amount = 0.5  # context amount for the exemplar\n    ratios = [0.33, 0.5, 1, 2, 3]\n    scales = [8, ]\n    anchor_num = len(ratios) * len(scales)\n    round_dight = 0\n    anchor = []\n\n    def update(self, newparam=None, anchors=None):\n        if newparam:\n            for key, value in newparam.items():\n                setattr(self, key, value)\n        if anchors is not None:\n            if isinstance(anchors, dict):\n                anchors = Anchors(anchors)\n            if isinstance(anchors, Anchors):\n                self.total_stride = anchors.stride\n                self.ratios = anchors.ratios\n                self.scales = anchors.scales\n                self.round_dight = anchors.round_dight\n        self.renew()\n\n    def renew(self):\n        self.score_size = (self.instance_size - self.exemplar_size) // self.total_stride + 1 + self.base_size\n        self.anchor_num = len(self.ratios) * len(self.scales)\n\n\n\n\n"""
data/coco/gen_json.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom pycocotools.coco import COCO\nfrom os.path import join\nimport json\n\n\ndataDir = '.'\nfor data_subset in ['val2017', 'train2017']:\n    dataset = dict()\n    annFile = '{}/annotations/instances_{}.json'.format(dataDir, data_subset)\n    coco = COCO(annFile)\n    n_imgs = len(coco.imgs)\n    for n, img_id in enumerate(coco.imgs):\n        print('subset: {} image id: {:04d} / {:04d}'.format(data_subset, n, n_imgs))\n        img = coco.loadImgs(img_id)[0]\n        annIds = coco.getAnnIds(imgIds=img['id'], iscrowd=None)\n        anns = coco.loadAnns(annIds)\n        crop_base_path = join(data_subset, img['file_name'].split('/')[-1].split('.')[0])\n        \n        if len(anns) > 0:\n            dataset[crop_base_path] = dict()\n\n        for track_id, ann in enumerate(anns):\n            rect = ann['bbox']\n            if rect[2] <= 0 or rect[3] <= 0:  # lead nan error in cls.\n                continue\n            bbox = [rect[0], rect[1], rect[0]+rect[2]-1, rect[1]+rect[3]-1]  # x1,y1,x2,y2\n\n            dataset[crop_base_path]['{:02d}'.format(track_id)] = {'000000': bbox}\n\n    print('save json (dataset), please wait 20 seconds~')\n    json.dump(dataset, open('{}.json'.format(data_subset), 'w'), indent=4, sort_keys=True)\n    print('done!')\n\n"""
data/coco/par_crop.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom pycocotools.coco import COCO\nimport cv2\nimport numpy as np\nfrom os.path import join, isdir\nfrom os import mkdir, makedirs\nfrom concurrent import futures\nimport sys\nimport time\nimport argparse\n\nparser = argparse.ArgumentParser(description=\'COCO Parallel Preprocessing for SiamMask\')\nparser.add_argument(\'--exemplar_size\', type=int, default=127, help=\'size of exemplar\')\nparser.add_argument(\'--context_amount\', type=float, default=0.5, help=\'context amount\')\nparser.add_argument(\'--search_size\', type=int, default=511, help=\'size of cropped search region\')\nparser.add_argument(\'--enable_mask\', action=\'store_true\', help=\'whether crop mask\')\nparser.add_argument(\'--num_threads\', type=int, default=24, help=\'number of threads\')\nargs = parser.parse_args()\n\n\n# Print iterations progress (thanks StackOverflow)\ndef printProgress(iteration, total, prefix=\'\', suffix=\'\', decimals=1, barLength=100):\n    """"""\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        barLength   - Optional  : character length of bar (Int)\n    """"""\n    formatStr       = ""{0:."" + str(decimals) + ""f}""\n    percents        = formatStr.format(100 * (iteration / float(total)))\n    filledLength    = int(round(barLength * iteration / float(total)))\n    bar             = \'\' * filledLength + \'-\' * (barLength - filledLength)\n    sys.stdout.write(\'\\r%s |%s| %s%s %s\' % (prefix, bar, percents, \'%\', suffix)),\n    if iteration == total:\n        sys.stdout.write(\'\\x1b[2K\\r\')\n    sys.stdout.flush()\n\n\ndef crop_hwc(image, bbox, out_sz, padding=(0, 0, 0)):\n    a = (out_sz-1) / (bbox[2]-bbox[0])\n    b = (out_sz-1) / (bbox[3]-bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz),\n                          borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return crop\n\n\ndef pos_s_2_bbox(pos, s):\n    return [pos[0]-s/2, pos[1]-s/2, pos[0]+s/2, pos[1]+s/2]\n\n\ndef crop_like_SiamFCx(image, bbox, exemplar_size=127, context_amount=0.5, search_size=255, padding=(0, 0, 0)):\n    target_pos = [(bbox[2]+bbox[0])/2., (bbox[3]+bbox[1])/2.]\n    target_size = [bbox[2]-bbox[0]+1, bbox[3]-bbox[1]+1]\n    wc_z = target_size[1] + context_amount * sum(target_size)\n    hc_z = target_size[0] + context_amount * sum(target_size)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = exemplar_size / s_z\n    d_search = (search_size - exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    x = crop_hwc(image, pos_s_2_bbox(target_pos, s_x), search_size, padding)\n    return x\n\n\ndef crop_img(img, anns, set_crop_base_path, set_img_base_path,\n             exemplar_size=127, context_amount=0.5, search_size=511, enable_mask=True):\n    frame_crop_base_path = join(set_crop_base_path, img[\'file_name\'].split(\'/\')[-1].split(\'.\')[0])\n    if not isdir(frame_crop_base_path): makedirs(frame_crop_base_path)\n\n    im = cv2.imread(\'{}/{}\'.format(set_img_base_path, img[\'file_name\']))\n    avg_chans = np.mean(im, axis=(0, 1))\n    for track_id, ann in enumerate(anns):\n        rect = ann[\'bbox\']\n        if rect[2] <= 0 or rect[3] <= 0:\n            continue\n        bbox = [rect[0], rect[1], rect[0]+rect[2]-1, rect[1]+rect[3]-1]\n\n        x = crop_like_SiamFCx(im, bbox, exemplar_size=exemplar_size, context_amount=context_amount,\n                              search_size=search_size, padding=avg_chans)\n        cv2.imwrite(join(frame_crop_base_path, \'{:06d}.{:02d}.x.jpg\'.format(0, track_id)), x)\n\n        if enable_mask:\n            im_mask = coco.annToMask(ann).astype(np.float32)\n            x = (crop_like_SiamFCx(im_mask, bbox, exemplar_size=exemplar_size, context_amount=context_amount,\n                                   search_size=search_size) > 0.5).astype(np.uint8) * 255\n            cv2.imwrite(join(frame_crop_base_path, \'{:06d}.{:02d}.m.png\'.format(0, track_id)), x)\n\n\ndef main(exemplar_size=127, context_amount=0.5, search_size=511, enable_mask=True, num_threads=24):\n    global coco  # will used for generate mask\n    data_dir = \'.\'\n    crop_path = \'./crop{:d}\'.format(search_size)\n    if not isdir(crop_path): mkdir(crop_path)\n\n    for data_subset in [\'val2017\', \'train2017\']:\n        set_crop_base_path = join(crop_path, data_subset)\n        set_img_base_path = join(data_dir, data_subset)\n\n        anno_file = \'{}/annotations/instances_{}.json\'.format(data_dir, data_subset)\n        coco = COCO(anno_file)\n        n_imgs = len(coco.imgs)\n        with futures.ProcessPoolExecutor(max_workers=num_threads) as executor:\n            fs = [executor.submit(crop_img, coco.loadImgs(id)[0],\n                                  coco.loadAnns(coco.getAnnIds(imgIds=id, iscrowd=None)),\n                                  set_crop_base_path, set_img_base_path,\n                                  exemplar_size, context_amount, search_size,\n                                  enable_mask) for id in coco.imgs]\n            for i, f in enumerate(futures.as_completed(fs)):\n                printProgress(i, n_imgs, prefix=data_subset, suffix=\'Done \', barLength=40)\n    print(\'done\')\n\n\nif __name__ == \'__main__\':\n    since = time.time()\n    main(args.exemplar_size, args.context_amount, args.search_size, args.enable_mask, args.num_threads)\n    time_elapsed = time.time() - since\n    print(\'Total complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n'"
data/coco/visual.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom pycocotools.coco import COCO\nimport cv2\nimport numpy as np\n\ncolor_bar = np.random.randint(0, 255, (90, 3))\n\nvisual = True\n\ndataDir = '.'\ndataType = 'val2017'\nannFile = '{}/annotations/instances_{}.json'.format(dataDir,dataType)\ncoco = COCO(annFile)\n\nfor img_id in coco.imgs:\n    img = coco.loadImgs(img_id)[0]\n    annIds = coco.getAnnIds(imgIds=img['id'], iscrowd=None)\n    anns = coco.loadAnns(annIds)\n    im = cv2.imread('{}/{}/{}'.format(dataDir, dataType, img['file_name']))\n    for ann in anns:\n        rect = ann['bbox']\n        c = ann['category_id']\n        if visual:\n            pt1 = (int(rect[0]), int(rect[1]))\n            pt2 = (int(rect[0]+rect[2]-1), int(rect[1]+rect[3]-1))\n            cv2.rectangle(im, pt1, pt2, color_bar[c-1], 3)\n    cv2.imshow('img', im)\n    cv2.waitKey(200)\nprint('done')\n\n"""
data/det/gen_json.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join, isdir\nfrom os import mkdir\nimport glob\nimport xml.etree.ElementTree as ET\nimport json\n\njs = {}\nVID_base_path = './ILSVRC2015'\nann_base_path = join(VID_base_path, 'Annotations/DET/train/')\nsub_sets = ('ILSVRC2013_train', 'ILSVRC2013_train_extra0', 'ILSVRC2013_train_extra1', 'ILSVRC2013_train_extra2', 'ILSVRC2013_train_extra3', 'ILSVRC2013_train_extra4', 'ILSVRC2013_train_extra5', 'ILSVRC2013_train_extra6', 'ILSVRC2013_train_extra7', 'ILSVRC2013_train_extra8', 'ILSVRC2013_train_extra9', 'ILSVRC2013_train_extra10', 'ILSVRC2014_train_0000', 'ILSVRC2014_train_0001','ILSVRC2014_train_0002','ILSVRC2014_train_0003','ILSVRC2014_train_0004','ILSVRC2014_train_0005','ILSVRC2014_train_0006')\nfor sub_set in sub_sets:\n    sub_set_base_path = join(ann_base_path, sub_set)\n\n    if 'ILSVRC2013_train' == sub_set:\n        xmls = sorted(glob.glob(join(sub_set_base_path, '*', '*.xml')))\n    else:\n        xmls = sorted(glob.glob(join(sub_set_base_path, '*.xml')))\n    n_imgs = len(xmls)\n    for f, xml in enumerate(xmls):\n        print('subset: {} frame id: {:08d} / {:08d}'.format(sub_set, f, n_imgs))\n        xmltree = ET.parse(xml)\n        objects = xmltree.findall('object')\n\n        video = join(sub_set, xml.split('/')[-1].split('.')[0])\n\n        for id, object_iter in enumerate(objects):\n            bndbox = object_iter.find('bndbox')\n            bbox = [int(bndbox.find('xmin').text), int(bndbox.find('ymin').text),\n                    int(bndbox.find('xmax').text), int(bndbox.find('ymax').text)]\n            frame = '%06d' % (0)\n            obj = '%02d' % (id)\n            if video not in js:\n                js[video] = {}\n            if obj not in js[video]:\n                js[video][obj] = {}\n            js[video][obj][frame] = bbox\n\njson.dump(js, open('train.json', 'w'), indent=4, sort_keys=True)\n"""
data/det/par_crop.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join, isdir\nfrom os import mkdir, makedirs\nimport cv2\nimport numpy as np\nimport glob\nimport xml.etree.ElementTree as ET\nfrom concurrent import futures\nimport time\nimport sys\n\n\n# Print iterations progress (thanks StackOverflow)\ndef printProgress(iteration, total, prefix=\'\', suffix=\'\', decimals=1, barLength=100):\n    """"""\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        barLength   - Optional  : character length of bar (Int)\n    """"""\n    formatStr       = ""{0:."" + str(decimals) + ""f}""\n    percents        = formatStr.format(100 * (iteration / float(total)))\n    filledLength    = int(round(barLength * iteration / float(total)))\n    bar             = \'\' * filledLength + \'-\' * (barLength - filledLength)\n    sys.stdout.write(\'\\r%s |%s| %s%s %s\' % (prefix, bar, percents, \'%\', suffix)),\n    if iteration == total:\n        sys.stdout.write(\'\\x1b[2K\\r\')\n    sys.stdout.flush()\n\n\ndef crop_hwc(image, bbox, out_sz, padding=(0, 0, 0)):\n    a = (out_sz - 1) / (bbox[2] - bbox[0])\n    b = (out_sz - 1) / (bbox[3] - bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz), borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return crop\n\n\ndef pos_s_2_bbox(pos, s):\n    return [pos[0] - s / 2, pos[1] - s / 2, pos[0] + s / 2, pos[1] + s / 2]\n\n\ndef crop_like_SiamFC(image, bbox, context_amount=0.5, exemplar_size=127, instanc_size=255, padding=(0, 0, 0)):\n    target_pos = [(bbox[2] + bbox[0]) / 2., (bbox[3] + bbox[1]) / 2.]\n    target_size = [bbox[2] - bbox[0], bbox[3] - bbox[1]]\n    wc_z = target_size[1] + context_amount * sum(target_size)\n    hc_z = target_size[0] + context_amount * sum(target_size)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = exemplar_size / s_z\n    d_search = (instanc_size - exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    z = crop_hwc(image, pos_s_2_bbox(target_pos, s_z), exemplar_size, padding)\n    x = crop_hwc(image, pos_s_2_bbox(target_pos, s_x), instanc_size, padding)\n    return z, x\n\n\ndef crop_like_SiamFCx(image, bbox, context_amount=0.5, exemplar_size=127, instanc_size=255, padding=(0, 0, 0)):\n    target_pos = [(bbox[2] + bbox[0]) / 2., (bbox[3] + bbox[1]) / 2.]\n    target_size = [bbox[2] - bbox[0], bbox[3] - bbox[1]]\n    wc_z = target_size[1] + context_amount * sum(target_size)\n    hc_z = target_size[0] + context_amount * sum(target_size)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = exemplar_size / s_z\n    d_search = (instanc_size - exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    x = crop_hwc(image, pos_s_2_bbox(target_pos, s_x), instanc_size, padding)\n    return x\n\n\ndef crop_xml(xml, sub_set_crop_path, instanc_size=511):\n    xmltree = ET.parse(xml)\n    objects = xmltree.findall(\'object\')\n\n    frame_crop_base_path = join(sub_set_crop_path, xml.split(\'/\')[-1].split(\'.\')[0])\n    if not isdir(frame_crop_base_path): makedirs(frame_crop_base_path)\n\n    img_path = xml.replace(\'xml\', \'JPEG\').replace(\'Annotations\', \'Data\')\n\n    im = cv2.imread(img_path)\n    avg_chans = np.mean(im, axis=(0, 1))\n\n    for id, object_iter in enumerate(objects):\n        bndbox = object_iter.find(\'bndbox\')\n        bbox = [int(bndbox.find(\'xmin\').text), int(bndbox.find(\'ymin\').text),\n                int(bndbox.find(\'xmax\').text), int(bndbox.find(\'ymax\').text)]\n\n        # z, x = crop_like_SiamFC(im, bbox, instanc_size=instanc_size, padding=avg_chans)\n        # x = crop_like_SiamFCx(im, bbox, instanc_size=instanc_size, padding=avg_chans)\n        # cv2.imwrite(join(frame_crop_base_path, \'{:06d}.{:02d}.z.jpg\'.format(0, id)), z)\n        x = crop_like_SiamFCx(im, bbox, instanc_size=instanc_size, padding=avg_chans)\n        cv2.imwrite(join(frame_crop_base_path, \'{:06d}.{:02d}.x.jpg\'.format(0, id)), x)\n\n\ndef main(instanc_size=511, num_threads=24):\n    crop_path = \'./crop{:d}\'.format(instanc_size)\n    if not isdir(crop_path): mkdir(crop_path)\n    VID_base_path = \'./ILSVRC2015\'\n    ann_base_path = join(VID_base_path, \'Annotations/DET/train/\')\n    sub_sets = (\'ILSVRC2013_train\', \'ILSVRC2013_train_extra0\', \'ILSVRC2013_train_extra1\', \'ILSVRC2013_train_extra2\', \'ILSVRC2013_train_extra3\', \'ILSVRC2013_train_extra4\', \'ILSVRC2013_train_extra5\', \'ILSVRC2013_train_extra6\', \'ILSVRC2013_train_extra7\', \'ILSVRC2013_train_extra8\', \'ILSVRC2013_train_extra9\', \'ILSVRC2013_train_extra10\', \'ILSVRC2014_train_0000\', \'ILSVRC2014_train_0001\',\'ILSVRC2014_train_0002\',\'ILSVRC2014_train_0003\',\'ILSVRC2014_train_0004\',\'ILSVRC2014_train_0005\',\'ILSVRC2014_train_0006\')\n    for sub_set in sub_sets:\n        sub_set_base_path = join(ann_base_path, sub_set)\n        if \'ILSVRC2013_train\' == sub_set:\n            xmls = sorted(glob.glob(join(sub_set_base_path, \'*\', \'*.xml\')))\n        else:\n            xmls = sorted(glob.glob(join(sub_set_base_path, \'*.xml\')))\n\n        n_imgs = len(xmls)\n        sub_set_crop_path = join(crop_path, sub_set)\n        with futures.ProcessPoolExecutor(max_workers=num_threads) as executor:\n            fs = [executor.submit(crop_xml, xml, sub_set_crop_path, instanc_size) for xml in xmls]\n            for i, f in enumerate(futures.as_completed(fs)):\n                printProgress(i, n_imgs, prefix=sub_set, suffix=\'Done \', barLength=80)\n\n\nif __name__ == \'__main__\':\n    since = time.time()\n    main(int(sys.argv[1]), int(sys.argv[2]))\n    time_elapsed = time.time() - since\n    print(\'Total complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n'"
data/det/visual.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join\nfrom os import listdir\nimport cv2\nimport numpy as np\nimport glob\nimport xml.etree.ElementTree as ET\n\nvisual = False\ncolor_bar = np.random.randint(0, 255, (90, 3))\n\nVID_base_path = './ILSVRC2015'\nann_base_path = join(VID_base_path, 'Annotations/DET/train/')\nimg_base_path = join(VID_base_path, 'Data/DET/train/')\nsub_sets = sorted({'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'})\nfor sub_set in sub_sets:\n    sub_set_base_path = join(ann_base_path, sub_set)\n    class_names = sorted(listdir(sub_set_base_path))\n    for vi, class_name in enumerate(class_names):\n        print('subset: {} video id: {:04d} / {:04d}'.format(sub_set, vi, len(class_names)))\n\n        class_base_path = join(sub_set_base_path, class_name)\n        xmls = sorted(glob.glob(join(class_base_path, '*.xml')))\n        for xml in xmls:\n            f = dict()\n            xmltree = ET.parse(xml)\n            size = xmltree.findall('size')[0]\n            frame_sz = [int(it.text) for it in size]\n            objects = xmltree.findall('object')\n            # if visual:\n            img_path = xml.replace('xml', 'JPEG').replace('Annotations', 'Data')\n            im = cv2.imread(img_path)\n            for object_iter in objects:\n                bndbox = object_iter.find('bndbox')\n                bbox = [int(bndbox.find('xmin').text), int(bndbox.find('ymin').text),\n                        int(bndbox.find('xmax').text), int(bndbox.find('ymax').text)]\n                if visual:\n                    pt1 = (int(bbox[0]), int(bbox[1]))\n                    pt2 = (int(bbox[2]), int(bbox[3]))\n                    cv2.rectangle(im, pt1, pt2, color_bar[vi], 3)\n            if visual:\n                cv2.imshow('img', im)\n                cv2.waitKey(500)\n\nprint('done!')\n"""
data/vid/gen_json.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join\nfrom os import listdir\nimport json\nimport numpy as np\n\nprint('load json (raw vid info), please wait 20 seconds~')\nvid = json.load(open('vid.json', 'r'))\n\n\ndef check_size(frame_sz, bbox):\n    min_ratio = 0.1\n    max_ratio = 0.75\n    # only accept objects >10% and <75% of the total frame\n    area_ratio = np.sqrt((bbox[2]-bbox[0])*(bbox[3]-bbox[1])/float(np.prod(frame_sz)))\n    ok = (area_ratio > min_ratio) and (area_ratio < max_ratio)\n    return ok\n\n\ndef check_borders(frame_sz, bbox):\n    dist_from_border = 0.05 * (bbox[2] - bbox[0] + bbox[3] - bbox[1])/2\n    ok = (bbox[0] > dist_from_border) and (bbox[1] > dist_from_border) and \\\n         ((frame_sz[0] - bbox[2]) > dist_from_border) and \\\n         ((frame_sz[1] - bbox[3]) > dist_from_border)\n    return ok\n\n\nsnippets = dict()\nn_snippets = 0\nn_videos = 0\nfor subset in vid:\n    for video in subset:\n        n_videos += 1\n        frames = video['frame']\n        id_set = []\n        id_frames = [[]] * 60  # at most 60 objects\n        for f, frame in enumerate(frames):\n            objs = frame['objs']\n            frame_sz = frame['frame_sz']\n            for obj in objs:\n                trackid = obj['trackid']\n                occluded = obj['occ']\n                bbox = obj['bbox']\n                # if occluded:\n                #     continue\n                #\n                # if not(check_size(frame_sz, bbox) and check_borders(frame_sz, bbox)):\n                #     continue\n                #\n                # if obj['c'] in ['n01674464', 'n01726692', 'n04468005', 'n02062744']:\n                #     continue\n\n                if trackid not in id_set:\n                    id_set.append(trackid)\n                    id_frames[trackid] = []\n                id_frames[trackid].append(f)\n        if len(id_set) > 0:\n            snippets[video['base_path']] = dict()\n        for selected in id_set:\n            frame_ids = sorted(id_frames[selected])\n            sequences = np.split(frame_ids, np.array(np.where(np.diff(frame_ids) > 1)[0]) + 1)\n            sequences = [s for s in sequences if len(s) > 1]  # remove isolated frame.\n            for seq in sequences:\n                snippet = dict()\n                for frame_id in seq:\n                    frame = frames[frame_id]\n                    for obj in frame['objs']:\n                        if obj['trackid'] == selected:\n                            o = obj\n                            continue\n                    snippet[frame['img_path'].split('.')[0]] = o['bbox']\n                snippets[video['base_path']]['{:02d}'.format(selected)] = snippet\n                n_snippets += 1\n        print('video: {:d} snippets_num: {:d}'.format(n_videos, n_snippets))\n        \ntrain = {k:v for (k,v) in snippets.items() if 'train' in k}\nval = {k:v for (k,v) in snippets.items() if 'val' in k}\n\njson.dump(train, open('train.json', 'w'), indent=4, sort_keys=True)\njson.dump(val, open('val.json', 'w'), indent=4, sort_keys=True)\nprint('done!')\n"""
data/vid/par_crop.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join, isdir\nfrom os import listdir, mkdir, makedirs\nimport cv2\nimport numpy as np\nimport glob\nimport xml.etree.ElementTree as ET\nfrom concurrent import futures\nimport sys\nimport time\n\nVID_base_path = \'./ILSVRC2015\'\nann_base_path = join(VID_base_path, \'Annotations/VID/train/\')\nsub_sets= sorted({\'ILSVRC2015_VID_train_0000\', \'ILSVRC2015_VID_train_0001\', \'ILSVRC2015_VID_train_0002\', \'ILSVRC2015_VID_train_0003\', \'val\'})\n# Print iterations progress (thanks StackOverflow)\ndef printProgress(iteration, total, prefix=\'\', suffix=\'\', decimals=1, barLength=100):\n    """"""\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        barLength   - Optional  : character length of bar (Int)\n    """"""\n    formatStr       = ""{0:."" + str(decimals) + ""f}""\n    percents        = formatStr.format(100 * (iteration / float(total)))\n    filledLength    = int(round(barLength * iteration / float(total)))\n    bar             = \'\' * filledLength + \'-\' * (barLength - filledLength)\n    sys.stdout.write(\'\\r%s |%s| %s%s %s\' % (prefix, bar, percents, \'%\', suffix)),\n    if iteration == total:\n        sys.stdout.write(\'\\x1b[2K\\r\')\n    sys.stdout.flush()\n\n\ndef crop_hwc(image, bbox, out_sz, padding=(0, 0, 0)):\n    a = (out_sz-1) / (bbox[2]-bbox[0])\n    b = (out_sz-1) / (bbox[3]-bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz), borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return crop\n\n\ndef pos_s_2_bbox(pos, s):\n    return [pos[0]-s/2, pos[1]-s/2, pos[0]+s/2, pos[1]+s/2]\n\n\ndef crop_like_SiamFC(image, bbox, context_amount=0.5, exemplar_size=127, instanc_size=255, padding=(0, 0, 0)):\n    target_pos = [(bbox[2]+bbox[0])/2., (bbox[3]+bbox[1])/2.]\n    target_size = [bbox[2]-bbox[0], bbox[3]-bbox[1]]\n    wc_z = target_size[1] + context_amount * sum(target_size)\n    hc_z = target_size[0] + context_amount * sum(target_size)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = exemplar_size / s_z\n    d_search = (instanc_size - exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    z = crop_hwc(image, pos_s_2_bbox(target_pos, s_z), exemplar_size, padding)\n    x = crop_hwc(image, pos_s_2_bbox(target_pos, s_x), instanc_size, padding)\n    return z, x\n\n\ndef crop_like_SiamFCx(image, bbox, context_amount=0.5, exemplar_size=127, instanc_size=255, padding=(0, 0, 0)):\n    target_pos = [(bbox[2]+bbox[0])/2., (bbox[3]+bbox[1])/2.]\n    target_size = [bbox[2]-bbox[0], bbox[3]-bbox[1]]\n    wc_z = target_size[1] + context_amount * sum(target_size)\n    hc_z = target_size[0] + context_amount * sum(target_size)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = exemplar_size / s_z\n    d_search = (instanc_size - exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    x = crop_hwc(image, pos_s_2_bbox(target_pos, s_x), instanc_size, padding)\n    return x\n\n\ndef crop_video(sub_set, video, crop_path, instanc_size):\n    video_crop_base_path = join(crop_path, sub_set, video)\n    if not isdir(video_crop_base_path): makedirs(video_crop_base_path)\n\n    sub_set_base_path = join(ann_base_path, sub_set)\n    xmls = sorted(glob.glob(join(sub_set_base_path, video, \'*.xml\')))\n    for xml in xmls:\n        xmltree = ET.parse(xml)\n        # size = xmltree.findall(\'size\')[0]\n        # frame_sz = [int(it.text) for it in size]\n        objects = xmltree.findall(\'object\')\n        objs = []\n        filename = xmltree.findall(\'filename\')[0].text\n\n        im = cv2.imread(xml.replace(\'xml\', \'JPEG\').replace(\'Annotations\', \'Data\'))\n        avg_chans = np.mean(im, axis=(0, 1))\n        for object_iter in objects:\n            trackid = int(object_iter.find(\'trackid\').text)\n            # name = (object_iter.find(\'name\')).text\n            bndbox = object_iter.find(\'bndbox\')\n            # occluded = int(object_iter.find(\'occluded\').text)\n\n            bbox = [int(bndbox.find(\'xmin\').text), int(bndbox.find(\'ymin\').text),\n                    int(bndbox.find(\'xmax\').text), int(bndbox.find(\'ymax\').text)]\n            # z, x = crop_like_SiamFC(im, bbox, instanc_size=instanc_size, padding=avg_chans)\n            # cv2.imwrite(join(video_crop_base_path, \'{:06d}.{:02d}.z.jpg\'.format(int(filename), trackid)), z)\n            # cv2.imwrite(join(video_crop_base_path, \'{:06d}.{:02d}.x.jpg\'.format(int(filename), trackid)), x)\n\n            x = crop_like_SiamFCx(im, bbox, instanc_size=instanc_size, padding=avg_chans)\n            cv2.imwrite(join(video_crop_base_path, \'{:06d}.{:02d}.x.jpg\'.format(int(filename), trackid)), x)\n\n\ndef main(instanc_size=511, num_threads=24):\n    crop_path = \'./crop{:d}\'.format(instanc_size)\n    if not isdir(crop_path): mkdir(crop_path)\n\n    for sub_set in sub_sets:\n        sub_set_base_path = join(ann_base_path, sub_set)\n        videos = sorted(listdir(sub_set_base_path))\n        n_videos = len(videos)\n        with futures.ProcessPoolExecutor(max_workers=num_threads) as executor:\n            fs = [executor.submit(crop_video, sub_set, video, crop_path, instanc_size) for video in videos]\n            for i, f in enumerate(futures.as_completed(fs)):\n                # Write progress to error so that it can be seen\n                printProgress(i, n_videos, prefix=sub_set, suffix=\'Done \', barLength=40)\n\n\nif __name__ == \'__main__\':\n    since = time.time()\n    main(int(sys.argv[1]), int(sys.argv[2]))\n    time_elapsed = time.time() - since\n    print(\'Total complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n'"
data/vid/parse_vid.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join\nfrom os import listdir\nimport json\nimport glob\nimport xml.etree.ElementTree as ET\n\nVID_base_path = './ILSVRC2015'\nann_base_path = join(VID_base_path, 'Annotations/VID/train/')\nimg_base_path = join(VID_base_path, 'Data/VID/train/')\nsub_sets = sorted({'ILSVRC2015_VID_train_0000', 'ILSVRC2015_VID_train_0001', 'ILSVRC2015_VID_train_0002', 'ILSVRC2015_VID_train_0003', 'val'})\n\nvid = []\nfor sub_set in sub_sets:\n    sub_set_base_path = join(ann_base_path, sub_set)\n    videos = sorted(listdir(sub_set_base_path))\n    s = []\n    for vi, video in enumerate(videos):\n        print('subset: {} video id: {:04d} / {:04d}'.format(sub_set, vi, len(videos)))\n        v = dict()\n        v['base_path'] = join(sub_set, video)\n        v['frame'] = []\n        video_base_path = join(sub_set_base_path, video)\n        xmls = sorted(glob.glob(join(video_base_path, '*.xml')))\n        for xml in xmls:\n            f = dict()\n            xmltree = ET.parse(xml)\n            size = xmltree.findall('size')[0]\n            frame_sz = [int(it.text) for it in size]\n            objects = xmltree.findall('object')\n            objs = []\n            for object_iter in objects:\n                trackid = int(object_iter.find('trackid').text)\n                name = (object_iter.find('name')).text\n                bndbox = object_iter.find('bndbox')\n                occluded = int(object_iter.find('occluded').text)\n                o = dict()\n                o['c'] = name\n                o['bbox'] = [int(bndbox.find('xmin').text), int(bndbox.find('ymin').text),\n                             int(bndbox.find('xmax').text), int(bndbox.find('ymax').text)]\n                o['trackid'] = trackid\n                o['occ'] = occluded\n                objs.append(o)\n            f['frame_sz'] = frame_sz\n            f['img_path'] = xml.split('/')[-1].replace('xml', 'JPEG')\n            f['objs'] = objs\n            v['frame'].append(f)\n        s.append(v)\n    vid.append(s)\nprint('save json (raw vid info), please wait 1 min~')\njson.dump(vid, open('vid.json', 'w'), indent=4, sort_keys=True)\nprint('done!')\n"""
data/vid/visual.py,0,"b""# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom os.path import join\nfrom os import listdir\nimport cv2\nimport numpy as np\nimport glob\nimport xml.etree.ElementTree as ET\n\nvisual = False\ncolor_bar = np.random.randint(0, 255, (90, 3))\n\nVID_base_path = './ILSVRC2015'\nann_base_path = join(VID_base_path, 'Annotations/VID/train/')\nimg_base_path = join(VID_base_path, 'Data/VID/train/')\nsub_sets = sorted({'a', 'b', 'c', 'd', 'e'})\nfor sub_set in sub_sets:\n    sub_set_base_path = join(ann_base_path, sub_set)\n    videos = sorted(listdir(sub_set_base_path))\n    for vi, video in enumerate(videos):\n        print('subset: {} video id: {:04d} / {:04d}'.format(sub_set, vi, len(videos)))\n\n        video_base_path = join(sub_set_base_path, video)\n        xmls = sorted(glob.glob(join(video_base_path, '*.xml')))\n        for xml in xmls:\n            f = dict()\n            xmltree = ET.parse(xml)\n            size = xmltree.findall('size')[0]\n            frame_sz = [int(it.text) for it in size]\n            objects = xmltree.findall('object')\n            if visual:\n                im = cv2.imread(xml.replace('xml', 'JPEG').replace('Annotations', 'Data'))\n            for object_iter in objects:\n                trackid = int(object_iter.find('trackid').text)\n                bndbox = object_iter.find('bndbox')\n                bbox = [int(bndbox.find('xmin').text), int(bndbox.find('ymin').text),\n                        int(bndbox.find('xmax').text), int(bndbox.find('ymax').text)]\n                if visual:\n                    pt1 = (int(bbox[0]), int(bbox[1]))\n                    pt2 = (int(bbox[2]), int(bbox[3]))\n                    cv2.rectangle(im, pt1, pt2, color_bar[trackid], 3)\n            if visual:\n                cv2.imshow('img', im)\n                cv2.waitKey(1)\n\nprint('done!')\n"""
data/ytb_vos/download_from_gdrive.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport os.path as osp\nimport re\nimport shutil\nimport sys\nimport tempfile\n\nimport requests\nimport six\nimport tqdm\n\n\n# BORROWED FROM GDOWN\n\n\n\nCHUNK_SIZE = 512 * 1024  # 512KB\n\n\ndef get_url_from_gdrive_confirmation(contents):\n    url = \'\'\n    for line in contents.splitlines():\n        m = re.search(\'href=""(\\/uc\\?export=download[^""]+)\', line)\n        if m:\n            url = \'https://docs.google.com\' + m.groups()[0]\n            url = url.replace(\'&amp;\', \'&\')\n            return url\n        m = re.search(\'confirm=([^;&]+)\', line)\n        if m:\n            confirm = m.groups()[0]\n            url = re.sub(r\'confirm=([^;&]+)\', r\'confirm=\'+confirm, url)\n            return url\n        m = re.search(\'""downloadUrl"":""([^""]+)\', line)\n        if m:\n            url = m.groups()[0]\n            url = url.replace(\'\\\\u003d\', \'=\')\n            url = url.replace(\'\\\\u0026\', \'&\')\n            return url\n\n\ndef is_google_drive_url(url):\n    m = re.match(\'^https?://drive.google.com/uc\\?id=.*$\', url)\n    return m is not None\n\n\ndef download(url, output, quiet):\n    url_origin = url\n    sess = requests.session()\n\n    is_gdrive = is_google_drive_url(url)\n\n    while True:\n        res = sess.get(url, stream=True)\n        if \'Content-Disposition\' in res.headers:\n            # This is the file\n            break\n        if not is_gdrive:\n            break\n\n        # Need to redirect with confiramtion\n        url = get_url_from_gdrive_confirmation(res.text)\n\n        if url is None:\n            print(\'Permission denied: %s\' % url_origin, file=sys.stderr)\n            print(""Maybe you need to change permission over ""\n                  ""\'Anyone with the link\'?"", file=sys.stderr)\n            return\n\n    if output is None:\n        if is_gdrive:\n            m = re.search(\'filename=""(.*)""\',\n                          res.headers[\'Content-Disposition\'])\n            output = m.groups()[0]\n        else:\n            output = osp.basename(url)\n\n    output_is_path = isinstance(output, six.string_types)\n\n    if not quiet:\n        print(\'Downloading...\', file=sys.stderr)\n        print(\'From:\', url_origin, file=sys.stderr)\n        print(\'To:\', osp.abspath(output) if output_is_path else output,\n              file=sys.stderr)\n\n    if output_is_path:\n        tmp_file = tempfile.mktemp(\n            suffix=tempfile.template,\n            prefix=osp.basename(output),\n            dir=osp.dirname(output),\n        )\n        f = open(tmp_file, \'wb\')\n    else:\n        tmp_file = None\n        f = output\n\n    try:\n        total = res.headers.get(\'Content-Length\')\n        if total is not None:\n            total = int(total)\n        if not quiet:\n            pbar = tqdm.tqdm(total=total, unit=\'B\', unit_scale=True)\n        for chunk in res.iter_content(chunk_size=CHUNK_SIZE):\n            f.write(chunk)\n            if not quiet:\n                pbar.update(len(chunk))\n        if not quiet:\n            pbar.close()\n        if tmp_file:\n            f.close()\n            shutil.copy(tmp_file, output)\n    except IOError as e:\n        print(e, file=sys.stderr)\n        return\n    finally:\n        try:\n            if tmp_file:\n                os.remove(tmp_file)\n        except OSError:\n            pass\n\n    return output\n\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\n        \'url_or_id\', help=\'url or file id (with --id) to download file from\')\n    parser.add_argument(\'-O\', \'--output\', help=\'output filename\')\n    parser.add_argument(\'-q\', \'--quiet\', action=\'store_true\',\n                        help=\'suppress standard output\')\n    parser.add_argument(\'--id\', action=\'store_true\',\n                        help=\'flag to specify file id instead of url\')\n    args = parser.parse_args()\n\n    print(args)\n    if args.output == \'-\':\n        if six.PY3:\n            args.output = sys.stdout.buffer\n        else:\n            args.output = sys.stdout\n\n    download(args.url_or_id, args.output, args.quiet)\n\nif __name__ == \'__main__\':\n    main()\n'"
data/ytb_vos/gen_json.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport json\n\nprint(\'load json (raw ytb_vos info), please wait 10 seconds~\')\nytb_vos = json.load(open(\'instances_train.json\', \'r\'))\nsnippets = dict()\nfor k, v in ytb_vos.items():\n    video = dict()\n    for i, o in enumerate(list(v)):\n        obj = v[o]\n        snippet = dict()\n        trackid = ""{:02d}"".format(i)\n        for frame in obj:\n            file_name = frame[\'file_name\']\n            frame_name = \'{:06d}\'.format(int(file_name.split(\'/\')[-1]))\n            bbox = frame[\'bbox\']\n            bbox[2] += bbox[0]\n            bbox[3] += bbox[1]\n            snippet[frame_name] = bbox\n        video[trackid] = snippet\n    snippets[\'train/\'+k] = video\n        \ntrain = snippets\n\njson.dump(train, open(\'train.json\', \'w\'), indent=4, sort_keys=True)\nprint(\'done!\')\n'"
data/ytb_vos/par_crop.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nimport cv2\nimport numpy as np\nfrom os.path import join, isdir\nfrom os import mkdir, makedirs\nfrom concurrent import futures\nimport sys\nimport time\nimport json\nimport glob\n\n\n# Print iterations progress (thanks StackOverflow)\ndef printProgress(iteration, total, prefix=\'\', suffix=\'\', decimals=1, barLength=100):\n    """"""\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        barLength   - Optional  : character length of bar (Int)\n    """"""\n    formatStr       = ""{0:."" + str(decimals) + ""f}""\n    percents        = formatStr.format(100 * (iteration / float(total)))\n    filledLength    = int(round(barLength * iteration / float(total)))\n    bar             = \'\' * filledLength + \'-\' * (barLength - filledLength)\n    sys.stdout.write(\'\\r%s |%s| %s%s %s\' % (prefix, bar, percents, \'%\', suffix)),\n    if iteration == total:\n        sys.stdout.write(\'\\x1b[2K\\r\')\n    sys.stdout.flush()\n\n\ndef crop_hwc(image, bbox, out_sz, padding=(0, 0, 0)):\n    a = (out_sz-1) / (bbox[2]-bbox[0])\n    b = (out_sz-1) / (bbox[3]-bbox[1])\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz), borderMode=cv2.BORDER_CONSTANT, borderValue=padding)\n    return crop\n\n\ndef pos_s_2_bbox(pos, s):\n    return [pos[0]-s/2, pos[1]-s/2, pos[0]+s/2, pos[1]+s/2]\n\n\ndef crop_like_SiamFC(image, bbox, context_amount=0.5, exemplar_size=127, instanc_size=255, padding=(0, 0, 0)):\n    target_pos = [(bbox[2]+bbox[0])/2., (bbox[3]+bbox[1])/2.]\n    target_size = [bbox[2]-bbox[0], bbox[3]-bbox[1]]\n    wc_z = target_size[1] + context_amount * sum(target_size)\n    hc_z = target_size[0] + context_amount * sum(target_size)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = exemplar_size / s_z\n    d_search = (instanc_size - exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    z = crop_hwc(image, pos_s_2_bbox(target_pos, s_z), exemplar_size, padding)\n    x = crop_hwc(image, pos_s_2_bbox(target_pos, s_x), instanc_size, padding)\n    return z, x\n\n\ndef crop_like_SiamFCx(image, bbox, context_amount=0.5, exemplar_size=127, instanc_size=255, padding=(0, 0, 0)):\n    target_pos = [(bbox[2]+bbox[0])/2., (bbox[3]+bbox[1])/2.]\n    target_size = [bbox[2]-bbox[0], bbox[3]-bbox[1]]\n    wc_z = target_size[1] + context_amount * sum(target_size)\n    hc_z = target_size[0] + context_amount * sum(target_size)\n    s_z = np.sqrt(wc_z * hc_z)\n    scale_z = exemplar_size / s_z\n    d_search = (instanc_size - exemplar_size) / 2\n    pad = d_search / scale_z\n    s_x = s_z + 2 * pad\n\n    x = crop_hwc(image, pos_s_2_bbox(target_pos, s_x), instanc_size, padding)\n    return x\n\n\ndef crop_video(video, v, crop_path, data_path, instanc_size):\n    video_crop_base_path = join(crop_path, video)\n    if not isdir(video_crop_base_path): makedirs(video_crop_base_path)\n\n    anno_base_path = join(data_path, \'Annotations\')\n    img_base_path = join(data_path, \'JPEGImages\')\n\n    for trackid, o in enumerate(list(v)):\n        obj = v[o]\n        for frame in obj:\n            file_name = frame[\'file_name\']\n            ann_path = join(anno_base_path, file_name+\'.png\')\n            img_path = join(img_base_path, file_name+\'.jpg\')\n            im = cv2.imread(img_path)\n            label = cv2.imread(ann_path, 0)\n            avg_chans = np.mean(im, axis=(0, 1))\n            bbox = frame[\'bbox\']\n            bbox[2] += bbox[0]\n            bbox[3] += bbox[1]\n            x = crop_like_SiamFCx(im, bbox, instanc_size=instanc_size, padding=avg_chans)\n            cv2.imwrite(join(video_crop_base_path, \'{:06d}.{:02d}.x.jpg\'.format(int(file_name.split(\'/\')[-1]), trackid)), x)\n            mask = crop_like_SiamFCx((label==int(o)).astype(np.float32), bbox, instanc_size=instanc_size, padding=0)\n            mask = ((mask > 0.2)*255).astype(np.uint8)\n            x[:,:,0] = mask + (mask == 0)*x[:,:,0]\n            # cv2.imshow(\'maskonx\', x)\n            # cv2.waitKey(0)\n            cv2.imwrite(join(video_crop_base_path, \'{:06d}.{:02d}.m.png\'.format(int(file_name.split(\'/\')[-1]), trackid)), mask)\n\n\ndef main(instanc_size=511, num_threads=12):\n    dataDir = \'.\'\n    crop_path = \'./crop{:d}\'.format(instanc_size)\n    if not isdir(crop_path): mkdir(crop_path)\n\n    for dataType in [\'train\']:\n        set_crop_base_path = join(crop_path, dataType)\n        set_img_base_path = join(dataDir, dataType)\n\n        annFile = \'{}/instances_{}.json\'.format(dataDir, dataType)\n        ytb_vos = json.load(open(annFile,\'r\'))\n        n_video = len(ytb_vos)\n        with futures.ProcessPoolExecutor(max_workers=num_threads) as executor:\n            fs = [executor.submit(crop_video, k, v, set_crop_base_path, set_img_base_path, instanc_size)\n                  for k,v in ytb_vos.items()]\n            for i, f in enumerate(futures.as_completed(fs)):\n                # Write progress to error so that it can be seen\n                printProgress(i, n_video, prefix=dataType, suffix=\'Done \', barLength=40)\n    print(\'done\')\n\n\nif __name__ == \'__main__\':\n    since = time.time()\n    main(int(sys.argv[1]), int(sys.argv[2]))\n    time_elapsed = time.time() - since\n    print(\'Total complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n'"
data/ytb_vos/parse_ytb_vos.py,0,"b'# --------------------------------------------------------\n# SiamMask\n# Licensed under The MIT License\n# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport h5py\nimport json\nimport os\nimport scipy.misc\nimport sys\nimport numpy as np\nimport cv2\nfrom os.path import join\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Convert dataset\')\n    parser.add_argument(\'--outdir\', default=\'./\', type=str,\n                        help=""output dir for json files"")\n    parser.add_argument(\'--datadir\', default=\'./\', type=str,\n                        help=""data dir for annotations to be converted"")\n    return parser.parse_args()\n\n\ndef xyxy_to_xywh(xyxy):\n    """"""Convert [x1 y1 x2 y2] box format to [x1 y1 w h] format.""""""\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xyxy) == 4\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1 + 1\n        h = xyxy[3] - y1 + 1\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack((xyxy[:, 0:2], xyxy[:, 2:4] - xyxy[:, 0:2] + 1))\n    else:\n        raise TypeError(\'Argument xyxy must be a list, tuple, or numpy array.\')\n\n\ndef polys_to_boxes(polys):\n    """"""Convert a list of polygons into an array of tight bounding boxes.""""""\n    boxes_from_polys = np.zeros((len(polys), 4), dtype=np.float32)\n    for i in range(len(polys)):\n        poly = polys[i]\n        x0 = min(min(p[::2]) for p in poly)\n        x1 = max(max(p[::2]) for p in poly)\n        y0 = min(min(p[1::2]) for p in poly)\n        y1 = max(max(p[1::2]) for p in poly)\n        boxes_from_polys[i, :] = [x0, y0, x1, y1]\n    return boxes_from_polys\n\n\nclass Instance(object):\n    instID     = 0\n    pixelCount = 0\n\n    def __init__(self, imgNp, instID):\n        if (instID ==0 ):\n            return\n        self.instID     = int(instID)\n        self.pixelCount = int(self.getInstancePixels(imgNp, instID))\n\n    def getInstancePixels(self, imgNp, instLabel):\n        return (imgNp == instLabel).sum()\n\n    def toDict(self):\n        buildDict = {}\n        buildDict[""instID""]     = self.instID\n        buildDict[""pixelCount""] = self.pixelCount\n        return buildDict\n\n    def __str__(self):\n        return ""(""+str(self.instID)+"")""\n\n\ndef convert_ytb_vos(data_dir, out_dir):\n    sets = [\'train\']\n    ann_dirs = [\'train/Annotations/\']\n    json_name = \'instances_%s.json\'\n    num_obj = 0\n    num_ann = 0\n    for data_set, ann_dir in zip(sets, ann_dirs):\n        print(\'Starting %s\' % data_set)\n        ann_dict = {}\n        ann_dir = os.path.join(data_dir, ann_dir)\n        json_ann = json.load(open(os.path.join(ann_dir, \'../meta.json\')))\n        for vid, video in enumerate(json_ann[\'videos\']):\n            v = json_ann[\'videos\'][video]\n            frames = []\n            for obj in v[\'objects\']:\n                o = v[\'objects\'][obj]\n                frames.extend(o[\'frames\'])\n            frames = sorted(set(frames))\n\n            annotations = []\n            instanceIds = []\n            for frame in frames:\n                file_name = join(video, frame)\n                fullname = os.path.join(ann_dir, file_name+\'.png\')\n                img = cv2.imread(fullname, 0)\n                h, w = img.shape[:2]\n\n                objects = dict()\n                for instanceId in np.unique(img):\n                    if instanceId == 0:\n                        continue\n                    instanceObj = Instance(img, instanceId)\n                    instanceObj_dict = instanceObj.toDict()\n                    mask = (img == instanceId).astype(np.uint8)\n                    _, contour, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n                    polygons = [c.reshape(-1).tolist() for c in contour]\n                    instanceObj_dict[\'contours\'] = [p for p in polygons if len(p) > 4]\n                    if len(instanceObj_dict[\'contours\']) and instanceObj_dict[\'pixelCount\'] > 1000:\n                        objects[instanceId] = instanceObj_dict\n                    # else:\n                    #     cv2.imshow(""disappear?"", mask)\n                    #     cv2.waitKey(0)\n\n                for objId in objects:\n                    if len(objects[objId]) == 0:\n                        continue\n                    obj = objects[objId]\n                    len_p = [len(p) for p in obj[\'contours\']]\n                    if min(len_p) <= 4:\n                        print(\'Warning: invalid contours.\')\n                        continue  # skip non-instance categories\n\n                    ann = dict()\n                    ann[\'h\'] = h\n                    ann[\'w\'] = w\n                    ann[\'file_name\'] = file_name\n                    ann[\'id\'] = int(objId)\n                    # ann[\'segmentation\'] = obj[\'contours\']\n                    # ann[\'iscrowd\'] = 0\n                    ann[\'area\'] = obj[\'pixelCount\']\n                    ann[\'bbox\'] = xyxy_to_xywh(polys_to_boxes([obj[\'contours\']])).tolist()[0]\n\n                    annotations.append(ann)\n                    instanceIds.append(objId)\n                    num_ann += 1\n            instanceIds = sorted(set(instanceIds))\n            num_obj += len(instanceIds)\n            video_ann = {str(iId): [] for iId in instanceIds}\n            for ann in annotations:\n                video_ann[str(ann[\'id\'])].append(ann)\n\n            ann_dict[video] = video_ann\n            if vid % 50 == 0 and vid != 0:\n                print(""process: %d video"" % (vid+1))\n\n        print(""Num Videos: %d"" % len(ann_dict))\n        print(""Num Objects: %d"" % num_obj)\n        print(""Num Annotations: %d"" % num_ann)\n\n        items = list(ann_dict.items())\n        train_dict = dict(items[:3000])\n        val_dict = dict(items[3000:])\n        with open(os.path.join(out_dir, json_name % \'train\'), \'w\') as outfile:\n            json.dump(train_dict, outfile)\n\n        with open(os.path.join(out_dir, json_name % \'val\'), \'w\') as outfile:\n            json.dump(val_dict, outfile)\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    convert_ytb_vos(args.datadir, args.outdir)\n'"
experiments/siammask_base/custom.py,1,"b""from models.siammask import SiamMask\nfrom models.features import MultiStageFeature\nfrom models.rpn import RPN, DepthCorr\nfrom models.mask import Mask\nimport torch\nimport torch.nn as nn\nfrom utils.load_helper import load_pretrain\nfrom resnet import resnet50\n\n\nclass ResDownS(nn.Module):\n    def __init__(self, inplane, outplane):\n        super(ResDownS, self).__init__()\n        self.downsample = nn.Sequential(\n                nn.Conv2d(inplane, outplane, kernel_size=1, bias=False),\n                nn.BatchNorm2d(outplane))\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if x.size(3) < 20:\n            l = 4\n            r = -4\n            x = x[:, :, l:r, l:r]\n        return x\n\n\nclass ResDown(MultiStageFeature):\n    def __init__(self, pretrain=False):\n        super(ResDown, self).__init__()\n        self.features = resnet50(layer3=True, layer4=False)\n        if pretrain:\n            load_pretrain(self.features, 'resnet.model')\n\n        self.downsample = ResDownS(1024, 256)\n\n        self.layers = [self.downsample, self.features.layer2, self.features.layer3]\n        self.train_nums = [1, 3]\n        self.change_point = [0, 0.5]\n\n        self.unfix(0.0)\n\n    def param_groups(self, start_lr, feature_mult=1):\n        lr = start_lr * feature_mult\n\n        def _params(module, mult=1):\n            params = list(filter(lambda x:x.requires_grad, module.parameters()))\n            if len(params):\n                return [{'params': params, 'lr': lr * mult}]\n            else:\n                return []\n\n        groups = []\n        groups += _params(self.downsample)\n        groups += _params(self.features, 0.1)\n        return groups\n\n    def forward(self, x):\n        output = self.features(x)\n        p3 = self.downsample(output[1])\n        return p3\n\n\nclass UP(RPN):\n    def __init__(self, anchor_num=5, feature_in=256, feature_out=256):\n        super(UP, self).__init__()\n\n        self.anchor_num = anchor_num\n        self.feature_in = feature_in\n        self.feature_out = feature_out\n\n        self.cls_output = 2 * self.anchor_num\n        self.loc_output = 4 * self.anchor_num\n\n        self.cls = DepthCorr(feature_in, feature_out, self.cls_output)\n        self.loc = DepthCorr(feature_in, feature_out, self.loc_output)\n\n    def forward(self, z_f, x_f):\n        cls = self.cls(z_f, x_f)\n        loc = self.loc(z_f, x_f)\n        return cls, loc\n\n\nclass MaskCorr(Mask):\n    def __init__(self, oSz=63):\n        super(MaskCorr, self).__init__()\n        self.oSz = oSz\n        self.mask = DepthCorr(256, 256, self.oSz**2)\n\n    def forward(self, z, x):\n        return self.mask(z, x)\n\n\nclass Custom(SiamMask):\n    def __init__(self, pretrain=False, **kwargs):\n        super(Custom, self).__init__(**kwargs)\n        self.features = ResDown(pretrain=pretrain)\n        self.rpn_model = UP(anchor_num=self.anchor_num, feature_in=256, feature_out=256)\n        self.mask_model = MaskCorr()\n\n    def template(self, template):\n        self.zf = self.features(template)\n\n    def track(self, search):\n        search = self.features(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, search)\n        return rpn_pred_cls, rpn_pred_loc\n\n    def track_mask(self, search):\n        search = self.features(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, search)\n        pred_mask = self.mask(self.zf, search)\n        return rpn_pred_cls, rpn_pred_loc, pred_mask\n\n"""
experiments/siammask_base/resnet.py,12,"b'import torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom models.features import Features\nfrom utils.log_helper import log_once\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(Features):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        # padding = (2 - stride) + (dilation // 2 - 1)\n        padding = 2 - stride\n        assert stride==1 or dilation==1, ""stride and dilation must have one equals to zero at least""\n        if dilation > 1:\n            padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                padding=padding, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if out.size() != residual.size():\n            print(out.size(), residual.size())\n        out += residual\n\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck_nop(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck_nop, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        s = residual.size(3)\n        residual = residual[:, :, 1:s-1, 1:s-1]\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, layer4=False, layer3=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0, # 3\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # 31x31, 15x15\n\n        self.feature_size = 128 * block.expansion\n\n        if layer3:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2) # 15x15, 7x7\n            self.feature_size = (256 + 128) * block.expansion\n        else:\n            self.layer3 = lambda x:x # identity\n\n        if layer4:\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4) # 7x7, 3x3\n            self.feature_size = 512 * block.expansion\n        else:\n            self.layer4 = lambda x:x  # identity\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        dd = dilation\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if stride == 1 and dilation == 1:\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n            else:\n                if dilation > 1:\n                    dd = dilation // 2\n                    padding = dd\n                else:\n                    dd = 1\n                    padding = 0\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=3, stride=stride, bias=False,\n                              padding=padding, dilation=dd),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n\n        layers = []\n        # layers.append(block(self.inplanes, planes, stride, downsample, dilation=dilation))\n        layers.append(block(self.inplanes, planes, stride, downsample, dilation=dd))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        # print x.size()\n        x = self.maxpool(x)\n        # print x.size()\n\n        p1 = self.layer1(x)\n        p2 = self.layer2(p1)\n        p3 = self.layer3(p2)\n        # p3 = torch.cat([p2, p3], 1)\n\n        log_once(""p3 {}"".format(p3.size()))\n        p4 = self.layer4(p3)\n\n        return p2, p3, p4\n\n\nclass ResAdjust(nn.Module):\n    def __init__(self,\n            block=Bottleneck,\n            out_channels=256,\n            adjust_number=1,\n            fuse_layers=[2,3,4]):\n        super(ResAdjust, self).__init__()\n        self.fuse_layers = set(fuse_layers)\n\n        if 2 in self.fuse_layers:\n            self.layer2 = self._make_layer(block, 128, 1, out_channels, adjust_number)\n        if 3 in self.fuse_layers:\n            self.layer3 = self._make_layer(block, 256, 2, out_channels, adjust_number)\n        if 4 in self.fuse_layers:\n            self.layer4 = self._make_layer(block, 512, 4, out_channels, adjust_number)\n\n        self.feature_size = out_channels * len(self.fuse_layers)\n\n\n    def _make_layer(self, block, plances, dilation, out, number=1):\n\n        layers = []\n\n        for _ in range(number):\n            layer = block(plances * block.expansion, plances, dilation=dilation)\n            layers.append(layer)\n\n        downsample = nn.Sequential(\n                nn.Conv2d(plances * block.expansion, out, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out)\n                )\n        layers.append(downsample)\n\n        return nn.Sequential(*layers)\n\n    def forward(self, p2, p3, p4):\n\n        outputs = []\n\n        if 2 in self.fuse_layers:\n            outputs.append(self.layer2(p2))\n        if 3 in self.fuse_layers:\n            outputs.append(self.layer3(p3))\n        if 4 in self.fuse_layers:\n            outputs.append(self.layer4(p4))\n        # return torch.cat(outputs, 1)\n        return outputs\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\n\nif __name__ == \'__main__\':\n    net = resnet50()\n    print(net)\n    net = net.cuda()\n\n    var = torch.FloatTensor(1,3,127,127).cuda()\n    var = Variable(var)\n\n    net(var)\n    print(\'*************\')\n    var = torch.FloatTensor(1,3,255,255).cuda()\n    var = Variable(var)\n\n    net(var)\n\n'"
experiments/siammask_sharp/custom.py,8,"b""from models.siammask_sharp import SiamMask\nfrom models.features import MultiStageFeature\nfrom models.rpn import RPN, DepthCorr\nfrom models.mask import Mask\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils.load_helper import load_pretrain\nfrom resnet import resnet50\n\n\nclass ResDownS(nn.Module):\n    def __init__(self, inplane, outplane):\n        super(ResDownS, self).__init__()\n        self.downsample = nn.Sequential(\n                nn.Conv2d(inplane, outplane, kernel_size=1, bias=False),\n                nn.BatchNorm2d(outplane))\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if x.size(3) < 20:\n            l = 4\n            r = -4\n            x = x[:, :, l:r, l:r]\n        return x\n\n\nclass ResDown(MultiStageFeature):\n    def __init__(self, pretrain=False):\n        super(ResDown, self).__init__()\n        self.features = resnet50(layer3=True, layer4=False)\n        if pretrain:\n            load_pretrain(self.features, 'resnet.model')\n\n        self.downsample = ResDownS(1024, 256)\n\n        self.layers = [self.downsample, self.features.layer2, self.features.layer3]\n        self.train_nums = [1, 3]\n        self.change_point = [0, 0.5]\n\n        self.unfix(0.0)\n\n    def param_groups(self, start_lr, feature_mult=1):\n        lr = start_lr * feature_mult\n\n        def _params(module, mult=1):\n            params = list(filter(lambda x:x.requires_grad, module.parameters()))\n            if len(params):\n                return [{'params': params, 'lr': lr * mult}]\n            else:\n                return []\n\n        groups = []\n        groups += _params(self.downsample)\n        groups += _params(self.features, 0.1)\n        return groups\n\n    def forward(self, x):\n        output = self.features(x)\n        p3 = self.downsample(output[-1])\n        return p3\n\n    def forward_all(self, x):\n        output = self.features(x)\n        p3 = self.downsample(output[-1])\n        return output, p3\n\n\nclass UP(RPN):\n    def __init__(self, anchor_num=5, feature_in=256, feature_out=256):\n        super(UP, self).__init__()\n\n        self.anchor_num = anchor_num\n        self.feature_in = feature_in\n        self.feature_out = feature_out\n\n        self.cls_output = 2 * self.anchor_num\n        self.loc_output = 4 * self.anchor_num\n\n        self.cls = DepthCorr(feature_in, feature_out, self.cls_output)\n        self.loc = DepthCorr(feature_in, feature_out, self.loc_output)\n\n    def forward(self, z_f, x_f):\n        cls = self.cls(z_f, x_f)\n        loc = self.loc(z_f, x_f)\n        return cls, loc\n\n\nclass MaskCorr(Mask):\n    def __init__(self, oSz=63):\n        super(MaskCorr, self).__init__()\n        self.oSz = oSz\n        self.mask = DepthCorr(256, 256, self.oSz**2)\n\n    def forward(self, z, x):\n        return self.mask(z, x)\n\n\nclass Refine(nn.Module):\n    def __init__(self):\n        super(Refine, self).__init__()\n        self.v0 = nn.Sequential(nn.Conv2d(64, 16, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(16, 4, 3, padding=1),nn.ReLU())\n\n        self.v1 = nn.Sequential(nn.Conv2d(256, 64, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(64, 16, 3, padding=1), nn.ReLU())\n\n        self.v2 = nn.Sequential(nn.Conv2d(512, 128, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(128, 32, 3, padding=1), nn.ReLU())\n\n        self.h2 = nn.Sequential(nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(32, 32, 3, padding=1), nn.ReLU())\n\n        self.h1 = nn.Sequential(nn.Conv2d(16, 16, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(16, 16, 3, padding=1), nn.ReLU())\n\n        self.h0 = nn.Sequential(nn.Conv2d(4, 4, 3, padding=1), nn.ReLU(),\n                           nn.Conv2d(4, 4, 3, padding=1), nn.ReLU())\n\n        self.deconv = nn.ConvTranspose2d(256, 32, 15, 15)\n\n        self.post0 = nn.Conv2d(32, 16, 3, padding=1)\n        self.post1 = nn.Conv2d(16, 4, 3, padding=1)\n        self.post2 = nn.Conv2d(4, 1, 3, padding=1)\n        \n        for modules in [self.v0, self.v1, self.v2, self.h2, self.h1, self.h0, self.deconv, self.post0, self.post1, self.post2,]:\n            for l in modules.modules():\n                if isinstance(l, nn.Conv2d):\n                    nn.init.kaiming_uniform_(l.weight, a=1)\n\n    def forward(self, f, corr_feature, pos=None, test=False):\n        if test:\n            p0 = torch.nn.functional.pad(f[0], [16, 16, 16, 16])[:, :, 4*pos[0]:4*pos[0]+61, 4*pos[1]:4*pos[1]+61]\n            p1 = torch.nn.functional.pad(f[1], [8, 8, 8, 8])[:, :, 2 * pos[0]:2 * pos[0] + 31, 2 * pos[1]:2 * pos[1] + 31]\n            p2 = torch.nn.functional.pad(f[2], [4, 4, 4, 4])[:, :, pos[0]:pos[0] + 15, pos[1]:pos[1] + 15]\n        else:\n            p0 = F.unfold(f[0], (61, 61), padding=0, stride=4).permute(0, 2, 1).contiguous().view(-1, 64, 61, 61)\n            if not (pos is None): p0 = torch.index_select(p0, 0, pos)\n            p1 = F.unfold(f[1], (31, 31), padding=0, stride=2).permute(0, 2, 1).contiguous().view(-1, 256, 31, 31)\n            if not (pos is None): p1 = torch.index_select(p1, 0, pos)\n            p2 = F.unfold(f[2], (15, 15), padding=0, stride=1).permute(0, 2, 1).contiguous().view(-1, 512, 15, 15)\n            if not (pos is None): p2 = torch.index_select(p2, 0, pos)\n\n        if not(pos is None):\n            p3 = corr_feature[:, :, pos[0], pos[1]].view(-1, 256, 1, 1)\n        else:\n            p3 = corr_feature.permute(0, 2, 3, 1).contiguous().view(-1, 256, 1, 1)\n\n        out = self.deconv(p3)\n        out = self.post0(F.upsample(self.h2(out) + self.v2(p2), size=(31, 31)))\n        out = self.post1(F.upsample(self.h1(out) + self.v1(p1), size=(61, 61)))\n        out = self.post2(F.upsample(self.h0(out) + self.v0(p0), size=(127, 127)))\n        out = out.view(-1, 127*127)\n        return out\n\n    def param_groups(self, start_lr, feature_mult=1):\n        params = filter(lambda x:x.requires_grad, self.parameters())\n        params = [{'params': params, 'lr': start_lr * feature_mult}]\n        return params\n\n\nclass Custom(SiamMask):\n    def __init__(self, pretrain=False, **kwargs):\n        super(Custom, self).__init__(**kwargs)\n        self.features = ResDown(pretrain=pretrain)\n        self.rpn_model = UP(anchor_num=self.anchor_num, feature_in=256, feature_out=256)\n        self.mask_model = MaskCorr()\n        self.refine_model = Refine()\n\n    def refine(self, f, pos=None):\n        return self.refine_model(f, pos)\n\n    def template(self, template):\n        self.zf = self.features(template)\n\n    def track(self, search):\n        search = self.features(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, search)\n        return rpn_pred_cls, rpn_pred_loc\n\n    def track_mask(self, search):\n        self.feature, self.search = self.features.forward_all(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, self.search)\n        self.corr_feature = self.mask_model.mask.forward_corr(self.zf, self.search)\n        pred_mask = self.mask_model.mask.head(self.corr_feature)\n        return rpn_pred_cls, rpn_pred_loc, pred_mask\n\n    def track_refine(self, pos):\n        pred_mask = self.refine_model(self.feature, self.corr_feature, pos=pos, test=True)\n        return pred_mask\n\n"""
experiments/siammask_sharp/resnet.py,11,"b'import torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom models.features import Features\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(Features):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        # padding = (2 - stride) + (dilation // 2 - 1)\n        padding = 2 - stride\n        assert stride==1 or dilation==1, ""stride and dilation must have one equals to zero at least""\n        if dilation > 1:\n            padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                padding=padding, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if out.size() != residual.size():\n            print(out.size(), residual.size())\n        out += residual\n\n        out = self.relu(out)\n\n        return out\n\n\n\nclass Bottleneck_nop(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck_nop, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        s = residual.size(3)\n        residual = residual[:, :, 1:s-1, 1:s-1]\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, layer4=False, layer3=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0, # 3\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # 31x31, 15x15\n\n        self.feature_size = 128 * block.expansion\n\n        if layer3:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2) # 15x15, 7x7\n            self.feature_size = (256 + 128) * block.expansion\n        else:\n            self.layer3 = lambda x:x # identity\n\n        if layer4:\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4) # 7x7, 3x3\n            self.feature_size = 512 * block.expansion\n        else:\n            self.layer4 = lambda x:x  # identity\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        dd = dilation\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if stride == 1 and dilation == 1:\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n            else:\n                if dilation > 1:\n                    dd = dilation // 2\n                    padding = dd\n                else:\n                    dd = 1\n                    padding = 0\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=3, stride=stride, bias=False,\n                              padding=padding, dilation=dd),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n\n        layers = []\n        # layers.append(block(self.inplanes, planes, stride, downsample, dilation=dilation))\n        layers.append(block(self.inplanes, planes, stride, downsample, dilation=dd))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        p0 = self.relu(x)\n        x = self.maxpool(p0)\n\n        p1 = self.layer1(x)\n        p2 = self.layer2(p1)\n        p3 = self.layer3(p2)\n\n        return p0, p1, p2, p3\n\n\nclass ResAdjust(nn.Module):\n    def __init__(self,\n            block=Bottleneck,\n            out_channels=256,\n            adjust_number=1,\n            fuse_layers=[2,3,4]):\n        super(ResAdjust, self).__init__()\n        self.fuse_layers = set(fuse_layers)\n\n        if 2 in self.fuse_layers:\n            self.layer2 = self._make_layer(block, 128, 1, out_channels, adjust_number)\n        if 3 in self.fuse_layers:\n            self.layer3 = self._make_layer(block, 256, 2, out_channels, adjust_number)\n        if 4 in self.fuse_layers:\n            self.layer4 = self._make_layer(block, 512, 4, out_channels, adjust_number)\n\n        self.feature_size = out_channels * len(self.fuse_layers)\n\n\n    def _make_layer(self, block, plances, dilation, out, number=1):\n\n        layers = []\n\n        for _ in range(number):\n            layer = block(plances * block.expansion, plances, dilation=dilation)\n            layers.append(layer)\n\n        downsample = nn.Sequential(\n                nn.Conv2d(plances * block.expansion, out, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out)\n                )\n        layers.append(downsample)\n\n        return nn.Sequential(*layers)\n\n    def forward(self, p2, p3, p4):\n\n        outputs = []\n\n        if 2 in self.fuse_layers:\n            outputs.append(self.layer2(p2))\n        if 3 in self.fuse_layers:\n            outputs.append(self.layer3(p3))\n        if 4 in self.fuse_layers:\n            outputs.append(self.layer4(p4))\n        # return torch.cat(outputs, 1)\n        return outputs\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\nif __name__ == \'__main__\':\n    net = resnet50()\n    print(net)\n    net = net.cuda()\n\n    var = torch.FloatTensor(1,3,127,127).cuda()\n    var = Variable(var)\n\n    net(var)\n    print(\'*************\')\n    var = torch.FloatTensor(1,3,255,255).cuda()\n    var = Variable(var)\n\n    net(var)\n\n'"
experiments/siamrpn_resnet/custom.py,1,"b""from models.siamrpn import SiamRPN\nfrom models.features import MultiStageFeature\nfrom models.rpn import RPN, DepthCorr\nimport torch.nn as nn\nfrom utils.load_helper import load_pretrain\nfrom resnet import resnet50\n\n\nclass ResDownS(nn.Module):\n    def __init__(self, inplane, outplane):\n        super(ResDownS, self).__init__()\n        self.downsample = nn.Sequential(\n                nn.Conv2d(inplane, outplane, kernel_size=1, bias=False),\n                nn.BatchNorm2d(outplane))\n\n    def forward(self, x):\n        x = self.downsample(x)\n        if x.size(3) < 20:\n            l = 4\n            r = -4\n            x = x[:, :, l:r, l:r]\n        return x\n\n\nclass ResDown(MultiStageFeature):\n    def __init__(self, pretrain=False):\n        super(ResDown, self).__init__()\n        self.features = resnet50(layer3=True, layer4=False)\n        if pretrain:\n            load_pretrain(self.features, 'resnet.model')\n\n        self.downsample = ResDownS(1024, 256)\n        self.layers = [self.downsample, self.features.layer2, self.features.layer3]\n        self.train_nums = [1, 3]\n        self.change_point = [0, 0.5]\n\n        self.unfix(0.0)\n\n    def param_groups(self, start_lr, feature_mult=1):\n        lr = start_lr * feature_mult\n\n        def _params(module, mult=1):\n            params = list(filter(lambda x:x.requires_grad, module.parameters()))\n            if len(params):\n                return [{'params': params, 'lr': lr * mult}]\n            else:\n                return []\n\n        groups = []\n        groups += _params(self.downsample)\n        groups += _params(self.features, 0.1)\n        return groups\n\n    def forward(self, x):\n        output = self.features(x)\n        p2, p3, p4 = output\n        p3 = self.downsample(p3)\n        return p3\n\n\nclass UP(RPN):\n    def __init__(self, anchor_num=5, feature_in=256, feature_out=256):\n        super(UP, self).__init__()\n\n        self.anchor_num = anchor_num\n        self.feature_in = feature_in\n        self.feature_out = feature_out\n\n        self.cls_output = 2 * self.anchor_num\n        self.loc_output = 4 * self.anchor_num\n\n        self.cls = DepthCorr(feature_in, feature_out, self.cls_output)\n        self.loc = DepthCorr(feature_in, feature_out, self.loc_output)\n\n    def forward(self, z_f, x_f):\n        cls = self.cls(z_f, x_f)\n        loc = self.loc(z_f, x_f)\n        return cls, loc\n\n\nclass Custom(SiamRPN):\n    def __init__(self, pretrain=False, **kwargs):\n        super(Custom, self).__init__(**kwargs)\n        self.features = ResDown(pretrain=pretrain)\n        self.rpn_model = UP(anchor_num=self.anchor_num, feature_in=256, feature_out=256)\n\n    def template(self, template):\n        self.zf = self.features(template)\n\n    def track(self, search):\n        search = self.features(search)\n        rpn_pred_cls, rpn_pred_loc = self.rpn(self.zf, search)\n        return rpn_pred_cls, rpn_pred_loc\n"""
experiments/siamrpn_resnet/resnet.py,12,"b'import torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom models.features import Features\nfrom utils.log_helper import log_once\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(Features):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, dilation=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        # padding = (2 - stride) + (dilation // 2 - 1)\n        padding = 2 - stride\n        assert stride==1 or dilation==1, ""stride and dilation must have one equals to zero at least""\n        if dilation > 1:\n            padding = dilation\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                                padding=padding, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        if out.size() != residual.size():\n            print(out.size(), residual.size())\n        out += residual\n\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck_nop(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck_nop, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        s = residual.size(3)\n        residual = residual[:, :, 1:s-1, 1:s-1]\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, layer4=False, layer3=False):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0,  # 3\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # 31x31, 15x15\n\n        self.feature_size = 128 * block.expansion\n\n        if layer3:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2) # 15x15, 7x7\n            self.feature_size = (256 + 128) * block.expansion\n        else:\n            self.layer3 = lambda x:x # identity\n\n        if layer4:\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4) # 7x7, 3x3\n            self.feature_size = 512 * block.expansion\n        else:\n            self.layer4 = lambda x:x  # identity\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        dd = dilation\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if stride == 1 and dilation == 1:\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n            else:\n                if dilation > 1:\n                    dd = dilation // 2\n                    padding = dd\n                else:\n                    dd = 1\n                    padding = 0\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, planes * block.expansion,\n                              kernel_size=3, stride=stride, bias=False,\n                              padding=padding, dilation=dd),\n                    nn.BatchNorm2d(planes * block.expansion),\n                )\n\n        layers = []\n        # layers.append(block(self.inplanes, planes, stride, downsample, dilation=dilation))\n        layers.append(block(self.inplanes, planes, stride, downsample, dilation=dd))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        # print x.size()\n        x = self.maxpool(x)\n        # print x.size()\n\n        p1 = self.layer1(x)\n        p2 = self.layer2(p1)\n        p3 = self.layer3(p2)\n        # p3 = torch.cat([p2, p3], 1)\n\n        log_once(""p3 {}"".format(p3.size()))\n        p4 = self.layer4(p3)\n\n        return p2, p3, p4\n\n\nclass ResAdjust(nn.Module):\n    def __init__(self,\n            block=Bottleneck,\n            out_channels=256,\n            adjust_number=1,\n            fuse_layers=[2,3,4]):\n        super(ResAdjust, self).__init__()\n        self.fuse_layers = set(fuse_layers)\n\n        if 2 in self.fuse_layers:\n            self.layer2 = self._make_layer(block, 128, 1, out_channels, adjust_number)\n        if 3 in self.fuse_layers:\n            self.layer3 = self._make_layer(block, 256, 2, out_channels, adjust_number)\n        if 4 in self.fuse_layers:\n            self.layer4 = self._make_layer(block, 512, 4, out_channels, adjust_number)\n\n        self.feature_size = out_channels * len(self.fuse_layers)\n\n    def _make_layer(self, block, plances, dilation, out, number=1):\n\n        layers = []\n\n        for _ in range(number):\n            layer = block(plances * block.expansion, plances, dilation=dilation)\n            layers.append(layer)\n\n        downsample = nn.Sequential(\n                nn.Conv2d(plances * block.expansion, out, kernel_size=3, padding=1, bias=False),\n                nn.BatchNorm2d(out)\n                )\n        layers.append(downsample)\n\n        return nn.Sequential(*layers)\n\n    def forward(self, p2, p3, p4):\n\n        outputs = []\n\n        if 2 in self.fuse_layers:\n            outputs.append(self.layer2(p2))\n        if 3 in self.fuse_layers:\n            outputs.append(self.layer3(p3))\n        if 4 in self.fuse_layers:\n            outputs.append(self.layer4(p4))\n        # return torch.cat(outputs, 1)\n        return outputs\n\n\ndef resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n    return model\n\n\nif __name__ == \'__main__\':\n    net = resnet50()\n    print(net)\n    net = net.cuda()\n\n    var = torch.FloatTensor(1,3,127,127).cuda()\n    var = Variable(var)\n    template = net(var)\n    print(\'Examplar Size: {}\'.format(template.shape))\n\n    var = torch.FloatTensor(1,3,255,255).cuda()\n    var = Variable(var)\n\n    net(var)\n\n'"
utils/pysot/__init__.py,0,b''
utils/pyvotkit/__init__.py,0,b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom . import region\n'
utils/pyvotkit/setup.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\n\nsetup(\n    ext_modules = cythonize([Extension(""region"", [""region.pyx""])])\n)\n\n'"
data/coco/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
data/coco/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\n\ntry:\n    import matplotlib.pyplot as plt\n    from matplotlib.collections import PatchCollection\n    from matplotlib.patches import Polygon\nexcept Exception as e:\n    print(e)\n\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        #if type(resFile) == str or type(resFile) == unicode:\n        if type(resFile) == str:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m\n'"
data/coco/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
data/coco/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\n#import pycocotools._mask as _mask\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]\n'"
data/coco/pycocotools/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'_mask\',\n        sources=[\'common/maskApi.c\', \'_mask.pyx\'],\n        include_dirs = [np.get_include(), \'common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(name=\'pycocotools\',\n      packages=[\'pycocotools\'],\n      package_dir = {\'pycocotools\': \'.\'},\n      version=\'2.0\',\n      ext_modules=\n          cythonize(ext_modules)\n      )\n'"
utils/pysot/datasets/__init__.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom .vot import VOTDataset\n\n\nclass DatasetFactory(object):\n    @staticmethod\n    def create_dataset(**kwargs):\n        """"""\n        Args:\n            name: dataset name \'VOT2018\', \'VOT2016\'\n            dataset_root: dataset root\n        Return:\n            dataset\n        """"""\n        assert \'name\' in kwargs, ""should provide dataset name""\n        name = kwargs[\'name\']\n        if \'VOT2018\' == name or \'VOT2016\' == name:\n            dataset = VOTDataset(**kwargs)\n        else:\n            raise Exception(""unknow dataset {}"".format(kwargs[\'name\']))\n        return dataset\n\n'"
utils/pysot/datasets/dataset.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nclass Dataset(object):\n    def __init__(self, name, dataset_root):\n        self.name = name\n        self.dataset_root = dataset_root\n        self.videos = None\n\n    def __getitem__(self, idx):\n        if isinstance(idx, str):\n            return self.videos[idx]\n        elif isinstance(idx, int):\n            return self.videos[sorted(list(self.videos.keys()))[idx]]\n\n    def __len__(self):\n        return len(self.videos)\n\n    def __iter__(self):\n        keys = sorted(list(self.videos.keys()))\n        for key in keys:\n            yield self.videos[key]\n\n    def set_tracker(self, path, tracker_names):\n        """"""\n        Args:\n            path: path to tracker results,\n            tracker_names: list of tracker name\n        """"""\n        self.tracker_path = path\n        self.tracker_names = tracker_names\n'"
utils/pysot/datasets/video.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport os\nimport cv2\n\nfrom glob import glob\n\n\nclass Video(object):\n    def __init__(self, name, root, video_dir, init_rect, img_names,\n            gt_rect, attr):\n        self.name = name\n        self.video_dir = video_dir\n        self.init_rect = init_rect\n        self.gt_traj = gt_rect\n        self.attr = attr\n        self.pred_trajs = {}\n        self.img_names = [os.path.join(root, x) for x in img_names]\n        self.imgs = None\n\n    def load_tracker(self, path, tracker_names=None, store=True):\n        """"""\n        Args:\n            path(str): path to result\n            tracker_name(list): name of tracker\n        """"""\n        if not tracker_names:\n            tracker_names = [x.split(\'/\')[-1] for x in glob(path)\n                    if os.path.isdir(x)]\n        if isinstance(tracker_names, str):\n            tracker_names = [tracker_names]\n        for name in tracker_names:\n            traj_file = os.path.join(path, name, self.name+\'.txt\')\n            if os.path.exists(traj_file):\n                with open(traj_file, \'r\') as f :\n                    pred_traj = [list(map(float, x.strip().split(\',\')))\n                            for x in f.readlines()]\n                if len(pred_traj) != len(self.gt_traj):\n                    print(name, len(pred_traj), len(self.gt_traj), self.name)\n                if store:\n                    self.pred_trajs[name] = pred_traj\n                else:\n                    return pred_traj\n            else:\n                print(traj_file)\n        self.tracker_names = list(self.pred_trajs.keys())\n\n    def load_img(self):\n        if self.imgs is None:\n            self.imgs = [cv2.imread(x)\n                            for x in self.img_names]\n            self.width = self.imgs[0].shape[1]\n            self.height = self.imgs[0].shape[0]\n\n    def free_img(self):\n        self.imgs = None\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, idx):\n        if self.imgs is None:\n            return cv2.imread(self.img_names[idx]), \\\n                    self.gt_traj[idx]\n        else:\n            return self.imgs[idx], self.gt_traj[idx]\n\n    def __iter__(self):\n        for i in range(len(self.img_names)):\n            if self.imgs is not None:\n                yield self.imgs[i], self.gt_traj[i]\n            else:\n                yield cv2.imread(self.img_names[i]), \\\n                        self.gt_traj[i]\n'"
utils/pysot/datasets/vot.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport os\nimport json\nimport numpy as np\n\nfrom glob import glob\nfrom tqdm import tqdm\n\nfrom .dataset import Dataset\nfrom .video import Video\n\n\nclass VOTVideo(Video):\n    """"""\n    Args:\n        name: video name\n        root: dataset root\n        video_dir: video directory\n        init_rect: init rectangle\n        img_names: image names\n        gt_rect: groundtruth rectangle\n        camera_motion: camera motion tag\n        illum_change: illum change tag\n        motion_change: motion change tag\n        size_change: size change\n        occlusion: occlusion\n    """"""\n    def __init__(self, name, root, video_dir, init_rect, img_names, gt_rect,\n            camera_motion, illum_change, motion_change, size_change, occlusion, width, height):\n        super(VOTVideo, self).__init__(name, root, video_dir, init_rect, img_names, gt_rect, None)\n        self.tags= {\'all\': [1] * len(gt_rect)}\n        self.tags[\'camera_motion\'] = camera_motion\n        self.tags[\'illum_change\'] = illum_change\n        self.tags[\'motion_change\'] = motion_change\n        self.tags[\'size_change\'] = size_change\n        self.tags[\'occlusion\'] = occlusion\n\n        self.width = width\n        self.height = height\n\n        # empty tag\n        all_tag = [v for k, v in self.tags.items() if len(v) > 0 ]\n        self.tags[\'empty\'] = np.all(1 - np.array(all_tag), axis=1).astype(np.int32).tolist()\n\n        self.tag_names = list(self.tags.keys())\n\n    def select_tag(self, tag, start=0, end=0):\n        if tag == \'empty\':\n            return self.tags[tag]\n        return self.tags[tag][start:end]\n\n    def load_tracker(self, path, tracker_names=None, store=True):\n        """"""\n        Args:\n            path(str): path to result\n            tracker_name(list): name of tracker\n        """"""\n        if not tracker_names:\n            tracker_names = [x.split(\'/\')[-1] for x in glob(path)\n                    if os.path.isdir(x)]\n        if isinstance(tracker_names, str):\n            tracker_names = [tracker_names]\n        for name in tracker_names:\n            traj_files = glob(os.path.join(path, name, \'baseline\', self.name, \'*0*.txt\'))\n            if len(traj_files) == 15:\n                traj_files = traj_files\n            else:\n                traj_files = traj_files[0:1]\n            pred_traj = []\n            for traj_file in traj_files:\n                with open(traj_file, \'r\') as f:\n                    traj = [list(map(float, x.strip().split(\',\')))\n                            for x in f.readlines()]\n                    pred_traj.append(traj)\n            if store:\n                self.pred_trajs[name] = pred_traj\n            else:\n                return pred_traj\n\n\nclass VOTDataset(Dataset):\n    """"""\n    Args:\n        name: dataset name, should be \'VOT2018\', \'VOT2016\'\n        dataset_root: dataset root\n        load_img: wether to load all imgs\n    """"""\n    def __init__(self, name, dataset_root):\n        super(VOTDataset, self).__init__(name, dataset_root)\n        try:\n            with open(os.path.join(dataset_root, name+\'.json\'), \'r\') as f:\n                meta_data = json.load(f)\n        except:\n            download_str = \'# download json file for eval toolkit\\n\'+\\\n                           \'cd $SiamMask/data\\n\'+\\\n                           \'wget http://www.robots.ox.ac.uk/~qwang/VOT2016.json\\n\'+\\\n                           \'wget http://www.robots.ox.ac.uk/~qwang/VOT2018.json\'\n            print(download_str)\n            exit()\n\n        # load videos\n        pbar = tqdm(meta_data.keys(), desc=\'loading \'+name, ncols=100)\n        self.videos = {}\n        for video in pbar:\n            pbar.set_postfix_str(video)\n            self.videos[video] = VOTVideo(video,\n                                          dataset_root,\n                                          meta_data[video][\'video_dir\'],\n                                          meta_data[video][\'init_rect\'],\n                                          meta_data[video][\'img_names\'],\n                                          meta_data[video][\'gt_rect\'],\n                                          meta_data[video][\'camera_motion\'],\n                                          meta_data[video][\'illum_change\'],\n                                          meta_data[video][\'motion_change\'],\n                                          meta_data[video][\'size_change\'],\n                                          meta_data[video][\'occlusion\'],\n                                          meta_data[video][\'width\'],\n                                          meta_data[video][\'height\'])\n\n        self.tags = [\'all\', \'camera_motion\', \'illum_change\', \'motion_change\',\n                     \'size_change\', \'occlusion\', \'empty\']\n'"
utils/pysot/evaluation/__init__.py,0,b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom .ar_benchmark import AccuracyRobustnessBenchmark\nfrom .eao_benchmark import EAOBenchmark\n'
utils/pysot/evaluation/ar_benchmark.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\n\nimport warnings\nimport itertools\nimport numpy as np\n\nfrom colorama import Style, Fore\nfrom ..utils import calculate_failures, calculate_accuracy\n\n\nclass AccuracyRobustnessBenchmark:\n    """"""\n    Args:\n        dataset:\n        burnin:\n    """"""\n    def __init__(self, dataset, burnin=10):\n        self.dataset = dataset\n        self.burnin = burnin\n\n    def eval(self, eval_trackers=None):\n        """"""\n        Args:\n            eval_tags: list of tag\n            eval_trackers: list of tracker name\n        Returns:\n            ret: dict of results\n        """"""\n        if eval_trackers is None:\n            eval_trackers = self.dataset.tracker_names\n        if isinstance(eval_trackers, str):\n            eval_trackers = [eval_trackers]\n\n        result = {}\n        for tracker_name in eval_trackers:\n            accuracy, failures = self._calculate_accuracy_robustness(tracker_name)\n            result[tracker_name] = {\'overlaps\': accuracy,\n                                    \'failures\': failures}\n        return result\n\n    def show_result(self, result, eao_result=None, show_video_level=False, helight_threshold=0.5):\n        """"""pretty print result\n        Args:\n            result: returned dict from function eval\n        """"""\n        tracker_name_len = max((max([len(x) for x in result.keys()])+2), 12)\n        if eao_result is not None:\n            header = ""|{:^""+str(tracker_name_len)+""}|{:^10}|{:^12}|{:^13}|{:^7}|""\n            header = header.format(\'Tracker Name\',\n                    \'Accuracy\', \'Robustness\', \'Lost Number\', \'EAO\')\n            formatter = ""|{:^""+str(tracker_name_len)+""}|{:^10.3f}|{:^12.3f}|{:^13.1f}|{:^7.3f}|""\n        else:\n            header = ""|{:^""+str(tracker_name_len)+""}|{:^10}|{:^12}|{:^13}|""\n            header = header.format(\'Tracker Name\',\n                    \'Accuracy\', \'Robustness\', \'Lost Number\')\n            formatter = ""|{:^""+str(tracker_name_len)+""}|{:^10.3f}|{:^12.3f}|{:^13.1f}|""\n        bar = \'-\'*len(header)\n        print(bar)\n        print(header)\n        print(bar)\n        if eao_result is not None:\n            tracker_eao = sorted(eao_result.items(),\n                                 key=lambda x:x[1][\'all\'],\n                                 reverse=True)[:20]\n            tracker_names = [x[0] for x in tracker_eao]\n        else:\n            tracker_names = list(result.keys())\n        for tracker_name in tracker_names:\n            ret = result[tracker_name]\n            overlaps = list(itertools.chain(*ret[\'overlaps\'].values()))\n            accuracy = np.nanmean(overlaps)\n            length = sum([len(x) for x in ret[\'overlaps\'].values()])\n            failures = list(ret[\'failures\'].values())\n            lost_number = np.mean(np.sum(failures, axis=0))\n            robustness = np.mean(np.sum(np.array(failures), axis=0) / length) * 100\n            if eao_result is None:\n                print(formatter.format(tracker_name, accuracy, robustness, lost_number))\n            else:\n                print(formatter.format(tracker_name, accuracy, robustness, lost_number, eao_result[tracker_name][\'all\']))\n        print(bar)\n\n        if show_video_level and len(result) < 10:\n            print(\'\\n\\n\')\n            header1 = ""|{:^14}|"".format(""Tracker name"")\n            header2 = ""|{:^14}|"".format(""Video name"")\n            for tracker_name in result.keys():\n                header1 += (""{:^17}|"").format(tracker_name)\n                header2 += ""{:^8}|{:^8}|"".format(""Acc"", ""LN"")\n            print(\'-\'*len(header1))\n            print(header1)\n            print(\'-\'*len(header1))\n            print(header2)\n            print(\'-\'*len(header1))\n            videos = list(result[tracker_name][\'overlaps\'].keys())\n            for video in videos:\n                row = ""|{:^14}|"".format(video)\n                for tracker_name in result.keys():\n                    overlaps = result[tracker_name][\'overlaps\'][video]\n                    accuracy = np.nanmean(overlaps)\n                    failures = result[tracker_name][\'failures\'][video]\n                    lost_number = np.mean(failures)\n\n                    accuracy_str = ""{:^8.3f}"".format(accuracy)\n                    if accuracy < helight_threshold:\n                        row += f\'{Fore.RED}{accuracy_str}{Style.RESET_ALL}|\'\n                    else:\n                        row += accuracy_str+\'|\'\n                    lost_num_str = ""{:^8.3f}"".format(lost_number)\n                    if lost_number > 0:\n                        row += f\'{Fore.RED}{lost_num_str}{Style.RESET_ALL}|\'\n                    else:\n                        row += lost_num_str+\'|\'\n                print(row)\n            print(\'-\'*len(header1))\n\n    def _calculate_accuracy_robustness(self, tracker_name):\n        overlaps = {}\n        failures = {}\n        all_length = {}\n        for i in range(len(self.dataset)):\n            video = self.dataset[i]\n            gt_traj = video.gt_traj\n            if tracker_name not in video.pred_trajs:\n                tracker_trajs = video.load_tracker(self.dataset.tracker_path, tracker_name, False)\n            else:\n                tracker_trajs = video.pred_trajs[tracker_name]\n            overlaps_group = []\n            num_failures_group = []\n            for tracker_traj in tracker_trajs:\n                num_failures = calculate_failures(tracker_traj)[0]\n                overlaps_ = calculate_accuracy(tracker_traj, gt_traj,\n                        burnin=10, bound=(video.width, video.height))[1]\n                overlaps_group.append(overlaps_)\n                num_failures_group.append(num_failures)\n            with warnings.catch_warnings():\n                warnings.simplefilter(""ignore"", category=RuntimeWarning)\n                overlaps[video.name] = np.nanmean(overlaps_group, axis=0).tolist()\n                failures[video.name] = num_failures_group\n        return overlaps, failures\n'"
utils/pysot/evaluation/eao_benchmark.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport numpy as np\n\nfrom ..utils import calculate_failures, calculate_accuracy, calculate_expected_overlap\n\n\nclass EAOBenchmark:\n    """"""\n    Args:\n        dataset:\n    """"""\n    def __init__(self, dataset, skipping=5, tags=[\'all\']):\n        self.dataset = dataset\n        self.skipping = skipping\n        self.tags = tags\n        # NOTE we not use gmm to generate low, high, peak value\n        if dataset.name in [\'VOT2019\']:\n            self.low = 46\n            self.high = 291\n            self.peak = 128\n        elif dataset.name in [\'VOT2018\', \'VOT2017\']:\n            self.low = 100\n            self.high = 356\n            self.peak = 160\n        elif dataset.name == \'VOT2016\':\n            self.low = 100  # TODO\n            self.high = 356\n            self.peak = 160\n\n    def eval(self, eval_trackers=None):\n        """"""\n        Args:\n            eval_tags: list of tag\n            eval_trackers: list of tracker name\n        Returns:\n            eao: dict of results\n        """"""\n        if eval_trackers is None:\n            eval_trackers = self.dataset.tracker_names\n        if isinstance(eval_trackers, str):\n            eval_trackers = [eval_trackers]\n\n        ret = {}\n        for tracker_name in eval_trackers:\n            eao = self._calculate_eao(tracker_name, self.tags)\n            ret[tracker_name] = eao\n        return ret\n\n    def show_result(self, result, topk=10):\n        """"""pretty print result\n        Args:\n            result: returned dict from function eval\n        """"""\n        if len(self.tags) == 1:\n            tracker_name_len = max((max([len(x) for x in result.keys()])+2), 12)\n            header = (""|{:^""+str(tracker_name_len)+""}|{:^10}|"").format(\'Tracker Name\', \'EAO\')\n            bar = \'-\'*len(header)\n            formatter = ""|{:^20}|{:^10.3f}|""\n            print(bar)\n            print(header)\n            print(bar)\n            tracker_eao = sorted(result.items(), \n                                 key=lambda x: x[1][\'all\'], \n                                 reverse=True)[:topk]\n            for tracker_name, eao in tracker_eao:\n                print(formatter.format(tracker_name, eao))\n            print(bar)\n        else:\n            header = ""|{:^20}|"".format(\'Tracker Name\')\n            header += ""{:^7}|{:^15}|{:^14}|{:^15}|{:^13}|{:^11}|{:^7}|"".format(*self.tags)\n            bar = \'-\'*len(header)\n            formatter = ""{:^7.3f}|{:^15.3f}|{:^14.3f}|{:^15.3f}|{:^13.3f}|{:^11.3f}|{:^7.3f}|""\n            print(bar)\n            print(header)\n            print(bar)\n            sorted_tacker = sorted(result.items(), \n                                   key=lambda x: x[1][\'all\'],\n                                   reverse=True)[:topk]\n            sorted_tacker = [x[0] for x in sorted_tacker]\n            for tracker_name in sorted_tacker:\n                print(""|{:^20}|"".format(tracker_name)+formatter.format(\n                    *[result[tracker_name][x] for x in self.tags]))\n            print(bar)\n\n    def _calculate_eao(self, tracker_name, tags):\n        all_overlaps = []\n        all_failures = []\n        video_names = []\n        gt_traj_length = []\n        for video in self.dataset:\n            gt_traj = video.gt_traj\n            if tracker_name not in video.pred_trajs:\n                tracker_trajs = video.load_tracker(self.dataset.tracker_path, tracker_name, False)\n            else:\n                tracker_trajs = video.pred_trajs[tracker_name]\n            for tracker_traj in tracker_trajs:\n                gt_traj_length.append(len(gt_traj))\n                video_names.append(video.name)\n                overlaps = calculate_accuracy(tracker_traj, gt_traj, bound=(video.width-1, video.height-1))[1]\n                failures = calculate_failures(tracker_traj)[1]\n                all_overlaps.append(overlaps)\n                all_failures.append(failures)\n        fragment_num = sum([len(x)+1 for x in all_failures])\n        max_len = max([len(x) for x in all_overlaps])\n        seq_weight = 1 / len(tracker_trajs)\n\n        eao = {}\n        for tag in tags:\n            # prepare segments\n            fweights = np.ones((fragment_num)) * np.nan\n            fragments = np.ones((fragment_num, max_len)) * np.nan\n            seg_counter = 0\n            for name, traj_len, failures, overlaps in zip(video_names, gt_traj_length,\n                    all_failures, all_overlaps):\n                if len(failures) > 0:\n                    points = [x+self.skipping for x in failures if\n                            x+self.skipping <= len(overlaps)]\n                    points.insert(0, 0)\n                    for i in range(len(points)):\n                        if i != len(points) - 1:\n                            fragment = np.array(overlaps[points[i]:points[i+1]+1])\n                            fragments[seg_counter, :] = 0\n                        else:\n                            fragment = np.array(overlaps[points[i]:])\n                        fragment[np.isnan(fragment)] = 0\n                        fragments[seg_counter, :len(fragment)] = fragment\n                        if i != len(points) - 1:\n                            tag_value = self.dataset[name].select_tag(tag, points[i], points[i+1]+1)\n                            w = sum(tag_value) / (points[i+1] - points[i]+1)\n                            fweights[seg_counter] = seq_weight * w\n                        else:\n                            tag_value = self.dataset[name].select_tag(tag, points[i], len(overlaps))\n                            w = sum(tag_value) / (traj_len - points[i]+1e-16)\n                            fweights[seg_counter] = seq_weight * w\n                        seg_counter += 1\n                else:\n                    # no failure\n                    max_idx = min(len(overlaps), max_len)\n                    fragments[seg_counter, :max_idx] = overlaps[:max_idx]\n                    tag_value = self.dataset[name].select_tag(tag, 0, max_idx)\n                    w = sum(tag_value) / max_idx\n                    fweights[seg_counter] = seq_weight * w\n                    seg_counter += 1\n\n            expected_overlaps = calculate_expected_overlap(fragments, fweights)\n            # caculate eao\n            weight = np.zeros((len(expected_overlaps)))\n            weight[self.low-1:self.high-1+1] = 1\n            is_valid = np.logical_not(np.isnan(expected_overlaps))\n            eao_ = np.sum(expected_overlaps[is_valid] * weight[is_valid]) / np.sum(weight[is_valid])\n            eao[tag] = eao_\n        return eao\n'"
utils/pysot/utils/__init__.py,0,b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom . import region\nfrom .statistics import *\n'
utils/pysot/utils/misc.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nimport numpy as np\n\ndef determine_thresholds(confidence, resolution=100):\n    """"""choose threshold according to confidence\n\n    Args:\n        confidence: list or numpy array or numpy array\n        reolution: number of threshold to choose\n\n    Restures:\n        threshold: numpy array\n    """"""\n    if isinstance(confidence, list):\n        confidence = np.array(confidence)\n    confidence = confidence.flatten()\n    confidence = confidence[~np.isnan(confidence)]\n    confidence.sort()\n\n    assert len(confidence) > resolution and resolution > 2\n\n    thresholds = np.ones((resolution))\n    thresholds[0] = - np.inf\n    thresholds[-1] = np.inf\n    delta = np.floor(len(confidence) / (resolution - 2))\n    idxs = np.linspace(delta, len(confidence)-delta, resolution-2, dtype=np.int32)\n    thresholds[1:-1] =  confidence[idxs]\n    return thresholds\n'"
utils/pysot/utils/setup.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Build import cythonize\n\nsetup(\n    ext_modules = cythonize([Extension(""region"", [""region.pyx"", ""src/region.c""])]),\n)\n\n'"
utils/pysot/utils/statistics.py,0,"b'# --------------------------------------------------------\n# Python Single Object Tracking Evaluation\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Fangyi Zhang\n# @author fangyi.zhang@vipl.ict.ac.cn\n# @project https://github.com/StrangerZhang/pysot-toolkit.git\n# Revised for SiamMask by foolwood\n# --------------------------------------------------------\n\nimport numpy as np\nfrom numba import jit\nfrom . import region\n\ndef calculate_failures(trajectory):\n    """""" Calculate number of failures\n    Args:\n        trajectory: list of bbox\n    Returns:\n        num_failures: number of failures\n        failures: failures point in trajectory, start with 0\n    """"""\n    failures = [i for i, x in zip(range(len(trajectory)), trajectory)\n            if len(x) == 1 and x[0] == 2]\n    num_failures = len(failures)\n    return num_failures, failures\n\ndef calculate_accuracy(pred_trajectory, gt_trajectory,\n        burnin=0, ignore_unknown=True, bound=None):\n    """"""Caculate accuracy socre as average overlap over the entire sequence\n    Args:\n        trajectory: list of bbox\n        gt_trajectory: list of bbox\n        burnin: number of frames that have to be ignored after the failure\n        ignore_unknown: ignore frames where the overlap is unknown\n        bound: bounding region\n    Return:\n        acc: average overlap\n        overlaps: per frame overlaps\n    """"""\n    pred_trajectory_ = pred_trajectory\n    if not ignore_unknown:\n        unkown = [len(x)==1 and x[0] == 0 for x in pred_trajectory]\n    \n    if burnin > 0:\n        pred_trajectory_ = pred_trajectory[:]\n        mask = [len(x)==1 and x[0] == 1 for x in pred_trajectory]\n        for i in range(len(mask)):\n            if mask[i]:\n                for j in range(burnin):\n                    if i + j < len(mask):\n                        pred_trajectory_[i+j] = [0]\n    min_len = min(len(pred_trajectory_), len(gt_trajectory))\n    overlaps = region.vot_overlap_traj(pred_trajectory_[:min_len],\n            gt_trajectory[:min_len], bound)\n\n    if not ignore_unknown:\n        overlaps = [u if u else 0 for u in unkown]\n\n    acc = 0\n    if len(overlaps) > 0:\n        acc = np.nanmean(overlaps)\n    return acc, overlaps\n\n@jit(nopython=True)\ndef overlap_ratio(rect1, rect2):\n    \'\'\'Compute overlap ratio between two rects\n    Args\n        rect:2d array of N x [x,y,w,h]\n    Return:\n        iou\n    \'\'\'\n    # if rect1.ndim==1:\n    #     rect1 = rect1[np.newaxis, :]\n    # if rect2.ndim==1:\n    #     rect2 = rect2[np.newaxis, :]\n    left = np.maximum(rect1[:,0], rect2[:,0])\n    right = np.minimum(rect1[:,0]+rect1[:,2], rect2[:,0]+rect2[:,2])\n    top = np.maximum(rect1[:,1], rect2[:,1])\n    bottom = np.minimum(rect1[:,1]+rect1[:,3], rect2[:,1]+rect2[:,3])\n\n    intersect = np.maximum(0,right - left) * np.maximum(0,bottom - top)\n    union = rect1[:,2]*rect1[:,3] + rect2[:,2]*rect2[:,3] - intersect\n    iou = intersect / union\n    iou = np.maximum(np.minimum(1, iou), 0)\n    return iou\n\n@jit(nopython=True)\ndef success_overlap(gt_bb, result_bb, n_frame):\n    thresholds_overlap = np.arange(0, 1.05, 0.05)\n    success = np.zeros(len(thresholds_overlap))\n    iou = np.ones(len(gt_bb)) * (-1)\n    mask = np.sum(gt_bb > 0, axis=1) == 4\n    iou[mask] = overlap_ratio(gt_bb[mask], result_bb[mask])\n    for i in range(len(thresholds_overlap)):\n        success[i] = np.sum(iou > thresholds_overlap[i]) / float(n_frame)\n    return success\n\n@jit(nopython=True)\ndef success_error(gt_center, result_center, thresholds, n_frame):\n    # n_frame = len(gt_center)\n    success = np.zeros(len(thresholds))\n    dist = np.ones(len(gt_center)) * (-1)\n    mask = np.sum(gt_center > 0, axis=1) == 2\n    dist[mask] = np.sqrt(np.sum(\n        np.power(gt_center[mask] - result_center[mask], 2), axis=1))\n    for i in range(len(thresholds)):\n        success[i] = np.sum(dist <= thresholds[i]) / float(n_frame)\n    return success\n\n@jit(nopython=True)\ndef determine_thresholds(scores, resolution=100):\n    """"""\n    Args:\n        scores: 1d array of score\n    """"""\n    scores = np.sort(scores[np.logical_not(np.isnan(scores))])\n    delta = np.floor(len(scores) / (resolution - 2))\n    idxs = np.floor(np.linspace(delta-1, len(scores)-delta, resolution-2)+0.5).astype(np.int32)\n    thresholds = np.zeros((resolution))\n    thresholds[0] = - np.inf\n    thresholds[-1] = np.inf\n    thresholds[1:-1] = scores[idxs]\n    return thresholds\n\n@jit(nopython=True)\ndef calculate_f1(overlaps, score, bound, thresholds, N):\n    overlaps = np.array(overlaps)\n    overlaps[np.isnan(overlaps)] = 0\n    score = np.array(score)\n    score[np.isnan(score)] = 0\n    precision = np.zeros(len(thresholds))\n    recall = np.zeros(len(thresholds))\n    for i, th in enumerate(thresholds):\n        if th == - np.inf:\n            idx = score > 0\n        else:\n            idx = score >= th\n        if np.sum(idx) == 0:\n            precision[i] = 1\n            recall[i] = 0\n        else:\n            precision[i] = np.mean(overlaps[idx])\n            recall[i] = np.sum(overlaps[idx]) / N\n    f1 = 2 * precision * recall / (precision + recall)\n    return f1, precision, recall\n\n@jit(nopython=True)\ndef calculate_expected_overlap(fragments, fweights):\n    max_len = fragments.shape[1]\n    expected_overlaps = np.zeros((max_len), np.float32)\n    expected_overlaps[0] = 1\n\n    # TODO Speed Up \n    for i in range(1, max_len):\n        mask = np.logical_not(np.isnan(fragments[:, i]))\n        if np.any(mask):\n            fragment = fragments[mask, 1:i+1]\n            seq_mean = np.sum(fragment, 1) / fragment.shape[1]\n            expected_overlaps[i] = np.sum(seq_mean *\n                fweights[mask]) / np.sum(fweights[mask])\n    return expected_overlaps\n'"
