file_path,api_count,code
loader/__init__.py,0,"b""from .coco_loader import cocoDataset\n\n__dataset = {'coco': cocoDataset}\n\n\ndef get_loader(name):\n    return __dataset[name]\n\n\ndef dataset_names():\n    return list(__dataset.keys())"""
loader/coco_loader.py,5,"b'from os.path import join\nimport numpy as np\nimport random\nfrom collections import namedtuple\nimport cv2\nfrom PIL import Image\nfrom torch.utils import data\nfrom loader.pycocotools.coco import COCO\n\n_Config = namedtuple(\'Config\', [\'datadir\', \'iSz\', \'gSz\',\n                                 \'scale\', \'shift\', \'maxload\',\n                                 \'testmaxload\', \'batch\', \'hfreq\'])\n_config = _Config(datadir=\'/media/torrvision/Data/coco\',\n          iSz=160, gSz=112, scale=.25, shift=16,\n          maxload=4000, testmaxload=500, batch=32,\n          hfreq=0.5)\n\n\ndef range_end(start, stop, step=1):\n    return np.arange(start, stop+step, step)\n\n\nclass cocoDataset(data.Dataset):\n    """"""cocoDataset\n\n    http://cocodataset.org\n\n    Data is derived from COCO17, and can be downloaded from here:\n    http://cocodataset.org/#download\n    """"""\n    def __init__(self, config=None, split=\'val\'):\n        self.datadir = config.datadir if hasattr(config, \'datadir\') else join(\'data\', \'coco\')\n        self.split = split\n        self.annFile = join(self.datadir, \'annotations/instances_%s2017.json\' % split)\n        self.coco = COCO(self.annFile)\n\n        self.mean = np.array([0.485, 0.456, 0.406]).astype(np.float32).reshape(3, 1, 1)\n        self.std = np.array([0.229, 0.224, 0.225]).astype(np.float32).reshape(3, 1, 1)\n\n        self.iSz = config.iSz\n        self.objSz = np.ceil(config.iSz * 128 / 224)\n        self.wSz = config.iSz + 32\n        self.gSz = config.gSz\n        self.scale = config.scale\n        self.shift = config.shift\n        self.hfreq = config.hfreq\n\n        self.imgIds = self.coco.getImgIds()\n        self.annIds = self.coco.getAnnIds()\n        self.catIds = self.coco.getCatIds()\n        self.nImages = len(self.imgIds)\n        self.batch = config.batch\n\n        if split == \'train\':\n            self.__size = config.maxload * config.batch\n        elif split == \'val\':\n            self.__size = config.testmaxload * config.batch\n\n        self.rand_list = np.random.choice([0, 1], size=self.__size // self.batch,\n                                          p=[self.hfreq, 1-self.hfreq])\n\n        if config.hfreq < 1:\n            self.scales = range_end(-3, 2, .25)\n            self.createBBstruct(self.objSz, config.scale)\n\n    def __len__(self):\n        return self.__size\n\n    def __getitem__(self, index):\n        """"""Get a patch, label, and status.\n        Returns:\n            img: Image patch (b*3*H*W numpy.ndarray [0 ~ 1]).\n            label: Image label (b*1*gSz*gSz /b*1*1*1 numpy.ndarray [-1, 1]).\n            head_status: head status (0 for mask, 1 for score).\n        """"""\n        head_status = self.rand_list[index // self.batch]\n        if head_status == 0:\n            img, label = self.maskSampling()\n        elif head_status == 1:\n            img, label = self.scoreSampling()\n\n        if random.random() > .5:\n            img = img[:, :, ::-1].copy()\n            if head_status == 0:\n                label = label[:, :, ::-1].copy()\n\n        img = (img-self.mean)/self.std\n        return img.astype(np.float32), \\\n               label.astype(np.float32), head_status\n\n    def shuffle(self):\n        self.rand_list = np.random.choice([0, 1], size=self.__size // self.batch,\n                                          p=[self.hfreq, 1-self.hfreq])\n\n    def createBBstruct(self, objSz, scale):\n        bbStruct = []\n        for imId in self.imgIds:\n            annIds = self.coco.getAnnIds(imgIds=imId)\n            bbs = {\'scales\': []}\n            for annId in annIds:\n                ann = self.coco.loadAnns(annId)[0]\n                bbGt = ann[\'bbox\']\n                x0, y0, w, h = bbGt\n                xc, yc, maxDim = x0 + w / 2, y0 + h / 2, max(w, h)\n                for s in range_end(-32., 32.):\n                    d1 = objSz * 2 ** ((s - 1) * scale)\n                    d2 = objSz * 2 ** ((s + 1) * scale)\n                    if d1 < maxDim <= d2:\n                        ss = -s * scale\n                        xcS, ycS = xc * (2 ** ss), yc * (2 ** ss)\n                        sss = str(ss).replace(\'.\', \'_\')\n                        if sss not in bbs:\n                            bbs[sss] = []\n                            bbs[\'scales\'].append(sss)\n                        bbs[sss].append((xcS, ycS))\n                        break\n            bbStruct.append(bbs)\n        self.bbStruct = bbStruct\n\n    def maskSampling(self):\n        iSz, wSz, gSz = self.iSz, self.wSz, self.gSz\n        catId = random.choice(self.catIds)\n        annIds = self.coco.getAnnIds(catIds=catId)\n        ann = None\n        while not ann or ann[\'iscrowd\'] == 1 or ann[\'area\'] < 100 or \\\n                ann[\'bbox\'][2] < 5 or ann[\'bbox\'][3] < 5:\n            ann = self.coco.loadAnns(random.choice(annIds))[0]\n\n        bbox = self.jitterBox(ann[\'bbox\'])\n        imgName = self.coco.loadImgs(ann[\'image_id\'])[0][\'file_name\']\n        imgPath = \'%s/%s2017/%s\' % (self.datadir, self.split, imgName)\n\n        img = np.array(Image.open(imgPath).convert(\'RGB\'), dtype=np.float32)/255.\n        img = self.crop_hwc(img, bbox, wSz, (0.5, 0.5, 0.5))\n\n        iSzR = iSz * (bbox[3] / wSz)\n        xc, yc = bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2\n        bboxInpSz = [xc - iSzR / 2, yc - iSzR / 2, iSzR, iSzR]\n        mo = self.coco.annToMask(ann).astype(np.float32)  # float for bilinear interpolation\n        lbl = self.crop_hwc(mo, bboxInpSz, gSz) > 0.5\n        lbl = lbl * 2 - 1\n\n        img = np.transpose(img, (2, 0, 1))  # 3*H*W\n        lbl = np.expand_dims(lbl, axis=0)   # 1*H*W\n        return img, lbl\n\n    def scoreSampling(self):\n        while True:\n            idx = random.randint(0, self.nImages-1)\n            bb = self.bbStruct[idx]\n            if len(bb[\'scales\']) != 0:  # have object\n                break\n\n        imgId = self.imgIds[idx]\n        imgName = self.coco.loadImgs(imgId)[0][\'file_name\']\n        imgPath = \'%s/%s2017/%s\' % (self.datadir, self.split, imgName)\n\n        img = np.array(Image.open(imgPath).convert(\'RGB\'), dtype=np.float32)/255.\n\n        h, w = img.shape[:2]\n\n        if random.random() > 0.5:\n            x, y, scale = self.posSamplingBB(bb)\n            lbl = 1\n        else:\n            x, y, scale = self.negSamplingBB(bb, w, h)\n            lbl = -1\n        s = 2 ** -scale\n        x, y = min(max(x * s, 0), w), min(max(y * s, 0), h)\n        isz = max(self.wSz * s, 10)\n        inp = self.crop_hwc(img, [x-isz/2, y-isz/2, isz, isz], self.wSz, (0.5, 0.5, 0.5))\n\n        inp = np.transpose(inp, (2, 0, 1))  # 3*H*W\n        lbl = np.reshape(lbl, (1, 1, 1))    # 1*1*1\n        return inp, lbl\n\n    def jitterBox(self, box):\n        x, y, w, h = box\n        xc, yc = x + w / 2, y + h / 2\n        maxDim = max(w, h)\n        scale = np.log2(maxDim / self.objSz)\n        s = scale + random.uniform(-self.scale, self.scale)\n        xc = xc + random.uniform(-self.shift, self.shift) * 2 ** s\n        yc = yc + random.uniform(-self.shift, self.shift) * 2 ** s\n        w, h = self.wSz * 2 ** s, self.wSz * 2 ** s\n        return [xc - w / 2, yc - h / 2, w, h]\n\n    def crop_hwc(self, image, bbox, out_sz, padding=0):\n        a = (out_sz-1) / bbox[2]\n        b = (out_sz-1) / bbox[3]\n        c = -a * bbox[0]\n        d = -b * bbox[1]\n        mapping = np.array([[a, 0, c],\n                            [0, b, d]]).astype(np.float)\n        crop = cv2.warpAffine(image, mapping, (out_sz, out_sz),\n                              borderMode=cv2.BORDER_CONSTANT,\n                              borderValue=padding)\n        return crop\n\n    def posSamplingBB(self, bb):\n        scale = random.choice(bb[\'scales\'])\n        x, y = random.choice(bb[scale])\n        return x, y, float(scale.replace(\'_\', \'.\'))\n\n    def negSamplingBB(self, bb, w0, h0):\n        neg_flag, c = False, 0\n        while not neg_flag and c < 100:\n            scale = random.choice(self.scales)\n            x, y = random.uniform(0, w0*2**scale), random.uniform(0, h0*2**scale)\n            neg_flag = True\n            for s in range_end(-10, 10):\n                ss = scale + s*self.scale\n                sss = str(ss).replace(\'.\', \'_\')\n                if sss in bb:\n                    for cc in bb[sss]:\n                        dist = np.sqrt((x - cc[0])**2 + (y - cc[1])**2)\n                        if dist < 3*self.shift:\n                            neg_flag = False\n                            break\n                if not neg_flag:\n                    break\n            c += 1\n        return x, y, scale\n\n\nif __name__ == \'__main__\':\n    import time\n    import torch\n    import matplotlib.pyplot as plt  # visualization\n    import torchvision.utils as vutils  # visualization\n    from tools.train import BinaryMeter, IouMeter\n    visual = False\n\n    dst = cocoDataset(_config)\n    bs = _config.batch\n    train_loader = data.DataLoader(dst, batch_size=bs, num_workers=0)\n\n    score_meter = BinaryMeter()\n    score_meter.reset()\n    mask_meter = IouMeter(0.5, len(train_loader.dataset))\n    mask_meter.reset()\n\n    tic = time.time()\n    for i, data in enumerate(train_loader):\n        imgs, labels, head_status = data\n        print(i, imgs.shape, labels.shape, head_status.shape)\n\n        if head_status[0] == 0:\n            outputs = torch.flip(labels, [0, ])\n            mask_meter.add(outputs, labels)\n        else:\n            outputs = torch.flip(labels, [0, ])\n            score_meter.add(outputs, labels)\n\n        if visual:\n            img_show = vutils.make_grid(imgs, normalize=True, scale_each=True)\n            img_show_numpy = np.transpose(img_show.cpu().data.numpy(), axes=(1, 2, 0))\n\n            iSz_res = torch.nn.functional.interpolate(labels, size=(_config.iSz, _config.iSz))\n            pad_res = torch.nn.functional.pad(iSz_res, (16, 16, 16, 16))\n            mask_show = vutils.make_grid(pad_res, scale_each=True)\n            mask_show_numpy = np.transpose(mask_show.cpu().data.numpy(), axes=(1, 2, 0))\n\n            plt.imshow(img_show_numpy)\n            plt.imshow(mask_show_numpy[:, :, 0] > 0, alpha=.7, cmap=\'jet\')\n            plt.axis(\'off\')\n            plt.subplots_adjust(.05, .05, .95, .95)\n            plt.show()\n            plt.close()\n\n    print(\' IoU: mean %05.2f median %05.2f suc@.5 %05.2f suc@.7 %05.2f | acc %05.2f\' % (\n        mask_meter.value(\'mean\'), mask_meter.value(\'median\'), mask_meter.value(\'0.5\'), mask_meter.value(\'0.7\'),\n        score_meter.value()))\n    toc = time.time() - tic\n    print(\'Time used in 1 epoch: {:.2f}s\'.format(toc))\n\n'"
models/DeepMask.py,7,"b'import torch\nimport torch.nn as nn\nimport torchvision\nfrom collections import namedtuple\n\nConfig = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\'])\ndefault_config = Config(iSz=160, oSz=56, gSz=112)\n\n\nclass Reshape(nn.Module):\n    def __init__(self, oSz):\n        super(Reshape, self).__init__()\n        self.oSz = oSz\n\n    def forward(self, x):\n        b = x.shape[0]\n        return x.permute(0, 2, 3, 1).view(b, -1, self.oSz, self.oSz)\n\n\nclass SymmetricPad2d(nn.Module):\n    def __init__(self, padding):\n        super(SymmetricPad2d, self).__init__()\n        self.padding = padding\n        try:\n            self.pad_l, self.pad_b, self.pad_r, self.pad_t = padding\n        except:\n            self.pad_l, self.pad_b, self.pad_r, self.pad_t = [padding,]*4\n\n    def forward(self, input):\n        assert len(input.shape) == 4, ""only Dimension=4 implemented""\n        h = input.shape[2] + self.pad_t + self.pad_b\n        w = input.shape[3] + self.pad_l + self.pad_r\n        assert w >= 1 and h >= 1, ""input is too small""\n        output = torch.zeros(input.shape[0], input.shape[1], h, w).to(input.device)\n        c_input = input\n        if self.pad_t < 0:\n            c_input = c_input.narrow(2, -self.pad_t, c_input.shape[2] + self.pad_t)\n        if self.pad_b < 0:\n            c_input = c_input.narrow(2, 0, c_input.shape[2] + self.pad_b)\n        if self.pad_l < 0:\n            c_input = c_input.narrow(3, -self.pad_l, c_input.shape[3] + self.pad_l)\n        if self.pad_r < 0:\n            c_input = c_input.narrow(3, 0, c_input.shape[3] + self.pad_r)\n\n        c_output = output\n        if self.pad_t > 0:\n            c_output = c_output.narrow(2, self.pad_t, c_output.shape[2] - self.pad_t)\n        if self.pad_b > 0:\n            c_output = c_output.narrow(2, 0, c_output.shape[2] - self.pad_b)\n        if self.pad_l > 0:\n            c_output = c_output.narrow(3, self.pad_l, c_output.shape[3] - self.pad_l)\n        if self.pad_r > 0:\n            c_output = c_output.narrow(3, 0, c_output.shape[3] - self.pad_r)\n\n        c_output.copy_(c_input)\n\n        assert w >= 2*self.pad_l and w >= 2*self.pad_r and h >= 2*self.pad_t and h >= 2*self.pad_b\n        ""input is too small""\n        for i in range(self.pad_t):\n            output.narrow(2, self.pad_t-i-1, 1).copy_(output.narrow(2, self.pad_t+i, 1))\n        for i in range(self.pad_b):\n            output.narrow(2, output.shape[2] - self.pad_b + i, 1).copy_(\n                output.narrow(2, output.shape[2] - self.pad_b - i-1, 1))\n        for i in range(self.pad_l):\n            output.narrow(3, self.pad_l-i-1, 1).copy_(output.narrow(3, self.pad_l+i, 1))\n        for i in range(self.pad_r):\n            output.narrow(3, output.shape[3] - self.pad_r + i, 1).copy_(\n                output.narrow(3, output.shape[3] - self.pad_r - i-1, 1))\n        return output\n\n\ndef updatePadding(net, nn_padding):\n    typename = torch.typename(net)\n    # print(typename)\n    if typename.find(\'Sequential\') >= 0 or typename.find(\'Bottleneck\') >= 0:\n        modules_keys = list(net._modules.keys())\n        for i in reversed(range(len(modules_keys))):\n            subnet = net._modules[modules_keys[i]]\n            out = updatePadding(subnet, nn_padding)\n            if out != -1:\n                p = out\n                in_c, out_c, k, s, _, d, g, b = \\\n                    subnet.in_channels, subnet.out_channels, \\\n                    subnet.kernel_size[0], subnet.stride[0], \\\n                    subnet.padding[0], subnet.dilation[0], \\\n                    subnet.groups, subnet.bias,\n                conv_temple = nn.Conv2d(in_c, out_c, k, stride=s, padding=0,\n                                        dilation=d, groups=g, bias=b)\n                conv_temple.weight = subnet.weight\n                conv_temple.bias = subnet.bias\n                if p > 1:\n                    net._modules[modules_keys[i]] = nn.Sequential(SymmetricPad2d(p), conv_temple)\n                else:\n                    net._modules[modules_keys[i]] = nn.Sequential(nn_padding(p), conv_temple)\n    else:\n        if typename.find(\'torch.nn.modules.conv.Conv2d\') >= 0:\n            k_sz, p_sz = net.kernel_size[0], net.padding[0]\n            if ((k_sz == 3) or (k_sz == 7)) and p_sz != 0:\n                return p_sz\n    return -1\n\n\nclass DeepMask(nn.Module):\n    def __init__(self, config=default_config, context=True):\n        super(DeepMask, self).__init__()\n        self.config = config\n        self.context = context  # without context\n        self.strides = 16\n        self.fSz = -(-self.config.iSz // self.strides)  # ceil div\n        self.trunk = self.creatTrunk()\n        updatePadding(self.trunk, nn.ReflectionPad2d)\n        self.crop_trick = nn.ZeroPad2d(-16//self.strides)  # for training\n        self.maskBranch = self.createMaskBranch()\n        self.scoreBranch = self.createScoreBranch()\n\n        npt = sum(p.numel() for p in self.trunk.parameters()) / 1e+06\n        npm = sum(p.numel() for p in self.maskBranch.parameters()) / 1e+06\n        nps = sum(p.numel() for p in self.scoreBranch.parameters()) / 1e+06\n        print(\'| number of paramaters trunk: {:.3f} M\'.format(npt))\n        print(\'| number of paramaters mask branch: {:.3f} M\'.format(npm))\n        print(\'| number of paramaters score branch: {:.3f} M\'.format(nps))\n        print(\'| number of paramaters total: {:.3f} M\'.format(npt + nps + npm))\n\n    def forward(self, x):\n        feat = self.trunk(x)\n        if self.context:\n            feat = self.crop_trick(feat)\n        mask = self.maskBranch(feat)\n        score = self.scoreBranch(feat)\n        return mask, score\n\n    def creatTrunk(self):\n        resnet50 = torchvision.models.resnet50(pretrained=True)\n        trunk1 = nn.Sequential(*list(resnet50.children())[:-3])\n        trunk2 = nn.Sequential(\n            nn.Conv2d(1024, 128, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 512, self.fSz)\n        )\n        return nn.Sequential(trunk1, trunk2)\n\n    def createMaskBranch(self):\n        maskBranch = nn.Sequential(\n            nn.Conv2d(512, self.config.oSz**2, 1),\n            Reshape(self.config.oSz),\n        )\n        if self.config.gSz > self.config.oSz:\n            upSample = nn.UpsamplingBilinear2d(size=[self.config.gSz, self.config.gSz])\n            maskBranch = nn.Sequential(maskBranch, upSample)\n        return maskBranch\n\n    def createScoreBranch(self):\n        scoreBranch = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Conv2d(512, 1024, 1),\n            nn.Threshold(0, 1e-6),  # do not know why\n            nn.Dropout(0.5),\n            nn.Conv2d(1024, 1, 1),\n        )\n        return scoreBranch\n\n\nif __name__ == \'__main__\':\n    a = SymmetricPad2d(3)\n    x = torch.tensor([[[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]])\n    out = a(x)\n    print(out)\n    import torch\n    Config = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\'])\n    config = Config(iSz=160, oSz=56, gSz=112)\n    model = DeepMask(config).cuda()\n    # training mode\n    x = torch.rand(32, 3, config.iSz+32, config.iSz+32).cuda()\n    pred_mask, pred_cls = model(x)\n    print(""Output (training mode)"", pred_mask.shape, pred_cls.shape)\n\n    # full image testing mode\n    model.context = False  # really important!!\n    input_size = config.iSz + model.strides * 16 + (model.context * 32)\n    x = torch.rand(8, 3, input_size, input_size).cuda()\n    pred_mask, pred_cls = model(x)\n    print(""Output (testing mode)"", pred_mask.shape, pred_cls.shape)\n'"
models/SharpMask.py,3,"b'import os\nimport torch.nn as nn\nfrom collections import namedtuple\nfrom models.DeepMask import DeepMask\nfrom utils.load_helper import load_pretrain\n\nConfig = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\', \'km\', \'ks\'])\ndefault_config = Config(iSz=160, oSz=56, gSz=160, km=32, ks=32)\n\n\nclass RefineModule(nn.Module):\n    def __init__(self, l1, l2, l3):\n        super(RefineModule, self).__init__()\n        self.layer1 = l1\n        self.layer2 = l2\n        self.layer3 = l3\n\n    def forward(self, x):\n        x1 = self.layer1(x[0])\n        x2 = self.layer2(x[1])\n\n        y = x1 + x2\n        y = self.layer3(y)\n        return y\n\n\nclass SharpMask(nn.Module):\n    def __init__(self, config=default_config, context=True):\n        super(SharpMask, self).__init__()\n        self.context = context  # with context\n        self.km, self.ks = config.km, config.ks\n        self.skpos = [6, 5, 4, 2]\n\n        deepmask = DeepMask(config)\n        deeomask_resume = os.path.join(\'exps\', \'deepmask\', \'train\', \'model_best.pth.tar\')\n        assert os.path.exists(deeomask_resume), ""Please train DeepMask first""\n        deepmask = load_pretrain(deepmask, deeomask_resume)\n        self.trunk = deepmask.trunk\n        self.crop_trick = deepmask.crop_trick\n        self.scoreBranch = deepmask.scoreBranch\n        self.maskBranchDM = deepmask.maskBranch\n        self.fSz = deepmask.fSz\n\n        self.refs = self.createTopDownRefinement()  # create refinement modules\n\n        nph = sum(p.numel() for h in self.neths for p in h.parameters()) / 1e+06\n        npv = sum(p.numel() for h in self.netvs for p in h.parameters()) / 1e+06\n        print(\'| number of paramaters net h: {:.3f} M\'.format(nph))\n        print(\'| number of paramaters net v: {:.3f} M\'.format(npv))\n        print(\'| number of paramaters total: {:.3f} M\'.format(nph + npv))\n\n    def forward(self, x):\n        inps = list()\n        for i, l in enumerate(self.trunk.children()):\n            for j, ll in enumerate(l.children()):\n                x = ll(x)\n                if i == 0 and j == (len(l)-1) and self.context:\n                    x = self.crop_trick(x)\n                # print(x.shape)\n                if i == 0 and j in self.skpos:\n                    inps.append(x)\n        # forward refinement modules\n        currentOutput = self.refs[0](x)\n        for k in range(len(self.refs)-2):\n            x_f = inps[-(k+1)]\n            currentOutput = self.refs[k+1]((x_f, currentOutput))\n        currentOutput = self.refs[-1](currentOutput)\n        return currentOutput, self.scoreBranch(x)\n\n    def train(self, mode=True):\n        self.training = mode\n        if mode:\n            for module in self.children():\n                module.train(False)\n            for module in self.refs.children():\n                module.train(mode)\n        else:\n            for module in self.children():\n                module.train(mode)\n        return self\n\n    def createHorizontal(self):\n        neths = nn.ModuleList()\n        nhu1, nhu2, crop = 0, 0, 0\n        for i in range(len(self.skpos)):\n            h = []\n            nInps = self.ks // 2 ** i\n            if i == 0:\n                nhu1, nhu2, crop = 1024, 64, 0 if self.context else 0\n            elif i == 1:\n                nhu1, nhu2, crop = 512, 64, -2 if self.context else 0\n            elif i == 2:\n                nhu1, nhu2, crop = 256, 64, -4 if self.context else 0\n            elif i == 3:\n                nhu1, nhu2, crop = 64, 64, -8 if self.context else 0\n            if crop != 0:\n                h.append(nn.ZeroPad2d(crop))\n            h.append(nn.ReflectionPad2d(1))\n            h.append(nn.Conv2d(nhu1, nhu2, 3))\n            h.append(nn.ReLU(inplace=True))\n\n            h.append(nn.ReflectionPad2d(1))\n            h.append(nn.Conv2d(nhu2, nInps, 3))\n            h.append(nn.ReLU(inplace=True))\n\n            h.append(nn.ReflectionPad2d(1))\n            h.append(nn.Conv2d(nInps, nInps // 2, 3))\n\n            neths.append(nn.Sequential(*h))\n        return neths\n\n    def createVertical(self):\n        netvs = nn.ModuleList()\n        netvs.append(nn.ConvTranspose2d(512, self.km, self.fSz))\n\n        for i in range(len(self.skpos)):\n            netv = []\n            nInps = self.km // 2 ** i\n            netv.append(nn.ReflectionPad2d(1))\n            netv.append(nn.Conv2d(nInps, nInps, 3))\n            netv.append(nn.ReLU(inplace=True))\n\n            netv.append(nn.ReflectionPad2d(1))\n            netv.append(nn.Conv2d(nInps, nInps // 2, 3))\n\n            netvs.append(nn.Sequential(*netv))\n\n        return netvs\n\n    def refinement(self, neth, netv):\n        return RefineModule(neth, netv,\n                            nn.Sequential(nn.ReLU(inplace=True),\n                             nn.UpsamplingNearest2d(scale_factor=2)))\n\n    def createTopDownRefinement(self):\n        # create horizontal nets\n        self.neths = self.createHorizontal()\n\n        # create vertical nets\n        self.netvs = self.createVertical()\n\n        refs = nn.ModuleList()\n        refs.append(self.netvs[0])\n        for i in range(len(self.skpos)):\n            refs.append(self.refinement(self.neths[i], self.netvs[i+1]))\n        refs.append(nn.Sequential(nn.ReflectionPad2d(1),\n                                  nn.Conv2d(self.km // 2 ** (len(refs)-1), 1, 3)))\n\n        return refs\n\n\nif __name__ == \'__main__\':\n    import torch\n    Config = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\', \'km\', \'ks\'])\n    config = Config(iSz=160, oSz=56, gSz=160, km=32, ks=32)\n    model = SharpMask(config).cuda()\n    # training mode\n    x = torch.rand(32, 3, config.iSz+32, config.iSz+32).cuda()\n    pred_mask = model(x, True)\n    print(""Output (training mode)"", pred_mask.shape)\n\n    # full image testing mode\n    # x = torch.rand(8, 3, config.iSz + 160, config.iSz + 160).cuda()\n    # pred_mask, pred_cls = model(x, train=False)\n    # print(""Output (testing mode)"", pred_mask.shape, pred_cls.shape)\n'"
models/__init__.py,0,b'from .DeepMask import DeepMask\nfrom .SharpMask import SharpMask\n'
tools/InferDeepMask.py,3,"b""import time\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\n\nimport torchvision.utils as vutils  # visualization\nimport matplotlib.pyplot as plt  # visualization\n\n\nclass Infer(object):\n    def __init__(self, model, nps=500, scales=(1.,),\n                 meanstd={'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]},\n                 iSz=160, device='cpu', timer=True):\n        self.trunk = model.trunk\n        self.mHead = model.maskBranch\n        self.sHead = model.scoreBranch\n        self.nps = nps\n        self.mean = torch.from_numpy(np.array(meanstd['mean']).astype(np.float32)).view(1, 3, 1, 1).to(device)\n        self.std = torch.from_numpy(np.array(meanstd['std']).astype(np.float32)).view(1, 3, 1, 1).to(device)\n        self.iSz, self.bw = iSz, iSz // 2\n        self.device = device\n        self.timer = np.zeros(6)\n        self.display_time = timer\n\n        self.scales = scales\n        self.pyramid = [nn.UpsamplingBilinear2d(scale_factor=s).to(device) for s in self.scales]\n\n    def forward(self, img):\n        tic = time.time()\n        imgPyramid = [pyramid(img) for pyramid in self.pyramid]\n        self.timer[0] = time.time() - tic\n\n        self.mask, self.score = [], []\n        for inp in imgPyramid:\n            tic = time.time()\n            imgPad = nn.ConstantPad2d(self.bw, 0.5).cuda()(inp)\n            # cv2.imshow('pad image', np.transpose(imgPad.squeeze().cpu().data.numpy(), axes=(1, 2, 0))[:,::-1])\n            # cv2.waitKey(0)\n            imgPad = imgPad.sub_(self.mean).div_(self.std)\n            self.timer[1] += time.time() - tic\n\n            tic = time.time()\n            outTrunk = self.trunk(imgPad)\n            self.timer[2] += time.time() - tic\n\n            tic = time.time()\n            outMask = self.mHead(outTrunk)\n            self.timer[3] += time.time() - tic\n\n            tic = time.time()\n            outScore = self.sHead(outTrunk)\n            self.timer[4] += time.time() - tic\n\n            # mask_show = vutils.make_grid(outMask.sigmoid().transpose(0, 1), nrow=outScore.shape[-1], pad_value=0)\n            # mask_show_numpy = np.transpose(mask_show.cpu().data.numpy(), axes=(1, 2, 0))\n            # plt.imshow(mask_show_numpy[:,:,0], cmap='jet')\n            # plt.show()\n            self.mask.append(outMask.sigmoid().cpu().data.numpy())\n            self.score.append(outScore.sigmoid().cpu().data.numpy())\n\n    def getTopScores(self):\n        li = []\n        for i, sc in enumerate(self.score):\n            h, w = sc.shape[2:]\n            sc = sc.flatten()\n            sIds, sS = zip(*sorted(enumerate(sc), key=lambda a: a[1], reverse=True))\n            for ss, sid in zip(sS, sIds):\n                li.append([ss, i, sid, sid//w, sid % w])\n        li = sorted(li, key=lambda l: l[0], reverse=True)\n        topScores = li[:self.nps]\n        self.topScores = topScores\n\n    def crop_hwc(self, image, bbox, out_sz, padding=0):\n        a = (out_sz[0]-1) / bbox[2]\n        b = (out_sz[1]-1) / bbox[3]\n        c = -a * bbox[0]\n        d = -b * bbox[1]\n        mapping = np.array([[a, 0, c],\n                            [0, b, d]]).astype(np.float)\n        crop = cv2.warpAffine(image, mapping, (out_sz[0], out_sz[1]),\n                              borderMode=cv2.BORDER_CONSTANT,\n                              borderValue=padding)\n        return crop\n\n    def getTopMasks(self, thr, h, w):\n        masks, ts, nps = self.mask, self.topScores, self.nps\n        # thr = np.log(thr / (1 - thr))\n        topMasks = np.zeros((h, w, nps), dtype=np.uint8)\n        topScores = np.zeros(nps)\n        for i in range(nps):\n            scale, sid, x, y = ts[i][1:]\n            s = self.scales[scale]\n            mask = masks[scale][0, sid]\n            mask = cv2.resize(mask, (self.iSz, self.iSz))\n            # cv2.imshow('mask', mask)\n            # cv2.waitKey(0)\n            imgMask = self.crop_hwc(mask, (self.bw - 16*y, self.bw - 16*x, w*s, h*s), (w, h))\n            imgMask = imgMask > thr\n\n            topMasks[:, :, i] = imgMask.copy()\n            topScores[i] = ts[i][0]\n        return topMasks, topScores\n\n    def getTopProps(self, thr, h, w):\n        tic = time.time()\n\n        self.getTopScores()\n        topMasks, topScores = self.getTopMasks(thr, h, w)\n\n        self.timer[5] = time.time() - tic\n        if self.display_time:\n            self.printTiming()\n        return topMasks, topScores\n\n    def printTiming(self):\n        t = self.timer\n        print('| time pyramid: {:.1f} ms'.format(t[0]*1000))\n        print('| time pre-process: {:.1f} ms'.format(t[1]*1000))\n        print('| time trunk: {:.1f} ms'.format(t[2]*1000))\n        print('| time mask branch: {:.1f} ms'.format(t[3]*1000))\n        print('| time score branch: {:.1f} ms'.format(t[4]*1000))\n        print('| time post processing: {:.1f} ms'.format(t[5]*1000))\n        print('| time total: {:.1f} ms'.format(sum(t)*1000))\n"""
tools/computeProposals.py,2,"b'import argparse\nimport models\nimport numpy as np\nimport time\nimport cv2\nfrom PIL import Image\nimport torch\nfrom tools.InferDeepMask import Infer\nfrom utils.load_helper import load_pretrain\n\nmodel_names = sorted(name for name in models.__dict__\n                     if not name.startswith(""__"") and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch DeepMask/SharpMask evaluation\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'DeepMask\', choices=model_names,\n                    help=\'model architecture: \' + \' | \'.join(model_names) + \' (default: DeepMask)\')\nparser.add_argument(\'--resume\', default=\'exps/deepmask/train/model_best.pth.tar\',\n                    type=str, metavar=\'PATH\', help=\'path to checkpoint\')\nparser.add_argument(\'--img\', default=\'data/testImage.jpg\',\n                    help=\'path/to/test/image\')\nparser.add_argument(\'--nps\', default=10, type=int,\n                    help=\'number of proposals to save in test\')\nparser.add_argument(\'--si\', default=-2.5, type=float, help=\'initial scale\')\nparser.add_argument(\'--sf\', default=.5, type=float, help=\'final scale\')\nparser.add_argument(\'--ss\', default=.5, type=float, help=\'scale step\')\n\n\ndef range_end(start, stop, step=1):\n    return np.arange(start, stop+step, step)\n\n\ndef main():\n    global args\n    args = parser.parse_args()\n    # Setup device\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # Setup Model\n    from collections import namedtuple\n    Config = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\', \'batch\'])\n    config = Config(iSz=160, oSz=56, gSz=112, batch=1)  # default for training\n\n    model = (models.__dict__[args.arch](config))\n    model = load_pretrain(model, args.resume)\n    model = model.eval().to(device)\n\n    scales = [2**i for i in range_end(args.si, args.sf, args.ss)]\n    meanstd = {\'mean\':[0.485, 0.456, 0.406], \'std\':[0.229, 0.224, 0.225]}\n    infer = Infer(nps=args.nps, scales=scales, meanstd=meanstd, model=model, device=device)\n\n    print(\'| start\'); tic = time.time()\n    im = np.array(Image.open(args.img).convert(\'RGB\'), dtype=np.float32)\n    h, w = im.shape[:2]\n    img = np.expand_dims(np.transpose(im, (2, 0, 1)), axis=0).astype(np.float32)\n    img = torch.from_numpy(img / 255.).to(device)\n    infer.forward(img)\n    masks, scores = infer.getTopProps(.2, h, w)\n    toc = time.time() - tic\n    print(\'| done in %05.3f s\' % toc)\n\n    for i in range(masks.shape[2]):\n        res = im[:,:,::-1].copy().astype(np.uint8)\n        res[:, :, 2] = masks[:, :, i] * 255 + (1 - masks[:, :, i]) * res[:, :, 2]\n\n        mask = masks[:, :, i].astype(np.uint8)\n        _, contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        cnt_area = [cv2.contourArea(cnt) for cnt in contours]\n        cnt_max_id = np.argmax(cnt_area)\n        contour = contours[cnt_max_id]\n        polygons = contour.reshape(-1, 2)\n\n        predict_box = cv2.boundingRect(polygons)\n        predict_rbox = cv2.minAreaRect(polygons)\n        rbox = cv2.boxPoints(predict_rbox)\n        print(\'Segment Proposal Score: {:.3f}\'.format(scores[i]))\n\n        res = cv2.rectangle(res, (predict_box[0], predict_box[1]),\n                      (predict_box[0]+predict_box[2], predict_box[1]+predict_box[3]), (0, 255, 0), 3)\n        res = cv2.polylines(res, [np.int0(rbox)], True, (0, 255, 255), 3)\n        cv2.imshow(\'Proposal\', res)\n        cv2.waitKey(0)\n\n\nif __name__ == \'__main__\':\n    main()'"
tools/dasiamrpn_deepmask.py,3,"b'from tracker.net import SiamRPNvot\nfrom tracker.run_SiamRPN import SiamRPN_init, SiamRPN_track\n\nimport glob\nimport time\nimport numpy as np\nimport cv2\nimport torch\nfrom models import DeepMask\nfrom collections import namedtuple\nfrom utils.load_helper import load_pretrain\n\nimport matplotlib.pyplot as plt  # visualization\nimport matplotlib.patches as patches  # visualization\n\nVISUALIZATION=True\n\n\ndef crop_back(image, bbox, out_sz, padding=0):\n    a = (out_sz[0]-1) / bbox[2]\n    b = (out_sz[1]-1) / bbox[3]\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz[0], out_sz[1]),\n                          flags=cv2.INTER_LANCZOS4,\n                          borderMode=cv2.BORDER_CONSTANT,\n                          borderValue=padding)\n    return crop\n\n\ndef crop_chw(image, bbox, out_sz, padding=0):\n    a = (out_sz-1) / bbox[2]\n    b = (out_sz-1) / bbox[3]\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz),\n                          borderMode=cv2.BORDER_CONSTANT,\n                          borderValue=padding)\n    return np.transpose(crop, (2, 0, 1))\n\n\ndef vos(tracker_net, mask_net, image_files, box):\n\n    # tracker init\n    target_pos = np.array([box[0]+box[2]/2, box[1]+box[3]/2])\n    target_sz = np.array([box[2], box[3]])\n    im = cv2.imread(image_files[0])  # HxWxC\n    state = SiamRPN_init(im, target_pos, target_sz, tracker_net)\n\n    out_size = 160\n    crop_size = 192\n    context_amount = 224 / 128\n    mean = np.array([[[0.485]], [[0.456]], [[0.406]]]).astype(np.float32)\n    std = np.array([[[0.229]], [[0.224]], [[0.225]]]).astype(np.float32)\n\n    toc = 0\n    for f, image_file in enumerate(image_files):\n        im = cv2.imread(image_file)\n        tic = cv2.getTickCount()\n        state = SiamRPN_track(state, im)  # track\n\n        # STAGE2: DeepMask Segmentation\n        im_ = (im[:, :, ::-1]).astype(np.float32) / 255.\n\n        target_pos, target_sz = state[\'target_pos\'], state[\'target_sz\']\n\n        max_sz = max(target_sz)\n        s_z = context_amount * max_sz\n        s_z_c = s_z / out_size * crop_size\n        box = [target_pos[0] - s_z / 2, target_pos[1] - s_z / 2, s_z, s_z]\n        box_crop = [target_pos[0] - s_z_c / 2, target_pos[1] - s_z_c / 2, s_z_c, s_z_c]\n        x_crop = crop_chw(im_, box_crop, crop_size, (0.5, 0.5, 0.5))\n\n        x_crop_norm = (x_crop - mean) / std\n        xs_torch = torch.from_numpy(np.expand_dims(x_crop_norm, axis=0))\n\n        mask, score = mask_net(xs_torch.cuda())\n        mask = mask.sigmoid().squeeze().cpu().data.numpy()\n        # score = score.sigmoid().squeeze().cpu().data.numpy()\n\n        im_w, im_h = im.shape[1], im.shape[0]\n        s = out_size / box[2]\n        back_box = [-box[0] * s, -box[1] * s, im_w * s, im_h * s]\n        mask_in_img = crop_back(mask, back_box, (im_w, im_h))\n        toc += cv2.getTickCount() - tic\n\n        if VISUALIZATION:\n            plt.cla()\n            im_save = im\n            plt.imshow(im_save[:, :, ::-1])\n\n            mask = (mask_in_img > 0.3).astype(np.uint8)  # threshold!\n            _, contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n            for contour in contours:\n                edge = patches.Polygon(contour.reshape(-1, 2), linewidth=2, edgecolor=\'lawngreen\',\n                                       facecolor=\'none\')\n                ax.add_patch(edge)\n                edge = patches.Polygon(contour.reshape(-1, 2), linewidth=2, facecolor=\'lawngreen\',\n                                       alpha=0.5)\n                ax.add_patch(edge)\n\n            plt.axis(\'off\')\n            plt.subplots_adjust(.0, .0, 1, 1)\n            plt.draw()\n            plt.pause(0.01)\n\n        # if f % 3 == 1:\n        #     plt.savefig(\'%05d.jpg\' % f)\n\n    return toc/cv2.getTickFrequency()\n\n\nif __name__ == ""__main__"":\n    Config = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\'])\n    default_config = Config(iSz=160, oSz=56, gSz=160)\n    mask_net = DeepMask(default_config)\n    mask_net = load_pretrain(mask_net, \'./pretrained/deepmask/DeepMask.pth.tar\')\n    mask_net = mask_net.eval().cuda()\n\n    # load net\n    tracker_net = SiamRPNvot()\n    tracker_net.load_state_dict(torch.load(\'SiamRPNVOT.model\'))\n    tracker_net.eval().cuda()\n\n    image_files = sorted(glob.glob(\'./tracker/bag/*.jpg\'))\n    tic = time.time()\n    if VISUALIZATION:\n        try:\n            fig\n        except NameError:\n            fig, ax = plt.subplots(1)\n\n    cv2.namedWindow(""SiamMask"", cv2.WND_PROP_FULLSCREEN)\n    cv2.setWindowProperty(""SiamMask"", cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n    try:\n        init_rect = cv2.selectROI(\'SiamMask\', cv2.imread(image_files[0]), False, False)\n        x, y, w, h = init_rect\n        if not (x | y | w | h): exit()\n    except:\n        exit()\n\n    toc = vos(tracker_net, mask_net, image_files, init_rect)\n    print(\'Speed: {:.1f} FPS and {:.1f}s\'.format(len(image_files)/toc, toc))\n    \n\n'"
tools/evalPerImage.py,2,"b'import argparse\nimport models\nimport numpy as np\nimport time\nimport json\nfrom os.path import join, isdir\nfrom os import makedirs\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nfrom loader.pycocotools.mask import encode\nfrom loader.pycocotools.coco import COCO\nfrom loader.pycocotools.cocoeval import COCOeval\nfrom tools.InferDeepMask import Infer\nfrom utils.load_helper import load_pretrain\n\nmodel_names = sorted(name for name in models.__dict__\n                     if not name.startswith(""__"") and callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch full scene evaluation of DeepMask/SharpMask\')\n\nparser.add_argument(\'--rundir\', default=\'./exps/\', help=\'experiments directory\')\nparser.add_argument(\'--datadir\', default=\'data/coco\', help=\'data directory\')\nparser.add_argument(\'--split\', default=\'val\', help=\'dataset split to be used (train/val)\')\nparser.add_argument(\'--nps\', default=1000, type=int, help=\'number of proposals\')\nparser.add_argument(\'--thr\', default=.2, type=float, help=\'mask binary threshold\')\nparser.add_argument(\'--smin\', default=-2.5, type=float, help=\'min scale\')\nparser.add_argument(\'--smax\', default=1, type=float, help=\'max scale\')\nparser.add_argument(\'--sstep\', default=.5, type=float, help=\'scale step\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'DeepMask\', choices=model_names,\n                    help=\'model architecture: \' + \' | \'.join(model_names) + \' (default: DeepMask)\')\nparser.add_argument(\'--resume\', default=\'exps/deepmask/train/model_best.pth.tar\', help=\'model to load\')\n\n\ndef range_end(start, stop, step=1):\n    return np.arange(start, stop+step, step)\n\n\ndef main():\n    global args\n    args = parser.parse_args()\n    # Setup device\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # Setup Model\n    from collections import namedtuple\n    Config = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\', \'batch\'])\n    config = Config(iSz=160, oSz=56, gSz=112, batch=1)  # default for training\n\n    model = (models.__dict__[args.arch](config))\n    model = load_pretrain(model, args.resume)\n    model = model.eval().to(device)\n\n    scales = [2**i for i in range_end(args.smin, args.smax, args.sstep)]\n    meanstd = {\'mean\':[0.485, 0.456, 0.406], \'std\':[0.229, 0.224, 0.225]}\n    infer = Infer(nps=args.nps, scales=scales, meanstd=meanstd, model=model, device=device, timer=False)\n\n    annFile = \'{}/annotations/instances_{}2017.json\'.format(args.datadir, args.split)\n    coco = COCO(annFile)\n    imgIds = coco.getImgIds()\n    imgIds = sorted(imgIds)  # [:500] for fast test\n    segm_props = []\n    print(\'| start eval\'); tic = time.time()\n    for k, imgId in enumerate(imgIds):\n        ann = coco.loadImgs(imgId)[0]\n        fileName = ann[\'file_name\']\n        pathImg = \'{}/{}2017/{}\'.format(args.datadir, args.split, fileName)\n        im = np.array(Image.open(pathImg).convert(\'RGB\'), dtype=np.float32)\n        h, w = im.shape[:2]\n        img = np.expand_dims(np.transpose(im, (2, 0, 1)), axis=0).astype(np.float32)\n        img = torch.from_numpy(img / 255.).to(device)\n\n        infer.forward(img)\n        masks, scores = infer.getTopProps(args.thr, h, w)\n\n        enc = encode(np.asfortranarray(masks))\n        for i in range(args.nps):\n            enc[i][\'counts\'] = enc[i][\'counts\'].decode(\'utf-8\')\n            elem = {\'segmentation\': enc[i], \'image_id\': imgId, \'category_id\': 1, \'score\': scores[i]}\n            segm_props.append(elem)\n\n        # for i in range(args.nps):\n        #     res = im[:, :, ::-1].copy().astype(np.uint8)\n        #     res[:, :, 2] = masks[:, :, i] * 255 + (1 - masks[:, :, i]) * res[:, :, 2]\n        #\n        #     mask = masks[i].astype(np.uint8)\n        #     _, contour, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        #     polygons = [c.reshape(-1, 2) for c in contour]\n        #\n        #     predict_box = cv2.boundingRect(polygons[0])\n        #     predict_rbox = cv2.minAreaRect(polygons[0])\n        #     box = cv2.boxPoints(predict_rbox)\n        #     print(\'Segment Proposal Score: {:.3f}\'.format(scores[i]))\n        #\n        #     res = cv2.rectangle(res, (predict_box[0], predict_box[1]),\n        #                         (predict_box[0] + predict_box[2], predict_box[1] + predict_box[3]), (0, 255, 0), 3)\n        #     res = cv2.polylines(res, [np.int0(box)], True, (0, 255, 255), 3)\n        #     cv2.imshow(\'Proposal\', res)\n        #     cv2.waitKey(0)\n        # plt.imshow(im); plt.axis(\'off\')\n        # coco.showAnns([props[-1]])\n        # plt.show()\n        if (k+1) % 10 == 0:\n            toc = time.time() - tic\n            print(\'| process %05d in %010.3f s\' % (k+1, toc))\n\n    toc = time.time() - tic\n    print(\'| finish in %010.3f s\' % toc)\n\n    pathsv = \'sharpmask/eval_coco\' if args.arch == \'SharpMask\' else \'deepmask/eval_coco\'\n    args.rundir = join(args.rundir, pathsv)\n    try:\n        if not isdir(args.rundir):\n            makedirs(args.rundir)\n    except OSError as err:\n        print(err)\n\n    result_path = join(args.rundir, \'segm_proposals.json\')\n    with open(result_path, \'w\') as outfile:\n        json.dump(segm_props, outfile)\n\n    cocoDt = coco.loadRes(result_path)\n\n    print(\'\\n\\nBox Proposals Evalution\\n\\n\')\n    cocoEval = COCOeval(coco, cocoDt)\n\n    max_dets = [10, 100, 1000]\n    useSegm = False\n    useCats = False\n\n    cocoEval.params.imgIds = imgIds\n    cocoEval.params.maxDets = max_dets\n    cocoEval.params.useSegm = useSegm\n    cocoEval.params.useCats = useCats\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n    print(\'\\n\\nSegmentation Proposals Evalution\\n\\n\')\n    cocoEval = COCOeval(coco, cocoDt)\n\n    max_dets = [10, 100, 1000]\n    useSegm = True\n    useCats = False\n\n    cocoEval.params.imgIds = imgIds\n    cocoEval.params.maxDets = max_dets\n    cocoEval.params.useSegm = useSegm\n    cocoEval.params.useCats = useCats\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train.py,21,"b'import os\nimport time\nfrom os import makedirs\nfrom os.path import isdir, join\nimport argparse\nimport logging\nimport random\nimport shutil\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data\nimport torch.backends.cudnn as cudnn\nfrom torch.optim.lr_scheduler import MultiStepLR\n\nimport models\nfrom loader import get_loader, dataset_names\nfrom utils.log_helper import init_log, print_speed, add_file_handler\nimport matplotlib.pyplot as plt  # visualization\nimport torchvision.utils as vutils  # visualization\nimport colorama\nfrom tensorboardX import SummaryWriter\n\nmodel_names = sorted(name for name in models.__dict__\n                     if not name.startswith(""__"") and\n                     callable(models.__dict__[name]))\n\nparser = argparse.ArgumentParser(description=\'PyTorch DeepMask/SharpMask Training\')\nparser.add_argument(\'--rundir\', default=\'./exps/\', help=\'experiments directory\')\nparser.add_argument(\'--dataset\', default=\'coco\', choices=dataset_names(),\n                    help=\'data set\')\nparser.add_argument(\'--seed\', default=1, type=int, help=\'manually set RNG seed\')\nparser.add_argument(\'-j\', \'--workers\', default=12, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 12)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-b\', \'--batch\', default=32, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 32)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=1e-3, type=float,\n                    metavar=\'LR\', help=\'learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 5e-4)\')\nparser.add_argument(\'--maxload\', default=4000, type=int, metavar=\'N\',\n                    help=\'max number of training batches per epoch\')\nparser.add_argument(\'--testmaxload\', default=500, type=int, metavar=\'N\',\n                    help=\'max number of testing batches\')\nparser.add_argument(\'--maxepoch\', default=300, type=int, metavar=\'N\',\n                    help=\'max number of training epochs\')\nparser.add_argument(\'--iSz\', default=160, type=int, metavar=\'N\',\n                    help=\'input size\')\nparser.add_argument(\'--oSz\', default=56, type=int, metavar=\'N\',\n                    help=\'output size\')\nparser.add_argument(\'--gSz\', default=112, type=int, metavar=\'N\',\n                    help=\'ground truth size\')\nparser.add_argument(\'--shift\', default=16, type=int, metavar=\'N\',\n                    help=\'shift jitter allowed\')\nparser.add_argument(\'--scale\', default=.25, type=float,\n                    help=\'scale jitter allowed\')\nparser.add_argument(\'--hfreq\', default=.5, type=float,\n                    help=\'mask/score head sampling frequency\')\nparser.add_argument(\'--scratch\', action=\'store_true\',\n                    help=\'train DeepMask with randomly initialize weights\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'DeepMask\',\n                    choices=model_names,\n                    help=\'model architecture: \' + \' | \'.join(model_names) +\n                         \' (default: DeepMask)\')\nparser.add_argument(\'--km\', default=32, type=int, help=\'km\')\nparser.add_argument(\'--ks\', default=32, type=int, help=\'ks\')\nparser.add_argument(\'--freeze_bn\', action=\'store_true\',\n                    help=\'freeze running statistics in BatchNorm layers during training (default: False)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--print-freq\', \'-p\', default=20, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 20)\')\nparser.add_argument(\'-v\', \'--visualize\', action=\'store_true\',\n                    help=\'visualize the result heatmap\')\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass BinaryMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.acc = 0\n        self.n = 0\n\n    def add(self, output, target):\n        target, output = target.squeeze(), output.squeeze()\n        assert output.numel() == target.numel(), \'target and output do not match\'\n\n        acc = torch.mul(output, target)\n        self.acc += acc.ge(0).sum().item()\n        self.n += output.size(0)\n\n    def value(self):\n        res = self.acc / self.n if self.n > 0 else 0\n        return res*100\n\n\nclass IouMeter(object):\n    def __init__(self, thr, sz):\n        self.sz = sz\n        self.iou = torch.zeros(sz, dtype=torch.float32)\n        self.thr = np.log(thr / (1 - thr))\n        self.reset()\n\n    def reset(self):\n        self.iou.zero_()\n        self.n = 0\n\n    def add(self, output, target):\n        target, output = target.squeeze(), output.squeeze()\n        assert output.numel() == target.numel(), \'target and output do not match\'\n\n        batch, h, w = output.shape\n        pred = output.ge(self.thr)\n        mask_sum = pred.eq(1).add(target.eq(1))\n        intxn = torch.sum(mask_sum == 2, dim=(1, 2)).float()\n        union = torch.sum(mask_sum > 0, dim=(1, 2)).float()\n        for i in range(batch):\n            if union[i].item() > 0:\n                self.iou[self.n+i] = intxn[i].item() / union[i].item()\n        self.n += batch\n\n    def value(self, s):\n        nb = max(self.iou.ne(0).sum(), 1)\n        iou = self.iou.narrow(0, 0, nb)\n\n        def is_number(s):\n            try:\n                float(s)\n                return True\n            except ValueError:\n                return False\n        if s == \'mean\':\n            res = iou.mean().item()\n        elif s == \'var\':\n            res = iou.var().item()\n        elif s == \'median\':\n            res = iou.median().squeeze().item()\n        elif is_number(s):\n            iou_sort, _ = iou.sort()\n            res = iou_sort.ge(float(s)).sum().float().item() / float(nb)\n        return res * 100\n\n\ndef visual_batch(img, pred_mask, label=None):\n    img_show = vutils.make_grid(img, normalize=True, scale_each=True)\n    img_show_numpy = np.transpose(img_show.cpu().data.numpy(), axes=(1, 2, 0))\n\n    iSz_res = torch.nn.functional.interpolate(pred_mask, size=(args.iSz, args.iSz))\n    pad_res = torch.nn.functional.pad(iSz_res, (16, 16, 16, 16))\n    mask_show = vutils.make_grid(pad_res, scale_each=True)\n    mask_show_numpy = np.transpose(mask_show.cpu().data.numpy(), axes=(1, 2, 0))\n\n    if str(type(label)).find(\'torch\'):\n        iSz_label = torch.nn.functional.interpolate(label, size=(args.iSz, args.iSz))\n        pad_label = torch.nn.functional.pad(iSz_label, (16, 16, 16, 16), value=-1)\n        label_show = vutils.make_grid(pad_label, normalize=True, scale_each=True)\n        label_show_numpy = np.transpose(label_show.cpu().data.numpy(), axes=(1, 2, 0))\n\n        fig, (ax1, ax2) = plt.subplots(nrows=2)\n        ax1.imshow(img_show_numpy)\n        ax1.imshow(mask_show_numpy[:, :, 0], alpha=.5, cmap=\'jet\')\n        # ax1.imshow(mask_show_numpy[:, :, 0] > 0.5, alpha=.5)\n        ax1.axis(\'off\')\n        ax2.imshow(img_show_numpy)\n        ax2.imshow(label_show_numpy, alpha=.5)\n        ax2.axis(\'off\')\n    else:\n        plt.imshow(img_show_numpy)\n        plt.imshow(mask_show_numpy[:, :, 0], alpha=.5, cmap=\'jet\')\n        # ax1.imshow(mask_show_numpy[:, :, 0] > 0.2, alpha=.5)\n        plt.axis(\'off\')\n    plt.subplots_adjust(.05, .05, .95, .95)\n    plt.show()\n    plt.close()\n\n\ndef BNtoFixed(m):\n    # From https://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/utils/torchtools.py\n    # 1. no update for running mean and var\n    # 2. scale and shift parameters are still trainable\n    class_name = m.__class__.__name__\n    if class_name.find(\'BatchNorm\') != -1:\n        m.eval()\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    logger = logging.getLogger(\'global\')\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    mask_losses = AverageMeter()\n    score_losses = AverageMeter()\n\n    # switch to train mode\n    model.train()\n    if args.freeze_bn:\n        model.apply(BNtoFixed)\n    train_loader.dataset.shuffle()\n\n    end = time.time()\n    for i, (img, target, head_status) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        img = img.to(device)\n        target = target.to(device)\n\n        # compute output\n        output = model(img)\n        loss = criterion(output[head_status[0]], target)\n\n        # measure and record loss\n        if head_status[0] == 0:\n            mask_losses.update(loss.item(), img.size(0))\n            loss.mul_(img.size(0))\n        else:\n            score_losses.update(loss.item(), img.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()  # gradOutputs:mul(self.inputs:size(1))\n        # torch.nn.utils.clip_grad_norm_(model.parameters(), 10)  # REMOVE?\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if args.visualize and head_status[0] == 0:\n            visual_batch(img, output[0].sigmoid(), target)\n\n        if i % args.print_freq == 0:\n            logger.info(\'Epoch: [{0}][{1}/{2}]\\t\'\n                        \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                        \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\n\'\n                        \'LR {lr:.1e} \\t Mask Loss {mask_loss.val:.4f} ({mask_loss.avg:.4f})\\t\'\n                        \'Score Loss {score_loss.val:.3f} ({score_loss.avg:.3f})\'.format(\n                epoch, i, len(train_loader), batch_time=batch_time, lr=optimizer.param_groups[0][\'lr\'],\n                data_time=data_time, mask_loss=mask_losses, score_loss=score_losses))\n            print_speed(epoch * len(train_loader) + i + 1, batch_time.avg, args.maxepoch * len(train_loader))\n\n    writer.add_scalar(\'train_loss/mask_loss\', mask_losses.avg, epoch)\n    writer.add_scalar(\'train_loss/score_losses\', score_losses.avg, epoch)\n\n\ndef validate(val_loader, model, criterion, epoch=0):\n    logger = logging.getLogger(\'global\')\n    batch_time = AverageMeter()\n    mask_losses = AverageMeter()\n    score_losses = AverageMeter()\n\n    mask_meter = IouMeter(0.5, len(val_loader.dataset))\n    score_meter = BinaryMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        end = time.time()\n        for i, (img, target, head_status) in enumerate(val_loader):\n            img = img.to(device)\n            target = target.to(device)\n\n            # compute output\n            output = model(img)\n            loss = criterion(output[head_status[0]], target)\n\n            # measure accuracy and record loss\n            if head_status[0] == 0:\n                mask_losses.update(loss.item(), img.size(0))\n                mask_meter.add(output[head_status[0]], target)\n            else:\n                score_losses.update(loss.item(), img.size(0))\n                score_meter.add(output[head_status[0]], target)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n        acc = mask_meter.value(\'0.7\')\n\n        logger.info(\' * Epoch [{0}]Mask Loss {mask_loss.avg:.3f} Score Loss {mask_loss.avg:.3f}\'.format(\n            epoch, mask_loss=mask_losses, cls_loss=score_losses))\n        logger.info(\' * Epoch [%03d] | IoU: mean %05.2f median %05.2f suc@.5 %05.2f suc@.7 %05.2f \'\n                    \'| acc %05.2f | bestmodel %s\' % (epoch, mask_meter.value(\'mean\'),\n                    mask_meter.value(\'median\'), mask_meter.value(\'0.5\'), mask_meter.value(\'0.7\'),\n                    score_meter.value(), \'y\' if acc > max_acc else \'n\'))\n\n        writer.add_scalar(\'val_loss/mask_loss\', mask_losses.avg, epoch)\n        writer.add_scalar(\'val_loss/score_losses\', score_losses.avg, epoch)\n        writer.add_scalar(\'val_meter/mask_meter_mean\', mask_meter.value(\'mean\'), epoch)\n        writer.add_scalar(\'val_meter/mask_meter_median\', mask_meter.value(\'median\'), epoch)\n        writer.add_scalar(\'val_meter/mask_meter_0.5\', mask_meter.value(\'0.5\'), epoch)\n        writer.add_scalar(\'val_meter/mask_meter_0.7\', mask_meter.value(\'0.7\'), epoch)\n        writer.add_scalar(\'val_meter/score_meter\', score_meter.value(), epoch)\n\n    return acc\n\n\ndef save_checkpoint(state, is_best, file_path=\'\', filename=\'checkpoint.pth.tar\'):\n    torch.save(state, join(file_path, filename))\n    if is_best:\n        shutil.copyfile(join(file_path, filename), join(file_path, \'model_best.pth.tar\'))\n\n\ndef main():\n    global args, device, max_acc, writer\n\n    max_acc = -1\n    args = parser.parse_args()\n    if args.arch == \'SharpMask\':\n        trainSm = True\n        args.hfreq = 1\n        args.gSz = args.iSz\n    else:\n        trainSm = False\n\n    # Setup experiments results path\n    pathsv = \'sharpmask/train\' if trainSm else \'deepmask/train\'\n    args.rundir = join(args.rundir, pathsv)\n    try:\n        if not isdir(args.rundir):\n            makedirs(args.rundir)\n    except OSError as err:\n        print(err)\n\n    # Setup logger\n    init_log(\'global\', logging.INFO)\n    add_file_handler(\'global\', join(args.rundir, \'train.log\'), logging.INFO)\n    logger = logging.getLogger(\'global\')\n    logger.info(\'running in directory %s\' % args.rundir)\n    logger.info(args)\n    writer = SummaryWriter(log_dir=join(args.rundir, \'tb\'))\n\n    # Get argument defaults (hastag #thisisahack)\n    parser.add_argument(\'--IGNORE\', action=\'store_true\')\n    defaults = vars(parser.parse_args([\'--IGNORE\']))\n\n    # Print all arguments, color the non-defaults\n    for argument, value in sorted(vars(args).items()):\n        reset = colorama.Style.RESET_ALL\n        color = reset if value == defaults[argument] else colorama.Fore.MAGENTA\n        logger.info(\'{}{}: {}{}\'.format(color, argument, value, reset))\n\n    # Setup seeds\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n\n    # Setup device\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    # Setup Model\n    model = (models.__dict__[args.arch](args)).to(device)\n    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n    logger.info(model)\n\n    # Setup data loader\n    train_dataset = get_loader(args.dataset)(args, split=\'train\')\n    val_dataset = get_loader(args.dataset)(args, split=\'val\')\n    train_loader = data.DataLoader(\n        train_dataset, batch_size=args.batch, num_workers=args.workers,\n        pin_memory=True, sampler=None)\n    val_loader = data.DataLoader(\n        val_dataset, batch_size=args.batch, num_workers=args.workers,\n        pin_memory=True, sampler=None)\n\n    # Setup Metrics\n    criterion = nn.SoftMarginLoss().to(device)\n\n    # Setup optimizer, lr_scheduler and loss function\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n    scheduler = MultiStepLR(optimizer, milestones=[50, 120], gamma=0.3)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            logger.info(""loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            max_acc = checkpoint[\'max_acc\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            logger.info(""loaded checkpoint \'{}\' (epoch {})"".format(\n                args.resume, checkpoint[\'epoch\']))\n        else:\n            logger.warning(""no checkpoint found at \'{}\'"".format(args.resume))\n\n    cudnn.benchmark = True\n\n    for epoch in range(args.start_epoch, args.maxepoch):\n        scheduler.step(epoch=epoch)\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        if epoch % 2 == 1:\n            acc = validate(val_loader, model, criterion, epoch)\n\n            is_best = acc > max_acc\n            max_acc = max(acc, max_acc)\n            # remember best mean loss and save checkpoint\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'max_acc\': max_acc,\n                \'optimizer\': optimizer.state_dict(),\n            }, is_best, args.rundir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/yolo_deepmask.py,2,"b'from ctypes import *\nimport random\n\nimport glob\nimport time\nimport numpy as np\nimport cv2\nimport torch\nfrom models import DeepMask\nfrom collections import namedtuple\nfrom utils.load_helper import load_pretrain\n\nimport matplotlib.pyplot as plt  # visualization\nimport matplotlib.patches as patches  # visualization\n\nVISUALIZATION=True\n\ndef sample(probs):\n    s = sum(probs)\n    probs = [a/s for a in probs]\n    r = random.uniform(0, 1)\n    for i in range(len(probs)):\n        r = r - probs[i]\n        if r <= 0:\n            return i\n    return len(probs)-1\n\n\ndef c_array(ctype, values):\n    arr = (ctype*len(values))()\n    arr[:] = values\n    return arr\n\n\nclass BOX(Structure):\n    _fields_ = [(""x"", c_float),\n                (""y"", c_float),\n                (""w"", c_float),\n                (""h"", c_float)]\n\n\nclass DETECTION(Structure):\n    _fields_ = [(""bbox"", BOX),\n                (""classes"", c_int),\n                (""prob"", POINTER(c_float)),\n                (""mask"", POINTER(c_float)),\n                (""objectness"", c_float),\n                (""sort_class"", c_int)]\n\n\nclass IMAGE(Structure):\n    _fields_ = [(""w"", c_int),\n                (""h"", c_int),\n                (""c"", c_int),\n                (""data"", POINTER(c_float))]\n\n\nclass METADATA(Structure):\n    _fields_ = [(""classes"", c_int),\n                (""names"", POINTER(c_char_p))]\n\n\nlib = CDLL(""./darknet/libdarknet.so"", RTLD_GLOBAL)\nlib.network_width.argtypes = [c_void_p]\nlib.network_width.restype = c_int\nlib.network_height.argtypes = [c_void_p]\nlib.network_height.restype = c_int\n\npredict = lib.network_predict\npredict.argtypes = [c_void_p, POINTER(c_float)]\npredict.restype = POINTER(c_float)\n\nset_gpu = lib.cuda_set_device\nset_gpu.argtypes = [c_int]\n\nmake_image = lib.make_image\nmake_image.argtypes = [c_int, c_int, c_int]\nmake_image.restype = IMAGE\n\nget_network_boxes = lib.get_network_boxes\nget_network_boxes.argtypes = [c_void_p, c_int, c_int, c_float, c_float, POINTER(c_int), c_int, POINTER(c_int)]\nget_network_boxes.restype = POINTER(DETECTION)\n\nmake_network_boxes = lib.make_network_boxes\nmake_network_boxes.argtypes = [c_void_p]\nmake_network_boxes.restype = POINTER(DETECTION)\n\nfree_detections = lib.free_detections\nfree_detections.argtypes = [POINTER(DETECTION), c_int]\n\nfree_ptrs = lib.free_ptrs\nfree_ptrs.argtypes = [POINTER(c_void_p), c_int]\n\nnetwork_predict = lib.network_predict\nnetwork_predict.argtypes = [c_void_p, POINTER(c_float)]\n\nreset_rnn = lib.reset_rnn\nreset_rnn.argtypes = [c_void_p]\n\nload_net = lib.load_network\nload_net.argtypes = [c_char_p, c_char_p, c_int]\nload_net.restype = c_void_p\n\ndo_nms_obj = lib.do_nms_obj\ndo_nms_obj.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]\n\ndo_nms_sort = lib.do_nms_sort\ndo_nms_sort.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]\n\nfree_image = lib.free_image\nfree_image.argtypes = [IMAGE]\n\nletterbox_image = lib.letterbox_image\nletterbox_image.argtypes = [IMAGE, c_int, c_int]\nletterbox_image.restype = IMAGE\n\nload_meta = lib.get_metadata\nlib.get_metadata.argtypes = [c_char_p]\nlib.get_metadata.restype = METADATA\n\nload_image = lib.load_image_color\nload_image.argtypes = [c_char_p, c_int, c_int]\nload_image.restype = IMAGE\n\nrgbgr_image = lib.rgbgr_image\nrgbgr_image.argtypes = [IMAGE]\n\npredict_image = lib.network_predict_image\npredict_image.argtypes = [c_void_p, IMAGE]\npredict_image.restype = POINTER(c_float)\n\n\ndef classify(net, meta, im):\n    out = predict_image(net, im)\n    res = []\n    for i in range(meta.classes):\n        res.append((meta.names[i], out[i]))\n    res = sorted(res, key=lambda x: -x[1])\n    return res\n\n\ndef crop_back(image, bbox, out_sz, padding=0):\n    a = (out_sz[0]-1) / bbox[2]\n    b = (out_sz[1]-1) / bbox[3]\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz[0], out_sz[1]),\n                          flags=cv2.INTER_LANCZOS4,\n                          borderMode=cv2.BORDER_CONSTANT,\n                          borderValue=padding)\n    return crop\n\n\ndef crop_chw(image, bbox, out_sz, padding=0):\n    a = (out_sz-1) / bbox[2]\n    b = (out_sz-1) / bbox[3]\n    c = -a * bbox[0]\n    d = -b * bbox[1]\n    mapping = np.array([[a, 0, c],\n                        [0, b, d]]).astype(np.float)\n    crop = cv2.warpAffine(image, mapping, (out_sz, out_sz),\n                          borderMode=cv2.BORDER_CONSTANT,\n                          borderValue=padding)\n    return np.transpose(crop, (2, 0, 1))\n\n\ndef detect(net, meta, image, thresh=.5, hier_thresh=.5, nms=.45, mask_model=None):\n    im = load_image(image, 0, 0)\n    num = c_int(0)\n    pnum = pointer(num)\n    predict_image(net, im)\n    dets = get_network_boxes(net, im.w, im.h, thresh, hier_thresh, None, 0, pnum)\n    num = pnum[0]\n    if (nms): do_nms_obj(dets, num, meta.classes, nms);\n\n    res = []\n    for j in range(num):\n        for i in range(meta.classes):\n            if dets[j].prob[i] > 0:\n                b = dets[j].bbox\n                res.append((meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h)))\n    res = sorted(res, key=lambda x: -x[1])\n\n    # STAGE2: DeepMask Segmentation\n    im_raw = cv2.imread(image.decode(""utf-8""))\n    im_ = (im_raw[:, :, ::-1]).astype(np.float32) / 255.\n    out_size = 160\n    crop_size = 192\n    context_amount = 224 / 128\n    mean = np.array([[[0.485]], [[0.456]], [[0.406]]]).astype(np.float32)\n    std = np.array([[[0.229]], [[0.224]], [[0.225]]]).astype(np.float32)\n    masks_in_img = []\n    for i, a in enumerate(res):\n        b = a[2]\n        cx, cy, w, h = np.array(b)\n        x = cx - w / 2\n        y = cy - h / 2\n        x, y, w, h = int(x), int(y), int(w), int(h)\n\n        target_pos = np.array([cx, cy])\n        target_sz = np.array([w, h])\n\n        max_sz = max(target_sz)\n\n        s_z = context_amount * max_sz\n        s_z_c = s_z / out_size * crop_size\n        box = [target_pos[0] - s_z / 2, target_pos[1] - s_z / 2, s_z, s_z]\n        box_crop = [target_pos[0] - s_z_c / 2, target_pos[1] - s_z_c / 2, s_z_c, s_z_c]\n        x_crop = crop_chw(im_, box_crop, crop_size, (0.5, 0.5, 0.5))\n\n        x_crop_norm = (x_crop - mean) / std\n        xs_torch = torch.from_numpy(np.expand_dims(x_crop_norm, axis=0))\n\n        mask, score = model(xs_torch.cuda())\n        mask = mask.sigmoid().squeeze().cpu().data.numpy()\n        # score = score.sigmoid().squeeze().cpu().data.numpy()\n\n        im_w, im_h = im_raw.shape[1], im_raw.shape[0]\n        s = out_size / box[2]\n        back_box = [-box[0] * s, -box[1] * s, im_w * s, im_h * s]\n        mask_in_img = crop_back(mask, back_box, (im_w, im_h))\n        masks_in_img.append(mask_in_img)\n\n    if VISUALIZATION:\n        plt.cla()\n        im_save = im_raw\n        plt.imshow(im_save[:, :, ::-1])\n\n        for id in range(len(masks_in_img)):\n            mask = (masks_in_img[id] > 0.3).astype(np.uint8)\n            _, contours, _ = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n            for contour in contours:\n                mvos_color = [\'r\', \'lawngreen\', \'aqua\', \'fuchsia\', \'yellow\', \'blue\'][id%6]\n                edge = patches.Polygon(contour.reshape(-1, 2), linewidth=2, edgecolor=mvos_color,\n                                       facecolor=\'none\')\n                ax.add_patch(edge)\n                edge = patches.Polygon(contour.reshape(-1, 2), linewidth=2, facecolor=mvos_color,\n                                       alpha=0.5)\n                ax.add_patch(edge)\n\n        for id, a in enumerate(res):\n            cx, cy, w, h = np.array(a[2])\n            x = cx - w / 2\n            y = cy - h / 2\n            mvos_color = [\'r\', \'lawngreen\', \'aqua\', \'fuchsia\', \'yellow\', \'blue\'][id % 6]\n            rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=mvos_color, facecolor=\'none\')\n            ax.add_patch(rect)\n\n        plt.axis(\'off\')\n        plt.subplots_adjust(.0, .0, 1, 1)\n        plt.draw()\n        plt.pause(0.5)\n\n    free_image(im)\n    free_detections(dets, num)\n    return res\n\n\nif __name__ == ""__main__"":\n    Config = namedtuple(\'Config\', [\'iSz\', \'oSz\', \'gSz\'])\n    default_config = Config(iSz=160, oSz=56, gSz=160)\n    model = DeepMask(default_config)\n    model = load_pretrain(model, \'./pretrained/deepmask/DeepMask.pth.tar\')\n    model = model.eval().to(\'cuda\')\n\n    # net = load_net(b""./darknet/cfg/yolov3-tiny.cfg"", b""yolov3-tiny.weights"", 0)\n    net = load_net(b""./darknet/cfg/yolov3.cfg"", b""yolov3.weights"", 0)\n    meta = load_meta(b""./darknet/cfg/coco.data"")\n    image_files = glob.glob(\'./data/coco/val2017/*.jpg\')\n    tic = time.time()\n    if VISUALIZATION:\n        fig, ax = plt.subplots(1)\n\n    for i, image_file in enumerate(image_files):\n        r = detect(net, meta, image_file.encode(), mask_model=model)\n        print(r)\n        # plt.savefig(\'%05d.jpg\' % i)\n    toc = time.time() - tic\n    print(\'Speed: {:.1f} FPS and {:.1f}s\'.format(len(image_files)/toc, toc))\n    \n\n'"
utils/__init__.py,0,b''
utils/load_helper.py,4,"b'import torch\nimport logging\nlogger = logging.getLogger(\'global\')\n\n\ndef check_keys(model, pretrained_state_dict):\n    ckpt_keys = set(pretrained_state_dict.keys())\n    model_keys = set(model.state_dict().keys())\n    used_pretrained_keys = model_keys & ckpt_keys\n    unused_pretrained_keys = ckpt_keys - model_keys\n    missing_keys = model_keys - ckpt_keys\n    logger.info(\'missing keys:{}\'.format(len(missing_keys)))\n    logger.info(\'unused checkpoint keys:{}\'.format(len(unused_pretrained_keys)))\n    logger.info(\'used keys:{}\'.format(len(used_pretrained_keys)))\n    assert len(used_pretrained_keys) > 0, \'load NONE from pretrained checkpoint\'\n    return True\n\n\ndef remove_prefix(state_dict, prefix):\n    \'\'\' Old style model is stored with all names of parameters share common prefix \'module.\' \'\'\'\n    logger.info(\'remove prefix \\\'{}\\\'\'.format(prefix))\n    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n    return {f(key): value for key, value in state_dict.items()}\n\n\ndef load_pretrain(model, pretrained_path):\n    logger.info(\'load pretrained model from {}\'.format(pretrained_path))\n    device = torch.cuda.current_device()\n    pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n    if ""state_dict"" in pretrained_dict.keys():\n        pretrained_dict = remove_prefix(pretrained_dict[\'state_dict\'], \'module.\')\n    else:\n        pretrained_dict = remove_prefix(pretrained_dict, \'module.\')\n    check_keys(model, pretrained_dict)\n    model.load_state_dict(pretrained_dict, strict=False)\n    return model\n\n\ndef restore_from(model, optimizer, ckpt_path):\n    logger.info(\'restore from {}\'.format(ckpt_path))\n    device = torch.cuda.current_device()\n    ckpt = torch.load(ckpt_path, map_location=lambda storage, loc: storage.cuda(device))\n    epoch = ckpt[\'epoch\']\n    best_acc = ckpt[\'best_acc\']\n    arch = ckpt[\'arch\']\n    ckpt_model_dict = remove_prefix(ckpt[\'state_dict\'], \'module.\')\n    check_keys(model, ckpt_model_dict)\n    model.load_state_dict(ckpt_model_dict, strict=False)\n\n    optimizer.load_state_dict(ckpt[\'optimizer\'])\n    return model, optimizer, epoch, best_acc, arch\n\n\n'"
utils/log_helper.py,0,"b'#encoding: utf8\nfrom __future__ import division\n\nimport os\nimport logging\nimport math\n\nlogs = set()\n\nclass Filter:\n    def __init__(self, flag):\n        self.flag = flag\n    def filter(self, x): return self.flag\n\n\ndef get_format(logger, level):\n    if \'SLURM_PROCID\' in os.environ:\n        rank = int(os.environ[\'SLURM_PROCID\'])\n\n        if level == logging.INFO:\n            logger.addFilter(Filter(rank == 0))\n    else:\n        rank = 0\n    format_str = \'[%(asctime)s-rk{}-%(filename)s#%(lineno)3d] %(message)s\'.format(rank)\n    formatter = logging.Formatter(format_str)\n    return formatter\n\n\ndef init_log(name, level = logging.INFO):\n    if (name, level) in logs: return\n    logs.add((name, level))\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    formatter = get_format(logger, level)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n\ndef add_file_handler(name, log_file, level = logging.INFO):\n    logger = logging.getLogger(name)\n    fh = logging.FileHandler(log_file)\n    fh.setFormatter(get_format(logger, level))\n    logger.addHandler(fh)\n\n\ninit_log(\'global\')\n# init_log(\'global\', logging.WARN)\n\n\ndef print_speed(i, i_time, n):\n    """"""print_speed(index, index_time, total_iteration)""""""\n    logger = logging.getLogger(\'global\')\n    average_time = i_time\n    remaining_time = (n - i) * average_time\n    remaining_day = math.floor(remaining_time / 86400)\n    remaining_hour = math.floor(remaining_time / 3600 - remaining_day * 24)\n    remaining_min = math.floor(remaining_time / 60 - remaining_day * 1440 - remaining_hour * 60)\n    logger.info(\'Progress: %d / %d [%d%%], Speed: %.3f s/iter, ETA %d:%02d:%02d (D:H:M)\\n\' % (i, n, i/n*100, average_time, remaining_day, remaining_hour, remaining_min))\n\n\ndef main():\n    for i, lvl in enumerate([logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL]):\n        log_name = str(lvl)\n        init_log(log_name, lvl)\n        logger = logging.getLogger(log_name)\n        print(\'****cur lvl:{}\'.format(lvl))\n        logger.debug(\'debug\')\n        logger.info(\'info\')\n        logger.warning(\'warning\')\n        logger.error(\'error\')\n        logger.critical(\'critiacal\')\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
loader/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
loader/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\n\ntry:\n    import matplotlib.pyplot as plt\n    from matplotlib.collections import PatchCollection\n    from matplotlib.patches import Polygon\nexcept Exception as e:\n    print(e)\n\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        #if type(resFile) == str or type(resFile) == unicode:\n        if type(resFile) == str:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m\n'"
loader/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
loader/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\n#import pycocotools._mask as _mask\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]\n'"
loader/pycocotools/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'_mask\',\n        sources=[\'common/maskApi.c\', \'_mask.pyx\'],\n        include_dirs = [np.get_include(), \'common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(name=\'pycocotools\',\n      packages=[\'pycocotools\'],\n      package_dir = {\'pycocotools\': \'.\'},\n      version=\'2.0\',\n      ext_modules=\n          cythonize(ext_modules)\n      )\n'"
