file_path,api_count,code
main.py,13,"b'from __future__ import print_function\nimport os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\nfrom vgg import vgg\nimport shutil\n\n# Training settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR training\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar10\',\n                    help=\'training dataset (default: cifar10)\')\nparser.add_argument(\'--sparsity-regularization\', \'-sr\', dest=\'sr\', action=\'store_true\',\n                    help=\'train with channel sparsity regularization\')\nparser.add_argument(\'--s\', type=float, default=0.0001,\n                    help=\'scale sparse rate (default: 0.0001)\')\nparser.add_argument(\'--refine\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'refine from prune model\')\nparser.add_argument(\'--batch-size\', type=int, default=100, metavar=\'N\',\n                    help=\'input batch size for training (default: 100)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--epochs\', type=int, default=160, metavar=\'N\',\n                    help=\'number of epochs to train (default: 160)\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--lr\', type=float, default=0.1, metavar=\'LR\',\n                    help=\'learning rate (default: 0.1)\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.9)\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=100, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n\nkwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.CIFAR10(\'./data\', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.Pad(4),\n                       transforms.RandomCrop(32),\n                       transforms.RandomHorizontalFlip(),\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                   ])),\n    batch_size=args.batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.CIFAR10(\'./data\', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                   ])),\n    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\nif args.refine:\n    checkpoint = torch.load(args.refine)\n    model = vgg(cfg=checkpoint[\'cfg\'])\n    model.cuda()\n    model.load_state_dict(checkpoint[\'state_dict\'])\nelse:\n    model = vgg()\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\nif args.resume:\n    if os.path.isfile(args.resume):\n        print(""=> loading checkpoint \'{}\'"".format(args.resume))\n        checkpoint = torch.load(args.resume)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.resume, checkpoint[\'epoch\'], best_prec1))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n# additional subgradient descent on the sparsity-induced penalty term\ndef updateBN():\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.weight.grad.data.add_(args.s*torch.sign(m.weight.data))  # L1\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        if args.sr:\n            updateBN()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.1f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.cross_entropy(output, target, size_average=False).data[0] # sum up batch loss\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, \'model_best.pth.tar\')\n\nbest_prec1 = 0.\nfor epoch in range(args.start_epoch, args.epochs):\n    if epoch in [args.epochs*0.5, args.epochs*0.75]:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] *= 0.1\n    train(epoch)\n    prec1 = test()\n    is_best = prec1 > best_prec1\n    best_prec1 = max(prec1, best_prec1)\n    save_checkpoint({\n        \'epoch\': epoch + 1,\n        \'state_dict\': model.state_dict(),\n        \'best_prec1\': best_prec1,\n        \'optimizer\': optimizer.state_dict(),\n    }, is_best)\n'"
prune.py,12,"b'import os\nimport argparse\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\nfrom vgg import vgg\nimport numpy as np\n\n# Prune settings\nparser = argparse.ArgumentParser(description=\'PyTorch Slimming CIFAR prune\')\nparser.add_argument(\'--dataset\', type=str, default=\'cifar10\',\n                    help=\'training dataset (default: cifar10)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'disables CUDA training\')\nparser.add_argument(\'--percent\', type=float, default=0.5,\n                    help=\'scale sparse rate (default: 0.5)\')\nparser.add_argument(\'--model\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to raw trained model (default: none)\')\nparser.add_argument(\'--save\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to save prune model (default: none)\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\nmodel = vgg()\nif args.cuda:\n    model.cuda()\nif args.model:\n    if os.path.isfile(args.model):\n        print(""=> loading checkpoint \'{}\'"".format(args.model))\n        checkpoint = torch.load(args.model)\n        args.start_epoch = checkpoint[\'epoch\']\n        best_prec1 = checkpoint[\'best_prec1\']\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        print(""=> loaded checkpoint \'{}\' (epoch {}) Prec1: {:f}""\n              .format(args.model, checkpoint[\'epoch\'], best_prec1))\n    else:\n        print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\nprint(model)\ntotal = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        total += m.weight.data.shape[0]\n\nbn = torch.zeros(total)\nindex = 0\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n        size = m.weight.data.shape[0]\n        bn[index:(index+size)] = m.weight.data.abs().clone()\n        index += size\n\ny, i = torch.sort(bn)\nthre_index = int(total * args.percent)\nthre = y[thre_index]\n\npruned = 0\ncfg = []\ncfg_mask = []\nfor k, m in enumerate(model.modules()):\n    if isinstance(m, nn.BatchNorm2d):\n        weight_copy = m.weight.data.clone()\n        mask = weight_copy.abs().gt(thre).float().cuda()\n        pruned = pruned + mask.shape[0] - torch.sum(mask)\n        m.weight.data.mul_(mask)\n        m.bias.data.mul_(mask)\n        cfg.append(int(torch.sum(mask)))\n        cfg_mask.append(mask.clone())\n        print(\'layer index: {:d} \\t total channel: {:d} \\t remaining channel: {:d}\'.\n            format(k, mask.shape[0], int(torch.sum(mask))))\n    elif isinstance(m, nn.MaxPool2d):\n        cfg.append(\'M\')\n\npruned_ratio = pruned/total\n\nprint(\'Pre-processing Successful!\')\n\n\n# simple test model after Pre-processing prune (simple set BN scales to zeros)\ndef test():\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data\', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n    model.eval()\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    print(\'\\nTest set: Accuracy: {}/{} ({:.1f}%)\\n\'.format(\n        correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n    return correct / float(len(test_loader.dataset))\n\ntest()\n\n\n# Make real prune\nprint(cfg)\nnewmodel = vgg(cfg=cfg)\nnewmodel.cuda()\n\nlayer_id_in_cfg = 0\nstart_mask = torch.ones(3)\nend_mask = cfg_mask[layer_id_in_cfg]\nfor [m0, m1] in zip(model.modules(), newmodel.modules()):\n    if isinstance(m0, nn.BatchNorm2d):\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        m1.weight.data = m0.weight.data[idx1].clone()\n        m1.bias.data = m0.bias.data[idx1].clone()\n        m1.running_mean = m0.running_mean[idx1].clone()\n        m1.running_var = m0.running_var[idx1].clone()\n        layer_id_in_cfg += 1\n        start_mask = end_mask.clone()\n        if layer_id_in_cfg < len(cfg_mask):  # do not change in Final FC\n            end_mask = cfg_mask[layer_id_in_cfg]\n    elif isinstance(m0, nn.Conv2d):\n        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n        idx1 = np.squeeze(np.argwhere(np.asarray(end_mask.cpu().numpy())))\n        print(\'In shape: {:d} Out shape:{:d}\'.format(idx0.shape[0], idx1.shape[0]))\n        w = m0.weight.data[:, idx0, :, :].clone()\n        w = w[idx1, :, :, :].clone()\n        m1.weight.data = w.clone()\n        # m1.bias.data = m0.bias.data[idx1].clone()\n    elif isinstance(m0, nn.Linear):\n        idx0 = np.squeeze(np.argwhere(np.asarray(start_mask.cpu().numpy())))\n        m1.weight.data = m0.weight.data[:, idx0].clone()\n\n\ntorch.save({\'cfg\': cfg, \'state_dict\': newmodel.state_dict()}, args.save)\n\nprint(newmodel)\nmodel = newmodel\ntest()'"
vgg.py,3,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport math  # init\n\n\nclass vgg(nn.Module):\n\n    def __init__(self, dataset='cifar10', init_weights=True, cfg=None):\n        super(vgg, self).__init__()\n        if cfg is None:\n            cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512]\n        self.feature = self.make_layers(cfg, True)\n\n        if dataset == 'cifar100':\n            num_classes = 100\n        elif dataset == 'cifar10':\n            num_classes = 10\n        self.classifier = nn.Linear(cfg[-1], num_classes)\n        if init_weights:\n            self._initialize_weights()\n\n    def make_layers(self, cfg, batch_norm=False):\n        layers = []\n        in_channels = 3\n        for v in cfg:\n            if v == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n                if batch_norm:\n                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = v\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.feature(x)\n        x = nn.AvgPool2d(2)(x)\n        x = x.view(x.size(0), -1)\n        y = self.classifier(x)\n        return y\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\nif __name__ == '__main__':\n    net = vgg()\n    x = Variable(torch.FloatTensor(16, 3, 40, 40))\n    y = net(x)\n    print(y.data.shape)\n\n"""
